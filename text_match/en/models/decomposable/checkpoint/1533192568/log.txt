Train: 2018-08-02T14:49:33.539281: step 1, loss 7.82245.
Train: 2018-08-02T14:49:33.617303: step 2, loss 5.5406.
Train: 2018-08-02T14:49:33.679758: step 3, loss 5.16283.
Train: 2018-08-02T14:49:33.742273: step 4, loss 4.40729.
Train: 2018-08-02T14:49:33.804759: step 5, loss 3.65176.
Train: 2018-08-02T14:49:33.867214: step 6, loss 3.9036.
Train: 2018-08-02T14:49:33.914110: step 7, loss 3.65176.
Train: 2018-08-02T14:49:33.976594: step 8, loss 3.9036.
Train: 2018-08-02T14:49:34.039049: step 9, loss 3.65176.
Train: 2018-08-02T14:49:34.101564: step 10, loss 3.14807.
Test: 2018-08-02T14:49:34.476471: step 10, loss 3.81863.
Train: 2018-08-02T14:49:34.538962: step 11, loss 4.78506.
Train: 2018-08-02T14:49:34.601418: step 12, loss 3.65176.
Train: 2018-08-02T14:49:34.663902: step 13, loss 3.77768.
Train: 2018-08-02T14:49:34.710799: step 14, loss 3.65176.
Train: 2018-08-02T14:49:34.773252: step 15, loss 3.9036.
Train: 2018-08-02T14:49:34.835767: step 16, loss 4.40729.
Train: 2018-08-02T14:49:34.882634: step 17, loss 3.9036.
Train: 2018-08-02T14:49:34.945087: step 18, loss 4.15545.
Train: 2018-08-02T14:49:35.007603: step 19, loss 2.51845.
Train: 2018-08-02T14:49:35.070087: step 20, loss 4.02952.
Test: 2018-08-02T14:49:35.257543: step 20, loss 3.81863.
Train: 2018-08-02T14:49:35.304378: step 21, loss 4.15545.
Train: 2018-08-02T14:49:35.366862: step 22, loss 3.9036.
Train: 2018-08-02T14:49:35.429378: step 23, loss 4.15545.
Train: 2018-08-02T14:49:35.491835: step 24, loss 4.28137.
Train: 2018-08-02T14:49:35.538698: step 25, loss 4.65914.
Train: 2018-08-02T14:49:35.601213: step 26, loss 3.65176.
Train: 2018-08-02T14:49:35.648078: step 27, loss 4.02952.
Train: 2018-08-02T14:49:35.710562: step 28, loss 4.02952.
Train: 2018-08-02T14:49:35.773054: step 29, loss 3.39991.
Train: 2018-08-02T14:49:35.835533: step 30, loss 4.91098.
Test: 2018-08-02T14:49:36.022990: step 30, loss 3.81863.
Train: 2018-08-02T14:49:36.085445: step 31, loss 4.15545.
Train: 2018-08-02T14:49:36.147960: step 32, loss 4.53321.
Train: 2018-08-02T14:49:36.194824: step 33, loss 3.39991.
Train: 2018-08-02T14:49:36.257281: step 34, loss 5.16283.
Train: 2018-08-02T14:49:36.319794: step 35, loss 4.65914.
Train: 2018-08-02T14:49:36.366659: step 36, loss 4.02952.
Train: 2018-08-02T14:49:36.429117: step 37, loss 4.91098.
Train: 2018-08-02T14:49:36.507224: step 38, loss 2.89622.
Train: 2018-08-02T14:49:36.554111: step 39, loss 3.77768.
Train: 2018-08-02T14:49:36.616570: step 40, loss 3.52583.
Test: 2018-08-02T14:49:36.818567: step 40, loss 3.81863.
Train: 2018-08-02T14:49:36.865433: step 41, loss 3.39991.
Train: 2018-08-02T14:49:36.927947: step 42, loss 4.02952.
Train: 2018-08-02T14:49:36.990403: step 43, loss 3.77768.
Train: 2018-08-02T14:49:37.052889: step 44, loss 3.14807.
Train: 2018-08-02T14:49:37.099782: step 45, loss 2.64437.
Train: 2018-08-02T14:49:37.162261: step 46, loss 4.40729.
Train: 2018-08-02T14:49:37.224753: step 47, loss 3.27399.
Train: 2018-08-02T14:49:37.271586: step 48, loss 3.14807.
Train: 2018-08-02T14:49:37.334072: step 49, loss 4.78506.
Train: 2018-08-02T14:49:37.380936: step 50, loss 4.15545.
Test: 2018-08-02T14:49:37.584037: step 50, loss 3.81863.
Train: 2018-08-02T14:49:37.646525: step 51, loss 3.39991.
Train: 2018-08-02T14:49:37.709014: step 52, loss 3.65176.
Train: 2018-08-02T14:49:37.755879: step 53, loss 3.27922.
Train: 2018-08-02T14:49:37.818368: step 54, loss 4.02952.
Train: 2018-08-02T14:49:37.880850: step 55, loss 5.0369.
Train: 2018-08-02T14:49:37.943338: step 56, loss 2.89622.
Train: 2018-08-02T14:49:37.990186: step 57, loss 4.41249.
Train: 2018-08-02T14:49:38.052684: step 58, loss 3.27399.
Train: 2018-08-02T14:49:38.115172: step 59, loss 4.40729.
Train: 2018-08-02T14:49:38.162004: step 60, loss 3.14807.
Test: 2018-08-02T14:49:38.365081: step 60, loss 3.81863.
Train: 2018-08-02T14:49:38.427595: step 61, loss 4.53321.
Train: 2018-08-02T14:49:38.474431: step 62, loss 4.28137.
Train: 2018-08-02T14:49:38.536946: step 63, loss 4.02952.
Train: 2018-08-02T14:49:38.599431: step 64, loss 3.9036.
Train: 2018-08-02T14:49:38.661919: step 65, loss 3.9036.
Train: 2018-08-02T14:49:38.708751: step 66, loss 4.15545.
Train: 2018-08-02T14:49:38.771266: step 67, loss 4.02952.
Train: 2018-08-02T14:49:38.818099: step 68, loss 5.0369.
Train: 2018-08-02T14:49:38.880616: step 69, loss 4.15545.
Train: 2018-08-02T14:49:38.943070: step 70, loss 3.14807.
Test: 2018-08-02T14:49:39.130550: step 70, loss 3.81863.
Train: 2018-08-02T14:49:39.193045: step 71, loss 4.91098.
Train: 2018-08-02T14:49:39.255498: step 72, loss 3.77768.
Train: 2018-08-02T14:49:39.318016: step 73, loss 3.77768.
Train: 2018-08-02T14:49:39.364878: step 74, loss 4.65914.
Train: 2018-08-02T14:49:39.427333: step 75, loss 5.41467.
Train: 2018-08-02T14:49:39.489816: step 76, loss 4.78506.
Train: 2018-08-02T14:49:39.536716: step 77, loss 4.53321.
Train: 2018-08-02T14:49:39.599167: step 78, loss 2.7703.
Train: 2018-08-02T14:49:39.661677: step 79, loss 3.77768.
Train: 2018-08-02T14:49:39.708546: step 80, loss 3.39991.
Test: 2018-08-02T14:49:39.911592: step 80, loss 3.81863.
Train: 2018-08-02T14:49:39.974109: step 81, loss 4.53321.
Train: 2018-08-02T14:49:40.020942: step 82, loss 4.40729.
Train: 2018-08-02T14:49:40.083427: step 83, loss 4.28137.
Train: 2018-08-02T14:49:40.145944: step 84, loss 3.9036.
Train: 2018-08-02T14:49:40.208430: step 85, loss 4.02952.
Train: 2018-08-02T14:49:40.255262: step 86, loss 3.52583.
Train: 2018-08-02T14:49:40.317781: step 87, loss 4.65914.
Train: 2018-08-02T14:49:40.380259: step 88, loss 4.28137.
Train: 2018-08-02T14:49:40.427128: step 89, loss 3.77768.
Train: 2018-08-02T14:49:40.489583: step 90, loss 4.02952.
Test: 2018-08-02T14:49:40.692659: step 90, loss 3.81863.
Train: 2018-08-02T14:49:40.739551: step 91, loss 4.78506.
Train: 2018-08-02T14:49:40.802038: step 92, loss 3.65176.
Train: 2018-08-02T14:49:40.864519: step 93, loss 4.91098.
Train: 2018-08-02T14:49:40.927015: step 94, loss 5.79244.
Train: 2018-08-02T14:49:40.973875: step 95, loss 3.27399.
Train: 2018-08-02T14:49:41.036360: step 96, loss 3.9036.
Train: 2018-08-02T14:49:41.098840: step 97, loss 4.28137.
Train: 2018-08-02T14:49:41.145710: step 98, loss 3.9036.
Train: 2018-08-02T14:49:41.208189: step 99, loss 4.40729.
Train: 2018-08-02T14:49:41.270650: step 100, loss 3.9036.
Test: 2018-08-02T14:49:41.458130: step 100, loss 3.81863.
Train: 2018-08-02T14:49:42.082991: step 101, loss 4.15545.
Train: 2018-08-02T14:49:42.145474: step 102, loss 2.7703.
Train: 2018-08-02T14:49:42.192310: step 103, loss 3.9036.
Train: 2018-08-02T14:49:42.270440: step 104, loss 4.03094.
Train: 2018-08-02T14:49:42.317314: step 105, loss 3.77768.
Train: 2018-08-02T14:49:42.379795: step 106, loss 3.9036.
Train: 2018-08-02T14:49:42.442279: step 107, loss 3.9036.
Train: 2018-08-02T14:49:42.489140: step 108, loss 3.9036.
Train: 2018-08-02T14:49:42.551600: step 109, loss 5.16283.
Train: 2018-08-02T14:49:42.614111: step 110, loss 4.02959.
Test: 2018-08-02T14:49:42.801540: step 110, loss 3.81863.
Train: 2018-08-02T14:49:42.864057: step 111, loss 4.28137.
Train: 2018-08-02T14:49:42.926539: step 112, loss 4.65914.
Train: 2018-08-02T14:49:42.973407: step 113, loss 3.77768.
Train: 2018-08-02T14:49:43.035890: step 114, loss 5.5406.
Train: 2018-08-02T14:49:43.098378: step 115, loss 4.40729.
Train: 2018-08-02T14:49:43.145210: step 116, loss 4.15545.
Train: 2018-08-02T14:49:43.207696: step 117, loss 4.78506.
Train: 2018-08-02T14:49:43.270208: step 118, loss 4.28137.
Train: 2018-08-02T14:49:43.332668: step 119, loss 5.28875.
Train: 2018-08-02T14:49:43.379556: step 120, loss 3.65176.
Test: 2018-08-02T14:49:43.582607: step 120, loss 3.81863.
Train: 2018-08-02T14:49:43.629503: step 121, loss 4.15545.
Train: 2018-08-02T14:49:43.691958: step 122, loss 4.91098.
Train: 2018-08-02T14:49:43.754474: step 123, loss 3.14807.
Train: 2018-08-02T14:49:43.816964: step 124, loss 4.28137.
Train: 2018-08-02T14:49:43.863793: step 125, loss 3.9036.
Train: 2018-08-02T14:49:43.926305: step 126, loss 3.14807.
Train: 2018-08-02T14:49:43.973141: step 127, loss 5.03691.
Train: 2018-08-02T14:49:44.051263: step 128, loss 3.14807.
Train: 2018-08-02T14:49:44.098113: step 129, loss 3.02214.
Train: 2018-08-02T14:49:44.160630: step 130, loss 3.77768.
Test: 2018-08-02T14:49:44.348085: step 130, loss 3.81863.
Train: 2018-08-02T14:49:44.410572: step 131, loss 3.14807.
Train: 2018-08-02T14:49:44.473025: step 132, loss 4.91098.
Train: 2018-08-02T14:49:44.535543: step 133, loss 4.15545.
Train: 2018-08-02T14:49:44.582406: step 134, loss 3.9036.
Train: 2018-08-02T14:49:44.644886: step 135, loss 4.65914.
Train: 2018-08-02T14:49:44.707375: step 136, loss 3.9036.
Train: 2018-08-02T14:49:44.754226: step 137, loss 3.9036.
Train: 2018-08-02T14:49:44.816724: step 138, loss 4.65914.
Train: 2018-08-02T14:49:44.879211: step 139, loss 3.77768.
Train: 2018-08-02T14:49:44.926044: step 140, loss 4.53321.
Test: 2018-08-02T14:49:45.129147: step 140, loss 3.81863.
Train: 2018-08-02T14:49:45.191640: step 141, loss 4.65914.
Train: 2018-08-02T14:49:45.238495: step 142, loss 3.77768.
Train: 2018-08-02T14:49:45.300984: step 143, loss 3.39991.
Train: 2018-08-02T14:49:45.363442: step 144, loss 3.52583.
Train: 2018-08-02T14:49:45.425962: step 145, loss 3.27399.
Train: 2018-08-02T14:49:45.472821: step 146, loss 4.15545.
Train: 2018-08-02T14:49:45.535277: step 147, loss 3.52583.
Train: 2018-08-02T14:49:45.597793: step 148, loss 3.14807.
Train: 2018-08-02T14:49:45.644625: step 149, loss 4.40729.
Train: 2018-08-02T14:49:45.707135: step 150, loss 4.28137.
Test: 2018-08-02T14:49:45.910188: step 150, loss 3.81863.
Train: 2018-08-02T14:49:45.941431: step 151, loss 2.68635.
Train: 2018-08-02T14:49:46.003947: step 152, loss 4.15545.
Train: 2018-08-02T14:49:46.066431: step 153, loss 3.78103.
Train: 2018-08-02T14:49:46.128889: step 154, loss 4.40729.
Train: 2018-08-02T14:49:46.175779: step 155, loss 5.41467.
Train: 2018-08-02T14:49:46.238236: step 156, loss 4.65914.
Train: 2018-08-02T14:49:46.300755: step 157, loss 4.91098.
Train: 2018-08-02T14:49:46.347587: step 158, loss 3.52583.
Train: 2018-08-02T14:49:46.410072: step 159, loss 4.28137.
Train: 2018-08-02T14:49:46.472587: step 160, loss 3.9036.
Test: 2018-08-02T14:49:46.660038: step 160, loss 3.81863.
Train: 2018-08-02T14:49:46.722497: step 161, loss 4.15545.
Train: 2018-08-02T14:49:46.784983: step 162, loss 3.9036.
Train: 2018-08-02T14:49:46.831873: step 163, loss 3.9036.
Train: 2018-08-02T14:49:46.894335: step 164, loss 4.91098.
Train: 2018-08-02T14:49:46.956820: step 165, loss 4.02952.
Train: 2018-08-02T14:49:47.003682: step 166, loss 4.65914.
Train: 2018-08-02T14:49:47.066202: step 167, loss 4.65914.
Train: 2018-08-02T14:49:47.113066: step 168, loss 3.39991.
Train: 2018-08-02T14:49:47.175517: step 169, loss 5.0369.
Train: 2018-08-02T14:49:47.238002: step 170, loss 4.78506.
Test: 2018-08-02T14:49:47.425482: step 170, loss 3.81863.
Train: 2018-08-02T14:49:47.487977: step 171, loss 3.27399.
Train: 2018-08-02T14:49:47.534808: step 172, loss 4.28137.
Train: 2018-08-02T14:49:47.597324: step 173, loss 3.65176.
Train: 2018-08-02T14:49:47.659780: step 174, loss 4.02952.
Train: 2018-08-02T14:49:47.722296: step 175, loss 4.53321.
Train: 2018-08-02T14:49:47.769154: step 176, loss 4.53321.
Train: 2018-08-02T14:49:47.831644: step 177, loss 3.65176.
Train: 2018-08-02T14:49:47.894099: step 178, loss 4.15545.
Train: 2018-08-02T14:49:47.940997: step 179, loss 4.28137.
Train: 2018-08-02T14:49:48.003449: step 180, loss 4.53321.
Test: 2018-08-02T14:49:48.190906: step 180, loss 3.81863.
Train: 2018-08-02T14:49:48.253390: step 181, loss 2.51845.
Train: 2018-08-02T14:49:48.315909: step 182, loss 5.28875.
Train: 2018-08-02T14:49:48.378390: step 183, loss 3.65176.
Train: 2018-08-02T14:49:48.440880: step 184, loss 4.40729.
Train: 2018-08-02T14:49:48.487741: step 185, loss 3.14807.
Train: 2018-08-02T14:49:48.550196: step 186, loss 4.15545.
Train: 2018-08-02T14:49:48.597090: step 187, loss 3.9036.
Train: 2018-08-02T14:49:48.659574: step 188, loss 2.64438.
Train: 2018-08-02T14:49:48.722060: step 189, loss 3.14807.
Train: 2018-08-02T14:49:48.768894: step 190, loss 4.15545.
Test: 2018-08-02T14:49:48.971996: step 190, loss 3.81863.
Train: 2018-08-02T14:49:49.034458: step 191, loss 4.40729.
Train: 2018-08-02T14:49:49.096969: step 192, loss 4.28137.
Train: 2018-08-02T14:49:49.143837: step 193, loss 5.5406.
Train: 2018-08-02T14:49:49.206323: step 194, loss 3.9036.
Train: 2018-08-02T14:49:49.268808: step 195, loss 4.65914.
Train: 2018-08-02T14:49:49.315670: step 196, loss 4.65914.
Train: 2018-08-02T14:49:49.378156: step 197, loss 3.14807.
Train: 2018-08-02T14:49:49.440613: step 198, loss 4.91098.
Train: 2018-08-02T14:49:49.487507: step 199, loss 3.27399.
Train: 2018-08-02T14:49:49.549991: step 200, loss 3.77768.
Test: 2018-08-02T14:49:49.753040: step 200, loss 3.81863.
Train: 2018-08-02T14:49:50.206088: step 201, loss 3.9036.
Train: 2018-08-02T14:49:50.268573: step 202, loss 4.53321.
Train: 2018-08-02T14:49:50.331058: step 203, loss 3.65176.
Train: 2018-08-02T14:49:50.393514: step 204, loss 3.9036.
Train: 2018-08-02T14:49:50.440379: step 205, loss 3.14807.
Train: 2018-08-02T14:49:50.502864: step 206, loss 4.40729.
Train: 2018-08-02T14:49:50.549756: step 207, loss 4.02952.
Train: 2018-08-02T14:49:50.612243: step 208, loss 4.91098.
Train: 2018-08-02T14:49:50.674725: step 209, loss 4.78506.
Train: 2018-08-02T14:49:50.737182: step 210, loss 4.40729.
Test: 2018-08-02T14:49:50.924639: step 210, loss 3.81863.
Train: 2018-08-02T14:49:50.987126: step 211, loss 3.77768.
Train: 2018-08-02T14:49:51.049610: step 212, loss 3.27399.
Train: 2018-08-02T14:49:51.096504: step 213, loss 3.77768.
Train: 2018-08-02T14:49:51.158968: step 214, loss 4.91098.
Train: 2018-08-02T14:49:51.221476: step 215, loss 4.40729.
Train: 2018-08-02T14:49:51.283933: step 216, loss 4.40729.
Train: 2018-08-02T14:49:51.330825: step 217, loss 4.15545.
Train: 2018-08-02T14:49:51.393281: step 218, loss 4.65914.
Train: 2018-08-02T14:49:51.455765: step 219, loss 4.53321.
Train: 2018-08-02T14:49:51.502654: step 220, loss 3.65176.
Test: 2018-08-02T14:49:51.705705: step 220, loss 3.81863.
Train: 2018-08-02T14:49:51.768221: step 221, loss 3.65176.
Train: 2018-08-02T14:49:51.815096: step 222, loss 3.9036.
Train: 2018-08-02T14:49:51.877541: step 223, loss 4.78506.
Train: 2018-08-02T14:49:51.940061: step 224, loss 4.40729.
Train: 2018-08-02T14:49:52.002543: step 225, loss 4.02952.
Train: 2018-08-02T14:49:52.049377: step 226, loss 3.52583.
Train: 2018-08-02T14:49:52.111892: step 227, loss 4.53321.
Train: 2018-08-02T14:49:52.174348: step 228, loss 3.14807.
Train: 2018-08-02T14:49:52.221241: step 229, loss 4.02952.
Train: 2018-08-02T14:49:52.283697: step 230, loss 4.15545.
Test: 2018-08-02T14:49:52.471151: step 230, loss 3.81863.
Train: 2018-08-02T14:49:52.533671: step 231, loss 4.28137.
Train: 2018-08-02T14:49:52.596147: step 232, loss 3.39991.
Train: 2018-08-02T14:49:52.658639: step 233, loss 3.77768.
Train: 2018-08-02T14:49:52.705503: step 234, loss 4.40729.
Train: 2018-08-02T14:49:52.767988: step 235, loss 4.40729.
Train: 2018-08-02T14:49:52.830469: step 236, loss 4.15545.
Train: 2018-08-02T14:49:52.877333: step 237, loss 3.65176.
Train: 2018-08-02T14:49:52.939828: step 238, loss 4.15545.
Train: 2018-08-02T14:49:53.002309: step 239, loss 4.65914.
Train: 2018-08-02T14:49:53.049142: step 240, loss 3.39991.
Test: 2018-08-02T14:49:53.252249: step 240, loss 3.81863.
Train: 2018-08-02T14:49:53.314733: step 241, loss 4.65914.
Train: 2018-08-02T14:49:53.361599: step 242, loss 4.02952.
Train: 2018-08-02T14:49:53.424054: step 243, loss 3.14807.
Train: 2018-08-02T14:49:53.486541: step 244, loss 4.40729.
Train: 2018-08-02T14:49:53.549056: step 245, loss 4.40729.
Train: 2018-08-02T14:49:53.595890: step 246, loss 5.16283.
Train: 2018-08-02T14:49:53.658404: step 247, loss 4.15545.
Train: 2018-08-02T14:49:53.705268: step 248, loss 4.53321.
Train: 2018-08-02T14:49:53.767753: step 249, loss 3.77768.
Train: 2018-08-02T14:49:53.830208: step 250, loss 4.28137.
Test: 2018-08-02T14:49:54.017664: step 250, loss 3.81863.
Train: 2018-08-02T14:49:54.080151: step 251, loss 4.28137.
Train: 2018-08-02T14:49:54.142637: step 252, loss 3.39991.
Train: 2018-08-02T14:49:54.205121: step 253, loss 3.52583.
Train: 2018-08-02T14:49:54.251987: step 254, loss 3.39991.
Train: 2018-08-02T14:49:54.314471: step 255, loss 4.40729.
Train: 2018-08-02T14:49:54.376957: step 256, loss 3.02214.
Train: 2018-08-02T14:49:54.423848: step 257, loss 2.89622.
Train: 2018-08-02T14:49:54.486330: step 258, loss 4.15545.
Train: 2018-08-02T14:49:54.548821: step 259, loss 3.39991.
Train: 2018-08-02T14:49:54.611306: step 260, loss 4.65914.
Test: 2018-08-02T14:49:54.798732: step 260, loss 3.81863.
Train: 2018-08-02T14:49:54.861250: step 261, loss 3.9036.
Train: 2018-08-02T14:49:54.923731: step 262, loss 3.14807.
Train: 2018-08-02T14:49:54.986188: step 263, loss 3.27399.
Train: 2018-08-02T14:49:55.033078: step 264, loss 4.28137.
Train: 2018-08-02T14:49:55.095569: step 265, loss 3.77768.
Train: 2018-08-02T14:49:55.158023: step 266, loss 4.65914.
Train: 2018-08-02T14:49:55.204913: step 267, loss 3.52583.
Train: 2018-08-02T14:49:55.267376: step 268, loss 3.52583.
Train: 2018-08-02T14:49:55.329888: step 269, loss 4.02952.
Train: 2018-08-02T14:49:55.376722: step 270, loss 3.9036.
Test: 2018-08-02T14:49:55.579829: step 270, loss 3.81863.
Train: 2018-08-02T14:49:55.642284: step 271, loss 4.40729.
Train: 2018-08-02T14:49:55.689149: step 272, loss 4.28137.
Train: 2018-08-02T14:49:55.751659: step 273, loss 2.89622.
Train: 2018-08-02T14:49:55.814153: step 274, loss 4.15545.
Train: 2018-08-02T14:49:55.876605: step 275, loss 4.40729.
Train: 2018-08-02T14:49:55.923496: step 276, loss 3.27399.
Train: 2018-08-02T14:49:55.985955: step 277, loss 3.9036.
Train: 2018-08-02T14:49:56.048468: step 278, loss 4.02952.
Train: 2018-08-02T14:49:56.095334: step 279, loss 3.52583.
Train: 2018-08-02T14:49:56.157788: step 280, loss 3.9036.
Test: 2018-08-02T14:49:56.360866: step 280, loss 3.81863.
Train: 2018-08-02T14:49:56.407764: step 281, loss 3.65176.
Train: 2018-08-02T14:49:56.470216: step 282, loss 4.15545.
Train: 2018-08-02T14:49:56.532734: step 283, loss 3.77768.
Train: 2018-08-02T14:49:56.595219: step 284, loss 3.02214.
Train: 2018-08-02T14:49:56.642082: step 285, loss 3.9036.
Train: 2018-08-02T14:49:56.704536: step 286, loss 3.14807.
Train: 2018-08-02T14:49:56.767051: step 287, loss 4.53321.
Train: 2018-08-02T14:49:56.813918: step 288, loss 2.64437.
Train: 2018-08-02T14:49:56.876370: step 289, loss 4.28137.
Train: 2018-08-02T14:49:56.938889: step 290, loss 3.9036.
Test: 2018-08-02T14:49:57.126336: step 290, loss 3.81863.
Train: 2018-08-02T14:49:57.188830: step 291, loss 3.9036.
Train: 2018-08-02T14:49:57.251313: step 292, loss 4.02952.
Train: 2018-08-02T14:49:57.298147: step 293, loss 4.15545.
Train: 2018-08-02T14:49:57.360663: step 294, loss 4.65914.
Train: 2018-08-02T14:49:57.423118: step 295, loss 3.52583.
Train: 2018-08-02T14:49:57.485628: step 296, loss 3.39991.
Train: 2018-08-02T14:49:57.532467: step 297, loss 3.52583.
Train: 2018-08-02T14:49:57.594953: step 298, loss 3.77768.
Train: 2018-08-02T14:49:57.657439: step 299, loss 3.65176.
Train: 2018-08-02T14:49:57.704331: step 300, loss 3.9036.
Test: 2018-08-02T14:49:57.907404: step 300, loss 3.81863.
Train: 2018-08-02T14:49:58.469748: step 301, loss 3.52583.
Train: 2018-08-02T14:49:58.516612: step 302, loss 3.49225.
Train: 2018-08-02T14:49:58.579126: step 303, loss 4.02952.
Train: 2018-08-02T14:49:58.641584: step 304, loss 5.5406.
Train: 2018-08-02T14:49:58.704099: step 305, loss 3.77768.
Train: 2018-08-02T14:49:58.750932: step 306, loss 3.9036.
Train: 2018-08-02T14:49:58.813451: step 307, loss 4.91098.
Train: 2018-08-02T14:49:58.875933: step 308, loss 3.65176.
Train: 2018-08-02T14:49:58.938389: step 309, loss 3.65176.
Train: 2018-08-02T14:49:58.985282: step 310, loss 2.7703.
Test: 2018-08-02T14:49:59.188368: step 310, loss 3.81863.
Train: 2018-08-02T14:49:59.250814: step 311, loss 5.28875.
Train: 2018-08-02T14:49:59.297706: step 312, loss 3.65176.
Train: 2018-08-02T14:49:59.360195: step 313, loss 3.77768.
Train: 2018-08-02T14:49:59.422682: step 314, loss 3.65176.
Train: 2018-08-02T14:49:59.469543: step 315, loss 3.39991.
Train: 2018-08-02T14:49:59.532031: step 316, loss 3.9036.
Train: 2018-08-02T14:49:59.594514: step 317, loss 3.9036.
Train: 2018-08-02T14:49:59.641349: step 318, loss 3.9036.
Train: 2018-08-02T14:49:59.703833: step 319, loss 5.41467.
Train: 2018-08-02T14:49:59.766319: step 320, loss 3.39991.
Test: 2018-08-02T14:49:59.953798: step 320, loss 3.81863.
Train: 2018-08-02T14:50:00.016260: step 321, loss 4.53321.
Train: 2018-08-02T14:50:00.078770: step 322, loss 3.9036.
Train: 2018-08-02T14:50:00.141230: step 323, loss 4.65914.
Train: 2018-08-02T14:50:00.203747: step 324, loss 3.02214.
Train: 2018-08-02T14:50:00.250613: step 325, loss 3.02214.
Train: 2018-08-02T14:50:00.313066: step 326, loss 3.65176.
Train: 2018-08-02T14:50:00.375577: step 327, loss 3.9036.
Train: 2018-08-02T14:50:00.422443: step 328, loss 4.40729.
Train: 2018-08-02T14:50:00.484926: step 329, loss 4.53321.
Train: 2018-08-02T14:50:00.547415: step 330, loss 4.02952.
Test: 2018-08-02T14:50:00.734866: step 330, loss 3.81863.
Train: 2018-08-02T14:50:00.797327: step 331, loss 4.65914.
Train: 2018-08-02T14:50:00.859813: step 332, loss 3.39991.
Train: 2018-08-02T14:50:00.906677: step 333, loss 4.15545.
Train: 2018-08-02T14:50:00.969194: step 334, loss 4.40729.
Train: 2018-08-02T14:50:01.031680: step 335, loss 4.02952.
Train: 2018-08-02T14:50:01.094134: step 336, loss 4.15545.
Train: 2018-08-02T14:50:01.140996: step 337, loss 2.7703.
Train: 2018-08-02T14:50:01.203513: step 338, loss 3.14807.
Train: 2018-08-02T14:50:01.265988: step 339, loss 4.15545.
Train: 2018-08-02T14:50:01.312866: step 340, loss 4.15545.
Test: 2018-08-02T14:50:01.515933: step 340, loss 3.81863.
Train: 2018-08-02T14:50:01.578424: step 341, loss 3.39991.
Train: 2018-08-02T14:50:01.640879: step 342, loss 3.14807.
Train: 2018-08-02T14:50:01.687772: step 343, loss 3.9036.
Train: 2018-08-02T14:50:01.750260: step 344, loss 4.28137.
Train: 2018-08-02T14:50:01.812749: step 345, loss 4.15545.
Train: 2018-08-02T14:50:01.859609: step 346, loss 4.15545.
Train: 2018-08-02T14:50:01.922063: step 347, loss 4.02952.
Train: 2018-08-02T14:50:01.984576: step 348, loss 3.52583.
Train: 2018-08-02T14:50:02.031439: step 349, loss 3.77768.
Train: 2018-08-02T14:50:02.093929: step 350, loss 3.27399.
Test: 2018-08-02T14:50:02.297000: step 350, loss 3.81863.
Train: 2018-08-02T14:50:02.359461: step 351, loss 3.77768.
Train: 2018-08-02T14:50:02.406358: step 352, loss 3.9036.
Train: 2018-08-02T14:50:02.468810: step 353, loss 4.15545.
Train: 2018-08-02T14:50:02.531330: step 354, loss 4.02952.
Train: 2018-08-02T14:50:02.593807: step 355, loss 4.15545.
Train: 2018-08-02T14:50:02.640676: step 356, loss 3.02214.
Train: 2018-08-02T14:50:02.703161: step 357, loss 3.52583.
Train: 2018-08-02T14:50:02.765618: step 358, loss 3.77768.
Train: 2018-08-02T14:50:02.812480: step 359, loss 3.77768.
Train: 2018-08-02T14:50:02.875002: step 360, loss 3.52583.
Test: 2018-08-02T14:50:03.062455: step 360, loss 3.81863.
Train: 2018-08-02T14:50:03.124937: step 361, loss 4.65914.
Train: 2018-08-02T14:50:03.187426: step 362, loss 4.40729.
Train: 2018-08-02T14:50:03.249908: step 363, loss 4.65914.
Train: 2018-08-02T14:50:03.296742: step 364, loss 4.65914.
Train: 2018-08-02T14:50:03.359228: step 365, loss 4.02952.
Train: 2018-08-02T14:50:03.421713: step 366, loss 4.40729.
Train: 2018-08-02T14:50:03.468607: step 367, loss 3.9036.
Train: 2018-08-02T14:50:03.531063: step 368, loss 4.15545.
Train: 2018-08-02T14:50:03.593548: step 369, loss 4.65914.
Train: 2018-08-02T14:50:03.640412: step 370, loss 4.28137.
Test: 2018-08-02T14:50:03.843513: step 370, loss 3.81863.
Train: 2018-08-02T14:50:03.905975: step 371, loss 4.40729.
Train: 2018-08-02T14:50:03.968489: step 372, loss 3.77768.
Train: 2018-08-02T14:50:04.015354: step 373, loss 3.65176.
Train: 2018-08-02T14:50:04.077839: step 374, loss 4.15545.
Train: 2018-08-02T14:50:04.140324: step 375, loss 3.65176.
Train: 2018-08-02T14:50:04.187198: step 376, loss 3.27399.
Train: 2018-08-02T14:50:04.249674: step 377, loss 4.02952.
Train: 2018-08-02T14:50:04.312163: step 378, loss 5.0369.
Train: 2018-08-02T14:50:04.359023: step 379, loss 3.77768.
Train: 2018-08-02T14:50:04.421506: step 380, loss 4.78506.
Test: 2018-08-02T14:50:04.608935: step 380, loss 3.81863.
Train: 2018-08-02T14:50:04.671452: step 381, loss 3.39991.
Train: 2018-08-02T14:50:04.733905: step 382, loss 5.28875.
Train: 2018-08-02T14:50:04.796390: step 383, loss 3.77768.
Train: 2018-08-02T14:50:04.858903: step 384, loss 4.15545.
Train: 2018-08-02T14:50:04.905773: step 385, loss 3.52583.
Train: 2018-08-02T14:50:04.968256: step 386, loss 3.65176.
Train: 2018-08-02T14:50:05.030740: step 387, loss 4.15545.
Train: 2018-08-02T14:50:05.077601: step 388, loss 4.53321.
Train: 2018-08-02T14:50:05.140089: step 389, loss 5.5406.
Train: 2018-08-02T14:50:05.202547: step 390, loss 3.9036.
Test: 2018-08-02T14:50:05.390000: step 390, loss 3.81863.
Train: 2018-08-02T14:50:05.452520: step 391, loss 4.15545.
Train: 2018-08-02T14:50:05.515004: step 392, loss 4.40729.
Train: 2018-08-02T14:50:05.561865: step 393, loss 3.9036.
Train: 2018-08-02T14:50:05.624336: step 394, loss 3.52583.
Train: 2018-08-02T14:50:05.686841: step 395, loss 3.39991.
Train: 2018-08-02T14:50:05.749324: step 396, loss 4.02952.
Train: 2018-08-02T14:50:05.796184: step 397, loss 4.28137.
Train: 2018-08-02T14:50:05.858642: step 398, loss 4.15545.
Train: 2018-08-02T14:50:05.905506: step 399, loss 4.02952.
Train: 2018-08-02T14:50:05.968021: step 400, loss 4.91098.
Test: 2018-08-02T14:50:06.171093: step 400, loss 3.81863.
Train: 2018-08-02T14:50:06.639739: step 401, loss 3.14807.
Train: 2018-08-02T14:50:06.702194: step 402, loss 4.02952.
Train: 2018-08-02T14:50:06.749103: step 403, loss 4.40729.
Train: 2018-08-02T14:50:06.811545: step 404, loss 4.28137.
Train: 2018-08-02T14:50:06.874056: step 405, loss 4.02952.
Train: 2018-08-02T14:50:06.920921: step 406, loss 2.89622.
Train: 2018-08-02T14:50:06.983409: step 407, loss 3.52583.
Train: 2018-08-02T14:50:07.045893: step 408, loss 4.65914.
Train: 2018-08-02T14:50:07.092728: step 409, loss 3.77768.
Train: 2018-08-02T14:50:07.155213: step 410, loss 3.9036.
Test: 2018-08-02T14:50:07.342700: step 410, loss 3.81863.
Train: 2018-08-02T14:50:07.405156: step 411, loss 3.77768.
Train: 2018-08-02T14:50:07.467671: step 412, loss 3.27399.
Train: 2018-08-02T14:50:07.514535: step 413, loss 4.15545.
Train: 2018-08-02T14:50:07.577021: step 414, loss 3.27399.
Train: 2018-08-02T14:50:07.639506: step 415, loss 4.91098.
Train: 2018-08-02T14:50:07.701991: step 416, loss 3.77768.
Train: 2018-08-02T14:50:07.748824: step 417, loss 4.78506.
Train: 2018-08-02T14:50:07.811310: step 418, loss 5.41467.
Train: 2018-08-02T14:50:07.873830: step 419, loss 3.65176.
Train: 2018-08-02T14:50:07.920659: step 420, loss 3.02214.
Test: 2018-08-02T14:50:08.123766: step 420, loss 3.81863.
Train: 2018-08-02T14:50:08.186255: step 421, loss 4.40729.
Train: 2018-08-02T14:50:08.248707: step 422, loss 4.65914.
Train: 2018-08-02T14:50:08.295605: step 423, loss 4.02952.
Train: 2018-08-02T14:50:08.358058: step 424, loss 3.78309.
Train: 2018-08-02T14:50:08.420570: step 425, loss 4.40729.
Train: 2018-08-02T14:50:08.467437: step 426, loss 3.52583.
Train: 2018-08-02T14:50:08.529922: step 427, loss 3.77768.
Train: 2018-08-02T14:50:08.592411: step 428, loss 3.77768.
Train: 2018-08-02T14:50:08.639274: step 429, loss 3.9036.
Train: 2018-08-02T14:50:08.701752: step 430, loss 5.28875.
Test: 2018-08-02T14:50:08.904833: step 430, loss 3.81863.
Train: 2018-08-02T14:50:08.967319: step 431, loss 4.40729.
Train: 2018-08-02T14:50:09.014183: step 432, loss 3.9036.
Train: 2018-08-02T14:50:09.076640: step 433, loss 3.14807.
Train: 2018-08-02T14:50:09.139155: step 434, loss 4.28137.
Train: 2018-08-02T14:50:09.201611: step 435, loss 3.52583.
Train: 2018-08-02T14:50:09.248473: step 436, loss 5.28875.
Train: 2018-08-02T14:50:09.310992: step 437, loss 4.53321.
Train: 2018-08-02T14:50:09.373473: step 438, loss 2.7703.
Train: 2018-08-02T14:50:09.420307: step 439, loss 3.77768.
Train: 2018-08-02T14:50:09.482794: step 440, loss 4.91098.
Test: 2018-08-02T14:50:09.670274: step 440, loss 3.81863.
Train: 2018-08-02T14:50:09.732765: step 441, loss 3.9036.
Train: 2018-08-02T14:50:09.795250: step 442, loss 4.91098.
Train: 2018-08-02T14:50:09.842110: step 443, loss 4.28137.
Train: 2018-08-02T14:50:09.904572: step 444, loss 3.27399.
Train: 2018-08-02T14:50:09.967086: step 445, loss 3.77768.
Train: 2018-08-02T14:50:10.013950: step 446, loss 3.77768.
Train: 2018-08-02T14:50:10.076435: step 447, loss 4.28137.
Train: 2018-08-02T14:50:10.138890: step 448, loss 3.65695.
Train: 2018-08-02T14:50:10.185783: step 449, loss 4.15545.
Train: 2018-08-02T14:50:10.248296: step 450, loss 2.7703.
Test: 2018-08-02T14:50:10.451346: step 450, loss 3.81863.
Train: 2018-08-02T14:50:10.513802: step 451, loss 4.02952.
Train: 2018-08-02T14:50:10.560694: step 452, loss 4.15545.
Train: 2018-08-02T14:50:10.607531: step 453, loss 5.3727.
Train: 2018-08-02T14:50:10.670045: step 454, loss 3.27399.
Train: 2018-08-02T14:50:10.732531: step 455, loss 3.52583.
Train: 2018-08-02T14:50:10.779366: step 456, loss 4.28137.
Train: 2018-08-02T14:50:10.841851: step 457, loss 3.77768.
Train: 2018-08-02T14:50:10.904338: step 458, loss 5.28875.
Train: 2018-08-02T14:50:10.966852: step 459, loss 3.52583.
Train: 2018-08-02T14:50:11.013685: step 460, loss 4.65914.
Test: 2018-08-02T14:50:11.216792: step 460, loss 3.81863.
Train: 2018-08-02T14:50:11.279278: step 461, loss 3.9036.
Train: 2018-08-02T14:50:11.341733: step 462, loss 3.14807.
Train: 2018-08-02T14:50:11.404243: step 463, loss 3.77768.
Train: 2018-08-02T14:50:11.451083: step 464, loss 3.77768.
Train: 2018-08-02T14:50:11.513600: step 465, loss 5.28875.
Train: 2018-08-02T14:50:11.576083: step 466, loss 3.52583.
Train: 2018-08-02T14:50:11.622944: step 467, loss 3.65176.
Train: 2018-08-02T14:50:11.685437: step 468, loss 2.89622.
Train: 2018-08-02T14:50:11.732297: step 469, loss 3.77768.
Train: 2018-08-02T14:50:11.794784: step 470, loss 4.03458.
Test: 2018-08-02T14:50:11.997829: step 470, loss 3.81863.
Train: 2018-08-02T14:50:12.060348: step 471, loss 3.9036.
Train: 2018-08-02T14:50:12.122801: step 472, loss 3.77768.
Train: 2018-08-02T14:50:12.169692: step 473, loss 3.52583.
Train: 2018-08-02T14:50:12.232157: step 474, loss 4.02952.
Train: 2018-08-02T14:50:12.294663: step 475, loss 4.28137.
Train: 2018-08-02T14:50:12.357150: step 476, loss 3.9036.
Train: 2018-08-02T14:50:12.403984: step 477, loss 3.9036.
Train: 2018-08-02T14:50:12.466496: step 478, loss 4.15545.
Train: 2018-08-02T14:50:12.528956: step 479, loss 3.27399.
Train: 2018-08-02T14:50:12.575820: step 480, loss 2.64437.
Test: 2018-08-02T14:50:12.778927: step 480, loss 3.81863.
Train: 2018-08-02T14:50:12.841415: step 481, loss 3.27399.
Train: 2018-08-02T14:50:12.888246: step 482, loss 3.14807.
Train: 2018-08-02T14:50:12.950762: step 483, loss 3.65176.
Train: 2018-08-02T14:50:13.013219: step 484, loss 4.53321.
Train: 2018-08-02T14:50:13.075704: step 485, loss 4.28137.
Train: 2018-08-02T14:50:13.122596: step 486, loss 4.02952.
Train: 2018-08-02T14:50:13.185085: step 487, loss 3.77768.
Train: 2018-08-02T14:50:13.247537: step 488, loss 3.9036.
Train: 2018-08-02T14:50:13.294429: step 489, loss 5.5406.
Train: 2018-08-02T14:50:13.356916: step 490, loss 3.9036.
Test: 2018-08-02T14:50:13.559997: step 490, loss 3.81863.
Train: 2018-08-02T14:50:13.606828: step 491, loss 4.15545.
Train: 2018-08-02T14:50:13.669312: step 492, loss 4.28137.
Train: 2018-08-02T14:50:13.731829: step 493, loss 3.77768.
Train: 2018-08-02T14:50:13.794286: step 494, loss 3.9036.
Train: 2018-08-02T14:50:13.841149: step 495, loss 4.65914.
Train: 2018-08-02T14:50:13.903633: step 496, loss 3.39991.
Train: 2018-08-02T14:50:13.966144: step 497, loss 5.28875.
Train: 2018-08-02T14:50:14.013008: step 498, loss 3.9036.
Train: 2018-08-02T14:50:14.075468: step 499, loss 3.65176.
Train: 2018-08-02T14:50:14.137952: step 500, loss 3.65176.
Test: 2018-08-02T14:50:14.325433: step 500, loss 3.81863.
Train: 2018-08-02T14:50:14.794081: step 501, loss 3.52583.
Train: 2018-08-02T14:50:14.840944: step 502, loss 4.78506.
Train: 2018-08-02T14:50:14.903400: step 503, loss 4.78506.
Train: 2018-08-02T14:50:14.965886: step 504, loss 3.9036.
Train: 2018-08-02T14:50:15.028401: step 505, loss 4.15545.
Train: 2018-08-02T14:50:15.090884: step 506, loss 4.40729.
Train: 2018-08-02T14:50:15.137749: step 507, loss 4.40729.
Train: 2018-08-02T14:50:15.200236: step 508, loss 4.28137.
Train: 2018-08-02T14:50:15.262716: step 509, loss 4.78506.
Train: 2018-08-02T14:50:15.309587: step 510, loss 4.02952.
Test: 2018-08-02T14:50:15.512632: step 510, loss 3.81863.
Train: 2018-08-02T14:50:15.575123: step 511, loss 3.65176.
Train: 2018-08-02T14:50:15.621981: step 512, loss 3.77768.
Train: 2018-08-02T14:50:15.684467: step 513, loss 3.14807.
Train: 2018-08-02T14:50:15.746979: step 514, loss 4.15545.
Train: 2018-08-02T14:50:15.793843: step 515, loss 3.65176.
Train: 2018-08-02T14:50:15.856331: step 516, loss 2.7703.
Train: 2018-08-02T14:50:15.918816: step 517, loss 3.27399.
Train: 2018-08-02T14:50:15.981274: step 518, loss 4.40729.
Train: 2018-08-02T14:50:16.028169: step 519, loss 3.39991.
Train: 2018-08-02T14:50:16.090651: step 520, loss 3.77768.
Test: 2018-08-02T14:50:16.278077: step 520, loss 3.81863.
Train: 2018-08-02T14:50:16.340593: step 521, loss 3.77768.
Train: 2018-08-02T14:50:16.403047: step 522, loss 3.39991.
Train: 2018-08-02T14:50:16.465564: step 523, loss 4.65914.
Train: 2018-08-02T14:50:16.528068: step 524, loss 3.02214.
Train: 2018-08-02T14:50:16.574914: step 525, loss 4.40729.
Train: 2018-08-02T14:50:16.637368: step 526, loss 4.15545.
Train: 2018-08-02T14:50:16.699884: step 527, loss 2.7703.
Train: 2018-08-02T14:50:16.746748: step 528, loss 3.77768.
Train: 2018-08-02T14:50:16.809236: step 529, loss 4.65914.
Train: 2018-08-02T14:50:16.871687: step 530, loss 5.0369.
Test: 2018-08-02T14:50:17.059144: step 530, loss 3.81863.
Train: 2018-08-02T14:50:17.121630: step 531, loss 3.14807.
Train: 2018-08-02T14:50:17.184144: step 532, loss 3.77768.
Train: 2018-08-02T14:50:17.246625: step 533, loss 3.65176.
Train: 2018-08-02T14:50:17.293492: step 534, loss 3.77768.
Train: 2018-08-02T14:50:17.355951: step 535, loss 3.39991.
Train: 2018-08-02T14:50:17.418435: step 536, loss 5.16283.
Train: 2018-08-02T14:50:17.480922: step 537, loss 4.28137.
Train: 2018-08-02T14:50:17.527786: step 538, loss 3.39991.
Train: 2018-08-02T14:50:17.590295: step 539, loss 4.91098.
Train: 2018-08-02T14:50:17.652781: step 540, loss 4.53321.
Test: 2018-08-02T14:50:17.840242: step 540, loss 3.81863.
Train: 2018-08-02T14:50:17.902729: step 541, loss 3.65176.
Train: 2018-08-02T14:50:17.965206: step 542, loss 3.27399.
Train: 2018-08-02T14:50:18.012071: step 543, loss 3.27399.
Train: 2018-08-02T14:50:18.074533: step 544, loss 3.9036.
Train: 2018-08-02T14:50:18.137041: step 545, loss 4.78506.
Train: 2018-08-02T14:50:18.199502: step 546, loss 4.28137.
Train: 2018-08-02T14:50:18.246396: step 547, loss 4.78506.
Train: 2018-08-02T14:50:18.308852: step 548, loss 4.15545.
Train: 2018-08-02T14:50:18.371367: step 549, loss 4.53321.
Train: 2018-08-02T14:50:18.418201: step 550, loss 4.65914.
Test: 2018-08-02T14:50:18.621302: step 550, loss 3.81863.
Train: 2018-08-02T14:50:18.683792: step 551, loss 2.89622.
Train: 2018-08-02T14:50:18.730659: step 552, loss 4.78506.
Train: 2018-08-02T14:50:18.793142: step 553, loss 3.52583.
Train: 2018-08-02T14:50:18.855601: step 554, loss 4.28137.
Train: 2018-08-02T14:50:18.918116: step 555, loss 4.15545.
Train: 2018-08-02T14:50:18.980569: step 556, loss 4.78506.
Train: 2018-08-02T14:50:19.027463: step 557, loss 3.77768.
Train: 2018-08-02T14:50:19.089921: step 558, loss 4.28137.
Train: 2018-08-02T14:50:19.152436: step 559, loss 4.40729.
Train: 2018-08-02T14:50:19.199268: step 560, loss 4.65914.
Test: 2018-08-02T14:50:19.402344: step 560, loss 3.81863.
Train: 2018-08-02T14:50:19.449240: step 561, loss 4.28137.
Train: 2018-08-02T14:50:19.511726: step 562, loss 3.77768.
Train: 2018-08-02T14:50:19.574211: step 563, loss 3.14807.
Train: 2018-08-02T14:50:19.636669: step 564, loss 3.9036.
Train: 2018-08-02T14:50:19.699185: step 565, loss 4.53321.
Train: 2018-08-02T14:50:19.746047: step 566, loss 3.9036.
Train: 2018-08-02T14:50:19.808533: step 567, loss 4.53321.
Train: 2018-08-02T14:50:19.871017: step 568, loss 4.53321.
Train: 2018-08-02T14:50:19.933473: step 569, loss 4.02952.
Train: 2018-08-02T14:50:19.980366: step 570, loss 3.65176.
Test: 2018-08-02T14:50:20.183411: step 570, loss 3.81863.
Train: 2018-08-02T14:50:20.245923: step 571, loss 4.65914.
Train: 2018-08-02T14:50:20.308383: step 572, loss 3.65176.
Train: 2018-08-02T14:50:20.355277: step 573, loss 4.28137.
Train: 2018-08-02T14:50:20.417761: step 574, loss 3.77768.
Train: 2018-08-02T14:50:20.480219: step 575, loss 4.40729.
Train: 2018-08-02T14:50:20.527115: step 576, loss 3.14807.
Train: 2018-08-02T14:50:20.589598: step 577, loss 3.65176.
Train: 2018-08-02T14:50:20.652087: step 578, loss 4.15545.
Train: 2018-08-02T14:50:20.698918: step 579, loss 4.15545.
Train: 2018-08-02T14:50:20.761428: step 580, loss 3.14807.
Test: 2018-08-02T14:50:20.964479: step 580, loss 3.81863.
Train: 2018-08-02T14:50:21.026995: step 581, loss 3.77768.
Train: 2018-08-02T14:50:21.089481: step 582, loss 3.14807.
Train: 2018-08-02T14:50:21.136314: step 583, loss 4.28137.
Train: 2018-08-02T14:50:21.198826: step 584, loss 3.27399.
Train: 2018-08-02T14:50:21.261316: step 585, loss 3.65176.
Train: 2018-08-02T14:50:21.323770: step 586, loss 4.91098.
Train: 2018-08-02T14:50:21.370635: step 587, loss 4.02952.
Train: 2018-08-02T14:50:21.433155: step 588, loss 4.40729.
Train: 2018-08-02T14:50:21.495632: step 589, loss 5.0369.
Train: 2018-08-02T14:50:21.542497: step 590, loss 3.39991.
Test: 2018-08-02T14:50:21.745547: step 590, loss 3.81863.
Train: 2018-08-02T14:50:21.808032: step 591, loss 4.65914.
Train: 2018-08-02T14:50:21.854897: step 592, loss 4.53321.
Train: 2018-08-02T14:50:21.917412: step 593, loss 4.91098.
Train: 2018-08-02T14:50:21.979871: step 594, loss 4.53321.
Train: 2018-08-02T14:50:22.042387: step 595, loss 3.9036.
Train: 2018-08-02T14:50:22.089247: step 596, loss 4.28137.
Train: 2018-08-02T14:50:22.151702: step 597, loss 3.77768.
Train: 2018-08-02T14:50:22.214194: step 598, loss 5.5406.
Train: 2018-08-02T14:50:22.261051: step 599, loss 4.65914.
Train: 2018-08-02T14:50:22.323536: step 600, loss 4.53321.
Test: 2018-08-02T14:50:22.526644: step 600, loss 3.81863.
Train: 2018-08-02T14:50:23.010906: step 601, loss 3.14807.
Train: 2018-08-02T14:50:23.073360: step 602, loss 4.40729.
Train: 2018-08-02T14:50:23.135871: step 603, loss 3.77768.
Train: 2018-08-02T14:50:23.182745: step 604, loss 4.56679.
Train: 2018-08-02T14:50:23.245222: step 605, loss 5.0369.
Train: 2018-08-02T14:50:23.292088: step 606, loss 4.40729.
Train: 2018-08-02T14:50:23.354545: step 607, loss 4.28137.
Train: 2018-08-02T14:50:23.417059: step 608, loss 3.65176.
Train: 2018-08-02T14:50:23.463925: step 609, loss 4.78506.
Train: 2018-08-02T14:50:23.526380: step 610, loss 3.39991.
Test: 2018-08-02T14:50:23.713860: step 610, loss 3.81863.
Train: 2018-08-02T14:50:23.776355: step 611, loss 4.53321.
Train: 2018-08-02T14:50:23.838837: step 612, loss 4.78506.
Train: 2018-08-02T14:50:23.901322: step 613, loss 4.65914.
Train: 2018-08-02T14:50:23.948189: step 614, loss 4.53321.
Train: 2018-08-02T14:50:24.010643: step 615, loss 4.02952.
Train: 2018-08-02T14:50:24.073157: step 616, loss 4.15545.
Train: 2018-08-02T14:50:24.119991: step 617, loss 4.78506.
Train: 2018-08-02T14:50:24.182477: step 618, loss 4.78506.
Train: 2018-08-02T14:50:24.244963: step 619, loss 3.65176.
Train: 2018-08-02T14:50:24.291854: step 620, loss 2.89622.
Test: 2018-08-02T14:50:24.494928: step 620, loss 3.81863.
Train: 2018-08-02T14:50:24.557389: step 621, loss 3.14807.
Train: 2018-08-02T14:50:24.619904: step 622, loss 4.28137.
Train: 2018-08-02T14:50:24.682358: step 623, loss 3.77768.
Train: 2018-08-02T14:50:24.729225: step 624, loss 4.91098.
Train: 2018-08-02T14:50:24.791743: step 625, loss 4.15545.
Train: 2018-08-02T14:50:24.854221: step 626, loss 3.39991.
Train: 2018-08-02T14:50:24.901091: step 627, loss 4.02952.
Train: 2018-08-02T14:50:24.963551: step 628, loss 3.65176.
Train: 2018-08-02T14:50:25.026031: step 629, loss 3.39991.
Train: 2018-08-02T14:50:25.088513: step 630, loss 3.77768.
Test: 2018-08-02T14:50:25.275969: step 630, loss 3.81863.
Train: 2018-08-02T14:50:25.338455: step 631, loss 3.77768.
Train: 2018-08-02T14:50:25.400971: step 632, loss 4.40729.
Train: 2018-08-02T14:50:25.447835: step 633, loss 4.02952.
Train: 2018-08-02T14:50:25.510292: step 634, loss 3.77768.
Train: 2018-08-02T14:50:25.572810: step 635, loss 3.9036.
Train: 2018-08-02T14:50:25.619665: step 636, loss 4.15545.
Train: 2018-08-02T14:50:25.682154: step 637, loss 2.39253.
Train: 2018-08-02T14:50:25.744612: step 638, loss 2.51845.
Train: 2018-08-02T14:50:25.791502: step 639, loss 2.26661.
Train: 2018-08-02T14:50:25.853993: step 640, loss 3.27399.
Test: 2018-08-02T14:50:26.057036: step 640, loss 3.81863.
Train: 2018-08-02T14:50:26.119555: step 641, loss 5.28875.
Train: 2018-08-02T14:50:26.182007: step 642, loss 4.28137.
Train: 2018-08-02T14:50:26.228897: step 643, loss 3.77768.
Train: 2018-08-02T14:50:26.291387: step 644, loss 3.9036.
Train: 2018-08-02T14:50:26.353871: step 645, loss 4.78506.
Train: 2018-08-02T14:50:26.400707: step 646, loss 3.27399.
Train: 2018-08-02T14:50:26.463193: step 647, loss 3.65176.
Train: 2018-08-02T14:50:26.525710: step 648, loss 3.9036.
Train: 2018-08-02T14:50:26.588173: step 649, loss 3.65176.
Train: 2018-08-02T14:50:26.635027: step 650, loss 5.5406.
Test: 2018-08-02T14:50:26.838129: step 650, loss 3.81863.
Train: 2018-08-02T14:50:26.900620: step 651, loss 3.65176.
Train: 2018-08-02T14:50:26.963100: step 652, loss 3.77768.
Train: 2018-08-02T14:50:27.009969: step 653, loss 4.65914.
Train: 2018-08-02T14:50:27.072451: step 654, loss 4.65914.
Train: 2018-08-02T14:50:27.134911: step 655, loss 4.15545.
Train: 2018-08-02T14:50:27.197395: step 656, loss 4.53321.
Train: 2018-08-02T14:50:27.244260: step 657, loss 3.39991.
Train: 2018-08-02T14:50:27.306752: step 658, loss 3.14807.
Train: 2018-08-02T14:50:27.369267: step 659, loss 4.78506.
Train: 2018-08-02T14:50:27.416122: step 660, loss 3.65176.
Test: 2018-08-02T14:50:27.619171: step 660, loss 3.81863.
Train: 2018-08-02T14:50:27.681681: step 661, loss 4.91098.
Train: 2018-08-02T14:50:27.744182: step 662, loss 3.27399.
Train: 2018-08-02T14:50:27.791034: step 663, loss 3.65176.
Train: 2018-08-02T14:50:27.853493: step 664, loss 2.89622.
Train: 2018-08-02T14:50:27.916005: step 665, loss 4.53321.
Train: 2018-08-02T14:50:27.962872: step 666, loss 2.7703.
Train: 2018-08-02T14:50:28.025358: step 667, loss 4.15545.
Train: 2018-08-02T14:50:28.087815: step 668, loss 3.9036.
Train: 2018-08-02T14:50:28.134676: step 669, loss 3.52583.
Train: 2018-08-02T14:50:28.197191: step 670, loss 6.17021.
Test: 2018-08-02T14:50:28.384618: step 670, loss 3.81863.
Train: 2018-08-02T14:50:28.447133: step 671, loss 4.65914.
Train: 2018-08-02T14:50:28.509618: step 672, loss 4.40729.
Train: 2018-08-02T14:50:28.572103: step 673, loss 3.77768.
Train: 2018-08-02T14:50:28.618943: step 674, loss 3.77768.
Train: 2018-08-02T14:50:28.681424: step 675, loss 4.53321.
Train: 2018-08-02T14:50:28.743909: step 676, loss 3.27399.
Train: 2018-08-02T14:50:28.790800: step 677, loss 3.77768.
Train: 2018-08-02T14:50:28.853259: step 678, loss 5.03691.
Train: 2018-08-02T14:50:28.915742: step 679, loss 3.65176.
Train: 2018-08-02T14:50:28.978228: step 680, loss 4.02952.
Test: 2018-08-02T14:50:29.165685: step 680, loss 3.81863.
Train: 2018-08-02T14:50:29.228199: step 681, loss 3.52583.
Train: 2018-08-02T14:50:29.290684: step 682, loss 4.02952.
Train: 2018-08-02T14:50:29.337549: step 683, loss 3.14807.
Train: 2018-08-02T14:50:29.400006: step 684, loss 3.27399.
Train: 2018-08-02T14:50:29.462493: step 685, loss 3.52583.
Train: 2018-08-02T14:50:29.509378: step 686, loss 3.65176.
Train: 2018-08-02T14:50:29.571865: step 687, loss 4.78506.
Train: 2018-08-02T14:50:29.634355: step 688, loss 3.9036.
Train: 2018-08-02T14:50:29.681189: step 689, loss 4.02952.
Train: 2018-08-02T14:50:29.743704: step 690, loss 4.78506.
Test: 2018-08-02T14:50:29.946753: step 690, loss 3.81863.
Train: 2018-08-02T14:50:29.993651: step 691, loss 2.89622.
Train: 2018-08-02T14:50:30.056134: step 692, loss 3.39991.
Train: 2018-08-02T14:50:30.118616: step 693, loss 3.9036.
Train: 2018-08-02T14:50:30.181073: step 694, loss 4.28137.
Train: 2018-08-02T14:50:30.227966: step 695, loss 3.9036.
Train: 2018-08-02T14:50:30.290421: step 696, loss 3.65176.
Train: 2018-08-02T14:50:30.352906: step 697, loss 4.53321.
Train: 2018-08-02T14:50:30.399771: step 698, loss 3.65176.
Train: 2018-08-02T14:50:30.462286: step 699, loss 4.15545.
Train: 2018-08-02T14:50:30.524771: step 700, loss 3.9036.
Test: 2018-08-02T14:50:30.712221: step 700, loss 3.81863.
Train: 2018-08-02T14:50:31.180838: step 701, loss 4.65914.
Train: 2018-08-02T14:50:31.227700: step 702, loss 3.9036.
Train: 2018-08-02T14:50:31.290187: step 703, loss 4.65914.
Train: 2018-08-02T14:50:31.352703: step 704, loss 3.52583.
Train: 2018-08-02T14:50:31.415194: step 705, loss 3.9036.
Train: 2018-08-02T14:50:31.462052: step 706, loss 3.52583.
Train: 2018-08-02T14:50:31.524539: step 707, loss 3.9036.
Train: 2018-08-02T14:50:31.586993: step 708, loss 2.7703.
Train: 2018-08-02T14:50:31.649504: step 709, loss 4.53321.
Train: 2018-08-02T14:50:31.696341: step 710, loss 4.65914.
Test: 2018-08-02T14:50:31.899418: step 710, loss 3.81863.
Train: 2018-08-02T14:50:31.961934: step 711, loss 2.89622.
Train: 2018-08-02T14:50:32.024390: step 712, loss 3.77768.
Train: 2018-08-02T14:50:32.071279: step 713, loss 3.77768.
Train: 2018-08-02T14:50:32.133765: step 714, loss 5.28875.
Train: 2018-08-02T14:50:32.196250: step 715, loss 4.15545.
Train: 2018-08-02T14:50:32.243120: step 716, loss 4.40729.
Train: 2018-08-02T14:50:32.305608: step 717, loss 3.65176.
Train: 2018-08-02T14:50:32.368090: step 718, loss 5.0369.
Train: 2018-08-02T14:50:32.414948: step 719, loss 4.53321.
Train: 2018-08-02T14:50:32.477409: step 720, loss 3.39991.
Test: 2018-08-02T14:50:32.680485: step 720, loss 3.81863.
Train: 2018-08-02T14:50:32.727350: step 721, loss 3.39991.
Train: 2018-08-02T14:50:32.789835: step 722, loss 4.28137.
Train: 2018-08-02T14:50:32.852321: step 723, loss 4.78506.
Train: 2018-08-02T14:50:32.914835: step 724, loss 3.39991.
Train: 2018-08-02T14:50:32.961696: step 725, loss 3.39991.
Train: 2018-08-02T14:50:33.024157: step 726, loss 4.53321.
Train: 2018-08-02T14:50:33.086666: step 727, loss 4.91098.
Train: 2018-08-02T14:50:33.133505: step 728, loss 3.27399.
Train: 2018-08-02T14:50:33.195990: step 729, loss 4.91098.
Train: 2018-08-02T14:50:33.258505: step 730, loss 3.39991.
Test: 2018-08-02T14:50:33.445931: step 730, loss 3.81863.
Train: 2018-08-02T14:50:33.508448: step 731, loss 4.40729.
Train: 2018-08-02T14:50:33.570903: step 732, loss 5.16283.
Train: 2018-08-02T14:50:33.633388: step 733, loss 3.9036.
Train: 2018-08-02T14:50:33.695873: step 734, loss 4.28137.
Train: 2018-08-02T14:50:33.742764: step 735, loss 3.77768.
Train: 2018-08-02T14:50:33.805251: step 736, loss 4.02952.
Train: 2018-08-02T14:50:33.867707: step 737, loss 4.91098.
Train: 2018-08-02T14:50:33.914600: step 738, loss 5.0369.
Train: 2018-08-02T14:50:33.977056: step 739, loss 4.15545.
Train: 2018-08-02T14:50:34.039567: step 740, loss 4.28137.
Test: 2018-08-02T14:50:34.227024: step 740, loss 3.81863.
Train: 2018-08-02T14:50:34.289485: step 741, loss 3.9036.
Train: 2018-08-02T14:50:34.352000: step 742, loss 3.52583.
Train: 2018-08-02T14:50:34.414485: step 743, loss 4.02952.
Train: 2018-08-02T14:50:34.461319: step 744, loss 3.65176.
Train: 2018-08-02T14:50:34.523830: step 745, loss 3.9036.
Train: 2018-08-02T14:50:34.586291: step 746, loss 4.40729.
Train: 2018-08-02T14:50:34.633155: step 747, loss 4.53321.
Train: 2018-08-02T14:50:34.695670: step 748, loss 4.28137.
Train: 2018-08-02T14:50:34.758155: step 749, loss 4.15545.
Train: 2018-08-02T14:50:34.804988: step 750, loss 2.39253.
Test: 2018-08-02T14:50:35.008067: step 750, loss 3.81863.
Train: 2018-08-02T14:50:35.070581: step 751, loss 4.65914.
Train: 2018-08-02T14:50:35.133066: step 752, loss 3.9036.
Train: 2018-08-02T14:50:35.179934: step 753, loss 4.53321.
Train: 2018-08-02T14:50:35.242387: step 754, loss 3.9036.
Train: 2018-08-02T14:50:35.289280: step 755, loss 2.95498.
Train: 2018-08-02T14:50:35.351735: step 756, loss 3.9036.
Train: 2018-08-02T14:50:35.398600: step 757, loss 3.14807.
Train: 2018-08-02T14:50:35.461085: step 758, loss 4.65914.
Train: 2018-08-02T14:50:35.523595: step 759, loss 3.14807.
Train: 2018-08-02T14:50:35.570434: step 760, loss 4.91098.
Test: 2018-08-02T14:50:35.757896: step 760, loss 3.81863.
Train: 2018-08-02T14:50:35.820405: step 761, loss 4.40729.
Train: 2018-08-02T14:50:35.882861: step 762, loss 4.53321.
Train: 2018-08-02T14:50:35.945345: step 763, loss 4.15545.
Train: 2018-08-02T14:50:36.007833: step 764, loss 3.9036.
Train: 2018-08-02T14:50:36.054730: step 765, loss 4.15545.
Train: 2018-08-02T14:50:36.117211: step 766, loss 4.02952.
Train: 2018-08-02T14:50:36.179700: step 767, loss 3.52583.
Train: 2018-08-02T14:50:36.226530: step 768, loss 5.28875.
Train: 2018-08-02T14:50:36.289045: step 769, loss 3.27399.
Train: 2018-08-02T14:50:36.351532: step 770, loss 3.27399.
Test: 2018-08-02T14:50:36.538958: step 770, loss 3.81863.
Train: 2018-08-02T14:50:36.601473: step 771, loss 3.14807.
Train: 2018-08-02T14:50:36.663929: step 772, loss 3.77768.
Train: 2018-08-02T14:50:36.726444: step 773, loss 4.78506.
Train: 2018-08-02T14:50:36.788929: step 774, loss 3.39991.
Train: 2018-08-02T14:50:36.835794: step 775, loss 4.15545.
Train: 2018-08-02T14:50:36.898279: step 776, loss 3.65176.
Train: 2018-08-02T14:50:36.960733: step 777, loss 4.40729.
Train: 2018-08-02T14:50:37.007628: step 778, loss 4.28137.
Train: 2018-08-02T14:50:37.070108: step 779, loss 4.78506.
Train: 2018-08-02T14:50:37.132599: step 780, loss 3.27399.
Test: 2018-08-02T14:50:37.320052: step 780, loss 3.81863.
Train: 2018-08-02T14:50:37.382540: step 781, loss 4.28137.
Train: 2018-08-02T14:50:37.445020: step 782, loss 3.9036.
Train: 2018-08-02T14:50:37.491859: step 783, loss 3.9036.
Train: 2018-08-02T14:50:37.554346: step 784, loss 2.89622.
Train: 2018-08-02T14:50:37.616831: step 785, loss 5.0369.
Train: 2018-08-02T14:50:37.679317: step 786, loss 4.65914.
Train: 2018-08-02T14:50:37.726180: step 787, loss 2.39253.
Train: 2018-08-02T14:50:37.788693: step 788, loss 4.91098.
Train: 2018-08-02T14:50:37.851150: step 789, loss 3.9036.
Train: 2018-08-02T14:50:37.898047: step 790, loss 3.9036.
Test: 2018-08-02T14:50:38.101116: step 790, loss 3.81863.
Train: 2018-08-02T14:50:38.163578: step 791, loss 4.15545.
Train: 2018-08-02T14:50:38.226088: step 792, loss 2.89622.
Train: 2018-08-02T14:50:38.288578: step 793, loss 3.77768.
Train: 2018-08-02T14:50:38.335437: step 794, loss 3.27399.
Train: 2018-08-02T14:50:38.397928: step 795, loss 4.15545.
Train: 2018-08-02T14:50:38.460407: step 796, loss 3.14807.
Train: 2018-08-02T14:50:38.522899: step 797, loss 4.65914.
Train: 2018-08-02T14:50:38.569735: step 798, loss 4.40729.
Train: 2018-08-02T14:50:38.647863: step 799, loss 4.78506.
Train: 2018-08-02T14:50:38.710355: step 800, loss 4.53321.
Test: 2018-08-02T14:50:38.913403: step 800, loss 3.81863.
Train: 2018-08-02T14:50:39.413315: step 801, loss 3.77768.
Train: 2018-08-02T14:50:39.460173: step 802, loss 4.78506.
Train: 2018-08-02T14:50:39.522633: step 803, loss 3.65176.
Train: 2018-08-02T14:50:39.585121: step 804, loss 3.9036.
Train: 2018-08-02T14:50:39.647636: step 805, loss 4.65914.
Train: 2018-08-02T14:50:39.694470: step 806, loss 3.39991.
Train: 2018-08-02T14:50:39.756985: step 807, loss 3.14807.
Train: 2018-08-02T14:50:39.819441: step 808, loss 3.52583.
Train: 2018-08-02T14:50:39.881955: step 809, loss 2.89622.
Train: 2018-08-02T14:50:39.928790: step 810, loss 3.65176.
Test: 2018-08-02T14:50:40.131865: step 810, loss 3.81863.
Train: 2018-08-02T14:50:40.194382: step 811, loss 4.40729.
Train: 2018-08-02T14:50:40.256838: step 812, loss 5.5406.
Train: 2018-08-02T14:50:40.303730: step 813, loss 4.02952.
Train: 2018-08-02T14:50:40.366213: step 814, loss 3.65176.
Train: 2018-08-02T14:50:40.428706: step 815, loss 4.15545.
Train: 2018-08-02T14:50:40.475566: step 816, loss 3.14807.
Train: 2018-08-02T14:50:40.538051: step 817, loss 5.28875.
Train: 2018-08-02T14:50:40.600537: step 818, loss 3.9036.
Train: 2018-08-02T14:50:40.647399: step 819, loss 4.53321.
Train: 2018-08-02T14:50:40.709886: step 820, loss 4.02952.
Test: 2018-08-02T14:50:40.897336: step 820, loss 3.81863.
Train: 2018-08-02T14:50:40.959827: step 821, loss 3.52583.
Train: 2018-08-02T14:50:41.022282: step 822, loss 4.65914.
Train: 2018-08-02T14:50:41.084799: step 823, loss 4.02952.
Train: 2018-08-02T14:50:41.131665: step 824, loss 3.27399.
Train: 2018-08-02T14:50:41.194154: step 825, loss 3.9036.
Train: 2018-08-02T14:50:41.256604: step 826, loss 4.15545.
Train: 2018-08-02T14:50:41.319119: step 827, loss 3.52583.
Train: 2018-08-02T14:50:41.365984: step 828, loss 3.52583.
Train: 2018-08-02T14:50:41.428439: step 829, loss 3.78309.
Train: 2018-08-02T14:50:41.490952: step 830, loss 3.65176.
Test: 2018-08-02T14:50:41.678412: step 830, loss 3.81863.
Train: 2018-08-02T14:50:41.740894: step 831, loss 3.65176.
Train: 2018-08-02T14:50:41.803357: step 832, loss 3.77768.
Train: 2018-08-02T14:50:41.850239: step 833, loss 4.91098.
Train: 2018-08-02T14:50:41.912728: step 834, loss 3.9036.
Train: 2018-08-02T14:50:41.975185: step 835, loss 4.02952.
Train: 2018-08-02T14:50:42.022081: step 836, loss 5.28875.
Train: 2018-08-02T14:50:42.084534: step 837, loss 3.9036.
Train: 2018-08-02T14:50:42.147020: step 838, loss 4.02952.
Train: 2018-08-02T14:50:42.193908: step 839, loss 3.14807.
Train: 2018-08-02T14:50:42.256368: step 840, loss 3.77768.
Test: 2018-08-02T14:50:42.459478: step 840, loss 3.81863.
Train: 2018-08-02T14:50:42.506310: step 841, loss 3.9036.
Train: 2018-08-02T14:50:42.568825: step 842, loss 3.27399.
Train: 2018-08-02T14:50:42.631306: step 843, loss 3.65176.
Train: 2018-08-02T14:50:42.693788: step 844, loss 4.91098.
Train: 2018-08-02T14:50:42.740661: step 845, loss 3.39991.
Train: 2018-08-02T14:50:42.803149: step 846, loss 4.91098.
Train: 2018-08-02T14:50:42.865601: step 847, loss 4.02952.
Train: 2018-08-02T14:50:42.928087: step 848, loss 3.52583.
Train: 2018-08-02T14:50:42.974981: step 849, loss 4.28137.
Train: 2018-08-02T14:50:43.037435: step 850, loss 4.53321.
Test: 2018-08-02T14:50:43.240537: step 850, loss 3.81863.
Train: 2018-08-02T14:50:43.303032: step 851, loss 3.77768.
Train: 2018-08-02T14:50:43.349893: step 852, loss 3.9036.
Train: 2018-08-02T14:50:43.412373: step 853, loss 5.41467.
Train: 2018-08-02T14:50:43.474835: step 854, loss 4.53321.
Train: 2018-08-02T14:50:43.521723: step 855, loss 3.77768.
Train: 2018-08-02T14:50:43.584213: step 856, loss 3.77768.
Train: 2018-08-02T14:50:43.646668: step 857, loss 3.52583.
Train: 2018-08-02T14:50:43.693563: step 858, loss 3.77768.
Train: 2018-08-02T14:50:43.756049: step 859, loss 3.39991.
Train: 2018-08-02T14:50:43.818533: step 860, loss 3.9036.
Test: 2018-08-02T14:50:44.005959: step 860, loss 3.81863.
Train: 2018-08-02T14:50:44.068475: step 861, loss 4.28137.
Train: 2018-08-02T14:50:44.130961: step 862, loss 3.9036.
Train: 2018-08-02T14:50:44.193445: step 863, loss 3.65176.
Train: 2018-08-02T14:50:44.240306: step 864, loss 3.65176.
Train: 2018-08-02T14:50:44.302765: step 865, loss 4.53321.
Train: 2018-08-02T14:50:44.365276: step 866, loss 2.7703.
Train: 2018-08-02T14:50:44.412143: step 867, loss 3.65176.
Train: 2018-08-02T14:50:44.474630: step 868, loss 4.91098.
Train: 2018-08-02T14:50:44.537109: step 869, loss 4.40729.
Train: 2018-08-02T14:50:44.583979: step 870, loss 5.16283.
Test: 2018-08-02T14:50:44.787050: step 870, loss 3.81863.
Train: 2018-08-02T14:50:44.849512: step 871, loss 4.02952.
Train: 2018-08-02T14:50:44.912029: step 872, loss 3.9036.
Train: 2018-08-02T14:50:44.958862: step 873, loss 4.40729.
Train: 2018-08-02T14:50:45.021348: step 874, loss 3.9036.
Train: 2018-08-02T14:50:45.083863: step 875, loss 3.52583.
Train: 2018-08-02T14:50:45.146347: step 876, loss 4.02952.
Train: 2018-08-02T14:50:45.193207: step 877, loss 5.16283.
Train: 2018-08-02T14:50:45.255697: step 878, loss 4.78506.
Train: 2018-08-02T14:50:45.318154: step 879, loss 3.65176.
Train: 2018-08-02T14:50:45.365046: step 880, loss 4.28137.
Test: 2018-08-02T14:50:45.568092: step 880, loss 3.81863.
Train: 2018-08-02T14:50:45.630578: step 881, loss 4.40729.
Train: 2018-08-02T14:50:45.677442: step 882, loss 4.02952.
Train: 2018-08-02T14:50:45.739958: step 883, loss 3.77768.
Train: 2018-08-02T14:50:45.802432: step 884, loss 3.77768.
Train: 2018-08-02T14:50:45.849303: step 885, loss 4.78506.
Train: 2018-08-02T14:50:45.911791: step 886, loss 2.7703.
Train: 2018-08-02T14:50:45.974279: step 887, loss 4.65914.
Train: 2018-08-02T14:50:46.021113: step 888, loss 3.39991.
Train: 2018-08-02T14:50:46.083627: step 889, loss 3.39991.
Train: 2018-08-02T14:50:46.146108: step 890, loss 2.64438.
Test: 2018-08-02T14:50:46.333539: step 890, loss 3.81863.
Train: 2018-08-02T14:50:46.396057: step 891, loss 4.15545.
Train: 2018-08-02T14:50:46.458509: step 892, loss 5.5406.
Train: 2018-08-02T14:50:46.521025: step 893, loss 4.15545.
Train: 2018-08-02T14:50:46.567859: step 894, loss 3.9036.
Train: 2018-08-02T14:50:46.630346: step 895, loss 3.77768.
Train: 2018-08-02T14:50:46.692860: step 896, loss 4.78506.
Train: 2018-08-02T14:50:46.739694: step 897, loss 4.53321.
Train: 2018-08-02T14:50:46.802214: step 898, loss 3.65176.
Train: 2018-08-02T14:50:46.864696: step 899, loss 4.65914.
Train: 2018-08-02T14:50:46.911529: step 900, loss 5.28875.
Test: 2018-08-02T14:50:47.114636: step 900, loss 3.81863.
Train: 2018-08-02T14:50:47.583277: step 901, loss 3.52583.
Train: 2018-08-02T14:50:47.645766: step 902, loss 3.65176.
Train: 2018-08-02T14:50:47.708217: step 903, loss 4.40729.
Train: 2018-08-02T14:50:47.770736: step 904, loss 4.15545.
Train: 2018-08-02T14:50:47.833221: step 905, loss 4.28137.
Train: 2018-08-02T14:50:47.880077: step 906, loss 1.88044.
Train: 2018-08-02T14:50:47.926916: step 907, loss 3.77768.
Train: 2018-08-02T14:50:47.989432: step 908, loss 4.15545.
Train: 2018-08-02T14:50:48.051920: step 909, loss 4.65914.
Train: 2018-08-02T14:50:48.098776: step 910, loss 4.40729.
Test: 2018-08-02T14:50:48.301828: step 910, loss 3.81863.
Train: 2018-08-02T14:50:48.364338: step 911, loss 3.14807.
Train: 2018-08-02T14:50:48.426832: step 912, loss 4.15545.
Train: 2018-08-02T14:50:48.489314: step 913, loss 3.77768.
Train: 2018-08-02T14:50:48.536177: step 914, loss 5.41467.
Train: 2018-08-02T14:50:48.598665: step 915, loss 4.15545.
Train: 2018-08-02T14:50:48.661149: step 916, loss 3.65176.
Train: 2018-08-02T14:50:48.707983: step 917, loss 3.14807.
Train: 2018-08-02T14:50:48.770502: step 918, loss 4.28137.
Train: 2018-08-02T14:50:48.832985: step 919, loss 3.65176.
Train: 2018-08-02T14:50:48.879817: step 920, loss 3.15352.
Test: 2018-08-02T14:50:49.082920: step 920, loss 3.81863.
Train: 2018-08-02T14:50:49.145381: step 921, loss 4.02952.
Train: 2018-08-02T14:50:49.207896: step 922, loss 4.65914.
Train: 2018-08-02T14:50:49.270382: step 923, loss 3.14807.
Train: 2018-08-02T14:50:49.317241: step 924, loss 4.28137.
Train: 2018-08-02T14:50:49.379702: step 925, loss 3.27399.
Train: 2018-08-02T14:50:49.442186: step 926, loss 4.65914.
Train: 2018-08-02T14:50:49.489080: step 927, loss 4.40729.
Train: 2018-08-02T14:50:49.551595: step 928, loss 3.65176.
Train: 2018-08-02T14:50:49.614022: step 929, loss 3.65176.
Train: 2018-08-02T14:50:49.660912: step 930, loss 3.14807.
Test: 2018-08-02T14:50:49.863961: step 930, loss 3.81863.
Train: 2018-08-02T14:50:49.926476: step 931, loss 4.40729.
Train: 2018-08-02T14:50:49.988943: step 932, loss 4.65914.
Train: 2018-08-02T14:50:50.035826: step 933, loss 3.02214.
Train: 2018-08-02T14:50:50.098322: step 934, loss 3.9036.
Train: 2018-08-02T14:50:50.160797: step 935, loss 3.77768.
Train: 2018-08-02T14:50:50.207663: step 936, loss 3.39991.
Train: 2018-08-02T14:50:50.270147: step 937, loss 3.5315.
Train: 2018-08-02T14:50:50.332634: step 938, loss 3.39991.
Train: 2018-08-02T14:50:50.379467: step 939, loss 5.29398.
Train: 2018-08-02T14:50:50.441952: step 940, loss 4.65914.
Test: 2018-08-02T14:50:50.645028: step 940, loss 3.81863.
Train: 2018-08-02T14:50:50.707545: step 941, loss 3.90882.
Train: 2018-08-02T14:50:50.769999: step 942, loss 3.65176.
Train: 2018-08-02T14:50:50.816894: step 943, loss 5.5406.
Train: 2018-08-02T14:50:50.879380: step 944, loss 4.28137.
Train: 2018-08-02T14:50:50.941861: step 945, loss 4.02952.
Train: 2018-08-02T14:50:51.004350: step 946, loss 3.77768.
Train: 2018-08-02T14:50:51.051185: step 947, loss 4.40729.
Train: 2018-08-02T14:50:51.113701: step 948, loss 5.5406.
Train: 2018-08-02T14:50:51.176158: step 949, loss 3.77768.
Train: 2018-08-02T14:50:51.238670: step 950, loss 4.78506.
Test: 2018-08-02T14:50:51.426121: step 950, loss 3.81863.
Train: 2018-08-02T14:50:51.488581: step 951, loss 3.02214.
Train: 2018-08-02T14:50:51.551100: step 952, loss 3.27399.
Train: 2018-08-02T14:50:51.597962: step 953, loss 3.65176.
Train: 2018-08-02T14:50:51.660445: step 954, loss 4.40729.
Train: 2018-08-02T14:50:51.722934: step 955, loss 3.77768.
Train: 2018-08-02T14:50:51.785419: step 956, loss 4.40729.
Train: 2018-08-02T14:50:51.832252: step 957, loss 3.39991.
Train: 2018-08-02T14:50:51.894770: step 958, loss 4.65914.
Train: 2018-08-02T14:50:51.957252: step 959, loss 4.15545.
Train: 2018-08-02T14:50:52.004116: step 960, loss 3.02214.
Test: 2018-08-02T14:50:52.207164: step 960, loss 3.81863.
Train: 2018-08-02T14:50:52.269648: step 961, loss 4.40729.
Train: 2018-08-02T14:50:52.332135: step 962, loss 3.02214.
Train: 2018-08-02T14:50:52.379029: step 963, loss 4.65914.
Train: 2018-08-02T14:50:52.441514: step 964, loss 3.39991.
Train: 2018-08-02T14:50:52.504000: step 965, loss 4.78506.
Train: 2018-08-02T14:50:52.550867: step 966, loss 5.16283.
Train: 2018-08-02T14:50:52.613349: step 967, loss 3.9036.
Train: 2018-08-02T14:50:52.675806: step 968, loss 4.53321.
Train: 2018-08-02T14:50:52.722668: step 969, loss 3.14807.
Train: 2018-08-02T14:50:52.785180: step 970, loss 4.28137.
Test: 2018-08-02T14:50:52.988231: step 970, loss 3.81863.
Train: 2018-08-02T14:50:53.035122: step 971, loss 4.28137.
Train: 2018-08-02T14:50:53.097580: step 972, loss 3.27399.
Train: 2018-08-02T14:50:53.160089: step 973, loss 3.52583.
Train: 2018-08-02T14:50:53.222582: step 974, loss 4.40729.
Train: 2018-08-02T14:50:53.285066: step 975, loss 3.65176.
Train: 2018-08-02T14:50:53.331932: step 976, loss 4.02952.
Train: 2018-08-02T14:50:53.394386: step 977, loss 3.77768.
Train: 2018-08-02T14:50:53.441280: step 978, loss 4.15545.
Train: 2018-08-02T14:50:53.503735: step 979, loss 3.27399.
Train: 2018-08-02T14:50:53.566220: step 980, loss 4.40729.
Test: 2018-08-02T14:50:53.753701: step 980, loss 3.81863.
Train: 2018-08-02T14:50:53.816193: step 981, loss 4.15545.
Train: 2018-08-02T14:50:53.878647: step 982, loss 3.14807.
Train: 2018-08-02T14:50:53.941166: step 983, loss 3.77768.
Train: 2018-08-02T14:50:53.988027: step 984, loss 3.14807.
Train: 2018-08-02T14:50:54.050520: step 985, loss 5.16283.
Train: 2018-08-02T14:50:54.097376: step 986, loss 4.91098.
Train: 2018-08-02T14:50:54.159859: step 987, loss 4.40729.
Train: 2018-08-02T14:50:54.222348: step 988, loss 3.52583.
Train: 2018-08-02T14:50:54.284802: step 989, loss 3.14807.
Train: 2018-08-02T14:50:54.331697: step 990, loss 5.28875.
Test: 2018-08-02T14:50:54.534742: step 990, loss 3.81863.
Train: 2018-08-02T14:50:54.597229: step 991, loss 4.53321.
Train: 2018-08-02T14:50:54.659744: step 992, loss 4.02952.
Train: 2018-08-02T14:50:54.706578: step 993, loss 3.77768.
Train: 2018-08-02T14:50:54.769097: step 994, loss 4.15545.
Train: 2018-08-02T14:50:54.831580: step 995, loss 3.65176.
Train: 2018-08-02T14:50:54.894064: step 996, loss 3.77768.
Train: 2018-08-02T14:50:54.940923: step 997, loss 2.7703.
Train: 2018-08-02T14:50:55.003415: step 998, loss 3.52583.
Train: 2018-08-02T14:50:55.065894: step 999, loss 3.77768.
Train: 2018-08-02T14:50:55.112764: step 1000, loss 4.65914.
Test: 2018-08-02T14:50:55.315837: step 1000, loss 3.81863.
Train: 2018-08-02T14:50:55.815724: step 1001, loss 4.65914.
Train: 2018-08-02T14:50:55.878208: step 1002, loss 4.15545.
Train: 2018-08-02T14:50:55.940695: step 1003, loss 4.02952.
Train: 2018-08-02T14:50:55.987555: step 1004, loss 3.9036.
Train: 2018-08-02T14:50:56.050047: step 1005, loss 5.42025.
Train: 2018-08-02T14:50:56.112528: step 1006, loss 3.9036.
Train: 2018-08-02T14:50:56.159364: step 1007, loss 3.02214.
Train: 2018-08-02T14:50:56.221850: step 1008, loss 4.15545.
Train: 2018-08-02T14:50:56.284363: step 1009, loss 3.39991.
Train: 2018-08-02T14:50:56.346844: step 1010, loss 3.02214.
Test: 2018-08-02T14:50:56.534299: step 1010, loss 3.81863.
Train: 2018-08-02T14:50:56.596791: step 1011, loss 4.15545.
Train: 2018-08-02T14:50:56.659276: step 1012, loss 3.65176.
Train: 2018-08-02T14:50:56.721761: step 1013, loss 3.52583.
Train: 2018-08-02T14:50:56.784218: step 1014, loss 3.9036.
Train: 2018-08-02T14:50:56.831082: step 1015, loss 3.02214.
Train: 2018-08-02T14:50:56.893566: step 1016, loss 3.9036.
Train: 2018-08-02T14:50:56.956080: step 1017, loss 3.52583.
Train: 2018-08-02T14:50:57.002946: step 1018, loss 5.0369.
Train: 2018-08-02T14:50:57.065400: step 1019, loss 3.77768.
Train: 2018-08-02T14:50:57.127886: step 1020, loss 2.89622.
Test: 2018-08-02T14:50:57.315371: step 1020, loss 3.81863.
Train: 2018-08-02T14:50:57.377830: step 1021, loss 4.65914.
Train: 2018-08-02T14:50:57.440312: step 1022, loss 5.41467.
Train: 2018-08-02T14:50:57.502828: step 1023, loss 4.15545.
Train: 2018-08-02T14:50:57.549662: step 1024, loss 3.52583.
Train: 2018-08-02T14:50:57.612149: step 1025, loss 3.9036.
Train: 2018-08-02T14:50:57.674633: step 1026, loss 4.02952.
Train: 2018-08-02T14:50:57.721498: step 1027, loss 3.14807.
Train: 2018-08-02T14:50:57.784012: step 1028, loss 3.9036.
Train: 2018-08-02T14:50:57.846499: step 1029, loss 3.9036.
Train: 2018-08-02T14:50:57.893331: step 1030, loss 4.78506.
Test: 2018-08-02T14:50:58.096408: step 1030, loss 3.81863.
Train: 2018-08-02T14:50:58.158924: step 1031, loss 3.9036.
Train: 2018-08-02T14:50:58.221410: step 1032, loss 4.02952.
Train: 2018-08-02T14:50:58.268244: step 1033, loss 4.02952.
Train: 2018-08-02T14:50:58.330760: step 1034, loss 4.40729.
Train: 2018-08-02T14:50:58.393216: step 1035, loss 4.65914.
Train: 2018-08-02T14:50:58.455725: step 1036, loss 4.53321.
Train: 2018-08-02T14:50:58.502594: step 1037, loss 3.9036.
Train: 2018-08-02T14:50:58.565080: step 1038, loss 2.64438.
Train: 2018-08-02T14:50:58.627566: step 1039, loss 5.0369.
Train: 2018-08-02T14:50:58.674436: step 1040, loss 4.53321.
Test: 2018-08-02T14:50:58.877477: step 1040, loss 3.81863.
Train: 2018-08-02T14:50:58.939992: step 1041, loss 4.40729.
Train: 2018-08-02T14:50:59.002479: step 1042, loss 3.39991.
Train: 2018-08-02T14:50:59.064962: step 1043, loss 4.53321.
Train: 2018-08-02T14:50:59.111837: step 1044, loss 4.15545.
Train: 2018-08-02T14:50:59.174316: step 1045, loss 3.9036.
Train: 2018-08-02T14:50:59.236798: step 1046, loss 4.28137.
Train: 2018-08-02T14:50:59.283659: step 1047, loss 3.27399.
Train: 2018-08-02T14:50:59.346148: step 1048, loss 4.53321.
Train: 2018-08-02T14:50:59.408632: step 1049, loss 4.65914.
Train: 2018-08-02T14:50:59.455496: step 1050, loss 4.78506.
Test: 2018-08-02T14:50:59.658568: step 1050, loss 3.81863.
Train: 2018-08-02T14:50:59.721059: step 1051, loss 3.9036.
Train: 2018-08-02T14:50:59.783544: step 1052, loss 4.02952.
Train: 2018-08-02T14:50:59.830378: step 1053, loss 4.15545.
Train: 2018-08-02T14:50:59.892889: step 1054, loss 4.15545.
Train: 2018-08-02T14:50:59.955379: step 1055, loss 4.65914.
Train: 2018-08-02T14:51:00.017863: step 1056, loss 4.02952.
Train: 2018-08-02T14:51:00.049111: step 1057, loss 2.95498.
Train: 2018-08-02T14:51:00.111593: step 1058, loss 3.65176.
Train: 2018-08-02T14:51:00.174081: step 1059, loss 4.15545.
Train: 2018-08-02T14:51:00.236563: step 1060, loss 3.65176.
Test: 2018-08-02T14:51:00.424013: step 1060, loss 3.81863.
Train: 2018-08-02T14:51:00.486476: step 1061, loss 4.91098.
Train: 2018-08-02T14:51:00.548959: step 1062, loss 3.27399.
Train: 2018-08-02T14:51:00.611445: step 1063, loss 3.27399.
Train: 2018-08-02T14:51:00.673960: step 1064, loss 3.65176.
Train: 2018-08-02T14:51:00.720826: step 1065, loss 3.52583.
Train: 2018-08-02T14:51:00.783281: step 1066, loss 3.77768.
Train: 2018-08-02T14:51:00.845796: step 1067, loss 3.39991.
Train: 2018-08-02T14:51:00.892662: step 1068, loss 3.39991.
Train: 2018-08-02T14:51:00.955146: step 1069, loss 3.14807.
Train: 2018-08-02T14:51:01.017599: step 1070, loss 4.28137.
Test: 2018-08-02T14:51:01.205086: step 1070, loss 3.81863.
Train: 2018-08-02T14:51:01.267568: step 1071, loss 3.39991.
Train: 2018-08-02T14:51:01.330057: step 1072, loss 5.41467.
Train: 2018-08-02T14:51:01.392543: step 1073, loss 5.28875.
Train: 2018-08-02T14:51:01.455024: step 1074, loss 5.0369.
Train: 2018-08-02T14:51:01.501888: step 1075, loss 3.65176.
Train: 2018-08-02T14:51:01.564378: step 1076, loss 3.9036.
Train: 2018-08-02T14:51:01.626833: step 1077, loss 3.27399.
Train: 2018-08-02T14:51:01.673731: step 1078, loss 4.65914.
Train: 2018-08-02T14:51:01.736207: step 1079, loss 3.27399.
Train: 2018-08-02T14:51:01.798701: step 1080, loss 3.27399.
Test: 2018-08-02T14:51:01.986148: step 1080, loss 3.81863.
Train: 2018-08-02T14:51:02.048642: step 1081, loss 4.65914.
Train: 2018-08-02T14:51:02.111125: step 1082, loss 3.9036.
Train: 2018-08-02T14:51:02.157959: step 1083, loss 4.02952.
Train: 2018-08-02T14:51:02.220469: step 1084, loss 4.15545.
Train: 2018-08-02T14:51:02.282963: step 1085, loss 5.79244.
Train: 2018-08-02T14:51:02.329824: step 1086, loss 3.52583.
Train: 2018-08-02T14:51:02.392278: step 1087, loss 4.40729.
Train: 2018-08-02T14:51:02.454764: step 1088, loss 3.27399.
Train: 2018-08-02T14:51:02.517283: step 1089, loss 3.65176.
Train: 2018-08-02T14:51:02.564139: step 1090, loss 4.78506.
Test: 2018-08-02T14:51:02.767215: step 1090, loss 3.81863.
Train: 2018-08-02T14:51:02.829675: step 1091, loss 3.65176.
Train: 2018-08-02T14:51:02.892191: step 1092, loss 2.89622.
Train: 2018-08-02T14:51:02.939055: step 1093, loss 3.27399.
Train: 2018-08-02T14:51:03.001541: step 1094, loss 3.14807.
Train: 2018-08-02T14:51:03.064023: step 1095, loss 4.65914.
Train: 2018-08-02T14:51:03.126511: step 1096, loss 3.14807.
Train: 2018-08-02T14:51:03.173374: step 1097, loss 4.65914.
Train: 2018-08-02T14:51:03.235856: step 1098, loss 4.53321.
Train: 2018-08-02T14:51:03.298350: step 1099, loss 4.65914.
Train: 2018-08-02T14:51:03.345211: step 1100, loss 3.65176.
Test: 2018-08-02T14:51:03.548257: step 1100, loss 3.81863.
Train: 2018-08-02T14:51:04.048141: step 1101, loss 4.02952.
Train: 2018-08-02T14:51:04.095004: step 1102, loss 3.9036.
Train: 2018-08-02T14:51:04.157520: step 1103, loss 5.79244.
Train: 2018-08-02T14:51:04.220007: step 1104, loss 4.91098.
Train: 2018-08-02T14:51:04.282491: step 1105, loss 4.65914.
Train: 2018-08-02T14:51:04.329356: step 1106, loss 4.53321.
Train: 2018-08-02T14:51:04.391842: step 1107, loss 3.65176.
Train: 2018-08-02T14:51:04.454297: step 1108, loss 3.65176.
Train: 2018-08-02T14:51:04.501190: step 1109, loss 4.40729.
Train: 2018-08-02T14:51:04.563675: step 1110, loss 4.40729.
Test: 2018-08-02T14:51:04.766724: step 1110, loss 3.81863.
Train: 2018-08-02T14:51:04.813617: step 1111, loss 3.77768.
Train: 2018-08-02T14:51:04.876072: step 1112, loss 4.02952.
Train: 2018-08-02T14:51:04.938587: step 1113, loss 3.77768.
Train: 2018-08-02T14:51:05.001070: step 1114, loss 4.02952.
Train: 2018-08-02T14:51:05.047940: step 1115, loss 5.16283.
Train: 2018-08-02T14:51:05.110422: step 1116, loss 3.14807.
Train: 2018-08-02T14:51:05.172910: step 1117, loss 4.91098.
Train: 2018-08-02T14:51:05.219742: step 1118, loss 3.9036.
Train: 2018-08-02T14:51:05.282227: step 1119, loss 3.65176.
Train: 2018-08-02T14:51:05.344742: step 1120, loss 3.9036.
Test: 2018-08-02T14:51:05.547790: step 1120, loss 3.81863.
Train: 2018-08-02T14:51:05.594654: step 1121, loss 4.28137.
Train: 2018-08-02T14:51:05.657138: step 1122, loss 3.39991.
Train: 2018-08-02T14:51:05.719654: step 1123, loss 3.9036.
Train: 2018-08-02T14:51:05.782121: step 1124, loss 4.40729.
Train: 2018-08-02T14:51:05.844595: step 1125, loss 4.28137.
Train: 2018-08-02T14:51:05.891460: step 1126, loss 4.02952.
Train: 2018-08-02T14:51:05.953975: step 1127, loss 4.02952.
Train: 2018-08-02T14:51:06.016453: step 1128, loss 4.91098.
Train: 2018-08-02T14:51:06.063293: step 1129, loss 4.15545.
Train: 2018-08-02T14:51:06.125809: step 1130, loss 5.03691.
Test: 2018-08-02T14:51:06.328874: step 1130, loss 3.81863.
Train: 2018-08-02T14:51:06.375751: step 1131, loss 3.9036.
Train: 2018-08-02T14:51:06.438205: step 1132, loss 3.27399.
Train: 2018-08-02T14:51:06.500691: step 1133, loss 4.91098.
Train: 2018-08-02T14:51:06.547589: step 1134, loss 3.77768.
Train: 2018-08-02T14:51:06.610066: step 1135, loss 3.27399.
Train: 2018-08-02T14:51:06.672559: step 1136, loss 4.65914.
Train: 2018-08-02T14:51:06.719390: step 1137, loss 5.28875.
Train: 2018-08-02T14:51:06.781877: step 1138, loss 4.28137.
Train: 2018-08-02T14:51:06.844392: step 1139, loss 3.9036.
Train: 2018-08-02T14:51:06.891258: step 1140, loss 4.02952.
Test: 2018-08-02T14:51:07.094333: step 1140, loss 3.81863.
Train: 2018-08-02T14:51:07.156818: step 1141, loss 5.0369.
Train: 2018-08-02T14:51:07.219305: step 1142, loss 4.65914.
Train: 2018-08-02T14:51:07.266169: step 1143, loss 2.51845.
Train: 2018-08-02T14:51:07.328653: step 1144, loss 3.14807.
Train: 2018-08-02T14:51:07.391139: step 1145, loss 3.77768.
Train: 2018-08-02T14:51:07.453618: step 1146, loss 4.28137.
Train: 2018-08-02T14:51:07.516103: step 1147, loss 4.28137.
Train: 2018-08-02T14:51:07.562974: step 1148, loss 3.77768.
Train: 2018-08-02T14:51:07.625459: step 1149, loss 5.16283.
Train: 2018-08-02T14:51:07.687943: step 1150, loss 3.02214.
Test: 2018-08-02T14:51:07.875393: step 1150, loss 3.81863.
Train: 2018-08-02T14:51:07.937884: step 1151, loss 4.15545.
Train: 2018-08-02T14:51:08.000370: step 1152, loss 4.40729.
Train: 2018-08-02T14:51:08.062855: step 1153, loss 4.15545.
Train: 2018-08-02T14:51:08.109721: step 1154, loss 4.40729.
Train: 2018-08-02T14:51:08.172175: step 1155, loss 5.41467.
Train: 2018-08-02T14:51:08.234690: step 1156, loss 4.15545.
Train: 2018-08-02T14:51:08.281552: step 1157, loss 3.65176.
Train: 2018-08-02T14:51:08.344041: step 1158, loss 3.9036.
Train: 2018-08-02T14:51:08.406521: step 1159, loss 3.52583.
Train: 2018-08-02T14:51:08.453384: step 1160, loss 3.77768.
Test: 2018-08-02T14:51:08.656494: step 1160, loss 3.81863.
Train: 2018-08-02T14:51:08.718952: step 1161, loss 5.16283.
Train: 2018-08-02T14:51:08.781437: step 1162, loss 3.77768.
Train: 2018-08-02T14:51:08.828301: step 1163, loss 3.9036.
Train: 2018-08-02T14:51:08.890757: step 1164, loss 3.27399.
Train: 2018-08-02T14:51:08.953272: step 1165, loss 3.77768.
Train: 2018-08-02T14:51:09.000132: step 1166, loss 3.52583.
Train: 2018-08-02T14:51:09.062591: step 1167, loss 3.9036.
Train: 2018-08-02T14:51:09.125077: step 1168, loss 3.52583.
Train: 2018-08-02T14:51:09.171941: step 1169, loss 2.89622.
Train: 2018-08-02T14:51:09.234427: step 1170, loss 5.0369.
Test: 2018-08-02T14:51:09.437528: step 1170, loss 3.81863.
Train: 2018-08-02T14:51:09.484398: step 1171, loss 4.28137.
Train: 2018-08-02T14:51:09.546886: step 1172, loss 5.28875.
Train: 2018-08-02T14:51:09.609369: step 1173, loss 3.52583.
Train: 2018-08-02T14:51:09.671825: step 1174, loss 3.52583.
Train: 2018-08-02T14:51:09.734309: step 1175, loss 3.9036.
Train: 2018-08-02T14:51:09.781205: step 1176, loss 3.65176.
Train: 2018-08-02T14:51:09.843688: step 1177, loss 3.52583.
Train: 2018-08-02T14:51:09.890555: step 1178, loss 3.9036.
Train: 2018-08-02T14:51:09.953008: step 1179, loss 4.53321.
Train: 2018-08-02T14:51:10.015533: step 1180, loss 4.15545.
Test: 2018-08-02T14:51:10.202980: step 1180, loss 3.81863.
Train: 2018-08-02T14:51:10.265436: step 1181, loss 3.77768.
Train: 2018-08-02T14:51:10.327952: step 1182, loss 3.9036.
Train: 2018-08-02T14:51:10.390436: step 1183, loss 3.39991.
Train: 2018-08-02T14:51:10.437271: step 1184, loss 4.53321.
Train: 2018-08-02T14:51:10.499786: step 1185, loss 4.65914.
Train: 2018-08-02T14:51:10.562240: step 1186, loss 3.27399.
Train: 2018-08-02T14:51:10.609129: step 1187, loss 3.77768.
Train: 2018-08-02T14:51:10.671624: step 1188, loss 4.28137.
Train: 2018-08-02T14:51:10.734106: step 1189, loss 3.27399.
Train: 2018-08-02T14:51:10.780967: step 1190, loss 3.77768.
Test: 2018-08-02T14:51:10.984019: step 1190, loss 3.81863.
Train: 2018-08-02T14:51:11.046535: step 1191, loss 3.52583.
Train: 2018-08-02T14:51:11.109017: step 1192, loss 3.9036.
Train: 2018-08-02T14:51:11.155852: step 1193, loss 3.52583.
Train: 2018-08-02T14:51:11.218363: step 1194, loss 4.40729.
Train: 2018-08-02T14:51:11.280851: step 1195, loss 4.02952.
Train: 2018-08-02T14:51:11.343307: step 1196, loss 2.7703.
Train: 2018-08-02T14:51:11.390171: step 1197, loss 4.78506.
Train: 2018-08-02T14:51:11.452690: step 1198, loss 3.39991.
Train: 2018-08-02T14:51:11.515181: step 1199, loss 4.28137.
Train: 2018-08-02T14:51:11.562007: step 1200, loss 4.28137.
Test: 2018-08-02T14:51:11.765084: step 1200, loss 3.81863.
Train: 2018-08-02T14:51:12.264992: step 1201, loss 3.52583.
Train: 2018-08-02T14:51:12.327452: step 1202, loss 4.40729.
Train: 2018-08-02T14:51:12.374346: step 1203, loss 3.27399.
Train: 2018-08-02T14:51:12.436833: step 1204, loss 4.15545.
Train: 2018-08-02T14:51:12.499286: step 1205, loss 3.65176.
Train: 2018-08-02T14:51:12.561772: step 1206, loss 3.65717.
Train: 2018-08-02T14:51:12.608661: step 1207, loss 4.53321.
Train: 2018-08-02T14:51:12.655528: step 1208, loss 2.68635.
Train: 2018-08-02T14:51:12.718017: step 1209, loss 3.65176.
Train: 2018-08-02T14:51:12.780501: step 1210, loss 4.91098.
Test: 2018-08-02T14:51:12.967927: step 1210, loss 3.81863.
Train: 2018-08-02T14:51:13.030440: step 1211, loss 3.52583.
Train: 2018-08-02T14:51:13.092927: step 1212, loss 4.53321.
Train: 2018-08-02T14:51:13.155382: step 1213, loss 3.52583.
Train: 2018-08-02T14:51:13.217886: step 1214, loss 3.65176.
Train: 2018-08-02T14:51:13.264770: step 1215, loss 3.9036.
Train: 2018-08-02T14:51:13.327248: step 1216, loss 3.27399.
Train: 2018-08-02T14:51:13.389703: step 1217, loss 4.15545.
Train: 2018-08-02T14:51:13.436567: step 1218, loss 4.15545.
Train: 2018-08-02T14:51:13.499083: step 1219, loss 4.65914.
Train: 2018-08-02T14:51:13.561568: step 1220, loss 4.53321.
Test: 2018-08-02T14:51:13.764618: step 1220, loss 3.81863.
Train: 2018-08-02T14:51:13.811510: step 1221, loss 4.28137.
Train: 2018-08-02T14:51:13.873995: step 1222, loss 3.77768.
Train: 2018-08-02T14:51:13.936449: step 1223, loss 3.27399.
Train: 2018-08-02T14:51:13.998962: step 1224, loss 4.02952.
Train: 2018-08-02T14:51:14.061422: step 1225, loss 4.91098.
Train: 2018-08-02T14:51:14.108315: step 1226, loss 4.65914.
Train: 2018-08-02T14:51:14.170803: step 1227, loss 4.53321.
Train: 2018-08-02T14:51:14.233287: step 1228, loss 3.65176.
Train: 2018-08-02T14:51:14.280153: step 1229, loss 4.40729.
Train: 2018-08-02T14:51:14.342630: step 1230, loss 3.27399.
Test: 2018-08-02T14:51:14.530087: step 1230, loss 3.81863.
Train: 2018-08-02T14:51:14.592577: step 1231, loss 4.65914.
Train: 2018-08-02T14:51:14.655031: step 1232, loss 2.64438.
Train: 2018-08-02T14:51:14.717516: step 1233, loss 3.77768.
Train: 2018-08-02T14:51:14.764382: step 1234, loss 4.53321.
Train: 2018-08-02T14:51:14.826867: step 1235, loss 3.27399.
Train: 2018-08-02T14:51:14.889354: step 1236, loss 4.15545.
Train: 2018-08-02T14:51:14.936246: step 1237, loss 2.26661.
Train: 2018-08-02T14:51:14.998702: step 1238, loss 3.39991.
Train: 2018-08-02T14:51:15.061211: step 1239, loss 3.39991.
Train: 2018-08-02T14:51:15.108081: step 1240, loss 3.77768.
Test: 2018-08-02T14:51:15.311159: step 1240, loss 3.81863.
Train: 2018-08-02T14:51:15.373647: step 1241, loss 3.9036.
Train: 2018-08-02T14:51:15.436099: step 1242, loss 4.78506.
Train: 2018-08-02T14:51:15.482993: step 1243, loss 2.51845.
Train: 2018-08-02T14:51:15.545450: step 1244, loss 4.78506.
Train: 2018-08-02T14:51:15.607966: step 1245, loss 3.52583.
Train: 2018-08-02T14:51:15.670419: step 1246, loss 3.65176.
Train: 2018-08-02T14:51:15.717284: step 1247, loss 4.28137.
Train: 2018-08-02T14:51:15.779797: step 1248, loss 3.52583.
Train: 2018-08-02T14:51:15.842256: step 1249, loss 4.78506.
Train: 2018-08-02T14:51:15.889142: step 1250, loss 2.89622.
Test: 2018-08-02T14:51:16.092195: step 1250, loss 3.81863.
Train: 2018-08-02T14:51:16.154681: step 1251, loss 4.40729.
Train: 2018-08-02T14:51:16.217166: step 1252, loss 4.65914.
Train: 2018-08-02T14:51:16.264031: step 1253, loss 3.52583.
Train: 2018-08-02T14:51:16.326517: step 1254, loss 3.39991.
Train: 2018-08-02T14:51:16.389003: step 1255, loss 4.40729.
Train: 2018-08-02T14:51:16.435897: step 1256, loss 4.02952.
Train: 2018-08-02T14:51:16.498380: step 1257, loss 3.52583.
Train: 2018-08-02T14:51:16.560870: step 1258, loss 4.40729.
Train: 2018-08-02T14:51:16.607733: step 1259, loss 4.91098.
Train: 2018-08-02T14:51:16.670213: step 1260, loss 4.91098.
Test: 2018-08-02T14:51:16.873261: step 1260, loss 3.81863.
Train: 2018-08-02T14:51:16.935778: step 1261, loss 3.27399.
Train: 2018-08-02T14:51:16.982642: step 1262, loss 4.91098.
Train: 2018-08-02T14:51:17.045130: step 1263, loss 4.91098.
Train: 2018-08-02T14:51:17.107613: step 1264, loss 3.77768.
Train: 2018-08-02T14:51:17.170102: step 1265, loss 3.65176.
Train: 2018-08-02T14:51:17.216933: step 1266, loss 4.02952.
Train: 2018-08-02T14:51:17.279447: step 1267, loss 4.02952.
Train: 2018-08-02T14:51:17.341904: step 1268, loss 4.53321.
Train: 2018-08-02T14:51:17.388767: step 1269, loss 4.40729.
Train: 2018-08-02T14:51:17.451282: step 1270, loss 3.02214.
Test: 2018-08-02T14:51:17.654329: step 1270, loss 3.81863.
Train: 2018-08-02T14:51:17.701193: step 1271, loss 3.77768.
Train: 2018-08-02T14:51:17.763705: step 1272, loss 4.78506.
Train: 2018-08-02T14:51:17.826194: step 1273, loss 4.02952.
Train: 2018-08-02T14:51:17.888652: step 1274, loss 2.7703.
Train: 2018-08-02T14:51:17.951136: step 1275, loss 4.53321.
Train: 2018-08-02T14:51:17.997999: step 1276, loss 3.65176.
Train: 2018-08-02T14:51:18.060514: step 1277, loss 3.9036.
Train: 2018-08-02T14:51:18.122988: step 1278, loss 3.39991.
Train: 2018-08-02T14:51:18.169867: step 1279, loss 3.14807.
Train: 2018-08-02T14:51:18.232318: step 1280, loss 3.14807.
Test: 2018-08-02T14:51:18.435396: step 1280, loss 3.81863.
Train: 2018-08-02T14:51:18.482291: step 1281, loss 3.52583.
Train: 2018-08-02T14:51:18.544780: step 1282, loss 4.78506.
Train: 2018-08-02T14:51:18.607261: step 1283, loss 4.02952.
Train: 2018-08-02T14:51:18.669751: step 1284, loss 3.65176.
Train: 2018-08-02T14:51:18.716611: step 1285, loss 4.40729.
Train: 2018-08-02T14:51:18.779066: step 1286, loss 5.28875.
Train: 2018-08-02T14:51:18.841581: step 1287, loss 4.28137.
Train: 2018-08-02T14:51:18.888449: step 1288, loss 3.9036.
Train: 2018-08-02T14:51:18.950932: step 1289, loss 4.28137.
Train: 2018-08-02T14:51:19.013387: step 1290, loss 5.5406.
Test: 2018-08-02T14:51:19.216464: step 1290, loss 3.81863.
Train: 2018-08-02T14:51:19.278973: step 1291, loss 4.28137.
Train: 2018-08-02T14:51:19.325843: step 1292, loss 3.9036.
Train: 2018-08-02T14:51:19.388328: step 1293, loss 4.78506.
Train: 2018-08-02T14:51:19.450815: step 1294, loss 3.9036.
Train: 2018-08-02T14:51:19.513308: step 1295, loss 3.52583.
Train: 2018-08-02T14:51:19.560134: step 1296, loss 4.53321.
Train: 2018-08-02T14:51:19.622642: step 1297, loss 4.15545.
Train: 2018-08-02T14:51:19.669482: step 1298, loss 4.15545.
Train: 2018-08-02T14:51:19.731967: step 1299, loss 4.65914.
Train: 2018-08-02T14:51:19.794487: step 1300, loss 4.02952.
Test: 2018-08-02T14:51:19.981939: step 1300, loss 3.81863.
Train: 2018-08-02T14:51:20.466171: step 1301, loss 4.15545.
Train: 2018-08-02T14:51:20.528657: step 1302, loss 3.52583.
Train: 2018-08-02T14:51:20.591172: step 1303, loss 2.89622.
Train: 2018-08-02T14:51:20.653655: step 1304, loss 4.53321.
Train: 2018-08-02T14:51:20.716113: step 1305, loss 4.65914.
Train: 2018-08-02T14:51:20.763009: step 1306, loss 4.40729.
Train: 2018-08-02T14:51:20.825492: step 1307, loss 3.9036.
Train: 2018-08-02T14:51:20.887949: step 1308, loss 4.40729.
Train: 2018-08-02T14:51:20.950433: step 1309, loss 4.15545.
Train: 2018-08-02T14:51:20.997327: step 1310, loss 4.78506.
Test: 2018-08-02T14:51:21.200374: step 1310, loss 3.81863.
Train: 2018-08-02T14:51:21.262890: step 1311, loss 3.77768.
Train: 2018-08-02T14:51:21.325375: step 1312, loss 3.14807.
Train: 2018-08-02T14:51:21.372209: step 1313, loss 3.52583.
Train: 2018-08-02T14:51:21.434725: step 1314, loss 3.39991.
Train: 2018-08-02T14:51:21.497229: step 1315, loss 4.28137.
Train: 2018-08-02T14:51:21.559691: step 1316, loss 2.64438.
Train: 2018-08-02T14:51:21.606559: step 1317, loss 3.77768.
Train: 2018-08-02T14:51:21.669048: step 1318, loss 4.40729.
Train: 2018-08-02T14:51:21.731530: step 1319, loss 4.15545.
Train: 2018-08-02T14:51:21.778364: step 1320, loss 4.40729.
Test: 2018-08-02T14:51:21.981440: step 1320, loss 3.81863.
Train: 2018-08-02T14:51:22.043960: step 1321, loss 3.52583.
Train: 2018-08-02T14:51:22.106413: step 1322, loss 3.14807.
Train: 2018-08-02T14:51:22.168930: step 1323, loss 4.15545.
Train: 2018-08-02T14:51:22.215792: step 1324, loss 4.65914.
Train: 2018-08-02T14:51:22.278275: step 1325, loss 3.27399.
Train: 2018-08-02T14:51:22.340762: step 1326, loss 5.16283.
Train: 2018-08-02T14:51:22.387628: step 1327, loss 5.5406.
Train: 2018-08-02T14:51:22.450112: step 1328, loss 4.15545.
Train: 2018-08-02T14:51:22.512598: step 1329, loss 4.02952.
Train: 2018-08-02T14:51:22.559461: step 1330, loss 3.77768.
Test: 2018-08-02T14:51:22.762508: step 1330, loss 3.81863.
Train: 2018-08-02T14:51:22.825033: step 1331, loss 4.02952.
Train: 2018-08-02T14:51:22.887503: step 1332, loss 2.7703.
Train: 2018-08-02T14:51:22.934379: step 1333, loss 4.28137.
Train: 2018-08-02T14:51:22.996846: step 1334, loss 4.65914.
Train: 2018-08-02T14:51:23.059345: step 1335, loss 4.91098.
Train: 2018-08-02T14:51:23.106177: step 1336, loss 3.65176.
Train: 2018-08-02T14:51:23.168663: step 1337, loss 4.02952.
Train: 2018-08-02T14:51:23.231149: step 1338, loss 4.15545.
Train: 2018-08-02T14:51:23.278012: step 1339, loss 3.65176.
Train: 2018-08-02T14:51:23.340497: step 1340, loss 4.40729.
Test: 2018-08-02T14:51:23.543575: step 1340, loss 3.81863.
Train: 2018-08-02T14:51:23.606061: step 1341, loss 4.28137.
Train: 2018-08-02T14:51:23.668579: step 1342, loss 3.65176.
Train: 2018-08-02T14:51:23.715440: step 1343, loss 3.65176.
Train: 2018-08-02T14:51:23.777925: step 1344, loss 4.91098.
Train: 2018-08-02T14:51:23.840382: step 1345, loss 4.65914.
Train: 2018-08-02T14:51:23.902867: step 1346, loss 4.40729.
Train: 2018-08-02T14:51:23.949761: step 1347, loss 4.15545.
Train: 2018-08-02T14:51:24.012217: step 1348, loss 3.77768.
Train: 2018-08-02T14:51:24.074732: step 1349, loss 3.77768.
Train: 2018-08-02T14:51:24.121565: step 1350, loss 4.65914.
Test: 2018-08-02T14:51:24.324666: step 1350, loss 3.81863.
Train: 2018-08-02T14:51:24.387158: step 1351, loss 2.64438.
Train: 2018-08-02T14:51:24.449643: step 1352, loss 3.27399.
Train: 2018-08-02T14:51:24.496478: step 1353, loss 3.27399.
Train: 2018-08-02T14:51:24.558994: step 1354, loss 3.52583.
Train: 2018-08-02T14:51:24.621464: step 1355, loss 3.77768.
Train: 2018-08-02T14:51:24.683965: step 1356, loss 4.78506.
Train: 2018-08-02T14:51:24.730827: step 1357, loss 4.15545.
Train: 2018-08-02T14:51:24.793317: step 1358, loss 4.40729.
Train: 2018-08-02T14:51:24.840171: step 1359, loss 4.29816.
Train: 2018-08-02T14:51:24.902632: step 1360, loss 3.65176.
Test: 2018-08-02T14:51:25.090101: step 1360, loss 3.81863.
Train: 2018-08-02T14:51:25.152601: step 1361, loss 3.9036.
Train: 2018-08-02T14:51:25.215059: step 1362, loss 3.14807.
Train: 2018-08-02T14:51:25.277597: step 1363, loss 5.0369.
Train: 2018-08-02T14:51:25.324409: step 1364, loss 3.65176.
Train: 2018-08-02T14:51:25.386924: step 1365, loss 3.52583.
Train: 2018-08-02T14:51:25.449380: step 1366, loss 4.40729.
Train: 2018-08-02T14:51:25.496273: step 1367, loss 3.14807.
Train: 2018-08-02T14:51:25.558759: step 1368, loss 5.0369.
Train: 2018-08-02T14:51:25.621215: step 1369, loss 3.77768.
Train: 2018-08-02T14:51:25.668111: step 1370, loss 2.89622.
Test: 2018-08-02T14:51:25.871154: step 1370, loss 3.81863.
Train: 2018-08-02T14:51:25.933670: step 1371, loss 3.39991.
Train: 2018-08-02T14:51:25.996126: step 1372, loss 4.28137.
Train: 2018-08-02T14:51:26.043024: step 1373, loss 3.9036.
Train: 2018-08-02T14:51:26.105507: step 1374, loss 3.77768.
Train: 2018-08-02T14:51:26.167960: step 1375, loss 3.14807.
Train: 2018-08-02T14:51:26.230476: step 1376, loss 4.28137.
Train: 2018-08-02T14:51:26.277343: step 1377, loss 3.77768.
Train: 2018-08-02T14:51:26.339797: step 1378, loss 3.27399.
Train: 2018-08-02T14:51:26.402314: step 1379, loss 3.52583.
Train: 2018-08-02T14:51:26.449146: step 1380, loss 4.28137.
Test: 2018-08-02T14:51:26.652221: step 1380, loss 3.81863.
Train: 2018-08-02T14:51:26.714733: step 1381, loss 4.28137.
Train: 2018-08-02T14:51:26.777223: step 1382, loss 3.39991.
Train: 2018-08-02T14:51:26.824087: step 1383, loss 4.28137.
Train: 2018-08-02T14:51:26.886575: step 1384, loss 3.02214.
Train: 2018-08-02T14:51:26.949058: step 1385, loss 5.16283.
Train: 2018-08-02T14:51:26.995925: step 1386, loss 2.89622.
Train: 2018-08-02T14:51:27.058407: step 1387, loss 3.14807.
Train: 2018-08-02T14:51:27.120864: step 1388, loss 4.40729.
Train: 2018-08-02T14:51:27.183379: step 1389, loss 2.7703.
Train: 2018-08-02T14:51:27.230212: step 1390, loss 4.91098.
Test: 2018-08-02T14:51:27.433289: step 1390, loss 3.81863.
Train: 2018-08-02T14:51:27.495800: step 1391, loss 4.40729.
Train: 2018-08-02T14:51:27.542638: step 1392, loss 4.15545.
Train: 2018-08-02T14:51:27.605154: step 1393, loss 3.9036.
Train: 2018-08-02T14:51:27.667644: step 1394, loss 3.52583.
Train: 2018-08-02T14:51:27.730095: step 1395, loss 4.40729.
Train: 2018-08-02T14:51:27.776987: step 1396, loss 4.02952.
Train: 2018-08-02T14:51:27.839475: step 1397, loss 4.91098.
Train: 2018-08-02T14:51:27.901957: step 1398, loss 2.7703.
Train: 2018-08-02T14:51:27.964415: step 1399, loss 5.66652.
Train: 2018-08-02T14:51:28.011312: step 1400, loss 4.28137.
Test: 2018-08-02T14:51:28.214386: step 1400, loss 3.81863.
Train: 2018-08-02T14:51:28.823589: step 1401, loss 4.02952.
Train: 2018-08-02T14:51:28.901696: step 1402, loss 4.15545.
Train: 2018-08-02T14:51:28.964180: step 1403, loss 4.28137.
Train: 2018-08-02T14:51:29.026667: step 1404, loss 3.65176.
Train: 2018-08-02T14:51:29.089153: step 1405, loss 4.40729.
Train: 2018-08-02T14:51:29.151636: step 1406, loss 5.41467.
Train: 2018-08-02T14:51:29.214148: step 1407, loss 3.9036.
Train: 2018-08-02T14:51:29.260987: step 1408, loss 4.91098.
Train: 2018-08-02T14:51:29.323489: step 1409, loss 3.27399.
Train: 2018-08-02T14:51:29.385957: step 1410, loss 4.02952.
Test: 2018-08-02T14:51:29.582704: step 1410, loss 3.81863.
Train: 2018-08-02T14:51:29.660810: step 1411, loss 3.65176.
Train: 2018-08-02T14:51:29.723327: step 1412, loss 5.0369.
Train: 2018-08-02T14:51:29.770191: step 1413, loss 4.02952.
Train: 2018-08-02T14:51:29.832675: step 1414, loss 3.52583.
Train: 2018-08-02T14:51:29.895131: step 1415, loss 4.40729.
Train: 2018-08-02T14:51:29.941996: step 1416, loss 4.02952.
Train: 2018-08-02T14:51:30.004511: step 1417, loss 4.02952.
Train: 2018-08-02T14:51:30.066995: step 1418, loss 3.77768.
Train: 2018-08-02T14:51:30.113860: step 1419, loss 3.9036.
Train: 2018-08-02T14:51:30.176348: step 1420, loss 4.91098.
Test: 2018-08-02T14:51:30.379391: step 1420, loss 3.81863.
Train: 2018-08-02T14:51:30.441908: step 1421, loss 3.65176.
Train: 2018-08-02T14:51:30.488772: step 1422, loss 3.39991.
Train: 2018-08-02T14:51:30.551228: step 1423, loss 4.02952.
Train: 2018-08-02T14:51:30.613714: step 1424, loss 4.65914.
Train: 2018-08-02T14:51:30.676231: step 1425, loss 4.28137.
Train: 2018-08-02T14:51:30.723061: step 1426, loss 3.77768.
Train: 2018-08-02T14:51:30.785578: step 1427, loss 3.9036.
Train: 2018-08-02T14:51:30.848058: step 1428, loss 5.5406.
Train: 2018-08-02T14:51:30.894927: step 1429, loss 2.89622.
Train: 2018-08-02T14:51:30.957415: step 1430, loss 4.28137.
Test: 2018-08-02T14:51:31.160460: step 1430, loss 3.81863.
Train: 2018-08-02T14:51:31.222945: step 1431, loss 4.40729.
Train: 2018-08-02T14:51:31.269810: step 1432, loss 3.52583.
Train: 2018-08-02T14:51:31.332328: step 1433, loss 4.91098.
Train: 2018-08-02T14:51:31.394780: step 1434, loss 3.52583.
Train: 2018-08-02T14:51:31.441672: step 1435, loss 4.15545.
Train: 2018-08-02T14:51:31.504154: step 1436, loss 4.15545.
Train: 2018-08-02T14:51:31.566614: step 1437, loss 4.02952.
Train: 2018-08-02T14:51:31.613504: step 1438, loss 5.0369.
Train: 2018-08-02T14:51:31.675989: step 1439, loss 4.91098.
Train: 2018-08-02T14:51:31.738480: step 1440, loss 4.28137.
Test: 2018-08-02T14:51:31.925929: step 1440, loss 3.81863.
Train: 2018-08-02T14:51:31.988416: step 1441, loss 3.9036.
Train: 2018-08-02T14:51:32.050909: step 1442, loss 3.39991.
Train: 2018-08-02T14:51:32.113392: step 1443, loss 4.91098.
Train: 2018-08-02T14:51:32.160256: step 1444, loss 3.52583.
Train: 2018-08-02T14:51:32.222737: step 1445, loss 4.65914.
Train: 2018-08-02T14:51:32.285197: step 1446, loss 3.77768.
Train: 2018-08-02T14:51:32.332089: step 1447, loss 4.15545.
Train: 2018-08-02T14:51:32.394546: step 1448, loss 4.15545.
Train: 2018-08-02T14:51:32.457065: step 1449, loss 3.77768.
Train: 2018-08-02T14:51:32.503919: step 1450, loss 3.65176.
Test: 2018-08-02T14:51:32.707006: step 1450, loss 3.81863.
Train: 2018-08-02T14:51:32.769458: step 1451, loss 4.91098.
Train: 2018-08-02T14:51:32.831976: step 1452, loss 4.40729.
Train: 2018-08-02T14:51:32.894459: step 1453, loss 4.40729.
Train: 2018-08-02T14:51:32.941321: step 1454, loss 4.02952.
Train: 2018-08-02T14:51:33.003782: step 1455, loss 4.15545.
Train: 2018-08-02T14:51:33.066290: step 1456, loss 3.77768.
Train: 2018-08-02T14:51:33.113152: step 1457, loss 3.77768.
Train: 2018-08-02T14:51:33.175613: step 1458, loss 3.39991.
Train: 2018-08-02T14:51:33.238129: step 1459, loss 3.65176.
Train: 2018-08-02T14:51:33.284992: step 1460, loss 3.65176.
Test: 2018-08-02T14:51:33.488038: step 1460, loss 3.81863.
Train: 2018-08-02T14:51:33.550555: step 1461, loss 3.02214.
Train: 2018-08-02T14:51:33.613040: step 1462, loss 3.65176.
Train: 2018-08-02T14:51:33.659899: step 1463, loss 3.65176.
Train: 2018-08-02T14:51:33.722360: step 1464, loss 3.52583.
Train: 2018-08-02T14:51:33.784847: step 1465, loss 2.51845.
Train: 2018-08-02T14:51:33.847331: step 1466, loss 3.39991.
Train: 2018-08-02T14:51:33.894225: step 1467, loss 4.02952.
Train: 2018-08-02T14:51:33.956681: step 1468, loss 3.9036.
Train: 2018-08-02T14:51:34.019196: step 1469, loss 3.52583.
Train: 2018-08-02T14:51:34.066056: step 1470, loss 3.77768.
Test: 2018-08-02T14:51:34.269106: step 1470, loss 3.81863.
Train: 2018-08-02T14:51:34.331624: step 1471, loss 3.9036.
Train: 2018-08-02T14:51:34.394107: step 1472, loss 4.02952.
Train: 2018-08-02T14:51:34.440941: step 1473, loss 4.15545.
Train: 2018-08-02T14:51:34.503429: step 1474, loss 4.02952.
Train: 2018-08-02T14:51:34.565940: step 1475, loss 4.28137.
Train: 2018-08-02T14:51:34.628427: step 1476, loss 3.39991.
Train: 2018-08-02T14:51:34.675294: step 1477, loss 3.39991.
Train: 2018-08-02T14:51:34.737778: step 1478, loss 3.39991.
Train: 2018-08-02T14:51:34.800232: step 1479, loss 3.65176.
Train: 2018-08-02T14:51:34.847126: step 1480, loss 4.28137.
Test: 2018-08-02T14:51:35.050203: step 1480, loss 3.81863.
Train: 2018-08-02T14:51:35.112660: step 1481, loss 4.15545.
Train: 2018-08-02T14:51:35.175169: step 1482, loss 4.28137.
Train: 2018-08-02T14:51:35.237654: step 1483, loss 4.65914.
Train: 2018-08-02T14:51:35.284520: step 1484, loss 4.02952.
Train: 2018-08-02T14:51:35.346980: step 1485, loss 3.65176.
Train: 2018-08-02T14:51:35.409494: step 1486, loss 3.77768.
Train: 2018-08-02T14:51:35.456329: step 1487, loss 4.53321.
Train: 2018-08-02T14:51:35.518815: step 1488, loss 4.15545.
Train: 2018-08-02T14:51:35.581300: step 1489, loss 4.28137.
Train: 2018-08-02T14:51:35.643809: step 1490, loss 4.02952.
Test: 2018-08-02T14:51:35.831274: step 1490, loss 3.81863.
Train: 2018-08-02T14:51:35.893756: step 1491, loss 3.27399.
Train: 2018-08-02T14:51:35.956239: step 1492, loss 4.28137.
Train: 2018-08-02T14:51:36.003106: step 1493, loss 4.40729.
Train: 2018-08-02T14:51:36.065589: step 1494, loss 3.52583.
Train: 2018-08-02T14:51:36.128066: step 1495, loss 3.77768.
Train: 2018-08-02T14:51:36.174941: step 1496, loss 4.15545.
Train: 2018-08-02T14:51:36.237425: step 1497, loss 5.0369.
Train: 2018-08-02T14:51:36.299912: step 1498, loss 3.52583.
Train: 2018-08-02T14:51:36.362397: step 1499, loss 3.9036.
Train: 2018-08-02T14:51:36.409262: step 1500, loss 4.65914.
Test: 2018-08-02T14:51:36.612338: step 1500, loss 3.81863.
Train: 2018-08-02T14:51:37.127811: step 1501, loss 3.65176.
Train: 2018-08-02T14:51:37.190327: step 1502, loss 4.40729.
Train: 2018-08-02T14:51:37.252783: step 1503, loss 5.5406.
Train: 2018-08-02T14:51:37.299678: step 1504, loss 4.91098.
Train: 2018-08-02T14:51:37.362133: step 1505, loss 3.77768.
Train: 2018-08-02T14:51:37.424647: step 1506, loss 3.52583.
Train: 2018-08-02T14:51:37.471486: step 1507, loss 4.02952.
Train: 2018-08-02T14:51:37.533969: step 1508, loss 4.40729.
Train: 2018-08-02T14:51:37.596483: step 1509, loss 5.0369.
Train: 2018-08-02T14:51:37.643343: step 1510, loss 4.56679.
Test: 2018-08-02T14:51:37.830773: step 1510, loss 3.81863.
Train: 2018-08-02T14:51:37.893258: step 1511, loss 4.40729.
Train: 2018-08-02T14:51:37.955743: step 1512, loss 3.52583.
Train: 2018-08-02T14:51:38.018228: step 1513, loss 4.02952.
Train: 2018-08-02T14:51:38.065121: step 1514, loss 3.27399.
Train: 2018-08-02T14:51:38.127580: step 1515, loss 3.77768.
Train: 2018-08-02T14:51:38.190091: step 1516, loss 4.15545.
Train: 2018-08-02T14:51:38.236927: step 1517, loss 4.28137.
Train: 2018-08-02T14:51:38.299446: step 1518, loss 5.28875.
Train: 2018-08-02T14:51:38.361929: step 1519, loss 3.65176.
Train: 2018-08-02T14:51:38.424383: step 1520, loss 4.65914.
Test: 2018-08-02T14:51:38.627472: step 1520, loss 3.81863.
Train: 2018-08-02T14:51:38.674358: step 1521, loss 4.53321.
Train: 2018-08-02T14:51:38.736840: step 1522, loss 3.65176.
Train: 2018-08-02T14:51:38.799296: step 1523, loss 3.27399.
Train: 2018-08-02T14:51:38.861799: step 1524, loss 3.27399.
Train: 2018-08-02T14:51:38.908678: step 1525, loss 3.27399.
Train: 2018-08-02T14:51:38.971161: step 1526, loss 4.02952.
Train: 2018-08-02T14:51:39.033641: step 1527, loss 4.78506.
Train: 2018-08-02T14:51:39.096102: step 1528, loss 3.39991.
Train: 2018-08-02T14:51:39.142966: step 1529, loss 4.28137.
Train: 2018-08-02T14:51:39.205450: step 1530, loss 3.52583.
Test: 2018-08-02T14:51:39.392907: step 1530, loss 3.81863.
Train: 2018-08-02T14:51:39.455392: step 1531, loss 4.15545.
Train: 2018-08-02T14:51:39.517877: step 1532, loss 4.40729.
Train: 2018-08-02T14:51:39.580396: step 1533, loss 4.78506.
Train: 2018-08-02T14:51:39.642849: step 1534, loss 3.65176.
Train: 2018-08-02T14:51:39.705334: step 1535, loss 4.53321.
Train: 2018-08-02T14:51:39.752197: step 1536, loss 3.65176.
Train: 2018-08-02T14:51:39.814684: step 1537, loss 4.40729.
Train: 2018-08-02T14:51:39.877215: step 1538, loss 4.40729.
Train: 2018-08-02T14:51:39.924031: step 1539, loss 3.77768.
Train: 2018-08-02T14:51:39.986548: step 1540, loss 4.65914.
Test: 2018-08-02T14:51:40.189595: step 1540, loss 3.81863.
Train: 2018-08-02T14:51:40.252112: step 1541, loss 4.15545.
Train: 2018-08-02T14:51:40.298977: step 1542, loss 3.65176.
Train: 2018-08-02T14:51:40.361460: step 1543, loss 3.77768.
Train: 2018-08-02T14:51:40.423949: step 1544, loss 3.52583.
Train: 2018-08-02T14:51:40.470810: step 1545, loss 3.9036.
Train: 2018-08-02T14:51:40.533264: step 1546, loss 4.40729.
Train: 2018-08-02T14:51:40.595775: step 1547, loss 4.02952.
Train: 2018-08-02T14:51:40.658267: step 1548, loss 4.15545.
Train: 2018-08-02T14:51:40.705133: step 1549, loss 5.0369.
Train: 2018-08-02T14:51:40.767615: step 1550, loss 3.14807.
Test: 2018-08-02T14:51:40.970686: step 1550, loss 3.81863.
Train: 2018-08-02T14:51:41.033148: step 1551, loss 3.9036.
Train: 2018-08-02T14:51:41.095663: step 1552, loss 3.53125.
Train: 2018-08-02T14:51:41.142527: step 1553, loss 3.14807.
Train: 2018-08-02T14:51:41.205012: step 1554, loss 3.39991.
Train: 2018-08-02T14:51:41.267498: step 1555, loss 4.02952.
Train: 2018-08-02T14:51:41.314360: step 1556, loss 3.02214.
Train: 2018-08-02T14:51:41.376847: step 1557, loss 3.9036.
Train: 2018-08-02T14:51:41.439331: step 1558, loss 3.65176.
Train: 2018-08-02T14:51:41.486167: step 1559, loss 4.28137.
Train: 2018-08-02T14:51:41.548682: step 1560, loss 4.53321.
Test: 2018-08-02T14:51:41.751728: step 1560, loss 3.81863.
Train: 2018-08-02T14:51:41.798626: step 1561, loss 4.28137.
Train: 2018-08-02T14:51:41.861078: step 1562, loss 4.28137.
Train: 2018-08-02T14:51:41.923565: step 1563, loss 3.65176.
Train: 2018-08-02T14:51:41.986052: step 1564, loss 3.65176.
Train: 2018-08-02T14:51:42.048538: step 1565, loss 4.28137.
Train: 2018-08-02T14:51:42.095430: step 1566, loss 4.65914.
Train: 2018-08-02T14:51:42.157885: step 1567, loss 4.40729.
Train: 2018-08-02T14:51:42.220423: step 1568, loss 3.02214.
Train: 2018-08-02T14:51:42.267233: step 1569, loss 3.39991.
Train: 2018-08-02T14:51:42.329719: step 1570, loss 3.39991.
Test: 2018-08-02T14:51:42.532812: step 1570, loss 3.81863.
Train: 2018-08-02T14:51:42.579694: step 1571, loss 3.77768.
Train: 2018-08-02T14:51:42.642176: step 1572, loss 4.91098.
Train: 2018-08-02T14:51:42.704664: step 1573, loss 3.27399.
Train: 2018-08-02T14:51:42.767118: step 1574, loss 5.0369.
Train: 2018-08-02T14:51:42.814013: step 1575, loss 4.02952.
Train: 2018-08-02T14:51:42.876466: step 1576, loss 4.28137.
Train: 2018-08-02T14:51:42.938981: step 1577, loss 4.65914.
Train: 2018-08-02T14:51:42.985816: step 1578, loss 2.51845.
Train: 2018-08-02T14:51:43.048331: step 1579, loss 4.28137.
Train: 2018-08-02T14:51:43.110810: step 1580, loss 4.15545.
Test: 2018-08-02T14:51:43.298272: step 1580, loss 3.81863.
Train: 2018-08-02T14:51:43.360727: step 1581, loss 3.77768.
Train: 2018-08-02T14:51:43.423212: step 1582, loss 3.27399.
Train: 2018-08-02T14:51:43.485715: step 1583, loss 4.15545.
Train: 2018-08-02T14:51:43.548214: step 1584, loss 4.02952.
Train: 2018-08-02T14:51:43.610693: step 1585, loss 4.02952.
Train: 2018-08-02T14:51:43.657564: step 1586, loss 5.16283.
Train: 2018-08-02T14:51:43.720043: step 1587, loss 3.77768.
Train: 2018-08-02T14:51:43.782535: step 1588, loss 3.9036.
Train: 2018-08-02T14:51:43.845020: step 1589, loss 3.77768.
Train: 2018-08-02T14:51:43.891854: step 1590, loss 3.52583.
Test: 2018-08-02T14:51:44.094929: step 1590, loss 3.81863.
Train: 2018-08-02T14:51:44.157446: step 1591, loss 4.40729.
Train: 2018-08-02T14:51:44.219902: step 1592, loss 4.40729.
Train: 2018-08-02T14:51:44.282417: step 1593, loss 3.39991.
Train: 2018-08-02T14:51:44.329285: step 1594, loss 3.77768.
Train: 2018-08-02T14:51:44.391767: step 1595, loss 3.9036.
Train: 2018-08-02T14:51:44.454255: step 1596, loss 5.16283.
Train: 2018-08-02T14:51:44.501116: step 1597, loss 4.02952.
Train: 2018-08-02T14:51:44.563599: step 1598, loss 4.91098.
Train: 2018-08-02T14:51:44.626083: step 1599, loss 5.16283.
Train: 2018-08-02T14:51:44.672953: step 1600, loss 2.89622.
Test: 2018-08-02T14:51:44.876022: step 1600, loss 3.81863.
Train: 2018-08-02T14:51:45.375881: step 1601, loss 3.65176.
Train: 2018-08-02T14:51:45.438396: step 1602, loss 3.27399.
Train: 2018-08-02T14:51:45.485260: step 1603, loss 5.16283.
Train: 2018-08-02T14:51:45.547749: step 1604, loss 3.65176.
Train: 2018-08-02T14:51:45.610231: step 1605, loss 4.28137.
Train: 2018-08-02T14:51:45.657097: step 1606, loss 3.65176.
Train: 2018-08-02T14:51:45.719550: step 1607, loss 3.52583.
Train: 2018-08-02T14:51:45.782038: step 1608, loss 2.64437.
Train: 2018-08-02T14:51:45.844552: step 1609, loss 2.89622.
Train: 2018-08-02T14:51:45.891415: step 1610, loss 4.02952.
Test: 2018-08-02T14:51:46.094461: step 1610, loss 3.81863.
Train: 2018-08-02T14:51:46.156978: step 1611, loss 3.77768.
Train: 2018-08-02T14:51:46.219464: step 1612, loss 4.65914.
Train: 2018-08-02T14:51:46.266327: step 1613, loss 3.39991.
Train: 2018-08-02T14:51:46.328816: step 1614, loss 4.15545.
Train: 2018-08-02T14:51:46.391298: step 1615, loss 4.28137.
Train: 2018-08-02T14:51:46.438159: step 1616, loss 3.02214.
Train: 2018-08-02T14:51:46.500617: step 1617, loss 3.65176.
Train: 2018-08-02T14:51:46.563104: step 1618, loss 4.78506.
Train: 2018-08-02T14:51:46.625588: step 1619, loss 4.65914.
Train: 2018-08-02T14:51:46.672477: step 1620, loss 3.52583.
Test: 2018-08-02T14:51:46.875530: step 1620, loss 3.81863.
Train: 2018-08-02T14:51:46.938045: step 1621, loss 3.14807.
Train: 2018-08-02T14:51:47.000530: step 1622, loss 4.15545.
Train: 2018-08-02T14:51:47.047398: step 1623, loss 4.65914.
Train: 2018-08-02T14:51:47.109850: step 1624, loss 3.39991.
Train: 2018-08-02T14:51:47.172366: step 1625, loss 4.53321.
Train: 2018-08-02T14:51:47.219229: step 1626, loss 2.64438.
Train: 2018-08-02T14:51:47.281684: step 1627, loss 3.52583.
Train: 2018-08-02T14:51:47.344206: step 1628, loss 4.53321.
Train: 2018-08-02T14:51:47.391033: step 1629, loss 3.77768.
Train: 2018-08-02T14:51:47.453553: step 1630, loss 3.39991.
Test: 2018-08-02T14:51:47.656632: step 1630, loss 3.81863.
Train: 2018-08-02T14:51:47.703494: step 1631, loss 4.65914.
Train: 2018-08-02T14:51:47.765976: step 1632, loss 3.52583.
Train: 2018-08-02T14:51:47.828464: step 1633, loss 4.15545.
Train: 2018-08-02T14:51:47.890917: step 1634, loss 3.9036.
Train: 2018-08-02T14:51:47.953436: step 1635, loss 4.15545.
Train: 2018-08-02T14:51:48.000296: step 1636, loss 3.65176.
Train: 2018-08-02T14:51:48.062779: step 1637, loss 4.91098.
Train: 2018-08-02T14:51:48.125268: step 1638, loss 3.52583.
Train: 2018-08-02T14:51:48.172133: step 1639, loss 5.16283.
Train: 2018-08-02T14:51:48.234586: step 1640, loss 4.15545.
Test: 2018-08-02T14:51:48.437688: step 1640, loss 3.81863.
Train: 2018-08-02T14:51:48.500150: step 1641, loss 4.40729.
Train: 2018-08-02T14:51:48.562664: step 1642, loss 3.52583.
Train: 2018-08-02T14:51:48.609529: step 1643, loss 4.53321.
Train: 2018-08-02T14:51:48.671985: step 1644, loss 3.77768.
Train: 2018-08-02T14:51:48.734471: step 1645, loss 4.02952.
Train: 2018-08-02T14:51:48.781363: step 1646, loss 4.53321.
Train: 2018-08-02T14:51:48.843819: step 1647, loss 4.40729.
Train: 2018-08-02T14:51:48.906305: step 1648, loss 4.02952.
Train: 2018-08-02T14:51:48.953202: step 1649, loss 5.0369.
Train: 2018-08-02T14:51:49.015684: step 1650, loss 4.28137.
Test: 2018-08-02T14:51:49.218731: step 1650, loss 3.81863.
Train: 2018-08-02T14:51:49.281242: step 1651, loss 4.78506.
Train: 2018-08-02T14:51:49.328114: step 1652, loss 3.9036.
Train: 2018-08-02T14:51:49.390565: step 1653, loss 4.78506.
Train: 2018-08-02T14:51:49.453050: step 1654, loss 3.77768.
Train: 2018-08-02T14:51:49.515562: step 1655, loss 3.14807.
Train: 2018-08-02T14:51:49.562428: step 1656, loss 3.9036.
Train: 2018-08-02T14:51:49.624911: step 1657, loss 4.53321.
Train: 2018-08-02T14:51:49.687405: step 1658, loss 4.15545.
Train: 2018-08-02T14:51:49.734279: step 1659, loss 5.16283.
Train: 2018-08-02T14:51:49.796722: step 1660, loss 4.91098.
Test: 2018-08-02T14:51:49.999799: step 1660, loss 3.81863.
Train: 2018-08-02T14:51:50.031072: step 1661, loss 4.29816.
Train: 2018-08-02T14:51:50.093557: step 1662, loss 5.5406.
Train: 2018-08-02T14:51:50.156012: step 1663, loss 4.65914.
Train: 2018-08-02T14:51:50.218526: step 1664, loss 3.52583.
Train: 2018-08-02T14:51:50.281011: step 1665, loss 3.52583.
Train: 2018-08-02T14:51:50.327875: step 1666, loss 4.91098.
Train: 2018-08-02T14:51:50.390333: step 1667, loss 2.64978.
Train: 2018-08-02T14:51:50.452851: step 1668, loss 4.40729.
Train: 2018-08-02T14:51:50.499706: step 1669, loss 3.77768.
Train: 2018-08-02T14:51:50.562196: step 1670, loss 4.28137.
Test: 2018-08-02T14:51:50.765268: step 1670, loss 3.81863.
Train: 2018-08-02T14:51:50.827729: step 1671, loss 4.40729.
Train: 2018-08-02T14:51:50.874623: step 1672, loss 4.78506.
Train: 2018-08-02T14:51:50.937108: step 1673, loss 3.77768.
Train: 2018-08-02T14:51:50.999566: step 1674, loss 3.9036.
Train: 2018-08-02T14:51:51.062078: step 1675, loss 2.51845.
Train: 2018-08-02T14:51:51.108944: step 1676, loss 4.15545.
Train: 2018-08-02T14:51:51.171431: step 1677, loss 4.53321.
Train: 2018-08-02T14:51:51.233885: step 1678, loss 4.28137.
Train: 2018-08-02T14:51:51.280778: step 1679, loss 3.9036.
Train: 2018-08-02T14:51:51.343264: step 1680, loss 4.02952.
Test: 2018-08-02T14:51:51.546341: step 1680, loss 3.81863.
Train: 2018-08-02T14:51:51.608827: step 1681, loss 3.27399.
Train: 2018-08-02T14:51:51.655690: step 1682, loss 4.15545.
Train: 2018-08-02T14:51:51.718146: step 1683, loss 4.40729.
Train: 2018-08-02T14:51:51.780632: step 1684, loss 3.65176.
Train: 2018-08-02T14:51:51.843118: step 1685, loss 3.52583.
Train: 2018-08-02T14:51:51.889980: step 1686, loss 4.15545.
Train: 2018-08-02T14:51:51.952465: step 1687, loss 5.28875.
Train: 2018-08-02T14:51:52.014977: step 1688, loss 3.9036.
Train: 2018-08-02T14:51:52.061842: step 1689, loss 3.77768.
Train: 2018-08-02T14:51:52.124324: step 1690, loss 4.40729.
Test: 2018-08-02T14:51:52.327377: step 1690, loss 3.81863.
Train: 2018-08-02T14:51:52.389864: step 1691, loss 3.77768.
Train: 2018-08-02T14:51:52.436727: step 1692, loss 3.14807.
Train: 2018-08-02T14:51:52.499237: step 1693, loss 2.7703.
Train: 2018-08-02T14:51:52.561732: step 1694, loss 3.27399.
Train: 2018-08-02T14:51:52.624184: step 1695, loss 5.16283.
Train: 2018-08-02T14:51:52.671072: step 1696, loss 4.28137.
Train: 2018-08-02T14:51:52.733563: step 1697, loss 3.39991.
Train: 2018-08-02T14:51:52.796044: step 1698, loss 1.88884.
Train: 2018-08-02T14:51:52.842912: step 1699, loss 4.28137.
Train: 2018-08-02T14:51:52.905398: step 1700, loss 5.28875.
Test: 2018-08-02T14:51:53.108445: step 1700, loss 3.81863.
Train: 2018-08-02T14:51:53.670813: step 1701, loss 3.27399.
Train: 2018-08-02T14:51:53.717707: step 1702, loss 3.77768.
Train: 2018-08-02T14:51:53.780162: step 1703, loss 3.77768.
Train: 2018-08-02T14:51:53.842649: step 1704, loss 4.40729.
Train: 2018-08-02T14:51:53.889543: step 1705, loss 4.02952.
Train: 2018-08-02T14:51:53.952032: step 1706, loss 4.53321.
Train: 2018-08-02T14:51:54.014512: step 1707, loss 4.53321.
Train: 2018-08-02T14:51:54.061375: step 1708, loss 4.78506.
Train: 2018-08-02T14:51:54.123833: step 1709, loss 4.15545.
Train: 2018-08-02T14:51:54.186345: step 1710, loss 3.65176.
Test: 2018-08-02T14:51:54.389397: step 1710, loss 3.81863.
Train: 2018-08-02T14:51:54.436260: step 1711, loss 4.15545.
Train: 2018-08-02T14:51:54.498745: step 1712, loss 3.9036.
Train: 2018-08-02T14:51:54.561259: step 1713, loss 4.78506.
Train: 2018-08-02T14:51:54.623746: step 1714, loss 3.39991.
Train: 2018-08-02T14:51:54.686229: step 1715, loss 3.65176.
Train: 2018-08-02T14:51:54.733095: step 1716, loss 4.02952.
Train: 2018-08-02T14:51:54.795575: step 1717, loss 3.9036.
Train: 2018-08-02T14:51:54.858066: step 1718, loss 3.14807.
Train: 2018-08-02T14:51:54.904929: step 1719, loss 4.28137.
Train: 2018-08-02T14:51:54.967414: step 1720, loss 4.53321.
Test: 2018-08-02T14:51:55.170486: step 1720, loss 3.81863.
Train: 2018-08-02T14:51:55.232977: step 1721, loss 3.65176.
Train: 2018-08-02T14:51:55.279838: step 1722, loss 4.02952.
Train: 2018-08-02T14:51:55.342299: step 1723, loss 4.65914.
Train: 2018-08-02T14:51:55.404813: step 1724, loss 3.52583.
Train: 2018-08-02T14:51:55.467268: step 1725, loss 3.02214.
Train: 2018-08-02T14:51:55.529752: step 1726, loss 5.0369.
Train: 2018-08-02T14:51:55.576650: step 1727, loss 3.52583.
Train: 2018-08-02T14:51:55.639103: step 1728, loss 4.40729.
Train: 2018-08-02T14:51:55.701589: step 1729, loss 3.02214.
Train: 2018-08-02T14:51:55.748482: step 1730, loss 4.53321.
Test: 2018-08-02T14:51:55.951553: step 1730, loss 3.81863.
Train: 2018-08-02T14:51:56.014045: step 1731, loss 4.02952.
Train: 2018-08-02T14:51:56.076500: step 1732, loss 4.78506.
Train: 2018-08-02T14:51:56.123395: step 1733, loss 5.0369.
Train: 2018-08-02T14:51:56.185850: step 1734, loss 3.52583.
Train: 2018-08-02T14:51:56.248366: step 1735, loss 5.0369.
Train: 2018-08-02T14:51:56.295232: step 1736, loss 3.27399.
Train: 2018-08-02T14:51:56.357715: step 1737, loss 4.40729.
Train: 2018-08-02T14:51:56.420172: step 1738, loss 3.9036.
Train: 2018-08-02T14:51:56.482680: step 1739, loss 3.9036.
Train: 2018-08-02T14:51:56.529549: step 1740, loss 4.40729.
Test: 2018-08-02T14:51:56.732595: step 1740, loss 3.81863.
Train: 2018-08-02T14:51:56.795111: step 1741, loss 4.65914.
Train: 2018-08-02T14:51:56.857598: step 1742, loss 3.14807.
Train: 2018-08-02T14:51:56.904462: step 1743, loss 4.15545.
Train: 2018-08-02T14:51:56.966920: step 1744, loss 4.53321.
Train: 2018-08-02T14:51:57.029402: step 1745, loss 5.28875.
Train: 2018-08-02T14:51:57.076299: step 1746, loss 3.9036.
Train: 2018-08-02T14:51:57.138784: step 1747, loss 3.52583.
Train: 2018-08-02T14:51:57.201271: step 1748, loss 4.02952.
Train: 2018-08-02T14:51:57.263722: step 1749, loss 5.41467.
Train: 2018-08-02T14:51:57.310613: step 1750, loss 5.91836.
Test: 2018-08-02T14:51:57.513662: step 1750, loss 3.81863.
Train: 2018-08-02T14:51:57.576177: step 1751, loss 2.51845.
Train: 2018-08-02T14:51:57.638663: step 1752, loss 4.65914.
Train: 2018-08-02T14:51:57.701121: step 1753, loss 3.52583.
Train: 2018-08-02T14:51:57.748013: step 1754, loss 3.9036.
Train: 2018-08-02T14:51:57.810470: step 1755, loss 3.9036.
Train: 2018-08-02T14:51:57.872984: step 1756, loss 3.27399.
Train: 2018-08-02T14:51:57.919819: step 1757, loss 4.65914.
Train: 2018-08-02T14:51:57.982338: step 1758, loss 3.27399.
Train: 2018-08-02T14:51:58.044790: step 1759, loss 3.65176.
Train: 2018-08-02T14:51:58.107307: step 1760, loss 4.28137.
Test: 2018-08-02T14:51:58.294743: step 1760, loss 3.81863.
Train: 2018-08-02T14:51:58.357256: step 1761, loss 3.77768.
Train: 2018-08-02T14:51:58.419731: step 1762, loss 2.89622.
Train: 2018-08-02T14:51:58.482216: step 1763, loss 2.89622.
Train: 2018-08-02T14:51:58.529082: step 1764, loss 5.04236.
Train: 2018-08-02T14:51:58.591538: step 1765, loss 4.28137.
Train: 2018-08-02T14:51:58.654047: step 1766, loss 4.02952.
Train: 2018-08-02T14:51:58.700886: step 1767, loss 3.65176.
Train: 2018-08-02T14:51:58.763403: step 1768, loss 4.15545.
Train: 2018-08-02T14:51:58.825890: step 1769, loss 4.40729.
Train: 2018-08-02T14:51:58.872750: step 1770, loss 3.52583.
Test: 2018-08-02T14:51:59.075798: step 1770, loss 3.81863.
Train: 2018-08-02T14:51:59.138282: step 1771, loss 3.9036.
Train: 2018-08-02T14:51:59.200801: step 1772, loss 4.15545.
Train: 2018-08-02T14:51:59.263253: step 1773, loss 5.03691.
Train: 2018-08-02T14:51:59.310146: step 1774, loss 4.53321.
Train: 2018-08-02T14:51:59.372634: step 1775, loss 3.02214.
Train: 2018-08-02T14:51:59.435089: step 1776, loss 3.27399.
Train: 2018-08-02T14:51:59.481983: step 1777, loss 4.40729.
Train: 2018-08-02T14:51:59.544439: step 1778, loss 4.28137.
Train: 2018-08-02T14:51:59.606955: step 1779, loss 4.28137.
Train: 2018-08-02T14:51:59.653788: step 1780, loss 3.14807.
Test: 2018-08-02T14:51:59.856863: step 1780, loss 3.81863.
Train: 2018-08-02T14:51:59.919349: step 1781, loss 5.0369.
Train: 2018-08-02T14:51:59.981836: step 1782, loss 3.39991.
Train: 2018-08-02T14:52:00.044323: step 1783, loss 4.91098.
Train: 2018-08-02T14:52:00.106837: step 1784, loss 4.40729.
Train: 2018-08-02T14:52:00.169316: step 1785, loss 3.52583.
Train: 2018-08-02T14:52:00.216155: step 1786, loss 4.15545.
Train: 2018-08-02T14:52:00.278641: step 1787, loss 3.02214.
Train: 2018-08-02T14:52:00.341134: step 1788, loss 3.52583.
Train: 2018-08-02T14:52:00.388016: step 1789, loss 3.9036.
Train: 2018-08-02T14:52:00.450509: step 1790, loss 4.40729.
Test: 2018-08-02T14:52:00.637931: step 1790, loss 3.81863.
Train: 2018-08-02T14:52:00.700418: step 1791, loss 3.65176.
Train: 2018-08-02T14:52:00.762903: step 1792, loss 4.02952.
Train: 2018-08-02T14:52:00.825421: step 1793, loss 4.53321.
Train: 2018-08-02T14:52:00.872283: step 1794, loss 4.91098.
Train: 2018-08-02T14:52:00.934738: step 1795, loss 3.9036.
Train: 2018-08-02T14:52:00.997253: step 1796, loss 4.15545.
Train: 2018-08-02T14:52:01.059737: step 1797, loss 3.9036.
Train: 2018-08-02T14:52:01.106598: step 1798, loss 3.77768.
Train: 2018-08-02T14:52:01.169090: step 1799, loss 3.65176.
Train: 2018-08-02T14:52:01.231573: step 1800, loss 4.91098.
Test: 2018-08-02T14:52:01.419029: step 1800, loss 3.81863.
Train: 2018-08-02T14:52:01.950125: step 1801, loss 4.15545.
Train: 2018-08-02T14:52:02.012638: step 1802, loss 4.40729.
Train: 2018-08-02T14:52:02.075094: step 1803, loss 3.02214.
Train: 2018-08-02T14:52:02.121990: step 1804, loss 3.27399.
Train: 2018-08-02T14:52:02.184478: step 1805, loss 3.02214.
Train: 2018-08-02T14:52:02.246929: step 1806, loss 3.77768.
Train: 2018-08-02T14:52:02.293826: step 1807, loss 5.16283.
Train: 2018-08-02T14:52:02.356282: step 1808, loss 3.39991.
Train: 2018-08-02T14:52:02.418766: step 1809, loss 3.9036.
Train: 2018-08-02T14:52:02.465660: step 1810, loss 3.39991.
Test: 2018-08-02T14:52:02.668730: step 1810, loss 3.81863.
Train: 2018-08-02T14:52:02.731192: step 1811, loss 4.02952.
Train: 2018-08-02T14:52:02.778055: step 1812, loss 4.02952.
Train: 2018-08-02T14:52:02.840572: step 1813, loss 4.40729.
Train: 2018-08-02T14:52:02.903056: step 1814, loss 2.7703.
Train: 2018-08-02T14:52:02.949923: step 1815, loss 2.7703.
Train: 2018-08-02T14:52:03.012405: step 1816, loss 4.15545.
Train: 2018-08-02T14:52:03.059267: step 1817, loss 4.02952.
Train: 2018-08-02T14:52:03.121725: step 1818, loss 3.14807.
Train: 2018-08-02T14:52:03.184238: step 1819, loss 4.40729.
Train: 2018-08-02T14:52:03.246695: step 1820, loss 3.52583.
Test: 2018-08-02T14:52:03.434176: step 1820, loss 3.81863.
Train: 2018-08-02T14:52:03.496662: step 1821, loss 4.15545.
Train: 2018-08-02T14:52:03.559152: step 1822, loss 4.15545.
Train: 2018-08-02T14:52:03.606016: step 1823, loss 3.9036.
Train: 2018-08-02T14:52:03.668506: step 1824, loss 4.02952.
Train: 2018-08-02T14:52:03.730958: step 1825, loss 5.0369.
Train: 2018-08-02T14:52:03.793472: step 1826, loss 4.53321.
Train: 2018-08-02T14:52:03.840337: step 1827, loss 4.02952.
Train: 2018-08-02T14:52:03.902793: step 1828, loss 3.77768.
Train: 2018-08-02T14:52:03.965278: step 1829, loss 4.53321.
Train: 2018-08-02T14:52:04.012141: step 1830, loss 3.52583.
Test: 2018-08-02T14:52:04.215242: step 1830, loss 3.81863.
Train: 2018-08-02T14:52:04.277705: step 1831, loss 4.28137.
Train: 2018-08-02T14:52:04.340220: step 1832, loss 4.65914.
Train: 2018-08-02T14:52:04.402675: step 1833, loss 3.27399.
Train: 2018-08-02T14:52:04.449545: step 1834, loss 4.53321.
Train: 2018-08-02T14:52:04.512055: step 1835, loss 4.28137.
Train: 2018-08-02T14:52:04.574510: step 1836, loss 3.9036.
Train: 2018-08-02T14:52:04.621405: step 1837, loss 4.15545.
Train: 2018-08-02T14:52:04.683888: step 1838, loss 4.53321.
Train: 2018-08-02T14:52:04.746371: step 1839, loss 4.02952.
Train: 2018-08-02T14:52:04.793242: step 1840, loss 5.16283.
Test: 2018-08-02T14:52:04.996310: step 1840, loss 3.81863.
Train: 2018-08-02T14:52:05.058770: step 1841, loss 3.27399.
Train: 2018-08-02T14:52:05.121257: step 1842, loss 3.65176.
Train: 2018-08-02T14:52:05.168154: step 1843, loss 4.02952.
Train: 2018-08-02T14:52:05.230608: step 1844, loss 4.40729.
Train: 2018-08-02T14:52:05.293092: step 1845, loss 4.02952.
Train: 2018-08-02T14:52:05.355607: step 1846, loss 4.15545.
Train: 2018-08-02T14:52:05.402441: step 1847, loss 4.02952.
Train: 2018-08-02T14:52:05.464927: step 1848, loss 4.53321.
Train: 2018-08-02T14:52:05.527442: step 1849, loss 4.02952.
Train: 2018-08-02T14:52:05.574276: step 1850, loss 2.89622.
Test: 2018-08-02T14:52:05.777377: step 1850, loss 3.81863.
Train: 2018-08-02T14:52:05.839869: step 1851, loss 4.40729.
Train: 2018-08-02T14:52:05.902323: step 1852, loss 4.15545.
Train: 2018-08-02T14:52:05.949221: step 1853, loss 3.14807.
Train: 2018-08-02T14:52:06.011705: step 1854, loss 4.15545.
Train: 2018-08-02T14:52:06.074159: step 1855, loss 3.9036.
Train: 2018-08-02T14:52:06.121053: step 1856, loss 4.78506.
Train: 2018-08-02T14:52:06.183536: step 1857, loss 4.53321.
Train: 2018-08-02T14:52:06.245993: step 1858, loss 4.15545.
Train: 2018-08-02T14:52:06.308503: step 1859, loss 4.5386.
Train: 2018-08-02T14:52:06.355343: step 1860, loss 4.15545.
Test: 2018-08-02T14:52:06.558421: step 1860, loss 3.81863.
Train: 2018-08-02T14:52:06.620936: step 1861, loss 4.40729.
Train: 2018-08-02T14:52:06.683418: step 1862, loss 4.02952.
Train: 2018-08-02T14:52:06.745901: step 1863, loss 4.15545.
Train: 2018-08-02T14:52:06.808362: step 1864, loss 3.52583.
Train: 2018-08-02T14:52:06.855252: step 1865, loss 4.78506.
Train: 2018-08-02T14:52:06.917747: step 1866, loss 3.65176.
Train: 2018-08-02T14:52:06.980226: step 1867, loss 3.02214.
Train: 2018-08-02T14:52:07.027060: step 1868, loss 3.65176.
Train: 2018-08-02T14:52:07.089545: step 1869, loss 4.15545.
Train: 2018-08-02T14:52:07.152061: step 1870, loss 3.65176.
Test: 2018-08-02T14:52:07.355109: step 1870, loss 3.81863.
Train: 2018-08-02T14:52:07.402003: step 1871, loss 4.40729.
Train: 2018-08-02T14:52:07.464483: step 1872, loss 3.65176.
Train: 2018-08-02T14:52:07.526974: step 1873, loss 4.15564.
Train: 2018-08-02T14:52:07.589459: step 1874, loss 3.65176.
Train: 2018-08-02T14:52:07.636318: step 1875, loss 3.9036.
Train: 2018-08-02T14:52:07.698778: step 1876, loss 3.65176.
Train: 2018-08-02T14:52:07.761293: step 1877, loss 3.27399.
Train: 2018-08-02T14:52:07.808128: step 1878, loss 3.9036.
Train: 2018-08-02T14:52:07.870614: step 1879, loss 3.14807.
Train: 2018-08-02T14:52:07.933099: step 1880, loss 3.65176.
Test: 2018-08-02T14:52:08.136176: step 1880, loss 3.81863.
Train: 2018-08-02T14:52:08.183068: step 1881, loss 4.28137.
Train: 2018-08-02T14:52:08.245526: step 1882, loss 4.15545.
Train: 2018-08-02T14:52:08.308041: step 1883, loss 3.27399.
Train: 2018-08-02T14:52:08.370540: step 1884, loss 3.65176.
Train: 2018-08-02T14:52:08.417360: step 1885, loss 3.52583.
Train: 2018-08-02T14:52:08.479875: step 1886, loss 3.02214.
Train: 2018-08-02T14:52:08.542360: step 1887, loss 4.28137.
Train: 2018-08-02T14:52:08.589194: step 1888, loss 3.65176.
Train: 2018-08-02T14:52:08.651710: step 1889, loss 4.40729.
Train: 2018-08-02T14:52:08.714166: step 1890, loss 4.02952.
Test: 2018-08-02T14:52:08.901620: step 1890, loss 3.81863.
Train: 2018-08-02T14:52:08.964137: step 1891, loss 2.89622.
Train: 2018-08-02T14:52:09.026593: step 1892, loss 4.28137.
Train: 2018-08-02T14:52:09.089107: step 1893, loss 4.65914.
Train: 2018-08-02T14:52:09.135972: step 1894, loss 3.02214.
Train: 2018-08-02T14:52:09.198455: step 1895, loss 5.5406.
Train: 2018-08-02T14:52:09.260912: step 1896, loss 3.39991.
Train: 2018-08-02T14:52:09.307804: step 1897, loss 3.39991.
Train: 2018-08-02T14:52:09.370295: step 1898, loss 3.52583.
Train: 2018-08-02T14:52:09.432775: step 1899, loss 4.02952.
Train: 2018-08-02T14:52:09.479611: step 1900, loss 4.91098.
Test: 2018-08-02T14:52:09.682721: step 1900, loss 3.81863.
Train: 2018-08-02T14:52:10.166975: step 1901, loss 3.52583.
Train: 2018-08-02T14:52:10.229435: step 1902, loss 5.16283.
Train: 2018-08-02T14:52:10.291951: step 1903, loss 4.65914.
Train: 2018-08-02T14:52:10.354438: step 1904, loss 4.53321.
Train: 2018-08-02T14:52:10.401270: step 1905, loss 3.77768.
Train: 2018-08-02T14:52:10.463756: step 1906, loss 3.52583.
Train: 2018-08-02T14:52:10.526271: step 1907, loss 4.78506.
Train: 2018-08-02T14:52:10.573134: step 1908, loss 3.9036.
Train: 2018-08-02T14:52:10.635621: step 1909, loss 4.28137.
Train: 2018-08-02T14:52:10.698109: step 1910, loss 4.53321.
Test: 2018-08-02T14:52:10.901155: step 1910, loss 3.81863.
Train: 2018-08-02T14:52:10.948017: step 1911, loss 4.15545.
Train: 2018-08-02T14:52:11.010503: step 1912, loss 4.02952.
Train: 2018-08-02T14:52:11.072988: step 1913, loss 3.14807.
Train: 2018-08-02T14:52:11.135474: step 1914, loss 4.40729.
Train: 2018-08-02T14:52:11.182337: step 1915, loss 3.77768.
Train: 2018-08-02T14:52:11.244847: step 1916, loss 4.28137.
Train: 2018-08-02T14:52:11.307308: step 1917, loss 5.0369.
Train: 2018-08-02T14:52:11.354203: step 1918, loss 3.02214.
Train: 2018-08-02T14:52:11.416658: step 1919, loss 3.9036.
Train: 2018-08-02T14:52:11.479174: step 1920, loss 4.53321.
Test: 2018-08-02T14:52:11.666623: step 1920, loss 3.81863.
Train: 2018-08-02T14:52:11.729117: step 1921, loss 4.40729.
Train: 2018-08-02T14:52:11.791601: step 1922, loss 3.9036.
Train: 2018-08-02T14:52:11.854085: step 1923, loss 3.39991.
Train: 2018-08-02T14:52:11.900951: step 1924, loss 3.27399.
Train: 2018-08-02T14:52:11.963435: step 1925, loss 4.15545.
Train: 2018-08-02T14:52:12.025918: step 1926, loss 4.02952.
Train: 2018-08-02T14:52:12.072753: step 1927, loss 4.65914.
Train: 2018-08-02T14:52:12.135270: step 1928, loss 3.9036.
Train: 2018-08-02T14:52:12.197759: step 1929, loss 3.65176.
Train: 2018-08-02T14:52:12.244620: step 1930, loss 4.53321.
Test: 2018-08-02T14:52:12.447665: step 1930, loss 3.81863.
Train: 2018-08-02T14:52:12.510182: step 1931, loss 3.65176.
Train: 2018-08-02T14:52:12.572637: step 1932, loss 3.9036.
Train: 2018-08-02T14:52:12.619531: step 1933, loss 4.53321.
Train: 2018-08-02T14:52:12.681987: step 1934, loss 4.02952.
Train: 2018-08-02T14:52:12.744474: step 1935, loss 4.28137.
Train: 2018-08-02T14:52:12.806987: step 1936, loss 4.40729.
Train: 2018-08-02T14:52:12.853822: step 1937, loss 3.65176.
Train: 2018-08-02T14:52:12.916340: step 1938, loss 4.15545.
Train: 2018-08-02T14:52:12.978792: step 1939, loss 5.0369.
Train: 2018-08-02T14:52:13.025686: step 1940, loss 3.39991.
Test: 2018-08-02T14:52:13.228763: step 1940, loss 3.81863.
Train: 2018-08-02T14:52:13.291249: step 1941, loss 4.40729.
Train: 2018-08-02T14:52:13.353728: step 1942, loss 3.77768.
Train: 2018-08-02T14:52:13.400568: step 1943, loss 3.27399.
Train: 2018-08-02T14:52:13.463085: step 1944, loss 5.16283.
Train: 2018-08-02T14:52:13.525570: step 1945, loss 4.15545.
Train: 2018-08-02T14:52:13.572402: step 1946, loss 4.53321.
Train: 2018-08-02T14:52:13.634918: step 1947, loss 4.15545.
Train: 2018-08-02T14:52:13.697374: step 1948, loss 4.28137.
Train: 2018-08-02T14:52:13.744268: step 1949, loss 3.9036.
Train: 2018-08-02T14:52:13.806751: step 1950, loss 3.39991.
Test: 2018-08-02T14:52:14.009801: step 1950, loss 3.81863.
Train: 2018-08-02T14:52:14.056693: step 1951, loss 3.65176.
Train: 2018-08-02T14:52:14.119181: step 1952, loss 3.27399.
Train: 2018-08-02T14:52:14.181665: step 1953, loss 3.14807.
Train: 2018-08-02T14:52:14.244147: step 1954, loss 4.53321.
Train: 2018-08-02T14:52:14.290984: step 1955, loss 4.78506.
Train: 2018-08-02T14:52:14.353495: step 1956, loss 3.52583.
Train: 2018-08-02T14:52:14.415981: step 1957, loss 3.9036.
Train: 2018-08-02T14:52:14.478442: step 1958, loss 5.0369.
Train: 2018-08-02T14:52:14.525304: step 1959, loss 3.52583.
Train: 2018-08-02T14:52:14.587819: step 1960, loss 4.40729.
Test: 2018-08-02T14:52:14.790867: step 1960, loss 3.81863.
Train: 2018-08-02T14:52:14.837762: step 1961, loss 3.9036.
Train: 2018-08-02T14:52:14.900218: step 1962, loss 3.77768.
Train: 2018-08-02T14:52:14.947081: step 1963, loss 5.10406.
Train: 2018-08-02T14:52:15.009566: step 1964, loss 3.14807.
Train: 2018-08-02T14:52:15.072084: step 1965, loss 4.40729.
Train: 2018-08-02T14:52:15.118941: step 1966, loss 3.9036.
Train: 2018-08-02T14:52:15.181428: step 1967, loss 3.77768.
Train: 2018-08-02T14:52:15.243912: step 1968, loss 4.15545.
Train: 2018-08-02T14:52:15.290783: step 1969, loss 3.27399.
Train: 2018-08-02T14:52:15.353236: step 1970, loss 4.40729.
Test: 2018-08-02T14:52:15.556345: step 1970, loss 3.81863.
Train: 2018-08-02T14:52:15.618798: step 1971, loss 4.53321.
Train: 2018-08-02T14:52:15.665662: step 1972, loss 3.77768.
Train: 2018-08-02T14:52:15.728148: step 1973, loss 4.28137.
Train: 2018-08-02T14:52:15.790634: step 1974, loss 3.65176.
Train: 2018-08-02T14:52:15.853150: step 1975, loss 4.28137.
Train: 2018-08-02T14:52:15.899982: step 1976, loss 3.52583.
Train: 2018-08-02T14:52:15.962498: step 1977, loss 3.9036.
Train: 2018-08-02T14:52:16.009333: step 1978, loss 3.9036.
Train: 2018-08-02T14:52:16.071818: step 1979, loss 4.15545.
Train: 2018-08-02T14:52:16.134328: step 1980, loss 3.52583.
Test: 2018-08-02T14:52:16.337382: step 1980, loss 3.81863.
Train: 2018-08-02T14:52:16.384278: step 1981, loss 3.65176.
Train: 2018-08-02T14:52:16.446763: step 1982, loss 2.64438.
Train: 2018-08-02T14:52:16.509215: step 1983, loss 3.9036.
Train: 2018-08-02T14:52:16.571735: step 1984, loss 4.65914.
Train: 2018-08-02T14:52:16.618595: step 1985, loss 3.77768.
Train: 2018-08-02T14:52:16.681049: step 1986, loss 4.15545.
Train: 2018-08-02T14:52:16.743565: step 1987, loss 5.16283.
Train: 2018-08-02T14:52:16.790432: step 1988, loss 3.52583.
Train: 2018-08-02T14:52:16.852910: step 1989, loss 4.65914.
Train: 2018-08-02T14:52:16.915369: step 1990, loss 4.53321.
Test: 2018-08-02T14:52:17.102856: step 1990, loss 3.81863.
Train: 2018-08-02T14:52:17.165311: step 1991, loss 4.65914.
Train: 2018-08-02T14:52:17.227827: step 1992, loss 3.52583.
Train: 2018-08-02T14:52:17.290312: step 1993, loss 4.02952.
Train: 2018-08-02T14:52:17.352767: step 1994, loss 4.78506.
Train: 2018-08-02T14:52:17.399633: step 1995, loss 4.15545.
Train: 2018-08-02T14:52:17.462116: step 1996, loss 3.65176.
Train: 2018-08-02T14:52:17.524601: step 1997, loss 3.65176.
Train: 2018-08-02T14:52:17.587118: step 1998, loss 4.02952.
Train: 2018-08-02T14:52:17.633951: step 1999, loss 4.15545.
Train: 2018-08-02T14:52:17.696470: step 2000, loss 3.65176.
Test: 2018-08-02T14:52:17.899513: step 2000, loss 3.81863.
Train: 2018-08-02T14:52:18.493157: step 2001, loss 3.27399.
Train: 2018-08-02T14:52:18.540023: step 2002, loss 3.52583.
Train: 2018-08-02T14:52:18.602505: step 2003, loss 4.15545.
Train: 2018-08-02T14:52:18.664966: step 2004, loss 4.02952.
Train: 2018-08-02T14:52:18.727476: step 2005, loss 3.9036.
Train: 2018-08-02T14:52:18.774310: step 2006, loss 4.91098.
Train: 2018-08-02T14:52:18.836795: step 2007, loss 3.52583.
Train: 2018-08-02T14:52:18.899306: step 2008, loss 3.9036.
Train: 2018-08-02T14:52:18.961766: step 2009, loss 3.9036.
Train: 2018-08-02T14:52:19.008660: step 2010, loss 3.65176.
Test: 2018-08-02T14:52:19.211706: step 2010, loss 3.81863.
Train: 2018-08-02T14:52:19.274223: step 2011, loss 4.02952.
Train: 2018-08-02T14:52:19.336677: step 2012, loss 4.91098.
Train: 2018-08-02T14:52:19.383572: step 2013, loss 3.77768.
Train: 2018-08-02T14:52:19.446061: step 2014, loss 4.02952.
Train: 2018-08-02T14:52:19.508538: step 2015, loss 4.78506.
Train: 2018-08-02T14:52:19.555404: step 2016, loss 3.77768.
Train: 2018-08-02T14:52:19.617891: step 2017, loss 3.77768.
Train: 2018-08-02T14:52:19.680348: step 2018, loss 3.14807.
Train: 2018-08-02T14:52:19.727242: step 2019, loss 3.77768.
Train: 2018-08-02T14:52:19.789730: step 2020, loss 5.16283.
Test: 2018-08-02T14:52:19.992798: step 2020, loss 3.81863.
Train: 2018-08-02T14:52:20.055287: step 2021, loss 4.91098.
Train: 2018-08-02T14:52:20.102154: step 2022, loss 3.65176.
Train: 2018-08-02T14:52:20.164634: step 2023, loss 3.39991.
Train: 2018-08-02T14:52:20.227125: step 2024, loss 4.53321.
Train: 2018-08-02T14:52:20.289610: step 2025, loss 4.15545.
Train: 2018-08-02T14:52:20.336475: step 2026, loss 5.0369.
Train: 2018-08-02T14:52:20.398959: step 2027, loss 3.65176.
Train: 2018-08-02T14:52:20.461445: step 2028, loss 2.64438.
Train: 2018-08-02T14:52:20.508309: step 2029, loss 2.64438.
Train: 2018-08-02T14:52:20.570763: step 2030, loss 4.15545.
Test: 2018-08-02T14:52:20.773866: step 2030, loss 3.81863.
Train: 2018-08-02T14:52:20.820739: step 2031, loss 3.39991.
Train: 2018-08-02T14:52:20.883220: step 2032, loss 3.39991.
Train: 2018-08-02T14:52:20.945677: step 2033, loss 4.28137.
Train: 2018-08-02T14:52:21.008193: step 2034, loss 3.27399.
Train: 2018-08-02T14:52:21.070648: step 2035, loss 4.02952.
Train: 2018-08-02T14:52:21.117540: step 2036, loss 3.65176.
Train: 2018-08-02T14:52:21.180026: step 2037, loss 3.65176.
Train: 2018-08-02T14:52:21.242513: step 2038, loss 3.77768.
Train: 2018-08-02T14:52:21.289379: step 2039, loss 4.02952.
Train: 2018-08-02T14:52:21.351860: step 2040, loss 3.02214.
Test: 2018-08-02T14:52:21.554907: step 2040, loss 3.81863.
Train: 2018-08-02T14:52:21.601802: step 2041, loss 4.28137.
Train: 2018-08-02T14:52:21.664291: step 2042, loss 3.77768.
Train: 2018-08-02T14:52:21.726773: step 2043, loss 4.28137.
Train: 2018-08-02T14:52:21.789266: step 2044, loss 2.89622.
Train: 2018-08-02T14:52:21.836122: step 2045, loss 3.27399.
Train: 2018-08-02T14:52:21.898607: step 2046, loss 3.9036.
Train: 2018-08-02T14:52:21.961093: step 2047, loss 3.65176.
Train: 2018-08-02T14:52:22.023581: step 2048, loss 4.65914.
Train: 2018-08-02T14:52:22.070446: step 2049, loss 3.9036.
Train: 2018-08-02T14:52:22.132929: step 2050, loss 3.9036.
Test: 2018-08-02T14:52:22.335974: step 2050, loss 3.81863.
Train: 2018-08-02T14:52:22.398461: step 2051, loss 5.28875.
Train: 2018-08-02T14:52:22.445359: step 2052, loss 4.28137.
Train: 2018-08-02T14:52:22.507840: step 2053, loss 5.0369.
Train: 2018-08-02T14:52:22.570325: step 2054, loss 4.65914.
Train: 2018-08-02T14:52:22.632806: step 2055, loss 4.28137.
Train: 2018-08-02T14:52:22.679674: step 2056, loss 4.28137.
Train: 2018-08-02T14:52:22.742158: step 2057, loss 6.29613.
Train: 2018-08-02T14:52:22.804644: step 2058, loss 4.78506.
Train: 2018-08-02T14:52:22.851510: step 2059, loss 4.28137.
Train: 2018-08-02T14:52:22.913998: step 2060, loss 5.0369.
Test: 2018-08-02T14:52:23.117072: step 2060, loss 3.81863.
Train: 2018-08-02T14:52:23.179558: step 2061, loss 3.52583.
Train: 2018-08-02T14:52:23.226423: step 2062, loss 3.39991.
Train: 2018-08-02T14:52:23.288907: step 2063, loss 4.65914.
Train: 2018-08-02T14:52:23.351365: step 2064, loss 3.9036.
Train: 2018-08-02T14:52:23.413870: step 2065, loss 4.78506.
Train: 2018-08-02T14:52:23.460713: step 2066, loss 3.39991.
Train: 2018-08-02T14:52:23.523197: step 2067, loss 3.65176.
Train: 2018-08-02T14:52:23.570061: step 2068, loss 3.65176.
Train: 2018-08-02T14:52:23.632577: step 2069, loss 2.7703.
Train: 2018-08-02T14:52:23.695033: step 2070, loss 3.39991.
Test: 2018-08-02T14:52:23.898110: step 2070, loss 3.81863.
Train: 2018-08-02T14:52:23.945007: step 2071, loss 4.40729.
Train: 2018-08-02T14:52:24.007490: step 2072, loss 4.28137.
Train: 2018-08-02T14:52:24.069978: step 2073, loss 3.9036.
Train: 2018-08-02T14:52:24.132432: step 2074, loss 4.53321.
Train: 2018-08-02T14:52:24.179321: step 2075, loss 4.65914.
Train: 2018-08-02T14:52:24.241779: step 2076, loss 4.65914.
Train: 2018-08-02T14:52:24.288677: step 2077, loss 4.91098.
Train: 2018-08-02T14:52:24.351129: step 2078, loss 3.9036.
Train: 2018-08-02T14:52:24.413613: step 2079, loss 4.53321.
Train: 2018-08-02T14:52:24.460477: step 2080, loss 3.52583.
Test: 2018-08-02T14:52:24.663585: step 2080, loss 3.81863.
Train: 2018-08-02T14:52:24.726041: step 2081, loss 5.16283.
Train: 2018-08-02T14:52:24.788556: step 2082, loss 2.64438.
Train: 2018-08-02T14:52:24.851042: step 2083, loss 3.39991.
Train: 2018-08-02T14:52:24.897907: step 2084, loss 3.65176.
Train: 2018-08-02T14:52:24.960362: step 2085, loss 4.02952.
Train: 2018-08-02T14:52:25.022846: step 2086, loss 4.28137.
Train: 2018-08-02T14:52:25.085355: step 2087, loss 4.40729.
Train: 2018-08-02T14:52:25.132227: step 2088, loss 4.28137.
Train: 2018-08-02T14:52:25.194715: step 2089, loss 3.27399.
Train: 2018-08-02T14:52:25.257198: step 2090, loss 4.53321.
Test: 2018-08-02T14:52:25.444623: step 2090, loss 3.81863.
Train: 2018-08-02T14:52:25.507109: step 2091, loss 4.40729.
Train: 2018-08-02T14:52:25.569593: step 2092, loss 4.28137.
Train: 2018-08-02T14:52:25.616487: step 2093, loss 2.64438.
Train: 2018-08-02T14:52:25.678971: step 2094, loss 4.78506.
Train: 2018-08-02T14:52:25.741461: step 2095, loss 3.77768.
Train: 2018-08-02T14:52:25.803945: step 2096, loss 3.9036.
Train: 2018-08-02T14:52:25.850778: step 2097, loss 4.15545.
Train: 2018-08-02T14:52:25.913263: step 2098, loss 3.77768.
Train: 2018-08-02T14:52:25.975749: step 2099, loss 4.02952.
Train: 2018-08-02T14:52:26.022613: step 2100, loss 4.65914.
Test: 2018-08-02T14:52:26.225689: step 2100, loss 3.81863.
Train: 2018-08-02T14:52:26.756845: step 2101, loss 3.77768.
Train: 2018-08-02T14:52:26.819301: step 2102, loss 5.16283.
Train: 2018-08-02T14:52:26.881787: step 2103, loss 3.65176.
Train: 2018-08-02T14:52:26.928675: step 2104, loss 3.65176.
Train: 2018-08-02T14:52:26.991137: step 2105, loss 4.40729.
Train: 2018-08-02T14:52:27.053649: step 2106, loss 3.02214.
Train: 2018-08-02T14:52:27.100485: step 2107, loss 5.0369.
Train: 2018-08-02T14:52:27.162972: step 2108, loss 2.51845.
Train: 2018-08-02T14:52:27.225486: step 2109, loss 3.52583.
Train: 2018-08-02T14:52:27.272350: step 2110, loss 4.15545.
Test: 2018-08-02T14:52:27.475426: step 2110, loss 3.81863.
Train: 2018-08-02T14:52:27.537912: step 2111, loss 4.40729.
Train: 2018-08-02T14:52:27.600398: step 2112, loss 3.9036.
Train: 2018-08-02T14:52:27.647262: step 2113, loss 4.40729.
Train: 2018-08-02T14:52:27.694125: step 2114, loss 4.56679.
Train: 2018-08-02T14:52:27.756606: step 2115, loss 3.52583.
Train: 2018-08-02T14:52:27.819090: step 2116, loss 3.52583.
Train: 2018-08-02T14:52:27.865962: step 2117, loss 3.52583.
Train: 2018-08-02T14:52:27.928444: step 2118, loss 3.9036.
Train: 2018-08-02T14:52:27.990901: step 2119, loss 3.39991.
Train: 2018-08-02T14:52:28.037791: step 2120, loss 3.65176.
Test: 2018-08-02T14:52:28.240870: step 2120, loss 3.81863.
Train: 2018-08-02T14:52:28.303356: step 2121, loss 3.27399.
Train: 2018-08-02T14:52:28.365843: step 2122, loss 4.02952.
Train: 2018-08-02T14:52:28.412710: step 2123, loss 4.91098.
Train: 2018-08-02T14:52:28.475190: step 2124, loss 3.02214.
Train: 2018-08-02T14:52:28.537678: step 2125, loss 4.53321.
Train: 2018-08-02T14:52:28.600164: step 2126, loss 3.9036.
Train: 2018-08-02T14:52:28.646998: step 2127, loss 4.15545.
Train: 2018-08-02T14:52:28.709484: step 2128, loss 3.39991.
Train: 2018-08-02T14:52:28.772000: step 2129, loss 3.9036.
Train: 2018-08-02T14:52:28.818831: step 2130, loss 4.40729.
Test: 2018-08-02T14:52:29.021910: step 2130, loss 3.81863.
Train: 2018-08-02T14:52:29.084395: step 2131, loss 4.40729.
Train: 2018-08-02T14:52:29.146905: step 2132, loss 4.40729.
Train: 2018-08-02T14:52:29.209366: step 2133, loss 4.40729.
Train: 2018-08-02T14:52:29.256264: step 2134, loss 3.39991.
Train: 2018-08-02T14:52:29.318716: step 2135, loss 4.40729.
Train: 2018-08-02T14:52:29.381201: step 2136, loss 4.02952.
Train: 2018-08-02T14:52:29.443719: step 2137, loss 5.16283.
Train: 2018-08-02T14:52:29.490581: step 2138, loss 3.9036.
Train: 2018-08-02T14:52:29.553064: step 2139, loss 3.65176.
Train: 2018-08-02T14:52:29.615551: step 2140, loss 5.0369.
Test: 2018-08-02T14:52:29.802977: step 2140, loss 3.81863.
Train: 2018-08-02T14:52:29.865492: step 2141, loss 4.02952.
Train: 2018-08-02T14:52:29.927948: step 2142, loss 4.15545.
Train: 2018-08-02T14:52:29.990432: step 2143, loss 3.14807.
Train: 2018-08-02T14:52:30.037299: step 2144, loss 4.78506.
Train: 2018-08-02T14:52:30.099808: step 2145, loss 3.02214.
Train: 2018-08-02T14:52:30.162268: step 2146, loss 3.02214.
Train: 2018-08-02T14:52:30.209132: step 2147, loss 4.40729.
Train: 2018-08-02T14:52:30.271649: step 2148, loss 3.39991.
Train: 2018-08-02T14:52:30.334133: step 2149, loss 4.02952.
Train: 2018-08-02T14:52:30.380997: step 2150, loss 4.53321.
Test: 2018-08-02T14:52:30.584069: step 2150, loss 3.81863.
Train: 2018-08-02T14:52:30.646555: step 2151, loss 3.77768.
Train: 2018-08-02T14:52:30.709015: step 2152, loss 4.40729.
Train: 2018-08-02T14:52:30.771530: step 2153, loss 3.52583.
Train: 2018-08-02T14:52:30.818398: step 2154, loss 4.40729.
Train: 2018-08-02T14:52:30.880880: step 2155, loss 3.9036.
Train: 2018-08-02T14:52:30.943359: step 2156, loss 4.15545.
Train: 2018-08-02T14:52:30.990228: step 2157, loss 3.77768.
Train: 2018-08-02T14:52:31.052713: step 2158, loss 4.53321.
Train: 2018-08-02T14:52:31.115170: step 2159, loss 3.77768.
Train: 2018-08-02T14:52:31.162033: step 2160, loss 4.03425.
Test: 2018-08-02T14:52:31.365111: step 2160, loss 3.81863.
Train: 2018-08-02T14:52:31.427596: step 2161, loss 3.02214.
Train: 2018-08-02T14:52:31.490111: step 2162, loss 3.27399.
Train: 2018-08-02T14:52:31.552595: step 2163, loss 4.65914.
Train: 2018-08-02T14:52:31.599462: step 2164, loss 4.15545.
Train: 2018-08-02T14:52:31.661919: step 2165, loss 3.65176.
Train: 2018-08-02T14:52:31.724428: step 2166, loss 5.79244.
Train: 2018-08-02T14:52:31.786917: step 2167, loss 5.16283.
Train: 2018-08-02T14:52:31.833783: step 2168, loss 3.77768.
Train: 2018-08-02T14:52:31.896269: step 2169, loss 4.53321.
Train: 2018-08-02T14:52:31.958752: step 2170, loss 4.78506.
Test: 2018-08-02T14:52:32.146204: step 2170, loss 3.81863.
Train: 2018-08-02T14:52:32.208693: step 2171, loss 3.65176.
Train: 2018-08-02T14:52:32.271178: step 2172, loss 3.77768.
Train: 2018-08-02T14:52:32.318047: step 2173, loss 5.03691.
Train: 2018-08-02T14:52:32.380503: step 2174, loss 4.53321.
Train: 2018-08-02T14:52:32.442984: step 2175, loss 3.02214.
Train: 2018-08-02T14:52:32.489848: step 2176, loss 3.52583.
Train: 2018-08-02T14:52:32.552333: step 2177, loss 3.52583.
Train: 2018-08-02T14:52:32.614820: step 2178, loss 3.77768.
Train: 2018-08-02T14:52:32.677338: step 2179, loss 4.40729.
Train: 2018-08-02T14:52:32.724199: step 2180, loss 3.65176.
Test: 2018-08-02T14:52:32.927269: step 2180, loss 3.81863.
Train: 2018-08-02T14:52:32.989761: step 2181, loss 4.53321.
Train: 2018-08-02T14:52:33.052246: step 2182, loss 3.77768.
Train: 2018-08-02T14:52:33.099080: step 2183, loss 4.40729.
Train: 2018-08-02T14:52:33.161567: step 2184, loss 3.39991.
Train: 2018-08-02T14:52:33.224085: step 2185, loss 5.16283.
Train: 2018-08-02T14:52:33.270915: step 2186, loss 3.39991.
Train: 2018-08-02T14:52:33.333401: step 2187, loss 3.02214.
Train: 2018-08-02T14:52:33.395914: step 2188, loss 4.40729.
Train: 2018-08-02T14:52:33.442783: step 2189, loss 4.15545.
Train: 2018-08-02T14:52:33.505265: step 2190, loss 3.77768.
Test: 2018-08-02T14:52:33.708337: step 2190, loss 3.81863.
Train: 2018-08-02T14:52:33.770798: step 2191, loss 3.9036.
Train: 2018-08-02T14:52:33.833308: step 2192, loss 2.7703.
Train: 2018-08-02T14:52:33.880177: step 2193, loss 3.14807.
Train: 2018-08-02T14:52:33.942662: step 2194, loss 4.78506.
Train: 2018-08-02T14:52:34.005144: step 2195, loss 3.14807.
Train: 2018-08-02T14:52:34.052007: step 2196, loss 4.53321.
Train: 2018-08-02T14:52:34.114498: step 2197, loss 3.52583.
Train: 2018-08-02T14:52:34.176953: step 2198, loss 3.27399.
Train: 2018-08-02T14:52:34.239438: step 2199, loss 4.91098.
Train: 2018-08-02T14:52:34.301925: step 2200, loss 4.28137.
Test: 2018-08-02T14:52:34.489410: step 2200, loss 3.81863.
Train: 2018-08-02T14:52:35.051776: step 2201, loss 3.52583.
Train: 2018-08-02T14:52:35.114263: step 2202, loss 3.52583.
Train: 2018-08-02T14:52:35.161130: step 2203, loss 4.40729.
Train: 2018-08-02T14:52:35.223584: step 2204, loss 4.28137.
Train: 2018-08-02T14:52:35.286094: step 2205, loss 4.91098.
Train: 2018-08-02T14:52:35.348554: step 2206, loss 3.9036.
Train: 2018-08-02T14:52:35.395418: step 2207, loss 3.77768.
Train: 2018-08-02T14:52:35.457904: step 2208, loss 4.78506.
Train: 2018-08-02T14:52:35.520422: step 2209, loss 3.65176.
Train: 2018-08-02T14:52:35.582873: step 2210, loss 4.02952.
Test: 2018-08-02T14:52:35.770330: step 2210, loss 3.81863.
Train: 2018-08-02T14:52:35.832815: step 2211, loss 4.28137.
Train: 2018-08-02T14:52:35.895325: step 2212, loss 2.7703.
Train: 2018-08-02T14:52:35.957816: step 2213, loss 3.39991.
Train: 2018-08-02T14:52:36.020273: step 2214, loss 4.02952.
Train: 2018-08-02T14:52:36.082794: step 2215, loss 2.7703.
Train: 2018-08-02T14:52:36.129620: step 2216, loss 4.40729.
Train: 2018-08-02T14:52:36.192135: step 2217, loss 3.14807.
Train: 2018-08-02T14:52:36.254592: step 2218, loss 3.65176.
Train: 2018-08-02T14:52:36.301455: step 2219, loss 4.78506.
Train: 2018-08-02T14:52:36.363971: step 2220, loss 4.15545.
Test: 2018-08-02T14:52:36.567018: step 2220, loss 3.81863.
Train: 2018-08-02T14:52:36.629504: step 2221, loss 4.28137.
Train: 2018-08-02T14:52:36.676368: step 2222, loss 3.52583.
Train: 2018-08-02T14:52:36.738854: step 2223, loss 3.14807.
Train: 2018-08-02T14:52:36.801367: step 2224, loss 5.03691.
Train: 2018-08-02T14:52:36.863824: step 2225, loss 3.14807.
Train: 2018-08-02T14:52:36.910688: step 2226, loss 3.39991.
Train: 2018-08-02T14:52:36.973204: step 2227, loss 4.15545.
Train: 2018-08-02T14:52:37.035692: step 2228, loss 3.77768.
Train: 2018-08-02T14:52:37.098145: step 2229, loss 3.9036.
Train: 2018-08-02T14:52:37.145034: step 2230, loss 3.27399.
Test: 2018-08-02T14:52:37.344041: step 2230, loss 3.81863.
Train: 2018-08-02T14:52:37.406502: step 2231, loss 4.78506.
Train: 2018-08-02T14:52:37.468987: step 2232, loss 5.28875.
Train: 2018-08-02T14:52:37.531473: step 2233, loss 3.39991.
Train: 2018-08-02T14:52:37.578354: step 2234, loss 4.40729.
Train: 2018-08-02T14:52:37.640853: step 2235, loss 4.15545.
Train: 2018-08-02T14:52:37.703337: step 2236, loss 3.14807.
Train: 2018-08-02T14:52:37.750202: step 2237, loss 4.65914.
Train: 2018-08-02T14:52:37.812656: step 2238, loss 3.65176.
Train: 2018-08-02T14:52:37.875168: step 2239, loss 4.02952.
Train: 2018-08-02T14:52:37.922037: step 2240, loss 4.28137.
Test: 2018-08-02T14:52:38.125084: step 2240, loss 3.81863.
Train: 2018-08-02T14:52:38.187598: step 2241, loss 4.15545.
Train: 2018-08-02T14:52:38.250085: step 2242, loss 4.40729.
Train: 2018-08-02T14:52:38.312569: step 2243, loss 5.0369.
Train: 2018-08-02T14:52:38.359404: step 2244, loss 4.28137.
Train: 2018-08-02T14:52:38.421890: step 2245, loss 4.15545.
Train: 2018-08-02T14:52:38.484375: step 2246, loss 4.15545.
Train: 2018-08-02T14:52:38.531238: step 2247, loss 3.9036.
Train: 2018-08-02T14:52:38.593752: step 2248, loss 4.28137.
Train: 2018-08-02T14:52:38.656209: step 2249, loss 4.15545.
Train: 2018-08-02T14:52:38.718718: step 2250, loss 4.02952.
Test: 2018-08-02T14:52:38.906175: step 2250, loss 3.81863.
Train: 2018-08-02T14:52:38.968635: step 2251, loss 4.53321.
Train: 2018-08-02T14:52:39.031151: step 2252, loss 3.9036.
Train: 2018-08-02T14:52:39.078018: step 2253, loss 4.28137.
Train: 2018-08-02T14:52:39.140503: step 2254, loss 4.02952.
Train: 2018-08-02T14:52:39.202957: step 2255, loss 5.28875.
Train: 2018-08-02T14:52:39.265442: step 2256, loss 4.65914.
Train: 2018-08-02T14:52:39.312336: step 2257, loss 4.91098.
Train: 2018-08-02T14:52:39.374822: step 2258, loss 3.02214.
Train: 2018-08-02T14:52:39.437310: step 2259, loss 3.77768.
Train: 2018-08-02T14:52:39.484172: step 2260, loss 4.40729.
Test: 2018-08-02T14:52:39.687248: step 2260, loss 3.81863.
Train: 2018-08-02T14:52:39.749733: step 2261, loss 4.02952.
Train: 2018-08-02T14:52:39.796597: step 2262, loss 4.53321.
Train: 2018-08-02T14:52:39.859085: step 2263, loss 3.9036.
Train: 2018-08-02T14:52:39.921538: step 2264, loss 4.02952.
Train: 2018-08-02T14:52:39.968432: step 2265, loss 3.49225.
Train: 2018-08-02T14:52:40.015297: step 2266, loss 3.77768.
Train: 2018-08-02T14:52:40.077781: step 2267, loss 4.02952.
Train: 2018-08-02T14:52:40.140236: step 2268, loss 5.0369.
Train: 2018-08-02T14:52:40.187100: step 2269, loss 4.02952.
Train: 2018-08-02T14:52:40.249586: step 2270, loss 3.14807.
Test: 2018-08-02T14:52:40.452693: step 2270, loss 3.81863.
Train: 2018-08-02T14:52:40.515149: step 2271, loss 4.65914.
Train: 2018-08-02T14:52:40.577664: step 2272, loss 3.77768.
Train: 2018-08-02T14:52:40.624529: step 2273, loss 3.52583.
Train: 2018-08-02T14:52:40.687009: step 2274, loss 3.65176.
Train: 2018-08-02T14:52:40.749469: step 2275, loss 3.77768.
Train: 2018-08-02T14:52:40.796364: step 2276, loss 4.15545.
Train: 2018-08-02T14:52:40.858818: step 2277, loss 3.52583.
Train: 2018-08-02T14:52:40.921335: step 2278, loss 4.53321.
Train: 2018-08-02T14:52:40.983823: step 2279, loss 3.39991.
Train: 2018-08-02T14:52:41.030684: step 2280, loss 4.02952.
Test: 2018-08-02T14:52:41.218109: step 2280, loss 3.81863.
Train: 2018-08-02T14:52:41.280625: step 2281, loss 4.15545.
Train: 2018-08-02T14:52:41.343080: step 2282, loss 4.53321.
Train: 2018-08-02T14:52:41.405565: step 2283, loss 3.52583.
Train: 2018-08-02T14:52:41.452437: step 2284, loss 4.40729.
Train: 2018-08-02T14:52:41.514941: step 2285, loss 4.28137.
Train: 2018-08-02T14:52:41.577430: step 2286, loss 3.77768.
Train: 2018-08-02T14:52:41.624294: step 2287, loss 4.53321.
Train: 2018-08-02T14:52:41.686750: step 2288, loss 3.27399.
Train: 2018-08-02T14:52:41.749261: step 2289, loss 3.02214.
Train: 2018-08-02T14:52:41.811720: step 2290, loss 4.02952.
Test: 2018-08-02T14:52:41.999175: step 2290, loss 3.81863.
Train: 2018-08-02T14:52:42.061661: step 2291, loss 5.41467.
Train: 2018-08-02T14:52:42.124148: step 2292, loss 3.27399.
Train: 2018-08-02T14:52:42.186663: step 2293, loss 4.02952.
Train: 2018-08-02T14:52:42.249149: step 2294, loss 4.02952.
Train: 2018-08-02T14:52:42.296008: step 2295, loss 3.02214.
Train: 2018-08-02T14:52:42.358499: step 2296, loss 4.91098.
Train: 2018-08-02T14:52:42.420952: step 2297, loss 3.39991.
Train: 2018-08-02T14:52:42.467816: step 2298, loss 3.02214.
Train: 2018-08-02T14:52:42.530302: step 2299, loss 4.28137.
Train: 2018-08-02T14:52:42.592787: step 2300, loss 3.27399.
Test: 2018-08-02T14:52:42.795866: step 2300, loss 3.81863.
Train: 2018-08-02T14:52:43.295778: step 2301, loss 4.40729.
Train: 2018-08-02T14:52:43.358265: step 2302, loss 5.79244.
Train: 2018-08-02T14:52:43.405097: step 2303, loss 4.53321.
Train: 2018-08-02T14:52:43.467611: step 2304, loss 3.02214.
Train: 2018-08-02T14:52:43.530093: step 2305, loss 3.52583.
Train: 2018-08-02T14:52:43.592552: step 2306, loss 4.53321.
Train: 2018-08-02T14:52:43.639438: step 2307, loss 3.14807.
Train: 2018-08-02T14:52:43.701904: step 2308, loss 4.53321.
Train: 2018-08-02T14:52:43.764389: step 2309, loss 3.9036.
Train: 2018-08-02T14:52:43.811253: step 2310, loss 3.65176.
Test: 2018-08-02T14:52:44.014329: step 2310, loss 3.81863.
Train: 2018-08-02T14:52:44.076816: step 2311, loss 3.65176.
Train: 2018-08-02T14:52:44.139300: step 2312, loss 2.64437.
Train: 2018-08-02T14:52:44.186190: step 2313, loss 3.02214.
Train: 2018-08-02T14:52:44.248651: step 2314, loss 3.77768.
Train: 2018-08-02T14:52:44.311166: step 2315, loss 3.9036.
Train: 2018-08-02T14:52:44.373620: step 2316, loss 3.9036.
Train: 2018-08-02T14:52:44.420515: step 2317, loss 3.9036.
Train: 2018-08-02T14:52:44.482996: step 2318, loss 4.78506.
Train: 2018-08-02T14:52:44.545487: step 2319, loss 5.5406.
Train: 2018-08-02T14:52:44.592320: step 2320, loss 3.9036.
Test: 2018-08-02T14:52:44.795397: step 2320, loss 3.81863.
Train: 2018-08-02T14:52:44.857914: step 2321, loss 4.15545.
Train: 2018-08-02T14:52:44.920397: step 2322, loss 3.65176.
Train: 2018-08-02T14:52:44.982853: step 2323, loss 4.78506.
Train: 2018-08-02T14:52:45.029719: step 2324, loss 3.39991.
Train: 2018-08-02T14:52:45.092233: step 2325, loss 4.02952.
Train: 2018-08-02T14:52:45.154693: step 2326, loss 4.15545.
Train: 2018-08-02T14:52:45.201584: step 2327, loss 3.27399.
Train: 2018-08-02T14:52:45.264039: step 2328, loss 3.02214.
Train: 2018-08-02T14:52:45.326524: step 2329, loss 2.7703.
Train: 2018-08-02T14:52:45.373420: step 2330, loss 4.02952.
Test: 2018-08-02T14:52:45.576487: step 2330, loss 3.81863.
Train: 2018-08-02T14:52:45.638949: step 2331, loss 4.02952.
Train: 2018-08-02T14:52:45.701433: step 2332, loss 4.65914.
Train: 2018-08-02T14:52:45.763950: step 2333, loss 4.15545.
Train: 2018-08-02T14:52:45.810787: step 2334, loss 3.14807.
Train: 2018-08-02T14:52:45.873300: step 2335, loss 3.27399.
Train: 2018-08-02T14:52:45.935760: step 2336, loss 3.27399.
Train: 2018-08-02T14:52:45.982652: step 2337, loss 3.77768.
Train: 2018-08-02T14:52:46.045106: step 2338, loss 2.89622.
Train: 2018-08-02T14:52:46.107623: step 2339, loss 4.02952.
Train: 2018-08-02T14:52:46.154452: step 2340, loss 3.9036.
Test: 2018-08-02T14:52:46.357563: step 2340, loss 3.81863.
Train: 2018-08-02T14:52:46.420016: step 2341, loss 4.40729.
Train: 2018-08-02T14:52:46.482534: step 2342, loss 4.65914.
Train: 2018-08-02T14:52:46.529396: step 2343, loss 4.28137.
Train: 2018-08-02T14:52:46.591850: step 2344, loss 4.15545.
Train: 2018-08-02T14:52:46.654337: step 2345, loss 4.91098.
Train: 2018-08-02T14:52:46.701200: step 2346, loss 4.91098.
Train: 2018-08-02T14:52:46.763711: step 2347, loss 4.02952.
Train: 2018-08-02T14:52:46.826204: step 2348, loss 4.53321.
Train: 2018-08-02T14:52:46.873066: step 2349, loss 3.39991.
Train: 2018-08-02T14:52:46.935552: step 2350, loss 5.16283.
Test: 2018-08-02T14:52:47.138597: step 2350, loss 3.81863.
Train: 2018-08-02T14:52:47.201107: step 2351, loss 4.02952.
Train: 2018-08-02T14:52:47.263598: step 2352, loss 4.02952.
Train: 2018-08-02T14:52:47.310463: step 2353, loss 4.28137.
Train: 2018-08-02T14:52:47.372949: step 2354, loss 4.15545.
Train: 2018-08-02T14:52:47.435440: step 2355, loss 2.14068.
Train: 2018-08-02T14:52:47.482267: step 2356, loss 4.40729.
Train: 2018-08-02T14:52:47.544752: step 2357, loss 4.02952.
Train: 2018-08-02T14:52:47.607272: step 2358, loss 4.40729.
Train: 2018-08-02T14:52:47.669731: step 2359, loss 4.91098.
Train: 2018-08-02T14:52:47.716615: step 2360, loss 3.65176.
Test: 2018-08-02T14:52:47.919694: step 2360, loss 3.81863.
Train: 2018-08-02T14:52:47.982182: step 2361, loss 4.02952.
Train: 2018-08-02T14:52:48.029044: step 2362, loss 3.14807.
Train: 2018-08-02T14:52:48.091500: step 2363, loss 4.40729.
Train: 2018-08-02T14:52:48.154016: step 2364, loss 3.14807.
Train: 2018-08-02T14:52:48.216499: step 2365, loss 3.9036.
Train: 2018-08-02T14:52:48.263364: step 2366, loss 3.14807.
Train: 2018-08-02T14:52:48.325819: step 2367, loss 5.66652.
Train: 2018-08-02T14:52:48.388305: step 2368, loss 4.91098.
Train: 2018-08-02T14:52:48.435168: step 2369, loss 4.15545.
Train: 2018-08-02T14:52:48.497654: step 2370, loss 4.40729.
Test: 2018-08-02T14:52:48.700731: step 2370, loss 3.81863.
Train: 2018-08-02T14:52:48.763247: step 2371, loss 3.52583.
Train: 2018-08-02T14:52:48.810126: step 2372, loss 4.15545.
Train: 2018-08-02T14:52:48.872567: step 2373, loss 4.91098.
Train: 2018-08-02T14:52:48.935055: step 2374, loss 3.14807.
Train: 2018-08-02T14:52:48.997567: step 2375, loss 4.91098.
Train: 2018-08-02T14:52:49.044401: step 2376, loss 4.02952.
Train: 2018-08-02T14:52:49.106919: step 2377, loss 4.28137.
Train: 2018-08-02T14:52:49.169403: step 2378, loss 5.79244.
Train: 2018-08-02T14:52:49.216265: step 2379, loss 4.02952.
Train: 2018-08-02T14:52:49.278752: step 2380, loss 3.52583.
Test: 2018-08-02T14:52:49.481822: step 2380, loss 3.81863.
Train: 2018-08-02T14:52:49.528664: step 2381, loss 4.28137.
Train: 2018-08-02T14:52:49.591177: step 2382, loss 4.40729.
Train: 2018-08-02T14:52:49.653663: step 2383, loss 4.53321.
Train: 2018-08-02T14:52:49.716155: step 2384, loss 3.77768.
Train: 2018-08-02T14:52:49.778638: step 2385, loss 4.40729.
Train: 2018-08-02T14:52:49.841123: step 2386, loss 3.9036.
Train: 2018-08-02T14:52:49.887984: step 2387, loss 4.40729.
Train: 2018-08-02T14:52:49.950473: step 2388, loss 3.77768.
Train: 2018-08-02T14:52:49.997334: step 2389, loss 3.39991.
Train: 2018-08-02T14:52:50.059814: step 2390, loss 3.52583.
Test: 2018-08-02T14:52:50.262896: step 2390, loss 3.81863.
Train: 2018-08-02T14:52:50.325377: step 2391, loss 4.02952.
Train: 2018-08-02T14:52:50.387836: step 2392, loss 4.28137.
Train: 2018-08-02T14:52:50.434726: step 2393, loss 4.53321.
Train: 2018-08-02T14:52:50.497217: step 2394, loss 3.52583.
Train: 2018-08-02T14:52:50.559673: step 2395, loss 3.39991.
Train: 2018-08-02T14:52:50.606568: step 2396, loss 5.0369.
Train: 2018-08-02T14:52:50.669050: step 2397, loss 4.78506.
Train: 2018-08-02T14:52:50.731535: step 2398, loss 2.89622.
Train: 2018-08-02T14:52:50.778400: step 2399, loss 4.28137.
Train: 2018-08-02T14:52:50.840884: step 2400, loss 4.28137.
Test: 2018-08-02T14:52:51.043957: step 2400, loss 3.81863.
Train: 2018-08-02T14:52:51.575089: step 2401, loss 3.77768.
Train: 2018-08-02T14:52:51.621954: step 2402, loss 4.28137.
Train: 2018-08-02T14:52:51.684438: step 2403, loss 5.0369.
Train: 2018-08-02T14:52:51.746924: step 2404, loss 3.77768.
Train: 2018-08-02T14:52:51.809405: step 2405, loss 4.91098.
Train: 2018-08-02T14:52:51.856243: step 2406, loss 3.77768.
Train: 2018-08-02T14:52:51.918761: step 2407, loss 4.02952.
Train: 2018-08-02T14:52:51.981243: step 2408, loss 4.91098.
Train: 2018-08-02T14:52:52.028110: step 2409, loss 4.40729.
Train: 2018-08-02T14:52:52.090591: step 2410, loss 4.53321.
Test: 2018-08-02T14:52:52.293641: step 2410, loss 3.81863.
Train: 2018-08-02T14:52:52.356159: step 2411, loss 3.65176.
Train: 2018-08-02T14:52:52.403020: step 2412, loss 3.02214.
Train: 2018-08-02T14:52:52.465503: step 2413, loss 3.52583.
Train: 2018-08-02T14:52:52.527991: step 2414, loss 3.65176.
Train: 2018-08-02T14:52:52.574857: step 2415, loss 4.28137.
Train: 2018-08-02T14:52:52.621714: step 2416, loss 2.95498.
Train: 2018-08-02T14:52:52.684175: step 2417, loss 3.52583.
Train: 2018-08-02T14:52:52.746684: step 2418, loss 4.40729.
Train: 2018-08-02T14:52:52.793523: step 2419, loss 4.15545.
Train: 2018-08-02T14:52:52.856042: step 2420, loss 5.16283.
Test: 2018-08-02T14:52:53.059085: step 2420, loss 3.81863.
Train: 2018-08-02T14:52:53.121601: step 2421, loss 5.16283.
Train: 2018-08-02T14:52:53.168461: step 2422, loss 4.40729.
Train: 2018-08-02T14:52:53.230949: step 2423, loss 3.14807.
Train: 2018-08-02T14:52:53.293438: step 2424, loss 4.40729.
Train: 2018-08-02T14:52:53.355892: step 2425, loss 4.40729.
Train: 2018-08-02T14:52:53.402756: step 2426, loss 4.53321.
Train: 2018-08-02T14:52:53.465269: step 2427, loss 3.52583.
Train: 2018-08-02T14:52:53.527758: step 2428, loss 3.9036.
Train: 2018-08-02T14:52:53.574623: step 2429, loss 5.28875.
Train: 2018-08-02T14:52:53.637106: step 2430, loss 4.78506.
Test: 2018-08-02T14:52:53.840153: step 2430, loss 3.81863.
Train: 2018-08-02T14:52:53.887043: step 2431, loss 3.65176.
Train: 2018-08-02T14:52:53.949536: step 2432, loss 4.53321.
Train: 2018-08-02T14:52:54.011987: step 2433, loss 2.64438.
Train: 2018-08-02T14:52:54.074508: step 2434, loss 5.16283.
Train: 2018-08-02T14:52:54.136990: step 2435, loss 4.40729.
Train: 2018-08-02T14:52:54.183823: step 2436, loss 3.65176.
Train: 2018-08-02T14:52:54.246309: step 2437, loss 3.77768.
Train: 2018-08-02T14:52:54.308795: step 2438, loss 5.28875.
Train: 2018-08-02T14:52:54.355690: step 2439, loss 5.41467.
Train: 2018-08-02T14:52:54.418173: step 2440, loss 3.65176.
Test: 2018-08-02T14:52:54.621220: step 2440, loss 3.81863.
Train: 2018-08-02T14:52:54.668114: step 2441, loss 3.52583.
Train: 2018-08-02T14:52:54.730597: step 2442, loss 3.65176.
Train: 2018-08-02T14:52:54.793081: step 2443, loss 5.5406.
Train: 2018-08-02T14:52:54.855574: step 2444, loss 2.64438.
Train: 2018-08-02T14:52:54.902406: step 2445, loss 4.78506.
Train: 2018-08-02T14:52:54.964918: step 2446, loss 3.9036.
Train: 2018-08-02T14:52:55.027375: step 2447, loss 4.15545.
Train: 2018-08-02T14:52:55.074239: step 2448, loss 4.02952.
Train: 2018-08-02T14:52:55.136755: step 2449, loss 3.39991.
Train: 2018-08-02T14:52:55.199243: step 2450, loss 3.14807.
Test: 2018-08-02T14:52:55.402289: step 2450, loss 3.81863.
Train: 2018-08-02T14:52:55.464772: step 2451, loss 2.7703.
Train: 2018-08-02T14:52:55.527288: step 2452, loss 3.52583.
Train: 2018-08-02T14:52:55.574147: step 2453, loss 3.52583.
Train: 2018-08-02T14:52:55.636639: step 2454, loss 4.15545.
Train: 2018-08-02T14:52:55.699125: step 2455, loss 2.64438.
Train: 2018-08-02T14:52:55.745987: step 2456, loss 4.40729.
Train: 2018-08-02T14:52:55.808472: step 2457, loss 4.28137.
Train: 2018-08-02T14:52:55.870965: step 2458, loss 3.52583.
Train: 2018-08-02T14:52:55.917793: step 2459, loss 4.65914.
Train: 2018-08-02T14:52:55.980305: step 2460, loss 3.52583.
Test: 2018-08-02T14:52:56.183354: step 2460, loss 3.81863.
Train: 2018-08-02T14:52:56.245870: step 2461, loss 3.65176.
Train: 2018-08-02T14:52:56.292735: step 2462, loss 3.02214.
Train: 2018-08-02T14:52:56.355191: step 2463, loss 3.52583.
Train: 2018-08-02T14:52:56.417677: step 2464, loss 3.9036.
Train: 2018-08-02T14:52:56.480194: step 2465, loss 4.91098.
Train: 2018-08-02T14:52:56.542675: step 2466, loss 3.39991.
Train: 2018-08-02T14:52:56.589511: step 2467, loss 3.65176.
Train: 2018-08-02T14:52:56.652026: step 2468, loss 3.40533.
Train: 2018-08-02T14:52:56.714481: step 2469, loss 3.77768.
Train: 2018-08-02T14:52:56.761375: step 2470, loss 2.7703.
Test: 2018-08-02T14:52:56.964447: step 2470, loss 3.81863.
Train: 2018-08-02T14:52:57.026938: step 2471, loss 4.15545.
Train: 2018-08-02T14:52:57.073804: step 2472, loss 3.52583.
Train: 2018-08-02T14:52:57.136256: step 2473, loss 3.02214.
Train: 2018-08-02T14:52:57.198775: step 2474, loss 4.28137.
Train: 2018-08-02T14:52:57.261228: step 2475, loss 3.39991.
Train: 2018-08-02T14:52:57.308124: step 2476, loss 3.9036.
Train: 2018-08-02T14:52:57.370577: step 2477, loss 2.7703.
Train: 2018-08-02T14:52:57.433088: step 2478, loss 3.65176.
Train: 2018-08-02T14:52:57.479951: step 2479, loss 4.65914.
Train: 2018-08-02T14:52:57.542435: step 2480, loss 3.52583.
Test: 2018-08-02T14:52:57.745513: step 2480, loss 3.81863.
Train: 2018-08-02T14:52:57.808004: step 2481, loss 4.53321.
Train: 2018-08-02T14:52:57.870460: step 2482, loss 4.40729.
Train: 2018-08-02T14:52:57.917354: step 2483, loss 5.41467.
Train: 2018-08-02T14:52:57.979808: step 2484, loss 3.39991.
Train: 2018-08-02T14:52:58.042323: step 2485, loss 3.77768.
Train: 2018-08-02T14:52:58.089187: step 2486, loss 3.52583.
Train: 2018-08-02T14:52:58.151643: step 2487, loss 4.02952.
Train: 2018-08-02T14:52:58.214158: step 2488, loss 3.9036.
Train: 2018-08-02T14:52:58.261024: step 2489, loss 3.77768.
Train: 2018-08-02T14:52:58.323478: step 2490, loss 4.40729.
Test: 2018-08-02T14:52:58.526555: step 2490, loss 3.81863.
Train: 2018-08-02T14:52:58.589042: step 2491, loss 4.02952.
Train: 2018-08-02T14:52:58.651557: step 2492, loss 4.65914.
Train: 2018-08-02T14:52:58.698391: step 2493, loss 3.14807.
Train: 2018-08-02T14:52:58.760878: step 2494, loss 4.78506.
Train: 2018-08-02T14:52:58.823396: step 2495, loss 3.77768.
Train: 2018-08-02T14:52:58.885872: step 2496, loss 4.40729.
Train: 2018-08-02T14:52:58.932741: step 2497, loss 4.40729.
Train: 2018-08-02T14:52:58.995228: step 2498, loss 3.52583.
Train: 2018-08-02T14:52:59.057712: step 2499, loss 3.52583.
Train: 2018-08-02T14:52:59.104574: step 2500, loss 3.77768.
Test: 2018-08-02T14:52:59.307653: step 2500, loss 3.81863.
Train: 2018-08-02T14:52:59.760672: step 2501, loss 4.53321.
Train: 2018-08-02T14:52:59.823127: step 2502, loss 3.9036.
Train: 2018-08-02T14:52:59.885613: step 2503, loss 4.28137.
Train: 2018-08-02T14:52:59.948099: step 2504, loss 3.14807.
Train: 2018-08-02T14:52:59.994993: step 2505, loss 5.03691.
Train: 2018-08-02T14:53:00.057478: step 2506, loss 4.15545.
Train: 2018-08-02T14:53:00.119963: step 2507, loss 3.14807.
Train: 2018-08-02T14:53:00.166796: step 2508, loss 5.16283.
Train: 2018-08-02T14:53:00.229315: step 2509, loss 4.40729.
Train: 2018-08-02T14:53:00.291798: step 2510, loss 4.28137.
Test: 2018-08-02T14:53:00.479222: step 2510, loss 3.81863.
Train: 2018-08-02T14:53:00.541743: step 2511, loss 4.65914.
Train: 2018-08-02T14:53:00.604227: step 2512, loss 3.27399.
Train: 2018-08-02T14:53:00.666679: step 2513, loss 4.02952.
Train: 2018-08-02T14:53:00.713573: step 2514, loss 3.65176.
Train: 2018-08-02T14:53:00.776030: step 2515, loss 4.40729.
Train: 2018-08-02T14:53:00.838544: step 2516, loss 3.14807.
Train: 2018-08-02T14:53:00.885410: step 2517, loss 3.52583.
Train: 2018-08-02T14:53:00.947866: step 2518, loss 4.02952.
Train: 2018-08-02T14:53:01.010348: step 2519, loss 4.28137.
Train: 2018-08-02T14:53:01.072859: step 2520, loss 4.28137.
Test: 2018-08-02T14:53:01.260320: step 2520, loss 3.81863.
Train: 2018-08-02T14:53:01.322776: step 2521, loss 3.65176.
Train: 2018-08-02T14:53:01.385287: step 2522, loss 3.14807.
Train: 2018-08-02T14:53:01.447778: step 2523, loss 4.91098.
Train: 2018-08-02T14:53:01.510232: step 2524, loss 4.65914.
Train: 2018-08-02T14:53:01.557097: step 2525, loss 3.27399.
Train: 2018-08-02T14:53:01.619612: step 2526, loss 4.15545.
Train: 2018-08-02T14:53:01.682118: step 2527, loss 4.78506.
Train: 2018-08-02T14:53:01.728930: step 2528, loss 3.65176.
Train: 2018-08-02T14:53:01.791446: step 2529, loss 5.16283.
Train: 2018-08-02T14:53:01.853901: step 2530, loss 3.14807.
Test: 2018-08-02T14:53:02.041382: step 2530, loss 3.81863.
Train: 2018-08-02T14:53:02.103845: step 2531, loss 4.02952.
Train: 2018-08-02T14:53:02.166360: step 2532, loss 3.02214.
Train: 2018-08-02T14:53:02.228814: step 2533, loss 4.53321.
Train: 2018-08-02T14:53:02.275679: step 2534, loss 4.91098.
Train: 2018-08-02T14:53:02.338198: step 2535, loss 5.03691.
Train: 2018-08-02T14:53:02.400678: step 2536, loss 3.39991.
Train: 2018-08-02T14:53:02.463135: step 2537, loss 3.77768.
Train: 2018-08-02T14:53:02.510029: step 2538, loss 3.9036.
Train: 2018-08-02T14:53:02.572517: step 2539, loss 4.15545.
Train: 2018-08-02T14:53:02.634999: step 2540, loss 4.15545.
Test: 2018-08-02T14:53:02.822449: step 2540, loss 3.81863.
Train: 2018-08-02T14:53:02.884947: step 2541, loss 3.52583.
Train: 2018-08-02T14:53:02.947396: step 2542, loss 4.65914.
Train: 2018-08-02T14:53:03.009911: step 2543, loss 4.28137.
Train: 2018-08-02T14:53:03.072393: step 2544, loss 4.28137.
Train: 2018-08-02T14:53:03.119232: step 2545, loss 4.65914.
Train: 2018-08-02T14:53:03.181741: step 2546, loss 3.02214.
Train: 2018-08-02T14:53:03.244231: step 2547, loss 3.77768.
Train: 2018-08-02T14:53:03.291065: step 2548, loss 4.02952.
Train: 2018-08-02T14:53:03.353584: step 2549, loss 5.79244.
Train: 2018-08-02T14:53:03.416065: step 2550, loss 5.16283.
Test: 2018-08-02T14:53:03.603491: step 2550, loss 3.81863.
Train: 2018-08-02T14:53:03.665977: step 2551, loss 4.65914.
Train: 2018-08-02T14:53:03.728462: step 2552, loss 4.91098.
Train: 2018-08-02T14:53:03.790947: step 2553, loss 4.02952.
Train: 2018-08-02T14:53:03.837841: step 2554, loss 2.7703.
Train: 2018-08-02T14:53:03.900299: step 2555, loss 4.15545.
Train: 2018-08-02T14:53:03.962782: step 2556, loss 4.02952.
Train: 2018-08-02T14:53:04.025299: step 2557, loss 3.39991.
Train: 2018-08-02T14:53:04.072158: step 2558, loss 3.77768.
Train: 2018-08-02T14:53:04.134619: step 2559, loss 4.15545.
Train: 2018-08-02T14:53:04.197133: step 2560, loss 3.65176.
Test: 2018-08-02T14:53:04.384583: step 2560, loss 3.81863.
Train: 2018-08-02T14:53:04.447075: step 2561, loss 4.28137.
Train: 2018-08-02T14:53:04.509560: step 2562, loss 4.28137.
Train: 2018-08-02T14:53:04.572045: step 2563, loss 3.27399.
Train: 2018-08-02T14:53:04.618912: step 2564, loss 4.15545.
Train: 2018-08-02T14:53:04.681396: step 2565, loss 2.89622.
Train: 2018-08-02T14:53:04.743851: step 2566, loss 4.28137.
Train: 2018-08-02T14:53:04.790745: step 2567, loss 4.83543.
Train: 2018-08-02T14:53:04.853201: step 2568, loss 3.65176.
Train: 2018-08-02T14:53:04.900094: step 2569, loss 2.64438.
Train: 2018-08-02T14:53:04.962548: step 2570, loss 4.91098.
Test: 2018-08-02T14:53:05.165626: step 2570, loss 3.81863.
Train: 2018-08-02T14:53:05.212491: step 2571, loss 4.78506.
Train: 2018-08-02T14:53:05.275008: step 2572, loss 4.40729.
Train: 2018-08-02T14:53:05.337493: step 2573, loss 3.65176.
Train: 2018-08-02T14:53:05.399952: step 2574, loss 4.40729.
Train: 2018-08-02T14:53:05.462457: step 2575, loss 4.65914.
Train: 2018-08-02T14:53:05.509296: step 2576, loss 4.28137.
Train: 2018-08-02T14:53:05.571781: step 2577, loss 3.65176.
Train: 2018-08-02T14:53:05.634299: step 2578, loss 2.64438.
Train: 2018-08-02T14:53:05.681163: step 2579, loss 3.77768.
Train: 2018-08-02T14:53:05.743641: step 2580, loss 3.9036.
Test: 2018-08-02T14:53:05.946717: step 2580, loss 3.81863.
Train: 2018-08-02T14:53:06.009209: step 2581, loss 4.40729.
Train: 2018-08-02T14:53:06.056073: step 2582, loss 4.02952.
Train: 2018-08-02T14:53:06.118528: step 2583, loss 3.14807.
Train: 2018-08-02T14:53:06.181039: step 2584, loss 3.77768.
Train: 2018-08-02T14:53:06.243530: step 2585, loss 5.28875.
Train: 2018-08-02T14:53:06.290363: step 2586, loss 3.65176.
Train: 2018-08-02T14:53:06.352849: step 2587, loss 4.28137.
Train: 2018-08-02T14:53:06.415359: step 2588, loss 3.77768.
Train: 2018-08-02T14:53:06.462226: step 2589, loss 3.65176.
Train: 2018-08-02T14:53:06.524708: step 2590, loss 3.39991.
Test: 2018-08-02T14:53:06.727761: step 2590, loss 3.81863.
Train: 2018-08-02T14:53:06.790276: step 2591, loss 3.39991.
Train: 2018-08-02T14:53:06.852731: step 2592, loss 4.15545.
Train: 2018-08-02T14:53:06.899596: step 2593, loss 4.91098.
Train: 2018-08-02T14:53:06.962108: step 2594, loss 4.15545.
Train: 2018-08-02T14:53:07.024595: step 2595, loss 3.27399.
Train: 2018-08-02T14:53:07.071463: step 2596, loss 4.53321.
Train: 2018-08-02T14:53:07.133915: step 2597, loss 3.39991.
Train: 2018-08-02T14:53:07.196426: step 2598, loss 3.39991.
Train: 2018-08-02T14:53:07.258887: step 2599, loss 4.04035.
Train: 2018-08-02T14:53:07.305783: step 2600, loss 2.64437.
Test: 2018-08-02T14:53:07.508852: step 2600, loss 3.81863.
Train: 2018-08-02T14:53:07.977501: step 2601, loss 4.15545.
Train: 2018-08-02T14:53:08.039983: step 2602, loss 3.65176.
Train: 2018-08-02T14:53:08.102471: step 2603, loss 4.28137.
Train: 2018-08-02T14:53:08.164925: step 2604, loss 4.28137.
Train: 2018-08-02T14:53:08.211816: step 2605, loss 4.15545.
Train: 2018-08-02T14:53:08.274278: step 2606, loss 5.16283.
Train: 2018-08-02T14:53:08.336786: step 2607, loss 5.0369.
Train: 2018-08-02T14:53:08.383622: step 2608, loss 3.39991.
Train: 2018-08-02T14:53:08.446134: step 2609, loss 2.26661.
Train: 2018-08-02T14:53:08.508623: step 2610, loss 3.27399.
Test: 2018-08-02T14:53:08.711671: step 2610, loss 3.81863.
Train: 2018-08-02T14:53:08.774181: step 2611, loss 3.77768.
Train: 2018-08-02T14:53:08.821053: step 2612, loss 3.65176.
Train: 2018-08-02T14:53:08.883536: step 2613, loss 4.40729.
Train: 2018-08-02T14:53:08.946024: step 2614, loss 5.16283.
Train: 2018-08-02T14:53:09.008478: step 2615, loss 2.7703.
Train: 2018-08-02T14:53:09.070961: step 2616, loss 4.02952.
Train: 2018-08-02T14:53:09.117827: step 2617, loss 5.66652.
Train: 2018-08-02T14:53:09.180343: step 2618, loss 4.91098.
Train: 2018-08-02T14:53:09.242796: step 2619, loss 3.27399.
Train: 2018-08-02T14:53:09.289660: step 2620, loss 3.39991.
Test: 2018-08-02T14:53:09.492762: step 2620, loss 3.81863.
Train: 2018-08-02T14:53:09.555254: step 2621, loss 4.53321.
Train: 2018-08-02T14:53:09.617709: step 2622, loss 4.15545.
Train: 2018-08-02T14:53:09.664603: step 2623, loss 4.15545.
Train: 2018-08-02T14:53:09.727092: step 2624, loss 3.77768.
Train: 2018-08-02T14:53:09.789575: step 2625, loss 3.27399.
Train: 2018-08-02T14:53:09.836408: step 2626, loss 4.02952.
Train: 2018-08-02T14:53:09.898923: step 2627, loss 3.39991.
Train: 2018-08-02T14:53:09.961407: step 2628, loss 3.52583.
Train: 2018-08-02T14:53:10.023869: step 2629, loss 3.02214.
Train: 2018-08-02T14:53:10.070759: step 2630, loss 4.40729.
Test: 2018-08-02T14:53:10.273804: step 2630, loss 3.81863.
Train: 2018-08-02T14:53:10.336318: step 2631, loss 5.79244.
Train: 2018-08-02T14:53:10.398776: step 2632, loss 4.28137.
Train: 2018-08-02T14:53:10.445672: step 2633, loss 3.77768.
Train: 2018-08-02T14:53:10.508157: step 2634, loss 4.28137.
Train: 2018-08-02T14:53:10.570642: step 2635, loss 3.65176.
Train: 2018-08-02T14:53:10.633096: step 2636, loss 4.02952.
Train: 2018-08-02T14:53:10.679990: step 2637, loss 3.77768.
Train: 2018-08-02T14:53:10.742447: step 2638, loss 4.78506.
Train: 2018-08-02T14:53:10.804956: step 2639, loss 4.02952.
Train: 2018-08-02T14:53:10.851828: step 2640, loss 3.9036.
Test: 2018-08-02T14:53:11.054902: step 2640, loss 3.81863.
Train: 2018-08-02T14:53:11.117381: step 2641, loss 4.53321.
Train: 2018-08-02T14:53:11.179873: step 2642, loss 3.65176.
Train: 2018-08-02T14:53:11.226709: step 2643, loss 3.52583.
Train: 2018-08-02T14:53:11.289223: step 2644, loss 2.90129.
Train: 2018-08-02T14:53:11.351678: step 2645, loss 3.65176.
Train: 2018-08-02T14:53:11.398572: step 2646, loss 5.5406.
Train: 2018-08-02T14:53:11.461054: step 2647, loss 3.77768.
Train: 2018-08-02T14:53:11.523514: step 2648, loss 4.53321.
Train: 2018-08-02T14:53:11.570409: step 2649, loss 3.77768.
Train: 2018-08-02T14:53:11.632861: step 2650, loss 3.65176.
Test: 2018-08-02T14:53:11.835963: step 2650, loss 3.81863.
Train: 2018-08-02T14:53:11.898448: step 2651, loss 4.65914.
Train: 2018-08-02T14:53:11.945321: step 2652, loss 3.14807.
Train: 2018-08-02T14:53:12.007774: step 2653, loss 4.02952.
Train: 2018-08-02T14:53:12.070261: step 2654, loss 4.15545.
Train: 2018-08-02T14:53:12.132776: step 2655, loss 3.9036.
Train: 2018-08-02T14:53:12.179608: step 2656, loss 3.65176.
Train: 2018-08-02T14:53:12.242094: step 2657, loss 4.53321.
Train: 2018-08-02T14:53:12.304580: step 2658, loss 3.02214.
Train: 2018-08-02T14:53:12.351476: step 2659, loss 3.39991.
Train: 2018-08-02T14:53:12.413929: step 2660, loss 4.91098.
Test: 2018-08-02T14:53:12.617005: step 2660, loss 3.81863.
Train: 2018-08-02T14:53:12.679522: step 2661, loss 3.39991.
Train: 2018-08-02T14:53:12.726356: step 2662, loss 3.9036.
Train: 2018-08-02T14:53:12.788841: step 2663, loss 3.65176.
Train: 2018-08-02T14:53:12.851328: step 2664, loss 3.39991.
Train: 2018-08-02T14:53:12.913812: step 2665, loss 4.40729.
Train: 2018-08-02T14:53:12.960675: step 2666, loss 4.78506.
Train: 2018-08-02T14:53:13.023162: step 2667, loss 4.53321.
Train: 2018-08-02T14:53:13.085676: step 2668, loss 3.39991.
Train: 2018-08-02T14:53:13.132541: step 2669, loss 4.78506.
Train: 2018-08-02T14:53:13.194995: step 2670, loss 3.39991.
Test: 2018-08-02T14:53:13.398098: step 2670, loss 3.81863.
Train: 2018-08-02T14:53:13.460558: step 2671, loss 4.40729.
Train: 2018-08-02T14:53:13.507456: step 2672, loss 3.27399.
Train: 2018-08-02T14:53:13.569941: step 2673, loss 4.02952.
Train: 2018-08-02T14:53:13.632395: step 2674, loss 4.02952.
Train: 2018-08-02T14:53:13.679288: step 2675, loss 4.02952.
Train: 2018-08-02T14:53:13.741768: step 2676, loss 3.65176.
Train: 2018-08-02T14:53:13.804230: step 2677, loss 3.65176.
Train: 2018-08-02T14:53:13.866739: step 2678, loss 3.77768.
Train: 2018-08-02T14:53:13.913608: step 2679, loss 4.40729.
Train: 2018-08-02T14:53:13.976064: step 2680, loss 4.15545.
Test: 2018-08-02T14:53:14.179141: step 2680, loss 3.81863.
Train: 2018-08-02T14:53:14.241656: step 2681, loss 5.0369.
Train: 2018-08-02T14:53:14.295549: step 2682, loss 3.39991.
Train: 2018-08-02T14:53:14.358004: step 2683, loss 4.40729.
Train: 2018-08-02T14:53:14.420490: step 2684, loss 4.28137.
Train: 2018-08-02T14:53:14.482976: step 2685, loss 4.78506.
Train: 2018-08-02T14:53:14.545460: step 2686, loss 4.02952.
Train: 2018-08-02T14:53:14.592324: step 2687, loss 3.02214.
Train: 2018-08-02T14:53:14.654811: step 2688, loss 3.77768.
Train: 2018-08-02T14:53:14.717321: step 2689, loss 4.65914.
Train: 2018-08-02T14:53:14.779780: step 2690, loss 4.78506.
Test: 2018-08-02T14:53:14.982888: step 2690, loss 3.81863.
Train: 2018-08-02T14:53:15.045344: step 2691, loss 4.40729.
Train: 2018-08-02T14:53:15.107829: step 2692, loss 4.53321.
Train: 2018-08-02T14:53:15.170313: step 2693, loss 3.9036.
Train: 2018-08-02T14:53:15.232799: step 2694, loss 3.27399.
Train: 2018-08-02T14:53:15.295316: step 2695, loss 4.53321.
Train: 2018-08-02T14:53:15.342150: step 2696, loss 4.53321.
Train: 2018-08-02T14:53:15.404664: step 2697, loss 4.40729.
Train: 2018-08-02T14:53:15.467145: step 2698, loss 4.40729.
Train: 2018-08-02T14:53:15.529635: step 2699, loss 4.40729.
Train: 2018-08-02T14:53:15.576500: step 2700, loss 4.02952.
Test: 2018-08-02T14:53:15.779546: step 2700, loss 3.81863.
Train: 2018-08-02T14:53:16.279459: step 2701, loss 4.28137.
Train: 2018-08-02T14:53:16.341944: step 2702, loss 4.40729.
Train: 2018-08-02T14:53:16.404400: step 2703, loss 4.78506.
Train: 2018-08-02T14:53:16.451295: step 2704, loss 4.40729.
Train: 2018-08-02T14:53:16.513750: step 2705, loss 4.40729.
Train: 2018-08-02T14:53:16.576261: step 2706, loss 3.39991.
Train: 2018-08-02T14:53:16.638719: step 2707, loss 3.9036.
Train: 2018-08-02T14:53:16.685614: step 2708, loss 4.28137.
Train: 2018-08-02T14:53:16.748099: step 2709, loss 4.15545.
Train: 2018-08-02T14:53:16.810585: step 2710, loss 3.9036.
Test: 2018-08-02T14:53:17.013633: step 2710, loss 3.81863.
Train: 2018-08-02T14:53:17.060497: step 2711, loss 3.02214.
Train: 2018-08-02T14:53:17.123009: step 2712, loss 3.77768.
Train: 2018-08-02T14:53:17.185493: step 2713, loss 4.91098.
Train: 2018-08-02T14:53:17.247983: step 2714, loss 4.78506.
Train: 2018-08-02T14:53:17.310438: step 2715, loss 3.52583.
Train: 2018-08-02T14:53:17.357303: step 2716, loss 4.53321.
Train: 2018-08-02T14:53:17.419787: step 2717, loss 3.9036.
Train: 2018-08-02T14:53:17.466650: step 2718, loss 1.88044.
Train: 2018-08-02T14:53:17.529165: step 2719, loss 3.39991.
Train: 2018-08-02T14:53:17.591621: step 2720, loss 5.5406.
Test: 2018-08-02T14:53:17.794734: step 2720, loss 3.81863.
Train: 2018-08-02T14:53:17.857184: step 2721, loss 2.7703.
Train: 2018-08-02T14:53:17.919672: step 2722, loss 3.65176.
Train: 2018-08-02T14:53:17.982156: step 2723, loss 3.52583.
Train: 2018-08-02T14:53:18.044641: step 2724, loss 4.78506.
Train: 2018-08-02T14:53:18.091505: step 2725, loss 4.15545.
Train: 2018-08-02T14:53:18.153990: step 2726, loss 4.40729.
Train: 2018-08-02T14:53:18.216475: step 2727, loss 2.89622.
Train: 2018-08-02T14:53:18.278961: step 2728, loss 2.89622.
Train: 2018-08-02T14:53:18.341447: step 2729, loss 4.02952.
Train: 2018-08-02T14:53:18.388344: step 2730, loss 2.39253.
Test: 2018-08-02T14:53:18.591405: step 2730, loss 3.81863.
Train: 2018-08-02T14:53:18.653917: step 2731, loss 3.9036.
Train: 2018-08-02T14:53:18.716359: step 2732, loss 2.89622.
Train: 2018-08-02T14:53:18.778844: step 2733, loss 3.65176.
Train: 2018-08-02T14:53:18.841329: step 2734, loss 4.28137.
Train: 2018-08-02T14:53:18.903815: step 2735, loss 4.53321.
Train: 2018-08-02T14:53:18.950705: step 2736, loss 4.02952.
Train: 2018-08-02T14:53:19.013165: step 2737, loss 3.39991.
Train: 2018-08-02T14:53:19.075675: step 2738, loss 4.02952.
Train: 2018-08-02T14:53:19.138163: step 2739, loss 3.77768.
Train: 2018-08-02T14:53:19.200645: step 2740, loss 4.65914.
Test: 2018-08-02T14:53:19.403722: step 2740, loss 3.81863.
Train: 2018-08-02T14:53:19.466213: step 2741, loss 3.9036.
Train: 2018-08-02T14:53:19.513080: step 2742, loss 4.02952.
Train: 2018-08-02T14:53:19.575563: step 2743, loss 3.39991.
Train: 2018-08-02T14:53:19.638051: step 2744, loss 3.9036.
Train: 2018-08-02T14:53:19.700534: step 2745, loss 4.28137.
Train: 2018-08-02T14:53:19.747392: step 2746, loss 3.27399.
Train: 2018-08-02T14:53:19.809877: step 2747, loss 3.65176.
Train: 2018-08-02T14:53:19.872351: step 2748, loss 4.28137.
Train: 2018-08-02T14:53:19.919232: step 2749, loss 4.40729.
Train: 2018-08-02T14:53:19.981721: step 2750, loss 3.65176.
Test: 2018-08-02T14:53:20.184794: step 2750, loss 3.81863.
Train: 2018-08-02T14:53:20.247279: step 2751, loss 3.52583.
Train: 2018-08-02T14:53:20.309765: step 2752, loss 3.02214.
Train: 2018-08-02T14:53:20.356599: step 2753, loss 4.02952.
Train: 2018-08-02T14:53:20.419116: step 2754, loss 4.65914.
Train: 2018-08-02T14:53:20.481601: step 2755, loss 4.28137.
Train: 2018-08-02T14:53:20.528464: step 2756, loss 3.02214.
Train: 2018-08-02T14:53:20.590945: step 2757, loss 3.14807.
Train: 2018-08-02T14:53:20.653405: step 2758, loss 4.91098.
Train: 2018-08-02T14:53:20.700299: step 2759, loss 3.27399.
Train: 2018-08-02T14:53:20.762787: step 2760, loss 3.02214.
Test: 2018-08-02T14:53:20.965861: step 2760, loss 3.81863.
Train: 2018-08-02T14:53:21.028321: step 2761, loss 5.16283.
Train: 2018-08-02T14:53:21.075182: step 2762, loss 4.91098.
Train: 2018-08-02T14:53:21.137700: step 2763, loss 4.53321.
Train: 2018-08-02T14:53:21.200184: step 2764, loss 3.27399.
Train: 2018-08-02T14:53:21.262666: step 2765, loss 3.14807.
Train: 2018-08-02T14:53:21.309532: step 2766, loss 3.9036.
Train: 2018-08-02T14:53:21.372016: step 2767, loss 3.39991.
Train: 2018-08-02T14:53:21.434503: step 2768, loss 3.65176.
Train: 2018-08-02T14:53:21.481369: step 2769, loss 4.40729.
Train: 2018-08-02T14:53:21.543847: step 2770, loss 4.78506.
Test: 2018-08-02T14:53:21.731307: step 2770, loss 3.81863.
Train: 2018-08-02T14:53:21.793788: step 2771, loss 4.15545.
Train: 2018-08-02T14:53:21.856281: step 2772, loss 3.27399.
Train: 2018-08-02T14:53:21.918733: step 2773, loss 4.78506.
Train: 2018-08-02T14:53:21.965628: step 2774, loss 4.53321.
Train: 2018-08-02T14:53:22.028113: step 2775, loss 4.15545.
Train: 2018-08-02T14:53:22.090599: step 2776, loss 2.7703.
Train: 2018-08-02T14:53:22.137457: step 2777, loss 5.5406.
Train: 2018-08-02T14:53:22.199943: step 2778, loss 4.40729.
Train: 2018-08-02T14:53:22.262433: step 2779, loss 3.52583.
Train: 2018-08-02T14:53:22.309298: step 2780, loss 4.65914.
Test: 2018-08-02T14:53:22.512344: step 2780, loss 3.81863.
Train: 2018-08-02T14:53:22.574860: step 2781, loss 3.65176.
Train: 2018-08-02T14:53:22.637348: step 2782, loss 3.65176.
Train: 2018-08-02T14:53:22.699824: step 2783, loss 4.02952.
Train: 2018-08-02T14:53:22.746666: step 2784, loss 4.15545.
Train: 2018-08-02T14:53:22.809151: step 2785, loss 4.40729.
Train: 2018-08-02T14:53:22.871665: step 2786, loss 4.65914.
Train: 2018-08-02T14:53:22.918499: step 2787, loss 3.65176.
Train: 2018-08-02T14:53:22.981036: step 2788, loss 4.15545.
Train: 2018-08-02T14:53:23.043501: step 2789, loss 3.77768.
Train: 2018-08-02T14:53:23.090364: step 2790, loss 3.65176.
Test: 2018-08-02T14:53:23.293410: step 2790, loss 3.81863.
Train: 2018-08-02T14:53:23.355896: step 2791, loss 4.15545.
Train: 2018-08-02T14:53:23.418381: step 2792, loss 3.52583.
Train: 2018-08-02T14:53:23.465246: step 2793, loss 4.65914.
Train: 2018-08-02T14:53:23.527762: step 2794, loss 4.40729.
Train: 2018-08-02T14:53:23.590244: step 2795, loss 4.28137.
Train: 2018-08-02T14:53:23.637115: step 2796, loss 5.0369.
Train: 2018-08-02T14:53:23.699565: step 2797, loss 4.28137.
Train: 2018-08-02T14:53:23.762086: step 2798, loss 4.40729.
Train: 2018-08-02T14:53:23.824539: step 2799, loss 4.02952.
Train: 2018-08-02T14:53:23.871429: step 2800, loss 3.52583.
Test: 2018-08-02T14:53:24.074477: step 2800, loss 3.81863.
Train: 2018-08-02T14:53:24.590014: step 2801, loss 4.53321.
Train: 2018-08-02T14:53:24.636848: step 2802, loss 3.77768.
Train: 2018-08-02T14:53:24.699333: step 2803, loss 3.52583.
Train: 2018-08-02T14:53:24.761819: step 2804, loss 3.52583.
Train: 2018-08-02T14:53:24.808713: step 2805, loss 4.53321.
Train: 2018-08-02T14:53:24.871195: step 2806, loss 2.14068.
Train: 2018-08-02T14:53:24.933652: step 2807, loss 4.15545.
Train: 2018-08-02T14:53:24.980517: step 2808, loss 4.02952.
Train: 2018-08-02T14:53:25.043001: step 2809, loss 3.9036.
Train: 2018-08-02T14:53:25.105517: step 2810, loss 4.53321.
Test: 2018-08-02T14:53:25.292943: step 2810, loss 3.81863.
Train: 2018-08-02T14:53:25.355462: step 2811, loss 2.51845.
Train: 2018-08-02T14:53:25.417915: step 2812, loss 5.0369.
Train: 2018-08-02T14:53:25.480398: step 2813, loss 4.65914.
Train: 2018-08-02T14:53:25.527264: step 2814, loss 4.78506.
Train: 2018-08-02T14:53:25.589779: step 2815, loss 4.65914.
Train: 2018-08-02T14:53:25.652265: step 2816, loss 4.53321.
Train: 2018-08-02T14:53:25.699099: step 2817, loss 4.40729.
Train: 2018-08-02T14:53:25.761618: step 2818, loss 4.15545.
Train: 2018-08-02T14:53:25.824070: step 2819, loss 4.02952.
Train: 2018-08-02T14:53:25.870958: step 2820, loss 5.16283.
Test: 2018-08-02T14:53:26.074010: step 2820, loss 3.81863.
Train: 2018-08-02T14:53:26.136495: step 2821, loss 4.15545.
Train: 2018-08-02T14:53:26.198981: step 2822, loss 4.28137.
Train: 2018-08-02T14:53:26.245845: step 2823, loss 4.28137.
Train: 2018-08-02T14:53:26.308331: step 2824, loss 2.89622.
Train: 2018-08-02T14:53:26.370816: step 2825, loss 4.40729.
Train: 2018-08-02T14:53:26.417680: step 2826, loss 4.65914.
Train: 2018-08-02T14:53:26.480196: step 2827, loss 3.52583.
Train: 2018-08-02T14:53:26.542678: step 2828, loss 3.52583.
Train: 2018-08-02T14:53:26.589545: step 2829, loss 4.53321.
Train: 2018-08-02T14:53:26.652030: step 2830, loss 4.28137.
Test: 2018-08-02T14:53:26.855077: step 2830, loss 3.81863.
Train: 2018-08-02T14:53:26.917592: step 2831, loss 4.15545.
Train: 2018-08-02T14:53:26.980078: step 2832, loss 3.27399.
Train: 2018-08-02T14:53:27.026946: step 2833, loss 4.53321.
Train: 2018-08-02T14:53:27.089398: step 2834, loss 3.9036.
Train: 2018-08-02T14:53:27.151914: step 2835, loss 3.27399.
Train: 2018-08-02T14:53:27.214399: step 2836, loss 3.27399.
Train: 2018-08-02T14:53:27.261233: step 2837, loss 5.16283.
Train: 2018-08-02T14:53:27.323749: step 2838, loss 3.39991.
Train: 2018-08-02T14:53:27.386204: step 2839, loss 4.15545.
Train: 2018-08-02T14:53:27.433067: step 2840, loss 3.77768.
Test: 2018-08-02T14:53:27.636144: step 2840, loss 3.81863.
Train: 2018-08-02T14:53:27.698631: step 2841, loss 3.39991.
Train: 2018-08-02T14:53:27.761116: step 2842, loss 4.02952.
Train: 2018-08-02T14:53:27.808009: step 2843, loss 4.28137.
Train: 2018-08-02T14:53:27.870465: step 2844, loss 4.15545.
Train: 2018-08-02T14:53:27.932951: step 2845, loss 2.89622.
Train: 2018-08-02T14:53:27.995464: step 2846, loss 3.52583.
Train: 2018-08-02T14:53:28.042299: step 2847, loss 3.77768.
Train: 2018-08-02T14:53:28.104814: step 2848, loss 4.40729.
Train: 2018-08-02T14:53:28.167300: step 2849, loss 4.28137.
Train: 2018-08-02T14:53:28.214134: step 2850, loss 4.15545.
Test: 2018-08-02T14:53:28.417211: step 2850, loss 3.81863.
Train: 2018-08-02T14:53:28.479726: step 2851, loss 3.65176.
Train: 2018-08-02T14:53:28.542182: step 2852, loss 4.02952.
Train: 2018-08-02T14:53:28.604697: step 2853, loss 4.40729.
Train: 2018-08-02T14:53:28.667182: step 2854, loss 4.28137.
Train: 2018-08-02T14:53:28.714019: step 2855, loss 5.0369.
Train: 2018-08-02T14:53:28.776533: step 2856, loss 3.39991.
Train: 2018-08-02T14:53:28.823396: step 2857, loss 3.77768.
Train: 2018-08-02T14:53:28.885853: step 2858, loss 4.15545.
Train: 2018-08-02T14:53:28.995230: step 2859, loss 5.0369.
Train: 2018-08-02T14:53:29.042095: step 2860, loss 4.15545.
Test: 2018-08-02T14:53:29.245176: step 2860, loss 3.81863.
Train: 2018-08-02T14:53:29.307629: step 2861, loss 4.15545.
Train: 2018-08-02T14:53:29.370143: step 2862, loss 3.65176.
Train: 2018-08-02T14:53:29.432629: step 2863, loss 4.40729.
Train: 2018-08-02T14:53:29.479497: step 2864, loss 3.9036.
Train: 2018-08-02T14:53:29.541950: step 2865, loss 3.52583.
Train: 2018-08-02T14:53:29.604464: step 2866, loss 5.0369.
Train: 2018-08-02T14:53:29.651316: step 2867, loss 4.78506.
Train: 2018-08-02T14:53:29.713821: step 2868, loss 5.41467.
Train: 2018-08-02T14:53:29.760680: step 2869, loss 3.76089.
Train: 2018-08-02T14:53:29.823157: step 2870, loss 4.15545.
Test: 2018-08-02T14:53:30.026234: step 2870, loss 3.81863.
Train: 2018-08-02T14:53:30.119966: step 2871, loss 4.28137.
Train: 2018-08-02T14:53:30.182453: step 2872, loss 4.91098.
Train: 2018-08-02T14:53:30.244936: step 2873, loss 3.14807.
Train: 2018-08-02T14:53:30.307394: step 2874, loss 4.02952.
Train: 2018-08-02T14:53:30.354283: step 2875, loss 3.77768.
Train: 2018-08-02T14:53:30.416769: step 2876, loss 4.78506.
Train: 2018-08-02T14:53:30.479244: step 2877, loss 4.15545.
Train: 2018-08-02T14:53:30.541715: step 2878, loss 3.77768.
Train: 2018-08-02T14:53:30.604225: step 2879, loss 3.77768.
Train: 2018-08-02T14:53:30.666715: step 2880, loss 4.02952.
Test: 2018-08-02T14:53:30.869786: step 2880, loss 3.81863.
Train: 2018-08-02T14:53:30.932277: step 2881, loss 3.02214.
Train: 2018-08-02T14:53:30.979138: step 2882, loss 4.02952.
Train: 2018-08-02T14:53:31.041628: step 2883, loss 3.9036.
Train: 2018-08-02T14:53:31.104084: step 2884, loss 3.9036.
Train: 2018-08-02T14:53:31.166600: step 2885, loss 3.52583.
Train: 2018-08-02T14:53:31.213462: step 2886, loss 3.02214.
Train: 2018-08-02T14:53:31.275943: step 2887, loss 3.65176.
Train: 2018-08-02T14:53:31.338433: step 2888, loss 4.78506.
Train: 2018-08-02T14:53:31.385267: step 2889, loss 4.15545.
Train: 2018-08-02T14:53:31.447751: step 2890, loss 3.27399.
Test: 2018-08-02T14:53:31.650831: step 2890, loss 3.81863.
Train: 2018-08-02T14:53:31.713339: step 2891, loss 4.28137.
Train: 2018-08-02T14:53:31.760180: step 2892, loss 3.65176.
Train: 2018-08-02T14:53:31.822695: step 2893, loss 3.53125.
Train: 2018-08-02T14:53:31.885184: step 2894, loss 4.40729.
Train: 2018-08-02T14:53:31.947637: step 2895, loss 3.77768.
Train: 2018-08-02T14:53:31.994500: step 2896, loss 3.65176.
Train: 2018-08-02T14:53:32.057014: step 2897, loss 4.15545.
Train: 2018-08-02T14:53:32.119470: step 2898, loss 3.39991.
Train: 2018-08-02T14:53:32.166364: step 2899, loss 3.02214.
Train: 2018-08-02T14:53:32.228820: step 2900, loss 4.53321.
Test: 2018-08-02T14:53:32.431920: step 2900, loss 3.81863.
Train: 2018-08-02T14:53:32.931809: step 2901, loss 3.39991.
Train: 2018-08-02T14:53:32.994298: step 2902, loss 3.65176.
Train: 2018-08-02T14:53:33.056750: step 2903, loss 4.02952.
Train: 2018-08-02T14:53:33.103616: step 2904, loss 3.65176.
Train: 2018-08-02T14:53:33.166131: step 2905, loss 4.65914.
Train: 2018-08-02T14:53:33.228585: step 2906, loss 3.02214.
Train: 2018-08-02T14:53:33.291070: step 2907, loss 4.15545.
Train: 2018-08-02T14:53:33.337935: step 2908, loss 3.52583.
Train: 2018-08-02T14:53:33.400422: step 2909, loss 4.28137.
Train: 2018-08-02T14:53:33.447283: step 2910, loss 3.52583.
Test: 2018-08-02T14:53:33.650386: step 2910, loss 3.81863.
Train: 2018-08-02T14:53:33.712877: step 2911, loss 2.7703.
Train: 2018-08-02T14:53:33.775331: step 2912, loss 3.52583.
Train: 2018-08-02T14:53:33.822227: step 2913, loss 4.53321.
Train: 2018-08-02T14:53:33.884682: step 2914, loss 4.15545.
Train: 2018-08-02T14:53:33.947194: step 2915, loss 4.65914.
Train: 2018-08-02T14:53:33.994059: step 2916, loss 4.65914.
Train: 2018-08-02T14:53:34.056517: step 2917, loss 4.28137.
Train: 2018-08-02T14:53:34.119030: step 2918, loss 4.65914.
Train: 2018-08-02T14:53:34.181488: step 2919, loss 4.15545.
Train: 2018-08-02T14:53:34.228351: step 2920, loss 3.14807.
Test: 2018-08-02T14:53:34.431429: step 2920, loss 3.81863.
Train: 2018-08-02T14:53:34.493943: step 2921, loss 4.40729.
Train: 2018-08-02T14:53:34.556398: step 2922, loss 4.78506.
Train: 2018-08-02T14:53:34.603264: step 2923, loss 4.53321.
Train: 2018-08-02T14:53:34.665781: step 2924, loss 3.27399.
Train: 2018-08-02T14:53:34.728258: step 2925, loss 3.14807.
Train: 2018-08-02T14:53:34.790719: step 2926, loss 3.65176.
Train: 2018-08-02T14:53:34.837613: step 2927, loss 4.40729.
Train: 2018-08-02T14:53:34.900099: step 2928, loss 3.9036.
Train: 2018-08-02T14:53:34.962583: step 2929, loss 3.77768.
Train: 2018-08-02T14:53:35.009452: step 2930, loss 3.65176.
Test: 2018-08-02T14:53:35.212494: step 2930, loss 3.81863.
Train: 2018-08-02T14:53:35.274981: step 2931, loss 4.02952.
Train: 2018-08-02T14:53:35.337466: step 2932, loss 3.9036.
Train: 2018-08-02T14:53:35.384363: step 2933, loss 4.15545.
Train: 2018-08-02T14:53:35.446817: step 2934, loss 4.53321.
Train: 2018-08-02T14:53:35.509335: step 2935, loss 4.02952.
Train: 2018-08-02T14:53:35.571786: step 2936, loss 5.16283.
Train: 2018-08-02T14:53:35.618679: step 2937, loss 5.16283.
Train: 2018-08-02T14:53:35.681166: step 2938, loss 3.65176.
Train: 2018-08-02T14:53:35.743621: step 2939, loss 4.53321.
Train: 2018-08-02T14:53:35.790516: step 2940, loss 4.02952.
Test: 2018-08-02T14:53:35.993595: step 2940, loss 3.81863.
Train: 2018-08-02T14:53:36.056073: step 2941, loss 4.40729.
Train: 2018-08-02T14:53:36.102944: step 2942, loss 4.15545.
Train: 2018-08-02T14:53:36.165427: step 2943, loss 4.02952.
Train: 2018-08-02T14:53:36.227885: step 2944, loss 4.78506.
Train: 2018-08-02T14:53:36.290399: step 2945, loss 4.53321.
Train: 2018-08-02T14:53:36.337262: step 2946, loss 3.65176.
Train: 2018-08-02T14:53:36.399717: step 2947, loss 4.53321.
Train: 2018-08-02T14:53:36.462205: step 2948, loss 4.28137.
Train: 2018-08-02T14:53:36.524719: step 2949, loss 3.77768.
Train: 2018-08-02T14:53:36.571551: step 2950, loss 4.02952.
Test: 2018-08-02T14:53:36.774660: step 2950, loss 3.81863.
Train: 2018-08-02T14:53:36.837145: step 2951, loss 3.27399.
Train: 2018-08-02T14:53:36.899631: step 2952, loss 3.9036.
Train: 2018-08-02T14:53:36.962116: step 2953, loss 3.52583.
Train: 2018-08-02T14:53:37.008951: step 2954, loss 3.27399.
Train: 2018-08-02T14:53:37.071446: step 2955, loss 3.27399.
Train: 2018-08-02T14:53:37.133921: step 2956, loss 4.28137.
Train: 2018-08-02T14:53:37.180815: step 2957, loss 4.15545.
Train: 2018-08-02T14:53:37.243271: step 2958, loss 4.40729.
Train: 2018-08-02T14:53:37.305786: step 2959, loss 5.0369.
Train: 2018-08-02T14:53:37.368241: step 2960, loss 4.28137.
Test: 2018-08-02T14:53:37.555727: step 2960, loss 3.81863.
Train: 2018-08-02T14:53:37.618213: step 2961, loss 3.52583.
Train: 2018-08-02T14:53:37.680697: step 2962, loss 4.40729.
Train: 2018-08-02T14:53:37.727564: step 2963, loss 3.65176.
Train: 2018-08-02T14:53:37.790048: step 2964, loss 3.77768.
Train: 2018-08-02T14:53:37.852503: step 2965, loss 4.65914.
Train: 2018-08-02T14:53:37.915018: step 2966, loss 4.02952.
Train: 2018-08-02T14:53:37.961881: step 2967, loss 3.14807.
Train: 2018-08-02T14:53:38.024368: step 2968, loss 4.65914.
Train: 2018-08-02T14:53:38.086858: step 2969, loss 4.02952.
Train: 2018-08-02T14:53:38.149338: step 2970, loss 3.39991.
Test: 2018-08-02T14:53:38.336764: step 2970, loss 3.81863.
Train: 2018-08-02T14:53:38.399273: step 2971, loss 3.02214.
Train: 2018-08-02T14:53:38.461764: step 2972, loss 4.15545.
Train: 2018-08-02T14:53:38.508632: step 2973, loss 4.65914.
Train: 2018-08-02T14:53:38.571114: step 2974, loss 3.65176.
Train: 2018-08-02T14:53:38.633602: step 2975, loss 4.53321.
Train: 2018-08-02T14:53:38.680434: step 2976, loss 4.28137.
Train: 2018-08-02T14:53:38.742946: step 2977, loss 4.28137.
Train: 2018-08-02T14:53:38.805405: step 2978, loss 3.39991.
Train: 2018-08-02T14:53:38.867946: step 2979, loss 3.52583.
Train: 2018-08-02T14:53:38.914753: step 2980, loss 2.26661.
Test: 2018-08-02T14:53:39.117861: step 2980, loss 3.81863.
Train: 2018-08-02T14:53:39.180350: step 2981, loss 4.65914.
Train: 2018-08-02T14:53:39.242801: step 2982, loss 4.15545.
Train: 2018-08-02T14:53:39.289666: step 2983, loss 2.51845.
Train: 2018-08-02T14:53:39.352153: step 2984, loss 3.9036.
Train: 2018-08-02T14:53:39.414663: step 2985, loss 4.53321.
Train: 2018-08-02T14:53:39.477152: step 2986, loss 4.28137.
Train: 2018-08-02T14:53:39.523985: step 2987, loss 5.16283.
Train: 2018-08-02T14:53:39.586506: step 2988, loss 4.78506.
Train: 2018-08-02T14:53:39.648987: step 2989, loss 3.77768.
Train: 2018-08-02T14:53:39.695851: step 2990, loss 4.40729.
Test: 2018-08-02T14:53:39.898921: step 2990, loss 3.81863.
Train: 2018-08-02T14:53:39.961414: step 2991, loss 3.14807.
Train: 2018-08-02T14:53:40.023898: step 2992, loss 3.52583.
Train: 2018-08-02T14:53:40.086407: step 2993, loss 4.78506.
Train: 2018-08-02T14:53:40.133252: step 2994, loss 4.65914.
Train: 2018-08-02T14:53:40.195705: step 2995, loss 3.65176.
Train: 2018-08-02T14:53:40.258188: step 2996, loss 4.65914.
Train: 2018-08-02T14:53:40.320706: step 2997, loss 3.27399.
Train: 2018-08-02T14:53:40.367567: step 2998, loss 4.65914.
Train: 2018-08-02T14:53:40.430024: step 2999, loss 3.52583.
Train: 2018-08-02T14:53:40.492534: step 3000, loss 3.77768.
Test: 2018-08-02T14:53:40.679989: step 3000, loss 3.81863.
Train: 2018-08-02T14:53:41.179879: step 3001, loss 3.52583.
Train: 2018-08-02T14:53:41.242361: step 3002, loss 4.15545.
Train: 2018-08-02T14:53:41.304849: step 3003, loss 4.65914.
Train: 2018-08-02T14:53:41.367305: step 3004, loss 4.28137.
Train: 2018-08-02T14:53:41.414194: step 3005, loss 4.15545.
Train: 2018-08-02T14:53:41.476654: step 3006, loss 3.77768.
Train: 2018-08-02T14:53:41.539169: step 3007, loss 3.52583.
Train: 2018-08-02T14:53:41.586034: step 3008, loss 4.91098.
Train: 2018-08-02T14:53:41.648519: step 3009, loss 3.9036.
Train: 2018-08-02T14:53:41.710974: step 3010, loss 5.0369.
Test: 2018-08-02T14:53:41.898430: step 3010, loss 3.81863.
Train: 2018-08-02T14:53:41.960945: step 3011, loss 3.9036.
Train: 2018-08-02T14:53:42.023401: step 3012, loss 5.28875.
Train: 2018-08-02T14:53:42.085918: step 3013, loss 4.28137.
Train: 2018-08-02T14:53:42.148407: step 3014, loss 5.28875.
Train: 2018-08-02T14:53:42.195236: step 3015, loss 4.15545.
Train: 2018-08-02T14:53:42.257753: step 3016, loss 3.77768.
Train: 2018-08-02T14:53:42.320207: step 3017, loss 3.52583.
Train: 2018-08-02T14:53:42.367102: step 3018, loss 4.15545.
Train: 2018-08-02T14:53:42.429585: step 3019, loss 3.9036.
Train: 2018-08-02T14:53:42.476443: step 3020, loss 3.49225.
Test: 2018-08-02T14:53:42.663906: step 3020, loss 3.81863.
Train: 2018-08-02T14:53:42.726394: step 3021, loss 3.27399.
Train: 2018-08-02T14:53:42.788846: step 3022, loss 4.15545.
Train: 2018-08-02T14:53:42.851362: step 3023, loss 4.28137.
Train: 2018-08-02T14:53:42.898221: step 3024, loss 3.27399.
Train: 2018-08-02T14:53:42.960706: step 3025, loss 3.77768.
Train: 2018-08-02T14:53:43.023197: step 3026, loss 4.65914.
Train: 2018-08-02T14:53:43.070060: step 3027, loss 3.39991.
Train: 2018-08-02T14:53:43.132551: step 3028, loss 3.9036.
Train: 2018-08-02T14:53:43.195032: step 3029, loss 4.91098.
Train: 2018-08-02T14:53:43.241893: step 3030, loss 3.9036.
Test: 2018-08-02T14:53:43.444941: step 3030, loss 3.81863.
Train: 2018-08-02T14:53:43.507461: step 3031, loss 5.41467.
Train: 2018-08-02T14:53:43.569914: step 3032, loss 3.52583.
Train: 2018-08-02T14:53:43.616802: step 3033, loss 5.28875.
Train: 2018-08-02T14:53:43.679293: step 3034, loss 2.51845.
Train: 2018-08-02T14:53:43.741750: step 3035, loss 4.53321.
Train: 2018-08-02T14:53:43.804263: step 3036, loss 4.53321.
Train: 2018-08-02T14:53:43.851131: step 3037, loss 3.52583.
Train: 2018-08-02T14:53:43.913613: step 3038, loss 3.27399.
Train: 2018-08-02T14:53:43.976102: step 3039, loss 3.52583.
Train: 2018-08-02T14:53:44.022932: step 3040, loss 4.40729.
Test: 2018-08-02T14:53:44.226040: step 3040, loss 3.81863.
Train: 2018-08-02T14:53:44.288525: step 3041, loss 4.15545.
Train: 2018-08-02T14:53:44.350981: step 3042, loss 3.65176.
Train: 2018-08-02T14:53:44.397845: step 3043, loss 4.02952.
Train: 2018-08-02T14:53:44.460335: step 3044, loss 4.78506.
Train: 2018-08-02T14:53:44.522846: step 3045, loss 4.53321.
Train: 2018-08-02T14:53:44.569709: step 3046, loss 3.9036.
Train: 2018-08-02T14:53:44.632198: step 3047, loss 4.40729.
Train: 2018-08-02T14:53:44.694686: step 3048, loss 3.9036.
Train: 2018-08-02T14:53:44.757169: step 3049, loss 3.65176.
Train: 2018-08-02T14:53:44.804029: step 3050, loss 3.9036.
Test: 2018-08-02T14:53:45.007100: step 3050, loss 3.81863.
Train: 2018-08-02T14:53:45.069593: step 3051, loss 5.41467.
Train: 2018-08-02T14:53:45.132080: step 3052, loss 4.40729.
Train: 2018-08-02T14:53:45.178912: step 3053, loss 4.65914.
Train: 2018-08-02T14:53:45.241398: step 3054, loss 4.91098.
Train: 2018-08-02T14:53:45.303912: step 3055, loss 3.14807.
Train: 2018-08-02T14:53:45.366396: step 3056, loss 4.53321.
Train: 2018-08-02T14:53:45.413231: step 3057, loss 3.65176.
Train: 2018-08-02T14:53:45.475718: step 3058, loss 3.52583.
Train: 2018-08-02T14:53:45.538203: step 3059, loss 3.27399.
Train: 2018-08-02T14:53:45.600717: step 3060, loss 4.02952.
Test: 2018-08-02T14:53:45.788144: step 3060, loss 3.81863.
Train: 2018-08-02T14:53:45.850653: step 3061, loss 3.65176.
Train: 2018-08-02T14:53:45.913144: step 3062, loss 4.28137.
Train: 2018-08-02T14:53:45.960011: step 3063, loss 3.27399.
Train: 2018-08-02T14:53:46.022495: step 3064, loss 4.40729.
Train: 2018-08-02T14:53:46.084950: step 3065, loss 4.40729.
Train: 2018-08-02T14:53:46.147465: step 3066, loss 3.77768.
Train: 2018-08-02T14:53:46.194330: step 3067, loss 3.39991.
Train: 2018-08-02T14:53:46.256818: step 3068, loss 4.53863.
Train: 2018-08-02T14:53:46.319271: step 3069, loss 3.65176.
Train: 2018-08-02T14:53:46.381788: step 3070, loss 5.0369.
Test: 2018-08-02T14:53:46.569211: step 3070, loss 3.81863.
Train: 2018-08-02T14:53:46.631730: step 3071, loss 3.9036.
Train: 2018-08-02T14:53:46.694211: step 3072, loss 3.77768.
Train: 2018-08-02T14:53:46.741045: step 3073, loss 4.15545.
Train: 2018-08-02T14:53:46.803563: step 3074, loss 4.28137.
Train: 2018-08-02T14:53:46.866017: step 3075, loss 3.9036.
Train: 2018-08-02T14:53:46.928502: step 3076, loss 3.27399.
Train: 2018-08-02T14:53:46.975399: step 3077, loss 3.27399.
Train: 2018-08-02T14:53:47.037877: step 3078, loss 3.9036.
Train: 2018-08-02T14:53:47.100367: step 3079, loss 4.40729.
Train: 2018-08-02T14:53:47.147231: step 3080, loss 4.28137.
Test: 2018-08-02T14:53:47.350308: step 3080, loss 3.81863.
Train: 2018-08-02T14:53:47.412764: step 3081, loss 3.39991.
Train: 2018-08-02T14:53:47.475249: step 3082, loss 3.65176.
Train: 2018-08-02T14:53:47.537764: step 3083, loss 3.39991.
Train: 2018-08-02T14:53:47.584600: step 3084, loss 3.14807.
Train: 2018-08-02T14:53:47.647110: step 3085, loss 4.53321.
Train: 2018-08-02T14:53:47.709570: step 3086, loss 4.78506.
Train: 2018-08-02T14:53:47.756458: step 3087, loss 3.39991.
Train: 2018-08-02T14:53:47.818957: step 3088, loss 3.02214.
Train: 2018-08-02T14:53:47.881437: step 3089, loss 3.9036.
Train: 2018-08-02T14:53:47.928298: step 3090, loss 2.39253.
Test: 2018-08-02T14:53:48.131344: step 3090, loss 3.81863.
Train: 2018-08-02T14:53:48.193861: step 3091, loss 3.77768.
Train: 2018-08-02T14:53:48.256340: step 3092, loss 4.15545.
Train: 2018-08-02T14:53:48.318831: step 3093, loss 5.28875.
Train: 2018-08-02T14:53:48.365694: step 3094, loss 3.65176.
Train: 2018-08-02T14:53:48.428181: step 3095, loss 3.39991.
Train: 2018-08-02T14:53:48.490636: step 3096, loss 4.02952.
Train: 2018-08-02T14:53:48.553152: step 3097, loss 3.9036.
Train: 2018-08-02T14:53:48.599987: step 3098, loss 4.02952.
Train: 2018-08-02T14:53:48.662502: step 3099, loss 3.52583.
Train: 2018-08-02T14:53:48.724986: step 3100, loss 3.52583.
Test: 2018-08-02T14:53:48.912412: step 3100, loss 3.81863.
Train: 2018-08-02T14:53:49.443538: step 3101, loss 3.77768.
Train: 2018-08-02T14:53:49.490429: step 3102, loss 3.39991.
Train: 2018-08-02T14:53:49.552888: step 3103, loss 3.52583.
Train: 2018-08-02T14:53:49.615402: step 3104, loss 3.65176.
Train: 2018-08-02T14:53:49.677859: step 3105, loss 4.02952.
Train: 2018-08-02T14:53:49.724723: step 3106, loss 3.9036.
Train: 2018-08-02T14:53:49.787238: step 3107, loss 3.39991.
Train: 2018-08-02T14:53:49.849719: step 3108, loss 4.28137.
Train: 2018-08-02T14:53:49.896591: step 3109, loss 4.28137.
Train: 2018-08-02T14:53:49.959072: step 3110, loss 4.40729.
Test: 2018-08-02T14:53:50.162149: step 3110, loss 3.81863.
Train: 2018-08-02T14:53:50.224635: step 3111, loss 4.40729.
Train: 2018-08-02T14:53:50.271470: step 3112, loss 4.91098.
Train: 2018-08-02T14:53:50.333984: step 3113, loss 3.52583.
Train: 2018-08-02T14:53:50.396441: step 3114, loss 3.65176.
Train: 2018-08-02T14:53:50.458956: step 3115, loss 3.27399.
Train: 2018-08-02T14:53:50.505788: step 3116, loss 3.9036.
Train: 2018-08-02T14:53:50.568275: step 3117, loss 3.27399.
Train: 2018-08-02T14:53:50.630760: step 3118, loss 3.02214.
Train: 2018-08-02T14:53:50.677623: step 3119, loss 4.53321.
Train: 2018-08-02T14:53:50.740139: step 3120, loss 5.28875.
Test: 2018-08-02T14:53:50.943212: step 3120, loss 3.81863.
Train: 2018-08-02T14:53:51.005701: step 3121, loss 3.65176.
Train: 2018-08-02T14:53:51.052566: step 3122, loss 4.02952.
Train: 2018-08-02T14:53:51.115049: step 3123, loss 3.52583.
Train: 2018-08-02T14:53:51.177509: step 3124, loss 4.02952.
Train: 2018-08-02T14:53:51.240023: step 3125, loss 4.91098.
Train: 2018-08-02T14:53:51.302477: step 3126, loss 4.91098.
Train: 2018-08-02T14:53:51.349342: step 3127, loss 4.02952.
Train: 2018-08-02T14:53:51.411829: step 3128, loss 4.40729.
Train: 2018-08-02T14:53:51.474337: step 3129, loss 4.40729.
Train: 2018-08-02T14:53:51.521209: step 3130, loss 3.14807.
Test: 2018-08-02T14:53:51.724278: step 3130, loss 3.81863.
Train: 2018-08-02T14:53:51.786739: step 3131, loss 4.91098.
Train: 2018-08-02T14:53:51.849225: step 3132, loss 4.15545.
Train: 2018-08-02T14:53:51.911740: step 3133, loss 4.78506.
Train: 2018-08-02T14:53:51.958604: step 3134, loss 3.65176.
Train: 2018-08-02T14:53:52.021061: step 3135, loss 3.9036.
Train: 2018-08-02T14:53:52.083575: step 3136, loss 4.40729.
Train: 2018-08-02T14:53:52.130409: step 3137, loss 3.27399.
Train: 2018-08-02T14:53:52.192896: step 3138, loss 3.77768.
Train: 2018-08-02T14:53:52.255409: step 3139, loss 4.65914.
Train: 2018-08-02T14:53:52.317865: step 3140, loss 4.02952.
Test: 2018-08-02T14:53:52.505345: step 3140, loss 3.81863.
Train: 2018-08-02T14:53:52.567836: step 3141, loss 4.15545.
Train: 2018-08-02T14:53:52.630292: step 3142, loss 4.65914.
Train: 2018-08-02T14:53:52.692807: step 3143, loss 4.02952.
Train: 2018-08-02T14:53:52.755296: step 3144, loss 4.28137.
Train: 2018-08-02T14:53:52.802129: step 3145, loss 4.28137.
Train: 2018-08-02T14:53:52.864640: step 3146, loss 4.65914.
Train: 2018-08-02T14:53:52.927097: step 3147, loss 2.7703.
Train: 2018-08-02T14:53:52.989612: step 3148, loss 3.39991.
Train: 2018-08-02T14:53:53.036447: step 3149, loss 3.65176.
Train: 2018-08-02T14:53:53.098962: step 3150, loss 3.9036.
Test: 2018-08-02T14:53:53.302009: step 3150, loss 3.81863.
Train: 2018-08-02T14:53:53.364526: step 3151, loss 3.9036.
Train: 2018-08-02T14:53:53.411358: step 3152, loss 4.02952.
Train: 2018-08-02T14:53:53.473869: step 3153, loss 3.9036.
Train: 2018-08-02T14:53:53.536361: step 3154, loss 3.27399.
Train: 2018-08-02T14:53:53.598845: step 3155, loss 5.79244.
Train: 2018-08-02T14:53:53.661300: step 3156, loss 4.40729.
Train: 2018-08-02T14:53:53.708194: step 3157, loss 5.0369.
Train: 2018-08-02T14:53:53.770682: step 3158, loss 4.28137.
Train: 2018-08-02T14:53:53.833166: step 3159, loss 4.15545.
Train: 2018-08-02T14:53:53.879999: step 3160, loss 4.91098.
Test: 2018-08-02T14:53:54.083075: step 3160, loss 3.81863.
Train: 2018-08-02T14:53:54.145592: step 3161, loss 3.14807.
Train: 2018-08-02T14:53:54.208079: step 3162, loss 4.28137.
Train: 2018-08-02T14:53:54.254944: step 3163, loss 4.28137.
Train: 2018-08-02T14:53:54.317410: step 3164, loss 3.52583.
Train: 2018-08-02T14:53:54.379911: step 3165, loss 3.65176.
Train: 2018-08-02T14:53:54.426776: step 3166, loss 4.65914.
Train: 2018-08-02T14:53:54.489261: step 3167, loss 4.28137.
Train: 2018-08-02T14:53:54.551717: step 3168, loss 4.40729.
Train: 2018-08-02T14:53:54.598614: step 3169, loss 3.65176.
Train: 2018-08-02T14:53:54.661066: step 3170, loss 4.15545.
Test: 2018-08-02T14:53:54.864175: step 3170, loss 3.81863.
Train: 2018-08-02T14:53:54.911008: step 3171, loss 4.83543.
Train: 2018-08-02T14:53:54.973523: step 3172, loss 4.53321.
Train: 2018-08-02T14:53:55.020387: step 3173, loss 3.39991.
Train: 2018-08-02T14:53:55.082874: step 3174, loss 4.91098.
Train: 2018-08-02T14:53:55.145353: step 3175, loss 3.02214.
Train: 2018-08-02T14:53:55.207842: step 3176, loss 3.9036.
Train: 2018-08-02T14:53:55.254711: step 3177, loss 4.65914.
Train: 2018-08-02T14:53:55.317163: step 3178, loss 4.40729.
Train: 2018-08-02T14:53:55.379648: step 3179, loss 2.89622.
Train: 2018-08-02T14:53:55.426511: step 3180, loss 3.77768.
Test: 2018-08-02T14:53:55.629622: step 3180, loss 3.81863.
Train: 2018-08-02T14:53:55.692074: step 3181, loss 3.27399.
Train: 2018-08-02T14:53:55.754559: step 3182, loss 4.40729.
Train: 2018-08-02T14:53:55.801454: step 3183, loss 3.52583.
Train: 2018-08-02T14:53:55.863911: step 3184, loss 4.40729.
Train: 2018-08-02T14:53:55.926395: step 3185, loss 3.77768.
Train: 2018-08-02T14:53:55.973292: step 3186, loss 3.39991.
Train: 2018-08-02T14:53:56.035775: step 3187, loss 3.52583.
Train: 2018-08-02T14:53:56.098264: step 3188, loss 4.65914.
Train: 2018-08-02T14:53:56.160716: step 3189, loss 2.64438.
Train: 2018-08-02T14:53:56.207609: step 3190, loss 4.02952.
Test: 2018-08-02T14:53:56.410655: step 3190, loss 3.81863.
Train: 2018-08-02T14:53:56.473142: step 3191, loss 4.78506.
Train: 2018-08-02T14:53:56.535656: step 3192, loss 4.53321.
Train: 2018-08-02T14:53:56.582522: step 3193, loss 3.27399.
Train: 2018-08-02T14:53:56.645008: step 3194, loss 5.28875.
Train: 2018-08-02T14:53:56.707461: step 3195, loss 3.65176.
Train: 2018-08-02T14:53:56.754326: step 3196, loss 5.0369.
Train: 2018-08-02T14:53:56.816841: step 3197, loss 3.39991.
Train: 2018-08-02T14:53:56.879331: step 3198, loss 2.7703.
Train: 2018-08-02T14:53:56.941813: step 3199, loss 4.91098.
Train: 2018-08-02T14:53:56.988645: step 3200, loss 3.39991.
Test: 2018-08-02T14:53:57.191754: step 3200, loss 3.81863.
Train: 2018-08-02T14:53:57.738498: step 3201, loss 3.77768.
Train: 2018-08-02T14:53:57.800989: step 3202, loss 4.15545.
Train: 2018-08-02T14:53:57.863442: step 3203, loss 5.5406.
Train: 2018-08-02T14:53:57.925928: step 3204, loss 3.77768.
Train: 2018-08-02T14:53:57.988444: step 3205, loss 3.02214.
Train: 2018-08-02T14:53:58.035277: step 3206, loss 4.53321.
Train: 2018-08-02T14:53:58.097791: step 3207, loss 4.65914.
Train: 2018-08-02T14:53:58.160248: step 3208, loss 4.40729.
Train: 2018-08-02T14:53:58.207112: step 3209, loss 3.77768.
Train: 2018-08-02T14:53:58.269626: step 3210, loss 4.40729.
Test: 2018-08-02T14:53:58.472703: step 3210, loss 3.81863.
Train: 2018-08-02T14:53:58.535184: step 3211, loss 4.78506.
Train: 2018-08-02T14:53:58.597673: step 3212, loss 3.52583.
Train: 2018-08-02T14:53:58.644540: step 3213, loss 2.39253.
Train: 2018-08-02T14:53:58.707025: step 3214, loss 3.39991.
Train: 2018-08-02T14:53:58.769509: step 3215, loss 4.53321.
Train: 2018-08-02T14:53:58.831995: step 3216, loss 3.9036.
Train: 2018-08-02T14:53:58.878828: step 3217, loss 3.27399.
Train: 2018-08-02T14:53:58.941340: step 3218, loss 4.91098.
Train: 2018-08-02T14:53:59.003801: step 3219, loss 3.39991.
Train: 2018-08-02T14:53:59.050694: step 3220, loss 4.28137.
Test: 2018-08-02T14:53:59.253739: step 3220, loss 3.81863.
Train: 2018-08-02T14:53:59.316225: step 3221, loss 4.40729.
Train: 2018-08-02T14:53:59.378711: step 3222, loss 3.77768.
Train: 2018-08-02T14:53:59.425575: step 3223, loss 4.91098.
Train: 2018-08-02T14:53:59.488063: step 3224, loss 3.65176.
Train: 2018-08-02T14:53:59.566169: step 3225, loss 4.15545.
Train: 2018-08-02T14:53:59.613032: step 3226, loss 3.52583.
Train: 2018-08-02T14:53:59.691139: step 3227, loss 3.14807.
Train: 2018-08-02T14:53:59.753657: step 3228, loss 3.77768.
Train: 2018-08-02T14:53:59.800518: step 3229, loss 3.77768.
Train: 2018-08-02T14:53:59.863007: step 3230, loss 3.77768.
Test: 2018-08-02T14:54:00.066051: step 3230, loss 3.81863.
Train: 2018-08-02T14:54:00.128569: step 3231, loss 3.77768.
Train: 2018-08-02T14:54:00.191052: step 3232, loss 5.16283.
Train: 2018-08-02T14:54:00.237913: step 3233, loss 4.15545.
Train: 2018-08-02T14:54:00.300398: step 3234, loss 3.39991.
Train: 2018-08-02T14:54:00.362886: step 3235, loss 4.15545.
Train: 2018-08-02T14:54:00.425371: step 3236, loss 3.52583.
Train: 2018-08-02T14:54:00.472239: step 3237, loss 5.41467.
Train: 2018-08-02T14:54:00.534692: step 3238, loss 3.39991.
Train: 2018-08-02T14:54:00.597207: step 3239, loss 3.9036.
Train: 2018-08-02T14:54:00.644041: step 3240, loss 3.65176.
Test: 2018-08-02T14:54:00.847116: step 3240, loss 3.81863.
Train: 2018-08-02T14:54:00.909633: step 3241, loss 4.91098.
Train: 2018-08-02T14:54:00.972117: step 3242, loss 2.51845.
Train: 2018-08-02T14:54:01.018982: step 3243, loss 3.9036.
Train: 2018-08-02T14:54:01.081469: step 3244, loss 3.9036.
Train: 2018-08-02T14:54:01.143953: step 3245, loss 4.02952.
Train: 2018-08-02T14:54:01.190812: step 3246, loss 4.53321.
Train: 2018-08-02T14:54:01.253306: step 3247, loss 4.40729.
Train: 2018-08-02T14:54:01.315789: step 3248, loss 4.02952.
Train: 2018-08-02T14:54:01.378244: step 3249, loss 4.40729.
Train: 2018-08-02T14:54:01.425133: step 3250, loss 3.65176.
Test: 2018-08-02T14:54:01.628218: step 3250, loss 3.81863.
Train: 2018-08-02T14:54:01.690670: step 3251, loss 3.77768.
Train: 2018-08-02T14:54:01.737562: step 3252, loss 3.77768.
Train: 2018-08-02T14:54:01.800050: step 3253, loss 4.40729.
Train: 2018-08-02T14:54:01.862505: step 3254, loss 3.65176.
Train: 2018-08-02T14:54:01.924991: step 3255, loss 3.9036.
Train: 2018-08-02T14:54:01.971854: step 3256, loss 2.89622.
Train: 2018-08-02T14:54:02.034365: step 3257, loss 4.53321.
Train: 2018-08-02T14:54:02.096858: step 3258, loss 4.65914.
Train: 2018-08-02T14:54:02.143719: step 3259, loss 3.9036.
Train: 2018-08-02T14:54:02.206173: step 3260, loss 3.9036.
Test: 2018-08-02T14:54:02.409281: step 3260, loss 3.81863.
Train: 2018-08-02T14:54:02.471737: step 3261, loss 4.78506.
Train: 2018-08-02T14:54:02.534253: step 3262, loss 3.9036.
Train: 2018-08-02T14:54:02.581117: step 3263, loss 4.53321.
Train: 2018-08-02T14:54:02.643603: step 3264, loss 3.39991.
Train: 2018-08-02T14:54:02.706090: step 3265, loss 3.52583.
Train: 2018-08-02T14:54:02.768567: step 3266, loss 3.39991.
Train: 2018-08-02T14:54:02.815437: step 3267, loss 3.02214.
Train: 2018-08-02T14:54:02.877926: step 3268, loss 4.53321.
Train: 2018-08-02T14:54:02.940379: step 3269, loss 4.28137.
Train: 2018-08-02T14:54:02.987242: step 3270, loss 3.27399.
Test: 2018-08-02T14:54:03.190319: step 3270, loss 3.81863.
Train: 2018-08-02T14:54:03.252837: step 3271, loss 3.27399.
Train: 2018-08-02T14:54:03.299698: step 3272, loss 4.15545.
Train: 2018-08-02T14:54:03.362181: step 3273, loss 4.15545.
Train: 2018-08-02T14:54:03.424640: step 3274, loss 3.52583.
Train: 2018-08-02T14:54:03.487126: step 3275, loss 2.89622.
Train: 2018-08-02T14:54:03.549609: step 3276, loss 4.15545.
Train: 2018-08-02T14:54:03.596474: step 3277, loss 4.40729.
Train: 2018-08-02T14:54:03.658960: step 3278, loss 4.53321.
Train: 2018-08-02T14:54:03.721475: step 3279, loss 3.39991.
Train: 2018-08-02T14:54:03.768339: step 3280, loss 4.02952.
Test: 2018-08-02T14:54:03.971385: step 3280, loss 3.81863.
Train: 2018-08-02T14:54:04.033896: step 3281, loss 4.02952.
Train: 2018-08-02T14:54:04.096357: step 3282, loss 3.77768.
Train: 2018-08-02T14:54:04.143251: step 3283, loss 4.28137.
Train: 2018-08-02T14:54:04.205736: step 3284, loss 5.16283.
Train: 2018-08-02T14:54:04.268223: step 3285, loss 4.53321.
Train: 2018-08-02T14:54:04.315086: step 3286, loss 3.9036.
Train: 2018-08-02T14:54:04.377574: step 3287, loss 3.52583.
Train: 2018-08-02T14:54:04.440057: step 3288, loss 4.28137.
Train: 2018-08-02T14:54:04.486891: step 3289, loss 4.28137.
Train: 2018-08-02T14:54:04.549376: step 3290, loss 4.65914.
Test: 2018-08-02T14:54:04.752453: step 3290, loss 3.81863.
Train: 2018-08-02T14:54:04.814963: step 3291, loss 4.28137.
Train: 2018-08-02T14:54:04.877424: step 3292, loss 3.27399.
Train: 2018-08-02T14:54:04.924287: step 3293, loss 4.78506.
Train: 2018-08-02T14:54:04.986801: step 3294, loss 4.65914.
Train: 2018-08-02T14:54:05.049289: step 3295, loss 4.15545.
Train: 2018-08-02T14:54:05.111774: step 3296, loss 3.27399.
Train: 2018-08-02T14:54:05.158638: step 3297, loss 3.02214.
Train: 2018-08-02T14:54:05.221094: step 3298, loss 3.77768.
Train: 2018-08-02T14:54:05.283579: step 3299, loss 5.16283.
Train: 2018-08-02T14:54:05.330468: step 3300, loss 3.9036.
Test: 2018-08-02T14:54:05.533545: step 3300, loss 3.81863.
Train: 2018-08-02T14:54:06.049055: step 3301, loss 4.40729.
Train: 2018-08-02T14:54:06.111537: step 3302, loss 4.02952.
Train: 2018-08-02T14:54:06.173995: step 3303, loss 3.77768.
Train: 2018-08-02T14:54:06.236509: step 3304, loss 3.14807.
Train: 2018-08-02T14:54:06.298968: step 3305, loss 4.65914.
Train: 2018-08-02T14:54:06.345830: step 3306, loss 4.65914.
Train: 2018-08-02T14:54:06.408347: step 3307, loss 6.04429.
Train: 2018-08-02T14:54:06.470835: step 3308, loss 5.0369.
Train: 2018-08-02T14:54:06.533287: step 3309, loss 3.77768.
Train: 2018-08-02T14:54:06.580181: step 3310, loss 4.15545.
Test: 2018-08-02T14:54:06.783257: step 3310, loss 3.81863.
Train: 2018-08-02T14:54:06.845746: step 3311, loss 4.40729.
Train: 2018-08-02T14:54:06.908228: step 3312, loss 3.27399.
Train: 2018-08-02T14:54:06.955093: step 3313, loss 4.28137.
Train: 2018-08-02T14:54:07.017579: step 3314, loss 3.52583.
Train: 2018-08-02T14:54:07.080063: step 3315, loss 4.28137.
Train: 2018-08-02T14:54:07.142548: step 3316, loss 4.65914.
Train: 2018-08-02T14:54:07.189407: step 3317, loss 4.15545.
Train: 2018-08-02T14:54:07.251893: step 3318, loss 3.52583.
Train: 2018-08-02T14:54:07.314383: step 3319, loss 4.40729.
Train: 2018-08-02T14:54:07.361245: step 3320, loss 3.77768.
Test: 2018-08-02T14:54:07.564293: step 3320, loss 3.81863.
Train: 2018-08-02T14:54:07.626813: step 3321, loss 4.02952.
Train: 2018-08-02T14:54:07.673673: step 3322, loss 4.56679.
Train: 2018-08-02T14:54:07.736161: step 3323, loss 3.02214.
Train: 2018-08-02T14:54:07.798646: step 3324, loss 3.9036.
Train: 2018-08-02T14:54:07.845512: step 3325, loss 2.7703.
Train: 2018-08-02T14:54:07.907995: step 3326, loss 3.52583.
Train: 2018-08-02T14:54:07.970480: step 3327, loss 4.53321.
Train: 2018-08-02T14:54:08.032940: step 3328, loss 4.15545.
Train: 2018-08-02T14:54:08.079829: step 3329, loss 4.53321.
Train: 2018-08-02T14:54:08.142285: step 3330, loss 3.02214.
Test: 2018-08-02T14:54:08.329740: step 3330, loss 3.81863.
Train: 2018-08-02T14:54:08.392226: step 3331, loss 3.52583.
Train: 2018-08-02T14:54:08.454737: step 3332, loss 4.15545.
Train: 2018-08-02T14:54:08.517195: step 3333, loss 4.28137.
Train: 2018-08-02T14:54:08.564092: step 3334, loss 5.79244.
Train: 2018-08-02T14:54:08.626580: step 3335, loss 3.77768.
Train: 2018-08-02T14:54:08.689060: step 3336, loss 4.28137.
Train: 2018-08-02T14:54:08.735923: step 3337, loss 4.40729.
Train: 2018-08-02T14:54:08.798406: step 3338, loss 4.28137.
Train: 2018-08-02T14:54:08.860897: step 3339, loss 4.65914.
Train: 2018-08-02T14:54:08.923376: step 3340, loss 4.28137.
Test: 2018-08-02T14:54:09.110831: step 3340, loss 3.81863.
Train: 2018-08-02T14:54:09.173293: step 3341, loss 3.02214.
Train: 2018-08-02T14:54:09.235806: step 3342, loss 4.53321.
Train: 2018-08-02T14:54:09.298263: step 3343, loss 4.28137.
Train: 2018-08-02T14:54:09.345161: step 3344, loss 5.16283.
Train: 2018-08-02T14:54:09.407617: step 3345, loss 3.9036.
Train: 2018-08-02T14:54:09.470099: step 3346, loss 3.9036.
Train: 2018-08-02T14:54:09.532613: step 3347, loss 3.27399.
Train: 2018-08-02T14:54:09.579449: step 3348, loss 4.15545.
Train: 2018-08-02T14:54:09.641964: step 3349, loss 4.53321.
Train: 2018-08-02T14:54:09.688798: step 3350, loss 3.9036.
Test: 2018-08-02T14:54:09.891905: step 3350, loss 3.81863.
Train: 2018-08-02T14:54:09.954390: step 3351, loss 4.91098.
Train: 2018-08-02T14:54:10.016846: step 3352, loss 3.65176.
Train: 2018-08-02T14:54:10.079331: step 3353, loss 3.27399.
Train: 2018-08-02T14:54:10.141847: step 3354, loss 4.78506.
Train: 2018-08-02T14:54:10.204333: step 3355, loss 3.02214.
Train: 2018-08-02T14:54:10.251189: step 3356, loss 4.65914.
Train: 2018-08-02T14:54:10.313681: step 3357, loss 4.28137.
Train: 2018-08-02T14:54:10.376137: step 3358, loss 4.78506.
Train: 2018-08-02T14:54:10.423000: step 3359, loss 4.78506.
Train: 2018-08-02T14:54:10.485485: step 3360, loss 3.39991.
Test: 2018-08-02T14:54:10.688587: step 3360, loss 3.81863.
Train: 2018-08-02T14:54:10.751049: step 3361, loss 3.52583.
Train: 2018-08-02T14:54:10.797942: step 3362, loss 3.52583.
Train: 2018-08-02T14:54:10.860398: step 3363, loss 3.77768.
Train: 2018-08-02T14:54:10.922884: step 3364, loss 3.52583.
Train: 2018-08-02T14:54:10.969780: step 3365, loss 4.53321.
Train: 2018-08-02T14:54:11.032264: step 3366, loss 3.77768.
Train: 2018-08-02T14:54:11.094743: step 3367, loss 4.40729.
Train: 2018-08-02T14:54:11.157206: step 3368, loss 3.65176.
Train: 2018-08-02T14:54:11.204067: step 3369, loss 3.39991.
Train: 2018-08-02T14:54:11.266583: step 3370, loss 3.52583.
Test: 2018-08-02T14:54:11.469629: step 3370, loss 3.81863.
Train: 2018-08-02T14:54:11.532141: step 3371, loss 4.40729.
Train: 2018-08-02T14:54:11.594600: step 3372, loss 4.78506.
Train: 2018-08-02T14:54:11.641495: step 3373, loss 3.77768.
Train: 2018-08-02T14:54:11.703954: step 3374, loss 4.15545.
Train: 2018-08-02T14:54:11.766438: step 3375, loss 4.28137.
Train: 2018-08-02T14:54:11.828924: step 3376, loss 3.52583.
Train: 2018-08-02T14:54:11.875787: step 3377, loss 4.28137.
Train: 2018-08-02T14:54:11.938271: step 3378, loss 5.41467.
Train: 2018-08-02T14:54:12.000781: step 3379, loss 4.53321.
Train: 2018-08-02T14:54:12.047621: step 3380, loss 2.89622.
Test: 2018-08-02T14:54:12.250730: step 3380, loss 3.81863.
Train: 2018-08-02T14:54:12.313213: step 3381, loss 3.9036.
Train: 2018-08-02T14:54:12.375667: step 3382, loss 3.65176.
Train: 2018-08-02T14:54:12.438153: step 3383, loss 3.52583.
Train: 2018-08-02T14:54:12.485045: step 3384, loss 4.65914.
Train: 2018-08-02T14:54:12.547504: step 3385, loss 3.27399.
Train: 2018-08-02T14:54:12.610018: step 3386, loss 2.39253.
Train: 2018-08-02T14:54:12.656882: step 3387, loss 3.52583.
Train: 2018-08-02T14:54:12.719338: step 3388, loss 4.65914.
Train: 2018-08-02T14:54:12.781827: step 3389, loss 3.65176.
Train: 2018-08-02T14:54:12.844307: step 3390, loss 4.02952.
Test: 2018-08-02T14:54:13.031763: step 3390, loss 3.81863.
Train: 2018-08-02T14:54:13.094283: step 3391, loss 4.15545.
Train: 2018-08-02T14:54:13.156735: step 3392, loss 3.77768.
Train: 2018-08-02T14:54:13.219253: step 3393, loss 2.7703.
Train: 2018-08-02T14:54:13.266116: step 3394, loss 3.27399.
Train: 2018-08-02T14:54:13.328601: step 3395, loss 3.77768.
Train: 2018-08-02T14:54:13.391055: step 3396, loss 4.40729.
Train: 2018-08-02T14:54:13.437949: step 3397, loss 4.28137.
Train: 2018-08-02T14:54:13.500439: step 3398, loss 3.77768.
Train: 2018-08-02T14:54:13.562922: step 3399, loss 4.53321.
Train: 2018-08-02T14:54:13.625376: step 3400, loss 4.02952.
Test: 2018-08-02T14:54:13.812855: step 3400, loss 3.81863.
Train: 2018-08-02T14:54:14.359579: step 3401, loss 3.39991.
Train: 2018-08-02T14:54:14.422089: step 3402, loss 2.89622.
Train: 2018-08-02T14:54:14.484576: step 3403, loss 3.02214.
Train: 2018-08-02T14:54:14.547065: step 3404, loss 4.65914.
Train: 2018-08-02T14:54:14.593932: step 3405, loss 4.02952.
Train: 2018-08-02T14:54:14.656408: step 3406, loss 3.52583.
Train: 2018-08-02T14:54:14.718899: step 3407, loss 4.40729.
Train: 2018-08-02T14:54:14.765764: step 3408, loss 4.40729.
Train: 2018-08-02T14:54:14.828219: step 3409, loss 4.15545.
Train: 2018-08-02T14:54:14.890736: step 3410, loss 3.14807.
Test: 2018-08-02T14:54:15.078159: step 3410, loss 3.81863.
Train: 2018-08-02T14:54:15.140645: step 3411, loss 4.53321.
Train: 2018-08-02T14:54:15.203160: step 3412, loss 3.52583.
Train: 2018-08-02T14:54:15.265615: step 3413, loss 4.02952.
Train: 2018-08-02T14:54:15.312480: step 3414, loss 3.14807.
Train: 2018-08-02T14:54:15.374999: step 3415, loss 2.64438.
Train: 2018-08-02T14:54:15.437451: step 3416, loss 4.78506.
Train: 2018-08-02T14:54:15.484342: step 3417, loss 4.65914.
Train: 2018-08-02T14:54:15.546832: step 3418, loss 3.65176.
Train: 2018-08-02T14:54:15.609314: step 3419, loss 3.9036.
Train: 2018-08-02T14:54:15.671801: step 3420, loss 3.9036.
Test: 2018-08-02T14:54:15.859251: step 3420, loss 3.81863.
Train: 2018-08-02T14:54:15.921742: step 3421, loss 4.65914.
Train: 2018-08-02T14:54:15.984228: step 3422, loss 4.65914.
Train: 2018-08-02T14:54:16.031092: step 3423, loss 4.78506.
Train: 2018-08-02T14:54:16.093580: step 3424, loss 5.16283.
Train: 2018-08-02T14:54:16.156033: step 3425, loss 4.40729.
Train: 2018-08-02T14:54:16.202927: step 3426, loss 3.02214.
Train: 2018-08-02T14:54:16.265412: step 3427, loss 3.52583.
Train: 2018-08-02T14:54:16.327898: step 3428, loss 4.28137.
Train: 2018-08-02T14:54:16.374765: step 3429, loss 4.40729.
Train: 2018-08-02T14:54:16.437247: step 3430, loss 4.40729.
Test: 2018-08-02T14:54:16.640318: step 3430, loss 3.81863.
Train: 2018-08-02T14:54:16.702780: step 3431, loss 3.9036.
Train: 2018-08-02T14:54:16.749672: step 3432, loss 3.77768.
Train: 2018-08-02T14:54:16.812129: step 3433, loss 3.52583.
Train: 2018-08-02T14:54:16.874649: step 3434, loss 4.40729.
Train: 2018-08-02T14:54:16.937130: step 3435, loss 4.40729.
Train: 2018-08-02T14:54:16.999589: step 3436, loss 3.77768.
Train: 2018-08-02T14:54:17.046479: step 3437, loss 4.40729.
Train: 2018-08-02T14:54:17.108962: step 3438, loss 3.39991.
Train: 2018-08-02T14:54:17.171420: step 3439, loss 3.65176.
Train: 2018-08-02T14:54:17.218314: step 3440, loss 4.78506.
Test: 2018-08-02T14:54:17.421361: step 3440, loss 3.81863.
Train: 2018-08-02T14:54:17.483847: step 3441, loss 4.53321.
Train: 2018-08-02T14:54:17.546331: step 3442, loss 4.40729.
Train: 2018-08-02T14:54:17.593229: step 3443, loss 4.40729.
Train: 2018-08-02T14:54:17.655716: step 3444, loss 3.9036.
Train: 2018-08-02T14:54:17.718198: step 3445, loss 4.40729.
Train: 2018-08-02T14:54:17.765031: step 3446, loss 4.78506.
Train: 2018-08-02T14:54:17.827546: step 3447, loss 3.65176.
Train: 2018-08-02T14:54:17.890003: step 3448, loss 4.91098.
Train: 2018-08-02T14:54:17.936896: step 3449, loss 3.65176.
Train: 2018-08-02T14:54:17.999351: step 3450, loss 3.27399.
Test: 2018-08-02T14:54:18.186838: step 3450, loss 3.81863.
Train: 2018-08-02T14:54:18.249326: step 3451, loss 3.9036.
Train: 2018-08-02T14:54:18.311803: step 3452, loss 3.65176.
Train: 2018-08-02T14:54:18.374296: step 3453, loss 4.65914.
Train: 2018-08-02T14:54:18.436775: step 3454, loss 3.52583.
Train: 2018-08-02T14:54:18.499233: step 3455, loss 4.91098.
Train: 2018-08-02T14:54:18.561749: step 3456, loss 4.40729.
Train: 2018-08-02T14:54:18.608613: step 3457, loss 2.89622.
Train: 2018-08-02T14:54:18.671100: step 3458, loss 5.03691.
Train: 2018-08-02T14:54:18.733582: step 3459, loss 4.40729.
Train: 2018-08-02T14:54:18.780443: step 3460, loss 4.53321.
Test: 2018-08-02T14:54:18.983525: step 3460, loss 3.81863.
Train: 2018-08-02T14:54:19.046011: step 3461, loss 3.52583.
Train: 2018-08-02T14:54:19.108496: step 3462, loss 5.5406.
Train: 2018-08-02T14:54:19.155331: step 3463, loss 3.9036.
Train: 2018-08-02T14:54:19.217817: step 3464, loss 4.02952.
Train: 2018-08-02T14:54:19.280332: step 3465, loss 4.02952.
Train: 2018-08-02T14:54:19.342816: step 3466, loss 3.14807.
Train: 2018-08-02T14:54:19.389680: step 3467, loss 4.15545.
Train: 2018-08-02T14:54:19.452166: step 3468, loss 3.27399.
Train: 2018-08-02T14:54:19.514627: step 3469, loss 5.16283.
Train: 2018-08-02T14:54:19.577137: step 3470, loss 3.27399.
Test: 2018-08-02T14:54:19.764591: step 3470, loss 3.81863.
Train: 2018-08-02T14:54:19.827049: step 3471, loss 3.9036.
Train: 2018-08-02T14:54:19.889563: step 3472, loss 3.39991.
Train: 2018-08-02T14:54:19.936397: step 3473, loss 4.02952.
Train: 2018-08-02T14:54:19.998911: step 3474, loss 5.16283.
Train: 2018-08-02T14:54:20.045772: step 3475, loss 4.28137.
Train: 2018-08-02T14:54:20.108231: step 3476, loss 3.14807.
Train: 2018-08-02T14:54:20.155096: step 3477, loss 3.77768.
Train: 2018-08-02T14:54:20.217606: step 3478, loss 4.15545.
Train: 2018-08-02T14:54:20.280097: step 3479, loss 4.15545.
Train: 2018-08-02T14:54:20.342552: step 3480, loss 3.77768.
Test: 2018-08-02T14:54:20.530007: step 3480, loss 3.81863.
Train: 2018-08-02T14:54:20.592529: step 3481, loss 3.65176.
Train: 2018-08-02T14:54:20.655004: step 3482, loss 3.27399.
Train: 2018-08-02T14:54:20.717497: step 3483, loss 4.15545.
Train: 2018-08-02T14:54:20.764362: step 3484, loss 2.51845.
Train: 2018-08-02T14:54:20.826845: step 3485, loss 4.53321.
Train: 2018-08-02T14:54:20.889329: step 3486, loss 4.15545.
Train: 2018-08-02T14:54:20.936164: step 3487, loss 4.15545.
Train: 2018-08-02T14:54:20.998683: step 3488, loss 3.27399.
Train: 2018-08-02T14:54:21.061159: step 3489, loss 5.28875.
Train: 2018-08-02T14:54:21.123652: step 3490, loss 4.65914.
Test: 2018-08-02T14:54:21.311075: step 3490, loss 3.81863.
Train: 2018-08-02T14:54:21.373594: step 3491, loss 4.15545.
Train: 2018-08-02T14:54:21.436046: step 3492, loss 3.02214.
Train: 2018-08-02T14:54:21.498532: step 3493, loss 4.40729.
Train: 2018-08-02T14:54:21.561024: step 3494, loss 3.9036.
Train: 2018-08-02T14:54:21.607911: step 3495, loss 3.77768.
Train: 2018-08-02T14:54:21.670396: step 3496, loss 3.65176.
Train: 2018-08-02T14:54:21.717231: step 3497, loss 3.27399.
Train: 2018-08-02T14:54:21.779716: step 3498, loss 4.53321.
Train: 2018-08-02T14:54:21.842231: step 3499, loss 3.52583.
Train: 2018-08-02T14:54:21.889065: step 3500, loss 4.15545.
Test: 2018-08-02T14:54:22.092141: step 3500, loss 3.81863.
Train: 2018-08-02T14:54:22.607675: step 3501, loss 3.27399.
Train: 2018-08-02T14:54:22.670131: step 3502, loss 4.02952.
Train: 2018-08-02T14:54:22.732650: step 3503, loss 3.52583.
Train: 2018-08-02T14:54:22.779482: step 3504, loss 3.52583.
Train: 2018-08-02T14:54:22.842001: step 3505, loss 3.27399.
Train: 2018-08-02T14:54:22.904478: step 3506, loss 4.53321.
Train: 2018-08-02T14:54:22.951315: step 3507, loss 4.40729.
Train: 2018-08-02T14:54:23.013837: step 3508, loss 4.53321.
Train: 2018-08-02T14:54:23.076289: step 3509, loss 5.16283.
Train: 2018-08-02T14:54:23.138805: step 3510, loss 4.53321.
Test: 2018-08-02T14:54:23.326227: step 3510, loss 3.81863.
Train: 2018-08-02T14:54:23.388713: step 3511, loss 4.15545.
Train: 2018-08-02T14:54:23.451198: step 3512, loss 3.39991.
Train: 2018-08-02T14:54:23.513715: step 3513, loss 3.39991.
Train: 2018-08-02T14:54:23.560580: step 3514, loss 4.15545.
Train: 2018-08-02T14:54:23.623063: step 3515, loss 5.41467.
Train: 2018-08-02T14:54:23.685544: step 3516, loss 2.89622.
Train: 2018-08-02T14:54:23.732382: step 3517, loss 2.51845.
Train: 2018-08-02T14:54:23.794898: step 3518, loss 3.9036.
Train: 2018-08-02T14:54:23.857385: step 3519, loss 4.15545.
Train: 2018-08-02T14:54:23.904218: step 3520, loss 3.65176.
Test: 2018-08-02T14:54:24.107319: step 3520, loss 3.81863.
Train: 2018-08-02T14:54:24.169781: step 3521, loss 4.65914.
Train: 2018-08-02T14:54:24.232294: step 3522, loss 3.39991.
Train: 2018-08-02T14:54:24.294751: step 3523, loss 4.53321.
Train: 2018-08-02T14:54:24.341649: step 3524, loss 4.40729.
Train: 2018-08-02T14:54:24.404102: step 3525, loss 3.39991.
Train: 2018-08-02T14:54:24.466612: step 3526, loss 4.02952.
Train: 2018-08-02T14:54:24.513481: step 3527, loss 4.02952.
Train: 2018-08-02T14:54:24.575961: step 3528, loss 4.91098.
Train: 2018-08-02T14:54:24.638423: step 3529, loss 3.02214.
Train: 2018-08-02T14:54:24.700931: step 3530, loss 3.52583.
Test: 2018-08-02T14:54:24.888363: step 3530, loss 3.81863.
Train: 2018-08-02T14:54:24.950878: step 3531, loss 4.28137.
Train: 2018-08-02T14:54:25.013334: step 3532, loss 3.77768.
Train: 2018-08-02T14:54:25.075845: step 3533, loss 4.28137.
Train: 2018-08-02T14:54:25.138305: step 3534, loss 3.90902.
Train: 2018-08-02T14:54:25.185199: step 3535, loss 4.53321.
Train: 2018-08-02T14:54:25.247686: step 3536, loss 4.02952.
Train: 2018-08-02T14:54:25.310168: step 3537, loss 5.16283.
Train: 2018-08-02T14:54:25.357003: step 3538, loss 4.65914.
Train: 2018-08-02T14:54:25.419519: step 3539, loss 2.51845.
Train: 2018-08-02T14:54:25.482003: step 3540, loss 4.02952.
Test: 2018-08-02T14:54:25.669453: step 3540, loss 3.81863.
Train: 2018-08-02T14:54:25.731942: step 3541, loss 4.53321.
Train: 2018-08-02T14:54:25.794431: step 3542, loss 3.65176.
Train: 2018-08-02T14:54:25.856915: step 3543, loss 2.89622.
Train: 2018-08-02T14:54:25.919373: step 3544, loss 3.77768.
Train: 2018-08-02T14:54:25.966260: step 3545, loss 4.15545.
Train: 2018-08-02T14:54:26.028748: step 3546, loss 3.77768.
Train: 2018-08-02T14:54:26.091237: step 3547, loss 3.27399.
Train: 2018-08-02T14:54:26.153693: step 3548, loss 3.77768.
Train: 2018-08-02T14:54:26.200585: step 3549, loss 4.28137.
Train: 2018-08-02T14:54:26.263040: step 3550, loss 2.89622.
Test: 2018-08-02T14:54:26.466144: step 3550, loss 3.81863.
Train: 2018-08-02T14:54:26.528603: step 3551, loss 3.02214.
Train: 2018-08-02T14:54:26.575498: step 3552, loss 5.16283.
Train: 2018-08-02T14:54:26.637953: step 3553, loss 4.28137.
Train: 2018-08-02T14:54:26.700439: step 3554, loss 4.28137.
Train: 2018-08-02T14:54:26.747303: step 3555, loss 5.28875.
Train: 2018-08-02T14:54:26.809813: step 3556, loss 3.9036.
Train: 2018-08-02T14:54:26.872306: step 3557, loss 4.65914.
Train: 2018-08-02T14:54:26.919167: step 3558, loss 4.65914.
Train: 2018-08-02T14:54:26.981655: step 3559, loss 3.65176.
Train: 2018-08-02T14:54:27.044108: step 3560, loss 4.78506.
Test: 2018-08-02T14:54:27.247185: step 3560, loss 3.81863.
Train: 2018-08-02T14:54:27.294079: step 3561, loss 4.91098.
Train: 2018-08-02T14:54:27.356563: step 3562, loss 3.52583.
Train: 2018-08-02T14:54:27.419050: step 3563, loss 3.65176.
Train: 2018-08-02T14:54:27.481534: step 3564, loss 4.15545.
Train: 2018-08-02T14:54:27.528394: step 3565, loss 3.02214.
Train: 2018-08-02T14:54:27.590880: step 3566, loss 3.9036.
Train: 2018-08-02T14:54:27.653370: step 3567, loss 5.28875.
Train: 2018-08-02T14:54:27.700237: step 3568, loss 4.40729.
Train: 2018-08-02T14:54:27.762720: step 3569, loss 4.15545.
Train: 2018-08-02T14:54:27.825175: step 3570, loss 4.65914.
Test: 2018-08-02T14:54:28.028253: step 3570, loss 3.81863.
Train: 2018-08-02T14:54:28.075147: step 3571, loss 4.15545.
Train: 2018-08-02T14:54:28.137632: step 3572, loss 4.53321.
Train: 2018-08-02T14:54:28.200117: step 3573, loss 3.14807.
Train: 2018-08-02T14:54:28.262573: step 3574, loss 4.02952.
Train: 2018-08-02T14:54:28.309436: step 3575, loss 5.5406.
Train: 2018-08-02T14:54:28.371921: step 3576, loss 4.65914.
Train: 2018-08-02T14:54:28.434407: step 3577, loss 3.9036.
Train: 2018-08-02T14:54:28.481299: step 3578, loss 4.65914.
Train: 2018-08-02T14:54:28.543756: step 3579, loss 3.65176.
Train: 2018-08-02T14:54:28.606270: step 3580, loss 2.89622.
Test: 2018-08-02T14:54:28.809319: step 3580, loss 3.81863.
Train: 2018-08-02T14:54:28.871838: step 3581, loss 3.9036.
Train: 2018-08-02T14:54:28.918699: step 3582, loss 4.65914.
Train: 2018-08-02T14:54:28.981182: step 3583, loss 3.9036.
Train: 2018-08-02T14:54:29.043671: step 3584, loss 2.39253.
Train: 2018-08-02T14:54:29.106150: step 3585, loss 4.02952.
Train: 2018-08-02T14:54:29.153013: step 3586, loss 3.9036.
Train: 2018-08-02T14:54:29.215507: step 3587, loss 3.65176.
Train: 2018-08-02T14:54:29.262364: step 3588, loss 4.02952.
Train: 2018-08-02T14:54:29.324823: step 3589, loss 4.53321.
Train: 2018-08-02T14:54:29.387339: step 3590, loss 3.14807.
Test: 2018-08-02T14:54:29.590386: step 3590, loss 3.81863.
Train: 2018-08-02T14:54:29.637281: step 3591, loss 3.65176.
Train: 2018-08-02T14:54:29.699766: step 3592, loss 4.28137.
Train: 2018-08-02T14:54:29.762222: step 3593, loss 3.9036.
Train: 2018-08-02T14:54:29.824739: step 3594, loss 4.78506.
Train: 2018-08-02T14:54:29.887193: step 3595, loss 5.03691.
Train: 2018-08-02T14:54:29.934057: step 3596, loss 4.28137.
Train: 2018-08-02T14:54:29.996570: step 3597, loss 4.02952.
Train: 2018-08-02T14:54:30.059057: step 3598, loss 4.02952.
Train: 2018-08-02T14:54:30.105892: step 3599, loss 4.65914.
Train: 2018-08-02T14:54:30.168406: step 3600, loss 3.14807.
Test: 2018-08-02T14:54:30.371452: step 3600, loss 3.81863.
Train: 2018-08-02T14:54:30.855743: step 3601, loss 4.40729.
Train: 2018-08-02T14:54:30.918201: step 3602, loss 2.51845.
Train: 2018-08-02T14:54:30.980714: step 3603, loss 3.14807.
Train: 2018-08-02T14:54:31.043204: step 3604, loss 4.40729.
Train: 2018-08-02T14:54:31.105684: step 3605, loss 3.02214.
Train: 2018-08-02T14:54:31.152527: step 3606, loss 5.16283.
Train: 2018-08-02T14:54:31.215040: step 3607, loss 4.40729.
Train: 2018-08-02T14:54:31.277522: step 3608, loss 3.9036.
Train: 2018-08-02T14:54:31.339977: step 3609, loss 5.28875.
Train: 2018-08-02T14:54:31.386866: step 3610, loss 3.65176.
Test: 2018-08-02T14:54:31.589942: step 3610, loss 3.81863.
Train: 2018-08-02T14:54:31.652433: step 3611, loss 3.65176.
Train: 2018-08-02T14:54:31.714888: step 3612, loss 3.52583.
Train: 2018-08-02T14:54:31.761752: step 3613, loss 4.65914.
Train: 2018-08-02T14:54:31.824240: step 3614, loss 4.65914.
Train: 2018-08-02T14:54:31.886726: step 3615, loss 3.77768.
Train: 2018-08-02T14:54:31.933618: step 3616, loss 4.28137.
Train: 2018-08-02T14:54:31.996106: step 3617, loss 2.89622.
Train: 2018-08-02T14:54:32.058586: step 3618, loss 4.15545.
Train: 2018-08-02T14:54:32.121049: step 3619, loss 4.65914.
Train: 2018-08-02T14:54:32.167909: step 3620, loss 4.65914.
Test: 2018-08-02T14:54:32.371018: step 3620, loss 3.81863.
Train: 2018-08-02T14:54:32.433471: step 3621, loss 4.40729.
Train: 2018-08-02T14:54:32.495956: step 3622, loss 4.02952.
Train: 2018-08-02T14:54:32.558442: step 3623, loss 3.27399.
Train: 2018-08-02T14:54:32.605332: step 3624, loss 4.29816.
Train: 2018-08-02T14:54:32.667816: step 3625, loss 3.52583.
Train: 2018-08-02T14:54:32.714688: step 3626, loss 3.39991.
Train: 2018-08-02T14:54:32.777140: step 3627, loss 4.53321.
Train: 2018-08-02T14:54:32.839658: step 3628, loss 4.02952.
Train: 2018-08-02T14:54:32.886544: step 3629, loss 3.52583.
Train: 2018-08-02T14:54:32.949005: step 3630, loss 3.65176.
Test: 2018-08-02T14:54:33.136462: step 3630, loss 3.81863.
Train: 2018-08-02T14:54:33.198946: step 3631, loss 4.15545.
Train: 2018-08-02T14:54:33.261432: step 3632, loss 3.77768.
Train: 2018-08-02T14:54:33.308297: step 3633, loss 3.14807.
Train: 2018-08-02T14:54:33.370786: step 3634, loss 5.41467.
Train: 2018-08-02T14:54:33.433262: step 3635, loss 4.02952.
Train: 2018-08-02T14:54:33.495755: step 3636, loss 4.40729.
Train: 2018-08-02T14:54:33.542616: step 3637, loss 4.15545.
Train: 2018-08-02T14:54:33.605099: step 3638, loss 4.65914.
Train: 2018-08-02T14:54:33.667571: step 3639, loss 5.5406.
Train: 2018-08-02T14:54:33.714446: step 3640, loss 3.27399.
Test: 2018-08-02T14:54:33.917497: step 3640, loss 3.81863.
Train: 2018-08-02T14:54:33.979984: step 3641, loss 4.15545.
Train: 2018-08-02T14:54:34.042468: step 3642, loss 3.39991.
Train: 2018-08-02T14:54:34.104953: step 3643, loss 4.65914.
Train: 2018-08-02T14:54:34.151843: step 3644, loss 3.02214.
Train: 2018-08-02T14:54:34.214334: step 3645, loss 4.28137.
Train: 2018-08-02T14:54:34.276788: step 3646, loss 2.51845.
Train: 2018-08-02T14:54:34.339307: step 3647, loss 4.15545.
Train: 2018-08-02T14:54:34.386169: step 3648, loss 3.27399.
Train: 2018-08-02T14:54:34.448624: step 3649, loss 3.39991.
Train: 2018-08-02T14:54:34.511139: step 3650, loss 3.9036.
Test: 2018-08-02T14:54:34.698595: step 3650, loss 3.81863.
Train: 2018-08-02T14:54:34.761050: step 3651, loss 3.9036.
Train: 2018-08-02T14:54:34.823536: step 3652, loss 4.40729.
Train: 2018-08-02T14:54:34.886022: step 3653, loss 3.9036.
Train: 2018-08-02T14:54:34.948538: step 3654, loss 4.15545.
Train: 2018-08-02T14:54:34.995373: step 3655, loss 4.91098.
Train: 2018-08-02T14:54:35.057857: step 3656, loss 4.28137.
Train: 2018-08-02T14:54:35.120375: step 3657, loss 4.40729.
Train: 2018-08-02T14:54:35.167206: step 3658, loss 3.65176.
Train: 2018-08-02T14:54:35.229691: step 3659, loss 4.65914.
Train: 2018-08-02T14:54:35.292176: step 3660, loss 4.15545.
Test: 2018-08-02T14:54:35.479656: step 3660, loss 3.81863.
Train: 2018-08-02T14:54:35.542143: step 3661, loss 4.02952.
Train: 2018-08-02T14:54:35.604603: step 3662, loss 4.15545.
Train: 2018-08-02T14:54:35.667118: step 3663, loss 3.65176.
Train: 2018-08-02T14:54:35.729604: step 3664, loss 2.89622.
Train: 2018-08-02T14:54:35.776468: step 3665, loss 4.65914.
Train: 2018-08-02T14:54:35.838953: step 3666, loss 4.53321.
Train: 2018-08-02T14:54:35.901409: step 3667, loss 3.9036.
Train: 2018-08-02T14:54:35.948272: step 3668, loss 3.65176.
Train: 2018-08-02T14:54:36.010759: step 3669, loss 4.65914.
Train: 2018-08-02T14:54:36.073273: step 3670, loss 4.65914.
Test: 2018-08-02T14:54:36.276345: step 3670, loss 3.81863.
Train: 2018-08-02T14:54:36.323184: step 3671, loss 4.53321.
Train: 2018-08-02T14:54:36.385696: step 3672, loss 4.02952.
Train: 2018-08-02T14:54:36.448185: step 3673, loss 3.9036.
Train: 2018-08-02T14:54:36.510672: step 3674, loss 3.9036.
Train: 2018-08-02T14:54:36.573157: step 3675, loss 3.77768.
Train: 2018-08-02T14:54:36.620024: step 3676, loss 3.14807.
Train: 2018-08-02T14:54:36.682474: step 3677, loss 3.77768.
Train: 2018-08-02T14:54:36.744963: step 3678, loss 4.40729.
Train: 2018-08-02T14:54:36.791855: step 3679, loss 4.15545.
Train: 2018-08-02T14:54:36.854335: step 3680, loss 4.28137.
Test: 2018-08-02T14:54:37.057387: step 3680, loss 3.81863.
Train: 2018-08-02T14:54:37.119906: step 3681, loss 4.15545.
Train: 2018-08-02T14:54:37.182388: step 3682, loss 4.15545.
Train: 2018-08-02T14:54:37.229250: step 3683, loss 3.52583.
Train: 2018-08-02T14:54:37.291734: step 3684, loss 4.40729.
Train: 2018-08-02T14:54:37.354194: step 3685, loss 3.39991.
Train: 2018-08-02T14:54:37.416708: step 3686, loss 3.9036.
Train: 2018-08-02T14:54:37.463575: step 3687, loss 3.77768.
Train: 2018-08-02T14:54:37.526054: step 3688, loss 2.51845.
Train: 2018-08-02T14:54:37.588548: step 3689, loss 4.15545.
Train: 2018-08-02T14:54:37.635408: step 3690, loss 4.02952.
Test: 2018-08-02T14:54:37.838480: step 3690, loss 3.81863.
Train: 2018-08-02T14:54:37.900941: step 3691, loss 4.65914.
Train: 2018-08-02T14:54:37.963426: step 3692, loss 3.9036.
Train: 2018-08-02T14:54:38.010289: step 3693, loss 3.02214.
Train: 2018-08-02T14:54:38.072777: step 3694, loss 4.02952.
Train: 2018-08-02T14:54:38.135263: step 3695, loss 4.65914.
Train: 2018-08-02T14:54:38.197747: step 3696, loss 3.9036.
Train: 2018-08-02T14:54:38.244643: step 3697, loss 4.91098.
Train: 2018-08-02T14:54:38.307097: step 3698, loss 5.16283.
Train: 2018-08-02T14:54:38.369583: step 3699, loss 3.14807.
Train: 2018-08-02T14:54:38.416445: step 3700, loss 3.65176.
Test: 2018-08-02T14:54:38.619563: step 3700, loss 3.81863.
Train: 2018-08-02T14:54:39.744287: step 3701, loss 4.40729.
Train: 2018-08-02T14:54:39.806769: step 3702, loss 4.53321.
Train: 2018-08-02T14:54:39.869262: step 3703, loss 4.15545.
Train: 2018-08-02T14:54:39.931745: step 3704, loss 2.7703.
Train: 2018-08-02T14:54:39.994200: step 3705, loss 4.28137.
Train: 2018-08-02T14:54:40.041095: step 3706, loss 4.53321.
Train: 2018-08-02T14:54:40.103578: step 3707, loss 4.53321.
Train: 2018-08-02T14:54:40.166066: step 3708, loss 4.40729.
Train: 2018-08-02T14:54:40.228525: step 3709, loss 4.9164.
Train: 2018-08-02T14:54:40.275415: step 3710, loss 3.77768.
Test: 2018-08-02T14:54:40.478494: step 3710, loss 3.81863.
Train: 2018-08-02T14:54:40.540977: step 3711, loss 4.40729.
Train: 2018-08-02T14:54:40.587844: step 3712, loss 4.40729.
Train: 2018-08-02T14:54:40.650328: step 3713, loss 5.66652.
Train: 2018-08-02T14:54:40.712815: step 3714, loss 5.16283.
Train: 2018-08-02T14:54:40.775299: step 3715, loss 3.77768.
Train: 2018-08-02T14:54:40.822131: step 3716, loss 3.77768.
Train: 2018-08-02T14:54:40.884642: step 3717, loss 3.77768.
Train: 2018-08-02T14:54:40.947134: step 3718, loss 3.77768.
Train: 2018-08-02T14:54:40.993967: step 3719, loss 4.28137.
Train: 2018-08-02T14:54:41.056452: step 3720, loss 3.77768.
Test: 2018-08-02T14:54:41.259553: step 3720, loss 3.81863.
Train: 2018-08-02T14:54:41.322044: step 3721, loss 3.77768.
Train: 2018-08-02T14:54:41.384524: step 3722, loss 3.39991.
Train: 2018-08-02T14:54:41.431394: step 3723, loss 3.39991.
Train: 2018-08-02T14:54:41.493883: step 3724, loss 3.77768.
Train: 2018-08-02T14:54:41.556337: step 3725, loss 4.40729.
Train: 2018-08-02T14:54:41.618853: step 3726, loss 4.91098.
Train: 2018-08-02T14:54:41.665683: step 3727, loss 3.65176.
Train: 2018-08-02T14:54:41.728170: step 3728, loss 3.39991.
Train: 2018-08-02T14:54:41.790685: step 3729, loss 3.27399.
Train: 2018-08-02T14:54:41.837549: step 3730, loss 4.53321.
Test: 2018-08-02T14:54:42.040597: step 3730, loss 3.81863.
Train: 2018-08-02T14:54:42.103111: step 3731, loss 3.77768.
Train: 2018-08-02T14:54:42.149976: step 3732, loss 4.40729.
Train: 2018-08-02T14:54:42.212463: step 3733, loss 3.52583.
Train: 2018-08-02T14:54:42.274916: step 3734, loss 4.02952.
Train: 2018-08-02T14:54:42.337400: step 3735, loss 3.77768.
Train: 2018-08-02T14:54:42.384265: step 3736, loss 3.77768.
Train: 2018-08-02T14:54:42.446750: step 3737, loss 4.15545.
Train: 2018-08-02T14:54:42.509236: step 3738, loss 5.16283.
Train: 2018-08-02T14:54:42.556100: step 3739, loss 4.28137.
Train: 2018-08-02T14:54:42.618619: step 3740, loss 4.02952.
Test: 2018-08-02T14:54:42.821662: step 3740, loss 3.81863.
Train: 2018-08-02T14:54:42.884178: step 3741, loss 3.77768.
Train: 2018-08-02T14:54:42.931013: step 3742, loss 3.65176.
Train: 2018-08-02T14:54:42.993531: step 3743, loss 2.64437.
Train: 2018-08-02T14:54:43.056014: step 3744, loss 4.28137.
Train: 2018-08-02T14:54:43.118469: step 3745, loss 4.15545.
Train: 2018-08-02T14:54:43.165363: step 3746, loss 4.40729.
Train: 2018-08-02T14:54:43.227849: step 3747, loss 5.28875.
Train: 2018-08-02T14:54:43.290334: step 3748, loss 3.9036.
Train: 2018-08-02T14:54:43.337200: step 3749, loss 3.77768.
Train: 2018-08-02T14:54:43.399654: step 3750, loss 4.02952.
Test: 2018-08-02T14:54:43.602754: step 3750, loss 3.81863.
Train: 2018-08-02T14:54:43.665245: step 3751, loss 3.02214.
Train: 2018-08-02T14:54:43.727730: step 3752, loss 4.53321.
Train: 2018-08-02T14:54:43.774590: step 3753, loss 3.77768.
Train: 2018-08-02T14:54:43.837077: step 3754, loss 3.27399.
Train: 2018-08-02T14:54:43.899566: step 3755, loss 4.40729.
Train: 2018-08-02T14:54:43.946401: step 3756, loss 4.15545.
Train: 2018-08-02T14:54:44.008884: step 3757, loss 3.9036.
Train: 2018-08-02T14:54:44.071371: step 3758, loss 3.14807.
Train: 2018-08-02T14:54:44.133856: step 3759, loss 3.65176.
Train: 2018-08-02T14:54:44.180720: step 3760, loss 2.89622.
Test: 2018-08-02T14:54:44.383797: step 3760, loss 3.81863.
Train: 2018-08-02T14:54:44.446312: step 3761, loss 4.15545.
Train: 2018-08-02T14:54:44.493179: step 3762, loss 3.77768.
Train: 2018-08-02T14:54:44.555657: step 3763, loss 4.53321.
Train: 2018-08-02T14:54:44.618151: step 3764, loss 3.77768.
Train: 2018-08-02T14:54:44.680634: step 3765, loss 4.78506.
Train: 2018-08-02T14:54:44.727466: step 3766, loss 3.9036.
Train: 2018-08-02T14:54:44.789952: step 3767, loss 3.52583.
Train: 2018-08-02T14:54:44.852449: step 3768, loss 4.02952.
Train: 2018-08-02T14:54:44.899332: step 3769, loss 5.0369.
Train: 2018-08-02T14:54:44.961819: step 3770, loss 3.9036.
Test: 2018-08-02T14:54:45.164864: step 3770, loss 3.81863.
Train: 2018-08-02T14:54:45.227380: step 3771, loss 2.64438.
Train: 2018-08-02T14:54:45.274238: step 3772, loss 4.40729.
Train: 2018-08-02T14:54:45.336731: step 3773, loss 4.28137.
Train: 2018-08-02T14:54:45.399186: step 3774, loss 3.39991.
Train: 2018-08-02T14:54:45.446077: step 3775, loss 5.10406.
Train: 2018-08-02T14:54:45.508565: step 3776, loss 4.40729.
Train: 2018-08-02T14:54:45.555428: step 3777, loss 3.9036.
Train: 2018-08-02T14:54:45.617916: step 3778, loss 4.15545.
Train: 2018-08-02T14:54:45.680398: step 3779, loss 3.65176.
Train: 2018-08-02T14:54:45.727261: step 3780, loss 3.77768.
Test: 2018-08-02T14:54:45.930334: step 3780, loss 3.81863.
Train: 2018-08-02T14:54:45.992829: step 3781, loss 4.53321.
Train: 2018-08-02T14:54:46.055281: step 3782, loss 4.78506.
Train: 2018-08-02T14:54:46.117799: step 3783, loss 4.40729.
Train: 2018-08-02T14:54:46.164632: step 3784, loss 4.28137.
Train: 2018-08-02T14:54:46.227117: step 3785, loss 4.28137.
Train: 2018-08-02T14:54:46.289600: step 3786, loss 4.65914.
Train: 2018-08-02T14:54:46.336497: step 3787, loss 4.65914.
Train: 2018-08-02T14:54:46.398975: step 3788, loss 4.65914.
Train: 2018-08-02T14:54:46.461466: step 3789, loss 5.16283.
Train: 2018-08-02T14:54:46.508300: step 3790, loss 4.53321.
Test: 2018-08-02T14:54:46.711376: step 3790, loss 3.81863.
Train: 2018-08-02T14:54:46.773893: step 3791, loss 3.27399.
Train: 2018-08-02T14:54:46.836347: step 3792, loss 4.28137.
Train: 2018-08-02T14:54:46.898832: step 3793, loss 4.53321.
Train: 2018-08-02T14:54:46.945728: step 3794, loss 4.65914.
Train: 2018-08-02T14:54:47.008208: step 3795, loss 3.9036.
Train: 2018-08-02T14:54:47.070698: step 3796, loss 2.64438.
Train: 2018-08-02T14:54:47.133183: step 3797, loss 3.02214.
Train: 2018-08-02T14:54:47.180048: step 3798, loss 4.40729.
Train: 2018-08-02T14:54:47.242537: step 3799, loss 4.53321.
Train: 2018-08-02T14:54:47.305018: step 3800, loss 3.52583.
Test: 2018-08-02T14:54:47.508067: step 3800, loss 3.81863.
Train: 2018-08-02T14:54:48.007949: step 3801, loss 2.64438.
Train: 2018-08-02T14:54:48.070465: step 3802, loss 4.40729.
Train: 2018-08-02T14:54:48.132950: step 3803, loss 3.52583.
Train: 2018-08-02T14:54:48.195435: step 3804, loss 3.39991.
Train: 2018-08-02T14:54:48.257891: step 3805, loss 4.53321.
Train: 2018-08-02T14:54:48.320402: step 3806, loss 3.02214.
Train: 2018-08-02T14:54:48.367238: step 3807, loss 3.14807.
Train: 2018-08-02T14:54:48.429755: step 3808, loss 3.9036.
Train: 2018-08-02T14:54:48.492243: step 3809, loss 5.0369.
Train: 2018-08-02T14:54:48.539099: step 3810, loss 3.9036.
Test: 2018-08-02T14:54:48.742176: step 3810, loss 3.81863.
Train: 2018-08-02T14:54:48.804667: step 3811, loss 4.40729.
Train: 2018-08-02T14:54:48.867147: step 3812, loss 3.14807.
Train: 2018-08-02T14:54:48.913987: step 3813, loss 2.7703.
Train: 2018-08-02T14:54:48.976501: step 3814, loss 3.9036.
Train: 2018-08-02T14:54:49.038988: step 3815, loss 4.40729.
Train: 2018-08-02T14:54:49.085853: step 3816, loss 3.27399.
Train: 2018-08-02T14:54:49.148336: step 3817, loss 3.14807.
Train: 2018-08-02T14:54:49.210796: step 3818, loss 3.65176.
Train: 2018-08-02T14:54:49.273307: step 3819, loss 4.78506.
Train: 2018-08-02T14:54:49.320172: step 3820, loss 3.14807.
Test: 2018-08-02T14:54:49.523243: step 3820, loss 3.81863.
Train: 2018-08-02T14:54:49.585729: step 3821, loss 3.52583.
Train: 2018-08-02T14:54:49.648215: step 3822, loss 3.02214.
Train: 2018-08-02T14:54:49.695084: step 3823, loss 4.02952.
Train: 2018-08-02T14:54:49.757569: step 3824, loss 3.77768.
Train: 2018-08-02T14:54:49.820023: step 3825, loss 4.53321.
Train: 2018-08-02T14:54:49.866922: step 3826, loss 4.15545.
Train: 2018-08-02T14:54:49.929403: step 3827, loss 4.15545.
Train: 2018-08-02T14:54:49.991860: step 3828, loss 3.77768.
Train: 2018-08-02T14:54:50.038723: step 3829, loss 3.39991.
Train: 2018-08-02T14:54:50.101237: step 3830, loss 3.65176.
Test: 2018-08-02T14:54:50.304310: step 3830, loss 3.81863.
Train: 2018-08-02T14:54:50.366800: step 3831, loss 3.77768.
Train: 2018-08-02T14:54:50.429289: step 3832, loss 3.65176.
Train: 2018-08-02T14:54:50.476120: step 3833, loss 3.77768.
Train: 2018-08-02T14:54:50.538607: step 3834, loss 3.9036.
Train: 2018-08-02T14:54:50.601120: step 3835, loss 3.52583.
Train: 2018-08-02T14:54:50.663577: step 3836, loss 3.9036.
Train: 2018-08-02T14:54:50.710474: step 3837, loss 3.39991.
Train: 2018-08-02T14:54:50.772927: step 3838, loss 4.53321.
Train: 2018-08-02T14:54:50.835412: step 3839, loss 4.40729.
Train: 2018-08-02T14:54:50.882317: step 3840, loss 4.02952.
Test: 2018-08-02T14:54:51.085354: step 3840, loss 3.81863.
Train: 2018-08-02T14:54:51.147838: step 3841, loss 4.02952.
Train: 2018-08-02T14:54:51.210324: step 3842, loss 4.65914.
Train: 2018-08-02T14:54:51.257187: step 3843, loss 4.28137.
Train: 2018-08-02T14:54:51.319704: step 3844, loss 4.91098.
Train: 2018-08-02T14:54:51.382163: step 3845, loss 3.77768.
Train: 2018-08-02T14:54:51.429053: step 3846, loss 3.65176.
Train: 2018-08-02T14:54:51.491507: step 3847, loss 3.02214.
Train: 2018-08-02T14:54:51.554024: step 3848, loss 5.28875.
Train: 2018-08-02T14:54:51.616513: step 3849, loss 3.02214.
Train: 2018-08-02T14:54:51.663342: step 3850, loss 4.02952.
Test: 2018-08-02T14:54:51.866421: step 3850, loss 3.81863.
Train: 2018-08-02T14:54:51.928935: step 3851, loss 4.53321.
Train: 2018-08-02T14:54:51.975770: step 3852, loss 4.53321.
Train: 2018-08-02T14:54:52.038285: step 3853, loss 2.64438.
Train: 2018-08-02T14:54:52.100740: step 3854, loss 5.16283.
Train: 2018-08-02T14:54:52.163225: step 3855, loss 4.15545.
Train: 2018-08-02T14:54:52.210122: step 3856, loss 3.02214.
Train: 2018-08-02T14:54:52.272605: step 3857, loss 4.65914.
Train: 2018-08-02T14:54:52.335096: step 3858, loss 3.9036.
Train: 2018-08-02T14:54:52.397576: step 3859, loss 3.52583.
Train: 2018-08-02T14:54:52.444409: step 3860, loss 4.15545.
Test: 2018-08-02T14:54:52.647486: step 3860, loss 3.81863.
Train: 2018-08-02T14:54:52.710013: step 3861, loss 4.28137.
Train: 2018-08-02T14:54:52.772489: step 3862, loss 3.77768.
Train: 2018-08-02T14:54:52.834967: step 3863, loss 4.78506.
Train: 2018-08-02T14:54:52.897459: step 3864, loss 4.53321.
Train: 2018-08-02T14:54:52.944293: step 3865, loss 3.02214.
Train: 2018-08-02T14:54:53.006810: step 3866, loss 4.53321.
Train: 2018-08-02T14:54:53.069294: step 3867, loss 4.28137.
Train: 2018-08-02T14:54:53.116161: step 3868, loss 4.02952.
Train: 2018-08-02T14:54:53.178613: step 3869, loss 3.52583.
Train: 2018-08-02T14:54:53.241132: step 3870, loss 4.28137.
Test: 2018-08-02T14:54:53.428584: step 3870, loss 3.81863.
Train: 2018-08-02T14:54:53.491040: step 3871, loss 3.52583.
Train: 2018-08-02T14:54:53.553524: step 3872, loss 3.77768.
Train: 2018-08-02T14:54:53.600388: step 3873, loss 3.9036.
Train: 2018-08-02T14:54:53.662874: step 3874, loss 4.02952.
Train: 2018-08-02T14:54:53.725388: step 3875, loss 5.41467.
Train: 2018-08-02T14:54:53.772254: step 3876, loss 3.77768.
Train: 2018-08-02T14:54:53.834710: step 3877, loss 3.02214.
Train: 2018-08-02T14:54:53.897221: step 3878, loss 4.15545.
Train: 2018-08-02T14:54:53.944089: step 3879, loss 4.91098.
Train: 2018-08-02T14:54:54.006576: step 3880, loss 5.0369.
Test: 2018-08-02T14:54:54.209621: step 3880, loss 3.81863.
Train: 2018-08-02T14:54:54.272147: step 3881, loss 2.64438.
Train: 2018-08-02T14:54:54.334622: step 3882, loss 3.52583.
Train: 2018-08-02T14:54:54.381457: step 3883, loss 4.53321.
Train: 2018-08-02T14:54:54.443942: step 3884, loss 3.27399.
Train: 2018-08-02T14:54:54.506453: step 3885, loss 3.65176.
Train: 2018-08-02T14:54:54.568942: step 3886, loss 3.52583.
Train: 2018-08-02T14:54:54.615777: step 3887, loss 3.65176.
Train: 2018-08-02T14:54:54.678287: step 3888, loss 4.65914.
Train: 2018-08-02T14:54:54.740780: step 3889, loss 4.91098.
Train: 2018-08-02T14:54:54.787642: step 3890, loss 5.0369.
Test: 2018-08-02T14:54:54.990713: step 3890, loss 3.81863.
Train: 2018-08-02T14:54:55.053173: step 3891, loss 4.91098.
Train: 2018-08-02T14:54:55.115692: step 3892, loss 4.53321.
Train: 2018-08-02T14:54:55.178143: step 3893, loss 5.0369.
Train: 2018-08-02T14:54:55.225036: step 3894, loss 3.52583.
Train: 2018-08-02T14:54:55.287525: step 3895, loss 4.40729.
Train: 2018-08-02T14:54:55.349980: step 3896, loss 4.02952.
Train: 2018-08-02T14:54:55.396873: step 3897, loss 4.02952.
Train: 2018-08-02T14:54:55.459332: step 3898, loss 4.02952.
Train: 2018-08-02T14:54:55.521814: step 3899, loss 4.78506.
Train: 2018-08-02T14:54:55.568709: step 3900, loss 3.27399.
Test: 2018-08-02T14:54:55.771785: step 3900, loss 3.81863.
Train: 2018-08-02T14:54:56.302882: step 3901, loss 3.9036.
Train: 2018-08-02T14:54:56.365400: step 3902, loss 3.14807.
Train: 2018-08-02T14:54:56.427882: step 3903, loss 3.14807.
Train: 2018-08-02T14:54:56.474744: step 3904, loss 3.52583.
Train: 2018-08-02T14:54:56.537201: step 3905, loss 4.65914.
Train: 2018-08-02T14:54:56.599687: step 3906, loss 4.28137.
Train: 2018-08-02T14:54:56.646575: step 3907, loss 4.15545.
Train: 2018-08-02T14:54:56.709038: step 3908, loss 3.77768.
Train: 2018-08-02T14:54:56.771552: step 3909, loss 4.65914.
Train: 2018-08-02T14:54:56.818415: step 3910, loss 3.77768.
Test: 2018-08-02T14:54:57.021486: step 3910, loss 3.81863.
Train: 2018-08-02T14:54:57.083981: step 3911, loss 5.91836.
Train: 2018-08-02T14:54:57.146464: step 3912, loss 4.40729.
Train: 2018-08-02T14:54:57.208949: step 3913, loss 3.77768.
Train: 2018-08-02T14:54:57.271407: step 3914, loss 4.40729.
Train: 2018-08-02T14:54:57.333920: step 3915, loss 3.14807.
Train: 2018-08-02T14:54:57.380784: step 3916, loss 3.77768.
Train: 2018-08-02T14:54:57.443269: step 3917, loss 3.65176.
Train: 2018-08-02T14:54:57.505725: step 3918, loss 3.52583.
Train: 2018-08-02T14:54:57.552588: step 3919, loss 3.14807.
Train: 2018-08-02T14:54:57.615107: step 3920, loss 6.42205.
Test: 2018-08-02T14:54:57.802560: step 3920, loss 3.81863.
Train: 2018-08-02T14:54:57.865016: step 3921, loss 4.15545.
Train: 2018-08-02T14:54:57.927530: step 3922, loss 4.40729.
Train: 2018-08-02T14:54:57.990019: step 3923, loss 4.78506.
Train: 2018-08-02T14:54:58.036852: step 3924, loss 2.89622.
Train: 2018-08-02T14:54:58.099361: step 3925, loss 3.52583.
Train: 2018-08-02T14:54:58.146231: step 3926, loss 4.02952.
Train: 2018-08-02T14:54:58.208684: step 3927, loss 4.65914.
Train: 2018-08-02T14:54:58.255579: step 3928, loss 3.77768.
Train: 2018-08-02T14:54:58.318035: step 3929, loss 4.15545.
Train: 2018-08-02T14:54:58.380520: step 3930, loss 4.91098.
Test: 2018-08-02T14:54:58.583598: step 3930, loss 3.81863.
Train: 2018-08-02T14:54:58.630491: step 3931, loss 3.65176.
Train: 2018-08-02T14:54:58.692976: step 3932, loss 5.16283.
Train: 2018-08-02T14:54:58.755462: step 3933, loss 4.28137.
Train: 2018-08-02T14:54:58.817919: step 3934, loss 4.15545.
Train: 2018-08-02T14:54:58.864782: step 3935, loss 3.27399.
Train: 2018-08-02T14:54:58.927297: step 3936, loss 5.16283.
Train: 2018-08-02T14:54:58.989785: step 3937, loss 4.02952.
Train: 2018-08-02T14:54:59.036640: step 3938, loss 4.15545.
Train: 2018-08-02T14:54:59.099131: step 3939, loss 4.15545.
Train: 2018-08-02T14:54:59.161617: step 3940, loss 3.02214.
Test: 2018-08-02T14:54:59.364698: step 3940, loss 3.81863.
Train: 2018-08-02T14:54:59.411558: step 3941, loss 3.65176.
Train: 2018-08-02T14:54:59.474041: step 3942, loss 2.89622.
Train: 2018-08-02T14:54:59.536499: step 3943, loss 4.02952.
Train: 2018-08-02T14:54:59.599001: step 3944, loss 4.40729.
Train: 2018-08-02T14:54:59.661504: step 3945, loss 3.77768.
Train: 2018-08-02T14:54:59.708366: step 3946, loss 4.65914.
Train: 2018-08-02T14:54:59.770848: step 3947, loss 4.40729.
Train: 2018-08-02T14:54:59.833310: step 3948, loss 4.15545.
Train: 2018-08-02T14:54:59.880193: step 3949, loss 3.27399.
Train: 2018-08-02T14:54:59.942684: step 3950, loss 4.53321.
Test: 2018-08-02T14:55:00.145755: step 3950, loss 3.81863.
Train: 2018-08-02T14:55:00.208231: step 3951, loss 3.9036.
Train: 2018-08-02T14:55:00.255080: step 3952, loss 5.0369.
Train: 2018-08-02T14:55:00.317596: step 3953, loss 3.39991.
Train: 2018-08-02T14:55:00.380053: step 3954, loss 4.40729.
Train: 2018-08-02T14:55:00.442571: step 3955, loss 4.15545.
Train: 2018-08-02T14:55:00.489401: step 3956, loss 3.14807.
Train: 2018-08-02T14:55:00.551911: step 3957, loss 3.27399.
Train: 2018-08-02T14:55:00.614373: step 3958, loss 4.65914.
Train: 2018-08-02T14:55:00.661236: step 3959, loss 4.40729.
Train: 2018-08-02T14:55:00.723720: step 3960, loss 3.27399.
Test: 2018-08-02T14:55:00.926828: step 3960, loss 3.81863.
Train: 2018-08-02T14:55:00.989317: step 3961, loss 4.28137.
Train: 2018-08-02T14:55:01.036179: step 3962, loss 4.53321.
Train: 2018-08-02T14:55:01.098664: step 3963, loss 4.15545.
Train: 2018-08-02T14:55:01.161118: step 3964, loss 3.77768.
Train: 2018-08-02T14:55:01.223633: step 3965, loss 4.40729.
Train: 2018-08-02T14:55:01.270498: step 3966, loss 2.64438.
Train: 2018-08-02T14:55:01.332983: step 3967, loss 3.52583.
Train: 2018-08-02T14:55:01.395471: step 3968, loss 3.39991.
Train: 2018-08-02T14:55:01.442303: step 3969, loss 4.28137.
Train: 2018-08-02T14:55:01.504788: step 3970, loss 4.28137.
Test: 2018-08-02T14:55:01.707864: step 3970, loss 3.81863.
Train: 2018-08-02T14:55:01.770351: step 3971, loss 4.65914.
Train: 2018-08-02T14:55:01.817214: step 3972, loss 3.65176.
Train: 2018-08-02T14:55:01.879731: step 3973, loss 3.39991.
Train: 2018-08-02T14:55:01.942191: step 3974, loss 4.28137.
Train: 2018-08-02T14:55:02.004702: step 3975, loss 3.65176.
Train: 2018-08-02T14:55:02.051534: step 3976, loss 3.77768.
Train: 2018-08-02T14:55:02.114048: step 3977, loss 4.40729.
Train: 2018-08-02T14:55:02.176536: step 3978, loss 5.16283.
Train: 2018-08-02T14:55:02.223398: step 3979, loss 3.27399.
Train: 2018-08-02T14:55:02.285885: step 3980, loss 4.02952.
Test: 2018-08-02T14:55:02.488965: step 3980, loss 3.81863.
Train: 2018-08-02T14:55:02.551422: step 3981, loss 3.52583.
Train: 2018-08-02T14:55:02.598309: step 3982, loss 4.40729.
Train: 2018-08-02T14:55:02.660798: step 3983, loss 4.02952.
Train: 2018-08-02T14:55:02.723281: step 3984, loss 3.9036.
Train: 2018-08-02T14:55:02.785769: step 3985, loss 4.91098.
Train: 2018-08-02T14:55:02.832635: step 3986, loss 4.65914.
Train: 2018-08-02T14:55:02.895112: step 3987, loss 4.40729.
Train: 2018-08-02T14:55:02.957579: step 3988, loss 3.27399.
Train: 2018-08-02T14:55:03.004437: step 3989, loss 4.02952.
Train: 2018-08-02T14:55:03.066922: step 3990, loss 4.28137.
Test: 2018-08-02T14:55:03.270000: step 3990, loss 3.81863.
Train: 2018-08-02T14:55:03.332485: step 3991, loss 3.39991.
Train: 2018-08-02T14:55:03.394969: step 3992, loss 3.39991.
Train: 2018-08-02T14:55:03.441835: step 3993, loss 4.78506.
Train: 2018-08-02T14:55:03.504351: step 3994, loss 4.65914.
Train: 2018-08-02T14:55:03.566808: step 3995, loss 4.53321.
Train: 2018-08-02T14:55:03.629291: step 3996, loss 4.91098.
Train: 2018-08-02T14:55:03.676185: step 3997, loss 3.9036.
Train: 2018-08-02T14:55:03.738669: step 3998, loss 3.77768.
Train: 2018-08-02T14:55:03.801155: step 3999, loss 3.77768.
Train: 2018-08-02T14:55:03.848017: step 4000, loss 4.28137.
Test: 2018-08-02T14:55:04.051091: step 4000, loss 3.81863.
Train: 2018-08-02T14:55:04.550951: step 4001, loss 4.91098.
Train: 2018-08-02T14:55:04.613469: step 4002, loss 3.77768.
Train: 2018-08-02T14:55:04.675945: step 4003, loss 4.78506.
Train: 2018-08-02T14:55:04.738440: step 4004, loss 2.64438.
Train: 2018-08-02T14:55:04.800891: step 4005, loss 3.39991.
Train: 2018-08-02T14:55:04.847785: step 4006, loss 3.77768.
Train: 2018-08-02T14:55:04.910270: step 4007, loss 4.40729.
Train: 2018-08-02T14:55:04.972727: step 4008, loss 5.28875.
Train: 2018-08-02T14:55:05.019620: step 4009, loss 3.9036.
Train: 2018-08-02T14:55:05.082111: step 4010, loss 4.02952.
Test: 2018-08-02T14:55:05.285177: step 4010, loss 3.81863.
Train: 2018-08-02T14:55:05.332049: step 4011, loss 3.77768.
Train: 2018-08-02T14:55:05.394503: step 4012, loss 3.52583.
Train: 2018-08-02T14:55:05.457018: step 4013, loss 3.77768.
Train: 2018-08-02T14:55:05.519504: step 4014, loss 4.53321.
Train: 2018-08-02T14:55:05.581994: step 4015, loss 4.78506.
Train: 2018-08-02T14:55:05.628853: step 4016, loss 4.40729.
Train: 2018-08-02T14:55:05.691337: step 4017, loss 2.7703.
Train: 2018-08-02T14:55:05.753823: step 4018, loss 4.40729.
Train: 2018-08-02T14:55:05.800658: step 4019, loss 4.28137.
Train: 2018-08-02T14:55:05.863176: step 4020, loss 4.78506.
Test: 2018-08-02T14:55:06.066220: step 4020, loss 3.81863.
Train: 2018-08-02T14:55:06.128735: step 4021, loss 4.91098.
Train: 2018-08-02T14:55:06.175600: step 4022, loss 3.14807.
Train: 2018-08-02T14:55:06.238054: step 4023, loss 3.65176.
Train: 2018-08-02T14:55:06.300571: step 4024, loss 4.28137.
Train: 2018-08-02T14:55:06.363026: step 4025, loss 4.02952.
Train: 2018-08-02T14:55:06.409914: step 4026, loss 3.52583.
Train: 2018-08-02T14:55:06.472402: step 4027, loss 3.65176.
Train: 2018-08-02T14:55:06.534892: step 4028, loss 3.02214.
Train: 2018-08-02T14:55:06.597380: step 4029, loss 4.28137.
Train: 2018-08-02T14:55:06.644210: step 4030, loss 3.9036.
Test: 2018-08-02T14:55:06.847286: step 4030, loss 3.81863.
Train: 2018-08-02T14:55:06.909803: step 4031, loss 3.14807.
Train: 2018-08-02T14:55:06.972287: step 4032, loss 4.40729.
Train: 2018-08-02T14:55:07.019151: step 4033, loss 2.89622.
Train: 2018-08-02T14:55:07.081633: step 4034, loss 4.65914.
Train: 2018-08-02T14:55:07.144123: step 4035, loss 3.77768.
Train: 2018-08-02T14:55:07.206603: step 4036, loss 3.52583.
Train: 2018-08-02T14:55:07.253472: step 4037, loss 4.65914.
Train: 2018-08-02T14:55:07.315929: step 4038, loss 3.52583.
Train: 2018-08-02T14:55:07.378414: step 4039, loss 4.40729.
Train: 2018-08-02T14:55:07.425277: step 4040, loss 4.28137.
Test: 2018-08-02T14:55:07.628378: step 4040, loss 3.81863.
Train: 2018-08-02T14:55:07.690869: step 4041, loss 4.78506.
Train: 2018-08-02T14:55:07.753354: step 4042, loss 4.65914.
Train: 2018-08-02T14:55:07.800219: step 4043, loss 3.39991.
Train: 2018-08-02T14:55:07.862709: step 4044, loss 3.39991.
Train: 2018-08-02T14:55:07.925191: step 4045, loss 4.65914.
Train: 2018-08-02T14:55:07.987645: step 4046, loss 4.78506.
Train: 2018-08-02T14:55:08.034508: step 4047, loss 4.53321.
Train: 2018-08-02T14:55:08.097022: step 4048, loss 3.9036.
Train: 2018-08-02T14:55:08.159480: step 4049, loss 3.52583.
Train: 2018-08-02T14:55:08.206376: step 4050, loss 3.14807.
Test: 2018-08-02T14:55:08.409451: step 4050, loss 3.81863.
Train: 2018-08-02T14:55:08.471939: step 4051, loss 4.28137.
Train: 2018-08-02T14:55:08.534421: step 4052, loss 4.28137.
Train: 2018-08-02T14:55:08.596877: step 4053, loss 4.91098.
Train: 2018-08-02T14:55:08.643772: step 4054, loss 4.91104.
Train: 2018-08-02T14:55:08.706254: step 4055, loss 3.39991.
Train: 2018-08-02T14:55:08.768742: step 4056, loss 2.7703.
Train: 2018-08-02T14:55:08.815606: step 4057, loss 3.77768.
Train: 2018-08-02T14:55:08.878092: step 4058, loss 4.40729.
Train: 2018-08-02T14:55:08.940547: step 4059, loss 4.02952.
Train: 2018-08-02T14:55:08.987444: step 4060, loss 3.27399.
Test: 2018-08-02T14:55:09.190487: step 4060, loss 3.81863.
Train: 2018-08-02T14:55:09.252998: step 4061, loss 3.27399.
Train: 2018-08-02T14:55:09.315489: step 4062, loss 4.65914.
Train: 2018-08-02T14:55:09.377968: step 4063, loss 3.52583.
Train: 2018-08-02T14:55:09.424836: step 4064, loss 3.39991.
Train: 2018-08-02T14:55:09.487328: step 4065, loss 5.41467.
Train: 2018-08-02T14:55:09.549780: step 4066, loss 3.52583.
Train: 2018-08-02T14:55:09.596675: step 4067, loss 4.02952.
Train: 2018-08-02T14:55:09.659129: step 4068, loss 3.77768.
Train: 2018-08-02T14:55:09.721614: step 4069, loss 3.77768.
Train: 2018-08-02T14:55:09.768478: step 4070, loss 3.14807.
Test: 2018-08-02T14:55:09.971554: step 4070, loss 3.81863.
Train: 2018-08-02T14:55:10.034040: step 4071, loss 3.65176.
Train: 2018-08-02T14:55:10.096554: step 4072, loss 4.15545.
Train: 2018-08-02T14:55:10.159010: step 4073, loss 3.52583.
Train: 2018-08-02T14:55:10.205909: step 4074, loss 3.27399.
Train: 2018-08-02T14:55:10.268362: step 4075, loss 3.65176.
Train: 2018-08-02T14:55:10.330846: step 4076, loss 4.02952.
Train: 2018-08-02T14:55:10.377739: step 4077, loss 5.10406.
Train: 2018-08-02T14:55:10.424575: step 4078, loss 3.65176.
Train: 2018-08-02T14:55:10.487060: step 4079, loss 3.52583.
Train: 2018-08-02T14:55:10.549574: step 4080, loss 4.65914.
Test: 2018-08-02T14:55:10.752623: step 4080, loss 3.81863.
Train: 2018-08-02T14:55:10.815108: step 4081, loss 3.9036.
Train: 2018-08-02T14:55:10.861971: step 4082, loss 4.28137.
Train: 2018-08-02T14:55:10.924483: step 4083, loss 3.27399.
Train: 2018-08-02T14:55:10.986943: step 4084, loss 3.77768.
Train: 2018-08-02T14:55:11.049464: step 4085, loss 4.15545.
Train: 2018-08-02T14:55:11.096291: step 4086, loss 3.52583.
Train: 2018-08-02T14:55:11.158802: step 4087, loss 4.40729.
Train: 2018-08-02T14:55:11.221291: step 4088, loss 4.40729.
Train: 2018-08-02T14:55:11.283775: step 4089, loss 4.28137.
Train: 2018-08-02T14:55:11.330611: step 4090, loss 5.28875.
Test: 2018-08-02T14:55:11.533688: step 4090, loss 3.81863.
Train: 2018-08-02T14:55:11.580586: step 4091, loss 3.39991.
Train: 2018-08-02T14:55:11.643039: step 4092, loss 2.7703.
Train: 2018-08-02T14:55:11.705557: step 4093, loss 4.28137.
Train: 2018-08-02T14:55:11.768036: step 4094, loss 5.28875.
Train: 2018-08-02T14:55:11.814899: step 4095, loss 3.52583.
Train: 2018-08-02T14:55:11.877359: step 4096, loss 3.65176.
Train: 2018-08-02T14:55:11.939875: step 4097, loss 4.40729.
Train: 2018-08-02T14:55:11.986742: step 4098, loss 5.16283.
Train: 2018-08-02T14:55:12.049195: step 4099, loss 4.78506.
Train: 2018-08-02T14:55:12.111712: step 4100, loss 4.02952.
Test: 2018-08-02T14:55:12.314787: step 4100, loss 3.81863.
Train: 2018-08-02T14:55:12.814641: step 4101, loss 4.53321.
Train: 2018-08-02T14:55:12.861532: step 4102, loss 4.53321.
Train: 2018-08-02T14:55:12.924019: step 4103, loss 3.02214.
Train: 2018-08-02T14:55:12.986509: step 4104, loss 4.02952.
Train: 2018-08-02T14:55:13.048989: step 4105, loss 2.64438.
Train: 2018-08-02T14:55:13.095825: step 4106, loss 4.65914.
Train: 2018-08-02T14:55:13.158339: step 4107, loss 3.39991.
Train: 2018-08-02T14:55:13.220823: step 4108, loss 4.02952.
Train: 2018-08-02T14:55:13.283280: step 4109, loss 4.53321.
Train: 2018-08-02T14:55:13.330177: step 4110, loss 4.40729.
Test: 2018-08-02T14:55:13.533251: step 4110, loss 3.81863.
Train: 2018-08-02T14:55:13.595734: step 4111, loss 4.53321.
Train: 2018-08-02T14:55:13.642603: step 4112, loss 4.78506.
Train: 2018-08-02T14:55:13.705088: step 4113, loss 3.77768.
Train: 2018-08-02T14:55:13.767572: step 4114, loss 4.91098.
Train: 2018-08-02T14:55:13.814436: step 4115, loss 4.15545.
Train: 2018-08-02T14:55:13.876937: step 4116, loss 4.40729.
Train: 2018-08-02T14:55:13.939386: step 4117, loss 4.65914.
Train: 2018-08-02T14:55:13.986241: step 4118, loss 3.39991.
Train: 2018-08-02T14:55:14.048758: step 4119, loss 2.64438.
Train: 2018-08-02T14:55:14.111244: step 4120, loss 4.02952.
Test: 2018-08-02T14:55:14.314288: step 4120, loss 3.81863.
Train: 2018-08-02T14:55:14.376774: step 4121, loss 4.28137.
Train: 2018-08-02T14:55:14.423637: step 4122, loss 3.27399.
Train: 2018-08-02T14:55:14.486152: step 4123, loss 4.65914.
Train: 2018-08-02T14:55:14.548641: step 4124, loss 4.02952.
Train: 2018-08-02T14:55:14.611095: step 4125, loss 3.77768.
Train: 2018-08-02T14:55:14.657959: step 4126, loss 4.02952.
Train: 2018-08-02T14:55:14.720443: step 4127, loss 3.77768.
Train: 2018-08-02T14:55:14.782929: step 4128, loss 3.14807.
Train: 2018-08-02T14:55:14.829792: step 4129, loss 3.65176.
Train: 2018-08-02T14:55:14.892308: step 4130, loss 4.91098.
Test: 2018-08-02T14:55:15.095382: step 4130, loss 3.81863.
Train: 2018-08-02T14:55:15.157871: step 4131, loss 4.02952.
Train: 2018-08-02T14:55:15.220356: step 4132, loss 3.65176.
Train: 2018-08-02T14:55:15.267221: step 4133, loss 4.40729.
Train: 2018-08-02T14:55:15.329710: step 4134, loss 4.40729.
Train: 2018-08-02T14:55:15.392191: step 4135, loss 4.40729.
Train: 2018-08-02T14:55:15.454676: step 4136, loss 4.65914.
Train: 2018-08-02T14:55:15.501511: step 4137, loss 3.27399.
Train: 2018-08-02T14:55:15.564021: step 4138, loss 3.77768.
Train: 2018-08-02T14:55:15.626511: step 4139, loss 3.9036.
Train: 2018-08-02T14:55:15.673376: step 4140, loss 4.15545.
Test: 2018-08-02T14:55:15.876446: step 4140, loss 3.81863.
Train: 2018-08-02T14:55:15.938938: step 4141, loss 4.53321.
Train: 2018-08-02T14:55:16.001392: step 4142, loss 4.53321.
Train: 2018-08-02T14:55:16.063908: step 4143, loss 3.9036.
Train: 2018-08-02T14:55:16.110777: step 4144, loss 3.52583.
Train: 2018-08-02T14:55:16.173230: step 4145, loss 4.28137.
Train: 2018-08-02T14:55:16.235747: step 4146, loss 3.77768.
Train: 2018-08-02T14:55:16.282607: step 4147, loss 2.64438.
Train: 2018-08-02T14:55:16.345064: step 4148, loss 4.02952.
Train: 2018-08-02T14:55:16.407549: step 4149, loss 3.14807.
Train: 2018-08-02T14:55:16.454412: step 4150, loss 3.9036.
Test: 2018-08-02T14:55:16.657489: step 4150, loss 3.81863.
Train: 2018-08-02T14:55:16.720003: step 4151, loss 4.40729.
Train: 2018-08-02T14:55:16.782490: step 4152, loss 4.15545.
Train: 2018-08-02T14:55:16.844944: step 4153, loss 4.40729.
Train: 2018-08-02T14:55:16.891836: step 4154, loss 4.53321.
Train: 2018-08-02T14:55:16.954295: step 4155, loss 3.77768.
Train: 2018-08-02T14:55:17.016805: step 4156, loss 4.78506.
Train: 2018-08-02T14:55:17.063645: step 4157, loss 3.65176.
Train: 2018-08-02T14:55:17.126129: step 4158, loss 4.02952.
Train: 2018-08-02T14:55:17.188646: step 4159, loss 3.9036.
Train: 2018-08-02T14:55:17.251130: step 4160, loss 4.40729.
Test: 2018-08-02T14:55:17.454180: step 4160, loss 3.81863.
Train: 2018-08-02T14:55:17.516664: step 4161, loss 3.39991.
Train: 2018-08-02T14:55:17.563528: step 4162, loss 3.39991.
Train: 2018-08-02T14:55:17.626043: step 4163, loss 3.39991.
Train: 2018-08-02T14:55:17.688529: step 4164, loss 4.02952.
Train: 2018-08-02T14:55:17.751014: step 4165, loss 3.65176.
Train: 2018-08-02T14:55:17.813493: step 4166, loss 4.15545.
Train: 2018-08-02T14:55:17.860361: step 4167, loss 4.65914.
Train: 2018-08-02T14:55:17.922820: step 4168, loss 4.28137.
Train: 2018-08-02T14:55:17.985338: step 4169, loss 5.0369.
Train: 2018-08-02T14:55:18.032199: step 4170, loss 3.9036.
Test: 2018-08-02T14:55:18.235244: step 4170, loss 3.81863.
Train: 2018-08-02T14:55:18.297731: step 4171, loss 2.89622.
Train: 2018-08-02T14:55:18.360249: step 4172, loss 5.28875.
Train: 2018-08-02T14:55:18.407111: step 4173, loss 3.27399.
Train: 2018-08-02T14:55:18.469568: step 4174, loss 3.14807.
Train: 2018-08-02T14:55:18.532050: step 4175, loss 4.02952.
Train: 2018-08-02T14:55:18.578947: step 4176, loss 3.9036.
Train: 2018-08-02T14:55:18.657021: step 4177, loss 2.89622.
Train: 2018-08-02T14:55:18.703919: step 4178, loss 4.02952.
Train: 2018-08-02T14:55:18.766371: step 4179, loss 4.40729.
Train: 2018-08-02T14:55:18.828855: step 4180, loss 4.28137.
Test: 2018-08-02T14:55:19.016313: step 4180, loss 3.81863.
Train: 2018-08-02T14:55:19.078830: step 4181, loss 4.40729.
Train: 2018-08-02T14:55:19.141311: step 4182, loss 3.52583.
Train: 2018-08-02T14:55:19.203794: step 4183, loss 4.65914.
Train: 2018-08-02T14:55:19.266280: step 4184, loss 3.9036.
Train: 2018-08-02T14:55:19.313148: step 4185, loss 3.39991.
Train: 2018-08-02T14:55:19.375602: step 4186, loss 2.51845.
Train: 2018-08-02T14:55:19.438119: step 4187, loss 4.91098.
Train: 2018-08-02T14:55:19.484953: step 4188, loss 4.40729.
Train: 2018-08-02T14:55:19.547438: step 4189, loss 3.77768.
Train: 2018-08-02T14:55:19.609924: step 4190, loss 2.51845.
Test: 2018-08-02T14:55:19.813000: step 4190, loss 3.81863.
Train: 2018-08-02T14:55:19.875486: step 4191, loss 3.9036.
Train: 2018-08-02T14:55:19.922351: step 4192, loss 3.14807.
Train: 2018-08-02T14:55:19.984836: step 4193, loss 2.7703.
Train: 2018-08-02T14:55:20.047339: step 4194, loss 4.15545.
Train: 2018-08-02T14:55:20.109840: step 4195, loss 3.77768.
Train: 2018-08-02T14:55:20.172291: step 4196, loss 4.65914.
Train: 2018-08-02T14:55:20.219156: step 4197, loss 4.02952.
Train: 2018-08-02T14:55:20.281692: step 4198, loss 4.40729.
Train: 2018-08-02T14:55:20.344154: step 4199, loss 4.02952.
Train: 2018-08-02T14:55:20.391020: step 4200, loss 4.40729.
Test: 2018-08-02T14:55:20.594067: step 4200, loss 3.81863.
Train: 2018-08-02T14:55:21.125224: step 4201, loss 4.40729.
Train: 2018-08-02T14:55:21.187708: step 4202, loss 4.28137.
Train: 2018-08-02T14:55:21.250165: step 4203, loss 4.40729.
Train: 2018-08-02T14:55:21.312649: step 4204, loss 3.27399.
Train: 2018-08-02T14:55:21.359515: step 4205, loss 3.65176.
Train: 2018-08-02T14:55:21.421998: step 4206, loss 4.65914.
Train: 2018-08-02T14:55:21.484517: step 4207, loss 3.52583.
Train: 2018-08-02T14:55:21.531379: step 4208, loss 5.0369.
Train: 2018-08-02T14:55:21.593862: step 4209, loss 5.16283.
Train: 2018-08-02T14:55:21.656344: step 4210, loss 4.02952.
Test: 2018-08-02T14:55:21.859430: step 4210, loss 3.81863.
Train: 2018-08-02T14:55:21.906285: step 4211, loss 2.89622.
Train: 2018-08-02T14:55:21.968776: step 4212, loss 5.16283.
Train: 2018-08-02T14:55:22.031232: step 4213, loss 3.27399.
Train: 2018-08-02T14:55:22.093745: step 4214, loss 3.65176.
Train: 2018-08-02T14:55:22.156203: step 4215, loss 3.77768.
Train: 2018-08-02T14:55:22.203098: step 4216, loss 4.28137.
Train: 2018-08-02T14:55:22.265582: step 4217, loss 3.65176.
Train: 2018-08-02T14:55:22.328062: step 4218, loss 3.27399.
Train: 2018-08-02T14:55:22.374931: step 4219, loss 4.02952.
Train: 2018-08-02T14:55:22.437416: step 4220, loss 5.28875.
Test: 2018-08-02T14:55:22.640463: step 4220, loss 3.81863.
Train: 2018-08-02T14:55:22.702978: step 4221, loss 4.15545.
Train: 2018-08-02T14:55:22.749813: step 4222, loss 3.39991.
Train: 2018-08-02T14:55:22.812329: step 4223, loss 3.14807.
Train: 2018-08-02T14:55:22.874814: step 4224, loss 4.15545.
Train: 2018-08-02T14:55:22.937300: step 4225, loss 4.15545.
Train: 2018-08-02T14:55:22.984173: step 4226, loss 4.53321.
Train: 2018-08-02T14:55:23.046649: step 4227, loss 4.91098.
Train: 2018-08-02T14:55:23.093510: step 4228, loss 4.29816.
Train: 2018-08-02T14:55:23.155993: step 4229, loss 4.28137.
Train: 2018-08-02T14:55:23.202864: step 4230, loss 4.28137.
Test: 2018-08-02T14:55:23.405939: step 4230, loss 3.81863.
Train: 2018-08-02T14:55:23.468395: step 4231, loss 2.89622.
Train: 2018-08-02T14:55:23.515284: step 4232, loss 3.39991.
Train: 2018-08-02T14:55:23.577774: step 4233, loss 3.39991.
Train: 2018-08-02T14:55:23.640260: step 4234, loss 3.52583.
Train: 2018-08-02T14:55:23.702748: step 4235, loss 4.78506.
Train: 2018-08-02T14:55:23.749603: step 4236, loss 3.27399.
Train: 2018-08-02T14:55:23.812094: step 4237, loss 4.78506.
Train: 2018-08-02T14:55:23.874575: step 4238, loss 4.65914.
Train: 2018-08-02T14:55:23.921443: step 4239, loss 3.27399.
Train: 2018-08-02T14:55:23.983932: step 4240, loss 3.14807.
Test: 2018-08-02T14:55:24.187000: step 4240, loss 3.81863.
Train: 2018-08-02T14:55:24.249462: step 4241, loss 3.9036.
Train: 2018-08-02T14:55:24.311946: step 4242, loss 4.15545.
Train: 2018-08-02T14:55:24.358845: step 4243, loss 3.65176.
Train: 2018-08-02T14:55:24.421297: step 4244, loss 4.02952.
Train: 2018-08-02T14:55:24.483817: step 4245, loss 3.39991.
Train: 2018-08-02T14:55:24.546291: step 4246, loss 4.02952.
Train: 2018-08-02T14:55:24.593132: step 4247, loss 3.52583.
Train: 2018-08-02T14:55:24.655618: step 4248, loss 4.15545.
Train: 2018-08-02T14:55:24.718103: step 4249, loss 4.78506.
Train: 2018-08-02T14:55:24.764996: step 4250, loss 4.65914.
Test: 2018-08-02T14:55:24.968067: step 4250, loss 3.81863.
Train: 2018-08-02T14:55:25.030559: step 4251, loss 4.40729.
Train: 2018-08-02T14:55:25.093013: step 4252, loss 4.28137.
Train: 2018-08-02T14:55:25.139904: step 4253, loss 4.15545.
Train: 2018-08-02T14:55:25.202366: step 4254, loss 5.0369.
Train: 2018-08-02T14:55:25.264881: step 4255, loss 3.27399.
Train: 2018-08-02T14:55:25.327364: step 4256, loss 4.28137.
Train: 2018-08-02T14:55:25.374198: step 4257, loss 3.65176.
Train: 2018-08-02T14:55:25.436686: step 4258, loss 3.9036.
Train: 2018-08-02T14:55:25.499170: step 4259, loss 3.77768.
Train: 2018-08-02T14:55:25.546064: step 4260, loss 4.78506.
Test: 2018-08-02T14:55:25.749140: step 4260, loss 3.81863.
Train: 2018-08-02T14:55:25.811621: step 4261, loss 3.02214.
Train: 2018-08-02T14:55:25.874112: step 4262, loss 4.65914.
Train: 2018-08-02T14:55:25.920945: step 4263, loss 3.14807.
Train: 2018-08-02T14:55:25.983465: step 4264, loss 4.28137.
Train: 2018-08-02T14:55:26.045918: step 4265, loss 4.15545.
Train: 2018-08-02T14:55:26.108426: step 4266, loss 4.53321.
Train: 2018-08-02T14:55:26.155266: step 4267, loss 4.15545.
Train: 2018-08-02T14:55:26.217779: step 4268, loss 3.52583.
Train: 2018-08-02T14:55:26.280236: step 4269, loss 4.78506.
Train: 2018-08-02T14:55:26.327100: step 4270, loss 4.53321.
Test: 2018-08-02T14:55:26.530207: step 4270, loss 3.81863.
Train: 2018-08-02T14:55:26.592694: step 4271, loss 4.02952.
Train: 2018-08-02T14:55:26.655179: step 4272, loss 4.02952.
Train: 2018-08-02T14:55:26.717665: step 4273, loss 3.27399.
Train: 2018-08-02T14:55:26.764498: step 4274, loss 4.28137.
Train: 2018-08-02T14:55:26.826984: step 4275, loss 4.53321.
Train: 2018-08-02T14:55:26.889498: step 4276, loss 3.39991.
Train: 2018-08-02T14:55:26.936363: step 4277, loss 4.40729.
Train: 2018-08-02T14:55:26.998819: step 4278, loss 4.02952.
Train: 2018-08-02T14:55:27.061305: step 4279, loss 5.16283.
Train: 2018-08-02T14:55:27.108167: step 4280, loss 3.9036.
Test: 2018-08-02T14:55:27.311270: step 4280, loss 3.81863.
Train: 2018-08-02T14:55:27.373760: step 4281, loss 4.15545.
Train: 2018-08-02T14:55:27.436240: step 4282, loss 4.15545.
Train: 2018-08-02T14:55:27.498731: step 4283, loss 4.15545.
Train: 2018-08-02T14:55:27.545595: step 4284, loss 3.02214.
Train: 2018-08-02T14:55:27.608075: step 4285, loss 3.9036.
Train: 2018-08-02T14:55:27.670565: step 4286, loss 3.65176.
Train: 2018-08-02T14:55:27.717399: step 4287, loss 3.65176.
Train: 2018-08-02T14:55:27.779892: step 4288, loss 4.40729.
Train: 2018-08-02T14:55:27.842401: step 4289, loss 3.39991.
Train: 2018-08-02T14:55:27.904886: step 4290, loss 3.39991.
Test: 2018-08-02T14:55:28.092346: step 4290, loss 3.81863.
Train: 2018-08-02T14:55:28.154822: step 4291, loss 4.02952.
Train: 2018-08-02T14:55:28.217313: step 4292, loss 3.39991.
Train: 2018-08-02T14:55:28.279798: step 4293, loss 3.52583.
Train: 2018-08-02T14:55:28.342296: step 4294, loss 3.52583.
Train: 2018-08-02T14:55:28.389143: step 4295, loss 4.40729.
Train: 2018-08-02T14:55:28.451603: step 4296, loss 4.78506.
Train: 2018-08-02T14:55:28.514118: step 4297, loss 3.14807.
Train: 2018-08-02T14:55:28.560982: step 4298, loss 4.15545.
Train: 2018-08-02T14:55:28.623437: step 4299, loss 4.40729.
Train: 2018-08-02T14:55:28.685922: step 4300, loss 3.14807.
Test: 2018-08-02T14:55:28.873403: step 4300, loss 3.81863.
Train: 2018-08-02T14:55:29.373293: step 4301, loss 4.91098.
Train: 2018-08-02T14:55:29.467020: step 4302, loss 4.65914.
Train: 2018-08-02T14:55:29.529505: step 4303, loss 3.39991.
Train: 2018-08-02T14:55:29.591990: step 4304, loss 3.9036.
Train: 2018-08-02T14:55:29.638851: step 4305, loss 4.40729.
Train: 2018-08-02T14:55:29.701335: step 4306, loss 3.39991.
Train: 2018-08-02T14:55:29.763796: step 4307, loss 3.02214.
Train: 2018-08-02T14:55:29.810690: step 4308, loss 3.77768.
Train: 2018-08-02T14:55:29.873145: step 4309, loss 2.89622.
Train: 2018-08-02T14:55:29.935661: step 4310, loss 3.65176.
Test: 2018-08-02T14:55:30.138727: step 4310, loss 3.81863.
Train: 2018-08-02T14:55:30.232469: step 4311, loss 3.9036.
Train: 2018-08-02T14:55:30.294920: step 4312, loss 4.40729.
Train: 2018-08-02T14:55:30.341813: step 4313, loss 3.14807.
Train: 2018-08-02T14:55:30.404271: step 4314, loss 4.28137.
Train: 2018-08-02T14:55:30.466756: step 4315, loss 4.40729.
Train: 2018-08-02T14:55:30.513653: step 4316, loss 4.53321.
Train: 2018-08-02T14:55:30.576104: step 4317, loss 3.39991.
Train: 2018-08-02T14:55:30.638621: step 4318, loss 3.39991.
Train: 2018-08-02T14:55:30.701076: step 4319, loss 3.27399.
Train: 2018-08-02T14:55:30.747940: step 4320, loss 3.9036.
Test: 2018-08-02T14:55:30.951018: step 4320, loss 3.81863.
Train: 2018-08-02T14:55:31.013503: step 4321, loss 5.28875.
Train: 2018-08-02T14:55:31.075989: step 4322, loss 4.65914.
Train: 2018-08-02T14:55:31.138472: step 4323, loss 3.77768.
Train: 2018-08-02T14:55:31.185368: step 4324, loss 5.41467.
Train: 2018-08-02T14:55:31.247854: step 4325, loss 4.15545.
Train: 2018-08-02T14:55:31.310339: step 4326, loss 4.78506.
Train: 2018-08-02T14:55:31.372824: step 4327, loss 4.28137.
Train: 2018-08-02T14:55:31.419659: step 4328, loss 4.40729.
Train: 2018-08-02T14:55:31.482177: step 4329, loss 5.0369.
Train: 2018-08-02T14:55:31.544658: step 4330, loss 3.77768.
Test: 2018-08-02T14:55:31.732109: step 4330, loss 3.81863.
Train: 2018-08-02T14:55:31.794569: step 4331, loss 3.27399.
Train: 2018-08-02T14:55:31.857055: step 4332, loss 4.40729.
Train: 2018-08-02T14:55:31.919570: step 4333, loss 4.28137.
Train: 2018-08-02T14:55:31.982027: step 4334, loss 3.39991.
Train: 2018-08-02T14:55:32.028916: step 4335, loss 4.28137.
Train: 2018-08-02T14:55:32.091404: step 4336, loss 3.77768.
Train: 2018-08-02T14:55:32.153890: step 4337, loss 3.14807.
Train: 2018-08-02T14:55:32.200749: step 4338, loss 4.28137.
Train: 2018-08-02T14:55:32.263240: step 4339, loss 5.0369.
Train: 2018-08-02T14:55:32.325695: step 4340, loss 3.52583.
Test: 2018-08-02T14:55:32.528772: step 4340, loss 3.81863.
Train: 2018-08-02T14:55:32.591288: step 4341, loss 2.64438.
Train: 2018-08-02T14:55:32.638152: step 4342, loss 3.39991.
Train: 2018-08-02T14:55:32.700640: step 4343, loss 3.52583.
Train: 2018-08-02T14:55:32.763107: step 4344, loss 4.02952.
Train: 2018-08-02T14:55:32.825579: step 4345, loss 3.9036.
Train: 2018-08-02T14:55:32.888064: step 4346, loss 4.02952.
Train: 2018-08-02T14:55:32.934958: step 4347, loss 3.52583.
Train: 2018-08-02T14:55:32.997444: step 4348, loss 5.0369.
Train: 2018-08-02T14:55:33.059930: step 4349, loss 4.65914.
Train: 2018-08-02T14:55:33.106761: step 4350, loss 4.02952.
Test: 2018-08-02T14:55:33.309870: step 4350, loss 3.81863.
Train: 2018-08-02T14:55:33.372356: step 4351, loss 4.28137.
Train: 2018-08-02T14:55:33.434812: step 4352, loss 4.91098.
Train: 2018-08-02T14:55:33.497296: step 4353, loss 5.16283.
Train: 2018-08-02T14:55:33.544161: step 4354, loss 4.15545.
Train: 2018-08-02T14:55:33.606671: step 4355, loss 3.39991.
Train: 2018-08-02T14:55:33.669160: step 4356, loss 3.65176.
Train: 2018-08-02T14:55:33.716023: step 4357, loss 3.77768.
Train: 2018-08-02T14:55:33.778511: step 4358, loss 3.52583.
Train: 2018-08-02T14:55:33.840997: step 4359, loss 3.9036.
Train: 2018-08-02T14:55:33.903451: step 4360, loss 4.65914.
Test: 2018-08-02T14:55:34.090931: step 4360, loss 3.81863.
Train: 2018-08-02T14:55:34.153418: step 4361, loss 4.40729.
Train: 2018-08-02T14:55:34.215879: step 4362, loss 4.91098.
Train: 2018-08-02T14:55:34.262772: step 4363, loss 4.40729.
Train: 2018-08-02T14:55:34.325259: step 4364, loss 4.91098.
Train: 2018-08-02T14:55:34.387742: step 4365, loss 3.52583.
Train: 2018-08-02T14:55:34.450231: step 4366, loss 4.28137.
Train: 2018-08-02T14:55:34.497061: step 4367, loss 3.9036.
Train: 2018-08-02T14:55:34.559578: step 4368, loss 3.39991.
Train: 2018-08-02T14:55:34.622065: step 4369, loss 5.28875.
Train: 2018-08-02T14:55:34.668897: step 4370, loss 4.15545.
Test: 2018-08-02T14:55:34.887599: step 4370, loss 3.81863.
Train: 2018-08-02T14:55:34.934459: step 4371, loss 4.28137.
Train: 2018-08-02T14:55:34.996945: step 4372, loss 3.9036.
Train: 2018-08-02T14:55:35.059429: step 4373, loss 3.52583.
Train: 2018-08-02T14:55:35.121947: step 4374, loss 4.65914.
Train: 2018-08-02T14:55:35.184427: step 4375, loss 3.65176.
Train: 2018-08-02T14:55:35.231294: step 4376, loss 3.65176.
Train: 2018-08-02T14:55:35.293781: step 4377, loss 3.14807.
Train: 2018-08-02T14:55:35.356238: step 4378, loss 4.28137.
Train: 2018-08-02T14:55:35.387478: step 4379, loss 4.83543.
Train: 2018-08-02T14:55:35.449996: step 4380, loss 3.65176.
Test: 2018-08-02T14:55:35.653071: step 4380, loss 3.81863.
Train: 2018-08-02T14:55:35.715555: step 4381, loss 3.9036.
Train: 2018-08-02T14:55:35.778011: step 4382, loss 3.14807.
Train: 2018-08-02T14:55:35.824904: step 4383, loss 3.52583.
Train: 2018-08-02T14:55:35.887363: step 4384, loss 4.15545.
Train: 2018-08-02T14:55:35.949847: step 4385, loss 4.40729.
Train: 2018-08-02T14:55:36.012358: step 4386, loss 3.52583.
Train: 2018-08-02T14:55:36.059227: step 4387, loss 3.27399.
Train: 2018-08-02T14:55:36.121713: step 4388, loss 4.53321.
Train: 2018-08-02T14:55:36.184197: step 4389, loss 4.28137.
Train: 2018-08-02T14:55:36.231032: step 4390, loss 2.26661.
Test: 2018-08-02T14:55:36.434132: step 4390, loss 3.81863.
Train: 2018-08-02T14:55:36.496593: step 4391, loss 3.9036.
Train: 2018-08-02T14:55:36.559079: step 4392, loss 2.39253.
Train: 2018-08-02T14:55:36.621595: step 4393, loss 3.39991.
Train: 2018-08-02T14:55:36.668454: step 4394, loss 3.65176.
Train: 2018-08-02T14:55:36.730938: step 4395, loss 5.79244.
Train: 2018-08-02T14:55:36.793423: step 4396, loss 3.39991.
Train: 2018-08-02T14:55:36.840291: step 4397, loss 4.28137.
Train: 2018-08-02T14:55:36.902749: step 4398, loss 4.40729.
Train: 2018-08-02T14:55:36.965263: step 4399, loss 5.0369.
Train: 2018-08-02T14:55:37.027750: step 4400, loss 3.39991.
Test: 2018-08-02T14:55:37.215199: step 4400, loss 3.81863.
Train: 2018-08-02T14:55:37.746331: step 4401, loss 3.65176.
Train: 2018-08-02T14:55:37.808817: step 4402, loss 4.40729.
Train: 2018-08-02T14:55:37.871302: step 4403, loss 3.02214.
Train: 2018-08-02T14:55:37.933783: step 4404, loss 4.65914.
Train: 2018-08-02T14:55:37.980655: step 4405, loss 4.53321.
Train: 2018-08-02T14:55:38.043137: step 4406, loss 4.65914.
Train: 2018-08-02T14:55:38.105591: step 4407, loss 3.77768.
Train: 2018-08-02T14:55:38.168079: step 4408, loss 3.52583.
Train: 2018-08-02T14:55:38.214941: step 4409, loss 4.91098.
Train: 2018-08-02T14:55:38.277427: step 4410, loss 3.27399.
Test: 2018-08-02T14:55:38.480504: step 4410, loss 3.81863.
Train: 2018-08-02T14:55:38.543021: step 4411, loss 5.0369.
Train: 2018-08-02T14:55:38.589855: step 4412, loss 4.15545.
Train: 2018-08-02T14:55:38.652365: step 4413, loss 4.15545.
Train: 2018-08-02T14:55:38.714825: step 4414, loss 3.39991.
Train: 2018-08-02T14:55:38.792941: step 4415, loss 4.28137.
Train: 2018-08-02T14:55:38.839823: step 4416, loss 4.53321.
Train: 2018-08-02T14:55:38.902306: step 4417, loss 4.40729.
Train: 2018-08-02T14:55:38.964794: step 4418, loss 4.91098.
Train: 2018-08-02T14:55:39.011632: step 4419, loss 4.53321.
Train: 2018-08-02T14:55:39.074115: step 4420, loss 4.02952.
Test: 2018-08-02T14:55:39.277223: step 4420, loss 3.81863.
Train: 2018-08-02T14:55:39.339677: step 4421, loss 3.77768.
Train: 2018-08-02T14:55:39.402194: step 4422, loss 4.53321.
Train: 2018-08-02T14:55:39.449028: step 4423, loss 2.89622.
Train: 2018-08-02T14:55:39.511515: step 4424, loss 4.02952.
Train: 2018-08-02T14:55:39.574028: step 4425, loss 3.39991.
Train: 2018-08-02T14:55:39.620892: step 4426, loss 3.52583.
Train: 2018-08-02T14:55:39.683378: step 4427, loss 3.02214.
Train: 2018-08-02T14:55:39.745834: step 4428, loss 4.53321.
Train: 2018-08-02T14:55:39.808349: step 4429, loss 4.15545.
Train: 2018-08-02T14:55:39.855206: step 4430, loss 3.77768.
Test: 2018-08-02T14:55:40.058283: step 4430, loss 3.81863.
Train: 2018-08-02T14:55:40.120745: step 4431, loss 4.02952.
Train: 2018-08-02T14:55:40.183231: step 4432, loss 2.51845.
Train: 2018-08-02T14:55:40.230094: step 4433, loss 4.91098.
Train: 2018-08-02T14:55:40.292611: step 4434, loss 3.27399.
Train: 2018-08-02T14:55:40.355066: step 4435, loss 3.39991.
Train: 2018-08-02T14:55:40.401959: step 4436, loss 3.02214.
Train: 2018-08-02T14:55:40.464442: step 4437, loss 3.9036.
Train: 2018-08-02T14:55:40.526932: step 4438, loss 3.65176.
Train: 2018-08-02T14:55:40.573763: step 4439, loss 4.65914.
Train: 2018-08-02T14:55:40.636251: step 4440, loss 4.40729.
Test: 2018-08-02T14:55:40.839359: step 4440, loss 3.81863.
Train: 2018-08-02T14:55:40.886221: step 4441, loss 3.52583.
Train: 2018-08-02T14:55:40.948709: step 4442, loss 5.28875.
Train: 2018-08-02T14:55:41.011163: step 4443, loss 3.14807.
Train: 2018-08-02T14:55:41.073681: step 4444, loss 3.65176.
Train: 2018-08-02T14:55:41.120510: step 4445, loss 3.65176.
Train: 2018-08-02T14:55:41.183030: step 4446, loss 2.51845.
Train: 2018-08-02T14:55:41.245482: step 4447, loss 3.27399.
Train: 2018-08-02T14:55:41.292376: step 4448, loss 4.53321.
Train: 2018-08-02T14:55:41.354831: step 4449, loss 4.78506.
Train: 2018-08-02T14:55:41.417315: step 4450, loss 4.53321.
Test: 2018-08-02T14:55:41.620394: step 4450, loss 3.81863.
Train: 2018-08-02T14:55:41.667289: step 4451, loss 3.14807.
Train: 2018-08-02T14:55:41.729776: step 4452, loss 4.40729.
Train: 2018-08-02T14:55:41.792262: step 4453, loss 4.28137.
Train: 2018-08-02T14:55:41.854748: step 4454, loss 3.14807.
Train: 2018-08-02T14:55:41.917231: step 4455, loss 4.78506.
Train: 2018-08-02T14:55:41.964063: step 4456, loss 5.5406.
Train: 2018-08-02T14:55:42.026549: step 4457, loss 4.15545.
Train: 2018-08-02T14:55:42.089068: step 4458, loss 4.28137.
Train: 2018-08-02T14:55:42.135928: step 4459, loss 4.53321.
Train: 2018-08-02T14:55:42.198414: step 4460, loss 4.91098.
Test: 2018-08-02T14:55:42.401491: step 4460, loss 3.81863.
Train: 2018-08-02T14:55:42.463977: step 4461, loss 4.02952.
Train: 2018-08-02T14:55:42.510842: step 4462, loss 4.65914.
Train: 2018-08-02T14:55:42.573329: step 4463, loss 4.15545.
Train: 2018-08-02T14:55:42.635815: step 4464, loss 4.15545.
Train: 2018-08-02T14:55:42.698297: step 4465, loss 4.53321.
Train: 2018-08-02T14:55:42.760785: step 4466, loss 4.53321.
Train: 2018-08-02T14:55:42.807642: step 4467, loss 5.0369.
Train: 2018-08-02T14:55:42.870138: step 4468, loss 4.78506.
Train: 2018-08-02T14:55:42.932612: step 4469, loss 3.77768.
Train: 2018-08-02T14:55:42.979484: step 4470, loss 4.15545.
Test: 2018-08-02T14:55:43.182553: step 4470, loss 3.81863.
Train: 2018-08-02T14:55:43.245014: step 4471, loss 3.39991.
Train: 2018-08-02T14:55:43.291908: step 4472, loss 3.9036.
Train: 2018-08-02T14:55:43.354396: step 4473, loss 4.78506.
Train: 2018-08-02T14:55:43.416879: step 4474, loss 3.27399.
Train: 2018-08-02T14:55:43.479334: step 4475, loss 4.15545.
Train: 2018-08-02T14:55:43.526234: step 4476, loss 4.40729.
Train: 2018-08-02T14:55:43.588682: step 4477, loss 3.52583.
Train: 2018-08-02T14:55:43.651195: step 4478, loss 3.77768.
Train: 2018-08-02T14:55:43.698065: step 4479, loss 3.14807.
Train: 2018-08-02T14:55:43.760548: step 4480, loss 3.27399.
Test: 2018-08-02T14:55:43.963596: step 4480, loss 3.81863.
Train: 2018-08-02T14:55:44.026105: step 4481, loss 4.02952.
Train: 2018-08-02T14:55:44.088596: step 4482, loss 4.28137.
Train: 2018-08-02T14:55:44.135460: step 4483, loss 4.15545.
Train: 2018-08-02T14:55:44.197941: step 4484, loss 3.77768.
Train: 2018-08-02T14:55:44.260431: step 4485, loss 4.15545.
Train: 2018-08-02T14:55:44.322915: step 4486, loss 3.65176.
Train: 2018-08-02T14:55:44.369749: step 4487, loss 3.65176.
Train: 2018-08-02T14:55:44.432266: step 4488, loss 3.77768.
Train: 2018-08-02T14:55:44.494758: step 4489, loss 4.28137.
Train: 2018-08-02T14:55:44.541610: step 4490, loss 5.41467.
Test: 2018-08-02T14:55:44.744692: step 4490, loss 3.81863.
Train: 2018-08-02T14:55:44.807147: step 4491, loss 3.65176.
Train: 2018-08-02T14:55:44.869663: step 4492, loss 4.15545.
Train: 2018-08-02T14:55:44.932149: step 4493, loss 4.15545.
Train: 2018-08-02T14:55:44.978982: step 4494, loss 3.27399.
Train: 2018-08-02T14:55:45.041499: step 4495, loss 3.77768.
Train: 2018-08-02T14:55:45.103952: step 4496, loss 4.02952.
Train: 2018-08-02T14:55:45.166469: step 4497, loss 3.9036.
Train: 2018-08-02T14:55:45.213302: step 4498, loss 3.14807.
Train: 2018-08-02T14:55:45.275790: step 4499, loss 4.53321.
Train: 2018-08-02T14:55:45.338274: step 4500, loss 4.15545.
Test: 2018-08-02T14:55:45.525753: step 4500, loss 3.81863.
Train: 2018-08-02T14:55:46.025613: step 4501, loss 4.65914.
Train: 2018-08-02T14:55:46.088099: step 4502, loss 2.89622.
Train: 2018-08-02T14:55:46.134992: step 4503, loss 3.52583.
Train: 2018-08-02T14:55:46.197484: step 4504, loss 3.77768.
Train: 2018-08-02T14:55:46.259963: step 4505, loss 4.02952.
Train: 2018-08-02T14:55:46.306822: step 4506, loss 4.02952.
Train: 2018-08-02T14:55:46.369311: step 4507, loss 4.78506.
Train: 2018-08-02T14:55:46.431798: step 4508, loss 4.40729.
Train: 2018-08-02T14:55:46.478631: step 4509, loss 3.27399.
Train: 2018-08-02T14:55:46.541116: step 4510, loss 4.15545.
Test: 2018-08-02T14:55:46.744224: step 4510, loss 3.81863.
Train: 2018-08-02T14:55:46.806680: step 4511, loss 3.52583.
Train: 2018-08-02T14:55:46.853544: step 4512, loss 4.53321.
Train: 2018-08-02T14:55:46.916059: step 4513, loss 4.91098.
Train: 2018-08-02T14:55:46.978548: step 4514, loss 4.02952.
Train: 2018-08-02T14:55:47.040999: step 4515, loss 4.15545.
Train: 2018-08-02T14:55:47.087888: step 4516, loss 3.65176.
Train: 2018-08-02T14:55:47.150379: step 4517, loss 3.52583.
Train: 2018-08-02T14:55:47.212863: step 4518, loss 4.02952.
Train: 2018-08-02T14:55:47.259698: step 4519, loss 4.91098.
Train: 2018-08-02T14:55:47.322209: step 4520, loss 4.02952.
Test: 2018-08-02T14:55:47.525262: step 4520, loss 3.81863.
Train: 2018-08-02T14:55:47.587777: step 4521, loss 4.02952.
Train: 2018-08-02T14:55:47.634640: step 4522, loss 4.53321.
Train: 2018-08-02T14:55:47.697095: step 4523, loss 4.02952.
Train: 2018-08-02T14:55:47.759612: step 4524, loss 3.39991.
Train: 2018-08-02T14:55:47.822067: step 4525, loss 4.65914.
Train: 2018-08-02T14:55:47.868930: step 4526, loss 4.65914.
Train: 2018-08-02T14:55:47.931443: step 4527, loss 4.02952.
Train: 2018-08-02T14:55:47.993930: step 4528, loss 4.40729.
Train: 2018-08-02T14:55:48.040766: step 4529, loss 4.40729.
Train: 2018-08-02T14:55:48.087658: step 4530, loss 4.29816.
Test: 2018-08-02T14:55:48.290732: step 4530, loss 3.81863.
Train: 2018-08-02T14:55:48.353221: step 4531, loss 3.52583.
Train: 2018-08-02T14:55:48.415702: step 4532, loss 5.0369.
Train: 2018-08-02T14:55:48.478196: step 4533, loss 4.28137.
Train: 2018-08-02T14:55:48.525059: step 4534, loss 3.02214.
Train: 2018-08-02T14:55:48.587541: step 4535, loss 4.15545.
Train: 2018-08-02T14:55:48.650028: step 4536, loss 2.89622.
Train: 2018-08-02T14:55:48.696863: step 4537, loss 3.65176.
Train: 2018-08-02T14:55:48.759349: step 4538, loss 2.89622.
Train: 2018-08-02T14:55:48.821835: step 4539, loss 4.02952.
Train: 2018-08-02T14:55:48.884343: step 4540, loss 3.77768.
Test: 2018-08-02T14:55:49.071804: step 4540, loss 3.81863.
Train: 2018-08-02T14:55:49.134286: step 4541, loss 4.53321.
Train: 2018-08-02T14:55:49.196745: step 4542, loss 3.9036.
Train: 2018-08-02T14:55:49.259260: step 4543, loss 4.53321.
Train: 2018-08-02T14:55:49.306096: step 4544, loss 4.15545.
Train: 2018-08-02T14:55:49.368582: step 4545, loss 4.53321.
Train: 2018-08-02T14:55:49.431094: step 4546, loss 3.9036.
Train: 2018-08-02T14:55:49.477963: step 4547, loss 4.02952.
Train: 2018-08-02T14:55:49.540445: step 4548, loss 5.16283.
Train: 2018-08-02T14:55:49.602934: step 4549, loss 4.15545.
Train: 2018-08-02T14:55:49.665386: step 4550, loss 4.40729.
Test: 2018-08-02T14:55:49.852841: step 4550, loss 3.81863.
Train: 2018-08-02T14:55:49.915357: step 4551, loss 4.28137.
Train: 2018-08-02T14:55:49.977845: step 4552, loss 3.14807.
Train: 2018-08-02T14:55:50.040297: step 4553, loss 3.65176.
Train: 2018-08-02T14:55:50.087190: step 4554, loss 4.40729.
Train: 2018-08-02T14:55:50.149648: step 4555, loss 3.9036.
Train: 2018-08-02T14:55:50.212166: step 4556, loss 4.28137.
Train: 2018-08-02T14:55:50.258997: step 4557, loss 3.77768.
Train: 2018-08-02T14:55:50.321514: step 4558, loss 3.39991.
Train: 2018-08-02T14:55:50.383967: step 4559, loss 3.77768.
Train: 2018-08-02T14:55:50.446485: step 4560, loss 4.78506.
Test: 2018-08-02T14:55:50.633907: step 4560, loss 3.81863.
Train: 2018-08-02T14:55:50.696393: step 4561, loss 4.91098.
Train: 2018-08-02T14:55:50.758879: step 4562, loss 4.40729.
Train: 2018-08-02T14:55:50.821394: step 4563, loss 5.16283.
Train: 2018-08-02T14:55:50.868229: step 4564, loss 3.77768.
Train: 2018-08-02T14:55:50.930744: step 4565, loss 3.52583.
Train: 2018-08-02T14:55:50.993244: step 4566, loss 4.15545.
Train: 2018-08-02T14:55:51.040097: step 4567, loss 4.65914.
Train: 2018-08-02T14:55:51.102556: step 4568, loss 3.39991.
Train: 2018-08-02T14:55:51.165034: step 4569, loss 3.77768.
Train: 2018-08-02T14:55:51.227520: step 4570, loss 4.28137.
Test: 2018-08-02T14:55:51.414975: step 4570, loss 3.81863.
Train: 2018-08-02T14:55:51.477494: step 4571, loss 3.27399.
Train: 2018-08-02T14:55:51.539976: step 4572, loss 5.0369.
Train: 2018-08-02T14:55:51.602431: step 4573, loss 5.41467.
Train: 2018-08-02T14:55:51.649297: step 4574, loss 4.40729.
Train: 2018-08-02T14:55:51.711809: step 4575, loss 4.40729.
Train: 2018-08-02T14:55:51.774296: step 4576, loss 3.65176.
Train: 2018-08-02T14:55:51.821130: step 4577, loss 3.39991.
Train: 2018-08-02T14:55:51.883618: step 4578, loss 4.91098.
Train: 2018-08-02T14:55:51.946135: step 4579, loss 3.77768.
Train: 2018-08-02T14:55:51.992965: step 4580, loss 4.15545.
Test: 2018-08-02T14:55:52.196042: step 4580, loss 3.81863.
Train: 2018-08-02T14:55:52.258557: step 4581, loss 4.15545.
Train: 2018-08-02T14:55:52.321042: step 4582, loss 4.28137.
Train: 2018-08-02T14:55:52.367908: step 4583, loss 4.15545.
Train: 2018-08-02T14:55:52.430364: step 4584, loss 4.02952.
Train: 2018-08-02T14:55:52.492864: step 4585, loss 4.28137.
Train: 2018-08-02T14:55:52.555333: step 4586, loss 3.9036.
Train: 2018-08-02T14:55:52.602226: step 4587, loss 3.40533.
Train: 2018-08-02T14:55:52.664713: step 4588, loss 2.64438.
Train: 2018-08-02T14:55:52.727168: step 4589, loss 3.77768.
Train: 2018-08-02T14:55:52.774032: step 4590, loss 3.77768.
Test: 2018-08-02T14:55:52.977108: step 4590, loss 3.81863.
Train: 2018-08-02T14:55:53.039625: step 4591, loss 3.77768.
Train: 2018-08-02T14:55:53.102113: step 4592, loss 4.15545.
Train: 2018-08-02T14:55:53.164566: step 4593, loss 4.02952.
Train: 2018-08-02T14:55:53.211431: step 4594, loss 4.40729.
Train: 2018-08-02T14:55:53.273947: step 4595, loss 3.77768.
Train: 2018-08-02T14:55:53.336428: step 4596, loss 3.39991.
Train: 2018-08-02T14:55:53.383294: step 4597, loss 4.15545.
Train: 2018-08-02T14:55:53.445783: step 4598, loss 3.39991.
Train: 2018-08-02T14:55:53.508237: step 4599, loss 3.65176.
Train: 2018-08-02T14:55:53.570721: step 4600, loss 2.89622.
Test: 2018-08-02T14:55:53.758176: step 4600, loss 3.81863.
Train: 2018-08-02T14:55:54.242469: step 4601, loss 4.53321.
Train: 2018-08-02T14:55:54.304953: step 4602, loss 3.39991.
Train: 2018-08-02T14:55:54.367439: step 4603, loss 3.52583.
Train: 2018-08-02T14:55:54.429928: step 4604, loss 4.53321.
Train: 2018-08-02T14:55:54.492404: step 4605, loss 3.39991.
Train: 2018-08-02T14:55:54.539244: step 4606, loss 3.9036.
Train: 2018-08-02T14:55:54.601762: step 4607, loss 4.40729.
Train: 2018-08-02T14:55:54.664246: step 4608, loss 4.65914.
Train: 2018-08-02T14:55:54.711106: step 4609, loss 4.15545.
Train: 2018-08-02T14:55:54.773590: step 4610, loss 3.39991.
Test: 2018-08-02T14:55:54.976641: step 4610, loss 3.81863.
Train: 2018-08-02T14:55:55.039157: step 4611, loss 4.15545.
Train: 2018-08-02T14:55:55.085990: step 4612, loss 4.53321.
Train: 2018-08-02T14:55:55.148475: step 4613, loss 4.78506.
Train: 2018-08-02T14:55:55.210990: step 4614, loss 5.5406.
Train: 2018-08-02T14:55:55.273477: step 4615, loss 4.15545.
Train: 2018-08-02T14:55:55.335962: step 4616, loss 3.65176.
Train: 2018-08-02T14:55:55.382797: step 4617, loss 3.9036.
Train: 2018-08-02T14:55:55.445283: step 4618, loss 3.9036.
Train: 2018-08-02T14:55:55.507792: step 4619, loss 3.02214.
Train: 2018-08-02T14:55:55.554663: step 4620, loss 4.28137.
Test: 2018-08-02T14:55:55.757738: step 4620, loss 3.81863.
Train: 2018-08-02T14:55:55.820194: step 4621, loss 3.77768.
Train: 2018-08-02T14:55:55.867059: step 4622, loss 4.78506.
Train: 2018-08-02T14:55:55.929543: step 4623, loss 3.39991.
Train: 2018-08-02T14:55:55.992030: step 4624, loss 3.77768.
Train: 2018-08-02T14:55:56.054514: step 4625, loss 3.27399.
Train: 2018-08-02T14:55:56.101411: step 4626, loss 4.28137.
Train: 2018-08-02T14:55:56.163862: step 4627, loss 3.9036.
Train: 2018-08-02T14:55:56.226350: step 4628, loss 4.78506.
Train: 2018-08-02T14:55:56.273243: step 4629, loss 4.53321.
Train: 2018-08-02T14:55:56.335726: step 4630, loss 4.65914.
Test: 2018-08-02T14:55:56.538806: step 4630, loss 3.81863.
Train: 2018-08-02T14:55:56.601291: step 4631, loss 3.9036.
Train: 2018-08-02T14:55:56.648155: step 4632, loss 3.77768.
Train: 2018-08-02T14:55:56.710637: step 4633, loss 5.0369.
Train: 2018-08-02T14:55:56.773127: step 4634, loss 4.53321.
Train: 2018-08-02T14:55:56.835613: step 4635, loss 3.77768.
Train: 2018-08-02T14:55:56.882475: step 4636, loss 4.40729.
Train: 2018-08-02T14:55:56.944966: step 4637, loss 4.02952.
Train: 2018-08-02T14:55:57.007417: step 4638, loss 4.28137.
Train: 2018-08-02T14:55:57.069902: step 4639, loss 3.02214.
Train: 2018-08-02T14:55:57.116795: step 4640, loss 4.02952.
Test: 2018-08-02T14:55:57.319867: step 4640, loss 3.81863.
Train: 2018-08-02T14:55:57.382353: step 4641, loss 4.28137.
Train: 2018-08-02T14:55:57.444846: step 4642, loss 3.14807.
Train: 2018-08-02T14:55:57.507328: step 4643, loss 4.40729.
Train: 2018-08-02T14:55:57.569784: step 4644, loss 4.02952.
Train: 2018-08-02T14:55:57.616679: step 4645, loss 3.39991.
Train: 2018-08-02T14:55:57.679134: step 4646, loss 5.79244.
Train: 2018-08-02T14:55:57.741648: step 4647, loss 3.9036.
Train: 2018-08-02T14:55:57.788508: step 4648, loss 3.77768.
Train: 2018-08-02T14:55:57.851002: step 4649, loss 4.28137.
Train: 2018-08-02T14:55:57.913453: step 4650, loss 5.16283.
Test: 2018-08-02T14:55:58.100910: step 4650, loss 3.81863.
Train: 2018-08-02T14:55:58.163395: step 4651, loss 3.39991.
Train: 2018-08-02T14:55:58.225881: step 4652, loss 4.15545.
Train: 2018-08-02T14:55:58.272745: step 4653, loss 4.65914.
Train: 2018-08-02T14:55:58.335231: step 4654, loss 4.78506.
Train: 2018-08-02T14:55:58.397746: step 4655, loss 3.39991.
Train: 2018-08-02T14:55:58.460231: step 4656, loss 4.28137.
Train: 2018-08-02T14:55:58.507064: step 4657, loss 3.9036.
Train: 2018-08-02T14:55:58.569584: step 4658, loss 4.28137.
Train: 2018-08-02T14:55:58.632066: step 4659, loss 3.77768.
Train: 2018-08-02T14:55:58.678899: step 4660, loss 3.02214.
Test: 2018-08-02T14:55:58.882006: step 4660, loss 3.81863.
Train: 2018-08-02T14:55:58.944462: step 4661, loss 3.77768.
Train: 2018-08-02T14:55:59.006977: step 4662, loss 4.02952.
Train: 2018-08-02T14:55:59.053842: step 4663, loss 4.15545.
Train: 2018-08-02T14:55:59.116328: step 4664, loss 4.40729.
Train: 2018-08-02T14:55:59.178808: step 4665, loss 4.91098.
Train: 2018-08-02T14:55:59.241297: step 4666, loss 2.89622.
Train: 2018-08-02T14:55:59.288162: step 4667, loss 3.65176.
Train: 2018-08-02T14:55:59.350621: step 4668, loss 3.9036.
Train: 2018-08-02T14:55:59.413121: step 4669, loss 3.65176.
Train: 2018-08-02T14:55:59.475621: step 4670, loss 3.40529.
Test: 2018-08-02T14:55:59.663043: step 4670, loss 3.81863.
Train: 2018-08-02T14:55:59.725529: step 4671, loss 3.77768.
Train: 2018-08-02T14:55:59.788015: step 4672, loss 3.9036.
Train: 2018-08-02T14:55:59.850533: step 4673, loss 3.52583.
Train: 2018-08-02T14:55:59.913012: step 4674, loss 3.65176.
Train: 2018-08-02T14:55:59.975471: step 4675, loss 3.52583.
Train: 2018-08-02T14:56:00.037958: step 4676, loss 4.40729.
Train: 2018-08-02T14:56:00.084848: step 4677, loss 3.27399.
Train: 2018-08-02T14:56:00.147331: step 4678, loss 3.9036.
Train: 2018-08-02T14:56:00.209791: step 4679, loss 4.28137.
Train: 2018-08-02T14:56:00.256680: step 4680, loss 3.52583.
Test: 2018-08-02T14:56:00.459765: step 4680, loss 3.81863.
Train: 2018-08-02T14:56:00.506626: step 4681, loss 2.68635.
Train: 2018-08-02T14:56:00.569108: step 4682, loss 4.28137.
Train: 2018-08-02T14:56:00.615947: step 4683, loss 5.28875.
Train: 2018-08-02T14:56:00.678431: step 4684, loss 4.91098.
Train: 2018-08-02T14:56:00.740946: step 4685, loss 4.15545.
Train: 2018-08-02T14:56:00.803431: step 4686, loss 3.77768.
Train: 2018-08-02T14:56:00.850265: step 4687, loss 3.9036.
Train: 2018-08-02T14:56:00.912782: step 4688, loss 3.14807.
Train: 2018-08-02T14:56:00.975237: step 4689, loss 3.39991.
Train: 2018-08-02T14:56:01.037751: step 4690, loss 3.65176.
Test: 2018-08-02T14:56:01.225177: step 4690, loss 3.81863.
Train: 2018-08-02T14:56:01.287694: step 4691, loss 5.16283.
Train: 2018-08-02T14:56:01.350149: step 4692, loss 4.02952.
Train: 2018-08-02T14:56:01.412634: step 4693, loss 4.02952.
Train: 2018-08-02T14:56:01.475119: step 4694, loss 4.53321.
Train: 2018-08-02T14:56:01.521985: step 4695, loss 4.02952.
Train: 2018-08-02T14:56:01.584496: step 4696, loss 3.9036.
Train: 2018-08-02T14:56:01.646984: step 4697, loss 4.28137.
Train: 2018-08-02T14:56:01.693817: step 4698, loss 4.28137.
Train: 2018-08-02T14:56:01.756304: step 4699, loss 3.77768.
Train: 2018-08-02T14:56:01.818819: step 4700, loss 3.27399.
Test: 2018-08-02T14:56:02.021868: step 4700, loss 3.81863.
Train: 2018-08-02T14:56:02.506128: step 4701, loss 3.14807.
Train: 2018-08-02T14:56:02.568644: step 4702, loss 3.9036.
Train: 2018-08-02T14:56:02.631098: step 4703, loss 3.02214.
Train: 2018-08-02T14:56:02.693617: step 4704, loss 3.65176.
Train: 2018-08-02T14:56:02.740478: step 4705, loss 3.39991.
Train: 2018-08-02T14:56:02.802964: step 4706, loss 4.02952.
Train: 2018-08-02T14:56:02.865419: step 4707, loss 3.77768.
Train: 2018-08-02T14:56:02.927905: step 4708, loss 4.28137.
Train: 2018-08-02T14:56:02.974769: step 4709, loss 3.52583.
Train: 2018-08-02T14:56:03.037252: step 4710, loss 4.15545.
Test: 2018-08-02T14:56:03.240356: step 4710, loss 3.81863.
Train: 2018-08-02T14:56:03.302847: step 4711, loss 4.15545.
Train: 2018-08-02T14:56:03.349680: step 4712, loss 4.28137.
Train: 2018-08-02T14:56:03.412196: step 4713, loss 4.28137.
Train: 2018-08-02T14:56:03.474688: step 4714, loss 4.65914.
Train: 2018-08-02T14:56:03.537136: step 4715, loss 3.9036.
Train: 2018-08-02T14:56:03.584029: step 4716, loss 4.40729.
Train: 2018-08-02T14:56:03.646517: step 4717, loss 4.40729.
Train: 2018-08-02T14:56:03.709001: step 4718, loss 4.02952.
Train: 2018-08-02T14:56:03.771487: step 4719, loss 3.52583.
Train: 2018-08-02T14:56:03.818354: step 4720, loss 4.02952.
Test: 2018-08-02T14:56:04.021397: step 4720, loss 3.81863.
Train: 2018-08-02T14:56:04.083913: step 4721, loss 5.16283.
Train: 2018-08-02T14:56:04.146369: step 4722, loss 3.52583.
Train: 2018-08-02T14:56:04.208884: step 4723, loss 5.28875.
Train: 2018-08-02T14:56:04.271370: step 4724, loss 3.65176.
Train: 2018-08-02T14:56:04.318234: step 4725, loss 5.41467.
Train: 2018-08-02T14:56:04.380719: step 4726, loss 3.65176.
Train: 2018-08-02T14:56:04.443204: step 4727, loss 3.27399.
Train: 2018-08-02T14:56:04.490063: step 4728, loss 4.78506.
Train: 2018-08-02T14:56:04.552549: step 4729, loss 3.9036.
Train: 2018-08-02T14:56:04.615039: step 4730, loss 4.15545.
Test: 2018-08-02T14:56:04.802496: step 4730, loss 3.81863.
Train: 2018-08-02T14:56:04.864981: step 4731, loss 3.52583.
Train: 2018-08-02T14:56:04.927462: step 4732, loss 4.28137.
Train: 2018-08-02T14:56:04.989951: step 4733, loss 3.9036.
Train: 2018-08-02T14:56:05.052408: step 4734, loss 3.39991.
Train: 2018-08-02T14:56:05.099305: step 4735, loss 4.78506.
Train: 2018-08-02T14:56:05.161757: step 4736, loss 4.65914.
Train: 2018-08-02T14:56:05.208647: step 4737, loss 5.0369.
Train: 2018-08-02T14:56:05.271130: step 4738, loss 3.14807.
Train: 2018-08-02T14:56:05.333619: step 4739, loss 4.91098.
Train: 2018-08-02T14:56:05.396101: step 4740, loss 4.15545.
Test: 2018-08-02T14:56:05.599155: step 4740, loss 3.81863.
Train: 2018-08-02T14:56:05.661668: step 4741, loss 5.0369.
Train: 2018-08-02T14:56:05.708530: step 4742, loss 3.9036.
Train: 2018-08-02T14:56:05.770989: step 4743, loss 3.02214.
Train: 2018-08-02T14:56:05.833501: step 4744, loss 3.65176.
Train: 2018-08-02T14:56:05.895991: step 4745, loss 3.27399.
Train: 2018-08-02T14:56:05.942853: step 4746, loss 3.27399.
Train: 2018-08-02T14:56:06.005339: step 4747, loss 4.02952.
Train: 2018-08-02T14:56:06.067827: step 4748, loss 4.15545.
Train: 2018-08-02T14:56:06.114688: step 4749, loss 4.15545.
Train: 2018-08-02T14:56:06.177174: step 4750, loss 3.9036.
Test: 2018-08-02T14:56:06.380220: step 4750, loss 3.81863.
Train: 2018-08-02T14:56:06.427115: step 4751, loss 4.65914.
Train: 2018-08-02T14:56:06.489571: step 4752, loss 2.89622.
Train: 2018-08-02T14:56:06.552085: step 4753, loss 4.78506.
Train: 2018-08-02T14:56:06.614574: step 4754, loss 4.40729.
Train: 2018-08-02T14:56:06.677058: step 4755, loss 3.77768.
Train: 2018-08-02T14:56:06.739542: step 4756, loss 3.9036.
Train: 2018-08-02T14:56:06.786376: step 4757, loss 4.28137.
Train: 2018-08-02T14:56:06.848895: step 4758, loss 4.15545.
Train: 2018-08-02T14:56:06.911348: step 4759, loss 4.78506.
Train: 2018-08-02T14:56:06.958242: step 4760, loss 4.15545.
Test: 2018-08-02T14:56:07.161288: step 4760, loss 3.81863.
Train: 2018-08-02T14:56:07.223773: step 4761, loss 4.40729.
Train: 2018-08-02T14:56:07.270638: step 4762, loss 4.02952.
Train: 2018-08-02T14:56:07.333150: step 4763, loss 3.27399.
Train: 2018-08-02T14:56:07.395639: step 4764, loss 4.53321.
Train: 2018-08-02T14:56:07.458123: step 4765, loss 3.77768.
Train: 2018-08-02T14:56:07.504988: step 4766, loss 4.15545.
Train: 2018-08-02T14:56:07.567472: step 4767, loss 3.39991.
Train: 2018-08-02T14:56:07.629959: step 4768, loss 3.65176.
Train: 2018-08-02T14:56:07.676824: step 4769, loss 3.77768.
Train: 2018-08-02T14:56:07.739308: step 4770, loss 3.9036.
Test: 2018-08-02T14:56:07.942355: step 4770, loss 3.81863.
Train: 2018-08-02T14:56:08.004841: step 4771, loss 3.52583.
Train: 2018-08-02T14:56:08.067325: step 4772, loss 4.15545.
Train: 2018-08-02T14:56:08.114220: step 4773, loss 3.39991.
Train: 2018-08-02T14:56:08.176707: step 4774, loss 4.53321.
Train: 2018-08-02T14:56:08.239192: step 4775, loss 4.78506.
Train: 2018-08-02T14:56:08.286055: step 4776, loss 4.02952.
Train: 2018-08-02T14:56:08.348509: step 4777, loss 4.02952.
Train: 2018-08-02T14:56:08.411027: step 4778, loss 2.64438.
Train: 2018-08-02T14:56:08.473516: step 4779, loss 3.27399.
Train: 2018-08-02T14:56:08.520375: step 4780, loss 3.52583.
Test: 2018-08-02T14:56:08.723422: step 4780, loss 3.81863.
Train: 2018-08-02T14:56:08.785932: step 4781, loss 4.53321.
Train: 2018-08-02T14:56:08.848392: step 4782, loss 4.02952.
Train: 2018-08-02T14:56:08.910877: step 4783, loss 4.65914.
Train: 2018-08-02T14:56:08.957744: step 4784, loss 3.27399.
Train: 2018-08-02T14:56:09.020227: step 4785, loss 4.65914.
Train: 2018-08-02T14:56:09.082743: step 4786, loss 3.65176.
Train: 2018-08-02T14:56:09.145228: step 4787, loss 4.78506.
Train: 2018-08-02T14:56:09.192064: step 4788, loss 2.64438.
Train: 2018-08-02T14:56:09.254579: step 4789, loss 3.39991.
Train: 2018-08-02T14:56:09.317034: step 4790, loss 4.28137.
Test: 2018-08-02T14:56:09.504514: step 4790, loss 3.81863.
Train: 2018-08-02T14:56:09.567005: step 4791, loss 4.40729.
Train: 2018-08-02T14:56:09.629490: step 4792, loss 4.28137.
Train: 2018-08-02T14:56:09.691944: step 4793, loss 3.27399.
Train: 2018-08-02T14:56:09.738840: step 4794, loss 5.0369.
Train: 2018-08-02T14:56:09.801324: step 4795, loss 3.65176.
Train: 2018-08-02T14:56:09.863811: step 4796, loss 4.02952.
Train: 2018-08-02T14:56:09.910644: step 4797, loss 3.39991.
Train: 2018-08-02T14:56:09.973164: step 4798, loss 4.02952.
Train: 2018-08-02T14:56:10.035646: step 4799, loss 4.53321.
Train: 2018-08-02T14:56:10.098128: step 4800, loss 3.52583.
Test: 2018-08-02T14:56:10.285586: step 4800, loss 3.81863.
Train: 2018-08-02T14:56:10.785470: step 4801, loss 3.39991.
Train: 2018-08-02T14:56:10.847924: step 4802, loss 3.52583.
Train: 2018-08-02T14:56:10.910409: step 4803, loss 3.77768.
Train: 2018-08-02T14:56:10.957276: step 4804, loss 4.79048.
Train: 2018-08-02T14:56:11.019789: step 4805, loss 3.27399.
Train: 2018-08-02T14:56:11.082244: step 4806, loss 4.15545.
Train: 2018-08-02T14:56:11.144760: step 4807, loss 4.15545.
Train: 2018-08-02T14:56:11.191628: step 4808, loss 4.40729.
Train: 2018-08-02T14:56:11.254105: step 4809, loss 4.02952.
Train: 2018-08-02T14:56:11.316590: step 4810, loss 3.65176.
Test: 2018-08-02T14:56:11.504051: step 4810, loss 3.81863.
Train: 2018-08-02T14:56:11.566506: step 4811, loss 3.65176.
Train: 2018-08-02T14:56:11.628992: step 4812, loss 4.40729.
Train: 2018-08-02T14:56:11.691507: step 4813, loss 4.78506.
Train: 2018-08-02T14:56:11.738344: step 4814, loss 3.77768.
Train: 2018-08-02T14:56:11.800852: step 4815, loss 3.77768.
Train: 2018-08-02T14:56:11.863342: step 4816, loss 3.39991.
Train: 2018-08-02T14:56:11.910206: step 4817, loss 4.15545.
Train: 2018-08-02T14:56:11.972695: step 4818, loss 4.91098.
Train: 2018-08-02T14:56:12.035147: step 4819, loss 3.77768.
Train: 2018-08-02T14:56:12.082010: step 4820, loss 3.02214.
Test: 2018-08-02T14:56:12.285112: step 4820, loss 3.81863.
Train: 2018-08-02T14:56:12.347598: step 4821, loss 4.02952.
Train: 2018-08-02T14:56:12.410088: step 4822, loss 4.28137.
Train: 2018-08-02T14:56:12.456922: step 4823, loss 3.77768.
Train: 2018-08-02T14:56:12.519434: step 4824, loss 3.77768.
Train: 2018-08-02T14:56:12.581896: step 4825, loss 3.9036.
Train: 2018-08-02T14:56:12.644403: step 4826, loss 3.65176.
Train: 2018-08-02T14:56:12.691275: step 4827, loss 4.65914.
Train: 2018-08-02T14:56:12.753730: step 4828, loss 4.02952.
Train: 2018-08-02T14:56:12.816216: step 4829, loss 5.28875.
Train: 2018-08-02T14:56:12.863110: step 4830, loss 3.27399.
Test: 2018-08-02T14:56:13.066180: step 4830, loss 3.81863.
Train: 2018-08-02T14:56:13.128640: step 4831, loss 3.77768.
Train: 2018-08-02T14:56:13.175538: step 4832, loss 5.64133.
Train: 2018-08-02T14:56:13.238023: step 4833, loss 3.65176.
Train: 2018-08-02T14:56:13.284853: step 4834, loss 3.14807.
Train: 2018-08-02T14:56:13.347372: step 4835, loss 4.15545.
Train: 2018-08-02T14:56:13.409855: step 4836, loss 2.64438.
Train: 2018-08-02T14:56:13.472343: step 4837, loss 5.41467.
Train: 2018-08-02T14:56:13.519205: step 4838, loss 3.27399.
Train: 2018-08-02T14:56:13.581661: step 4839, loss 4.15545.
Train: 2018-08-02T14:56:13.644174: step 4840, loss 3.14807.
Test: 2018-08-02T14:56:13.847222: step 4840, loss 3.81863.
Train: 2018-08-02T14:56:13.894087: step 4841, loss 4.65914.
Train: 2018-08-02T14:56:13.956604: step 4842, loss 3.39991.
Train: 2018-08-02T14:56:14.019056: step 4843, loss 4.02952.
Train: 2018-08-02T14:56:14.081597: step 4844, loss 3.52583.
Train: 2018-08-02T14:56:14.128437: step 4845, loss 4.15545.
Train: 2018-08-02T14:56:14.190921: step 4846, loss 4.40729.
Train: 2018-08-02T14:56:14.253407: step 4847, loss 4.02952.
Train: 2018-08-02T14:56:14.300272: step 4848, loss 4.02952.
Train: 2018-08-02T14:56:14.362726: step 4849, loss 4.53321.
Train: 2018-08-02T14:56:14.425211: step 4850, loss 3.77768.
Test: 2018-08-02T14:56:14.628289: step 4850, loss 3.81863.
Train: 2018-08-02T14:56:14.690775: step 4851, loss 5.16283.
Train: 2018-08-02T14:56:14.749181: step 4852, loss 3.52583.
Train: 2018-08-02T14:56:14.796078: step 4853, loss 4.65914.
Train: 2018-08-02T14:56:14.858561: step 4854, loss 3.52583.
Train: 2018-08-02T14:56:14.921048: step 4855, loss 3.77768.
Train: 2018-08-02T14:56:14.983501: step 4856, loss 4.02952.
Train: 2018-08-02T14:56:15.030398: step 4857, loss 3.27399.
Train: 2018-08-02T14:56:15.092883: step 4858, loss 4.91098.
Train: 2018-08-02T14:56:15.155369: step 4859, loss 4.28137.
Train: 2018-08-02T14:56:15.202231: step 4860, loss 3.65176.
Test: 2018-08-02T14:56:15.405310: step 4860, loss 3.81863.
Train: 2018-08-02T14:56:15.467764: step 4861, loss 5.03691.
Train: 2018-08-02T14:56:15.530248: step 4862, loss 4.15545.
Train: 2018-08-02T14:56:15.577113: step 4863, loss 3.52583.
Train: 2018-08-02T14:56:15.639600: step 4864, loss 3.52583.
Train: 2018-08-02T14:56:15.702084: step 4865, loss 4.15545.
Train: 2018-08-02T14:56:15.748947: step 4866, loss 3.14807.
Train: 2018-08-02T14:56:15.811464: step 4867, loss 3.27399.
Train: 2018-08-02T14:56:15.873952: step 4868, loss 4.40729.
Train: 2018-08-02T14:56:15.936405: step 4869, loss 4.15545.
Train: 2018-08-02T14:56:15.983268: step 4870, loss 4.15545.
Test: 2018-08-02T14:56:16.186375: step 4870, loss 3.81863.
Train: 2018-08-02T14:56:16.248855: step 4871, loss 4.28137.
Train: 2018-08-02T14:56:16.311346: step 4872, loss 4.15545.
Train: 2018-08-02T14:56:16.358210: step 4873, loss 3.39991.
Train: 2018-08-02T14:56:16.420698: step 4874, loss 3.77768.
Train: 2018-08-02T14:56:16.483152: step 4875, loss 4.53321.
Train: 2018-08-02T14:56:16.530015: step 4876, loss 4.28137.
Train: 2018-08-02T14:56:16.592525: step 4877, loss 5.41467.
Train: 2018-08-02T14:56:16.654987: step 4878, loss 4.15545.
Train: 2018-08-02T14:56:16.717501: step 4879, loss 3.9036.
Train: 2018-08-02T14:56:16.764368: step 4880, loss 3.39991.
Test: 2018-08-02T14:56:16.967411: step 4880, loss 3.81863.
Train: 2018-08-02T14:56:17.029923: step 4881, loss 3.52583.
Train: 2018-08-02T14:56:17.092413: step 4882, loss 3.9036.
Train: 2018-08-02T14:56:17.139248: step 4883, loss 3.27399.
Train: 2018-08-02T14:56:17.201763: step 4884, loss 4.02952.
Train: 2018-08-02T14:56:17.264252: step 4885, loss 3.27399.
Train: 2018-08-02T14:56:17.326704: step 4886, loss 3.14807.
Train: 2018-08-02T14:56:17.373597: step 4887, loss 4.02952.
Train: 2018-08-02T14:56:17.436084: step 4888, loss 4.28137.
Train: 2018-08-02T14:56:17.498568: step 4889, loss 3.65176.
Train: 2018-08-02T14:56:17.545402: step 4890, loss 4.15545.
Test: 2018-08-02T14:56:17.748479: step 4890, loss 3.81863.
Train: 2018-08-02T14:56:17.810964: step 4891, loss 4.28137.
Train: 2018-08-02T14:56:17.873451: step 4892, loss 5.03691.
Train: 2018-08-02T14:56:17.920346: step 4893, loss 5.66652.
Train: 2018-08-02T14:56:17.982800: step 4894, loss 3.02214.
Train: 2018-08-02T14:56:18.045317: step 4895, loss 3.52583.
Train: 2018-08-02T14:56:18.092179: step 4896, loss 4.28137.
Train: 2018-08-02T14:56:18.154634: step 4897, loss 3.9036.
Train: 2018-08-02T14:56:18.217152: step 4898, loss 4.91098.
Train: 2018-08-02T14:56:18.264015: step 4899, loss 4.15545.
Train: 2018-08-02T14:56:18.326470: step 4900, loss 4.40729.
Test: 2018-08-02T14:56:18.529546: step 4900, loss 3.81863.
Train: 2018-08-02T14:56:19.060703: step 4901, loss 3.9036.
Train: 2018-08-02T14:56:19.123187: step 4902, loss 3.27399.
Train: 2018-08-02T14:56:19.170022: step 4903, loss 4.53321.
Train: 2018-08-02T14:56:19.232541: step 4904, loss 4.65914.
Train: 2018-08-02T14:56:19.295024: step 4905, loss 4.02952.
Train: 2018-08-02T14:56:19.357479: step 4906, loss 4.02952.
Train: 2018-08-02T14:56:19.404341: step 4907, loss 4.65914.
Train: 2018-08-02T14:56:19.466858: step 4908, loss 4.53321.
Train: 2018-08-02T14:56:19.529345: step 4909, loss 4.28137.
Train: 2018-08-02T14:56:19.576206: step 4910, loss 3.77768.
Test: 2018-08-02T14:56:19.779278: step 4910, loss 3.81863.
Train: 2018-08-02T14:56:19.841740: step 4911, loss 3.9036.
Train: 2018-08-02T14:56:19.904252: step 4912, loss 4.28137.
Train: 2018-08-02T14:56:19.966735: step 4913, loss 3.77768.
Train: 2018-08-02T14:56:20.029195: step 4914, loss 3.9036.
Train: 2018-08-02T14:56:20.091681: step 4915, loss 2.51845.
Train: 2018-08-02T14:56:20.138575: step 4916, loss 3.27399.
Train: 2018-08-02T14:56:20.201029: step 4917, loss 3.52583.
Train: 2018-08-02T14:56:20.263517: step 4918, loss 4.15545.
Train: 2018-08-02T14:56:20.310380: step 4919, loss 4.40729.
Train: 2018-08-02T14:56:20.372866: step 4920, loss 2.7703.
Test: 2018-08-02T14:56:20.575943: step 4920, loss 3.81863.
Train: 2018-08-02T14:56:20.622807: step 4921, loss 4.02952.
Train: 2018-08-02T14:56:20.685317: step 4922, loss 4.02952.
Train: 2018-08-02T14:56:20.747777: step 4923, loss 3.65176.
Train: 2018-08-02T14:56:20.810294: step 4924, loss 4.78506.
Train: 2018-08-02T14:56:20.857127: step 4925, loss 3.9036.
Train: 2018-08-02T14:56:20.919644: step 4926, loss 4.15545.
Train: 2018-08-02T14:56:20.982127: step 4927, loss 4.40729.
Train: 2018-08-02T14:56:21.044615: step 4928, loss 3.65176.
Train: 2018-08-02T14:56:21.091446: step 4929, loss 4.15545.
Train: 2018-08-02T14:56:21.153960: step 4930, loss 3.77768.
Test: 2018-08-02T14:56:21.341413: step 4930, loss 3.81863.
Train: 2018-08-02T14:56:21.403873: step 4931, loss 4.28137.
Train: 2018-08-02T14:56:21.466358: step 4932, loss 4.53321.
Train: 2018-08-02T14:56:21.528846: step 4933, loss 4.02952.
Train: 2018-08-02T14:56:21.591360: step 4934, loss 4.02952.
Train: 2018-08-02T14:56:21.638195: step 4935, loss 3.65176.
Train: 2018-08-02T14:56:21.700678: step 4936, loss 3.52583.
Train: 2018-08-02T14:56:21.763195: step 4937, loss 3.65176.
Train: 2018-08-02T14:56:21.810053: step 4938, loss 3.77768.
Train: 2018-08-02T14:56:21.872514: step 4939, loss 3.52583.
Train: 2018-08-02T14:56:21.935000: step 4940, loss 4.65914.
Test: 2018-08-02T14:56:22.138078: step 4940, loss 3.81863.
Train: 2018-08-02T14:56:22.200592: step 4941, loss 4.65914.
Train: 2018-08-02T14:56:22.247456: step 4942, loss 3.9036.
Train: 2018-08-02T14:56:22.309911: step 4943, loss 3.39991.
Train: 2018-08-02T14:56:22.372400: step 4944, loss 4.02952.
Train: 2018-08-02T14:56:22.434885: step 4945, loss 4.02952.
Train: 2018-08-02T14:56:22.497369: step 4946, loss 3.9036.
Train: 2018-08-02T14:56:22.559857: step 4947, loss 5.28875.
Train: 2018-08-02T14:56:22.622340: step 4948, loss 4.40729.
Train: 2018-08-02T14:56:22.684827: step 4949, loss 4.53321.
Train: 2018-08-02T14:56:22.731689: step 4950, loss 3.9036.
Test: 2018-08-02T14:56:22.950386: step 4950, loss 3.81863.
Train: 2018-08-02T14:56:23.012902: step 4951, loss 3.9036.
Train: 2018-08-02T14:56:23.059761: step 4952, loss 3.77768.
Train: 2018-08-02T14:56:23.122221: step 4953, loss 2.89622.
Train: 2018-08-02T14:56:23.184738: step 4954, loss 3.77768.
Train: 2018-08-02T14:56:23.247193: step 4955, loss 4.15545.
Train: 2018-08-02T14:56:23.294087: step 4956, loss 2.89622.
Train: 2018-08-02T14:56:23.356575: step 4957, loss 3.77768.
Train: 2018-08-02T14:56:23.419056: step 4958, loss 4.65914.
Train: 2018-08-02T14:56:23.481512: step 4959, loss 3.9036.
Train: 2018-08-02T14:56:23.528408: step 4960, loss 4.40729.
Test: 2018-08-02T14:56:23.731452: step 4960, loss 3.81863.
Train: 2018-08-02T14:56:23.793970: step 4961, loss 4.53321.
Train: 2018-08-02T14:56:23.840833: step 4962, loss 4.65914.
Train: 2018-08-02T14:56:23.903315: step 4963, loss 4.40729.
Train: 2018-08-02T14:56:23.965805: step 4964, loss 4.40729.
Train: 2018-08-02T14:56:24.028260: step 4965, loss 4.53321.
Train: 2018-08-02T14:56:24.075123: step 4966, loss 4.65914.
Train: 2018-08-02T14:56:24.137609: step 4967, loss 4.15545.
Train: 2018-08-02T14:56:24.200125: step 4968, loss 4.53321.
Train: 2018-08-02T14:56:24.262580: step 4969, loss 3.65176.
Train: 2018-08-02T14:56:24.309473: step 4970, loss 5.16283.
Test: 2018-08-02T14:56:24.512519: step 4970, loss 3.81863.
Train: 2018-08-02T14:56:24.575031: step 4971, loss 3.14807.
Train: 2018-08-02T14:56:24.637491: step 4972, loss 4.28137.
Train: 2018-08-02T14:56:24.700006: step 4973, loss 3.52583.
Train: 2018-08-02T14:56:24.746844: step 4974, loss 4.28137.
Train: 2018-08-02T14:56:24.809356: step 4975, loss 3.02214.
Train: 2018-08-02T14:56:24.871811: step 4976, loss 3.77768.
Train: 2018-08-02T14:56:24.918707: step 4977, loss 4.15545.
Train: 2018-08-02T14:56:24.981165: step 4978, loss 3.39991.
Train: 2018-08-02T14:56:25.043678: step 4979, loss 4.40729.
Train: 2018-08-02T14:56:25.090511: step 4980, loss 4.53321.
Test: 2018-08-02T14:56:25.309223: step 4980, loss 3.81863.
Train: 2018-08-02T14:56:25.356074: step 4981, loss 3.9036.
Train: 2018-08-02T14:56:25.418592: step 4982, loss 4.28137.
Train: 2018-08-02T14:56:25.465453: step 4983, loss 4.29816.
Train: 2018-08-02T14:56:25.527937: step 4984, loss 4.40729.
Train: 2018-08-02T14:56:25.590423: step 4985, loss 4.40729.
Train: 2018-08-02T14:56:25.637258: step 4986, loss 3.65176.
Train: 2018-08-02T14:56:25.699772: step 4987, loss 3.14807.
Train: 2018-08-02T14:56:25.762261: step 4988, loss 3.39991.
Train: 2018-08-02T14:56:25.809118: step 4989, loss 4.15545.
Train: 2018-08-02T14:56:25.871603: step 4990, loss 3.9036.
Test: 2018-08-02T14:56:26.074678: step 4990, loss 3.81863.
Train: 2018-08-02T14:56:26.137170: step 4991, loss 4.28137.
Train: 2018-08-02T14:56:26.199655: step 4992, loss 5.79244.
Train: 2018-08-02T14:56:26.246520: step 4993, loss 2.89622.
Train: 2018-08-02T14:56:26.308976: step 4994, loss 4.91098.
Train: 2018-08-02T14:56:26.371491: step 4995, loss 3.77768.
Train: 2018-08-02T14:56:26.433946: step 4996, loss 4.65914.
Train: 2018-08-02T14:56:26.480840: step 4997, loss 4.91098.
Train: 2018-08-02T14:56:26.543328: step 4998, loss 3.77768.
Train: 2018-08-02T14:56:26.605813: step 4999, loss 4.15545.
Train: 2018-08-02T14:56:26.652645: step 5000, loss 3.77768.
Test: 2018-08-02T14:56:26.855722: step 5000, loss 3.81863.
Train: 2018-08-02T14:56:27.371226: step 5001, loss 4.65914.
Train: 2018-08-02T14:56:27.433711: step 5002, loss 4.28137.
Train: 2018-08-02T14:56:27.480575: step 5003, loss 3.52583.
Train: 2018-08-02T14:56:27.543093: step 5004, loss 4.02952.
Train: 2018-08-02T14:56:27.605577: step 5005, loss 4.02952.
Train: 2018-08-02T14:56:27.668063: step 5006, loss 4.02952.
Train: 2018-08-02T14:56:27.714921: step 5007, loss 4.40729.
Train: 2018-08-02T14:56:27.777383: step 5008, loss 3.02214.
Train: 2018-08-02T14:56:27.839893: step 5009, loss 3.14807.
Train: 2018-08-02T14:56:27.886764: step 5010, loss 3.27399.
Test: 2018-08-02T14:56:28.089832: step 5010, loss 3.81863.
Train: 2018-08-02T14:56:28.152293: step 5011, loss 4.15545.
Train: 2018-08-02T14:56:28.199187: step 5012, loss 4.28137.
Train: 2018-08-02T14:56:28.261675: step 5013, loss 3.39991.
Train: 2018-08-02T14:56:28.324160: step 5014, loss 3.52583.
Train: 2018-08-02T14:56:28.386615: step 5015, loss 4.02952.
Train: 2018-08-02T14:56:28.433508: step 5016, loss 3.02214.
Train: 2018-08-02T14:56:28.495964: step 5017, loss 5.41467.
Train: 2018-08-02T14:56:28.558475: step 5018, loss 4.15545.
Train: 2018-08-02T14:56:28.605342: step 5019, loss 4.15545.
Train: 2018-08-02T14:56:28.667828: step 5020, loss 2.89622.
Test: 2018-08-02T14:56:28.870901: step 5020, loss 3.81863.
Train: 2018-08-02T14:56:28.933392: step 5021, loss 3.77768.
Train: 2018-08-02T14:56:28.995871: step 5022, loss 5.16283.
Train: 2018-08-02T14:56:29.042710: step 5023, loss 3.52583.
Train: 2018-08-02T14:56:29.105198: step 5024, loss 3.39991.
Train: 2018-08-02T14:56:29.167715: step 5025, loss 4.15545.
Train: 2018-08-02T14:56:29.230196: step 5026, loss 4.65914.
Train: 2018-08-02T14:56:29.277030: step 5027, loss 3.65176.
Train: 2018-08-02T14:56:29.339515: step 5028, loss 3.52583.
Train: 2018-08-02T14:56:29.402030: step 5029, loss 2.39253.
Train: 2018-08-02T14:56:29.448890: step 5030, loss 3.9036.
Test: 2018-08-02T14:56:29.651967: step 5030, loss 3.81863.
Train: 2018-08-02T14:56:29.714427: step 5031, loss 5.28875.
Train: 2018-08-02T14:56:29.776913: step 5032, loss 3.39991.
Train: 2018-08-02T14:56:29.823807: step 5033, loss 4.15545.
Train: 2018-08-02T14:56:29.886295: step 5034, loss 4.02952.
Train: 2018-08-02T14:56:29.948778: step 5035, loss 3.02214.
Train: 2018-08-02T14:56:30.011232: step 5036, loss 3.77768.
Train: 2018-08-02T14:56:30.058127: step 5037, loss 4.15545.
Train: 2018-08-02T14:56:30.120613: step 5038, loss 3.9036.
Train: 2018-08-02T14:56:30.183094: step 5039, loss 3.39991.
Train: 2018-08-02T14:56:30.229962: step 5040, loss 4.02952.
Test: 2018-08-02T14:56:30.433008: step 5040, loss 3.81863.
Train: 2018-08-02T14:56:30.495524: step 5041, loss 4.15545.
Train: 2018-08-02T14:56:30.558010: step 5042, loss 4.53321.
Train: 2018-08-02T14:56:30.620466: step 5043, loss 5.41467.
Train: 2018-08-02T14:56:30.667330: step 5044, loss 3.65176.
Train: 2018-08-02T14:56:30.729815: step 5045, loss 4.65914.
Train: 2018-08-02T14:56:30.792331: step 5046, loss 4.91098.
Train: 2018-08-02T14:56:30.839164: step 5047, loss 3.65176.
Train: 2018-08-02T14:56:30.901682: step 5048, loss 2.64438.
Train: 2018-08-02T14:56:30.964161: step 5049, loss 4.28137.
Train: 2018-08-02T14:56:31.011031: step 5050, loss 3.65176.
Test: 2018-08-02T14:56:31.214075: step 5050, loss 3.81863.
Train: 2018-08-02T14:56:31.276562: step 5051, loss 4.15545.
Train: 2018-08-02T14:56:31.339076: step 5052, loss 5.41467.
Train: 2018-08-02T14:56:31.401562: step 5053, loss 3.9036.
Train: 2018-08-02T14:56:31.448427: step 5054, loss 3.65176.
Train: 2018-08-02T14:56:31.510908: step 5055, loss 3.77768.
Train: 2018-08-02T14:56:31.573368: step 5056, loss 3.77768.
Train: 2018-08-02T14:56:31.635885: step 5057, loss 3.77768.
Train: 2018-08-02T14:56:31.682748: step 5058, loss 3.77768.
Train: 2018-08-02T14:56:31.745201: step 5059, loss 4.15545.
Train: 2018-08-02T14:56:31.807717: step 5060, loss 4.40729.
Test: 2018-08-02T14:56:32.010795: step 5060, loss 3.81863.
Train: 2018-08-02T14:56:32.073249: step 5061, loss 3.27399.
Train: 2018-08-02T14:56:32.120144: step 5062, loss 5.16283.
Train: 2018-08-02T14:56:32.182599: step 5063, loss 5.16283.
Train: 2018-08-02T14:56:32.245115: step 5064, loss 3.65176.
Train: 2018-08-02T14:56:32.307571: step 5065, loss 3.02214.
Train: 2018-08-02T14:56:32.370085: step 5066, loss 4.02952.
Train: 2018-08-02T14:56:32.416957: step 5067, loss 2.89622.
Train: 2018-08-02T14:56:32.479407: step 5068, loss 4.15545.
Train: 2018-08-02T14:56:32.541923: step 5069, loss 4.65914.
Train: 2018-08-02T14:56:32.588778: step 5070, loss 4.02952.
Test: 2018-08-02T14:56:32.791855: step 5070, loss 3.81863.
Train: 2018-08-02T14:56:32.854347: step 5071, loss 5.41467.
Train: 2018-08-02T14:56:32.916829: step 5072, loss 4.28137.
Train: 2018-08-02T14:56:32.963667: step 5073, loss 4.02952.
Train: 2018-08-02T14:56:33.026153: step 5074, loss 3.65176.
Train: 2018-08-02T14:56:33.088668: step 5075, loss 3.39991.
Train: 2018-08-02T14:56:33.135502: step 5076, loss 3.52583.
Train: 2018-08-02T14:56:33.198012: step 5077, loss 4.78506.
Train: 2018-08-02T14:56:33.260505: step 5078, loss 3.77768.
Train: 2018-08-02T14:56:33.307366: step 5079, loss 4.78506.
Train: 2018-08-02T14:56:33.369851: step 5080, loss 3.65176.
Test: 2018-08-02T14:56:33.572898: step 5080, loss 3.81863.
Train: 2018-08-02T14:56:33.635413: step 5081, loss 4.91098.
Train: 2018-08-02T14:56:33.697870: step 5082, loss 5.16283.
Train: 2018-08-02T14:56:33.744763: step 5083, loss 4.02952.
Train: 2018-08-02T14:56:33.807250: step 5084, loss 3.9036.
Train: 2018-08-02T14:56:33.869735: step 5085, loss 4.65914.
Train: 2018-08-02T14:56:33.916600: step 5086, loss 3.77768.
Train: 2018-08-02T14:56:33.979084: step 5087, loss 4.15545.
Train: 2018-08-02T14:56:34.041566: step 5088, loss 4.28137.
Train: 2018-08-02T14:56:34.104027: step 5089, loss 5.16283.
Train: 2018-08-02T14:56:34.150920: step 5090, loss 3.65176.
Test: 2018-08-02T14:56:34.353966: step 5090, loss 3.81863.
Train: 2018-08-02T14:56:34.416452: step 5091, loss 4.91098.
Train: 2018-08-02T14:56:34.478967: step 5092, loss 4.02952.
Train: 2018-08-02T14:56:34.541421: step 5093, loss 4.65914.
Train: 2018-08-02T14:56:34.588319: step 5094, loss 3.9036.
Train: 2018-08-02T14:56:34.650773: step 5095, loss 5.0369.
Train: 2018-08-02T14:56:34.713257: step 5096, loss 3.39991.
Train: 2018-08-02T14:56:34.775743: step 5097, loss 4.53321.
Train: 2018-08-02T14:56:34.822607: step 5098, loss 3.65176.
Train: 2018-08-02T14:56:34.885125: step 5099, loss 4.78506.
Train: 2018-08-02T14:56:34.947606: step 5100, loss 3.65176.
Test: 2018-08-02T14:56:35.135063: step 5100, loss 3.81863.
Train: 2018-08-02T14:56:35.666185: step 5101, loss 3.39991.
Train: 2018-08-02T14:56:35.728644: step 5102, loss 3.14807.
Train: 2018-08-02T14:56:35.775540: step 5103, loss 4.40729.
Train: 2018-08-02T14:56:35.838026: step 5104, loss 4.40729.
Train: 2018-08-02T14:56:35.900503: step 5105, loss 3.9036.
Train: 2018-08-02T14:56:35.962989: step 5106, loss 3.27399.
Train: 2018-08-02T14:56:36.009828: step 5107, loss 4.15545.
Train: 2018-08-02T14:56:36.072315: step 5108, loss 3.27399.
Train: 2018-08-02T14:56:36.134800: step 5109, loss 3.52583.
Train: 2018-08-02T14:56:36.181693: step 5110, loss 3.65176.
Test: 2018-08-02T14:56:36.384770: step 5110, loss 3.81863.
Train: 2018-08-02T14:56:36.447226: step 5111, loss 3.27399.
Train: 2018-08-02T14:56:36.509744: step 5112, loss 3.02214.
Train: 2018-08-02T14:56:36.572221: step 5113, loss 5.0369.
Train: 2018-08-02T14:56:36.634682: step 5114, loss 3.39991.
Train: 2018-08-02T14:56:36.681578: step 5115, loss 4.78506.
Train: 2018-08-02T14:56:36.744032: step 5116, loss 3.39991.
Train: 2018-08-02T14:56:36.806542: step 5117, loss 4.15545.
Train: 2018-08-02T14:56:36.869027: step 5118, loss 3.52583.
Train: 2018-08-02T14:56:36.915898: step 5119, loss 4.28137.
Train: 2018-08-02T14:56:36.978383: step 5120, loss 2.64438.
Test: 2018-08-02T14:56:37.181429: step 5120, loss 3.81863.
Train: 2018-08-02T14:56:37.228293: step 5121, loss 4.02952.
Train: 2018-08-02T14:56:37.290810: step 5122, loss 4.28137.
Train: 2018-08-02T14:56:37.353294: step 5123, loss 4.91098.
Train: 2018-08-02T14:56:37.415781: step 5124, loss 3.9036.
Train: 2018-08-02T14:56:37.462643: step 5125, loss 3.65176.
Train: 2018-08-02T14:56:37.525125: step 5126, loss 4.15545.
Train: 2018-08-02T14:56:37.587583: step 5127, loss 4.15545.
Train: 2018-08-02T14:56:37.634449: step 5128, loss 4.15545.
Train: 2018-08-02T14:56:37.696933: step 5129, loss 3.77768.
Train: 2018-08-02T14:56:37.759448: step 5130, loss 3.9036.
Test: 2018-08-02T14:56:37.946898: step 5130, loss 3.81863.
Train: 2018-08-02T14:56:38.009359: step 5131, loss 3.39991.
Train: 2018-08-02T14:56:38.071845: step 5132, loss 4.53321.
Train: 2018-08-02T14:56:38.134360: step 5133, loss 4.40729.
Train: 2018-08-02T14:56:38.181221: step 5134, loss 3.22362.
Train: 2018-08-02T14:56:38.243681: step 5135, loss 2.89622.
Train: 2018-08-02T14:56:38.290575: step 5136, loss 3.39991.
Train: 2018-08-02T14:56:38.353055: step 5137, loss 4.40729.
Train: 2018-08-02T14:56:38.415548: step 5138, loss 3.65176.
Train: 2018-08-02T14:56:38.462404: step 5139, loss 3.52583.
Train: 2018-08-02T14:56:38.524902: step 5140, loss 5.66652.
Test: 2018-08-02T14:56:38.727942: step 5140, loss 3.81863.
Train: 2018-08-02T14:56:38.790458: step 5141, loss 3.9036.
Train: 2018-08-02T14:56:38.837322: step 5142, loss 3.27399.
Train: 2018-08-02T14:56:38.899808: step 5143, loss 3.39991.
Train: 2018-08-02T14:56:38.962292: step 5144, loss 4.28137.
Train: 2018-08-02T14:56:39.024774: step 5145, loss 4.65914.
Train: 2018-08-02T14:56:39.071612: step 5146, loss 4.53321.
Train: 2018-08-02T14:56:39.134096: step 5147, loss 4.65914.
Train: 2018-08-02T14:56:39.196613: step 5148, loss 3.39991.
Train: 2018-08-02T14:56:39.243446: step 5149, loss 4.40729.
Train: 2018-08-02T14:56:39.305931: step 5150, loss 3.39991.
Test: 2018-08-02T14:56:39.509038: step 5150, loss 3.81863.
Train: 2018-08-02T14:56:39.571495: step 5151, loss 4.15545.
Train: 2018-08-02T14:56:39.618360: step 5152, loss 4.28137.
Train: 2018-08-02T14:56:39.680843: step 5153, loss 4.15545.
Train: 2018-08-02T14:56:39.743360: step 5154, loss 4.78506.
Train: 2018-08-02T14:56:39.805815: step 5155, loss 4.02952.
Train: 2018-08-02T14:56:39.852708: step 5156, loss 3.65176.
Train: 2018-08-02T14:56:39.915164: step 5157, loss 4.65914.
Train: 2018-08-02T14:56:39.977678: step 5158, loss 4.15545.
Train: 2018-08-02T14:56:40.024513: step 5159, loss 3.65176.
Train: 2018-08-02T14:56:40.087028: step 5160, loss 5.0369.
Test: 2018-08-02T14:56:40.290075: step 5160, loss 3.81863.
Train: 2018-08-02T14:56:40.336971: step 5161, loss 3.39991.
Train: 2018-08-02T14:56:40.399425: step 5162, loss 4.28137.
Train: 2018-08-02T14:56:40.461910: step 5163, loss 3.65176.
Train: 2018-08-02T14:56:40.524424: step 5164, loss 4.53321.
Train: 2018-08-02T14:56:40.586884: step 5165, loss 3.52583.
Train: 2018-08-02T14:56:40.633778: step 5166, loss 4.40729.
Train: 2018-08-02T14:56:40.696261: step 5167, loss 4.40729.
Train: 2018-08-02T14:56:40.758750: step 5168, loss 3.39991.
Train: 2018-08-02T14:56:40.805612: step 5169, loss 4.91098.
Train: 2018-08-02T14:56:40.868099: step 5170, loss 4.15545.
Test: 2018-08-02T14:56:41.071167: step 5170, loss 3.81863.
Train: 2018-08-02T14:56:41.133654: step 5171, loss 3.77768.
Train: 2018-08-02T14:56:41.196112: step 5172, loss 3.9036.
Train: 2018-08-02T14:56:41.258628: step 5173, loss 4.91098.
Train: 2018-08-02T14:56:41.305495: step 5174, loss 4.40729.
Train: 2018-08-02T14:56:41.367979: step 5175, loss 3.14807.
Train: 2018-08-02T14:56:41.430435: step 5176, loss 4.40729.
Train: 2018-08-02T14:56:41.477329: step 5177, loss 3.65176.
Train: 2018-08-02T14:56:41.539816: step 5178, loss 2.89622.
Train: 2018-08-02T14:56:41.602298: step 5179, loss 3.52583.
Train: 2018-08-02T14:56:41.664755: step 5180, loss 4.02952.
Test: 2018-08-02T14:56:41.852234: step 5180, loss 3.81863.
Train: 2018-08-02T14:56:41.914727: step 5181, loss 3.52583.
Train: 2018-08-02T14:56:41.977180: step 5182, loss 3.77768.
Train: 2018-08-02T14:56:42.039666: step 5183, loss 3.14807.
Train: 2018-08-02T14:56:42.102190: step 5184, loss 4.15545.
Train: 2018-08-02T14:56:42.149015: step 5185, loss 4.91098.
Train: 2018-08-02T14:56:42.211530: step 5186, loss 4.28137.
Train: 2018-08-02T14:56:42.274018: step 5187, loss 2.89622.
Train: 2018-08-02T14:56:42.320881: step 5188, loss 3.14807.
Train: 2018-08-02T14:56:42.383361: step 5189, loss 3.77768.
Train: 2018-08-02T14:56:42.445852: step 5190, loss 4.65914.
Test: 2018-08-02T14:56:42.633301: step 5190, loss 3.81863.
Train: 2018-08-02T14:56:42.695763: step 5191, loss 3.39991.
Train: 2018-08-02T14:56:42.758278: step 5192, loss 4.40729.
Train: 2018-08-02T14:56:42.820758: step 5193, loss 4.40729.
Train: 2018-08-02T14:56:42.883245: step 5194, loss 3.9036.
Train: 2018-08-02T14:56:42.930114: step 5195, loss 3.02214.
Train: 2018-08-02T14:56:42.992596: step 5196, loss 4.28137.
Train: 2018-08-02T14:56:43.055054: step 5197, loss 4.02952.
Train: 2018-08-02T14:56:43.117589: step 5198, loss 2.89622.
Train: 2018-08-02T14:56:43.164434: step 5199, loss 4.15545.
Train: 2018-08-02T14:56:43.226918: step 5200, loss 3.9036.
Test: 2018-08-02T14:56:43.429990: step 5200, loss 3.81863.
Train: 2018-08-02T14:56:43.976712: step 5201, loss 3.02214.
Train: 2018-08-02T14:56:44.039229: step 5202, loss 3.14807.
Train: 2018-08-02T14:56:44.101713: step 5203, loss 3.27399.
Train: 2018-08-02T14:56:44.148575: step 5204, loss 4.02952.
Train: 2018-08-02T14:56:44.211063: step 5205, loss 5.79244.
Train: 2018-08-02T14:56:44.273519: step 5206, loss 3.77768.
Train: 2018-08-02T14:56:44.336003: step 5207, loss 3.27399.
Train: 2018-08-02T14:56:44.382901: step 5208, loss 3.52583.
Train: 2018-08-02T14:56:44.445354: step 5209, loss 4.28137.
Train: 2018-08-02T14:56:44.507869: step 5210, loss 4.28137.
Test: 2018-08-02T14:56:44.695319: step 5210, loss 3.81863.
Train: 2018-08-02T14:56:44.757779: step 5211, loss 5.16283.
Train: 2018-08-02T14:56:44.820265: step 5212, loss 5.41467.
Train: 2018-08-02T14:56:44.867130: step 5213, loss 3.9036.
Train: 2018-08-02T14:56:44.929615: step 5214, loss 4.40729.
Train: 2018-08-02T14:56:44.992127: step 5215, loss 4.65914.
Train: 2018-08-02T14:56:45.054587: step 5216, loss 4.28137.
Train: 2018-08-02T14:56:45.101449: step 5217, loss 4.02952.
Train: 2018-08-02T14:56:45.163936: step 5218, loss 3.14807.
Train: 2018-08-02T14:56:45.226451: step 5219, loss 3.9036.
Train: 2018-08-02T14:56:45.273284: step 5220, loss 3.27399.
Test: 2018-08-02T14:56:45.476391: step 5220, loss 3.81863.
Train: 2018-08-02T14:56:45.538847: step 5221, loss 4.91098.
Train: 2018-08-02T14:56:45.601362: step 5222, loss 3.27399.
Train: 2018-08-02T14:56:45.648228: step 5223, loss 3.9036.
Train: 2018-08-02T14:56:45.710683: step 5224, loss 4.91098.
Train: 2018-08-02T14:56:45.773195: step 5225, loss 4.40729.
Train: 2018-08-02T14:56:45.835683: step 5226, loss 4.02952.
Train: 2018-08-02T14:56:45.882516: step 5227, loss 3.65176.
Train: 2018-08-02T14:56:45.945042: step 5228, loss 3.65176.
Train: 2018-08-02T14:56:46.007512: step 5229, loss 5.0369.
Train: 2018-08-02T14:56:46.054351: step 5230, loss 4.02952.
Test: 2018-08-02T14:56:46.257458: step 5230, loss 3.81863.
Train: 2018-08-02T14:56:46.319944: step 5231, loss 4.15545.
Train: 2018-08-02T14:56:46.382429: step 5232, loss 3.9036.
Train: 2018-08-02T14:56:46.444934: step 5233, loss 3.77768.
Train: 2018-08-02T14:56:46.491775: step 5234, loss 3.65176.
Train: 2018-08-02T14:56:46.554236: step 5235, loss 4.15545.
Train: 2018-08-02T14:56:46.616744: step 5236, loss 4.28137.
Train: 2018-08-02T14:56:46.663615: step 5237, loss 3.14807.
Train: 2018-08-02T14:56:46.726099: step 5238, loss 3.77768.
Train: 2018-08-02T14:56:46.788580: step 5239, loss 4.65914.
Train: 2018-08-02T14:56:46.835423: step 5240, loss 5.41467.
Test: 2018-08-02T14:56:47.038494: step 5240, loss 3.81863.
Train: 2018-08-02T14:56:47.101011: step 5241, loss 3.9036.
Train: 2018-08-02T14:56:47.163490: step 5242, loss 2.64437.
Train: 2018-08-02T14:56:47.210362: step 5243, loss 3.39991.
Train: 2018-08-02T14:56:47.272817: step 5244, loss 3.39991.
Train: 2018-08-02T14:56:47.335327: step 5245, loss 4.40729.
Train: 2018-08-02T14:56:47.397787: step 5246, loss 5.5406.
Train: 2018-08-02T14:56:47.444651: step 5247, loss 3.52583.
Train: 2018-08-02T14:56:47.507138: step 5248, loss 3.9036.
Train: 2018-08-02T14:56:47.569668: step 5249, loss 3.65176.
Train: 2018-08-02T14:56:47.632137: step 5250, loss 3.52583.
Test: 2018-08-02T14:56:47.819593: step 5250, loss 3.81863.
Train: 2018-08-02T14:56:47.882073: step 5251, loss 4.28137.
Train: 2018-08-02T14:56:47.944564: step 5252, loss 4.65914.
Train: 2018-08-02T14:56:48.007048: step 5253, loss 3.77768.
Train: 2018-08-02T14:56:48.069528: step 5254, loss 2.7703.
Train: 2018-08-02T14:56:48.116400: step 5255, loss 3.9036.
Train: 2018-08-02T14:56:48.178883: step 5256, loss 4.40729.
Train: 2018-08-02T14:56:48.241368: step 5257, loss 3.39991.
Train: 2018-08-02T14:56:48.288234: step 5258, loss 4.78506.
Train: 2018-08-02T14:56:48.350718: step 5259, loss 4.02952.
Train: 2018-08-02T14:56:48.413198: step 5260, loss 3.14807.
Test: 2018-08-02T14:56:48.600630: step 5260, loss 3.81863.
Train: 2018-08-02T14:56:48.663147: step 5261, loss 4.65914.
Train: 2018-08-02T14:56:48.725600: step 5262, loss 4.40729.
Train: 2018-08-02T14:56:48.788086: step 5263, loss 4.53321.
Train: 2018-08-02T14:56:48.850572: step 5264, loss 3.14807.
Train: 2018-08-02T14:56:48.897435: step 5265, loss 4.40729.
Train: 2018-08-02T14:56:48.959921: step 5266, loss 3.9036.
Train: 2018-08-02T14:56:49.022435: step 5267, loss 4.02952.
Train: 2018-08-02T14:56:49.069301: step 5268, loss 3.65176.
Train: 2018-08-02T14:56:49.131781: step 5269, loss 4.02952.
Train: 2018-08-02T14:56:49.194271: step 5270, loss 3.65176.
Test: 2018-08-02T14:56:49.397318: step 5270, loss 3.81863.
Train: 2018-08-02T14:56:49.444212: step 5271, loss 3.9036.
Train: 2018-08-02T14:56:49.506700: step 5272, loss 4.02952.
Train: 2018-08-02T14:56:49.569182: step 5273, loss 5.0369.
Train: 2018-08-02T14:56:49.631638: step 5274, loss 4.53321.
Train: 2018-08-02T14:56:49.694126: step 5275, loss 4.28137.
Train: 2018-08-02T14:56:49.740988: step 5276, loss 3.52583.
Train: 2018-08-02T14:56:49.803503: step 5277, loss 4.53321.
Train: 2018-08-02T14:56:49.865984: step 5278, loss 4.78506.
Train: 2018-08-02T14:56:49.912854: step 5279, loss 4.78506.
Train: 2018-08-02T14:56:49.975338: step 5280, loss 3.9036.
Test: 2018-08-02T14:56:50.178384: step 5280, loss 3.81863.
Train: 2018-08-02T14:56:50.240900: step 5281, loss 3.65176.
Train: 2018-08-02T14:56:50.303385: step 5282, loss 4.28137.
Train: 2018-08-02T14:56:50.350252: step 5283, loss 4.91098.
Train: 2018-08-02T14:56:50.412738: step 5284, loss 3.39991.
Train: 2018-08-02T14:56:50.459600: step 5285, loss 3.76089.
Train: 2018-08-02T14:56:50.522086: step 5286, loss 3.02214.
Train: 2018-08-02T14:56:50.584570: step 5287, loss 4.65914.
Train: 2018-08-02T14:56:50.631404: step 5288, loss 3.52583.
Train: 2018-08-02T14:56:50.693936: step 5289, loss 2.26661.
Train: 2018-08-02T14:56:50.756374: step 5290, loss 3.52583.
Test: 2018-08-02T14:56:50.943855: step 5290, loss 3.81863.
Train: 2018-08-02T14:56:51.006318: step 5291, loss 5.5406.
Train: 2018-08-02T14:56:51.068832: step 5292, loss 5.16283.
Train: 2018-08-02T14:56:51.131321: step 5293, loss 3.65176.
Train: 2018-08-02T14:56:51.193773: step 5294, loss 3.65176.
Train: 2018-08-02T14:56:51.240668: step 5295, loss 3.9036.
Train: 2018-08-02T14:56:51.303153: step 5296, loss 4.15545.
Train: 2018-08-02T14:56:51.365638: step 5297, loss 4.65914.
Train: 2018-08-02T14:56:51.428094: step 5298, loss 4.02952.
Train: 2018-08-02T14:56:51.474957: step 5299, loss 4.78506.
Train: 2018-08-02T14:56:51.537473: step 5300, loss 3.65176.
Test: 2018-08-02T14:56:51.740543: step 5300, loss 3.81863.
Train: 2018-08-02T14:56:52.224811: step 5301, loss 4.28137.
Train: 2018-08-02T14:56:52.287266: step 5302, loss 4.53321.
Train: 2018-08-02T14:56:52.349782: step 5303, loss 3.77768.
Train: 2018-08-02T14:56:52.412269: step 5304, loss 3.39991.
Train: 2018-08-02T14:56:52.474723: step 5305, loss 3.77768.
Train: 2018-08-02T14:56:52.521586: step 5306, loss 3.9036.
Train: 2018-08-02T14:56:52.584104: step 5307, loss 4.02952.
Train: 2018-08-02T14:56:52.646585: step 5308, loss 4.91098.
Train: 2018-08-02T14:56:52.709074: step 5309, loss 4.78506.
Train: 2018-08-02T14:56:52.755931: step 5310, loss 4.28137.
Test: 2018-08-02T14:56:52.959014: step 5310, loss 3.81863.
Train: 2018-08-02T14:56:53.021470: step 5311, loss 3.77768.
Train: 2018-08-02T14:56:53.083979: step 5312, loss 4.91098.
Train: 2018-08-02T14:56:53.146470: step 5313, loss 4.28137.
Train: 2018-08-02T14:56:53.193307: step 5314, loss 4.28137.
Train: 2018-08-02T14:56:53.255791: step 5315, loss 4.53321.
Train: 2018-08-02T14:56:53.318276: step 5316, loss 3.39991.
Train: 2018-08-02T14:56:53.365170: step 5317, loss 4.40729.
Train: 2018-08-02T14:56:53.427625: step 5318, loss 3.39991.
Train: 2018-08-02T14:56:53.490136: step 5319, loss 4.02952.
Train: 2018-08-02T14:56:53.552594: step 5320, loss 4.02952.
Test: 2018-08-02T14:56:53.740082: step 5320, loss 3.81863.
Train: 2018-08-02T14:56:53.802561: step 5321, loss 2.7703.
Train: 2018-08-02T14:56:53.865047: step 5322, loss 3.9036.
Train: 2018-08-02T14:56:53.911918: step 5323, loss 4.28137.
Train: 2018-08-02T14:56:53.974402: step 5324, loss 4.78506.
Train: 2018-08-02T14:56:54.036884: step 5325, loss 4.02952.
Train: 2018-08-02T14:56:54.099343: step 5326, loss 3.65176.
Train: 2018-08-02T14:56:54.146234: step 5327, loss 3.77768.
Train: 2018-08-02T14:56:54.208723: step 5328, loss 4.15545.
Train: 2018-08-02T14:56:54.271209: step 5329, loss 3.77768.
Train: 2018-08-02T14:56:54.333688: step 5330, loss 3.14807.
Test: 2018-08-02T14:56:54.521142: step 5330, loss 3.81863.
Train: 2018-08-02T14:56:54.583634: step 5331, loss 3.77768.
Train: 2018-08-02T14:56:54.646116: step 5332, loss 3.14807.
Train: 2018-08-02T14:56:54.708574: step 5333, loss 3.27399.
Train: 2018-08-02T14:56:54.755440: step 5334, loss 4.91098.
Train: 2018-08-02T14:56:54.817952: step 5335, loss 3.77768.
Train: 2018-08-02T14:56:54.880437: step 5336, loss 4.28137.
Train: 2018-08-02T14:56:54.942895: step 5337, loss 4.65914.
Train: 2018-08-02T14:56:54.989760: step 5338, loss 5.0369.
Train: 2018-08-02T14:56:55.052275: step 5339, loss 3.65176.
Train: 2018-08-02T14:56:55.114760: step 5340, loss 3.77768.
Test: 2018-08-02T14:56:55.317807: step 5340, loss 3.81863.
Train: 2018-08-02T14:56:55.364702: step 5341, loss 3.65176.
Train: 2018-08-02T14:56:55.427186: step 5342, loss 5.79244.
Train: 2018-08-02T14:56:55.489671: step 5343, loss 3.65176.
Train: 2018-08-02T14:56:55.552159: step 5344, loss 5.28875.
Train: 2018-08-02T14:56:55.599021: step 5345, loss 3.52583.
Train: 2018-08-02T14:56:55.661502: step 5346, loss 3.65176.
Train: 2018-08-02T14:56:55.723985: step 5347, loss 3.9036.
Train: 2018-08-02T14:56:55.770858: step 5348, loss 4.28137.
Train: 2018-08-02T14:56:55.833336: step 5349, loss 3.9036.
Train: 2018-08-02T14:56:55.895829: step 5350, loss 2.51845.
Test: 2018-08-02T14:56:56.083282: step 5350, loss 3.81863.
Train: 2018-08-02T14:56:56.145770: step 5351, loss 4.53321.
Train: 2018-08-02T14:56:56.208253: step 5352, loss 3.652.
Train: 2018-08-02T14:56:56.270738: step 5353, loss 3.39991.
Train: 2018-08-02T14:56:56.333195: step 5354, loss 4.40729.
Train: 2018-08-02T14:56:56.380092: step 5355, loss 3.77768.
Train: 2018-08-02T14:56:56.442573: step 5356, loss 3.39991.
Train: 2018-08-02T14:56:56.505029: step 5357, loss 4.28137.
Train: 2018-08-02T14:56:56.551919: step 5358, loss 4.28137.
Train: 2018-08-02T14:56:56.614379: step 5359, loss 3.9036.
Train: 2018-08-02T14:56:56.676864: step 5360, loss 3.65176.
Test: 2018-08-02T14:56:56.879942: step 5360, loss 3.81863.
Train: 2018-08-02T14:56:56.926805: step 5361, loss 4.40729.
Train: 2018-08-02T14:56:56.989290: step 5362, loss 5.41467.
Train: 2018-08-02T14:56:57.051805: step 5363, loss 3.9036.
Train: 2018-08-02T14:56:57.114287: step 5364, loss 3.77768.
Train: 2018-08-02T14:56:57.161124: step 5365, loss 4.15545.
Train: 2018-08-02T14:56:57.223610: step 5366, loss 4.02952.
Train: 2018-08-02T14:56:57.286127: step 5367, loss 3.14807.
Train: 2018-08-02T14:56:57.348614: step 5368, loss 3.77768.
Train: 2018-08-02T14:56:57.395477: step 5369, loss 3.9036.
Train: 2018-08-02T14:56:57.457961: step 5370, loss 4.15545.
Test: 2018-08-02T14:56:57.661032: step 5370, loss 3.81863.
Train: 2018-08-02T14:56:57.723493: step 5371, loss 4.15545.
Train: 2018-08-02T14:56:57.770358: step 5372, loss 4.53321.
Train: 2018-08-02T14:56:57.832873: step 5373, loss 4.78506.
Train: 2018-08-02T14:56:57.895329: step 5374, loss 3.52583.
Train: 2018-08-02T14:56:57.957845: step 5375, loss 3.52583.
Train: 2018-08-02T14:56:58.020330: step 5376, loss 4.40729.
Train: 2018-08-02T14:56:58.067192: step 5377, loss 3.77768.
Train: 2018-08-02T14:56:58.129682: step 5378, loss 3.39991.
Train: 2018-08-02T14:56:58.192134: step 5379, loss 4.65914.
Train: 2018-08-02T14:56:58.238998: step 5380, loss 3.77768.
Test: 2018-08-02T14:56:58.442074: step 5380, loss 3.81863.
Train: 2018-08-02T14:56:58.504590: step 5381, loss 4.02952.
Train: 2018-08-02T14:56:58.551455: step 5382, loss 3.27399.
Train: 2018-08-02T14:56:58.613940: step 5383, loss 2.64438.
Train: 2018-08-02T14:56:58.676399: step 5384, loss 4.40729.
Train: 2018-08-02T14:56:58.738905: step 5385, loss 4.91098.
Train: 2018-08-02T14:56:58.785769: step 5386, loss 3.52583.
Train: 2018-08-02T14:56:58.848260: step 5387, loss 3.9036.
Train: 2018-08-02T14:56:58.910716: step 5388, loss 2.64437.
Train: 2018-08-02T14:56:58.973232: step 5389, loss 4.02952.
Train: 2018-08-02T14:56:59.020065: step 5390, loss 3.14807.
Test: 2018-08-02T14:56:59.223172: step 5390, loss 3.81863.
Train: 2018-08-02T14:56:59.285627: step 5391, loss 4.28137.
Train: 2018-08-02T14:56:59.348147: step 5392, loss 4.40729.
Train: 2018-08-02T14:56:59.395004: step 5393, loss 3.65176.
Train: 2018-08-02T14:56:59.457495: step 5394, loss 4.78506.
Train: 2018-08-02T14:56:59.519977: step 5395, loss 3.52583.
Train: 2018-08-02T14:56:59.566842: step 5396, loss 4.02952.
Train: 2018-08-02T14:56:59.629327: step 5397, loss 3.77768.
Train: 2018-08-02T14:56:59.691813: step 5398, loss 4.91098.
Train: 2018-08-02T14:56:59.754295: step 5399, loss 3.65176.
Train: 2018-08-02T14:56:59.801133: step 5400, loss 3.39991.
Test: 2018-08-02T14:57:00.004209: step 5400, loss 3.81863.
Train: 2018-08-02T14:57:00.535362: step 5401, loss 5.16283.
Train: 2018-08-02T14:57:00.597850: step 5402, loss 4.40729.
Train: 2018-08-02T14:57:00.660337: step 5403, loss 4.15545.
Train: 2018-08-02T14:57:00.707196: step 5404, loss 4.28137.
Train: 2018-08-02T14:57:00.769687: step 5405, loss 3.9036.
Train: 2018-08-02T14:57:00.832170: step 5406, loss 3.52583.
Train: 2018-08-02T14:57:00.879037: step 5407, loss 4.28137.
Train: 2018-08-02T14:57:00.941491: step 5408, loss 3.65176.
Train: 2018-08-02T14:57:01.003977: step 5409, loss 3.52583.
Train: 2018-08-02T14:57:01.066460: step 5410, loss 4.65914.
Test: 2018-08-02T14:57:01.253941: step 5410, loss 3.81863.
Train: 2018-08-02T14:57:01.316427: step 5411, loss 3.9036.
Train: 2018-08-02T14:57:01.378917: step 5412, loss 4.28137.
Train: 2018-08-02T14:57:01.441402: step 5413, loss 3.39991.
Train: 2018-08-02T14:57:01.503889: step 5414, loss 4.78506.
Train: 2018-08-02T14:57:01.550757: step 5415, loss 4.40729.
Train: 2018-08-02T14:57:01.613237: step 5416, loss 3.65176.
Train: 2018-08-02T14:57:01.675693: step 5417, loss 4.65914.
Train: 2018-08-02T14:57:01.722590: step 5418, loss 4.40729.
Train: 2018-08-02T14:57:01.785074: step 5419, loss 4.65914.
Train: 2018-08-02T14:57:01.847552: step 5420, loss 4.15545.
Test: 2018-08-02T14:57:02.034984: step 5420, loss 3.81863.
Train: 2018-08-02T14:57:02.097500: step 5421, loss 3.9036.
Train: 2018-08-02T14:57:02.159985: step 5422, loss 4.15545.
Train: 2018-08-02T14:57:02.222465: step 5423, loss 4.53321.
Train: 2018-08-02T14:57:02.284953: step 5424, loss 2.89622.
Train: 2018-08-02T14:57:02.347436: step 5425, loss 4.15545.
Train: 2018-08-02T14:57:02.394274: step 5426, loss 3.39991.
Train: 2018-08-02T14:57:02.456784: step 5427, loss 3.39991.
Train: 2018-08-02T14:57:02.519246: step 5428, loss 4.40729.
Train: 2018-08-02T14:57:02.566110: step 5429, loss 4.40729.
Train: 2018-08-02T14:57:02.628596: step 5430, loss 4.02952.
Test: 2018-08-02T14:57:02.831696: step 5430, loss 3.81863.
Train: 2018-08-02T14:57:02.878569: step 5431, loss 4.40729.
Train: 2018-08-02T14:57:02.941023: step 5432, loss 4.40729.
Train: 2018-08-02T14:57:03.003508: step 5433, loss 4.91098.
Train: 2018-08-02T14:57:03.065994: step 5434, loss 3.27399.
Train: 2018-08-02T14:57:03.112887: step 5435, loss 3.27399.
Train: 2018-08-02T14:57:03.159747: step 5436, loss 3.49225.
Train: 2018-08-02T14:57:03.222236: step 5437, loss 4.40729.
Train: 2018-08-02T14:57:03.284722: step 5438, loss 4.02952.
Train: 2018-08-02T14:57:03.331556: step 5439, loss 3.77768.
Train: 2018-08-02T14:57:03.394041: step 5440, loss 3.27399.
Test: 2018-08-02T14:57:03.597148: step 5440, loss 3.81863.
Train: 2018-08-02T14:57:03.659603: step 5441, loss 3.9036.
Train: 2018-08-02T14:57:03.706468: step 5442, loss 3.9036.
Train: 2018-08-02T14:57:03.768989: step 5443, loss 4.02952.
Train: 2018-08-02T14:57:03.831469: step 5444, loss 3.77768.
Train: 2018-08-02T14:57:03.893951: step 5445, loss 5.28875.
Train: 2018-08-02T14:57:03.940821: step 5446, loss 4.53321.
Train: 2018-08-02T14:57:04.003298: step 5447, loss 3.02214.
Train: 2018-08-02T14:57:04.065789: step 5448, loss 3.65176.
Train: 2018-08-02T14:57:04.112652: step 5449, loss 3.27399.
Train: 2018-08-02T14:57:04.175138: step 5450, loss 4.40729.
Test: 2018-08-02T14:57:04.378210: step 5450, loss 3.81863.
Train: 2018-08-02T14:57:04.440671: step 5451, loss 3.39991.
Train: 2018-08-02T14:57:04.487565: step 5452, loss 3.65176.
Train: 2018-08-02T14:57:04.550020: step 5453, loss 4.78506.
Train: 2018-08-02T14:57:04.612507: step 5454, loss 3.77768.
Train: 2018-08-02T14:57:04.674993: step 5455, loss 4.02952.
Train: 2018-08-02T14:57:04.721855: step 5456, loss 4.02952.
Train: 2018-08-02T14:57:04.784340: step 5457, loss 4.53321.
Train: 2018-08-02T14:57:04.846852: step 5458, loss 3.02214.
Train: 2018-08-02T14:57:04.893717: step 5459, loss 3.65176.
Train: 2018-08-02T14:57:04.956199: step 5460, loss 4.40729.
Test: 2018-08-02T14:57:05.143663: step 5460, loss 3.81863.
Train: 2018-08-02T14:57:05.206115: step 5461, loss 4.65914.
Train: 2018-08-02T14:57:05.268601: step 5462, loss 3.39991.
Train: 2018-08-02T14:57:05.331087: step 5463, loss 4.02952.
Train: 2018-08-02T14:57:05.393572: step 5464, loss 4.40729.
Train: 2018-08-02T14:57:05.440437: step 5465, loss 5.16283.
Train: 2018-08-02T14:57:05.502921: step 5466, loss 4.91098.
Train: 2018-08-02T14:57:05.565407: step 5467, loss 5.5406.
Train: 2018-08-02T14:57:05.612296: step 5468, loss 4.78506.
Train: 2018-08-02T14:57:05.674781: step 5469, loss 4.91098.
Train: 2018-08-02T14:57:05.737272: step 5470, loss 4.40729.
Test: 2018-08-02T14:57:05.940321: step 5470, loss 3.81863.
Train: 2018-08-02T14:57:05.987213: step 5471, loss 3.39991.
Train: 2018-08-02T14:57:06.049699: step 5472, loss 5.04232.
Train: 2018-08-02T14:57:06.112179: step 5473, loss 3.27399.
Train: 2018-08-02T14:57:06.174640: step 5474, loss 3.39991.
Train: 2018-08-02T14:57:06.221537: step 5475, loss 3.77768.
Train: 2018-08-02T14:57:06.283989: step 5476, loss 4.65914.
Train: 2018-08-02T14:57:06.346475: step 5477, loss 4.28137.
Train: 2018-08-02T14:57:06.393338: step 5478, loss 3.52583.
Train: 2018-08-02T14:57:06.455849: step 5479, loss 2.64437.
Train: 2018-08-02T14:57:06.518309: step 5480, loss 3.77768.
Test: 2018-08-02T14:57:06.721396: step 5480, loss 3.81863.
Train: 2018-08-02T14:57:06.783903: step 5481, loss 3.02214.
Train: 2018-08-02T14:57:06.830735: step 5482, loss 4.15545.
Train: 2018-08-02T14:57:06.893248: step 5483, loss 4.65914.
Train: 2018-08-02T14:57:06.955732: step 5484, loss 4.28137.
Train: 2018-08-02T14:57:07.018224: step 5485, loss 4.40729.
Train: 2018-08-02T14:57:07.080678: step 5486, loss 4.78506.
Train: 2018-08-02T14:57:07.127571: step 5487, loss 3.52583.
Train: 2018-08-02T14:57:07.190026: step 5488, loss 4.91098.
Train: 2018-08-02T14:57:07.252538: step 5489, loss 4.15545.
Train: 2018-08-02T14:57:07.299406: step 5490, loss 4.65914.
Test: 2018-08-02T14:57:07.502478: step 5490, loss 3.81863.
Train: 2018-08-02T14:57:07.564970: step 5491, loss 2.89622.
Train: 2018-08-02T14:57:07.627454: step 5492, loss 4.28137.
Train: 2018-08-02T14:57:07.674289: step 5493, loss 4.15545.
Train: 2018-08-02T14:57:07.736805: step 5494, loss 4.02952.
Train: 2018-08-02T14:57:07.799295: step 5495, loss 3.39991.
Train: 2018-08-02T14:57:07.846123: step 5496, loss 4.40729.
Train: 2018-08-02T14:57:07.908640: step 5497, loss 3.52583.
Train: 2018-08-02T14:57:07.971125: step 5498, loss 4.02952.
Train: 2018-08-02T14:57:08.033580: step 5499, loss 3.65176.
Train: 2018-08-02T14:57:08.080473: step 5500, loss 4.02952.
Test: 2018-08-02T14:57:08.283520: step 5500, loss 3.81863.
Train: 2018-08-02T14:57:08.799054: step 5501, loss 3.65176.
Train: 2018-08-02T14:57:08.861510: step 5502, loss 3.39991.
Train: 2018-08-02T14:57:08.908406: step 5503, loss 4.40729.
Train: 2018-08-02T14:57:08.970890: step 5504, loss 5.0369.
Train: 2018-08-02T14:57:09.033376: step 5505, loss 4.15545.
Train: 2018-08-02T14:57:09.095860: step 5506, loss 3.52583.
Train: 2018-08-02T14:57:09.142721: step 5507, loss 3.27399.
Train: 2018-08-02T14:57:09.205180: step 5508, loss 3.27399.
Train: 2018-08-02T14:57:09.267693: step 5509, loss 4.28137.
Train: 2018-08-02T14:57:09.314530: step 5510, loss 3.52583.
Test: 2018-08-02T14:57:09.517607: step 5510, loss 3.81863.
Train: 2018-08-02T14:57:09.580117: step 5511, loss 3.77768.
Train: 2018-08-02T14:57:09.642577: step 5512, loss 3.02214.
Train: 2018-08-02T14:57:09.705090: step 5513, loss 3.39991.
Train: 2018-08-02T14:57:09.751960: step 5514, loss 3.65176.
Train: 2018-08-02T14:57:09.814414: step 5515, loss 4.15545.
Train: 2018-08-02T14:57:09.876897: step 5516, loss 4.53321.
Train: 2018-08-02T14:57:09.923792: step 5517, loss 5.0369.
Train: 2018-08-02T14:57:09.986278: step 5518, loss 4.40729.
Train: 2018-08-02T14:57:10.048763: step 5519, loss 3.77768.
Train: 2018-08-02T14:57:10.111247: step 5520, loss 3.9036.
Test: 2018-08-02T14:57:10.298698: step 5520, loss 3.81863.
Train: 2018-08-02T14:57:10.361189: step 5521, loss 3.77768.
Train: 2018-08-02T14:57:10.423674: step 5522, loss 5.5406.
Train: 2018-08-02T14:57:10.486154: step 5523, loss 2.89622.
Train: 2018-08-02T14:57:10.533010: step 5524, loss 4.28137.
Train: 2018-08-02T14:57:10.595510: step 5525, loss 4.53321.
Train: 2018-08-02T14:57:10.657995: step 5526, loss 4.40729.
Train: 2018-08-02T14:57:10.704828: step 5527, loss 4.02952.
Train: 2018-08-02T14:57:10.767347: step 5528, loss 4.91098.
Train: 2018-08-02T14:57:10.829830: step 5529, loss 3.02214.
Train: 2018-08-02T14:57:10.876664: step 5530, loss 4.53321.
Test: 2018-08-02T14:57:11.079741: step 5530, loss 3.81863.
Train: 2018-08-02T14:57:11.142226: step 5531, loss 4.15545.
Train: 2018-08-02T14:57:11.204712: step 5532, loss 3.77768.
Train: 2018-08-02T14:57:11.267226: step 5533, loss 3.77768.
Train: 2018-08-02T14:57:11.314092: step 5534, loss 4.02952.
Train: 2018-08-02T14:57:11.376580: step 5535, loss 3.65176.
Train: 2018-08-02T14:57:11.439031: step 5536, loss 3.39991.
Train: 2018-08-02T14:57:11.501547: step 5537, loss 3.14807.
Train: 2018-08-02T14:57:11.564009: step 5538, loss 3.65176.
Train: 2018-08-02T14:57:11.610897: step 5539, loss 3.65176.
Train: 2018-08-02T14:57:11.673377: step 5540, loss 4.02952.
Test: 2018-08-02T14:57:11.876428: step 5540, loss 3.81863.
Train: 2018-08-02T14:57:11.938948: step 5541, loss 5.5406.
Train: 2018-08-02T14:57:11.985779: step 5542, loss 4.53321.
Train: 2018-08-02T14:57:12.048297: step 5543, loss 4.78506.
Train: 2018-08-02T14:57:12.110751: step 5544, loss 4.65914.
Train: 2018-08-02T14:57:12.157645: step 5545, loss 3.65176.
Train: 2018-08-02T14:57:12.220123: step 5546, loss 3.65176.
Train: 2018-08-02T14:57:12.282615: step 5547, loss 3.27399.
Train: 2018-08-02T14:57:12.329478: step 5548, loss 4.15545.
Train: 2018-08-02T14:57:12.391934: step 5549, loss 3.52583.
Train: 2018-08-02T14:57:12.454443: step 5550, loss 4.28137.
Test: 2018-08-02T14:57:12.641906: step 5550, loss 3.81863.
Train: 2018-08-02T14:57:12.704391: step 5551, loss 3.39991.
Train: 2018-08-02T14:57:12.766873: step 5552, loss 4.15545.
Train: 2018-08-02T14:57:12.829361: step 5553, loss 3.39991.
Train: 2018-08-02T14:57:12.891844: step 5554, loss 3.39991.
Train: 2018-08-02T14:57:12.938682: step 5555, loss 3.52583.
Train: 2018-08-02T14:57:13.001196: step 5556, loss 3.65176.
Train: 2018-08-02T14:57:13.063681: step 5557, loss 3.27399.
Train: 2018-08-02T14:57:13.110515: step 5558, loss 3.02214.
Train: 2018-08-02T14:57:13.173033: step 5559, loss 3.27399.
Train: 2018-08-02T14:57:13.235485: step 5560, loss 3.52583.
Test: 2018-08-02T14:57:13.422941: step 5560, loss 3.81863.
Train: 2018-08-02T14:57:13.485428: step 5561, loss 4.91098.
Train: 2018-08-02T14:57:13.547942: step 5562, loss 4.53321.
Train: 2018-08-02T14:57:13.610398: step 5563, loss 3.27399.
Train: 2018-08-02T14:57:13.672885: step 5564, loss 4.78506.
Train: 2018-08-02T14:57:13.735394: step 5565, loss 4.28137.
Train: 2018-08-02T14:57:13.782233: step 5566, loss 4.78506.
Train: 2018-08-02T14:57:13.844742: step 5567, loss 3.65176.
Train: 2018-08-02T14:57:13.907205: step 5568, loss 4.28137.
Train: 2018-08-02T14:57:13.969689: step 5569, loss 4.15545.
Train: 2018-08-02T14:57:14.016580: step 5570, loss 3.77768.
Test: 2018-08-02T14:57:14.219655: step 5570, loss 3.81863.
Train: 2018-08-02T14:57:14.282115: step 5571, loss 5.16283.
Train: 2018-08-02T14:57:14.344601: step 5572, loss 3.27399.
Train: 2018-08-02T14:57:14.407086: step 5573, loss 4.40729.
Train: 2018-08-02T14:57:14.453952: step 5574, loss 4.78506.
Train: 2018-08-02T14:57:14.516436: step 5575, loss 3.77768.
Train: 2018-08-02T14:57:14.578921: step 5576, loss 4.65914.
Train: 2018-08-02T14:57:14.625810: step 5577, loss 3.52583.
Train: 2018-08-02T14:57:14.688302: step 5578, loss 4.02952.
Train: 2018-08-02T14:57:14.750787: step 5579, loss 4.15545.
Train: 2018-08-02T14:57:14.797650: step 5580, loss 4.28137.
Test: 2018-08-02T14:57:15.000721: step 5580, loss 3.81863.
Train: 2018-08-02T14:57:15.063182: step 5581, loss 3.39991.
Train: 2018-08-02T14:57:15.125698: step 5582, loss 3.9036.
Train: 2018-08-02T14:57:15.172563: step 5583, loss 4.15545.
Train: 2018-08-02T14:57:15.235019: step 5584, loss 4.40729.
Train: 2018-08-02T14:57:15.297534: step 5585, loss 4.15545.
Train: 2018-08-02T14:57:15.344368: step 5586, loss 3.52583.
Train: 2018-08-02T14:57:15.391258: step 5587, loss 5.10406.
Train: 2018-08-02T14:57:15.453742: step 5588, loss 3.9036.
Train: 2018-08-02T14:57:15.516233: step 5589, loss 4.15545.
Train: 2018-08-02T14:57:15.563096: step 5590, loss 2.89622.
Test: 2018-08-02T14:57:15.766142: step 5590, loss 3.81863.
Train: 2018-08-02T14:57:15.828659: step 5591, loss 4.53321.
Train: 2018-08-02T14:57:15.891143: step 5592, loss 4.28137.
Train: 2018-08-02T14:57:15.953629: step 5593, loss 3.77768.
Train: 2018-08-02T14:57:16.000497: step 5594, loss 3.39991.
Train: 2018-08-02T14:57:16.062949: step 5595, loss 2.89622.
Train: 2018-08-02T14:57:16.125435: step 5596, loss 4.40729.
Train: 2018-08-02T14:57:16.172298: step 5597, loss 5.0369.
Train: 2018-08-02T14:57:16.234816: step 5598, loss 3.77768.
Train: 2018-08-02T14:57:16.297299: step 5599, loss 4.15545.
Train: 2018-08-02T14:57:16.344132: step 5600, loss 4.02952.
Test: 2018-08-02T14:57:16.547210: step 5600, loss 3.81863.
Train: 2018-08-02T14:57:17.047123: step 5601, loss 3.9036.
Train: 2018-08-02T14:57:17.109608: step 5602, loss 3.39991.
Train: 2018-08-02T14:57:17.156474: step 5603, loss 4.91098.
Train: 2018-08-02T14:57:17.218960: step 5604, loss 3.39991.
Train: 2018-08-02T14:57:17.281445: step 5605, loss 4.53321.
Train: 2018-08-02T14:57:17.328308: step 5606, loss 4.15545.
Train: 2018-08-02T14:57:17.390763: step 5607, loss 4.40729.
Train: 2018-08-02T14:57:17.453278: step 5608, loss 3.52583.
Train: 2018-08-02T14:57:17.515736: step 5609, loss 3.65176.
Train: 2018-08-02T14:57:17.562627: step 5610, loss 4.15545.
Test: 2018-08-02T14:57:17.765699: step 5610, loss 3.81863.
Train: 2018-08-02T14:57:17.828190: step 5611, loss 3.52583.
Train: 2018-08-02T14:57:17.890675: step 5612, loss 4.28137.
Train: 2018-08-02T14:57:17.937510: step 5613, loss 3.14807.
Train: 2018-08-02T14:57:17.999997: step 5614, loss 4.02952.
Train: 2018-08-02T14:57:18.062511: step 5615, loss 3.14807.
Train: 2018-08-02T14:57:18.124995: step 5616, loss 3.27399.
Train: 2018-08-02T14:57:18.171860: step 5617, loss 2.51845.
Train: 2018-08-02T14:57:18.234317: step 5618, loss 3.39991.
Train: 2018-08-02T14:57:18.296826: step 5619, loss 4.53321.
Train: 2018-08-02T14:57:18.343695: step 5620, loss 4.65914.
Test: 2018-08-02T14:57:18.546742: step 5620, loss 3.81863.
Train: 2018-08-02T14:57:18.609257: step 5621, loss 4.53321.
Train: 2018-08-02T14:57:18.671740: step 5622, loss 4.02952.
Train: 2018-08-02T14:57:18.718601: step 5623, loss 3.77768.
Train: 2018-08-02T14:57:18.781117: step 5624, loss 3.9036.
Train: 2018-08-02T14:57:18.843549: step 5625, loss 3.65176.
Train: 2018-08-02T14:57:18.890444: step 5626, loss 3.9036.
Train: 2018-08-02T14:57:18.952898: step 5627, loss 3.77768.
Train: 2018-08-02T14:57:19.015382: step 5628, loss 3.65176.
Train: 2018-08-02T14:57:19.077876: step 5629, loss 3.52583.
Train: 2018-08-02T14:57:19.140385: step 5630, loss 4.78506.
Test: 2018-08-02T14:57:19.327810: step 5630, loss 3.81863.
Train: 2018-08-02T14:57:19.390326: step 5631, loss 5.0369.
Train: 2018-08-02T14:57:19.452811: step 5632, loss 3.02214.
Train: 2018-08-02T14:57:19.515297: step 5633, loss 4.02952.
Train: 2018-08-02T14:57:19.577781: step 5634, loss 3.27399.
Train: 2018-08-02T14:57:19.640237: step 5635, loss 3.77768.
Train: 2018-08-02T14:57:19.687100: step 5636, loss 3.52583.
Train: 2018-08-02T14:57:19.749615: step 5637, loss 2.7703.
Train: 2018-08-02T14:57:19.812071: step 5638, loss 5.91836.
Train: 2018-08-02T14:57:19.858934: step 5639, loss 4.28137.
Train: 2018-08-02T14:57:19.921445: step 5640, loss 5.16283.
Test: 2018-08-02T14:57:20.124498: step 5640, loss 3.81863.
Train: 2018-08-02T14:57:20.187013: step 5641, loss 3.9036.
Train: 2018-08-02T14:57:20.249498: step 5642, loss 4.15545.
Train: 2018-08-02T14:57:20.296364: step 5643, loss 3.52583.
Train: 2018-08-02T14:57:20.358849: step 5644, loss 3.27399.
Train: 2018-08-02T14:57:20.421336: step 5645, loss 4.15545.
Train: 2018-08-02T14:57:20.483787: step 5646, loss 4.91098.
Train: 2018-08-02T14:57:20.530653: step 5647, loss 4.28137.
Train: 2018-08-02T14:57:20.593169: step 5648, loss 4.40729.
Train: 2018-08-02T14:57:20.655651: step 5649, loss 4.02952.
Train: 2018-08-02T14:57:20.702488: step 5650, loss 3.65176.
Test: 2018-08-02T14:57:20.905588: step 5650, loss 3.81863.
Train: 2018-08-02T14:57:20.968080: step 5651, loss 4.15545.
Train: 2018-08-02T14:57:21.014915: step 5652, loss 4.02952.
Train: 2018-08-02T14:57:21.077399: step 5653, loss 3.52583.
Train: 2018-08-02T14:57:21.139922: step 5654, loss 3.27399.
Train: 2018-08-02T14:57:21.202401: step 5655, loss 4.91098.
Train: 2018-08-02T14:57:21.249233: step 5656, loss 4.53321.
Train: 2018-08-02T14:57:21.311750: step 5657, loss 3.77768.
Train: 2018-08-02T14:57:21.374238: step 5658, loss 3.77768.
Train: 2018-08-02T14:57:21.421069: step 5659, loss 3.77768.
Train: 2018-08-02T14:57:21.483555: step 5660, loss 4.02952.
Test: 2018-08-02T14:57:21.686665: step 5660, loss 3.81863.
Train: 2018-08-02T14:57:21.749141: step 5661, loss 4.15545.
Train: 2018-08-02T14:57:21.811632: step 5662, loss 4.28137.
Train: 2018-08-02T14:57:21.858466: step 5663, loss 3.27399.
Train: 2018-08-02T14:57:21.920983: step 5664, loss 2.89622.
Train: 2018-08-02T14:57:21.983464: step 5665, loss 3.52583.
Train: 2018-08-02T14:57:22.045948: step 5666, loss 3.9036.
Train: 2018-08-02T14:57:22.092787: step 5667, loss 4.15545.
Train: 2018-08-02T14:57:22.155304: step 5668, loss 3.27399.
Train: 2018-08-02T14:57:22.217790: step 5669, loss 3.77768.
Train: 2018-08-02T14:57:22.264622: step 5670, loss 3.02214.
Test: 2018-08-02T14:57:22.467699: step 5670, loss 3.81863.
Train: 2018-08-02T14:57:22.530185: step 5671, loss 4.28137.
Train: 2018-08-02T14:57:22.592670: step 5672, loss 3.9036.
Train: 2018-08-02T14:57:22.655186: step 5673, loss 4.02952.
Train: 2018-08-02T14:57:22.717641: step 5674, loss 4.65914.
Train: 2018-08-02T14:57:22.780157: step 5675, loss 3.9036.
Train: 2018-08-02T14:57:22.827019: step 5676, loss 3.77768.
Train: 2018-08-02T14:57:22.889476: step 5677, loss 4.78506.
Train: 2018-08-02T14:57:22.951992: step 5678, loss 3.39991.
Train: 2018-08-02T14:57:22.998825: step 5679, loss 6.17021.
Train: 2018-08-02T14:57:23.061341: step 5680, loss 4.02952.
Test: 2018-08-02T14:57:23.264387: step 5680, loss 3.81863.
Train: 2018-08-02T14:57:23.326898: step 5681, loss 5.28875.
Train: 2018-08-02T14:57:23.389387: step 5682, loss 3.65176.
Train: 2018-08-02T14:57:23.436222: step 5683, loss 4.02952.
Train: 2018-08-02T14:57:23.498742: step 5684, loss 3.39991.
Train: 2018-08-02T14:57:23.561224: step 5685, loss 5.16283.
Train: 2018-08-02T14:57:23.608087: step 5686, loss 3.77768.
Train: 2018-08-02T14:57:23.670542: step 5687, loss 4.65914.
Train: 2018-08-02T14:57:23.733059: step 5688, loss 4.02952.
Train: 2018-08-02T14:57:23.795544: step 5689, loss 4.02952.
Train: 2018-08-02T14:57:23.842377: step 5690, loss 4.15545.
Test: 2018-08-02T14:57:24.045453: step 5690, loss 3.81863.
Train: 2018-08-02T14:57:24.107971: step 5691, loss 4.28137.
Train: 2018-08-02T14:57:24.170425: step 5692, loss 3.77768.
Train: 2018-08-02T14:57:24.232941: step 5693, loss 3.39991.
Train: 2018-08-02T14:57:24.279776: step 5694, loss 4.40729.
Train: 2018-08-02T14:57:24.342260: step 5695, loss 4.78506.
Train: 2018-08-02T14:57:24.404776: step 5696, loss 3.39991.
Train: 2018-08-02T14:57:24.451610: step 5697, loss 4.02952.
Train: 2018-08-02T14:57:24.514098: step 5698, loss 3.77768.
Train: 2018-08-02T14:57:24.576611: step 5699, loss 3.39991.
Train: 2018-08-02T14:57:24.623443: step 5700, loss 4.78506.
Test: 2018-08-02T14:57:24.826551: step 5700, loss 3.81863.
Train: 2018-08-02T14:57:25.357677: step 5701, loss 3.52583.
Train: 2018-08-02T14:57:25.420162: step 5702, loss 3.52583.
Train: 2018-08-02T14:57:25.482650: step 5703, loss 4.91098.
Train: 2018-08-02T14:57:25.545129: step 5704, loss 4.65914.
Train: 2018-08-02T14:57:25.592001: step 5705, loss 3.52583.
Train: 2018-08-02T14:57:25.654453: step 5706, loss 4.15545.
Train: 2018-08-02T14:57:25.716937: step 5707, loss 3.9036.
Train: 2018-08-02T14:57:25.779426: step 5708, loss 4.78506.
Train: 2018-08-02T14:57:25.826288: step 5709, loss 4.40729.
Train: 2018-08-02T14:57:25.888772: step 5710, loss 3.65176.
Test: 2018-08-02T14:57:26.091880: step 5710, loss 3.81863.
Train: 2018-08-02T14:57:26.154337: step 5711, loss 4.28137.
Train: 2018-08-02T14:57:26.216846: step 5712, loss 4.02952.
Train: 2018-08-02T14:57:26.263712: step 5713, loss 4.65914.
Train: 2018-08-02T14:57:26.326171: step 5714, loss 5.79244.
Train: 2018-08-02T14:57:26.388657: step 5715, loss 4.28137.
Train: 2018-08-02T14:57:26.435519: step 5716, loss 3.02214.
Train: 2018-08-02T14:57:26.498006: step 5717, loss 5.0369.
Train: 2018-08-02T14:57:26.560521: step 5718, loss 3.39991.
Train: 2018-08-02T14:57:26.622977: step 5719, loss 2.7703.
Train: 2018-08-02T14:57:26.669840: step 5720, loss 5.91836.
Test: 2018-08-02T14:57:26.872917: step 5720, loss 3.81863.
Train: 2018-08-02T14:57:26.935433: step 5721, loss 3.9036.
Train: 2018-08-02T14:57:26.997918: step 5722, loss 4.02952.
Train: 2018-08-02T14:57:27.060373: step 5723, loss 4.78506.
Train: 2018-08-02T14:57:27.107268: step 5724, loss 4.02952.
Train: 2018-08-02T14:57:27.169754: step 5725, loss 3.77768.
Train: 2018-08-02T14:57:27.232238: step 5726, loss 3.77768.
Train: 2018-08-02T14:57:27.279072: step 5727, loss 4.02952.
Train: 2018-08-02T14:57:27.341589: step 5728, loss 4.78506.
Train: 2018-08-02T14:57:27.404044: step 5729, loss 4.40729.
Train: 2018-08-02T14:57:27.466559: step 5730, loss 4.15545.
Test: 2018-08-02T14:57:27.654008: step 5730, loss 3.81863.
Train: 2018-08-02T14:57:27.716500: step 5731, loss 4.15545.
Train: 2018-08-02T14:57:27.778986: step 5732, loss 3.27399.
Train: 2018-08-02T14:57:27.841471: step 5733, loss 3.9036.
Train: 2018-08-02T14:57:27.903953: step 5734, loss 3.77768.
Train: 2018-08-02T14:57:27.950821: step 5735, loss 4.15545.
Train: 2018-08-02T14:57:28.013307: step 5736, loss 3.39991.
Train: 2018-08-02T14:57:28.075768: step 5737, loss 4.40729.
Train: 2018-08-02T14:57:28.122654: step 5738, loss 3.22362.
Train: 2018-08-02T14:57:28.185140: step 5739, loss 3.52583.
Train: 2018-08-02T14:57:28.232004: step 5740, loss 3.65176.
Test: 2018-08-02T14:57:28.435050: step 5740, loss 3.81863.
Train: 2018-08-02T14:57:28.497536: step 5741, loss 4.15545.
Train: 2018-08-02T14:57:28.560052: step 5742, loss 4.15545.
Train: 2018-08-02T14:57:28.606916: step 5743, loss 3.65176.
Train: 2018-08-02T14:57:28.669403: step 5744, loss 3.77768.
Train: 2018-08-02T14:57:28.731887: step 5745, loss 4.53321.
Train: 2018-08-02T14:57:28.794343: step 5746, loss 4.28137.
Train: 2018-08-02T14:57:28.841236: step 5747, loss 3.27399.
Train: 2018-08-02T14:57:28.903694: step 5748, loss 4.65914.
Train: 2018-08-02T14:57:28.966203: step 5749, loss 3.77768.
Train: 2018-08-02T14:57:29.013069: step 5750, loss 4.40729.
Test: 2018-08-02T14:57:29.216149: step 5750, loss 3.81863.
Train: 2018-08-02T14:57:29.278603: step 5751, loss 2.7703.
Train: 2018-08-02T14:57:29.341090: step 5752, loss 3.65176.
Train: 2018-08-02T14:57:29.403606: step 5753, loss 3.65176.
Train: 2018-08-02T14:57:29.481681: step 5754, loss 3.27399.
Train: 2018-08-02T14:57:29.528546: step 5755, loss 4.02952.
Train: 2018-08-02T14:57:29.591061: step 5756, loss 3.9036.
Train: 2018-08-02T14:57:29.653547: step 5757, loss 4.40729.
Train: 2018-08-02T14:57:29.700381: step 5758, loss 4.65914.
Train: 2018-08-02T14:57:29.762892: step 5759, loss 4.15545.
Train: 2018-08-02T14:57:29.825351: step 5760, loss 4.91098.
Test: 2018-08-02T14:57:30.028430: step 5760, loss 3.81863.
Train: 2018-08-02T14:57:30.075323: step 5761, loss 3.27399.
Train: 2018-08-02T14:57:30.137814: step 5762, loss 4.78506.
Train: 2018-08-02T14:57:30.200293: step 5763, loss 4.02952.
Train: 2018-08-02T14:57:30.262776: step 5764, loss 4.53321.
Train: 2018-08-02T14:57:30.309643: step 5765, loss 5.28875.
Train: 2018-08-02T14:57:30.372124: step 5766, loss 3.9036.
Train: 2018-08-02T14:57:30.434584: step 5767, loss 2.64438.
Train: 2018-08-02T14:57:30.481478: step 5768, loss 3.77768.
Train: 2018-08-02T14:57:30.543963: step 5769, loss 3.77768.
Train: 2018-08-02T14:57:30.606443: step 5770, loss 3.77768.
Test: 2018-08-02T14:57:30.809496: step 5770, loss 3.81863.
Train: 2018-08-02T14:57:30.903224: step 5771, loss 4.02952.
Train: 2018-08-02T14:57:30.965739: step 5772, loss 5.16283.
Train: 2018-08-02T14:57:31.028226: step 5773, loss 3.65176.
Train: 2018-08-02T14:57:31.090704: step 5774, loss 4.15545.
Train: 2018-08-02T14:57:31.137544: step 5775, loss 3.77768.
Train: 2018-08-02T14:57:31.200059: step 5776, loss 4.40729.
Train: 2018-08-02T14:57:31.262547: step 5777, loss 3.14807.
Train: 2018-08-02T14:57:31.309377: step 5778, loss 4.53321.
Train: 2018-08-02T14:57:31.371889: step 5779, loss 4.15545.
Train: 2018-08-02T14:57:31.434349: step 5780, loss 4.53321.
Test: 2018-08-02T14:57:31.637425: step 5780, loss 3.81863.
Train: 2018-08-02T14:57:31.684321: step 5781, loss 4.02952.
Train: 2018-08-02T14:57:31.746809: step 5782, loss 4.91098.
Train: 2018-08-02T14:57:31.809262: step 5783, loss 4.53321.
Train: 2018-08-02T14:57:31.871748: step 5784, loss 3.52583.
Train: 2018-08-02T14:57:31.918644: step 5785, loss 3.39991.
Train: 2018-08-02T14:57:31.981122: step 5786, loss 4.02952.
Train: 2018-08-02T14:57:32.043581: step 5787, loss 4.40729.
Train: 2018-08-02T14:57:32.106069: step 5788, loss 3.52583.
Train: 2018-08-02T14:57:32.152963: step 5789, loss 4.15545.
Train: 2018-08-02T14:57:32.215416: step 5790, loss 4.40729.
Test: 2018-08-02T14:57:32.402876: step 5790, loss 3.81863.
Train: 2018-08-02T14:57:32.465359: step 5791, loss 4.15545.
Train: 2018-08-02T14:57:32.527874: step 5792, loss 4.02952.
Train: 2018-08-02T14:57:32.590359: step 5793, loss 3.77768.
Train: 2018-08-02T14:57:32.652846: step 5794, loss 3.65176.
Train: 2018-08-02T14:57:32.699710: step 5795, loss 4.40729.
Train: 2018-08-02T14:57:32.762190: step 5796, loss 3.39991.
Train: 2018-08-02T14:57:32.824679: step 5797, loss 3.14807.
Train: 2018-08-02T14:57:32.887171: step 5798, loss 3.39991.
Train: 2018-08-02T14:57:32.934028: step 5799, loss 4.40729.
Train: 2018-08-02T14:57:32.996519: step 5800, loss 2.89622.
Test: 2018-08-02T14:57:33.199560: step 5800, loss 3.81863.
Train: 2018-08-02T14:57:33.715094: step 5801, loss 3.39991.
Train: 2018-08-02T14:57:33.777549: step 5802, loss 3.39991.
Train: 2018-08-02T14:57:33.824439: step 5803, loss 4.65914.
Train: 2018-08-02T14:57:33.886928: step 5804, loss 4.28153.
Train: 2018-08-02T14:57:33.949411: step 5805, loss 3.9036.
Train: 2018-08-02T14:57:34.011870: step 5806, loss 3.39991.
Train: 2018-08-02T14:57:34.058765: step 5807, loss 4.78506.
Train: 2018-08-02T14:57:34.121221: step 5808, loss 3.14807.
Train: 2018-08-02T14:57:34.183735: step 5809, loss 4.78506.
Train: 2018-08-02T14:57:34.230570: step 5810, loss 3.52583.
Test: 2018-08-02T14:57:34.433647: step 5810, loss 3.81863.
Train: 2018-08-02T14:57:34.496132: step 5811, loss 3.9036.
Train: 2018-08-02T14:57:34.558618: step 5812, loss 4.15545.
Train: 2018-08-02T14:57:34.621105: step 5813, loss 3.52583.
Train: 2018-08-02T14:57:34.667969: step 5814, loss 2.89622.
Train: 2018-08-02T14:57:34.730483: step 5815, loss 4.02952.
Train: 2018-08-02T14:57:34.792938: step 5816, loss 4.40729.
Train: 2018-08-02T14:57:34.839832: step 5817, loss 3.9036.
Train: 2018-08-02T14:57:34.902319: step 5818, loss 3.14807.
Train: 2018-08-02T14:57:34.964803: step 5819, loss 4.40729.
Train: 2018-08-02T14:57:35.011663: step 5820, loss 4.53321.
Test: 2018-08-02T14:57:35.214738: step 5820, loss 3.81863.
Train: 2018-08-02T14:57:35.277200: step 5821, loss 4.28137.
Train: 2018-08-02T14:57:35.339715: step 5822, loss 3.9036.
Train: 2018-08-02T14:57:35.402169: step 5823, loss 4.28137.
Train: 2018-08-02T14:57:35.464686: step 5824, loss 4.15545.
Train: 2018-08-02T14:57:35.511547: step 5825, loss 4.02952.
Train: 2018-08-02T14:57:35.574035: step 5826, loss 3.65176.
Train: 2018-08-02T14:57:35.636517: step 5827, loss 3.9036.
Train: 2018-08-02T14:57:35.683385: step 5828, loss 4.15545.
Train: 2018-08-02T14:57:35.745872: step 5829, loss 3.39991.
Train: 2018-08-02T14:57:35.808324: step 5830, loss 4.91098.
Test: 2018-08-02T14:57:36.011401: step 5830, loss 3.81863.
Train: 2018-08-02T14:57:36.058291: step 5831, loss 3.65176.
Train: 2018-08-02T14:57:36.120782: step 5832, loss 4.78506.
Train: 2018-08-02T14:57:36.183267: step 5833, loss 3.39991.
Train: 2018-08-02T14:57:36.245725: step 5834, loss 3.27399.
Train: 2018-08-02T14:57:36.308238: step 5835, loss 4.78506.
Train: 2018-08-02T14:57:36.355098: step 5836, loss 4.40729.
Train: 2018-08-02T14:57:36.417557: step 5837, loss 5.79244.
Train: 2018-08-02T14:57:36.480043: step 5838, loss 2.64437.
Train: 2018-08-02T14:57:36.526938: step 5839, loss 3.65176.
Train: 2018-08-02T14:57:36.589393: step 5840, loss 4.02952.
Test: 2018-08-02T14:57:36.792470: step 5840, loss 3.81863.
Train: 2018-08-02T14:57:36.854985: step 5841, loss 3.52583.
Train: 2018-08-02T14:57:36.901819: step 5842, loss 3.77768.
Train: 2018-08-02T14:57:36.964305: step 5843, loss 3.39991.
Train: 2018-08-02T14:57:37.026791: step 5844, loss 3.9036.
Train: 2018-08-02T14:57:37.073655: step 5845, loss 3.65176.
Train: 2018-08-02T14:57:37.136139: step 5846, loss 3.65176.
Train: 2018-08-02T14:57:37.198654: step 5847, loss 4.91098.
Train: 2018-08-02T14:57:37.245489: step 5848, loss 4.91098.
Train: 2018-08-02T14:57:37.307973: step 5849, loss 3.14807.
Train: 2018-08-02T14:57:37.370489: step 5850, loss 3.9036.
Test: 2018-08-02T14:57:37.573561: step 5850, loss 3.81863.
Train: 2018-08-02T14:57:37.636052: step 5851, loss 3.77768.
Train: 2018-08-02T14:57:37.682917: step 5852, loss 3.52583.
Train: 2018-08-02T14:57:37.745372: step 5853, loss 4.40729.
Train: 2018-08-02T14:57:37.807858: step 5854, loss 4.65914.
Train: 2018-08-02T14:57:37.870343: step 5855, loss 4.40729.
Train: 2018-08-02T14:57:37.917206: step 5856, loss 4.78506.
Train: 2018-08-02T14:57:37.979722: step 5857, loss 4.28137.
Train: 2018-08-02T14:57:38.042177: step 5858, loss 4.02952.
Train: 2018-08-02T14:57:38.104693: step 5859, loss 4.02952.
Train: 2018-08-02T14:57:38.151526: step 5860, loss 4.15545.
Test: 2018-08-02T14:57:38.354602: step 5860, loss 3.81863.
Train: 2018-08-02T14:57:38.417119: step 5861, loss 3.27399.
Train: 2018-08-02T14:57:38.479603: step 5862, loss 4.91098.
Train: 2018-08-02T14:57:38.526439: step 5863, loss 5.16283.
Train: 2018-08-02T14:57:38.588955: step 5864, loss 4.02952.
Train: 2018-08-02T14:57:38.651410: step 5865, loss 4.40729.
Train: 2018-08-02T14:57:38.713917: step 5866, loss 4.28137.
Train: 2018-08-02T14:57:38.760785: step 5867, loss 4.91098.
Train: 2018-08-02T14:57:38.823246: step 5868, loss 4.02952.
Train: 2018-08-02T14:57:38.885762: step 5869, loss 3.52583.
Train: 2018-08-02T14:57:38.932623: step 5870, loss 3.77768.
Test: 2018-08-02T14:57:39.135694: step 5870, loss 3.81863.
Train: 2018-08-02T14:57:39.198186: step 5871, loss 3.39991.
Train: 2018-08-02T14:57:39.260673: step 5872, loss 4.28137.
Train: 2018-08-02T14:57:39.323126: step 5873, loss 4.28137.
Train: 2018-08-02T14:57:39.385640: step 5874, loss 3.9036.
Train: 2018-08-02T14:57:39.432502: step 5875, loss 4.02952.
Train: 2018-08-02T14:57:39.494986: step 5876, loss 3.77768.
Train: 2018-08-02T14:57:39.557447: step 5877, loss 4.40729.
Train: 2018-08-02T14:57:39.604310: step 5878, loss 4.53321.
Train: 2018-08-02T14:57:39.666826: step 5879, loss 4.91098.
Train: 2018-08-02T14:57:39.729282: step 5880, loss 4.91098.
Test: 2018-08-02T14:57:39.916767: step 5880, loss 3.81863.
Train: 2018-08-02T14:57:39.979223: step 5881, loss 4.15545.
Train: 2018-08-02T14:57:40.041739: step 5882, loss 3.27399.
Train: 2018-08-02T14:57:40.104193: step 5883, loss 4.53321.
Train: 2018-08-02T14:57:40.151060: step 5884, loss 3.14807.
Train: 2018-08-02T14:57:40.213578: step 5885, loss 3.77768.
Train: 2018-08-02T14:57:40.276060: step 5886, loss 4.02952.
Train: 2018-08-02T14:57:40.338544: step 5887, loss 3.39991.
Train: 2018-08-02T14:57:40.385410: step 5888, loss 4.28137.
Train: 2018-08-02T14:57:40.432242: step 5889, loss 4.29816.
Train: 2018-08-02T14:57:40.494728: step 5890, loss 3.14807.
Test: 2018-08-02T14:57:40.697836: step 5890, loss 3.81863.
Train: 2018-08-02T14:57:40.760291: step 5891, loss 4.28137.
Train: 2018-08-02T14:57:40.807155: step 5892, loss 3.27399.
Train: 2018-08-02T14:57:40.869671: step 5893, loss 3.14807.
Train: 2018-08-02T14:57:40.932156: step 5894, loss 4.91098.
Train: 2018-08-02T14:57:40.994636: step 5895, loss 4.91098.
Train: 2018-08-02T14:57:41.041505: step 5896, loss 4.78506.
Train: 2018-08-02T14:57:41.103960: step 5897, loss 4.28137.
Train: 2018-08-02T14:57:41.166476: step 5898, loss 4.02952.
Train: 2018-08-02T14:57:41.213341: step 5899, loss 4.40729.
Train: 2018-08-02T14:57:41.275794: step 5900, loss 4.15545.
Test: 2018-08-02T14:57:41.478896: step 5900, loss 3.81863.
Train: 2018-08-02T14:57:41.978785: step 5901, loss 4.53321.
Train: 2018-08-02T14:57:42.041270: step 5902, loss 3.52583.
Train: 2018-08-02T14:57:42.103755: step 5903, loss 4.53321.
Train: 2018-08-02T14:57:42.150590: step 5904, loss 2.51845.
Train: 2018-08-02T14:57:42.213074: step 5905, loss 3.77768.
Train: 2018-08-02T14:57:42.275592: step 5906, loss 4.78506.
Train: 2018-08-02T14:57:42.322454: step 5907, loss 4.28137.
Train: 2018-08-02T14:57:42.384942: step 5908, loss 3.65176.
Train: 2018-08-02T14:57:42.447425: step 5909, loss 3.52583.
Train: 2018-08-02T14:57:42.494291: step 5910, loss 4.65914.
Test: 2018-08-02T14:57:42.697367: step 5910, loss 3.81863.
Train: 2018-08-02T14:57:42.759823: step 5911, loss 3.39991.
Train: 2018-08-02T14:57:42.822331: step 5912, loss 4.02952.
Train: 2018-08-02T14:57:42.869201: step 5913, loss 3.52583.
Train: 2018-08-02T14:57:42.931688: step 5914, loss 4.02952.
Train: 2018-08-02T14:57:42.994169: step 5915, loss 3.65176.
Train: 2018-08-02T14:57:43.041037: step 5916, loss 3.39991.
Train: 2018-08-02T14:57:43.103491: step 5917, loss 4.02952.
Train: 2018-08-02T14:57:43.166008: step 5918, loss 3.77768.
Train: 2018-08-02T14:57:43.212841: step 5919, loss 3.52583.
Train: 2018-08-02T14:57:43.275358: step 5920, loss 4.28137.
Test: 2018-08-02T14:57:43.478428: step 5920, loss 3.81863.
Train: 2018-08-02T14:57:43.540889: step 5921, loss 4.02952.
Train: 2018-08-02T14:57:43.603404: step 5922, loss 3.27399.
Train: 2018-08-02T14:57:43.650272: step 5923, loss 3.9036.
Train: 2018-08-02T14:57:43.712755: step 5924, loss 4.15545.
Train: 2018-08-02T14:57:43.775242: step 5925, loss 3.77768.
Train: 2018-08-02T14:57:43.837728: step 5926, loss 3.77768.
Train: 2018-08-02T14:57:43.884591: step 5927, loss 4.02952.
Train: 2018-08-02T14:57:43.947075: step 5928, loss 3.65176.
Train: 2018-08-02T14:57:44.009531: step 5929, loss 4.02952.
Train: 2018-08-02T14:57:44.056423: step 5930, loss 3.39991.
Test: 2018-08-02T14:57:44.259470: step 5930, loss 3.81863.
Train: 2018-08-02T14:57:44.321955: step 5931, loss 3.9036.
Train: 2018-08-02T14:57:44.384441: step 5932, loss 4.02952.
Train: 2018-08-02T14:57:44.446927: step 5933, loss 4.15545.
Train: 2018-08-02T14:57:44.493825: step 5934, loss 4.15545.
Train: 2018-08-02T14:57:44.556278: step 5935, loss 4.28137.
Train: 2018-08-02T14:57:44.618763: step 5936, loss 3.52583.
Train: 2018-08-02T14:57:44.665651: step 5937, loss 4.78506.
Train: 2018-08-02T14:57:44.728113: step 5938, loss 3.52583.
Train: 2018-08-02T14:57:44.790598: step 5939, loss 5.41467.
Train: 2018-08-02T14:57:44.853085: step 5940, loss 4.02952.
Test: 2018-08-02T14:57:45.040562: step 5940, loss 3.81863.
Train: 2018-08-02T14:57:45.103024: step 5941, loss 3.52583.
Train: 2018-08-02T14:57:45.165507: step 5942, loss 3.52583.
Train: 2018-08-02T14:57:45.228024: step 5943, loss 4.28137.
Train: 2018-08-02T14:57:45.274859: step 5944, loss 3.65176.
Train: 2018-08-02T14:57:45.337345: step 5945, loss 3.39991.
Train: 2018-08-02T14:57:45.399828: step 5946, loss 3.27399.
Train: 2018-08-02T14:57:45.462325: step 5947, loss 4.28137.
Train: 2018-08-02T14:57:45.509178: step 5948, loss 4.02952.
Train: 2018-08-02T14:57:45.571694: step 5949, loss 4.78506.
Train: 2018-08-02T14:57:45.634178: step 5950, loss 4.28137.
Test: 2018-08-02T14:57:45.821604: step 5950, loss 3.81863.
Train: 2018-08-02T14:57:45.884123: step 5951, loss 3.27399.
Train: 2018-08-02T14:57:45.946576: step 5952, loss 3.65176.
Train: 2018-08-02T14:57:46.009085: step 5953, loss 3.27399.
Train: 2018-08-02T14:57:46.071576: step 5954, loss 3.9036.
Train: 2018-08-02T14:57:46.118443: step 5955, loss 4.02952.
Train: 2018-08-02T14:57:46.180926: step 5956, loss 4.91098.
Train: 2018-08-02T14:57:46.243411: step 5957, loss 4.53321.
Train: 2018-08-02T14:57:46.290246: step 5958, loss 4.53321.
Train: 2018-08-02T14:57:46.352730: step 5959, loss 3.14807.
Train: 2018-08-02T14:57:46.415246: step 5960, loss 4.78506.
Test: 2018-08-02T14:57:46.618294: step 5960, loss 3.81863.
Train: 2018-08-02T14:57:46.680809: step 5961, loss 4.53321.
Train: 2018-08-02T14:57:46.727677: step 5962, loss 4.65914.
Train: 2018-08-02T14:57:46.790158: step 5963, loss 3.52583.
Train: 2018-08-02T14:57:46.852615: step 5964, loss 3.77768.
Train: 2018-08-02T14:57:46.915124: step 5965, loss 2.7703.
Train: 2018-08-02T14:57:46.961987: step 5966, loss 4.78506.
Train: 2018-08-02T14:57:47.024480: step 5967, loss 3.52583.
Train: 2018-08-02T14:57:47.086943: step 5968, loss 5.0369.
Train: 2018-08-02T14:57:47.133797: step 5969, loss 3.9036.
Train: 2018-08-02T14:57:47.196310: step 5970, loss 3.14807.
Test: 2018-08-02T14:57:47.399392: step 5970, loss 3.81863.
Train: 2018-08-02T14:57:47.461876: step 5971, loss 4.02952.
Train: 2018-08-02T14:57:47.508742: step 5972, loss 3.52583.
Train: 2018-08-02T14:57:47.571225: step 5973, loss 3.9036.
Train: 2018-08-02T14:57:47.633713: step 5974, loss 3.65176.
Train: 2018-08-02T14:57:47.696167: step 5975, loss 4.28137.
Train: 2018-08-02T14:57:47.743063: step 5976, loss 4.78506.
Train: 2018-08-02T14:57:47.805546: step 5977, loss 3.27399.
Train: 2018-08-02T14:57:47.868036: step 5978, loss 4.78506.
Train: 2018-08-02T14:57:47.914894: step 5979, loss 4.53321.
Train: 2018-08-02T14:57:47.977351: step 5980, loss 3.39991.
Test: 2018-08-02T14:57:48.180428: step 5980, loss 3.81863.
Train: 2018-08-02T14:57:48.242944: step 5981, loss 4.15545.
Train: 2018-08-02T14:57:48.289807: step 5982, loss 4.91098.
Train: 2018-08-02T14:57:48.352262: step 5983, loss 3.77768.
Train: 2018-08-02T14:57:48.414781: step 5984, loss 4.65914.
Train: 2018-08-02T14:57:48.477235: step 5985, loss 4.02952.
Train: 2018-08-02T14:57:48.524098: step 5986, loss 4.15545.
Train: 2018-08-02T14:57:48.586613: step 5987, loss 4.40729.
Train: 2018-08-02T14:57:48.649068: step 5988, loss 5.0369.
Train: 2018-08-02T14:57:48.695963: step 5989, loss 4.28137.
Train: 2018-08-02T14:57:48.758449: step 5990, loss 4.91098.
Test: 2018-08-02T14:57:48.961493: step 5990, loss 3.81863.
Train: 2018-08-02T14:57:49.008359: step 5991, loss 4.02952.
Train: 2018-08-02T14:57:49.070874: step 5992, loss 4.02952.
Train: 2018-08-02T14:57:49.133330: step 5993, loss 3.77768.
Train: 2018-08-02T14:57:49.195817: step 5994, loss 4.28137.
Train: 2018-08-02T14:57:49.258333: step 5995, loss 3.52583.
Train: 2018-08-02T14:57:49.305189: step 5996, loss 3.52583.
Train: 2018-08-02T14:57:49.367649: step 5997, loss 4.53321.
Train: 2018-08-02T14:57:49.430136: step 5998, loss 4.53321.
Train: 2018-08-02T14:57:49.477029: step 5999, loss 4.28137.
Train: 2018-08-02T14:57:49.539518: step 6000, loss 4.40729.
Test: 2018-08-02T14:57:49.742588: step 6000, loss 3.81863.
Train: 2018-08-02T14:57:50.226853: step 6001, loss 5.16283.
Train: 2018-08-02T14:57:50.289339: step 6002, loss 4.02952.
Train: 2018-08-02T14:57:50.351824: step 6003, loss 4.40729.
Train: 2018-08-02T14:57:50.414311: step 6004, loss 2.26661.
Train: 2018-08-02T14:57:50.476775: step 6005, loss 3.77768.
Train: 2018-08-02T14:57:50.523661: step 6006, loss 4.65914.
Train: 2018-08-02T14:57:50.586113: step 6007, loss 2.39253.
Train: 2018-08-02T14:57:50.648601: step 6008, loss 4.02952.
Train: 2018-08-02T14:57:50.695494: step 6009, loss 3.52583.
Train: 2018-08-02T14:57:50.757950: step 6010, loss 3.14807.
Test: 2018-08-02T14:57:50.961050: step 6010, loss 3.81863.
Train: 2018-08-02T14:57:51.023512: step 6011, loss 3.9036.
Train: 2018-08-02T14:57:51.070407: step 6012, loss 4.15545.
Train: 2018-08-02T14:57:51.132861: step 6013, loss 5.0369.
Train: 2018-08-02T14:57:51.195373: step 6014, loss 3.39991.
Train: 2018-08-02T14:57:51.257864: step 6015, loss 4.15545.
Train: 2018-08-02T14:57:51.304727: step 6016, loss 4.28137.
Train: 2018-08-02T14:57:51.367182: step 6017, loss 4.28137.
Train: 2018-08-02T14:57:51.429692: step 6018, loss 4.15545.
Train: 2018-08-02T14:57:51.476530: step 6019, loss 3.65176.
Train: 2018-08-02T14:57:51.539046: step 6020, loss 4.78506.
Test: 2018-08-02T14:57:51.742095: step 6020, loss 3.81863.
Train: 2018-08-02T14:57:51.804610: step 6021, loss 3.65176.
Train: 2018-08-02T14:57:51.851444: step 6022, loss 3.9036.
Train: 2018-08-02T14:57:51.913927: step 6023, loss 3.27399.
Train: 2018-08-02T14:57:51.976444: step 6024, loss 4.15545.
Train: 2018-08-02T14:57:52.038924: step 6025, loss 5.0369.
Train: 2018-08-02T14:57:52.085794: step 6026, loss 4.40729.
Train: 2018-08-02T14:57:52.148249: step 6027, loss 3.39991.
Train: 2018-08-02T14:57:52.210750: step 6028, loss 4.53321.
Train: 2018-08-02T14:57:52.257625: step 6029, loss 4.02952.
Train: 2018-08-02T14:57:52.320114: step 6030, loss 4.02952.
Test: 2018-08-02T14:57:52.523198: step 6030, loss 3.81863.
Train: 2018-08-02T14:57:52.585648: step 6031, loss 4.65914.
Train: 2018-08-02T14:57:52.648132: step 6032, loss 3.52583.
Train: 2018-08-02T14:57:52.710616: step 6033, loss 4.28137.
Train: 2018-08-02T14:57:52.757483: step 6034, loss 3.77768.
Train: 2018-08-02T14:57:52.819997: step 6035, loss 3.9036.
Train: 2018-08-02T14:57:52.882481: step 6036, loss 3.9036.
Train: 2018-08-02T14:57:52.929346: step 6037, loss 3.9036.
Train: 2018-08-02T14:57:52.991827: step 6038, loss 4.02952.
Train: 2018-08-02T14:57:53.054314: step 6039, loss 3.77768.
Train: 2018-08-02T14:57:53.101180: step 6040, loss 3.76089.
Test: 2018-08-02T14:57:53.288638: step 6040, loss 3.81863.
Train: 2018-08-02T14:57:53.351091: step 6041, loss 4.65914.
Train: 2018-08-02T14:57:53.413607: step 6042, loss 3.52583.
Train: 2018-08-02T14:57:53.476092: step 6043, loss 3.14807.
Train: 2018-08-02T14:57:53.538578: step 6044, loss 3.9036.
Train: 2018-08-02T14:57:53.585445: step 6045, loss 3.9036.
Train: 2018-08-02T14:57:53.647929: step 6046, loss 3.52583.
Train: 2018-08-02T14:57:53.710412: step 6047, loss 4.28137.
Train: 2018-08-02T14:57:53.772900: step 6048, loss 3.52583.
Train: 2018-08-02T14:57:53.819763: step 6049, loss 4.15545.
Train: 2018-08-02T14:57:53.882217: step 6050, loss 3.27399.
Test: 2018-08-02T14:57:54.085294: step 6050, loss 3.81863.
Train: 2018-08-02T14:57:54.147781: step 6051, loss 4.02952.
Train: 2018-08-02T14:57:54.194674: step 6052, loss 4.53321.
Train: 2018-08-02T14:57:54.257130: step 6053, loss 4.53321.
Train: 2018-08-02T14:57:54.319646: step 6054, loss 3.27399.
Train: 2018-08-02T14:57:54.382102: step 6055, loss 4.40729.
Train: 2018-08-02T14:57:54.428995: step 6056, loss 4.28137.
Train: 2018-08-02T14:57:54.491474: step 6057, loss 2.89622.
Train: 2018-08-02T14:57:54.553966: step 6058, loss 3.52583.
Train: 2018-08-02T14:57:54.600832: step 6059, loss 3.52583.
Train: 2018-08-02T14:57:54.663284: step 6060, loss 3.77768.
Test: 2018-08-02T14:57:54.866362: step 6060, loss 3.81863.
Train: 2018-08-02T14:57:54.928848: step 6061, loss 3.77768.
Train: 2018-08-02T14:57:54.975743: step 6062, loss 4.40729.
Train: 2018-08-02T14:57:55.038230: step 6063, loss 4.02952.
Train: 2018-08-02T14:57:55.100684: step 6064, loss 3.77768.
Train: 2018-08-02T14:57:55.163199: step 6065, loss 4.53321.
Train: 2018-08-02T14:57:55.225653: step 6066, loss 4.78506.
Train: 2018-08-02T14:57:55.272516: step 6067, loss 3.77768.
Train: 2018-08-02T14:57:55.335003: step 6068, loss 3.9036.
Train: 2018-08-02T14:57:55.397489: step 6069, loss 4.15545.
Train: 2018-08-02T14:57:55.444382: step 6070, loss 4.15545.
Test: 2018-08-02T14:57:55.647460: step 6070, loss 3.81863.
Train: 2018-08-02T14:57:55.709914: step 6071, loss 4.78506.
Train: 2018-08-02T14:57:55.772432: step 6072, loss 4.53321.
Train: 2018-08-02T14:57:55.834886: step 6073, loss 4.65914.
Train: 2018-08-02T14:57:55.897371: step 6074, loss 4.28137.
Train: 2018-08-02T14:57:55.944266: step 6075, loss 4.40729.
Train: 2018-08-02T14:57:56.006752: step 6076, loss 3.52583.
Train: 2018-08-02T14:57:56.069235: step 6077, loss 3.9036.
Train: 2018-08-02T14:57:56.131690: step 6078, loss 3.27399.
Train: 2018-08-02T14:57:56.178556: step 6079, loss 4.40729.
Train: 2018-08-02T14:57:56.241073: step 6080, loss 3.39991.
Test: 2018-08-02T14:57:56.444118: step 6080, loss 3.81863.
Train: 2018-08-02T14:57:56.506635: step 6081, loss 3.52583.
Train: 2018-08-02T14:57:56.569088: step 6082, loss 3.52583.
Train: 2018-08-02T14:57:56.615985: step 6083, loss 4.28137.
Train: 2018-08-02T14:57:56.678465: step 6084, loss 3.39991.
Train: 2018-08-02T14:57:56.740954: step 6085, loss 4.40729.
Train: 2018-08-02T14:57:56.787786: step 6086, loss 3.27399.
Train: 2018-08-02T14:57:56.850272: step 6087, loss 3.77768.
Train: 2018-08-02T14:57:56.912758: step 6088, loss 3.52583.
Train: 2018-08-02T14:57:56.975244: step 6089, loss 4.02952.
Train: 2018-08-02T14:57:57.022107: step 6090, loss 4.53321.
Test: 2018-08-02T14:57:57.225183: step 6090, loss 3.81863.
Train: 2018-08-02T14:57:57.287700: step 6091, loss 4.15545.
Train: 2018-08-02T14:57:57.350156: step 6092, loss 4.40729.
Train: 2018-08-02T14:57:57.397051: step 6093, loss 3.65176.
Train: 2018-08-02T14:57:57.459513: step 6094, loss 4.53321.
Train: 2018-08-02T14:57:57.522022: step 6095, loss 4.40729.
Train: 2018-08-02T14:57:57.584476: step 6096, loss 4.15545.
Train: 2018-08-02T14:57:57.631371: step 6097, loss 3.65176.
Train: 2018-08-02T14:57:57.693857: step 6098, loss 3.9036.
Train: 2018-08-02T14:57:57.756310: step 6099, loss 3.39991.
Train: 2018-08-02T14:57:57.818795: step 6100, loss 2.7703.
Test: 2018-08-02T14:57:58.006250: step 6100, loss 3.81863.
Train: 2018-08-02T14:57:58.506166: step 6101, loss 4.40729.
Train: 2018-08-02T14:57:58.568650: step 6102, loss 4.65914.
Train: 2018-08-02T14:57:58.615517: step 6103, loss 4.91098.
Train: 2018-08-02T14:57:58.677970: step 6104, loss 3.02214.
Train: 2018-08-02T14:57:58.740488: step 6105, loss 4.91098.
Train: 2018-08-02T14:57:58.802941: step 6106, loss 4.28137.
Train: 2018-08-02T14:57:58.865455: step 6107, loss 4.65914.
Train: 2018-08-02T14:57:58.912320: step 6108, loss 3.39991.
Train: 2018-08-02T14:57:58.974776: step 6109, loss 2.89622.
Train: 2018-08-02T14:57:59.037293: step 6110, loss 4.91098.
Test: 2018-08-02T14:57:59.224746: step 6110, loss 3.81863.
Train: 2018-08-02T14:57:59.287210: step 6111, loss 3.39991.
Train: 2018-08-02T14:57:59.349686: step 6112, loss 3.27399.
Train: 2018-08-02T14:57:59.412202: step 6113, loss 4.15545.
Train: 2018-08-02T14:57:59.459048: step 6114, loss 5.16283.
Train: 2018-08-02T14:57:59.521554: step 6115, loss 4.28137.
Train: 2018-08-02T14:57:59.584038: step 6116, loss 4.15545.
Train: 2018-08-02T14:57:59.630899: step 6117, loss 3.9036.
Train: 2018-08-02T14:57:59.693392: step 6118, loss 3.27399.
Train: 2018-08-02T14:57:59.755844: step 6119, loss 5.16283.
Train: 2018-08-02T14:57:59.802737: step 6120, loss 3.9036.
Test: 2018-08-02T14:58:00.005783: step 6120, loss 3.81863.
Train: 2018-08-02T14:58:00.068299: step 6121, loss 3.9036.
Train: 2018-08-02T14:58:00.130783: step 6122, loss 4.65914.
Train: 2018-08-02T14:58:00.193270: step 6123, loss 3.77768.
Train: 2018-08-02T14:58:00.255757: step 6124, loss 4.40729.
Train: 2018-08-02T14:58:00.302620: step 6125, loss 4.28137.
Train: 2018-08-02T14:58:00.365104: step 6126, loss 3.9036.
Train: 2018-08-02T14:58:00.427590: step 6127, loss 3.77768.
Train: 2018-08-02T14:58:00.490076: step 6128, loss 4.28137.
Train: 2018-08-02T14:58:00.536910: step 6129, loss 4.65914.
Train: 2018-08-02T14:58:00.599424: step 6130, loss 4.53321.
Test: 2018-08-02T14:58:00.786878: step 6130, loss 3.81863.
Train: 2018-08-02T14:58:00.849366: step 6131, loss 4.53321.
Train: 2018-08-02T14:58:00.911820: step 6132, loss 4.28137.
Train: 2018-08-02T14:58:00.974307: step 6133, loss 3.9036.
Train: 2018-08-02T14:58:01.021202: step 6134, loss 3.77768.
Train: 2018-08-02T14:58:01.083694: step 6135, loss 3.52583.
Train: 2018-08-02T14:58:01.146142: step 6136, loss 4.02952.
Train: 2018-08-02T14:58:01.208657: step 6137, loss 4.40729.
Train: 2018-08-02T14:58:01.255529: step 6138, loss 4.28137.
Train: 2018-08-02T14:58:01.317998: step 6139, loss 5.28875.
Train: 2018-08-02T14:58:01.380461: step 6140, loss 4.53321.
Test: 2018-08-02T14:58:01.583539: step 6140, loss 3.81863.
Train: 2018-08-02T14:58:01.630404: step 6141, loss 3.9036.
Train: 2018-08-02T14:58:01.692919: step 6142, loss 4.02952.
Train: 2018-08-02T14:58:01.755374: step 6143, loss 4.02952.
Train: 2018-08-02T14:58:01.817860: step 6144, loss 4.40729.
Train: 2018-08-02T14:58:01.864723: step 6145, loss 4.15545.
Train: 2018-08-02T14:58:01.927238: step 6146, loss 3.9036.
Train: 2018-08-02T14:58:01.989694: step 6147, loss 3.52583.
Train: 2018-08-02T14:58:02.052205: step 6148, loss 3.65176.
Train: 2018-08-02T14:58:02.099042: step 6149, loss 4.15545.
Train: 2018-08-02T14:58:02.161559: step 6150, loss 5.03691.
Test: 2018-08-02T14:58:02.364605: step 6150, loss 3.81863.
Train: 2018-08-02T14:58:02.427122: step 6151, loss 3.02214.
Train: 2018-08-02T14:58:02.473986: step 6152, loss 3.27399.
Train: 2018-08-02T14:58:02.536466: step 6153, loss 4.40729.
Train: 2018-08-02T14:58:02.598957: step 6154, loss 3.52583.
Train: 2018-08-02T14:58:02.661414: step 6155, loss 4.02952.
Train: 2018-08-02T14:58:02.723926: step 6156, loss 4.28137.
Train: 2018-08-02T14:58:02.770793: step 6157, loss 3.52583.
Train: 2018-08-02T14:58:02.833272: step 6158, loss 4.28137.
Train: 2018-08-02T14:58:02.895759: step 6159, loss 4.40729.
Train: 2018-08-02T14:58:02.942595: step 6160, loss 4.02952.
Test: 2018-08-02T14:58:03.145697: step 6160, loss 3.81863.
Train: 2018-08-02T14:58:03.208189: step 6161, loss 3.65176.
Train: 2018-08-02T14:58:03.270675: step 6162, loss 4.15545.
Train: 2018-08-02T14:58:03.333129: step 6163, loss 3.27399.
Train: 2018-08-02T14:58:03.395647: step 6164, loss 4.28137.
Train: 2018-08-02T14:58:03.442484: step 6165, loss 3.77768.
Train: 2018-08-02T14:58:03.504964: step 6166, loss 2.14068.
Train: 2018-08-02T14:58:03.567450: step 6167, loss 4.40729.
Train: 2018-08-02T14:58:03.614346: step 6168, loss 3.52583.
Train: 2018-08-02T14:58:03.676829: step 6169, loss 2.89622.
Train: 2018-08-02T14:58:03.739314: step 6170, loss 3.27399.
Test: 2018-08-02T14:58:03.926770: step 6170, loss 3.81863.
Train: 2018-08-02T14:58:03.989251: step 6171, loss 3.77768.
Train: 2018-08-02T14:58:04.051742: step 6172, loss 3.65176.
Train: 2018-08-02T14:58:04.114230: step 6173, loss 4.53321.
Train: 2018-08-02T14:58:04.176712: step 6174, loss 4.91098.
Train: 2018-08-02T14:58:04.239168: step 6175, loss 4.28137.
Train: 2018-08-02T14:58:04.286055: step 6176, loss 3.9036.
Train: 2018-08-02T14:58:04.348547: step 6177, loss 4.02952.
Train: 2018-08-02T14:58:04.395408: step 6178, loss 5.16283.
Train: 2018-08-02T14:58:04.457896: step 6179, loss 4.15545.
Train: 2018-08-02T14:58:04.520351: step 6180, loss 3.77768.
Test: 2018-08-02T14:58:04.707837: step 6180, loss 3.81863.
Train: 2018-08-02T14:58:04.770319: step 6181, loss 3.9036.
Train: 2018-08-02T14:58:04.832803: step 6182, loss 3.52583.
Train: 2018-08-02T14:58:04.895263: step 6183, loss 3.14807.
Train: 2018-08-02T14:58:04.957779: step 6184, loss 2.64438.
Train: 2018-08-02T14:58:05.004615: step 6185, loss 4.02952.
Train: 2018-08-02T14:58:05.067129: step 6186, loss 4.02952.
Train: 2018-08-02T14:58:05.129583: step 6187, loss 4.78506.
Train: 2018-08-02T14:58:05.192081: step 6188, loss 4.15545.
Train: 2018-08-02T14:58:05.238966: step 6189, loss 4.65914.
Train: 2018-08-02T14:58:05.301418: step 6190, loss 5.5406.
Test: 2018-08-02T14:58:05.504520: step 6190, loss 3.81863.
Train: 2018-08-02T14:58:05.551360: step 6191, loss 5.10406.
Train: 2018-08-02T14:58:05.613875: step 6192, loss 4.53321.
Train: 2018-08-02T14:58:05.660709: step 6193, loss 3.65176.
Train: 2018-08-02T14:58:05.723225: step 6194, loss 4.02952.
Train: 2018-08-02T14:58:05.785705: step 6195, loss 4.91098.
Train: 2018-08-02T14:58:05.848195: step 6196, loss 4.15545.
Train: 2018-08-02T14:58:05.895056: step 6197, loss 4.15545.
Train: 2018-08-02T14:58:05.957545: step 6198, loss 4.15545.
Train: 2018-08-02T14:58:06.020034: step 6199, loss 3.65176.
Train: 2018-08-02T14:58:06.066894: step 6200, loss 3.65176.
Test: 2018-08-02T14:58:06.269942: step 6200, loss 3.81863.
Train: 2018-08-02T14:58:06.801097: step 6201, loss 4.40729.
Train: 2018-08-02T14:58:06.863582: step 6202, loss 3.77768.
Train: 2018-08-02T14:58:06.910443: step 6203, loss 5.41467.
Train: 2018-08-02T14:58:06.972904: step 6204, loss 3.02214.
Train: 2018-08-02T14:58:07.035418: step 6205, loss 3.27399.
Train: 2018-08-02T14:58:07.082277: step 6206, loss 5.28875.
Train: 2018-08-02T14:58:07.144769: step 6207, loss 4.53321.
Train: 2018-08-02T14:58:07.207223: step 6208, loss 3.9036.
Train: 2018-08-02T14:58:07.269740: step 6209, loss 4.53321.
Train: 2018-08-02T14:58:07.316601: step 6210, loss 4.91098.
Test: 2018-08-02T14:58:07.519681: step 6210, loss 3.81863.
Train: 2018-08-02T14:58:07.582159: step 6211, loss 4.28137.
Train: 2018-08-02T14:58:07.644646: step 6212, loss 4.91098.
Train: 2018-08-02T14:58:07.707104: step 6213, loss 4.78506.
Train: 2018-08-02T14:58:07.754002: step 6214, loss 4.02952.
Train: 2018-08-02T14:58:07.816454: step 6215, loss 3.27399.
Train: 2018-08-02T14:58:07.878940: step 6216, loss 4.02952.
Train: 2018-08-02T14:58:07.925848: step 6217, loss 4.02952.
Train: 2018-08-02T14:58:07.988315: step 6218, loss 3.52583.
Train: 2018-08-02T14:58:08.066396: step 6219, loss 2.39253.
Train: 2018-08-02T14:58:08.113261: step 6220, loss 4.65914.
Test: 2018-08-02T14:58:08.316340: step 6220, loss 3.81863.
Train: 2018-08-02T14:58:08.378852: step 6221, loss 4.65914.
Train: 2018-08-02T14:58:08.441338: step 6222, loss 3.9036.
Train: 2018-08-02T14:58:08.488197: step 6223, loss 5.66652.
Train: 2018-08-02T14:58:08.550697: step 6224, loss 3.52583.
Train: 2018-08-02T14:58:08.613176: step 6225, loss 3.77768.
Train: 2018-08-02T14:58:08.675659: step 6226, loss 4.78506.
Train: 2018-08-02T14:58:08.722522: step 6227, loss 3.39991.
Train: 2018-08-02T14:58:08.784979: step 6228, loss 3.65176.
Train: 2018-08-02T14:58:08.847494: step 6229, loss 4.40729.
Train: 2018-08-02T14:58:08.909978: step 6230, loss 4.78506.
Test: 2018-08-02T14:58:09.097403: step 6230, loss 3.81863.
Train: 2018-08-02T14:58:09.159921: step 6231, loss 3.27399.
Train: 2018-08-02T14:58:09.222405: step 6232, loss 2.7703.
Train: 2018-08-02T14:58:09.284885: step 6233, loss 4.53321.
Train: 2018-08-02T14:58:09.347376: step 6234, loss 4.28137.
Train: 2018-08-02T14:58:09.394242: step 6235, loss 4.15545.
Train: 2018-08-02T14:58:09.456695: step 6236, loss 3.9036.
Train: 2018-08-02T14:58:09.519212: step 6237, loss 3.52583.
Train: 2018-08-02T14:58:09.566044: step 6238, loss 4.28137.
Train: 2018-08-02T14:58:09.628562: step 6239, loss 3.77768.
Train: 2018-08-02T14:58:09.691016: step 6240, loss 3.77768.
Test: 2018-08-02T14:58:09.894092: step 6240, loss 3.81863.
Train: 2018-08-02T14:58:09.940987: step 6241, loss 3.14807.
Train: 2018-08-02T14:58:10.003469: step 6242, loss 4.02952.
Train: 2018-08-02T14:58:10.065928: step 6243, loss 3.39991.
Train: 2018-08-02T14:58:10.128415: step 6244, loss 3.52583.
Train: 2018-08-02T14:58:10.190900: step 6245, loss 4.02952.
Train: 2018-08-02T14:58:10.253384: step 6246, loss 3.39991.
Train: 2018-08-02T14:58:10.300248: step 6247, loss 3.39991.
Train: 2018-08-02T14:58:10.362766: step 6248, loss 4.02952.
Train: 2018-08-02T14:58:10.425251: step 6249, loss 4.15545.
Train: 2018-08-02T14:58:10.472118: step 6250, loss 5.0369.
Test: 2018-08-02T14:58:10.675184: step 6250, loss 3.81863.
Train: 2018-08-02T14:58:10.737675: step 6251, loss 3.65176.
Train: 2018-08-02T14:58:10.800160: step 6252, loss 3.9036.
Train: 2018-08-02T14:58:10.862625: step 6253, loss 4.02952.
Train: 2018-08-02T14:58:10.909506: step 6254, loss 3.27399.
Train: 2018-08-02T14:58:10.971996: step 6255, loss 3.52583.
Train: 2018-08-02T14:58:11.034452: step 6256, loss 4.02952.
Train: 2018-08-02T14:58:11.081345: step 6257, loss 2.7703.
Train: 2018-08-02T14:58:11.143833: step 6258, loss 3.39991.
Train: 2018-08-02T14:58:11.206286: step 6259, loss 4.65914.
Train: 2018-08-02T14:58:11.253149: step 6260, loss 4.02952.
Test: 2018-08-02T14:58:11.456256: step 6260, loss 3.81863.
Train: 2018-08-02T14:58:11.518713: step 6261, loss 4.65914.
Train: 2018-08-02T14:58:11.581227: step 6262, loss 4.78506.
Train: 2018-08-02T14:58:11.643685: step 6263, loss 4.91098.
Train: 2018-08-02T14:58:11.706201: step 6264, loss 4.40729.
Train: 2018-08-02T14:58:11.753065: step 6265, loss 3.77768.
Train: 2018-08-02T14:58:11.815548: step 6266, loss 3.02214.
Train: 2018-08-02T14:58:11.878032: step 6267, loss 4.53321.
Train: 2018-08-02T14:58:11.924869: step 6268, loss 4.02952.
Train: 2018-08-02T14:58:11.987382: step 6269, loss 3.14807.
Train: 2018-08-02T14:58:12.049869: step 6270, loss 3.77768.
Test: 2018-08-02T14:58:12.237293: step 6270, loss 3.81863.
Train: 2018-08-02T14:58:12.299805: step 6271, loss 4.91098.
Train: 2018-08-02T14:58:12.362265: step 6272, loss 3.77768.
Train: 2018-08-02T14:58:12.424780: step 6273, loss 4.91098.
Train: 2018-08-02T14:58:12.487236: step 6274, loss 3.77768.
Train: 2018-08-02T14:58:12.549751: step 6275, loss 2.51845.
Train: 2018-08-02T14:58:12.596610: step 6276, loss 3.52583.
Train: 2018-08-02T14:58:12.659071: step 6277, loss 3.9036.
Train: 2018-08-02T14:58:12.721557: step 6278, loss 4.28137.
Train: 2018-08-02T14:58:12.784054: step 6279, loss 3.65176.
Train: 2018-08-02T14:58:12.830936: step 6280, loss 3.14807.
Test: 2018-08-02T14:58:13.034006: step 6280, loss 3.81863.
Train: 2018-08-02T14:58:13.096498: step 6281, loss 3.65176.
Train: 2018-08-02T14:58:13.158984: step 6282, loss 2.26661.
Train: 2018-08-02T14:58:13.205817: step 6283, loss 4.78506.
Train: 2018-08-02T14:58:13.268305: step 6284, loss 4.28137.
Train: 2018-08-02T14:58:13.330819: step 6285, loss 4.78506.
Train: 2018-08-02T14:58:13.393273: step 6286, loss 3.02214.
Train: 2018-08-02T14:58:13.440167: step 6287, loss 4.15545.
Train: 2018-08-02T14:58:13.502623: step 6288, loss 3.52583.
Train: 2018-08-02T14:58:13.565109: step 6289, loss 4.65914.
Train: 2018-08-02T14:58:13.612005: step 6290, loss 3.77768.
Test: 2018-08-02T14:58:13.815049: step 6290, loss 3.81863.
Train: 2018-08-02T14:58:13.877535: step 6291, loss 4.28137.
Train: 2018-08-02T14:58:13.940050: step 6292, loss 4.15545.
Train: 2018-08-02T14:58:14.002533: step 6293, loss 3.52583.
Train: 2018-08-02T14:58:14.049372: step 6294, loss 3.9036.
Train: 2018-08-02T14:58:14.111888: step 6295, loss 4.65914.
Train: 2018-08-02T14:58:14.174371: step 6296, loss 4.02952.
Train: 2018-08-02T14:58:14.221206: step 6297, loss 3.39991.
Train: 2018-08-02T14:58:14.283721: step 6298, loss 4.65914.
Train: 2018-08-02T14:58:14.346206: step 6299, loss 3.27399.
Train: 2018-08-02T14:58:14.408690: step 6300, loss 4.15545.
Test: 2018-08-02T14:58:14.596116: step 6300, loss 3.81863.
Train: 2018-08-02T14:58:15.096030: step 6301, loss 2.7703.
Train: 2018-08-02T14:58:15.158484: step 6302, loss 4.78506.
Train: 2018-08-02T14:58:15.220970: step 6303, loss 2.89622.
Train: 2018-08-02T14:58:15.283456: step 6304, loss 4.78506.
Train: 2018-08-02T14:58:15.330320: step 6305, loss 3.39991.
Train: 2018-08-02T14:58:15.392805: step 6306, loss 4.15545.
Train: 2018-08-02T14:58:15.455290: step 6307, loss 3.9036.
Train: 2018-08-02T14:58:15.517808: step 6308, loss 4.02952.
Train: 2018-08-02T14:58:15.564640: step 6309, loss 5.28875.
Train: 2018-08-02T14:58:15.627157: step 6310, loss 4.78506.
Test: 2018-08-02T14:58:15.830201: step 6310, loss 3.81863.
Train: 2018-08-02T14:58:15.892719: step 6311, loss 3.65176.
Train: 2018-08-02T14:58:15.939582: step 6312, loss 4.15545.
Train: 2018-08-02T14:58:16.002064: step 6313, loss 3.77768.
Train: 2018-08-02T14:58:16.064525: step 6314, loss 3.65176.
Train: 2018-08-02T14:58:16.127039: step 6315, loss 6.54798.
Train: 2018-08-02T14:58:16.173902: step 6316, loss 3.65176.
Train: 2018-08-02T14:58:16.236383: step 6317, loss 3.65176.
Train: 2018-08-02T14:58:16.298846: step 6318, loss 4.65914.
Train: 2018-08-02T14:58:16.345732: step 6319, loss 3.77768.
Train: 2018-08-02T14:58:16.408223: step 6320, loss 3.77768.
Test: 2018-08-02T14:58:16.611270: step 6320, loss 3.81863.
Train: 2018-08-02T14:58:16.673756: step 6321, loss 5.0369.
Train: 2018-08-02T14:58:16.736272: step 6322, loss 3.9036.
Train: 2018-08-02T14:58:16.783105: step 6323, loss 4.28137.
Train: 2018-08-02T14:58:16.845621: step 6324, loss 4.40729.
Train: 2018-08-02T14:58:16.908077: step 6325, loss 5.28875.
Train: 2018-08-02T14:58:16.970561: step 6326, loss 3.77768.
Train: 2018-08-02T14:58:17.017424: step 6327, loss 4.65914.
Train: 2018-08-02T14:58:17.079941: step 6328, loss 4.28137.
Train: 2018-08-02T14:58:17.142428: step 6329, loss 3.65176.
Train: 2018-08-02T14:58:17.189293: step 6330, loss 3.02214.
Test: 2018-08-02T14:58:17.392337: step 6330, loss 3.81863.
Train: 2018-08-02T14:58:17.454823: step 6331, loss 3.52583.
Train: 2018-08-02T14:58:17.517308: step 6332, loss 3.52583.
Train: 2018-08-02T14:58:17.579793: step 6333, loss 4.65914.
Train: 2018-08-02T14:58:17.626657: step 6334, loss 4.40729.
Train: 2018-08-02T14:58:17.689143: step 6335, loss 4.40729.
Train: 2018-08-02T14:58:17.751657: step 6336, loss 5.16283.
Train: 2018-08-02T14:58:17.798522: step 6337, loss 3.39991.
Train: 2018-08-02T14:58:17.860977: step 6338, loss 3.27399.
Train: 2018-08-02T14:58:17.923465: step 6339, loss 3.02214.
Train: 2018-08-02T14:58:17.985963: step 6340, loss 3.52583.
Test: 2018-08-02T14:58:18.189027: step 6340, loss 3.81863.
Train: 2018-08-02T14:58:18.235922: step 6341, loss 4.53321.
Train: 2018-08-02T14:58:18.282752: step 6342, loss 3.76089.
Train: 2018-08-02T14:58:18.345270: step 6343, loss 3.77768.
Train: 2018-08-02T14:58:18.407754: step 6344, loss 4.15545.
Train: 2018-08-02T14:58:18.470240: step 6345, loss 3.65176.
Train: 2018-08-02T14:58:18.517074: step 6346, loss 4.28137.
Train: 2018-08-02T14:58:18.579589: step 6347, loss 4.28137.
Train: 2018-08-02T14:58:18.642077: step 6348, loss 4.78506.
Train: 2018-08-02T14:58:18.704529: step 6349, loss 3.02214.
Train: 2018-08-02T14:58:18.751421: step 6350, loss 3.27399.
Test: 2018-08-02T14:58:18.954470: step 6350, loss 3.81863.
Train: 2018-08-02T14:58:19.016997: step 6351, loss 4.15545.
Train: 2018-08-02T14:58:19.079442: step 6352, loss 4.15545.
Train: 2018-08-02T14:58:19.141959: step 6353, loss 2.7703.
Train: 2018-08-02T14:58:19.204414: step 6354, loss 4.91098.
Train: 2018-08-02T14:58:19.266898: step 6355, loss 4.65914.
Train: 2018-08-02T14:58:19.313763: step 6356, loss 3.14807.
Train: 2018-08-02T14:58:19.376280: step 6357, loss 3.77768.
Train: 2018-08-02T14:58:19.423111: step 6358, loss 3.39991.
Train: 2018-08-02T14:58:19.485597: step 6359, loss 4.40729.
Train: 2018-08-02T14:58:19.548082: step 6360, loss 2.7703.
Test: 2018-08-02T14:58:19.751192: step 6360, loss 3.81863.
Train: 2018-08-02T14:58:19.798055: step 6361, loss 3.9036.
Train: 2018-08-02T14:58:19.860539: step 6362, loss 3.9036.
Train: 2018-08-02T14:58:19.922994: step 6363, loss 4.78506.
Train: 2018-08-02T14:58:19.985481: step 6364, loss 4.02952.
Train: 2018-08-02T14:58:20.047995: step 6365, loss 2.89622.
Train: 2018-08-02T14:58:20.094830: step 6366, loss 5.66652.
Train: 2018-08-02T14:58:20.157315: step 6367, loss 4.40729.
Train: 2018-08-02T14:58:20.219801: step 6368, loss 3.65176.
Train: 2018-08-02T14:58:20.266695: step 6369, loss 4.28137.
Train: 2018-08-02T14:58:20.329148: step 6370, loss 4.78506.
Test: 2018-08-02T14:58:20.532227: step 6370, loss 3.81863.
Train: 2018-08-02T14:58:20.594714: step 6371, loss 4.28137.
Train: 2018-08-02T14:58:20.657231: step 6372, loss 4.53321.
Train: 2018-08-02T14:58:20.704062: step 6373, loss 4.53321.
Train: 2018-08-02T14:58:20.766580: step 6374, loss 2.89622.
Train: 2018-08-02T14:58:20.829034: step 6375, loss 4.28137.
Train: 2018-08-02T14:58:20.875928: step 6376, loss 3.65176.
Train: 2018-08-02T14:58:20.938412: step 6377, loss 4.02952.
Train: 2018-08-02T14:58:21.000900: step 6378, loss 3.14807.
Train: 2018-08-02T14:58:21.047761: step 6379, loss 4.40729.
Train: 2018-08-02T14:58:21.110249: step 6380, loss 4.40729.
Test: 2018-08-02T14:58:21.313318: step 6380, loss 3.81863.
Train: 2018-08-02T14:58:21.375806: step 6381, loss 3.52583.
Train: 2018-08-02T14:58:21.438266: step 6382, loss 3.77768.
Train: 2018-08-02T14:58:21.485162: step 6383, loss 3.39991.
Train: 2018-08-02T14:58:21.547616: step 6384, loss 2.89622.
Train: 2018-08-02T14:58:21.610111: step 6385, loss 5.79244.
Train: 2018-08-02T14:58:21.672614: step 6386, loss 3.65176.
Train: 2018-08-02T14:58:21.719484: step 6387, loss 4.02952.
Train: 2018-08-02T14:58:21.781937: step 6388, loss 4.53321.
Train: 2018-08-02T14:58:21.844451: step 6389, loss 4.28137.
Train: 2018-08-02T14:58:21.891315: step 6390, loss 3.9036.
Test: 2018-08-02T14:58:22.094359: step 6390, loss 3.81863.
Train: 2018-08-02T14:58:22.156847: step 6391, loss 4.53321.
Train: 2018-08-02T14:58:22.219361: step 6392, loss 4.02952.
Train: 2018-08-02T14:58:22.281817: step 6393, loss 4.40729.
Train: 2018-08-02T14:58:22.328712: step 6394, loss 4.28137.
Train: 2018-08-02T14:58:22.391168: step 6395, loss 3.14807.
Train: 2018-08-02T14:58:22.453681: step 6396, loss 3.9036.
Train: 2018-08-02T14:58:22.500516: step 6397, loss 4.02952.
Train: 2018-08-02T14:58:22.563033: step 6398, loss 3.9036.
Train: 2018-08-02T14:58:22.625514: step 6399, loss 3.39991.
Train: 2018-08-02T14:58:22.688001: step 6400, loss 3.77768.
Test: 2018-08-02T14:58:22.875427: step 6400, loss 3.81863.
Train: 2018-08-02T14:58:23.437827: step 6401, loss 3.65176.
Train: 2018-08-02T14:58:23.500283: step 6402, loss 3.39991.
Train: 2018-08-02T14:58:23.547171: step 6403, loss 4.53321.
Train: 2018-08-02T14:58:23.609633: step 6404, loss 3.77768.
Train: 2018-08-02T14:58:23.672117: step 6405, loss 3.77768.
Train: 2018-08-02T14:58:23.734631: step 6406, loss 3.52583.
Train: 2018-08-02T14:58:23.781491: step 6407, loss 4.40729.
Train: 2018-08-02T14:58:23.843952: step 6408, loss 3.9036.
Train: 2018-08-02T14:58:23.906439: step 6409, loss 4.02952.
Train: 2018-08-02T14:58:23.953300: step 6410, loss 4.15545.
Test: 2018-08-02T14:58:24.156378: step 6410, loss 3.81863.
Train: 2018-08-02T14:58:24.218864: step 6411, loss 4.65914.
Train: 2018-08-02T14:58:24.281348: step 6412, loss 4.02952.
Train: 2018-08-02T14:58:24.328213: step 6413, loss 4.91098.
Train: 2018-08-02T14:58:24.390726: step 6414, loss 4.02952.
Train: 2018-08-02T14:58:24.453215: step 6415, loss 3.65176.
Train: 2018-08-02T14:58:24.500048: step 6416, loss 3.77768.
Train: 2018-08-02T14:58:24.562534: step 6417, loss 4.15545.
Train: 2018-08-02T14:58:24.625020: step 6418, loss 3.52583.
Train: 2018-08-02T14:58:24.687533: step 6419, loss 3.52583.
Train: 2018-08-02T14:58:24.734400: step 6420, loss 4.40729.
Test: 2018-08-02T14:58:24.937444: step 6420, loss 3.81863.
Train: 2018-08-02T14:58:24.999931: step 6421, loss 4.40729.
Train: 2018-08-02T14:58:25.062446: step 6422, loss 4.65914.
Train: 2018-08-02T14:58:25.124930: step 6423, loss 4.53321.
Train: 2018-08-02T14:58:25.171767: step 6424, loss 3.65176.
Train: 2018-08-02T14:58:25.234283: step 6425, loss 3.02214.
Train: 2018-08-02T14:58:25.296766: step 6426, loss 3.65176.
Train: 2018-08-02T14:58:25.343601: step 6427, loss 4.15545.
Train: 2018-08-02T14:58:25.406088: step 6428, loss 4.91098.
Train: 2018-08-02T14:58:25.468601: step 6429, loss 4.40729.
Train: 2018-08-02T14:58:25.515466: step 6430, loss 4.02952.
Test: 2018-08-02T14:58:25.718542: step 6430, loss 3.81863.
Train: 2018-08-02T14:58:25.781029: step 6431, loss 3.39991.
Train: 2018-08-02T14:58:25.843482: step 6432, loss 3.77768.
Train: 2018-08-02T14:58:25.890378: step 6433, loss 3.65176.
Train: 2018-08-02T14:58:25.952864: step 6434, loss 4.78506.
Train: 2018-08-02T14:58:26.015319: step 6435, loss 3.27399.
Train: 2018-08-02T14:58:26.077803: step 6436, loss 4.53321.
Train: 2018-08-02T14:58:26.124699: step 6437, loss 3.9036.
Train: 2018-08-02T14:58:26.187183: step 6438, loss 4.65914.
Train: 2018-08-02T14:58:26.249638: step 6439, loss 4.65914.
Train: 2018-08-02T14:58:26.296527: step 6440, loss 4.28137.
Test: 2018-08-02T14:58:26.499578: step 6440, loss 3.81863.
Train: 2018-08-02T14:58:26.562064: step 6441, loss 4.02952.
Train: 2018-08-02T14:58:26.624582: step 6442, loss 3.9036.
Train: 2018-08-02T14:58:26.687069: step 6443, loss 3.9036.
Train: 2018-08-02T14:58:26.733901: step 6444, loss 3.65176.
Train: 2018-08-02T14:58:26.796415: step 6445, loss 3.77768.
Train: 2018-08-02T14:58:26.858900: step 6446, loss 4.65914.
Train: 2018-08-02T14:58:26.921385: step 6447, loss 4.15545.
Train: 2018-08-02T14:58:26.968250: step 6448, loss 4.15545.
Train: 2018-08-02T14:58:27.030706: step 6449, loss 4.78506.
Train: 2018-08-02T14:58:27.093191: step 6450, loss 2.7703.
Test: 2018-08-02T14:58:27.296268: step 6450, loss 3.81863.
Train: 2018-08-02T14:58:27.343162: step 6451, loss 3.39991.
Train: 2018-08-02T14:58:27.405618: step 6452, loss 4.28137.
Train: 2018-08-02T14:58:27.468132: step 6453, loss 4.02952.
Train: 2018-08-02T14:58:27.530589: step 6454, loss 3.65176.
Train: 2018-08-02T14:58:27.593082: step 6455, loss 2.89622.
Train: 2018-08-02T14:58:27.639964: step 6456, loss 2.89622.
Train: 2018-08-02T14:58:27.702453: step 6457, loss 4.78506.
Train: 2018-08-02T14:58:27.749319: step 6458, loss 5.41467.
Train: 2018-08-02T14:58:27.811773: step 6459, loss 4.65914.
Train: 2018-08-02T14:58:27.874290: step 6460, loss 3.65176.
Test: 2018-08-02T14:58:28.077334: step 6460, loss 3.81863.
Train: 2018-08-02T14:58:28.139852: step 6461, loss 5.5406.
Train: 2018-08-02T14:58:28.186714: step 6462, loss 3.27399.
Train: 2018-08-02T14:58:28.249169: step 6463, loss 4.65914.
Train: 2018-08-02T14:58:28.311686: step 6464, loss 2.7703.
Train: 2018-08-02T14:58:28.374168: step 6465, loss 3.65176.
Train: 2018-08-02T14:58:28.421005: step 6466, loss 3.9036.
Train: 2018-08-02T14:58:28.483490: step 6467, loss 3.9036.
Train: 2018-08-02T14:58:28.546001: step 6468, loss 4.28137.
Train: 2018-08-02T14:58:28.592840: step 6469, loss 3.9036.
Train: 2018-08-02T14:58:28.655329: step 6470, loss 4.15545.
Test: 2018-08-02T14:58:28.858402: step 6470, loss 3.81863.
Train: 2018-08-02T14:58:28.920917: step 6471, loss 4.53321.
Train: 2018-08-02T14:58:28.983403: step 6472, loss 4.15545.
Train: 2018-08-02T14:58:29.030267: step 6473, loss 5.03691.
Train: 2018-08-02T14:58:29.092725: step 6474, loss 3.9036.
Train: 2018-08-02T14:58:29.155221: step 6475, loss 3.65176.
Train: 2018-08-02T14:58:29.217723: step 6476, loss 4.91098.
Train: 2018-08-02T14:58:29.264588: step 6477, loss 4.40729.
Train: 2018-08-02T14:58:29.327068: step 6478, loss 3.39991.
Train: 2018-08-02T14:58:29.389559: step 6479, loss 4.65914.
Train: 2018-08-02T14:58:29.436422: step 6480, loss 3.27399.
Test: 2018-08-02T14:58:29.639468: step 6480, loss 3.81863.
Train: 2018-08-02T14:58:29.701987: step 6481, loss 4.40729.
Train: 2018-08-02T14:58:29.764471: step 6482, loss 3.27399.
Train: 2018-08-02T14:58:29.811303: step 6483, loss 3.39991.
Train: 2018-08-02T14:58:29.873822: step 6484, loss 3.9036.
Train: 2018-08-02T14:58:29.936275: step 6485, loss 4.65914.
Train: 2018-08-02T14:58:29.998785: step 6486, loss 3.65176.
Train: 2018-08-02T14:58:30.045654: step 6487, loss 3.9036.
Train: 2018-08-02T14:58:30.108109: step 6488, loss 3.65176.
Train: 2018-08-02T14:58:30.170626: step 6489, loss 4.02952.
Train: 2018-08-02T14:58:30.217484: step 6490, loss 4.78506.
Test: 2018-08-02T14:58:30.420566: step 6490, loss 3.81863.
Train: 2018-08-02T14:58:30.483021: step 6491, loss 4.15545.
Train: 2018-08-02T14:58:30.545506: step 6492, loss 3.39991.
Train: 2018-08-02T14:58:30.592401: step 6493, loss 4.29816.
Train: 2018-08-02T14:58:30.654858: step 6494, loss 4.28137.
Train: 2018-08-02T14:58:30.701751: step 6495, loss 3.52583.
Train: 2018-08-02T14:58:30.764232: step 6496, loss 4.02952.
Train: 2018-08-02T14:58:30.826721: step 6497, loss 5.0369.
Train: 2018-08-02T14:58:30.889178: step 6498, loss 3.39991.
Train: 2018-08-02T14:58:30.936071: step 6499, loss 3.52583.
Train: 2018-08-02T14:58:30.998555: step 6500, loss 3.14807.
Test: 2018-08-02T14:58:31.201602: step 6500, loss 3.81863.
Train: 2018-08-02T14:58:31.717137: step 6501, loss 3.52583.
Train: 2018-08-02T14:58:31.779593: step 6502, loss 3.77768.
Train: 2018-08-02T14:58:31.842077: step 6503, loss 3.39991.
Train: 2018-08-02T14:58:31.888970: step 6504, loss 3.52583.
Train: 2018-08-02T14:58:31.951473: step 6505, loss 3.77768.
Train: 2018-08-02T14:58:32.013944: step 6506, loss 3.27399.
Train: 2018-08-02T14:58:32.060777: step 6507, loss 3.65176.
Train: 2018-08-02T14:58:32.123290: step 6508, loss 4.02952.
Train: 2018-08-02T14:58:32.185774: step 6509, loss 3.9036.
Train: 2018-08-02T14:58:32.248262: step 6510, loss 3.77768.
Test: 2018-08-02T14:58:32.435714: step 6510, loss 3.81863.
Train: 2018-08-02T14:58:32.498173: step 6511, loss 5.16283.
Train: 2018-08-02T14:58:32.560660: step 6512, loss 4.15545.
Train: 2018-08-02T14:58:32.607556: step 6513, loss 3.9036.
Train: 2018-08-02T14:58:32.670041: step 6514, loss 4.40729.
Train: 2018-08-02T14:58:32.732495: step 6515, loss 4.28137.
Train: 2018-08-02T14:58:32.794980: step 6516, loss 3.02214.
Train: 2018-08-02T14:58:32.841843: step 6517, loss 3.52583.
Train: 2018-08-02T14:58:32.904366: step 6518, loss 4.15545.
Train: 2018-08-02T14:58:32.966844: step 6519, loss 4.65914.
Train: 2018-08-02T14:58:33.013711: step 6520, loss 4.02952.
Test: 2018-08-02T14:58:33.216758: step 6520, loss 3.81863.
Train: 2018-08-02T14:58:33.279272: step 6521, loss 4.91098.
Train: 2018-08-02T14:58:33.341763: step 6522, loss 3.77768.
Train: 2018-08-02T14:58:33.404211: step 6523, loss 4.02952.
Train: 2018-08-02T14:58:33.451103: step 6524, loss 4.53321.
Train: 2018-08-02T14:58:33.513593: step 6525, loss 4.40729.
Train: 2018-08-02T14:58:33.576048: step 6526, loss 3.27399.
Train: 2018-08-02T14:58:33.622943: step 6527, loss 3.77768.
Train: 2018-08-02T14:58:33.685429: step 6528, loss 4.53321.
Train: 2018-08-02T14:58:33.747884: step 6529, loss 4.02952.
Train: 2018-08-02T14:58:33.794776: step 6530, loss 4.02952.
Test: 2018-08-02T14:58:33.997855: step 6530, loss 3.81863.
Train: 2018-08-02T14:58:34.060339: step 6531, loss 5.16283.
Train: 2018-08-02T14:58:34.122823: step 6532, loss 3.02214.
Train: 2018-08-02T14:58:34.185310: step 6533, loss 5.16283.
Train: 2018-08-02T14:58:34.232175: step 6534, loss 3.77768.
Train: 2018-08-02T14:58:34.294655: step 6535, loss 4.02952.
Train: 2018-08-02T14:58:34.357117: step 6536, loss 4.40729.
Train: 2018-08-02T14:58:34.403980: step 6537, loss 4.91098.
Train: 2018-08-02T14:58:34.466466: step 6538, loss 3.9036.
Train: 2018-08-02T14:58:34.528951: step 6539, loss 3.39991.
Train: 2018-08-02T14:58:34.591437: step 6540, loss 3.77768.
Test: 2018-08-02T14:58:34.794514: step 6540, loss 3.81863.
Train: 2018-08-02T14:58:34.841377: step 6541, loss 4.02952.
Train: 2018-08-02T14:58:34.903862: step 6542, loss 4.02952.
Train: 2018-08-02T14:58:34.966376: step 6543, loss 3.52583.
Train: 2018-08-02T14:58:35.028834: step 6544, loss 4.28137.
Train: 2018-08-02T14:58:35.075697: step 6545, loss 4.53321.
Train: 2018-08-02T14:58:35.138182: step 6546, loss 4.15545.
Train: 2018-08-02T14:58:35.200696: step 6547, loss 4.40729.
Train: 2018-08-02T14:58:35.247567: step 6548, loss 4.28137.
Train: 2018-08-02T14:58:35.310040: step 6549, loss 4.40729.
Train: 2018-08-02T14:58:35.372501: step 6550, loss 3.14807.
Test: 2018-08-02T14:58:35.559982: step 6550, loss 3.81863.
Train: 2018-08-02T14:58:35.622444: step 6551, loss 3.77768.
Train: 2018-08-02T14:58:35.684959: step 6552, loss 3.39991.
Train: 2018-08-02T14:58:35.747414: step 6553, loss 3.52583.
Train: 2018-08-02T14:58:35.809930: step 6554, loss 4.02952.
Train: 2018-08-02T14:58:35.856789: step 6555, loss 4.91098.
Train: 2018-08-02T14:58:35.919279: step 6556, loss 3.39991.
Train: 2018-08-02T14:58:35.981733: step 6557, loss 3.52583.
Train: 2018-08-02T14:58:36.028629: step 6558, loss 3.39991.
Train: 2018-08-02T14:58:36.091113: step 6559, loss 4.40729.
Train: 2018-08-02T14:58:36.153598: step 6560, loss 4.02952.
Test: 2018-08-02T14:58:36.341052: step 6560, loss 3.81863.
Train: 2018-08-02T14:58:36.403535: step 6561, loss 4.40729.
Train: 2018-08-02T14:58:36.466025: step 6562, loss 4.65914.
Train: 2018-08-02T14:58:36.528481: step 6563, loss 3.65176.
Train: 2018-08-02T14:58:36.575377: step 6564, loss 4.40729.
Train: 2018-08-02T14:58:36.637867: step 6565, loss 4.15545.
Train: 2018-08-02T14:58:36.700346: step 6566, loss 4.91098.
Train: 2018-08-02T14:58:36.747209: step 6567, loss 4.02952.
Train: 2018-08-02T14:58:36.809666: step 6568, loss 4.02952.
Train: 2018-08-02T14:58:36.872179: step 6569, loss 3.52583.
Train: 2018-08-02T14:58:36.919015: step 6570, loss 4.53321.
Test: 2018-08-02T14:58:37.122092: step 6570, loss 3.81863.
Train: 2018-08-02T14:58:37.184607: step 6571, loss 4.91098.
Train: 2018-08-02T14:58:37.231474: step 6572, loss 4.40729.
Train: 2018-08-02T14:58:37.293927: step 6573, loss 4.28137.
Train: 2018-08-02T14:58:37.356413: step 6574, loss 3.9036.
Train: 2018-08-02T14:58:37.418928: step 6575, loss 4.53321.
Train: 2018-08-02T14:58:37.465791: step 6576, loss 3.65176.
Train: 2018-08-02T14:58:37.528246: step 6577, loss 4.28137.
Train: 2018-08-02T14:58:37.590759: step 6578, loss 4.15545.
Train: 2018-08-02T14:58:37.653243: step 6579, loss 3.9036.
Train: 2018-08-02T14:58:37.700082: step 6580, loss 2.01476.
Test: 2018-08-02T14:58:37.903158: step 6580, loss 3.81863.
Train: 2018-08-02T14:58:37.965675: step 6581, loss 3.39991.
Train: 2018-08-02T14:58:38.028130: step 6582, loss 4.40729.
Train: 2018-08-02T14:58:38.075024: step 6583, loss 3.39991.
Train: 2018-08-02T14:58:38.137512: step 6584, loss 3.9036.
Train: 2018-08-02T14:58:38.199996: step 6585, loss 3.52583.
Train: 2018-08-02T14:58:38.262481: step 6586, loss 4.91098.
Train: 2018-08-02T14:58:38.309342: step 6587, loss 4.02952.
Train: 2018-08-02T14:58:38.371827: step 6588, loss 2.89622.
Train: 2018-08-02T14:58:38.434310: step 6589, loss 4.02952.
Train: 2018-08-02T14:58:38.481149: step 6590, loss 3.9036.
Test: 2018-08-02T14:58:38.684225: step 6590, loss 3.81863.
Train: 2018-08-02T14:58:38.746744: step 6591, loss 4.40729.
Train: 2018-08-02T14:58:38.809196: step 6592, loss 3.77768.
Train: 2018-08-02T14:58:38.871683: step 6593, loss 4.40729.
Train: 2018-08-02T14:58:38.934168: step 6594, loss 4.28137.
Train: 2018-08-02T14:58:38.981032: step 6595, loss 4.02952.
Train: 2018-08-02T14:58:39.043550: step 6596, loss 4.40729.
Train: 2018-08-02T14:58:39.106032: step 6597, loss 3.39991.
Train: 2018-08-02T14:58:39.152897: step 6598, loss 2.64438.
Train: 2018-08-02T14:58:39.215383: step 6599, loss 4.53321.
Train: 2018-08-02T14:58:39.277867: step 6600, loss 3.77768.
Test: 2018-08-02T14:58:39.465293: step 6600, loss 3.81863.
Train: 2018-08-02T14:58:40.012071: step 6601, loss 4.15545.
Train: 2018-08-02T14:58:40.074526: step 6602, loss 3.65176.
Train: 2018-08-02T14:58:40.121421: step 6603, loss 3.65176.
Train: 2018-08-02T14:58:40.183905: step 6604, loss 2.89622.
Train: 2018-08-02T14:58:40.246361: step 6605, loss 4.65914.
Train: 2018-08-02T14:58:40.308875: step 6606, loss 3.39991.
Train: 2018-08-02T14:58:40.355709: step 6607, loss 3.14807.
Train: 2018-08-02T14:58:40.418226: step 6608, loss 4.15545.
Train: 2018-08-02T14:58:40.480681: step 6609, loss 3.27399.
Train: 2018-08-02T14:58:40.527544: step 6610, loss 4.91098.
Test: 2018-08-02T14:58:40.730622: step 6610, loss 3.81863.
Train: 2018-08-02T14:58:40.793108: step 6611, loss 3.14807.
Train: 2018-08-02T14:58:40.855621: step 6612, loss 3.14807.
Train: 2018-08-02T14:58:40.918108: step 6613, loss 4.40729.
Train: 2018-08-02T14:58:40.964973: step 6614, loss 4.40729.
Train: 2018-08-02T14:58:41.027428: step 6615, loss 4.78506.
Train: 2018-08-02T14:58:41.089942: step 6616, loss 4.15545.
Train: 2018-08-02T14:58:41.136808: step 6617, loss 4.40729.
Train: 2018-08-02T14:58:41.199264: step 6618, loss 3.9036.
Train: 2018-08-02T14:58:41.261748: step 6619, loss 5.0369.
Train: 2018-08-02T14:58:41.324260: step 6620, loss 5.28875.
Test: 2018-08-02T14:58:41.511689: step 6620, loss 3.81863.
Train: 2018-08-02T14:58:41.574204: step 6621, loss 3.39991.
Train: 2018-08-02T14:58:41.636659: step 6622, loss 4.53321.
Train: 2018-08-02T14:58:41.683554: step 6623, loss 4.78506.
Train: 2018-08-02T14:58:41.746056: step 6624, loss 4.15545.
Train: 2018-08-02T14:58:41.808495: step 6625, loss 4.91098.
Train: 2018-08-02T14:58:41.855391: step 6626, loss 3.9036.
Train: 2018-08-02T14:58:41.917868: step 6627, loss 4.15545.
Train: 2018-08-02T14:58:41.980331: step 6628, loss 4.78506.
Train: 2018-08-02T14:58:42.042845: step 6629, loss 3.77768.
Train: 2018-08-02T14:58:42.089709: step 6630, loss 3.14807.
Test: 2018-08-02T14:58:42.292756: step 6630, loss 3.81863.
Train: 2018-08-02T14:58:42.355242: step 6631, loss 4.02952.
Train: 2018-08-02T14:58:42.417756: step 6632, loss 4.15545.
Train: 2018-08-02T14:58:42.480242: step 6633, loss 4.40729.
Train: 2018-08-02T14:58:42.527106: step 6634, loss 3.14807.
Train: 2018-08-02T14:58:42.589561: step 6635, loss 4.15545.
Train: 2018-08-02T14:58:42.652076: step 6636, loss 3.14807.
Train: 2018-08-02T14:58:42.698941: step 6637, loss 3.14807.
Train: 2018-08-02T14:58:42.761398: step 6638, loss 4.15545.
Train: 2018-08-02T14:58:42.823907: step 6639, loss 3.77768.
Train: 2018-08-02T14:58:42.870772: step 6640, loss 4.15545.
Test: 2018-08-02T14:58:43.073823: step 6640, loss 3.81863.
Train: 2018-08-02T14:58:43.136308: step 6641, loss 4.02952.
Train: 2018-08-02T14:58:43.198794: step 6642, loss 5.28875.
Train: 2018-08-02T14:58:43.261279: step 6643, loss 4.53321.
Train: 2018-08-02T14:58:43.308177: step 6644, loss 4.83543.
Train: 2018-08-02T14:58:43.370659: step 6645, loss 3.9036.
Train: 2018-08-02T14:58:43.417522: step 6646, loss 3.77768.
Train: 2018-08-02T14:58:43.480008: step 6647, loss 3.27399.
Train: 2018-08-02T14:58:43.542494: step 6648, loss 3.14807.
Train: 2018-08-02T14:58:43.604951: step 6649, loss 4.53321.
Train: 2018-08-02T14:58:43.651846: step 6650, loss 5.0369.
Test: 2018-08-02T14:58:43.854889: step 6650, loss 3.81863.
Train: 2018-08-02T14:58:43.917375: step 6651, loss 3.52583.
Train: 2018-08-02T14:58:43.964270: step 6652, loss 4.53321.
Train: 2018-08-02T14:58:44.026756: step 6653, loss 4.15545.
Train: 2018-08-02T14:58:44.089238: step 6654, loss 4.65914.
Train: 2018-08-02T14:58:44.151697: step 6655, loss 3.77768.
Train: 2018-08-02T14:58:44.198590: step 6656, loss 4.53321.
Train: 2018-08-02T14:58:44.261045: step 6657, loss 4.15545.
Train: 2018-08-02T14:58:44.323532: step 6658, loss 5.28875.
Train: 2018-08-02T14:58:44.370419: step 6659, loss 4.02952.
Train: 2018-08-02T14:58:44.432910: step 6660, loss 4.02952.
Test: 2018-08-02T14:58:44.635982: step 6660, loss 3.81863.
Train: 2018-08-02T14:58:44.682851: step 6661, loss 3.77768.
Train: 2018-08-02T14:58:44.745333: step 6662, loss 4.15545.
Train: 2018-08-02T14:58:44.807822: step 6663, loss 4.65914.
Train: 2018-08-02T14:58:44.870278: step 6664, loss 5.0369.
Train: 2018-08-02T14:58:44.917172: step 6665, loss 3.52583.
Train: 2018-08-02T14:58:44.979657: step 6666, loss 4.28137.
Train: 2018-08-02T14:58:45.042142: step 6667, loss 4.65914.
Train: 2018-08-02T14:58:45.088977: step 6668, loss 3.65176.
Train: 2018-08-02T14:58:45.151486: step 6669, loss 3.77768.
Train: 2018-08-02T14:58:45.213976: step 6670, loss 3.9036.
Test: 2018-08-02T14:58:45.417054: step 6670, loss 3.81863.
Train: 2018-08-02T14:58:45.463889: step 6671, loss 3.52583.
Train: 2018-08-02T14:58:45.526374: step 6672, loss 3.65176.
Train: 2018-08-02T14:58:45.588889: step 6673, loss 4.15545.
Train: 2018-08-02T14:58:45.651344: step 6674, loss 3.77768.
Train: 2018-08-02T14:58:45.698239: step 6675, loss 2.7703.
Train: 2018-08-02T14:58:45.760693: step 6676, loss 3.9036.
Train: 2018-08-02T14:58:45.823209: step 6677, loss 4.40729.
Train: 2018-08-02T14:58:45.885697: step 6678, loss 3.14807.
Train: 2018-08-02T14:58:45.932559: step 6679, loss 3.9036.
Train: 2018-08-02T14:58:45.995041: step 6680, loss 3.9036.
Test: 2018-08-02T14:58:46.198105: step 6680, loss 3.81863.
Train: 2018-08-02T14:58:46.244983: step 6681, loss 4.28137.
Train: 2018-08-02T14:58:46.307440: step 6682, loss 3.77768.
Train: 2018-08-02T14:58:46.369956: step 6683, loss 4.78506.
Train: 2018-08-02T14:58:46.432443: step 6684, loss 4.28137.
Train: 2018-08-02T14:58:46.479308: step 6685, loss 4.53321.
Train: 2018-08-02T14:58:46.541791: step 6686, loss 3.9036.
Train: 2018-08-02T14:58:46.604276: step 6687, loss 3.65176.
Train: 2018-08-02T14:58:46.651140: step 6688, loss 4.40729.
Train: 2018-08-02T14:58:46.713626: step 6689, loss 4.15545.
Train: 2018-08-02T14:58:46.776082: step 6690, loss 4.78506.
Test: 2018-08-02T14:58:46.963561: step 6690, loss 3.81863.
Train: 2018-08-02T14:58:47.026022: step 6691, loss 4.65914.
Train: 2018-08-02T14:58:47.088532: step 6692, loss 3.27399.
Train: 2018-08-02T14:58:47.151023: step 6693, loss 4.15545.
Train: 2018-08-02T14:58:47.213509: step 6694, loss 3.14807.
Train: 2018-08-02T14:58:47.260370: step 6695, loss 4.28137.
Train: 2018-08-02T14:58:47.322855: step 6696, loss 4.78506.
Train: 2018-08-02T14:58:47.385338: step 6697, loss 3.77768.
Train: 2018-08-02T14:58:47.432207: step 6698, loss 3.14807.
Train: 2018-08-02T14:58:47.494663: step 6699, loss 4.65914.
Train: 2018-08-02T14:58:47.557148: step 6700, loss 3.27399.
Test: 2018-08-02T14:58:47.760257: step 6700, loss 3.81863.
Train: 2018-08-02T14:58:48.291381: step 6701, loss 4.53321.
Train: 2018-08-02T14:58:48.338240: step 6702, loss 3.98496.
Train: 2018-08-02T14:58:48.400726: step 6703, loss 4.02952.
Train: 2018-08-02T14:58:48.463217: step 6704, loss 4.28137.
Train: 2018-08-02T14:58:48.525671: step 6705, loss 4.65914.
Train: 2018-08-02T14:58:48.572576: step 6706, loss 4.53321.
Train: 2018-08-02T14:58:48.635052: step 6707, loss 4.40729.
Train: 2018-08-02T14:58:48.697537: step 6708, loss 4.40729.
Train: 2018-08-02T14:58:48.760023: step 6709, loss 5.16283.
Train: 2018-08-02T14:58:48.806886: step 6710, loss 3.77768.
Test: 2018-08-02T14:58:49.009964: step 6710, loss 3.81863.
Train: 2018-08-02T14:58:49.072449: step 6711, loss 4.65914.
Train: 2018-08-02T14:58:49.134935: step 6712, loss 3.9036.
Train: 2018-08-02T14:58:49.181798: step 6713, loss 3.9036.
Train: 2018-08-02T14:58:49.244254: step 6714, loss 4.28137.
Train: 2018-08-02T14:58:49.306764: step 6715, loss 3.39991.
Train: 2018-08-02T14:58:49.369223: step 6716, loss 3.27399.
Train: 2018-08-02T14:58:49.416119: step 6717, loss 3.39991.
Train: 2018-08-02T14:58:49.478575: step 6718, loss 3.52583.
Train: 2018-08-02T14:58:49.541058: step 6719, loss 3.39991.
Train: 2018-08-02T14:58:49.587948: step 6720, loss 3.65176.
Test: 2018-08-02T14:58:49.791031: step 6720, loss 3.81863.
Train: 2018-08-02T14:58:49.853510: step 6721, loss 3.9036.
Train: 2018-08-02T14:58:49.916001: step 6722, loss 4.28137.
Train: 2018-08-02T14:58:49.978456: step 6723, loss 3.77768.
Train: 2018-08-02T14:58:50.040990: step 6724, loss 4.28137.
Train: 2018-08-02T14:58:50.087830: step 6725, loss 5.28875.
Train: 2018-08-02T14:58:50.150290: step 6726, loss 4.40729.
Train: 2018-08-02T14:58:50.212806: step 6727, loss 3.9036.
Train: 2018-08-02T14:58:50.259672: step 6728, loss 3.77768.
Train: 2018-08-02T14:58:50.322125: step 6729, loss 4.40729.
Train: 2018-08-02T14:58:50.384642: step 6730, loss 4.53321.
Test: 2018-08-02T14:58:50.572091: step 6730, loss 3.81863.
Train: 2018-08-02T14:58:50.634552: step 6731, loss 5.5406.
Train: 2018-08-02T14:58:50.697068: step 6732, loss 3.77768.
Train: 2018-08-02T14:58:50.759554: step 6733, loss 4.65914.
Train: 2018-08-02T14:58:50.822040: step 6734, loss 3.27399.
Train: 2018-08-02T14:58:50.868904: step 6735, loss 4.15545.
Train: 2018-08-02T14:58:50.931382: step 6736, loss 4.40729.
Train: 2018-08-02T14:58:50.993872: step 6737, loss 4.78506.
Train: 2018-08-02T14:58:51.040733: step 6738, loss 4.53321.
Train: 2018-08-02T14:58:51.103226: step 6739, loss 3.27399.
Train: 2018-08-02T14:58:51.165677: step 6740, loss 3.27399.
Test: 2018-08-02T14:58:51.353158: step 6740, loss 3.81863.
Train: 2018-08-02T14:58:51.415621: step 6741, loss 3.65176.
Train: 2018-08-02T14:58:51.478131: step 6742, loss 3.39991.
Train: 2018-08-02T14:58:51.540589: step 6743, loss 4.02952.
Train: 2018-08-02T14:58:51.603106: step 6744, loss 3.65176.
Train: 2018-08-02T14:58:51.649971: step 6745, loss 3.9036.
Train: 2018-08-02T14:58:51.712456: step 6746, loss 3.52583.
Train: 2018-08-02T14:58:51.774935: step 6747, loss 3.14807.
Train: 2018-08-02T14:58:51.821800: step 6748, loss 4.65914.
Train: 2018-08-02T14:58:51.884261: step 6749, loss 4.02952.
Train: 2018-08-02T14:58:51.946746: step 6750, loss 4.53321.
Test: 2018-08-02T14:58:52.134233: step 6750, loss 3.81863.
Train: 2018-08-02T14:58:52.196716: step 6751, loss 2.89622.
Train: 2018-08-02T14:58:52.259198: step 6752, loss 5.41467.
Train: 2018-08-02T14:58:52.321658: step 6753, loss 4.02952.
Train: 2018-08-02T14:58:52.384174: step 6754, loss 3.52583.
Train: 2018-08-02T14:58:52.431038: step 6755, loss 3.52583.
Train: 2018-08-02T14:58:52.493493: step 6756, loss 3.9036.
Train: 2018-08-02T14:58:52.556002: step 6757, loss 4.28137.
Train: 2018-08-02T14:58:52.602874: step 6758, loss 4.78506.
Train: 2018-08-02T14:58:52.665326: step 6759, loss 3.52583.
Train: 2018-08-02T14:58:52.727843: step 6760, loss 4.65914.
Test: 2018-08-02T14:58:52.930889: step 6760, loss 3.81863.
Train: 2018-08-02T14:58:52.993407: step 6761, loss 3.77768.
Train: 2018-08-02T14:58:53.040239: step 6762, loss 3.27399.
Train: 2018-08-02T14:58:53.102757: step 6763, loss 3.9036.
Train: 2018-08-02T14:58:53.165211: step 6764, loss 4.15545.
Train: 2018-08-02T14:58:53.227722: step 6765, loss 4.02952.
Train: 2018-08-02T14:58:53.274585: step 6766, loss 4.28137.
Train: 2018-08-02T14:58:53.337045: step 6767, loss 3.9036.
Train: 2018-08-02T14:58:53.399532: step 6768, loss 3.52583.
Train: 2018-08-02T14:58:53.446394: step 6769, loss 3.9036.
Train: 2018-08-02T14:58:53.508910: step 6770, loss 4.40729.
Test: 2018-08-02T14:58:53.711980: step 6770, loss 3.81863.
Train: 2018-08-02T14:58:53.758822: step 6771, loss 3.77768.
Train: 2018-08-02T14:58:53.821336: step 6772, loss 3.65176.
Train: 2018-08-02T14:58:53.883825: step 6773, loss 4.02952.
Train: 2018-08-02T14:58:53.946303: step 6774, loss 3.39991.
Train: 2018-08-02T14:58:54.008764: step 6775, loss 3.02214.
Train: 2018-08-02T14:58:54.055652: step 6776, loss 3.52583.
Train: 2018-08-02T14:58:54.118141: step 6777, loss 4.53321.
Train: 2018-08-02T14:58:54.180597: step 6778, loss 3.77768.
Train: 2018-08-02T14:58:54.227461: step 6779, loss 3.9036.
Train: 2018-08-02T14:58:54.289977: step 6780, loss 4.40729.
Test: 2018-08-02T14:58:54.493056: step 6780, loss 3.81863.
Train: 2018-08-02T14:58:54.555509: step 6781, loss 3.14807.
Train: 2018-08-02T14:58:54.617995: step 6782, loss 3.9036.
Train: 2018-08-02T14:58:54.664889: step 6783, loss 3.9036.
Train: 2018-08-02T14:58:54.727346: step 6784, loss 4.40729.
Train: 2018-08-02T14:58:54.789830: step 6785, loss 4.65914.
Train: 2018-08-02T14:58:54.836724: step 6786, loss 4.28137.
Train: 2018-08-02T14:58:54.899209: step 6787, loss 3.65176.
Train: 2018-08-02T14:58:54.961696: step 6788, loss 3.39991.
Train: 2018-08-02T14:58:55.024151: step 6789, loss 3.77768.
Train: 2018-08-02T14:58:55.071038: step 6790, loss 3.14807.
Test: 2018-08-02T14:58:55.274115: step 6790, loss 3.81863.
Train: 2018-08-02T14:58:55.336607: step 6791, loss 3.9036.
Train: 2018-08-02T14:58:55.399091: step 6792, loss 3.9036.
Train: 2018-08-02T14:58:55.445925: step 6793, loss 3.9036.
Train: 2018-08-02T14:58:55.508444: step 6794, loss 3.77768.
Train: 2018-08-02T14:58:55.555306: step 6795, loss 4.02952.
Train: 2018-08-02T14:58:55.617790: step 6796, loss 3.77768.
Train: 2018-08-02T14:58:55.680273: step 6797, loss 3.9036.
Train: 2018-08-02T14:58:55.727140: step 6798, loss 4.78506.
Train: 2018-08-02T14:58:55.789625: step 6799, loss 4.65914.
Train: 2018-08-02T14:58:55.852112: step 6800, loss 5.0369.
Test: 2018-08-02T14:58:56.039560: step 6800, loss 3.81863.
Train: 2018-08-02T14:58:56.570692: step 6801, loss 4.40729.
Train: 2018-08-02T14:58:56.633148: step 6802, loss 3.65176.
Train: 2018-08-02T14:58:56.695676: step 6803, loss 3.9036.
Train: 2018-08-02T14:58:56.758151: step 6804, loss 2.14068.
Train: 2018-08-02T14:58:56.804984: step 6805, loss 3.39991.
Train: 2018-08-02T14:58:56.867500: step 6806, loss 4.15545.
Train: 2018-08-02T14:58:56.929984: step 6807, loss 4.40729.
Train: 2018-08-02T14:58:56.992439: step 6808, loss 4.40729.
Train: 2018-08-02T14:58:57.039332: step 6809, loss 4.02952.
Train: 2018-08-02T14:58:57.101787: step 6810, loss 4.78506.
Test: 2018-08-02T14:58:57.304864: step 6810, loss 3.81863.
Train: 2018-08-02T14:58:57.367350: step 6811, loss 4.02952.
Train: 2018-08-02T14:58:57.429866: step 6812, loss 4.65914.
Train: 2018-08-02T14:58:57.476733: step 6813, loss 4.28137.
Train: 2018-08-02T14:58:57.539188: step 6814, loss 3.14807.
Train: 2018-08-02T14:58:57.601676: step 6815, loss 3.39991.
Train: 2018-08-02T14:58:57.664157: step 6816, loss 4.02952.
Train: 2018-08-02T14:58:57.711020: step 6817, loss 4.02952.
Train: 2018-08-02T14:58:57.773540: step 6818, loss 4.15545.
Train: 2018-08-02T14:58:57.836022: step 6819, loss 4.78506.
Train: 2018-08-02T14:58:57.882882: step 6820, loss 4.53321.
Test: 2018-08-02T14:58:58.085931: step 6820, loss 3.81863.
Train: 2018-08-02T14:58:58.148448: step 6821, loss 4.65914.
Train: 2018-08-02T14:58:58.210933: step 6822, loss 3.14807.
Train: 2018-08-02T14:58:58.257799: step 6823, loss 4.65914.
Train: 2018-08-02T14:58:58.320283: step 6824, loss 3.02214.
Train: 2018-08-02T14:58:58.382737: step 6825, loss 3.65176.
Train: 2018-08-02T14:58:58.445253: step 6826, loss 3.77768.
Train: 2018-08-02T14:58:58.492114: step 6827, loss 3.9036.
Train: 2018-08-02T14:58:58.554605: step 6828, loss 4.78506.
Train: 2018-08-02T14:58:58.617058: step 6829, loss 3.39991.
Train: 2018-08-02T14:58:58.663922: step 6830, loss 4.28137.
Test: 2018-08-02T14:58:58.867024: step 6830, loss 3.81863.
Train: 2018-08-02T14:58:58.929516: step 6831, loss 4.02952.
Train: 2018-08-02T14:58:58.992002: step 6832, loss 5.16283.
Train: 2018-08-02T14:58:59.054481: step 6833, loss 3.14807.
Train: 2018-08-02T14:58:59.101321: step 6834, loss 4.28137.
Train: 2018-08-02T14:58:59.163836: step 6835, loss 3.52583.
Train: 2018-08-02T14:58:59.226289: step 6836, loss 3.9036.
Train: 2018-08-02T14:58:59.273178: step 6837, loss 3.77768.
Train: 2018-08-02T14:58:59.335642: step 6838, loss 3.39991.
Train: 2018-08-02T14:58:59.398126: step 6839, loss 3.27399.
Train: 2018-08-02T14:58:59.445019: step 6840, loss 2.64438.
Test: 2018-08-02T14:58:59.648065: step 6840, loss 3.81863.
Train: 2018-08-02T14:58:59.710583: step 6841, loss 4.91098.
Train: 2018-08-02T14:58:59.773063: step 6842, loss 4.02952.
Train: 2018-08-02T14:58:59.819932: step 6843, loss 3.77768.
Train: 2018-08-02T14:58:59.882419: step 6844, loss 4.28137.
Train: 2018-08-02T14:58:59.944903: step 6845, loss 4.40729.
Train: 2018-08-02T14:59:00.007358: step 6846, loss 3.77768.
Train: 2018-08-02T14:59:00.054221: step 6847, loss 5.16283.
Train: 2018-08-02T14:59:00.116735: step 6848, loss 3.27399.
Train: 2018-08-02T14:59:00.179223: step 6849, loss 4.53321.
Train: 2018-08-02T14:59:00.226089: step 6850, loss 3.77768.
Test: 2018-08-02T14:59:00.429158: step 6850, loss 3.81863.
Train: 2018-08-02T14:59:00.491651: step 6851, loss 2.89622.
Train: 2018-08-02T14:59:00.554135: step 6852, loss 3.52583.
Train: 2018-08-02T14:59:00.600968: step 6853, loss 4.02952.
Train: 2018-08-02T14:59:00.663456: step 6854, loss 4.28137.
Train: 2018-08-02T14:59:00.725940: step 6855, loss 3.9036.
Train: 2018-08-02T14:59:00.772834: step 6856, loss 3.27399.
Train: 2018-08-02T14:59:00.835288: step 6857, loss 4.78506.
Train: 2018-08-02T14:59:00.897775: step 6858, loss 5.41467.
Train: 2018-08-02T14:59:00.944638: step 6859, loss 3.77768.
Train: 2018-08-02T14:59:01.007155: step 6860, loss 4.65914.
Test: 2018-08-02T14:59:01.210200: step 6860, loss 3.81863.
Train: 2018-08-02T14:59:01.272716: step 6861, loss 3.39991.
Train: 2018-08-02T14:59:01.335196: step 6862, loss 4.40729.
Train: 2018-08-02T14:59:01.397656: step 6863, loss 4.65914.
Train: 2018-08-02T14:59:01.444552: step 6864, loss 3.9036.
Train: 2018-08-02T14:59:01.507008: step 6865, loss 4.40729.
Train: 2018-08-02T14:59:01.569492: step 6866, loss 3.14807.
Train: 2018-08-02T14:59:01.632009: step 6867, loss 3.02214.
Train: 2018-08-02T14:59:01.678875: step 6868, loss 4.02952.
Train: 2018-08-02T14:59:01.741328: step 6869, loss 4.28137.
Train: 2018-08-02T14:59:01.803812: step 6870, loss 4.15545.
Test: 2018-08-02T14:59:02.006889: step 6870, loss 3.81863.
Train: 2018-08-02T14:59:02.069405: step 6871, loss 4.40729.
Train: 2018-08-02T14:59:02.116271: step 6872, loss 3.77768.
Train: 2018-08-02T14:59:02.178725: step 6873, loss 4.15545.
Train: 2018-08-02T14:59:02.241238: step 6874, loss 4.28137.
Train: 2018-08-02T14:59:02.288104: step 6875, loss 2.64438.
Train: 2018-08-02T14:59:02.350559: step 6876, loss 3.14807.
Train: 2018-08-02T14:59:02.413045: step 6877, loss 4.28137.
Train: 2018-08-02T14:59:02.475531: step 6878, loss 4.40729.
Train: 2018-08-02T14:59:02.522424: step 6879, loss 3.65176.
Train: 2018-08-02T14:59:02.584910: step 6880, loss 4.15545.
Test: 2018-08-02T14:59:02.787991: step 6880, loss 3.81863.
Train: 2018-08-02T14:59:02.850473: step 6881, loss 3.14807.
Train: 2018-08-02T14:59:02.897336: step 6882, loss 2.14068.
Train: 2018-08-02T14:59:02.959791: step 6883, loss 4.28137.
Train: 2018-08-02T14:59:03.022307: step 6884, loss 3.52583.
Train: 2018-08-02T14:59:03.084763: step 6885, loss 3.02214.
Train: 2018-08-02T14:59:03.131653: step 6886, loss 5.28875.
Train: 2018-08-02T14:59:03.194113: step 6887, loss 3.52583.
Train: 2018-08-02T14:59:03.256629: step 6888, loss 4.65914.
Train: 2018-08-02T14:59:03.303491: step 6889, loss 3.9036.
Train: 2018-08-02T14:59:03.365946: step 6890, loss 3.9036.
Test: 2018-08-02T14:59:03.569022: step 6890, loss 3.81863.
Train: 2018-08-02T14:59:03.631509: step 6891, loss 4.02952.
Train: 2018-08-02T14:59:03.694025: step 6892, loss 4.65914.
Train: 2018-08-02T14:59:03.740859: step 6893, loss 4.02952.
Train: 2018-08-02T14:59:03.803345: step 6894, loss 3.9036.
Train: 2018-08-02T14:59:03.865829: step 6895, loss 4.15545.
Train: 2018-08-02T14:59:03.928347: step 6896, loss 4.40729.
Train: 2018-08-02T14:59:03.990829: step 6897, loss 3.52583.
Train: 2018-08-02T14:59:04.037666: step 6898, loss 3.14807.
Train: 2018-08-02T14:59:04.100150: step 6899, loss 4.78506.
Train: 2018-08-02T14:59:04.162664: step 6900, loss 3.02214.
Test: 2018-08-02T14:59:04.350090: step 6900, loss 3.81863.
Train: 2018-08-02T14:59:04.896838: step 6901, loss 3.65176.
Train: 2018-08-02T14:59:04.959352: step 6902, loss 5.79244.
Train: 2018-08-02T14:59:05.021808: step 6903, loss 4.28137.
Train: 2018-08-02T14:59:05.068701: step 6904, loss 5.16283.
Train: 2018-08-02T14:59:05.131159: step 6905, loss 2.7703.
Train: 2018-08-02T14:59:05.193674: step 6906, loss 3.27399.
Train: 2018-08-02T14:59:05.240539: step 6907, loss 4.40729.
Train: 2018-08-02T14:59:05.303024: step 6908, loss 3.9036.
Train: 2018-08-02T14:59:05.365511: step 6909, loss 4.15545.
Train: 2018-08-02T14:59:05.412372: step 6910, loss 4.15545.
Test: 2018-08-02T14:59:05.615444: step 6910, loss 3.81863.
Train: 2018-08-02T14:59:05.677934: step 6911, loss 3.77768.
Train: 2018-08-02T14:59:05.740421: step 6912, loss 5.28875.
Train: 2018-08-02T14:59:05.802875: step 6913, loss 4.40729.
Train: 2018-08-02T14:59:05.849773: step 6914, loss 4.02952.
Train: 2018-08-02T14:59:05.912224: step 6915, loss 4.40729.
Train: 2018-08-02T14:59:05.974740: step 6916, loss 2.7703.
Train: 2018-08-02T14:59:06.021599: step 6917, loss 3.52583.
Train: 2018-08-02T14:59:06.084093: step 6918, loss 3.65176.
Train: 2018-08-02T14:59:06.146547: step 6919, loss 4.15545.
Train: 2018-08-02T14:59:06.193439: step 6920, loss 4.91098.
Test: 2018-08-02T14:59:06.396511: step 6920, loss 3.81863.
Train: 2018-08-02T14:59:06.459002: step 6921, loss 3.65176.
Train: 2018-08-02T14:59:06.521488: step 6922, loss 3.9036.
Train: 2018-08-02T14:59:06.583942: step 6923, loss 4.40729.
Train: 2018-08-02T14:59:06.646429: step 6924, loss 3.27399.
Train: 2018-08-02T14:59:06.693324: step 6925, loss 4.02952.
Train: 2018-08-02T14:59:06.755811: step 6926, loss 2.89622.
Train: 2018-08-02T14:59:06.818293: step 6927, loss 3.77768.
Train: 2018-08-02T14:59:06.865159: step 6928, loss 4.02952.
Train: 2018-08-02T14:59:06.927613: step 6929, loss 3.14807.
Train: 2018-08-02T14:59:06.990129: step 6930, loss 4.40729.
Test: 2018-08-02T14:59:07.177583: step 6930, loss 3.81863.
Train: 2018-08-02T14:59:07.240066: step 6931, loss 4.53321.
Train: 2018-08-02T14:59:07.302525: step 6932, loss 4.40729.
Train: 2018-08-02T14:59:07.365039: step 6933, loss 5.28875.
Train: 2018-08-02T14:59:07.411905: step 6934, loss 3.65176.
Train: 2018-08-02T14:59:07.474392: step 6935, loss 4.65914.
Train: 2018-08-02T14:59:07.536869: step 6936, loss 4.28137.
Train: 2018-08-02T14:59:07.583739: step 6937, loss 4.15545.
Train: 2018-08-02T14:59:07.646195: step 6938, loss 4.15545.
Train: 2018-08-02T14:59:07.708685: step 6939, loss 4.65914.
Train: 2018-08-02T14:59:07.755544: step 6940, loss 5.16283.
Test: 2018-08-02T14:59:07.958621: step 6940, loss 3.81863.
Train: 2018-08-02T14:59:08.021137: step 6941, loss 3.65176.
Train: 2018-08-02T14:59:08.083592: step 6942, loss 5.0369.
Train: 2018-08-02T14:59:08.146102: step 6943, loss 4.15545.
Train: 2018-08-02T14:59:08.192972: step 6944, loss 4.15545.
Train: 2018-08-02T14:59:08.255428: step 6945, loss 3.65176.
Train: 2018-08-02T14:59:08.302291: step 6946, loss 3.22362.
Train: 2018-08-02T14:59:08.364775: step 6947, loss 4.40729.
Train: 2018-08-02T14:59:08.411640: step 6948, loss 4.53321.
Train: 2018-08-02T14:59:08.474157: step 6949, loss 4.02952.
Train: 2018-08-02T14:59:08.536635: step 6950, loss 3.14807.
Test: 2018-08-02T14:59:08.739688: step 6950, loss 3.81863.
Train: 2018-08-02T14:59:08.786586: step 6951, loss 4.40729.
Train: 2018-08-02T14:59:08.849036: step 6952, loss 3.39991.
Train: 2018-08-02T14:59:08.911553: step 6953, loss 3.65176.
Train: 2018-08-02T14:59:08.974041: step 6954, loss 4.65914.
Train: 2018-08-02T14:59:09.036520: step 6955, loss 4.40729.
Train: 2018-08-02T14:59:09.083356: step 6956, loss 3.9036.
Train: 2018-08-02T14:59:09.145868: step 6957, loss 4.28137.
Train: 2018-08-02T14:59:09.208332: step 6958, loss 5.41467.
Train: 2018-08-02T14:59:09.255192: step 6959, loss 3.14807.
Train: 2018-08-02T14:59:09.317677: step 6960, loss 3.65176.
Test: 2018-08-02T14:59:09.520779: step 6960, loss 3.81863.
Train: 2018-08-02T14:59:09.583270: step 6961, loss 4.28137.
Train: 2018-08-02T14:59:09.630104: step 6962, loss 4.15545.
Train: 2018-08-02T14:59:09.692620: step 6963, loss 4.15545.
Train: 2018-08-02T14:59:09.755076: step 6964, loss 4.78506.
Train: 2018-08-02T14:59:09.817591: step 6965, loss 4.40729.
Train: 2018-08-02T14:59:09.864461: step 6966, loss 3.27399.
Train: 2018-08-02T14:59:09.926940: step 6967, loss 5.0369.
Train: 2018-08-02T14:59:09.989428: step 6968, loss 3.14807.
Train: 2018-08-02T14:59:10.036289: step 6969, loss 4.02952.
Train: 2018-08-02T14:59:10.098775: step 6970, loss 3.52583.
Test: 2018-08-02T14:59:10.301846: step 6970, loss 3.81863.
Train: 2018-08-02T14:59:10.364336: step 6971, loss 3.65176.
Train: 2018-08-02T14:59:10.426817: step 6972, loss 3.52583.
Train: 2018-08-02T14:59:10.473656: step 6973, loss 4.28137.
Train: 2018-08-02T14:59:10.536173: step 6974, loss 4.02952.
Train: 2018-08-02T14:59:10.598658: step 6975, loss 3.77768.
Train: 2018-08-02T14:59:10.661143: step 6976, loss 4.28137.
Train: 2018-08-02T14:59:10.707976: step 6977, loss 3.9036.
Train: 2018-08-02T14:59:10.770462: step 6978, loss 4.78506.
Train: 2018-08-02T14:59:10.832952: step 6979, loss 3.27399.
Train: 2018-08-02T14:59:10.879843: step 6980, loss 4.65914.
Test: 2018-08-02T14:59:11.082889: step 6980, loss 3.81863.
Train: 2018-08-02T14:59:11.145405: step 6981, loss 4.02952.
Train: 2018-08-02T14:59:11.207890: step 6982, loss 4.02952.
Train: 2018-08-02T14:59:11.270375: step 6983, loss 3.65176.
Train: 2018-08-02T14:59:11.317210: step 6984, loss 4.65914.
Train: 2018-08-02T14:59:11.379722: step 6985, loss 3.77768.
Train: 2018-08-02T14:59:11.442210: step 6986, loss 3.65176.
Train: 2018-08-02T14:59:11.504666: step 6987, loss 4.28137.
Train: 2018-08-02T14:59:11.551560: step 6988, loss 4.78506.
Train: 2018-08-02T14:59:11.614014: step 6989, loss 4.02952.
Train: 2018-08-02T14:59:11.676500: step 6990, loss 4.91098.
Test: 2018-08-02T14:59:11.863980: step 6990, loss 3.81863.
Train: 2018-08-02T14:59:11.926472: step 6991, loss 3.9036.
Train: 2018-08-02T14:59:11.988956: step 6992, loss 3.65176.
Train: 2018-08-02T14:59:12.051442: step 6993, loss 4.53321.
Train: 2018-08-02T14:59:12.098310: step 6994, loss 4.40729.
Train: 2018-08-02T14:59:12.160763: step 6995, loss 4.02952.
Train: 2018-08-02T14:59:12.223247: step 6996, loss 4.78506.
Train: 2018-08-02T14:59:12.270141: step 6997, loss 4.53321.
Train: 2018-08-02T14:59:12.332598: step 6998, loss 4.78506.
Train: 2018-08-02T14:59:12.395113: step 6999, loss 3.77768.
Train: 2018-08-02T14:59:12.441946: step 7000, loss 4.02952.
Test: 2018-08-02T14:59:12.645023: step 7000, loss 3.81863.
Train: 2018-08-02T14:59:13.160558: step 7001, loss 2.89622.
Train: 2018-08-02T14:59:13.223045: step 7002, loss 3.9036.
Train: 2018-08-02T14:59:13.285498: step 7003, loss 4.65914.
Train: 2018-08-02T14:59:13.347984: step 7004, loss 4.28137.
Train: 2018-08-02T14:59:13.394878: step 7005, loss 4.02952.
Train: 2018-08-02T14:59:13.457360: step 7006, loss 2.89622.
Train: 2018-08-02T14:59:13.519848: step 7007, loss 3.39991.
Train: 2018-08-02T14:59:13.566683: step 7008, loss 4.02952.
Train: 2018-08-02T14:59:13.629201: step 7009, loss 4.15545.
Train: 2018-08-02T14:59:13.691684: step 7010, loss 3.65176.
Test: 2018-08-02T14:59:13.879108: step 7010, loss 3.81863.
Train: 2018-08-02T14:59:13.941595: step 7011, loss 3.02214.
Train: 2018-08-02T14:59:14.004106: step 7012, loss 4.02952.
Train: 2018-08-02T14:59:14.066591: step 7013, loss 3.65176.
Train: 2018-08-02T14:59:14.113462: step 7014, loss 4.15545.
Train: 2018-08-02T14:59:14.175914: step 7015, loss 3.65176.
Train: 2018-08-02T14:59:14.238400: step 7016, loss 4.78506.
Train: 2018-08-02T14:59:14.285263: step 7017, loss 4.28137.
Train: 2018-08-02T14:59:14.347775: step 7018, loss 3.65176.
Train: 2018-08-02T14:59:14.410266: step 7019, loss 4.53321.
Train: 2018-08-02T14:59:14.457100: step 7020, loss 3.52583.
Test: 2018-08-02T14:59:14.660206: step 7020, loss 3.81863.
Train: 2018-08-02T14:59:14.722662: step 7021, loss 2.89622.
Train: 2018-08-02T14:59:14.785177: step 7022, loss 3.27399.
Train: 2018-08-02T14:59:14.847662: step 7023, loss 4.40729.
Train: 2018-08-02T14:59:14.910148: step 7024, loss 3.9036.
Train: 2018-08-02T14:59:14.956993: step 7025, loss 5.5406.
Train: 2018-08-02T14:59:15.019467: step 7026, loss 3.65176.
Train: 2018-08-02T14:59:15.081951: step 7027, loss 5.0369.
Train: 2018-08-02T14:59:15.144441: step 7028, loss 3.9036.
Train: 2018-08-02T14:59:15.186689: step 7029, loss 4.02952.
Train: 2018-08-02T14:59:15.249211: step 7030, loss 4.15545.
Test: 2018-08-02T14:59:15.452261: step 7030, loss 3.81863.
Train: 2018-08-02T14:59:15.514741: step 7031, loss 4.53321.
Train: 2018-08-02T14:59:15.577257: step 7032, loss 3.52583.
Train: 2018-08-02T14:59:15.639743: step 7033, loss 4.65914.
Train: 2018-08-02T14:59:15.686607: step 7034, loss 3.27399.
Train: 2018-08-02T14:59:15.749063: step 7035, loss 2.7703.
Train: 2018-08-02T14:59:15.811546: step 7036, loss 4.15545.
Train: 2018-08-02T14:59:15.858438: step 7037, loss 3.9036.
Train: 2018-08-02T14:59:15.920898: step 7038, loss 4.15545.
Train: 2018-08-02T14:59:15.983410: step 7039, loss 5.41467.
Train: 2018-08-02T14:59:16.045891: step 7040, loss 4.15545.
Test: 2018-08-02T14:59:16.233355: step 7040, loss 3.81863.
Train: 2018-08-02T14:59:16.295835: step 7041, loss 3.39991.
Train: 2018-08-02T14:59:16.358324: step 7042, loss 3.39991.
Train: 2018-08-02T14:59:16.420805: step 7043, loss 4.15545.
Train: 2018-08-02T14:59:16.467678: step 7044, loss 4.02952.
Train: 2018-08-02T14:59:16.530160: step 7045, loss 4.02952.
Train: 2018-08-02T14:59:16.592647: step 7046, loss 5.0369.
Train: 2018-08-02T14:59:16.639508: step 7047, loss 3.65176.
Train: 2018-08-02T14:59:16.701990: step 7048, loss 3.65176.
Train: 2018-08-02T14:59:16.764474: step 7049, loss 3.39991.
Train: 2018-08-02T14:59:16.811312: step 7050, loss 4.40729.
Test: 2018-08-02T14:59:17.014389: step 7050, loss 3.81863.
Train: 2018-08-02T14:59:17.076875: step 7051, loss 3.9036.
Train: 2018-08-02T14:59:17.139391: step 7052, loss 3.52583.
Train: 2018-08-02T14:59:17.186225: step 7053, loss 4.28137.
Train: 2018-08-02T14:59:17.248713: step 7054, loss 3.9036.
Train: 2018-08-02T14:59:17.311226: step 7055, loss 3.77768.
Train: 2018-08-02T14:59:17.373681: step 7056, loss 3.65176.
Train: 2018-08-02T14:59:17.420544: step 7057, loss 5.0369.
Train: 2018-08-02T14:59:17.483031: step 7058, loss 4.40729.
Train: 2018-08-02T14:59:17.545549: step 7059, loss 3.77768.
Train: 2018-08-02T14:59:17.592413: step 7060, loss 5.0369.
Test: 2018-08-02T14:59:17.795459: step 7060, loss 3.81863.
Train: 2018-08-02T14:59:17.857943: step 7061, loss 4.15545.
Train: 2018-08-02T14:59:17.920458: step 7062, loss 3.65176.
Train: 2018-08-02T14:59:17.967324: step 7063, loss 3.14807.
Train: 2018-08-02T14:59:18.029779: step 7064, loss 4.02952.
Train: 2018-08-02T14:59:18.092289: step 7065, loss 4.91098.
Train: 2018-08-02T14:59:18.139160: step 7066, loss 4.28137.
Train: 2018-08-02T14:59:18.201613: step 7067, loss 3.65176.
Train: 2018-08-02T14:59:18.264129: step 7068, loss 3.9036.
Train: 2018-08-02T14:59:18.310963: step 7069, loss 4.65914.
Train: 2018-08-02T14:59:18.373448: step 7070, loss 3.9036.
Test: 2018-08-02T14:59:18.576549: step 7070, loss 3.81863.
Train: 2018-08-02T14:59:18.639011: step 7071, loss 4.15545.
Train: 2018-08-02T14:59:18.701495: step 7072, loss 4.40729.
Train: 2018-08-02T14:59:18.748386: step 7073, loss 4.40729.
Train: 2018-08-02T14:59:18.810871: step 7074, loss 4.28137.
Train: 2018-08-02T14:59:18.873332: step 7075, loss 4.15545.
Train: 2018-08-02T14:59:18.935839: step 7076, loss 3.77768.
Train: 2018-08-02T14:59:18.998301: step 7077, loss 3.52583.
Train: 2018-08-02T14:59:19.060801: step 7078, loss 4.15545.
Train: 2018-08-02T14:59:19.107650: step 7079, loss 4.15545.
Train: 2018-08-02T14:59:19.170167: step 7080, loss 3.02214.
Test: 2018-08-02T14:59:19.357591: step 7080, loss 3.81863.
Train: 2018-08-02T14:59:19.420107: step 7081, loss 3.14807.
Train: 2018-08-02T14:59:19.482594: step 7082, loss 3.14807.
Train: 2018-08-02T14:59:19.545077: step 7083, loss 4.28137.
Train: 2018-08-02T14:59:19.591945: step 7084, loss 4.02952.
Train: 2018-08-02T14:59:19.654399: step 7085, loss 3.39991.
Train: 2018-08-02T14:59:19.716882: step 7086, loss 3.9036.
Train: 2018-08-02T14:59:19.779368: step 7087, loss 3.65176.
Train: 2018-08-02T14:59:19.826263: step 7088, loss 2.39253.
Train: 2018-08-02T14:59:19.888719: step 7089, loss 4.78506.
Train: 2018-08-02T14:59:19.951233: step 7090, loss 4.02952.
Test: 2018-08-02T14:59:20.138659: step 7090, loss 3.81863.
Train: 2018-08-02T14:59:20.201175: step 7091, loss 3.77768.
Train: 2018-08-02T14:59:20.263660: step 7092, loss 4.15545.
Train: 2018-08-02T14:59:20.326145: step 7093, loss 4.65914.
Train: 2018-08-02T14:59:20.388631: step 7094, loss 4.02952.
Train: 2018-08-02T14:59:20.435495: step 7095, loss 3.52583.
Train: 2018-08-02T14:59:20.497950: step 7096, loss 3.27399.
Train: 2018-08-02T14:59:20.544813: step 7097, loss 4.56679.
Train: 2018-08-02T14:59:20.607329: step 7098, loss 4.15545.
Train: 2018-08-02T14:59:20.654162: step 7099, loss 3.52583.
Train: 2018-08-02T14:59:20.716649: step 7100, loss 3.9036.
Test: 2018-08-02T14:59:20.919726: step 7100, loss 3.81863.
Train: 2018-08-02T14:59:21.419610: step 7101, loss 5.16283.
Train: 2018-08-02T14:59:21.482124: step 7102, loss 4.65914.
Train: 2018-08-02T14:59:21.528988: step 7103, loss 4.02952.
Train: 2018-08-02T14:59:21.591477: step 7104, loss 4.40729.
Train: 2018-08-02T14:59:21.653929: step 7105, loss 4.91098.
Train: 2018-08-02T14:59:21.716415: step 7106, loss 4.02952.
Train: 2018-08-02T14:59:21.763303: step 7107, loss 3.77768.
Train: 2018-08-02T14:59:21.825794: step 7108, loss 3.77768.
Train: 2018-08-02T14:59:21.888250: step 7109, loss 4.02952.
Train: 2018-08-02T14:59:21.935143: step 7110, loss 4.15545.
Test: 2018-08-02T14:59:22.153842: step 7110, loss 3.81863.
Train: 2018-08-02T14:59:22.216296: step 7111, loss 3.77768.
Train: 2018-08-02T14:59:22.263162: step 7112, loss 3.77768.
Train: 2018-08-02T14:59:22.325677: step 7113, loss 3.9036.
Train: 2018-08-02T14:59:22.388132: step 7114, loss 3.52583.
Train: 2018-08-02T14:59:22.450619: step 7115, loss 3.65176.
Train: 2018-08-02T14:59:22.513132: step 7116, loss 3.02214.
Train: 2018-08-02T14:59:22.559965: step 7117, loss 4.53321.
Train: 2018-08-02T14:59:22.622453: step 7118, loss 5.16283.
Train: 2018-08-02T14:59:22.684968: step 7119, loss 4.91098.
Train: 2018-08-02T14:59:22.731834: step 7120, loss 4.15545.
Test: 2018-08-02T14:59:22.934908: step 7120, loss 3.81863.
Train: 2018-08-02T14:59:22.997364: step 7121, loss 3.9036.
Train: 2018-08-02T14:59:23.059879: step 7122, loss 3.65176.
Train: 2018-08-02T14:59:23.122335: step 7123, loss 4.28137.
Train: 2018-08-02T14:59:23.169225: step 7124, loss 3.9036.
Train: 2018-08-02T14:59:23.231693: step 7125, loss 4.78506.
Train: 2018-08-02T14:59:23.294200: step 7126, loss 3.14807.
Train: 2018-08-02T14:59:23.341067: step 7127, loss 4.53321.
Train: 2018-08-02T14:59:23.403520: step 7128, loss 4.65914.
Train: 2018-08-02T14:59:23.466005: step 7129, loss 4.65914.
Train: 2018-08-02T14:59:23.528490: step 7130, loss 4.40729.
Test: 2018-08-02T14:59:23.731570: step 7130, loss 3.81863.
Train: 2018-08-02T14:59:23.778461: step 7131, loss 4.15545.
Train: 2018-08-02T14:59:23.840916: step 7132, loss 2.89622.
Train: 2018-08-02T14:59:23.903432: step 7133, loss 4.91098.
Train: 2018-08-02T14:59:23.965888: step 7134, loss 4.91098.
Train: 2018-08-02T14:59:24.028374: step 7135, loss 3.65176.
Train: 2018-08-02T14:59:24.075262: step 7136, loss 3.9036.
Train: 2018-08-02T14:59:24.137752: step 7137, loss 5.16283.
Train: 2018-08-02T14:59:24.200214: step 7138, loss 4.53321.
Train: 2018-08-02T14:59:24.247071: step 7139, loss 3.39991.
Train: 2018-08-02T14:59:24.309557: step 7140, loss 3.65176.
Test: 2018-08-02T14:59:24.512633: step 7140, loss 3.81863.
Train: 2018-08-02T14:59:24.575149: step 7141, loss 3.65176.
Train: 2018-08-02T14:59:24.622016: step 7142, loss 4.15545.
Train: 2018-08-02T14:59:24.684499: step 7143, loss 3.77768.
Train: 2018-08-02T14:59:24.746987: step 7144, loss 3.77768.
Train: 2018-08-02T14:59:24.809471: step 7145, loss 4.53321.
Train: 2018-08-02T14:59:24.856304: step 7146, loss 4.65914.
Train: 2018-08-02T14:59:24.918819: step 7147, loss 4.40729.
Train: 2018-08-02T14:59:24.981305: step 7148, loss 4.02952.
Train: 2018-08-02T14:59:25.028169: step 7149, loss 4.65914.
Train: 2018-08-02T14:59:25.090654: step 7150, loss 3.52583.
Test: 2018-08-02T14:59:25.293731: step 7150, loss 3.81863.
Train: 2018-08-02T14:59:25.340599: step 7151, loss 3.27399.
Train: 2018-08-02T14:59:25.403051: step 7152, loss 4.28137.
Train: 2018-08-02T14:59:25.465569: step 7153, loss 3.65176.
Train: 2018-08-02T14:59:25.528029: step 7154, loss 3.02214.
Train: 2018-08-02T14:59:25.574916: step 7155, loss 4.53321.
Train: 2018-08-02T14:59:25.637401: step 7156, loss 3.65176.
Train: 2018-08-02T14:59:25.699886: step 7157, loss 4.53321.
Train: 2018-08-02T14:59:25.746753: step 7158, loss 4.40729.
Train: 2018-08-02T14:59:25.809236: step 7159, loss 3.9036.
Train: 2018-08-02T14:59:25.871692: step 7160, loss 3.39991.
Test: 2018-08-02T14:59:26.074799: step 7160, loss 3.81863.
Train: 2018-08-02T14:59:26.137278: step 7161, loss 3.9036.
Train: 2018-08-02T14:59:26.184148: step 7162, loss 3.77768.
Train: 2018-08-02T14:59:26.246631: step 7163, loss 3.77768.
Train: 2018-08-02T14:59:26.309091: step 7164, loss 3.9036.
Train: 2018-08-02T14:59:26.371607: step 7165, loss 4.02952.
Train: 2018-08-02T14:59:26.418439: step 7166, loss 4.02952.
Train: 2018-08-02T14:59:26.480956: step 7167, loss 3.77768.
Train: 2018-08-02T14:59:26.543439: step 7168, loss 4.65914.
Train: 2018-08-02T14:59:26.605895: step 7169, loss 3.02214.
Train: 2018-08-02T14:59:26.652785: step 7170, loss 4.53321.
Test: 2018-08-02T14:59:26.855834: step 7170, loss 3.81863.
Train: 2018-08-02T14:59:26.918352: step 7171, loss 4.91098.
Train: 2018-08-02T14:59:26.980805: step 7172, loss 5.16283.
Train: 2018-08-02T14:59:27.043292: step 7173, loss 3.9036.
Train: 2018-08-02T14:59:27.090157: step 7174, loss 3.77768.
Train: 2018-08-02T14:59:27.152672: step 7175, loss 4.02952.
Train: 2018-08-02T14:59:27.215152: step 7176, loss 4.15545.
Train: 2018-08-02T14:59:27.262021: step 7177, loss 3.77768.
Train: 2018-08-02T14:59:27.324503: step 7178, loss 4.91098.
Train: 2018-08-02T14:59:27.386987: step 7179, loss 4.40729.
Train: 2018-08-02T14:59:27.433855: step 7180, loss 3.14807.
Test: 2018-08-02T14:59:27.636903: step 7180, loss 3.81863.
Train: 2018-08-02T14:59:27.699388: step 7181, loss 3.39991.
Train: 2018-08-02T14:59:27.761903: step 7182, loss 3.9036.
Train: 2018-08-02T14:59:27.824358: step 7183, loss 3.52583.
Train: 2018-08-02T14:59:27.871256: step 7184, loss 3.52583.
Train: 2018-08-02T14:59:27.933709: step 7185, loss 5.28875.
Train: 2018-08-02T14:59:27.996225: step 7186, loss 4.65914.
Train: 2018-08-02T14:59:28.043087: step 7187, loss 3.77768.
Train: 2018-08-02T14:59:28.105557: step 7188, loss 3.39991.
Train: 2018-08-02T14:59:28.168058: step 7189, loss 3.77768.
Train: 2018-08-02T14:59:28.214925: step 7190, loss 4.15545.
Test: 2018-08-02T14:59:28.417994: step 7190, loss 3.81863.
Train: 2018-08-02T14:59:28.480485: step 7191, loss 4.40729.
Train: 2018-08-02T14:59:28.542971: step 7192, loss 3.77768.
Train: 2018-08-02T14:59:28.589803: step 7193, loss 3.02214.
Train: 2018-08-02T14:59:28.652321: step 7194, loss 3.65176.
Train: 2018-08-02T14:59:28.714809: step 7195, loss 4.65914.
Train: 2018-08-02T14:59:28.777292: step 7196, loss 2.89622.
Train: 2018-08-02T14:59:28.824155: step 7197, loss 3.02214.
Train: 2018-08-02T14:59:28.886641: step 7198, loss 4.40729.
Train: 2018-08-02T14:59:28.949097: step 7199, loss 3.77768.
Train: 2018-08-02T14:59:28.995992: step 7200, loss 4.02952.
Test: 2018-08-02T14:59:29.199035: step 7200, loss 3.81863.
Train: 2018-08-02T14:59:29.745816: step 7201, loss 3.77768.
Train: 2018-08-02T14:59:29.823891: step 7202, loss 3.02214.
Train: 2018-08-02T14:59:29.886406: step 7203, loss 5.16283.
Train: 2018-08-02T14:59:29.948891: step 7204, loss 4.53321.
Train: 2018-08-02T14:59:30.011372: step 7205, loss 3.14807.
Train: 2018-08-02T14:59:30.073832: step 7206, loss 3.14807.
Train: 2018-08-02T14:59:30.120730: step 7207, loss 4.15545.
Train: 2018-08-02T14:59:30.183213: step 7208, loss 3.9036.
Train: 2018-08-02T14:59:30.245695: step 7209, loss 4.15545.
Train: 2018-08-02T14:59:30.308182: step 7210, loss 3.9036.
Test: 2018-08-02T14:59:30.495632: step 7210, loss 3.81863.
Train: 2018-08-02T14:59:30.558094: step 7211, loss 3.77768.
Train: 2018-08-02T14:59:30.620581: step 7212, loss 4.65914.
Train: 2018-08-02T14:59:30.683090: step 7213, loss 4.28137.
Train: 2018-08-02T14:59:30.729938: step 7214, loss 3.39991.
Train: 2018-08-02T14:59:30.792445: step 7215, loss 3.02214.
Train: 2018-08-02T14:59:30.854901: step 7216, loss 4.02952.
Train: 2018-08-02T14:59:30.917386: step 7217, loss 3.77768.
Train: 2018-08-02T14:59:30.979872: step 7218, loss 3.77768.
Train: 2018-08-02T14:59:31.026765: step 7219, loss 4.02952.
Train: 2018-08-02T14:59:31.089252: step 7220, loss 5.0369.
Test: 2018-08-02T14:59:31.292321: step 7220, loss 3.81863.
Train: 2018-08-02T14:59:31.370436: step 7221, loss 3.27399.
Train: 2018-08-02T14:59:31.432918: step 7222, loss 3.77768.
Train: 2018-08-02T14:59:31.495405: step 7223, loss 3.52583.
Train: 2018-08-02T14:59:31.542240: step 7224, loss 3.27399.
Train: 2018-08-02T14:59:31.604727: step 7225, loss 4.15545.
Train: 2018-08-02T14:59:31.667240: step 7226, loss 2.89622.
Train: 2018-08-02T14:59:31.714106: step 7227, loss 4.28137.
Train: 2018-08-02T14:59:31.776589: step 7228, loss 4.40729.
Train: 2018-08-02T14:59:31.839056: step 7229, loss 4.02952.
Train: 2018-08-02T14:59:31.885938: step 7230, loss 3.39991.
Test: 2018-08-02T14:59:32.088984: step 7230, loss 3.81863.
Train: 2018-08-02T14:59:32.151500: step 7231, loss 4.40729.
Train: 2018-08-02T14:59:32.213985: step 7232, loss 4.28137.
Train: 2018-08-02T14:59:32.260850: step 7233, loss 4.65914.
Train: 2018-08-02T14:59:32.323337: step 7234, loss 4.28137.
Train: 2018-08-02T14:59:32.385793: step 7235, loss 4.91098.
Train: 2018-08-02T14:59:32.432685: step 7236, loss 3.52583.
Train: 2018-08-02T14:59:32.495172: step 7237, loss 4.53321.
Train: 2018-08-02T14:59:32.557657: step 7238, loss 3.65176.
Train: 2018-08-02T14:59:32.620111: step 7239, loss 3.52583.
Train: 2018-08-02T14:59:32.666974: step 7240, loss 4.53321.
Test: 2018-08-02T14:59:32.870090: step 7240, loss 3.81863.
Train: 2018-08-02T14:59:32.932563: step 7241, loss 3.14807.
Train: 2018-08-02T14:59:32.995055: step 7242, loss 4.40729.
Train: 2018-08-02T14:59:33.057533: step 7243, loss 3.9036.
Train: 2018-08-02T14:59:33.104405: step 7244, loss 4.28137.
Train: 2018-08-02T14:59:33.166884: step 7245, loss 2.89622.
Train: 2018-08-02T14:59:33.229373: step 7246, loss 3.77768.
Train: 2018-08-02T14:59:33.276232: step 7247, loss 4.02952.
Train: 2018-08-02T14:59:33.323101: step 7248, loss 6.1786.
Train: 2018-08-02T14:59:33.385557: step 7249, loss 3.39991.
Train: 2018-08-02T14:59:33.448066: step 7250, loss 3.14807.
Test: 2018-08-02T14:59:33.635525: step 7250, loss 3.81863.
Train: 2018-08-02T14:59:33.698008: step 7251, loss 4.40729.
Train: 2018-08-02T14:59:33.760470: step 7252, loss 4.78506.
Train: 2018-08-02T14:59:33.822983: step 7253, loss 4.40729.
Train: 2018-08-02T14:59:33.885465: step 7254, loss 3.39991.
Train: 2018-08-02T14:59:33.932334: step 7255, loss 3.77768.
Train: 2018-08-02T14:59:33.994821: step 7256, loss 3.14807.
Train: 2018-08-02T14:59:34.057304: step 7257, loss 2.64438.
Train: 2018-08-02T14:59:34.104138: step 7258, loss 4.02952.
Train: 2018-08-02T14:59:34.166654: step 7259, loss 3.52583.
Train: 2018-08-02T14:59:34.229110: step 7260, loss 3.65176.
Test: 2018-08-02T14:59:34.432217: step 7260, loss 3.81863.
Train: 2018-08-02T14:59:34.494704: step 7261, loss 4.91098.
Train: 2018-08-02T14:59:34.541566: step 7262, loss 4.02952.
Train: 2018-08-02T14:59:34.604049: step 7263, loss 4.53321.
Train: 2018-08-02T14:59:34.666537: step 7264, loss 2.7703.
Train: 2018-08-02T14:59:34.729022: step 7265, loss 3.27399.
Train: 2018-08-02T14:59:34.791509: step 7266, loss 4.40729.
Train: 2018-08-02T14:59:34.838341: step 7267, loss 4.78506.
Train: 2018-08-02T14:59:34.900859: step 7268, loss 3.39991.
Train: 2018-08-02T14:59:34.963341: step 7269, loss 3.14807.
Train: 2018-08-02T14:59:35.010206: step 7270, loss 4.40729.
Test: 2018-08-02T14:59:35.213277: step 7270, loss 3.81863.
Train: 2018-08-02T14:59:35.275769: step 7271, loss 4.53321.
Train: 2018-08-02T14:59:35.338224: step 7272, loss 4.02952.
Train: 2018-08-02T14:59:35.400709: step 7273, loss 4.28137.
Train: 2018-08-02T14:59:35.447573: step 7274, loss 4.28137.
Train: 2018-08-02T14:59:35.510059: step 7275, loss 3.39991.
Train: 2018-08-02T14:59:35.572574: step 7276, loss 5.79244.
Train: 2018-08-02T14:59:35.635055: step 7277, loss 4.28137.
Train: 2018-08-02T14:59:35.681925: step 7278, loss 3.39991.
Train: 2018-08-02T14:59:35.744380: step 7279, loss 3.9036.
Train: 2018-08-02T14:59:35.806889: step 7280, loss 3.9036.
Test: 2018-08-02T14:59:36.009973: step 7280, loss 3.81863.
Train: 2018-08-02T14:59:36.072457: step 7281, loss 4.02952.
Train: 2018-08-02T14:59:36.119291: step 7282, loss 3.65176.
Train: 2018-08-02T14:59:36.181809: step 7283, loss 4.15545.
Train: 2018-08-02T14:59:36.244290: step 7284, loss 3.9036.
Train: 2018-08-02T14:59:36.306774: step 7285, loss 4.28137.
Train: 2018-08-02T14:59:36.353639: step 7286, loss 3.65176.
Train: 2018-08-02T14:59:36.416122: step 7287, loss 3.52583.
Train: 2018-08-02T14:59:36.478620: step 7288, loss 4.65914.
Train: 2018-08-02T14:59:36.525446: step 7289, loss 4.02952.
Train: 2018-08-02T14:59:36.587962: step 7290, loss 4.28137.
Test: 2018-08-02T14:59:36.791033: step 7290, loss 3.81863.
Train: 2018-08-02T14:59:36.853524: step 7291, loss 4.28755.
Train: 2018-08-02T14:59:36.916010: step 7292, loss 4.65914.
Train: 2018-08-02T14:59:36.962876: step 7293, loss 3.65176.
Train: 2018-08-02T14:59:37.025360: step 7294, loss 3.27399.
Train: 2018-08-02T14:59:37.087815: step 7295, loss 4.02952.
Train: 2018-08-02T14:59:37.150300: step 7296, loss 3.02214.
Train: 2018-08-02T14:59:37.197194: step 7297, loss 4.91098.
Train: 2018-08-02T14:59:37.259680: step 7298, loss 3.52583.
Train: 2018-08-02T14:59:37.322166: step 7299, loss 3.77768.
Train: 2018-08-02T14:59:37.368999: step 7300, loss 4.15545.
Test: 2018-08-02T14:59:37.572075: step 7300, loss 3.81863.
Train: 2018-08-02T14:59:38.103229: step 7301, loss 4.15545.
Train: 2018-08-02T14:59:38.165686: step 7302, loss 3.9036.
Train: 2018-08-02T14:59:38.228202: step 7303, loss 3.65176.
Train: 2018-08-02T14:59:38.275068: step 7304, loss 4.15545.
Train: 2018-08-02T14:59:38.337552: step 7305, loss 4.02952.
Train: 2018-08-02T14:59:38.400037: step 7306, loss 4.15545.
Train: 2018-08-02T14:59:38.462493: step 7307, loss 3.39991.
Train: 2018-08-02T14:59:38.509388: step 7308, loss 4.28137.
Train: 2018-08-02T14:59:38.571843: step 7309, loss 3.65176.
Train: 2018-08-02T14:59:38.634358: step 7310, loss 3.65176.
Test: 2018-08-02T14:59:38.821807: step 7310, loss 3.81863.
Train: 2018-08-02T14:59:38.884299: step 7311, loss 4.78506.
Train: 2018-08-02T14:59:38.946784: step 7312, loss 4.02952.
Train: 2018-08-02T14:59:39.009240: step 7313, loss 3.27399.
Train: 2018-08-02T14:59:39.056135: step 7314, loss 3.65176.
Train: 2018-08-02T14:59:39.118621: step 7315, loss 3.77768.
Train: 2018-08-02T14:59:39.181104: step 7316, loss 4.65914.
Train: 2018-08-02T14:59:39.227938: step 7317, loss 3.9036.
Train: 2018-08-02T14:59:39.290426: step 7318, loss 4.15545.
Train: 2018-08-02T14:59:39.352937: step 7319, loss 4.02952.
Train: 2018-08-02T14:59:39.399774: step 7320, loss 3.77768.
Test: 2018-08-02T14:59:39.602880: step 7320, loss 3.81863.
Train: 2018-08-02T14:59:39.665337: step 7321, loss 4.02952.
Train: 2018-08-02T14:59:39.727822: step 7322, loss 3.39991.
Train: 2018-08-02T14:59:39.790338: step 7323, loss 3.27399.
Train: 2018-08-02T14:59:39.852821: step 7324, loss 4.15545.
Train: 2018-08-02T14:59:39.899682: step 7325, loss 4.15545.
Train: 2018-08-02T14:59:39.962171: step 7326, loss 3.9036.
Train: 2018-08-02T14:59:40.024656: step 7327, loss 3.77768.
Train: 2018-08-02T14:59:40.071521: step 7328, loss 4.02952.
Train: 2018-08-02T14:59:40.134002: step 7329, loss 3.77768.
Train: 2018-08-02T14:59:40.196491: step 7330, loss 4.15545.
Test: 2018-08-02T14:59:40.383942: step 7330, loss 3.81863.
Train: 2018-08-02T14:59:40.446434: step 7331, loss 4.15545.
Train: 2018-08-02T14:59:40.508919: step 7332, loss 3.14807.
Train: 2018-08-02T14:59:40.571374: step 7333, loss 4.65914.
Train: 2018-08-02T14:59:40.618239: step 7334, loss 4.78506.
Train: 2018-08-02T14:59:40.680724: step 7335, loss 4.40729.
Train: 2018-08-02T14:59:40.743240: step 7336, loss 3.65176.
Train: 2018-08-02T14:59:40.790072: step 7337, loss 3.9036.
Train: 2018-08-02T14:59:40.852558: step 7338, loss 4.15545.
Train: 2018-08-02T14:59:40.915044: step 7339, loss 4.65914.
Train: 2018-08-02T14:59:40.977558: step 7340, loss 4.53321.
Test: 2018-08-02T14:59:41.164984: step 7340, loss 3.81863.
Train: 2018-08-02T14:59:41.227507: step 7341, loss 4.02952.
Train: 2018-08-02T14:59:41.289956: step 7342, loss 4.53321.
Train: 2018-08-02T14:59:41.352471: step 7343, loss 4.15545.
Train: 2018-08-02T14:59:41.414926: step 7344, loss 4.40729.
Train: 2018-08-02T14:59:41.461791: step 7345, loss 3.77768.
Train: 2018-08-02T14:59:41.524300: step 7346, loss 3.52583.
Train: 2018-08-02T14:59:41.586760: step 7347, loss 3.39991.
Train: 2018-08-02T14:59:41.633656: step 7348, loss 3.65176.
Train: 2018-08-02T14:59:41.696110: step 7349, loss 2.64438.
Train: 2018-08-02T14:59:41.758626: step 7350, loss 4.28137.
Test: 2018-08-02T14:59:41.961687: step 7350, loss 3.81863.
Train: 2018-08-02T14:59:42.008538: step 7351, loss 4.65914.
Train: 2018-08-02T14:59:42.071022: step 7352, loss 4.65914.
Train: 2018-08-02T14:59:42.133538: step 7353, loss 3.52583.
Train: 2018-08-02T14:59:42.196045: step 7354, loss 4.15545.
Train: 2018-08-02T14:59:42.258480: step 7355, loss 4.02952.
Train: 2018-08-02T14:59:42.305373: step 7356, loss 4.28137.
Train: 2018-08-02T14:59:42.367829: step 7357, loss 4.65914.
Train: 2018-08-02T14:59:42.430315: step 7358, loss 3.52583.
Train: 2018-08-02T14:59:42.477177: step 7359, loss 4.02952.
Train: 2018-08-02T14:59:42.539662: step 7360, loss 4.02952.
Test: 2018-08-02T14:59:42.742739: step 7360, loss 3.81863.
Train: 2018-08-02T14:59:42.805256: step 7361, loss 4.53321.
Train: 2018-08-02T14:59:42.867723: step 7362, loss 3.9036.
Train: 2018-08-02T14:59:42.914608: step 7363, loss 2.7703.
Train: 2018-08-02T14:59:42.977089: step 7364, loss 4.53321.
Train: 2018-08-02T14:59:43.039545: step 7365, loss 3.9036.
Train: 2018-08-02T14:59:43.102030: step 7366, loss 5.16283.
Train: 2018-08-02T14:59:43.148921: step 7367, loss 3.77768.
Train: 2018-08-02T14:59:43.211414: step 7368, loss 4.02952.
Train: 2018-08-02T14:59:43.273868: step 7369, loss 4.28137.
Train: 2018-08-02T14:59:43.336381: step 7370, loss 4.65914.
Test: 2018-08-02T14:59:43.523806: step 7370, loss 3.81863.
Train: 2018-08-02T14:59:43.586325: step 7371, loss 4.91098.
Train: 2018-08-02T14:59:43.648778: step 7372, loss 4.28137.
Train: 2018-08-02T14:59:43.695674: step 7373, loss 3.9036.
Train: 2018-08-02T14:59:43.758159: step 7374, loss 3.52583.
Train: 2018-08-02T14:59:43.820659: step 7375, loss 4.15545.
Train: 2018-08-02T14:59:43.883098: step 7376, loss 3.65176.
Train: 2018-08-02T14:59:43.929993: step 7377, loss 3.39991.
Train: 2018-08-02T14:59:43.992447: step 7378, loss 4.15545.
Train: 2018-08-02T14:59:44.054964: step 7379, loss 3.9036.
Train: 2018-08-02T14:59:44.101796: step 7380, loss 3.77768.
Test: 2018-08-02T14:59:44.304898: step 7380, loss 3.81863.
Train: 2018-08-02T14:59:44.367392: step 7381, loss 4.91098.
Train: 2018-08-02T14:59:44.429875: step 7382, loss 3.52583.
Train: 2018-08-02T14:59:44.492331: step 7383, loss 3.39991.
Train: 2018-08-02T14:59:44.539226: step 7384, loss 4.53321.
Train: 2018-08-02T14:59:44.601681: step 7385, loss 3.65176.
Train: 2018-08-02T14:59:44.664196: step 7386, loss 5.79244.
Train: 2018-08-02T14:59:44.711029: step 7387, loss 4.91098.
Train: 2018-08-02T14:59:44.773545: step 7388, loss 5.5406.
Train: 2018-08-02T14:59:44.836033: step 7389, loss 4.28137.
Train: 2018-08-02T14:59:44.898515: step 7390, loss 4.78506.
Test: 2018-08-02T14:59:45.085940: step 7390, loss 3.81863.
Train: 2018-08-02T14:59:45.148457: step 7391, loss 2.51845.
Train: 2018-08-02T14:59:45.210912: step 7392, loss 4.28137.
Train: 2018-08-02T14:59:45.273422: step 7393, loss 3.9036.
Train: 2018-08-02T14:59:45.320261: step 7394, loss 3.52583.
Train: 2018-08-02T14:59:45.382779: step 7395, loss 4.28137.
Train: 2018-08-02T14:59:45.445232: step 7396, loss 3.52583.
Train: 2018-08-02T14:59:45.492129: step 7397, loss 4.15545.
Train: 2018-08-02T14:59:45.554612: step 7398, loss 4.28137.
Train: 2018-08-02T14:59:45.601445: step 7399, loss 4.02952.
Train: 2018-08-02T14:59:45.663961: step 7400, loss 4.15545.
Test: 2018-08-02T14:59:45.851412: step 7400, loss 3.81863.
Train: 2018-08-02T14:59:46.382513: step 7401, loss 3.65176.
Train: 2018-08-02T14:59:46.429407: step 7402, loss 3.02214.
Train: 2018-08-02T14:59:46.491863: step 7403, loss 3.39991.
Train: 2018-08-02T14:59:46.554380: step 7404, loss 4.65914.
Train: 2018-08-02T14:59:46.616866: step 7405, loss 3.39991.
Train: 2018-08-02T14:59:46.663727: step 7406, loss 4.28137.
Train: 2018-08-02T14:59:46.726211: step 7407, loss 3.14807.
Train: 2018-08-02T14:59:46.788669: step 7408, loss 3.77768.
Train: 2018-08-02T14:59:46.851154: step 7409, loss 4.28137.
Train: 2018-08-02T14:59:46.898042: step 7410, loss 3.39991.
Test: 2018-08-02T14:59:47.101119: step 7410, loss 3.81863.
Train: 2018-08-02T14:59:47.163580: step 7411, loss 4.28137.
Train: 2018-08-02T14:59:47.226064: step 7412, loss 3.77768.
Train: 2018-08-02T14:59:47.272960: step 7413, loss 2.64438.
Train: 2018-08-02T14:59:47.335448: step 7414, loss 3.77768.
Train: 2018-08-02T14:59:47.397931: step 7415, loss 4.53321.
Train: 2018-08-02T14:59:47.460416: step 7416, loss 4.15545.
Train: 2018-08-02T14:59:47.507280: step 7417, loss 3.39991.
Train: 2018-08-02T14:59:47.569736: step 7418, loss 3.14807.
Train: 2018-08-02T14:59:47.632251: step 7419, loss 3.39991.
Train: 2018-08-02T14:59:47.679114: step 7420, loss 4.28137.
Test: 2018-08-02T14:59:47.882193: step 7420, loss 3.81863.
Train: 2018-08-02T14:59:47.944648: step 7421, loss 3.77768.
Train: 2018-08-02T14:59:48.007162: step 7422, loss 4.15545.
Train: 2018-08-02T14:59:48.054027: step 7423, loss 4.15545.
Train: 2018-08-02T14:59:48.116516: step 7424, loss 3.9036.
Train: 2018-08-02T14:59:48.178997: step 7425, loss 3.77768.
Train: 2018-08-02T14:59:48.225830: step 7426, loss 3.27399.
Train: 2018-08-02T14:59:48.288316: step 7427, loss 3.27399.
Train: 2018-08-02T14:59:48.350830: step 7428, loss 4.15545.
Train: 2018-08-02T14:59:48.397667: step 7429, loss 3.27399.
Train: 2018-08-02T14:59:48.460151: step 7430, loss 4.53321.
Test: 2018-08-02T14:59:48.663253: step 7430, loss 3.81863.
Train: 2018-08-02T14:59:48.725741: step 7431, loss 3.52583.
Train: 2018-08-02T14:59:48.788199: step 7432, loss 3.65176.
Train: 2018-08-02T14:59:48.850685: step 7433, loss 3.27399.
Train: 2018-08-02T14:59:48.897579: step 7434, loss 5.66652.
Train: 2018-08-02T14:59:48.960069: step 7435, loss 4.28137.
Train: 2018-08-02T14:59:49.022521: step 7436, loss 4.65914.
Train: 2018-08-02T14:59:49.069385: step 7437, loss 3.77768.
Train: 2018-08-02T14:59:49.131870: step 7438, loss 3.65176.
Train: 2018-08-02T14:59:49.194357: step 7439, loss 4.40729.
Train: 2018-08-02T14:59:49.256870: step 7440, loss 4.40729.
Test: 2018-08-02T14:59:49.444295: step 7440, loss 3.81863.
Train: 2018-08-02T14:59:49.506818: step 7441, loss 2.89622.
Train: 2018-08-02T14:59:49.569266: step 7442, loss 4.53321.
Train: 2018-08-02T14:59:49.631782: step 7443, loss 3.9036.
Train: 2018-08-02T14:59:49.694255: step 7444, loss 4.40729.
Train: 2018-08-02T14:59:49.741133: step 7445, loss 4.28137.
Train: 2018-08-02T14:59:49.803612: step 7446, loss 4.53321.
Train: 2018-08-02T14:59:49.866072: step 7447, loss 4.02952.
Train: 2018-08-02T14:59:49.928560: step 7448, loss 3.65176.
Train: 2018-08-02T14:59:49.975449: step 7449, loss 3.52583.
Train: 2018-08-02T14:59:50.037937: step 7450, loss 3.39991.
Test: 2018-08-02T14:59:50.241008: step 7450, loss 3.81863.
Train: 2018-08-02T14:59:50.303470: step 7451, loss 4.78506.
Train: 2018-08-02T14:59:50.350334: step 7452, loss 4.15545.
Train: 2018-08-02T14:59:50.412849: step 7453, loss 3.77768.
Train: 2018-08-02T14:59:50.475333: step 7454, loss 4.78506.
Train: 2018-08-02T14:59:50.537821: step 7455, loss 4.53321.
Train: 2018-08-02T14:59:50.584686: step 7456, loss 4.40729.
Train: 2018-08-02T14:59:50.647169: step 7457, loss 3.65176.
Train: 2018-08-02T14:59:50.709658: step 7458, loss 4.02952.
Train: 2018-08-02T14:59:50.756518: step 7459, loss 4.28137.
Train: 2018-08-02T14:59:50.818975: step 7460, loss 3.52583.
Test: 2018-08-02T14:59:51.022081: step 7460, loss 3.81863.
Train: 2018-08-02T14:59:51.084536: step 7461, loss 3.9036.
Train: 2018-08-02T14:59:51.131432: step 7462, loss 4.53321.
Train: 2018-08-02T14:59:51.193887: step 7463, loss 5.28875.
Train: 2018-08-02T14:59:51.256403: step 7464, loss 4.02952.
Train: 2018-08-02T14:59:51.318859: step 7465, loss 3.27399.
Train: 2018-08-02T14:59:51.365753: step 7466, loss 3.77768.
Train: 2018-08-02T14:59:51.428236: step 7467, loss 4.65914.
Train: 2018-08-02T14:59:51.490725: step 7468, loss 3.9036.
Train: 2018-08-02T14:59:51.553178: step 7469, loss 3.65176.
Train: 2018-08-02T14:59:51.600069: step 7470, loss 3.65176.
Test: 2018-08-02T14:59:51.803119: step 7470, loss 3.81863.
Train: 2018-08-02T14:59:51.865604: step 7471, loss 3.52583.
Train: 2018-08-02T14:59:51.928089: step 7472, loss 3.77768.
Train: 2018-08-02T14:59:51.974986: step 7473, loss 3.39991.
Train: 2018-08-02T14:59:52.037476: step 7474, loss 3.77768.
Train: 2018-08-02T14:59:52.099925: step 7475, loss 4.40729.
Train: 2018-08-02T14:59:52.162434: step 7476, loss 3.52583.
Train: 2018-08-02T14:59:52.209274: step 7477, loss 4.40729.
Train: 2018-08-02T14:59:52.271784: step 7478, loss 4.02952.
Train: 2018-08-02T14:59:52.334271: step 7479, loss 3.14807.
Train: 2018-08-02T14:59:52.396753: step 7480, loss 4.02952.
Test: 2018-08-02T14:59:52.584186: step 7480, loss 3.81863.
Train: 2018-08-02T14:59:52.646672: step 7481, loss 4.65914.
Train: 2018-08-02T14:59:52.709157: step 7482, loss 3.39991.
Train: 2018-08-02T14:59:52.771672: step 7483, loss 4.65914.
Train: 2018-08-02T14:59:52.818518: step 7484, loss 3.65176.
Train: 2018-08-02T14:59:52.881016: step 7485, loss 4.28137.
Train: 2018-08-02T14:59:52.943477: step 7486, loss 4.02952.
Train: 2018-08-02T14:59:52.990373: step 7487, loss 3.27399.
Train: 2018-08-02T14:59:53.052864: step 7488, loss 3.65176.
Train: 2018-08-02T14:59:53.115312: step 7489, loss 4.40729.
Train: 2018-08-02T14:59:53.177826: step 7490, loss 4.65914.
Test: 2018-08-02T14:59:53.365277: step 7490, loss 3.81863.
Train: 2018-08-02T14:59:53.427768: step 7491, loss 4.53321.
Train: 2018-08-02T14:59:53.490256: step 7492, loss 4.53321.
Train: 2018-08-02T14:59:53.552709: step 7493, loss 3.14807.
Train: 2018-08-02T14:59:53.599602: step 7494, loss 3.65176.
Train: 2018-08-02T14:59:53.662089: step 7495, loss 4.28137.
Train: 2018-08-02T14:59:53.724543: step 7496, loss 3.39991.
Train: 2018-08-02T14:59:53.771439: step 7497, loss 5.16283.
Train: 2018-08-02T14:59:53.833926: step 7498, loss 3.39991.
Train: 2018-08-02T14:59:53.896379: step 7499, loss 3.52583.
Train: 2018-08-02T14:59:53.943272: step 7500, loss 4.02952.
Test: 2018-08-02T14:59:54.146319: step 7500, loss 3.81863.
Train: 2018-08-02T14:59:54.677446: step 7501, loss 3.9036.
Train: 2018-08-02T14:59:54.724310: step 7502, loss 3.65176.
Train: 2018-08-02T14:59:54.786826: step 7503, loss 5.28875.
Train: 2018-08-02T14:59:54.849308: step 7504, loss 4.91098.
Train: 2018-08-02T14:59:54.911795: step 7505, loss 2.89622.
Train: 2018-08-02T14:59:54.974280: step 7506, loss 4.65914.
Train: 2018-08-02T14:59:55.021114: step 7507, loss 3.9036.
Train: 2018-08-02T14:59:55.083602: step 7508, loss 4.15545.
Train: 2018-08-02T14:59:55.146117: step 7509, loss 4.15545.
Train: 2018-08-02T14:59:55.192949: step 7510, loss 4.02952.
Test: 2018-08-02T14:59:55.396057: step 7510, loss 3.81863.
Train: 2018-08-02T14:59:55.458513: step 7511, loss 4.78506.
Train: 2018-08-02T14:59:55.520999: step 7512, loss 4.91098.
Train: 2018-08-02T14:59:55.567893: step 7513, loss 4.15545.
Train: 2018-08-02T14:59:55.630379: step 7514, loss 3.14807.
Train: 2018-08-02T14:59:55.692864: step 7515, loss 4.40729.
Train: 2018-08-02T14:59:55.755348: step 7516, loss 5.16283.
Train: 2018-08-02T14:59:55.802212: step 7517, loss 4.15545.
Train: 2018-08-02T14:59:55.864668: step 7518, loss 4.02952.
Train: 2018-08-02T14:59:55.927153: step 7519, loss 3.14807.
Train: 2018-08-02T14:59:55.974018: step 7520, loss 4.02952.
Test: 2018-08-02T14:59:56.177093: step 7520, loss 3.81863.
Train: 2018-08-02T14:59:56.239579: step 7521, loss 5.5406.
Train: 2018-08-02T14:59:56.302066: step 7522, loss 4.65914.
Train: 2018-08-02T14:59:56.364551: step 7523, loss 3.52583.
Train: 2018-08-02T14:59:56.427036: step 7524, loss 3.65176.
Train: 2018-08-02T14:59:56.473901: step 7525, loss 3.65176.
Train: 2018-08-02T14:59:56.536387: step 7526, loss 4.02952.
Train: 2018-08-02T14:59:56.598871: step 7527, loss 4.65914.
Train: 2018-08-02T14:59:56.645734: step 7528, loss 3.9036.
Train: 2018-08-02T14:59:56.708250: step 7529, loss 4.40729.
Train: 2018-08-02T14:59:56.770706: step 7530, loss 5.16283.
Test: 2018-08-02T14:59:56.958192: step 7530, loss 3.81863.
Train: 2018-08-02T14:59:57.020647: step 7531, loss 4.65914.
Train: 2018-08-02T14:59:57.083162: step 7532, loss 3.9036.
Train: 2018-08-02T14:59:57.145647: step 7533, loss 3.52583.
Train: 2018-08-02T14:59:57.192512: step 7534, loss 5.41467.
Train: 2018-08-02T14:59:57.254998: step 7535, loss 4.40729.
Train: 2018-08-02T14:59:57.317477: step 7536, loss 3.91074.
Train: 2018-08-02T14:59:57.379962: step 7537, loss 4.78506.
Train: 2018-08-02T14:59:57.426804: step 7538, loss 4.65914.
Train: 2018-08-02T14:59:57.489288: step 7539, loss 3.65176.
Train: 2018-08-02T14:59:57.551804: step 7540, loss 3.9036.
Test: 2018-08-02T14:59:57.739252: step 7540, loss 3.81863.
Train: 2018-08-02T14:59:57.801722: step 7541, loss 3.14807.
Train: 2018-08-02T14:59:57.864227: step 7542, loss 3.9036.
Train: 2018-08-02T14:59:57.926715: step 7543, loss 4.91098.
Train: 2018-08-02T14:59:57.989202: step 7544, loss 4.15545.
Train: 2018-08-02T14:59:58.036065: step 7545, loss 3.27399.
Train: 2018-08-02T14:59:58.098549: step 7546, loss 4.40729.
Train: 2018-08-02T14:59:58.161034: step 7547, loss 3.39991.
Train: 2018-08-02T14:59:58.207869: step 7548, loss 3.77768.
Train: 2018-08-02T14:59:58.270381: step 7549, loss 3.52583.
Train: 2018-08-02T14:59:58.317248: step 7550, loss 3.22362.
Test: 2018-08-02T14:59:58.520296: step 7550, loss 3.81863.
Train: 2018-08-02T14:59:58.582812: step 7551, loss 3.52583.
Train: 2018-08-02T14:59:58.629645: step 7552, loss 3.77768.
Train: 2018-08-02T14:59:58.692162: step 7553, loss 3.39991.
Train: 2018-08-02T14:59:58.754641: step 7554, loss 3.27399.
Train: 2018-08-02T14:59:58.817132: step 7555, loss 4.65914.
Train: 2018-08-02T14:59:58.879616: step 7556, loss 4.02952.
Train: 2018-08-02T14:59:58.926450: step 7557, loss 3.52583.
Train: 2018-08-02T14:59:58.988964: step 7558, loss 3.77768.
Train: 2018-08-02T14:59:59.051423: step 7559, loss 4.15545.
Train: 2018-08-02T14:59:59.098316: step 7560, loss 4.28137.
Test: 2018-08-02T14:59:59.301362: step 7560, loss 3.81863.
Train: 2018-08-02T14:59:59.363849: step 7561, loss 3.77768.
Train: 2018-08-02T14:59:59.426334: step 7562, loss 3.52583.
Train: 2018-08-02T14:59:59.473225: step 7563, loss 4.40729.
Train: 2018-08-02T14:59:59.535685: step 7564, loss 4.02952.
Train: 2018-08-02T14:59:59.598196: step 7565, loss 4.40729.
Train: 2018-08-02T14:59:59.645062: step 7566, loss 3.52583.
Train: 2018-08-02T14:59:59.707547: step 7567, loss 3.39991.
Train: 2018-08-02T14:59:59.770004: step 7568, loss 4.02952.
Train: 2018-08-02T14:59:59.816897: step 7569, loss 4.15545.
Train: 2018-08-02T14:59:59.879382: step 7570, loss 4.02952.
Test: 2018-08-02T15:00:00.082454: step 7570, loss 3.81863.
Train: 2018-08-02T15:00:00.144944: step 7571, loss 4.28137.
Train: 2018-08-02T15:00:00.191779: step 7572, loss 4.65914.
Train: 2018-08-02T15:00:00.254295: step 7573, loss 4.53321.
Train: 2018-08-02T15:00:00.316752: step 7574, loss 4.28137.
Train: 2018-08-02T15:00:00.379235: step 7575, loss 4.28137.
Train: 2018-08-02T15:00:00.426099: step 7576, loss 4.02952.
Train: 2018-08-02T15:00:00.488616: step 7577, loss 4.28137.
Train: 2018-08-02T15:00:00.551072: step 7578, loss 4.15545.
Train: 2018-08-02T15:00:00.597962: step 7579, loss 3.65176.
Train: 2018-08-02T15:00:00.660445: step 7580, loss 2.7703.
Test: 2018-08-02T15:00:00.863529: step 7580, loss 3.81863.
Train: 2018-08-02T15:00:00.925983: step 7581, loss 3.65176.
Train: 2018-08-02T15:00:00.988469: step 7582, loss 3.77768.
Train: 2018-08-02T15:00:01.035331: step 7583, loss 5.16283.
Train: 2018-08-02T15:00:01.097845: step 7584, loss 3.52583.
Train: 2018-08-02T15:00:01.160333: step 7585, loss 4.28137.
Train: 2018-08-02T15:00:01.222817: step 7586, loss 4.91098.
Train: 2018-08-02T15:00:01.269682: step 7587, loss 3.39991.
Train: 2018-08-02T15:00:01.332170: step 7588, loss 3.77768.
Train: 2018-08-02T15:00:01.394649: step 7589, loss 5.91836.
Train: 2018-08-02T15:00:01.457168: step 7590, loss 4.65914.
Test: 2018-08-02T15:00:01.644564: step 7590, loss 3.81863.
Train: 2018-08-02T15:00:01.707080: step 7591, loss 4.15545.
Train: 2018-08-02T15:00:01.769564: step 7592, loss 3.39991.
Train: 2018-08-02T15:00:01.832050: step 7593, loss 5.16283.
Train: 2018-08-02T15:00:01.878915: step 7594, loss 3.14807.
Train: 2018-08-02T15:00:01.941371: step 7595, loss 4.28137.
Train: 2018-08-02T15:00:02.003884: step 7596, loss 3.27399.
Train: 2018-08-02T15:00:02.050749: step 7597, loss 4.15545.
Train: 2018-08-02T15:00:02.113233: step 7598, loss 4.02952.
Train: 2018-08-02T15:00:02.175690: step 7599, loss 3.39991.
Train: 2018-08-02T15:00:02.222554: step 7600, loss 5.0369.
Test: 2018-08-02T15:00:02.425631: step 7600, loss 3.81863.
Train: 2018-08-02T15:00:02.941135: step 7601, loss 2.51845.
Train: 2018-08-02T15:00:03.003651: step 7602, loss 3.52583.
Train: 2018-08-02T15:00:03.050484: step 7603, loss 4.02952.
Train: 2018-08-02T15:00:03.113004: step 7604, loss 4.65914.
Train: 2018-08-02T15:00:03.175456: step 7605, loss 3.65176.
Train: 2018-08-02T15:00:03.237973: step 7606, loss 4.53321.
Train: 2018-08-02T15:00:03.284805: step 7607, loss 3.52583.
Train: 2018-08-02T15:00:03.347290: step 7608, loss 4.40729.
Train: 2018-08-02T15:00:03.409776: step 7609, loss 3.39991.
Train: 2018-08-02T15:00:03.472261: step 7610, loss 4.15545.
Test: 2018-08-02T15:00:03.659749: step 7610, loss 3.81863.
Train: 2018-08-02T15:00:03.722202: step 7611, loss 4.02952.
Train: 2018-08-02T15:00:03.784687: step 7612, loss 4.28137.
Train: 2018-08-02T15:00:03.831584: step 7613, loss 3.27399.
Train: 2018-08-02T15:00:03.894070: step 7614, loss 4.28137.
Train: 2018-08-02T15:00:03.956524: step 7615, loss 4.15545.
Train: 2018-08-02T15:00:04.019009: step 7616, loss 3.14807.
Train: 2018-08-02T15:00:04.065871: step 7617, loss 4.78506.
Train: 2018-08-02T15:00:04.128388: step 7618, loss 4.15545.
Train: 2018-08-02T15:00:04.190873: step 7619, loss 4.91098.
Train: 2018-08-02T15:00:04.237739: step 7620, loss 3.52583.
Test: 2018-08-02T15:00:04.440809: step 7620, loss 3.81863.
Train: 2018-08-02T15:00:04.503270: step 7621, loss 3.52583.
Train: 2018-08-02T15:00:04.565755: step 7622, loss 4.53321.
Train: 2018-08-02T15:00:04.628240: step 7623, loss 4.28137.
Train: 2018-08-02T15:00:04.675135: step 7624, loss 3.9036.
Train: 2018-08-02T15:00:04.737590: step 7625, loss 4.40729.
Train: 2018-08-02T15:00:04.800075: step 7626, loss 4.15545.
Train: 2018-08-02T15:00:04.846967: step 7627, loss 3.77768.
Train: 2018-08-02T15:00:04.909459: step 7628, loss 3.65176.
Train: 2018-08-02T15:00:04.971940: step 7629, loss 3.52583.
Train: 2018-08-02T15:00:05.018804: step 7630, loss 4.78506.
Test: 2018-08-02T15:00:05.221850: step 7630, loss 3.81863.
Train: 2018-08-02T15:00:05.284337: step 7631, loss 4.65914.
Train: 2018-08-02T15:00:05.346849: step 7632, loss 4.65914.
Train: 2018-08-02T15:00:05.409337: step 7633, loss 4.78506.
Train: 2018-08-02T15:00:05.471794: step 7634, loss 3.39991.
Train: 2018-08-02T15:00:05.518658: step 7635, loss 4.65914.
Train: 2018-08-02T15:00:05.581174: step 7636, loss 4.40729.
Train: 2018-08-02T15:00:05.643627: step 7637, loss 3.52583.
Train: 2018-08-02T15:00:05.690522: step 7638, loss 3.39991.
Train: 2018-08-02T15:00:05.752976: step 7639, loss 3.77768.
Train: 2018-08-02T15:00:05.815492: step 7640, loss 3.77768.
Test: 2018-08-02T15:00:06.002918: step 7640, loss 3.81863.
Train: 2018-08-02T15:00:06.065435: step 7641, loss 3.65176.
Train: 2018-08-02T15:00:06.127919: step 7642, loss 4.02952.
Train: 2018-08-02T15:00:06.190404: step 7643, loss 4.28137.
Train: 2018-08-02T15:00:06.252886: step 7644, loss 3.77768.
Train: 2018-08-02T15:00:06.299725: step 7645, loss 3.52583.
Train: 2018-08-02T15:00:06.362241: step 7646, loss 3.65176.
Train: 2018-08-02T15:00:06.409074: step 7647, loss 4.91098.
Train: 2018-08-02T15:00:06.471559: step 7648, loss 3.02214.
Train: 2018-08-02T15:00:06.534074: step 7649, loss 4.28137.
Train: 2018-08-02T15:00:06.580907: step 7650, loss 3.77768.
Test: 2018-08-02T15:00:06.784010: step 7650, loss 3.81863.
Train: 2018-08-02T15:00:06.846498: step 7651, loss 4.02952.
Train: 2018-08-02T15:00:06.908985: step 7652, loss 3.27399.
Train: 2018-08-02T15:00:06.955853: step 7653, loss 4.15545.
Train: 2018-08-02T15:00:07.018305: step 7654, loss 4.91098.
Train: 2018-08-02T15:00:07.080824: step 7655, loss 4.28137.
Train: 2018-08-02T15:00:07.143309: step 7656, loss 3.14807.
Train: 2018-08-02T15:00:07.190170: step 7657, loss 4.15545.
Train: 2018-08-02T15:00:07.252657: step 7658, loss 4.91098.
Train: 2018-08-02T15:00:07.315142: step 7659, loss 3.9036.
Train: 2018-08-02T15:00:07.377597: step 7660, loss 3.52583.
Test: 2018-08-02T15:00:07.565082: step 7660, loss 3.81863.
Train: 2018-08-02T15:00:07.627568: step 7661, loss 5.0369.
Train: 2018-08-02T15:00:07.690052: step 7662, loss 3.77768.
Train: 2018-08-02T15:00:07.752509: step 7663, loss 3.77768.
Train: 2018-08-02T15:00:07.799403: step 7664, loss 3.52583.
Train: 2018-08-02T15:00:07.861859: step 7665, loss 4.15545.
Train: 2018-08-02T15:00:07.924374: step 7666, loss 3.65176.
Train: 2018-08-02T15:00:07.971207: step 7667, loss 3.9036.
Train: 2018-08-02T15:00:08.033695: step 7668, loss 4.40729.
Train: 2018-08-02T15:00:08.096180: step 7669, loss 3.65176.
Train: 2018-08-02T15:00:08.143073: step 7670, loss 4.15545.
Test: 2018-08-02T15:00:08.346145: step 7670, loss 3.81863.
Train: 2018-08-02T15:00:08.408605: step 7671, loss 3.65176.
Train: 2018-08-02T15:00:08.471091: step 7672, loss 4.53321.
Train: 2018-08-02T15:00:08.517985: step 7673, loss 3.27399.
Train: 2018-08-02T15:00:08.580473: step 7674, loss 4.53321.
Train: 2018-08-02T15:00:08.642955: step 7675, loss 3.77768.
Train: 2018-08-02T15:00:08.705444: step 7676, loss 4.65914.
Train: 2018-08-02T15:00:08.752274: step 7677, loss 3.27399.
Train: 2018-08-02T15:00:08.814786: step 7678, loss 3.52583.
Train: 2018-08-02T15:00:08.877276: step 7679, loss 4.78506.
Train: 2018-08-02T15:00:08.939760: step 7680, loss 4.28137.
Test: 2018-08-02T15:00:09.127186: step 7680, loss 3.81863.
Train: 2018-08-02T15:00:09.189673: step 7681, loss 3.9036.
Train: 2018-08-02T15:00:09.252158: step 7682, loss 4.40729.
Train: 2018-08-02T15:00:09.314642: step 7683, loss 5.03691.
Train: 2018-08-02T15:00:09.361541: step 7684, loss 4.91098.
Train: 2018-08-02T15:00:09.424023: step 7685, loss 4.15545.
Train: 2018-08-02T15:00:09.486510: step 7686, loss 4.78506.
Train: 2018-08-02T15:00:09.533341: step 7687, loss 3.52583.
Train: 2018-08-02T15:00:09.595855: step 7688, loss 3.9036.
Train: 2018-08-02T15:00:09.658343: step 7689, loss 4.53321.
Train: 2018-08-02T15:00:09.705209: step 7690, loss 3.9036.
Test: 2018-08-02T15:00:09.908253: step 7690, loss 3.81863.
Train: 2018-08-02T15:00:09.970739: step 7691, loss 3.9036.
Train: 2018-08-02T15:00:10.033249: step 7692, loss 3.9036.
Train: 2018-08-02T15:00:10.095737: step 7693, loss 3.9036.
Train: 2018-08-02T15:00:10.158196: step 7694, loss 3.27399.
Train: 2018-08-02T15:00:10.205093: step 7695, loss 3.27399.
Train: 2018-08-02T15:00:10.267574: step 7696, loss 3.27399.
Train: 2018-08-02T15:00:10.330030: step 7697, loss 3.39991.
Train: 2018-08-02T15:00:10.376925: step 7698, loss 3.14807.
Train: 2018-08-02T15:00:10.439409: step 7699, loss 3.9036.
Train: 2018-08-02T15:00:10.501894: step 7700, loss 4.40729.
Test: 2018-08-02T15:00:10.689345: step 7700, loss 3.81863.
Train: 2018-08-02T15:00:11.157991: step 7701, loss 5.64133.
Train: 2018-08-02T15:00:11.220476: step 7702, loss 3.52583.
Train: 2018-08-02T15:00:11.267341: step 7703, loss 4.02952.
Train: 2018-08-02T15:00:11.329796: step 7704, loss 3.02214.
Train: 2018-08-02T15:00:11.392313: step 7705, loss 4.15545.
Train: 2018-08-02T15:00:11.454800: step 7706, loss 4.40729.
Train: 2018-08-02T15:00:11.501630: step 7707, loss 4.28137.
Train: 2018-08-02T15:00:11.564118: step 7708, loss 4.40729.
Train: 2018-08-02T15:00:11.626627: step 7709, loss 5.28875.
Train: 2018-08-02T15:00:11.689116: step 7710, loss 4.40729.
Test: 2018-08-02T15:00:11.876572: step 7710, loss 3.81863.
Train: 2018-08-02T15:00:11.939059: step 7711, loss 3.27399.
Train: 2018-08-02T15:00:12.001543: step 7712, loss 3.65176.
Train: 2018-08-02T15:00:12.064029: step 7713, loss 4.65914.
Train: 2018-08-02T15:00:12.110865: step 7714, loss 4.15545.
Train: 2018-08-02T15:00:12.173350: step 7715, loss 4.15545.
Train: 2018-08-02T15:00:12.235866: step 7716, loss 4.28137.
Train: 2018-08-02T15:00:12.282697: step 7717, loss 3.02214.
Train: 2018-08-02T15:00:12.345185: step 7718, loss 4.53321.
Train: 2018-08-02T15:00:12.407671: step 7719, loss 5.0369.
Train: 2018-08-02T15:00:12.454565: step 7720, loss 4.28137.
Test: 2018-08-02T15:00:12.657610: step 7720, loss 3.81863.
Train: 2018-08-02T15:00:12.720096: step 7721, loss 4.28137.
Train: 2018-08-02T15:00:12.782582: step 7722, loss 4.02952.
Train: 2018-08-02T15:00:12.845096: step 7723, loss 3.65176.
Train: 2018-08-02T15:00:12.891962: step 7724, loss 3.27399.
Train: 2018-08-02T15:00:12.954443: step 7725, loss 4.53321.
Train: 2018-08-02T15:00:13.016900: step 7726, loss 3.39991.
Train: 2018-08-02T15:00:13.063791: step 7727, loss 4.65914.
Train: 2018-08-02T15:00:13.126281: step 7728, loss 3.39991.
Train: 2018-08-02T15:00:13.188737: step 7729, loss 3.65176.
Train: 2018-08-02T15:00:13.235599: step 7730, loss 2.64438.
Test: 2018-08-02T15:00:13.438677: step 7730, loss 3.81863.
Train: 2018-08-02T15:00:13.501192: step 7731, loss 4.91098.
Train: 2018-08-02T15:00:13.563678: step 7732, loss 4.02952.
Train: 2018-08-02T15:00:13.626163: step 7733, loss 3.52583.
Train: 2018-08-02T15:00:13.672998: step 7734, loss 3.14807.
Train: 2018-08-02T15:00:13.735516: step 7735, loss 2.89622.
Train: 2018-08-02T15:00:13.797968: step 7736, loss 4.65914.
Train: 2018-08-02T15:00:13.860486: step 7737, loss 4.53321.
Train: 2018-08-02T15:00:13.907318: step 7738, loss 3.65176.
Train: 2018-08-02T15:00:13.969832: step 7739, loss 4.40729.
Train: 2018-08-02T15:00:14.032288: step 7740, loss 3.14807.
Test: 2018-08-02T15:00:14.235365: step 7740, loss 3.81863.
Train: 2018-08-02T15:00:14.282259: step 7741, loss 4.28137.
Train: 2018-08-02T15:00:14.344743: step 7742, loss 4.28137.
Train: 2018-08-02T15:00:14.407230: step 7743, loss 3.77768.
Train: 2018-08-02T15:00:14.469688: step 7744, loss 4.53321.
Train: 2018-08-02T15:00:14.516575: step 7745, loss 4.65914.
Train: 2018-08-02T15:00:14.579065: step 7746, loss 3.02214.
Train: 2018-08-02T15:00:14.625900: step 7747, loss 3.65176.
Train: 2018-08-02T15:00:14.688385: step 7748, loss 3.77768.
Train: 2018-08-02T15:00:14.750871: step 7749, loss 4.78506.
Train: 2018-08-02T15:00:14.813354: step 7750, loss 3.65176.
Test: 2018-08-02T15:00:15.000835: step 7750, loss 3.81863.
Train: 2018-08-02T15:00:15.063327: step 7751, loss 3.27399.
Train: 2018-08-02T15:00:15.125812: step 7752, loss 4.02952.
Train: 2018-08-02T15:00:15.188268: step 7753, loss 4.53321.
Train: 2018-08-02T15:00:15.250754: step 7754, loss 3.77768.
Train: 2018-08-02T15:00:15.297648: step 7755, loss 4.28137.
Train: 2018-08-02T15:00:15.360135: step 7756, loss 4.02952.
Train: 2018-08-02T15:00:15.422618: step 7757, loss 3.77768.
Train: 2018-08-02T15:00:15.469479: step 7758, loss 4.91098.
Train: 2018-08-02T15:00:15.531967: step 7759, loss 4.15545.
Train: 2018-08-02T15:00:15.594421: step 7760, loss 4.15545.
Test: 2018-08-02T15:00:15.797501: step 7760, loss 3.81863.
Train: 2018-08-02T15:00:15.844364: step 7761, loss 3.39991.
Train: 2018-08-02T15:00:15.906875: step 7762, loss 4.40729.
Train: 2018-08-02T15:00:15.969366: step 7763, loss 3.52583.
Train: 2018-08-02T15:00:16.031850: step 7764, loss 3.39991.
Train: 2018-08-02T15:00:16.078716: step 7765, loss 3.77768.
Train: 2018-08-02T15:00:16.141195: step 7766, loss 3.39991.
Train: 2018-08-02T15:00:16.203654: step 7767, loss 4.28137.
Train: 2018-08-02T15:00:16.250549: step 7768, loss 4.02952.
Train: 2018-08-02T15:00:16.313004: step 7769, loss 4.53321.
Train: 2018-08-02T15:00:16.375489: step 7770, loss 3.27399.
Test: 2018-08-02T15:00:16.562945: step 7770, loss 3.81863.
Train: 2018-08-02T15:00:16.625462: step 7771, loss 4.28137.
Train: 2018-08-02T15:00:16.687946: step 7772, loss 4.02952.
Train: 2018-08-02T15:00:16.750402: step 7773, loss 4.40729.
Train: 2018-08-02T15:00:16.797267: step 7774, loss 4.02952.
Train: 2018-08-02T15:00:16.859752: step 7775, loss 3.52583.
Train: 2018-08-02T15:00:16.922266: step 7776, loss 3.39991.
Train: 2018-08-02T15:00:16.969101: step 7777, loss 5.91836.
Train: 2018-08-02T15:00:17.031588: step 7778, loss 3.65176.
Train: 2018-08-02T15:00:17.094102: step 7779, loss 4.65914.
Train: 2018-08-02T15:00:17.156565: step 7780, loss 3.02214.
Test: 2018-08-02T15:00:17.344036: step 7780, loss 3.81863.
Train: 2018-08-02T15:00:17.406529: step 7781, loss 3.77768.
Train: 2018-08-02T15:00:17.469013: step 7782, loss 3.27399.
Train: 2018-08-02T15:00:17.531499: step 7783, loss 3.39991.
Train: 2018-08-02T15:00:17.578367: step 7784, loss 2.7703.
Train: 2018-08-02T15:00:17.640852: step 7785, loss 4.53321.
Train: 2018-08-02T15:00:17.703303: step 7786, loss 4.53321.
Train: 2018-08-02T15:00:17.750201: step 7787, loss 4.40729.
Train: 2018-08-02T15:00:17.812652: step 7788, loss 3.77768.
Train: 2018-08-02T15:00:17.875171: step 7789, loss 4.53321.
Train: 2018-08-02T15:00:17.922032: step 7790, loss 4.40729.
Test: 2018-08-02T15:00:18.125078: step 7790, loss 3.81863.
Train: 2018-08-02T15:00:18.187590: step 7791, loss 4.15545.
Train: 2018-08-02T15:00:18.250051: step 7792, loss 3.65176.
Train: 2018-08-02T15:00:18.312566: step 7793, loss 5.16283.
Train: 2018-08-02T15:00:18.375022: step 7794, loss 3.9036.
Train: 2018-08-02T15:00:18.437507: step 7795, loss 4.02952.
Train: 2018-08-02T15:00:18.484403: step 7796, loss 4.15545.
Train: 2018-08-02T15:00:18.546889: step 7797, loss 4.65914.
Train: 2018-08-02T15:00:18.609342: step 7798, loss 4.28137.
Train: 2018-08-02T15:00:18.656205: step 7799, loss 4.02952.
Train: 2018-08-02T15:00:18.718721: step 7800, loss 5.0369.
Test: 2018-08-02T15:00:18.921768: step 7800, loss 3.81863.
Train: 2018-08-02T15:00:19.437272: step 7801, loss 3.77768.
Train: 2018-08-02T15:00:19.499788: step 7802, loss 4.15545.
Train: 2018-08-02T15:00:19.562244: step 7803, loss 4.28137.
Train: 2018-08-02T15:00:19.624730: step 7804, loss 3.9036.
Train: 2018-08-02T15:00:19.671623: step 7805, loss 3.9036.
Train: 2018-08-02T15:00:19.734105: step 7806, loss 3.9036.
Train: 2018-08-02T15:00:19.796593: step 7807, loss 5.28875.
Train: 2018-08-02T15:00:19.843455: step 7808, loss 3.52583.
Train: 2018-08-02T15:00:19.905946: step 7809, loss 4.15545.
Train: 2018-08-02T15:00:19.968398: step 7810, loss 4.91098.
Test: 2018-08-02T15:00:20.171477: step 7810, loss 3.81863.
Train: 2018-08-02T15:00:20.233960: step 7811, loss 3.9036.
Train: 2018-08-02T15:00:20.296446: step 7812, loss 3.9036.
Train: 2018-08-02T15:00:20.343338: step 7813, loss 4.02952.
Train: 2018-08-02T15:00:20.405826: step 7814, loss 3.39991.
Train: 2018-08-02T15:00:20.468280: step 7815, loss 4.15545.
Train: 2018-08-02T15:00:20.530791: step 7816, loss 4.40729.
Train: 2018-08-02T15:00:20.577662: step 7817, loss 3.77768.
Train: 2018-08-02T15:00:20.640135: step 7818, loss 4.28137.
Train: 2018-08-02T15:00:20.702627: step 7819, loss 5.16283.
Train: 2018-08-02T15:00:20.749496: step 7820, loss 4.78506.
Test: 2018-08-02T15:00:20.952543: step 7820, loss 3.81863.
Train: 2018-08-02T15:00:21.015028: step 7821, loss 2.64437.
Train: 2018-08-02T15:00:21.077513: step 7822, loss 4.02952.
Train: 2018-08-02T15:00:21.140004: step 7823, loss 3.52583.
Train: 2018-08-02T15:00:21.202515: step 7824, loss 4.40729.
Train: 2018-08-02T15:00:21.249379: step 7825, loss 3.02214.
Train: 2018-08-02T15:00:21.311858: step 7826, loss 4.65914.
Train: 2018-08-02T15:00:21.374347: step 7827, loss 4.02952.
Train: 2018-08-02T15:00:21.421183: step 7828, loss 4.28137.
Train: 2018-08-02T15:00:21.483700: step 7829, loss 4.02952.
Train: 2018-08-02T15:00:21.546179: step 7830, loss 3.65176.
Test: 2018-08-02T15:00:21.733608: step 7830, loss 3.81863.
Train: 2018-08-02T15:00:21.796125: step 7831, loss 3.02214.
Train: 2018-08-02T15:00:21.858580: step 7832, loss 4.02952.
Train: 2018-08-02T15:00:21.921066: step 7833, loss 3.77768.
Train: 2018-08-02T15:00:21.983551: step 7834, loss 3.27399.
Train: 2018-08-02T15:00:22.030446: step 7835, loss 3.65176.
Train: 2018-08-02T15:00:22.092928: step 7836, loss 4.65914.
Train: 2018-08-02T15:00:22.155410: step 7837, loss 4.28137.
Train: 2018-08-02T15:00:22.202274: step 7838, loss 5.0369.
Train: 2018-08-02T15:00:22.264736: step 7839, loss 3.65176.
Train: 2018-08-02T15:00:22.327251: step 7840, loss 4.28137.
Test: 2018-08-02T15:00:22.530298: step 7840, loss 3.81863.
Train: 2018-08-02T15:00:22.577195: step 7841, loss 3.52583.
Train: 2018-08-02T15:00:22.639677: step 7842, loss 3.02214.
Train: 2018-08-02T15:00:22.702162: step 7843, loss 3.65176.
Train: 2018-08-02T15:00:22.764620: step 7844, loss 4.02952.
Train: 2018-08-02T15:00:22.827104: step 7845, loss 4.02952.
Train: 2018-08-02T15:00:22.873998: step 7846, loss 3.77768.
Train: 2018-08-02T15:00:22.936453: step 7847, loss 4.40729.
Train: 2018-08-02T15:00:22.998971: step 7848, loss 4.78506.
Train: 2018-08-02T15:00:23.045833: step 7849, loss 3.39991.
Train: 2018-08-02T15:00:23.108320: step 7850, loss 4.40729.
Test: 2018-08-02T15:00:23.311395: step 7850, loss 3.81863.
Train: 2018-08-02T15:00:23.373875: step 7851, loss 4.15545.
Train: 2018-08-02T15:00:23.420744: step 7852, loss 4.02952.
Train: 2018-08-02T15:00:23.467610: step 7853, loss 3.9036.
Train: 2018-08-02T15:00:23.530094: step 7854, loss 3.65176.
Train: 2018-08-02T15:00:23.592549: step 7855, loss 3.02214.
Train: 2018-08-02T15:00:23.655050: step 7856, loss 3.9036.
Train: 2018-08-02T15:00:23.701898: step 7857, loss 3.27399.
Train: 2018-08-02T15:00:23.764386: step 7858, loss 4.78506.
Train: 2018-08-02T15:00:23.826871: step 7859, loss 3.77768.
Train: 2018-08-02T15:00:23.873785: step 7860, loss 4.28137.
Test: 2018-08-02T15:00:24.076811: step 7860, loss 3.81863.
Train: 2018-08-02T15:00:24.139335: step 7861, loss 3.27399.
Train: 2018-08-02T15:00:24.201811: step 7862, loss 3.77768.
Train: 2018-08-02T15:00:24.248646: step 7863, loss 3.65176.
Train: 2018-08-02T15:00:24.311157: step 7864, loss 3.52583.
Train: 2018-08-02T15:00:24.373617: step 7865, loss 3.77768.
Train: 2018-08-02T15:00:24.420514: step 7866, loss 4.28137.
Train: 2018-08-02T15:00:24.482998: step 7867, loss 3.9036.
Train: 2018-08-02T15:00:24.545453: step 7868, loss 3.52583.
Train: 2018-08-02T15:00:24.607970: step 7869, loss 5.5406.
Train: 2018-08-02T15:00:24.654832: step 7870, loss 4.15545.
Test: 2018-08-02T15:00:24.857902: step 7870, loss 3.81863.
Train: 2018-08-02T15:00:24.920363: step 7871, loss 5.0369.
Train: 2018-08-02T15:00:24.982881: step 7872, loss 4.40729.
Train: 2018-08-02T15:00:25.045334: step 7873, loss 4.15545.
Train: 2018-08-02T15:00:25.092226: step 7874, loss 3.65176.
Train: 2018-08-02T15:00:25.154709: step 7875, loss 3.39991.
Train: 2018-08-02T15:00:25.217169: step 7876, loss 4.65914.
Train: 2018-08-02T15:00:25.264033: step 7877, loss 3.14807.
Train: 2018-08-02T15:00:25.326552: step 7878, loss 4.15545.
Train: 2018-08-02T15:00:25.389004: step 7879, loss 3.77768.
Train: 2018-08-02T15:00:25.435867: step 7880, loss 4.02952.
Test: 2018-08-02T15:00:25.638944: step 7880, loss 3.81863.
Train: 2018-08-02T15:00:25.701463: step 7881, loss 3.77768.
Train: 2018-08-02T15:00:25.763947: step 7882, loss 5.28875.
Train: 2018-08-02T15:00:25.826434: step 7883, loss 3.27399.
Train: 2018-08-02T15:00:25.873269: step 7884, loss 4.65914.
Train: 2018-08-02T15:00:25.935750: step 7885, loss 3.65176.
Train: 2018-08-02T15:00:25.998236: step 7886, loss 3.65176.
Train: 2018-08-02T15:00:26.060751: step 7887, loss 5.41467.
Train: 2018-08-02T15:00:26.107619: step 7888, loss 4.78506.
Train: 2018-08-02T15:00:26.170102: step 7889, loss 5.0369.
Train: 2018-08-02T15:00:26.232588: step 7890, loss 3.65176.
Test: 2018-08-02T15:00:26.420036: step 7890, loss 3.81863.
Train: 2018-08-02T15:00:26.482524: step 7891, loss 4.78506.
Train: 2018-08-02T15:00:26.545013: step 7892, loss 3.39991.
Train: 2018-08-02T15:00:26.607468: step 7893, loss 4.02952.
Train: 2018-08-02T15:00:26.669956: step 7894, loss 3.39991.
Train: 2018-08-02T15:00:26.716846: step 7895, loss 4.65914.
Train: 2018-08-02T15:00:26.779334: step 7896, loss 4.91098.
Train: 2018-08-02T15:00:26.826167: step 7897, loss 3.52583.
Train: 2018-08-02T15:00:26.888682: step 7898, loss 3.27399.
Train: 2018-08-02T15:00:26.951167: step 7899, loss 4.40729.
Train: 2018-08-02T15:00:27.013654: step 7900, loss 4.15545.
Test: 2018-08-02T15:00:27.201103: step 7900, loss 3.81863.
Train: 2018-08-02T15:00:27.763477: step 7901, loss 4.40729.
Train: 2018-08-02T15:00:27.810337: step 7902, loss 5.79244.
Train: 2018-08-02T15:00:27.872822: step 7903, loss 4.02952.
Train: 2018-08-02T15:00:27.935284: step 7904, loss 3.02214.
Train: 2018-08-02T15:00:27.997791: step 7905, loss 3.52583.
Train: 2018-08-02T15:00:28.044633: step 7906, loss 4.02952.
Train: 2018-08-02T15:00:28.107149: step 7907, loss 4.28137.
Train: 2018-08-02T15:00:28.169633: step 7908, loss 4.15545.
Train: 2018-08-02T15:00:28.216494: step 7909, loss 4.40729.
Train: 2018-08-02T15:00:28.278981: step 7910, loss 4.02952.
Test: 2018-08-02T15:00:28.482053: step 7910, loss 3.81863.
Train: 2018-08-02T15:00:28.544513: step 7911, loss 3.02214.
Train: 2018-08-02T15:00:28.591406: step 7912, loss 4.40729.
Train: 2018-08-02T15:00:28.653894: step 7913, loss 3.52583.
Train: 2018-08-02T15:00:28.716378: step 7914, loss 3.77768.
Train: 2018-08-02T15:00:28.763244: step 7915, loss 4.02952.
Train: 2018-08-02T15:00:28.825732: step 7916, loss 4.40729.
Train: 2018-08-02T15:00:28.888183: step 7917, loss 2.64438.
Train: 2018-08-02T15:00:28.935080: step 7918, loss 4.65914.
Train: 2018-08-02T15:00:28.997534: step 7919, loss 3.65176.
Train: 2018-08-02T15:00:29.060044: step 7920, loss 3.65176.
Test: 2018-08-02T15:00:29.247505: step 7920, loss 3.81863.
Train: 2018-08-02T15:00:29.309961: step 7921, loss 4.53321.
Train: 2018-08-02T15:00:29.372445: step 7922, loss 3.77768.
Train: 2018-08-02T15:00:29.434931: step 7923, loss 3.77768.
Train: 2018-08-02T15:00:29.481828: step 7924, loss 4.91098.
Train: 2018-08-02T15:00:29.544312: step 7925, loss 3.27399.
Train: 2018-08-02T15:00:29.606798: step 7926, loss 4.02952.
Train: 2018-08-02T15:00:29.653631: step 7927, loss 4.15545.
Train: 2018-08-02T15:00:29.716117: step 7928, loss 4.40729.
Train: 2018-08-02T15:00:29.778631: step 7929, loss 3.14807.
Train: 2018-08-02T15:00:29.825497: step 7930, loss 4.65914.
Test: 2018-08-02T15:00:30.044165: step 7930, loss 3.81863.
Train: 2018-08-02T15:00:30.091055: step 7931, loss 3.52583.
Train: 2018-08-02T15:00:30.153538: step 7932, loss 3.9036.
Train: 2018-08-02T15:00:30.216025: step 7933, loss 4.15545.
Train: 2018-08-02T15:00:30.278515: step 7934, loss 4.28137.
Train: 2018-08-02T15:00:30.341000: step 7935, loss 5.41467.
Train: 2018-08-02T15:00:30.387833: step 7936, loss 4.53321.
Train: 2018-08-02T15:00:30.450348: step 7937, loss 4.78506.
Train: 2018-08-02T15:00:30.512830: step 7938, loss 3.39991.
Train: 2018-08-02T15:00:30.559694: step 7939, loss 4.44623.
Train: 2018-08-02T15:00:30.622183: step 7940, loss 3.77768.
Test: 2018-08-02T15:00:30.825230: step 7940, loss 3.81863.
Train: 2018-08-02T15:00:30.887746: step 7941, loss 3.9036.
Train: 2018-08-02T15:00:30.934610: step 7942, loss 4.15545.
Train: 2018-08-02T15:00:30.997099: step 7943, loss 3.52583.
Train: 2018-08-02T15:00:31.059582: step 7944, loss 4.02952.
Train: 2018-08-02T15:00:31.122037: step 7945, loss 3.77768.
Train: 2018-08-02T15:00:31.168931: step 7946, loss 4.28137.
Train: 2018-08-02T15:00:31.231417: step 7947, loss 4.02952.
Train: 2018-08-02T15:00:31.293902: step 7948, loss 3.02214.
Train: 2018-08-02T15:00:31.340763: step 7949, loss 3.9036.
Train: 2018-08-02T15:00:31.403245: step 7950, loss 3.27399.
Test: 2018-08-02T15:00:31.606296: step 7950, loss 3.81863.
Train: 2018-08-02T15:00:31.668784: step 7951, loss 5.0369.
Train: 2018-08-02T15:00:31.715647: step 7952, loss 3.77768.
Train: 2018-08-02T15:00:31.778133: step 7953, loss 4.53321.
Train: 2018-08-02T15:00:31.840646: step 7954, loss 3.52583.
Train: 2018-08-02T15:00:31.903129: step 7955, loss 4.02952.
Train: 2018-08-02T15:00:31.949968: step 7956, loss 5.16283.
Train: 2018-08-02T15:00:32.012483: step 7957, loss 2.89622.
Train: 2018-08-02T15:00:32.074971: step 7958, loss 5.28875.
Train: 2018-08-02T15:00:32.137424: step 7959, loss 4.78506.
Train: 2018-08-02T15:00:32.184318: step 7960, loss 4.40729.
Test: 2018-08-02T15:00:32.387389: step 7960, loss 3.81863.
Train: 2018-08-02T15:00:32.449851: step 7961, loss 3.77768.
Train: 2018-08-02T15:00:32.512335: step 7962, loss 3.77768.
Train: 2018-08-02T15:00:32.574851: step 7963, loss 4.15545.
Train: 2018-08-02T15:00:32.621705: step 7964, loss 3.52583.
Train: 2018-08-02T15:00:32.684172: step 7965, loss 3.65176.
Train: 2018-08-02T15:00:32.746680: step 7966, loss 4.91098.
Train: 2018-08-02T15:00:32.809141: step 7967, loss 3.65176.
Train: 2018-08-02T15:00:32.856006: step 7968, loss 2.89622.
Train: 2018-08-02T15:00:32.918521: step 7969, loss 3.27399.
Train: 2018-08-02T15:00:32.980999: step 7970, loss 4.53321.
Test: 2018-08-02T15:00:33.168457: step 7970, loss 3.81863.
Train: 2018-08-02T15:00:33.230917: step 7971, loss 3.65176.
Train: 2018-08-02T15:00:33.293429: step 7972, loss 2.7703.
Train: 2018-08-02T15:00:33.355920: step 7973, loss 4.02952.
Train: 2018-08-02T15:00:33.402751: step 7974, loss 3.27399.
Train: 2018-08-02T15:00:33.465266: step 7975, loss 3.65176.
Train: 2018-08-02T15:00:33.527752: step 7976, loss 4.15545.
Train: 2018-08-02T15:00:33.590238: step 7977, loss 3.39991.
Train: 2018-08-02T15:00:33.637075: step 7978, loss 4.15545.
Train: 2018-08-02T15:00:33.699588: step 7979, loss 4.28137.
Train: 2018-08-02T15:00:33.762067: step 7980, loss 3.65176.
Test: 2018-08-02T15:00:33.949499: step 7980, loss 3.81863.
Train: 2018-08-02T15:00:34.012015: step 7981, loss 4.15545.
Train: 2018-08-02T15:00:34.074470: step 7982, loss 5.5406.
Train: 2018-08-02T15:00:34.136987: step 7983, loss 3.77768.
Train: 2018-08-02T15:00:34.199475: step 7984, loss 3.9036.
Train: 2018-08-02T15:00:34.246305: step 7985, loss 4.02952.
Train: 2018-08-02T15:00:34.308822: step 7986, loss 4.15545.
Train: 2018-08-02T15:00:34.371304: step 7987, loss 4.02952.
Train: 2018-08-02T15:00:34.418140: step 7988, loss 3.9036.
Train: 2018-08-02T15:00:34.480657: step 7989, loss 4.28137.
Train: 2018-08-02T15:00:34.543140: step 7990, loss 3.39991.
Test: 2018-08-02T15:00:34.746189: step 7990, loss 3.81863.
Train: 2018-08-02T15:00:34.793077: step 7991, loss 4.02952.
Train: 2018-08-02T15:00:34.855536: step 7992, loss 4.02952.
Train: 2018-08-02T15:00:34.918052: step 7993, loss 3.65176.
Train: 2018-08-02T15:00:34.980541: step 7994, loss 3.27399.
Train: 2018-08-02T15:00:35.043024: step 7995, loss 5.0369.
Train: 2018-08-02T15:00:35.089884: step 7996, loss 4.02952.
Train: 2018-08-02T15:00:35.152342: step 7997, loss 3.14807.
Train: 2018-08-02T15:00:35.214830: step 7998, loss 4.28137.
Train: 2018-08-02T15:00:35.277344: step 7999, loss 3.77768.
Train: 2018-08-02T15:00:35.324178: step 8000, loss 4.91098.
Test: 2018-08-02T15:00:35.527254: step 8000, loss 3.81863.
Train: 2018-08-02T15:00:36.042790: step 8001, loss 4.02952.
Train: 2018-08-02T15:00:36.105276: step 8002, loss 3.65176.
Train: 2018-08-02T15:00:36.152133: step 8003, loss 3.76089.
Train: 2018-08-02T15:00:36.214594: step 8004, loss 3.65176.
Train: 2018-08-02T15:00:36.277081: step 8005, loss 2.64438.
Train: 2018-08-02T15:00:36.339594: step 8006, loss 3.39991.
Train: 2018-08-02T15:00:36.386453: step 8007, loss 3.27399.
Train: 2018-08-02T15:00:36.448948: step 8008, loss 4.15545.
Train: 2018-08-02T15:00:36.511399: step 8009, loss 4.02952.
Train: 2018-08-02T15:00:36.558262: step 8010, loss 4.40729.
Test: 2018-08-02T15:00:36.761364: step 8010, loss 3.81863.
Train: 2018-08-02T15:00:36.823827: step 8011, loss 4.65914.
Train: 2018-08-02T15:00:36.886341: step 8012, loss 3.39991.
Train: 2018-08-02T15:00:36.948797: step 8013, loss 2.39253.
Train: 2018-08-02T15:00:37.011314: step 8014, loss 3.27399.
Train: 2018-08-02T15:00:37.058172: step 8015, loss 4.02952.
Train: 2018-08-02T15:00:37.120632: step 8016, loss 3.77768.
Train: 2018-08-02T15:00:37.183146: step 8017, loss 3.27399.
Train: 2018-08-02T15:00:37.230013: step 8018, loss 4.28137.
Train: 2018-08-02T15:00:37.292467: step 8019, loss 4.28137.
Train: 2018-08-02T15:00:37.355010: step 8020, loss 3.77768.
Test: 2018-08-02T15:00:37.542431: step 8020, loss 3.81863.
Train: 2018-08-02T15:00:37.604925: step 8021, loss 4.02952.
Train: 2018-08-02T15:00:37.667408: step 8022, loss 3.9036.
Train: 2018-08-02T15:00:37.729895: step 8023, loss 4.91098.
Train: 2018-08-02T15:00:37.792379: step 8024, loss 3.65176.
Train: 2018-08-02T15:00:37.839214: step 8025, loss 3.9036.
Train: 2018-08-02T15:00:37.901729: step 8026, loss 5.41467.
Train: 2018-08-02T15:00:37.964212: step 8027, loss 4.02952.
Train: 2018-08-02T15:00:38.011078: step 8028, loss 4.15545.
Train: 2018-08-02T15:00:38.073558: step 8029, loss 4.28137.
Train: 2018-08-02T15:00:38.136048: step 8030, loss 3.65176.
Test: 2018-08-02T15:00:38.323499: step 8030, loss 3.81863.
Train: 2018-08-02T15:00:38.385991: step 8031, loss 3.77768.
Train: 2018-08-02T15:00:38.448476: step 8032, loss 4.40729.
Train: 2018-08-02T15:00:38.510930: step 8033, loss 4.02952.
Train: 2018-08-02T15:00:38.573442: step 8034, loss 4.91098.
Train: 2018-08-02T15:00:38.620312: step 8035, loss 4.65914.
Train: 2018-08-02T15:00:38.682795: step 8036, loss 6.04429.
Train: 2018-08-02T15:00:38.729662: step 8037, loss 3.77768.
Train: 2018-08-02T15:00:38.792145: step 8038, loss 4.53321.
Train: 2018-08-02T15:00:38.854632: step 8039, loss 4.53321.
Train: 2018-08-02T15:00:38.917086: step 8040, loss 3.27399.
Test: 2018-08-02T15:00:39.104574: step 8040, loss 3.81863.
Train: 2018-08-02T15:00:39.167080: step 8041, loss 4.53321.
Train: 2018-08-02T15:00:39.229544: step 8042, loss 5.0369.
Train: 2018-08-02T15:00:39.292027: step 8043, loss 4.02952.
Train: 2018-08-02T15:00:39.338864: step 8044, loss 4.53321.
Train: 2018-08-02T15:00:39.401348: step 8045, loss 3.27399.
Train: 2018-08-02T15:00:39.463857: step 8046, loss 3.52583.
Train: 2018-08-02T15:00:39.510697: step 8047, loss 4.02952.
Train: 2018-08-02T15:00:39.573184: step 8048, loss 3.27399.
Train: 2018-08-02T15:00:39.635692: step 8049, loss 4.40729.
Train: 2018-08-02T15:00:39.698183: step 8050, loss 4.91098.
Test: 2018-08-02T15:00:39.885638: step 8050, loss 3.81863.
Train: 2018-08-02T15:00:39.948124: step 8051, loss 5.0369.
Train: 2018-08-02T15:00:40.010580: step 8052, loss 4.40729.
Train: 2018-08-02T15:00:40.073095: step 8053, loss 3.27399.
Train: 2018-08-02T15:00:40.119960: step 8054, loss 3.65176.
Train: 2018-08-02T15:00:40.182445: step 8055, loss 4.02952.
Train: 2018-08-02T15:00:40.244930: step 8056, loss 4.02952.
Train: 2018-08-02T15:00:40.307415: step 8057, loss 4.15545.
Train: 2018-08-02T15:00:40.354251: step 8058, loss 3.52583.
Train: 2018-08-02T15:00:40.416766: step 8059, loss 3.9036.
Train: 2018-08-02T15:00:40.479252: step 8060, loss 4.78506.
Test: 2018-08-02T15:00:40.666707: step 8060, loss 3.81863.
Train: 2018-08-02T15:00:40.729194: step 8061, loss 4.40729.
Train: 2018-08-02T15:00:40.791676: step 8062, loss 3.77768.
Train: 2018-08-02T15:00:40.838538: step 8063, loss 3.02214.
Train: 2018-08-02T15:00:40.901027: step 8064, loss 3.52583.
Train: 2018-08-02T15:00:40.963510: step 8065, loss 4.15545.
Train: 2018-08-02T15:00:41.025967: step 8066, loss 3.52583.
Train: 2018-08-02T15:00:41.072863: step 8067, loss 3.52583.
Train: 2018-08-02T15:00:41.135347: step 8068, loss 3.9036.
Train: 2018-08-02T15:00:41.197833: step 8069, loss 4.40729.
Train: 2018-08-02T15:00:41.244696: step 8070, loss 4.65914.
Test: 2018-08-02T15:00:41.447767: step 8070, loss 3.81863.
Train: 2018-08-02T15:00:41.510258: step 8071, loss 4.02952.
Train: 2018-08-02T15:00:41.572714: step 8072, loss 3.65176.
Train: 2018-08-02T15:00:41.619608: step 8073, loss 3.65176.
Train: 2018-08-02T15:00:41.682094: step 8074, loss 3.9036.
Train: 2018-08-02T15:00:41.744580: step 8075, loss 4.40729.
Train: 2018-08-02T15:00:41.807089: step 8076, loss 4.53321.
Train: 2018-08-02T15:00:41.853898: step 8077, loss 3.9036.
Train: 2018-08-02T15:00:41.916385: step 8078, loss 3.14807.
Train: 2018-08-02T15:00:41.978870: step 8079, loss 4.28137.
Train: 2018-08-02T15:00:42.025764: step 8080, loss 3.52583.
Test: 2018-08-02T15:00:42.228810: step 8080, loss 3.81863.
Train: 2018-08-02T15:00:42.291295: step 8081, loss 3.52583.
Train: 2018-08-02T15:00:42.353781: step 8082, loss 3.65176.
Train: 2018-08-02T15:00:42.416266: step 8083, loss 4.65914.
Train: 2018-08-02T15:00:42.463158: step 8084, loss 4.40729.
Train: 2018-08-02T15:00:42.525647: step 8085, loss 3.9036.
Train: 2018-08-02T15:00:42.588133: step 8086, loss 3.27399.
Train: 2018-08-02T15:00:42.634996: step 8087, loss 4.53321.
Train: 2018-08-02T15:00:42.697484: step 8088, loss 5.16283.
Train: 2018-08-02T15:00:42.759966: step 8089, loss 3.39991.
Train: 2018-08-02T15:00:42.822451: step 8090, loss 4.91098.
Test: 2018-08-02T15:00:43.009902: step 8090, loss 3.81863.
Train: 2018-08-02T15:00:43.072392: step 8091, loss 3.27399.
Train: 2018-08-02T15:00:43.119260: step 8092, loss 3.39991.
Train: 2018-08-02T15:00:43.181711: step 8093, loss 3.52583.
Train: 2018-08-02T15:00:43.244199: step 8094, loss 3.27399.
Train: 2018-08-02T15:00:43.306716: step 8095, loss 3.39991.
Train: 2018-08-02T15:00:43.353546: step 8096, loss 4.02952.
Train: 2018-08-02T15:00:43.416062: step 8097, loss 4.65914.
Train: 2018-08-02T15:00:43.478517: step 8098, loss 4.78506.
Train: 2018-08-02T15:00:43.525382: step 8099, loss 4.02952.
Train: 2018-08-02T15:00:43.587899: step 8100, loss 2.89622.
Test: 2018-08-02T15:00:43.790974: step 8100, loss 3.81863.
Train: 2018-08-02T15:00:44.306449: step 8101, loss 3.9036.
Train: 2018-08-02T15:00:44.353312: step 8102, loss 4.65914.
Train: 2018-08-02T15:00:44.415828: step 8103, loss 4.40729.
Train: 2018-08-02T15:00:44.478285: step 8104, loss 3.77768.
Train: 2018-08-02T15:00:44.540794: step 8105, loss 3.52583.
Train: 2018-08-02T15:00:44.603253: step 8106, loss 4.28137.
Train: 2018-08-02T15:00:44.650117: step 8107, loss 4.65914.
Train: 2018-08-02T15:00:44.712639: step 8108, loss 4.78506.
Train: 2018-08-02T15:00:44.775115: step 8109, loss 2.51845.
Train: 2018-08-02T15:00:44.821985: step 8110, loss 3.39991.
Test: 2018-08-02T15:00:45.025060: step 8110, loss 3.81863.
Train: 2018-08-02T15:00:45.087545: step 8111, loss 4.40729.
Train: 2018-08-02T15:00:45.134381: step 8112, loss 3.52583.
Train: 2018-08-02T15:00:45.196898: step 8113, loss 4.65914.
Train: 2018-08-02T15:00:45.259382: step 8114, loss 3.9036.
Train: 2018-08-02T15:00:45.321837: step 8115, loss 3.65176.
Train: 2018-08-02T15:00:45.368731: step 8116, loss 4.15545.
Train: 2018-08-02T15:00:45.431215: step 8117, loss 3.77768.
Train: 2018-08-02T15:00:45.493671: step 8118, loss 4.15545.
Train: 2018-08-02T15:00:45.556182: step 8119, loss 3.9036.
Train: 2018-08-02T15:00:45.603021: step 8120, loss 3.39991.
Test: 2018-08-02T15:00:45.806097: step 8120, loss 3.81863.
Train: 2018-08-02T15:00:45.868613: step 8121, loss 3.27399.
Train: 2018-08-02T15:00:45.931098: step 8122, loss 4.40729.
Train: 2018-08-02T15:00:45.977965: step 8123, loss 3.77768.
Train: 2018-08-02T15:00:46.040448: step 8124, loss 3.39991.
Train: 2018-08-02T15:00:46.102904: step 8125, loss 5.41467.
Train: 2018-08-02T15:00:46.165418: step 8126, loss 5.16283.
Train: 2018-08-02T15:00:46.212252: step 8127, loss 3.52583.
Train: 2018-08-02T15:00:46.274769: step 8128, loss 4.02952.
Train: 2018-08-02T15:00:46.337224: step 8129, loss 4.28137.
Train: 2018-08-02T15:00:46.399708: step 8130, loss 5.16283.
Test: 2018-08-02T15:00:46.587188: step 8130, loss 3.81863.
Train: 2018-08-02T15:00:46.649680: step 8131, loss 4.15545.
Train: 2018-08-02T15:00:46.712135: step 8132, loss 4.53321.
Train: 2018-08-02T15:00:46.774621: step 8133, loss 3.9036.
Train: 2018-08-02T15:00:46.837139: step 8134, loss 3.27399.
Train: 2018-08-02T15:00:46.884001: step 8135, loss 4.40729.
Train: 2018-08-02T15:00:46.946457: step 8136, loss 3.65176.
Train: 2018-08-02T15:00:47.008971: step 8137, loss 4.15545.
Train: 2018-08-02T15:00:47.055805: step 8138, loss 3.52583.
Train: 2018-08-02T15:00:47.118320: step 8139, loss 3.14807.
Train: 2018-08-02T15:00:47.180808: step 8140, loss 4.02952.
Test: 2018-08-02T15:00:47.368255: step 8140, loss 3.81863.
Train: 2018-08-02T15:00:47.430747: step 8141, loss 3.52583.
Train: 2018-08-02T15:00:47.493202: step 8142, loss 4.15545.
Train: 2018-08-02T15:00:47.555688: step 8143, loss 3.9036.
Train: 2018-08-02T15:00:47.618206: step 8144, loss 3.52583.
Train: 2018-08-02T15:00:47.665068: step 8145, loss 4.40729.
Train: 2018-08-02T15:00:47.727555: step 8146, loss 5.28875.
Train: 2018-08-02T15:00:47.790038: step 8147, loss 4.65914.
Train: 2018-08-02T15:00:47.852518: step 8148, loss 3.9036.
Train: 2018-08-02T15:00:47.899357: step 8149, loss 4.40729.
Train: 2018-08-02T15:00:47.961843: step 8150, loss 4.02952.
Test: 2018-08-02T15:00:48.164919: step 8150, loss 3.81863.
Train: 2018-08-02T15:00:48.211817: step 8151, loss 3.9036.
Train: 2018-08-02T15:00:48.274299: step 8152, loss 3.27399.
Train: 2018-08-02T15:00:48.336755: step 8153, loss 4.28137.
Train: 2018-08-02T15:00:48.383648: step 8154, loss 5.10406.
Train: 2018-08-02T15:00:48.446104: step 8155, loss 4.65914.
Train: 2018-08-02T15:00:48.492999: step 8156, loss 3.77768.
Train: 2018-08-02T15:00:48.555483: step 8157, loss 3.65176.
Train: 2018-08-02T15:00:48.617940: step 8158, loss 4.28137.
Train: 2018-08-02T15:00:48.680455: step 8159, loss 3.77768.
Train: 2018-08-02T15:00:48.727320: step 8160, loss 2.89622.
Test: 2018-08-02T15:00:48.930365: step 8160, loss 3.81863.
Train: 2018-08-02T15:00:48.992852: step 8161, loss 3.02214.
Train: 2018-08-02T15:00:49.055336: step 8162, loss 4.02952.
Train: 2018-08-02T15:00:49.102228: step 8163, loss 3.9036.
Train: 2018-08-02T15:00:49.164713: step 8164, loss 4.15545.
Train: 2018-08-02T15:00:49.227204: step 8165, loss 2.7703.
Train: 2018-08-02T15:00:49.289688: step 8166, loss 4.02952.
Train: 2018-08-02T15:00:49.336549: step 8167, loss 4.40729.
Train: 2018-08-02T15:00:49.399042: step 8168, loss 2.64438.
Train: 2018-08-02T15:00:49.461521: step 8169, loss 4.15545.
Train: 2018-08-02T15:00:49.508380: step 8170, loss 3.65176.
Test: 2018-08-02T15:00:49.711458: step 8170, loss 3.81863.
Train: 2018-08-02T15:00:49.773919: step 8171, loss 4.15545.
Train: 2018-08-02T15:00:49.836436: step 8172, loss 3.27399.
Train: 2018-08-02T15:00:49.883268: step 8173, loss 4.02952.
Train: 2018-08-02T15:00:49.945787: step 8174, loss 4.40729.
Train: 2018-08-02T15:00:50.008240: step 8175, loss 2.7703.
Train: 2018-08-02T15:00:50.055133: step 8176, loss 3.65176.
Train: 2018-08-02T15:00:50.117618: step 8177, loss 3.77768.
Train: 2018-08-02T15:00:50.180103: step 8178, loss 4.91098.
Train: 2018-08-02T15:00:50.242586: step 8179, loss 4.91098.
Train: 2018-08-02T15:00:50.289423: step 8180, loss 4.28137.
Test: 2018-08-02T15:00:50.492499: step 8180, loss 3.81863.
Train: 2018-08-02T15:00:50.555010: step 8181, loss 4.28137.
Train: 2018-08-02T15:00:50.617471: step 8182, loss 3.39991.
Train: 2018-08-02T15:00:50.664365: step 8183, loss 3.65176.
Train: 2018-08-02T15:00:50.726853: step 8184, loss 5.16283.
Train: 2018-08-02T15:00:50.789336: step 8185, loss 3.02214.
Train: 2018-08-02T15:00:50.851823: step 8186, loss 4.53321.
Train: 2018-08-02T15:00:50.898685: step 8187, loss 3.52583.
Train: 2018-08-02T15:00:50.961143: step 8188, loss 3.65176.
Train: 2018-08-02T15:00:51.023657: step 8189, loss 3.65176.
Train: 2018-08-02T15:00:51.086141: step 8190, loss 4.15545.
Test: 2018-08-02T15:00:51.273602: step 8190, loss 3.81863.
Train: 2018-08-02T15:00:51.336053: step 8191, loss 4.02952.
Train: 2018-08-02T15:00:51.398571: step 8192, loss 3.9036.
Train: 2018-08-02T15:00:51.461023: step 8193, loss 3.9036.
Train: 2018-08-02T15:00:51.507914: step 8194, loss 3.52583.
Train: 2018-08-02T15:00:51.570405: step 8195, loss 3.77768.
Train: 2018-08-02T15:00:51.632863: step 8196, loss 4.78506.
Train: 2018-08-02T15:00:51.695342: step 8197, loss 3.27399.
Train: 2018-08-02T15:00:51.742209: step 8198, loss 5.5406.
Train: 2018-08-02T15:00:51.804694: step 8199, loss 3.9036.
Train: 2018-08-02T15:00:51.867208: step 8200, loss 3.65176.
Test: 2018-08-02T15:00:52.054633: step 8200, loss 3.81863.
Train: 2018-08-02T15:00:52.617027: step 8201, loss 5.28875.
Train: 2018-08-02T15:00:52.679488: step 8202, loss 2.89622.
Train: 2018-08-02T15:00:52.742003: step 8203, loss 5.0369.
Train: 2018-08-02T15:00:52.804490: step 8204, loss 3.14807.
Train: 2018-08-02T15:00:52.851355: step 8205, loss 4.15545.
Train: 2018-08-02T15:00:52.913809: step 8206, loss 4.53321.
Train: 2018-08-02T15:00:52.976325: step 8207, loss 4.65914.
Train: 2018-08-02T15:00:53.023187: step 8208, loss 4.40729.
Train: 2018-08-02T15:00:53.085670: step 8209, loss 4.40729.
Train: 2018-08-02T15:00:53.148158: step 8210, loss 3.14807.
Test: 2018-08-02T15:00:53.351206: step 8210, loss 3.81863.
Train: 2018-08-02T15:00:53.398070: step 8211, loss 3.39991.
Train: 2018-08-02T15:00:53.460582: step 8212, loss 4.28137.
Train: 2018-08-02T15:00:53.523071: step 8213, loss 6.04429.
Train: 2018-08-02T15:00:53.585528: step 8214, loss 3.9036.
Train: 2018-08-02T15:00:53.648010: step 8215, loss 3.77768.
Train: 2018-08-02T15:00:53.694908: step 8216, loss 4.91098.
Train: 2018-08-02T15:00:53.757360: step 8217, loss 4.15545.
Train: 2018-08-02T15:00:53.819880: step 8218, loss 3.77768.
Train: 2018-08-02T15:00:53.866710: step 8219, loss 4.02952.
Train: 2018-08-02T15:00:53.929196: step 8220, loss 4.15545.
Test: 2018-08-02T15:00:54.132271: step 8220, loss 3.81863.
Train: 2018-08-02T15:00:54.194788: step 8221, loss 3.65176.
Train: 2018-08-02T15:00:54.257274: step 8222, loss 4.28137.
Train: 2018-08-02T15:00:54.304133: step 8223, loss 4.02952.
Train: 2018-08-02T15:00:54.366595: step 8224, loss 3.52583.
Train: 2018-08-02T15:00:54.429108: step 8225, loss 3.65176.
Train: 2018-08-02T15:00:54.491593: step 8226, loss 3.52583.
Train: 2018-08-02T15:00:54.538458: step 8227, loss 4.15545.
Train: 2018-08-02T15:00:54.600941: step 8228, loss 3.39991.
Train: 2018-08-02T15:00:54.663400: step 8229, loss 3.14807.
Train: 2018-08-02T15:00:54.710262: step 8230, loss 3.14807.
Test: 2018-08-02T15:00:54.913340: step 8230, loss 3.81863.
Train: 2018-08-02T15:00:54.960204: step 8231, loss 3.27399.
Train: 2018-08-02T15:00:55.022719: step 8232, loss 3.9036.
Train: 2018-08-02T15:00:55.085175: step 8233, loss 4.28137.
Train: 2018-08-02T15:00:55.147691: step 8234, loss 4.53321.
Train: 2018-08-02T15:00:55.194524: step 8235, loss 3.9036.
Train: 2018-08-02T15:00:55.257009: step 8236, loss 3.9036.
Train: 2018-08-02T15:00:55.319525: step 8237, loss 5.28875.
Train: 2018-08-02T15:00:55.366389: step 8238, loss 4.15545.
Train: 2018-08-02T15:00:55.428874: step 8239, loss 3.9036.
Train: 2018-08-02T15:00:55.491360: step 8240, loss 4.02952.
Test: 2018-08-02T15:00:55.694438: step 8240, loss 3.81863.
Train: 2018-08-02T15:00:55.741302: step 8241, loss 5.16283.
Train: 2018-08-02T15:00:55.803786: step 8242, loss 3.02214.
Train: 2018-08-02T15:00:55.866241: step 8243, loss 4.28137.
Train: 2018-08-02T15:00:55.928753: step 8244, loss 3.9036.
Train: 2018-08-02T15:00:55.975623: step 8245, loss 3.14807.
Train: 2018-08-02T15:00:56.038107: step 8246, loss 4.15545.
Train: 2018-08-02T15:00:56.100594: step 8247, loss 3.52583.
Train: 2018-08-02T15:00:56.163072: step 8248, loss 3.02214.
Train: 2018-08-02T15:00:56.209912: step 8249, loss 5.28875.
Train: 2018-08-02T15:00:56.272397: step 8250, loss 3.52583.
Test: 2018-08-02T15:00:56.475473: step 8250, loss 3.81863.
Train: 2018-08-02T15:00:56.537959: step 8251, loss 3.52583.
Train: 2018-08-02T15:00:56.584824: step 8252, loss 3.77768.
Train: 2018-08-02T15:00:56.647309: step 8253, loss 4.02952.
Train: 2018-08-02T15:00:56.709796: step 8254, loss 3.9036.
Train: 2018-08-02T15:00:56.772281: step 8255, loss 3.9036.
Train: 2018-08-02T15:00:56.834793: step 8256, loss 4.78506.
Train: 2018-08-02T15:00:56.881659: step 8257, loss 3.39991.
Train: 2018-08-02T15:00:56.944115: step 8258, loss 4.65914.
Train: 2018-08-02T15:00:57.006630: step 8259, loss 4.02952.
Train: 2018-08-02T15:00:57.053464: step 8260, loss 3.9036.
Test: 2018-08-02T15:00:57.256540: step 8260, loss 3.81863.
Train: 2018-08-02T15:00:57.319057: step 8261, loss 4.65914.
Train: 2018-08-02T15:00:57.381541: step 8262, loss 4.02952.
Train: 2018-08-02T15:00:57.443998: step 8263, loss 4.65914.
Train: 2018-08-02T15:00:57.490863: step 8264, loss 4.02952.
Train: 2018-08-02T15:00:57.553378: step 8265, loss 4.28137.
Train: 2018-08-02T15:00:57.615832: step 8266, loss 4.02952.
Train: 2018-08-02T15:00:57.662726: step 8267, loss 4.53321.
Train: 2018-08-02T15:00:57.725214: step 8268, loss 4.53321.
Train: 2018-08-02T15:00:57.787668: step 8269, loss 4.53321.
Train: 2018-08-02T15:00:57.850151: step 8270, loss 3.77768.
Test: 2018-08-02T15:00:58.037633: step 8270, loss 3.81863.
Train: 2018-08-02T15:00:58.100126: step 8271, loss 4.15545.
Train: 2018-08-02T15:00:58.162580: step 8272, loss 4.91098.
Train: 2018-08-02T15:00:58.225097: step 8273, loss 4.53321.
Train: 2018-08-02T15:00:58.287580: step 8274, loss 3.39991.
Train: 2018-08-02T15:00:58.350067: step 8275, loss 4.91098.
Train: 2018-08-02T15:00:58.396925: step 8276, loss 4.53321.
Train: 2018-08-02T15:00:58.459414: step 8277, loss 4.02952.
Train: 2018-08-02T15:00:58.506279: step 8278, loss 3.65176.
Train: 2018-08-02T15:00:58.568763: step 8279, loss 4.28137.
Train: 2018-08-02T15:00:58.631218: step 8280, loss 3.77768.
Test: 2018-08-02T15:00:58.818676: step 8280, loss 3.81863.
Train: 2018-08-02T15:00:58.881160: step 8281, loss 2.89622.
Train: 2018-08-02T15:00:58.943671: step 8282, loss 4.28137.
Train: 2018-08-02T15:00:59.006132: step 8283, loss 3.39991.
Train: 2018-08-02T15:00:59.068647: step 8284, loss 4.40729.
Train: 2018-08-02T15:00:59.115512: step 8285, loss 3.52583.
Train: 2018-08-02T15:00:59.177968: step 8286, loss 4.78506.
Train: 2018-08-02T15:00:59.240451: step 8287, loss 4.53321.
Train: 2018-08-02T15:00:59.302968: step 8288, loss 4.28137.
Train: 2018-08-02T15:00:59.349829: step 8289, loss 4.65914.
Train: 2018-08-02T15:00:59.412312: step 8290, loss 4.78506.
Test: 2018-08-02T15:00:59.599749: step 8290, loss 3.81863.
Train: 2018-08-02T15:00:59.662258: step 8291, loss 2.89622.
Train: 2018-08-02T15:00:59.724740: step 8292, loss 4.28137.
Train: 2018-08-02T15:00:59.787229: step 8293, loss 4.15545.
Train: 2018-08-02T15:00:59.849712: step 8294, loss 3.14807.
Train: 2018-08-02T15:00:59.912169: step 8295, loss 3.9036.
Train: 2018-08-02T15:00:59.959061: step 8296, loss 4.65914.
Train: 2018-08-02T15:01:00.021518: step 8297, loss 3.65176.
Train: 2018-08-02T15:01:00.084032: step 8298, loss 5.0369.
Train: 2018-08-02T15:01:00.130899: step 8299, loss 4.02952.
Train: 2018-08-02T15:01:00.193378: step 8300, loss 4.65914.
Test: 2018-08-02T15:01:00.396431: step 8300, loss 3.81863.
Train: 2018-08-02T15:01:00.911934: step 8301, loss 4.15545.
Train: 2018-08-02T15:01:00.974421: step 8302, loss 3.52583.
Train: 2018-08-02T15:01:01.036906: step 8303, loss 4.53321.
Train: 2018-08-02T15:01:01.083771: step 8304, loss 2.64438.
Train: 2018-08-02T15:01:01.130660: step 8305, loss 3.76089.
Train: 2018-08-02T15:01:01.193119: step 8306, loss 4.78506.
Train: 2018-08-02T15:01:01.255604: step 8307, loss 3.27399.
Train: 2018-08-02T15:01:01.302470: step 8308, loss 4.02952.
Train: 2018-08-02T15:01:01.364954: step 8309, loss 3.52583.
Train: 2018-08-02T15:01:01.427438: step 8310, loss 4.15545.
Test: 2018-08-02T15:01:01.630518: step 8310, loss 3.81863.
Train: 2018-08-02T15:01:01.693032: step 8311, loss 4.15545.
Train: 2018-08-02T15:01:01.739897: step 8312, loss 3.52583.
Train: 2018-08-02T15:01:01.802352: step 8313, loss 5.28875.
Train: 2018-08-02T15:01:01.864838: step 8314, loss 4.28137.
Train: 2018-08-02T15:01:01.927324: step 8315, loss 3.65176.
Train: 2018-08-02T15:01:01.974187: step 8316, loss 3.77768.
Train: 2018-08-02T15:01:02.036672: step 8317, loss 3.27399.
Train: 2018-08-02T15:01:02.099158: step 8318, loss 4.40729.
Train: 2018-08-02T15:01:02.146051: step 8319, loss 4.53321.
Train: 2018-08-02T15:01:02.208536: step 8320, loss 3.14807.
Test: 2018-08-02T15:01:02.411607: step 8320, loss 3.81863.
Train: 2018-08-02T15:01:02.474070: step 8321, loss 3.9036.
Train: 2018-08-02T15:01:02.536584: step 8322, loss 4.15545.
Train: 2018-08-02T15:01:02.583419: step 8323, loss 4.15545.
Train: 2018-08-02T15:01:02.645935: step 8324, loss 4.02952.
Train: 2018-08-02T15:01:02.708420: step 8325, loss 3.9036.
Train: 2018-08-02T15:01:02.770904: step 8326, loss 4.02952.
Train: 2018-08-02T15:01:02.817769: step 8327, loss 3.39991.
Train: 2018-08-02T15:01:02.880225: step 8328, loss 3.39991.
Train: 2018-08-02T15:01:02.942754: step 8329, loss 2.89622.
Train: 2018-08-02T15:01:02.989604: step 8330, loss 4.15545.
Test: 2018-08-02T15:01:03.192651: step 8330, loss 3.81863.
Train: 2018-08-02T15:01:03.255166: step 8331, loss 3.14807.
Train: 2018-08-02T15:01:03.302027: step 8332, loss 4.78506.
Train: 2018-08-02T15:01:03.364486: step 8333, loss 3.39991.
Train: 2018-08-02T15:01:03.426971: step 8334, loss 3.52583.
Train: 2018-08-02T15:01:03.489488: step 8335, loss 4.02952.
Train: 2018-08-02T15:01:03.536321: step 8336, loss 4.28137.
Train: 2018-08-02T15:01:03.598838: step 8337, loss 3.77768.
Train: 2018-08-02T15:01:03.661317: step 8338, loss 4.40729.
Train: 2018-08-02T15:01:03.708183: step 8339, loss 4.91098.
Train: 2018-08-02T15:01:03.770670: step 8340, loss 3.65176.
Test: 2018-08-02T15:01:03.973742: step 8340, loss 3.81863.
Train: 2018-08-02T15:01:04.036229: step 8341, loss 3.14807.
Train: 2018-08-02T15:01:04.083093: step 8342, loss 4.40729.
Train: 2018-08-02T15:01:04.145577: step 8343, loss 4.65914.
Train: 2018-08-02T15:01:04.208071: step 8344, loss 5.16283.
Train: 2018-08-02T15:01:04.270555: step 8345, loss 3.65176.
Train: 2018-08-02T15:01:04.317418: step 8346, loss 4.28137.
Train: 2018-08-02T15:01:04.379874: step 8347, loss 2.7703.
Train: 2018-08-02T15:01:04.442360: step 8348, loss 4.28137.
Train: 2018-08-02T15:01:04.504875: step 8349, loss 3.52583.
Train: 2018-08-02T15:01:04.551707: step 8350, loss 2.7703.
Test: 2018-08-02T15:01:04.754815: step 8350, loss 3.81863.
Train: 2018-08-02T15:01:04.817300: step 8351, loss 2.7703.
Train: 2018-08-02T15:01:04.879755: step 8352, loss 4.91098.
Train: 2018-08-02T15:01:04.942271: step 8353, loss 3.65176.
Train: 2018-08-02T15:01:05.004727: step 8354, loss 4.40729.
Train: 2018-08-02T15:01:05.067242: step 8355, loss 4.53321.
Train: 2018-08-02T15:01:05.114106: step 8356, loss 4.28137.
Train: 2018-08-02T15:01:05.176586: step 8357, loss 4.53321.
Train: 2018-08-02T15:01:05.239078: step 8358, loss 4.02952.
Train: 2018-08-02T15:01:05.285910: step 8359, loss 5.0369.
Train: 2018-08-02T15:01:05.348395: step 8360, loss 4.40729.
Test: 2018-08-02T15:01:05.551503: step 8360, loss 3.81863.
Train: 2018-08-02T15:01:05.613959: step 8361, loss 5.16283.
Train: 2018-08-02T15:01:05.676474: step 8362, loss 4.15545.
Train: 2018-08-02T15:01:05.723341: step 8363, loss 5.16283.
Train: 2018-08-02T15:01:05.785795: step 8364, loss 5.0369.
Train: 2018-08-02T15:01:05.848278: step 8365, loss 4.40729.
Train: 2018-08-02T15:01:05.910794: step 8366, loss 4.40729.
Train: 2018-08-02T15:01:05.957629: step 8367, loss 2.89622.
Train: 2018-08-02T15:01:06.020145: step 8368, loss 3.14807.
Train: 2018-08-02T15:01:06.082599: step 8369, loss 4.65914.
Train: 2018-08-02T15:01:06.129493: step 8370, loss 4.02952.
Test: 2018-08-02T15:01:06.332573: step 8370, loss 3.81863.
Train: 2018-08-02T15:01:06.395051: step 8371, loss 4.40729.
Train: 2018-08-02T15:01:06.457541: step 8372, loss 4.91098.
Train: 2018-08-02T15:01:06.519997: step 8373, loss 4.28137.
Train: 2018-08-02T15:01:06.566877: step 8374, loss 3.39991.
Train: 2018-08-02T15:01:06.629377: step 8375, loss 4.02952.
Train: 2018-08-02T15:01:06.691833: step 8376, loss 3.52583.
Train: 2018-08-02T15:01:06.738721: step 8377, loss 4.65914.
Train: 2018-08-02T15:01:06.801215: step 8378, loss 4.53321.
Train: 2018-08-02T15:01:06.863693: step 8379, loss 4.28137.
Train: 2018-08-02T15:01:06.926182: step 8380, loss 4.15545.
Test: 2018-08-02T15:01:07.113608: step 8380, loss 3.81863.
Train: 2018-08-02T15:01:07.176123: step 8381, loss 3.77768.
Train: 2018-08-02T15:01:07.238609: step 8382, loss 4.65914.
Train: 2018-08-02T15:01:07.301088: step 8383, loss 4.65914.
Train: 2018-08-02T15:01:07.347960: step 8384, loss 4.78506.
Train: 2018-08-02T15:01:07.410415: step 8385, loss 4.65914.
Train: 2018-08-02T15:01:07.472899: step 8386, loss 3.65176.
Train: 2018-08-02T15:01:07.535384: step 8387, loss 2.89622.
Train: 2018-08-02T15:01:07.597894: step 8388, loss 4.15545.
Train: 2018-08-02T15:01:07.644759: step 8389, loss 4.53321.
Train: 2018-08-02T15:01:07.707251: step 8390, loss 3.52583.
Test: 2018-08-02T15:01:07.910295: step 8390, loss 3.81863.
Train: 2018-08-02T15:01:07.972782: step 8391, loss 3.52583.
Train: 2018-08-02T15:01:08.019646: step 8392, loss 3.9036.
Train: 2018-08-02T15:01:08.082158: step 8393, loss 3.65176.
Train: 2018-08-02T15:01:08.144618: step 8394, loss 4.65914.
Train: 2018-08-02T15:01:08.207133: step 8395, loss 3.14807.
Train: 2018-08-02T15:01:08.253997: step 8396, loss 3.9036.
Train: 2018-08-02T15:01:08.316452: step 8397, loss 3.52583.
Train: 2018-08-02T15:01:08.378937: step 8398, loss 3.9036.
Train: 2018-08-02T15:01:08.425826: step 8399, loss 4.15545.
Train: 2018-08-02T15:01:08.488315: step 8400, loss 3.02214.
Test: 2018-08-02T15:01:08.691362: step 8400, loss 3.81863.
Train: 2018-08-02T15:01:09.175652: step 8401, loss 3.77768.
Train: 2018-08-02T15:01:09.222521: step 8402, loss 3.52583.
Train: 2018-08-02T15:01:09.284974: step 8403, loss 4.65914.
Train: 2018-08-02T15:01:09.347492: step 8404, loss 3.52583.
Train: 2018-08-02T15:01:09.409972: step 8405, loss 4.02952.
Train: 2018-08-02T15:01:09.472455: step 8406, loss 3.39991.
Train: 2018-08-02T15:01:09.519324: step 8407, loss 4.28137.
Train: 2018-08-02T15:01:09.581779: step 8408, loss 5.41467.
Train: 2018-08-02T15:01:09.644266: step 8409, loss 4.40729.
Train: 2018-08-02T15:01:09.691160: step 8410, loss 3.9036.
Test: 2018-08-02T15:01:09.894237: step 8410, loss 3.81863.
Train: 2018-08-02T15:01:09.956691: step 8411, loss 2.7703.
Train: 2018-08-02T15:01:10.019207: step 8412, loss 4.15545.
Train: 2018-08-02T15:01:10.081662: step 8413, loss 4.02952.
Train: 2018-08-02T15:01:10.128558: step 8414, loss 3.65176.
Train: 2018-08-02T15:01:10.191038: step 8415, loss 4.91098.
Train: 2018-08-02T15:01:10.253498: step 8416, loss 3.9036.
Train: 2018-08-02T15:01:10.300361: step 8417, loss 3.9036.
Train: 2018-08-02T15:01:10.362879: step 8418, loss 4.28137.
Train: 2018-08-02T15:01:10.425365: step 8419, loss 5.41467.
Train: 2018-08-02T15:01:10.472228: step 8420, loss 4.02952.
Test: 2018-08-02T15:01:10.675298: step 8420, loss 3.81863.
Train: 2018-08-02T15:01:10.737758: step 8421, loss 3.9036.
Train: 2018-08-02T15:01:10.800274: step 8422, loss 3.52583.
Train: 2018-08-02T15:01:10.862759: step 8423, loss 3.02214.
Train: 2018-08-02T15:01:10.909625: step 8424, loss 4.40729.
Train: 2018-08-02T15:01:10.972107: step 8425, loss 3.77768.
Train: 2018-08-02T15:01:11.034590: step 8426, loss 4.28137.
Train: 2018-08-02T15:01:11.081430: step 8427, loss 3.9036.
Train: 2018-08-02T15:01:11.143945: step 8428, loss 3.9036.
Train: 2018-08-02T15:01:11.222053: step 8429, loss 4.53321.
Train: 2018-08-02T15:01:11.284536: step 8430, loss 3.39991.
Test: 2018-08-02T15:01:11.471964: step 8430, loss 3.81863.
Train: 2018-08-02T15:01:11.534478: step 8431, loss 4.78506.
Train: 2018-08-02T15:01:11.596932: step 8432, loss 4.15545.
Train: 2018-08-02T15:01:11.659448: step 8433, loss 3.52583.
Train: 2018-08-02T15:01:11.721936: step 8434, loss 3.65176.
Train: 2018-08-02T15:01:11.768768: step 8435, loss 4.28137.
Train: 2018-08-02T15:01:11.831254: step 8436, loss 5.0369.
Train: 2018-08-02T15:01:11.893768: step 8437, loss 4.02952.
Train: 2018-08-02T15:01:11.940603: step 8438, loss 3.65176.
Train: 2018-08-02T15:01:12.003118: step 8439, loss 4.28137.
Train: 2018-08-02T15:01:12.065599: step 8440, loss 4.53321.
Test: 2018-08-02T15:01:12.268674: step 8440, loss 3.81863.
Train: 2018-08-02T15:01:12.315547: step 8441, loss 2.7703.
Train: 2018-08-02T15:01:12.378025: step 8442, loss 4.91098.
Train: 2018-08-02T15:01:12.440485: step 8443, loss 3.02214.
Train: 2018-08-02T15:01:12.503002: step 8444, loss 3.77768.
Train: 2018-08-02T15:01:12.565483: step 8445, loss 2.64438.
Train: 2018-08-02T15:01:12.612350: step 8446, loss 3.9036.
Train: 2018-08-02T15:01:12.674836: step 8447, loss 3.9036.
Train: 2018-08-02T15:01:12.737292: step 8448, loss 4.28137.
Train: 2018-08-02T15:01:12.784156: step 8449, loss 4.02952.
Train: 2018-08-02T15:01:12.846640: step 8450, loss 4.40729.
Test: 2018-08-02T15:01:13.049716: step 8450, loss 3.81863.
Train: 2018-08-02T15:01:13.096614: step 8451, loss 3.52583.
Train: 2018-08-02T15:01:13.159096: step 8452, loss 2.39253.
Train: 2018-08-02T15:01:13.221552: step 8453, loss 4.02952.
Train: 2018-08-02T15:01:13.284069: step 8454, loss 4.40729.
Train: 2018-08-02T15:01:13.330930: step 8455, loss 4.15545.
Train: 2018-08-02T15:01:13.377790: step 8456, loss 4.83543.
Train: 2018-08-02T15:01:13.440281: step 8457, loss 2.89622.
Train: 2018-08-02T15:01:13.487148: step 8458, loss 3.77768.
Train: 2018-08-02T15:01:13.549631: step 8459, loss 4.53321.
Train: 2018-08-02T15:01:13.612112: step 8460, loss 3.02214.
Test: 2018-08-02T15:01:13.815162: step 8460, loss 3.81863.
Train: 2018-08-02T15:01:13.877678: step 8461, loss 4.02952.
Train: 2018-08-02T15:01:13.940164: step 8462, loss 3.14807.
Train: 2018-08-02T15:01:13.987023: step 8463, loss 4.40729.
Train: 2018-08-02T15:01:14.049486: step 8464, loss 3.65176.
Train: 2018-08-02T15:01:14.112000: step 8465, loss 3.27399.
Train: 2018-08-02T15:01:14.158863: step 8466, loss 4.28137.
Train: 2018-08-02T15:01:14.221349: step 8467, loss 3.77768.
Train: 2018-08-02T15:01:14.283835: step 8468, loss 4.78506.
Train: 2018-08-02T15:01:14.346290: step 8469, loss 4.28137.
Train: 2018-08-02T15:01:14.393154: step 8470, loss 3.39991.
Test: 2018-08-02T15:01:14.596256: step 8470, loss 3.81863.
Train: 2018-08-02T15:01:14.658746: step 8471, loss 3.77768.
Train: 2018-08-02T15:01:14.721233: step 8472, loss 3.9036.
Train: 2018-08-02T15:01:14.783716: step 8473, loss 4.28137.
Train: 2018-08-02T15:01:14.830579: step 8474, loss 4.65914.
Train: 2018-08-02T15:01:14.893037: step 8475, loss 4.91098.
Train: 2018-08-02T15:01:14.955522: step 8476, loss 3.39991.
Train: 2018-08-02T15:01:15.002416: step 8477, loss 4.02952.
Train: 2018-08-02T15:01:15.064875: step 8478, loss 3.27399.
Train: 2018-08-02T15:01:15.127387: step 8479, loss 4.28137.
Train: 2018-08-02T15:01:15.174220: step 8480, loss 4.91098.
Test: 2018-08-02T15:01:15.377327: step 8480, loss 3.81863.
Train: 2018-08-02T15:01:15.439816: step 8481, loss 3.39991.
Train: 2018-08-02T15:01:15.502268: step 8482, loss 3.9036.
Train: 2018-08-02T15:01:15.564784: step 8483, loss 5.5406.
Train: 2018-08-02T15:01:15.611649: step 8484, loss 4.02952.
Train: 2018-08-02T15:01:15.674134: step 8485, loss 5.0369.
Train: 2018-08-02T15:01:15.736614: step 8486, loss 4.28137.
Train: 2018-08-02T15:01:15.783483: step 8487, loss 3.02214.
Train: 2018-08-02T15:01:15.845940: step 8488, loss 3.02214.
Train: 2018-08-02T15:01:15.908454: step 8489, loss 5.16283.
Train: 2018-08-02T15:01:15.955315: step 8490, loss 4.15545.
Test: 2018-08-02T15:01:16.158389: step 8490, loss 3.81863.
Train: 2018-08-02T15:01:16.220882: step 8491, loss 3.9036.
Train: 2018-08-02T15:01:16.283336: step 8492, loss 4.78506.
Train: 2018-08-02T15:01:16.345853: step 8493, loss 2.89622.
Train: 2018-08-02T15:01:16.408307: step 8494, loss 4.40729.
Train: 2018-08-02T15:01:16.470826: step 8495, loss 5.0369.
Train: 2018-08-02T15:01:16.517685: step 8496, loss 4.53321.
Train: 2018-08-02T15:01:16.580170: step 8497, loss 4.40729.
Train: 2018-08-02T15:01:16.642639: step 8498, loss 3.9036.
Train: 2018-08-02T15:01:16.689491: step 8499, loss 3.27399.
Train: 2018-08-02T15:01:16.751976: step 8500, loss 3.77768.
Test: 2018-08-02T15:01:16.955086: step 8500, loss 3.81863.
Train: 2018-08-02T15:01:17.439327: step 8501, loss 3.02214.
Train: 2018-08-02T15:01:17.486208: step 8502, loss 4.15545.
Train: 2018-08-02T15:01:17.548665: step 8503, loss 3.65176.
Train: 2018-08-02T15:01:17.611151: step 8504, loss 3.27399.
Train: 2018-08-02T15:01:17.673634: step 8505, loss 4.02952.
Train: 2018-08-02T15:01:17.736146: step 8506, loss 3.77768.
Train: 2018-08-02T15:01:17.783010: step 8507, loss 3.02214.
Train: 2018-08-02T15:01:17.845503: step 8508, loss 3.77768.
Train: 2018-08-02T15:01:17.907956: step 8509, loss 4.53321.
Train: 2018-08-02T15:01:17.954847: step 8510, loss 3.02214.
Test: 2018-08-02T15:01:18.157924: step 8510, loss 3.81863.
Train: 2018-08-02T15:01:18.220414: step 8511, loss 5.16283.
Train: 2018-08-02T15:01:18.282893: step 8512, loss 3.52583.
Train: 2018-08-02T15:01:18.345384: step 8513, loss 3.27399.
Train: 2018-08-02T15:01:18.407881: step 8514, loss 4.65914.
Train: 2018-08-02T15:01:18.454704: step 8515, loss 4.15545.
Train: 2018-08-02T15:01:18.517191: step 8516, loss 3.52583.
Train: 2018-08-02T15:01:18.579702: step 8517, loss 4.28137.
Train: 2018-08-02T15:01:18.626538: step 8518, loss 4.53321.
Train: 2018-08-02T15:01:18.689055: step 8519, loss 3.02214.
Train: 2018-08-02T15:01:18.751532: step 8520, loss 2.7703.
Test: 2018-08-02T15:01:18.954618: step 8520, loss 3.81863.
Train: 2018-08-02T15:01:19.001479: step 8521, loss 4.53321.
Train: 2018-08-02T15:01:19.063935: step 8522, loss 4.53321.
Train: 2018-08-02T15:01:19.126453: step 8523, loss 3.52583.
Train: 2018-08-02T15:01:19.188938: step 8524, loss 3.65176.
Train: 2018-08-02T15:01:19.251421: step 8525, loss 4.02952.
Train: 2018-08-02T15:01:19.298254: step 8526, loss 4.53321.
Train: 2018-08-02T15:01:19.360743: step 8527, loss 4.28137.
Train: 2018-08-02T15:01:19.423262: step 8528, loss 4.40729.
Train: 2018-08-02T15:01:19.485742: step 8529, loss 4.15545.
Train: 2018-08-02T15:01:19.532575: step 8530, loss 3.77768.
Test: 2018-08-02T15:01:19.735654: step 8530, loss 3.81863.
Train: 2018-08-02T15:01:19.798139: step 8531, loss 4.53321.
Train: 2018-08-02T15:01:19.860623: step 8532, loss 3.27399.
Train: 2018-08-02T15:01:19.907512: step 8533, loss 5.0369.
Train: 2018-08-02T15:01:19.969974: step 8534, loss 3.65176.
Train: 2018-08-02T15:01:20.032494: step 8535, loss 3.9036.
Train: 2018-08-02T15:01:20.079321: step 8536, loss 4.78506.
Train: 2018-08-02T15:01:20.141837: step 8537, loss 3.14807.
Train: 2018-08-02T15:01:20.204321: step 8538, loss 3.39991.
Train: 2018-08-02T15:01:20.266804: step 8539, loss 4.28137.
Train: 2018-08-02T15:01:20.313643: step 8540, loss 4.15545.
Test: 2018-08-02T15:01:20.516718: step 8540, loss 3.81863.
Train: 2018-08-02T15:01:20.579204: step 8541, loss 3.39991.
Train: 2018-08-02T15:01:20.641720: step 8542, loss 3.14807.
Train: 2018-08-02T15:01:20.704203: step 8543, loss 3.39991.
Train: 2018-08-02T15:01:20.751065: step 8544, loss 4.53321.
Train: 2018-08-02T15:01:20.813558: step 8545, loss 4.40729.
Train: 2018-08-02T15:01:20.876035: step 8546, loss 3.39991.
Train: 2018-08-02T15:01:20.938525: step 8547, loss 4.40729.
Train: 2018-08-02T15:01:20.985391: step 8548, loss 3.27399.
Train: 2018-08-02T15:01:21.047846: step 8549, loss 4.02952.
Train: 2018-08-02T15:01:21.110360: step 8550, loss 4.02952.
Test: 2018-08-02T15:01:21.313416: step 8550, loss 3.81863.
Train: 2018-08-02T15:01:21.360302: step 8551, loss 5.41467.
Train: 2018-08-02T15:01:21.422758: step 8552, loss 3.9036.
Train: 2018-08-02T15:01:21.485242: step 8553, loss 5.5406.
Train: 2018-08-02T15:01:21.547729: step 8554, loss 5.16283.
Train: 2018-08-02T15:01:21.594624: step 8555, loss 4.28137.
Train: 2018-08-02T15:01:21.657102: step 8556, loss 4.15545.
Train: 2018-08-02T15:01:21.719587: step 8557, loss 2.7703.
Train: 2018-08-02T15:01:21.782050: step 8558, loss 3.27399.
Train: 2018-08-02T15:01:21.828944: step 8559, loss 2.64437.
Train: 2018-08-02T15:01:21.891396: step 8560, loss 4.15545.
Test: 2018-08-02T15:01:22.094499: step 8560, loss 3.81863.
Train: 2018-08-02T15:01:22.141369: step 8561, loss 3.14807.
Train: 2018-08-02T15:01:22.203851: step 8562, loss 4.78506.
Train: 2018-08-02T15:01:22.266335: step 8563, loss 4.40729.
Train: 2018-08-02T15:01:22.328822: step 8564, loss 4.28137.
Train: 2018-08-02T15:01:22.375685: step 8565, loss 5.5406.
Train: 2018-08-02T15:01:22.438145: step 8566, loss 5.28875.
Train: 2018-08-02T15:01:22.500660: step 8567, loss 4.40729.
Train: 2018-08-02T15:01:22.547494: step 8568, loss 4.65914.
Train: 2018-08-02T15:01:22.610010: step 8569, loss 3.77768.
Train: 2018-08-02T15:01:22.672495: step 8570, loss 5.28875.
Test: 2018-08-02T15:01:22.875573: step 8570, loss 3.81863.
Train: 2018-08-02T15:01:22.938056: step 8571, loss 3.77768.
Train: 2018-08-02T15:01:22.984921: step 8572, loss 3.65176.
Train: 2018-08-02T15:01:23.047376: step 8573, loss 5.28875.
Train: 2018-08-02T15:01:23.109863: step 8574, loss 4.15545.
Train: 2018-08-02T15:01:23.172378: step 8575, loss 3.65176.
Train: 2018-08-02T15:01:23.219211: step 8576, loss 5.0369.
Train: 2018-08-02T15:01:23.281727: step 8577, loss 4.53321.
Train: 2018-08-02T15:01:23.344215: step 8578, loss 2.89622.
Train: 2018-08-02T15:01:23.406669: step 8579, loss 3.9036.
Train: 2018-08-02T15:01:23.453531: step 8580, loss 3.02214.
Test: 2018-08-02T15:01:23.656609: step 8580, loss 3.81863.
Train: 2018-08-02T15:01:23.719126: step 8581, loss 4.28137.
Train: 2018-08-02T15:01:23.781610: step 8582, loss 4.78506.
Train: 2018-08-02T15:01:23.828471: step 8583, loss 4.28137.
Train: 2018-08-02T15:01:23.890960: step 8584, loss 3.9036.
Train: 2018-08-02T15:01:23.953415: step 8585, loss 3.65176.
Train: 2018-08-02T15:01:24.015930: step 8586, loss 4.28137.
Train: 2018-08-02T15:01:24.062764: step 8587, loss 3.52583.
Train: 2018-08-02T15:01:24.125251: step 8588, loss 3.65176.
Train: 2018-08-02T15:01:24.187765: step 8589, loss 3.65176.
Train: 2018-08-02T15:01:24.234628: step 8590, loss 4.40729.
Test: 2018-08-02T15:01:24.437700: step 8590, loss 3.81863.
Train: 2018-08-02T15:01:24.500193: step 8591, loss 4.53321.
Train: 2018-08-02T15:01:24.562671: step 8592, loss 3.14807.
Train: 2018-08-02T15:01:24.625132: step 8593, loss 4.15545.
Train: 2018-08-02T15:01:24.671996: step 8594, loss 3.9036.
Train: 2018-08-02T15:01:24.734508: step 8595, loss 5.16283.
Train: 2018-08-02T15:01:24.796992: step 8596, loss 3.9036.
Train: 2018-08-02T15:01:24.843863: step 8597, loss 4.15545.
Train: 2018-08-02T15:01:24.906334: step 8598, loss 4.65914.
Train: 2018-08-02T15:01:24.968803: step 8599, loss 3.65176.
Train: 2018-08-02T15:01:25.015665: step 8600, loss 3.77768.
Test: 2018-08-02T15:01:25.218767: step 8600, loss 3.81863.
Train: 2018-08-02T15:01:25.765521: step 8601, loss 3.77768.
Train: 2018-08-02T15:01:25.828005: step 8602, loss 3.9036.
Train: 2018-08-02T15:01:25.874870: step 8603, loss 3.77768.
Train: 2018-08-02T15:01:25.937328: step 8604, loss 4.02952.
Train: 2018-08-02T15:01:25.999842: step 8605, loss 2.14068.
Train: 2018-08-02T15:01:26.046705: step 8606, loss 3.77768.
Train: 2018-08-02T15:01:26.093540: step 8607, loss 4.83543.
Train: 2018-08-02T15:01:26.156023: step 8608, loss 4.40729.
Train: 2018-08-02T15:01:26.218541: step 8609, loss 3.77768.
Train: 2018-08-02T15:01:26.265374: step 8610, loss 4.28137.
Test: 2018-08-02T15:01:26.484072: step 8610, loss 3.81863.
Train: 2018-08-02T15:01:26.546558: step 8611, loss 3.39991.
Train: 2018-08-02T15:01:26.593422: step 8612, loss 4.65914.
Train: 2018-08-02T15:01:26.655932: step 8613, loss 4.15545.
Train: 2018-08-02T15:01:26.718394: step 8614, loss 5.0369.
Train: 2018-08-02T15:01:26.780878: step 8615, loss 5.16283.
Train: 2018-08-02T15:01:26.843394: step 8616, loss 3.65176.
Train: 2018-08-02T15:01:26.890257: step 8617, loss 3.65176.
Train: 2018-08-02T15:01:26.952746: step 8618, loss 3.77768.
Train: 2018-08-02T15:01:27.015222: step 8619, loss 4.65914.
Train: 2018-08-02T15:01:27.062061: step 8620, loss 4.15545.
Test: 2018-08-02T15:01:27.265139: step 8620, loss 3.81863.
Train: 2018-08-02T15:01:27.327654: step 8621, loss 4.40729.
Train: 2018-08-02T15:01:27.390139: step 8622, loss 4.28137.
Train: 2018-08-02T15:01:27.452627: step 8623, loss 4.53321.
Train: 2018-08-02T15:01:27.499490: step 8624, loss 4.28137.
Train: 2018-08-02T15:01:27.561945: step 8625, loss 3.52583.
Train: 2018-08-02T15:01:27.624455: step 8626, loss 3.77768.
Train: 2018-08-02T15:01:27.686915: step 8627, loss 3.77768.
Train: 2018-08-02T15:01:27.733811: step 8628, loss 3.77768.
Train: 2018-08-02T15:01:27.796265: step 8629, loss 4.15545.
Train: 2018-08-02T15:01:27.858779: step 8630, loss 3.65176.
Test: 2018-08-02T15:01:28.046206: step 8630, loss 3.81863.
Train: 2018-08-02T15:01:28.108721: step 8631, loss 3.77768.
Train: 2018-08-02T15:01:28.171207: step 8632, loss 4.28137.
Train: 2018-08-02T15:01:28.233691: step 8633, loss 3.77768.
Train: 2018-08-02T15:01:28.296148: step 8634, loss 4.53321.
Train: 2018-08-02T15:01:28.343045: step 8635, loss 3.65176.
Train: 2018-08-02T15:01:28.405529: step 8636, loss 3.9036.
Train: 2018-08-02T15:01:28.467982: step 8637, loss 4.02952.
Train: 2018-08-02T15:01:28.514847: step 8638, loss 4.28137.
Train: 2018-08-02T15:01:28.577361: step 8639, loss 3.52583.
Train: 2018-08-02T15:01:28.639848: step 8640, loss 4.02952.
Test: 2018-08-02T15:01:28.842919: step 8640, loss 3.81863.
Train: 2018-08-02T15:01:28.905381: step 8641, loss 3.52583.
Train: 2018-08-02T15:01:28.952244: step 8642, loss 3.65176.
Train: 2018-08-02T15:01:29.014759: step 8643, loss 4.28137.
Train: 2018-08-02T15:01:29.077243: step 8644, loss 3.27399.
Train: 2018-08-02T15:01:29.139705: step 8645, loss 4.65914.
Train: 2018-08-02T15:01:29.202217: step 8646, loss 3.77768.
Train: 2018-08-02T15:01:29.249050: step 8647, loss 4.53321.
Train: 2018-08-02T15:01:29.311536: step 8648, loss 3.77768.
Train: 2018-08-02T15:01:29.374021: step 8649, loss 4.53321.
Train: 2018-08-02T15:01:29.420914: step 8650, loss 4.78506.
Test: 2018-08-02T15:01:29.623962: step 8650, loss 3.81863.
Train: 2018-08-02T15:01:29.686473: step 8651, loss 3.65176.
Train: 2018-08-02T15:01:29.748962: step 8652, loss 4.15545.
Train: 2018-08-02T15:01:29.873929: step 8653, loss 4.15545.
Train: 2018-08-02T15:01:29.936389: step 8654, loss 3.77768.
Train: 2018-08-02T15:01:29.983283: step 8655, loss 3.02214.
Train: 2018-08-02T15:01:30.045738: step 8656, loss 4.15545.
Train: 2018-08-02T15:01:30.108249: step 8657, loss 3.27399.
Train: 2018-08-02T15:01:30.170739: step 8658, loss 4.15545.
Train: 2018-08-02T15:01:30.217604: step 8659, loss 2.26661.
Train: 2018-08-02T15:01:30.280087: step 8660, loss 4.28137.
Test: 2018-08-02T15:01:30.483135: step 8660, loss 3.81863.
Train: 2018-08-02T15:01:30.545645: step 8661, loss 3.27399.
Train: 2018-08-02T15:01:30.592518: step 8662, loss 5.16283.
Train: 2018-08-02T15:01:30.655001: step 8663, loss 3.77768.
Train: 2018-08-02T15:01:30.717483: step 8664, loss 4.02952.
Train: 2018-08-02T15:01:30.779973: step 8665, loss 4.40729.
Train: 2018-08-02T15:01:30.826832: step 8666, loss 4.53321.
Train: 2018-08-02T15:01:30.889320: step 8667, loss 3.27399.
Train: 2018-08-02T15:01:30.951804: step 8668, loss 2.89622.
Train: 2018-08-02T15:01:30.998670: step 8669, loss 4.40729.
Train: 2018-08-02T15:01:31.061154: step 8670, loss 4.28137.
Test: 2018-08-02T15:01:31.264202: step 8670, loss 3.81863.
Train: 2018-08-02T15:01:31.326687: step 8671, loss 4.40729.
Train: 2018-08-02T15:01:31.389173: step 8672, loss 3.65176.
Train: 2018-08-02T15:01:31.436038: step 8673, loss 3.9036.
Train: 2018-08-02T15:01:31.498524: step 8674, loss 3.39991.
Train: 2018-08-02T15:01:31.561009: step 8675, loss 4.02952.
Train: 2018-08-02T15:01:31.623493: step 8676, loss 2.26661.
Train: 2018-08-02T15:01:31.670357: step 8677, loss 4.28137.
Train: 2018-08-02T15:01:31.732869: step 8678, loss 5.0369.
Train: 2018-08-02T15:01:31.795358: step 8679, loss 3.27399.
Train: 2018-08-02T15:01:31.842219: step 8680, loss 4.78506.
Test: 2018-08-02T15:01:32.045269: step 8680, loss 3.81863.
Train: 2018-08-02T15:01:32.170270: step 8681, loss 5.0369.
Train: 2018-08-02T15:01:32.217104: step 8682, loss 3.65176.
Train: 2018-08-02T15:01:32.279616: step 8683, loss 4.65914.
Train: 2018-08-02T15:01:32.342108: step 8684, loss 4.78506.
Train: 2018-08-02T15:01:32.404560: step 8685, loss 3.39991.
Train: 2018-08-02T15:01:32.451452: step 8686, loss 3.27399.
Train: 2018-08-02T15:01:32.513943: step 8687, loss 5.5406.
Train: 2018-08-02T15:01:32.576396: step 8688, loss 4.15545.
Train: 2018-08-02T15:01:32.623259: step 8689, loss 5.28875.
Train: 2018-08-02T15:01:32.685769: step 8690, loss 3.27399.
Test: 2018-08-02T15:01:32.888820: step 8690, loss 3.81863.
Train: 2018-08-02T15:01:32.951337: step 8691, loss 4.40729.
Train: 2018-08-02T15:01:33.013825: step 8692, loss 3.9036.
Train: 2018-08-02T15:01:33.060657: step 8693, loss 3.9036.
Train: 2018-08-02T15:01:33.123143: step 8694, loss 4.02952.
Train: 2018-08-02T15:01:33.185661: step 8695, loss 4.40729.
Train: 2018-08-02T15:01:33.248113: step 8696, loss 4.53321.
Train: 2018-08-02T15:01:33.294976: step 8697, loss 3.27399.
Train: 2018-08-02T15:01:33.357461: step 8698, loss 3.02214.
Train: 2018-08-02T15:01:33.419978: step 8699, loss 3.9036.
Train: 2018-08-02T15:01:33.466844: step 8700, loss 3.65176.
Test: 2018-08-02T15:01:33.669888: step 8700, loss 3.81863.
Train: 2018-08-02T15:01:34.169802: step 8701, loss 3.52583.
Train: 2018-08-02T15:01:34.232287: step 8702, loss 3.9036.
Train: 2018-08-02T15:01:34.279151: step 8703, loss 4.53321.
Train: 2018-08-02T15:01:34.341640: step 8704, loss 3.65176.
Train: 2018-08-02T15:01:34.404118: step 8705, loss 3.77768.
Train: 2018-08-02T15:01:34.466577: step 8706, loss 4.15545.
Train: 2018-08-02T15:01:34.513472: step 8707, loss 4.78506.
Train: 2018-08-02T15:01:34.575928: step 8708, loss 4.02952.
Train: 2018-08-02T15:01:34.638414: step 8709, loss 4.02952.
Train: 2018-08-02T15:01:34.685309: step 8710, loss 4.40729.
Test: 2018-08-02T15:01:34.888377: step 8710, loss 3.81863.
Train: 2018-08-02T15:01:34.950840: step 8711, loss 4.28137.
Train: 2018-08-02T15:01:35.013325: step 8712, loss 3.52583.
Train: 2018-08-02T15:01:35.075809: step 8713, loss 4.28137.
Train: 2018-08-02T15:01:35.138320: step 8714, loss 4.91098.
Train: 2018-08-02T15:01:35.185189: step 8715, loss 3.9036.
Train: 2018-08-02T15:01:35.247675: step 8716, loss 3.14807.
Train: 2018-08-02T15:01:35.310130: step 8717, loss 3.77768.
Train: 2018-08-02T15:01:35.357024: step 8718, loss 3.9036.
Train: 2018-08-02T15:01:35.419479: step 8719, loss 2.7703.
Train: 2018-08-02T15:01:35.481995: step 8720, loss 5.0369.
Test: 2018-08-02T15:01:35.685073: step 8720, loss 3.81863.
Train: 2018-08-02T15:01:35.747527: step 8721, loss 3.77768.
Train: 2018-08-02T15:01:35.810012: step 8722, loss 4.02952.
Train: 2018-08-02T15:01:35.856908: step 8723, loss 3.9036.
Train: 2018-08-02T15:01:35.919393: step 8724, loss 4.02952.
Train: 2018-08-02T15:01:35.981879: step 8725, loss 4.02952.
Train: 2018-08-02T15:01:36.044332: step 8726, loss 3.65176.
Train: 2018-08-02T15:01:36.091229: step 8727, loss 3.39991.
Train: 2018-08-02T15:01:36.153684: step 8728, loss 4.28137.
Train: 2018-08-02T15:01:36.216200: step 8729, loss 3.65176.
Train: 2018-08-02T15:01:36.263033: step 8730, loss 3.77768.
Test: 2018-08-02T15:01:36.466108: step 8730, loss 3.81863.
Train: 2018-08-02T15:01:36.528594: step 8731, loss 3.52583.
Train: 2018-08-02T15:01:36.591104: step 8732, loss 3.65176.
Train: 2018-08-02T15:01:36.637974: step 8733, loss 4.28137.
Train: 2018-08-02T15:01:36.700458: step 8734, loss 4.65914.
Train: 2018-08-02T15:01:36.762916: step 8735, loss 3.27399.
Train: 2018-08-02T15:01:36.809809: step 8736, loss 3.65176.
Train: 2018-08-02T15:01:36.872294: step 8737, loss 3.9036.
Train: 2018-08-02T15:01:36.934751: step 8738, loss 4.53321.
Train: 2018-08-02T15:01:36.981644: step 8739, loss 5.0369.
Train: 2018-08-02T15:01:37.044099: step 8740, loss 4.78506.
Test: 2018-08-02T15:01:37.247206: step 8740, loss 3.81863.
Train: 2018-08-02T15:01:37.309693: step 8741, loss 4.28137.
Train: 2018-08-02T15:01:37.372146: step 8742, loss 3.27399.
Train: 2018-08-02T15:01:37.419036: step 8743, loss 3.77768.
Train: 2018-08-02T15:01:37.481530: step 8744, loss 3.77768.
Train: 2018-08-02T15:01:37.544008: step 8745, loss 4.28137.
Train: 2018-08-02T15:01:37.606492: step 8746, loss 3.39991.
Train: 2018-08-02T15:01:37.653331: step 8747, loss 4.65914.
Train: 2018-08-02T15:01:37.715849: step 8748, loss 3.65176.
Train: 2018-08-02T15:01:37.778303: step 8749, loss 3.77768.
Train: 2018-08-02T15:01:37.825167: step 8750, loss 4.02952.
Test: 2018-08-02T15:01:38.028268: step 8750, loss 3.81863.
Train: 2018-08-02T15:01:38.090761: step 8751, loss 4.28137.
Train: 2018-08-02T15:01:38.153244: step 8752, loss 4.40729.
Train: 2018-08-02T15:01:38.215723: step 8753, loss 3.52583.
Train: 2018-08-02T15:01:38.278209: step 8754, loss 4.78506.
Train: 2018-08-02T15:01:38.325077: step 8755, loss 4.78506.
Train: 2018-08-02T15:01:38.387564: step 8756, loss 3.9036.
Train: 2018-08-02T15:01:38.450049: step 8757, loss 3.77768.
Train: 2018-08-02T15:01:38.481296: step 8758, loss 3.76089.
Train: 2018-08-02T15:01:38.543773: step 8759, loss 4.02952.
Train: 2018-08-02T15:01:38.606264: step 8760, loss 3.77768.
Test: 2018-08-02T15:01:38.809311: step 8760, loss 3.81863.
Train: 2018-08-02T15:01:38.856175: step 8761, loss 4.15545.
Train: 2018-08-02T15:01:38.918690: step 8762, loss 3.77768.
Train: 2018-08-02T15:01:38.981169: step 8763, loss 3.77768.
Train: 2018-08-02T15:01:39.043656: step 8764, loss 4.53321.
Train: 2018-08-02T15:01:39.090527: step 8765, loss 4.02952.
Train: 2018-08-02T15:01:39.152980: step 8766, loss 3.9036.
Train: 2018-08-02T15:01:39.215465: step 8767, loss 4.65914.
Train: 2018-08-02T15:01:39.262360: step 8768, loss 3.9036.
Train: 2018-08-02T15:01:39.324815: step 8769, loss 4.02952.
Train: 2018-08-02T15:01:39.387339: step 8770, loss 4.65914.
Test: 2018-08-02T15:01:39.574757: step 8770, loss 3.81863.
Train: 2018-08-02T15:01:39.637242: step 8771, loss 4.40729.
Train: 2018-08-02T15:01:39.699754: step 8772, loss 5.16283.
Train: 2018-08-02T15:01:39.762213: step 8773, loss 5.5406.
Train: 2018-08-02T15:01:39.824731: step 8774, loss 4.78506.
Train: 2018-08-02T15:01:39.871602: step 8775, loss 3.65176.
Train: 2018-08-02T15:01:39.934047: step 8776, loss 3.27399.
Train: 2018-08-02T15:01:39.996558: step 8777, loss 3.02214.
Train: 2018-08-02T15:01:40.043428: step 8778, loss 3.65176.
Train: 2018-08-02T15:01:40.105907: step 8779, loss 3.52583.
Train: 2018-08-02T15:01:40.168391: step 8780, loss 5.0369.
Test: 2018-08-02T15:01:40.355822: step 8780, loss 3.81863.
Train: 2018-08-02T15:01:40.418333: step 8781, loss 4.78506.
Train: 2018-08-02T15:01:40.480824: step 8782, loss 3.65176.
Train: 2018-08-02T15:01:40.543310: step 8783, loss 3.65176.
Train: 2018-08-02T15:01:40.590143: step 8784, loss 5.28875.
Train: 2018-08-02T15:01:40.652661: step 8785, loss 3.77768.
Train: 2018-08-02T15:01:40.715147: step 8786, loss 4.40729.
Train: 2018-08-02T15:01:40.762008: step 8787, loss 4.02952.
Train: 2018-08-02T15:01:40.824464: step 8788, loss 5.0369.
Train: 2018-08-02T15:01:40.886974: step 8789, loss 4.28137.
Train: 2018-08-02T15:01:40.949434: step 8790, loss 4.40729.
Test: 2018-08-02T15:01:41.152537: step 8790, loss 3.81863.
Train: 2018-08-02T15:01:41.214997: step 8791, loss 4.02952.
Train: 2018-08-02T15:01:41.261892: step 8792, loss 3.65176.
Train: 2018-08-02T15:01:41.324374: step 8793, loss 4.28137.
Train: 2018-08-02T15:01:41.386832: step 8794, loss 3.27399.
Train: 2018-08-02T15:01:41.449318: step 8795, loss 4.65914.
Train: 2018-08-02T15:01:41.496180: step 8796, loss 4.40729.
Train: 2018-08-02T15:01:41.558697: step 8797, loss 3.14807.
Train: 2018-08-02T15:01:41.621157: step 8798, loss 3.77768.
Train: 2018-08-02T15:01:41.668046: step 8799, loss 3.9036.
Train: 2018-08-02T15:01:41.730532: step 8800, loss 3.39991.
Test: 2018-08-02T15:01:41.933580: step 8800, loss 3.81863.
Train: 2018-08-02T15:01:42.464732: step 8801, loss 3.14807.
Train: 2018-08-02T15:01:42.511598: step 8802, loss 3.27399.
Train: 2018-08-02T15:01:42.574081: step 8803, loss 4.78506.
Train: 2018-08-02T15:01:42.636572: step 8804, loss 3.27399.
Train: 2018-08-02T15:01:42.699053: step 8805, loss 4.53321.
Train: 2018-08-02T15:01:42.745926: step 8806, loss 3.52583.
Train: 2018-08-02T15:01:42.808407: step 8807, loss 3.27399.
Train: 2018-08-02T15:01:42.870885: step 8808, loss 3.77768.
Train: 2018-08-02T15:01:42.933376: step 8809, loss 4.02952.
Train: 2018-08-02T15:01:42.980239: step 8810, loss 3.27399.
Test: 2018-08-02T15:01:43.183288: step 8810, loss 3.81863.
Train: 2018-08-02T15:01:43.245797: step 8811, loss 3.02214.
Train: 2018-08-02T15:01:43.308286: step 8812, loss 4.65914.
Train: 2018-08-02T15:01:43.355121: step 8813, loss 3.9036.
Train: 2018-08-02T15:01:43.417638: step 8814, loss 4.15545.
Train: 2018-08-02T15:01:43.480091: step 8815, loss 3.77768.
Train: 2018-08-02T15:01:43.542577: step 8816, loss 4.28137.
Train: 2018-08-02T15:01:43.589442: step 8817, loss 3.77768.
Train: 2018-08-02T15:01:43.651928: step 8818, loss 4.15545.
Train: 2018-08-02T15:01:43.714411: step 8819, loss 3.52583.
Train: 2018-08-02T15:01:43.761275: step 8820, loss 3.39991.
Test: 2018-08-02T15:01:43.964385: step 8820, loss 3.81863.
Train: 2018-08-02T15:01:44.026865: step 8821, loss 4.78506.
Train: 2018-08-02T15:01:44.089324: step 8822, loss 3.77768.
Train: 2018-08-02T15:01:44.136188: step 8823, loss 4.54165.
Train: 2018-08-02T15:01:44.198702: step 8824, loss 3.27399.
Train: 2018-08-02T15:01:44.261158: step 8825, loss 5.79244.
Train: 2018-08-02T15:01:44.323669: step 8826, loss 4.28137.
Train: 2018-08-02T15:01:44.370533: step 8827, loss 2.26661.
Train: 2018-08-02T15:01:44.433022: step 8828, loss 4.65914.
Train: 2018-08-02T15:01:44.495481: step 8829, loss 2.64437.
Train: 2018-08-02T15:01:44.542342: step 8830, loss 4.28137.
Test: 2018-08-02T15:01:44.745419: step 8830, loss 3.81863.
Train: 2018-08-02T15:01:44.807937: step 8831, loss 3.77768.
Train: 2018-08-02T15:01:44.870422: step 8832, loss 4.53321.
Train: 2018-08-02T15:01:44.932876: step 8833, loss 3.65176.
Train: 2018-08-02T15:01:44.979772: step 8834, loss 3.65176.
Train: 2018-08-02T15:01:45.042226: step 8835, loss 3.77768.
Train: 2018-08-02T15:01:45.104737: step 8836, loss 4.28137.
Train: 2018-08-02T15:01:45.167227: step 8837, loss 3.14807.
Train: 2018-08-02T15:01:45.214089: step 8838, loss 5.41467.
Train: 2018-08-02T15:01:45.276572: step 8839, loss 3.39991.
Train: 2018-08-02T15:01:45.339032: step 8840, loss 4.65914.
Test: 2018-08-02T15:01:45.526517: step 8840, loss 3.81863.
Train: 2018-08-02T15:01:45.589005: step 8841, loss 3.39991.
Train: 2018-08-02T15:01:45.651488: step 8842, loss 5.16283.
Train: 2018-08-02T15:01:45.713975: step 8843, loss 3.9036.
Train: 2018-08-02T15:01:45.760809: step 8844, loss 4.15545.
Train: 2018-08-02T15:01:45.823322: step 8845, loss 4.15545.
Train: 2018-08-02T15:01:45.885803: step 8846, loss 4.65914.
Train: 2018-08-02T15:01:45.932643: step 8847, loss 2.89622.
Train: 2018-08-02T15:01:45.995131: step 8848, loss 3.52583.
Train: 2018-08-02T15:01:46.057615: step 8849, loss 4.78506.
Train: 2018-08-02T15:01:46.120128: step 8850, loss 3.27399.
Test: 2018-08-02T15:01:46.323176: step 8850, loss 3.81863.
Train: 2018-08-02T15:01:46.370071: step 8851, loss 4.28137.
Train: 2018-08-02T15:01:46.432524: step 8852, loss 3.65176.
Train: 2018-08-02T15:01:46.495010: step 8853, loss 3.14807.
Train: 2018-08-02T15:01:46.557496: step 8854, loss 4.15545.
Train: 2018-08-02T15:01:46.604367: step 8855, loss 3.77768.
Train: 2018-08-02T15:01:46.666845: step 8856, loss 4.40729.
Train: 2018-08-02T15:01:46.729331: step 8857, loss 4.15545.
Train: 2018-08-02T15:01:46.776196: step 8858, loss 3.39991.
Train: 2018-08-02T15:01:46.838681: step 8859, loss 3.77768.
Train: 2018-08-02T15:01:46.901195: step 8860, loss 5.0369.
Test: 2018-08-02T15:01:47.088623: step 8860, loss 3.81863.
Train: 2018-08-02T15:01:47.151137: step 8861, loss 4.02952.
Train: 2018-08-02T15:01:47.213624: step 8862, loss 5.66652.
Train: 2018-08-02T15:01:47.276077: step 8863, loss 4.65914.
Train: 2018-08-02T15:01:47.338591: step 8864, loss 3.27399.
Train: 2018-08-02T15:01:47.385458: step 8865, loss 3.52583.
Train: 2018-08-02T15:01:47.447943: step 8866, loss 4.40729.
Train: 2018-08-02T15:01:47.510428: step 8867, loss 3.27411.
Train: 2018-08-02T15:01:47.572930: step 8868, loss 3.27399.
Train: 2018-08-02T15:01:47.619747: step 8869, loss 4.15545.
Train: 2018-08-02T15:01:47.682263: step 8870, loss 2.90182.
Test: 2018-08-02T15:01:47.869719: step 8870, loss 3.81863.
Train: 2018-08-02T15:01:47.932174: step 8871, loss 3.27399.
Train: 2018-08-02T15:01:47.994689: step 8872, loss 4.40729.
Train: 2018-08-02T15:01:48.057146: step 8873, loss 3.77768.
Train: 2018-08-02T15:01:48.119633: step 8874, loss 4.65914.
Train: 2018-08-02T15:01:48.166495: step 8875, loss 4.40729.
Train: 2018-08-02T15:01:48.228980: step 8876, loss 3.65176.
Train: 2018-08-02T15:01:48.291495: step 8877, loss 4.40729.
Train: 2018-08-02T15:01:48.338361: step 8878, loss 4.02952.
Train: 2018-08-02T15:01:48.400815: step 8879, loss 4.65914.
Train: 2018-08-02T15:01:48.463300: step 8880, loss 3.52583.
Test: 2018-08-02T15:01:48.666409: step 8880, loss 3.81863.
Train: 2018-08-02T15:01:48.728863: step 8881, loss 4.28137.
Train: 2018-08-02T15:01:48.791378: step 8882, loss 3.77768.
Train: 2018-08-02T15:01:48.838212: step 8883, loss 3.9036.
Train: 2018-08-02T15:01:48.900728: step 8884, loss 3.14807.
Train: 2018-08-02T15:01:48.963215: step 8885, loss 3.52583.
Train: 2018-08-02T15:01:49.010078: step 8886, loss 4.65914.
Train: 2018-08-02T15:01:49.072557: step 8887, loss 4.28137.
Train: 2018-08-02T15:01:49.135044: step 8888, loss 3.52583.
Train: 2018-08-02T15:01:49.197503: step 8889, loss 4.40729.
Train: 2018-08-02T15:01:49.244367: step 8890, loss 3.27399.
Test: 2018-08-02T15:01:49.447477: step 8890, loss 3.81863.
Train: 2018-08-02T15:01:49.509955: step 8891, loss 2.89622.
Train: 2018-08-02T15:01:49.572443: step 8892, loss 4.28137.
Train: 2018-08-02T15:01:49.634925: step 8893, loss 3.77768.
Train: 2018-08-02T15:01:49.681795: step 8894, loss 4.78506.
Train: 2018-08-02T15:01:49.744251: step 8895, loss 4.40729.
Train: 2018-08-02T15:01:49.806736: step 8896, loss 5.28875.
Train: 2018-08-02T15:01:49.853630: step 8897, loss 3.77768.
Train: 2018-08-02T15:01:49.916119: step 8898, loss 3.9036.
Train: 2018-08-02T15:01:49.978631: step 8899, loss 4.28137.
Train: 2018-08-02T15:01:50.041056: step 8900, loss 3.77768.
Test: 2018-08-02T15:01:50.228541: step 8900, loss 3.81863.
Train: 2018-08-02T15:01:50.744046: step 8901, loss 5.16283.
Train: 2018-08-02T15:01:50.806534: step 8902, loss 4.15545.
Train: 2018-08-02T15:01:50.868987: step 8903, loss 3.65176.
Train: 2018-08-02T15:01:50.931498: step 8904, loss 3.9036.
Train: 2018-08-02T15:01:50.978338: step 8905, loss 5.66652.
Train: 2018-08-02T15:01:51.040821: step 8906, loss 3.27399.
Train: 2018-08-02T15:01:51.103336: step 8907, loss 3.77768.
Train: 2018-08-02T15:01:51.150170: step 8908, loss 3.65176.
Train: 2018-08-02T15:01:51.197062: step 8909, loss 3.49225.
Train: 2018-08-02T15:01:51.259550: step 8910, loss 4.28137.
Test: 2018-08-02T15:01:51.462629: step 8910, loss 3.81863.
Train: 2018-08-02T15:01:51.525082: step 8911, loss 4.15545.
Train: 2018-08-02T15:01:51.571979: step 8912, loss 4.65914.
Train: 2018-08-02T15:01:51.634462: step 8913, loss 4.02952.
Train: 2018-08-02T15:01:51.696950: step 8914, loss 3.65176.
Train: 2018-08-02T15:01:51.759436: step 8915, loss 4.53321.
Train: 2018-08-02T15:01:51.806299: step 8916, loss 3.77768.
Train: 2018-08-02T15:01:51.868782: step 8917, loss 3.52583.
Train: 2018-08-02T15:01:51.931268: step 8918, loss 4.53321.
Train: 2018-08-02T15:01:51.993753: step 8919, loss 4.15545.
Train: 2018-08-02T15:01:52.040587: step 8920, loss 5.0369.
Test: 2018-08-02T15:01:52.243665: step 8920, loss 3.81863.
Train: 2018-08-02T15:01:52.306151: step 8921, loss 3.9036.
Train: 2018-08-02T15:01:52.368635: step 8922, loss 4.40729.
Train: 2018-08-02T15:01:52.415499: step 8923, loss 4.78506.
Train: 2018-08-02T15:01:52.478013: step 8924, loss 4.28137.
Train: 2018-08-02T15:01:52.540501: step 8925, loss 4.40729.
Train: 2018-08-02T15:01:52.587335: step 8926, loss 3.27399.
Train: 2018-08-02T15:01:52.649819: step 8927, loss 4.28137.
Train: 2018-08-02T15:01:52.712307: step 8928, loss 4.65914.
Train: 2018-08-02T15:01:52.774821: step 8929, loss 4.40729.
Train: 2018-08-02T15:01:52.821654: step 8930, loss 3.27399.
Test: 2018-08-02T15:01:53.024730: step 8930, loss 3.81863.
Train: 2018-08-02T15:01:53.087249: step 8931, loss 5.0369.
Train: 2018-08-02T15:01:53.149702: step 8932, loss 3.9036.
Train: 2018-08-02T15:01:53.212188: step 8933, loss 3.27399.
Train: 2018-08-02T15:01:53.259077: step 8934, loss 3.9036.
Train: 2018-08-02T15:01:53.321539: step 8935, loss 4.28137.
Train: 2018-08-02T15:01:53.384052: step 8936, loss 3.9036.
Train: 2018-08-02T15:01:53.430920: step 8937, loss 4.53321.
Train: 2018-08-02T15:01:53.493403: step 8938, loss 3.65176.
Train: 2018-08-02T15:01:53.555858: step 8939, loss 3.65176.
Train: 2018-08-02T15:01:53.602722: step 8940, loss 5.0369.
Test: 2018-08-02T15:01:53.805823: step 8940, loss 3.81863.
Train: 2018-08-02T15:01:53.868285: step 8941, loss 3.52583.
Train: 2018-08-02T15:01:53.930769: step 8942, loss 4.40729.
Train: 2018-08-02T15:01:53.993279: step 8943, loss 4.28137.
Train: 2018-08-02T15:01:54.040119: step 8944, loss 3.65176.
Train: 2018-08-02T15:01:54.102607: step 8945, loss 4.53321.
Train: 2018-08-02T15:01:54.165090: step 8946, loss 4.78506.
Train: 2018-08-02T15:01:54.227605: step 8947, loss 4.91098.
Train: 2018-08-02T15:01:54.290086: step 8948, loss 4.28137.
Train: 2018-08-02T15:01:54.336925: step 8949, loss 4.02952.
Train: 2018-08-02T15:01:54.399439: step 8950, loss 4.28137.
Test: 2018-08-02T15:01:54.602512: step 8950, loss 3.81863.
Train: 2018-08-02T15:01:54.664998: step 8951, loss 4.91098.
Train: 2018-08-02T15:01:54.727471: step 8952, loss 4.28137.
Train: 2018-08-02T15:01:54.774353: step 8953, loss 4.28137.
Train: 2018-08-02T15:01:54.836809: step 8954, loss 4.40729.
Train: 2018-08-02T15:01:54.899324: step 8955, loss 4.78506.
Train: 2018-08-02T15:01:54.946184: step 8956, loss 3.52583.
Train: 2018-08-02T15:01:55.008642: step 8957, loss 4.02952.
Train: 2018-08-02T15:01:55.071156: step 8958, loss 3.39991.
Train: 2018-08-02T15:01:55.133613: step 8959, loss 4.28137.
Train: 2018-08-02T15:01:55.180510: step 8960, loss 3.02214.
Test: 2018-08-02T15:01:55.383578: step 8960, loss 3.81863.
Train: 2018-08-02T15:01:55.446038: step 8961, loss 3.02214.
Train: 2018-08-02T15:01:55.508549: step 8962, loss 4.15545.
Train: 2018-08-02T15:01:55.555389: step 8963, loss 3.77768.
Train: 2018-08-02T15:01:55.617914: step 8964, loss 4.28137.
Train: 2018-08-02T15:01:55.680361: step 8965, loss 4.53321.
Train: 2018-08-02T15:01:55.742845: step 8966, loss 3.39991.
Train: 2018-08-02T15:01:55.789709: step 8967, loss 3.52583.
Train: 2018-08-02T15:01:55.852204: step 8968, loss 3.77768.
Train: 2018-08-02T15:01:55.914711: step 8969, loss 4.40729.
Train: 2018-08-02T15:01:55.961577: step 8970, loss 4.65914.
Test: 2018-08-02T15:01:56.164620: step 8970, loss 3.81863.
Train: 2018-08-02T15:01:56.227136: step 8971, loss 4.91098.
Train: 2018-08-02T15:01:56.289591: step 8972, loss 3.9036.
Train: 2018-08-02T15:01:56.352107: step 8973, loss 3.77768.
Train: 2018-08-02T15:01:56.398942: step 8974, loss 4.53321.
Train: 2018-08-02T15:01:56.461460: step 8975, loss 4.78506.
Train: 2018-08-02T15:01:56.523942: step 8976, loss 3.9036.
Train: 2018-08-02T15:01:56.570775: step 8977, loss 3.9036.
Train: 2018-08-02T15:01:56.633288: step 8978, loss 2.7703.
Train: 2018-08-02T15:01:56.695775: step 8979, loss 3.77768.
Train: 2018-08-02T15:01:56.742610: step 8980, loss 4.53321.
Test: 2018-08-02T15:01:56.945720: step 8980, loss 3.81863.
Train: 2018-08-02T15:01:57.008204: step 8981, loss 4.02952.
Train: 2018-08-02T15:01:57.070658: step 8982, loss 3.9036.
Train: 2018-08-02T15:01:57.117557: step 8983, loss 3.77768.
Train: 2018-08-02T15:01:57.180010: step 8984, loss 4.28137.
Train: 2018-08-02T15:01:57.242520: step 8985, loss 3.65176.
Train: 2018-08-02T15:01:57.289359: step 8986, loss 4.02952.
Train: 2018-08-02T15:01:57.351874: step 8987, loss 3.39991.
Train: 2018-08-02T15:01:57.414359: step 8988, loss 4.28137.
Train: 2018-08-02T15:01:57.476834: step 8989, loss 3.9036.
Train: 2018-08-02T15:01:57.523679: step 8990, loss 5.16283.
Test: 2018-08-02T15:01:57.726785: step 8990, loss 3.81863.
Train: 2018-08-02T15:01:57.789274: step 8991, loss 3.27399.
Train: 2018-08-02T15:01:57.851757: step 8992, loss 3.27399.
Train: 2018-08-02T15:01:57.898621: step 8993, loss 3.77768.
Train: 2018-08-02T15:01:57.961076: step 8994, loss 4.40729.
Train: 2018-08-02T15:01:58.023592: step 8995, loss 3.77768.
Train: 2018-08-02T15:01:58.070453: step 8996, loss 5.28875.
Train: 2018-08-02T15:01:58.132936: step 8997, loss 4.91098.
Train: 2018-08-02T15:01:58.195424: step 8998, loss 3.9036.
Train: 2018-08-02T15:01:58.257883: step 8999, loss 4.28137.
Train: 2018-08-02T15:01:58.304771: step 9000, loss 3.77768.
Test: 2018-08-02T15:01:58.507852: step 9000, loss 3.81863.
Train: 2018-08-02T15:01:59.023356: step 9001, loss 4.02952.
Train: 2018-08-02T15:01:59.085813: step 9002, loss 3.14807.
Train: 2018-08-02T15:01:59.148322: step 9003, loss 4.28137.
Train: 2018-08-02T15:01:59.195189: step 9004, loss 4.40729.
Train: 2018-08-02T15:01:59.257677: step 9005, loss 4.65914.
Train: 2018-08-02T15:01:59.320132: step 9006, loss 3.65176.
Train: 2018-08-02T15:01:59.382647: step 9007, loss 4.15545.
Train: 2018-08-02T15:01:59.429508: step 9008, loss 4.28137.
Train: 2018-08-02T15:01:59.491993: step 9009, loss 3.14807.
Train: 2018-08-02T15:01:59.554488: step 9010, loss 4.65914.
Test: 2018-08-02T15:01:59.741907: step 9010, loss 3.81863.
Train: 2018-08-02T15:01:59.804393: step 9011, loss 4.02952.
Train: 2018-08-02T15:01:59.866910: step 9012, loss 4.53321.
Train: 2018-08-02T15:01:59.929364: step 9013, loss 3.77768.
Train: 2018-08-02T15:01:59.991851: step 9014, loss 4.15545.
Train: 2018-08-02T15:02:00.054335: step 9015, loss 4.40729.
Train: 2018-08-02T15:02:00.101199: step 9016, loss 3.9036.
Train: 2018-08-02T15:02:00.163715: step 9017, loss 3.77768.
Train: 2018-08-02T15:02:00.210581: step 9018, loss 4.40729.
Train: 2018-08-02T15:02:00.273065: step 9019, loss 4.28137.
Train: 2018-08-02T15:02:00.335519: step 9020, loss 3.65176.
Test: 2018-08-02T15:02:00.523001: step 9020, loss 3.81863.
Train: 2018-08-02T15:02:00.585462: step 9021, loss 4.78506.
Train: 2018-08-02T15:02:00.647976: step 9022, loss 4.40729.
Train: 2018-08-02T15:02:00.710431: step 9023, loss 3.39991.
Train: 2018-08-02T15:02:00.772917: step 9024, loss 3.39991.
Train: 2018-08-02T15:02:00.819783: step 9025, loss 3.14807.
Train: 2018-08-02T15:02:00.882297: step 9026, loss 3.39991.
Train: 2018-08-02T15:02:00.944784: step 9027, loss 3.14807.
Train: 2018-08-02T15:02:01.007263: step 9028, loss 3.77768.
Train: 2018-08-02T15:02:01.054102: step 9029, loss 3.77768.
Train: 2018-08-02T15:02:01.116587: step 9030, loss 2.64438.
Test: 2018-08-02T15:02:01.319663: step 9030, loss 3.81863.
Train: 2018-08-02T15:02:01.382151: step 9031, loss 4.02952.
Train: 2018-08-02T15:02:01.429037: step 9032, loss 2.7703.
Train: 2018-08-02T15:02:01.491529: step 9033, loss 3.9036.
Train: 2018-08-02T15:02:01.554016: step 9034, loss 3.65176.
Train: 2018-08-02T15:02:01.616501: step 9035, loss 3.77768.
Train: 2018-08-02T15:02:01.663364: step 9036, loss 3.65176.
Train: 2018-08-02T15:02:01.725818: step 9037, loss 4.02952.
Train: 2018-08-02T15:02:01.788306: step 9038, loss 4.02952.
Train: 2018-08-02T15:02:01.835169: step 9039, loss 2.89622.
Train: 2018-08-02T15:02:01.897683: step 9040, loss 3.27399.
Test: 2018-08-02T15:02:02.100755: step 9040, loss 3.81863.
Train: 2018-08-02T15:02:02.163217: step 9041, loss 3.39991.
Train: 2018-08-02T15:02:02.210113: step 9042, loss 4.15545.
Train: 2018-08-02T15:02:02.272591: step 9043, loss 3.02214.
Train: 2018-08-02T15:02:02.335088: step 9044, loss 5.66652.
Train: 2018-08-02T15:02:02.397537: step 9045, loss 4.28137.
Train: 2018-08-02T15:02:02.444428: step 9046, loss 3.77768.
Train: 2018-08-02T15:02:02.506911: step 9047, loss 4.78506.
Train: 2018-08-02T15:02:02.569372: step 9048, loss 4.78506.
Train: 2018-08-02T15:02:02.616236: step 9049, loss 3.9036.
Train: 2018-08-02T15:02:02.678751: step 9050, loss 3.65176.
Test: 2018-08-02T15:02:02.881797: step 9050, loss 3.81863.
Train: 2018-08-02T15:02:02.944283: step 9051, loss 3.9036.
Train: 2018-08-02T15:02:03.006769: step 9052, loss 3.9036.
Train: 2018-08-02T15:02:03.053666: step 9053, loss 3.52583.
Train: 2018-08-02T15:02:03.116150: step 9054, loss 4.53321.
Train: 2018-08-02T15:02:03.178634: step 9055, loss 3.9036.
Train: 2018-08-02T15:02:03.241089: step 9056, loss 2.89622.
Train: 2018-08-02T15:02:03.287983: step 9057, loss 3.77768.
Train: 2018-08-02T15:02:03.350439: step 9058, loss 3.77768.
Train: 2018-08-02T15:02:03.412942: step 9059, loss 3.27399.
Train: 2018-08-02T15:02:03.444194: step 9060, loss 3.22362.
Test: 2018-08-02T15:02:03.647278: step 9060, loss 3.81863.
Train: 2018-08-02T15:02:03.709730: step 9061, loss 3.65176.
Train: 2018-08-02T15:02:03.772240: step 9062, loss 4.28137.
Train: 2018-08-02T15:02:03.834732: step 9063, loss 3.65176.
Train: 2018-08-02T15:02:03.897185: step 9064, loss 3.52583.
Train: 2018-08-02T15:02:03.944050: step 9065, loss 3.77768.
Train: 2018-08-02T15:02:04.006560: step 9066, loss 3.77768.
Train: 2018-08-02T15:02:04.069050: step 9067, loss 3.39991.
Train: 2018-08-02T15:02:04.115922: step 9068, loss 3.02214.
Train: 2018-08-02T15:02:04.178369: step 9069, loss 3.52583.
Train: 2018-08-02T15:02:04.240885: step 9070, loss 4.53321.
Test: 2018-08-02T15:02:04.443933: step 9070, loss 3.81863.
Train: 2018-08-02T15:02:04.490827: step 9071, loss 3.9036.
Train: 2018-08-02T15:02:04.553315: step 9072, loss 4.40729.
Train: 2018-08-02T15:02:04.615798: step 9073, loss 4.15545.
Train: 2018-08-02T15:02:04.678253: step 9074, loss 4.65914.
Train: 2018-08-02T15:02:04.725147: step 9075, loss 3.14807.
Train: 2018-08-02T15:02:04.787631: step 9076, loss 3.39991.
Train: 2018-08-02T15:02:04.850113: step 9077, loss 3.27399.
Train: 2018-08-02T15:02:04.896951: step 9078, loss 4.65914.
Train: 2018-08-02T15:02:04.959469: step 9079, loss 4.02952.
Train: 2018-08-02T15:02:05.021952: step 9080, loss 3.39991.
Test: 2018-08-02T15:02:05.224999: step 9080, loss 3.81863.
Train: 2018-08-02T15:02:05.271895: step 9081, loss 3.9036.
Train: 2018-08-02T15:02:05.334381: step 9082, loss 3.52583.
Train: 2018-08-02T15:02:05.396865: step 9083, loss 5.0369.
Train: 2018-08-02T15:02:05.459322: step 9084, loss 5.16283.
Train: 2018-08-02T15:02:05.521835: step 9085, loss 5.28875.
Train: 2018-08-02T15:02:05.568668: step 9086, loss 4.02952.
Train: 2018-08-02T15:02:05.631185: step 9087, loss 3.14807.
Train: 2018-08-02T15:02:05.693669: step 9088, loss 3.52583.
Train: 2018-08-02T15:02:05.740534: step 9089, loss 3.9036.
Train: 2018-08-02T15:02:05.803022: step 9090, loss 4.02952.
Test: 2018-08-02T15:02:06.006066: step 9090, loss 3.81863.
Train: 2018-08-02T15:02:06.068551: step 9091, loss 4.78506.
Train: 2018-08-02T15:02:06.115415: step 9092, loss 4.78506.
Train: 2018-08-02T15:02:06.177928: step 9093, loss 4.02952.
Train: 2018-08-02T15:02:06.240388: step 9094, loss 3.65176.
Train: 2018-08-02T15:02:06.302872: step 9095, loss 4.65914.
Train: 2018-08-02T15:02:06.349770: step 9096, loss 3.77768.
Train: 2018-08-02T15:02:06.412222: step 9097, loss 4.40729.
Train: 2018-08-02T15:02:06.459118: step 9098, loss 4.02952.
Train: 2018-08-02T15:02:06.521602: step 9099, loss 3.65176.
Train: 2018-08-02T15:02:06.584088: step 9100, loss 3.52583.
Test: 2018-08-02T15:02:06.787135: step 9100, loss 3.81863.
Train: 2018-08-02T15:02:07.302668: step 9101, loss 4.65914.
Train: 2018-08-02T15:02:07.365153: step 9102, loss 4.02952.
Train: 2018-08-02T15:02:07.411988: step 9103, loss 4.28137.
Train: 2018-08-02T15:02:07.474473: step 9104, loss 5.16283.
Train: 2018-08-02T15:02:07.536960: step 9105, loss 4.15545.
Train: 2018-08-02T15:02:07.583855: step 9106, loss 4.02952.
Train: 2018-08-02T15:02:07.646338: step 9107, loss 3.9036.
Train: 2018-08-02T15:02:07.708794: step 9108, loss 4.53321.
Train: 2018-08-02T15:02:07.771280: step 9109, loss 3.9036.
Train: 2018-08-02T15:02:07.818172: step 9110, loss 3.77768.
Test: 2018-08-02T15:02:08.021220: step 9110, loss 3.81863.
Train: 2018-08-02T15:02:08.068112: step 9111, loss 3.77768.
Train: 2018-08-02T15:02:08.130600: step 9112, loss 4.65914.
Train: 2018-08-02T15:02:08.193055: step 9113, loss 4.15545.
Train: 2018-08-02T15:02:08.255571: step 9114, loss 4.28137.
Train: 2018-08-02T15:02:08.318025: step 9115, loss 3.52583.
Train: 2018-08-02T15:02:08.364889: step 9116, loss 4.53321.
Train: 2018-08-02T15:02:08.427375: step 9117, loss 4.40729.
Train: 2018-08-02T15:02:08.489891: step 9118, loss 4.04348.
Train: 2018-08-02T15:02:08.536754: step 9119, loss 4.15545.
Train: 2018-08-02T15:02:08.599210: step 9120, loss 4.65914.
Test: 2018-08-02T15:02:08.802287: step 9120, loss 3.81863.
Train: 2018-08-02T15:02:08.864802: step 9121, loss 3.27399.
Train: 2018-08-02T15:02:08.911666: step 9122, loss 3.39991.
Train: 2018-08-02T15:02:08.974152: step 9123, loss 3.65176.
Train: 2018-08-02T15:02:09.036608: step 9124, loss 4.28137.
Train: 2018-08-02T15:02:09.099122: step 9125, loss 4.28137.
Train: 2018-08-02T15:02:09.145982: step 9126, loss 4.15545.
Train: 2018-08-02T15:02:09.208466: step 9127, loss 3.65176.
Train: 2018-08-02T15:02:09.270959: step 9128, loss 3.65176.
Train: 2018-08-02T15:02:09.317792: step 9129, loss 3.52583.
Train: 2018-08-02T15:02:09.380314: step 9130, loss 4.02952.
Test: 2018-08-02T15:02:09.583354: step 9130, loss 3.81863.
Train: 2018-08-02T15:02:09.645839: step 9131, loss 3.65176.
Train: 2018-08-02T15:02:09.708356: step 9132, loss 4.78506.
Train: 2018-08-02T15:02:09.755222: step 9133, loss 4.02952.
Train: 2018-08-02T15:02:09.817675: step 9134, loss 3.9036.
Train: 2018-08-02T15:02:09.880191: step 9135, loss 4.53321.
Train: 2018-08-02T15:02:09.942645: step 9136, loss 3.39991.
Train: 2018-08-02T15:02:09.989510: step 9137, loss 3.39991.
Train: 2018-08-02T15:02:10.052020: step 9138, loss 2.7703.
Train: 2018-08-02T15:02:10.114509: step 9139, loss 4.28137.
Train: 2018-08-02T15:02:10.161373: step 9140, loss 4.02952.
Test: 2018-08-02T15:02:10.364446: step 9140, loss 3.81863.
Train: 2018-08-02T15:02:10.426905: step 9141, loss 3.39991.
Train: 2018-08-02T15:02:10.489421: step 9142, loss 5.41467.
Train: 2018-08-02T15:02:10.551907: step 9143, loss 4.28137.
Train: 2018-08-02T15:02:10.598773: step 9144, loss 3.52583.
Train: 2018-08-02T15:02:10.661253: step 9145, loss 3.9036.
Train: 2018-08-02T15:02:10.723742: step 9146, loss 4.15545.
Train: 2018-08-02T15:02:10.770575: step 9147, loss 4.02952.
Train: 2018-08-02T15:02:10.833062: step 9148, loss 3.65176.
Train: 2018-08-02T15:02:10.895579: step 9149, loss 3.65176.
Train: 2018-08-02T15:02:10.958032: step 9150, loss 3.65176.
Test: 2018-08-02T15:02:11.145513: step 9150, loss 3.81863.
Train: 2018-08-02T15:02:11.207982: step 9151, loss 3.9036.
Train: 2018-08-02T15:02:11.270459: step 9152, loss 3.52583.
Train: 2018-08-02T15:02:11.317324: step 9153, loss 3.77768.
Train: 2018-08-02T15:02:11.379835: step 9154, loss 4.28137.
Train: 2018-08-02T15:02:11.442329: step 9155, loss 4.53321.
Train: 2018-08-02T15:02:11.489189: step 9156, loss 4.78506.
Train: 2018-08-02T15:02:11.551676: step 9157, loss 2.7703.
Train: 2018-08-02T15:02:11.614154: step 9158, loss 4.28137.
Train: 2018-08-02T15:02:11.661009: step 9159, loss 4.53321.
Train: 2018-08-02T15:02:11.723477: step 9160, loss 3.52583.
Test: 2018-08-02T15:02:11.926582: step 9160, loss 3.81863.
Train: 2018-08-02T15:02:11.989071: step 9161, loss 4.78506.
Train: 2018-08-02T15:02:12.051525: step 9162, loss 5.41467.
Train: 2018-08-02T15:02:12.098391: step 9163, loss 4.15545.
Train: 2018-08-02T15:02:12.160901: step 9164, loss 3.9036.
Train: 2018-08-02T15:02:12.223387: step 9165, loss 3.77768.
Train: 2018-08-02T15:02:12.285876: step 9166, loss 4.53321.
Train: 2018-08-02T15:02:12.332743: step 9167, loss 3.39991.
Train: 2018-08-02T15:02:12.395197: step 9168, loss 4.53321.
Train: 2018-08-02T15:02:12.457714: step 9169, loss 3.65176.
Train: 2018-08-02T15:02:12.504546: step 9170, loss 4.40729.
Test: 2018-08-02T15:02:12.707646: step 9170, loss 3.81863.
Train: 2018-08-02T15:02:12.770137: step 9171, loss 3.02214.
Train: 2018-08-02T15:02:12.832625: step 9172, loss 4.02952.
Train: 2018-08-02T15:02:12.879458: step 9173, loss 3.9036.
Train: 2018-08-02T15:02:12.941944: step 9174, loss 3.39991.
Train: 2018-08-02T15:02:13.004458: step 9175, loss 5.5406.
Train: 2018-08-02T15:02:13.051324: step 9176, loss 3.77768.
Train: 2018-08-02T15:02:13.113777: step 9177, loss 4.65914.
Train: 2018-08-02T15:02:13.176264: step 9178, loss 4.53321.
Train: 2018-08-02T15:02:13.223152: step 9179, loss 2.7703.
Train: 2018-08-02T15:02:13.285613: step 9180, loss 4.28137.
Test: 2018-08-02T15:02:13.488690: step 9180, loss 3.81863.
Train: 2018-08-02T15:02:13.551175: step 9181, loss 3.77768.
Train: 2018-08-02T15:02:13.598038: step 9182, loss 3.65176.
Train: 2018-08-02T15:02:13.660549: step 9183, loss 5.28875.
Train: 2018-08-02T15:02:13.723036: step 9184, loss 3.27399.
Train: 2018-08-02T15:02:13.785495: step 9185, loss 4.28137.
Train: 2018-08-02T15:02:13.832359: step 9186, loss 4.15545.
Train: 2018-08-02T15:02:13.894875: step 9187, loss 3.27399.
Train: 2018-08-02T15:02:13.941741: step 9188, loss 4.65914.
Train: 2018-08-02T15:02:14.004220: step 9189, loss 3.27399.
Train: 2018-08-02T15:02:14.066707: step 9190, loss 4.28137.
Test: 2018-08-02T15:02:14.254159: step 9190, loss 3.81863.
Train: 2018-08-02T15:02:14.316648: step 9191, loss 4.53321.
Train: 2018-08-02T15:02:14.379106: step 9192, loss 3.65176.
Train: 2018-08-02T15:02:14.441621: step 9193, loss 4.02952.
Train: 2018-08-02T15:02:14.504077: step 9194, loss 2.64438.
Train: 2018-08-02T15:02:14.566595: step 9195, loss 5.16283.
Train: 2018-08-02T15:02:14.613456: step 9196, loss 3.27399.
Train: 2018-08-02T15:02:14.675911: step 9197, loss 3.77768.
Train: 2018-08-02T15:02:14.738427: step 9198, loss 3.65176.
Train: 2018-08-02T15:02:14.785262: step 9199, loss 4.15545.
Train: 2018-08-02T15:02:14.847747: step 9200, loss 3.14807.
Test: 2018-08-02T15:02:15.050822: step 9200, loss 3.81863.
Train: 2018-08-02T15:02:15.550739: step 9201, loss 4.53321.
Train: 2018-08-02T15:02:15.613218: step 9202, loss 4.02952.
Train: 2018-08-02T15:02:15.671117: step 9203, loss 4.02952.
Train: 2018-08-02T15:02:15.733578: step 9204, loss 3.39991.
Train: 2018-08-02T15:02:15.796064: step 9205, loss 4.65914.
Train: 2018-08-02T15:02:15.842958: step 9206, loss 4.15545.
Train: 2018-08-02T15:02:15.905413: step 9207, loss 3.9036.
Train: 2018-08-02T15:02:15.967928: step 9208, loss 4.78506.
Train: 2018-08-02T15:02:16.014793: step 9209, loss 3.27399.
Train: 2018-08-02T15:02:16.077277: step 9210, loss 4.28137.
Test: 2018-08-02T15:02:16.280354: step 9210, loss 3.81863.
Train: 2018-08-02T15:02:16.327218: step 9211, loss 5.10406.
Train: 2018-08-02T15:02:16.389704: step 9212, loss 2.64438.
Train: 2018-08-02T15:02:16.452184: step 9213, loss 3.39991.
Train: 2018-08-02T15:02:16.514675: step 9214, loss 4.53321.
Train: 2018-08-02T15:02:16.577167: step 9215, loss 3.52583.
Train: 2018-08-02T15:02:16.624025: step 9216, loss 5.0369.
Train: 2018-08-02T15:02:16.686504: step 9217, loss 4.28137.
Train: 2018-08-02T15:02:16.748968: step 9218, loss 4.40729.
Train: 2018-08-02T15:02:16.795852: step 9219, loss 4.53321.
Train: 2018-08-02T15:02:16.858344: step 9220, loss 3.65176.
Test: 2018-08-02T15:02:17.061392: step 9220, loss 3.81863.
Train: 2018-08-02T15:02:17.123908: step 9221, loss 3.39991.
Train: 2018-08-02T15:02:17.170741: step 9222, loss 3.77768.
Train: 2018-08-02T15:02:17.233225: step 9223, loss 3.65176.
Train: 2018-08-02T15:02:17.295743: step 9224, loss 3.39991.
Train: 2018-08-02T15:02:17.358223: step 9225, loss 3.9036.
Train: 2018-08-02T15:02:17.405092: step 9226, loss 3.9036.
Train: 2018-08-02T15:02:17.467546: step 9227, loss 3.02214.
Train: 2018-08-02T15:02:17.530033: step 9228, loss 5.0369.
Train: 2018-08-02T15:02:17.576895: step 9229, loss 4.53321.
Train: 2018-08-02T15:02:17.639412: step 9230, loss 2.89622.
Test: 2018-08-02T15:02:17.842483: step 9230, loss 3.81863.
Train: 2018-08-02T15:02:17.904974: step 9231, loss 4.78506.
Train: 2018-08-02T15:02:17.951807: step 9232, loss 3.65176.
Train: 2018-08-02T15:02:18.014324: step 9233, loss 3.9036.
Train: 2018-08-02T15:02:18.076805: step 9234, loss 4.02952.
Train: 2018-08-02T15:02:18.123642: step 9235, loss 3.9036.
Train: 2018-08-02T15:02:18.186153: step 9236, loss 5.91836.
Train: 2018-08-02T15:02:18.248637: step 9237, loss 3.39991.
Train: 2018-08-02T15:02:18.295508: step 9238, loss 3.9036.
Train: 2018-08-02T15:02:18.357963: step 9239, loss 4.28137.
Train: 2018-08-02T15:02:18.420480: step 9240, loss 4.02952.
Test: 2018-08-02T15:02:18.623526: step 9240, loss 3.81863.
Train: 2018-08-02T15:02:18.670420: step 9241, loss 4.02952.
Train: 2018-08-02T15:02:18.732908: step 9242, loss 3.77768.
Train: 2018-08-02T15:02:18.795386: step 9243, loss 3.9036.
Train: 2018-08-02T15:02:18.857851: step 9244, loss 4.53321.
Train: 2018-08-02T15:02:18.920357: step 9245, loss 3.27399.
Train: 2018-08-02T15:02:18.967220: step 9246, loss 3.27399.
Train: 2018-08-02T15:02:19.029710: step 9247, loss 4.15545.
Train: 2018-08-02T15:02:19.092196: step 9248, loss 4.65914.
Train: 2018-08-02T15:02:19.154682: step 9249, loss 3.52583.
Train: 2018-08-02T15:02:19.201574: step 9250, loss 3.77768.
Test: 2018-08-02T15:02:19.404617: step 9250, loss 3.81863.
Train: 2018-08-02T15:02:19.467109: step 9251, loss 3.39991.
Train: 2018-08-02T15:02:19.529563: step 9252, loss 4.65914.
Train: 2018-08-02T15:02:19.592073: step 9253, loss 4.02952.
Train: 2018-08-02T15:02:19.654559: step 9254, loss 4.02952.
Train: 2018-08-02T15:02:19.701424: step 9255, loss 4.78506.
Train: 2018-08-02T15:02:19.763882: step 9256, loss 3.14807.
Train: 2018-08-02T15:02:19.826398: step 9257, loss 3.27399.
Train: 2018-08-02T15:02:19.888854: step 9258, loss 3.65176.
Train: 2018-08-02T15:02:19.935749: step 9259, loss 3.52583.
Train: 2018-08-02T15:02:19.998235: step 9260, loss 4.15545.
Test: 2018-08-02T15:02:20.201280: step 9260, loss 3.81863.
Train: 2018-08-02T15:02:20.263792: step 9261, loss 2.7703.
Train: 2018-08-02T15:02:20.326276: step 9262, loss 4.65914.
Train: 2018-08-02T15:02:20.373116: step 9263, loss 3.65176.
Train: 2018-08-02T15:02:20.435632: step 9264, loss 4.02952.
Train: 2018-08-02T15:02:20.498118: step 9265, loss 4.28137.
Train: 2018-08-02T15:02:20.544950: step 9266, loss 3.27399.
Train: 2018-08-02T15:02:20.607460: step 9267, loss 3.27399.
Train: 2018-08-02T15:02:20.669922: step 9268, loss 4.28137.
Train: 2018-08-02T15:02:20.732437: step 9269, loss 4.02952.
Train: 2018-08-02T15:02:20.779301: step 9270, loss 4.65914.
Test: 2018-08-02T15:02:20.982348: step 9270, loss 3.81863.
Train: 2018-08-02T15:02:21.044834: step 9271, loss 3.39991.
Train: 2018-08-02T15:02:21.107348: step 9272, loss 5.5406.
Train: 2018-08-02T15:02:21.169804: step 9273, loss 3.65176.
Train: 2018-08-02T15:02:21.216670: step 9274, loss 5.16283.
Train: 2018-08-02T15:02:21.279183: step 9275, loss 3.52583.
Train: 2018-08-02T15:02:21.341669: step 9276, loss 5.28875.
Train: 2018-08-02T15:02:21.388529: step 9277, loss 3.77768.
Train: 2018-08-02T15:02:21.450988: step 9278, loss 4.91098.
Train: 2018-08-02T15:02:21.513475: step 9279, loss 4.28137.
Train: 2018-08-02T15:02:21.560368: step 9280, loss 3.27399.
Test: 2018-08-02T15:02:21.763415: step 9280, loss 3.81863.
Train: 2018-08-02T15:02:21.825931: step 9281, loss 4.91098.
Train: 2018-08-02T15:02:21.888386: step 9282, loss 4.40729.
Train: 2018-08-02T15:02:21.935250: step 9283, loss 3.9036.
Train: 2018-08-02T15:02:21.997737: step 9284, loss 4.15545.
Train: 2018-08-02T15:02:22.060250: step 9285, loss 2.7703.
Train: 2018-08-02T15:02:22.122737: step 9286, loss 3.14807.
Train: 2018-08-02T15:02:22.169600: step 9287, loss 3.65176.
Train: 2018-08-02T15:02:22.232095: step 9288, loss 4.28137.
Train: 2018-08-02T15:02:22.294571: step 9289, loss 4.53321.
Train: 2018-08-02T15:02:22.341436: step 9290, loss 4.15545.
Test: 2018-08-02T15:02:22.544506: step 9290, loss 3.81863.
Train: 2018-08-02T15:02:22.606968: step 9291, loss 3.27399.
Train: 2018-08-02T15:02:22.669454: step 9292, loss 3.52583.
Train: 2018-08-02T15:02:22.731939: step 9293, loss 3.52583.
Train: 2018-08-02T15:02:22.778827: step 9294, loss 4.65914.
Train: 2018-08-02T15:02:22.841313: step 9295, loss 4.53321.
Train: 2018-08-02T15:02:22.903803: step 9296, loss 4.78506.
Train: 2018-08-02T15:02:22.950668: step 9297, loss 5.41467.
Train: 2018-08-02T15:02:23.013124: step 9298, loss 3.9036.
Train: 2018-08-02T15:02:23.075640: step 9299, loss 3.65176.
Train: 2018-08-02T15:02:23.122504: step 9300, loss 3.02214.
Test: 2018-08-02T15:02:23.325574: step 9300, loss 3.81863.
Train: 2018-08-02T15:02:23.841099: step 9301, loss 3.14807.
Train: 2018-08-02T15:02:23.887949: step 9302, loss 3.65176.
Train: 2018-08-02T15:02:23.950433: step 9303, loss 4.65914.
Train: 2018-08-02T15:02:24.012920: step 9304, loss 3.77768.
Train: 2018-08-02T15:02:24.075404: step 9305, loss 3.77768.
Train: 2018-08-02T15:02:24.137858: step 9306, loss 3.27399.
Train: 2018-08-02T15:02:24.184754: step 9307, loss 3.65176.
Train: 2018-08-02T15:02:24.247210: step 9308, loss 4.40729.
Train: 2018-08-02T15:02:24.309695: step 9309, loss 3.39991.
Train: 2018-08-02T15:02:24.356591: step 9310, loss 4.78506.
Test: 2018-08-02T15:02:24.559660: step 9310, loss 3.81863.
Train: 2018-08-02T15:02:24.622152: step 9311, loss 3.52583.
Train: 2018-08-02T15:02:24.684605: step 9312, loss 5.16283.
Train: 2018-08-02T15:02:24.747092: step 9313, loss 4.65914.
Train: 2018-08-02T15:02:24.793957: step 9314, loss 4.40729.
Train: 2018-08-02T15:02:24.856440: step 9315, loss 4.91098.
Train: 2018-08-02T15:02:24.918950: step 9316, loss 4.91098.
Train: 2018-08-02T15:02:24.965820: step 9317, loss 2.89622.
Train: 2018-08-02T15:02:25.028306: step 9318, loss 3.65176.
Train: 2018-08-02T15:02:25.090763: step 9319, loss 3.65176.
Train: 2018-08-02T15:02:25.137624: step 9320, loss 4.91098.
Test: 2018-08-02T15:02:25.340727: step 9320, loss 3.81863.
Train: 2018-08-02T15:02:25.403217: step 9321, loss 4.02952.
Train: 2018-08-02T15:02:25.465703: step 9322, loss 4.91098.
Train: 2018-08-02T15:02:25.528190: step 9323, loss 4.53321.
Train: 2018-08-02T15:02:25.575054: step 9324, loss 4.15545.
Train: 2018-08-02T15:02:25.637539: step 9325, loss 4.15545.
Train: 2018-08-02T15:02:25.700024: step 9326, loss 4.28137.
Train: 2018-08-02T15:02:25.746858: step 9327, loss 3.39991.
Train: 2018-08-02T15:02:25.809345: step 9328, loss 4.02952.
Train: 2018-08-02T15:02:25.871859: step 9329, loss 4.40729.
Train: 2018-08-02T15:02:25.918722: step 9330, loss 3.39991.
Test: 2018-08-02T15:02:26.121769: step 9330, loss 3.81863.
Train: 2018-08-02T15:02:26.184256: step 9331, loss 5.03691.
Train: 2018-08-02T15:02:26.246740: step 9332, loss 4.15545.
Train: 2018-08-02T15:02:26.309256: step 9333, loss 4.28137.
Train: 2018-08-02T15:02:26.356091: step 9334, loss 3.14807.
Train: 2018-08-02T15:02:26.418605: step 9335, loss 3.52583.
Train: 2018-08-02T15:02:26.481090: step 9336, loss 4.28137.
Train: 2018-08-02T15:02:26.527925: step 9337, loss 3.9036.
Train: 2018-08-02T15:02:26.590410: step 9338, loss 3.65176.
Train: 2018-08-02T15:02:26.652926: step 9339, loss 3.52583.
Train: 2018-08-02T15:02:26.699789: step 9340, loss 3.39991.
Test: 2018-08-02T15:02:26.902837: step 9340, loss 3.81863.
Train: 2018-08-02T15:02:26.965352: step 9341, loss 4.78506.
Train: 2018-08-02T15:02:27.027809: step 9342, loss 4.28137.
Train: 2018-08-02T15:02:27.090322: step 9343, loss 4.02952.
Train: 2018-08-02T15:02:27.152813: step 9344, loss 4.02952.
Train: 2018-08-02T15:02:27.199673: step 9345, loss 4.15545.
Train: 2018-08-02T15:02:27.262154: step 9346, loss 3.14807.
Train: 2018-08-02T15:02:27.324614: step 9347, loss 4.02952.
Train: 2018-08-02T15:02:27.387131: step 9348, loss 5.41467.
Train: 2018-08-02T15:02:27.433993: step 9349, loss 4.53321.
Train: 2018-08-02T15:02:27.496447: step 9350, loss 4.28137.
Test: 2018-08-02T15:02:27.699555: step 9350, loss 3.81863.
Train: 2018-08-02T15:02:27.762037: step 9351, loss 3.9036.
Train: 2018-08-02T15:02:27.808875: step 9352, loss 4.28137.
Train: 2018-08-02T15:02:27.871361: step 9353, loss 3.52583.
Train: 2018-08-02T15:02:27.933876: step 9354, loss 3.77768.
Train: 2018-08-02T15:02:27.996358: step 9355, loss 3.52583.
Train: 2018-08-02T15:02:28.043195: step 9356, loss 3.77768.
Train: 2018-08-02T15:02:28.105681: step 9357, loss 4.15545.
Train: 2018-08-02T15:02:28.168196: step 9358, loss 4.28137.
Train: 2018-08-02T15:02:28.215060: step 9359, loss 4.15545.
Train: 2018-08-02T15:02:28.277545: step 9360, loss 3.39991.
Test: 2018-08-02T15:02:28.480626: step 9360, loss 3.81863.
Train: 2018-08-02T15:02:28.543107: step 9361, loss 3.65176.
Train: 2018-08-02T15:02:28.574321: step 9362, loss 5.3727.
Train: 2018-08-02T15:02:28.636836: step 9363, loss 3.39991.
Train: 2018-08-02T15:02:28.699321: step 9364, loss 4.02952.
Train: 2018-08-02T15:02:28.761807: step 9365, loss 4.65914.
Train: 2018-08-02T15:02:28.808670: step 9366, loss 4.28137.
Train: 2018-08-02T15:02:28.871157: step 9367, loss 4.65914.
Train: 2018-08-02T15:02:28.933642: step 9368, loss 5.0369.
Train: 2018-08-02T15:02:28.980475: step 9369, loss 4.78506.
Train: 2018-08-02T15:02:29.042961: step 9370, loss 3.77768.
Test: 2018-08-02T15:02:29.246052: step 9370, loss 3.81863.
Train: 2018-08-02T15:02:29.292933: step 9371, loss 3.52583.
Train: 2018-08-02T15:02:29.355414: step 9372, loss 5.16283.
Train: 2018-08-02T15:02:29.417873: step 9373, loss 3.39991.
Train: 2018-08-02T15:02:29.480391: step 9374, loss 4.28137.
Train: 2018-08-02T15:02:29.527223: step 9375, loss 3.77768.
Train: 2018-08-02T15:02:29.589737: step 9376, loss 3.9036.
Train: 2018-08-02T15:02:29.636572: step 9377, loss 4.02952.
Train: 2018-08-02T15:02:29.699089: step 9378, loss 3.65176.
Train: 2018-08-02T15:02:29.761573: step 9379, loss 4.40729.
Train: 2018-08-02T15:02:29.824057: step 9380, loss 4.02952.
Test: 2018-08-02T15:02:30.011514: step 9380, loss 3.81863.
Train: 2018-08-02T15:02:30.073994: step 9381, loss 3.02214.
Train: 2018-08-02T15:02:30.136486: step 9382, loss 4.15545.
Train: 2018-08-02T15:02:30.198939: step 9383, loss 3.77768.
Train: 2018-08-02T15:02:30.245836: step 9384, loss 4.15545.
Train: 2018-08-02T15:02:30.308290: step 9385, loss 3.77768.
Train: 2018-08-02T15:02:30.370806: step 9386, loss 3.39991.
Train: 2018-08-02T15:02:30.417669: step 9387, loss 4.15545.
Train: 2018-08-02T15:02:30.480126: step 9388, loss 4.65914.
Train: 2018-08-02T15:02:30.542640: step 9389, loss 4.28137.
Train: 2018-08-02T15:02:30.605125: step 9390, loss 3.65176.
Test: 2018-08-02T15:02:30.792549: step 9390, loss 3.81863.
Train: 2018-08-02T15:02:30.855037: step 9391, loss 3.65176.
Train: 2018-08-02T15:02:30.917522: step 9392, loss 5.03691.
Train: 2018-08-02T15:02:30.980037: step 9393, loss 3.27399.
Train: 2018-08-02T15:02:31.026902: step 9394, loss 4.02952.
Train: 2018-08-02T15:02:31.089360: step 9395, loss 4.78506.
Train: 2018-08-02T15:02:31.151872: step 9396, loss 4.28137.
Train: 2018-08-02T15:02:31.198705: step 9397, loss 4.28137.
Train: 2018-08-02T15:02:31.261223: step 9398, loss 4.28137.
Train: 2018-08-02T15:02:31.323677: step 9399, loss 3.65176.
Train: 2018-08-02T15:02:31.386193: step 9400, loss 4.65914.
Test: 2018-08-02T15:02:31.573616: step 9400, loss 3.81863.
Train: 2018-08-02T15:02:32.104768: step 9401, loss 3.9036.
Train: 2018-08-02T15:02:32.167229: step 9402, loss 3.65176.
Train: 2018-08-02T15:02:32.214123: step 9403, loss 3.52583.
Train: 2018-08-02T15:02:32.276610: step 9404, loss 3.39991.
Train: 2018-08-02T15:02:32.339094: step 9405, loss 4.65914.
Train: 2018-08-02T15:02:32.385959: step 9406, loss 3.39991.
Train: 2018-08-02T15:02:32.448443: step 9407, loss 3.27399.
Train: 2018-08-02T15:02:32.510900: step 9408, loss 4.15545.
Train: 2018-08-02T15:02:32.573410: step 9409, loss 3.9036.
Train: 2018-08-02T15:02:32.620274: step 9410, loss 4.15545.
Test: 2018-08-02T15:02:32.823325: step 9410, loss 3.81863.
Train: 2018-08-02T15:02:32.885842: step 9411, loss 3.9036.
Train: 2018-08-02T15:02:32.948296: step 9412, loss 4.91098.
Train: 2018-08-02T15:02:32.995191: step 9413, loss 3.14807.
Train: 2018-08-02T15:02:33.057675: step 9414, loss 3.77768.
Train: 2018-08-02T15:02:33.120131: step 9415, loss 3.9036.
Train: 2018-08-02T15:02:33.166994: step 9416, loss 5.0369.
Train: 2018-08-02T15:02:33.229510: step 9417, loss 3.65176.
Train: 2018-08-02T15:02:33.291996: step 9418, loss 3.52583.
Train: 2018-08-02T15:02:33.338860: step 9419, loss 3.65176.
Train: 2018-08-02T15:02:33.401344: step 9420, loss 3.77768.
Test: 2018-08-02T15:02:33.604422: step 9420, loss 3.81863.
Train: 2018-08-02T15:02:33.666902: step 9421, loss 4.15545.
Train: 2018-08-02T15:02:33.713773: step 9422, loss 2.7703.
Train: 2018-08-02T15:02:33.776228: step 9423, loss 4.28137.
Train: 2018-08-02T15:02:33.838715: step 9424, loss 3.39991.
Train: 2018-08-02T15:02:33.901229: step 9425, loss 4.15545.
Train: 2018-08-02T15:02:33.948091: step 9426, loss 3.02214.
Train: 2018-08-02T15:02:34.010578: step 9427, loss 2.51845.
Train: 2018-08-02T15:02:34.073034: step 9428, loss 5.28875.
Train: 2018-08-02T15:02:34.119897: step 9429, loss 3.77768.
Train: 2018-08-02T15:02:34.182383: step 9430, loss 3.39991.
Test: 2018-08-02T15:02:34.385460: step 9430, loss 3.81863.
Train: 2018-08-02T15:02:34.447944: step 9431, loss 3.39991.
Train: 2018-08-02T15:02:34.510461: step 9432, loss 4.53321.
Train: 2018-08-02T15:02:34.557295: step 9433, loss 4.40729.
Train: 2018-08-02T15:02:34.619781: step 9434, loss 4.78506.
Train: 2018-08-02T15:02:34.682295: step 9435, loss 4.02952.
Train: 2018-08-02T15:02:34.729130: step 9436, loss 3.77768.
Train: 2018-08-02T15:02:34.791645: step 9437, loss 2.64437.
Train: 2018-08-02T15:02:34.854100: step 9438, loss 4.02952.
Train: 2018-08-02T15:02:34.916611: step 9439, loss 3.39991.
Train: 2018-08-02T15:02:34.963450: step 9440, loss 3.52583.
Test: 2018-08-02T15:02:35.166551: step 9440, loss 3.81863.
Train: 2018-08-02T15:02:35.229042: step 9441, loss 4.15545.
Train: 2018-08-02T15:02:35.291497: step 9442, loss 3.9036.
Train: 2018-08-02T15:02:35.354012: step 9443, loss 3.77768.
Train: 2018-08-02T15:02:35.400848: step 9444, loss 3.77768.
Train: 2018-08-02T15:02:35.463331: step 9445, loss 3.52583.
Train: 2018-08-02T15:02:35.525842: step 9446, loss 3.77768.
Train: 2018-08-02T15:02:35.572681: step 9447, loss 4.28137.
Train: 2018-08-02T15:02:35.635168: step 9448, loss 4.91098.
Train: 2018-08-02T15:02:35.697679: step 9449, loss 4.15545.
Train: 2018-08-02T15:02:35.744548: step 9450, loss 2.7703.
Test: 2018-08-02T15:02:35.947592: step 9450, loss 3.81863.
Train: 2018-08-02T15:02:36.010110: step 9451, loss 3.77768.
Train: 2018-08-02T15:02:36.072594: step 9452, loss 2.89622.
Train: 2018-08-02T15:02:36.119460: step 9453, loss 5.41467.
Train: 2018-08-02T15:02:36.181945: step 9454, loss 4.78506.
Train: 2018-08-02T15:02:36.244401: step 9455, loss 4.53321.
Train: 2018-08-02T15:02:36.306885: step 9456, loss 3.27399.
Train: 2018-08-02T15:02:36.353774: step 9457, loss 3.77768.
Train: 2018-08-02T15:02:36.416259: step 9458, loss 4.28137.
Train: 2018-08-02T15:02:36.478750: step 9459, loss 3.9036.
Train: 2018-08-02T15:02:36.525613: step 9460, loss 3.77768.
Test: 2018-08-02T15:02:36.728684: step 9460, loss 3.81863.
Train: 2018-08-02T15:02:36.791176: step 9461, loss 5.0369.
Train: 2018-08-02T15:02:36.853661: step 9462, loss 3.65176.
Train: 2018-08-02T15:02:36.900496: step 9463, loss 4.91098.
Train: 2018-08-02T15:02:36.962982: step 9464, loss 3.52583.
Train: 2018-08-02T15:02:37.025493: step 9465, loss 4.65914.
Train: 2018-08-02T15:02:37.087953: step 9466, loss 3.39991.
Train: 2018-08-02T15:02:37.134816: step 9467, loss 4.15545.
Train: 2018-08-02T15:02:37.197327: step 9468, loss 3.52583.
Train: 2018-08-02T15:02:37.259788: step 9469, loss 4.40729.
Train: 2018-08-02T15:02:37.306651: step 9470, loss 5.16283.
Test: 2018-08-02T15:02:37.509727: step 9470, loss 3.81863.
Train: 2018-08-02T15:02:37.572238: step 9471, loss 3.52583.
Train: 2018-08-02T15:02:37.634698: step 9472, loss 4.02952.
Train: 2018-08-02T15:02:37.681562: step 9473, loss 3.65176.
Train: 2018-08-02T15:02:37.744048: step 9474, loss 3.77768.
Train: 2018-08-02T15:02:37.806564: step 9475, loss 3.65176.
Train: 2018-08-02T15:02:37.853398: step 9476, loss 5.03691.
Train: 2018-08-02T15:02:37.915912: step 9477, loss 4.28137.
Train: 2018-08-02T15:02:37.978401: step 9478, loss 5.0369.
Train: 2018-08-02T15:02:38.040871: step 9479, loss 3.39991.
Train: 2018-08-02T15:02:38.087717: step 9480, loss 4.28137.
Test: 2018-08-02T15:02:38.290795: step 9480, loss 3.81863.
Train: 2018-08-02T15:02:38.353281: step 9481, loss 2.7703.
Train: 2018-08-02T15:02:38.415766: step 9482, loss 4.53321.
Train: 2018-08-02T15:02:38.462660: step 9483, loss 5.28875.
Train: 2018-08-02T15:02:38.525116: step 9484, loss 3.14807.
Train: 2018-08-02T15:02:38.587638: step 9485, loss 4.02952.
Train: 2018-08-02T15:02:38.650111: step 9486, loss 4.40729.
Train: 2018-08-02T15:02:38.696980: step 9487, loss 4.15545.
Train: 2018-08-02T15:02:38.759436: step 9488, loss 3.39991.
Train: 2018-08-02T15:02:38.821972: step 9489, loss 5.0369.
Train: 2018-08-02T15:02:38.868784: step 9490, loss 4.53321.
Test: 2018-08-02T15:02:39.071892: step 9490, loss 3.81863.
Train: 2018-08-02T15:02:39.134374: step 9491, loss 4.15545.
Train: 2018-08-02T15:02:39.196863: step 9492, loss 4.65914.
Train: 2018-08-02T15:02:39.259349: step 9493, loss 5.0369.
Train: 2018-08-02T15:02:39.321805: step 9494, loss 3.77768.
Train: 2018-08-02T15:02:39.368692: step 9495, loss 4.28137.
Train: 2018-08-02T15:02:39.431153: step 9496, loss 4.15545.
Train: 2018-08-02T15:02:39.478050: step 9497, loss 4.15545.
Train: 2018-08-02T15:02:39.540533: step 9498, loss 4.91098.
Train: 2018-08-02T15:02:39.602988: step 9499, loss 4.02952.
Train: 2018-08-02T15:02:39.665499: step 9500, loss 3.14807.
Test: 2018-08-02T15:02:39.852954: step 9500, loss 3.81863.
Train: 2018-08-02T15:02:40.368465: step 9501, loss 4.28137.
Train: 2018-08-02T15:02:40.430918: step 9502, loss 3.65176.
Train: 2018-08-02T15:02:40.477815: step 9503, loss 3.77768.
Train: 2018-08-02T15:02:40.540270: step 9504, loss 4.78506.
Train: 2018-08-02T15:02:40.602779: step 9505, loss 4.02952.
Train: 2018-08-02T15:02:40.649617: step 9506, loss 3.65176.
Train: 2018-08-02T15:02:40.712103: step 9507, loss 4.40729.
Train: 2018-08-02T15:02:40.774589: step 9508, loss 5.41467.
Train: 2018-08-02T15:02:40.821453: step 9509, loss 2.39253.
Train: 2018-08-02T15:02:40.883963: step 9510, loss 4.53321.
Test: 2018-08-02T15:02:41.087015: step 9510, loss 3.81863.
Train: 2018-08-02T15:02:41.149500: step 9511, loss 4.40729.
Train: 2018-08-02T15:02:41.196395: step 9512, loss 2.39253.
Train: 2018-08-02T15:02:41.243261: step 9513, loss 3.49225.
Train: 2018-08-02T15:02:41.305714: step 9514, loss 3.14807.
Train: 2018-08-02T15:02:41.368199: step 9515, loss 4.28137.
Train: 2018-08-02T15:02:41.430683: step 9516, loss 4.78506.
Train: 2018-08-02T15:02:41.477575: step 9517, loss 4.02952.
Train: 2018-08-02T15:02:41.540065: step 9518, loss 4.28137.
Train: 2018-08-02T15:02:41.602553: step 9519, loss 2.89622.
Train: 2018-08-02T15:02:41.649414: step 9520, loss 3.27399.
Test: 2018-08-02T15:02:41.852485: step 9520, loss 3.81863.
Train: 2018-08-02T15:02:41.914946: step 9521, loss 4.53321.
Train: 2018-08-02T15:02:41.977431: step 9522, loss 3.77768.
Train: 2018-08-02T15:02:42.024327: step 9523, loss 3.27399.
Train: 2018-08-02T15:02:42.086811: step 9524, loss 4.15545.
Train: 2018-08-02T15:02:42.149267: step 9525, loss 4.28137.
Train: 2018-08-02T15:02:42.211781: step 9526, loss 4.15545.
Train: 2018-08-02T15:02:42.258642: step 9527, loss 3.9036.
Train: 2018-08-02T15:02:42.321135: step 9528, loss 2.89622.
Train: 2018-08-02T15:02:42.383587: step 9529, loss 3.52583.
Train: 2018-08-02T15:02:42.430481: step 9530, loss 4.28137.
Test: 2018-08-02T15:02:42.633552: step 9530, loss 3.81863.
Train: 2018-08-02T15:02:42.696040: step 9531, loss 4.15545.
Train: 2018-08-02T15:02:42.758530: step 9532, loss 5.5406.
Train: 2018-08-02T15:02:42.821013: step 9533, loss 3.39991.
Train: 2018-08-02T15:02:42.867880: step 9534, loss 3.9036.
Train: 2018-08-02T15:02:42.930364: step 9535, loss 3.52583.
Train: 2018-08-02T15:02:42.992850: step 9536, loss 4.40729.
Train: 2018-08-02T15:02:43.039714: step 9537, loss 4.40729.
Train: 2018-08-02T15:02:43.102169: step 9538, loss 3.9036.
Train: 2018-08-02T15:02:43.164655: step 9539, loss 3.77768.
Train: 2018-08-02T15:02:43.227169: step 9540, loss 4.65914.
Test: 2018-08-02T15:02:43.414619: step 9540, loss 3.81863.
Train: 2018-08-02T15:02:43.477113: step 9541, loss 4.28137.
Train: 2018-08-02T15:02:43.539591: step 9542, loss 4.65914.
Train: 2018-08-02T15:02:43.602081: step 9543, loss 4.15545.
Train: 2018-08-02T15:02:43.664538: step 9544, loss 4.28137.
Train: 2018-08-02T15:02:43.727048: step 9545, loss 3.02214.
Train: 2018-08-02T15:02:43.789508: step 9546, loss 3.9036.
Train: 2018-08-02T15:02:43.851994: step 9547, loss 4.02952.
Train: 2018-08-02T15:02:43.898888: step 9548, loss 3.52583.
Train: 2018-08-02T15:02:43.961372: step 9549, loss 4.28137.
Train: 2018-08-02T15:02:44.023857: step 9550, loss 4.40729.
Test: 2018-08-02T15:02:44.226905: step 9550, loss 3.81863.
Train: 2018-08-02T15:02:44.273794: step 9551, loss 3.27399.
Train: 2018-08-02T15:02:44.336285: step 9552, loss 4.15545.
Train: 2018-08-02T15:02:44.398739: step 9553, loss 4.15545.
Train: 2018-08-02T15:02:44.461225: step 9554, loss 3.27399.
Train: 2018-08-02T15:02:44.508088: step 9555, loss 3.39991.
Train: 2018-08-02T15:02:44.570604: step 9556, loss 4.28137.
Train: 2018-08-02T15:02:44.633059: step 9557, loss 4.91098.
Train: 2018-08-02T15:02:44.679923: step 9558, loss 3.02214.
Train: 2018-08-02T15:02:44.742440: step 9559, loss 4.78506.
Train: 2018-08-02T15:02:44.804894: step 9560, loss 3.39991.
Test: 2018-08-02T15:02:44.992349: step 9560, loss 3.81863.
Train: 2018-08-02T15:02:45.054836: step 9561, loss 5.16283.
Train: 2018-08-02T15:02:45.117357: step 9562, loss 3.52583.
Train: 2018-08-02T15:02:45.179837: step 9563, loss 4.65914.
Train: 2018-08-02T15:02:45.242325: step 9564, loss 3.14807.
Train: 2018-08-02T15:02:45.289181: step 9565, loss 4.53321.
Train: 2018-08-02T15:02:45.351671: step 9566, loss 3.65176.
Train: 2018-08-02T15:02:45.414151: step 9567, loss 4.65914.
Train: 2018-08-02T15:02:45.460991: step 9568, loss 4.40729.
Train: 2018-08-02T15:02:45.523476: step 9569, loss 3.39991.
Train: 2018-08-02T15:02:45.585992: step 9570, loss 2.89622.
Test: 2018-08-02T15:02:45.773449: step 9570, loss 3.81863.
Train: 2018-08-02T15:02:45.835904: step 9571, loss 3.77768.
Train: 2018-08-02T15:02:45.898419: step 9572, loss 3.14807.
Train: 2018-08-02T15:02:45.960905: step 9573, loss 3.52583.
Train: 2018-08-02T15:02:46.023390: step 9574, loss 4.65914.
Train: 2018-08-02T15:02:46.070225: step 9575, loss 5.03691.
Train: 2018-08-02T15:02:46.132708: step 9576, loss 3.27399.
Train: 2018-08-02T15:02:46.195223: step 9577, loss 3.77768.
Train: 2018-08-02T15:02:46.242089: step 9578, loss 3.14807.
Train: 2018-08-02T15:02:46.304568: step 9579, loss 5.0369.
Train: 2018-08-02T15:02:46.367060: step 9580, loss 2.89622.
Test: 2018-08-02T15:02:46.570130: step 9580, loss 3.81863.
Train: 2018-08-02T15:02:46.632636: step 9581, loss 4.02952.
Train: 2018-08-02T15:02:46.679485: step 9582, loss 4.28137.
Train: 2018-08-02T15:02:46.741965: step 9583, loss 4.65914.
Train: 2018-08-02T15:02:46.804459: step 9584, loss 3.9036.
Train: 2018-08-02T15:02:46.851323: step 9585, loss 4.53321.
Train: 2018-08-02T15:02:46.913809: step 9586, loss 4.40729.
Train: 2018-08-02T15:02:46.976291: step 9587, loss 4.65914.
Train: 2018-08-02T15:02:47.023126: step 9588, loss 4.28137.
Train: 2018-08-02T15:02:47.085636: step 9589, loss 4.15545.
Train: 2018-08-02T15:02:47.148127: step 9590, loss 5.0369.
Test: 2018-08-02T15:02:47.335550: step 9590, loss 3.81863.
Train: 2018-08-02T15:02:47.398068: step 9591, loss 5.5406.
Train: 2018-08-02T15:02:47.460553: step 9592, loss 4.40729.
Train: 2018-08-02T15:02:47.523038: step 9593, loss 3.39991.
Train: 2018-08-02T15:02:47.569873: step 9594, loss 3.9036.
Train: 2018-08-02T15:02:47.632384: step 9595, loss 4.53321.
Train: 2018-08-02T15:02:47.694873: step 9596, loss 3.77768.
Train: 2018-08-02T15:02:47.741738: step 9597, loss 3.39991.
Train: 2018-08-02T15:02:47.804193: step 9598, loss 4.53321.
Train: 2018-08-02T15:02:47.866710: step 9599, loss 2.89622.
Train: 2018-08-02T15:02:47.929193: step 9600, loss 3.77768.
Test: 2018-08-02T15:02:48.116619: step 9600, loss 3.81863.
Train: 2018-08-02T15:02:48.616502: step 9601, loss 3.52583.
Train: 2018-08-02T15:02:48.663391: step 9602, loss 4.15545.
Train: 2018-08-02T15:02:48.725882: step 9603, loss 3.77768.
Train: 2018-08-02T15:02:48.788368: step 9604, loss 4.15545.
Train: 2018-08-02T15:02:48.850823: step 9605, loss 4.91098.
Train: 2018-08-02T15:02:48.913337: step 9606, loss 4.02952.
Train: 2018-08-02T15:02:48.960202: step 9607, loss 4.28137.
Train: 2018-08-02T15:02:49.022658: step 9608, loss 4.40729.
Train: 2018-08-02T15:02:49.085142: step 9609, loss 4.02952.
Train: 2018-08-02T15:02:49.132037: step 9610, loss 4.65914.
Test: 2018-08-02T15:02:49.335113: step 9610, loss 3.81863.
Train: 2018-08-02T15:02:49.397599: step 9611, loss 4.53321.
Train: 2018-08-02T15:02:49.460085: step 9612, loss 4.91098.
Train: 2018-08-02T15:02:49.522539: step 9613, loss 3.39991.
Train: 2018-08-02T15:02:49.569459: step 9614, loss 3.52583.
Train: 2018-08-02T15:02:49.631915: step 9615, loss 3.52583.
Train: 2018-08-02T15:02:49.694400: step 9616, loss 4.78506.
Train: 2018-08-02T15:02:49.756860: step 9617, loss 4.65914.
Train: 2018-08-02T15:02:49.803728: step 9618, loss 3.9036.
Train: 2018-08-02T15:02:49.866236: step 9619, loss 4.15545.
Train: 2018-08-02T15:02:49.928694: step 9620, loss 3.77768.
Test: 2018-08-02T15:02:50.131804: step 9620, loss 3.81863.
Train: 2018-08-02T15:02:50.178666: step 9621, loss 3.9036.
Train: 2018-08-02T15:02:50.241122: step 9622, loss 5.0369.
Train: 2018-08-02T15:02:50.303637: step 9623, loss 3.27399.
Train: 2018-08-02T15:02:50.366117: step 9624, loss 3.39991.
Train: 2018-08-02T15:02:50.428578: step 9625, loss 5.0369.
Train: 2018-08-02T15:02:50.475441: step 9626, loss 3.9036.
Train: 2018-08-02T15:02:50.537957: step 9627, loss 3.77768.
Train: 2018-08-02T15:02:50.600446: step 9628, loss 4.02952.
Train: 2018-08-02T15:02:50.647307: step 9629, loss 4.53321.
Train: 2018-08-02T15:02:50.709762: step 9630, loss 4.53321.
Test: 2018-08-02T15:02:50.912840: step 9630, loss 3.81863.
Train: 2018-08-02T15:02:50.959733: step 9631, loss 3.27399.
Train: 2018-08-02T15:02:51.022189: step 9632, loss 2.51845.
Train: 2018-08-02T15:02:51.084704: step 9633, loss 4.53321.
Train: 2018-08-02T15:02:51.147161: step 9634, loss 4.28137.
Train: 2018-08-02T15:02:51.209646: step 9635, loss 4.28137.
Train: 2018-08-02T15:02:51.256509: step 9636, loss 2.7703.
Train: 2018-08-02T15:02:51.318994: step 9637, loss 3.77768.
Train: 2018-08-02T15:02:51.365859: step 9638, loss 4.53321.
Train: 2018-08-02T15:02:51.428343: step 9639, loss 5.03691.
Train: 2018-08-02T15:02:51.490859: step 9640, loss 3.77768.
Test: 2018-08-02T15:02:51.693938: step 9640, loss 3.81863.
Train: 2018-08-02T15:02:51.740802: step 9641, loss 3.9036.
Train: 2018-08-02T15:02:51.803281: step 9642, loss 4.02952.
Train: 2018-08-02T15:02:51.865772: step 9643, loss 4.15545.
Train: 2018-08-02T15:02:51.928228: step 9644, loss 4.65914.
Train: 2018-08-02T15:02:51.975116: step 9645, loss 3.14807.
Train: 2018-08-02T15:02:52.037607: step 9646, loss 4.02952.
Train: 2018-08-02T15:02:52.100091: step 9647, loss 3.77768.
Train: 2018-08-02T15:02:52.146955: step 9648, loss 4.02952.
Train: 2018-08-02T15:02:52.209410: step 9649, loss 4.40729.
Train: 2018-08-02T15:02:52.271896: step 9650, loss 3.39991.
Test: 2018-08-02T15:02:52.474973: step 9650, loss 3.81863.
Train: 2018-08-02T15:02:52.537458: step 9651, loss 4.40729.
Train: 2018-08-02T15:02:52.584322: step 9652, loss 3.14807.
Train: 2018-08-02T15:02:52.646842: step 9653, loss 4.53321.
Train: 2018-08-02T15:02:52.709324: step 9654, loss 4.53321.
Train: 2018-08-02T15:02:52.771809: step 9655, loss 3.02214.
Train: 2018-08-02T15:02:52.818668: step 9656, loss 4.15545.
Train: 2018-08-02T15:02:52.881159: step 9657, loss 4.28137.
Train: 2018-08-02T15:02:52.943645: step 9658, loss 3.39991.
Train: 2018-08-02T15:02:52.990509: step 9659, loss 4.53321.
Train: 2018-08-02T15:02:53.052992: step 9660, loss 4.02952.
Test: 2018-08-02T15:02:53.256041: step 9660, loss 3.81863.
Train: 2018-08-02T15:02:53.318525: step 9661, loss 3.27399.
Train: 2018-08-02T15:02:53.365426: step 9662, loss 4.15545.
Train: 2018-08-02T15:02:53.427905: step 9663, loss 3.27399.
Train: 2018-08-02T15:02:53.474739: step 9664, loss 3.22362.
Train: 2018-08-02T15:02:53.537255: step 9665, loss 4.78506.
Train: 2018-08-02T15:02:53.599740: step 9666, loss 3.52583.
Train: 2018-08-02T15:02:53.646574: step 9667, loss 4.15545.
Train: 2018-08-02T15:02:53.709085: step 9668, loss 3.65176.
Train: 2018-08-02T15:02:53.771575: step 9669, loss 4.28137.
Train: 2018-08-02T15:02:53.834056: step 9670, loss 3.9036.
Test: 2018-08-02T15:02:54.021485: step 9670, loss 3.81863.
Train: 2018-08-02T15:02:54.083972: step 9671, loss 3.14807.
Train: 2018-08-02T15:02:54.146488: step 9672, loss 4.15545.
Train: 2018-08-02T15:02:54.208972: step 9673, loss 3.9036.
Train: 2018-08-02T15:02:54.255838: step 9674, loss 4.40729.
Train: 2018-08-02T15:02:54.318322: step 9675, loss 3.65176.
Train: 2018-08-02T15:02:54.380776: step 9676, loss 4.28137.
Train: 2018-08-02T15:02:54.427641: step 9677, loss 3.77768.
Train: 2018-08-02T15:02:54.490128: step 9678, loss 4.02952.
Train: 2018-08-02T15:02:54.552612: step 9679, loss 4.40729.
Train: 2018-08-02T15:02:54.599476: step 9680, loss 4.65914.
Test: 2018-08-02T15:02:54.802578: step 9680, loss 3.81863.
Train: 2018-08-02T15:02:54.865070: step 9681, loss 3.02214.
Train: 2018-08-02T15:02:54.927525: step 9682, loss 2.7703.
Train: 2018-08-02T15:02:54.990009: step 9683, loss 4.02952.
Train: 2018-08-02T15:02:55.036906: step 9684, loss 4.91098.
Train: 2018-08-02T15:02:55.099359: step 9685, loss 3.39991.
Train: 2018-08-02T15:02:55.161875: step 9686, loss 4.28137.
Train: 2018-08-02T15:02:55.224359: step 9687, loss 3.14807.
Train: 2018-08-02T15:02:55.271196: step 9688, loss 3.52583.
Train: 2018-08-02T15:02:55.333705: step 9689, loss 3.39991.
Train: 2018-08-02T15:02:55.396165: step 9690, loss 3.14807.
Test: 2018-08-02T15:02:55.583645: step 9690, loss 3.81863.
Train: 2018-08-02T15:02:55.646107: step 9691, loss 4.15545.
Train: 2018-08-02T15:02:55.708622: step 9692, loss 3.39991.
Train: 2018-08-02T15:02:55.755456: step 9693, loss 3.39991.
Train: 2018-08-02T15:02:55.817971: step 9694, loss 3.77768.
Train: 2018-08-02T15:02:55.880457: step 9695, loss 3.65176.
Train: 2018-08-02T15:02:55.927289: step 9696, loss 2.89622.
Train: 2018-08-02T15:02:55.989774: step 9697, loss 4.53321.
Train: 2018-08-02T15:02:56.052262: step 9698, loss 4.02952.
Train: 2018-08-02T15:02:56.099124: step 9699, loss 3.39991.
Train: 2018-08-02T15:02:56.161610: step 9700, loss 4.28137.
Test: 2018-08-02T15:02:56.364724: step 9700, loss 3.81863.
Train: 2018-08-02T15:02:56.848948: step 9701, loss 2.7703.
Train: 2018-08-02T15:02:56.911465: step 9702, loss 3.39991.
Train: 2018-08-02T15:02:56.973945: step 9703, loss 3.27399.
Train: 2018-08-02T15:02:57.036405: step 9704, loss 4.15545.
Train: 2018-08-02T15:02:57.083309: step 9705, loss 4.28137.
Train: 2018-08-02T15:02:57.145785: step 9706, loss 4.15545.
Train: 2018-08-02T15:02:57.208241: step 9707, loss 4.40729.
Train: 2018-08-02T15:02:57.270751: step 9708, loss 4.15545.
Train: 2018-08-02T15:02:57.317619: step 9709, loss 3.9036.
Train: 2018-08-02T15:02:57.380105: step 9710, loss 3.9036.
Test: 2018-08-02T15:02:57.567556: step 9710, loss 3.81863.
Train: 2018-08-02T15:02:57.630016: step 9711, loss 3.9036.
Train: 2018-08-02T15:02:57.692500: step 9712, loss 4.15545.
Train: 2018-08-02T15:02:57.755017: step 9713, loss 2.89622.
Train: 2018-08-02T15:02:57.801853: step 9714, loss 5.41467.
Train: 2018-08-02T15:02:57.864369: step 9715, loss 2.64438.
Train: 2018-08-02T15:02:57.926852: step 9716, loss 4.53321.
Train: 2018-08-02T15:02:57.973719: step 9717, loss 3.9036.
Train: 2018-08-02T15:02:58.036171: step 9718, loss 3.65176.
Train: 2018-08-02T15:02:58.098687: step 9719, loss 3.77768.
Train: 2018-08-02T15:02:58.145552: step 9720, loss 4.65914.
Test: 2018-08-02T15:02:58.348598: step 9720, loss 3.81863.
Train: 2018-08-02T15:02:58.411083: step 9721, loss 3.27399.
Train: 2018-08-02T15:02:58.473598: step 9722, loss 4.40729.
Train: 2018-08-02T15:02:58.536078: step 9723, loss 3.52583.
Train: 2018-08-02T15:02:58.582949: step 9724, loss 3.14807.
Train: 2018-08-02T15:02:58.645430: step 9725, loss 3.77768.
Train: 2018-08-02T15:02:58.707919: step 9726, loss 3.27399.
Train: 2018-08-02T15:02:58.754753: step 9727, loss 4.40729.
Train: 2018-08-02T15:02:58.817269: step 9728, loss 2.89622.
Train: 2018-08-02T15:02:58.879725: step 9729, loss 3.52583.
Train: 2018-08-02T15:02:58.926618: step 9730, loss 5.0369.
Test: 2018-08-02T15:02:59.129665: step 9730, loss 3.81863.
Train: 2018-08-02T15:02:59.192151: step 9731, loss 4.28137.
Train: 2018-08-02T15:02:59.254667: step 9732, loss 4.91098.
Train: 2018-08-02T15:02:59.301499: step 9733, loss 4.40729.
Train: 2018-08-02T15:02:59.364018: step 9734, loss 4.91098.
Train: 2018-08-02T15:02:59.426471: step 9735, loss 4.53321.
Train: 2018-08-02T15:02:59.473335: step 9736, loss 4.78506.
Train: 2018-08-02T15:02:59.535849: step 9737, loss 3.39991.
Train: 2018-08-02T15:02:59.598307: step 9738, loss 3.9036.
Train: 2018-08-02T15:02:59.660848: step 9739, loss 4.15545.
Train: 2018-08-02T15:02:59.707680: step 9740, loss 4.02952.
Test: 2018-08-02T15:02:59.910731: step 9740, loss 3.81863.
Train: 2018-08-02T15:02:59.973218: step 9741, loss 3.65176.
Train: 2018-08-02T15:03:00.035729: step 9742, loss 4.28137.
Train: 2018-08-02T15:03:00.098218: step 9743, loss 3.39991.
Train: 2018-08-02T15:03:00.145083: step 9744, loss 3.65176.
Train: 2018-08-02T15:03:00.207538: step 9745, loss 4.28137.
Train: 2018-08-02T15:03:00.270052: step 9746, loss 3.65176.
Train: 2018-08-02T15:03:00.316888: step 9747, loss 4.02952.
Train: 2018-08-02T15:03:00.379404: step 9748, loss 2.89622.
Train: 2018-08-02T15:03:00.441890: step 9749, loss 4.15545.
Train: 2018-08-02T15:03:00.504343: step 9750, loss 5.41467.
Test: 2018-08-02T15:03:00.691798: step 9750, loss 3.81863.
Train: 2018-08-02T15:03:00.754284: step 9751, loss 5.41467.
Train: 2018-08-02T15:03:00.816771: step 9752, loss 4.15545.
Train: 2018-08-02T15:03:00.879286: step 9753, loss 5.5406.
Train: 2018-08-02T15:03:00.941770: step 9754, loss 3.14807.
Train: 2018-08-02T15:03:00.988604: step 9755, loss 4.15545.
Train: 2018-08-02T15:03:01.051120: step 9756, loss 4.65914.
Train: 2018-08-02T15:03:01.113575: step 9757, loss 4.91098.
Train: 2018-08-02T15:03:01.160469: step 9758, loss 3.65176.
Train: 2018-08-02T15:03:01.222957: step 9759, loss 4.15545.
Train: 2018-08-02T15:03:01.285409: step 9760, loss 4.15545.
Test: 2018-08-02T15:03:01.488489: step 9760, loss 3.81863.
Train: 2018-08-02T15:03:01.535382: step 9761, loss 4.40729.
Train: 2018-08-02T15:03:01.597867: step 9762, loss 3.9036.
Train: 2018-08-02T15:03:01.660352: step 9763, loss 4.40729.
Train: 2018-08-02T15:03:01.722808: step 9764, loss 3.65176.
Train: 2018-08-02T15:03:01.769703: step 9765, loss 4.65914.
Train: 2018-08-02T15:03:01.832160: step 9766, loss 3.14807.
Train: 2018-08-02T15:03:01.894670: step 9767, loss 3.9036.
Train: 2018-08-02T15:03:01.957129: step 9768, loss 4.28137.
Train: 2018-08-02T15:03:02.004023: step 9769, loss 4.65914.
Train: 2018-08-02T15:03:02.066477: step 9770, loss 4.65914.
Test: 2018-08-02T15:03:02.269555: step 9770, loss 3.81863.
Train: 2018-08-02T15:03:02.332039: step 9771, loss 4.28137.
Train: 2018-08-02T15:03:02.378930: step 9772, loss 4.28137.
Train: 2018-08-02T15:03:02.441388: step 9773, loss 5.16283.
Train: 2018-08-02T15:03:02.503907: step 9774, loss 4.15545.
Train: 2018-08-02T15:03:02.566386: step 9775, loss 4.78506.
Train: 2018-08-02T15:03:02.613256: step 9776, loss 3.9036.
Train: 2018-08-02T15:03:02.675710: step 9777, loss 3.65176.
Train: 2018-08-02T15:03:02.738222: step 9778, loss 3.77768.
Train: 2018-08-02T15:03:02.785089: step 9779, loss 3.39991.
Train: 2018-08-02T15:03:02.847574: step 9780, loss 4.78506.
Test: 2018-08-02T15:03:03.050622: step 9780, loss 3.81863.
Train: 2018-08-02T15:03:03.113131: step 9781, loss 3.65176.
Train: 2018-08-02T15:03:03.175592: step 9782, loss 4.15545.
Train: 2018-08-02T15:03:03.222490: step 9783, loss 3.9036.
Train: 2018-08-02T15:03:03.284943: step 9784, loss 3.27399.
Train: 2018-08-02T15:03:03.347458: step 9785, loss 4.53321.
Train: 2018-08-02T15:03:03.409913: step 9786, loss 4.65914.
Train: 2018-08-02T15:03:03.456777: step 9787, loss 4.65914.
Train: 2018-08-02T15:03:03.519292: step 9788, loss 4.41271.
Train: 2018-08-02T15:03:03.581777: step 9789, loss 4.15545.
Train: 2018-08-02T15:03:03.628612: step 9790, loss 4.28137.
Test: 2018-08-02T15:03:03.831719: step 9790, loss 3.81863.
Train: 2018-08-02T15:03:03.894205: step 9791, loss 3.77768.
Train: 2018-08-02T15:03:03.956689: step 9792, loss 4.02952.
Train: 2018-08-02T15:03:04.003555: step 9793, loss 4.78506.
Train: 2018-08-02T15:03:04.066040: step 9794, loss 4.78506.
Train: 2018-08-02T15:03:04.128526: step 9795, loss 3.65176.
Train: 2018-08-02T15:03:04.175388: step 9796, loss 3.65176.
Train: 2018-08-02T15:03:04.237875: step 9797, loss 3.39991.
Train: 2018-08-02T15:03:04.300331: step 9798, loss 3.39991.
Train: 2018-08-02T15:03:04.347194: step 9799, loss 3.27399.
Train: 2018-08-02T15:03:04.409714: step 9800, loss 4.28137.
Test: 2018-08-02T15:03:04.612756: step 9800, loss 3.81863.
Train: 2018-08-02T15:03:05.097050: step 9801, loss 4.02952.
Train: 2018-08-02T15:03:05.159533: step 9802, loss 4.28137.
Train: 2018-08-02T15:03:05.222027: step 9803, loss 4.02952.
Train: 2018-08-02T15:03:05.268884: step 9804, loss 4.15545.
Train: 2018-08-02T15:03:05.331373: step 9805, loss 4.02952.
Train: 2018-08-02T15:03:05.393853: step 9806, loss 4.65914.
Train: 2018-08-02T15:03:05.440718: step 9807, loss 5.0369.
Train: 2018-08-02T15:03:05.503204: step 9808, loss 2.64437.
Train: 2018-08-02T15:03:05.565690: step 9809, loss 5.16283.
Train: 2018-08-02T15:03:05.612554: step 9810, loss 3.9036.
Test: 2018-08-02T15:03:05.815598: step 9810, loss 3.81863.
Train: 2018-08-02T15:03:05.878115: step 9811, loss 4.28137.
Train: 2018-08-02T15:03:05.940601: step 9812, loss 3.65176.
Train: 2018-08-02T15:03:06.003086: step 9813, loss 4.28137.
Train: 2018-08-02T15:03:06.065572: step 9814, loss 3.65176.
Train: 2018-08-02T15:03:06.096817: step 9815, loss 3.49225.
Train: 2018-08-02T15:03:06.159268: step 9816, loss 4.15545.
Train: 2018-08-02T15:03:06.221753: step 9817, loss 3.65176.
Train: 2018-08-02T15:03:06.284240: step 9818, loss 3.77768.
Train: 2018-08-02T15:03:06.331138: step 9819, loss 3.65176.
Train: 2018-08-02T15:03:06.393619: step 9820, loss 4.40729.
Test: 2018-08-02T15:03:06.596666: step 9820, loss 3.81863.
Train: 2018-08-02T15:03:06.659181: step 9821, loss 4.78506.
Train: 2018-08-02T15:03:06.721661: step 9822, loss 4.15545.
Train: 2018-08-02T15:03:06.768531: step 9823, loss 4.15545.
Train: 2018-08-02T15:03:06.831011: step 9824, loss 4.15545.
Train: 2018-08-02T15:03:06.893502: step 9825, loss 4.91098.
Train: 2018-08-02T15:03:06.940367: step 9826, loss 3.39991.
Train: 2018-08-02T15:03:07.002822: step 9827, loss 3.9036.
Train: 2018-08-02T15:03:07.065341: step 9828, loss 4.65914.
Train: 2018-08-02T15:03:07.112202: step 9829, loss 2.89622.
Train: 2018-08-02T15:03:07.174680: step 9830, loss 2.89622.
Test: 2018-08-02T15:03:07.377766: step 9830, loss 3.81863.
Train: 2018-08-02T15:03:07.440249: step 9831, loss 5.41467.
Train: 2018-08-02T15:03:07.502705: step 9832, loss 3.77768.
Train: 2018-08-02T15:03:07.549598: step 9833, loss 3.52583.
Train: 2018-08-02T15:03:07.612086: step 9834, loss 4.28137.
Train: 2018-08-02T15:03:07.674538: step 9835, loss 3.52583.
Train: 2018-08-02T15:03:07.737048: step 9836, loss 4.28137.
Train: 2018-08-02T15:03:07.783919: step 9837, loss 4.40729.
Train: 2018-08-02T15:03:07.846375: step 9838, loss 4.40729.
Train: 2018-08-02T15:03:07.908860: step 9839, loss 3.14807.
Train: 2018-08-02T15:03:07.955722: step 9840, loss 4.15545.
Test: 2018-08-02T15:03:08.158830: step 9840, loss 3.81863.
Train: 2018-08-02T15:03:08.221285: step 9841, loss 4.53321.
Train: 2018-08-02T15:03:08.283771: step 9842, loss 3.65176.
Train: 2018-08-02T15:03:08.330665: step 9843, loss 3.77768.
Train: 2018-08-02T15:03:08.393121: step 9844, loss 4.28137.
Train: 2018-08-02T15:03:08.455607: step 9845, loss 4.53321.
Train: 2018-08-02T15:03:08.502470: step 9846, loss 4.15545.
Train: 2018-08-02T15:03:08.564986: step 9847, loss 3.14807.
Train: 2018-08-02T15:03:08.627473: step 9848, loss 4.28137.
Train: 2018-08-02T15:03:08.674304: step 9849, loss 4.53321.
Train: 2018-08-02T15:03:08.736827: step 9850, loss 3.77768.
Test: 2018-08-02T15:03:08.939868: step 9850, loss 3.81863.
Train: 2018-08-02T15:03:09.002384: step 9851, loss 4.28137.
Train: 2018-08-02T15:03:09.049248: step 9852, loss 4.40729.
Train: 2018-08-02T15:03:09.111729: step 9853, loss 3.77768.
Train: 2018-08-02T15:03:09.174188: step 9854, loss 3.9036.
Train: 2018-08-02T15:03:09.221085: step 9855, loss 4.53321.
Train: 2018-08-02T15:03:09.283537: step 9856, loss 3.9036.
Train: 2018-08-02T15:03:09.346023: step 9857, loss 4.15545.
Train: 2018-08-02T15:03:09.408509: step 9858, loss 3.39991.
Train: 2018-08-02T15:03:09.455402: step 9859, loss 4.40729.
Train: 2018-08-02T15:03:09.517857: step 9860, loss 4.15545.
Test: 2018-08-02T15:03:09.720958: step 9860, loss 3.81863.
Train: 2018-08-02T15:03:09.783450: step 9861, loss 4.15545.
Train: 2018-08-02T15:03:09.830315: step 9862, loss 4.65914.
Train: 2018-08-02T15:03:09.892770: step 9863, loss 3.27399.
Train: 2018-08-02T15:03:09.955285: step 9864, loss 4.28137.
Train: 2018-08-02T15:03:10.017741: step 9865, loss 4.40729.
Train: 2018-08-02T15:03:10.064635: step 9866, loss 4.40729.
Train: 2018-08-02T15:03:10.127115: step 9867, loss 3.14807.
Train: 2018-08-02T15:03:10.189601: step 9868, loss 3.77768.
Train: 2018-08-02T15:03:10.252091: step 9869, loss 4.28137.
Train: 2018-08-02T15:03:10.298955: step 9870, loss 4.28137.
Test: 2018-08-02T15:03:10.502032: step 9870, loss 3.81863.
Train: 2018-08-02T15:03:10.564517: step 9871, loss 5.41467.
Train: 2018-08-02T15:03:10.627003: step 9872, loss 4.65914.
Train: 2018-08-02T15:03:10.673866: step 9873, loss 4.65914.
Train: 2018-08-02T15:03:10.736354: step 9874, loss 4.02952.
Train: 2018-08-02T15:03:10.798838: step 9875, loss 3.14807.
Train: 2018-08-02T15:03:10.861324: step 9876, loss 3.77768.
Train: 2018-08-02T15:03:10.908156: step 9877, loss 4.28137.
Train: 2018-08-02T15:03:10.970644: step 9878, loss 3.65176.
Train: 2018-08-02T15:03:11.033128: step 9879, loss 4.15545.
Train: 2018-08-02T15:03:11.095643: step 9880, loss 4.28137.
Test: 2018-08-02T15:03:11.283069: step 9880, loss 3.81863.
Train: 2018-08-02T15:03:11.345587: step 9881, loss 4.15545.
Train: 2018-08-02T15:03:11.408039: step 9882, loss 5.04223.
Train: 2018-08-02T15:03:11.470525: step 9883, loss 3.52583.
Train: 2018-08-02T15:03:11.533010: step 9884, loss 4.53321.
Train: 2018-08-02T15:03:11.579900: step 9885, loss 3.65176.
Train: 2018-08-02T15:03:11.642390: step 9886, loss 3.02214.
Train: 2018-08-02T15:03:11.704846: step 9887, loss 3.77768.
Train: 2018-08-02T15:03:11.751740: step 9888, loss 3.27399.
Train: 2018-08-02T15:03:11.814194: step 9889, loss 4.15545.
Train: 2018-08-02T15:03:11.876679: step 9890, loss 3.14807.
Test: 2018-08-02T15:03:12.079792: step 9890, loss 3.81863.
Train: 2018-08-02T15:03:12.126652: step 9891, loss 3.14807.
Train: 2018-08-02T15:03:12.189137: step 9892, loss 4.02952.
Train: 2018-08-02T15:03:12.251593: step 9893, loss 4.65914.
Train: 2018-08-02T15:03:12.314109: step 9894, loss 3.52583.
Train: 2018-08-02T15:03:12.376594: step 9895, loss 2.51845.
Train: 2018-08-02T15:03:12.423453: step 9896, loss 3.65176.
Train: 2018-08-02T15:03:12.485939: step 9897, loss 3.39991.
Train: 2018-08-02T15:03:12.548398: step 9898, loss 4.40729.
Train: 2018-08-02T15:03:12.595294: step 9899, loss 3.39991.
Train: 2018-08-02T15:03:12.657777: step 9900, loss 3.02214.
Test: 2018-08-02T15:03:12.860848: step 9900, loss 3.81863.
Train: 2018-08-02T15:03:13.345087: step 9901, loss 2.64438.
Train: 2018-08-02T15:03:13.391980: step 9902, loss 5.41467.
Train: 2018-08-02T15:03:13.454469: step 9903, loss 4.40729.
Train: 2018-08-02T15:03:13.516963: step 9904, loss 3.27399.
Train: 2018-08-02T15:03:13.579437: step 9905, loss 4.28137.
Train: 2018-08-02T15:03:13.626302: step 9906, loss 3.65176.
Train: 2018-08-02T15:03:13.688757: step 9907, loss 4.02952.
Train: 2018-08-02T15:03:13.751274: step 9908, loss 5.0369.
Train: 2018-08-02T15:03:13.813757: step 9909, loss 3.39991.
Train: 2018-08-02T15:03:13.860622: step 9910, loss 5.5406.
Test: 2018-08-02T15:03:14.063698: step 9910, loss 3.81863.
Train: 2018-08-02T15:03:14.126177: step 9911, loss 3.9036.
Train: 2018-08-02T15:03:14.173017: step 9912, loss 2.14068.
Train: 2018-08-02T15:03:14.235534: step 9913, loss 4.40729.
Train: 2018-08-02T15:03:14.297991: step 9914, loss 4.91098.
Train: 2018-08-02T15:03:14.360473: step 9915, loss 4.02952.
Train: 2018-08-02T15:03:14.407367: step 9916, loss 3.65176.
Train: 2018-08-02T15:03:14.469852: step 9917, loss 4.28137.
Train: 2018-08-02T15:03:14.532310: step 9918, loss 2.7703.
Train: 2018-08-02T15:03:14.579202: step 9919, loss 4.40729.
Train: 2018-08-02T15:03:14.641687: step 9920, loss 4.15545.
Test: 2018-08-02T15:03:14.844735: step 9920, loss 3.81863.
Train: 2018-08-02T15:03:14.907250: step 9921, loss 4.40729.
Train: 2018-08-02T15:03:14.969735: step 9922, loss 3.65176.
Train: 2018-08-02T15:03:15.016569: step 9923, loss 3.65176.
Train: 2018-08-02T15:03:15.079057: step 9924, loss 3.77768.
Train: 2018-08-02T15:03:15.141544: step 9925, loss 4.28137.
Train: 2018-08-02T15:03:15.188430: step 9926, loss 3.27399.
Train: 2018-08-02T15:03:15.250889: step 9927, loss 4.15545.
Train: 2018-08-02T15:03:15.313406: step 9928, loss 3.9036.
Train: 2018-08-02T15:03:15.375862: step 9929, loss 4.78506.
Train: 2018-08-02T15:03:15.422725: step 9930, loss 5.0369.
Test: 2018-08-02T15:03:15.625802: step 9930, loss 3.81863.
Train: 2018-08-02T15:03:15.688317: step 9931, loss 4.02952.
Train: 2018-08-02T15:03:15.750804: step 9932, loss 3.65176.
Train: 2018-08-02T15:03:15.797667: step 9933, loss 3.65176.
Train: 2018-08-02T15:03:15.860155: step 9934, loss 4.28137.
Train: 2018-08-02T15:03:15.922637: step 9935, loss 4.15545.
Train: 2018-08-02T15:03:15.985125: step 9936, loss 5.16283.
Train: 2018-08-02T15:03:16.031956: step 9937, loss 3.39991.
Train: 2018-08-02T15:03:16.094444: step 9938, loss 2.89622.
Train: 2018-08-02T15:03:16.156928: step 9939, loss 4.15545.
Train: 2018-08-02T15:03:16.203823: step 9940, loss 3.27399.
Test: 2018-08-02T15:03:16.406894: step 9940, loss 3.81863.
Train: 2018-08-02T15:03:16.469387: step 9941, loss 3.77768.
Train: 2018-08-02T15:03:16.531871: step 9942, loss 3.9036.
Train: 2018-08-02T15:03:16.594325: step 9943, loss 5.41467.
Train: 2018-08-02T15:03:16.641190: step 9944, loss 3.77768.
Train: 2018-08-02T15:03:16.703706: step 9945, loss 5.16283.
Train: 2018-08-02T15:03:16.766159: step 9946, loss 4.15545.
Train: 2018-08-02T15:03:16.813049: step 9947, loss 3.9036.
Train: 2018-08-02T15:03:16.875510: step 9948, loss 3.9036.
Train: 2018-08-02T15:03:16.937997: step 9949, loss 4.40729.
Train: 2018-08-02T15:03:17.000480: step 9950, loss 4.40729.
Test: 2018-08-02T15:03:17.187966: step 9950, loss 3.81863.
Train: 2018-08-02T15:03:17.250420: step 9951, loss 3.52583.
Train: 2018-08-02T15:03:17.312932: step 9952, loss 3.52583.
Train: 2018-08-02T15:03:17.375423: step 9953, loss 4.78506.
Train: 2018-08-02T15:03:17.422287: step 9954, loss 3.27399.
Train: 2018-08-02T15:03:17.484774: step 9955, loss 4.40729.
Train: 2018-08-02T15:03:17.547227: step 9956, loss 4.28137.
Train: 2018-08-02T15:03:17.594090: step 9957, loss 3.65176.
Train: 2018-08-02T15:03:17.656577: step 9958, loss 3.52583.
Train: 2018-08-02T15:03:17.719092: step 9959, loss 3.77768.
Train: 2018-08-02T15:03:17.781578: step 9960, loss 3.52583.
Test: 2018-08-02T15:03:17.969002: step 9960, loss 3.81863.
Train: 2018-08-02T15:03:18.031520: step 9961, loss 4.53321.
Train: 2018-08-02T15:03:18.093975: step 9962, loss 4.40729.
Train: 2018-08-02T15:03:18.156489: step 9963, loss 4.02952.
Train: 2018-08-02T15:03:18.218975: step 9964, loss 4.02952.
Train: 2018-08-02T15:03:18.265817: step 9965, loss 4.65914.
Train: 2018-08-02T15:03:18.312672: step 9966, loss 5.10406.
Train: 2018-08-02T15:03:18.375187: step 9967, loss 4.15545.
Train: 2018-08-02T15:03:18.437644: step 9968, loss 4.02952.
Train: 2018-08-02T15:03:18.484539: step 9969, loss 4.91098.
Train: 2018-08-02T15:03:18.547023: step 9970, loss 3.27399.
Test: 2018-08-02T15:03:18.750069: step 9970, loss 3.81863.
Train: 2018-08-02T15:03:18.812586: step 9971, loss 4.65914.
Train: 2018-08-02T15:03:18.859446: step 9972, loss 4.91098.
Train: 2018-08-02T15:03:18.921904: step 9973, loss 3.39991.
Train: 2018-08-02T15:03:18.984391: step 9974, loss 4.15545.
Train: 2018-08-02T15:03:19.031254: step 9975, loss 4.02952.
Train: 2018-08-02T15:03:19.093739: step 9976, loss 3.9036.
Train: 2018-08-02T15:03:19.140603: step 9977, loss 4.53321.
Train: 2018-08-02T15:03:19.203119: step 9978, loss 3.39991.
Train: 2018-08-02T15:03:19.265605: step 9979, loss 3.52583.
Train: 2018-08-02T15:03:19.328059: step 9980, loss 4.91098.
Test: 2018-08-02T15:03:19.531138: step 9980, loss 3.81863.
Train: 2018-08-02T15:03:19.578031: step 9981, loss 4.78506.
Train: 2018-08-02T15:03:19.640512: step 9982, loss 3.52583.
Train: 2018-08-02T15:03:19.702972: step 9983, loss 3.65176.
Train: 2018-08-02T15:03:19.765459: step 9984, loss 3.52583.
Train: 2018-08-02T15:03:19.812352: step 9985, loss 3.52583.
Train: 2018-08-02T15:03:19.874807: step 9986, loss 4.91098.
Train: 2018-08-02T15:03:19.937292: step 9987, loss 3.65176.
Train: 2018-08-02T15:03:19.984156: step 9988, loss 4.02952.
Train: 2018-08-02T15:03:20.062263: step 9989, loss 3.65176.
Train: 2018-08-02T15:03:20.109126: step 9990, loss 4.40729.
Test: 2018-08-02T15:03:20.312204: step 9990, loss 3.81863.
Train: 2018-08-02T15:03:20.374689: step 9991, loss 3.9036.
Train: 2018-08-02T15:03:20.437205: step 9992, loss 3.39991.
Train: 2018-08-02T15:03:20.499660: step 9993, loss 3.27399.
Train: 2018-08-02T15:03:20.562170: step 9994, loss 3.77768.
Train: 2018-08-02T15:03:20.609037: step 9995, loss 5.0369.
Train: 2018-08-02T15:03:20.671521: step 9996, loss 3.39991.
Train: 2018-08-02T15:03:20.734012: step 9997, loss 2.89622.
Train: 2018-08-02T15:03:20.780870: step 9998, loss 3.77768.
Train: 2018-08-02T15:03:20.843330: step 9999, loss 4.15545.
Train: 2018-08-02T15:03:20.905841: step 10000, loss 2.89622.
Test: 2018-08-02T15:03:21.108917: step 10000, loss 3.81863.
Train: 2018-08-02T15:03:21.655639: step 10001, loss 5.16283.
Train: 2018-08-02T15:03:21.718155: step 10002, loss 3.77768.
Train: 2018-08-02T15:03:21.765020: step 10003, loss 4.02952.
Train: 2018-08-02T15:03:21.827505: step 10004, loss 5.0369.
Train: 2018-08-02T15:03:21.889991: step 10005, loss 5.0369.
Train: 2018-08-02T15:03:21.952478: step 10006, loss 3.14807.
Train: 2018-08-02T15:03:21.999340: step 10007, loss 3.9036.
Train: 2018-08-02T15:03:22.061826: step 10008, loss 4.15545.
Train: 2018-08-02T15:03:22.124311: step 10009, loss 4.15545.
Train: 2018-08-02T15:03:22.171175: step 10010, loss 4.02952.
Test: 2018-08-02T15:03:22.374220: step 10010, loss 3.81863.
Train: 2018-08-02T15:03:22.436706: step 10011, loss 3.77768.
Train: 2018-08-02T15:03:22.499223: step 10012, loss 3.52583.
Train: 2018-08-02T15:03:22.546056: step 10013, loss 4.65914.
Train: 2018-08-02T15:03:22.608574: step 10014, loss 3.39991.
Train: 2018-08-02T15:03:22.671058: step 10015, loss 3.39991.
Train: 2018-08-02T15:03:22.733528: step 10016, loss 3.27399.
Train: 2018-08-02T15:03:22.780407: step 10017, loss 4.02952.
Train: 2018-08-02T15:03:22.842896: step 10018, loss 4.53321.
Train: 2018-08-02T15:03:22.905378: step 10019, loss 4.28137.
Train: 2018-08-02T15:03:22.952212: step 10020, loss 3.39991.
Test: 2018-08-02T15:03:23.155316: step 10020, loss 3.81863.
Train: 2018-08-02T15:03:23.217804: step 10021, loss 4.40729.
Train: 2018-08-02T15:03:23.280289: step 10022, loss 3.52583.
Train: 2018-08-02T15:03:23.342775: step 10023, loss 3.52583.
Train: 2018-08-02T15:03:23.389639: step 10024, loss 3.39991.
Train: 2018-08-02T15:03:23.452094: step 10025, loss 4.28137.
Train: 2018-08-02T15:03:23.514610: step 10026, loss 4.15545.
Train: 2018-08-02T15:03:23.561478: step 10027, loss 5.16283.
Train: 2018-08-02T15:03:23.623931: step 10028, loss 3.52583.
Train: 2018-08-02T15:03:23.686440: step 10029, loss 3.39991.
Train: 2018-08-02T15:03:23.748931: step 10030, loss 3.39991.
Test: 2018-08-02T15:03:23.936385: step 10030, loss 3.81863.
Train: 2018-08-02T15:03:23.998845: step 10031, loss 3.77768.
Train: 2018-08-02T15:03:24.061351: step 10032, loss 4.15545.
Train: 2018-08-02T15:03:24.108222: step 10033, loss 4.02952.
Train: 2018-08-02T15:03:24.170708: step 10034, loss 3.14807.
Train: 2018-08-02T15:03:24.233186: step 10035, loss 4.78506.
Train: 2018-08-02T15:03:24.295677: step 10036, loss 5.16283.
Train: 2018-08-02T15:03:24.342566: step 10037, loss 4.78506.
Train: 2018-08-02T15:03:24.405026: step 10038, loss 2.7703.
Train: 2018-08-02T15:03:24.467483: step 10039, loss 5.16283.
Train: 2018-08-02T15:03:24.529977: step 10040, loss 4.40729.
Test: 2018-08-02T15:03:24.717422: step 10040, loss 3.81863.
Train: 2018-08-02T15:03:24.779938: step 10041, loss 4.28137.
Train: 2018-08-02T15:03:24.842423: step 10042, loss 3.9036.
Train: 2018-08-02T15:03:24.904879: step 10043, loss 3.77768.
Train: 2018-08-02T15:03:24.951775: step 10044, loss 3.77768.
Train: 2018-08-02T15:03:25.014260: step 10045, loss 3.14807.
Train: 2018-08-02T15:03:25.076744: step 10046, loss 4.78506.
Train: 2018-08-02T15:03:25.123609: step 10047, loss 4.02952.
Train: 2018-08-02T15:03:25.186065: step 10048, loss 4.91098.
Train: 2018-08-02T15:03:25.248549: step 10049, loss 4.28137.
Train: 2018-08-02T15:03:25.295442: step 10050, loss 5.0369.
Test: 2018-08-02T15:03:25.498514: step 10050, loss 3.81863.
Train: 2018-08-02T15:03:25.560975: step 10051, loss 2.26661.
Train: 2018-08-02T15:03:25.623491: step 10052, loss 4.53321.
Train: 2018-08-02T15:03:25.670355: step 10053, loss 4.91098.
Train: 2018-08-02T15:03:25.732812: step 10054, loss 2.7703.
Train: 2018-08-02T15:03:25.795297: step 10055, loss 5.41467.
Train: 2018-08-02T15:03:25.842192: step 10056, loss 4.53321.
Train: 2018-08-02T15:03:25.904675: step 10057, loss 3.52583.
Train: 2018-08-02T15:03:25.967163: step 10058, loss 4.15545.
Train: 2018-08-02T15:03:26.029646: step 10059, loss 2.89622.
Train: 2018-08-02T15:03:26.076511: step 10060, loss 4.15545.
Test: 2018-08-02T15:03:26.279588: step 10060, loss 3.81863.
Train: 2018-08-02T15:03:26.342043: step 10061, loss 5.28875.
Train: 2018-08-02T15:03:26.404558: step 10062, loss 4.15545.
Train: 2018-08-02T15:03:26.451392: step 10063, loss 3.52583.
Train: 2018-08-02T15:03:26.513909: step 10064, loss 3.14807.
Train: 2018-08-02T15:03:26.576395: step 10065, loss 4.78506.
Train: 2018-08-02T15:03:26.638878: step 10066, loss 3.9036.
Train: 2018-08-02T15:03:26.685711: step 10067, loss 5.28875.
Train: 2018-08-02T15:03:26.748200: step 10068, loss 3.77768.
Train: 2018-08-02T15:03:26.810685: step 10069, loss 3.52583.
Train: 2018-08-02T15:03:26.857546: step 10070, loss 4.78506.
Test: 2018-08-02T15:03:27.060654: step 10070, loss 3.81863.
Train: 2018-08-02T15:03:27.123142: step 10071, loss 3.14807.
Train: 2018-08-02T15:03:27.185595: step 10072, loss 3.02214.
Train: 2018-08-02T15:03:27.248079: step 10073, loss 4.02952.
Train: 2018-08-02T15:03:27.294976: step 10074, loss 4.78506.
Train: 2018-08-02T15:03:27.357457: step 10075, loss 3.39991.
Train: 2018-08-02T15:03:27.419916: step 10076, loss 4.78506.
Train: 2018-08-02T15:03:27.466778: step 10077, loss 4.78506.
Train: 2018-08-02T15:03:27.529296: step 10078, loss 4.78506.
Train: 2018-08-02T15:03:27.591750: step 10079, loss 4.28137.
Train: 2018-08-02T15:03:27.654236: step 10080, loss 3.27399.
Test: 2018-08-02T15:03:27.841690: step 10080, loss 3.81863.
Train: 2018-08-02T15:03:27.904208: step 10081, loss 4.53321.
Train: 2018-08-02T15:03:27.966694: step 10082, loss 3.52583.
Train: 2018-08-02T15:03:28.029177: step 10083, loss 4.02952.
Train: 2018-08-02T15:03:28.091667: step 10084, loss 3.9036.
Train: 2018-08-02T15:03:28.138522: step 10085, loss 3.39991.
Train: 2018-08-02T15:03:28.200982: step 10086, loss 3.39991.
Train: 2018-08-02T15:03:28.263498: step 10087, loss 3.39991.
Train: 2018-08-02T15:03:28.310364: step 10088, loss 4.28137.
Train: 2018-08-02T15:03:28.372842: step 10089, loss 2.39253.
Train: 2018-08-02T15:03:28.435303: step 10090, loss 4.02952.
Test: 2018-08-02T15:03:28.638411: step 10090, loss 3.81863.
Train: 2018-08-02T15:03:28.700895: step 10091, loss 4.15545.
Train: 2018-08-02T15:03:28.747760: step 10092, loss 4.40729.
Train: 2018-08-02T15:03:28.810245: step 10093, loss 5.66652.
Train: 2018-08-02T15:03:28.872702: step 10094, loss 3.39991.
Train: 2018-08-02T15:03:28.935187: step 10095, loss 3.39991.
Train: 2018-08-02T15:03:28.982076: step 10096, loss 3.52583.
Train: 2018-08-02T15:03:29.044561: step 10097, loss 3.39991.
Train: 2018-08-02T15:03:29.107066: step 10098, loss 4.15545.
Train: 2018-08-02T15:03:29.153915: step 10099, loss 4.65914.
Train: 2018-08-02T15:03:29.216369: step 10100, loss 3.9036.
Test: 2018-08-02T15:03:29.419478: step 10100, loss 3.81863.
Train: 2018-08-02T15:03:29.950602: step 10101, loss 3.39991.
Train: 2018-08-02T15:03:30.028709: step 10102, loss 3.39991.
Train: 2018-08-02T15:03:30.091164: step 10103, loss 3.9036.
Train: 2018-08-02T15:03:30.153680: step 10104, loss 3.52583.
Train: 2018-08-02T15:03:30.216137: step 10105, loss 5.0369.
Train: 2018-08-02T15:03:30.263000: step 10106, loss 3.52583.
Train: 2018-08-02T15:03:30.325515: step 10107, loss 4.02952.
Train: 2018-08-02T15:03:30.388001: step 10108, loss 4.15545.
Train: 2018-08-02T15:03:30.434865: step 10109, loss 4.53321.
Train: 2018-08-02T15:03:30.497350: step 10110, loss 4.02952.
Test: 2018-08-02T15:03:30.700398: step 10110, loss 3.81863.
Train: 2018-08-02T15:03:30.762912: step 10111, loss 3.77768.
Train: 2018-08-02T15:03:30.809746: step 10112, loss 4.28137.
Train: 2018-08-02T15:03:30.872262: step 10113, loss 3.77768.
Train: 2018-08-02T15:03:30.934718: step 10114, loss 4.65914.
Train: 2018-08-02T15:03:30.997215: step 10115, loss 4.91098.
Train: 2018-08-02T15:03:31.044066: step 10116, loss 3.52583.
Train: 2018-08-02T15:03:31.090930: step 10117, loss 5.64133.
Train: 2018-08-02T15:03:31.153446: step 10118, loss 5.28875.
Train: 2018-08-02T15:03:31.215927: step 10119, loss 4.15545.
Train: 2018-08-02T15:03:31.262795: step 10120, loss 5.0369.
Test: 2018-08-02T15:03:31.465843: step 10120, loss 3.81863.
Train: 2018-08-02T15:03:31.528358: step 10121, loss 3.52583.
Train: 2018-08-02T15:03:31.590844: step 10122, loss 2.7703.
Train: 2018-08-02T15:03:31.637708: step 10123, loss 4.28137.
Train: 2018-08-02T15:03:31.700194: step 10124, loss 3.65176.
Train: 2018-08-02T15:03:31.762679: step 10125, loss 4.02952.
Train: 2018-08-02T15:03:31.825163: step 10126, loss 4.28137.
Train: 2018-08-02T15:03:31.872029: step 10127, loss 4.78506.
Train: 2018-08-02T15:03:31.934514: step 10128, loss 3.65176.
Train: 2018-08-02T15:03:31.996995: step 10129, loss 4.02952.
Train: 2018-08-02T15:03:32.059453: step 10130, loss 4.65914.
Test: 2018-08-02T15:03:32.246911: step 10130, loss 3.81863.
Train: 2018-08-02T15:03:32.371913: step 10131, loss 3.65176.
Train: 2018-08-02T15:03:32.434395: step 10132, loss 3.9036.
Train: 2018-08-02T15:03:32.481231: step 10133, loss 4.15545.
Train: 2018-08-02T15:03:32.543745: step 10134, loss 3.52583.
Train: 2018-08-02T15:03:32.606201: step 10135, loss 4.53321.
Train: 2018-08-02T15:03:32.653065: step 10136, loss 3.65176.
Train: 2018-08-02T15:03:32.715583: step 10137, loss 3.52583.
Train: 2018-08-02T15:03:32.778040: step 10138, loss 4.28137.
Train: 2018-08-02T15:03:32.824929: step 10139, loss 3.9036.
Train: 2018-08-02T15:03:32.887415: step 10140, loss 3.52583.
Test: 2018-08-02T15:03:33.090461: step 10140, loss 3.81863.
Train: 2018-08-02T15:03:33.152977: step 10141, loss 3.9036.
Train: 2018-08-02T15:03:33.215457: step 10142, loss 3.27399.
Train: 2018-08-02T15:03:33.262297: step 10143, loss 4.40729.
Train: 2018-08-02T15:03:33.324784: step 10144, loss 5.28875.
Train: 2018-08-02T15:03:33.387298: step 10145, loss 3.14807.
Train: 2018-08-02T15:03:33.434131: step 10146, loss 4.40729.
Train: 2018-08-02T15:03:33.496617: step 10147, loss 4.28137.
Train: 2018-08-02T15:03:33.559103: step 10148, loss 3.65176.
Train: 2018-08-02T15:03:33.605998: step 10149, loss 3.9036.
Train: 2018-08-02T15:03:33.668451: step 10150, loss 4.40729.
Test: 2018-08-02T15:03:33.871529: step 10150, loss 3.81863.
Train: 2018-08-02T15:03:33.934014: step 10151, loss 4.53321.
Train: 2018-08-02T15:03:33.980879: step 10152, loss 5.91836.
Train: 2018-08-02T15:03:34.043365: step 10153, loss 3.77768.
Train: 2018-08-02T15:03:34.105849: step 10154, loss 4.53321.
Train: 2018-08-02T15:03:34.168359: step 10155, loss 4.15545.
Train: 2018-08-02T15:03:34.215198: step 10156, loss 4.02952.
Train: 2018-08-02T15:03:34.277714: step 10157, loss 4.53321.
Train: 2018-08-02T15:03:34.324578: step 10158, loss 3.9036.
Train: 2018-08-02T15:03:34.387034: step 10159, loss 4.78506.
Train: 2018-08-02T15:03:34.449549: step 10160, loss 3.14807.
Test: 2018-08-02T15:03:34.652598: step 10160, loss 3.81863.
Train: 2018-08-02T15:03:34.715112: step 10161, loss 4.15545.
Train: 2018-08-02T15:03:34.761976: step 10162, loss 4.15545.
Train: 2018-08-02T15:03:34.824433: step 10163, loss 4.40729.
Train: 2018-08-02T15:03:34.886948: step 10164, loss 4.02952.
Train: 2018-08-02T15:03:34.933782: step 10165, loss 3.77768.
Train: 2018-08-02T15:03:34.996297: step 10166, loss 3.9036.
Train: 2018-08-02T15:03:35.058755: step 10167, loss 4.40729.
Train: 2018-08-02T15:03:35.121268: step 10168, loss 2.89622.
Train: 2018-08-02T15:03:35.168100: step 10169, loss 4.53321.
Train: 2018-08-02T15:03:35.230617: step 10170, loss 3.27399.
Test: 2018-08-02T15:03:35.418042: step 10170, loss 3.81863.
Train: 2018-08-02T15:03:35.480552: step 10171, loss 3.65176.
Train: 2018-08-02T15:03:35.543013: step 10172, loss 4.91098.
Train: 2018-08-02T15:03:35.605524: step 10173, loss 4.40729.
Train: 2018-08-02T15:03:35.667984: step 10174, loss 4.40729.
Train: 2018-08-02T15:03:35.714881: step 10175, loss 2.7703.
Train: 2018-08-02T15:03:35.777363: step 10176, loss 4.78506.
Train: 2018-08-02T15:03:35.839850: step 10177, loss 3.39991.
Train: 2018-08-02T15:03:35.902329: step 10178, loss 4.15545.
Train: 2018-08-02T15:03:35.949193: step 10179, loss 3.9036.
Train: 2018-08-02T15:03:36.011679: step 10180, loss 3.52583.
Test: 2018-08-02T15:03:36.214755: step 10180, loss 3.81863.
Train: 2018-08-02T15:03:36.277215: step 10181, loss 2.89622.
Train: 2018-08-02T15:03:36.324110: step 10182, loss 4.53321.
Train: 2018-08-02T15:03:36.386591: step 10183, loss 3.14807.
Train: 2018-08-02T15:03:36.449053: step 10184, loss 2.89622.
Train: 2018-08-02T15:03:36.511568: step 10185, loss 4.40729.
Train: 2018-08-02T15:03:36.574051: step 10186, loss 4.40729.
Train: 2018-08-02T15:03:36.620886: step 10187, loss 3.77768.
Train: 2018-08-02T15:03:36.683404: step 10188, loss 5.0369.
Train: 2018-08-02T15:03:36.745888: step 10189, loss 4.65914.
Train: 2018-08-02T15:03:36.792720: step 10190, loss 5.41467.
Test: 2018-08-02T15:03:36.995822: step 10190, loss 3.81863.
Train: 2018-08-02T15:03:37.058313: step 10191, loss 4.15545.
Train: 2018-08-02T15:03:37.120769: step 10192, loss 4.15545.
Train: 2018-08-02T15:03:37.183254: step 10193, loss 4.53321.
Train: 2018-08-02T15:03:37.230122: step 10194, loss 3.77768.
Train: 2018-08-02T15:03:37.292604: step 10195, loss 4.91098.
Train: 2018-08-02T15:03:37.355125: step 10196, loss 5.03691.
Train: 2018-08-02T15:03:37.401983: step 10197, loss 3.39991.
Train: 2018-08-02T15:03:37.464472: step 10198, loss 4.15545.
Train: 2018-08-02T15:03:37.526955: step 10199, loss 3.39991.
Train: 2018-08-02T15:03:37.573786: step 10200, loss 3.02214.
Test: 2018-08-02T15:03:37.776864: step 10200, loss 3.81863.
Train: 2018-08-02T15:03:38.292398: step 10201, loss 4.53321.
Train: 2018-08-02T15:03:38.339266: step 10202, loss 4.40729.
Train: 2018-08-02T15:03:38.401719: step 10203, loss 4.02952.
Train: 2018-08-02T15:03:38.464206: step 10204, loss 4.15545.
Train: 2018-08-02T15:03:38.526689: step 10205, loss 4.65914.
Train: 2018-08-02T15:03:38.573553: step 10206, loss 4.02952.
Train: 2018-08-02T15:03:38.636038: step 10207, loss 3.14807.
Train: 2018-08-02T15:03:38.698523: step 10208, loss 3.77768.
Train: 2018-08-02T15:03:38.761040: step 10209, loss 3.02214.
Train: 2018-08-02T15:03:38.807904: step 10210, loss 3.52583.
Test: 2018-08-02T15:03:39.010951: step 10210, loss 3.81863.
Train: 2018-08-02T15:03:39.073467: step 10211, loss 3.52583.
Train: 2018-08-02T15:03:39.135921: step 10212, loss 3.14807.
Train: 2018-08-02T15:03:39.182815: step 10213, loss 4.15545.
Train: 2018-08-02T15:03:39.245272: step 10214, loss 3.65176.
Train: 2018-08-02T15:03:39.307787: step 10215, loss 2.89622.
Train: 2018-08-02T15:03:39.354652: step 10216, loss 3.27399.
Train: 2018-08-02T15:03:39.417136: step 10217, loss 5.0369.
Train: 2018-08-02T15:03:39.479593: step 10218, loss 5.16283.
Train: 2018-08-02T15:03:39.542107: step 10219, loss 3.52583.
Train: 2018-08-02T15:03:39.588973: step 10220, loss 3.52583.
Test: 2018-08-02T15:03:39.792048: step 10220, loss 3.81863.
Train: 2018-08-02T15:03:39.854535: step 10221, loss 5.0369.
Train: 2018-08-02T15:03:39.916989: step 10222, loss 3.9036.
Train: 2018-08-02T15:03:39.979505: step 10223, loss 3.52583.
Train: 2018-08-02T15:03:40.041986: step 10224, loss 3.52583.
Train: 2018-08-02T15:03:40.088852: step 10225, loss 4.02952.
Train: 2018-08-02T15:03:40.151342: step 10226, loss 3.9036.
Train: 2018-08-02T15:03:40.198203: step 10227, loss 4.15545.
Train: 2018-08-02T15:03:40.260657: step 10228, loss 5.0369.
Train: 2018-08-02T15:03:40.323173: step 10229, loss 4.53321.
Train: 2018-08-02T15:03:40.385653: step 10230, loss 2.89622.
Test: 2018-08-02T15:03:40.573109: step 10230, loss 3.81863.
Train: 2018-08-02T15:03:40.635595: step 10231, loss 3.65176.
Train: 2018-08-02T15:03:40.698085: step 10232, loss 3.9036.
Train: 2018-08-02T15:03:40.760542: step 10233, loss 3.39991.
Train: 2018-08-02T15:03:40.823057: step 10234, loss 4.65914.
Train: 2018-08-02T15:03:40.869922: step 10235, loss 3.9036.
Train: 2018-08-02T15:03:40.932401: step 10236, loss 4.02952.
Train: 2018-08-02T15:03:40.994892: step 10237, loss 3.9036.
Train: 2018-08-02T15:03:41.041725: step 10238, loss 3.9036.
Train: 2018-08-02T15:03:41.104242: step 10239, loss 3.02214.
Train: 2018-08-02T15:03:41.166721: step 10240, loss 3.39991.
Test: 2018-08-02T15:03:41.369773: step 10240, loss 3.81863.
Train: 2018-08-02T15:03:41.416668: step 10241, loss 5.5406.
Train: 2018-08-02T15:03:41.479154: step 10242, loss 3.39991.
Train: 2018-08-02T15:03:41.541638: step 10243, loss 2.89622.
Train: 2018-08-02T15:03:41.604095: step 10244, loss 4.15545.
Train: 2018-08-02T15:03:41.650983: step 10245, loss 3.65176.
Train: 2018-08-02T15:03:41.713476: step 10246, loss 4.40729.
Train: 2018-08-02T15:03:41.775929: step 10247, loss 3.27399.
Train: 2018-08-02T15:03:41.838414: step 10248, loss 4.02952.
Train: 2018-08-02T15:03:41.885308: step 10249, loss 4.02952.
Train: 2018-08-02T15:03:41.947792: step 10250, loss 3.77768.
Test: 2018-08-02T15:03:42.150840: step 10250, loss 3.81863.
Train: 2018-08-02T15:03:42.213356: step 10251, loss 4.28137.
Train: 2018-08-02T15:03:42.275844: step 10252, loss 3.52583.
Train: 2018-08-02T15:03:42.322705: step 10253, loss 3.9036.
Train: 2018-08-02T15:03:42.385192: step 10254, loss 3.39991.
Train: 2018-08-02T15:03:42.447677: step 10255, loss 3.77768.
Train: 2018-08-02T15:03:42.494536: step 10256, loss 4.91098.
Train: 2018-08-02T15:03:42.557026: step 10257, loss 4.65914.
Train: 2018-08-02T15:03:42.619515: step 10258, loss 4.28137.
Train: 2018-08-02T15:03:42.666375: step 10259, loss 4.02952.
Train: 2018-08-02T15:03:42.728863: step 10260, loss 3.27399.
Test: 2018-08-02T15:03:42.931908: step 10260, loss 3.81863.
Train: 2018-08-02T15:03:42.994423: step 10261, loss 4.02952.
Train: 2018-08-02T15:03:43.041257: step 10262, loss 4.53321.
Train: 2018-08-02T15:03:43.103742: step 10263, loss 4.15545.
Train: 2018-08-02T15:03:43.166261: step 10264, loss 4.53321.
Train: 2018-08-02T15:03:43.228714: step 10265, loss 4.15545.
Train: 2018-08-02T15:03:43.291209: step 10266, loss 3.52583.
Train: 2018-08-02T15:03:43.338092: step 10267, loss 4.91098.
Train: 2018-08-02T15:03:43.384927: step 10268, loss 2.95498.
Train: 2018-08-02T15:03:43.447442: step 10269, loss 4.02952.
Train: 2018-08-02T15:03:43.494307: step 10270, loss 3.27399.
Test: 2018-08-02T15:03:43.697377: step 10270, loss 3.81863.
Train: 2018-08-02T15:03:43.759868: step 10271, loss 4.28137.
Train: 2018-08-02T15:03:43.822323: step 10272, loss 3.65176.
Train: 2018-08-02T15:03:43.869189: step 10273, loss 4.53321.
Train: 2018-08-02T15:03:43.931705: step 10274, loss 3.65176.
Train: 2018-08-02T15:03:43.994190: step 10275, loss 4.40729.
Train: 2018-08-02T15:03:44.056674: step 10276, loss 4.28137.
Train: 2018-08-02T15:03:44.103508: step 10277, loss 4.28137.
Train: 2018-08-02T15:03:44.166024: step 10278, loss 3.65176.
Train: 2018-08-02T15:03:44.228511: step 10279, loss 3.65176.
Train: 2018-08-02T15:03:44.275343: step 10280, loss 4.53321.
Test: 2018-08-02T15:03:44.494043: step 10280, loss 3.81863.
Train: 2018-08-02T15:03:44.540937: step 10281, loss 3.52583.
Train: 2018-08-02T15:03:44.603418: step 10282, loss 5.28856.
Train: 2018-08-02T15:03:44.665907: step 10283, loss 5.16283.
Train: 2018-08-02T15:03:44.728394: step 10284, loss 3.65176.
Train: 2018-08-02T15:03:44.790871: step 10285, loss 3.15348.
Train: 2018-08-02T15:03:44.837737: step 10286, loss 3.52583.
Train: 2018-08-02T15:03:44.900226: step 10287, loss 3.65176.
Train: 2018-08-02T15:03:44.962684: step 10288, loss 2.7703.
Train: 2018-08-02T15:03:45.009576: step 10289, loss 3.77768.
Train: 2018-08-02T15:03:45.072056: step 10290, loss 3.77768.
Test: 2018-08-02T15:03:45.275108: step 10290, loss 3.81863.
Train: 2018-08-02T15:03:45.337594: step 10291, loss 4.28137.
Train: 2018-08-02T15:03:45.384458: step 10292, loss 4.28137.
Train: 2018-08-02T15:03:45.446974: step 10293, loss 4.53321.
Train: 2018-08-02T15:03:45.509431: step 10294, loss 4.15545.
Train: 2018-08-02T15:03:45.571941: step 10295, loss 3.77768.
Train: 2018-08-02T15:03:45.634430: step 10296, loss 4.40729.
Train: 2018-08-02T15:03:45.681295: step 10297, loss 4.78506.
Train: 2018-08-02T15:03:45.743781: step 10298, loss 4.28137.
Train: 2018-08-02T15:03:45.806266: step 10299, loss 4.65914.
Train: 2018-08-02T15:03:45.853100: step 10300, loss 3.52583.
Test: 2018-08-02T15:03:46.056176: step 10300, loss 3.81863.
Train: 2018-08-02T15:03:46.540467: step 10301, loss 4.53321.
Train: 2018-08-02T15:03:46.587334: step 10302, loss 4.53321.
Train: 2018-08-02T15:03:46.649817: step 10303, loss 4.78506.
Train: 2018-08-02T15:03:46.712274: step 10304, loss 3.65176.
Train: 2018-08-02T15:03:46.774788: step 10305, loss 4.02952.
Train: 2018-08-02T15:03:46.821623: step 10306, loss 4.65914.
Train: 2018-08-02T15:03:46.884107: step 10307, loss 3.9036.
Train: 2018-08-02T15:03:46.946594: step 10308, loss 4.91098.
Train: 2018-08-02T15:03:47.009079: step 10309, loss 3.9036.
Train: 2018-08-02T15:03:47.055972: step 10310, loss 3.52966.
Test: 2018-08-02T15:03:47.259049: step 10310, loss 3.81863.
Train: 2018-08-02T15:03:47.321530: step 10311, loss 3.27399.
Train: 2018-08-02T15:03:47.384015: step 10312, loss 4.28137.
Train: 2018-08-02T15:03:47.446505: step 10313, loss 3.39991.
Train: 2018-08-02T15:03:47.493371: step 10314, loss 3.77768.
Train: 2018-08-02T15:03:47.555825: step 10315, loss 4.40729.
Train: 2018-08-02T15:03:47.618341: step 10316, loss 3.65176.
Train: 2018-08-02T15:03:47.665205: step 10317, loss 3.65176.
Train: 2018-08-02T15:03:47.727691: step 10318, loss 4.28137.
Train: 2018-08-02T15:03:47.790145: step 10319, loss 3.27399.
Train: 2018-08-02T15:03:47.852642: step 10320, loss 4.02952.
Test: 2018-08-02T15:03:48.040086: step 10320, loss 3.81863.
Train: 2018-08-02T15:03:48.102603: step 10321, loss 3.39991.
Train: 2018-08-02T15:03:48.165081: step 10322, loss 3.52583.
Train: 2018-08-02T15:03:48.227542: step 10323, loss 3.9036.
Train: 2018-08-02T15:03:48.274438: step 10324, loss 2.64438.
Train: 2018-08-02T15:03:48.336893: step 10325, loss 4.78506.
Train: 2018-08-02T15:03:48.399377: step 10326, loss 3.39991.
Train: 2018-08-02T15:03:48.446241: step 10327, loss 6.54798.
Train: 2018-08-02T15:03:48.508758: step 10328, loss 3.27399.
Train: 2018-08-02T15:03:48.571212: step 10329, loss 2.39253.
Train: 2018-08-02T15:03:48.618075: step 10330, loss 4.65914.
Test: 2018-08-02T15:03:48.821154: step 10330, loss 3.81863.
Train: 2018-08-02T15:03:48.883638: step 10331, loss 3.9036.
Train: 2018-08-02T15:03:48.946124: step 10332, loss 3.39991.
Train: 2018-08-02T15:03:48.993019: step 10333, loss 3.27399.
Train: 2018-08-02T15:03:49.055505: step 10334, loss 4.15545.
Train: 2018-08-02T15:03:49.117961: step 10335, loss 3.77768.
Train: 2018-08-02T15:03:49.164856: step 10336, loss 5.0369.
Train: 2018-08-02T15:03:49.227339: step 10337, loss 4.28137.
Train: 2018-08-02T15:03:49.289795: step 10338, loss 5.28875.
Train: 2018-08-02T15:03:49.336690: step 10339, loss 3.9036.
Train: 2018-08-02T15:03:49.399144: step 10340, loss 4.02952.
Test: 2018-08-02T15:03:49.602244: step 10340, loss 3.81863.
Train: 2018-08-02T15:03:49.664735: step 10341, loss 4.40729.
Train: 2018-08-02T15:03:49.711571: step 10342, loss 3.9036.
Train: 2018-08-02T15:03:49.774089: step 10343, loss 4.02952.
Train: 2018-08-02T15:03:49.836542: step 10344, loss 3.77768.
Train: 2018-08-02T15:03:49.899028: step 10345, loss 4.15545.
Train: 2018-08-02T15:03:49.961512: step 10346, loss 2.64437.
Train: 2018-08-02T15:03:50.008401: step 10347, loss 5.41467.
Train: 2018-08-02T15:03:50.070892: step 10348, loss 3.27399.
Train: 2018-08-02T15:03:50.133347: step 10349, loss 4.66953.
Train: 2018-08-02T15:03:50.180240: step 10350, loss 4.15545.
Test: 2018-08-02T15:03:50.383288: step 10350, loss 3.81863.
Train: 2018-08-02T15:03:50.445803: step 10351, loss 4.65914.
Train: 2018-08-02T15:03:50.508288: step 10352, loss 3.14807.
Train: 2018-08-02T15:03:50.555123: step 10353, loss 4.15545.
Train: 2018-08-02T15:03:50.617640: step 10354, loss 4.53321.
Train: 2018-08-02T15:03:50.680123: step 10355, loss 4.02952.
Train: 2018-08-02T15:03:50.742580: step 10356, loss 4.02952.
Train: 2018-08-02T15:03:50.789442: step 10357, loss 4.65914.
Train: 2018-08-02T15:03:50.851955: step 10358, loss 6.17021.
Train: 2018-08-02T15:03:50.914440: step 10359, loss 3.52583.
Train: 2018-08-02T15:03:50.961278: step 10360, loss 4.40729.
Test: 2018-08-02T15:03:51.164385: step 10360, loss 3.81863.
Train: 2018-08-02T15:03:51.226872: step 10361, loss 4.28137.
Train: 2018-08-02T15:03:51.289325: step 10362, loss 4.78506.
Train: 2018-08-02T15:03:51.351842: step 10363, loss 5.16283.
Train: 2018-08-02T15:03:51.398706: step 10364, loss 3.9036.
Train: 2018-08-02T15:03:51.461161: step 10365, loss 3.77768.
Train: 2018-08-02T15:03:51.523669: step 10366, loss 5.28875.
Train: 2018-08-02T15:03:51.570540: step 10367, loss 4.40729.
Train: 2018-08-02T15:03:51.632995: step 10368, loss 2.89622.
Train: 2018-08-02T15:03:51.695481: step 10369, loss 4.28137.
Train: 2018-08-02T15:03:51.742376: step 10370, loss 4.65914.
Test: 2018-08-02T15:03:51.945446: step 10370, loss 3.81863.
Train: 2018-08-02T15:03:52.007937: step 10371, loss 3.27399.
Train: 2018-08-02T15:03:52.070423: step 10372, loss 3.14807.
Train: 2018-08-02T15:03:52.132877: step 10373, loss 4.91098.
Train: 2018-08-02T15:03:52.179773: step 10374, loss 4.91098.
Train: 2018-08-02T15:03:52.242229: step 10375, loss 3.65176.
Train: 2018-08-02T15:03:52.304743: step 10376, loss 4.40729.
Train: 2018-08-02T15:03:52.367197: step 10377, loss 4.91098.
Train: 2018-08-02T15:03:52.414093: step 10378, loss 4.02952.
Train: 2018-08-02T15:03:52.476574: step 10379, loss 4.53321.
Train: 2018-08-02T15:03:52.539033: step 10380, loss 3.14807.
Test: 2018-08-02T15:03:52.726489: step 10380, loss 3.81863.
Train: 2018-08-02T15:03:52.789004: step 10381, loss 3.9036.
Train: 2018-08-02T15:03:52.851496: step 10382, loss 4.15545.
Train: 2018-08-02T15:03:52.913944: step 10383, loss 3.77768.
Train: 2018-08-02T15:03:52.960839: step 10384, loss 5.0369.
Train: 2018-08-02T15:03:53.023296: step 10385, loss 4.28137.
Train: 2018-08-02T15:03:53.085812: step 10386, loss 4.28137.
Train: 2018-08-02T15:03:53.148295: step 10387, loss 3.77768.
Train: 2018-08-02T15:03:53.195132: step 10388, loss 4.15545.
Train: 2018-08-02T15:03:53.257645: step 10389, loss 4.28137.
Train: 2018-08-02T15:03:53.320130: step 10390, loss 4.28137.
Test: 2018-08-02T15:03:53.507580: step 10390, loss 3.81863.
Train: 2018-08-02T15:03:53.570071: step 10391, loss 3.9036.
Train: 2018-08-02T15:03:53.632557: step 10392, loss 5.5406.
Train: 2018-08-02T15:03:53.695011: step 10393, loss 4.53321.
Train: 2018-08-02T15:03:53.757499: step 10394, loss 4.15545.
Train: 2018-08-02T15:03:53.804388: step 10395, loss 4.78506.
Train: 2018-08-02T15:03:53.866847: step 10396, loss 4.40729.
Train: 2018-08-02T15:03:53.929363: step 10397, loss 4.15545.
Train: 2018-08-02T15:03:53.991844: step 10398, loss 2.7703.
Train: 2018-08-02T15:03:54.054333: step 10399, loss 4.40729.
Train: 2018-08-02T15:03:54.101166: step 10400, loss 3.39991.
Test: 2018-08-02T15:03:54.304274: step 10400, loss 3.81863.
Train: 2018-08-02T15:03:54.819779: step 10401, loss 3.02214.
Train: 2018-08-02T15:03:54.882264: step 10402, loss 4.65914.
Train: 2018-08-02T15:03:54.944720: step 10403, loss 4.40729.
Train: 2018-08-02T15:03:55.007231: step 10404, loss 3.65176.
Train: 2018-08-02T15:03:55.069720: step 10405, loss 4.79627.
Train: 2018-08-02T15:03:55.132205: step 10406, loss 4.02952.
Train: 2018-08-02T15:03:55.179039: step 10407, loss 3.27399.
Train: 2018-08-02T15:03:55.241558: step 10408, loss 3.77768.
Train: 2018-08-02T15:03:55.304010: step 10409, loss 3.14807.
Train: 2018-08-02T15:03:55.350906: step 10410, loss 5.41467.
Test: 2018-08-02T15:03:55.553982: step 10410, loss 3.81863.
Train: 2018-08-02T15:03:55.616468: step 10411, loss 4.91098.
Train: 2018-08-02T15:03:55.678953: step 10412, loss 3.77768.
Train: 2018-08-02T15:03:55.741439: step 10413, loss 3.39991.
Train: 2018-08-02T15:03:55.788271: step 10414, loss 5.0369.
Train: 2018-08-02T15:03:55.850790: step 10415, loss 5.0369.
Train: 2018-08-02T15:03:55.913243: step 10416, loss 4.40729.
Train: 2018-08-02T15:03:55.960138: step 10417, loss 3.14807.
Train: 2018-08-02T15:03:56.022622: step 10418, loss 4.02952.
Train: 2018-08-02T15:03:56.069486: step 10419, loss 1.88044.
Train: 2018-08-02T15:03:56.131941: step 10420, loss 4.15545.
Test: 2018-08-02T15:03:56.319427: step 10420, loss 3.81863.
Train: 2018-08-02T15:03:56.381913: step 10421, loss 3.39991.
Train: 2018-08-02T15:03:56.444399: step 10422, loss 3.27399.
Train: 2018-08-02T15:03:56.506879: step 10423, loss 4.40729.
Train: 2018-08-02T15:03:56.553744: step 10424, loss 5.41467.
Train: 2018-08-02T15:03:56.616230: step 10425, loss 2.7703.
Train: 2018-08-02T15:03:56.678718: step 10426, loss 4.28137.
Train: 2018-08-02T15:03:56.725584: step 10427, loss 5.03691.
Train: 2018-08-02T15:03:56.788069: step 10428, loss 4.91098.
Train: 2018-08-02T15:03:56.850555: step 10429, loss 4.02952.
Train: 2018-08-02T15:03:56.897418: step 10430, loss 4.78506.
Test: 2018-08-02T15:03:57.100488: step 10430, loss 3.81863.
Train: 2018-08-02T15:03:57.162980: step 10431, loss 4.53321.
Train: 2018-08-02T15:03:57.225435: step 10432, loss 3.77768.
Train: 2018-08-02T15:03:57.287921: step 10433, loss 4.28137.
Train: 2018-08-02T15:03:57.350438: step 10434, loss 4.15545.
Train: 2018-08-02T15:03:57.397296: step 10435, loss 3.27399.
Train: 2018-08-02T15:03:57.459786: step 10436, loss 4.15545.
Train: 2018-08-02T15:03:57.522271: step 10437, loss 4.02952.
Train: 2018-08-02T15:03:57.569135: step 10438, loss 5.66652.
Train: 2018-08-02T15:03:57.631621: step 10439, loss 4.53321.
Train: 2018-08-02T15:03:57.694105: step 10440, loss 3.9036.
Test: 2018-08-02T15:03:57.897154: step 10440, loss 3.81863.
Train: 2018-08-02T15:03:57.944048: step 10441, loss 3.77768.
Train: 2018-08-02T15:03:58.006502: step 10442, loss 3.65176.
Train: 2018-08-02T15:03:58.068987: step 10443, loss 3.65176.
Train: 2018-08-02T15:03:58.131504: step 10444, loss 3.9036.
Train: 2018-08-02T15:03:58.178368: step 10445, loss 4.40729.
Train: 2018-08-02T15:03:58.240824: step 10446, loss 5.28875.
Train: 2018-08-02T15:03:58.303338: step 10447, loss 4.40729.
Train: 2018-08-02T15:03:58.350197: step 10448, loss 4.40729.
Train: 2018-08-02T15:03:58.412682: step 10449, loss 4.78506.
Train: 2018-08-02T15:03:58.475169: step 10450, loss 5.91836.
Test: 2018-08-02T15:03:58.678220: step 10450, loss 3.81863.
Train: 2018-08-02T15:03:58.725116: step 10451, loss 3.65176.
Train: 2018-08-02T15:03:58.787599: step 10452, loss 3.02214.
Train: 2018-08-02T15:03:58.850086: step 10453, loss 3.14807.
Train: 2018-08-02T15:03:58.912572: step 10454, loss 3.9036.
Train: 2018-08-02T15:03:58.959437: step 10455, loss 4.02952.
Train: 2018-08-02T15:03:59.021920: step 10456, loss 4.28137.
Train: 2018-08-02T15:03:59.084405: step 10457, loss 4.28137.
Train: 2018-08-02T15:03:59.136789: step 10458, loss 4.15545.
Train: 2018-08-02T15:03:59.199304: step 10459, loss 3.39991.
Train: 2018-08-02T15:03:59.246139: step 10460, loss 4.91098.
Test: 2018-08-02T15:03:59.449215: step 10460, loss 3.81863.
Train: 2018-08-02T15:03:59.511702: step 10461, loss 3.39991.
Train: 2018-08-02T15:03:59.574220: step 10462, loss 5.41467.
Train: 2018-08-02T15:03:59.636702: step 10463, loss 4.40729.
Train: 2018-08-02T15:03:59.683537: step 10464, loss 4.78506.
Train: 2018-08-02T15:03:59.746022: step 10465, loss 3.27399.
Train: 2018-08-02T15:03:59.808507: step 10466, loss 3.9036.
Train: 2018-08-02T15:03:59.855371: step 10467, loss 2.89622.
Train: 2018-08-02T15:03:59.917856: step 10468, loss 3.9036.
Train: 2018-08-02T15:03:59.980342: step 10469, loss 4.91098.
Train: 2018-08-02T15:04:00.042826: step 10470, loss 4.15545.
Test: 2018-08-02T15:04:00.230283: step 10470, loss 3.81863.
Train: 2018-08-02T15:04:00.292769: step 10471, loss 3.9036.
Train: 2018-08-02T15:04:00.355287: step 10472, loss 4.40729.
Train: 2018-08-02T15:04:00.417768: step 10473, loss 4.40729.
Train: 2018-08-02T15:04:00.480254: step 10474, loss 4.78506.
Train: 2018-08-02T15:04:00.527093: step 10475, loss 2.7703.
Train: 2018-08-02T15:04:00.589603: step 10476, loss 4.40729.
Train: 2018-08-02T15:04:00.652088: step 10477, loss 3.77768.
Train: 2018-08-02T15:04:00.698923: step 10478, loss 4.40729.
Train: 2018-08-02T15:04:00.761438: step 10479, loss 4.02952.
Train: 2018-08-02T15:04:00.823893: step 10480, loss 4.53321.
Test: 2018-08-02T15:04:01.011380: step 10480, loss 3.81863.
Train: 2018-08-02T15:04:01.073868: step 10481, loss 3.39991.
Train: 2018-08-02T15:04:01.136346: step 10482, loss 3.65176.
Train: 2018-08-02T15:04:01.198835: step 10483, loss 3.65176.
Train: 2018-08-02T15:04:01.261321: step 10484, loss 4.15545.
Train: 2018-08-02T15:04:01.323802: step 10485, loss 4.28137.
Train: 2018-08-02T15:04:01.370665: step 10486, loss 3.65176.
Train: 2018-08-02T15:04:01.433156: step 10487, loss 3.65176.
Train: 2018-08-02T15:04:01.480018: step 10488, loss 4.02952.
Train: 2018-08-02T15:04:01.542506: step 10489, loss 4.28137.
Train: 2018-08-02T15:04:01.604994: step 10490, loss 3.02214.
Test: 2018-08-02T15:04:01.808039: step 10490, loss 3.81863.
Train: 2018-08-02T15:04:01.870554: step 10491, loss 2.89622.
Train: 2018-08-02T15:04:01.917388: step 10492, loss 3.9036.
Train: 2018-08-02T15:04:01.979901: step 10493, loss 4.41264.
Train: 2018-08-02T15:04:02.042388: step 10494, loss 4.65914.
Train: 2018-08-02T15:04:02.104873: step 10495, loss 5.16283.
Train: 2018-08-02T15:04:02.151707: step 10496, loss 4.02952.
Train: 2018-08-02T15:04:02.214193: step 10497, loss 4.40729.
Train: 2018-08-02T15:04:02.276679: step 10498, loss 4.91098.
Train: 2018-08-02T15:04:02.323576: step 10499, loss 3.77768.
Train: 2018-08-02T15:04:02.386028: step 10500, loss 3.52583.
Test: 2018-08-02T15:04:02.589106: step 10500, loss 3.81863.
Train: 2018-08-02T15:04:03.073400: step 10501, loss 4.40729.
Train: 2018-08-02T15:04:03.135851: step 10502, loss 3.14807.
Train: 2018-08-02T15:04:03.198338: step 10503, loss 4.91098.
Train: 2018-08-02T15:04:03.260853: step 10504, loss 3.02214.
Train: 2018-08-02T15:04:03.307687: step 10505, loss 4.40729.
Train: 2018-08-02T15:04:03.370202: step 10506, loss 4.40729.
Train: 2018-08-02T15:04:03.432688: step 10507, loss 4.02952.
Train: 2018-08-02T15:04:03.479521: step 10508, loss 4.91098.
Train: 2018-08-02T15:04:03.542035: step 10509, loss 4.78506.
Train: 2018-08-02T15:04:03.604526: step 10510, loss 2.7703.
Test: 2018-08-02T15:04:03.791973: step 10510, loss 3.81863.
Train: 2018-08-02T15:04:03.854434: step 10511, loss 4.15545.
Train: 2018-08-02T15:04:03.916950: step 10512, loss 4.65914.
Train: 2018-08-02T15:04:03.979405: step 10513, loss 4.53321.
Train: 2018-08-02T15:04:04.026270: step 10514, loss 4.28137.
Train: 2018-08-02T15:04:04.088782: step 10515, loss 3.27399.
Train: 2018-08-02T15:04:04.151240: step 10516, loss 4.15545.
Train: 2018-08-02T15:04:04.198134: step 10517, loss 5.16283.
Train: 2018-08-02T15:04:04.260589: step 10518, loss 4.02952.
Train: 2018-08-02T15:04:04.323075: step 10519, loss 4.65914.
Train: 2018-08-02T15:04:04.369968: step 10520, loss 4.02952.
Test: 2018-08-02T15:04:04.573014: step 10520, loss 3.81863.
Train: 2018-08-02T15:04:04.635531: step 10521, loss 4.02952.
Train: 2018-08-02T15:04:04.698011: step 10522, loss 3.39991.
Train: 2018-08-02T15:04:04.760471: step 10523, loss 2.7703.
Train: 2018-08-02T15:04:04.807362: step 10524, loss 4.78506.
Train: 2018-08-02T15:04:04.869852: step 10525, loss 4.53321.
Train: 2018-08-02T15:04:04.932307: step 10526, loss 4.15545.
Train: 2018-08-02T15:04:04.994796: step 10527, loss 4.78506.
Train: 2018-08-02T15:04:05.041658: step 10528, loss 3.02214.
Train: 2018-08-02T15:04:05.104175: step 10529, loss 3.9036.
Train: 2018-08-02T15:04:05.166657: step 10530, loss 3.9036.
Test: 2018-08-02T15:04:05.354107: step 10530, loss 3.81863.
Train: 2018-08-02T15:04:05.416593: step 10531, loss 4.40729.
Train: 2018-08-02T15:04:05.479087: step 10532, loss 4.28137.
Train: 2018-08-02T15:04:05.541538: step 10533, loss 2.7703.
Train: 2018-08-02T15:04:05.604059: step 10534, loss 3.65176.
Train: 2018-08-02T15:04:05.650889: step 10535, loss 3.52583.
Train: 2018-08-02T15:04:05.713402: step 10536, loss 4.28137.
Train: 2018-08-02T15:04:05.775884: step 10537, loss 4.02952.
Train: 2018-08-02T15:04:05.822756: step 10538, loss 4.40729.
Train: 2018-08-02T15:04:05.885234: step 10539, loss 3.39991.
Train: 2018-08-02T15:04:05.947723: step 10540, loss 4.53321.
Test: 2018-08-02T15:04:06.135174: step 10540, loss 3.81863.
Train: 2018-08-02T15:04:06.197666: step 10541, loss 3.9036.
Train: 2018-08-02T15:04:06.260152: step 10542, loss 4.65914.
Train: 2018-08-02T15:04:06.322640: step 10543, loss 3.39991.
Train: 2018-08-02T15:04:06.385091: step 10544, loss 4.78506.
Train: 2018-08-02T15:04:06.447576: step 10545, loss 4.53321.
Train: 2018-08-02T15:04:06.494466: step 10546, loss 4.02952.
Train: 2018-08-02T15:04:06.556926: step 10547, loss 3.52583.
Train: 2018-08-02T15:04:06.619427: step 10548, loss 4.02952.
Train: 2018-08-02T15:04:06.666275: step 10549, loss 3.27399.
Train: 2018-08-02T15:04:06.728794: step 10550, loss 3.52583.
Test: 2018-08-02T15:04:06.931862: step 10550, loss 3.81863.
Train: 2018-08-02T15:04:06.978703: step 10551, loss 4.28137.
Train: 2018-08-02T15:04:07.041219: step 10552, loss 4.15545.
Train: 2018-08-02T15:04:07.103673: step 10553, loss 2.7703.
Train: 2018-08-02T15:04:07.166159: step 10554, loss 3.65176.
Train: 2018-08-02T15:04:07.228678: step 10555, loss 4.53321.
Train: 2018-08-02T15:04:07.275508: step 10556, loss 4.02952.
Train: 2018-08-02T15:04:07.338023: step 10557, loss 4.15545.
Train: 2018-08-02T15:04:07.400512: step 10558, loss 3.77768.
Train: 2018-08-02T15:04:07.462964: step 10559, loss 4.02952.
Train: 2018-08-02T15:04:07.509829: step 10560, loss 4.91098.
Test: 2018-08-02T15:04:07.712929: step 10560, loss 3.81863.
Train: 2018-08-02T15:04:07.775390: step 10561, loss 4.91098.
Train: 2018-08-02T15:04:07.837906: step 10562, loss 4.40729.
Train: 2018-08-02T15:04:07.900392: step 10563, loss 4.65914.
Train: 2018-08-02T15:04:07.947258: step 10564, loss 4.91098.
Train: 2018-08-02T15:04:08.009753: step 10565, loss 4.65914.
Train: 2018-08-02T15:04:08.072226: step 10566, loss 4.28137.
Train: 2018-08-02T15:04:08.119060: step 10567, loss 3.27399.
Train: 2018-08-02T15:04:08.181576: step 10568, loss 3.9036.
Train: 2018-08-02T15:04:08.244057: step 10569, loss 3.27399.
Train: 2018-08-02T15:04:08.290895: step 10570, loss 4.29816.
Test: 2018-08-02T15:04:08.478384: step 10570, loss 3.81863.
Train: 2018-08-02T15:04:08.540862: step 10571, loss 4.28137.
Train: 2018-08-02T15:04:08.603346: step 10572, loss 5.0369.
Train: 2018-08-02T15:04:08.650186: step 10573, loss 4.28137.
Train: 2018-08-02T15:04:08.712700: step 10574, loss 3.27399.
Train: 2018-08-02T15:04:08.775188: step 10575, loss 3.77768.
Train: 2018-08-02T15:04:08.837672: step 10576, loss 2.89622.
Train: 2018-08-02T15:04:08.884506: step 10577, loss 4.02952.
Train: 2018-08-02T15:04:08.946993: step 10578, loss 4.78506.
Train: 2018-08-02T15:04:09.009506: step 10579, loss 2.7703.
Train: 2018-08-02T15:04:09.071963: step 10580, loss 3.65176.
Test: 2018-08-02T15:04:09.259417: step 10580, loss 3.81863.
Train: 2018-08-02T15:04:09.321934: step 10581, loss 4.15545.
Train: 2018-08-02T15:04:09.384413: step 10582, loss 3.27399.
Train: 2018-08-02T15:04:09.446899: step 10583, loss 3.02214.
Train: 2018-08-02T15:04:09.493738: step 10584, loss 3.52583.
Train: 2018-08-02T15:04:09.556254: step 10585, loss 3.65176.
Train: 2018-08-02T15:04:09.618739: step 10586, loss 4.65914.
Train: 2018-08-02T15:04:09.665604: step 10587, loss 4.15545.
Train: 2018-08-02T15:04:09.728087: step 10588, loss 4.15545.
Train: 2018-08-02T15:04:09.790545: step 10589, loss 4.78506.
Train: 2018-08-02T15:04:09.837409: step 10590, loss 4.02952.
Test: 2018-08-02T15:04:10.040515: step 10590, loss 3.81863.
Train: 2018-08-02T15:04:10.102971: step 10591, loss 3.77768.
Train: 2018-08-02T15:04:10.165487: step 10592, loss 3.39991.
Train: 2018-08-02T15:04:10.227940: step 10593, loss 4.02952.
Train: 2018-08-02T15:04:10.274807: step 10594, loss 3.9036.
Train: 2018-08-02T15:04:10.337321: step 10595, loss 5.0369.
Train: 2018-08-02T15:04:10.399775: step 10596, loss 3.52583.
Train: 2018-08-02T15:04:10.446665: step 10597, loss 3.52583.
Train: 2018-08-02T15:04:10.509157: step 10598, loss 5.28875.
Train: 2018-08-02T15:04:10.571641: step 10599, loss 5.66652.
Train: 2018-08-02T15:04:10.618509: step 10600, loss 3.65176.
Test: 2018-08-02T15:04:10.821582: step 10600, loss 3.81863.
Train: 2018-08-02T15:04:11.321435: step 10601, loss 3.27399.
Train: 2018-08-02T15:04:11.383952: step 10602, loss 3.39991.
Train: 2018-08-02T15:04:11.446407: step 10603, loss 4.40729.
Train: 2018-08-02T15:04:11.508922: step 10604, loss 5.16283.
Train: 2018-08-02T15:04:11.571378: step 10605, loss 3.02214.
Train: 2018-08-02T15:04:11.618241: step 10606, loss 4.28137.
Train: 2018-08-02T15:04:11.680727: step 10607, loss 4.65914.
Train: 2018-08-02T15:04:11.727623: step 10608, loss 4.28137.
Train: 2018-08-02T15:04:11.790106: step 10609, loss 4.02952.
Train: 2018-08-02T15:04:11.852594: step 10610, loss 4.02952.
Test: 2018-08-02T15:04:12.040041: step 10610, loss 3.81863.
Train: 2018-08-02T15:04:12.102503: step 10611, loss 3.39991.
Train: 2018-08-02T15:04:12.164988: step 10612, loss 3.65176.
Train: 2018-08-02T15:04:12.227506: step 10613, loss 4.65914.
Train: 2018-08-02T15:04:12.274363: step 10614, loss 5.0369.
Train: 2018-08-02T15:04:12.336823: step 10615, loss 3.9036.
Train: 2018-08-02T15:04:12.399338: step 10616, loss 4.02952.
Train: 2018-08-02T15:04:12.446205: step 10617, loss 5.16283.
Train: 2018-08-02T15:04:12.508659: step 10618, loss 4.15545.
Train: 2018-08-02T15:04:12.571168: step 10619, loss 4.65914.
Train: 2018-08-02T15:04:12.618032: step 10620, loss 5.5406.
Test: 2018-08-02T15:04:12.821108: step 10620, loss 3.81863.
Train: 2018-08-02T15:04:12.883569: step 10621, loss 3.77768.
Train: 2018-08-02T15:04:12.946088: step 10622, loss 4.53321.
Train: 2018-08-02T15:04:13.008570: step 10623, loss 4.28137.
Train: 2018-08-02T15:04:13.055430: step 10624, loss 3.52583.
Train: 2018-08-02T15:04:13.117920: step 10625, loss 4.65914.
Train: 2018-08-02T15:04:13.180406: step 10626, loss 3.27399.
Train: 2018-08-02T15:04:13.227239: step 10627, loss 3.9036.
Train: 2018-08-02T15:04:13.289755: step 10628, loss 4.78506.
Train: 2018-08-02T15:04:13.352210: step 10629, loss 3.52583.
Train: 2018-08-02T15:04:13.399074: step 10630, loss 4.02952.
Test: 2018-08-02T15:04:13.602176: step 10630, loss 3.81863.
Train: 2018-08-02T15:04:13.664667: step 10631, loss 3.9036.
Train: 2018-08-02T15:04:13.727122: step 10632, loss 5.41467.
Train: 2018-08-02T15:04:13.774016: step 10633, loss 4.53321.
Train: 2018-08-02T15:04:13.836472: step 10634, loss 4.40729.
Train: 2018-08-02T15:04:13.898987: step 10635, loss 2.7703.
Train: 2018-08-02T15:04:13.961443: step 10636, loss 5.0369.
Train: 2018-08-02T15:04:14.008336: step 10637, loss 4.91098.
Train: 2018-08-02T15:04:14.070793: step 10638, loss 4.53321.
Train: 2018-08-02T15:04:14.133307: step 10639, loss 4.02952.
Train: 2018-08-02T15:04:14.180167: step 10640, loss 3.77768.
Test: 2018-08-02T15:04:14.384729: step 10640, loss 3.81863.
Train: 2018-08-02T15:04:14.447215: step 10641, loss 4.02952.
Train: 2018-08-02T15:04:14.509731: step 10642, loss 3.65176.
Train: 2018-08-02T15:04:14.556591: step 10643, loss 4.53321.
Train: 2018-08-02T15:04:14.619081: step 10644, loss 4.28137.
Train: 2018-08-02T15:04:14.681567: step 10645, loss 3.65176.
Train: 2018-08-02T15:04:14.728430: step 10646, loss 4.15545.
Train: 2018-08-02T15:04:14.790914: step 10647, loss 5.0369.
Train: 2018-08-02T15:04:14.853396: step 10648, loss 4.02952.
Train: 2018-08-02T15:04:14.900258: step 10649, loss 2.89622.
Train: 2018-08-02T15:04:14.962719: step 10650, loss 3.52583.
Test: 2018-08-02T15:04:15.165798: step 10650, loss 3.81863.
Train: 2018-08-02T15:04:15.228284: step 10651, loss 3.65176.
Train: 2018-08-02T15:04:15.275148: step 10652, loss 3.14807.
Train: 2018-08-02T15:04:15.337631: step 10653, loss 3.9036.
Train: 2018-08-02T15:04:15.400117: step 10654, loss 3.9036.
Train: 2018-08-02T15:04:15.462628: step 10655, loss 4.02952.
Train: 2018-08-02T15:04:15.509497: step 10656, loss 4.53321.
Train: 2018-08-02T15:04:15.571951: step 10657, loss 3.65176.
Train: 2018-08-02T15:04:15.634436: step 10658, loss 4.78506.
Train: 2018-08-02T15:04:15.681301: step 10659, loss 3.27399.
Train: 2018-08-02T15:04:15.743811: step 10660, loss 5.0369.
Test: 2018-08-02T15:04:15.946864: step 10660, loss 3.81863.
Train: 2018-08-02T15:04:15.993728: step 10661, loss 5.28875.
Train: 2018-08-02T15:04:16.056244: step 10662, loss 4.40729.
Train: 2018-08-02T15:04:16.118729: step 10663, loss 3.77768.
Train: 2018-08-02T15:04:16.181184: step 10664, loss 4.02952.
Train: 2018-08-02T15:04:16.228078: step 10665, loss 4.78506.
Train: 2018-08-02T15:04:16.290565: step 10666, loss 4.15545.
Train: 2018-08-02T15:04:16.353019: step 10667, loss 3.52583.
Train: 2018-08-02T15:04:16.399916: step 10668, loss 4.40729.
Train: 2018-08-02T15:04:16.462398: step 10669, loss 4.15545.
Train: 2018-08-02T15:04:16.509232: step 10670, loss 3.52583.
Test: 2018-08-02T15:04:16.712339: step 10670, loss 3.81863.
Train: 2018-08-02T15:04:16.774821: step 10671, loss 4.65914.
Train: 2018-08-02T15:04:16.837280: step 10672, loss 4.78506.
Train: 2018-08-02T15:04:16.899766: step 10673, loss 3.65176.
Train: 2018-08-02T15:04:16.946655: step 10674, loss 3.02214.
Train: 2018-08-02T15:04:17.009140: step 10675, loss 4.28137.
Train: 2018-08-02T15:04:17.071630: step 10676, loss 3.39991.
Train: 2018-08-02T15:04:17.118465: step 10677, loss 3.77768.
Train: 2018-08-02T15:04:17.180981: step 10678, loss 4.78506.
Train: 2018-08-02T15:04:17.227844: step 10679, loss 4.02952.
Train: 2018-08-02T15:04:17.290300: step 10680, loss 4.65914.
Test: 2018-08-02T15:04:17.493377: step 10680, loss 3.81863.
Train: 2018-08-02T15:04:17.555894: step 10681, loss 4.28137.
Train: 2018-08-02T15:04:17.618348: step 10682, loss 4.15545.
Train: 2018-08-02T15:04:17.665211: step 10683, loss 4.78506.
Train: 2018-08-02T15:04:17.727727: step 10684, loss 4.28137.
Train: 2018-08-02T15:04:17.790207: step 10685, loss 4.53321.
Train: 2018-08-02T15:04:17.852668: step 10686, loss 2.7703.
Train: 2018-08-02T15:04:17.899565: step 10687, loss 3.65176.
Train: 2018-08-02T15:04:17.962019: step 10688, loss 3.52583.
Train: 2018-08-02T15:04:18.024535: step 10689, loss 3.65176.
Train: 2018-08-02T15:04:18.071367: step 10690, loss 3.9036.
Test: 2018-08-02T15:04:18.274467: step 10690, loss 3.81863.
Train: 2018-08-02T15:04:18.321307: step 10691, loss 4.40729.
Train: 2018-08-02T15:04:18.383824: step 10692, loss 4.40729.
Train: 2018-08-02T15:04:18.446278: step 10693, loss 3.65176.
Train: 2018-08-02T15:04:18.508795: step 10694, loss 3.9036.
Train: 2018-08-02T15:04:18.571250: step 10695, loss 4.53321.
Train: 2018-08-02T15:04:18.633749: step 10696, loss 5.0369.
Train: 2018-08-02T15:04:18.680600: step 10697, loss 4.02952.
Train: 2018-08-02T15:04:18.743115: step 10698, loss 4.28137.
Train: 2018-08-02T15:04:18.805570: step 10699, loss 4.02952.
Train: 2018-08-02T15:04:18.852433: step 10700, loss 3.65176.
Test: 2018-08-02T15:04:19.055535: step 10700, loss 3.81863.
Train: 2018-08-02T15:04:19.586637: step 10701, loss 3.02214.
Train: 2018-08-02T15:04:19.649122: step 10702, loss 4.53321.
Train: 2018-08-02T15:04:19.695986: step 10703, loss 3.65176.
Train: 2018-08-02T15:04:19.758502: step 10704, loss 3.02214.
Train: 2018-08-02T15:04:19.820956: step 10705, loss 4.53321.
Train: 2018-08-02T15:04:19.883473: step 10706, loss 3.65176.
Train: 2018-08-02T15:04:19.930337: step 10707, loss 2.7703.
Train: 2018-08-02T15:04:19.992822: step 10708, loss 3.9036.
Train: 2018-08-02T15:04:20.055301: step 10709, loss 4.65914.
Train: 2018-08-02T15:04:20.102141: step 10710, loss 4.40729.
Test: 2018-08-02T15:04:20.305217: step 10710, loss 3.81863.
Train: 2018-08-02T15:04:20.367704: step 10711, loss 5.28875.
Train: 2018-08-02T15:04:20.430220: step 10712, loss 5.66652.
Train: 2018-08-02T15:04:20.477053: step 10713, loss 4.02952.
Train: 2018-08-02T15:04:20.539571: step 10714, loss 4.02952.
Train: 2018-08-02T15:04:20.602053: step 10715, loss 4.65914.
Train: 2018-08-02T15:04:20.664510: step 10716, loss 4.15545.
Train: 2018-08-02T15:04:20.711375: step 10717, loss 4.15545.
Train: 2018-08-02T15:04:20.773892: step 10718, loss 4.78506.
Train: 2018-08-02T15:04:20.836370: step 10719, loss 3.65176.
Train: 2018-08-02T15:04:20.883238: step 10720, loss 3.9036.
Test: 2018-08-02T15:04:21.086321: step 10720, loss 3.81863.
Train: 2018-08-02T15:04:21.133175: step 10721, loss 4.56679.
Train: 2018-08-02T15:04:21.195660: step 10722, loss 3.77768.
Train: 2018-08-02T15:04:21.258150: step 10723, loss 4.02952.
Train: 2018-08-02T15:04:21.320636: step 10724, loss 4.65914.
Train: 2018-08-02T15:04:21.367468: step 10725, loss 4.91098.
Train: 2018-08-02T15:04:21.429981: step 10726, loss 5.16283.
Train: 2018-08-02T15:04:21.492471: step 10727, loss 4.53321.
Train: 2018-08-02T15:04:21.554958: step 10728, loss 3.9036.
Train: 2018-08-02T15:04:21.601821: step 10729, loss 3.02214.
Train: 2018-08-02T15:04:21.664306: step 10730, loss 3.39991.
Test: 2018-08-02T15:04:21.867376: step 10730, loss 3.81863.
Train: 2018-08-02T15:04:21.929871: step 10731, loss 3.65176.
Train: 2018-08-02T15:04:21.976732: step 10732, loss 2.64438.
Train: 2018-08-02T15:04:22.039220: step 10733, loss 4.9164.
Train: 2018-08-02T15:04:22.101703: step 10734, loss 4.65914.
Train: 2018-08-02T15:04:22.164186: step 10735, loss 3.14807.
Train: 2018-08-02T15:04:22.211021: step 10736, loss 4.15545.
Train: 2018-08-02T15:04:22.273532: step 10737, loss 3.77768.
Train: 2018-08-02T15:04:22.336023: step 10738, loss 3.27399.
Train: 2018-08-02T15:04:22.382888: step 10739, loss 3.77768.
Train: 2018-08-02T15:04:22.445372: step 10740, loss 4.40729.
Test: 2018-08-02T15:04:22.648420: step 10740, loss 3.81863.
Train: 2018-08-02T15:04:22.710905: step 10741, loss 4.40729.
Train: 2018-08-02T15:04:22.773415: step 10742, loss 4.15545.
Train: 2018-08-02T15:04:22.835900: step 10743, loss 4.53321.
Train: 2018-08-02T15:04:22.882740: step 10744, loss 4.53321.
Train: 2018-08-02T15:04:22.945225: step 10745, loss 4.15545.
Train: 2018-08-02T15:04:23.007740: step 10746, loss 4.15545.
Train: 2018-08-02T15:04:23.054574: step 10747, loss 3.9036.
Train: 2018-08-02T15:04:23.117091: step 10748, loss 3.77768.
Train: 2018-08-02T15:04:23.179577: step 10749, loss 3.02214.
Train: 2018-08-02T15:04:23.242061: step 10750, loss 4.15545.
Test: 2018-08-02T15:04:23.445109: step 10750, loss 3.81863.
Train: 2018-08-02T15:04:23.492003: step 10751, loss 4.53321.
Train: 2018-08-02T15:04:23.554456: step 10752, loss 5.0369.
Train: 2018-08-02T15:04:23.616973: step 10753, loss 4.91098.
Train: 2018-08-02T15:04:23.679460: step 10754, loss 3.77768.
Train: 2018-08-02T15:04:23.741944: step 10755, loss 3.65176.
Train: 2018-08-02T15:04:23.788778: step 10756, loss 3.77768.
Train: 2018-08-02T15:04:23.851264: step 10757, loss 4.40729.
Train: 2018-08-02T15:04:23.913748: step 10758, loss 3.14807.
Train: 2018-08-02T15:04:23.960613: step 10759, loss 3.52583.
Train: 2018-08-02T15:04:24.023123: step 10760, loss 4.78506.
Test: 2018-08-02T15:04:24.226175: step 10760, loss 3.81863.
Train: 2018-08-02T15:04:24.273072: step 10761, loss 3.39991.
Train: 2018-08-02T15:04:24.335524: step 10762, loss 3.39991.
Train: 2018-08-02T15:04:24.398040: step 10763, loss 4.53321.
Train: 2018-08-02T15:04:24.460526: step 10764, loss 3.65176.
Train: 2018-08-02T15:04:24.507390: step 10765, loss 4.53321.
Train: 2018-08-02T15:04:24.569874: step 10766, loss 3.65176.
Train: 2018-08-02T15:04:24.632360: step 10767, loss 4.02952.
Train: 2018-08-02T15:04:24.679225: step 10768, loss 4.02952.
Train: 2018-08-02T15:04:24.741710: step 10769, loss 3.77768.
Train: 2018-08-02T15:04:24.804165: step 10770, loss 3.52583.
Test: 2018-08-02T15:04:25.007266: step 10770, loss 3.81863.
Train: 2018-08-02T15:04:25.054107: step 10771, loss 3.52583.
Train: 2018-08-02T15:04:25.116622: step 10772, loss 4.40729.
Train: 2018-08-02T15:04:25.179110: step 10773, loss 4.15545.
Train: 2018-08-02T15:04:25.241593: step 10774, loss 4.78506.
Train: 2018-08-02T15:04:25.304050: step 10775, loss 4.78506.
Train: 2018-08-02T15:04:25.350942: step 10776, loss 4.02952.
Train: 2018-08-02T15:04:25.413424: step 10777, loss 3.65176.
Train: 2018-08-02T15:04:25.475885: step 10778, loss 3.77768.
Train: 2018-08-02T15:04:25.538368: step 10779, loss 4.15545.
Train: 2018-08-02T15:04:25.585232: step 10780, loss 3.65176.
Test: 2018-08-02T15:04:25.788341: step 10780, loss 3.81863.
Train: 2018-08-02T15:04:25.850794: step 10781, loss 3.77768.
Train: 2018-08-02T15:04:25.897688: step 10782, loss 3.27399.
Train: 2018-08-02T15:04:25.960174: step 10783, loss 6.29613.
Train: 2018-08-02T15:04:26.022629: step 10784, loss 4.78506.
Train: 2018-08-02T15:04:26.069494: step 10785, loss 4.91098.
Train: 2018-08-02T15:04:26.131978: step 10786, loss 3.77768.
Train: 2018-08-02T15:04:26.194494: step 10787, loss 4.02952.
Train: 2018-08-02T15:04:26.241354: step 10788, loss 4.78506.
Train: 2018-08-02T15:04:26.303839: step 10789, loss 3.9036.
Train: 2018-08-02T15:04:26.366329: step 10790, loss 4.28137.
Test: 2018-08-02T15:04:26.553780: step 10790, loss 3.81863.
Train: 2018-08-02T15:04:26.616241: step 10791, loss 3.02214.
Train: 2018-08-02T15:04:26.678757: step 10792, loss 3.02214.
Train: 2018-08-02T15:04:26.741241: step 10793, loss 4.78506.
Train: 2018-08-02T15:04:26.803697: step 10794, loss 3.14807.
Train: 2018-08-02T15:04:26.866207: step 10795, loss 4.15545.
Train: 2018-08-02T15:04:26.913078: step 10796, loss 3.77768.
Train: 2018-08-02T15:04:26.975556: step 10797, loss 3.77768.
Train: 2018-08-02T15:04:27.022428: step 10798, loss 4.28137.
Train: 2018-08-02T15:04:27.084882: step 10799, loss 4.78506.
Train: 2018-08-02T15:04:27.147396: step 10800, loss 4.02952.
Test: 2018-08-02T15:04:27.350474: step 10800, loss 3.81863.
Train: 2018-08-02T15:04:27.865978: step 10801, loss 4.53321.
Train: 2018-08-02T15:04:27.928464: step 10802, loss 4.53321.
Train: 2018-08-02T15:04:27.990919: step 10803, loss 4.15545.
Train: 2018-08-02T15:04:28.037814: step 10804, loss 4.15545.
Train: 2018-08-02T15:04:28.100267: step 10805, loss 3.39991.
Train: 2018-08-02T15:04:28.162785: step 10806, loss 3.02214.
Train: 2018-08-02T15:04:28.209647: step 10807, loss 5.5406.
Train: 2018-08-02T15:04:28.272132: step 10808, loss 3.77768.
Train: 2018-08-02T15:04:28.334618: step 10809, loss 4.53321.
Train: 2018-08-02T15:04:28.381483: step 10810, loss 4.28137.
Test: 2018-08-02T15:04:28.584560: step 10810, loss 3.81863.
Train: 2018-08-02T15:04:28.647015: step 10811, loss 3.52583.
Train: 2018-08-02T15:04:28.709500: step 10812, loss 4.65914.
Train: 2018-08-02T15:04:28.756395: step 10813, loss 3.9036.
Train: 2018-08-02T15:04:28.818881: step 10814, loss 5.03934.
Train: 2018-08-02T15:04:28.881336: step 10815, loss 4.15545.
Train: 2018-08-02T15:04:28.928200: step 10816, loss 3.77768.
Train: 2018-08-02T15:04:28.990715: step 10817, loss 3.39991.
Train: 2018-08-02T15:04:29.053202: step 10818, loss 4.78506.
Train: 2018-08-02T15:04:29.115656: step 10819, loss 3.52583.
Train: 2018-08-02T15:04:29.162546: step 10820, loss 4.02952.
Test: 2018-08-02T15:04:29.365596: step 10820, loss 3.81863.
Train: 2018-08-02T15:04:29.428083: step 10821, loss 4.65914.
Train: 2018-08-02T15:04:29.490598: step 10822, loss 4.15545.
Train: 2018-08-02T15:04:29.553054: step 10823, loss 3.77768.
Train: 2018-08-02T15:04:29.599917: step 10824, loss 4.53321.
Train: 2018-08-02T15:04:29.662434: step 10825, loss 3.14807.
Train: 2018-08-02T15:04:29.724917: step 10826, loss 3.52583.
Train: 2018-08-02T15:04:29.771784: step 10827, loss 3.27399.
Train: 2018-08-02T15:04:29.834238: step 10828, loss 4.91098.
Train: 2018-08-02T15:04:29.896724: step 10829, loss 4.02952.
Train: 2018-08-02T15:04:29.943587: step 10830, loss 3.65176.
Test: 2018-08-02T15:04:30.146664: step 10830, loss 3.81863.
Train: 2018-08-02T15:04:30.209179: step 10831, loss 3.77768.
Train: 2018-08-02T15:04:30.271634: step 10832, loss 3.27399.
Train: 2018-08-02T15:04:30.318528: step 10833, loss 3.77768.
Train: 2018-08-02T15:04:30.380984: step 10834, loss 4.78506.
Train: 2018-08-02T15:04:30.443499: step 10835, loss 3.52583.
Train: 2018-08-02T15:04:30.490333: step 10836, loss 4.65914.
Train: 2018-08-02T15:04:30.552850: step 10837, loss 4.02952.
Train: 2018-08-02T15:04:30.599714: step 10838, loss 5.16283.
Train: 2018-08-02T15:04:30.662169: step 10839, loss 4.02952.
Train: 2018-08-02T15:04:30.724683: step 10840, loss 4.40729.
Test: 2018-08-02T15:04:30.927762: step 10840, loss 3.81863.
Train: 2018-08-02T15:04:30.990246: step 10841, loss 3.14807.
Train: 2018-08-02T15:04:31.037081: step 10842, loss 3.14807.
Train: 2018-08-02T15:04:31.099596: step 10843, loss 4.65914.
Train: 2018-08-02T15:04:31.162079: step 10844, loss 4.28137.
Train: 2018-08-02T15:04:31.208946: step 10845, loss 4.78506.
Train: 2018-08-02T15:04:31.271430: step 10846, loss 5.91836.
Train: 2018-08-02T15:04:31.333915: step 10847, loss 4.53321.
Train: 2018-08-02T15:04:31.380780: step 10848, loss 3.39991.
Train: 2018-08-02T15:04:31.443266: step 10849, loss 4.02952.
Train: 2018-08-02T15:04:31.505744: step 10850, loss 3.9036.
Test: 2018-08-02T15:04:31.693176: step 10850, loss 3.81863.
Train: 2018-08-02T15:04:31.755688: step 10851, loss 4.15545.
Train: 2018-08-02T15:04:31.818182: step 10852, loss 4.53321.
Train: 2018-08-02T15:04:31.880633: step 10853, loss 3.52583.
Train: 2018-08-02T15:04:31.943147: step 10854, loss 4.02952.
Train: 2018-08-02T15:04:31.989983: step 10855, loss 3.9036.
Train: 2018-08-02T15:04:32.052494: step 10856, loss 4.53321.
Train: 2018-08-02T15:04:32.114953: step 10857, loss 4.53321.
Train: 2018-08-02T15:04:32.161849: step 10858, loss 4.65914.
Train: 2018-08-02T15:04:32.224333: step 10859, loss 3.39991.
Train: 2018-08-02T15:04:32.286789: step 10860, loss 3.52583.
Test: 2018-08-02T15:04:32.489864: step 10860, loss 3.81863.
Train: 2018-08-02T15:04:32.536730: step 10861, loss 4.28137.
Train: 2018-08-02T15:04:32.599245: step 10862, loss 4.53321.
Train: 2018-08-02T15:04:32.661729: step 10863, loss 3.52583.
Train: 2018-08-02T15:04:32.724216: step 10864, loss 3.77768.
Train: 2018-08-02T15:04:32.771081: step 10865, loss 4.53321.
Train: 2018-08-02T15:04:32.833533: step 10866, loss 4.78506.
Train: 2018-08-02T15:04:32.896051: step 10867, loss 5.28875.
Train: 2018-08-02T15:04:32.942914: step 10868, loss 3.27399.
Train: 2018-08-02T15:04:33.005395: step 10869, loss 4.53321.
Train: 2018-08-02T15:04:33.067853: step 10870, loss 4.53321.
Test: 2018-08-02T15:04:33.255335: step 10870, loss 3.81863.
Train: 2018-08-02T15:04:33.317826: step 10871, loss 3.77768.
Train: 2018-08-02T15:04:33.364660: step 10872, loss 4.83543.
Train: 2018-08-02T15:04:33.427146: step 10873, loss 3.9036.
Train: 2018-08-02T15:04:33.474040: step 10874, loss 4.65914.
Train: 2018-08-02T15:04:33.536532: step 10875, loss 4.02952.
Train: 2018-08-02T15:04:33.599010: step 10876, loss 5.16283.
Train: 2018-08-02T15:04:33.645877: step 10877, loss 3.9036.
Train: 2018-08-02T15:04:33.708332: step 10878, loss 4.53321.
Train: 2018-08-02T15:04:33.770816: step 10879, loss 4.41266.
Train: 2018-08-02T15:04:33.833300: step 10880, loss 4.91098.
Test: 2018-08-02T15:04:34.020756: step 10880, loss 3.81863.
Train: 2018-08-02T15:04:34.083242: step 10881, loss 4.65914.
Train: 2018-08-02T15:04:34.145760: step 10882, loss 4.15545.
Train: 2018-08-02T15:04:34.208242: step 10883, loss 3.27399.
Train: 2018-08-02T15:04:34.255107: step 10884, loss 4.02952.
Train: 2018-08-02T15:04:34.317593: step 10885, loss 3.9036.
Train: 2018-08-02T15:04:34.380077: step 10886, loss 4.02952.
Train: 2018-08-02T15:04:34.442563: step 10887, loss 3.9036.
Train: 2018-08-02T15:04:34.489427: step 10888, loss 3.65176.
Train: 2018-08-02T15:04:34.551914: step 10889, loss 4.02952.
Train: 2018-08-02T15:04:34.614397: step 10890, loss 4.53321.
Test: 2018-08-02T15:04:34.801847: step 10890, loss 3.81863.
Train: 2018-08-02T15:04:34.864334: step 10891, loss 3.52583.
Train: 2018-08-02T15:04:34.926821: step 10892, loss 3.52583.
Train: 2018-08-02T15:04:34.989310: step 10893, loss 4.78506.
Train: 2018-08-02T15:04:35.036176: step 10894, loss 3.02214.
Train: 2018-08-02T15:04:35.098631: step 10895, loss 4.15545.
Train: 2018-08-02T15:04:35.161145: step 10896, loss 4.28137.
Train: 2018-08-02T15:04:35.207977: step 10897, loss 4.40729.
Train: 2018-08-02T15:04:35.270497: step 10898, loss 3.77768.
Train: 2018-08-02T15:04:35.332950: step 10899, loss 3.52583.
Train: 2018-08-02T15:04:35.379813: step 10900, loss 5.41467.
Test: 2018-08-02T15:04:35.582890: step 10900, loss 3.81863.
Train: 2018-08-02T15:04:36.114016: step 10901, loss 3.77768.
Train: 2018-08-02T15:04:36.176533: step 10902, loss 4.40729.
Train: 2018-08-02T15:04:36.239017: step 10903, loss 5.41467.
Train: 2018-08-02T15:04:36.301505: step 10904, loss 3.9036.
Train: 2018-08-02T15:04:36.348367: step 10905, loss 4.78506.
Train: 2018-08-02T15:04:36.410823: step 10906, loss 5.16283.
Train: 2018-08-02T15:04:36.473331: step 10907, loss 4.91098.
Train: 2018-08-02T15:04:36.520203: step 10908, loss 4.40729.
Train: 2018-08-02T15:04:36.582687: step 10909, loss 2.7703.
Train: 2018-08-02T15:04:36.645173: step 10910, loss 4.28137.
Test: 2018-08-02T15:04:36.832597: step 10910, loss 3.81863.
Train: 2018-08-02T15:04:36.895116: step 10911, loss 3.77768.
Train: 2018-08-02T15:04:36.957568: step 10912, loss 3.27399.
Train: 2018-08-02T15:04:37.004462: step 10913, loss 3.77768.
Train: 2018-08-02T15:04:37.066949: step 10914, loss 3.52583.
Train: 2018-08-02T15:04:37.129404: step 10915, loss 4.65914.
Train: 2018-08-02T15:04:37.191914: step 10916, loss 3.9036.
Train: 2018-08-02T15:04:37.238778: step 10917, loss 4.28137.
Train: 2018-08-02T15:04:37.301270: step 10918, loss 3.27399.
Train: 2018-08-02T15:04:37.363754: step 10919, loss 3.27399.
Train: 2018-08-02T15:04:37.410587: step 10920, loss 2.7703.
Test: 2018-08-02T15:04:37.613664: step 10920, loss 3.81863.
Train: 2018-08-02T15:04:37.676182: step 10921, loss 3.52583.
Train: 2018-08-02T15:04:37.738666: step 10922, loss 4.15545.
Train: 2018-08-02T15:04:37.785532: step 10923, loss 3.65176.
Train: 2018-08-02T15:04:37.848016: step 10924, loss 2.89622.
Train: 2018-08-02T15:04:37.910471: step 10925, loss 3.39991.
Train: 2018-08-02T15:04:37.957365: step 10926, loss 3.14807.
Train: 2018-08-02T15:04:38.019850: step 10927, loss 3.52583.
Train: 2018-08-02T15:04:38.082336: step 10928, loss 5.16283.
Train: 2018-08-02T15:04:38.129199: step 10929, loss 4.15545.
Train: 2018-08-02T15:04:38.191685: step 10930, loss 4.15545.
Test: 2018-08-02T15:04:38.394762: step 10930, loss 3.81863.
Train: 2018-08-02T15:04:38.457248: step 10931, loss 4.28137.
Train: 2018-08-02T15:04:38.504112: step 10932, loss 4.40729.
Train: 2018-08-02T15:04:38.566567: step 10933, loss 3.65176.
Train: 2018-08-02T15:04:38.629083: step 10934, loss 3.9036.
Train: 2018-08-02T15:04:38.691539: step 10935, loss 5.41467.
Train: 2018-08-02T15:04:38.738432: step 10936, loss 3.52583.
Train: 2018-08-02T15:04:38.800918: step 10937, loss 4.15545.
Train: 2018-08-02T15:04:38.863372: step 10938, loss 3.9036.
Train: 2018-08-02T15:04:38.910263: step 10939, loss 3.77768.
Train: 2018-08-02T15:04:38.972746: step 10940, loss 3.39991.
Test: 2018-08-02T15:04:39.160203: step 10940, loss 3.81863.
Train: 2018-08-02T15:04:39.222694: step 10941, loss 3.9036.
Train: 2018-08-02T15:04:39.285148: step 10942, loss 3.77768.
Train: 2018-08-02T15:04:39.347659: step 10943, loss 4.15545.
Train: 2018-08-02T15:04:39.394499: step 10944, loss 3.52583.
Train: 2018-08-02T15:04:39.457014: step 10945, loss 2.7703.
Train: 2018-08-02T15:04:39.519499: step 10946, loss 4.15545.
Train: 2018-08-02T15:04:39.566332: step 10947, loss 4.65914.
Train: 2018-08-02T15:04:39.628820: step 10948, loss 4.53321.
Train: 2018-08-02T15:04:39.691334: step 10949, loss 5.91836.
Train: 2018-08-02T15:04:39.738168: step 10950, loss 4.78506.
Test: 2018-08-02T15:04:39.941270: step 10950, loss 3.81863.
Train: 2018-08-02T15:04:40.003760: step 10951, loss 3.77768.
Train: 2018-08-02T15:04:40.066239: step 10952, loss 4.15545.
Train: 2018-08-02T15:04:40.128730: step 10953, loss 3.39991.
Train: 2018-08-02T15:04:40.175566: step 10954, loss 3.27399.
Train: 2018-08-02T15:04:40.238052: step 10955, loss 4.91098.
Train: 2018-08-02T15:04:40.300536: step 10956, loss 4.53321.
Train: 2018-08-02T15:04:40.363046: step 10957, loss 4.40729.
Train: 2018-08-02T15:04:40.409886: step 10958, loss 5.5406.
Train: 2018-08-02T15:04:40.472371: step 10959, loss 4.53321.
Train: 2018-08-02T15:04:40.534883: step 10960, loss 4.40729.
Test: 2018-08-02T15:04:40.737933: step 10960, loss 3.81863.
Train: 2018-08-02T15:04:40.784825: step 10961, loss 3.14807.
Train: 2018-08-02T15:04:40.847313: step 10962, loss 2.7703.
Train: 2018-08-02T15:04:40.909795: step 10963, loss 4.28137.
Train: 2018-08-02T15:04:40.972285: step 10964, loss 4.02952.
Train: 2018-08-02T15:04:41.034769: step 10965, loss 4.02952.
Train: 2018-08-02T15:04:41.081633: step 10966, loss 4.78506.
Train: 2018-08-02T15:04:41.144121: step 10967, loss 3.02214.
Train: 2018-08-02T15:04:41.206574: step 10968, loss 3.77768.
Train: 2018-08-02T15:04:41.253469: step 10969, loss 5.16283.
Train: 2018-08-02T15:04:41.315949: step 10970, loss 4.65914.
Test: 2018-08-02T15:04:41.519024: step 10970, loss 3.81863.
Train: 2018-08-02T15:04:41.581486: step 10971, loss 4.65914.
Train: 2018-08-02T15:04:41.628380: step 10972, loss 4.15545.
Train: 2018-08-02T15:04:41.690836: step 10973, loss 3.52583.
Train: 2018-08-02T15:04:41.753352: step 10974, loss 3.9036.
Train: 2018-08-02T15:04:41.815835: step 10975, loss 2.7703.
Train: 2018-08-02T15:04:41.862695: step 10976, loss 3.14807.
Train: 2018-08-02T15:04:41.925155: step 10977, loss 3.77768.
Train: 2018-08-02T15:04:41.987642: step 10978, loss 4.78506.
Train: 2018-08-02T15:04:42.034505: step 10979, loss 3.39991.
Train: 2018-08-02T15:04:42.097015: step 10980, loss 4.91098.
Test: 2018-08-02T15:04:42.300091: step 10980, loss 3.81863.
Train: 2018-08-02T15:04:42.346963: step 10981, loss 5.16283.
Train: 2018-08-02T15:04:42.409448: step 10982, loss 4.53321.
Train: 2018-08-02T15:04:42.471932: step 10983, loss 4.28137.
Train: 2018-08-02T15:04:42.534387: step 10984, loss 4.28137.
Train: 2018-08-02T15:04:42.581251: step 10985, loss 3.9036.
Train: 2018-08-02T15:04:42.643737: step 10986, loss 3.65176.
Train: 2018-08-02T15:04:42.706248: step 10987, loss 4.91098.
Train: 2018-08-02T15:04:42.753117: step 10988, loss 4.02952.
Train: 2018-08-02T15:04:42.815608: step 10989, loss 2.51845.
Train: 2018-08-02T15:04:42.878057: step 10990, loss 4.78506.
Test: 2018-08-02T15:04:43.081136: step 10990, loss 3.81863.
Train: 2018-08-02T15:04:43.127998: step 10991, loss 3.77768.
Train: 2018-08-02T15:04:43.190514: step 10992, loss 4.15545.
Train: 2018-08-02T15:04:43.252970: step 10993, loss 3.52583.
Train: 2018-08-02T15:04:43.315457: step 10994, loss 4.65914.
Train: 2018-08-02T15:04:43.362349: step 10995, loss 4.53321.
Train: 2018-08-02T15:04:43.424805: step 10996, loss 4.15545.
Train: 2018-08-02T15:04:43.487322: step 10997, loss 5.91836.
Train: 2018-08-02T15:04:43.534154: step 10998, loss 3.65176.
Train: 2018-08-02T15:04:43.596669: step 10999, loss 4.65914.
Train: 2018-08-02T15:04:43.659156: step 11000, loss 4.28137.
Test: 2018-08-02T15:04:43.862204: step 11000, loss 3.81863.
Train: 2018-08-02T15:04:44.440216: step 11001, loss 3.14807.
Train: 2018-08-02T15:04:44.502707: step 11002, loss 4.02952.
Train: 2018-08-02T15:04:44.549572: step 11003, loss 4.65914.
Train: 2018-08-02T15:04:44.612027: step 11004, loss 4.28137.
Train: 2018-08-02T15:04:44.674538: step 11005, loss 3.27399.
Train: 2018-08-02T15:04:44.721375: step 11006, loss 4.53321.
Train: 2018-08-02T15:04:44.783888: step 11007, loss 4.28137.
Train: 2018-08-02T15:04:44.846371: step 11008, loss 5.0369.
Train: 2018-08-02T15:04:44.893241: step 11009, loss 4.15545.
Train: 2018-08-02T15:04:44.955697: step 11010, loss 4.02952.
Test: 2018-08-02T15:04:45.158773: step 11010, loss 3.81863.
Train: 2018-08-02T15:04:45.205668: step 11011, loss 4.15545.
Train: 2018-08-02T15:04:45.268123: step 11012, loss 2.51845.
Train: 2018-08-02T15:04:45.330638: step 11013, loss 4.28137.
Train: 2018-08-02T15:04:45.393094: step 11014, loss 3.9036.
Train: 2018-08-02T15:04:45.439959: step 11015, loss 4.65914.
Train: 2018-08-02T15:04:45.502473: step 11016, loss 4.02952.
Train: 2018-08-02T15:04:45.564927: step 11017, loss 2.89622.
Train: 2018-08-02T15:04:45.611822: step 11018, loss 5.41467.
Train: 2018-08-02T15:04:45.674278: step 11019, loss 3.65176.
Train: 2018-08-02T15:04:45.736794: step 11020, loss 2.7703.
Test: 2018-08-02T15:04:45.924219: step 11020, loss 3.81863.
Train: 2018-08-02T15:04:45.986704: step 11021, loss 3.39991.
Train: 2018-08-02T15:04:46.049221: step 11022, loss 3.52583.
Train: 2018-08-02T15:04:46.096079: step 11023, loss 4.56679.
Train: 2018-08-02T15:04:46.158572: step 11024, loss 3.9036.
Train: 2018-08-02T15:04:46.205403: step 11025, loss 4.15545.
Train: 2018-08-02T15:04:46.267889: step 11026, loss 5.5406.
Train: 2018-08-02T15:04:46.330386: step 11027, loss 4.28137.
Train: 2018-08-02T15:04:46.392860: step 11028, loss 2.64438.
Train: 2018-08-02T15:04:46.439723: step 11029, loss 3.14807.
Train: 2018-08-02T15:04:46.502210: step 11030, loss 4.65914.
Test: 2018-08-02T15:04:46.705285: step 11030, loss 3.81863.
Train: 2018-08-02T15:04:46.767771: step 11031, loss 3.9036.
Train: 2018-08-02T15:04:46.814664: step 11032, loss 4.65914.
Train: 2018-08-02T15:04:46.877121: step 11033, loss 3.65176.
Train: 2018-08-02T15:04:46.939605: step 11034, loss 4.53321.
Train: 2018-08-02T15:04:47.002092: step 11035, loss 4.53321.
Train: 2018-08-02T15:04:47.048989: step 11036, loss 4.15545.
Train: 2018-08-02T15:04:47.111440: step 11037, loss 2.26661.
Train: 2018-08-02T15:04:47.173925: step 11038, loss 3.77768.
Train: 2018-08-02T15:04:47.220815: step 11039, loss 5.16283.
Train: 2018-08-02T15:04:47.283303: step 11040, loss 5.28875.
Test: 2018-08-02T15:04:47.486352: step 11040, loss 3.81863.
Train: 2018-08-02T15:04:47.548869: step 11041, loss 3.77768.
Train: 2018-08-02T15:04:47.611324: step 11042, loss 5.28875.
Train: 2018-08-02T15:04:47.658219: step 11043, loss 3.77768.
Train: 2018-08-02T15:04:47.720701: step 11044, loss 3.52583.
Train: 2018-08-02T15:04:47.783191: step 11045, loss 3.52583.
Train: 2018-08-02T15:04:47.845673: step 11046, loss 3.65176.
Train: 2018-08-02T15:04:47.892541: step 11047, loss 3.39991.
Train: 2018-08-02T15:04:47.954992: step 11048, loss 2.39253.
Train: 2018-08-02T15:04:48.001888: step 11049, loss 4.02952.
Train: 2018-08-02T15:04:48.064368: step 11050, loss 4.78506.
Test: 2018-08-02T15:04:48.267419: step 11050, loss 3.81863.
Train: 2018-08-02T15:04:48.329936: step 11051, loss 4.53321.
Train: 2018-08-02T15:04:48.392391: step 11052, loss 4.15545.
Train: 2018-08-02T15:04:48.439255: step 11053, loss 4.28137.
Train: 2018-08-02T15:04:48.501766: step 11054, loss 3.9036.
Train: 2018-08-02T15:04:48.564257: step 11055, loss 3.52583.
Train: 2018-08-02T15:04:48.626736: step 11056, loss 3.9036.
Train: 2018-08-02T15:04:48.673607: step 11057, loss 5.16283.
Train: 2018-08-02T15:04:48.736092: step 11058, loss 4.28137.
Train: 2018-08-02T15:04:48.798579: step 11059, loss 4.91098.
Train: 2018-08-02T15:04:48.845435: step 11060, loss 3.39991.
Test: 2018-08-02T15:04:49.048511: step 11060, loss 3.81863.
Train: 2018-08-02T15:04:49.110972: step 11061, loss 4.53321.
Train: 2018-08-02T15:04:49.157869: step 11062, loss 4.91098.
Train: 2018-08-02T15:04:49.220354: step 11063, loss 3.14807.
Train: 2018-08-02T15:04:49.282840: step 11064, loss 2.64438.
Train: 2018-08-02T15:04:49.345294: step 11065, loss 5.0369.
Train: 2018-08-02T15:04:49.392158: step 11066, loss 5.0369.
Train: 2018-08-02T15:04:49.454672: step 11067, loss 4.28137.
Train: 2018-08-02T15:04:49.517160: step 11068, loss 4.02952.
Train: 2018-08-02T15:04:49.579661: step 11069, loss 3.77768.
Train: 2018-08-02T15:04:49.626502: step 11070, loss 3.02214.
Test: 2018-08-02T15:04:49.829579: step 11070, loss 3.81863.
Train: 2018-08-02T15:04:49.876449: step 11071, loss 4.40729.
Train: 2018-08-02T15:04:49.938934: step 11072, loss 3.14807.
Train: 2018-08-02T15:04:50.001389: step 11073, loss 3.65176.
Train: 2018-08-02T15:04:50.063906: step 11074, loss 4.40729.
Train: 2018-08-02T15:04:50.110769: step 11075, loss 3.39991.
Train: 2018-08-02T15:04:50.173225: step 11076, loss 4.15545.
Train: 2018-08-02T15:04:50.235736: step 11077, loss 4.02952.
Train: 2018-08-02T15:04:50.298226: step 11078, loss 3.77768.
Train: 2018-08-02T15:04:50.345058: step 11079, loss 4.28137.
Train: 2018-08-02T15:04:50.407572: step 11080, loss 3.65176.
Test: 2018-08-02T15:04:50.610622: step 11080, loss 3.81863.
Train: 2018-08-02T15:04:50.673138: step 11081, loss 5.03691.
Train: 2018-08-02T15:04:50.719972: step 11082, loss 3.27399.
Train: 2018-08-02T15:04:50.782457: step 11083, loss 3.77768.
Train: 2018-08-02T15:04:50.844971: step 11084, loss 3.27399.
Train: 2018-08-02T15:04:50.907428: step 11085, loss 3.77768.
Train: 2018-08-02T15:04:50.954290: step 11086, loss 3.9036.
Train: 2018-08-02T15:04:51.016777: step 11087, loss 4.78506.
Train: 2018-08-02T15:04:51.079291: step 11088, loss 3.77768.
Train: 2018-08-02T15:04:51.141780: step 11089, loss 4.40729.
Train: 2018-08-02T15:04:51.188635: step 11090, loss 3.9036.
Test: 2018-08-02T15:04:51.391713: step 11090, loss 3.81863.
Train: 2018-08-02T15:04:51.454174: step 11091, loss 3.9036.
Train: 2018-08-02T15:04:51.516660: step 11092, loss 3.77768.
Train: 2018-08-02T15:04:51.563554: step 11093, loss 3.65176.
Train: 2018-08-02T15:04:51.626040: step 11094, loss 4.02952.
Train: 2018-08-02T15:04:51.688524: step 11095, loss 4.65914.
Train: 2018-08-02T15:04:51.735390: step 11096, loss 3.27399.
Train: 2018-08-02T15:04:51.797843: step 11097, loss 4.02952.
Train: 2018-08-02T15:04:51.860359: step 11098, loss 4.91098.
Train: 2018-08-02T15:04:51.922846: step 11099, loss 5.0369.
Train: 2018-08-02T15:04:51.969702: step 11100, loss 4.02952.
Test: 2018-08-02T15:04:52.172787: step 11100, loss 3.81863.
Train: 2018-08-02T15:04:52.657049: step 11101, loss 3.27399.
Train: 2018-08-02T15:04:52.719503: step 11102, loss 3.9036.
Train: 2018-08-02T15:04:52.781987: step 11103, loss 3.77768.
Train: 2018-08-02T15:04:52.844474: step 11104, loss 4.91098.
Train: 2018-08-02T15:04:52.906959: step 11105, loss 4.15545.
Train: 2018-08-02T15:04:52.953855: step 11106, loss 4.28137.
Train: 2018-08-02T15:04:53.016341: step 11107, loss 4.02952.
Train: 2018-08-02T15:04:53.078796: step 11108, loss 4.40729.
Train: 2018-08-02T15:04:53.125688: step 11109, loss 3.77768.
Train: 2018-08-02T15:04:53.188172: step 11110, loss 4.28137.
Test: 2018-08-02T15:04:53.391247: step 11110, loss 3.81863.
Train: 2018-08-02T15:04:53.453735: step 11111, loss 4.28137.
Train: 2018-08-02T15:04:53.500570: step 11112, loss 4.15545.
Train: 2018-08-02T15:04:53.563085: step 11113, loss 4.78506.
Train: 2018-08-02T15:04:53.625571: step 11114, loss 3.14807.
Train: 2018-08-02T15:04:53.688026: step 11115, loss 4.78506.
Train: 2018-08-02T15:04:53.734919: step 11116, loss 3.52583.
Train: 2018-08-02T15:04:53.797406: step 11117, loss 4.91098.
Train: 2018-08-02T15:04:53.859891: step 11118, loss 3.39991.
Train: 2018-08-02T15:04:53.906725: step 11119, loss 3.77768.
Train: 2018-08-02T15:04:53.969239: step 11120, loss 4.28137.
Test: 2018-08-02T15:04:54.172318: step 11120, loss 3.81863.
Train: 2018-08-02T15:04:54.234803: step 11121, loss 3.9036.
Train: 2018-08-02T15:04:54.281662: step 11122, loss 3.9036.
Train: 2018-08-02T15:04:54.344147: step 11123, loss 3.9036.
Train: 2018-08-02T15:04:54.406640: step 11124, loss 4.40729.
Train: 2018-08-02T15:04:54.469127: step 11125, loss 3.02214.
Train: 2018-08-02T15:04:54.515957: step 11126, loss 4.15545.
Train: 2018-08-02T15:04:54.578472: step 11127, loss 3.77768.
Train: 2018-08-02T15:04:54.640927: step 11128, loss 3.14807.
Train: 2018-08-02T15:04:54.687791: step 11129, loss 4.53321.
Train: 2018-08-02T15:04:54.750308: step 11130, loss 4.40729.
Test: 2018-08-02T15:04:54.953359: step 11130, loss 3.81863.
Train: 2018-08-02T15:04:55.000249: step 11131, loss 3.9036.
Train: 2018-08-02T15:04:55.062728: step 11132, loss 3.39991.
Train: 2018-08-02T15:04:55.125214: step 11133, loss 4.15545.
Train: 2018-08-02T15:04:55.187706: step 11134, loss 3.9036.
Train: 2018-08-02T15:04:55.234538: step 11135, loss 4.78506.
Train: 2018-08-02T15:04:55.297025: step 11136, loss 3.77768.
Train: 2018-08-02T15:04:55.359539: step 11137, loss 4.53321.
Train: 2018-08-02T15:04:55.406400: step 11138, loss 4.78506.
Train: 2018-08-02T15:04:55.468889: step 11139, loss 3.02214.
Train: 2018-08-02T15:04:55.531344: step 11140, loss 4.28137.
Test: 2018-08-02T15:04:55.718799: step 11140, loss 3.81863.
Train: 2018-08-02T15:04:55.781312: step 11141, loss 3.77768.
Train: 2018-08-02T15:04:55.843801: step 11142, loss 4.02952.
Train: 2018-08-02T15:04:55.906283: step 11143, loss 3.52583.
Train: 2018-08-02T15:04:55.953152: step 11144, loss 4.28137.
Train: 2018-08-02T15:04:56.015607: step 11145, loss 4.15545.
Train: 2018-08-02T15:04:56.078122: step 11146, loss 3.9036.
Train: 2018-08-02T15:04:56.124956: step 11147, loss 3.9036.
Train: 2018-08-02T15:04:56.187441: step 11148, loss 4.28137.
Train: 2018-08-02T15:04:56.249925: step 11149, loss 3.77768.
Train: 2018-08-02T15:04:56.296816: step 11150, loss 3.77768.
Test: 2018-08-02T15:04:56.499892: step 11150, loss 3.81863.
Train: 2018-08-02T15:04:56.562353: step 11151, loss 3.52583.
Train: 2018-08-02T15:04:56.624868: step 11152, loss 4.15545.
Train: 2018-08-02T15:04:56.687323: step 11153, loss 4.78506.
Train: 2018-08-02T15:04:56.734219: step 11154, loss 4.91098.
Train: 2018-08-02T15:04:56.796673: step 11155, loss 3.02214.
Train: 2018-08-02T15:04:56.859189: step 11156, loss 4.02952.
Train: 2018-08-02T15:04:56.906054: step 11157, loss 4.28137.
Train: 2018-08-02T15:04:56.968539: step 11158, loss 5.28875.
Train: 2018-08-02T15:04:57.031026: step 11159, loss 4.78506.
Train: 2018-08-02T15:04:57.077887: step 11160, loss 3.77768.
Test: 2018-08-02T15:04:57.280962: step 11160, loss 3.81863.
Train: 2018-08-02T15:04:57.343450: step 11161, loss 4.40729.
Train: 2018-08-02T15:04:57.405904: step 11162, loss 3.27399.
Train: 2018-08-02T15:04:57.452770: step 11163, loss 5.0369.
Train: 2018-08-02T15:04:57.515257: step 11164, loss 3.77768.
Train: 2018-08-02T15:04:57.577740: step 11165, loss 5.0369.
Train: 2018-08-02T15:04:57.640250: step 11166, loss 4.78506.
Train: 2018-08-02T15:04:57.687114: step 11167, loss 3.39991.
Train: 2018-08-02T15:04:57.749606: step 11168, loss 5.0369.
Train: 2018-08-02T15:04:57.796472: step 11169, loss 2.39253.
Train: 2018-08-02T15:04:57.858954: step 11170, loss 4.53321.
Test: 2018-08-02T15:04:58.046412: step 11170, loss 3.81863.
Train: 2018-08-02T15:04:58.108900: step 11171, loss 3.39991.
Train: 2018-08-02T15:04:58.171352: step 11172, loss 5.0369.
Train: 2018-08-02T15:04:58.233866: step 11173, loss 3.77768.
Train: 2018-08-02T15:04:58.280733: step 11174, loss 3.76089.
Train: 2018-08-02T15:04:58.343187: step 11175, loss 4.65914.
Train: 2018-08-02T15:04:58.390083: step 11176, loss 4.16051.
Train: 2018-08-02T15:04:58.452565: step 11177, loss 4.15545.
Train: 2018-08-02T15:04:58.515053: step 11178, loss 3.9036.
Train: 2018-08-02T15:04:58.561915: step 11179, loss 3.77768.
Train: 2018-08-02T15:04:58.624370: step 11180, loss 5.79244.
Test: 2018-08-02T15:04:58.827472: step 11180, loss 3.81863.
Train: 2018-08-02T15:04:58.889963: step 11181, loss 3.39991.
Train: 2018-08-02T15:04:58.936797: step 11182, loss 4.28137.
Train: 2018-08-02T15:04:58.999318: step 11183, loss 4.02952.
Train: 2018-08-02T15:04:59.061797: step 11184, loss 4.65914.
Train: 2018-08-02T15:04:59.124283: step 11185, loss 4.28137.
Train: 2018-08-02T15:04:59.171150: step 11186, loss 5.0369.
Train: 2018-08-02T15:04:59.233633: step 11187, loss 3.77768.
Train: 2018-08-02T15:04:59.296119: step 11188, loss 4.15545.
Train: 2018-08-02T15:04:59.342952: step 11189, loss 3.27399.
Train: 2018-08-02T15:04:59.405464: step 11190, loss 3.77768.
Test: 2018-08-02T15:04:59.592918: step 11190, loss 3.81863.
Train: 2018-08-02T15:04:59.655379: step 11191, loss 5.5406.
Train: 2018-08-02T15:04:59.717864: step 11192, loss 3.77768.
Train: 2018-08-02T15:04:59.780380: step 11193, loss 3.9036.
Train: 2018-08-02T15:04:59.827244: step 11194, loss 4.91098.
Train: 2018-08-02T15:04:59.889698: step 11195, loss 3.9036.
Train: 2018-08-02T15:04:59.952213: step 11196, loss 4.40729.
Train: 2018-08-02T15:04:59.999078: step 11197, loss 3.39991.
Train: 2018-08-02T15:05:00.061565: step 11198, loss 4.40729.
Train: 2018-08-02T15:05:00.124018: step 11199, loss 4.53321.
Train: 2018-08-02T15:05:00.170910: step 11200, loss 4.40729.
Test: 2018-08-02T15:05:00.373987: step 11200, loss 3.81863.
Train: 2018-08-02T15:05:00.936359: step 11201, loss 3.27399.
Train: 2018-08-02T15:05:00.998814: step 11202, loss 3.52583.
Train: 2018-08-02T15:05:01.061300: step 11203, loss 4.02952.
Train: 2018-08-02T15:05:01.123815: step 11204, loss 4.40729.
Train: 2018-08-02T15:05:01.186269: step 11205, loss 3.65176.
Train: 2018-08-02T15:05:01.233134: step 11206, loss 3.9036.
Train: 2018-08-02T15:05:01.295620: step 11207, loss 3.9036.
Train: 2018-08-02T15:05:01.358135: step 11208, loss 3.27399.
Train: 2018-08-02T15:05:01.404995: step 11209, loss 4.02952.
Train: 2018-08-02T15:05:01.467453: step 11210, loss 3.27399.
Test: 2018-08-02T15:05:01.670556: step 11210, loss 3.81863.
Train: 2018-08-02T15:05:01.717427: step 11211, loss 3.77768.
Train: 2018-08-02T15:05:01.779907: step 11212, loss 3.9036.
Train: 2018-08-02T15:05:01.842396: step 11213, loss 4.53321.
Train: 2018-08-02T15:05:01.904879: step 11214, loss 4.40729.
Train: 2018-08-02T15:05:01.951746: step 11215, loss 4.65914.
Train: 2018-08-02T15:05:02.014233: step 11216, loss 2.7703.
Train: 2018-08-02T15:05:02.076718: step 11217, loss 4.65914.
Train: 2018-08-02T15:05:02.123581: step 11218, loss 3.65176.
Train: 2018-08-02T15:05:02.186067: step 11219, loss 3.65176.
Train: 2018-08-02T15:05:02.248551: step 11220, loss 4.15545.
Test: 2018-08-02T15:05:02.451599: step 11220, loss 3.81863.
Train: 2018-08-02T15:05:02.514114: step 11221, loss 3.02214.
Train: 2018-08-02T15:05:02.560948: step 11222, loss 4.15545.
Train: 2018-08-02T15:05:02.623464: step 11223, loss 3.77768.
Train: 2018-08-02T15:05:02.685949: step 11224, loss 3.77768.
Train: 2018-08-02T15:05:02.748436: step 11225, loss 5.16283.
Train: 2018-08-02T15:05:02.810889: step 11226, loss 3.39991.
Train: 2018-08-02T15:05:02.857753: step 11227, loss 2.89622.
Train: 2018-08-02T15:05:02.920270: step 11228, loss 3.27399.
Train: 2018-08-02T15:05:02.982726: step 11229, loss 4.15545.
Train: 2018-08-02T15:05:03.029613: step 11230, loss 4.40729.
Test: 2018-08-02T15:05:03.232695: step 11230, loss 3.81863.
Train: 2018-08-02T15:05:03.295182: step 11231, loss 4.53321.
Train: 2018-08-02T15:05:03.357666: step 11232, loss 3.77768.
Train: 2018-08-02T15:05:03.404533: step 11233, loss 4.02952.
Train: 2018-08-02T15:05:03.467011: step 11234, loss 3.52583.
Train: 2018-08-02T15:05:03.529470: step 11235, loss 4.02952.
Train: 2018-08-02T15:05:03.576359: step 11236, loss 5.16283.
Train: 2018-08-02T15:05:03.638854: step 11237, loss 4.28137.
Train: 2018-08-02T15:05:03.701336: step 11238, loss 4.28137.
Train: 2018-08-02T15:05:03.763816: step 11239, loss 3.39991.
Train: 2018-08-02T15:05:03.810686: step 11240, loss 4.78506.
Test: 2018-08-02T15:05:04.013762: step 11240, loss 3.81863.
Train: 2018-08-02T15:05:04.076218: step 11241, loss 4.28137.
Train: 2018-08-02T15:05:04.123107: step 11242, loss 3.65176.
Train: 2018-08-02T15:05:04.185593: step 11243, loss 4.91098.
Train: 2018-08-02T15:05:04.248053: step 11244, loss 4.15545.
Train: 2018-08-02T15:05:04.310568: step 11245, loss 4.91098.
Train: 2018-08-02T15:05:04.357433: step 11246, loss 2.89622.
Train: 2018-08-02T15:05:04.419919: step 11247, loss 5.16283.
Train: 2018-08-02T15:05:04.482404: step 11248, loss 4.53321.
Train: 2018-08-02T15:05:04.529269: step 11249, loss 3.39991.
Train: 2018-08-02T15:05:04.591753: step 11250, loss 4.02952.
Test: 2018-08-02T15:05:04.794799: step 11250, loss 3.81863.
Train: 2018-08-02T15:05:04.857315: step 11251, loss 4.28137.
Train: 2018-08-02T15:05:04.904176: step 11252, loss 3.9036.
Train: 2018-08-02T15:05:04.966660: step 11253, loss 3.52583.
Train: 2018-08-02T15:05:05.029150: step 11254, loss 3.9036.
Train: 2018-08-02T15:05:05.091636: step 11255, loss 4.53321.
Train: 2018-08-02T15:05:05.138469: step 11256, loss 4.40729.
Train: 2018-08-02T15:05:05.200954: step 11257, loss 5.5406.
Train: 2018-08-02T15:05:05.263441: step 11258, loss 4.78506.
Train: 2018-08-02T15:05:05.310334: step 11259, loss 5.0369.
Train: 2018-08-02T15:05:05.372789: step 11260, loss 4.40729.
Test: 2018-08-02T15:05:05.575866: step 11260, loss 3.81863.
Train: 2018-08-02T15:05:05.638352: step 11261, loss 4.53321.
Train: 2018-08-02T15:05:05.700837: step 11262, loss 4.02952.
Train: 2018-08-02T15:05:05.747727: step 11263, loss 3.77768.
Train: 2018-08-02T15:05:05.810219: step 11264, loss 4.02952.
Train: 2018-08-02T15:05:05.872672: step 11265, loss 4.28137.
Train: 2018-08-02T15:05:05.919537: step 11266, loss 4.02952.
Train: 2018-08-02T15:05:05.982023: step 11267, loss 4.53321.
Train: 2018-08-02T15:05:06.044534: step 11268, loss 4.15545.
Train: 2018-08-02T15:05:06.091401: step 11269, loss 4.40729.
Train: 2018-08-02T15:05:06.153888: step 11270, loss 3.39991.
Test: 2018-08-02T15:05:06.356965: step 11270, loss 3.81863.
Train: 2018-08-02T15:05:06.419451: step 11271, loss 3.77768.
Train: 2018-08-02T15:05:06.481935: step 11272, loss 4.53321.
Train: 2018-08-02T15:05:06.528796: step 11273, loss 4.02952.
Train: 2018-08-02T15:05:06.591286: step 11274, loss 4.28137.
Train: 2018-08-02T15:05:06.653771: step 11275, loss 4.15545.
Train: 2018-08-02T15:05:06.700634: step 11276, loss 4.91098.
Train: 2018-08-02T15:05:06.763119: step 11277, loss 4.53321.
Train: 2018-08-02T15:05:06.825575: step 11278, loss 2.89622.
Train: 2018-08-02T15:05:06.872464: step 11279, loss 4.53321.
Train: 2018-08-02T15:05:06.934954: step 11280, loss 4.28137.
Test: 2018-08-02T15:05:07.138000: step 11280, loss 3.81863.
Train: 2018-08-02T15:05:07.200516: step 11281, loss 3.52583.
Train: 2018-08-02T15:05:07.247386: step 11282, loss 3.65176.
Train: 2018-08-02T15:05:07.309866: step 11283, loss 4.65914.
Train: 2018-08-02T15:05:07.372351: step 11284, loss 4.65914.
Train: 2018-08-02T15:05:07.419217: step 11285, loss 4.15545.
Train: 2018-08-02T15:05:07.481701: step 11286, loss 2.64438.
Train: 2018-08-02T15:05:07.544183: step 11287, loss 3.77768.
Train: 2018-08-02T15:05:07.606642: step 11288, loss 3.02214.
Train: 2018-08-02T15:05:07.653538: step 11289, loss 2.64437.
Train: 2018-08-02T15:05:07.715991: step 11290, loss 4.15545.
Test: 2018-08-02T15:05:07.903446: step 11290, loss 3.81863.
Train: 2018-08-02T15:05:07.965963: step 11291, loss 4.40729.
Train: 2018-08-02T15:05:08.028447: step 11292, loss 5.5406.
Train: 2018-08-02T15:05:08.090935: step 11293, loss 3.39991.
Train: 2018-08-02T15:05:08.137800: step 11294, loss 4.28137.
Train: 2018-08-02T15:05:08.200252: step 11295, loss 2.7703.
Train: 2018-08-02T15:05:08.262738: step 11296, loss 4.15545.
Train: 2018-08-02T15:05:08.325254: step 11297, loss 4.02952.
Train: 2018-08-02T15:05:08.372089: step 11298, loss 3.02214.
Train: 2018-08-02T15:05:08.434573: step 11299, loss 4.02952.
Train: 2018-08-02T15:05:08.481462: step 11300, loss 2.64438.
Test: 2018-08-02T15:05:08.684514: step 11300, loss 3.81863.
Train: 2018-08-02T15:05:09.168807: step 11301, loss 4.28137.
Train: 2018-08-02T15:05:09.215640: step 11302, loss 3.52583.
Train: 2018-08-02T15:05:09.278157: step 11303, loss 4.40729.
Train: 2018-08-02T15:05:09.340611: step 11304, loss 3.65176.
Train: 2018-08-02T15:05:09.403128: step 11305, loss 3.77768.
Train: 2018-08-02T15:05:09.449960: step 11306, loss 3.77768.
Train: 2018-08-02T15:05:09.512445: step 11307, loss 3.77768.
Train: 2018-08-02T15:05:09.574930: step 11308, loss 4.65914.
Train: 2018-08-02T15:05:09.621794: step 11309, loss 4.28137.
Train: 2018-08-02T15:05:09.684311: step 11310, loss 3.65176.
Test: 2018-08-02T15:05:09.871762: step 11310, loss 3.81863.
Train: 2018-08-02T15:05:09.934222: step 11311, loss 3.39991.
Train: 2018-08-02T15:05:09.996732: step 11312, loss 4.40729.
Train: 2018-08-02T15:05:10.059191: step 11313, loss 3.52583.
Train: 2018-08-02T15:05:10.106056: step 11314, loss 3.39991.
Train: 2018-08-02T15:05:10.168573: step 11315, loss 4.78506.
Train: 2018-08-02T15:05:10.231027: step 11316, loss 5.0369.
Train: 2018-08-02T15:05:10.293544: step 11317, loss 4.91098.
Train: 2018-08-02T15:05:10.340378: step 11318, loss 3.9036.
Train: 2018-08-02T15:05:10.402892: step 11319, loss 4.15545.
Train: 2018-08-02T15:05:10.465371: step 11320, loss 3.9036.
Test: 2018-08-02T15:05:10.652804: step 11320, loss 3.81863.
Train: 2018-08-02T15:05:10.715319: step 11321, loss 4.78506.
Train: 2018-08-02T15:05:10.777805: step 11322, loss 3.52583.
Train: 2018-08-02T15:05:10.840289: step 11323, loss 3.27399.
Train: 2018-08-02T15:05:10.887123: step 11324, loss 3.77768.
Train: 2018-08-02T15:05:10.934018: step 11325, loss 5.90997.
Train: 2018-08-02T15:05:10.996473: step 11326, loss 4.15545.
Train: 2018-08-02T15:05:11.058988: step 11327, loss 3.02214.
Train: 2018-08-02T15:05:11.105847: step 11328, loss 4.02952.
Train: 2018-08-02T15:05:11.168339: step 11329, loss 3.02214.
Train: 2018-08-02T15:05:11.230823: step 11330, loss 4.53321.
Test: 2018-08-02T15:05:11.418248: step 11330, loss 3.81863.
Train: 2018-08-02T15:05:11.480735: step 11331, loss 4.65914.
Train: 2018-08-02T15:05:11.543220: step 11332, loss 4.78506.
Train: 2018-08-02T15:05:11.605705: step 11333, loss 3.02214.
Train: 2018-08-02T15:05:11.652602: step 11334, loss 3.65176.
Train: 2018-08-02T15:05:11.715085: step 11335, loss 3.9036.
Train: 2018-08-02T15:05:11.777576: step 11336, loss 4.28137.
Train: 2018-08-02T15:05:11.824433: step 11337, loss 3.9036.
Train: 2018-08-02T15:05:11.886891: step 11338, loss 2.51845.
Train: 2018-08-02T15:05:11.949405: step 11339, loss 4.15545.
Train: 2018-08-02T15:05:11.996239: step 11340, loss 4.91098.
Test: 2018-08-02T15:05:12.199340: step 11340, loss 3.81863.
Train: 2018-08-02T15:05:12.261831: step 11341, loss 3.02214.
Train: 2018-08-02T15:05:12.324316: step 11342, loss 4.28137.
Train: 2018-08-02T15:05:12.371151: step 11343, loss 4.65914.
Train: 2018-08-02T15:05:12.433667: step 11344, loss 4.02952.
Train: 2018-08-02T15:05:12.496147: step 11345, loss 4.15545.
Train: 2018-08-02T15:05:12.542986: step 11346, loss 5.5406.
Train: 2018-08-02T15:05:12.605501: step 11347, loss 4.78506.
Train: 2018-08-02T15:05:12.667988: step 11348, loss 4.02952.
Train: 2018-08-02T15:05:12.714851: step 11349, loss 4.78506.
Train: 2018-08-02T15:05:12.777306: step 11350, loss 3.65176.
Test: 2018-08-02T15:05:12.980383: step 11350, loss 3.81863.
Train: 2018-08-02T15:05:13.027247: step 11351, loss 4.28137.
Train: 2018-08-02T15:05:13.089732: step 11352, loss 3.65176.
Train: 2018-08-02T15:05:13.152217: step 11353, loss 3.65176.
Train: 2018-08-02T15:05:13.214734: step 11354, loss 3.39991.
Train: 2018-08-02T15:05:13.261567: step 11355, loss 4.65914.
Train: 2018-08-02T15:05:13.324052: step 11356, loss 3.77768.
Train: 2018-08-02T15:05:13.386538: step 11357, loss 4.28137.
Train: 2018-08-02T15:05:13.433402: step 11358, loss 3.65176.
Train: 2018-08-02T15:05:13.495918: step 11359, loss 5.28875.
Train: 2018-08-02T15:05:13.558403: step 11360, loss 3.27399.
Test: 2018-08-02T15:05:13.745829: step 11360, loss 3.81863.
Train: 2018-08-02T15:05:13.808344: step 11361, loss 4.02952.
Train: 2018-08-02T15:05:13.870833: step 11362, loss 3.39991.
Train: 2018-08-02T15:05:13.933314: step 11363, loss 4.40729.
Train: 2018-08-02T15:05:13.980151: step 11364, loss 3.77768.
Train: 2018-08-02T15:05:14.042666: step 11365, loss 3.65176.
Train: 2018-08-02T15:05:14.105150: step 11366, loss 4.40729.
Train: 2018-08-02T15:05:14.152016: step 11367, loss 5.28875.
Train: 2018-08-02T15:05:14.214495: step 11368, loss 4.15545.
Train: 2018-08-02T15:05:14.276987: step 11369, loss 4.15545.
Train: 2018-08-02T15:05:14.323819: step 11370, loss 3.52583.
Test: 2018-08-02T15:05:14.526920: step 11370, loss 3.81863.
Train: 2018-08-02T15:05:14.589415: step 11371, loss 3.77768.
Train: 2018-08-02T15:05:14.651866: step 11372, loss 5.41467.
Train: 2018-08-02T15:05:14.698756: step 11373, loss 4.28137.
Train: 2018-08-02T15:05:14.761218: step 11374, loss 4.28137.
Train: 2018-08-02T15:05:14.823731: step 11375, loss 3.77768.
Train: 2018-08-02T15:05:14.870566: step 11376, loss 2.64438.
Train: 2018-08-02T15:05:14.933050: step 11377, loss 4.15545.
Train: 2018-08-02T15:05:14.995536: step 11378, loss 3.77768.
Train: 2018-08-02T15:05:15.042400: step 11379, loss 3.52583.
Train: 2018-08-02T15:05:15.104916: step 11380, loss 3.77768.
Test: 2018-08-02T15:05:15.307988: step 11380, loss 3.81863.
Train: 2018-08-02T15:05:15.370478: step 11381, loss 4.02952.
Train: 2018-08-02T15:05:15.432963: step 11382, loss 3.9036.
Train: 2018-08-02T15:05:15.479828: step 11383, loss 3.65176.
Train: 2018-08-02T15:05:15.542286: step 11384, loss 3.14807.
Train: 2018-08-02T15:05:15.604800: step 11385, loss 3.52583.
Train: 2018-08-02T15:05:15.651632: step 11386, loss 3.77768.
Train: 2018-08-02T15:05:15.714148: step 11387, loss 4.65914.
Train: 2018-08-02T15:05:15.776604: step 11388, loss 3.52583.
Train: 2018-08-02T15:05:15.823497: step 11389, loss 3.27399.
Train: 2018-08-02T15:05:15.885952: step 11390, loss 3.65176.
Test: 2018-08-02T15:05:16.089060: step 11390, loss 3.81863.
Train: 2018-08-02T15:05:16.135927: step 11391, loss 3.39991.
Train: 2018-08-02T15:05:16.198410: step 11392, loss 3.77768.
Train: 2018-08-02T15:05:16.260866: step 11393, loss 3.9036.
Train: 2018-08-02T15:05:16.323381: step 11394, loss 5.16283.
Train: 2018-08-02T15:05:16.385869: step 11395, loss 3.77768.
Train: 2018-08-02T15:05:16.432730: step 11396, loss 3.65176.
Train: 2018-08-02T15:05:16.495186: step 11397, loss 4.28137.
Train: 2018-08-02T15:05:16.557701: step 11398, loss 3.39991.
Train: 2018-08-02T15:05:16.604565: step 11399, loss 3.14807.
Train: 2018-08-02T15:05:16.667047: step 11400, loss 3.77768.
Test: 2018-08-02T15:05:16.854500: step 11400, loss 3.81863.
Train: 2018-08-02T15:05:17.338767: step 11401, loss 4.15545.
Train: 2018-08-02T15:05:17.401252: step 11402, loss 4.78506.
Train: 2018-08-02T15:05:17.448087: step 11403, loss 5.16283.
Train: 2018-08-02T15:05:17.510573: step 11404, loss 4.53321.
Train: 2018-08-02T15:05:17.573083: step 11405, loss 4.02952.
Train: 2018-08-02T15:05:17.619952: step 11406, loss 4.15545.
Train: 2018-08-02T15:05:17.682407: step 11407, loss 3.39991.
Train: 2018-08-02T15:05:17.744893: step 11408, loss 4.91098.
Train: 2018-08-02T15:05:17.791788: step 11409, loss 3.39991.
Train: 2018-08-02T15:05:17.854267: step 11410, loss 4.02952.
Test: 2018-08-02T15:05:18.041701: step 11410, loss 3.81863.
Train: 2018-08-02T15:05:18.104184: step 11411, loss 4.91098.
Train: 2018-08-02T15:05:18.166700: step 11412, loss 4.15545.
Train: 2018-08-02T15:05:18.229184: step 11413, loss 4.65914.
Train: 2018-08-02T15:05:18.276018: step 11414, loss 4.53321.
Train: 2018-08-02T15:05:18.338533: step 11415, loss 3.27399.
Train: 2018-08-02T15:05:18.401018: step 11416, loss 4.28137.
Train: 2018-08-02T15:05:18.447883: step 11417, loss 3.65176.
Train: 2018-08-02T15:05:18.510371: step 11418, loss 3.27399.
Train: 2018-08-02T15:05:18.572825: step 11419, loss 4.78506.
Train: 2018-08-02T15:05:18.619718: step 11420, loss 5.91836.
Test: 2018-08-02T15:05:18.822764: step 11420, loss 3.81863.
Train: 2018-08-02T15:05:18.885275: step 11421, loss 4.78506.
Train: 2018-08-02T15:05:18.947765: step 11422, loss 4.40729.
Train: 2018-08-02T15:05:19.010251: step 11423, loss 4.15545.
Train: 2018-08-02T15:05:19.057116: step 11424, loss 4.15545.
Train: 2018-08-02T15:05:19.119571: step 11425, loss 4.15545.
Train: 2018-08-02T15:05:19.182086: step 11426, loss 4.40729.
Train: 2018-08-02T15:05:19.228949: step 11427, loss 4.40729.
Train: 2018-08-02T15:05:19.291437: step 11428, loss 3.9036.
Train: 2018-08-02T15:05:19.353892: step 11429, loss 4.91098.
Train: 2018-08-02T15:05:19.400786: step 11430, loss 4.40729.
Test: 2018-08-02T15:05:19.603857: step 11430, loss 3.81863.
Train: 2018-08-02T15:05:19.666344: step 11431, loss 4.53321.
Train: 2018-08-02T15:05:19.728832: step 11432, loss 3.52583.
Train: 2018-08-02T15:05:19.791304: step 11433, loss 4.15545.
Train: 2018-08-02T15:05:19.838152: step 11434, loss 4.28137.
Train: 2018-08-02T15:05:19.900639: step 11435, loss 4.28137.
Train: 2018-08-02T15:05:19.963152: step 11436, loss 4.40729.
Train: 2018-08-02T15:05:20.009987: step 11437, loss 3.65176.
Train: 2018-08-02T15:05:20.072504: step 11438, loss 5.0369.
Train: 2018-08-02T15:05:20.134990: step 11439, loss 4.53321.
Train: 2018-08-02T15:05:20.197444: step 11440, loss 3.65176.
Test: 2018-08-02T15:05:20.384901: step 11440, loss 3.81863.
Train: 2018-08-02T15:05:20.447385: step 11441, loss 3.52583.
Train: 2018-08-02T15:05:20.509900: step 11442, loss 4.91098.
Train: 2018-08-02T15:05:20.572413: step 11443, loss 3.52583.
Train: 2018-08-02T15:05:20.619221: step 11444, loss 4.65914.
Train: 2018-08-02T15:05:20.681729: step 11445, loss 4.15545.
Train: 2018-08-02T15:05:20.744220: step 11446, loss 3.77768.
Train: 2018-08-02T15:05:20.791086: step 11447, loss 3.39991.
Train: 2018-08-02T15:05:20.853540: step 11448, loss 4.91098.
Train: 2018-08-02T15:05:20.916057: step 11449, loss 3.9036.
Train: 2018-08-02T15:05:20.962921: step 11450, loss 3.77768.
Test: 2018-08-02T15:05:21.165990: step 11450, loss 3.81863.
Train: 2018-08-02T15:05:21.228451: step 11451, loss 4.78506.
Train: 2018-08-02T15:05:21.275315: step 11452, loss 4.53321.
Train: 2018-08-02T15:05:21.337831: step 11453, loss 3.14807.
Train: 2018-08-02T15:05:21.400288: step 11454, loss 3.52583.
Train: 2018-08-02T15:05:21.462803: step 11455, loss 4.15545.
Train: 2018-08-02T15:05:21.509668: step 11456, loss 4.40729.
Train: 2018-08-02T15:05:21.572121: step 11457, loss 3.52583.
Train: 2018-08-02T15:05:21.634639: step 11458, loss 4.40729.
Train: 2018-08-02T15:05:21.681470: step 11459, loss 3.52583.
Train: 2018-08-02T15:05:21.743956: step 11460, loss 2.7703.
Test: 2018-08-02T15:05:21.947063: step 11460, loss 3.81863.
Train: 2018-08-02T15:05:22.009550: step 11461, loss 3.77768.
Train: 2018-08-02T15:05:22.056413: step 11462, loss 4.02952.
Train: 2018-08-02T15:05:22.118868: step 11463, loss 4.78506.
Train: 2018-08-02T15:05:22.181384: step 11464, loss 4.40729.
Train: 2018-08-02T15:05:22.243869: step 11465, loss 2.89622.
Train: 2018-08-02T15:05:22.290702: step 11466, loss 4.15545.
Train: 2018-08-02T15:05:22.353218: step 11467, loss 4.53321.
Train: 2018-08-02T15:05:22.415674: step 11468, loss 4.28137.
Train: 2018-08-02T15:05:22.462569: step 11469, loss 4.28137.
Train: 2018-08-02T15:05:22.525054: step 11470, loss 3.65176.
Test: 2018-08-02T15:05:22.728125: step 11470, loss 3.81863.
Train: 2018-08-02T15:05:22.774991: step 11471, loss 4.40729.
Train: 2018-08-02T15:05:22.837484: step 11472, loss 4.53321.
Train: 2018-08-02T15:05:22.899964: step 11473, loss 3.39991.
Train: 2018-08-02T15:05:22.962423: step 11474, loss 4.53321.
Train: 2018-08-02T15:05:23.024934: step 11475, loss 4.53321.
Train: 2018-08-02T15:05:23.056179: step 11476, loss 4.29816.
Train: 2018-08-02T15:05:23.118664: step 11477, loss 3.52583.
Train: 2018-08-02T15:05:23.181120: step 11478, loss 4.40729.
Train: 2018-08-02T15:05:23.243629: step 11479, loss 4.78506.
Train: 2018-08-02T15:05:23.290500: step 11480, loss 3.77768.
Test: 2018-08-02T15:05:23.493570: step 11480, loss 3.81863.
Train: 2018-08-02T15:05:23.556056: step 11481, loss 3.52583.
Train: 2018-08-02T15:05:23.618546: step 11482, loss 3.9036.
Train: 2018-08-02T15:05:23.665407: step 11483, loss 3.9036.
Train: 2018-08-02T15:05:23.727869: step 11484, loss 4.65914.
Train: 2018-08-02T15:05:23.790384: step 11485, loss 5.0369.
Train: 2018-08-02T15:05:23.837223: step 11486, loss 4.53321.
Train: 2018-08-02T15:05:23.899700: step 11487, loss 3.52583.
Train: 2018-08-02T15:05:23.962213: step 11488, loss 3.02214.
Train: 2018-08-02T15:05:24.009076: step 11489, loss 4.40729.
Train: 2018-08-02T15:05:24.071566: step 11490, loss 4.53321.
Test: 2018-08-02T15:05:24.274613: step 11490, loss 3.81863.
Train: 2018-08-02T15:05:24.337099: step 11491, loss 4.15545.
Train: 2018-08-02T15:05:24.383963: step 11492, loss 4.02952.
Train: 2018-08-02T15:05:24.446473: step 11493, loss 4.28137.
Train: 2018-08-02T15:05:24.508934: step 11494, loss 3.02214.
Train: 2018-08-02T15:05:24.555828: step 11495, loss 4.40729.
Train: 2018-08-02T15:05:24.618283: step 11496, loss 3.65176.
Train: 2018-08-02T15:05:24.680798: step 11497, loss 4.28137.
Train: 2018-08-02T15:05:24.727662: step 11498, loss 4.65914.
Train: 2018-08-02T15:05:24.790117: step 11499, loss 3.65176.
Train: 2018-08-02T15:05:24.852633: step 11500, loss 2.64438.
Test: 2018-08-02T15:05:25.055691: step 11500, loss 3.81863.
Train: 2018-08-02T15:05:25.539942: step 11501, loss 3.77768.
Train: 2018-08-02T15:05:25.602458: step 11502, loss 3.39991.
Train: 2018-08-02T15:05:25.664944: step 11503, loss 3.39991.
Train: 2018-08-02T15:05:25.711807: step 11504, loss 3.65176.
Train: 2018-08-02T15:05:25.774262: step 11505, loss 3.9036.
Train: 2018-08-02T15:05:25.836778: step 11506, loss 3.77768.
Train: 2018-08-02T15:05:25.883644: step 11507, loss 4.65914.
Train: 2018-08-02T15:05:25.946122: step 11508, loss 4.78506.
Train: 2018-08-02T15:05:26.008613: step 11509, loss 3.52583.
Train: 2018-08-02T15:05:26.055476: step 11510, loss 4.40729.
Test: 2018-08-02T15:05:26.258523: step 11510, loss 3.81863.
Train: 2018-08-02T15:05:26.321010: step 11511, loss 4.28137.
Train: 2018-08-02T15:05:26.383505: step 11512, loss 4.02952.
Train: 2018-08-02T15:05:26.430359: step 11513, loss 3.65176.
Train: 2018-08-02T15:05:26.492874: step 11514, loss 4.40729.
Train: 2018-08-02T15:05:26.555330: step 11515, loss 3.9036.
Train: 2018-08-02T15:05:26.602224: step 11516, loss 4.28137.
Train: 2018-08-02T15:05:26.664679: step 11517, loss 5.0369.
Train: 2018-08-02T15:05:26.727165: step 11518, loss 5.41467.
Train: 2018-08-02T15:05:26.774027: step 11519, loss 3.14807.
Train: 2018-08-02T15:05:26.836513: step 11520, loss 4.40729.
Test: 2018-08-02T15:05:27.039591: step 11520, loss 3.81863.
Train: 2018-08-02T15:05:27.102106: step 11521, loss 3.39991.
Train: 2018-08-02T15:05:27.164588: step 11522, loss 4.02952.
Train: 2018-08-02T15:05:27.211458: step 11523, loss 6.29613.
Train: 2018-08-02T15:05:27.273943: step 11524, loss 4.65914.
Train: 2018-08-02T15:05:27.336396: step 11525, loss 4.91098.
Train: 2018-08-02T15:05:27.398918: step 11526, loss 3.9036.
Train: 2018-08-02T15:05:27.445745: step 11527, loss 3.14807.
Train: 2018-08-02T15:05:27.508261: step 11528, loss 4.28137.
Train: 2018-08-02T15:05:27.570747: step 11529, loss 3.52583.
Train: 2018-08-02T15:05:27.617611: step 11530, loss 4.02952.
Test: 2018-08-02T15:05:27.820657: step 11530, loss 3.81863.
Train: 2018-08-02T15:05:27.883176: step 11531, loss 4.40729.
Train: 2018-08-02T15:05:27.945628: step 11532, loss 4.53321.
Train: 2018-08-02T15:05:28.008144: step 11533, loss 4.78506.
Train: 2018-08-02T15:05:28.054979: step 11534, loss 4.91098.
Train: 2018-08-02T15:05:28.117494: step 11535, loss 3.27399.
Train: 2018-08-02T15:05:28.179980: step 11536, loss 4.40729.
Train: 2018-08-02T15:05:28.226814: step 11537, loss 3.9036.
Train: 2018-08-02T15:05:28.289298: step 11538, loss 4.40729.
Train: 2018-08-02T15:05:28.351784: step 11539, loss 4.53321.
Train: 2018-08-02T15:05:28.398648: step 11540, loss 3.9036.
Test: 2018-08-02T15:05:28.601725: step 11540, loss 3.81863.
Train: 2018-08-02T15:05:28.664242: step 11541, loss 4.02952.
Train: 2018-08-02T15:05:28.726722: step 11542, loss 3.52583.
Train: 2018-08-02T15:05:28.773586: step 11543, loss 3.39991.
Train: 2018-08-02T15:05:28.836076: step 11544, loss 3.77768.
Train: 2018-08-02T15:05:28.898563: step 11545, loss 4.02952.
Train: 2018-08-02T15:05:28.961046: step 11546, loss 2.7703.
Train: 2018-08-02T15:05:29.007910: step 11547, loss 4.02952.
Train: 2018-08-02T15:05:29.070395: step 11548, loss 3.27399.
Train: 2018-08-02T15:05:29.132883: step 11549, loss 4.40729.
Train: 2018-08-02T15:05:29.179745: step 11550, loss 3.52583.
Test: 2018-08-02T15:05:29.382816: step 11550, loss 3.81863.
Train: 2018-08-02T15:05:29.445277: step 11551, loss 4.40729.
Train: 2018-08-02T15:05:29.492172: step 11552, loss 4.28137.
Train: 2018-08-02T15:05:29.554657: step 11553, loss 3.27399.
Train: 2018-08-02T15:05:29.617144: step 11554, loss 3.9036.
Train: 2018-08-02T15:05:29.679599: step 11555, loss 3.52583.
Train: 2018-08-02T15:05:29.726494: step 11556, loss 4.91098.
Train: 2018-08-02T15:05:29.788978: step 11557, loss 4.78506.
Train: 2018-08-02T15:05:29.851463: step 11558, loss 3.27399.
Train: 2018-08-02T15:05:29.898297: step 11559, loss 4.53321.
Train: 2018-08-02T15:05:29.960812: step 11560, loss 4.28137.
Test: 2018-08-02T15:05:30.210748: step 11560, loss 3.81863.
Train: 2018-08-02T15:05:30.273235: step 11561, loss 4.02952.
Train: 2018-08-02T15:05:30.335724: step 11562, loss 3.39991.
Train: 2018-08-02T15:05:30.398178: step 11563, loss 4.15545.
Train: 2018-08-02T15:05:30.445074: step 11564, loss 4.15545.
Train: 2018-08-02T15:05:30.507530: step 11565, loss 3.77768.
Train: 2018-08-02T15:05:30.570044: step 11566, loss 4.40729.
Train: 2018-08-02T15:05:30.616908: step 11567, loss 4.02952.
Train: 2018-08-02T15:05:30.679394: step 11568, loss 5.41467.
Train: 2018-08-02T15:05:30.741883: step 11569, loss 5.0369.
Train: 2018-08-02T15:05:30.788739: step 11570, loss 3.27399.
Test: 2018-08-02T15:05:30.991790: step 11570, loss 3.81863.
Train: 2018-08-02T15:05:31.038686: step 11571, loss 3.9036.
Train: 2018-08-02T15:05:31.101170: step 11572, loss 3.9036.
Train: 2018-08-02T15:05:31.163657: step 11573, loss 3.14807.
Train: 2018-08-02T15:05:31.226142: step 11574, loss 4.28137.
Train: 2018-08-02T15:05:31.272975: step 11575, loss 4.15545.
Train: 2018-08-02T15:05:31.335488: step 11576, loss 4.02952.
Train: 2018-08-02T15:05:31.397977: step 11577, loss 3.65176.
Train: 2018-08-02T15:05:31.444840: step 11578, loss 4.28137.
Train: 2018-08-02T15:05:31.507324: step 11579, loss 3.14807.
Train: 2018-08-02T15:05:31.569810: step 11580, loss 4.78506.
Test: 2018-08-02T15:05:31.772857: step 11580, loss 3.81863.
Train: 2018-08-02T15:05:31.819751: step 11581, loss 4.02952.
Train: 2018-08-02T15:05:31.882239: step 11582, loss 4.53321.
Train: 2018-08-02T15:05:31.944693: step 11583, loss 4.02952.
Train: 2018-08-02T15:05:32.007203: step 11584, loss 4.02952.
Train: 2018-08-02T15:05:32.054073: step 11585, loss 4.65914.
Train: 2018-08-02T15:05:32.116552: step 11586, loss 5.41467.
Train: 2018-08-02T15:05:32.179013: step 11587, loss 3.65176.
Train: 2018-08-02T15:05:32.225907: step 11588, loss 4.91098.
Train: 2018-08-02T15:05:32.288393: step 11589, loss 3.77768.
Train: 2018-08-02T15:05:32.350877: step 11590, loss 5.16283.
Test: 2018-08-02T15:05:32.553949: step 11590, loss 3.81863.
Train: 2018-08-02T15:05:32.678925: step 11591, loss 4.40729.
Train: 2018-08-02T15:05:32.725760: step 11592, loss 3.9036.
Train: 2018-08-02T15:05:32.788278: step 11593, loss 3.39991.
Train: 2018-08-02T15:05:32.850760: step 11594, loss 3.27399.
Train: 2018-08-02T15:05:32.897626: step 11595, loss 2.7703.
Train: 2018-08-02T15:05:32.960080: step 11596, loss 4.40729.
Train: 2018-08-02T15:05:33.022597: step 11597, loss 4.15545.
Train: 2018-08-02T15:05:33.069460: step 11598, loss 4.02952.
Train: 2018-08-02T15:05:33.131938: step 11599, loss 4.65914.
Train: 2018-08-02T15:05:33.194429: step 11600, loss 4.28137.
Test: 2018-08-02T15:05:33.397501: step 11600, loss 3.81863.
Train: 2018-08-02T15:05:33.959875: step 11601, loss 3.77768.
Train: 2018-08-02T15:05:34.022362: step 11602, loss 4.40729.
Train: 2018-08-02T15:05:34.069225: step 11603, loss 4.28137.
Train: 2018-08-02T15:05:34.131682: step 11604, loss 4.78506.
Train: 2018-08-02T15:05:34.194165: step 11605, loss 4.53321.
Train: 2018-08-02T15:05:34.256650: step 11606, loss 3.39991.
Train: 2018-08-02T15:05:34.319160: step 11607, loss 4.28137.
Train: 2018-08-02T15:05:34.366002: step 11608, loss 4.28137.
Train: 2018-08-02T15:05:34.428516: step 11609, loss 3.39991.
Train: 2018-08-02T15:05:34.491001: step 11610, loss 3.77768.
Test: 2018-08-02T15:05:34.694050: step 11610, loss 3.81863.
Train: 2018-08-02T15:05:34.740938: step 11611, loss 4.28137.
Train: 2018-08-02T15:05:34.803422: step 11612, loss 4.02952.
Train: 2018-08-02T15:05:34.865882: step 11613, loss 3.77768.
Train: 2018-08-02T15:05:34.928402: step 11614, loss 4.28137.
Train: 2018-08-02T15:05:34.990854: step 11615, loss 3.39991.
Train: 2018-08-02T15:05:35.037718: step 11616, loss 3.77768.
Train: 2018-08-02T15:05:35.100233: step 11617, loss 4.02952.
Train: 2018-08-02T15:05:35.162721: step 11618, loss 3.9036.
Train: 2018-08-02T15:05:35.209583: step 11619, loss 3.52583.
Train: 2018-08-02T15:05:35.272069: step 11620, loss 3.77768.
Test: 2018-08-02T15:05:35.459520: step 11620, loss 3.81863.
Train: 2018-08-02T15:05:35.521980: step 11621, loss 4.28137.
Train: 2018-08-02T15:05:35.584495: step 11622, loss 5.16283.
Train: 2018-08-02T15:05:35.646982: step 11623, loss 4.15545.
Train: 2018-08-02T15:05:35.709467: step 11624, loss 3.77768.
Train: 2018-08-02T15:05:35.756333: step 11625, loss 4.02952.
Train: 2018-08-02T15:05:35.818784: step 11626, loss 2.89622.
Train: 2018-08-02T15:05:35.865656: step 11627, loss 4.29816.
Train: 2018-08-02T15:05:35.912513: step 11628, loss 3.77768.
Train: 2018-08-02T15:05:35.974999: step 11629, loss 3.52583.
Train: 2018-08-02T15:05:36.037515: step 11630, loss 3.77768.
Test: 2018-08-02T15:05:36.240590: step 11630, loss 3.81863.
Train: 2018-08-02T15:05:36.287426: step 11631, loss 3.27399.
Train: 2018-08-02T15:05:36.349941: step 11632, loss 4.28137.
Train: 2018-08-02T15:05:36.412427: step 11633, loss 3.77768.
Train: 2018-08-02T15:05:36.474912: step 11634, loss 4.02952.
Train: 2018-08-02T15:05:36.521777: step 11635, loss 3.39991.
Train: 2018-08-02T15:05:36.584256: step 11636, loss 4.65914.
Train: 2018-08-02T15:05:36.646743: step 11637, loss 4.65914.
Train: 2018-08-02T15:05:36.693613: step 11638, loss 5.41467.
Train: 2018-08-02T15:05:36.756092: step 11639, loss 3.52583.
Train: 2018-08-02T15:05:36.818551: step 11640, loss 4.02952.
Test: 2018-08-02T15:05:37.006007: step 11640, loss 3.81863.
Train: 2018-08-02T15:05:37.068492: step 11641, loss 3.52583.
Train: 2018-08-02T15:05:37.131007: step 11642, loss 5.16283.
Train: 2018-08-02T15:05:37.193493: step 11643, loss 3.39991.
Train: 2018-08-02T15:05:37.240329: step 11644, loss 3.52583.
Train: 2018-08-02T15:05:37.302843: step 11645, loss 3.52583.
Train: 2018-08-02T15:05:37.365327: step 11646, loss 3.9036.
Train: 2018-08-02T15:05:37.412161: step 11647, loss 3.77768.
Train: 2018-08-02T15:05:37.474679: step 11648, loss 3.02214.
Train: 2018-08-02T15:05:37.537133: step 11649, loss 4.40729.
Train: 2018-08-02T15:05:37.584027: step 11650, loss 4.40729.
Test: 2018-08-02T15:05:37.787098: step 11650, loss 3.81863.
Train: 2018-08-02T15:05:37.849560: step 11651, loss 3.9036.
Train: 2018-08-02T15:05:37.912076: step 11652, loss 5.16283.
Train: 2018-08-02T15:05:37.958939: step 11653, loss 3.77768.
Train: 2018-08-02T15:05:38.021422: step 11654, loss 3.9036.
Train: 2018-08-02T15:05:38.083911: step 11655, loss 3.9036.
Train: 2018-08-02T15:05:38.130776: step 11656, loss 4.02952.
Train: 2018-08-02T15:05:38.193254: step 11657, loss 4.40729.
Train: 2018-08-02T15:05:38.255747: step 11658, loss 4.40729.
Train: 2018-08-02T15:05:38.302609: step 11659, loss 4.15545.
Train: 2018-08-02T15:05:38.365094: step 11660, loss 3.39991.
Test: 2018-08-02T15:05:38.568171: step 11660, loss 3.81863.
Train: 2018-08-02T15:05:38.630657: step 11661, loss 4.91098.
Train: 2018-08-02T15:05:38.677521: step 11662, loss 4.15545.
Train: 2018-08-02T15:05:38.740001: step 11663, loss 4.78506.
Train: 2018-08-02T15:05:38.802492: step 11664, loss 4.65914.
Train: 2018-08-02T15:05:38.864948: step 11665, loss 4.02952.
Train: 2018-08-02T15:05:38.911811: step 11666, loss 3.9036.
Train: 2018-08-02T15:05:38.974295: step 11667, loss 4.15545.
Train: 2018-08-02T15:05:39.036806: step 11668, loss 4.15545.
Train: 2018-08-02T15:05:39.083645: step 11669, loss 4.53321.
Train: 2018-08-02T15:05:39.146163: step 11670, loss 4.40729.
Test: 2018-08-02T15:05:39.349216: step 11670, loss 3.81863.
Train: 2018-08-02T15:05:39.396072: step 11671, loss 4.02952.
Train: 2018-08-02T15:05:39.458559: step 11672, loss 4.28137.
Train: 2018-08-02T15:05:39.521043: step 11673, loss 3.39991.
Train: 2018-08-02T15:05:39.583529: step 11674, loss 4.53321.
Train: 2018-08-02T15:05:39.646014: step 11675, loss 4.15545.
Train: 2018-08-02T15:05:39.692908: step 11676, loss 3.77768.
Train: 2018-08-02T15:05:39.755393: step 11677, loss 4.15545.
Train: 2018-08-02T15:05:39.802252: step 11678, loss 4.15545.
Train: 2018-08-02T15:05:39.864744: step 11679, loss 3.77768.
Train: 2018-08-02T15:05:39.927251: step 11680, loss 3.9036.
Test: 2018-08-02T15:05:40.114678: step 11680, loss 3.81863.
Train: 2018-08-02T15:05:40.177169: step 11681, loss 4.28137.
Train: 2018-08-02T15:05:40.239655: step 11682, loss 4.40729.
Train: 2018-08-02T15:05:40.302141: step 11683, loss 4.15545.
Train: 2018-08-02T15:05:40.349008: step 11684, loss 3.77768.
Train: 2018-08-02T15:05:40.411491: step 11685, loss 4.15545.
Train: 2018-08-02T15:05:40.473974: step 11686, loss 3.77768.
Train: 2018-08-02T15:05:40.520841: step 11687, loss 3.39991.
Train: 2018-08-02T15:05:40.583294: step 11688, loss 4.40729.
Train: 2018-08-02T15:05:40.645811: step 11689, loss 3.39991.
Train: 2018-08-02T15:05:40.708296: step 11690, loss 3.65176.
Test: 2018-08-02T15:05:40.895721: step 11690, loss 3.81863.
Train: 2018-08-02T15:05:40.958237: step 11691, loss 3.27399.
Train: 2018-08-02T15:05:41.020692: step 11692, loss 4.15545.
Train: 2018-08-02T15:05:41.083176: step 11693, loss 4.15545.
Train: 2018-08-02T15:05:41.130067: step 11694, loss 4.53321.
Train: 2018-08-02T15:05:41.192551: step 11695, loss 4.15545.
Train: 2018-08-02T15:05:41.255013: step 11696, loss 4.65914.
Train: 2018-08-02T15:05:41.301906: step 11697, loss 4.78506.
Train: 2018-08-02T15:05:41.364361: step 11698, loss 3.9036.
Train: 2018-08-02T15:05:41.426848: step 11699, loss 4.28137.
Train: 2018-08-02T15:05:41.473736: step 11700, loss 4.40729.
Test: 2018-08-02T15:05:41.676789: step 11700, loss 3.81863.
Train: 2018-08-02T15:05:42.192322: step 11701, loss 4.40729.
Train: 2018-08-02T15:05:42.254812: step 11702, loss 4.40729.
Train: 2018-08-02T15:05:42.317293: step 11703, loss 4.40729.
Train: 2018-08-02T15:05:42.379782: step 11704, loss 3.39991.
Train: 2018-08-02T15:05:42.426644: step 11705, loss 3.77768.
Train: 2018-08-02T15:05:42.489129: step 11706, loss 5.41467.
Train: 2018-08-02T15:05:42.551614: step 11707, loss 3.65176.
Train: 2018-08-02T15:05:42.598478: step 11708, loss 3.27399.
Train: 2018-08-02T15:05:42.660933: step 11709, loss 4.28137.
Train: 2018-08-02T15:05:42.723449: step 11710, loss 3.77768.
Test: 2018-08-02T15:05:42.910874: step 11710, loss 3.81863.
Train: 2018-08-02T15:05:42.973385: step 11711, loss 3.9036.
Train: 2018-08-02T15:05:43.035876: step 11712, loss 4.02952.
Train: 2018-08-02T15:05:43.098330: step 11713, loss 3.65176.
Train: 2018-08-02T15:05:43.145227: step 11714, loss 3.14807.
Train: 2018-08-02T15:05:43.207711: step 11715, loss 3.65176.
Train: 2018-08-02T15:05:43.270164: step 11716, loss 3.65176.
Train: 2018-08-02T15:05:43.317059: step 11717, loss 4.78506.
Train: 2018-08-02T15:05:43.379548: step 11718, loss 3.39991.
Train: 2018-08-02T15:05:43.442001: step 11719, loss 3.77768.
Train: 2018-08-02T15:05:43.488864: step 11720, loss 4.78506.
Test: 2018-08-02T15:05:43.691940: step 11720, loss 3.81863.
Train: 2018-08-02T15:05:43.754426: step 11721, loss 4.78506.
Train: 2018-08-02T15:05:43.816942: step 11722, loss 3.9036.
Train: 2018-08-02T15:05:43.863777: step 11723, loss 3.52583.
Train: 2018-08-02T15:05:43.926262: step 11724, loss 4.28137.
Train: 2018-08-02T15:05:43.988747: step 11725, loss 4.53321.
Train: 2018-08-02T15:05:44.051232: step 11726, loss 4.53321.
Train: 2018-08-02T15:05:44.098096: step 11727, loss 4.65914.
Train: 2018-08-02T15:05:44.160581: step 11728, loss 4.15545.
Train: 2018-08-02T15:05:44.223067: step 11729, loss 4.91098.
Train: 2018-08-02T15:05:44.269930: step 11730, loss 4.28137.
Test: 2018-08-02T15:05:44.473032: step 11730, loss 3.81863.
Train: 2018-08-02T15:05:44.535525: step 11731, loss 4.15545.
Train: 2018-08-02T15:05:44.598009: step 11732, loss 3.52583.
Train: 2018-08-02T15:05:44.660495: step 11733, loss 4.40729.
Train: 2018-08-02T15:05:44.707359: step 11734, loss 3.52583.
Train: 2018-08-02T15:05:44.769814: step 11735, loss 3.14807.
Train: 2018-08-02T15:05:44.832299: step 11736, loss 4.53321.
Train: 2018-08-02T15:05:44.879196: step 11737, loss 4.28137.
Train: 2018-08-02T15:05:44.941680: step 11738, loss 3.77768.
Train: 2018-08-02T15:05:45.004135: step 11739, loss 4.28137.
Train: 2018-08-02T15:05:45.050998: step 11740, loss 4.15545.
Test: 2018-08-02T15:05:45.254106: step 11740, loss 3.81863.
Train: 2018-08-02T15:05:45.316593: step 11741, loss 4.65914.
Train: 2018-08-02T15:05:45.379076: step 11742, loss 3.02214.
Train: 2018-08-02T15:05:45.441561: step 11743, loss 2.64438.
Train: 2018-08-02T15:05:45.488422: step 11744, loss 4.02952.
Train: 2018-08-02T15:05:45.550906: step 11745, loss 4.53321.
Train: 2018-08-02T15:05:45.613367: step 11746, loss 4.28137.
Train: 2018-08-02T15:05:45.660262: step 11747, loss 3.65176.
Train: 2018-08-02T15:05:45.722747: step 11748, loss 4.91098.
Train: 2018-08-02T15:05:45.785233: step 11749, loss 3.27399.
Train: 2018-08-02T15:05:45.832096: step 11750, loss 2.51845.
Test: 2018-08-02T15:05:46.035166: step 11750, loss 3.81863.
Train: 2018-08-02T15:05:46.097628: step 11751, loss 3.9036.
Train: 2018-08-02T15:05:46.160113: step 11752, loss 4.65914.
Train: 2018-08-02T15:05:46.222599: step 11753, loss 4.02952.
Train: 2018-08-02T15:05:46.269496: step 11754, loss 5.28875.
Train: 2018-08-02T15:05:46.331979: step 11755, loss 3.9036.
Train: 2018-08-02T15:05:46.394460: step 11756, loss 4.28137.
Train: 2018-08-02T15:05:46.441328: step 11757, loss 3.65176.
Train: 2018-08-02T15:05:46.503816: step 11758, loss 3.9036.
Train: 2018-08-02T15:05:46.566299: step 11759, loss 4.02952.
Train: 2018-08-02T15:05:46.613162: step 11760, loss 3.02214.
Test: 2018-08-02T15:05:46.816209: step 11760, loss 3.81863.
Train: 2018-08-02T15:05:46.878725: step 11761, loss 4.53321.
Train: 2018-08-02T15:05:46.941205: step 11762, loss 4.15545.
Train: 2018-08-02T15:05:47.003696: step 11763, loss 4.28137.
Train: 2018-08-02T15:05:47.050556: step 11764, loss 4.91098.
Train: 2018-08-02T15:05:47.113016: step 11765, loss 4.02952.
Train: 2018-08-02T15:05:47.175500: step 11766, loss 3.52583.
Train: 2018-08-02T15:05:47.238016: step 11767, loss 4.15545.
Train: 2018-08-02T15:05:47.284882: step 11768, loss 4.02952.
Train: 2018-08-02T15:05:47.347366: step 11769, loss 4.28137.
Train: 2018-08-02T15:05:47.409852: step 11770, loss 3.65176.
Test: 2018-08-02T15:05:47.597276: step 11770, loss 3.81863.
Train: 2018-08-02T15:05:47.659761: step 11771, loss 5.16283.
Train: 2018-08-02T15:05:47.722248: step 11772, loss 5.16283.
Train: 2018-08-02T15:05:47.784733: step 11773, loss 5.0369.
Train: 2018-08-02T15:05:47.831628: step 11774, loss 2.7703.
Train: 2018-08-02T15:05:47.894110: step 11775, loss 3.9036.
Train: 2018-08-02T15:05:47.956601: step 11776, loss 4.65914.
Train: 2018-08-02T15:05:48.003432: step 11777, loss 4.40729.
Train: 2018-08-02T15:05:48.050296: step 11778, loss 5.90997.
Train: 2018-08-02T15:05:48.112782: step 11779, loss 4.91098.
Train: 2018-08-02T15:05:48.175296: step 11780, loss 3.9036.
Test: 2018-08-02T15:05:48.362746: step 11780, loss 3.81863.
Train: 2018-08-02T15:05:48.425209: step 11781, loss 4.15545.
Train: 2018-08-02T15:05:48.487723: step 11782, loss 4.15545.
Train: 2018-08-02T15:05:48.550178: step 11783, loss 3.65176.
Train: 2018-08-02T15:05:48.612695: step 11784, loss 4.53321.
Train: 2018-08-02T15:05:48.659529: step 11785, loss 3.65176.
Train: 2018-08-02T15:05:48.722043: step 11786, loss 3.52583.
Train: 2018-08-02T15:05:48.784523: step 11787, loss 3.27399.
Train: 2018-08-02T15:05:48.831393: step 11788, loss 3.27399.
Train: 2018-08-02T15:05:48.893875: step 11789, loss 3.27399.
Train: 2018-08-02T15:05:48.956358: step 11790, loss 4.78506.
Test: 2018-08-02T15:05:49.143790: step 11790, loss 3.81863.
Train: 2018-08-02T15:05:49.206276: step 11791, loss 4.02952.
Train: 2018-08-02T15:05:49.268789: step 11792, loss 4.65914.
Train: 2018-08-02T15:05:49.331245: step 11793, loss 4.53321.
Train: 2018-08-02T15:05:49.393761: step 11794, loss 5.28875.
Train: 2018-08-02T15:05:49.440626: step 11795, loss 3.52583.
Train: 2018-08-02T15:05:49.503105: step 11796, loss 4.28137.
Train: 2018-08-02T15:05:49.565590: step 11797, loss 5.28875.
Train: 2018-08-02T15:05:49.612461: step 11798, loss 4.40729.
Train: 2018-08-02T15:05:49.674945: step 11799, loss 4.02952.
Train: 2018-08-02T15:05:49.737400: step 11800, loss 3.52583.
Test: 2018-08-02T15:05:49.940477: step 11800, loss 3.81863.
Train: 2018-08-02T15:05:50.424739: step 11801, loss 3.52583.
Train: 2018-08-02T15:05:50.487224: step 11802, loss 4.40729.
Train: 2018-08-02T15:05:50.534120: step 11803, loss 3.27399.
Train: 2018-08-02T15:05:50.596606: step 11804, loss 5.28875.
Train: 2018-08-02T15:05:50.659091: step 11805, loss 4.78506.
Train: 2018-08-02T15:05:50.721544: step 11806, loss 3.39991.
Train: 2018-08-02T15:05:50.768441: step 11807, loss 3.52583.
Train: 2018-08-02T15:05:50.830925: step 11808, loss 4.28137.
Train: 2018-08-02T15:05:50.893412: step 11809, loss 4.40729.
Train: 2018-08-02T15:05:50.940274: step 11810, loss 5.16283.
Test: 2018-08-02T15:05:51.143350: step 11810, loss 3.81863.
Train: 2018-08-02T15:05:51.205808: step 11811, loss 4.65914.
Train: 2018-08-02T15:05:51.268292: step 11812, loss 4.28137.
Train: 2018-08-02T15:05:51.330807: step 11813, loss 3.52583.
Train: 2018-08-02T15:05:51.377675: step 11814, loss 4.02952.
Train: 2018-08-02T15:05:51.440156: step 11815, loss 4.15545.
Train: 2018-08-02T15:05:51.502642: step 11816, loss 4.28137.
Train: 2018-08-02T15:05:51.549508: step 11817, loss 4.65914.
Train: 2018-08-02T15:05:51.611987: step 11818, loss 3.9036.
Train: 2018-08-02T15:05:51.674472: step 11819, loss 4.91098.
Train: 2018-08-02T15:05:51.721341: step 11820, loss 3.77768.
Test: 2018-08-02T15:05:51.924418: step 11820, loss 3.81863.
Train: 2018-08-02T15:05:51.986874: step 11821, loss 3.02214.
Train: 2018-08-02T15:05:52.049388: step 11822, loss 3.14807.
Train: 2018-08-02T15:05:52.111874: step 11823, loss 3.77768.
Train: 2018-08-02T15:05:52.158740: step 11824, loss 4.15545.
Train: 2018-08-02T15:05:52.221226: step 11825, loss 3.77768.
Train: 2018-08-02T15:05:52.268090: step 11826, loss 3.65176.
Train: 2018-08-02T15:05:52.330543: step 11827, loss 4.02952.
Train: 2018-08-02T15:05:52.393030: step 11828, loss 2.7703.
Train: 2018-08-02T15:05:52.439923: step 11829, loss 3.65176.
Train: 2018-08-02T15:05:52.502411: step 11830, loss 4.15545.
Test: 2018-08-02T15:05:52.705456: step 11830, loss 3.81863.
Train: 2018-08-02T15:05:52.767940: step 11831, loss 4.91098.
Train: 2018-08-02T15:05:52.830455: step 11832, loss 4.65914.
Train: 2018-08-02T15:05:52.892944: step 11833, loss 3.65176.
Train: 2018-08-02T15:05:52.939776: step 11834, loss 4.28137.
Train: 2018-08-02T15:05:53.002262: step 11835, loss 4.40729.
Train: 2018-08-02T15:05:53.064776: step 11836, loss 3.52583.
Train: 2018-08-02T15:05:53.111635: step 11837, loss 4.91098.
Train: 2018-08-02T15:05:53.174128: step 11838, loss 2.39253.
Train: 2018-08-02T15:05:53.236582: step 11839, loss 3.52583.
Train: 2018-08-02T15:05:53.283446: step 11840, loss 4.15545.
Test: 2018-08-02T15:05:53.486522: step 11840, loss 3.81863.
Train: 2018-08-02T15:05:53.549038: step 11841, loss 3.39991.
Train: 2018-08-02T15:05:53.611523: step 11842, loss 4.53321.
Train: 2018-08-02T15:05:53.658389: step 11843, loss 4.53321.
Train: 2018-08-02T15:05:53.720875: step 11844, loss 3.65176.
Train: 2018-08-02T15:05:53.783360: step 11845, loss 3.52583.
Train: 2018-08-02T15:05:53.845843: step 11846, loss 3.27399.
Train: 2018-08-02T15:05:53.892711: step 11847, loss 3.65176.
Train: 2018-08-02T15:05:53.955193: step 11848, loss 3.9036.
Train: 2018-08-02T15:05:54.017680: step 11849, loss 3.9036.
Train: 2018-08-02T15:05:54.064542: step 11850, loss 4.53321.
Test: 2018-08-02T15:05:54.267590: step 11850, loss 3.81863.
Train: 2018-08-02T15:05:54.330104: step 11851, loss 3.02214.
Train: 2018-08-02T15:05:54.392559: step 11852, loss 4.53321.
Train: 2018-08-02T15:05:54.439449: step 11853, loss 4.02952.
Train: 2018-08-02T15:05:54.501943: step 11854, loss 4.53321.
Train: 2018-08-02T15:05:54.564426: step 11855, loss 4.65914.
Train: 2018-08-02T15:05:54.611259: step 11856, loss 3.65176.
Train: 2018-08-02T15:05:54.673775: step 11857, loss 4.15545.
Train: 2018-08-02T15:05:54.736232: step 11858, loss 4.40729.
Train: 2018-08-02T15:05:54.783126: step 11859, loss 3.65176.
Train: 2018-08-02T15:05:54.845580: step 11860, loss 3.39991.
Test: 2018-08-02T15:05:55.048681: step 11860, loss 3.81863.
Train: 2018-08-02T15:05:55.111172: step 11861, loss 2.7703.
Train: 2018-08-02T15:05:55.173627: step 11862, loss 3.9036.
Train: 2018-08-02T15:05:55.220516: step 11863, loss 3.9036.
Train: 2018-08-02T15:05:55.283009: step 11864, loss 4.91098.
Train: 2018-08-02T15:05:55.345494: step 11865, loss 3.9036.
Train: 2018-08-02T15:05:55.392326: step 11866, loss 3.27399.
Train: 2018-08-02T15:05:55.454845: step 11867, loss 3.39991.
Train: 2018-08-02T15:05:55.517329: step 11868, loss 4.28137.
Train: 2018-08-02T15:05:55.564160: step 11869, loss 4.40729.
Train: 2018-08-02T15:05:55.626672: step 11870, loss 4.02952.
Test: 2018-08-02T15:05:55.829748: step 11870, loss 3.81863.
Train: 2018-08-02T15:05:55.876621: step 11871, loss 5.28875.
Train: 2018-08-02T15:05:55.939104: step 11872, loss 3.39991.
Train: 2018-08-02T15:05:56.001591: step 11873, loss 4.91098.
Train: 2018-08-02T15:05:56.064069: step 11874, loss 3.9036.
Train: 2018-08-02T15:05:56.126563: step 11875, loss 3.65176.
Train: 2018-08-02T15:05:56.173424: step 11876, loss 3.65176.
Train: 2018-08-02T15:05:56.235879: step 11877, loss 4.78506.
Train: 2018-08-02T15:05:56.298364: step 11878, loss 5.5406.
Train: 2018-08-02T15:05:56.345252: step 11879, loss 4.40729.
Train: 2018-08-02T15:05:56.407744: step 11880, loss 4.53321.
Test: 2018-08-02T15:05:56.610814: step 11880, loss 3.81863.
Train: 2018-08-02T15:05:56.673307: step 11881, loss 4.91098.
Train: 2018-08-02T15:05:56.720141: step 11882, loss 3.77768.
Train: 2018-08-02T15:05:56.782657: step 11883, loss 4.28137.
Train: 2018-08-02T15:05:56.845136: step 11884, loss 4.40729.
Train: 2018-08-02T15:05:56.892002: step 11885, loss 4.91098.
Train: 2018-08-02T15:05:56.954490: step 11886, loss 3.39991.
Train: 2018-08-02T15:05:57.016947: step 11887, loss 4.53321.
Train: 2018-08-02T15:05:57.079432: step 11888, loss 4.40729.
Train: 2018-08-02T15:05:57.126294: step 11889, loss 3.9036.
Train: 2018-08-02T15:05:57.188810: step 11890, loss 3.77768.
Test: 2018-08-02T15:05:57.391883: step 11890, loss 3.81863.
Train: 2018-08-02T15:05:57.438723: step 11891, loss 4.91098.
Train: 2018-08-02T15:05:57.501240: step 11892, loss 3.02214.
Train: 2018-08-02T15:05:57.563724: step 11893, loss 3.39991.
Train: 2018-08-02T15:05:57.626211: step 11894, loss 5.5406.
Train: 2018-08-02T15:05:57.673042: step 11895, loss 4.40729.
Train: 2018-08-02T15:05:57.735553: step 11896, loss 4.15545.
Train: 2018-08-02T15:05:57.798042: step 11897, loss 3.9036.
Train: 2018-08-02T15:05:57.860523: step 11898, loss 3.27399.
Train: 2018-08-02T15:05:57.907394: step 11899, loss 5.0369.
Train: 2018-08-02T15:05:57.969878: step 11900, loss 3.14807.
Test: 2018-08-02T15:05:58.172924: step 11900, loss 3.81863.
Train: 2018-08-02T15:05:58.672808: step 11901, loss 4.40729.
Train: 2018-08-02T15:05:58.735324: step 11902, loss 3.52583.
Train: 2018-08-02T15:05:58.797811: step 11903, loss 4.78506.
Train: 2018-08-02T15:05:58.844674: step 11904, loss 3.65176.
Train: 2018-08-02T15:05:58.907160: step 11905, loss 4.40729.
Train: 2018-08-02T15:05:58.969645: step 11906, loss 4.28137.
Train: 2018-08-02T15:05:59.032100: step 11907, loss 3.65176.
Train: 2018-08-02T15:05:59.078964: step 11908, loss 5.16283.
Train: 2018-08-02T15:05:59.141478: step 11909, loss 3.77768.
Train: 2018-08-02T15:05:59.188338: step 11910, loss 4.91098.
Test: 2018-08-02T15:05:59.407044: step 11910, loss 3.81863.
Train: 2018-08-02T15:05:59.453905: step 11911, loss 3.14807.
Train: 2018-08-02T15:05:59.516385: step 11912, loss 3.52583.
Train: 2018-08-02T15:05:59.578876: step 11913, loss 4.65914.
Train: 2018-08-02T15:05:59.641359: step 11914, loss 4.40729.
Train: 2018-08-02T15:05:59.703842: step 11915, loss 4.91098.
Train: 2018-08-02T15:05:59.750680: step 11916, loss 3.39991.
Train: 2018-08-02T15:05:59.813196: step 11917, loss 4.65914.
Train: 2018-08-02T15:05:59.875685: step 11918, loss 3.52583.
Train: 2018-08-02T15:05:59.922541: step 11919, loss 3.77768.
Train: 2018-08-02T15:05:59.985002: step 11920, loss 4.28137.
Test: 2018-08-02T15:06:00.188102: step 11920, loss 3.81863.
Train: 2018-08-02T15:06:00.250563: step 11921, loss 4.02952.
Train: 2018-08-02T15:06:00.297458: step 11922, loss 4.78506.
Train: 2018-08-02T15:06:00.359943: step 11923, loss 3.9036.
Train: 2018-08-02T15:06:00.422430: step 11924, loss 5.16283.
Train: 2018-08-02T15:06:00.469293: step 11925, loss 4.40729.
Train: 2018-08-02T15:06:00.531779: step 11926, loss 4.15545.
Train: 2018-08-02T15:06:00.594232: step 11927, loss 5.16283.
Train: 2018-08-02T15:06:00.641129: step 11928, loss 3.14807.
Train: 2018-08-02T15:06:00.687987: step 11929, loss 4.83543.
Train: 2018-08-02T15:06:00.750477: step 11930, loss 4.65914.
Test: 2018-08-02T15:06:00.937904: step 11930, loss 3.81863.
Train: 2018-08-02T15:06:01.000422: step 11931, loss 4.28137.
Train: 2018-08-02T15:06:01.062904: step 11932, loss 3.39991.
Train: 2018-08-02T15:06:01.125384: step 11933, loss 5.0369.
Train: 2018-08-02T15:06:01.187845: step 11934, loss 4.53321.
Train: 2018-08-02T15:06:01.234708: step 11935, loss 4.78506.
Train: 2018-08-02T15:06:01.297193: step 11936, loss 3.77768.
Train: 2018-08-02T15:06:01.359678: step 11937, loss 3.9036.
Train: 2018-08-02T15:06:01.406574: step 11938, loss 5.16283.
Train: 2018-08-02T15:06:01.469059: step 11939, loss 4.28137.
Train: 2018-08-02T15:06:01.531545: step 11940, loss 4.40729.
Test: 2018-08-02T15:06:01.718970: step 11940, loss 3.81863.
Train: 2018-08-02T15:06:01.781486: step 11941, loss 4.02952.
Train: 2018-08-02T15:06:01.843970: step 11942, loss 3.65176.
Train: 2018-08-02T15:06:01.890836: step 11943, loss 2.89622.
Train: 2018-08-02T15:06:01.953321: step 11944, loss 4.53321.
Train: 2018-08-02T15:06:02.015777: step 11945, loss 4.28137.
Train: 2018-08-02T15:06:02.078261: step 11946, loss 3.9036.
Train: 2018-08-02T15:06:02.125157: step 11947, loss 3.52583.
Train: 2018-08-02T15:06:02.187641: step 11948, loss 3.77768.
Train: 2018-08-02T15:06:02.250125: step 11949, loss 3.9036.
Train: 2018-08-02T15:06:02.296989: step 11950, loss 3.39991.
Test: 2018-08-02T15:06:02.500067: step 11950, loss 3.81863.
Train: 2018-08-02T15:06:02.562552: step 11951, loss 3.77768.
Train: 2018-08-02T15:06:02.625008: step 11952, loss 2.89622.
Train: 2018-08-02T15:06:02.671902: step 11953, loss 4.53321.
Train: 2018-08-02T15:06:02.734359: step 11954, loss 4.91098.
Train: 2018-08-02T15:06:02.796868: step 11955, loss 4.53321.
Train: 2018-08-02T15:06:02.843740: step 11956, loss 3.65176.
Train: 2018-08-02T15:06:02.906191: step 11957, loss 3.77768.
Train: 2018-08-02T15:06:02.968708: step 11958, loss 3.27399.
Train: 2018-08-02T15:06:03.015568: step 11959, loss 3.27399.
Train: 2018-08-02T15:06:03.078027: step 11960, loss 3.65176.
Test: 2018-08-02T15:06:03.281102: step 11960, loss 3.81863.
Train: 2018-08-02T15:06:03.343589: step 11961, loss 4.65914.
Train: 2018-08-02T15:06:03.390454: step 11962, loss 3.27399.
Train: 2018-08-02T15:06:03.452938: step 11963, loss 3.9036.
Train: 2018-08-02T15:06:03.515451: step 11964, loss 4.15545.
Train: 2018-08-02T15:06:03.577940: step 11965, loss 4.78506.
Train: 2018-08-02T15:06:03.624806: step 11966, loss 2.7703.
Train: 2018-08-02T15:06:03.687259: step 11967, loss 4.65914.
Train: 2018-08-02T15:06:03.749777: step 11968, loss 3.9036.
Train: 2018-08-02T15:06:03.796639: step 11969, loss 3.9036.
Train: 2018-08-02T15:06:03.859094: step 11970, loss 5.16283.
Test: 2018-08-02T15:06:04.062194: step 11970, loss 3.81863.
Train: 2018-08-02T15:06:04.124681: step 11971, loss 3.65176.
Train: 2018-08-02T15:06:04.171546: step 11972, loss 4.53321.
Train: 2018-08-02T15:06:04.234005: step 11973, loss 3.9036.
Train: 2018-08-02T15:06:04.296523: step 11974, loss 4.65914.
Train: 2018-08-02T15:06:04.343356: step 11975, loss 4.02952.
Train: 2018-08-02T15:06:04.405841: step 11976, loss 4.40729.
Train: 2018-08-02T15:06:04.468356: step 11977, loss 4.91098.
Train: 2018-08-02T15:06:04.530813: step 11978, loss 3.9036.
Train: 2018-08-02T15:06:04.577706: step 11979, loss 4.40729.
Train: 2018-08-02T15:06:04.640192: step 11980, loss 4.02952.
Test: 2018-08-02T15:06:04.843237: step 11980, loss 3.81863.
Train: 2018-08-02T15:06:04.890134: step 11981, loss 3.9036.
Train: 2018-08-02T15:06:04.952587: step 11982, loss 3.52583.
Train: 2018-08-02T15:06:05.015100: step 11983, loss 3.9036.
Train: 2018-08-02T15:06:05.061937: step 11984, loss 4.15545.
Train: 2018-08-02T15:06:05.124455: step 11985, loss 4.02952.
Train: 2018-08-02T15:06:05.186938: step 11986, loss 5.0369.
Train: 2018-08-02T15:06:05.233801: step 11987, loss 3.52583.
Train: 2018-08-02T15:06:05.296288: step 11988, loss 4.40729.
Train: 2018-08-02T15:06:05.358774: step 11989, loss 4.02952.
Train: 2018-08-02T15:06:05.405638: step 11990, loss 3.77768.
Test: 2018-08-02T15:06:05.608683: step 11990, loss 3.81863.
Train: 2018-08-02T15:06:05.671199: step 11991, loss 4.53321.
Train: 2018-08-02T15:06:05.733655: step 11992, loss 3.39991.
Train: 2018-08-02T15:06:05.796139: step 11993, loss 4.53321.
Train: 2018-08-02T15:06:05.843035: step 11994, loss 3.65176.
Train: 2018-08-02T15:06:05.905516: step 11995, loss 5.41467.
Train: 2018-08-02T15:06:05.968005: step 11996, loss 4.28137.
Train: 2018-08-02T15:06:06.014865: step 11997, loss 4.65914.
Train: 2018-08-02T15:06:06.077355: step 11998, loss 3.39991.
Train: 2018-08-02T15:06:06.139810: step 11999, loss 4.65914.
Train: 2018-08-02T15:06:06.186705: step 12000, loss 4.65914.
Test: 2018-08-02T15:06:06.389750: step 12000, loss 3.81863.
Train: 2018-08-02T15:06:06.967740: step 12001, loss 4.28137.
Train: 2018-08-02T15:06:07.030226: step 12002, loss 3.39991.
Train: 2018-08-02T15:06:07.092711: step 12003, loss 3.9036.
Train: 2018-08-02T15:06:07.139608: step 12004, loss 3.9036.
Train: 2018-08-02T15:06:07.202091: step 12005, loss 2.89622.
Train: 2018-08-02T15:06:07.264545: step 12006, loss 4.91098.
Train: 2018-08-02T15:06:07.311440: step 12007, loss 4.02952.
Train: 2018-08-02T15:06:07.373896: step 12008, loss 3.65176.
Train: 2018-08-02T15:06:07.436412: step 12009, loss 4.40729.
Train: 2018-08-02T15:06:07.483275: step 12010, loss 5.28875.
Test: 2018-08-02T15:06:07.686321: step 12010, loss 3.81863.
Train: 2018-08-02T15:06:07.748807: step 12011, loss 4.53321.
Train: 2018-08-02T15:06:07.811324: step 12012, loss 3.65176.
Train: 2018-08-02T15:06:07.858187: step 12013, loss 4.02952.
Train: 2018-08-02T15:06:07.920674: step 12014, loss 4.02952.
Train: 2018-08-02T15:06:07.983159: step 12015, loss 3.27399.
Train: 2018-08-02T15:06:08.030023: step 12016, loss 5.16283.
Train: 2018-08-02T15:06:08.092510: step 12017, loss 4.65914.
Train: 2018-08-02T15:06:08.154995: step 12018, loss 3.14807.
Train: 2018-08-02T15:06:08.217474: step 12019, loss 4.40729.
Train: 2018-08-02T15:06:08.264344: step 12020, loss 3.9036.
Test: 2018-08-02T15:06:08.467419: step 12020, loss 3.81863.
Train: 2018-08-02T15:06:08.529906: step 12021, loss 4.53321.
Train: 2018-08-02T15:06:08.576769: step 12022, loss 3.77768.
Train: 2018-08-02T15:06:08.639256: step 12023, loss 3.77768.
Train: 2018-08-02T15:06:08.701712: step 12024, loss 4.28137.
Train: 2018-08-02T15:06:08.764240: step 12025, loss 4.40729.
Train: 2018-08-02T15:06:08.811059: step 12026, loss 3.52583.
Train: 2018-08-02T15:06:08.873575: step 12027, loss 4.02952.
Train: 2018-08-02T15:06:08.936062: step 12028, loss 5.28875.
Train: 2018-08-02T15:06:08.982923: step 12029, loss 3.39991.
Train: 2018-08-02T15:06:09.045403: step 12030, loss 2.89622.
Test: 2018-08-02T15:06:09.248457: step 12030, loss 3.81863.
Train: 2018-08-02T15:06:09.295320: step 12031, loss 4.53321.
Train: 2018-08-02T15:06:09.357807: step 12032, loss 3.9036.
Train: 2018-08-02T15:06:09.420322: step 12033, loss 4.65914.
Train: 2018-08-02T15:06:09.482778: step 12034, loss 4.15545.
Train: 2018-08-02T15:06:09.545293: step 12035, loss 5.28875.
Train: 2018-08-02T15:06:09.592156: step 12036, loss 3.77768.
Train: 2018-08-02T15:06:09.654612: step 12037, loss 3.9036.
Train: 2018-08-02T15:06:09.717124: step 12038, loss 4.65914.
Train: 2018-08-02T15:06:09.763993: step 12039, loss 4.15545.
Train: 2018-08-02T15:06:09.826471: step 12040, loss 4.91098.
Test: 2018-08-02T15:06:10.029555: step 12040, loss 3.81863.
Train: 2018-08-02T15:06:10.092009: step 12041, loss 5.16283.
Train: 2018-08-02T15:06:10.154493: step 12042, loss 3.77768.
Train: 2018-08-02T15:06:10.201358: step 12043, loss 3.39991.
Train: 2018-08-02T15:06:10.263845: step 12044, loss 3.65176.
Train: 2018-08-02T15:06:10.326360: step 12045, loss 4.65914.
Train: 2018-08-02T15:06:10.373226: step 12046, loss 3.65176.
Train: 2018-08-02T15:06:10.435703: step 12047, loss 3.77768.
Train: 2018-08-02T15:06:10.498195: step 12048, loss 4.28137.
Train: 2018-08-02T15:06:10.545058: step 12049, loss 4.65914.
Train: 2018-08-02T15:06:10.607537: step 12050, loss 3.9036.
Test: 2018-08-02T15:06:10.810615: step 12050, loss 3.81863.
Train: 2018-08-02T15:06:10.857485: step 12051, loss 3.39991.
Train: 2018-08-02T15:06:10.919971: step 12052, loss 5.41467.
Train: 2018-08-02T15:06:10.982425: step 12053, loss 4.02952.
Train: 2018-08-02T15:06:11.044912: step 12054, loss 4.53321.
Train: 2018-08-02T15:06:11.091806: step 12055, loss 3.77768.
Train: 2018-08-02T15:06:11.154292: step 12056, loss 4.15545.
Train: 2018-08-02T15:06:11.216776: step 12057, loss 3.14807.
Train: 2018-08-02T15:06:11.279256: step 12058, loss 3.27399.
Train: 2018-08-02T15:06:11.326125: step 12059, loss 3.39991.
Train: 2018-08-02T15:06:11.388580: step 12060, loss 4.40729.
Test: 2018-08-02T15:06:11.591657: step 12060, loss 3.81863.
Train: 2018-08-02T15:06:11.638549: step 12061, loss 4.40729.
Train: 2018-08-02T15:06:11.701007: step 12062, loss 5.16283.
Train: 2018-08-02T15:06:11.763520: step 12063, loss 4.53321.
Train: 2018-08-02T15:06:11.825979: step 12064, loss 3.27399.
Train: 2018-08-02T15:06:11.872874: step 12065, loss 4.02952.
Train: 2018-08-02T15:06:11.935358: step 12066, loss 4.06133.
Train: 2018-08-02T15:06:11.997843: step 12067, loss 5.41467.
Train: 2018-08-02T15:06:12.044677: step 12068, loss 6.42205.
Train: 2018-08-02T15:06:12.107162: step 12069, loss 5.16283.
Train: 2018-08-02T15:06:12.169678: step 12070, loss 6.42205.
Test: 2018-08-02T15:06:12.357134: step 12070, loss 6.17609.
Train: 2018-08-02T15:06:12.419619: step 12071, loss 8.05905.
Train: 2018-08-02T15:06:12.482075: step 12072, loss 7.17759.
Train: 2018-08-02T15:06:12.544590: step 12073, loss 8.56274.
Train: 2018-08-02T15:06:12.591455: step 12074, loss 8.70043.
Train: 2018-08-02T15:06:12.653936: step 12075, loss 8.81458.
Train: 2018-08-02T15:06:12.716394: step 12076, loss 9.19235.
Train: 2018-08-02T15:06:12.763258: step 12077, loss 9.19235.
Train: 2018-08-02T15:06:12.825744: step 12078, loss 9.06643.
Train: 2018-08-02T15:06:12.888229: step 12079, loss 10.5775.
Train: 2018-08-02T15:06:12.919472: step 12080, loss 9.40222.
Test: 2018-08-02T15:06:13.122575: step 12080, loss 12.2995.
Train: 2018-08-02T15:06:13.185034: step 12081, loss 9.69604.
Train: 2018-08-02T15:06:13.247550: step 12082, loss 9.57012.
Train: 2018-08-02T15:06:13.310037: step 12083, loss 9.4442.
Train: 2018-08-02T15:06:13.356900: step 12084, loss 8.56274.
Train: 2018-08-02T15:06:13.419389: step 12085, loss 8.18497.
Train: 2018-08-02T15:06:13.481840: step 12086, loss 9.4442.
Train: 2018-08-02T15:06:13.528736: step 12087, loss 9.69604.
Train: 2018-08-02T15:06:13.591216: step 12088, loss 9.31827.
Train: 2018-08-02T15:06:13.653702: step 12089, loss 8.44229.
Train: 2018-08-02T15:06:13.700569: step 12090, loss 10.8293.
Test: 2018-08-02T15:06:13.903617: step 12090, loss 12.2995.
Train: 2018-08-02T15:06:13.966103: step 12091, loss 9.31827.
Train: 2018-08-02T15:06:14.028587: step 12092, loss 8.68866.
Train: 2018-08-02T15:06:14.091073: step 12093, loss 9.19235.
Train: 2018-08-02T15:06:14.153591: step 12094, loss 11.0812.
Train: 2018-08-02T15:06:14.200452: step 12095, loss 11.2071.
Train: 2018-08-02T15:06:14.262934: step 12096, loss 10.9553.
Train: 2018-08-02T15:06:14.325394: step 12097, loss 9.82196.
Train: 2018-08-02T15:06:14.387910: step 12098, loss 9.69604.
Train: 2018-08-02T15:06:14.434772: step 12099, loss 9.69604.
Train: 2018-08-02T15:06:14.497257: step 12100, loss 10.0738.
Test: 2018-08-02T15:06:14.700305: step 12100, loss 12.2995.
Train: 2018-08-02T15:06:15.215835: step 12101, loss 9.57012.
Train: 2018-08-02T15:06:15.278321: step 12102, loss 10.5775.
Train: 2018-08-02T15:06:15.325184: step 12103, loss 9.69604.
Train: 2018-08-02T15:06:15.387645: step 12104, loss 11.459.
Train: 2018-08-02T15:06:15.450130: step 12105, loss 10.5775.
Train: 2018-08-02T15:06:15.496993: step 12106, loss 9.94789.
Train: 2018-08-02T15:06:15.559508: step 12107, loss 9.4442.
Train: 2018-08-02T15:06:15.621997: step 12108, loss 10.7034.
Train: 2018-08-02T15:06:15.668829: step 12109, loss 11.459.
Train: 2018-08-02T15:06:15.731344: step 12110, loss 9.69604.
Test: 2018-08-02T15:06:15.934415: step 12110, loss 12.2995.
Train: 2018-08-02T15:06:15.996901: step 12111, loss 9.69604.
Train: 2018-08-02T15:06:16.059361: step 12112, loss 10.5775.
Train: 2018-08-02T15:06:16.106257: step 12113, loss 10.9553.
Train: 2018-08-02T15:06:16.168742: step 12114, loss 9.06643.
Train: 2018-08-02T15:06:16.231198: step 12115, loss 9.94789.
Train: 2018-08-02T15:06:16.293711: step 12116, loss 9.69604.
Train: 2018-08-02T15:06:16.340545: step 12117, loss 10.5775.
Train: 2018-08-02T15:06:16.403058: step 12118, loss 9.82196.
Train: 2018-08-02T15:06:16.465518: step 12119, loss 10.4516.
Train: 2018-08-02T15:06:16.528033: step 12120, loss 10.3257.
Test: 2018-08-02T15:06:16.731085: step 12120, loss 12.2995.
Train: 2018-08-02T15:06:16.777943: step 12121, loss 10.1997.
Train: 2018-08-02T15:06:16.840429: step 12122, loss 9.4442.
Train: 2018-08-02T15:06:16.902945: step 12123, loss 9.31827.
Train: 2018-08-02T15:06:16.965401: step 12124, loss 9.57012.
Train: 2018-08-02T15:06:17.012290: step 12125, loss 10.8293.
Train: 2018-08-02T15:06:17.074779: step 12126, loss 9.82196.
Train: 2018-08-02T15:06:17.137237: step 12127, loss 9.4442.
Train: 2018-08-02T15:06:17.184129: step 12128, loss 9.82196.
Train: 2018-08-02T15:06:17.246616: step 12129, loss 11.7108.
Train: 2018-08-02T15:06:17.309068: step 12130, loss 9.69604.
Test: 2018-08-02T15:06:17.512147: step 12130, loss 12.2995.
Train: 2018-08-02T15:06:17.559041: step 12131, loss 11.2071.
Train: 2018-08-02T15:06:17.621529: step 12132, loss 10.7034.
Train: 2018-08-02T15:06:17.684012: step 12133, loss 10.3257.
Train: 2018-08-02T15:06:17.746497: step 12134, loss 10.7034.
Train: 2018-08-02T15:06:17.793361: step 12135, loss 9.31827.
Train: 2018-08-02T15:06:17.855817: step 12136, loss 9.57012.
Train: 2018-08-02T15:06:17.918332: step 12137, loss 10.1997.
Train: 2018-08-02T15:06:17.965165: step 12138, loss 8.81458.
Train: 2018-08-02T15:06:18.027682: step 12139, loss 8.94051.
Train: 2018-08-02T15:06:18.090166: step 12140, loss 8.81458.
Test: 2018-08-02T15:06:18.293215: step 12140, loss 12.2995.
Train: 2018-08-02T15:06:18.340108: step 12141, loss 8.18497.
Train: 2018-08-02T15:06:18.402594: step 12142, loss 9.94789.
Train: 2018-08-02T15:06:18.465079: step 12143, loss 9.4442.
Train: 2018-08-02T15:06:18.527536: step 12144, loss 9.94789.
Train: 2018-08-02T15:06:18.574429: step 12145, loss 9.82197.
Train: 2018-08-02T15:06:18.636910: step 12146, loss 9.31827.
Train: 2018-08-02T15:06:18.699368: step 12147, loss 8.6939.
Train: 2018-08-02T15:06:18.746233: step 12148, loss 10.1997.
Train: 2018-08-02T15:06:18.808717: step 12149, loss 10.5775.
Train: 2018-08-02T15:06:18.871233: step 12150, loss 9.06643.
Test: 2018-08-02T15:06:19.074311: step 12150, loss 12.2995.
Train: 2018-08-02T15:06:19.121174: step 12151, loss 9.69604.
Train: 2018-08-02T15:06:19.183661: step 12152, loss 8.31089.
Train: 2018-08-02T15:06:19.246115: step 12153, loss 6.79982.
Train: 2018-08-02T15:06:19.308626: step 12154, loss 7.42943.
Train: 2018-08-02T15:06:19.355496: step 12155, loss 8.81458.
Train: 2018-08-02T15:06:19.417951: step 12156, loss 8.56274.
Train: 2018-08-02T15:06:19.464845: step 12157, loss 8.31089.
Train: 2018-08-02T15:06:19.527331: step 12158, loss 8.05905.
Train: 2018-08-02T15:06:19.589815: step 12159, loss 7.93312.
Train: 2018-08-02T15:06:19.636649: step 12160, loss 7.42943.
Test: 2018-08-02T15:06:19.839726: step 12160, loss 8.64653.
Train: 2018-08-02T15:06:19.902243: step 12161, loss 8.43682.
Train: 2018-08-02T15:06:19.964726: step 12162, loss 8.68866.
Train: 2018-08-02T15:06:20.011588: step 12163, loss 7.05167.
Train: 2018-08-02T15:06:20.074049: step 12164, loss 9.19235.
Train: 2018-08-02T15:06:20.136559: step 12165, loss 7.8072.
Train: 2018-08-02T15:06:20.199018: step 12166, loss 7.17759.
Train: 2018-08-02T15:06:20.261534: step 12167, loss 7.55536.
Train: 2018-08-02T15:06:20.308397: step 12168, loss 7.42943.
Train: 2018-08-02T15:06:20.370853: step 12169, loss 7.68128.
Train: 2018-08-02T15:06:20.433337: step 12170, loss 7.17759.
Test: 2018-08-02T15:06:20.620824: step 12170, loss 7.26821.
Train: 2018-08-02T15:06:20.683310: step 12171, loss 7.93313.
Train: 2018-08-02T15:06:20.745789: step 12172, loss 9.4442.
Train: 2018-08-02T15:06:20.808280: step 12173, loss 8.31089.
Train: 2018-08-02T15:06:20.870765: step 12174, loss 7.30351.
Train: 2018-08-02T15:06:20.917630: step 12175, loss 7.42943.
Train: 2018-08-02T15:06:20.980115: step 12176, loss 7.42943.
Train: 2018-08-02T15:06:21.026980: step 12177, loss 8.31089.
Train: 2018-08-02T15:06:21.089434: step 12178, loss 8.56274.
Train: 2018-08-02T15:06:21.151920: step 12179, loss 7.42943.
Train: 2018-08-02T15:06:21.198814: step 12180, loss 7.68128.
Test: 2018-08-02T15:06:21.401884: step 12180, loss 6.58281.
Train: 2018-08-02T15:06:21.464371: step 12181, loss 7.55536.
Train: 2018-08-02T15:06:21.526862: step 12182, loss 8.56274.
Train: 2018-08-02T15:06:21.573697: step 12183, loss 7.05167.
Train: 2018-08-02T15:06:21.636213: step 12184, loss 7.68128.
Train: 2018-08-02T15:06:21.698666: step 12185, loss 7.68128.
Train: 2018-08-02T15:06:21.761150: step 12186, loss 6.79982.
Train: 2018-08-02T15:06:21.808046: step 12187, loss 6.92574.
Train: 2018-08-02T15:06:21.870503: step 12188, loss 8.05905.
Train: 2018-08-02T15:06:21.933017: step 12189, loss 8.18497.
Train: 2018-08-02T15:06:21.979880: step 12190, loss 6.42205.
Test: 2018-08-02T15:06:22.182927: step 12190, loss 5.88235.
Train: 2018-08-02T15:06:22.245446: step 12191, loss 6.92574.
Train: 2018-08-02T15:06:22.307929: step 12192, loss 8.56274.
Train: 2018-08-02T15:06:22.370414: step 12193, loss 5.66652.
Train: 2018-08-02T15:06:22.417278: step 12194, loss 7.55536.
Train: 2018-08-02T15:06:22.479734: step 12195, loss 7.42943.
Train: 2018-08-02T15:06:22.542248: step 12196, loss 8.18497.
Train: 2018-08-02T15:06:22.589082: step 12197, loss 7.30351.
Train: 2018-08-02T15:06:22.651599: step 12198, loss 8.31089.
Train: 2018-08-02T15:06:22.714055: step 12199, loss 8.05905.
Train: 2018-08-02T15:06:22.760949: step 12200, loss 8.18497.
Test: 2018-08-02T15:06:22.963994: step 12200, loss 5.30993.
Train: 2018-08-02T15:06:23.463877: step 12201, loss 7.55536.
Train: 2018-08-02T15:06:23.510742: step 12202, loss 7.42943.
Train: 2018-08-02T15:06:23.573257: step 12203, loss 6.92574.
Train: 2018-08-02T15:06:23.635712: step 12204, loss 6.42205.
Train: 2018-08-02T15:06:23.698198: step 12205, loss 6.54798.
Train: 2018-08-02T15:06:23.745063: step 12206, loss 7.05167.
Train: 2018-08-02T15:06:23.807577: step 12207, loss 6.79982.
Train: 2018-08-02T15:06:23.870034: step 12208, loss 7.42943.
Train: 2018-08-02T15:06:23.932548: step 12209, loss 7.30351.
Train: 2018-08-02T15:06:23.979412: step 12210, loss 6.92574.
Test: 2018-08-02T15:06:24.182483: step 12210, loss 4.73751.
Train: 2018-08-02T15:06:24.244970: step 12211, loss 6.17021.
Train: 2018-08-02T15:06:24.307460: step 12212, loss 7.8072.
Train: 2018-08-02T15:06:24.369916: step 12213, loss 7.30351.
Train: 2018-08-02T15:06:24.416810: step 12214, loss 7.30351.
Train: 2018-08-02T15:06:24.479296: step 12215, loss 7.42943.
Train: 2018-08-02T15:06:24.541750: step 12216, loss 8.18497.
Train: 2018-08-02T15:06:24.588645: step 12217, loss 6.17021.
Train: 2018-08-02T15:06:24.651132: step 12218, loss 7.93313.
Train: 2018-08-02T15:06:24.713585: step 12219, loss 8.05905.
Train: 2018-08-02T15:06:24.760448: step 12220, loss 7.30351.
Test: 2018-08-02T15:06:24.963557: step 12220, loss 4.22535.
Train: 2018-08-02T15:06:25.026043: step 12221, loss 7.55536.
Train: 2018-08-02T15:06:25.088527: step 12222, loss 8.56274.
Train: 2018-08-02T15:06:25.150983: step 12223, loss 6.79982.
Train: 2018-08-02T15:06:25.197877: step 12224, loss 7.30351.
Train: 2018-08-02T15:06:25.260364: step 12225, loss 6.79982.
Train: 2018-08-02T15:06:25.322847: step 12226, loss 7.55536.
Train: 2018-08-02T15:06:25.369682: step 12227, loss 9.69604.
Train: 2018-08-02T15:06:25.432198: step 12228, loss 7.30351.
Train: 2018-08-02T15:06:25.494653: step 12229, loss 7.93313.
Train: 2018-08-02T15:06:25.557162: step 12230, loss 7.05167.
Test: 2018-08-02T15:06:25.744618: step 12230, loss 3.89395.
Train: 2018-08-02T15:06:25.791489: step 12231, loss 7.52178.
Train: 2018-08-02T15:06:25.853968: step 12232, loss 8.43682.
Train: 2018-08-02T15:06:25.916484: step 12233, loss 8.81458.
Train: 2018-08-02T15:06:25.978946: step 12234, loss 7.05167.
Train: 2018-08-02T15:06:26.041400: step 12235, loss 6.42205.
Train: 2018-08-02T15:06:26.088293: step 12236, loss 6.04429.
Train: 2018-08-02T15:06:26.150748: step 12237, loss 6.79982.
Train: 2018-08-02T15:06:26.213235: step 12238, loss 7.17759.
Train: 2018-08-02T15:06:26.275747: step 12239, loss 7.8072.
Train: 2018-08-02T15:06:26.322609: step 12240, loss 7.8072.
Test: 2018-08-02T15:06:26.525661: step 12240, loss 3.81863.
Train: 2018-08-02T15:06:26.588177: step 12241, loss 6.6739.
Train: 2018-08-02T15:06:26.650631: step 12242, loss 8.56274.
Train: 2018-08-02T15:06:26.697496: step 12243, loss 6.54798.
Train: 2018-08-02T15:06:26.759981: step 12244, loss 6.79982.
Train: 2018-08-02T15:06:26.822497: step 12245, loss 7.55536.
Train: 2018-08-02T15:06:26.884951: step 12246, loss 7.17759.
Train: 2018-08-02T15:06:26.931846: step 12247, loss 8.18497.
Train: 2018-08-02T15:06:26.994302: step 12248, loss 7.8072.
Train: 2018-08-02T15:06:27.056817: step 12249, loss 7.42943.
Train: 2018-08-02T15:06:27.103650: step 12250, loss 7.93313.
Test: 2018-08-02T15:06:27.306727: step 12250, loss 3.81863.
Train: 2018-08-02T15:06:27.369243: step 12251, loss 7.42943.
Train: 2018-08-02T15:06:27.431730: step 12252, loss 7.55536.
Train: 2018-08-02T15:06:27.494216: step 12253, loss 7.30351.
Train: 2018-08-02T15:06:27.541050: step 12254, loss 8.31089.
Train: 2018-08-02T15:06:27.603533: step 12255, loss 8.43682.
Train: 2018-08-02T15:06:27.666018: step 12256, loss 7.68128.
Train: 2018-08-02T15:06:27.728534: step 12257, loss 7.55536.
Train: 2018-08-02T15:06:27.775401: step 12258, loss 6.54798.
Train: 2018-08-02T15:06:27.837884: step 12259, loss 6.42205.
Train: 2018-08-02T15:06:27.900370: step 12260, loss 7.17759.
Test: 2018-08-02T15:06:28.087819: step 12260, loss 3.81863.
Train: 2018-08-02T15:06:28.150312: step 12261, loss 8.43682.
Train: 2018-08-02T15:06:28.212796: step 12262, loss 8.43682.
Train: 2018-08-02T15:06:28.275282: step 12263, loss 8.05905.
Train: 2018-08-02T15:06:28.322143: step 12264, loss 6.92574.
Train: 2018-08-02T15:06:28.384628: step 12265, loss 8.05905.
Train: 2018-08-02T15:06:28.447085: step 12266, loss 6.54798.
Train: 2018-08-02T15:06:28.493950: step 12267, loss 6.04429.
Train: 2018-08-02T15:06:28.556465: step 12268, loss 6.17021.
Train: 2018-08-02T15:06:28.618945: step 12269, loss 6.6739.
Train: 2018-08-02T15:06:28.665814: step 12270, loss 6.6739.
Test: 2018-08-02T15:06:28.868891: step 12270, loss 3.81863.
Train: 2018-08-02T15:06:28.931346: step 12271, loss 7.68128.
Train: 2018-08-02T15:06:28.993832: step 12272, loss 6.6739.
Train: 2018-08-02T15:06:29.040730: step 12273, loss 7.42943.
Train: 2018-08-02T15:06:29.103214: step 12274, loss 7.93313.
Train: 2018-08-02T15:06:29.165698: step 12275, loss 7.42943.
Train: 2018-08-02T15:06:29.212532: step 12276, loss 6.42205.
Train: 2018-08-02T15:06:29.275048: step 12277, loss 7.55536.
Train: 2018-08-02T15:06:29.337504: step 12278, loss 7.8072.
Train: 2018-08-02T15:06:29.384398: step 12279, loss 6.79982.
Train: 2018-08-02T15:06:29.446882: step 12280, loss 8.05905.
Test: 2018-08-02T15:06:29.649928: step 12280, loss 3.81863.
Train: 2018-08-02T15:06:29.712445: step 12281, loss 6.79982.
Train: 2018-08-02T15:06:29.759279: step 12282, loss 6.92574.
Train: 2018-08-02T15:06:29.821788: step 12283, loss 6.92574.
Train: 2018-08-02T15:06:29.884282: step 12284, loss 7.68128.
Train: 2018-08-02T15:06:29.946760: step 12285, loss 6.92574.
Train: 2018-08-02T15:06:29.993622: step 12286, loss 7.55536.
Train: 2018-08-02T15:06:30.056114: step 12287, loss 6.17021.
Train: 2018-08-02T15:06:30.118595: step 12288, loss 7.17759.
Train: 2018-08-02T15:06:30.165463: step 12289, loss 7.68128.
Train: 2018-08-02T15:06:30.227949: step 12290, loss 6.17021.
Test: 2018-08-02T15:06:30.430995: step 12290, loss 3.81863.
Train: 2018-08-02T15:06:30.493482: step 12291, loss 7.17759.
Train: 2018-08-02T15:06:30.540376: step 12292, loss 6.54798.
Train: 2018-08-02T15:06:30.602857: step 12293, loss 7.42943.
Train: 2018-08-02T15:06:30.665318: step 12294, loss 6.29613.
Train: 2018-08-02T15:06:30.727832: step 12295, loss 7.42943.
Train: 2018-08-02T15:06:30.774697: step 12296, loss 7.17759.
Train: 2018-08-02T15:06:30.837150: step 12297, loss 6.92574.
Train: 2018-08-02T15:06:30.899668: step 12298, loss 7.68128.
Train: 2018-08-02T15:06:30.946531: step 12299, loss 7.30351.
Train: 2018-08-02T15:06:31.008985: step 12300, loss 7.8072.
Test: 2018-08-02T15:06:31.196466: step 12300, loss 3.81863.
Train: 2018-08-02T15:06:31.758839: step 12301, loss 7.55536.
Train: 2018-08-02T15:06:31.821296: step 12302, loss 6.92574.
Train: 2018-08-02T15:06:31.883812: step 12303, loss 7.30351.
Train: 2018-08-02T15:06:31.930676: step 12304, loss 6.17021.
Train: 2018-08-02T15:06:31.993163: step 12305, loss 6.6739.
Train: 2018-08-02T15:06:32.055647: step 12306, loss 6.29613.
Train: 2018-08-02T15:06:32.102480: step 12307, loss 6.92574.
Train: 2018-08-02T15:06:32.164997: step 12308, loss 6.42205.
Train: 2018-08-02T15:06:32.227481: step 12309, loss 6.42205.
Train: 2018-08-02T15:06:32.289936: step 12310, loss 7.42943.
Test: 2018-08-02T15:06:32.477422: step 12310, loss 3.81863.
Train: 2018-08-02T15:06:32.539908: step 12311, loss 8.31089.
Train: 2018-08-02T15:06:32.602363: step 12312, loss 7.93313.
Train: 2018-08-02T15:06:32.664874: step 12313, loss 6.79982.
Train: 2018-08-02T15:06:32.727334: step 12314, loss 6.79982.
Train: 2018-08-02T15:06:32.774198: step 12315, loss 6.6739.
Train: 2018-08-02T15:06:32.836684: step 12316, loss 6.42205.
Train: 2018-08-02T15:06:32.883546: step 12317, loss 7.42943.
Train: 2018-08-02T15:06:32.946057: step 12318, loss 7.55536.
Train: 2018-08-02T15:06:33.008517: step 12319, loss 6.79982.
Train: 2018-08-02T15:06:33.055381: step 12320, loss 7.30351.
Test: 2018-08-02T15:06:33.258484: step 12320, loss 3.81863.
Train: 2018-08-02T15:06:33.320974: step 12321, loss 7.17759.
Train: 2018-08-02T15:06:33.383461: step 12322, loss 6.92574.
Train: 2018-08-02T15:06:33.445940: step 12323, loss 6.54798.
Train: 2018-08-02T15:06:33.508410: step 12324, loss 6.6739.
Train: 2018-08-02T15:06:33.555295: step 12325, loss 7.42943.
Train: 2018-08-02T15:06:33.617780: step 12326, loss 8.31089.
Train: 2018-08-02T15:06:33.664645: step 12327, loss 6.79982.
Train: 2018-08-02T15:06:33.727099: step 12328, loss 6.54798.
Train: 2018-08-02T15:06:33.789585: step 12329, loss 7.8072.
Train: 2018-08-02T15:06:33.852069: step 12330, loss 6.17021.
Test: 2018-08-02T15:06:34.039526: step 12330, loss 3.81863.
Train: 2018-08-02T15:06:34.102043: step 12331, loss 6.79982.
Train: 2018-08-02T15:06:34.164527: step 12332, loss 6.79982.
Train: 2018-08-02T15:06:34.227012: step 12333, loss 7.42943.
Train: 2018-08-02T15:06:34.273848: step 12334, loss 7.05167.
Train: 2018-08-02T15:06:34.336362: step 12335, loss 7.17759.
Train: 2018-08-02T15:06:34.398816: step 12336, loss 7.55536.
Train: 2018-08-02T15:06:34.445712: step 12337, loss 7.68128.
Train: 2018-08-02T15:06:34.508166: step 12338, loss 6.6739.
Train: 2018-08-02T15:06:34.570683: step 12339, loss 6.54798.
Train: 2018-08-02T15:06:34.617546: step 12340, loss 5.91845.
Test: 2018-08-02T15:06:34.820618: step 12340, loss 3.81863.
Train: 2018-08-02T15:06:34.883079: step 12341, loss 7.30351.
Train: 2018-08-02T15:06:34.945590: step 12342, loss 6.92574.
Train: 2018-08-02T15:06:34.992427: step 12343, loss 7.30351.
Train: 2018-08-02T15:06:35.054939: step 12344, loss 8.31089.
Train: 2018-08-02T15:06:35.117423: step 12345, loss 8.43682.
Train: 2018-08-02T15:06:35.164293: step 12346, loss 7.55536.
Train: 2018-08-02T15:06:35.226749: step 12347, loss 8.81458.
Train: 2018-08-02T15:06:35.289235: step 12348, loss 9.31827.
Train: 2018-08-02T15:06:35.336125: step 12349, loss 8.94051.
Train: 2018-08-02T15:06:35.398582: step 12350, loss 8.56274.
Test: 2018-08-02T15:06:35.601660: step 12350, loss 12.2995.
Train: 2018-08-02T15:06:35.648525: step 12351, loss 8.56274.
Train: 2018-08-02T15:06:35.711011: step 12352, loss 9.19235.
Train: 2018-08-02T15:06:35.773527: step 12353, loss 9.31827.
Train: 2018-08-02T15:06:35.836006: step 12354, loss 9.31827.
Train: 2018-08-02T15:06:35.898491: step 12355, loss 9.82197.
Train: 2018-08-02T15:06:35.945360: step 12356, loss 8.43682.
Train: 2018-08-02T15:06:36.007846: step 12357, loss 9.69604.
Train: 2018-08-02T15:06:36.070327: step 12358, loss 7.93313.
Train: 2018-08-02T15:06:36.117197: step 12359, loss 8.68866.
Train: 2018-08-02T15:06:36.179679: step 12360, loss 8.94051.
Test: 2018-08-02T15:06:36.382727: step 12360, loss 12.2995.
Train: 2018-08-02T15:06:36.445243: step 12361, loss 8.68866.
Train: 2018-08-02T15:06:36.492110: step 12362, loss 7.93313.
Train: 2018-08-02T15:06:36.554593: step 12363, loss 9.4442.
Train: 2018-08-02T15:06:36.617081: step 12364, loss 8.43682.
Train: 2018-08-02T15:06:36.679564: step 12365, loss 8.94051.
Train: 2018-08-02T15:06:36.726427: step 12366, loss 7.55536.
Train: 2018-08-02T15:06:36.788883: step 12367, loss 10.3257.
Train: 2018-08-02T15:06:36.851398: step 12368, loss 8.94051.
Train: 2018-08-02T15:06:36.913884: step 12369, loss 10.1997.
Train: 2018-08-02T15:06:36.960749: step 12370, loss 8.94051.
Test: 2018-08-02T15:06:37.163793: step 12370, loss 12.2995.
Train: 2018-08-02T15:06:37.226305: step 12371, loss 7.42943.
Train: 2018-08-02T15:06:37.288766: step 12372, loss 8.94051.
Train: 2018-08-02T15:06:37.335660: step 12373, loss 8.43682.
Train: 2018-08-02T15:06:37.398115: step 12374, loss 8.68866.
Train: 2018-08-02T15:06:37.460631: step 12375, loss 7.8072.
Train: 2018-08-02T15:06:37.507491: step 12376, loss 9.57012.
Train: 2018-08-02T15:06:37.569979: step 12377, loss 8.31089.
Train: 2018-08-02T15:06:37.632462: step 12378, loss 8.68866.
Train: 2018-08-02T15:06:37.679330: step 12379, loss 8.56274.
Train: 2018-08-02T15:06:37.741784: step 12380, loss 9.82196.
Test: 2018-08-02T15:06:37.944892: step 12380, loss 12.2995.
Train: 2018-08-02T15:06:38.007371: step 12381, loss 8.56274.
Train: 2018-08-02T15:06:38.038614: step 12382, loss 9.40222.
Train: 2018-08-02T15:06:38.101105: step 12383, loss 8.31089.
Train: 2018-08-02T15:06:38.163560: step 12384, loss 8.94051.
Train: 2018-08-02T15:06:38.226077: step 12385, loss 8.18497.
Train: 2018-08-02T15:06:38.272939: step 12386, loss 9.31827.
Train: 2018-08-02T15:06:38.335425: step 12387, loss 9.57012.
Train: 2018-08-02T15:06:38.397882: step 12388, loss 8.05905.
Train: 2018-08-02T15:06:38.444769: step 12389, loss 8.56274.
Train: 2018-08-02T15:06:38.507229: step 12390, loss 9.82197.
Test: 2018-08-02T15:06:38.710332: step 12390, loss 12.2995.
Train: 2018-08-02T15:06:38.772824: step 12391, loss 9.4442.
Train: 2018-08-02T15:06:38.835278: step 12392, loss 7.8072.
Train: 2018-08-02T15:06:38.882174: step 12393, loss 8.43682.
Train: 2018-08-02T15:06:38.944658: step 12394, loss 8.31089.
Train: 2018-08-02T15:06:39.007113: step 12395, loss 8.56274.
Train: 2018-08-02T15:06:39.069628: step 12396, loss 10.0738.
Train: 2018-08-02T15:06:39.116462: step 12397, loss 8.68866.
Train: 2018-08-02T15:06:39.178979: step 12398, loss 9.31827.
Train: 2018-08-02T15:06:39.241433: step 12399, loss 9.57012.
Train: 2018-08-02T15:06:39.288296: step 12400, loss 8.31089.
Test: 2018-08-02T15:06:39.491408: step 12400, loss 12.2995.
Train: 2018-08-02T15:06:40.006905: step 12401, loss 8.81458.
Train: 2018-08-02T15:06:40.069394: step 12402, loss 8.68866.
Train: 2018-08-02T15:06:40.116260: step 12403, loss 7.8072.
Train: 2018-08-02T15:06:40.178715: step 12404, loss 8.94051.
Train: 2018-08-02T15:06:40.241230: step 12405, loss 8.81458.
Train: 2018-08-02T15:06:40.288093: step 12406, loss 8.56274.
Train: 2018-08-02T15:06:40.350549: step 12407, loss 7.05167.
Train: 2018-08-02T15:06:40.413064: step 12408, loss 9.4442.
Train: 2018-08-02T15:06:40.459922: step 12409, loss 8.56274.
Train: 2018-08-02T15:06:40.522409: step 12410, loss 9.94789.
Test: 2018-08-02T15:06:40.725485: step 12410, loss 12.2995.
Train: 2018-08-02T15:06:40.787976: step 12411, loss 9.19235.
Train: 2018-08-02T15:06:40.850431: step 12412, loss 8.43682.
Train: 2018-08-02T15:06:40.912917: step 12413, loss 8.56274.
Train: 2018-08-02T15:06:40.959811: step 12414, loss 9.57012.
Train: 2018-08-02T15:06:41.022267: step 12415, loss 8.81458.
Train: 2018-08-02T15:06:41.084781: step 12416, loss 9.94789.
Train: 2018-08-02T15:06:41.131647: step 12417, loss 8.43682.
Train: 2018-08-02T15:06:41.194102: step 12418, loss 9.19235.
Train: 2018-08-02T15:06:41.256619: step 12419, loss 7.17759.
Train: 2018-08-02T15:06:41.303451: step 12420, loss 9.69604.
Test: 2018-08-02T15:06:41.506528: step 12420, loss 12.2995.
Train: 2018-08-02T15:06:41.569043: step 12421, loss 9.57012.
Train: 2018-08-02T15:06:41.631528: step 12422, loss 9.94789.
Train: 2018-08-02T15:06:41.678362: step 12423, loss 9.06643.
Train: 2018-08-02T15:06:41.740873: step 12424, loss 9.94789.
Train: 2018-08-02T15:06:41.803364: step 12425, loss 9.19235.
Train: 2018-08-02T15:06:41.850227: step 12426, loss 8.94051.
Train: 2018-08-02T15:06:41.912713: step 12427, loss 7.8072.
Train: 2018-08-02T15:06:41.975198: step 12428, loss 8.05905.
Train: 2018-08-02T15:06:42.022032: step 12429, loss 6.92574.
Train: 2018-08-02T15:06:42.084542: step 12430, loss 7.68128.
Test: 2018-08-02T15:06:42.287620: step 12430, loss 12.2995.
Train: 2018-08-02T15:06:42.350110: step 12431, loss 7.93313.
Train: 2018-08-02T15:06:42.396943: step 12432, loss 8.68866.
Train: 2018-08-02T15:06:42.459449: step 12433, loss 8.56274.
Train: 2018-08-02T15:06:42.521942: step 12434, loss 7.93312.
Train: 2018-08-02T15:06:42.584426: step 12435, loss 8.56274.
Train: 2018-08-02T15:06:42.631266: step 12436, loss 8.56274.
Train: 2018-08-02T15:06:42.693780: step 12437, loss 9.19235.
Train: 2018-08-02T15:06:42.756268: step 12438, loss 9.4442.
Train: 2018-08-02T15:06:42.803123: step 12439, loss 8.94051.
Train: 2018-08-02T15:06:42.865617: step 12440, loss 8.05905.
Test: 2018-08-02T15:06:43.068661: step 12440, loss 12.2995.
Train: 2018-08-02T15:06:43.115558: step 12441, loss 7.68128.
Train: 2018-08-02T15:06:43.178043: step 12442, loss 9.31827.
Train: 2018-08-02T15:06:43.240497: step 12443, loss 8.56274.
Train: 2018-08-02T15:06:43.302983: step 12444, loss 9.4442.
Train: 2018-08-02T15:06:43.365498: step 12445, loss 9.57012.
Train: 2018-08-02T15:06:43.412360: step 12446, loss 9.19235.
Train: 2018-08-02T15:06:43.474847: step 12447, loss 7.8072.
Train: 2018-08-02T15:06:43.537303: step 12448, loss 9.57012.
Train: 2018-08-02T15:06:43.584197: step 12449, loss 8.05905.
Train: 2018-08-02T15:06:43.646678: step 12450, loss 7.8072.
Test: 2018-08-02T15:06:43.849753: step 12450, loss 12.2995.
Train: 2018-08-02T15:06:43.896593: step 12451, loss 7.17759.
Train: 2018-08-02T15:06:43.959111: step 12452, loss 8.56274.
Train: 2018-08-02T15:06:44.021595: step 12453, loss 9.19235.
Train: 2018-08-02T15:06:44.084080: step 12454, loss 8.94051.
Train: 2018-08-02T15:06:44.130940: step 12455, loss 7.68128.
Train: 2018-08-02T15:06:44.193398: step 12456, loss 8.18497.
Train: 2018-08-02T15:06:44.255911: step 12457, loss 8.94051.
Train: 2018-08-02T15:06:44.302773: step 12458, loss 8.43682.
Train: 2018-08-02T15:06:44.365234: step 12459, loss 8.94051.
Train: 2018-08-02T15:06:44.427748: step 12460, loss 7.93313.
Test: 2018-08-02T15:06:44.630827: step 12460, loss 12.2995.
Train: 2018-08-02T15:06:44.677691: step 12461, loss 8.94051.
Train: 2018-08-02T15:06:44.740146: step 12462, loss 8.94051.
Train: 2018-08-02T15:06:44.802632: step 12463, loss 8.94051.
Train: 2018-08-02T15:06:44.865149: step 12464, loss 8.18497.
Train: 2018-08-02T15:06:44.927602: step 12465, loss 8.94051.
Train: 2018-08-02T15:06:44.974496: step 12466, loss 8.94051.
Train: 2018-08-02T15:06:45.036951: step 12467, loss 9.31827.
Train: 2018-08-02T15:06:45.099467: step 12468, loss 9.4442.
Train: 2018-08-02T15:06:45.146331: step 12469, loss 9.4442.
Train: 2018-08-02T15:06:45.208817: step 12470, loss 8.68866.
Test: 2018-08-02T15:06:45.411863: step 12470, loss 12.2995.
Train: 2018-08-02T15:06:45.474378: step 12471, loss 8.05905.
Train: 2018-08-02T15:06:45.536864: step 12472, loss 9.06643.
Train: 2018-08-02T15:06:45.599319: step 12473, loss 8.68866.
Train: 2018-08-02T15:06:45.646215: step 12474, loss 7.68128.
Train: 2018-08-02T15:06:45.708669: step 12475, loss 8.31089.
Train: 2018-08-02T15:06:45.771153: step 12476, loss 8.68866.
Train: 2018-08-02T15:06:45.818048: step 12477, loss 9.06643.
Train: 2018-08-02T15:06:45.880531: step 12478, loss 9.19235.
Train: 2018-08-02T15:06:45.943019: step 12479, loss 8.18497.
Train: 2018-08-02T15:06:45.989853: step 12480, loss 8.68866.
Test: 2018-08-02T15:06:46.192954: step 12480, loss 12.2995.
Train: 2018-08-02T15:06:46.255415: step 12481, loss 8.31089.
Train: 2018-08-02T15:06:46.317932: step 12482, loss 8.68866.
Train: 2018-08-02T15:06:46.364798: step 12483, loss 8.18497.
Train: 2018-08-02T15:06:46.427276: step 12484, loss 9.06643.
Train: 2018-08-02T15:06:46.489736: step 12485, loss 7.30351.
Train: 2018-08-02T15:06:46.552220: step 12486, loss 8.43682.
Train: 2018-08-02T15:06:46.599086: step 12487, loss 9.19235.
Train: 2018-08-02T15:06:46.661571: step 12488, loss 8.43682.
Train: 2018-08-02T15:06:46.724057: step 12489, loss 8.31089.
Train: 2018-08-02T15:06:46.770919: step 12490, loss 9.06643.
Test: 2018-08-02T15:06:46.974021: step 12490, loss 12.2995.
Train: 2018-08-02T15:06:47.036483: step 12491, loss 8.68866.
Train: 2018-08-02T15:06:47.098998: step 12492, loss 8.81458.
Train: 2018-08-02T15:06:47.145832: step 12493, loss 8.81458.
Train: 2018-08-02T15:06:47.208348: step 12494, loss 9.82196.
Train: 2018-08-02T15:06:47.270835: step 12495, loss 9.06643.
Train: 2018-08-02T15:06:47.333318: step 12496, loss 7.42943.
Train: 2018-08-02T15:06:47.380179: step 12497, loss 8.43682.
Train: 2018-08-02T15:06:47.442669: step 12498, loss 9.31827.
Train: 2018-08-02T15:06:47.505124: step 12499, loss 7.93313.
Train: 2018-08-02T15:06:47.552017: step 12500, loss 8.43682.
Test: 2018-08-02T15:06:47.755065: step 12500, loss 12.2995.
Train: 2018-08-02T15:06:48.286190: step 12501, loss 7.8072.
Train: 2018-08-02T15:06:48.348675: step 12502, loss 8.68866.
Train: 2018-08-02T15:06:48.395540: step 12503, loss 8.81458.
Train: 2018-08-02T15:06:48.458025: step 12504, loss 8.56274.
Train: 2018-08-02T15:06:48.520542: step 12505, loss 8.94051.
Train: 2018-08-02T15:06:48.583013: step 12506, loss 9.82196.
Train: 2018-08-02T15:06:48.629891: step 12507, loss 7.8072.
Train: 2018-08-02T15:06:48.692376: step 12508, loss 7.55536.
Train: 2018-08-02T15:06:48.739209: step 12509, loss 9.31827.
Train: 2018-08-02T15:06:48.801695: step 12510, loss 9.57012.
Test: 2018-08-02T15:06:49.004772: step 12510, loss 12.2995.
Train: 2018-08-02T15:06:49.067287: step 12511, loss 9.82196.
Train: 2018-08-02T15:06:49.129741: step 12512, loss 8.81458.
Train: 2018-08-02T15:06:49.176639: step 12513, loss 9.06643.
Train: 2018-08-02T15:06:49.239125: step 12514, loss 9.31827.
Train: 2018-08-02T15:06:49.301608: step 12515, loss 8.81458.
Train: 2018-08-02T15:06:49.348471: step 12516, loss 10.1997.
Train: 2018-08-02T15:06:49.410926: step 12517, loss 8.68866.
Train: 2018-08-02T15:06:49.473444: step 12518, loss 8.56274.
Train: 2018-08-02T15:06:49.520307: step 12519, loss 8.18497.
Train: 2018-08-02T15:06:49.582761: step 12520, loss 7.93313.
Test: 2018-08-02T15:06:49.785863: step 12520, loss 12.2995.
Train: 2018-08-02T15:06:49.848354: step 12521, loss 8.56274.
Train: 2018-08-02T15:06:49.910839: step 12522, loss 8.31089.
Train: 2018-08-02T15:06:49.957699: step 12523, loss 8.94051.
Train: 2018-08-02T15:06:50.020191: step 12524, loss 8.56274.
Train: 2018-08-02T15:06:50.082675: step 12525, loss 9.31827.
Train: 2018-08-02T15:06:50.145161: step 12526, loss 7.30351.
Train: 2018-08-02T15:06:50.192024: step 12527, loss 8.18497.
Train: 2018-08-02T15:06:50.254478: step 12528, loss 8.68866.
Train: 2018-08-02T15:06:50.316966: step 12529, loss 8.05905.
Train: 2018-08-02T15:06:50.363853: step 12530, loss 9.06643.
Test: 2018-08-02T15:06:50.566905: step 12530, loss 12.2995.
Train: 2018-08-02T15:06:50.629392: step 12531, loss 9.06643.
Train: 2018-08-02T15:06:50.691907: step 12532, loss 7.8072.
Train: 2018-08-02T15:06:50.738770: step 12533, loss 9.40222.
Train: 2018-08-02T15:06:50.801257: step 12534, loss 8.56274.
Train: 2018-08-02T15:06:50.848120: step 12535, loss 8.43682.
Train: 2018-08-02T15:06:50.910576: step 12536, loss 7.30351.
Train: 2018-08-02T15:06:50.973061: step 12537, loss 8.43682.
Train: 2018-08-02T15:06:51.019957: step 12538, loss 8.94051.
Train: 2018-08-02T15:06:51.082409: step 12539, loss 8.81458.
Train: 2018-08-02T15:06:51.144896: step 12540, loss 8.43682.
Test: 2018-08-02T15:06:51.332376: step 12540, loss 12.2995.
Train: 2018-08-02T15:06:51.394838: step 12541, loss 7.55536.
Train: 2018-08-02T15:06:51.457352: step 12542, loss 10.3257.
Train: 2018-08-02T15:06:51.519808: step 12543, loss 10.0738.
Train: 2018-08-02T15:06:51.566703: step 12544, loss 8.31089.
Train: 2018-08-02T15:06:51.629157: step 12545, loss 10.4516.
Train: 2018-08-02T15:06:51.691674: step 12546, loss 8.56274.
Train: 2018-08-02T15:06:51.738539: step 12547, loss 9.57012.
Train: 2018-08-02T15:06:51.801022: step 12548, loss 7.93313.
Train: 2018-08-02T15:06:51.863508: step 12549, loss 7.68128.
Train: 2018-08-02T15:06:51.910372: step 12550, loss 7.68128.
Test: 2018-08-02T15:06:52.113450: step 12550, loss 12.2995.
Train: 2018-08-02T15:06:52.175929: step 12551, loss 8.43682.
Train: 2018-08-02T15:06:52.238419: step 12552, loss 8.68866.
Train: 2018-08-02T15:06:52.300875: step 12553, loss 6.92574.
Train: 2018-08-02T15:06:52.347769: step 12554, loss 8.81458.
Train: 2018-08-02T15:06:52.410250: step 12555, loss 7.55536.
Train: 2018-08-02T15:06:52.472710: step 12556, loss 8.43682.
Train: 2018-08-02T15:06:52.519605: step 12557, loss 8.43682.
Train: 2018-08-02T15:06:52.582089: step 12558, loss 7.93313.
Train: 2018-08-02T15:06:52.644546: step 12559, loss 10.0738.
Train: 2018-08-02T15:06:52.691409: step 12560, loss 9.4442.
Test: 2018-08-02T15:06:52.894485: step 12560, loss 12.2995.
Train: 2018-08-02T15:06:52.956970: step 12561, loss 9.06643.
Train: 2018-08-02T15:06:53.019456: step 12562, loss 9.31827.
Train: 2018-08-02T15:06:53.081971: step 12563, loss 8.43682.
Train: 2018-08-02T15:06:53.128838: step 12564, loss 7.8072.
Train: 2018-08-02T15:06:53.191322: step 12565, loss 9.19235.
Train: 2018-08-02T15:06:53.253776: step 12566, loss 7.30351.
Train: 2018-08-02T15:06:53.300640: step 12567, loss 9.31827.
Train: 2018-08-02T15:06:53.363126: step 12568, loss 8.18497.
Train: 2018-08-02T15:06:53.425612: step 12569, loss 8.18497.
Train: 2018-08-02T15:06:53.472505: step 12570, loss 8.94051.
Test: 2018-08-02T15:06:53.675577: step 12570, loss 12.2995.
Train: 2018-08-02T15:06:53.738039: step 12571, loss 8.94051.
Train: 2018-08-02T15:06:53.800524: step 12572, loss 8.05905.
Train: 2018-08-02T15:06:53.863033: step 12573, loss 9.57012.
Train: 2018-08-02T15:06:53.909904: step 12574, loss 8.05905.
Train: 2018-08-02T15:06:53.972389: step 12575, loss 8.56274.
Train: 2018-08-02T15:06:54.034845: step 12576, loss 9.4442.
Train: 2018-08-02T15:06:54.081708: step 12577, loss 8.56274.
Train: 2018-08-02T15:06:54.144225: step 12578, loss 8.81458.
Train: 2018-08-02T15:06:54.206709: step 12579, loss 9.82196.
Train: 2018-08-02T15:06:54.253574: step 12580, loss 8.18497.
Test: 2018-08-02T15:06:54.456644: step 12580, loss 12.2995.
Train: 2018-08-02T15:06:54.519136: step 12581, loss 8.31089.
Train: 2018-08-02T15:06:54.581621: step 12582, loss 8.68866.
Train: 2018-08-02T15:06:54.644108: step 12583, loss 8.68866.
Train: 2018-08-02T15:06:54.690971: step 12584, loss 7.05167.
Train: 2018-08-02T15:06:54.753453: step 12585, loss 8.56815.
Train: 2018-08-02T15:06:54.815936: step 12586, loss 8.94051.
Train: 2018-08-02T15:06:54.878413: step 12587, loss 8.56274.
Train: 2018-08-02T15:06:54.925262: step 12588, loss 8.43682.
Train: 2018-08-02T15:06:54.987776: step 12589, loss 7.42943.
Train: 2018-08-02T15:06:55.050231: step 12590, loss 8.94051.
Test: 2018-08-02T15:06:55.237711: step 12590, loss 12.2995.
Train: 2018-08-02T15:06:55.300202: step 12591, loss 7.55536.
Train: 2018-08-02T15:06:55.362682: step 12592, loss 8.43682.
Train: 2018-08-02T15:06:55.409552: step 12593, loss 8.68866.
Train: 2018-08-02T15:06:55.472038: step 12594, loss 9.19235.
Train: 2018-08-02T15:06:55.534494: step 12595, loss 8.94051.
Train: 2018-08-02T15:06:55.597008: step 12596, loss 7.8072.
Train: 2018-08-02T15:06:55.643868: step 12597, loss 8.18497.
Train: 2018-08-02T15:06:55.706358: step 12598, loss 6.6739.
Train: 2018-08-02T15:06:55.768844: step 12599, loss 8.18497.
Train: 2018-08-02T15:06:55.815708: step 12600, loss 8.31089.
Test: 2018-08-02T15:06:56.018785: step 12600, loss 12.2995.
Train: 2018-08-02T15:06:56.565524: step 12601, loss 7.55536.
Train: 2018-08-02T15:06:56.612366: step 12602, loss 8.05905.
Train: 2018-08-02T15:06:56.674881: step 12603, loss 9.19235.
Train: 2018-08-02T15:06:56.737369: step 12604, loss 8.43682.
Train: 2018-08-02T15:06:56.784225: step 12605, loss 7.93313.
Train: 2018-08-02T15:06:56.846711: step 12606, loss 8.56274.
Train: 2018-08-02T15:06:56.909170: step 12607, loss 7.93313.
Train: 2018-08-02T15:06:56.956066: step 12608, loss 7.93312.
Train: 2018-08-02T15:06:57.018551: step 12609, loss 8.31089.
Train: 2018-08-02T15:06:57.081035: step 12610, loss 7.42943.
Test: 2018-08-02T15:06:57.284085: step 12610, loss 12.2995.
Train: 2018-08-02T15:06:57.330979: step 12611, loss 7.8072.
Train: 2018-08-02T15:06:57.393464: step 12612, loss 7.68128.
Train: 2018-08-02T15:06:57.455918: step 12613, loss 7.8072.
Train: 2018-08-02T15:06:57.518435: step 12614, loss 7.17759.
Train: 2018-08-02T15:06:57.565292: step 12615, loss 8.43682.
Train: 2018-08-02T15:06:57.627782: step 12616, loss 9.4442.
Train: 2018-08-02T15:06:57.690267: step 12617, loss 8.56274.
Train: 2018-08-02T15:06:57.737135: step 12618, loss 8.68866.
Train: 2018-08-02T15:06:57.799617: step 12619, loss 9.4442.
Train: 2018-08-02T15:06:57.862102: step 12620, loss 9.69604.
Test: 2018-08-02T15:06:58.049527: step 12620, loss 12.2995.
Train: 2018-08-02T15:06:58.112043: step 12621, loss 7.05167.
Train: 2018-08-02T15:06:58.174524: step 12622, loss 7.42943.
Train: 2018-08-02T15:06:58.237015: step 12623, loss 9.19235.
Train: 2018-08-02T15:06:58.283850: step 12624, loss 7.30351.
Train: 2018-08-02T15:06:58.346359: step 12625, loss 8.05905.
Train: 2018-08-02T15:06:58.408850: step 12626, loss 8.68866.
Train: 2018-08-02T15:06:58.455714: step 12627, loss 8.31089.
Train: 2018-08-02T15:06:58.518170: step 12628, loss 8.94051.
Train: 2018-08-02T15:06:58.580684: step 12629, loss 7.30351.
Train: 2018-08-02T15:06:58.627519: step 12630, loss 8.81458.
Test: 2018-08-02T15:06:58.830596: step 12630, loss 12.2995.
Train: 2018-08-02T15:06:58.893080: step 12631, loss 8.68866.
Train: 2018-08-02T15:06:58.955596: step 12632, loss 7.93313.
Train: 2018-08-02T15:06:59.002467: step 12633, loss 9.4442.
Train: 2018-08-02T15:06:59.064947: step 12634, loss 8.68866.
Train: 2018-08-02T15:06:59.127402: step 12635, loss 8.31089.
Train: 2018-08-02T15:06:59.189887: step 12636, loss 8.56274.
Train: 2018-08-02T15:06:59.236780: step 12637, loss 8.94051.
Train: 2018-08-02T15:06:59.299267: step 12638, loss 8.94051.
Train: 2018-08-02T15:06:59.361752: step 12639, loss 7.93313.
Train: 2018-08-02T15:06:59.408615: step 12640, loss 7.93313.
Test: 2018-08-02T15:06:59.611687: step 12640, loss 12.2995.
Train: 2018-08-02T15:06:59.674178: step 12641, loss 9.57012.
Train: 2018-08-02T15:06:59.736634: step 12642, loss 7.42943.
Train: 2018-08-02T15:06:59.799142: step 12643, loss 8.81458.
Train: 2018-08-02T15:06:59.846009: step 12644, loss 7.8072.
Train: 2018-08-02T15:06:59.908499: step 12645, loss 8.43682.
Train: 2018-08-02T15:06:59.970954: step 12646, loss 7.42943.
Train: 2018-08-02T15:07:00.017843: step 12647, loss 8.31089.
Train: 2018-08-02T15:07:00.080329: step 12648, loss 7.93313.
Train: 2018-08-02T15:07:00.142790: step 12649, loss 8.18497.
Train: 2018-08-02T15:07:00.189683: step 12650, loss 8.05905.
Test: 2018-08-02T15:07:00.392761: step 12650, loss 12.2995.
Train: 2018-08-02T15:07:00.455246: step 12651, loss 7.68128.
Train: 2018-08-02T15:07:00.517731: step 12652, loss 7.55536.
Train: 2018-08-02T15:07:00.580186: step 12653, loss 9.4442.
Train: 2018-08-02T15:07:00.627077: step 12654, loss 9.82196.
Train: 2018-08-02T15:07:00.689565: step 12655, loss 8.68866.
Train: 2018-08-02T15:07:00.752052: step 12656, loss 8.94051.
Train: 2018-08-02T15:07:00.798915: step 12657, loss 9.31827.
Train: 2018-08-02T15:07:00.861370: step 12658, loss 7.8072.
Train: 2018-08-02T15:07:00.923856: step 12659, loss 8.31089.
Train: 2018-08-02T15:07:00.970745: step 12660, loss 8.68866.
Test: 2018-08-02T15:07:01.173797: step 12660, loss 12.2995.
Train: 2018-08-02T15:07:01.236283: step 12661, loss 8.94051.
Train: 2018-08-02T15:07:01.283146: step 12662, loss 8.43682.
Train: 2018-08-02T15:07:01.345633: step 12663, loss 8.56274.
Train: 2018-08-02T15:07:01.408148: step 12664, loss 8.81458.
Train: 2018-08-02T15:07:01.470604: step 12665, loss 8.94051.
Train: 2018-08-02T15:07:01.533117: step 12666, loss 7.68128.
Train: 2018-08-02T15:07:01.579951: step 12667, loss 8.18497.
Train: 2018-08-02T15:07:01.642469: step 12668, loss 7.8072.
Train: 2018-08-02T15:07:01.704949: step 12669, loss 8.94051.
Train: 2018-08-02T15:07:01.751813: step 12670, loss 9.06643.
Test: 2018-08-02T15:07:01.954888: step 12670, loss 12.2995.
Train: 2018-08-02T15:07:02.017349: step 12671, loss 7.68128.
Train: 2018-08-02T15:07:02.064214: step 12672, loss 9.4442.
Train: 2018-08-02T15:07:02.126698: step 12673, loss 8.31089.
Train: 2018-08-02T15:07:02.189186: step 12674, loss 8.81458.
Train: 2018-08-02T15:07:02.251669: step 12675, loss 7.8072.
Train: 2018-08-02T15:07:02.298564: step 12676, loss 8.81458.
Train: 2018-08-02T15:07:02.361019: step 12677, loss 7.68128.
Train: 2018-08-02T15:07:02.423504: step 12678, loss 8.94051.
Train: 2018-08-02T15:07:02.470396: step 12679, loss 8.31089.
Train: 2018-08-02T15:07:02.532854: step 12680, loss 7.30351.
Test: 2018-08-02T15:07:02.735930: step 12680, loss 12.2995.
Train: 2018-08-02T15:07:02.782795: step 12681, loss 8.68866.
Train: 2018-08-02T15:07:02.845312: step 12682, loss 7.8072.
Train: 2018-08-02T15:07:02.907797: step 12683, loss 8.81458.
Train: 2018-08-02T15:07:02.954629: step 12684, loss 8.86495.
Train: 2018-08-02T15:07:03.017115: step 12685, loss 8.81458.
Train: 2018-08-02T15:07:03.063980: step 12686, loss 7.30351.
Train: 2018-08-02T15:07:03.126465: step 12687, loss 9.57012.
Train: 2018-08-02T15:07:03.188951: step 12688, loss 8.31089.
Train: 2018-08-02T15:07:03.235814: step 12689, loss 7.8072.
Train: 2018-08-02T15:07:03.298329: step 12690, loss 8.81458.
Test: 2018-08-02T15:07:03.501376: step 12690, loss 12.2995.
Train: 2018-08-02T15:07:03.548272: step 12691, loss 8.81458.
Train: 2018-08-02T15:07:03.610759: step 12692, loss 8.18497.
Train: 2018-08-02T15:07:03.673246: step 12693, loss 9.06643.
Train: 2018-08-02T15:07:03.735698: step 12694, loss 8.56274.
Train: 2018-08-02T15:07:03.782591: step 12695, loss 9.31827.
Train: 2018-08-02T15:07:03.845072: step 12696, loss 9.31827.
Train: 2018-08-02T15:07:03.907531: step 12697, loss 8.94051.
Train: 2018-08-02T15:07:03.954425: step 12698, loss 7.05167.
Train: 2018-08-02T15:07:04.016907: step 12699, loss 9.31827.
Train: 2018-08-02T15:07:04.079396: step 12700, loss 9.06643.
Test: 2018-08-02T15:07:04.266823: step 12700, loss 12.2995.
Train: 2018-08-02T15:07:04.766737: step 12701, loss 8.68866.
Train: 2018-08-02T15:07:04.829220: step 12702, loss 8.81458.
Train: 2018-08-02T15:07:04.876055: step 12703, loss 8.31089.
Train: 2018-08-02T15:07:04.938542: step 12704, loss 8.68866.
Train: 2018-08-02T15:07:05.001055: step 12705, loss 8.43682.
Train: 2018-08-02T15:07:05.047922: step 12706, loss 8.18497.
Train: 2018-08-02T15:07:05.110406: step 12707, loss 8.43682.
Train: 2018-08-02T15:07:05.172862: step 12708, loss 9.19235.
Train: 2018-08-02T15:07:05.219754: step 12709, loss 8.81458.
Train: 2018-08-02T15:07:05.282239: step 12710, loss 10.0738.
Test: 2018-08-02T15:07:05.485318: step 12710, loss 12.2995.
Train: 2018-08-02T15:07:05.532152: step 12711, loss 8.94051.
Train: 2018-08-02T15:07:05.594667: step 12712, loss 8.56274.
Train: 2018-08-02T15:07:05.657152: step 12713, loss 8.94051.
Train: 2018-08-02T15:07:05.719608: step 12714, loss 8.56274.
Train: 2018-08-02T15:07:05.766498: step 12715, loss 7.93313.
Train: 2018-08-02T15:07:05.828987: step 12716, loss 7.8072.
Train: 2018-08-02T15:07:05.891473: step 12717, loss 7.8072.
Train: 2018-08-02T15:07:05.938336: step 12718, loss 8.68866.
Train: 2018-08-02T15:07:06.000834: step 12719, loss 7.8072.
Train: 2018-08-02T15:07:06.063277: step 12720, loss 8.94051.
Test: 2018-08-02T15:07:06.266356: step 12720, loss 12.2995.
Train: 2018-08-02T15:07:06.313249: step 12721, loss 9.4442.
Train: 2018-08-02T15:07:06.375730: step 12722, loss 8.81458.
Train: 2018-08-02T15:07:06.438219: step 12723, loss 9.06643.
Train: 2018-08-02T15:07:06.500707: step 12724, loss 8.18497.
Train: 2018-08-02T15:07:06.563191: step 12725, loss 8.18497.
Train: 2018-08-02T15:07:06.610057: step 12726, loss 7.8072.
Train: 2018-08-02T15:07:06.672535: step 12727, loss 8.68866.
Train: 2018-08-02T15:07:06.735026: step 12728, loss 8.94051.
Train: 2018-08-02T15:07:06.781858: step 12729, loss 8.18497.
Train: 2018-08-02T15:07:06.844343: step 12730, loss 8.56274.
Test: 2018-08-02T15:07:07.047422: step 12730, loss 12.2995.
Train: 2018-08-02T15:07:07.109936: step 12731, loss 6.54798.
Train: 2018-08-02T15:07:07.156802: step 12732, loss 7.30351.
Train: 2018-08-02T15:07:07.219281: step 12733, loss 8.68866.
Train: 2018-08-02T15:07:07.281743: step 12734, loss 7.8072.
Train: 2018-08-02T15:07:07.344257: step 12735, loss 7.8072.
Train: 2018-08-02T15:07:07.391090: step 12736, loss 9.57012.
Train: 2018-08-02T15:07:07.453576: step 12737, loss 9.82196.
Train: 2018-08-02T15:07:07.516094: step 12738, loss 9.31827.
Train: 2018-08-02T15:07:07.562925: step 12739, loss 8.56274.
Train: 2018-08-02T15:07:07.625443: step 12740, loss 8.43682.
Test: 2018-08-02T15:07:07.828514: step 12740, loss 12.2995.
Train: 2018-08-02T15:07:07.890974: step 12741, loss 7.55536.
Train: 2018-08-02T15:07:07.937869: step 12742, loss 6.54798.
Train: 2018-08-02T15:07:08.000324: step 12743, loss 8.18497.
Train: 2018-08-02T15:07:08.062809: step 12744, loss 8.56274.
Train: 2018-08-02T15:07:08.125322: step 12745, loss 8.94051.
Train: 2018-08-02T15:07:08.172188: step 12746, loss 8.56274.
Train: 2018-08-02T15:07:08.234643: step 12747, loss 9.06643.
Train: 2018-08-02T15:07:08.297130: step 12748, loss 8.68866.
Train: 2018-08-02T15:07:08.344023: step 12749, loss 8.05905.
Train: 2018-08-02T15:07:08.406508: step 12750, loss 7.8072.
Test: 2018-08-02T15:07:08.609555: step 12750, loss 12.2995.
Train: 2018-08-02T15:07:08.672042: step 12751, loss 7.42943.
Train: 2018-08-02T15:07:08.734557: step 12752, loss 8.68866.
Train: 2018-08-02T15:07:08.781421: step 12753, loss 9.19235.
Train: 2018-08-02T15:07:08.843907: step 12754, loss 8.81458.
Train: 2018-08-02T15:07:08.906387: step 12755, loss 9.06643.
Train: 2018-08-02T15:07:08.968846: step 12756, loss 8.94051.
Train: 2018-08-02T15:07:09.015710: step 12757, loss 7.8072.
Train: 2018-08-02T15:07:09.078197: step 12758, loss 7.55536.
Train: 2018-08-02T15:07:09.125092: step 12759, loss 8.18497.
Train: 2018-08-02T15:07:09.187575: step 12760, loss 7.17759.
Test: 2018-08-02T15:07:09.390622: step 12760, loss 12.2995.
Train: 2018-08-02T15:07:09.453138: step 12761, loss 7.8072.
Train: 2018-08-02T15:07:09.515618: step 12762, loss 8.43682.
Train: 2018-08-02T15:07:09.578079: step 12763, loss 8.18497.
Train: 2018-08-02T15:07:09.624944: step 12764, loss 8.31089.
Train: 2018-08-02T15:07:09.687453: step 12765, loss 8.94051.
Train: 2018-08-02T15:07:09.749914: step 12766, loss 7.93313.
Train: 2018-08-02T15:07:09.796808: step 12767, loss 8.56274.
Train: 2018-08-02T15:07:09.859296: step 12768, loss 7.68128.
Train: 2018-08-02T15:07:09.906128: step 12769, loss 8.68866.
Train: 2018-08-02T15:07:09.968642: step 12770, loss 7.42943.
Test: 2018-08-02T15:07:10.171715: step 12770, loss 12.2995.
Train: 2018-08-02T15:07:10.234175: step 12771, loss 8.05905.
Train: 2018-08-02T15:07:10.296661: step 12772, loss 7.8072.
Train: 2018-08-02T15:07:10.359148: step 12773, loss 9.82196.
Train: 2018-08-02T15:07:10.406010: step 12774, loss 8.18497.
Train: 2018-08-02T15:07:10.468495: step 12775, loss 9.06643.
Train: 2018-08-02T15:07:10.531011: step 12776, loss 8.18497.
Train: 2018-08-02T15:07:10.577876: step 12777, loss 8.31089.
Train: 2018-08-02T15:07:10.640362: step 12778, loss 8.81458.
Train: 2018-08-02T15:07:10.702846: step 12779, loss 6.92574.
Train: 2018-08-02T15:07:10.749711: step 12780, loss 8.68866.
Test: 2018-08-02T15:07:10.952788: step 12780, loss 12.2995.
Train: 2018-08-02T15:07:11.015268: step 12781, loss 8.43682.
Train: 2018-08-02T15:07:11.077758: step 12782, loss 8.81458.
Train: 2018-08-02T15:07:11.140244: step 12783, loss 8.94051.
Train: 2018-08-02T15:07:11.187079: step 12784, loss 6.79982.
Train: 2018-08-02T15:07:11.249562: step 12785, loss 8.81458.
Train: 2018-08-02T15:07:11.312048: step 12786, loss 8.56274.
Train: 2018-08-02T15:07:11.358945: step 12787, loss 8.81458.
Train: 2018-08-02T15:07:11.421399: step 12788, loss 8.56274.
Train: 2018-08-02T15:07:11.483884: step 12789, loss 8.05905.
Train: 2018-08-02T15:07:11.530771: step 12790, loss 9.31827.
Test: 2018-08-02T15:07:11.733823: step 12790, loss 12.2995.
Train: 2018-08-02T15:07:11.796310: step 12791, loss 9.19235.
Train: 2018-08-02T15:07:11.858824: step 12792, loss 7.17759.
Train: 2018-08-02T15:07:11.905658: step 12793, loss 8.05905.
Train: 2018-08-02T15:07:11.968176: step 12794, loss 9.06643.
Train: 2018-08-02T15:07:12.030661: step 12795, loss 10.1997.
Train: 2018-08-02T15:07:12.093145: step 12796, loss 8.68866.
Train: 2018-08-02T15:07:12.140010: step 12797, loss 8.18497.
Train: 2018-08-02T15:07:12.202465: step 12798, loss 7.05167.
Train: 2018-08-02T15:07:12.264982: step 12799, loss 8.56274.
Train: 2018-08-02T15:07:12.311845: step 12800, loss 7.93313.
Test: 2018-08-02T15:07:12.514922: step 12800, loss 12.2995.
Train: 2018-08-02T15:07:13.061668: step 12801, loss 8.94051.
Train: 2018-08-02T15:07:13.108533: step 12802, loss 8.94051.
Train: 2018-08-02T15:07:13.170986: step 12803, loss 7.93313.
Train: 2018-08-02T15:07:13.233475: step 12804, loss 8.56274.
Train: 2018-08-02T15:07:13.280368: step 12805, loss 9.19235.
Train: 2018-08-02T15:07:13.342848: step 12806, loss 7.17759.
Train: 2018-08-02T15:07:13.405339: step 12807, loss 8.05905.
Train: 2018-08-02T15:07:13.467794: step 12808, loss 5.91836.
Train: 2018-08-02T15:07:13.514689: step 12809, loss 7.8072.
Train: 2018-08-02T15:07:13.577168: step 12810, loss 7.42943.
Test: 2018-08-02T15:07:13.780244: step 12810, loss 12.2995.
Train: 2018-08-02T15:07:13.842706: step 12811, loss 7.93313.
Train: 2018-08-02T15:07:13.889600: step 12812, loss 8.18497.
Train: 2018-08-02T15:07:13.952086: step 12813, loss 8.94051.
Train: 2018-08-02T15:07:14.030191: step 12814, loss 7.93312.
Train: 2018-08-02T15:07:14.077026: step 12815, loss 8.56274.
Train: 2018-08-02T15:07:14.139542: step 12816, loss 8.05905.
Train: 2018-08-02T15:07:14.202025: step 12817, loss 7.42943.
Train: 2018-08-02T15:07:14.248892: step 12818, loss 7.93313.
Train: 2018-08-02T15:07:14.311375: step 12819, loss 8.81458.
Train: 2018-08-02T15:07:14.373829: step 12820, loss 7.55536.
Test: 2018-08-02T15:07:14.576933: step 12820, loss 12.2995.
Train: 2018-08-02T15:07:14.623799: step 12821, loss 10.1997.
Train: 2018-08-02T15:07:14.686288: step 12822, loss 8.56274.
Train: 2018-08-02T15:07:14.744688: step 12823, loss 7.93313.
Train: 2018-08-02T15:07:14.807168: step 12824, loss 8.81458.
Train: 2018-08-02T15:07:14.869628: step 12825, loss 8.81458.
Train: 2018-08-02T15:07:14.932143: step 12826, loss 6.42205.
Train: 2018-08-02T15:07:14.978977: step 12827, loss 8.56274.
Train: 2018-08-02T15:07:15.041493: step 12828, loss 7.93313.
Train: 2018-08-02T15:07:15.103949: step 12829, loss 8.31089.
Train: 2018-08-02T15:07:15.150843: step 12830, loss 8.94051.
Test: 2018-08-02T15:07:15.353888: step 12830, loss 12.2995.
Train: 2018-08-02T15:07:15.416405: step 12831, loss 8.56274.
Train: 2018-08-02T15:07:15.478884: step 12832, loss 7.8072.
Train: 2018-08-02T15:07:15.541376: step 12833, loss 9.31827.
Train: 2018-08-02T15:07:15.588238: step 12834, loss 8.31089.
Train: 2018-08-02T15:07:15.635099: step 12835, loss 9.40222.
Train: 2018-08-02T15:07:15.697589: step 12836, loss 8.31089.
Train: 2018-08-02T15:07:15.760074: step 12837, loss 9.06643.
Train: 2018-08-02T15:07:15.806939: step 12838, loss 8.56274.
Train: 2018-08-02T15:07:15.869424: step 12839, loss 9.19235.
Train: 2018-08-02T15:07:15.931879: step 12840, loss 8.43682.
Test: 2018-08-02T15:07:16.119359: step 12840, loss 12.2995.
Train: 2018-08-02T15:07:16.181844: step 12841, loss 8.05905.
Train: 2018-08-02T15:07:16.244305: step 12842, loss 8.43682.
Train: 2018-08-02T15:07:16.291200: step 12843, loss 8.05905.
Train: 2018-08-02T15:07:16.353682: step 12844, loss 7.68128.
Train: 2018-08-02T15:07:16.416166: step 12845, loss 7.55536.
Train: 2018-08-02T15:07:16.463004: step 12846, loss 9.19235.
Train: 2018-08-02T15:07:16.525516: step 12847, loss 8.68866.
Train: 2018-08-02T15:07:16.587977: step 12848, loss 8.18497.
Train: 2018-08-02T15:07:16.650491: step 12849, loss 8.18497.
Train: 2018-08-02T15:07:16.697326: step 12850, loss 8.18497.
Test: 2018-08-02T15:07:16.900401: step 12850, loss 12.2995.
Train: 2018-08-02T15:07:16.962917: step 12851, loss 8.43682.
Train: 2018-08-02T15:07:17.025403: step 12852, loss 7.42943.
Train: 2018-08-02T15:07:17.072237: step 12853, loss 8.81458.
Train: 2018-08-02T15:07:17.134753: step 12854, loss 8.05905.
Train: 2018-08-02T15:07:17.197241: step 12855, loss 7.93312.
Train: 2018-08-02T15:07:17.244102: step 12856, loss 7.93313.
Train: 2018-08-02T15:07:17.306609: step 12857, loss 8.43682.
Train: 2018-08-02T15:07:17.369072: step 12858, loss 7.55536.
Train: 2018-08-02T15:07:17.415907: step 12859, loss 8.18497.
Train: 2018-08-02T15:07:17.478423: step 12860, loss 8.94051.
Test: 2018-08-02T15:07:17.681494: step 12860, loss 12.2995.
Train: 2018-08-02T15:07:17.743981: step 12861, loss 9.06643.
Train: 2018-08-02T15:07:17.790850: step 12862, loss 8.31089.
Train: 2018-08-02T15:07:17.853331: step 12863, loss 6.54798.
Train: 2018-08-02T15:07:17.915820: step 12864, loss 8.68866.
Train: 2018-08-02T15:07:17.978307: step 12865, loss 8.18497.
Train: 2018-08-02T15:07:18.025169: step 12866, loss 8.31089.
Train: 2018-08-02T15:07:18.087654: step 12867, loss 8.56274.
Train: 2018-08-02T15:07:18.150135: step 12868, loss 8.05905.
Train: 2018-08-02T15:07:18.197007: step 12869, loss 9.82196.
Train: 2018-08-02T15:07:18.259459: step 12870, loss 8.18497.
Test: 2018-08-02T15:07:18.446915: step 12870, loss 12.2995.
Train: 2018-08-02T15:07:18.509431: step 12871, loss 7.93312.
Train: 2018-08-02T15:07:18.571887: step 12872, loss 7.55536.
Train: 2018-08-02T15:07:18.634398: step 12873, loss 8.18497.
Train: 2018-08-02T15:07:18.696890: step 12874, loss 8.05905.
Train: 2018-08-02T15:07:18.759342: step 12875, loss 8.56274.
Train: 2018-08-02T15:07:18.806206: step 12876, loss 6.92574.
Train: 2018-08-02T15:07:18.868691: step 12877, loss 8.56274.
Train: 2018-08-02T15:07:18.931177: step 12878, loss 9.4442.
Train: 2018-08-02T15:07:18.978040: step 12879, loss 7.55536.
Train: 2018-08-02T15:07:19.040526: step 12880, loss 7.68128.
Test: 2018-08-02T15:07:19.243604: step 12880, loss 12.2995.
Train: 2018-08-02T15:07:19.306119: step 12881, loss 6.92574.
Train: 2018-08-02T15:07:19.352953: step 12882, loss 8.18497.
Train: 2018-08-02T15:07:19.415469: step 12883, loss 7.8072.
Train: 2018-08-02T15:07:19.477923: step 12884, loss 9.57012.
Train: 2018-08-02T15:07:19.524818: step 12885, loss 9.31827.
Train: 2018-08-02T15:07:19.587285: step 12886, loss 8.31089.
Train: 2018-08-02T15:07:19.649782: step 12887, loss 8.43682.
Train: 2018-08-02T15:07:19.696655: step 12888, loss 7.68128.
Train: 2018-08-02T15:07:19.759108: step 12889, loss 7.42943.
Train: 2018-08-02T15:07:19.821623: step 12890, loss 8.18497.
Test: 2018-08-02T15:07:20.024670: step 12890, loss 12.2995.
Train: 2018-08-02T15:07:20.071568: step 12891, loss 9.19235.
Train: 2018-08-02T15:07:20.134021: step 12892, loss 8.05905.
Train: 2018-08-02T15:07:20.196506: step 12893, loss 8.31089.
Train: 2018-08-02T15:07:20.259022: step 12894, loss 8.81458.
Train: 2018-08-02T15:07:20.305885: step 12895, loss 7.93312.
Train: 2018-08-02T15:07:20.368340: step 12896, loss 8.81458.
Train: 2018-08-02T15:07:20.430824: step 12897, loss 8.56274.
Train: 2018-08-02T15:07:20.477721: step 12898, loss 7.8072.
Train: 2018-08-02T15:07:20.540205: step 12899, loss 9.19235.
Train: 2018-08-02T15:07:20.587067: step 12900, loss 7.8072.
Test: 2018-08-02T15:07:20.790141: step 12900, loss 12.2995.
Train: 2018-08-02T15:07:21.305651: step 12901, loss 8.94051.
Train: 2018-08-02T15:07:21.352517: step 12902, loss 9.4442.
Train: 2018-08-02T15:07:21.415003: step 12903, loss 7.55536.
Train: 2018-08-02T15:07:21.477488: step 12904, loss 8.81458.
Train: 2018-08-02T15:07:21.539971: step 12905, loss 7.30351.
Train: 2018-08-02T15:07:21.602426: step 12906, loss 7.93313.
Train: 2018-08-02T15:07:21.649318: step 12907, loss 9.57012.
Train: 2018-08-02T15:07:21.711777: step 12908, loss 8.05905.
Train: 2018-08-02T15:07:21.774291: step 12909, loss 8.31089.
Train: 2018-08-02T15:07:21.821159: step 12910, loss 6.04429.
Test: 2018-08-02T15:07:22.024203: step 12910, loss 12.2995.
Train: 2018-08-02T15:07:22.086715: step 12911, loss 9.82196.
Train: 2018-08-02T15:07:22.149174: step 12912, loss 8.81458.
Train: 2018-08-02T15:07:22.196037: step 12913, loss 7.8072.
Train: 2018-08-02T15:07:22.258552: step 12914, loss 8.56274.
Train: 2018-08-02T15:07:22.321010: step 12915, loss 7.68128.
Train: 2018-08-02T15:07:22.367896: step 12916, loss 8.56274.
Train: 2018-08-02T15:07:22.430357: step 12917, loss 9.4442.
Train: 2018-08-02T15:07:22.492843: step 12918, loss 9.69604.
Train: 2018-08-02T15:07:22.539737: step 12919, loss 7.93313.
Train: 2018-08-02T15:07:22.602224: step 12920, loss 8.31089.
Test: 2018-08-02T15:07:22.805296: step 12920, loss 12.2995.
Train: 2018-08-02T15:07:22.867786: step 12921, loss 9.19235.
Train: 2018-08-02T15:07:22.930239: step 12922, loss 7.30351.
Train: 2018-08-02T15:07:22.977105: step 12923, loss 9.06643.
Train: 2018-08-02T15:07:23.039620: step 12924, loss 8.18497.
Train: 2018-08-02T15:07:23.102108: step 12925, loss 8.18497.
Train: 2018-08-02T15:07:23.164593: step 12926, loss 8.31089.
Train: 2018-08-02T15:07:23.211452: step 12927, loss 7.93313.
Train: 2018-08-02T15:07:23.273911: step 12928, loss 8.81458.
Train: 2018-08-02T15:07:23.320808: step 12929, loss 7.93313.
Train: 2018-08-02T15:07:23.383291: step 12930, loss 7.8072.
Test: 2018-08-02T15:07:23.586369: step 12930, loss 12.2995.
Train: 2018-08-02T15:07:23.648852: step 12931, loss 8.43682.
Train: 2018-08-02T15:07:23.711339: step 12932, loss 7.17759.
Train: 2018-08-02T15:07:23.758202: step 12933, loss 7.30351.
Train: 2018-08-02T15:07:23.820656: step 12934, loss 8.81458.
Train: 2018-08-02T15:07:23.883143: step 12935, loss 8.81458.
Train: 2018-08-02T15:07:23.930037: step 12936, loss 8.18497.
Train: 2018-08-02T15:07:23.992522: step 12937, loss 8.43682.
Train: 2018-08-02T15:07:24.054977: step 12938, loss 8.81458.
Train: 2018-08-02T15:07:24.101840: step 12939, loss 8.56274.
Train: 2018-08-02T15:07:24.164357: step 12940, loss 8.56274.
Test: 2018-08-02T15:07:24.367404: step 12940, loss 12.2995.
Train: 2018-08-02T15:07:24.429919: step 12941, loss 8.43682.
Train: 2018-08-02T15:07:24.476785: step 12942, loss 8.94051.
Train: 2018-08-02T15:07:24.539269: step 12943, loss 7.55536.
Train: 2018-08-02T15:07:24.601724: step 12944, loss 8.05905.
Train: 2018-08-02T15:07:24.664235: step 12945, loss 8.94051.
Train: 2018-08-02T15:07:24.711101: step 12946, loss 7.8072.
Train: 2018-08-02T15:07:24.773588: step 12947, loss 7.42943.
Train: 2018-08-02T15:07:24.836071: step 12948, loss 6.6739.
Train: 2018-08-02T15:07:24.882939: step 12949, loss 8.68866.
Train: 2018-08-02T15:07:24.945393: step 12950, loss 7.93313.
Test: 2018-08-02T15:07:25.148469: step 12950, loss 12.2995.
Train: 2018-08-02T15:07:25.210986: step 12951, loss 8.56274.
Train: 2018-08-02T15:07:25.257821: step 12952, loss 7.93313.
Train: 2018-08-02T15:07:25.320306: step 12953, loss 8.68866.
Train: 2018-08-02T15:07:25.382792: step 12954, loss 9.06643.
Train: 2018-08-02T15:07:25.445311: step 12955, loss 7.93313.
Train: 2018-08-02T15:07:25.507791: step 12956, loss 8.68866.
Train: 2018-08-02T15:07:25.554625: step 12957, loss 9.4442.
Train: 2018-08-02T15:07:25.617143: step 12958, loss 7.8072.
Train: 2018-08-02T15:07:25.679627: step 12959, loss 8.18497.
Train: 2018-08-02T15:07:25.726488: step 12960, loss 7.93312.
Test: 2018-08-02T15:07:25.929562: step 12960, loss 12.2995.
Train: 2018-08-02T15:07:25.992049: step 12961, loss 8.05905.
Train: 2018-08-02T15:07:26.054539: step 12962, loss 8.56274.
Train: 2018-08-02T15:07:26.116995: step 12963, loss 6.92574.
Train: 2018-08-02T15:07:26.163889: step 12964, loss 7.68128.
Train: 2018-08-02T15:07:26.226376: step 12965, loss 7.30351.
Train: 2018-08-02T15:07:26.288828: step 12966, loss 8.68866.
Train: 2018-08-02T15:07:26.335693: step 12967, loss 9.19235.
Train: 2018-08-02T15:07:26.398209: step 12968, loss 8.31089.
Train: 2018-08-02T15:07:26.460665: step 12969, loss 8.05905.
Train: 2018-08-02T15:07:26.507559: step 12970, loss 7.55536.
Test: 2018-08-02T15:07:26.710633: step 12970, loss 12.2995.
Train: 2018-08-02T15:07:26.773120: step 12971, loss 8.94051.
Train: 2018-08-02T15:07:26.819953: step 12972, loss 7.93313.
Train: 2018-08-02T15:07:26.882440: step 12973, loss 9.4442.
Train: 2018-08-02T15:07:26.944925: step 12974, loss 7.55536.
Train: 2018-08-02T15:07:27.007412: step 12975, loss 8.43682.
Train: 2018-08-02T15:07:27.054274: step 12976, loss 7.17759.
Train: 2018-08-02T15:07:27.116791: step 12977, loss 8.31089.
Train: 2018-08-02T15:07:27.179276: step 12978, loss 7.55536.
Train: 2018-08-02T15:07:27.226110: step 12979, loss 8.68866.
Train: 2018-08-02T15:07:27.288626: step 12980, loss 7.30351.
Test: 2018-08-02T15:07:27.491672: step 12980, loss 12.2995.
Train: 2018-08-02T15:07:27.554187: step 12981, loss 7.17759.
Train: 2018-08-02T15:07:27.616643: step 12982, loss 7.8072.
Train: 2018-08-02T15:07:27.679158: step 12983, loss 8.05905.
Train: 2018-08-02T15:07:27.725993: step 12984, loss 8.05905.
Train: 2018-08-02T15:07:27.788479: step 12985, loss 8.43682.
Train: 2018-08-02T15:07:27.835342: step 12986, loss 8.05905.
Train: 2018-08-02T15:07:27.882206: step 12987, loss 8.43682.
Train: 2018-08-02T15:07:27.944723: step 12988, loss 8.94051.
Train: 2018-08-02T15:07:28.007210: step 12989, loss 6.6739.
Train: 2018-08-02T15:07:28.069661: step 12990, loss 9.06643.
Test: 2018-08-02T15:07:28.257116: step 12990, loss 12.2995.
Train: 2018-08-02T15:07:28.319604: step 12991, loss 9.31827.
Train: 2018-08-02T15:07:28.382119: step 12992, loss 8.05905.
Train: 2018-08-02T15:07:28.444604: step 12993, loss 8.94051.
Train: 2018-08-02T15:07:28.491470: step 12994, loss 7.42943.
Train: 2018-08-02T15:07:28.553954: step 12995, loss 7.05167.
Train: 2018-08-02T15:07:28.616434: step 12996, loss 7.17759.
Train: 2018-08-02T15:07:28.663274: step 12997, loss 8.43682.
Train: 2018-08-02T15:07:28.725791: step 12998, loss 8.94051.
Train: 2018-08-02T15:07:28.788274: step 12999, loss 7.93313.
Train: 2018-08-02T15:07:28.835140: step 13000, loss 8.18497.
Test: 2018-08-02T15:07:29.038184: step 13000, loss 12.2995.
Train: 2018-08-02T15:07:29.522446: step 13001, loss 8.94051.
Train: 2018-08-02T15:07:29.584933: step 13002, loss 8.31089.
Train: 2018-08-02T15:07:29.647417: step 13003, loss 7.55536.
Train: 2018-08-02T15:07:29.694281: step 13004, loss 7.30351.
Train: 2018-08-02T15:07:29.756798: step 13005, loss 8.56274.
Train: 2018-08-02T15:07:29.819306: step 13006, loss 8.05905.
Train: 2018-08-02T15:07:29.881767: step 13007, loss 7.68128.
Train: 2018-08-02T15:07:29.928602: step 13008, loss 8.18497.
Train: 2018-08-02T15:07:29.991088: step 13009, loss 7.05167.
Train: 2018-08-02T15:07:30.100468: step 13010, loss 8.31089.
Test: 2018-08-02T15:07:30.303512: step 13010, loss 12.2995.
Train: 2018-08-02T15:07:30.366025: step 13011, loss 9.57012.
Train: 2018-08-02T15:07:30.412864: step 13012, loss 10.3257.
Train: 2018-08-02T15:07:30.475349: step 13013, loss 6.6739.
Train: 2018-08-02T15:07:30.537865: step 13014, loss 9.4442.
Train: 2018-08-02T15:07:30.584697: step 13015, loss 7.6867.
Train: 2018-08-02T15:07:30.647213: step 13016, loss 7.93313.
Train: 2018-08-02T15:07:30.709698: step 13017, loss 8.05905.
Train: 2018-08-02T15:07:30.756533: step 13018, loss 7.68128.
Train: 2018-08-02T15:07:30.819048: step 13019, loss 8.81458.
Train: 2018-08-02T15:07:30.881534: step 13020, loss 7.42943.
Test: 2018-08-02T15:07:31.068958: step 13020, loss 12.2995.
Train: 2018-08-02T15:07:31.131444: step 13021, loss 8.94051.
Train: 2018-08-02T15:07:31.193963: step 13022, loss 8.56274.
Train: 2018-08-02T15:07:31.256441: step 13023, loss 7.30351.
Train: 2018-08-02T15:07:31.318902: step 13024, loss 7.42943.
Train: 2018-08-02T15:07:31.365765: step 13025, loss 6.6739.
Train: 2018-08-02T15:07:31.428250: step 13026, loss 8.56274.
Train: 2018-08-02T15:07:31.490765: step 13027, loss 7.68128.
Train: 2018-08-02T15:07:31.537600: step 13028, loss 6.92574.
Train: 2018-08-02T15:07:31.600085: step 13029, loss 7.30351.
Train: 2018-08-02T15:07:31.662600: step 13030, loss 7.8072.
Test: 2018-08-02T15:07:31.850051: step 13030, loss 12.2995.
Train: 2018-08-02T15:07:31.912512: step 13031, loss 7.8072.
Train: 2018-08-02T15:07:31.975024: step 13032, loss 7.05167.
Train: 2018-08-02T15:07:32.037483: step 13033, loss 6.79982.
Train: 2018-08-02T15:07:32.100000: step 13034, loss 7.30351.
Train: 2018-08-02T15:07:32.146862: step 13035, loss 9.19235.
Train: 2018-08-02T15:07:32.209318: step 13036, loss 8.18497.
Train: 2018-08-02T15:07:32.271832: step 13037, loss 8.31089.
Train: 2018-08-02T15:07:32.318666: step 13038, loss 7.68128.
Train: 2018-08-02T15:07:32.381182: step 13039, loss 8.56274.
Train: 2018-08-02T15:07:32.443637: step 13040, loss 8.05905.
Test: 2018-08-02T15:07:32.631092: step 13040, loss 12.2995.
Train: 2018-08-02T15:07:32.756095: step 13041, loss 8.18497.
Train: 2018-08-02T15:07:32.818583: step 13042, loss 8.18497.
Train: 2018-08-02T15:07:32.881035: step 13043, loss 8.18497.
Train: 2018-08-02T15:07:32.927929: step 13044, loss 8.43682.
Train: 2018-08-02T15:07:32.990415: step 13045, loss 8.94051.
Train: 2018-08-02T15:07:33.052871: step 13046, loss 7.68128.
Train: 2018-08-02T15:07:33.099761: step 13047, loss 9.06643.
Train: 2018-08-02T15:07:33.162250: step 13048, loss 8.05905.
Train: 2018-08-02T15:07:33.224704: step 13049, loss 8.31089.
Train: 2018-08-02T15:07:33.271599: step 13050, loss 8.56274.
Test: 2018-08-02T15:07:33.474645: step 13050, loss 12.2995.
Train: 2018-08-02T15:07:33.537132: step 13051, loss 7.93313.
Train: 2018-08-02T15:07:33.599650: step 13052, loss 7.30351.
Train: 2018-08-02T15:07:33.662102: step 13053, loss 7.68128.
Train: 2018-08-02T15:07:33.724620: step 13054, loss 8.56274.
Train: 2018-08-02T15:07:33.771453: step 13055, loss 8.43682.
Train: 2018-08-02T15:07:33.833936: step 13056, loss 7.55536.
Train: 2018-08-02T15:07:33.880801: step 13057, loss 9.19235.
Train: 2018-08-02T15:07:33.943286: step 13058, loss 7.68128.
Train: 2018-08-02T15:07:34.005771: step 13059, loss 7.8072.
Train: 2018-08-02T15:07:34.068282: step 13060, loss 7.05167.
Test: 2018-08-02T15:07:34.255738: step 13060, loss 12.2995.
Train: 2018-08-02T15:07:34.318228: step 13061, loss 9.19235.
Train: 2018-08-02T15:07:34.380714: step 13062, loss 7.05167.
Train: 2018-08-02T15:07:34.443170: step 13063, loss 9.4442.
Train: 2018-08-02T15:07:34.505685: step 13064, loss 9.19235.
Train: 2018-08-02T15:07:34.552549: step 13065, loss 8.05905.
Train: 2018-08-02T15:07:34.615035: step 13066, loss 8.94051.
Train: 2018-08-02T15:07:34.677521: step 13067, loss 8.18497.
Train: 2018-08-02T15:07:34.724352: step 13068, loss 9.31827.
Train: 2018-08-02T15:07:34.786869: step 13069, loss 8.56274.
Train: 2018-08-02T15:07:34.849357: step 13070, loss 9.19235.
Test: 2018-08-02T15:07:35.036779: step 13070, loss 12.2995.
Train: 2018-08-02T15:07:35.099297: step 13071, loss 9.19235.
Train: 2018-08-02T15:07:35.161752: step 13072, loss 8.31089.
Train: 2018-08-02T15:07:35.224268: step 13073, loss 9.19235.
Train: 2018-08-02T15:07:35.271101: step 13074, loss 7.30351.
Train: 2018-08-02T15:07:35.333588: step 13075, loss 8.68866.
Train: 2018-08-02T15:07:35.396099: step 13076, loss 7.42943.
Train: 2018-08-02T15:07:35.442968: step 13077, loss 8.05905.
Train: 2018-08-02T15:07:35.505451: step 13078, loss 8.43682.
Train: 2018-08-02T15:07:35.567937: step 13079, loss 7.05167.
Train: 2018-08-02T15:07:35.614795: step 13080, loss 8.81458.
Test: 2018-08-02T15:07:35.817871: step 13080, loss 12.2995.
Train: 2018-08-02T15:07:35.880363: step 13081, loss 8.31089.
Train: 2018-08-02T15:07:35.942817: step 13082, loss 8.05905.
Train: 2018-08-02T15:07:35.989708: step 13083, loss 7.68128.
Train: 2018-08-02T15:07:36.052167: step 13084, loss 8.94051.
Train: 2018-08-02T15:07:36.114653: step 13085, loss 7.05167.
Train: 2018-08-02T15:07:36.177169: step 13086, loss 7.93313.
Train: 2018-08-02T15:07:36.224004: step 13087, loss 8.18497.
Train: 2018-08-02T15:07:36.286518: step 13088, loss 8.05905.
Train: 2018-08-02T15:07:36.349005: step 13089, loss 8.68866.
Train: 2018-08-02T15:07:36.395868: step 13090, loss 7.42943.
Test: 2018-08-02T15:07:36.598939: step 13090, loss 12.2995.
Train: 2018-08-02T15:07:36.661430: step 13091, loss 7.8072.
Train: 2018-08-02T15:07:36.723917: step 13092, loss 8.94051.
Train: 2018-08-02T15:07:36.770780: step 13093, loss 8.18497.
Train: 2018-08-02T15:07:36.833263: step 13094, loss 8.18497.
Train: 2018-08-02T15:07:36.895746: step 13095, loss 7.93313.
Train: 2018-08-02T15:07:36.958235: step 13096, loss 8.94051.
Train: 2018-08-02T15:07:37.005100: step 13097, loss 9.19235.
Train: 2018-08-02T15:07:37.067557: step 13098, loss 7.42943.
Train: 2018-08-02T15:07:37.130041: step 13099, loss 7.05167.
Train: 2018-08-02T15:07:37.176934: step 13100, loss 8.18497.
Test: 2018-08-02T15:07:37.379980: step 13100, loss 12.2995.
Train: 2018-08-02T15:07:37.864272: step 13101, loss 7.68128.
Train: 2018-08-02T15:07:37.911138: step 13102, loss 6.92574.
Train: 2018-08-02T15:07:37.973625: step 13103, loss 7.55536.
Train: 2018-08-02T15:07:38.036109: step 13104, loss 7.42943.
Train: 2018-08-02T15:07:38.098591: step 13105, loss 7.55536.
Train: 2018-08-02T15:07:38.161049: step 13106, loss 7.93313.
Train: 2018-08-02T15:07:38.207940: step 13107, loss 7.17759.
Train: 2018-08-02T15:07:38.270428: step 13108, loss 7.55536.
Train: 2018-08-02T15:07:38.332885: step 13109, loss 8.31089.
Train: 2018-08-02T15:07:38.395367: step 13110, loss 8.05905.
Test: 2018-08-02T15:07:38.582825: step 13110, loss 12.2995.
Train: 2018-08-02T15:07:38.645341: step 13111, loss 7.68128.
Train: 2018-08-02T15:07:38.707828: step 13112, loss 7.42943.
Train: 2018-08-02T15:07:38.770311: step 13113, loss 7.93313.
Train: 2018-08-02T15:07:38.832805: step 13114, loss 8.05905.
Train: 2018-08-02T15:07:38.879656: step 13115, loss 7.93313.
Train: 2018-08-02T15:07:38.942141: step 13116, loss 8.68866.
Train: 2018-08-02T15:07:39.004601: step 13117, loss 7.93313.
Train: 2018-08-02T15:07:39.051465: step 13118, loss 9.4442.
Train: 2018-08-02T15:07:39.113952: step 13119, loss 8.18497.
Train: 2018-08-02T15:07:39.176437: step 13120, loss 8.68866.
Test: 2018-08-02T15:07:39.379545: step 13120, loss 12.2995.
Train: 2018-08-02T15:07:39.426408: step 13121, loss 6.92574.
Train: 2018-08-02T15:07:39.488892: step 13122, loss 8.81458.
Train: 2018-08-02T15:07:39.551378: step 13123, loss 7.42943.
Train: 2018-08-02T15:07:39.613864: step 13124, loss 8.56274.
Train: 2018-08-02T15:07:39.660697: step 13125, loss 7.8072.
Train: 2018-08-02T15:07:39.723215: step 13126, loss 7.55536.
Train: 2018-08-02T15:07:39.785668: step 13127, loss 8.05905.
Train: 2018-08-02T15:07:39.832560: step 13128, loss 8.56274.
Train: 2018-08-02T15:07:39.895048: step 13129, loss 8.81458.
Train: 2018-08-02T15:07:39.957503: step 13130, loss 8.05905.
Test: 2018-08-02T15:07:40.144961: step 13130, loss 12.2995.
Train: 2018-08-02T15:07:40.207475: step 13131, loss 10.0738.
Train: 2018-08-02T15:07:40.269959: step 13132, loss 7.93312.
Train: 2018-08-02T15:07:40.332445: step 13133, loss 8.43682.
Train: 2018-08-02T15:07:40.379310: step 13134, loss 7.93313.
Train: 2018-08-02T15:07:40.441803: step 13135, loss 7.93313.
Train: 2018-08-02T15:07:40.504281: step 13136, loss 7.30351.
Train: 2018-08-02T15:07:40.535523: step 13137, loss 7.52178.
Train: 2018-08-02T15:07:40.598009: step 13138, loss 6.17021.
Train: 2018-08-02T15:07:40.660493: step 13139, loss 7.55536.
Train: 2018-08-02T15:07:40.722948: step 13140, loss 8.68866.
Test: 2018-08-02T15:07:40.910404: step 13140, loss 12.2995.
Train: 2018-08-02T15:07:40.972891: step 13141, loss 7.42943.
Train: 2018-08-02T15:07:41.035375: step 13142, loss 7.8072.
Train: 2018-08-02T15:07:41.097892: step 13143, loss 7.55536.
Train: 2018-08-02T15:07:41.160378: step 13144, loss 8.18497.
Train: 2018-08-02T15:07:41.207210: step 13145, loss 8.31089.
Train: 2018-08-02T15:07:41.269696: step 13146, loss 8.94051.
Train: 2018-08-02T15:07:41.332211: step 13147, loss 8.18497.
Train: 2018-08-02T15:07:41.379076: step 13148, loss 8.43682.
Train: 2018-08-02T15:07:41.441556: step 13149, loss 8.18497.
Train: 2018-08-02T15:07:41.504045: step 13150, loss 8.43682.
Test: 2018-08-02T15:07:41.707094: step 13150, loss 12.2995.
Train: 2018-08-02T15:07:41.753988: step 13151, loss 8.81458.
Train: 2018-08-02T15:07:41.816468: step 13152, loss 7.93313.
Train: 2018-08-02T15:07:41.878928: step 13153, loss 8.81458.
Train: 2018-08-02T15:07:41.925793: step 13154, loss 7.93313.
Train: 2018-08-02T15:07:41.988310: step 13155, loss 8.05905.
Train: 2018-08-02T15:07:42.050792: step 13156, loss 8.56274.
Train: 2018-08-02T15:07:42.097659: step 13157, loss 8.56274.
Train: 2018-08-02T15:07:42.160143: step 13158, loss 8.56274.
Train: 2018-08-02T15:07:42.222630: step 13159, loss 7.17759.
Train: 2018-08-02T15:07:42.285108: step 13160, loss 9.69604.
Test: 2018-08-02T15:07:42.472570: step 13160, loss 12.2995.
Train: 2018-08-02T15:07:42.535025: step 13161, loss 7.93313.
Train: 2018-08-02T15:07:42.597541: step 13162, loss 9.06643.
Train: 2018-08-02T15:07:42.660024: step 13163, loss 8.68866.
Train: 2018-08-02T15:07:42.706887: step 13164, loss 8.18497.
Train: 2018-08-02T15:07:42.769346: step 13165, loss 9.06643.
Train: 2018-08-02T15:07:42.831829: step 13166, loss 7.93312.
Train: 2018-08-02T15:07:42.878725: step 13167, loss 8.56274.
Train: 2018-08-02T15:07:42.941180: step 13168, loss 8.56274.
Train: 2018-08-02T15:07:43.003690: step 13169, loss 8.43682.
Train: 2018-08-02T15:07:43.050559: step 13170, loss 7.93312.
Test: 2018-08-02T15:07:43.253636: step 13170, loss 12.2995.
Train: 2018-08-02T15:07:43.316124: step 13171, loss 7.55536.
Train: 2018-08-02T15:07:43.378607: step 13172, loss 8.18497.
Train: 2018-08-02T15:07:43.441095: step 13173, loss 8.18497.
Train: 2018-08-02T15:07:43.487957: step 13174, loss 6.92574.
Train: 2018-08-02T15:07:43.550441: step 13175, loss 7.17759.
Train: 2018-08-02T15:07:43.612897: step 13176, loss 8.43682.
Train: 2018-08-02T15:07:43.659762: step 13177, loss 8.05905.
Train: 2018-08-02T15:07:43.722279: step 13178, loss 8.94051.
Train: 2018-08-02T15:07:43.784732: step 13179, loss 7.93313.
Train: 2018-08-02T15:07:43.831595: step 13180, loss 7.8072.
Test: 2018-08-02T15:07:44.034703: step 13180, loss 12.2995.
Train: 2018-08-02T15:07:44.097157: step 13181, loss 7.93313.
Train: 2018-08-02T15:07:44.144053: step 13182, loss 7.55536.
Train: 2018-08-02T15:07:44.206535: step 13183, loss 7.30351.
Train: 2018-08-02T15:07:44.268995: step 13184, loss 8.56274.
Train: 2018-08-02T15:07:44.331509: step 13185, loss 7.42943.
Train: 2018-08-02T15:07:44.378371: step 13186, loss 7.8072.
Train: 2018-08-02T15:07:44.440858: step 13187, loss 8.31089.
Train: 2018-08-02T15:07:44.503344: step 13188, loss 8.18497.
Train: 2018-08-02T15:07:44.550177: step 13189, loss 8.18497.
Train: 2018-08-02T15:07:44.612693: step 13190, loss 8.18497.
Test: 2018-08-02T15:07:44.815770: step 13190, loss 12.2995.
Train: 2018-08-02T15:07:44.878255: step 13191, loss 8.94051.
Train: 2018-08-02T15:07:44.925089: step 13192, loss 7.93313.
Train: 2018-08-02T15:07:44.987605: step 13193, loss 9.31827.
Train: 2018-08-02T15:07:45.050091: step 13194, loss 7.8072.
Train: 2018-08-02T15:07:45.112576: step 13195, loss 6.79982.
Train: 2018-08-02T15:07:45.159409: step 13196, loss 7.55536.
Train: 2018-08-02T15:07:45.221927: step 13197, loss 8.81458.
Train: 2018-08-02T15:07:45.284414: step 13198, loss 8.81458.
Train: 2018-08-02T15:07:45.331245: step 13199, loss 8.31089.
Train: 2018-08-02T15:07:45.393762: step 13200, loss 7.30351.
Test: 2018-08-02T15:07:45.596808: step 13200, loss 12.2995.
Train: 2018-08-02T15:07:46.174827: step 13201, loss 7.30351.
Train: 2018-08-02T15:07:46.221661: step 13202, loss 8.68866.
Train: 2018-08-02T15:07:46.284177: step 13203, loss 7.68128.
Train: 2018-08-02T15:07:46.346639: step 13204, loss 7.05167.
Train: 2018-08-02T15:07:46.409118: step 13205, loss 8.94051.
Train: 2018-08-02T15:07:46.456009: step 13206, loss 8.05905.
Train: 2018-08-02T15:07:46.518498: step 13207, loss 8.05905.
Train: 2018-08-02T15:07:46.580980: step 13208, loss 7.42943.
Train: 2018-08-02T15:07:46.643473: step 13209, loss 8.18497.
Train: 2018-08-02T15:07:46.690302: step 13210, loss 7.42943.
Test: 2018-08-02T15:07:46.893422: step 13210, loss 12.2995.
Train: 2018-08-02T15:07:46.955891: step 13211, loss 7.8072.
Train: 2018-08-02T15:07:47.018374: step 13212, loss 7.68128.
Train: 2018-08-02T15:07:47.065246: step 13213, loss 9.06643.
Train: 2018-08-02T15:07:47.127698: step 13214, loss 7.42943.
Train: 2018-08-02T15:07:47.190214: step 13215, loss 8.43682.
Train: 2018-08-02T15:07:47.237078: step 13216, loss 7.68128.
Train: 2018-08-02T15:07:47.299534: step 13217, loss 8.31089.
Train: 2018-08-02T15:07:47.362050: step 13218, loss 7.68128.
Train: 2018-08-02T15:07:47.408913: step 13219, loss 7.55536.
Train: 2018-08-02T15:07:47.471395: step 13220, loss 8.81458.
Test: 2018-08-02T15:07:47.674470: step 13220, loss 12.2995.
Train: 2018-08-02T15:07:47.721342: step 13221, loss 9.4442.
Train: 2018-08-02T15:07:47.783829: step 13222, loss 5.91836.
Train: 2018-08-02T15:07:47.846312: step 13223, loss 8.43682.
Train: 2018-08-02T15:07:47.908767: step 13224, loss 8.81458.
Train: 2018-08-02T15:07:47.971251: step 13225, loss 7.93313.
Train: 2018-08-02T15:07:48.018146: step 13226, loss 7.8072.
Train: 2018-08-02T15:07:48.080630: step 13227, loss 7.55536.
Train: 2018-08-02T15:07:48.143117: step 13228, loss 7.93313.
Train: 2018-08-02T15:07:48.189980: step 13229, loss 8.68866.
Train: 2018-08-02T15:07:48.252466: step 13230, loss 8.31089.
Test: 2018-08-02T15:07:48.455543: step 13230, loss 12.2995.
Train: 2018-08-02T15:07:48.502407: step 13231, loss 7.42943.
Train: 2018-08-02T15:07:48.564895: step 13232, loss 7.68128.
Train: 2018-08-02T15:07:48.627347: step 13233, loss 8.18497.
Train: 2018-08-02T15:07:48.689867: step 13234, loss 7.68128.
Train: 2018-08-02T15:07:48.752330: step 13235, loss 7.30351.
Train: 2018-08-02T15:07:48.799182: step 13236, loss 8.18497.
Train: 2018-08-02T15:07:48.861698: step 13237, loss 8.94051.
Train: 2018-08-02T15:07:48.924154: step 13238, loss 6.54798.
Train: 2018-08-02T15:07:48.971049: step 13239, loss 9.06643.
Train: 2018-08-02T15:07:49.033501: step 13240, loss 7.05167.
Test: 2018-08-02T15:07:49.236610: step 13240, loss 12.2995.
Train: 2018-08-02T15:07:49.299066: step 13241, loss 7.05167.
Train: 2018-08-02T15:07:49.345930: step 13242, loss 7.17759.
Train: 2018-08-02T15:07:49.408415: step 13243, loss 8.18497.
Train: 2018-08-02T15:07:49.470933: step 13244, loss 7.55536.
Train: 2018-08-02T15:07:49.533412: step 13245, loss 8.18497.
Train: 2018-08-02T15:07:49.580277: step 13246, loss 8.18497.
Train: 2018-08-02T15:07:49.642764: step 13247, loss 9.06643.
Train: 2018-08-02T15:07:49.705248: step 13248, loss 8.43682.
Train: 2018-08-02T15:07:49.752115: step 13249, loss 8.18497.
Train: 2018-08-02T15:07:49.814602: step 13250, loss 10.9553.
Test: 2018-08-02T15:07:50.017671: step 13250, loss 12.2995.
Train: 2018-08-02T15:07:50.080164: step 13251, loss 8.94051.
Train: 2018-08-02T15:07:50.142647: step 13252, loss 9.31827.
Train: 2018-08-02T15:07:50.189482: step 13253, loss 8.43682.
Train: 2018-08-02T15:07:50.251998: step 13254, loss 8.31089.
Train: 2018-08-02T15:07:50.314486: step 13255, loss 8.05905.
Train: 2018-08-02T15:07:50.361341: step 13256, loss 8.81458.
Train: 2018-08-02T15:07:50.423833: step 13257, loss 8.05905.
Train: 2018-08-02T15:07:50.486318: step 13258, loss 8.05905.
Train: 2018-08-02T15:07:50.533185: step 13259, loss 7.93854.
Train: 2018-08-02T15:07:50.595669: step 13260, loss 8.68866.
Test: 2018-08-02T15:07:50.798744: step 13260, loss 12.2995.
Train: 2018-08-02T15:07:50.861225: step 13261, loss 9.31827.
Train: 2018-08-02T15:07:50.923715: step 13262, loss 7.8072.
Train: 2018-08-02T15:07:50.970549: step 13263, loss 9.31827.
Train: 2018-08-02T15:07:51.033035: step 13264, loss 8.05905.
Train: 2018-08-02T15:07:51.095547: step 13265, loss 7.8072.
Train: 2018-08-02T15:07:51.158006: step 13266, loss 8.81458.
Train: 2018-08-02T15:07:51.204870: step 13267, loss 7.05167.
Train: 2018-08-02T15:07:51.267386: step 13268, loss 7.8072.
Train: 2018-08-02T15:07:51.329870: step 13269, loss 6.92574.
Train: 2018-08-02T15:07:51.376734: step 13270, loss 8.68866.
Test: 2018-08-02T15:07:51.579782: step 13270, loss 12.2995.
Train: 2018-08-02T15:07:51.642292: step 13271, loss 7.17759.
Train: 2018-08-02T15:07:51.704751: step 13272, loss 7.05167.
Train: 2018-08-02T15:07:51.751646: step 13273, loss 7.8072.
Train: 2018-08-02T15:07:51.814135: step 13274, loss 8.56274.
Train: 2018-08-02T15:07:51.876587: step 13275, loss 7.8072.
Train: 2018-08-02T15:07:51.923450: step 13276, loss 7.30351.
Train: 2018-08-02T15:07:51.985936: step 13277, loss 7.05167.
Train: 2018-08-02T15:07:52.048423: step 13278, loss 8.43682.
Train: 2018-08-02T15:07:52.095318: step 13279, loss 8.31089.
Train: 2018-08-02T15:07:52.157801: step 13280, loss 7.93313.
Test: 2018-08-02T15:07:52.360878: step 13280, loss 12.2995.
Train: 2018-08-02T15:07:52.423364: step 13281, loss 6.92574.
Train: 2018-08-02T15:07:52.470197: step 13282, loss 7.42943.
Train: 2018-08-02T15:07:52.532713: step 13283, loss 8.18497.
Train: 2018-08-02T15:07:52.595171: step 13284, loss 8.68866.
Train: 2018-08-02T15:07:52.657685: step 13285, loss 8.05905.
Train: 2018-08-02T15:07:52.704519: step 13286, loss 8.05905.
Train: 2018-08-02T15:07:52.767034: step 13287, loss 8.05905.
Train: 2018-08-02T15:07:52.813897: step 13288, loss 6.44724.
Train: 2018-08-02T15:07:52.876382: step 13289, loss 8.05905.
Train: 2018-08-02T15:07:52.923247: step 13290, loss 8.18497.
Test: 2018-08-02T15:07:53.126318: step 13290, loss 12.2995.
Train: 2018-08-02T15:07:53.188780: step 13291, loss 8.43682.
Train: 2018-08-02T15:07:53.251294: step 13292, loss 7.68128.
Train: 2018-08-02T15:07:53.313750: step 13293, loss 8.18497.
Train: 2018-08-02T15:07:53.360645: step 13294, loss 8.05905.
Train: 2018-08-02T15:07:53.423131: step 13295, loss 8.31089.
Train: 2018-08-02T15:07:53.485586: step 13296, loss 7.42943.
Train: 2018-08-02T15:07:53.548101: step 13297, loss 9.06643.
Train: 2018-08-02T15:07:53.594967: step 13298, loss 8.43682.
Train: 2018-08-02T15:07:53.657450: step 13299, loss 9.57012.
Train: 2018-08-02T15:07:53.719937: step 13300, loss 8.31089.
Test: 2018-08-02T15:07:53.907391: step 13300, loss 12.2995.
Train: 2018-08-02T15:07:54.454108: step 13301, loss 7.68128.
Train: 2018-08-02T15:07:54.516594: step 13302, loss 8.56274.
Train: 2018-08-02T15:07:54.579108: step 13303, loss 8.18497.
Train: 2018-08-02T15:07:54.641589: step 13304, loss 6.54798.
Train: 2018-08-02T15:07:54.688428: step 13305, loss 6.79982.
Train: 2018-08-02T15:07:54.750944: step 13306, loss 8.05905.
Train: 2018-08-02T15:07:54.813430: step 13307, loss 9.31827.
Train: 2018-08-02T15:07:54.875884: step 13308, loss 7.8072.
Train: 2018-08-02T15:07:54.922749: step 13309, loss 8.05905.
Train: 2018-08-02T15:07:54.985234: step 13310, loss 7.30351.
Test: 2018-08-02T15:07:55.188341: step 13310, loss 12.2995.
Train: 2018-08-02T15:07:55.235205: step 13311, loss 8.43682.
Train: 2018-08-02T15:07:55.297691: step 13312, loss 7.68128.
Train: 2018-08-02T15:07:55.360175: step 13313, loss 8.43682.
Train: 2018-08-02T15:07:55.422658: step 13314, loss 8.43682.
Train: 2018-08-02T15:07:55.469496: step 13315, loss 7.68128.
Train: 2018-08-02T15:07:55.531981: step 13316, loss 8.94051.
Train: 2018-08-02T15:07:55.594494: step 13317, loss 8.31089.
Train: 2018-08-02T15:07:55.656952: step 13318, loss 7.8072.
Train: 2018-08-02T15:07:55.703843: step 13319, loss 8.68866.
Train: 2018-08-02T15:07:55.766331: step 13320, loss 8.43682.
Test: 2018-08-02T15:07:55.969378: step 13320, loss 12.2995.
Train: 2018-08-02T15:07:56.016273: step 13321, loss 6.42205.
Train: 2018-08-02T15:07:56.078755: step 13322, loss 7.30351.
Train: 2018-08-02T15:07:56.141212: step 13323, loss 7.05167.
Train: 2018-08-02T15:07:56.203700: step 13324, loss 7.05167.
Train: 2018-08-02T15:07:56.266185: step 13325, loss 7.68128.
Train: 2018-08-02T15:07:56.313047: step 13326, loss 7.93312.
Train: 2018-08-02T15:07:56.375534: step 13327, loss 8.05905.
Train: 2018-08-02T15:07:56.438018: step 13328, loss 8.31089.
Train: 2018-08-02T15:07:56.500505: step 13329, loss 8.43682.
Train: 2018-08-02T15:07:56.547398: step 13330, loss 7.8072.
Test: 2018-08-02T15:07:56.750469: step 13330, loss 12.2995.
Train: 2018-08-02T15:07:56.797309: step 13331, loss 7.55536.
Train: 2018-08-02T15:07:56.859825: step 13332, loss 8.68866.
Train: 2018-08-02T15:07:56.922309: step 13333, loss 6.29613.
Train: 2018-08-02T15:07:56.984767: step 13334, loss 6.54798.
Train: 2018-08-02T15:07:57.031630: step 13335, loss 6.54798.
Train: 2018-08-02T15:07:57.094116: step 13336, loss 8.68866.
Train: 2018-08-02T15:07:57.156630: step 13337, loss 8.18497.
Train: 2018-08-02T15:07:57.203492: step 13338, loss 8.68866.
Train: 2018-08-02T15:07:57.265949: step 13339, loss 8.05905.
Train: 2018-08-02T15:07:57.328465: step 13340, loss 9.57012.
Test: 2018-08-02T15:07:57.515891: step 13340, loss 12.2995.
Train: 2018-08-02T15:07:57.578376: step 13341, loss 7.68128.
Train: 2018-08-02T15:07:57.640893: step 13342, loss 8.05905.
Train: 2018-08-02T15:07:57.703377: step 13343, loss 7.8072.
Train: 2018-08-02T15:07:57.765863: step 13344, loss 7.68128.
Train: 2018-08-02T15:07:57.812723: step 13345, loss 7.68128.
Train: 2018-08-02T15:07:57.875183: step 13346, loss 7.55536.
Train: 2018-08-02T15:07:57.937698: step 13347, loss 7.55536.
Train: 2018-08-02T15:07:58.000179: step 13348, loss 8.81458.
Train: 2018-08-02T15:07:58.047047: step 13349, loss 7.17759.
Train: 2018-08-02T15:07:58.109532: step 13350, loss 7.05167.
Test: 2018-08-02T15:07:58.312579: step 13350, loss 12.2995.
Train: 2018-08-02T15:07:58.359474: step 13351, loss 9.31827.
Train: 2018-08-02T15:07:58.421959: step 13352, loss 7.8072.
Train: 2018-08-02T15:07:58.484444: step 13353, loss 8.56274.
Train: 2018-08-02T15:07:58.546931: step 13354, loss 8.31089.
Train: 2018-08-02T15:07:58.593796: step 13355, loss 8.18497.
Train: 2018-08-02T15:07:58.656279: step 13356, loss 6.92574.
Train: 2018-08-02T15:07:58.718767: step 13357, loss 8.81458.
Train: 2018-08-02T15:07:58.781220: step 13358, loss 7.42943.
Train: 2018-08-02T15:07:58.828108: step 13359, loss 7.8072.
Train: 2018-08-02T15:07:58.890570: step 13360, loss 8.31089.
Test: 2018-08-02T15:07:59.093646: step 13360, loss 12.2995.
Train: 2018-08-02T15:07:59.156157: step 13361, loss 8.43682.
Train: 2018-08-02T15:07:59.203024: step 13362, loss 5.5406.
Train: 2018-08-02T15:07:59.265482: step 13363, loss 9.06643.
Train: 2018-08-02T15:07:59.327999: step 13364, loss 8.05905.
Train: 2018-08-02T15:07:59.390483: step 13365, loss 8.05905.
Train: 2018-08-02T15:07:59.437346: step 13366, loss 8.56274.
Train: 2018-08-02T15:07:59.499831: step 13367, loss 8.56274.
Train: 2018-08-02T15:07:59.562320: step 13368, loss 7.8072.
Train: 2018-08-02T15:07:59.609151: step 13369, loss 8.18497.
Train: 2018-08-02T15:07:59.671637: step 13370, loss 9.57012.
Test: 2018-08-02T15:07:59.874738: step 13370, loss 12.2995.
Train: 2018-08-02T15:07:59.937199: step 13371, loss 8.18497.
Train: 2018-08-02T15:07:59.999714: step 13372, loss 7.30351.
Train: 2018-08-02T15:08:00.046575: step 13373, loss 7.17759.
Train: 2018-08-02T15:08:00.109065: step 13374, loss 10.0738.
Train: 2018-08-02T15:08:00.171532: step 13375, loss 8.43682.
Train: 2018-08-02T15:08:00.234035: step 13376, loss 8.18497.
Train: 2018-08-02T15:08:00.280894: step 13377, loss 7.30351.
Train: 2018-08-02T15:08:00.343387: step 13378, loss 8.05905.
Train: 2018-08-02T15:08:00.405866: step 13379, loss 9.19235.
Train: 2018-08-02T15:08:00.452703: step 13380, loss 8.56274.
Test: 2018-08-02T15:08:00.655805: step 13380, loss 12.2995.
Train: 2018-08-02T15:08:00.718298: step 13381, loss 6.79982.
Train: 2018-08-02T15:08:00.780783: step 13382, loss 6.92574.
Train: 2018-08-02T15:08:00.843236: step 13383, loss 7.55536.
Train: 2018-08-02T15:08:00.890132: step 13384, loss 8.68866.
Train: 2018-08-02T15:08:00.952617: step 13385, loss 9.19235.
Train: 2018-08-02T15:08:01.015102: step 13386, loss 7.8072.
Train: 2018-08-02T15:08:01.061936: step 13387, loss 8.56274.
Train: 2018-08-02T15:08:01.124446: step 13388, loss 6.92574.
Train: 2018-08-02T15:08:01.186940: step 13389, loss 8.43682.
Train: 2018-08-02T15:08:01.233802: step 13390, loss 7.30351.
Test: 2018-08-02T15:08:01.436847: step 13390, loss 12.2995.
Train: 2018-08-02T15:08:01.499363: step 13391, loss 7.93313.
Train: 2018-08-02T15:08:01.561851: step 13392, loss 9.4442.
Train: 2018-08-02T15:08:01.608683: step 13393, loss 8.05905.
Train: 2018-08-02T15:08:01.671199: step 13394, loss 7.55536.
Train: 2018-08-02T15:08:01.733654: step 13395, loss 6.92574.
Train: 2018-08-02T15:08:01.796163: step 13396, loss 7.17759.
Train: 2018-08-02T15:08:01.843004: step 13397, loss 7.55536.
Train: 2018-08-02T15:08:01.905488: step 13398, loss 7.68128.
Train: 2018-08-02T15:08:01.967974: step 13399, loss 7.30351.
Train: 2018-08-02T15:08:02.014869: step 13400, loss 7.30351.
Test: 2018-08-02T15:08:02.217939: step 13400, loss 12.2995.
Train: 2018-08-02T15:08:02.764686: step 13401, loss 7.8072.
Train: 2018-08-02T15:08:02.827147: step 13402, loss 8.18497.
Train: 2018-08-02T15:08:02.874011: step 13403, loss 8.31089.
Train: 2018-08-02T15:08:02.936497: step 13404, loss 8.05905.
Train: 2018-08-02T15:08:02.998984: step 13405, loss 7.30351.
Train: 2018-08-02T15:08:03.045846: step 13406, loss 8.31089.
Train: 2018-08-02T15:08:03.108362: step 13407, loss 7.93313.
Train: 2018-08-02T15:08:03.170848: step 13408, loss 7.93313.
Train: 2018-08-02T15:08:03.217713: step 13409, loss 8.43682.
Train: 2018-08-02T15:08:03.280196: step 13410, loss 5.5406.
Test: 2018-08-02T15:08:03.483268: step 13410, loss 12.2995.
Train: 2018-08-02T15:08:03.530133: step 13411, loss 7.93313.
Train: 2018-08-02T15:08:03.592626: step 13412, loss 7.30351.
Train: 2018-08-02T15:08:03.655103: step 13413, loss 9.06643.
Train: 2018-08-02T15:08:03.717596: step 13414, loss 7.93313.
Train: 2018-08-02T15:08:03.764453: step 13415, loss 7.93312.
Train: 2018-08-02T15:08:03.826941: step 13416, loss 7.93313.
Train: 2018-08-02T15:08:03.889423: step 13417, loss 8.81458.
Train: 2018-08-02T15:08:03.936296: step 13418, loss 7.42943.
Train: 2018-08-02T15:08:03.998778: step 13419, loss 8.43682.
Train: 2018-08-02T15:08:04.061232: step 13420, loss 8.68866.
Test: 2018-08-02T15:08:04.248714: step 13420, loss 12.2995.
Train: 2018-08-02T15:08:04.311206: step 13421, loss 8.94051.
Train: 2018-08-02T15:08:04.373661: step 13422, loss 7.42943.
Train: 2018-08-02T15:08:04.436178: step 13423, loss 8.18497.
Train: 2018-08-02T15:08:04.498632: step 13424, loss 7.93313.
Train: 2018-08-02T15:08:04.545526: step 13425, loss 7.17759.
Train: 2018-08-02T15:08:04.607981: step 13426, loss 7.05167.
Train: 2018-08-02T15:08:04.654869: step 13427, loss 6.92574.
Train: 2018-08-02T15:08:04.717362: step 13428, loss 8.43682.
Train: 2018-08-02T15:08:04.779815: step 13429, loss 7.8072.
Train: 2018-08-02T15:08:04.842332: step 13430, loss 8.81458.
Test: 2018-08-02T15:08:05.029781: step 13430, loss 12.2995.
Train: 2018-08-02T15:08:05.092273: step 13431, loss 7.55536.
Train: 2018-08-02T15:08:05.154728: step 13432, loss 6.04429.
Train: 2018-08-02T15:08:05.217245: step 13433, loss 6.92574.
Train: 2018-08-02T15:08:05.264108: step 13434, loss 6.42205.
Train: 2018-08-02T15:08:05.326590: step 13435, loss 7.93313.
Train: 2018-08-02T15:08:05.389081: step 13436, loss 7.55536.
Train: 2018-08-02T15:08:05.435944: step 13437, loss 7.8072.
Train: 2018-08-02T15:08:05.498427: step 13438, loss 7.93313.
Train: 2018-08-02T15:08:05.545260: step 13439, loss 6.1786.
Train: 2018-08-02T15:08:05.607776: step 13440, loss 8.05905.
Test: 2018-08-02T15:08:05.795232: step 13440, loss 12.2995.
Train: 2018-08-02T15:08:05.857689: step 13441, loss 8.18497.
Train: 2018-08-02T15:08:05.920204: step 13442, loss 8.18497.
Train: 2018-08-02T15:08:05.982659: step 13443, loss 7.55536.
Train: 2018-08-02T15:08:06.029553: step 13444, loss 7.8072.
Train: 2018-08-02T15:08:06.092009: step 13445, loss 7.17759.
Train: 2018-08-02T15:08:06.154523: step 13446, loss 6.92574.
Train: 2018-08-02T15:08:06.201357: step 13447, loss 7.8072.
Train: 2018-08-02T15:08:06.263845: step 13448, loss 8.81458.
Train: 2018-08-02T15:08:06.326328: step 13449, loss 7.93313.
Train: 2018-08-02T15:08:06.373222: step 13450, loss 8.43682.
Test: 2018-08-02T15:08:06.576294: step 13450, loss 12.2995.
Train: 2018-08-02T15:08:06.638755: step 13451, loss 7.30351.
Train: 2018-08-02T15:08:06.701239: step 13452, loss 8.43682.
Train: 2018-08-02T15:08:06.748134: step 13453, loss 8.05905.
Train: 2018-08-02T15:08:06.810623: step 13454, loss 8.18497.
Train: 2018-08-02T15:08:06.873076: step 13455, loss 6.29613.
Train: 2018-08-02T15:08:06.935561: step 13456, loss 8.94051.
Train: 2018-08-02T15:08:06.982423: step 13457, loss 7.8072.
Train: 2018-08-02T15:08:07.044941: step 13458, loss 7.8072.
Train: 2018-08-02T15:08:07.107426: step 13459, loss 7.8072.
Train: 2018-08-02T15:08:07.154260: step 13460, loss 7.05167.
Test: 2018-08-02T15:08:07.357361: step 13460, loss 12.2995.
Train: 2018-08-02T15:08:07.419823: step 13461, loss 7.8072.
Train: 2018-08-02T15:08:07.482306: step 13462, loss 8.18497.
Train: 2018-08-02T15:08:07.544822: step 13463, loss 6.92574.
Train: 2018-08-02T15:08:07.591689: step 13464, loss 7.30351.
Train: 2018-08-02T15:08:07.654143: step 13465, loss 8.43682.
Train: 2018-08-02T15:08:07.716654: step 13466, loss 7.55536.
Train: 2018-08-02T15:08:07.763521: step 13467, loss 6.79982.
Train: 2018-08-02T15:08:07.826004: step 13468, loss 8.31089.
Train: 2018-08-02T15:08:07.888493: step 13469, loss 9.4442.
Train: 2018-08-02T15:08:07.935360: step 13470, loss 7.42943.
Test: 2018-08-02T15:08:08.138434: step 13470, loss 12.2995.
Train: 2018-08-02T15:08:08.200918: step 13471, loss 9.19235.
Train: 2018-08-02T15:08:08.263405: step 13472, loss 7.42943.
Train: 2018-08-02T15:08:08.310269: step 13473, loss 7.68128.
Train: 2018-08-02T15:08:08.372755: step 13474, loss 8.56274.
Train: 2018-08-02T15:08:08.435244: step 13475, loss 8.31089.
Train: 2018-08-02T15:08:08.497695: step 13476, loss 9.19235.
Train: 2018-08-02T15:08:08.544589: step 13477, loss 7.55536.
Train: 2018-08-02T15:08:08.607069: step 13478, loss 7.8072.
Train: 2018-08-02T15:08:08.669560: step 13479, loss 7.55536.
Train: 2018-08-02T15:08:08.716425: step 13480, loss 8.05905.
Test: 2018-08-02T15:08:08.919470: step 13480, loss 12.2995.
Train: 2018-08-02T15:08:08.981956: step 13481, loss 8.18497.
Train: 2018-08-02T15:08:09.044471: step 13482, loss 7.30351.
Train: 2018-08-02T15:08:09.106926: step 13483, loss 8.56274.
Train: 2018-08-02T15:08:09.153792: step 13484, loss 8.18497.
Train: 2018-08-02T15:08:09.216278: step 13485, loss 7.42943.
Train: 2018-08-02T15:08:09.278791: step 13486, loss 7.42943.
Train: 2018-08-02T15:08:09.341278: step 13487, loss 8.43682.
Train: 2018-08-02T15:08:09.388145: step 13488, loss 8.56274.
Train: 2018-08-02T15:08:09.450627: step 13489, loss 6.92574.
Train: 2018-08-02T15:08:09.513112: step 13490, loss 7.17759.
Test: 2018-08-02T15:08:09.700562: step 13490, loss 12.2995.
Train: 2018-08-02T15:08:09.763024: step 13491, loss 7.17759.
Train: 2018-08-02T15:08:09.825536: step 13492, loss 7.30351.
Train: 2018-08-02T15:08:09.888024: step 13493, loss 6.54798.
Train: 2018-08-02T15:08:09.950517: step 13494, loss 7.42943.
Train: 2018-08-02T15:08:09.997375: step 13495, loss 8.81458.
Train: 2018-08-02T15:08:10.059859: step 13496, loss 6.6739.
Train: 2018-08-02T15:08:10.122344: step 13497, loss 7.42943.
Train: 2018-08-02T15:08:10.169178: step 13498, loss 7.68128.
Train: 2018-08-02T15:08:10.231663: step 13499, loss 8.31089.
Train: 2018-08-02T15:08:10.294178: step 13500, loss 8.31089.
Test: 2018-08-02T15:08:10.481604: step 13500, loss 12.2995.
Train: 2018-08-02T15:08:11.059596: step 13501, loss 6.79982.
Train: 2018-08-02T15:08:11.122080: step 13502, loss 7.55536.
Train: 2018-08-02T15:08:11.184598: step 13503, loss 9.06643.
Train: 2018-08-02T15:08:11.231460: step 13504, loss 6.6739.
Train: 2018-08-02T15:08:11.293946: step 13505, loss 7.17759.
Train: 2018-08-02T15:08:11.356425: step 13506, loss 7.55536.
Train: 2018-08-02T15:08:11.418915: step 13507, loss 7.17759.
Train: 2018-08-02T15:08:11.465751: step 13508, loss 8.18497.
Train: 2018-08-02T15:08:11.528265: step 13509, loss 7.93313.
Train: 2018-08-02T15:08:11.590752: step 13510, loss 7.55536.
Test: 2018-08-02T15:08:11.793823: step 13510, loss 12.2995.
Train: 2018-08-02T15:08:11.840662: step 13511, loss 6.29613.
Train: 2018-08-02T15:08:11.903172: step 13512, loss 7.68128.
Train: 2018-08-02T15:08:11.965634: step 13513, loss 6.92574.
Train: 2018-08-02T15:08:12.028149: step 13514, loss 7.68128.
Train: 2018-08-02T15:08:12.075009: step 13515, loss 7.93313.
Train: 2018-08-02T15:08:12.137497: step 13516, loss 6.42205.
Train: 2018-08-02T15:08:12.199952: step 13517, loss 7.05167.
Train: 2018-08-02T15:08:12.246847: step 13518, loss 7.8072.
Train: 2018-08-02T15:08:12.309332: step 13519, loss 7.30351.
Train: 2018-08-02T15:08:12.371811: step 13520, loss 7.68128.
Test: 2018-08-02T15:08:12.559267: step 13520, loss 12.2995.
Train: 2018-08-02T15:08:12.621759: step 13521, loss 7.05167.
Train: 2018-08-02T15:08:12.684244: step 13522, loss 8.05905.
Train: 2018-08-02T15:08:12.746730: step 13523, loss 7.05167.
Train: 2018-08-02T15:08:12.809215: step 13524, loss 6.54798.
Train: 2018-08-02T15:08:12.856050: step 13525, loss 6.92574.
Train: 2018-08-02T15:08:12.918564: step 13526, loss 7.05167.
Train: 2018-08-02T15:08:12.981020: step 13527, loss 6.92574.
Train: 2018-08-02T15:08:13.027914: step 13528, loss 7.42943.
Train: 2018-08-02T15:08:13.090368: step 13529, loss 6.79982.
Train: 2018-08-02T15:08:13.152855: step 13530, loss 7.8072.
Test: 2018-08-02T15:08:13.340335: step 13530, loss 12.2995.
Train: 2018-08-02T15:08:13.402796: step 13531, loss 5.79244.
Train: 2018-08-02T15:08:13.465282: step 13532, loss 7.93313.
Train: 2018-08-02T15:08:13.527797: step 13533, loss 8.81458.
Train: 2018-08-02T15:08:13.574632: step 13534, loss 7.17759.
Train: 2018-08-02T15:08:13.637116: step 13535, loss 6.04429.
Train: 2018-08-02T15:08:13.699601: step 13536, loss 6.54798.
Train: 2018-08-02T15:08:13.746466: step 13537, loss 7.68128.
Train: 2018-08-02T15:08:13.808977: step 13538, loss 8.68866.
Train: 2018-08-02T15:08:13.871436: step 13539, loss 7.8072.
Train: 2018-08-02T15:08:13.933922: step 13540, loss 7.42943.
Test: 2018-08-02T15:08:14.137001: step 13540, loss 12.2995.
Train: 2018-08-02T15:08:14.199484: step 13541, loss 8.56274.
Train: 2018-08-02T15:08:14.246378: step 13542, loss 7.05167.
Train: 2018-08-02T15:08:14.308866: step 13543, loss 8.68866.
Train: 2018-08-02T15:08:14.371350: step 13544, loss 7.68128.
Train: 2018-08-02T15:08:14.418213: step 13545, loss 7.8072.
Train: 2018-08-02T15:08:14.480669: step 13546, loss 7.68128.
Train: 2018-08-02T15:08:14.543154: step 13547, loss 7.93313.
Train: 2018-08-02T15:08:14.590018: step 13548, loss 7.30351.
Train: 2018-08-02T15:08:14.652533: step 13549, loss 8.18497.
Train: 2018-08-02T15:08:14.715020: step 13550, loss 7.8072.
Test: 2018-08-02T15:08:14.918066: step 13550, loss 12.2995.
Train: 2018-08-02T15:08:14.964930: step 13551, loss 7.93313.
Train: 2018-08-02T15:08:15.027415: step 13552, loss 7.17759.
Train: 2018-08-02T15:08:15.089900: step 13553, loss 8.05905.
Train: 2018-08-02T15:08:15.152386: step 13554, loss 7.17759.
Train: 2018-08-02T15:08:15.199278: step 13555, loss 8.56274.
Train: 2018-08-02T15:08:15.261760: step 13556, loss 6.42205.
Train: 2018-08-02T15:08:15.324248: step 13557, loss 7.8072.
Train: 2018-08-02T15:08:15.386738: step 13558, loss 7.42943.
Train: 2018-08-02T15:08:15.433570: step 13559, loss 7.42943.
Train: 2018-08-02T15:08:15.496056: step 13560, loss 6.92574.
Test: 2018-08-02T15:08:15.699166: step 13560, loss 12.2995.
Train: 2018-08-02T15:08:15.746028: step 13561, loss 9.19235.
Train: 2018-08-02T15:08:15.808483: step 13562, loss 7.55536.
Train: 2018-08-02T15:08:15.870968: step 13563, loss 6.6739.
Train: 2018-08-02T15:08:15.933455: step 13564, loss 7.30351.
Train: 2018-08-02T15:08:15.980342: step 13565, loss 7.68128.
Train: 2018-08-02T15:08:16.042828: step 13566, loss 6.79982.
Train: 2018-08-02T15:08:16.105289: step 13567, loss 7.68128.
Train: 2018-08-02T15:08:16.152182: step 13568, loss 7.68128.
Train: 2018-08-02T15:08:16.214670: step 13569, loss 8.43682.
Train: 2018-08-02T15:08:16.277153: step 13570, loss 7.93313.
Test: 2018-08-02T15:08:16.480200: step 13570, loss 12.2995.
Train: 2018-08-02T15:08:16.527095: step 13571, loss 7.30351.
Train: 2018-08-02T15:08:16.589550: step 13572, loss 7.55536.
Train: 2018-08-02T15:08:16.652065: step 13573, loss 7.05167.
Train: 2018-08-02T15:08:16.714553: step 13574, loss 8.05905.
Train: 2018-08-02T15:08:16.761394: step 13575, loss 6.79982.
Train: 2018-08-02T15:08:16.823869: step 13576, loss 7.68128.
Train: 2018-08-02T15:08:16.886380: step 13577, loss 8.18497.
Train: 2018-08-02T15:08:16.948865: step 13578, loss 9.4442.
Train: 2018-08-02T15:08:16.995736: step 13579, loss 7.17759.
Train: 2018-08-02T15:08:17.058191: step 13580, loss 7.05167.
Test: 2018-08-02T15:08:17.261266: step 13580, loss 12.2995.
Train: 2018-08-02T15:08:17.323784: step 13581, loss 7.42943.
Train: 2018-08-02T15:08:17.370618: step 13582, loss 7.55536.
Train: 2018-08-02T15:08:17.433135: step 13583, loss 7.30351.
Train: 2018-08-02T15:08:17.495588: step 13584, loss 8.68866.
Train: 2018-08-02T15:08:17.558107: step 13585, loss 8.05905.
Train: 2018-08-02T15:08:17.604937: step 13586, loss 7.30351.
Train: 2018-08-02T15:08:17.667421: step 13587, loss 7.68128.
Train: 2018-08-02T15:08:17.729937: step 13588, loss 7.93313.
Train: 2018-08-02T15:08:17.776772: step 13589, loss 7.55536.
Train: 2018-08-02T15:08:17.823636: step 13590, loss 7.79041.
Test: 2018-08-02T15:08:18.011091: step 13590, loss 12.2995.
Train: 2018-08-02T15:08:18.073577: step 13591, loss 7.17759.
Train: 2018-08-02T15:08:18.136062: step 13592, loss 6.92574.
Train: 2018-08-02T15:08:18.198577: step 13593, loss 6.79982.
Train: 2018-08-02T15:08:18.261034: step 13594, loss 8.18497.
Train: 2018-08-02T15:08:18.307928: step 13595, loss 8.43682.
Train: 2018-08-02T15:08:18.370414: step 13596, loss 8.81458.
Train: 2018-08-02T15:08:18.432898: step 13597, loss 7.42943.
Train: 2018-08-02T15:08:18.479758: step 13598, loss 8.05905.
Train: 2018-08-02T15:08:18.542247: step 13599, loss 8.43682.
Train: 2018-08-02T15:08:18.604703: step 13600, loss 8.05905.
Test: 2018-08-02T15:08:18.792188: step 13600, loss 12.2995.
Train: 2018-08-02T15:08:19.307695: step 13601, loss 7.93313.
Train: 2018-08-02T15:08:19.370179: step 13602, loss 6.92574.
Train: 2018-08-02T15:08:19.432634: step 13603, loss 7.93313.
Train: 2018-08-02T15:08:19.495151: step 13604, loss 6.92574.
Train: 2018-08-02T15:08:19.542014: step 13605, loss 6.54798.
Train: 2018-08-02T15:08:19.604499: step 13606, loss 6.79982.
Train: 2018-08-02T15:08:19.666984: step 13607, loss 8.31089.
Train: 2018-08-02T15:08:19.729471: step 13608, loss 7.68128.
Train: 2018-08-02T15:08:19.776334: step 13609, loss 6.79982.
Train: 2018-08-02T15:08:19.838821: step 13610, loss 7.05167.
Test: 2018-08-02T15:08:20.041866: step 13610, loss 12.2995.
Train: 2018-08-02T15:08:20.088762: step 13611, loss 7.93313.
Train: 2018-08-02T15:08:20.151246: step 13612, loss 7.30351.
Train: 2018-08-02T15:08:20.213700: step 13613, loss 6.92574.
Train: 2018-08-02T15:08:20.276186: step 13614, loss 7.8072.
Train: 2018-08-02T15:08:20.323078: step 13615, loss 6.79982.
Train: 2018-08-02T15:08:20.385561: step 13616, loss 7.30351.
Train: 2018-08-02T15:08:20.432433: step 13617, loss 7.55536.
Train: 2018-08-02T15:08:20.494916: step 13618, loss 8.43682.
Train: 2018-08-02T15:08:20.557372: step 13619, loss 7.93313.
Train: 2018-08-02T15:08:20.619857: step 13620, loss 8.05905.
Test: 2018-08-02T15:08:20.807311: step 13620, loss 12.2995.
Train: 2018-08-02T15:08:20.869797: step 13621, loss 6.6739.
Train: 2018-08-02T15:08:20.932316: step 13622, loss 7.68128.
Train: 2018-08-02T15:08:20.994799: step 13623, loss 7.17759.
Train: 2018-08-02T15:08:21.057284: step 13624, loss 6.92574.
Train: 2018-08-02T15:08:21.104118: step 13625, loss 7.30351.
Train: 2018-08-02T15:08:21.166603: step 13626, loss 8.68866.
Train: 2018-08-02T15:08:21.229119: step 13627, loss 8.18497.
Train: 2018-08-02T15:08:21.275982: step 13628, loss 7.93312.
Train: 2018-08-02T15:08:21.338437: step 13629, loss 7.17759.
Train: 2018-08-02T15:08:21.400923: step 13630, loss 7.8072.
Test: 2018-08-02T15:08:21.604031: step 13630, loss 12.2995.
Train: 2018-08-02T15:08:21.650864: step 13631, loss 6.92574.
Train: 2018-08-02T15:08:21.713351: step 13632, loss 8.18497.
Train: 2018-08-02T15:08:21.775865: step 13633, loss 7.05167.
Train: 2018-08-02T15:08:21.838353: step 13634, loss 7.68128.
Train: 2018-08-02T15:08:21.900807: step 13635, loss 7.68128.
Train: 2018-08-02T15:08:21.947697: step 13636, loss 8.31089.
Train: 2018-08-02T15:08:22.010181: step 13637, loss 8.56274.
Train: 2018-08-02T15:08:22.072643: step 13638, loss 7.55536.
Train: 2018-08-02T15:08:22.119504: step 13639, loss 7.68128.
Train: 2018-08-02T15:08:22.181991: step 13640, loss 7.30351.
Test: 2018-08-02T15:08:22.385092: step 13640, loss 12.2995.
Train: 2018-08-02T15:08:22.447579: step 13641, loss 7.55536.
Train: 2018-08-02T15:08:22.510038: step 13642, loss 7.30351.
Train: 2018-08-02T15:08:22.556903: step 13643, loss 7.68128.
Train: 2018-08-02T15:08:22.619388: step 13644, loss 6.29613.
Train: 2018-08-02T15:08:22.681914: step 13645, loss 8.43682.
Train: 2018-08-02T15:08:22.728762: step 13646, loss 7.30351.
Train: 2018-08-02T15:08:22.791222: step 13647, loss 7.8072.
Train: 2018-08-02T15:08:22.853739: step 13648, loss 7.93313.
Train: 2018-08-02T15:08:22.916224: step 13649, loss 6.29613.
Train: 2018-08-02T15:08:22.963081: step 13650, loss 8.31089.
Test: 2018-08-02T15:08:23.166164: step 13650, loss 12.2995.
Train: 2018-08-02T15:08:23.228650: step 13651, loss 7.42943.
Train: 2018-08-02T15:08:23.291106: step 13652, loss 7.55536.
Train: 2018-08-02T15:08:23.338001: step 13653, loss 7.68128.
Train: 2018-08-02T15:08:23.400485: step 13654, loss 6.92574.
Train: 2018-08-02T15:08:23.462940: step 13655, loss 9.06643.
Train: 2018-08-02T15:08:23.509829: step 13656, loss 7.05167.
Train: 2018-08-02T15:08:23.572317: step 13657, loss 7.8072.
Train: 2018-08-02T15:08:23.634806: step 13658, loss 7.55536.
Train: 2018-08-02T15:08:23.697261: step 13659, loss 7.68128.
Train: 2018-08-02T15:08:23.744154: step 13660, loss 6.92574.
Test: 2018-08-02T15:08:23.947233: step 13660, loss 12.2995.
Train: 2018-08-02T15:08:24.009716: step 13661, loss 6.42205.
Train: 2018-08-02T15:08:24.072202: step 13662, loss 7.42943.
Train: 2018-08-02T15:08:24.119067: step 13663, loss 8.94051.
Train: 2018-08-02T15:08:24.181555: step 13664, loss 7.8072.
Train: 2018-08-02T15:08:24.244038: step 13665, loss 7.30351.
Train: 2018-08-02T15:08:24.306517: step 13666, loss 8.18497.
Train: 2018-08-02T15:08:24.353386: step 13667, loss 7.55536.
Train: 2018-08-02T15:08:24.415873: step 13668, loss 7.05167.
Train: 2018-08-02T15:08:24.478329: step 13669, loss 7.55536.
Train: 2018-08-02T15:08:24.525222: step 13670, loss 7.17759.
Test: 2018-08-02T15:08:24.728293: step 13670, loss 12.2995.
Train: 2018-08-02T15:08:24.790754: step 13671, loss 8.56274.
Train: 2018-08-02T15:08:24.837617: step 13672, loss 7.42943.
Train: 2018-08-02T15:08:24.900134: step 13673, loss 9.06643.
Train: 2018-08-02T15:08:24.962621: step 13674, loss 9.31827.
Train: 2018-08-02T15:08:25.009452: step 13675, loss 7.8072.
Train: 2018-08-02T15:08:25.071971: step 13676, loss 7.30351.
Train: 2018-08-02T15:08:25.134450: step 13677, loss 8.81458.
Train: 2018-08-02T15:08:25.196911: step 13678, loss 6.6739.
Train: 2018-08-02T15:08:25.243772: step 13679, loss 7.93313.
Train: 2018-08-02T15:08:25.306291: step 13680, loss 8.05905.
Test: 2018-08-02T15:08:25.509360: step 13680, loss 12.2995.
Train: 2018-08-02T15:08:25.571853: step 13681, loss 6.92574.
Train: 2018-08-02T15:08:25.618710: step 13682, loss 6.79982.
Train: 2018-08-02T15:08:25.681204: step 13683, loss 6.79982.
Train: 2018-08-02T15:08:25.743688: step 13684, loss 7.55536.
Train: 2018-08-02T15:08:25.806143: step 13685, loss 7.17759.
Train: 2018-08-02T15:08:25.868657: step 13686, loss 7.42943.
Train: 2018-08-02T15:08:25.915490: step 13687, loss 5.91836.
Train: 2018-08-02T15:08:25.978008: step 13688, loss 7.42943.
Train: 2018-08-02T15:08:26.040492: step 13689, loss 6.79982.
Train: 2018-08-02T15:08:26.087363: step 13690, loss 7.93313.
Test: 2018-08-02T15:08:26.290403: step 13690, loss 12.2995.
Train: 2018-08-02T15:08:26.352918: step 13691, loss 7.55536.
Train: 2018-08-02T15:08:26.415382: step 13692, loss 6.29613.
Train: 2018-08-02T15:08:26.462238: step 13693, loss 7.55536.
Train: 2018-08-02T15:08:26.524724: step 13694, loss 7.42943.
Train: 2018-08-02T15:08:26.587241: step 13695, loss 7.42943.
Train: 2018-08-02T15:08:26.634072: step 13696, loss 7.42943.
Train: 2018-08-02T15:08:26.696557: step 13697, loss 7.93313.
Train: 2018-08-02T15:08:26.759043: step 13698, loss 7.17759.
Train: 2018-08-02T15:08:26.805907: step 13699, loss 7.93313.
Train: 2018-08-02T15:08:26.868392: step 13700, loss 7.8072.
Test: 2018-08-02T15:08:27.071502: step 13700, loss 12.2995.
Train: 2018-08-02T15:08:27.586974: step 13701, loss 6.29613.
Train: 2018-08-02T15:08:27.649491: step 13702, loss 7.30351.
Train: 2018-08-02T15:08:27.696356: step 13703, loss 6.92574.
Train: 2018-08-02T15:08:27.758840: step 13704, loss 7.68128.
Train: 2018-08-02T15:08:27.821295: step 13705, loss 8.18497.
Train: 2018-08-02T15:08:27.883780: step 13706, loss 6.17021.
Train: 2018-08-02T15:08:27.930676: step 13707, loss 7.55536.
Train: 2018-08-02T15:08:27.993131: step 13708, loss 7.42943.
Train: 2018-08-02T15:08:28.055640: step 13709, loss 5.79244.
Train: 2018-08-02T15:08:28.118129: step 13710, loss 6.54798.
Test: 2018-08-02T15:08:28.321202: step 13710, loss 12.2995.
Train: 2018-08-02T15:08:28.368041: step 13711, loss 8.56274.
Train: 2018-08-02T15:08:28.430557: step 13712, loss 6.6739.
Train: 2018-08-02T15:08:28.493037: step 13713, loss 7.42943.
Train: 2018-08-02T15:08:28.555531: step 13714, loss 7.68128.
Train: 2018-08-02T15:08:28.602362: step 13715, loss 7.42943.
Train: 2018-08-02T15:08:28.664846: step 13716, loss 7.05167.
Train: 2018-08-02T15:08:28.727362: step 13717, loss 8.31089.
Train: 2018-08-02T15:08:28.774227: step 13718, loss 7.8072.
Train: 2018-08-02T15:08:28.836714: step 13719, loss 6.6739.
Train: 2018-08-02T15:08:28.899166: step 13720, loss 6.79982.
Test: 2018-08-02T15:08:29.086655: step 13720, loss 12.2995.
Train: 2018-08-02T15:08:29.149139: step 13721, loss 7.17759.
Train: 2018-08-02T15:08:29.211594: step 13722, loss 8.31089.
Train: 2018-08-02T15:08:29.258488: step 13723, loss 6.29613.
Train: 2018-08-02T15:08:29.320944: step 13724, loss 6.92574.
Train: 2018-08-02T15:08:29.383459: step 13725, loss 6.79982.
Train: 2018-08-02T15:08:29.445913: step 13726, loss 7.8072.
Train: 2018-08-02T15:08:29.492778: step 13727, loss 8.05905.
Train: 2018-08-02T15:08:29.555296: step 13728, loss 8.18497.
Train: 2018-08-02T15:08:29.617783: step 13729, loss 7.68128.
Train: 2018-08-02T15:08:29.664638: step 13730, loss 5.79244.
Test: 2018-08-02T15:08:29.867715: step 13730, loss 12.2995.
Train: 2018-08-02T15:08:29.930175: step 13731, loss 6.6739.
Train: 2018-08-02T15:08:29.992661: step 13732, loss 7.30351.
Train: 2018-08-02T15:08:30.039524: step 13733, loss 7.55536.
Train: 2018-08-02T15:08:30.102041: step 13734, loss 7.17759.
Train: 2018-08-02T15:08:30.164527: step 13735, loss 7.30351.
Train: 2018-08-02T15:08:30.211392: step 13736, loss 7.93313.
Train: 2018-08-02T15:08:30.273870: step 13737, loss 7.93313.
Train: 2018-08-02T15:08:30.336358: step 13738, loss 8.31089.
Train: 2018-08-02T15:08:30.398846: step 13739, loss 8.05905.
Train: 2018-08-02T15:08:30.445707: step 13740, loss 6.92574.
Test: 2018-08-02T15:08:30.648782: step 13740, loss 12.2995.
Train: 2018-08-02T15:08:30.695649: step 13741, loss 8.32768.
Train: 2018-08-02T15:08:30.758106: step 13742, loss 9.19235.
Train: 2018-08-02T15:08:30.820624: step 13743, loss 8.43682.
Train: 2018-08-02T15:08:30.867486: step 13744, loss 7.93313.
Train: 2018-08-02T15:08:30.929976: step 13745, loss 7.42943.
Train: 2018-08-02T15:08:30.992451: step 13746, loss 6.54798.
Train: 2018-08-02T15:08:31.054936: step 13747, loss 6.42205.
Train: 2018-08-02T15:08:31.101778: step 13748, loss 7.42943.
Train: 2018-08-02T15:08:31.164262: step 13749, loss 7.05167.
Train: 2018-08-02T15:08:31.226777: step 13750, loss 8.18497.
Test: 2018-08-02T15:08:31.414233: step 13750, loss 12.2995.
Train: 2018-08-02T15:08:31.476690: step 13751, loss 7.42943.
Train: 2018-08-02T15:08:31.539205: step 13752, loss 7.68128.
Train: 2018-08-02T15:08:31.586070: step 13753, loss 6.92574.
Train: 2018-08-02T15:08:31.648554: step 13754, loss 7.05167.
Train: 2018-08-02T15:08:31.711041: step 13755, loss 7.55536.
Train: 2018-08-02T15:08:31.757919: step 13756, loss 7.30351.
Train: 2018-08-02T15:08:31.820358: step 13757, loss 7.17759.
Train: 2018-08-02T15:08:31.882843: step 13758, loss 7.55536.
Train: 2018-08-02T15:08:31.945328: step 13759, loss 6.42205.
Train: 2018-08-02T15:08:31.992224: step 13760, loss 7.05167.
Test: 2018-08-02T15:08:32.195269: step 13760, loss 12.2995.
Train: 2018-08-02T15:08:32.257785: step 13761, loss 7.55536.
Train: 2018-08-02T15:08:32.320241: step 13762, loss 7.68128.
Train: 2018-08-02T15:08:32.367135: step 13763, loss 8.31089.
Train: 2018-08-02T15:08:32.429624: step 13764, loss 7.30351.
Train: 2018-08-02T15:08:32.492076: step 13765, loss 7.55536.
Train: 2018-08-02T15:08:32.538939: step 13766, loss 6.6739.
Train: 2018-08-02T15:08:32.601424: step 13767, loss 8.43682.
Train: 2018-08-02T15:08:32.663911: step 13768, loss 7.17759.
Train: 2018-08-02T15:08:32.710774: step 13769, loss 6.6739.
Train: 2018-08-02T15:08:32.773259: step 13770, loss 6.29613.
Test: 2018-08-02T15:08:32.976337: step 13770, loss 12.2995.
Train: 2018-08-02T15:08:33.038854: step 13771, loss 6.42205.
Train: 2018-08-02T15:08:33.085718: step 13772, loss 8.56274.
Train: 2018-08-02T15:08:33.148201: step 13773, loss 8.05905.
Train: 2018-08-02T15:08:33.210690: step 13774, loss 8.81458.
Train: 2018-08-02T15:08:33.273169: step 13775, loss 7.17759.
Train: 2018-08-02T15:08:33.320034: step 13776, loss 7.93313.
Train: 2018-08-02T15:08:33.382523: step 13777, loss 7.42943.
Train: 2018-08-02T15:08:33.445005: step 13778, loss 8.18497.
Train: 2018-08-02T15:08:33.491867: step 13779, loss 6.6739.
Train: 2018-08-02T15:08:33.554360: step 13780, loss 7.93313.
Test: 2018-08-02T15:08:33.757404: step 13780, loss 12.2995.
Train: 2018-08-02T15:08:33.819919: step 13781, loss 7.30351.
Train: 2018-08-02T15:08:33.866784: step 13782, loss 7.55536.
Train: 2018-08-02T15:08:33.929264: step 13783, loss 7.55536.
Train: 2018-08-02T15:08:33.991754: step 13784, loss 7.05167.
Train: 2018-08-02T15:08:34.054243: step 13785, loss 8.43682.
Train: 2018-08-02T15:08:34.101073: step 13786, loss 7.8072.
Train: 2018-08-02T15:08:34.163584: step 13787, loss 7.55536.
Train: 2018-08-02T15:08:34.226074: step 13788, loss 6.54798.
Train: 2018-08-02T15:08:34.272909: step 13789, loss 7.42943.
Train: 2018-08-02T15:08:34.335422: step 13790, loss 8.68866.
Test: 2018-08-02T15:08:34.538495: step 13790, loss 12.2995.
Train: 2018-08-02T15:08:34.600986: step 13791, loss 7.55536.
Train: 2018-08-02T15:08:34.647851: step 13792, loss 7.05167.
Train: 2018-08-02T15:08:34.710338: step 13793, loss 7.42943.
Train: 2018-08-02T15:08:34.772821: step 13794, loss 7.93312.
Train: 2018-08-02T15:08:34.835308: step 13795, loss 7.93313.
Train: 2018-08-02T15:08:34.882172: step 13796, loss 7.68128.
Train: 2018-08-02T15:08:34.944627: step 13797, loss 8.56274.
Train: 2018-08-02T15:08:35.007138: step 13798, loss 8.05905.
Train: 2018-08-02T15:08:35.054007: step 13799, loss 7.05167.
Train: 2018-08-02T15:08:35.116492: step 13800, loss 7.8072.
Test: 2018-08-02T15:08:35.319571: step 13800, loss 12.2995.
Train: 2018-08-02T15:08:35.819449: step 13801, loss 7.55536.
Train: 2018-08-02T15:08:35.881933: step 13802, loss 7.93313.
Train: 2018-08-02T15:08:35.944419: step 13803, loss 5.41467.
Train: 2018-08-02T15:08:36.006909: step 13804, loss 6.79982.
Train: 2018-08-02T15:08:36.053768: step 13805, loss 7.05167.
Train: 2018-08-02T15:08:36.116253: step 13806, loss 8.68866.
Train: 2018-08-02T15:08:36.178712: step 13807, loss 8.05905.
Train: 2018-08-02T15:08:36.225606: step 13808, loss 7.8072.
Train: 2018-08-02T15:08:36.288061: step 13809, loss 8.18497.
Train: 2018-08-02T15:08:36.350576: step 13810, loss 7.42943.
Test: 2018-08-02T15:08:36.553623: step 13810, loss 12.2995.
Train: 2018-08-02T15:08:36.600519: step 13811, loss 8.81458.
Train: 2018-08-02T15:08:36.663004: step 13812, loss 7.30351.
Train: 2018-08-02T15:08:36.725484: step 13813, loss 7.42943.
Train: 2018-08-02T15:08:36.787946: step 13814, loss 6.79982.
Train: 2018-08-02T15:08:36.834810: step 13815, loss 8.43682.
Train: 2018-08-02T15:08:36.897325: step 13816, loss 8.31089.
Train: 2018-08-02T15:08:36.959812: step 13817, loss 7.68128.
Train: 2018-08-02T15:08:37.006644: step 13818, loss 7.68128.
Train: 2018-08-02T15:08:37.069159: step 13819, loss 7.93313.
Train: 2018-08-02T15:08:37.131644: step 13820, loss 6.17021.
Test: 2018-08-02T15:08:37.334690: step 13820, loss 12.2995.
Train: 2018-08-02T15:08:37.381556: step 13821, loss 7.55536.
Train: 2018-08-02T15:08:37.444068: step 13822, loss 7.42943.
Train: 2018-08-02T15:08:37.506527: step 13823, loss 7.55536.
Train: 2018-08-02T15:08:37.569040: step 13824, loss 7.30351.
Train: 2018-08-02T15:08:37.615906: step 13825, loss 7.93313.
Train: 2018-08-02T15:08:37.678391: step 13826, loss 7.93313.
Train: 2018-08-02T15:08:37.740877: step 13827, loss 7.8072.
Train: 2018-08-02T15:08:37.787736: step 13828, loss 7.05167.
Train: 2018-08-02T15:08:37.850226: step 13829, loss 7.30351.
Train: 2018-08-02T15:08:37.912711: step 13830, loss 8.43682.
Test: 2018-08-02T15:08:38.115791: step 13830, loss 12.2995.
Train: 2018-08-02T15:08:38.162653: step 13831, loss 5.79244.
Train: 2018-08-02T15:08:38.225138: step 13832, loss 7.93313.
Train: 2018-08-02T15:08:38.287624: step 13833, loss 7.05167.
Train: 2018-08-02T15:08:38.350110: step 13834, loss 7.8072.
Train: 2018-08-02T15:08:38.396974: step 13835, loss 7.42943.
Train: 2018-08-02T15:08:38.459460: step 13836, loss 7.68128.
Train: 2018-08-02T15:08:38.521914: step 13837, loss 7.42943.
Train: 2018-08-02T15:08:38.584432: step 13838, loss 7.8072.
Train: 2018-08-02T15:08:38.631293: step 13839, loss 6.92574.
Train: 2018-08-02T15:08:38.693776: step 13840, loss 7.05167.
Test: 2018-08-02T15:08:38.896850: step 13840, loss 12.2995.
Train: 2018-08-02T15:08:38.943717: step 13841, loss 7.93313.
Train: 2018-08-02T15:08:39.006175: step 13842, loss 8.68866.
Train: 2018-08-02T15:08:39.068688: step 13843, loss 9.06643.
Train: 2018-08-02T15:08:39.131146: step 13844, loss 7.42943.
Train: 2018-08-02T15:08:39.193631: step 13845, loss 8.05905.
Train: 2018-08-02T15:08:39.256116: step 13846, loss 8.05905.
Train: 2018-08-02T15:08:39.303013: step 13847, loss 8.94051.
Train: 2018-08-02T15:08:39.365468: step 13848, loss 7.8072.
Train: 2018-08-02T15:08:39.427951: step 13849, loss 8.05905.
Train: 2018-08-02T15:08:39.474846: step 13850, loss 6.79982.
Test: 2018-08-02T15:08:39.677917: step 13850, loss 12.2995.
Train: 2018-08-02T15:08:39.740408: step 13851, loss 8.05905.
Train: 2018-08-02T15:08:39.802864: step 13852, loss 7.68128.
Train: 2018-08-02T15:08:39.849757: step 13853, loss 6.92574.
Train: 2018-08-02T15:08:39.912243: step 13854, loss 7.68128.
Train: 2018-08-02T15:08:39.974729: step 13855, loss 7.8072.
Train: 2018-08-02T15:08:40.021563: step 13856, loss 7.30351.
Train: 2018-08-02T15:08:40.084050: step 13857, loss 7.17759.
Train: 2018-08-02T15:08:40.146559: step 13858, loss 8.68866.
Train: 2018-08-02T15:08:40.209050: step 13859, loss 7.93313.
Train: 2018-08-02T15:08:40.255882: step 13860, loss 7.8072.
Test: 2018-08-02T15:08:40.458959: step 13860, loss 12.2995.
Train: 2018-08-02T15:08:40.505855: step 13861, loss 9.06643.
Train: 2018-08-02T15:08:40.568342: step 13862, loss 6.92574.
Train: 2018-08-02T15:08:40.630795: step 13863, loss 6.6739.
Train: 2018-08-02T15:08:40.693308: step 13864, loss 6.6739.
Train: 2018-08-02T15:08:40.740175: step 13865, loss 7.55536.
Train: 2018-08-02T15:08:40.802662: step 13866, loss 6.79982.
Train: 2018-08-02T15:08:40.865115: step 13867, loss 8.31089.
Train: 2018-08-02T15:08:40.912009: step 13868, loss 8.05905.
Train: 2018-08-02T15:08:40.974489: step 13869, loss 7.30351.
Train: 2018-08-02T15:08:41.036949: step 13870, loss 6.54798.
Test: 2018-08-02T15:08:41.224406: step 13870, loss 12.2995.
Train: 2018-08-02T15:08:41.286916: step 13871, loss 7.30351.
Train: 2018-08-02T15:08:41.349406: step 13872, loss 7.17759.
Train: 2018-08-02T15:08:41.411891: step 13873, loss 7.68128.
Train: 2018-08-02T15:08:41.458755: step 13874, loss 7.68128.
Train: 2018-08-02T15:08:41.521213: step 13875, loss 6.79982.
Train: 2018-08-02T15:08:41.583729: step 13876, loss 7.8072.
Train: 2018-08-02T15:08:41.646213: step 13877, loss 8.43682.
Train: 2018-08-02T15:08:41.693073: step 13878, loss 9.19235.
Train: 2018-08-02T15:08:41.755562: step 13879, loss 7.68128.
Train: 2018-08-02T15:08:41.802395: step 13880, loss 7.68128.
Test: 2018-08-02T15:08:42.005473: step 13880, loss 12.2995.
Train: 2018-08-02T15:08:42.067988: step 13881, loss 7.8072.
Train: 2018-08-02T15:08:42.130473: step 13882, loss 7.8072.
Train: 2018-08-02T15:08:42.177340: step 13883, loss 7.30351.
Train: 2018-08-02T15:08:42.239824: step 13884, loss 7.05167.
Train: 2018-08-02T15:08:42.302280: step 13885, loss 7.42943.
Train: 2018-08-02T15:08:42.364794: step 13886, loss 6.79982.
Train: 2018-08-02T15:08:42.411658: step 13887, loss 7.8072.
Train: 2018-08-02T15:08:42.474114: step 13888, loss 8.31089.
Train: 2018-08-02T15:08:42.536624: step 13889, loss 8.43682.
Train: 2018-08-02T15:08:42.583463: step 13890, loss 7.42943.
Test: 2018-08-02T15:08:42.786539: step 13890, loss 12.2995.
Train: 2018-08-02T15:08:42.833405: step 13891, loss 7.17759.
Train: 2018-08-02T15:08:42.880268: step 13892, loss 8.86495.
Train: 2018-08-02T15:08:42.942752: step 13893, loss 7.30351.
Train: 2018-08-02T15:08:43.005240: step 13894, loss 6.92574.
Train: 2018-08-02T15:08:43.067757: step 13895, loss 7.30351.
Train: 2018-08-02T15:08:43.114618: step 13896, loss 7.17759.
Train: 2018-08-02T15:08:43.177074: step 13897, loss 6.92574.
Train: 2018-08-02T15:08:43.239589: step 13898, loss 8.18497.
Train: 2018-08-02T15:08:43.286448: step 13899, loss 8.05905.
Train: 2018-08-02T15:08:43.348938: step 13900, loss 7.42943.
Test: 2018-08-02T15:08:43.551985: step 13900, loss 12.2995.
Train: 2018-08-02T15:08:44.051870: step 13901, loss 7.42943.
Train: 2018-08-02T15:08:44.114385: step 13902, loss 8.68866.
Train: 2018-08-02T15:08:44.176872: step 13903, loss 7.42943.
Train: 2018-08-02T15:08:44.239356: step 13904, loss 7.68128.
Train: 2018-08-02T15:08:44.286219: step 13905, loss 7.42943.
Train: 2018-08-02T15:08:44.348674: step 13906, loss 8.31089.
Train: 2018-08-02T15:08:44.411191: step 13907, loss 8.18497.
Train: 2018-08-02T15:08:44.458051: step 13908, loss 7.30351.
Train: 2018-08-02T15:08:44.520539: step 13909, loss 7.30351.
Train: 2018-08-02T15:08:44.583022: step 13910, loss 7.17759.
Test: 2018-08-02T15:08:44.786071: step 13910, loss 12.2995.
Train: 2018-08-02T15:08:44.832968: step 13911, loss 8.43682.
Train: 2018-08-02T15:08:44.895452: step 13912, loss 8.18497.
Train: 2018-08-02T15:08:44.957938: step 13913, loss 7.17759.
Train: 2018-08-02T15:08:45.020423: step 13914, loss 8.18497.
Train: 2018-08-02T15:08:45.067283: step 13915, loss 7.30351.
Train: 2018-08-02T15:08:45.129771: step 13916, loss 8.69401.
Train: 2018-08-02T15:08:45.192256: step 13917, loss 5.79244.
Train: 2018-08-02T15:08:45.239121: step 13918, loss 6.92574.
Train: 2018-08-02T15:08:45.301609: step 13919, loss 6.79982.
Train: 2018-08-02T15:08:45.364061: step 13920, loss 6.92574.
Test: 2018-08-02T15:08:45.567138: step 13920, loss 3.81863.
Train: 2018-08-02T15:08:45.614004: step 13921, loss 4.91098.
Train: 2018-08-02T15:08:45.676522: step 13922, loss 5.16283.
Train: 2018-08-02T15:08:45.739006: step 13923, loss 5.66652.
Train: 2018-08-02T15:08:45.801459: step 13924, loss 6.17021.
Train: 2018-08-02T15:08:45.848322: step 13925, loss 6.17021.
Train: 2018-08-02T15:08:45.910808: step 13926, loss 6.79982.
Train: 2018-08-02T15:08:45.973324: step 13927, loss 5.16283.
Train: 2018-08-02T15:08:46.020189: step 13928, loss 5.66652.
Train: 2018-08-02T15:08:46.082643: step 13929, loss 6.04429.
Train: 2018-08-02T15:08:46.145154: step 13930, loss 5.5406.
Test: 2018-08-02T15:08:46.332609: step 13930, loss 3.81863.
Train: 2018-08-02T15:08:46.395071: step 13931, loss 5.28875.
Train: 2018-08-02T15:08:46.457587: step 13932, loss 5.41467.
Train: 2018-08-02T15:08:46.520070: step 13933, loss 5.16283.
Train: 2018-08-02T15:08:46.566933: step 13934, loss 6.42205.
Train: 2018-08-02T15:08:46.629420: step 13935, loss 6.17021.
Train: 2018-08-02T15:08:46.691905: step 13936, loss 5.91836.
Train: 2018-08-02T15:08:46.754386: step 13937, loss 6.54798.
Train: 2018-08-02T15:08:46.801259: step 13938, loss 5.41467.
Train: 2018-08-02T15:08:46.863736: step 13939, loss 6.92574.
Train: 2018-08-02T15:08:46.926196: step 13940, loss 5.79244.
Test: 2018-08-02T15:08:47.113682: step 13940, loss 3.81863.
Train: 2018-08-02T15:08:47.176138: step 13941, loss 6.6739.
Train: 2018-08-02T15:08:47.238652: step 13942, loss 5.79244.
Train: 2018-08-02T15:08:47.285486: step 13943, loss 4.65914.
Train: 2018-08-02T15:08:47.348002: step 13944, loss 5.41467.
Train: 2018-08-02T15:08:47.410488: step 13945, loss 5.41467.
Train: 2018-08-02T15:08:47.472946: step 13946, loss 4.53321.
Train: 2018-08-02T15:08:47.519839: step 13947, loss 5.79244.
Train: 2018-08-02T15:08:47.582319: step 13948, loss 6.42205.
Train: 2018-08-02T15:08:47.629186: step 13949, loss 6.29613.
Train: 2018-08-02T15:08:47.691641: step 13950, loss 6.04429.
Test: 2018-08-02T15:08:47.894751: step 13950, loss 3.81863.
Train: 2018-08-02T15:08:47.957231: step 13951, loss 6.04429.
Train: 2018-08-02T15:08:48.004098: step 13952, loss 7.30351.
Train: 2018-08-02T15:08:48.066577: step 13953, loss 5.0369.
Train: 2018-08-02T15:08:48.129039: step 13954, loss 5.16283.
Train: 2018-08-02T15:08:48.175904: step 13955, loss 4.91098.
Train: 2018-08-02T15:08:48.238418: step 13956, loss 5.79244.
Train: 2018-08-02T15:08:48.300897: step 13957, loss 6.17021.
Train: 2018-08-02T15:08:48.347768: step 13958, loss 6.6739.
Train: 2018-08-02T15:08:48.410223: step 13959, loss 6.6739.
Train: 2018-08-02T15:08:48.472739: step 13960, loss 5.91836.
Test: 2018-08-02T15:08:48.660189: step 13960, loss 3.81863.
Train: 2018-08-02T15:08:48.722680: step 13961, loss 6.17021.
Train: 2018-08-02T15:08:48.785161: step 13962, loss 6.29613.
Train: 2018-08-02T15:08:48.847620: step 13963, loss 6.17021.
Train: 2018-08-02T15:08:48.894515: step 13964, loss 5.66652.
Train: 2018-08-02T15:08:48.956971: step 13965, loss 6.79982.
Train: 2018-08-02T15:08:49.019486: step 13966, loss 5.91836.
Train: 2018-08-02T15:08:49.066352: step 13967, loss 5.16283.
Train: 2018-08-02T15:08:49.128830: step 13968, loss 5.91836.
Train: 2018-08-02T15:08:49.191291: step 13969, loss 5.66652.
Train: 2018-08-02T15:08:49.238184: step 13970, loss 5.91836.
Test: 2018-08-02T15:08:49.441256: step 13970, loss 3.81863.
Train: 2018-08-02T15:08:49.503747: step 13971, loss 6.04429.
Train: 2018-08-02T15:08:49.566229: step 13972, loss 5.79244.
Train: 2018-08-02T15:08:49.628688: step 13973, loss 6.54798.
Train: 2018-08-02T15:08:49.675553: step 13974, loss 5.16283.
Train: 2018-08-02T15:08:49.738037: step 13975, loss 5.66652.
Train: 2018-08-02T15:08:49.800553: step 13976, loss 6.17021.
Train: 2018-08-02T15:08:49.863008: step 13977, loss 6.29613.
Train: 2018-08-02T15:08:49.909873: step 13978, loss 5.79244.
Train: 2018-08-02T15:08:49.972388: step 13979, loss 5.66652.
Train: 2018-08-02T15:08:50.019251: step 13980, loss 5.79244.
Test: 2018-08-02T15:08:50.222299: step 13980, loss 3.81863.
Train: 2018-08-02T15:08:50.284809: step 13981, loss 5.91836.
Train: 2018-08-02T15:08:50.347302: step 13982, loss 5.91836.
Train: 2018-08-02T15:08:50.409754: step 13983, loss 6.29613.
Train: 2018-08-02T15:08:50.456621: step 13984, loss 5.28875.
Train: 2018-08-02T15:08:50.519134: step 13985, loss 6.92574.
Train: 2018-08-02T15:08:50.581632: step 13986, loss 6.42205.
Train: 2018-08-02T15:08:50.628454: step 13987, loss 5.91836.
Train: 2018-08-02T15:08:50.690940: step 13988, loss 5.28875.
Train: 2018-08-02T15:08:50.753424: step 13989, loss 7.42943.
Train: 2018-08-02T15:08:50.800288: step 13990, loss 6.17021.
Test: 2018-08-02T15:08:51.003390: step 13990, loss 3.81863.
Train: 2018-08-02T15:08:51.065878: step 13991, loss 4.65914.
Train: 2018-08-02T15:08:51.128336: step 13992, loss 6.04429.
Train: 2018-08-02T15:08:51.175235: step 13993, loss 5.91836.
Train: 2018-08-02T15:08:51.237718: step 13994, loss 5.66652.
Train: 2018-08-02T15:08:51.300202: step 13995, loss 6.54798.
Train: 2018-08-02T15:08:51.362687: step 13996, loss 6.54798.
Train: 2018-08-02T15:08:51.409552: step 13997, loss 6.04429.
Train: 2018-08-02T15:08:51.472008: step 13998, loss 5.5406.
Train: 2018-08-02T15:08:51.534491: step 13999, loss 5.5406.
Train: 2018-08-02T15:08:51.581388: step 14000, loss 6.6739.
Test: 2018-08-02T15:08:51.784464: step 14000, loss 3.81863.
Train: 2018-08-02T15:08:52.299968: step 14001, loss 6.17021.
Train: 2018-08-02T15:08:52.362452: step 14002, loss 6.04429.
Train: 2018-08-02T15:08:52.424908: step 14003, loss 3.77768.
Train: 2018-08-02T15:08:52.471774: step 14004, loss 5.28875.
Train: 2018-08-02T15:08:52.534283: step 14005, loss 6.92574.
Train: 2018-08-02T15:08:52.596773: step 14006, loss 6.29613.
Train: 2018-08-02T15:08:52.643640: step 14007, loss 5.79244.
Train: 2018-08-02T15:08:52.706124: step 14008, loss 5.91836.
Train: 2018-08-02T15:08:52.768608: step 14009, loss 6.42205.
Train: 2018-08-02T15:08:52.815441: step 14010, loss 5.28875.
Test: 2018-08-02T15:08:53.018544: step 14010, loss 3.81863.
Train: 2018-08-02T15:08:53.081036: step 14011, loss 6.04429.
Train: 2018-08-02T15:08:53.143520: step 14012, loss 5.5406.
Train: 2018-08-02T15:08:53.190353: step 14013, loss 5.5406.
Train: 2018-08-02T15:08:53.252870: step 14014, loss 6.04429.
Train: 2018-08-02T15:08:53.315353: step 14015, loss 4.78506.
Train: 2018-08-02T15:08:53.362187: step 14016, loss 6.42205.
Train: 2018-08-02T15:08:53.424707: step 14017, loss 5.28875.
Train: 2018-08-02T15:08:53.487190: step 14018, loss 4.91098.
Train: 2018-08-02T15:08:53.534023: step 14019, loss 6.54798.
Train: 2018-08-02T15:08:53.596540: step 14020, loss 4.78506.
Test: 2018-08-02T15:08:53.799610: step 14020, loss 3.81863.
Train: 2018-08-02T15:08:53.862072: step 14021, loss 5.16283.
Train: 2018-08-02T15:08:53.924558: step 14022, loss 6.79982.
Train: 2018-08-02T15:08:53.971451: step 14023, loss 6.04429.
Train: 2018-08-02T15:08:54.033907: step 14024, loss 5.66652.
Train: 2018-08-02T15:08:54.096417: step 14025, loss 7.05167.
Train: 2018-08-02T15:08:54.143286: step 14026, loss 6.79982.
Train: 2018-08-02T15:08:54.205768: step 14027, loss 5.79244.
Train: 2018-08-02T15:08:54.268252: step 14028, loss 5.16283.
Train: 2018-08-02T15:08:54.315090: step 14029, loss 6.54798.
Train: 2018-08-02T15:08:54.377576: step 14030, loss 5.16283.
Test: 2018-08-02T15:08:54.580652: step 14030, loss 3.81863.
Train: 2018-08-02T15:08:54.643169: step 14031, loss 5.79244.
Train: 2018-08-02T15:08:54.690003: step 14032, loss 5.28875.
Train: 2018-08-02T15:08:54.752518: step 14033, loss 5.5406.
Train: 2018-08-02T15:08:54.814975: step 14034, loss 5.79244.
Train: 2018-08-02T15:08:54.877459: step 14035, loss 8.05905.
Train: 2018-08-02T15:08:54.924322: step 14036, loss 4.78506.
Train: 2018-08-02T15:08:54.986807: step 14037, loss 6.04429.
Train: 2018-08-02T15:08:55.049295: step 14038, loss 5.0369.
Train: 2018-08-02T15:08:55.096188: step 14039, loss 4.91098.
Train: 2018-08-02T15:08:55.158673: step 14040, loss 4.91098.
Test: 2018-08-02T15:08:55.361744: step 14040, loss 3.81863.
Train: 2018-08-02T15:08:55.424236: step 14041, loss 4.28137.
Train: 2018-08-02T15:08:55.486716: step 14042, loss 4.65914.
Train: 2018-08-02T15:08:55.517933: step 14043, loss 5.10406.
Train: 2018-08-02T15:08:55.580419: step 14044, loss 6.29613.
Train: 2018-08-02T15:08:55.642903: step 14045, loss 5.66652.
Train: 2018-08-02T15:08:55.705417: step 14046, loss 5.41467.
Train: 2018-08-02T15:08:55.752278: step 14047, loss 5.28875.
Train: 2018-08-02T15:08:55.814772: step 14048, loss 4.91098.
Train: 2018-08-02T15:08:55.877256: step 14049, loss 6.17021.
Train: 2018-08-02T15:08:55.939710: step 14050, loss 6.17021.
Test: 2018-08-02T15:08:56.142812: step 14050, loss 3.81863.
Train: 2018-08-02T15:08:56.205304: step 14051, loss 5.0369.
Train: 2018-08-02T15:08:56.252136: step 14052, loss 6.79982.
Train: 2018-08-02T15:08:56.314649: step 14053, loss 6.17021.
Train: 2018-08-02T15:08:56.377140: step 14054, loss 4.78506.
Train: 2018-08-02T15:08:56.439622: step 14055, loss 6.04429.
Train: 2018-08-02T15:08:56.486488: step 14056, loss 5.66652.
Train: 2018-08-02T15:08:56.548973: step 14057, loss 6.04429.
Train: 2018-08-02T15:08:56.611429: step 14058, loss 5.79244.
Train: 2018-08-02T15:08:56.658322: step 14059, loss 5.16283.
Train: 2018-08-02T15:08:56.720807: step 14060, loss 5.28875.
Test: 2018-08-02T15:08:56.908259: step 14060, loss 3.81863.
Train: 2018-08-02T15:08:56.970718: step 14061, loss 6.54798.
Train: 2018-08-02T15:08:57.033234: step 14062, loss 6.42205.
Train: 2018-08-02T15:08:57.095720: step 14063, loss 5.66652.
Train: 2018-08-02T15:08:57.158207: step 14064, loss 5.79244.
Train: 2018-08-02T15:08:57.205039: step 14065, loss 5.79244.
Train: 2018-08-02T15:08:57.267554: step 14066, loss 5.5406.
Train: 2018-08-02T15:08:57.330009: step 14067, loss 5.41467.
Train: 2018-08-02T15:08:57.376874: step 14068, loss 5.28875.
Train: 2018-08-02T15:08:57.439358: step 14069, loss 6.42205.
Train: 2018-08-02T15:08:57.501875: step 14070, loss 6.17021.
Test: 2018-08-02T15:08:57.689299: step 14070, loss 3.81863.
Train: 2018-08-02T15:08:57.751816: step 14071, loss 6.29613.
Train: 2018-08-02T15:08:57.814270: step 14072, loss 6.17021.
Train: 2018-08-02T15:08:57.876786: step 14073, loss 6.04429.
Train: 2018-08-02T15:08:57.939274: step 14074, loss 5.91836.
Train: 2018-08-02T15:08:57.986107: step 14075, loss 6.29613.
Train: 2018-08-02T15:08:58.048621: step 14076, loss 5.79244.
Train: 2018-08-02T15:08:58.111106: step 14077, loss 6.6739.
Train: 2018-08-02T15:08:58.173595: step 14078, loss 5.03771.
Train: 2018-08-02T15:08:58.220456: step 14079, loss 5.41467.
Train: 2018-08-02T15:08:58.282941: step 14080, loss 5.41467.
Test: 2018-08-02T15:08:58.485989: step 14080, loss 3.81863.
Train: 2018-08-02T15:08:58.548504: step 14081, loss 4.78506.
Train: 2018-08-02T15:08:58.595368: step 14082, loss 4.78506.
Train: 2018-08-02T15:08:58.657853: step 14083, loss 5.91836.
Train: 2018-08-02T15:08:58.720339: step 14084, loss 3.65176.
Train: 2018-08-02T15:08:58.782794: step 14085, loss 5.5406.
Train: 2018-08-02T15:08:58.829657: step 14086, loss 4.53321.
Train: 2018-08-02T15:08:58.892169: step 14087, loss 4.28137.
Train: 2018-08-02T15:08:58.954662: step 14088, loss 4.65914.
Train: 2018-08-02T15:08:59.017115: step 14089, loss 3.39991.
Train: 2018-08-02T15:08:59.064006: step 14090, loss 4.65914.
Test: 2018-08-02T15:08:59.267056: step 14090, loss 3.81863.
Train: 2018-08-02T15:08:59.329541: step 14091, loss 4.28137.
Train: 2018-08-02T15:08:59.376405: step 14092, loss 4.15545.
Train: 2018-08-02T15:08:59.438891: step 14093, loss 5.28875.
Train: 2018-08-02T15:08:59.501380: step 14094, loss 5.66652.
Train: 2018-08-02T15:08:59.563863: step 14095, loss 4.28137.
Train: 2018-08-02T15:08:59.626347: step 14096, loss 4.02952.
Train: 2018-08-02T15:08:59.673210: step 14097, loss 3.02214.
Train: 2018-08-02T15:08:59.735696: step 14098, loss 3.65176.
Train: 2018-08-02T15:08:59.798212: step 14099, loss 4.53321.
Train: 2018-08-02T15:08:59.845075: step 14100, loss 4.15545.
Test: 2018-08-02T15:09:00.048147: step 14100, loss 3.81863.
Train: 2018-08-02T15:09:00.594899: step 14101, loss 4.02952.
Train: 2018-08-02T15:09:00.641734: step 14102, loss 4.28137.
Train: 2018-08-02T15:09:00.704220: step 14103, loss 4.91098.
Train: 2018-08-02T15:09:00.766736: step 14104, loss 4.15545.
Train: 2018-08-02T15:09:00.829222: step 14105, loss 4.91098.
Train: 2018-08-02T15:09:00.876087: step 14106, loss 4.53321.
Train: 2018-08-02T15:09:00.938539: step 14107, loss 4.28137.
Train: 2018-08-02T15:09:01.001055: step 14108, loss 3.52583.
Train: 2018-08-02T15:09:01.063511: step 14109, loss 3.77768.
Train: 2018-08-02T15:09:01.110404: step 14110, loss 5.28875.
Test: 2018-08-02T15:09:01.313450: step 14110, loss 3.81863.
Train: 2018-08-02T15:09:01.375967: step 14111, loss 4.53321.
Train: 2018-08-02T15:09:01.438452: step 14112, loss 5.28875.
Train: 2018-08-02T15:09:01.500908: step 14113, loss 4.02952.
Train: 2018-08-02T15:09:01.547805: step 14114, loss 5.0369.
Train: 2018-08-02T15:09:01.610258: step 14115, loss 3.65176.
Train: 2018-08-02T15:09:01.672766: step 14116, loss 4.78506.
Train: 2018-08-02T15:09:01.719637: step 14117, loss 4.02952.
Train: 2018-08-02T15:09:01.782123: step 14118, loss 5.5406.
Train: 2018-08-02T15:09:01.844608: step 14119, loss 5.0369.
Train: 2018-08-02T15:09:01.907062: step 14120, loss 5.16283.
Test: 2018-08-02T15:09:02.094520: step 14120, loss 3.81863.
Train: 2018-08-02T15:09:02.157003: step 14121, loss 4.40729.
Train: 2018-08-02T15:09:02.203869: step 14122, loss 4.78506.
Train: 2018-08-02T15:09:02.266379: step 14123, loss 4.65914.
Train: 2018-08-02T15:09:02.328841: step 14124, loss 4.40956.
Train: 2018-08-02T15:09:02.375703: step 14125, loss 4.28137.
Train: 2018-08-02T15:09:02.438188: step 14126, loss 3.02214.
Train: 2018-08-02T15:09:02.500703: step 14127, loss 4.02952.
Train: 2018-08-02T15:09:02.547570: step 14128, loss 3.77768.
Train: 2018-08-02T15:09:02.610055: step 14129, loss 4.65914.
Train: 2018-08-02T15:09:02.672507: step 14130, loss 4.28137.
Test: 2018-08-02T15:09:02.875588: step 14130, loss 3.81863.
Train: 2018-08-02T15:09:02.922450: step 14131, loss 3.9036.
Train: 2018-08-02T15:09:02.984965: step 14132, loss 5.16283.
Train: 2018-08-02T15:09:03.047450: step 14133, loss 4.15545.
Train: 2018-08-02T15:09:03.109906: step 14134, loss 4.02952.
Train: 2018-08-02T15:09:03.172422: step 14135, loss 4.15545.
Train: 2018-08-02T15:09:03.219256: step 14136, loss 4.15545.
Train: 2018-08-02T15:09:03.281780: step 14137, loss 3.77768.
Train: 2018-08-02T15:09:03.344225: step 14138, loss 3.52583.
Train: 2018-08-02T15:09:03.391090: step 14139, loss 4.53321.
Train: 2018-08-02T15:09:03.453576: step 14140, loss 4.40729.
Test: 2018-08-02T15:09:03.656676: step 14140, loss 3.81863.
Train: 2018-08-02T15:09:03.719168: step 14141, loss 5.16283.
Train: 2018-08-02T15:09:03.766002: step 14142, loss 4.53321.
Train: 2018-08-02T15:09:03.828519: step 14143, loss 3.65176.
Train: 2018-08-02T15:09:03.891005: step 14144, loss 4.40729.
Train: 2018-08-02T15:09:03.953458: step 14145, loss 3.02214.
Train: 2018-08-02T15:09:04.000354: step 14146, loss 3.65176.
Train: 2018-08-02T15:09:04.062837: step 14147, loss 3.52583.
Train: 2018-08-02T15:09:04.125324: step 14148, loss 4.53321.
Train: 2018-08-02T15:09:04.172157: step 14149, loss 5.0369.
Train: 2018-08-02T15:09:04.234667: step 14150, loss 4.15545.
Test: 2018-08-02T15:09:04.437744: step 14150, loss 3.81863.
Train: 2018-08-02T15:09:04.500235: step 14151, loss 3.77768.
Train: 2018-08-02T15:09:04.547102: step 14152, loss 2.7703.
Train: 2018-08-02T15:09:04.609584: step 14153, loss 3.65176.
Train: 2018-08-02T15:09:04.672073: step 14154, loss 3.77768.
Train: 2018-08-02T15:09:04.734525: step 14155, loss 4.65914.
Train: 2018-08-02T15:09:04.781415: step 14156, loss 4.53321.
Train: 2018-08-02T15:09:04.843904: step 14157, loss 3.65176.
Train: 2018-08-02T15:09:04.906391: step 14158, loss 3.65176.
Train: 2018-08-02T15:09:04.953256: step 14159, loss 4.78506.
Train: 2018-08-02T15:09:05.015740: step 14160, loss 4.15545.
Test: 2018-08-02T15:09:05.218786: step 14160, loss 3.81863.
Train: 2018-08-02T15:09:05.281302: step 14161, loss 5.0369.
Train: 2018-08-02T15:09:05.343787: step 14162, loss 4.91098.
Train: 2018-08-02T15:09:05.390652: step 14163, loss 5.41467.
Train: 2018-08-02T15:09:05.453140: step 14164, loss 4.40729.
Train: 2018-08-02T15:09:05.515594: step 14165, loss 5.91836.
Train: 2018-08-02T15:09:05.562484: step 14166, loss 3.27399.
Train: 2018-08-02T15:09:05.624972: step 14167, loss 4.28137.
Train: 2018-08-02T15:09:05.687458: step 14168, loss 4.15545.
Train: 2018-08-02T15:09:05.734321: step 14169, loss 4.28137.
Train: 2018-08-02T15:09:05.796808: step 14170, loss 4.15545.
Test: 2018-08-02T15:09:05.999854: step 14170, loss 3.81863.
Train: 2018-08-02T15:09:06.062371: step 14171, loss 3.14807.
Train: 2018-08-02T15:09:06.124825: step 14172, loss 3.65176.
Train: 2018-08-02T15:09:06.171689: step 14173, loss 4.91098.
Train: 2018-08-02T15:09:06.234199: step 14174, loss 4.15545.
Train: 2018-08-02T15:09:06.296693: step 14175, loss 4.15545.
Train: 2018-08-02T15:09:06.359146: step 14176, loss 5.0369.
Train: 2018-08-02T15:09:06.406036: step 14177, loss 4.15545.
Train: 2018-08-02T15:09:06.468524: step 14178, loss 3.9036.
Train: 2018-08-02T15:09:06.530980: step 14179, loss 4.65914.
Train: 2018-08-02T15:09:06.577843: step 14180, loss 4.78506.
Test: 2018-08-02T15:09:06.780921: step 14180, loss 3.81863.
Train: 2018-08-02T15:09:06.843437: step 14181, loss 5.28875.
Train: 2018-08-02T15:09:06.905892: step 14182, loss 3.65176.
Train: 2018-08-02T15:09:06.952757: step 14183, loss 4.65914.
Train: 2018-08-02T15:09:07.015272: step 14184, loss 3.9036.
Train: 2018-08-02T15:09:07.077759: step 14185, loss 3.77768.
Train: 2018-08-02T15:09:07.124621: step 14186, loss 4.15545.
Train: 2018-08-02T15:09:07.187103: step 14187, loss 3.65176.
Train: 2018-08-02T15:09:07.249592: step 14188, loss 4.02952.
Train: 2018-08-02T15:09:07.296458: step 14189, loss 4.53321.
Train: 2018-08-02T15:09:07.358941: step 14190, loss 6.17021.
Test: 2018-08-02T15:09:07.562013: step 14190, loss 3.81863.
Train: 2018-08-02T15:09:07.624506: step 14191, loss 3.52583.
Train: 2018-08-02T15:09:07.671371: step 14192, loss 4.40729.
Train: 2018-08-02T15:09:07.733854: step 14193, loss 4.15545.
Train: 2018-08-02T15:09:07.780717: step 14194, loss 3.49225.
Train: 2018-08-02T15:09:07.843205: step 14195, loss 4.28137.
Train: 2018-08-02T15:09:07.905657: step 14196, loss 4.91098.
Train: 2018-08-02T15:09:07.952549: step 14197, loss 5.5406.
Train: 2018-08-02T15:09:08.015038: step 14198, loss 3.9036.
Train: 2018-08-02T15:09:08.077525: step 14199, loss 4.02952.
Train: 2018-08-02T15:09:08.124357: step 14200, loss 5.16283.
Test: 2018-08-02T15:09:08.327433: step 14200, loss 3.81863.
Train: 2018-08-02T15:09:08.858589: step 14201, loss 3.9036.
Train: 2018-08-02T15:09:08.905454: step 14202, loss 3.9036.
Train: 2018-08-02T15:09:08.967939: step 14203, loss 3.39991.
Train: 2018-08-02T15:09:09.030425: step 14204, loss 5.41467.
Train: 2018-08-02T15:09:09.077290: step 14205, loss 4.65914.
Train: 2018-08-02T15:09:09.139774: step 14206, loss 4.02952.
Train: 2018-08-02T15:09:09.202228: step 14207, loss 3.02214.
Train: 2018-08-02T15:09:09.264745: step 14208, loss 4.78506.
Train: 2018-08-02T15:09:09.311578: step 14209, loss 4.91098.
Train: 2018-08-02T15:09:09.374089: step 14210, loss 4.53321.
Test: 2018-08-02T15:09:09.561519: step 14210, loss 3.81863.
Train: 2018-08-02T15:09:09.624036: step 14211, loss 4.65914.
Train: 2018-08-02T15:09:09.686491: step 14212, loss 4.40729.
Train: 2018-08-02T15:09:09.748976: step 14213, loss 4.78506.
Train: 2018-08-02T15:09:09.811462: step 14214, loss 4.91098.
Train: 2018-08-02T15:09:09.858327: step 14215, loss 3.27399.
Train: 2018-08-02T15:09:09.920840: step 14216, loss 4.15545.
Train: 2018-08-02T15:09:09.983310: step 14217, loss 3.9036.
Train: 2018-08-02T15:09:10.030159: step 14218, loss 3.27399.
Train: 2018-08-02T15:09:10.092676: step 14219, loss 3.9036.
Train: 2018-08-02T15:09:10.155161: step 14220, loss 4.91098.
Test: 2018-08-02T15:09:10.342588: step 14220, loss 3.81863.
Train: 2018-08-02T15:09:10.405073: step 14221, loss 3.77768.
Train: 2018-08-02T15:09:10.467588: step 14222, loss 4.15545.
Train: 2018-08-02T15:09:10.530044: step 14223, loss 4.65914.
Train: 2018-08-02T15:09:10.576940: step 14224, loss 5.5406.
Train: 2018-08-02T15:09:10.639423: step 14225, loss 5.16283.
Train: 2018-08-02T15:09:10.701910: step 14226, loss 3.65176.
Train: 2018-08-02T15:09:10.748772: step 14227, loss 3.52583.
Train: 2018-08-02T15:09:10.811228: step 14228, loss 4.28137.
Train: 2018-08-02T15:09:10.873743: step 14229, loss 5.16283.
Train: 2018-08-02T15:09:10.920578: step 14230, loss 4.28137.
Test: 2018-08-02T15:09:11.123654: step 14230, loss 3.81863.
Train: 2018-08-02T15:09:11.186167: step 14231, loss 4.53321.
Train: 2018-08-02T15:09:11.233034: step 14232, loss 4.15545.
Train: 2018-08-02T15:09:11.295489: step 14233, loss 3.9036.
Train: 2018-08-02T15:09:11.358005: step 14234, loss 4.91098.
Train: 2018-08-02T15:09:11.420491: step 14235, loss 5.0369.
Train: 2018-08-02T15:09:11.467325: step 14236, loss 4.02952.
Train: 2018-08-02T15:09:11.529809: step 14237, loss 4.78506.
Train: 2018-08-02T15:09:11.592326: step 14238, loss 5.41467.
Train: 2018-08-02T15:09:11.639158: step 14239, loss 4.53321.
Train: 2018-08-02T15:09:11.701643: step 14240, loss 3.77768.
Test: 2018-08-02T15:09:11.904720: step 14240, loss 3.81863.
Train: 2018-08-02T15:09:11.967206: step 14241, loss 5.5406.
Train: 2018-08-02T15:09:12.014102: step 14242, loss 5.16283.
Train: 2018-08-02T15:09:12.076588: step 14243, loss 4.53321.
Train: 2018-08-02T15:09:12.139067: step 14244, loss 4.28137.
Train: 2018-08-02T15:09:12.201559: step 14245, loss 4.65914.
Train: 2018-08-02T15:09:12.248420: step 14246, loss 4.15545.
Train: 2018-08-02T15:09:12.310876: step 14247, loss 3.77768.
Train: 2018-08-02T15:09:12.373392: step 14248, loss 4.53321.
Train: 2018-08-02T15:09:12.420226: step 14249, loss 3.9036.
Train: 2018-08-02T15:09:12.482741: step 14250, loss 3.9036.
Test: 2018-08-02T15:09:12.685818: step 14250, loss 3.81863.
Train: 2018-08-02T15:09:12.748274: step 14251, loss 5.66652.
Train: 2018-08-02T15:09:12.810759: step 14252, loss 4.53321.
Train: 2018-08-02T15:09:12.857654: step 14253, loss 4.53321.
Train: 2018-08-02T15:09:12.920143: step 14254, loss 4.53321.
Train: 2018-08-02T15:09:12.982595: step 14255, loss 4.65914.
Train: 2018-08-02T15:09:13.045109: step 14256, loss 3.9036.
Train: 2018-08-02T15:09:13.091969: step 14257, loss 5.0369.
Train: 2018-08-02T15:09:13.154429: step 14258, loss 4.40729.
Train: 2018-08-02T15:09:13.216944: step 14259, loss 3.9036.
Train: 2018-08-02T15:09:13.263777: step 14260, loss 3.02214.
Test: 2018-08-02T15:09:13.466880: step 14260, loss 3.81863.
Train: 2018-08-02T15:09:13.529371: step 14261, loss 4.40729.
Train: 2018-08-02T15:09:13.591856: step 14262, loss 4.53321.
Train: 2018-08-02T15:09:13.638690: step 14263, loss 2.7703.
Train: 2018-08-02T15:09:13.701208: step 14264, loss 3.65176.
Train: 2018-08-02T15:09:13.763686: step 14265, loss 3.65176.
Train: 2018-08-02T15:09:13.826147: step 14266, loss 3.65176.
Train: 2018-08-02T15:09:13.873037: step 14267, loss 4.78506.
Train: 2018-08-02T15:09:13.935524: step 14268, loss 3.14807.
Train: 2018-08-02T15:09:13.997982: step 14269, loss 3.27399.
Train: 2018-08-02T15:09:14.044876: step 14270, loss 3.65176.
Test: 2018-08-02T15:09:14.247922: step 14270, loss 3.81863.
Train: 2018-08-02T15:09:14.310437: step 14271, loss 3.39991.
Train: 2018-08-02T15:09:14.372894: step 14272, loss 4.28137.
Train: 2018-08-02T15:09:14.435406: step 14273, loss 3.77768.
Train: 2018-08-02T15:09:14.497896: step 14274, loss 4.40729.
Train: 2018-08-02T15:09:14.544761: step 14275, loss 4.78506.
Train: 2018-08-02T15:09:14.607243: step 14276, loss 3.65176.
Train: 2018-08-02T15:09:14.669698: step 14277, loss 5.0369.
Train: 2018-08-02T15:09:14.716589: step 14278, loss 4.02952.
Train: 2018-08-02T15:09:14.779078: step 14279, loss 3.65176.
Train: 2018-08-02T15:09:14.841533: step 14280, loss 2.7703.
Test: 2018-08-02T15:09:15.044636: step 14280, loss 3.81863.
Train: 2018-08-02T15:09:15.091505: step 14281, loss 4.40729.
Train: 2018-08-02T15:09:15.153991: step 14282, loss 4.65914.
Train: 2018-08-02T15:09:15.216476: step 14283, loss 3.39991.
Train: 2018-08-02T15:09:15.278962: step 14284, loss 4.78506.
Train: 2018-08-02T15:09:15.325795: step 14285, loss 4.53321.
Train: 2018-08-02T15:09:15.388281: step 14286, loss 4.02952.
Train: 2018-08-02T15:09:15.450765: step 14287, loss 3.65176.
Train: 2018-08-02T15:09:15.497661: step 14288, loss 4.40729.
Train: 2018-08-02T15:09:15.560115: step 14289, loss 3.65176.
Train: 2018-08-02T15:09:15.622631: step 14290, loss 4.78506.
Test: 2018-08-02T15:09:15.825680: step 14290, loss 3.81863.
Train: 2018-08-02T15:09:15.872572: step 14291, loss 4.53321.
Train: 2018-08-02T15:09:15.935055: step 14292, loss 3.52583.
Train: 2018-08-02T15:09:15.997544: step 14293, loss 5.28875.
Train: 2018-08-02T15:09:16.059998: step 14294, loss 4.40729.
Train: 2018-08-02T15:09:16.106892: step 14295, loss 4.53321.
Train: 2018-08-02T15:09:16.169377: step 14296, loss 4.28137.
Train: 2018-08-02T15:09:16.231832: step 14297, loss 3.9036.
Train: 2018-08-02T15:09:16.278727: step 14298, loss 4.53321.
Train: 2018-08-02T15:09:16.341182: step 14299, loss 4.02952.
Train: 2018-08-02T15:09:16.403668: step 14300, loss 3.65176.
Test: 2018-08-02T15:09:16.606770: step 14300, loss 3.81863.
Train: 2018-08-02T15:09:17.106654: step 14301, loss 2.7703.
Train: 2018-08-02T15:09:17.169143: step 14302, loss 4.02952.
Train: 2018-08-02T15:09:17.231631: step 14303, loss 4.65914.
Train: 2018-08-02T15:09:17.278488: step 14304, loss 3.65176.
Train: 2018-08-02T15:09:17.340979: step 14305, loss 3.9036.
Train: 2018-08-02T15:09:17.403464: step 14306, loss 3.77768.
Train: 2018-08-02T15:09:17.450329: step 14307, loss 3.27399.
Train: 2018-08-02T15:09:17.512785: step 14308, loss 3.9036.
Train: 2018-08-02T15:09:17.575300: step 14309, loss 4.40729.
Train: 2018-08-02T15:09:17.622163: step 14310, loss 4.02952.
Test: 2018-08-02T15:09:17.825235: step 14310, loss 3.81863.
Train: 2018-08-02T15:09:17.887695: step 14311, loss 4.78506.
Train: 2018-08-02T15:09:17.950207: step 14312, loss 3.27399.
Train: 2018-08-02T15:09:18.012697: step 14313, loss 3.65176.
Train: 2018-08-02T15:09:18.075184: step 14314, loss 3.14807.
Train: 2018-08-02T15:09:18.122046: step 14315, loss 3.77768.
Train: 2018-08-02T15:09:18.184529: step 14316, loss 4.28137.
Train: 2018-08-02T15:09:18.247016: step 14317, loss 4.65914.
Train: 2018-08-02T15:09:18.293884: step 14318, loss 5.16283.
Train: 2018-08-02T15:09:18.356366: step 14319, loss 5.16283.
Train: 2018-08-02T15:09:18.418852: step 14320, loss 2.39253.
Test: 2018-08-02T15:09:18.621929: step 14320, loss 3.81863.
Train: 2018-08-02T15:09:18.684411: step 14321, loss 4.02952.
Train: 2018-08-02T15:09:18.746869: step 14322, loss 4.78506.
Train: 2018-08-02T15:09:18.793766: step 14323, loss 3.9036.
Train: 2018-08-02T15:09:18.856220: step 14324, loss 3.77768.
Train: 2018-08-02T15:09:18.918704: step 14325, loss 3.9036.
Train: 2018-08-02T15:09:18.981219: step 14326, loss 5.5406.
Train: 2018-08-02T15:09:19.028083: step 14327, loss 6.29613.
Train: 2018-08-02T15:09:19.090570: step 14328, loss 3.9036.
Train: 2018-08-02T15:09:19.153049: step 14329, loss 3.65176.
Train: 2018-08-02T15:09:19.199920: step 14330, loss 3.77768.
Test: 2018-08-02T15:09:19.402964: step 14330, loss 3.81863.
Train: 2018-08-02T15:09:19.465451: step 14331, loss 3.9036.
Train: 2018-08-02T15:09:19.527966: step 14332, loss 3.77768.
Train: 2018-08-02T15:09:19.590422: step 14333, loss 3.27399.
Train: 2018-08-02T15:09:19.637286: step 14334, loss 3.77768.
Train: 2018-08-02T15:09:19.699771: step 14335, loss 4.28137.
Train: 2018-08-02T15:09:19.762255: step 14336, loss 4.78506.
Train: 2018-08-02T15:09:19.809121: step 14337, loss 3.65176.
Train: 2018-08-02T15:09:19.871637: step 14338, loss 4.53321.
Train: 2018-08-02T15:09:19.934092: step 14339, loss 3.77768.
Train: 2018-08-02T15:09:19.980954: step 14340, loss 4.40729.
Test: 2018-08-02T15:09:20.184064: step 14340, loss 3.81863.
Train: 2018-08-02T15:09:20.246550: step 14341, loss 3.9036.
Train: 2018-08-02T15:09:20.309003: step 14342, loss 4.40729.
Train: 2018-08-02T15:09:20.371489: step 14343, loss 3.77768.
Train: 2018-08-02T15:09:20.418353: step 14344, loss 4.65914.
Train: 2018-08-02T15:09:20.465247: step 14345, loss 4.29816.
Train: 2018-08-02T15:09:20.527701: step 14346, loss 3.52583.
Train: 2018-08-02T15:09:20.590186: step 14347, loss 4.40729.
Train: 2018-08-02T15:09:20.637050: step 14348, loss 3.39991.
Train: 2018-08-02T15:09:20.699537: step 14349, loss 4.65914.
Train: 2018-08-02T15:09:20.762052: step 14350, loss 4.53321.
Test: 2018-08-02T15:09:20.949502: step 14350, loss 3.81863.
Train: 2018-08-02T15:09:21.011987: step 14351, loss 4.91098.
Train: 2018-08-02T15:09:21.074449: step 14352, loss 3.52583.
Train: 2018-08-02T15:09:21.136964: step 14353, loss 4.53321.
Train: 2018-08-02T15:09:21.199452: step 14354, loss 3.77768.
Train: 2018-08-02T15:09:21.246285: step 14355, loss 4.65914.
Train: 2018-08-02T15:09:21.308799: step 14356, loss 3.27399.
Train: 2018-08-02T15:09:21.371255: step 14357, loss 4.15545.
Train: 2018-08-02T15:09:21.418151: step 14358, loss 4.40729.
Train: 2018-08-02T15:09:21.480604: step 14359, loss 3.39991.
Train: 2018-08-02T15:09:21.543119: step 14360, loss 4.65914.
Test: 2018-08-02T15:09:21.746193: step 14360, loss 3.81863.
Train: 2018-08-02T15:09:21.808681: step 14361, loss 5.16283.
Train: 2018-08-02T15:09:21.855546: step 14362, loss 3.27399.
Train: 2018-08-02T15:09:21.918001: step 14363, loss 4.28137.
Train: 2018-08-02T15:09:21.980487: step 14364, loss 4.91098.
Train: 2018-08-02T15:09:22.042973: step 14365, loss 4.02952.
Train: 2018-08-02T15:09:22.089867: step 14366, loss 5.0369.
Train: 2018-08-02T15:09:22.152351: step 14367, loss 4.53321.
Train: 2018-08-02T15:09:22.214838: step 14368, loss 3.77768.
Train: 2018-08-02T15:09:22.261671: step 14369, loss 3.9036.
Train: 2018-08-02T15:09:22.324157: step 14370, loss 4.65914.
Test: 2018-08-02T15:09:22.511613: step 14370, loss 3.81863.
Train: 2018-08-02T15:09:22.574130: step 14371, loss 4.28137.
Train: 2018-08-02T15:09:22.636614: step 14372, loss 3.9036.
Train: 2018-08-02T15:09:22.699101: step 14373, loss 3.65176.
Train: 2018-08-02T15:09:22.761585: step 14374, loss 3.9036.
Train: 2018-08-02T15:09:22.808445: step 14375, loss 3.39991.
Train: 2018-08-02T15:09:22.870933: step 14376, loss 5.28875.
Train: 2018-08-02T15:09:22.933390: step 14377, loss 4.15545.
Train: 2018-08-02T15:09:22.995905: step 14378, loss 4.65914.
Train: 2018-08-02T15:09:23.042738: step 14379, loss 4.02952.
Train: 2018-08-02T15:09:23.105223: step 14380, loss 4.91098.
Test: 2018-08-02T15:09:23.308300: step 14380, loss 3.81863.
Train: 2018-08-02T15:09:23.370816: step 14381, loss 4.65914.
Train: 2018-08-02T15:09:23.433299: step 14382, loss 3.52583.
Train: 2018-08-02T15:09:23.495756: step 14383, loss 4.91098.
Train: 2018-08-02T15:09:23.542623: step 14384, loss 3.9036.
Train: 2018-08-02T15:09:23.605137: step 14385, loss 4.02952.
Train: 2018-08-02T15:09:23.667622: step 14386, loss 3.02214.
Train: 2018-08-02T15:09:23.714457: step 14387, loss 4.65914.
Train: 2018-08-02T15:09:23.776972: step 14388, loss 3.65176.
Train: 2018-08-02T15:09:23.839427: step 14389, loss 3.14807.
Train: 2018-08-02T15:09:23.886290: step 14390, loss 4.91098.
Test: 2018-08-02T15:09:24.089368: step 14390, loss 3.81863.
Train: 2018-08-02T15:09:24.151884: step 14391, loss 3.9036.
Train: 2018-08-02T15:09:24.214339: step 14392, loss 3.90902.
Train: 2018-08-02T15:09:24.261233: step 14393, loss 3.9036.
Train: 2018-08-02T15:09:24.323721: step 14394, loss 3.9036.
Train: 2018-08-02T15:09:24.386174: step 14395, loss 4.28137.
Train: 2018-08-02T15:09:24.433037: step 14396, loss 3.27399.
Train: 2018-08-02T15:09:24.495554: step 14397, loss 4.91098.
Train: 2018-08-02T15:09:24.558036: step 14398, loss 4.65914.
Train: 2018-08-02T15:09:24.620494: step 14399, loss 3.9036.
Train: 2018-08-02T15:09:24.667387: step 14400, loss 3.65176.
Test: 2018-08-02T15:09:24.870459: step 14400, loss 3.81863.
Train: 2018-08-02T15:09:25.370348: step 14401, loss 4.78506.
Train: 2018-08-02T15:09:25.417215: step 14402, loss 3.77768.
Train: 2018-08-02T15:09:25.479697: step 14403, loss 3.9036.
Train: 2018-08-02T15:09:25.542154: step 14404, loss 4.28137.
Train: 2018-08-02T15:09:25.589047: step 14405, loss 4.28137.
Train: 2018-08-02T15:09:25.651528: step 14406, loss 3.02214.
Train: 2018-08-02T15:09:25.714018: step 14407, loss 4.40729.
Train: 2018-08-02T15:09:25.760882: step 14408, loss 3.52583.
Train: 2018-08-02T15:09:25.823338: step 14409, loss 4.02952.
Train: 2018-08-02T15:09:25.885821: step 14410, loss 4.28137.
Test: 2018-08-02T15:09:26.073311: step 14410, loss 3.81863.
Train: 2018-08-02T15:09:26.135763: step 14411, loss 4.02952.
Train: 2018-08-02T15:09:26.198281: step 14412, loss 4.15545.
Train: 2018-08-02T15:09:26.260735: step 14413, loss 4.40729.
Train: 2018-08-02T15:09:26.307600: step 14414, loss 3.14807.
Train: 2018-08-02T15:09:26.370115: step 14415, loss 4.15545.
Train: 2018-08-02T15:09:26.432601: step 14416, loss 5.41467.
Train: 2018-08-02T15:09:26.479434: step 14417, loss 3.27399.
Train: 2018-08-02T15:09:26.541952: step 14418, loss 3.39991.
Train: 2018-08-02T15:09:26.604435: step 14419, loss 4.28137.
Train: 2018-08-02T15:09:26.651298: step 14420, loss 5.5406.
Test: 2018-08-02T15:09:26.854346: step 14420, loss 3.81863.
Train: 2018-08-02T15:09:26.916855: step 14421, loss 3.9036.
Train: 2018-08-02T15:09:26.963725: step 14422, loss 4.78506.
Train: 2018-08-02T15:09:27.026181: step 14423, loss 4.53321.
Train: 2018-08-02T15:09:27.088699: step 14424, loss 4.40729.
Train: 2018-08-02T15:09:27.151151: step 14425, loss 4.40729.
Train: 2018-08-02T15:09:27.198015: step 14426, loss 4.91098.
Train: 2018-08-02T15:09:27.260530: step 14427, loss 5.0369.
Train: 2018-08-02T15:09:27.323018: step 14428, loss 3.9036.
Train: 2018-08-02T15:09:27.369880: step 14429, loss 4.65914.
Train: 2018-08-02T15:09:27.432367: step 14430, loss 3.9036.
Test: 2018-08-02T15:09:27.635412: step 14430, loss 3.81863.
Train: 2018-08-02T15:09:27.697929: step 14431, loss 4.53321.
Train: 2018-08-02T15:09:27.760413: step 14432, loss 4.78506.
Train: 2018-08-02T15:09:27.807275: step 14433, loss 4.53321.
Train: 2018-08-02T15:09:27.869763: step 14434, loss 3.65176.
Train: 2018-08-02T15:09:27.932247: step 14435, loss 4.15545.
Train: 2018-08-02T15:09:27.994733: step 14436, loss 3.9036.
Train: 2018-08-02T15:09:28.041593: step 14437, loss 4.15545.
Train: 2018-08-02T15:09:28.104083: step 14438, loss 3.65176.
Train: 2018-08-02T15:09:28.166573: step 14439, loss 4.28137.
Train: 2018-08-02T15:09:28.213403: step 14440, loss 4.28137.
Test: 2018-08-02T15:09:28.416479: step 14440, loss 3.81863.
Train: 2018-08-02T15:09:28.478966: step 14441, loss 4.28137.
Train: 2018-08-02T15:09:28.525859: step 14442, loss 4.28137.
Train: 2018-08-02T15:09:28.588314: step 14443, loss 5.0369.
Train: 2018-08-02T15:09:28.650830: step 14444, loss 3.65176.
Train: 2018-08-02T15:09:28.713311: step 14445, loss 2.7703.
Train: 2018-08-02T15:09:28.760179: step 14446, loss 3.77768.
Train: 2018-08-02T15:09:28.822667: step 14447, loss 5.16283.
Train: 2018-08-02T15:09:28.885122: step 14448, loss 5.41467.
Train: 2018-08-02T15:09:28.947607: step 14449, loss 2.51845.
Train: 2018-08-02T15:09:28.994472: step 14450, loss 5.28875.
Test: 2018-08-02T15:09:29.197571: step 14450, loss 3.81863.
Train: 2018-08-02T15:09:29.260032: step 14451, loss 4.65914.
Train: 2018-08-02T15:09:29.322518: step 14452, loss 4.15545.
Train: 2018-08-02T15:09:29.385002: step 14453, loss 4.28137.
Train: 2018-08-02T15:09:29.447521: step 14454, loss 4.78506.
Train: 2018-08-02T15:09:29.494354: step 14455, loss 3.65176.
Train: 2018-08-02T15:09:29.556838: step 14456, loss 3.14807.
Train: 2018-08-02T15:09:29.619322: step 14457, loss 4.02952.
Train: 2018-08-02T15:09:29.666217: step 14458, loss 4.78506.
Train: 2018-08-02T15:09:29.728673: step 14459, loss 4.02952.
Train: 2018-08-02T15:09:29.791188: step 14460, loss 4.53321.
Test: 2018-08-02T15:09:29.994234: step 14460, loss 3.81863.
Train: 2018-08-02T15:09:30.056748: step 14461, loss 4.53321.
Train: 2018-08-02T15:09:30.166094: step 14462, loss 3.52583.
Train: 2018-08-02T15:09:30.228574: step 14463, loss 4.65914.
Train: 2018-08-02T15:09:30.275419: step 14464, loss 5.28875.
Train: 2018-08-02T15:09:30.337904: step 14465, loss 4.78506.
Train: 2018-08-02T15:09:30.400420: step 14466, loss 4.02952.
Train: 2018-08-02T15:09:30.447255: step 14467, loss 5.66652.
Train: 2018-08-02T15:09:30.509770: step 14468, loss 3.77768.
Train: 2018-08-02T15:09:30.572225: step 14469, loss 4.02952.
Train: 2018-08-02T15:09:30.619120: step 14470, loss 4.78506.
Test: 2018-08-02T15:09:30.822166: step 14470, loss 3.81863.
Train: 2018-08-02T15:09:30.884682: step 14471, loss 3.9036.
Train: 2018-08-02T15:09:30.947170: step 14472, loss 4.53321.
Train: 2018-08-02T15:09:31.009652: step 14473, loss 4.53321.
Train: 2018-08-02T15:09:31.056517: step 14474, loss 4.02952.
Train: 2018-08-02T15:09:31.119006: step 14475, loss 4.40729.
Train: 2018-08-02T15:09:31.181487: step 14476, loss 3.65176.
Train: 2018-08-02T15:09:31.228351: step 14477, loss 5.0369.
Train: 2018-08-02T15:09:31.290839: step 14478, loss 3.52583.
Train: 2018-08-02T15:09:31.353322: step 14479, loss 4.02952.
Train: 2018-08-02T15:09:31.400182: step 14480, loss 4.78506.
Test: 2018-08-02T15:09:31.603257: step 14480, loss 3.81863.
Train: 2018-08-02T15:09:31.665718: step 14481, loss 3.39991.
Train: 2018-08-02T15:09:31.712583: step 14482, loss 4.15545.
Train: 2018-08-02T15:09:31.775099: step 14483, loss 4.28137.
Train: 2018-08-02T15:09:31.837554: step 14484, loss 5.28875.
Train: 2018-08-02T15:09:31.900068: step 14485, loss 4.53321.
Train: 2018-08-02T15:09:31.946903: step 14486, loss 4.65914.
Train: 2018-08-02T15:09:32.009419: step 14487, loss 3.65176.
Train: 2018-08-02T15:09:32.071875: step 14488, loss 4.40729.
Train: 2018-08-02T15:09:32.118737: step 14489, loss 4.91098.
Train: 2018-08-02T15:09:32.181255: step 14490, loss 4.65914.
Test: 2018-08-02T15:09:32.384330: step 14490, loss 3.81863.
Train: 2018-08-02T15:09:32.446818: step 14491, loss 5.16283.
Train: 2018-08-02T15:09:32.509271: step 14492, loss 5.0369.
Train: 2018-08-02T15:09:32.556160: step 14493, loss 3.65176.
Train: 2018-08-02T15:09:32.618651: step 14494, loss 5.91836.
Train: 2018-08-02T15:09:32.681106: step 14495, loss 3.9036.
Train: 2018-08-02T15:09:32.727994: step 14496, loss 4.29816.
Train: 2018-08-02T15:09:32.774835: step 14497, loss 3.52583.
Train: 2018-08-02T15:09:32.837320: step 14498, loss 4.65914.
Train: 2018-08-02T15:09:32.899805: step 14499, loss 4.65914.
Train: 2018-08-02T15:09:32.962320: step 14500, loss 5.16283.
Test: 2018-08-02T15:09:33.149770: step 14500, loss 3.81863.
Train: 2018-08-02T15:09:33.665283: step 14501, loss 4.15545.
Train: 2018-08-02T15:09:33.727766: step 14502, loss 3.65176.
Train: 2018-08-02T15:09:33.790221: step 14503, loss 4.65914.
Train: 2018-08-02T15:09:33.852708: step 14504, loss 4.78506.
Train: 2018-08-02T15:09:33.915201: step 14505, loss 4.02952.
Train: 2018-08-02T15:09:33.962087: step 14506, loss 4.78506.
Train: 2018-08-02T15:09:34.024567: step 14507, loss 4.40729.
Train: 2018-08-02T15:09:34.087052: step 14508, loss 5.0369.
Train: 2018-08-02T15:09:34.133915: step 14509, loss 4.15545.
Train: 2018-08-02T15:09:34.196403: step 14510, loss 3.14807.
Test: 2018-08-02T15:09:34.383834: step 14510, loss 3.81863.
Train: 2018-08-02T15:09:34.446350: step 14511, loss 4.78506.
Train: 2018-08-02T15:09:34.508804: step 14512, loss 4.40729.
Train: 2018-08-02T15:09:34.571320: step 14513, loss 3.9036.
Train: 2018-08-02T15:09:34.633805: step 14514, loss 3.52583.
Train: 2018-08-02T15:09:34.680639: step 14515, loss 4.40729.
Train: 2018-08-02T15:09:34.743153: step 14516, loss 5.5406.
Train: 2018-08-02T15:09:34.805636: step 14517, loss 4.28137.
Train: 2018-08-02T15:09:34.868125: step 14518, loss 5.0369.
Train: 2018-08-02T15:09:34.914957: step 14519, loss 3.9036.
Train: 2018-08-02T15:09:34.977443: step 14520, loss 3.39991.
Test: 2018-08-02T15:09:35.164930: step 14520, loss 3.81863.
Train: 2018-08-02T15:09:35.227415: step 14521, loss 4.53321.
Train: 2018-08-02T15:09:35.289901: step 14522, loss 4.40729.
Train: 2018-08-02T15:09:35.352385: step 14523, loss 4.65914.
Train: 2018-08-02T15:09:35.414842: step 14524, loss 3.52583.
Train: 2018-08-02T15:09:35.461736: step 14525, loss 4.15545.
Train: 2018-08-02T15:09:35.524215: step 14526, loss 4.02952.
Train: 2018-08-02T15:09:35.586702: step 14527, loss 4.78506.
Train: 2018-08-02T15:09:35.649161: step 14528, loss 4.91098.
Train: 2018-08-02T15:09:35.696057: step 14529, loss 3.27399.
Train: 2018-08-02T15:09:35.758540: step 14530, loss 4.91098.
Test: 2018-08-02T15:09:35.945966: step 14530, loss 3.81863.
Train: 2018-08-02T15:09:36.008485: step 14531, loss 4.78506.
Train: 2018-08-02T15:09:36.070938: step 14532, loss 3.9036.
Train: 2018-08-02T15:09:36.133453: step 14533, loss 4.28137.
Train: 2018-08-02T15:09:36.180288: step 14534, loss 3.77768.
Train: 2018-08-02T15:09:36.242798: step 14535, loss 3.77768.
Train: 2018-08-02T15:09:36.305284: step 14536, loss 4.02952.
Train: 2018-08-02T15:09:36.352151: step 14537, loss 3.9036.
Train: 2018-08-02T15:09:36.414640: step 14538, loss 3.9036.
Train: 2018-08-02T15:09:36.477118: step 14539, loss 3.77768.
Train: 2018-08-02T15:09:36.523986: step 14540, loss 3.65176.
Test: 2018-08-02T15:09:36.727034: step 14540, loss 3.81863.
Train: 2018-08-02T15:09:36.789549: step 14541, loss 3.77768.
Train: 2018-08-02T15:09:36.852034: step 14542, loss 3.9036.
Train: 2018-08-02T15:09:36.914489: step 14543, loss 3.65717.
Train: 2018-08-02T15:09:36.976975: step 14544, loss 4.28137.
Train: 2018-08-02T15:09:37.023870: step 14545, loss 4.65914.
Train: 2018-08-02T15:09:37.086349: step 14546, loss 4.28137.
Train: 2018-08-02T15:09:37.148841: step 14547, loss 4.02952.
Train: 2018-08-02T15:09:37.195706: step 14548, loss 4.02952.
Train: 2018-08-02T15:09:37.258183: step 14549, loss 5.16283.
Train: 2018-08-02T15:09:37.320676: step 14550, loss 4.53321.
Test: 2018-08-02T15:09:37.508102: step 14550, loss 3.81863.
Train: 2018-08-02T15:09:37.570619: step 14551, loss 4.02952.
Train: 2018-08-02T15:09:37.633101: step 14552, loss 5.0369.
Train: 2018-08-02T15:09:37.695556: step 14553, loss 4.65914.
Train: 2018-08-02T15:09:37.758072: step 14554, loss 3.27399.
Train: 2018-08-02T15:09:37.804934: step 14555, loss 4.02952.
Train: 2018-08-02T15:09:37.867424: step 14556, loss 3.9036.
Train: 2018-08-02T15:09:37.929906: step 14557, loss 4.65914.
Train: 2018-08-02T15:09:37.976740: step 14558, loss 4.15545.
Train: 2018-08-02T15:09:38.039256: step 14559, loss 3.27399.
Train: 2018-08-02T15:09:38.101741: step 14560, loss 3.9036.
Test: 2018-08-02T15:09:38.289168: step 14560, loss 3.81863.
Train: 2018-08-02T15:09:38.351683: step 14561, loss 3.02214.
Train: 2018-08-02T15:09:38.414171: step 14562, loss 4.65914.
Train: 2018-08-02T15:09:38.476623: step 14563, loss 5.0369.
Train: 2018-08-02T15:09:38.539110: step 14564, loss 4.65914.
Train: 2018-08-02T15:09:38.601596: step 14565, loss 3.77768.
Train: 2018-08-02T15:09:38.648483: step 14566, loss 4.15545.
Train: 2018-08-02T15:09:38.710974: step 14567, loss 3.52583.
Train: 2018-08-02T15:09:38.773460: step 14568, loss 4.15545.
Train: 2018-08-02T15:09:38.820325: step 14569, loss 4.28137.
Train: 2018-08-02T15:09:38.882812: step 14570, loss 4.28137.
Test: 2018-08-02T15:09:39.085855: step 14570, loss 3.81863.
Train: 2018-08-02T15:09:39.148371: step 14571, loss 4.78506.
Train: 2018-08-02T15:09:39.195205: step 14572, loss 4.15545.
Train: 2018-08-02T15:09:39.257721: step 14573, loss 4.65914.
Train: 2018-08-02T15:09:39.320209: step 14574, loss 4.65914.
Train: 2018-08-02T15:09:39.382692: step 14575, loss 3.65176.
Train: 2018-08-02T15:09:39.429553: step 14576, loss 4.40729.
Train: 2018-08-02T15:09:39.492011: step 14577, loss 3.9036.
Train: 2018-08-02T15:09:39.554498: step 14578, loss 4.91098.
Train: 2018-08-02T15:09:39.601391: step 14579, loss 5.91836.
Train: 2018-08-02T15:09:39.663879: step 14580, loss 4.91098.
Test: 2018-08-02T15:09:39.866953: step 14580, loss 3.81863.
Train: 2018-08-02T15:09:39.929438: step 14581, loss 3.39991.
Train: 2018-08-02T15:09:39.991924: step 14582, loss 2.51845.
Train: 2018-08-02T15:09:40.038791: step 14583, loss 3.77768.
Train: 2018-08-02T15:09:40.101245: step 14584, loss 4.02952.
Train: 2018-08-02T15:09:40.163730: step 14585, loss 5.16283.
Train: 2018-08-02T15:09:40.226214: step 14586, loss 3.14807.
Train: 2018-08-02T15:09:40.273108: step 14587, loss 3.77768.
Train: 2018-08-02T15:09:40.335589: step 14588, loss 4.15545.
Train: 2018-08-02T15:09:40.398079: step 14589, loss 4.65914.
Train: 2018-08-02T15:09:40.444945: step 14590, loss 3.52583.
Test: 2018-08-02T15:09:40.647991: step 14590, loss 3.81863.
Train: 2018-08-02T15:09:40.710505: step 14591, loss 4.40729.
Train: 2018-08-02T15:09:40.772960: step 14592, loss 3.9036.
Train: 2018-08-02T15:09:40.819857: step 14593, loss 3.77768.
Train: 2018-08-02T15:09:40.882312: step 14594, loss 4.78506.
Train: 2018-08-02T15:09:40.944796: step 14595, loss 5.16283.
Train: 2018-08-02T15:09:41.007280: step 14596, loss 4.28137.
Train: 2018-08-02T15:09:41.054173: step 14597, loss 4.15545.
Train: 2018-08-02T15:09:41.116656: step 14598, loss 3.77768.
Train: 2018-08-02T15:09:41.179146: step 14599, loss 4.40729.
Train: 2018-08-02T15:09:41.226010: step 14600, loss 4.78506.
Test: 2018-08-02T15:09:41.429058: step 14600, loss 3.81863.
Train: 2018-08-02T15:09:41.928970: step 14601, loss 5.0369.
Train: 2018-08-02T15:09:41.991455: step 14602, loss 4.40729.
Train: 2018-08-02T15:09:42.053941: step 14603, loss 4.53321.
Train: 2018-08-02T15:09:42.100807: step 14604, loss 4.65914.
Train: 2018-08-02T15:09:42.163294: step 14605, loss 4.02952.
Train: 2018-08-02T15:09:42.225776: step 14606, loss 4.78506.
Train: 2018-08-02T15:09:42.288261: step 14607, loss 5.16283.
Train: 2018-08-02T15:09:42.335126: step 14608, loss 4.91098.
Train: 2018-08-02T15:09:42.397582: step 14609, loss 3.77768.
Train: 2018-08-02T15:09:42.460096: step 14610, loss 6.17021.
Test: 2018-08-02T15:09:42.663144: step 14610, loss 3.81863.
Train: 2018-08-02T15:09:42.710037: step 14611, loss 4.02952.
Train: 2018-08-02T15:09:42.772492: step 14612, loss 3.77768.
Train: 2018-08-02T15:09:42.835008: step 14613, loss 5.28875.
Train: 2018-08-02T15:09:42.897463: step 14614, loss 4.78506.
Train: 2018-08-02T15:09:42.944327: step 14615, loss 3.77768.
Train: 2018-08-02T15:09:43.006844: step 14616, loss 4.65914.
Train: 2018-08-02T15:09:43.069330: step 14617, loss 5.18661.
Train: 2018-08-02T15:09:43.116193: step 14618, loss 4.78506.
Train: 2018-08-02T15:09:43.178677: step 14619, loss 3.65176.
Train: 2018-08-02T15:09:43.241133: step 14620, loss 3.02214.
Test: 2018-08-02T15:09:43.444243: step 14620, loss 3.81863.
Train: 2018-08-02T15:09:43.506695: step 14621, loss 4.91098.
Train: 2018-08-02T15:09:43.553587: step 14622, loss 3.9036.
Train: 2018-08-02T15:09:43.616075: step 14623, loss 4.15545.
Train: 2018-08-02T15:09:43.678561: step 14624, loss 3.39991.
Train: 2018-08-02T15:09:43.741047: step 14625, loss 3.9036.
Train: 2018-08-02T15:09:43.787910: step 14626, loss 4.65914.
Train: 2018-08-02T15:09:43.850396: step 14627, loss 4.65914.
Train: 2018-08-02T15:09:43.912852: step 14628, loss 3.9036.
Train: 2018-08-02T15:09:43.959744: step 14629, loss 3.65176.
Train: 2018-08-02T15:09:44.022225: step 14630, loss 3.02214.
Test: 2018-08-02T15:09:44.225302: step 14630, loss 3.81863.
Train: 2018-08-02T15:09:44.272142: step 14631, loss 4.40729.
Train: 2018-08-02T15:09:44.334657: step 14632, loss 3.65176.
Train: 2018-08-02T15:09:44.397143: step 14633, loss 4.28137.
Train: 2018-08-02T15:09:44.459600: step 14634, loss 3.65176.
Train: 2018-08-02T15:09:44.522083: step 14635, loss 3.39991.
Train: 2018-08-02T15:09:44.568948: step 14636, loss 3.9036.
Train: 2018-08-02T15:09:44.631463: step 14637, loss 3.9036.
Train: 2018-08-02T15:09:44.693919: step 14638, loss 4.02952.
Train: 2018-08-02T15:09:44.740781: step 14639, loss 5.16283.
Train: 2018-08-02T15:09:44.803267: step 14640, loss 4.15545.
Test: 2018-08-02T15:09:45.006369: step 14640, loss 3.81863.
Train: 2018-08-02T15:09:45.068830: step 14641, loss 3.9036.
Train: 2018-08-02T15:09:45.115727: step 14642, loss 3.52583.
Train: 2018-08-02T15:09:45.178211: step 14643, loss 3.52583.
Train: 2018-08-02T15:09:45.240695: step 14644, loss 4.40729.
Train: 2018-08-02T15:09:45.303183: step 14645, loss 5.66652.
Train: 2018-08-02T15:09:45.350039: step 14646, loss 5.16283.
Train: 2018-08-02T15:09:45.396909: step 14647, loss 5.3727.
Train: 2018-08-02T15:09:45.459393: step 14648, loss 3.52583.
Train: 2018-08-02T15:09:45.521878: step 14649, loss 3.9036.
Train: 2018-08-02T15:09:45.568738: step 14650, loss 3.27399.
Test: 2018-08-02T15:09:45.771814: step 14650, loss 3.81863.
Train: 2018-08-02T15:09:45.834276: step 14651, loss 4.15545.
Train: 2018-08-02T15:09:45.896762: step 14652, loss 5.41467.
Train: 2018-08-02T15:09:45.943657: step 14653, loss 3.14807.
Train: 2018-08-02T15:09:46.006113: step 14654, loss 3.65176.
Train: 2018-08-02T15:09:46.068598: step 14655, loss 3.39991.
Train: 2018-08-02T15:09:46.131080: step 14656, loss 4.53321.
Train: 2018-08-02T15:09:46.177973: step 14657, loss 4.02952.
Train: 2018-08-02T15:09:46.240461: step 14658, loss 3.52583.
Train: 2018-08-02T15:09:46.302946: step 14659, loss 4.02952.
Train: 2018-08-02T15:09:46.349811: step 14660, loss 4.02952.
Test: 2018-08-02T15:09:46.552890: step 14660, loss 3.81863.
Train: 2018-08-02T15:09:46.615343: step 14661, loss 5.16283.
Train: 2018-08-02T15:09:46.677858: step 14662, loss 3.77768.
Train: 2018-08-02T15:09:46.724723: step 14663, loss 4.65914.
Train: 2018-08-02T15:09:46.787211: step 14664, loss 4.78506.
Train: 2018-08-02T15:09:46.849664: step 14665, loss 3.27399.
Train: 2018-08-02T15:09:46.896526: step 14666, loss 3.9036.
Train: 2018-08-02T15:09:46.959042: step 14667, loss 4.28137.
Train: 2018-08-02T15:09:47.021529: step 14668, loss 4.28137.
Train: 2018-08-02T15:09:47.068394: step 14669, loss 5.16283.
Train: 2018-08-02T15:09:47.130848: step 14670, loss 4.02952.
Test: 2018-08-02T15:09:47.333948: step 14670, loss 3.81863.
Train: 2018-08-02T15:09:47.396440: step 14671, loss 3.77768.
Train: 2018-08-02T15:09:47.458926: step 14672, loss 4.65914.
Train: 2018-08-02T15:09:47.505790: step 14673, loss 3.02214.
Train: 2018-08-02T15:09:47.568277: step 14674, loss 3.27399.
Train: 2018-08-02T15:09:47.630762: step 14675, loss 4.91098.
Train: 2018-08-02T15:09:47.677626: step 14676, loss 5.16283.
Train: 2018-08-02T15:09:47.740109: step 14677, loss 4.40729.
Train: 2018-08-02T15:09:47.802593: step 14678, loss 4.15545.
Train: 2018-08-02T15:09:47.849459: step 14679, loss 2.89622.
Train: 2018-08-02T15:09:47.911915: step 14680, loss 3.77768.
Test: 2018-08-02T15:09:48.114991: step 14680, loss 3.81863.
Train: 2018-08-02T15:09:48.177509: step 14681, loss 3.02214.
Train: 2018-08-02T15:09:48.239963: step 14682, loss 4.15545.
Train: 2018-08-02T15:09:48.286827: step 14683, loss 2.7703.
Train: 2018-08-02T15:09:48.349314: step 14684, loss 4.78506.
Train: 2018-08-02T15:09:48.411828: step 14685, loss 3.52583.
Train: 2018-08-02T15:09:48.474283: step 14686, loss 5.0369.
Train: 2018-08-02T15:09:48.521172: step 14687, loss 5.0369.
Train: 2018-08-02T15:09:48.583657: step 14688, loss 3.9036.
Train: 2018-08-02T15:09:48.646117: step 14689, loss 3.65176.
Train: 2018-08-02T15:09:48.693013: step 14690, loss 4.91098.
Test: 2018-08-02T15:09:48.896058: step 14690, loss 3.81863.
Train: 2018-08-02T15:09:48.958544: step 14691, loss 3.9036.
Train: 2018-08-02T15:09:49.021030: step 14692, loss 4.65914.
Train: 2018-08-02T15:09:49.067925: step 14693, loss 4.40729.
Train: 2018-08-02T15:09:49.130380: step 14694, loss 3.65176.
Train: 2018-08-02T15:09:49.192897: step 14695, loss 2.89622.
Train: 2018-08-02T15:09:49.239759: step 14696, loss 5.16283.
Train: 2018-08-02T15:09:49.302243: step 14697, loss 5.16283.
Train: 2018-08-02T15:09:49.364700: step 14698, loss 5.16283.
Train: 2018-08-02T15:09:49.411588: step 14699, loss 4.28137.
Train: 2018-08-02T15:09:49.474079: step 14700, loss 3.52583.
Test: 2018-08-02T15:09:49.661529: step 14700, loss 3.81863.
Train: 2018-08-02T15:09:50.177036: step 14701, loss 4.15545.
Train: 2018-08-02T15:09:50.239493: step 14702, loss 3.77768.
Train: 2018-08-02T15:09:50.302009: step 14703, loss 4.78506.
Train: 2018-08-02T15:09:50.348869: step 14704, loss 3.39991.
Train: 2018-08-02T15:09:50.411360: step 14705, loss 4.78506.
Train: 2018-08-02T15:09:50.473844: step 14706, loss 4.15545.
Train: 2018-08-02T15:09:50.520679: step 14707, loss 4.02952.
Train: 2018-08-02T15:09:50.583194: step 14708, loss 4.15545.
Train: 2018-08-02T15:09:50.645681: step 14709, loss 3.9036.
Train: 2018-08-02T15:09:50.708135: step 14710, loss 5.41467.
Test: 2018-08-02T15:09:50.895614: step 14710, loss 3.81863.
Train: 2018-08-02T15:09:50.958101: step 14711, loss 4.65914.
Train: 2018-08-02T15:09:51.020562: step 14712, loss 3.27399.
Train: 2018-08-02T15:09:51.083077: step 14713, loss 4.78506.
Train: 2018-08-02T15:09:51.129942: step 14714, loss 3.77768.
Train: 2018-08-02T15:09:51.192396: step 14715, loss 4.53321.
Train: 2018-08-02T15:09:51.254912: step 14716, loss 4.40729.
Train: 2018-08-02T15:09:51.317367: step 14717, loss 3.52583.
Train: 2018-08-02T15:09:51.364262: step 14718, loss 4.02952.
Train: 2018-08-02T15:09:51.426744: step 14719, loss 4.15545.
Train: 2018-08-02T15:09:51.489231: step 14720, loss 4.02952.
Test: 2018-08-02T15:09:51.692292: step 14720, loss 3.81863.
Train: 2018-08-02T15:09:51.739173: step 14721, loss 4.53321.
Train: 2018-08-02T15:09:51.801656: step 14722, loss 5.0369.
Train: 2018-08-02T15:09:51.864138: step 14723, loss 4.65914.
Train: 2018-08-02T15:09:51.926601: step 14724, loss 3.77768.
Train: 2018-08-02T15:09:51.989086: step 14725, loss 3.77768.
Train: 2018-08-02T15:09:52.035948: step 14726, loss 3.65176.
Train: 2018-08-02T15:09:52.098464: step 14727, loss 3.9036.
Train: 2018-08-02T15:09:52.160945: step 14728, loss 4.28137.
Train: 2018-08-02T15:09:52.207783: step 14729, loss 4.15545.
Train: 2018-08-02T15:09:52.270268: step 14730, loss 4.65914.
Test: 2018-08-02T15:09:52.473371: step 14730, loss 3.81863.
Train: 2018-08-02T15:09:52.535861: step 14731, loss 3.77768.
Train: 2018-08-02T15:09:52.598347: step 14732, loss 3.02214.
Train: 2018-08-02T15:09:52.645206: step 14733, loss 4.40729.
Train: 2018-08-02T15:09:52.707697: step 14734, loss 3.27399.
Train: 2018-08-02T15:09:52.770152: step 14735, loss 3.77768.
Train: 2018-08-02T15:09:52.817048: step 14736, loss 4.28137.
Train: 2018-08-02T15:09:52.879531: step 14737, loss 4.40729.
Train: 2018-08-02T15:09:52.941988: step 14738, loss 4.15545.
Train: 2018-08-02T15:09:52.988851: step 14739, loss 4.40729.
Train: 2018-08-02T15:09:53.051336: step 14740, loss 3.77768.
Test: 2018-08-02T15:09:53.254437: step 14740, loss 3.81863.
Train: 2018-08-02T15:09:53.316931: step 14741, loss 3.65176.
Train: 2018-08-02T15:09:53.363793: step 14742, loss 4.28137.
Train: 2018-08-02T15:09:53.426247: step 14743, loss 3.77768.
Train: 2018-08-02T15:09:53.488734: step 14744, loss 3.02214.
Train: 2018-08-02T15:09:53.551252: step 14745, loss 3.65176.
Train: 2018-08-02T15:09:53.598113: step 14746, loss 3.39991.
Train: 2018-08-02T15:09:53.660569: step 14747, loss 4.40729.
Train: 2018-08-02T15:09:53.723084: step 14748, loss 3.65176.
Train: 2018-08-02T15:09:53.769948: step 14749, loss 4.40729.
Train: 2018-08-02T15:09:53.832434: step 14750, loss 4.65914.
Test: 2018-08-02T15:09:54.035479: step 14750, loss 3.81863.
Train: 2018-08-02T15:09:54.082374: step 14751, loss 4.15545.
Train: 2018-08-02T15:09:54.144860: step 14752, loss 4.53321.
Train: 2018-08-02T15:09:54.207314: step 14753, loss 4.02952.
Train: 2018-08-02T15:09:54.269826: step 14754, loss 4.65914.
Train: 2018-08-02T15:09:54.316665: step 14755, loss 3.14807.
Train: 2018-08-02T15:09:54.379181: step 14756, loss 6.04429.
Train: 2018-08-02T15:09:54.441667: step 14757, loss 4.53321.
Train: 2018-08-02T15:09:54.488499: step 14758, loss 3.52583.
Train: 2018-08-02T15:09:54.551012: step 14759, loss 4.65914.
Train: 2018-08-02T15:09:54.613469: step 14760, loss 3.65176.
Test: 2018-08-02T15:09:54.816580: step 14760, loss 3.81863.
Train: 2018-08-02T15:09:54.879062: step 14761, loss 3.77768.
Train: 2018-08-02T15:09:54.925897: step 14762, loss 4.02952.
Train: 2018-08-02T15:09:54.988411: step 14763, loss 5.16283.
Train: 2018-08-02T15:09:55.050868: step 14764, loss 3.14807.
Train: 2018-08-02T15:09:55.113384: step 14765, loss 4.28137.
Train: 2018-08-02T15:09:55.160218: step 14766, loss 3.9036.
Train: 2018-08-02T15:09:55.222703: step 14767, loss 3.80354.
Train: 2018-08-02T15:09:55.285189: step 14768, loss 4.28137.
Train: 2018-08-02T15:09:55.347705: step 14769, loss 4.15545.
Train: 2018-08-02T15:09:55.394567: step 14770, loss 3.65176.
Test: 2018-08-02T15:09:55.597638: step 14770, loss 3.81863.
Train: 2018-08-02T15:09:55.644478: step 14771, loss 3.77768.
Train: 2018-08-02T15:09:55.706995: step 14772, loss 3.77768.
Train: 2018-08-02T15:09:55.769448: step 14773, loss 4.65914.
Train: 2018-08-02T15:09:55.831965: step 14774, loss 4.02952.
Train: 2018-08-02T15:09:55.878800: step 14775, loss 3.39991.
Train: 2018-08-02T15:09:55.941316: step 14776, loss 4.28137.
Train: 2018-08-02T15:09:56.003768: step 14777, loss 5.41467.
Train: 2018-08-02T15:09:56.050661: step 14778, loss 5.28875.
Train: 2018-08-02T15:09:56.113149: step 14779, loss 3.65176.
Train: 2018-08-02T15:09:56.175603: step 14780, loss 6.04429.
Test: 2018-08-02T15:09:56.363085: step 14780, loss 3.81863.
Train: 2018-08-02T15:09:56.425573: step 14781, loss 4.15545.
Train: 2018-08-02T15:09:56.488030: step 14782, loss 3.39991.
Train: 2018-08-02T15:09:56.550546: step 14783, loss 5.16283.
Train: 2018-08-02T15:09:56.613003: step 14784, loss 4.15545.
Train: 2018-08-02T15:09:56.659899: step 14785, loss 4.40729.
Train: 2018-08-02T15:09:56.722353: step 14786, loss 4.28137.
Train: 2018-08-02T15:09:56.784869: step 14787, loss 5.16283.
Train: 2018-08-02T15:09:56.847348: step 14788, loss 4.28137.
Train: 2018-08-02T15:09:56.894185: step 14789, loss 4.40729.
Train: 2018-08-02T15:09:56.956696: step 14790, loss 4.91098.
Test: 2018-08-02T15:09:57.159750: step 14790, loss 3.81863.
Train: 2018-08-02T15:09:57.222233: step 14791, loss 4.65914.
Train: 2018-08-02T15:09:57.269128: step 14792, loss 5.41467.
Train: 2018-08-02T15:09:57.331613: step 14793, loss 4.40729.
Train: 2018-08-02T15:09:57.394099: step 14794, loss 4.91098.
Train: 2018-08-02T15:09:57.456556: step 14795, loss 4.91098.
Train: 2018-08-02T15:09:57.503448: step 14796, loss 3.9036.
Train: 2018-08-02T15:09:57.565936: step 14797, loss 4.15545.
Train: 2018-08-02T15:09:57.612797: step 14798, loss 5.64133.
Train: 2018-08-02T15:09:57.675265: step 14799, loss 4.02952.
Train: 2018-08-02T15:09:57.722117: step 14800, loss 4.40729.
Test: 2018-08-02T15:09:57.925218: step 14800, loss 3.81863.
Train: 2018-08-02T15:09:58.440698: step 14801, loss 4.91098.
Train: 2018-08-02T15:09:58.503210: step 14802, loss 4.40729.
Train: 2018-08-02T15:09:58.565696: step 14803, loss 4.53321.
Train: 2018-08-02T15:09:58.628155: step 14804, loss 3.77768.
Train: 2018-08-02T15:09:58.675047: step 14805, loss 4.28137.
Train: 2018-08-02T15:09:58.737506: step 14806, loss 5.0369.
Train: 2018-08-02T15:09:58.800009: step 14807, loss 3.39991.
Train: 2018-08-02T15:09:58.846888: step 14808, loss 3.9036.
Train: 2018-08-02T15:09:58.909369: step 14809, loss 5.16283.
Train: 2018-08-02T15:09:58.971825: step 14810, loss 4.65914.
Test: 2018-08-02T15:09:59.174901: step 14810, loss 3.81863.
Train: 2018-08-02T15:09:59.221797: step 14811, loss 5.16283.
Train: 2018-08-02T15:09:59.284284: step 14812, loss 3.65176.
Train: 2018-08-02T15:09:59.346767: step 14813, loss 4.40729.
Train: 2018-08-02T15:09:59.409249: step 14814, loss 4.15545.
Train: 2018-08-02T15:09:59.471727: step 14815, loss 3.9036.
Train: 2018-08-02T15:09:59.518571: step 14816, loss 4.91098.
Train: 2018-08-02T15:09:59.581087: step 14817, loss 4.53321.
Train: 2018-08-02T15:09:59.643568: step 14818, loss 4.15545.
Train: 2018-08-02T15:09:59.690407: step 14819, loss 3.14807.
Train: 2018-08-02T15:09:59.752918: step 14820, loss 5.0369.
Test: 2018-08-02T15:09:59.940380: step 14820, loss 3.81863.
Train: 2018-08-02T15:10:00.002833: step 14821, loss 4.02952.
Train: 2018-08-02T15:10:00.065319: step 14822, loss 4.53321.
Train: 2018-08-02T15:10:00.127804: step 14823, loss 4.02952.
Train: 2018-08-02T15:10:00.174696: step 14824, loss 2.64438.
Train: 2018-08-02T15:10:00.237184: step 14825, loss 5.66652.
Train: 2018-08-02T15:10:00.299638: step 14826, loss 4.65914.
Train: 2018-08-02T15:10:00.346533: step 14827, loss 4.15545.
Train: 2018-08-02T15:10:00.409021: step 14828, loss 4.53321.
Train: 2018-08-02T15:10:00.471474: step 14829, loss 4.91098.
Train: 2018-08-02T15:10:00.518367: step 14830, loss 3.39991.
Test: 2018-08-02T15:10:00.721444: step 14830, loss 3.81863.
Train: 2018-08-02T15:10:00.783934: step 14831, loss 5.0369.
Train: 2018-08-02T15:10:00.846386: step 14832, loss 4.02952.
Train: 2018-08-02T15:10:00.908917: step 14833, loss 4.15545.
Train: 2018-08-02T15:10:00.955737: step 14834, loss 3.9036.
Train: 2018-08-02T15:10:01.018220: step 14835, loss 2.7703.
Train: 2018-08-02T15:10:01.080735: step 14836, loss 3.14807.
Train: 2018-08-02T15:10:01.127600: step 14837, loss 3.14807.
Train: 2018-08-02T15:10:01.190080: step 14838, loss 5.16283.
Train: 2018-08-02T15:10:01.252541: step 14839, loss 3.9036.
Train: 2018-08-02T15:10:01.299404: step 14840, loss 5.5406.
Test: 2018-08-02T15:10:01.502506: step 14840, loss 3.81863.
Train: 2018-08-02T15:10:01.564999: step 14841, loss 5.5406.
Train: 2018-08-02T15:10:01.627452: step 14842, loss 4.28137.
Train: 2018-08-02T15:10:01.689937: step 14843, loss 4.28137.
Train: 2018-08-02T15:10:01.736802: step 14844, loss 3.39991.
Train: 2018-08-02T15:10:01.799287: step 14845, loss 4.40729.
Train: 2018-08-02T15:10:01.861772: step 14846, loss 5.0369.
Train: 2018-08-02T15:10:01.908669: step 14847, loss 4.02952.
Train: 2018-08-02T15:10:01.971153: step 14848, loss 4.2869.
Train: 2018-08-02T15:10:02.033632: step 14849, loss 4.91098.
Train: 2018-08-02T15:10:02.080472: step 14850, loss 4.78506.
Test: 2018-08-02T15:10:02.283548: step 14850, loss 3.81863.
Train: 2018-08-02T15:10:02.346034: step 14851, loss 5.41467.
Train: 2018-08-02T15:10:02.408550: step 14852, loss 3.02214.
Train: 2018-08-02T15:10:02.471034: step 14853, loss 4.15545.
Train: 2018-08-02T15:10:02.517869: step 14854, loss 4.40729.
Train: 2018-08-02T15:10:02.580385: step 14855, loss 5.28875.
Train: 2018-08-02T15:10:02.642840: step 14856, loss 5.0369.
Train: 2018-08-02T15:10:02.689703: step 14857, loss 4.40729.
Train: 2018-08-02T15:10:02.752220: step 14858, loss 3.65176.
Train: 2018-08-02T15:10:02.814707: step 14859, loss 4.40729.
Train: 2018-08-02T15:10:02.861564: step 14860, loss 4.65914.
Test: 2018-08-02T15:10:03.064640: step 14860, loss 3.81863.
Train: 2018-08-02T15:10:03.111481: step 14861, loss 5.66652.
Train: 2018-08-02T15:10:03.174002: step 14862, loss 4.53321.
Train: 2018-08-02T15:10:03.236450: step 14863, loss 4.40729.
Train: 2018-08-02T15:10:03.298964: step 14864, loss 4.53321.
Train: 2018-08-02T15:10:03.361421: step 14865, loss 4.91098.
Train: 2018-08-02T15:10:03.408286: step 14866, loss 3.65176.
Train: 2018-08-02T15:10:03.470801: step 14867, loss 4.15545.
Train: 2018-08-02T15:10:03.533290: step 14868, loss 2.89622.
Train: 2018-08-02T15:10:03.580120: step 14869, loss 3.9036.
Train: 2018-08-02T15:10:03.642636: step 14870, loss 4.40729.
Test: 2018-08-02T15:10:03.845713: step 14870, loss 3.81863.
Train: 2018-08-02T15:10:03.908169: step 14871, loss 4.02952.
Train: 2018-08-02T15:10:03.970684: step 14872, loss 3.65176.
Train: 2018-08-02T15:10:04.017518: step 14873, loss 5.41467.
Train: 2018-08-02T15:10:04.080034: step 14874, loss 5.16283.
Train: 2018-08-02T15:10:04.142490: step 14875, loss 4.78506.
Train: 2018-08-02T15:10:04.205004: step 14876, loss 5.66652.
Train: 2018-08-02T15:10:04.251869: step 14877, loss 3.27399.
Train: 2018-08-02T15:10:04.314325: step 14878, loss 4.02952.
Train: 2018-08-02T15:10:04.376840: step 14879, loss 5.41467.
Train: 2018-08-02T15:10:04.423700: step 14880, loss 3.39991.
Test: 2018-08-02T15:10:04.626780: step 14880, loss 3.81863.
Train: 2018-08-02T15:10:04.689259: step 14881, loss 5.28875.
Train: 2018-08-02T15:10:04.751751: step 14882, loss 5.16283.
Train: 2018-08-02T15:10:04.798612: step 14883, loss 4.78506.
Train: 2018-08-02T15:10:04.861070: step 14884, loss 4.28137.
Train: 2018-08-02T15:10:04.923584: step 14885, loss 5.28875.
Train: 2018-08-02T15:10:04.970449: step 14886, loss 4.15545.
Train: 2018-08-02T15:10:05.032905: step 14887, loss 4.40729.
Train: 2018-08-02T15:10:05.095421: step 14888, loss 4.40729.
Train: 2018-08-02T15:10:05.142285: step 14889, loss 3.52583.
Train: 2018-08-02T15:10:05.204765: step 14890, loss 3.39991.
Test: 2018-08-02T15:10:05.407847: step 14890, loss 3.81863.
Train: 2018-08-02T15:10:05.470303: step 14891, loss 4.53321.
Train: 2018-08-02T15:10:05.517198: step 14892, loss 4.65914.
Train: 2018-08-02T15:10:05.579651: step 14893, loss 3.65176.
Train: 2018-08-02T15:10:05.642168: step 14894, loss 3.77768.
Train: 2018-08-02T15:10:05.704657: step 14895, loss 3.14807.
Train: 2018-08-02T15:10:05.751517: step 14896, loss 5.0369.
Train: 2018-08-02T15:10:05.814005: step 14897, loss 3.9036.
Train: 2018-08-02T15:10:05.876489: step 14898, loss 4.15545.
Train: 2018-08-02T15:10:05.923349: step 14899, loss 4.15545.
Train: 2018-08-02T15:10:05.985837: step 14900, loss 4.15545.
Test: 2018-08-02T15:10:06.188909: step 14900, loss 3.81863.
Train: 2018-08-02T15:10:06.735632: step 14901, loss 3.77768.
Train: 2018-08-02T15:10:06.782528: step 14902, loss 4.40729.
Train: 2018-08-02T15:10:06.844981: step 14903, loss 3.14807.
Train: 2018-08-02T15:10:06.907497: step 14904, loss 3.77768.
Train: 2018-08-02T15:10:06.969983: step 14905, loss 4.53321.
Train: 2018-08-02T15:10:07.016846: step 14906, loss 4.15545.
Train: 2018-08-02T15:10:07.079331: step 14907, loss 3.27399.
Train: 2018-08-02T15:10:07.141817: step 14908, loss 3.39991.
Train: 2018-08-02T15:10:07.204272: step 14909, loss 4.40729.
Train: 2018-08-02T15:10:07.251136: step 14910, loss 4.40729.
Test: 2018-08-02T15:10:07.454243: step 14910, loss 3.81863.
Train: 2018-08-02T15:10:07.516729: step 14911, loss 5.0369.
Train: 2018-08-02T15:10:07.579214: step 14912, loss 5.0369.
Train: 2018-08-02T15:10:07.626078: step 14913, loss 3.02214.
Train: 2018-08-02T15:10:07.688567: step 14914, loss 5.16283.
Train: 2018-08-02T15:10:07.751049: step 14915, loss 4.78506.
Train: 2018-08-02T15:10:07.813506: step 14916, loss 4.28137.
Train: 2018-08-02T15:10:07.860368: step 14917, loss 4.78506.
Train: 2018-08-02T15:10:07.922881: step 14918, loss 3.9036.
Train: 2018-08-02T15:10:07.969717: step 14919, loss 3.27399.
Train: 2018-08-02T15:10:08.032236: step 14920, loss 3.9036.
Test: 2018-08-02T15:10:08.235310: step 14920, loss 3.81863.
Train: 2018-08-02T15:10:08.297795: step 14921, loss 3.65176.
Train: 2018-08-02T15:10:08.360281: step 14922, loss 3.9036.
Train: 2018-08-02T15:10:08.422736: step 14923, loss 4.40729.
Train: 2018-08-02T15:10:08.469631: step 14924, loss 3.39991.
Train: 2018-08-02T15:10:08.532112: step 14925, loss 3.9036.
Train: 2018-08-02T15:10:08.594597: step 14926, loss 5.16283.
Train: 2018-08-02T15:10:08.641435: step 14927, loss 4.02952.
Train: 2018-08-02T15:10:08.703951: step 14928, loss 3.9036.
Train: 2018-08-02T15:10:08.766437: step 14929, loss 3.52583.
Train: 2018-08-02T15:10:08.813301: step 14930, loss 4.40729.
Test: 2018-08-02T15:10:09.016371: step 14930, loss 3.81863.
Train: 2018-08-02T15:10:09.078833: step 14931, loss 3.65176.
Train: 2018-08-02T15:10:09.141349: step 14932, loss 3.77768.
Train: 2018-08-02T15:10:09.203803: step 14933, loss 3.39991.
Train: 2018-08-02T15:10:09.266320: step 14934, loss 3.77768.
Train: 2018-08-02T15:10:09.313152: step 14935, loss 3.52583.
Train: 2018-08-02T15:10:09.375669: step 14936, loss 4.28137.
Train: 2018-08-02T15:10:09.438155: step 14937, loss 3.9036.
Train: 2018-08-02T15:10:09.500640: step 14938, loss 4.28137.
Train: 2018-08-02T15:10:09.547505: step 14939, loss 4.65914.
Train: 2018-08-02T15:10:09.609988: step 14940, loss 5.0369.
Test: 2018-08-02T15:10:09.813046: step 14940, loss 3.81863.
Train: 2018-08-02T15:10:09.875551: step 14941, loss 3.77768.
Train: 2018-08-02T15:10:09.922412: step 14942, loss 3.52583.
Train: 2018-08-02T15:10:09.984900: step 14943, loss 2.51845.
Train: 2018-08-02T15:10:10.047389: step 14944, loss 3.14807.
Train: 2018-08-02T15:10:10.109872: step 14945, loss 4.53321.
Train: 2018-08-02T15:10:10.156705: step 14946, loss 4.65914.
Train: 2018-08-02T15:10:10.219191: step 14947, loss 3.14811.
Train: 2018-08-02T15:10:10.281676: step 14948, loss 5.16283.
Train: 2018-08-02T15:10:10.312918: step 14949, loss 3.49225.
Train: 2018-08-02T15:10:10.375430: step 14950, loss 5.0369.
Test: 2018-08-02T15:10:10.578506: step 14950, loss 3.81863.
Train: 2018-08-02T15:10:10.625376: step 14951, loss 3.77768.
Train: 2018-08-02T15:10:10.687862: step 14952, loss 3.65176.
Train: 2018-08-02T15:10:10.750341: step 14953, loss 4.28137.
Train: 2018-08-02T15:10:10.812835: step 14954, loss 3.65176.
Train: 2018-08-02T15:10:10.875289: step 14955, loss 5.0369.
Train: 2018-08-02T15:10:10.922150: step 14956, loss 3.9036.
Train: 2018-08-02T15:10:10.984661: step 14957, loss 4.53321.
Train: 2018-08-02T15:10:11.047150: step 14958, loss 4.02952.
Train: 2018-08-02T15:10:11.094019: step 14959, loss 4.15545.
Train: 2018-08-02T15:10:11.156496: step 14960, loss 3.9036.
Test: 2018-08-02T15:10:11.359573: step 14960, loss 3.81863.
Train: 2018-08-02T15:10:11.422064: step 14961, loss 4.02952.
Train: 2018-08-02T15:10:11.468928: step 14962, loss 4.65914.
Train: 2018-08-02T15:10:11.531384: step 14963, loss 2.64438.
Train: 2018-08-02T15:10:11.593868: step 14964, loss 4.53321.
Train: 2018-08-02T15:10:11.656354: step 14965, loss 3.77768.
Train: 2018-08-02T15:10:11.703248: step 14966, loss 4.15545.
Train: 2018-08-02T15:10:11.765728: step 14967, loss 4.28137.
Train: 2018-08-02T15:10:11.828219: step 14968, loss 4.15545.
Train: 2018-08-02T15:10:11.875052: step 14969, loss 5.28875.
Train: 2018-08-02T15:10:11.937537: step 14970, loss 5.5406.
Test: 2018-08-02T15:10:12.140615: step 14970, loss 3.81863.
Train: 2018-08-02T15:10:12.187479: step 14971, loss 4.65914.
Train: 2018-08-02T15:10:12.249965: step 14972, loss 3.14807.
Train: 2018-08-02T15:10:12.312450: step 14973, loss 3.27399.
Train: 2018-08-02T15:10:12.374935: step 14974, loss 4.02952.
Train: 2018-08-02T15:10:12.421827: step 14975, loss 4.15545.
Train: 2018-08-02T15:10:12.484285: step 14976, loss 4.02952.
Train: 2018-08-02T15:10:12.546770: step 14977, loss 3.77768.
Train: 2018-08-02T15:10:12.593664: step 14978, loss 3.39991.
Train: 2018-08-02T15:10:12.656120: step 14979, loss 4.02952.
Train: 2018-08-02T15:10:12.718635: step 14980, loss 3.65176.
Test: 2018-08-02T15:10:12.921708: step 14980, loss 3.81863.
Train: 2018-08-02T15:10:12.968546: step 14981, loss 3.02214.
Train: 2018-08-02T15:10:13.031065: step 14982, loss 3.27399.
Train: 2018-08-02T15:10:13.093517: step 14983, loss 4.91098.
Train: 2018-08-02T15:10:13.156004: step 14984, loss 5.16283.
Train: 2018-08-02T15:10:13.202866: step 14985, loss 4.28137.
Train: 2018-08-02T15:10:13.265382: step 14986, loss 4.02952.
Train: 2018-08-02T15:10:13.327867: step 14987, loss 3.02214.
Train: 2018-08-02T15:10:13.390353: step 14988, loss 4.15545.
Train: 2018-08-02T15:10:13.437219: step 14989, loss 4.02952.
Train: 2018-08-02T15:10:13.499672: step 14990, loss 5.91836.
Test: 2018-08-02T15:10:13.702750: step 14990, loss 3.81863.
Train: 2018-08-02T15:10:13.749645: step 14991, loss 3.65176.
Train: 2018-08-02T15:10:13.812131: step 14992, loss 4.28137.
Train: 2018-08-02T15:10:13.874615: step 14993, loss 3.77768.
Train: 2018-08-02T15:10:13.937102: step 14994, loss 3.9036.
Train: 2018-08-02T15:10:13.983935: step 14995, loss 3.77768.
Train: 2018-08-02T15:10:14.046452: step 14996, loss 3.9036.
Train: 2018-08-02T15:10:14.093284: step 14997, loss 5.0369.
Train: 2018-08-02T15:10:14.155802: step 14998, loss 3.39991.
Train: 2018-08-02T15:10:14.218285: step 14999, loss 2.89622.
Train: 2018-08-02T15:10:14.265119: step 15000, loss 3.65176.
Test: 2018-08-02T15:10:14.468226: step 15000, loss 3.81863.
Train: 2018-08-02T15:10:14.968108: step 15001, loss 4.28137.
Train: 2018-08-02T15:10:15.030563: step 15002, loss 4.15545.
Train: 2018-08-02T15:10:15.093080: step 15003, loss 4.02952.
Train: 2018-08-02T15:10:15.139944: step 15004, loss 3.9036.
Train: 2018-08-02T15:10:15.198339: step 15005, loss 4.28137.
Train: 2018-08-02T15:10:15.260831: step 15006, loss 4.91098.
Train: 2018-08-02T15:10:15.307664: step 15007, loss 4.02952.
Train: 2018-08-02T15:10:15.370179: step 15008, loss 3.9036.
Train: 2018-08-02T15:10:15.432634: step 15009, loss 4.02952.
Train: 2018-08-02T15:10:15.479528: step 15010, loss 3.52583.
Test: 2018-08-02T15:10:15.682574: step 15010, loss 3.81863.
Train: 2018-08-02T15:10:15.745060: step 15011, loss 4.15545.
Train: 2018-08-02T15:10:15.807576: step 15012, loss 4.53321.
Train: 2018-08-02T15:10:15.870032: step 15013, loss 3.9036.
Train: 2018-08-02T15:10:15.932542: step 15014, loss 4.53321.
Train: 2018-08-02T15:10:15.979412: step 15015, loss 3.52583.
Train: 2018-08-02T15:10:16.041867: step 15016, loss 3.14807.
Train: 2018-08-02T15:10:16.088762: step 15017, loss 3.52583.
Train: 2018-08-02T15:10:16.151215: step 15018, loss 3.65176.
Train: 2018-08-02T15:10:16.213701: step 15019, loss 3.52583.
Train: 2018-08-02T15:10:16.276218: step 15020, loss 2.64438.
Test: 2018-08-02T15:10:16.463667: step 15020, loss 3.81863.
Train: 2018-08-02T15:10:16.526159: step 15021, loss 4.91098.
Train: 2018-08-02T15:10:16.588643: step 15022, loss 4.28137.
Train: 2018-08-02T15:10:16.651099: step 15023, loss 4.28137.
Train: 2018-08-02T15:10:16.713585: step 15024, loss 4.15545.
Train: 2018-08-02T15:10:16.776069: step 15025, loss 4.40729.
Train: 2018-08-02T15:10:16.822963: step 15026, loss 4.02952.
Train: 2018-08-02T15:10:16.885419: step 15027, loss 3.02214.
Train: 2018-08-02T15:10:16.932313: step 15028, loss 5.0369.
Train: 2018-08-02T15:10:16.994792: step 15029, loss 4.53321.
Train: 2018-08-02T15:10:17.057284: step 15030, loss 5.03691.
Test: 2018-08-02T15:10:17.260330: step 15030, loss 3.81863.
Train: 2018-08-02T15:10:17.322816: step 15031, loss 4.40729.
Train: 2018-08-02T15:10:17.369714: step 15032, loss 4.28137.
Train: 2018-08-02T15:10:17.432196: step 15033, loss 4.40729.
Train: 2018-08-02T15:10:17.494653: step 15034, loss 3.77768.
Train: 2018-08-02T15:10:17.557136: step 15035, loss 5.0369.
Train: 2018-08-02T15:10:17.604032: step 15036, loss 4.65914.
Train: 2018-08-02T15:10:17.666511: step 15037, loss 3.77768.
Train: 2018-08-02T15:10:17.729002: step 15038, loss 4.15545.
Train: 2018-08-02T15:10:17.775867: step 15039, loss 5.28875.
Train: 2018-08-02T15:10:17.838320: step 15040, loss 4.65914.
Test: 2018-08-02T15:10:18.025801: step 15040, loss 3.81863.
Train: 2018-08-02T15:10:18.088292: step 15041, loss 3.65176.
Train: 2018-08-02T15:10:18.150778: step 15042, loss 3.9036.
Train: 2018-08-02T15:10:18.213233: step 15043, loss 4.02952.
Train: 2018-08-02T15:10:18.260098: step 15044, loss 4.40729.
Train: 2018-08-02T15:10:18.322613: step 15045, loss 3.52583.
Train: 2018-08-02T15:10:18.385093: step 15046, loss 5.91836.
Train: 2018-08-02T15:10:18.431956: step 15047, loss 3.02214.
Train: 2018-08-02T15:10:18.494419: step 15048, loss 4.78506.
Train: 2018-08-02T15:10:18.556934: step 15049, loss 3.9036.
Train: 2018-08-02T15:10:18.603798: step 15050, loss 3.65176.
Test: 2018-08-02T15:10:18.806873: step 15050, loss 3.81863.
Train: 2018-08-02T15:10:18.869359: step 15051, loss 3.9036.
Train: 2018-08-02T15:10:18.931814: step 15052, loss 4.53321.
Train: 2018-08-02T15:10:18.994331: step 15053, loss 3.02214.
Train: 2018-08-02T15:10:19.041195: step 15054, loss 4.15545.
Train: 2018-08-02T15:10:19.103649: step 15055, loss 4.53321.
Train: 2018-08-02T15:10:19.166161: step 15056, loss 4.15545.
Train: 2018-08-02T15:10:19.228619: step 15057, loss 3.9036.
Train: 2018-08-02T15:10:19.275516: step 15058, loss 3.9036.
Train: 2018-08-02T15:10:19.338000: step 15059, loss 2.89622.
Train: 2018-08-02T15:10:19.400455: step 15060, loss 4.15545.
Test: 2018-08-02T15:10:19.603533: step 15060, loss 3.81863.
Train: 2018-08-02T15:10:19.650423: step 15061, loss 3.14807.
Train: 2018-08-02T15:10:19.712911: step 15062, loss 3.14807.
Train: 2018-08-02T15:10:19.775398: step 15063, loss 4.15545.
Train: 2018-08-02T15:10:19.837883: step 15064, loss 3.9036.
Train: 2018-08-02T15:10:19.884743: step 15065, loss 2.89622.
Train: 2018-08-02T15:10:19.947228: step 15066, loss 5.16283.
Train: 2018-08-02T15:10:20.009718: step 15067, loss 2.89622.
Train: 2018-08-02T15:10:20.072173: step 15068, loss 4.65914.
Train: 2018-08-02T15:10:20.119070: step 15069, loss 3.77768.
Train: 2018-08-02T15:10:20.181522: step 15070, loss 4.78506.
Test: 2018-08-02T15:10:20.384600: step 15070, loss 3.81863.
Train: 2018-08-02T15:10:20.431493: step 15071, loss 5.16283.
Train: 2018-08-02T15:10:20.493949: step 15072, loss 4.40729.
Train: 2018-08-02T15:10:20.556434: step 15073, loss 3.52583.
Train: 2018-08-02T15:10:20.618950: step 15074, loss 3.77768.
Train: 2018-08-02T15:10:20.665814: step 15075, loss 3.14807.
Train: 2018-08-02T15:10:20.728298: step 15076, loss 4.28137.
Train: 2018-08-02T15:10:20.790784: step 15077, loss 4.65914.
Train: 2018-08-02T15:10:20.853239: step 15078, loss 4.02952.
Train: 2018-08-02T15:10:20.915725: step 15079, loss 4.40729.
Train: 2018-08-02T15:10:20.962589: step 15080, loss 3.14807.
Test: 2018-08-02T15:10:21.165697: step 15080, loss 3.81863.
Train: 2018-08-02T15:10:21.228152: step 15081, loss 4.40729.
Train: 2018-08-02T15:10:21.290667: step 15082, loss 4.02952.
Train: 2018-08-02T15:10:21.353123: step 15083, loss 3.27399.
Train: 2018-08-02T15:10:21.415639: step 15084, loss 3.39991.
Train: 2018-08-02T15:10:21.462502: step 15085, loss 4.28137.
Train: 2018-08-02T15:10:21.524989: step 15086, loss 2.89622.
Train: 2018-08-02T15:10:21.587472: step 15087, loss 4.78506.
Train: 2018-08-02T15:10:21.634316: step 15088, loss 4.65914.
Train: 2018-08-02T15:10:21.696822: step 15089, loss 4.28137.
Train: 2018-08-02T15:10:21.759278: step 15090, loss 3.52583.
Test: 2018-08-02T15:10:21.962356: step 15090, loss 3.81863.
Train: 2018-08-02T15:10:22.024871: step 15091, loss 4.40729.
Train: 2018-08-02T15:10:22.087326: step 15092, loss 3.77768.
Train: 2018-08-02T15:10:22.134189: step 15093, loss 4.65914.
Train: 2018-08-02T15:10:22.196677: step 15094, loss 2.51845.
Train: 2018-08-02T15:10:22.259162: step 15095, loss 4.65914.
Train: 2018-08-02T15:10:22.306054: step 15096, loss 4.78506.
Train: 2018-08-02T15:10:22.368510: step 15097, loss 4.91098.
Train: 2018-08-02T15:10:22.431026: step 15098, loss 4.78506.
Train: 2018-08-02T15:10:22.493514: step 15099, loss 5.0369.
Train: 2018-08-02T15:10:22.524755: step 15100, loss 5.10406.
Test: 2018-08-02T15:10:22.727802: step 15100, loss 3.81863.
