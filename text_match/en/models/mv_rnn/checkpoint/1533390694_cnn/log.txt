Train: 2018-08-04T21:51:48.236185: step 1, loss 0.68249.
Train: 2018-08-04T21:51:51.939701: step 2, loss 4.12173.
Train: 2018-08-04T21:51:55.565087: step 3, loss 2.00944.
Train: 2018-08-04T21:51:59.143586: step 4, loss 0.642802.
Train: 2018-08-04T21:52:02.722089: step 5, loss 0.579054.
Train: 2018-08-04T21:52:06.316218: step 6, loss 0.618964.
Train: 2018-08-04T21:52:09.894722: step 7, loss 0.564623.
Train: 2018-08-04T21:52:13.473224: step 8, loss 0.649809.
Train: 2018-08-04T21:52:17.036100: step 9, loss 0.559159.
Train: 2018-08-04T21:52:20.614603: step 10, loss 0.550607.
Test: 2018-08-04T21:52:35.725574: step 10, loss 3.81632.
Train: 2018-08-04T21:52:39.272822: step 11, loss 0.718131.
Train: 2018-08-04T21:52:42.851325: step 12, loss 0.597826.
Train: 2018-08-04T21:52:46.414201: step 13, loss 0.619321.
Train: 2018-08-04T21:52:50.008330: step 14, loss 0.576849.
Train: 2018-08-04T21:52:53.602460: step 15, loss 0.612248.
Train: 2018-08-04T21:52:57.165336: step 16, loss 0.622214.
Train: 2018-08-04T21:53:00.759465: step 17, loss 0.627818.
Train: 2018-08-04T21:53:04.337968: step 18, loss 0.671966.
Train: 2018-08-04T21:53:07.916471: step 19, loss 0.468826.
Train: 2018-08-04T21:53:11.510600: step 20, loss 0.544745.
Test: 2018-08-04T21:53:26.480931: step 20, loss 3.80183.
Train: 2018-08-04T21:53:29.981300: step 21, loss 0.685856.
Train: 2018-08-04T21:53:33.481669: step 22, loss 0.538943.
Train: 2018-08-04T21:53:36.997665: step 23, loss 0.590992.
Train: 2018-08-04T21:53:40.529288: step 24, loss 0.629255.
Train: 2018-08-04T21:53:44.029658: step 25, loss 0.651023.
Train: 2018-08-04T21:53:47.545654: step 26, loss 0.599045.
Train: 2018-08-04T21:53:51.046024: step 27, loss 0.609078.
Train: 2018-08-04T21:53:54.562019: step 28, loss 0.560192.
Train: 2018-08-04T21:53:58.062389: step 29, loss 0.545925.
Train: 2018-08-04T21:54:01.562758: step 30, loss 0.66102.
Test: 2018-08-04T21:54:16.314316: step 30, loss 3.79604.
Train: 2018-08-04T21:54:19.877191: step 31, loss 0.587253.
Train: 2018-08-04T21:54:23.408814: step 32, loss 0.618556.
Train: 2018-08-04T21:54:26.924810: step 33, loss 0.53822.
Train: 2018-08-04T21:54:30.456433: step 34, loss 0.639875.
Train: 2018-08-04T21:54:33.941176: step 35, loss 0.622725.
Train: 2018-08-04T21:54:37.457172: step 36, loss 0.56438.
Train: 2018-08-04T21:54:40.988795: step 37, loss 0.603471.
Train: 2018-08-04T21:54:44.520418: step 38, loss 0.498358.
Train: 2018-08-04T21:54:48.036414: step 39, loss 0.570524.
Train: 2018-08-04T21:54:51.536784: step 40, loss 0.665756.
Test: 2018-08-04T21:55:06.303967: step 40, loss 3.84241.
Train: 2018-08-04T21:55:09.804337: step 41, loss 0.503797.
Train: 2018-08-04T21:55:13.289080: step 42, loss 0.576725.
Train: 2018-08-04T21:55:16.820703: step 43, loss 0.577932.
Train: 2018-08-04T21:55:20.367952: step 44, loss 0.57431.
Train: 2018-08-04T21:55:23.883948: step 45, loss 0.536638.
Train: 2018-08-04T21:55:27.384318: step 46, loss 0.576328.
Train: 2018-08-04T21:55:30.900314: step 47, loss 0.486038.
Train: 2018-08-04T21:55:34.385056: step 48, loss 0.522851.
Train: 2018-08-04T21:55:37.869799: step 49, loss 0.744663.
Train: 2018-08-04T21:55:41.417049: step 50, loss 0.589125.
Test: 2018-08-04T21:55:56.215486: step 50, loss 3.82212.
Train: 2018-08-04T21:55:59.731482: step 51, loss 0.536464.
Train: 2018-08-04T21:56:03.231852: step 52, loss 0.56099.
Train: 2018-08-04T21:56:06.732221: step 53, loss 0.547759.
Train: 2018-08-04T21:56:10.232591: step 54, loss 0.574058.
Train: 2018-08-04T21:56:13.748587: step 55, loss 0.635327.
Train: 2018-08-04T21:56:17.280209: step 56, loss 0.504458.
Train: 2018-08-04T21:56:20.780579: step 57, loss 0.608113.
Train: 2018-08-04T21:56:24.296575: step 58, loss 0.51426.
Train: 2018-08-04T21:56:27.796945: step 59, loss 0.583883.
Train: 2018-08-04T21:56:31.297314: step 60, loss 0.501263.
Test: 2018-08-04T21:56:46.173884: step 60, loss 3.82502.
Train: 2018-08-04T21:56:49.705507: step 61, loss 0.618447.
Train: 2018-08-04T21:56:53.252757: step 62, loss 0.601698.
Train: 2018-08-04T21:56:56.768753: step 63, loss 0.558322.
Train: 2018-08-04T21:57:00.300376: step 64, loss 0.544009.
Train: 2018-08-04T21:57:03.816372: step 65, loss 0.573205.
Train: 2018-08-04T21:57:07.301114: step 66, loss 0.611837.
Train: 2018-08-04T21:57:10.832737: step 67, loss 0.555165.
Train: 2018-08-04T21:57:14.364360: step 68, loss 0.637847.
Train: 2018-08-04T21:57:17.880356: step 69, loss 0.575757.
Train: 2018-08-04T21:57:21.380726: step 70, loss 0.515252.
Test: 2018-08-04T21:57:36.194790: step 70, loss 0.81627.
Train: 2018-08-04T21:57:39.710786: step 71, loss 0.637419.
Train: 2018-08-04T21:57:43.367422: step 72, loss 0.552533.
Train: 2018-08-04T21:57:46.883418: step 73, loss 0.571879.
Train: 2018-08-04T21:57:50.415041: step 74, loss 0.609537.
Train: 2018-08-04T21:57:53.931037: step 75, loss 0.644602.
Train: 2018-08-04T21:57:57.431406: step 76, loss 0.632659.
Train: 2018-08-04T21:58:00.947402: step 77, loss 0.604536.
Train: 2018-08-04T21:58:04.463398: step 78, loss 0.488782.
Train: 2018-08-04T21:58:08.026275: step 79, loss 0.547821.
Train: 2018-08-04T21:58:11.542271: step 80, loss 0.517351.
Test: 2018-08-04T21:58:26.371961: step 80, loss 3.83951.
Train: 2018-08-04T21:58:29.872331: step 81, loss 0.664897.
Train: 2018-08-04T21:58:33.403953: step 82, loss 0.643304.
Train: 2018-08-04T21:58:36.919950: step 83, loss 0.584775.
Train: 2018-08-04T21:58:40.435946: step 84, loss 0.571722.
Train: 2018-08-04T21:58:43.936315: step 85, loss 0.584186.
Train: 2018-08-04T21:58:47.514818: step 86, loss 0.572016.
Train: 2018-08-04T21:58:51.062067: step 87, loss 0.608659.
Train: 2018-08-04T21:58:54.546810: step 88, loss 0.606184.
Train: 2018-08-04T21:58:58.047180: step 89, loss 0.570048.
Train: 2018-08-04T21:59:01.563176: step 90, loss 0.580752.
Test: 2018-08-04T21:59:16.533507: step 90, loss 1.78897.
Train: 2018-08-04T21:59:20.065129: step 91, loss 0.612208.
Train: 2018-08-04T21:59:23.612378: step 92, loss 0.521619.
Train: 2018-08-04T21:59:27.144001: step 93, loss 0.616271.
Train: 2018-08-04T21:59:30.644371: step 94, loss 0.68884.
Train: 2018-08-04T21:59:34.160367: step 95, loss 0.522516.
Train: 2018-08-04T21:59:37.676363: step 96, loss 0.560055.
Train: 2018-08-04T21:59:41.192359: step 97, loss 0.553862.
Train: 2018-08-04T21:59:44.739608: step 98, loss 0.531783.
Train: 2018-08-04T21:59:48.271231: step 99, loss 0.573821.
Train: 2018-08-04T21:59:51.771601: step 100, loss 0.547438.
Test: 2018-08-04T22:00:06.663797: step 100, loss 0.569979.
Train: 2018-08-04T22:00:12.430032: step 101, loss 0.595767.
Train: 2018-08-04T22:00:15.930401: step 102, loss 0.475099.
Train: 2018-08-04T22:00:19.415144: step 103, loss 0.541188.
Train: 2018-08-04T22:00:22.962393: step 104, loss 0.601673.
Train: 2018-08-04T22:00:26.494016: step 105, loss 0.53743.
Train: 2018-08-04T22:00:29.994386: step 106, loss 0.571191.
Train: 2018-08-04T22:00:33.510382: step 107, loss 0.570637.
Train: 2018-08-04T22:00:37.026378: step 108, loss 0.541548.
Train: 2018-08-04T22:00:40.558001: step 109, loss 0.624758.
Train: 2018-08-04T22:00:44.105250: step 110, loss 0.592556.
Test: 2018-08-04T22:00:58.966193: step 110, loss 0.609111.
Train: 2018-08-04T22:01:02.482190: step 111, loss 0.586986.
Train: 2018-08-04T22:01:05.998186: step 112, loss 0.601612.
Train: 2018-08-04T22:01:09.498556: step 113, loss 0.543196.
Train: 2018-08-04T22:01:13.014552: step 114, loss 0.63877.
Train: 2018-08-04T22:01:16.499295: step 115, loss 0.589569.
Train: 2018-08-04T22:01:20.046544: step 116, loss 0.570913.
Train: 2018-08-04T22:01:23.578167: step 117, loss 0.62887.
Train: 2018-08-04T22:01:27.094163: step 118, loss 0.58729.
Train: 2018-08-04T22:01:30.594533: step 119, loss 0.640356.
Train: 2018-08-04T22:01:34.094902: step 120, loss 0.544203.
Test: 2018-08-04T22:01:48.908966: step 120, loss 0.559451.
Train: 2018-08-04T22:01:52.440589: step 121, loss 0.575001.
Train: 2018-08-04T22:01:56.034718: step 122, loss 0.626834.
Train: 2018-08-04T22:01:59.581967: step 123, loss 0.528055.
Train: 2018-08-04T22:02:03.066710: step 124, loss 0.581757.
Train: 2018-08-04T22:02:06.582707: step 125, loss 0.554841.
Train: 2018-08-04T22:02:10.067449: step 126, loss 0.523961.
Train: 2018-08-04T22:02:13.583445: step 127, loss 0.614037.
Train: 2018-08-04T22:02:17.130695: step 128, loss 0.511177.
Train: 2018-08-04T22:02:20.646691: step 129, loss 0.492632.
Train: 2018-08-04T22:02:24.162687: step 130, loss 0.596708.
Test: 2018-08-04T22:02:39.008005: step 130, loss 0.552758.
Train: 2018-08-04T22:02:42.508373: step 131, loss 0.512675.
Train: 2018-08-04T22:02:46.024370: step 132, loss 0.646358.
Train: 2018-08-04T22:02:49.524739: step 133, loss 0.581794.
Train: 2018-08-04T22:02:53.040735: step 134, loss 0.555507.
Train: 2018-08-04T22:02:56.572358: step 135, loss 0.592549.
Train: 2018-08-04T22:03:00.103981: step 136, loss 0.548707.
Train: 2018-08-04T22:03:03.604351: step 137, loss 0.568803.
Train: 2018-08-04T22:03:07.120347: step 138, loss 0.588816.
Train: 2018-08-04T22:03:10.651969: step 139, loss 0.541827.
Train: 2018-08-04T22:03:14.183592: step 140, loss 0.578844.
Test: 2018-08-04T22:03:29.060163: step 140, loss 0.564544.
Train: 2018-08-04T22:03:32.576158: step 141, loss 0.563302.
Train: 2018-08-04T22:03:36.107781: step 142, loss 0.547123.
Train: 2018-08-04T22:03:39.655031: step 143, loss 0.509489.
Train: 2018-08-04T22:03:43.171027: step 144, loss 0.509837.
Train: 2018-08-04T22:03:46.671396: step 145, loss 0.510223.
Train: 2018-08-04T22:03:50.218646: step 146, loss 0.596235.
Train: 2018-08-04T22:03:53.734642: step 147, loss 0.546546.
Train: 2018-08-04T22:03:57.281892: step 148, loss 0.508478.
Train: 2018-08-04T22:04:00.782261: step 149, loss 0.575479.
Train: 2018-08-04T22:04:04.298257: step 150, loss 0.58012.
Test: 2018-08-04T22:04:19.112321: step 150, loss 0.551612.
Train: 2018-08-04T22:04:20.862506: step 151, loss 0.48382.
Train: 2018-08-04T22:04:24.425382: step 152, loss 0.559318.
Train: 2018-08-04T22:04:27.957004: step 153, loss 0.625166.
Train: 2018-08-04T22:04:31.473001: step 154, loss 0.55186.
Train: 2018-08-04T22:04:34.988997: step 155, loss 0.560529.
Train: 2018-08-04T22:04:38.489367: step 156, loss 0.59402.
Train: 2018-08-04T22:04:42.020989: step 157, loss 0.566558.
Train: 2018-08-04T22:04:45.552612: step 158, loss 0.546419.
Train: 2018-08-04T22:04:49.084235: step 159, loss 0.532276.
Train: 2018-08-04T22:04:52.600231: step 160, loss 0.589315.
Test: 2018-08-04T22:05:07.476802: step 160, loss 0.602603.
Train: 2018-08-04T22:05:10.992797: step 161, loss 0.595116.
Train: 2018-08-04T22:05:14.508794: step 162, loss 0.534813.
Train: 2018-08-04T22:05:18.024789: step 163, loss 0.576944.
Train: 2018-08-04T22:05:21.572039: step 164, loss 0.557902.
Train: 2018-08-04T22:05:25.150542: step 165, loss 0.583778.
Train: 2018-08-04T22:05:28.650911: step 166, loss 0.630352.
Train: 2018-08-04T22:05:32.182535: step 167, loss 0.573011.
Train: 2018-08-04T22:05:35.667277: step 168, loss 0.595386.
Train: 2018-08-04T22:05:39.198899: step 169, loss 0.586117.
Train: 2018-08-04T22:05:42.746149: step 170, loss 0.615727.
Test: 2018-08-04T22:05:57.622720: step 170, loss 0.558493.
Train: 2018-08-04T22:06:01.138715: step 171, loss 0.555613.
Train: 2018-08-04T22:06:04.779725: step 172, loss 0.562938.
Train: 2018-08-04T22:06:08.280094: step 173, loss 0.604168.
Train: 2018-08-04T22:06:11.811717: step 174, loss 0.602421.
Train: 2018-08-04T22:06:15.327713: step 175, loss 0.542997.
Train: 2018-08-04T22:06:18.859336: step 176, loss 0.607774.
Train: 2018-08-04T22:06:22.406585: step 177, loss 0.56182.
Train: 2018-08-04T22:06:25.922582: step 178, loss 0.551291.
Train: 2018-08-04T22:06:29.422951: step 179, loss 0.570317.
Train: 2018-08-04T22:06:32.907694: step 180, loss 0.514655.
Test: 2018-08-04T22:06:47.799893: step 180, loss 1.21436.
Train: 2018-08-04T22:06:51.331514: step 181, loss 0.564355.
Train: 2018-08-04T22:06:54.878763: step 182, loss 0.558942.
Train: 2018-08-04T22:06:58.410386: step 183, loss 0.537578.
Train: 2018-08-04T22:07:01.942009: step 184, loss 0.634294.
Train: 2018-08-04T22:07:05.458005: step 185, loss 0.468248.
Train: 2018-08-04T22:07:08.974001: step 186, loss 0.512496.
Train: 2018-08-04T22:07:12.536877: step 187, loss 0.537077.
Train: 2018-08-04T22:07:16.068500: step 188, loss 0.476352.
Train: 2018-08-04T22:07:19.615749: step 189, loss 0.630339.
Train: 2018-08-04T22:07:23.116119: step 190, loss 0.549498.
Test: 2018-08-04T22:07:38.008318: step 190, loss 1.15347.
Train: 2018-08-04T22:07:41.508685: step 191, loss 0.58998.
Train: 2018-08-04T22:07:45.040308: step 192, loss 0.514239.
Train: 2018-08-04T22:07:48.571931: step 193, loss 0.604127.
Train: 2018-08-04T22:07:52.103554: step 194, loss 0.520482.
Train: 2018-08-04T22:07:55.635177: step 195, loss 0.540766.
Train: 2018-08-04T22:07:59.166799: step 196, loss 0.551923.
Train: 2018-08-04T22:08:02.682796: step 197, loss 0.63474.
Train: 2018-08-04T22:08:06.323805: step 198, loss 0.625833.
Train: 2018-08-04T22:08:09.839801: step 199, loss 0.584884.
Train: 2018-08-04T22:08:13.371424: step 200, loss 0.555531.
Test: 2018-08-04T22:08:28.216741: step 200, loss 1.07595.
Train: 2018-08-04T22:08:33.982975: step 201, loss 0.566686.
Train: 2018-08-04T22:08:37.483344: step 202, loss 0.562864.
Train: 2018-08-04T22:08:40.983714: step 203, loss 0.627534.
Train: 2018-08-04T22:08:44.499710: step 204, loss 0.614926.
Train: 2018-08-04T22:08:48.015706: step 205, loss 0.604107.
Train: 2018-08-04T22:08:51.531702: step 206, loss 0.54804.
Train: 2018-08-04T22:08:55.047698: step 207, loss 0.591752.
Train: 2018-08-04T22:08:58.563694: step 208, loss 0.531377.
Train: 2018-08-04T22:09:02.048437: step 209, loss 0.57979.
Train: 2018-08-04T22:09:05.580060: step 210, loss 0.564265.
Test: 2018-08-04T22:09:20.456631: step 210, loss 0.553958.
Train: 2018-08-04T22:09:23.988253: step 211, loss 0.565715.
Train: 2018-08-04T22:09:27.519876: step 212, loss 0.539697.
Train: 2018-08-04T22:09:31.067125: step 213, loss 0.542405.
Train: 2018-08-04T22:09:34.567495: step 214, loss 0.517645.
Train: 2018-08-04T22:09:38.067864: step 215, loss 0.541738.
Train: 2018-08-04T22:09:41.599487: step 216, loss 0.538598.
Train: 2018-08-04T22:09:45.099856: step 217, loss 0.624629.
Train: 2018-08-04T22:09:48.647106: step 218, loss 0.582779.
Train: 2018-08-04T22:09:52.209982: step 219, loss 0.529011.
Train: 2018-08-04T22:09:55.741605: step 220, loss 0.495675.
Test: 2018-08-04T22:10:10.618178: step 220, loss 0.547973.
Train: 2018-08-04T22:10:14.118545: step 221, loss 0.555881.
Train: 2018-08-04T22:10:17.681421: step 222, loss 0.537099.
Train: 2018-08-04T22:10:21.197417: step 223, loss 0.500215.
Train: 2018-08-04T22:10:24.729040: step 224, loss 0.550143.
Train: 2018-08-04T22:10:28.260662: step 225, loss 0.643061.
Train: 2018-08-04T22:10:31.776659: step 226, loss 0.551297.
Train: 2018-08-04T22:10:35.277028: step 227, loss 0.573244.
Train: 2018-08-04T22:10:38.839904: step 228, loss 0.548515.
Train: 2018-08-04T22:10:42.340274: step 229, loss 0.616695.
Train: 2018-08-04T22:10:45.871897: step 230, loss 0.601204.
Test: 2018-08-04T22:11:00.810974: step 230, loss 0.548718.
Train: 2018-08-04T22:11:04.342597: step 231, loss 0.601832.
Train: 2018-08-04T22:11:07.858592: step 232, loss 0.608521.
Train: 2018-08-04T22:11:11.374589: step 233, loss 0.550602.
Train: 2018-08-04T22:11:14.874958: step 234, loss 0.633838.
Train: 2018-08-04T22:11:18.406581: step 235, loss 0.564899.
Train: 2018-08-04T22:11:21.922577: step 236, loss 0.57292.
Train: 2018-08-04T22:11:25.516707: step 237, loss 0.605348.
Train: 2018-08-04T22:11:29.017076: step 238, loss 0.511327.
Train: 2018-08-04T22:11:32.533072: step 239, loss 0.562307.
Train: 2018-08-04T22:11:36.033442: step 240, loss 0.547578.
Test: 2018-08-04T22:11:50.941266: step 240, loss 0.55258.
Train: 2018-08-04T22:11:54.457261: step 241, loss 0.56578.
Train: 2018-08-04T22:11:58.004511: step 242, loss 0.620152.
Train: 2018-08-04T22:12:01.536134: step 243, loss 0.625688.
Train: 2018-08-04T22:12:05.114636: step 244, loss 0.545545.
Train: 2018-08-04T22:12:08.630632: step 245, loss 0.544533.
Train: 2018-08-04T22:12:12.146628: step 246, loss 0.500939.
Train: 2018-08-04T22:12:15.662625: step 247, loss 0.527484.
Train: 2018-08-04T22:12:19.241127: step 248, loss 0.568684.
Train: 2018-08-04T22:12:22.788377: step 249, loss 0.581166.
Train: 2018-08-04T22:12:26.320000: step 250, loss 0.60222.
Test: 2018-08-04T22:12:41.196571: step 250, loss 0.549168.
Train: 2018-08-04T22:12:44.696940: step 251, loss 0.601863.
Train: 2018-08-04T22:12:48.197309: step 252, loss 0.50771.
Train: 2018-08-04T22:12:51.728932: step 253, loss 0.527294.
Train: 2018-08-04T22:12:55.276181: step 254, loss 0.530996.
Train: 2018-08-04T22:12:58.807804: step 255, loss 0.54833.
Train: 2018-08-04T22:13:02.355053: step 256, loss 0.592306.
Train: 2018-08-04T22:13:05.886676: step 257, loss 0.524643.
Train: 2018-08-04T22:13:09.433926: step 258, loss 0.524157.
Train: 2018-08-04T22:13:12.934295: step 259, loss 0.587825.
Train: 2018-08-04T22:13:16.512798: step 260, loss 0.525205.
Test: 2018-08-04T22:13:31.373744: step 260, loss 0.546636.
Train: 2018-08-04T22:13:34.967871: step 261, loss 0.576773.
Train: 2018-08-04T22:13:38.499494: step 262, loss 0.636691.
Train: 2018-08-04T22:13:42.015490: step 263, loss 0.542905.
Train: 2018-08-04T22:13:45.547113: step 264, loss 0.582118.
Train: 2018-08-04T22:13:49.172495: step 265, loss 0.581394.
Train: 2018-08-04T22:13:52.688491: step 266, loss 0.466057.
Train: 2018-08-04T22:13:56.204488: step 267, loss 0.60353.
Train: 2018-08-04T22:13:59.751737: step 268, loss 0.577133.
Train: 2018-08-04T22:14:03.267734: step 269, loss 0.539675.
Train: 2018-08-04T22:14:06.799356: step 270, loss 0.501075.
Test: 2018-08-04T22:14:21.707182: step 270, loss 0.548989.
Train: 2018-08-04T22:14:25.238803: step 271, loss 0.560581.
Train: 2018-08-04T22:14:28.801679: step 272, loss 0.632983.
Train: 2018-08-04T22:14:32.380181: step 273, loss 0.605117.
Train: 2018-08-04T22:14:35.896178: step 274, loss 0.614165.
Train: 2018-08-04T22:14:39.443427: step 275, loss 0.502995.
Train: 2018-08-04T22:14:43.006303: step 276, loss 0.551731.
Train: 2018-08-04T22:14:46.491046: step 277, loss 0.565975.
Train: 2018-08-04T22:14:50.022669: step 278, loss 0.546065.
Train: 2018-08-04T22:14:53.569918: step 279, loss 0.53282.
Train: 2018-08-04T22:14:57.117168: step 280, loss 0.552479.
Test: 2018-08-04T22:15:11.962486: step 280, loss 0.548059.
Train: 2018-08-04T22:15:15.462854: step 281, loss 0.557438.
Train: 2018-08-04T22:15:18.978850: step 282, loss 0.566057.
Train: 2018-08-04T22:15:22.494846: step 283, loss 0.541017.
Train: 2018-08-04T22:15:26.010843: step 284, loss 0.5767.
Train: 2018-08-04T22:15:29.558092: step 285, loss 0.511426.
Train: 2018-08-04T22:15:33.105341: step 286, loss 0.575612.
Train: 2018-08-04T22:15:36.621337: step 287, loss 0.519071.
Train: 2018-08-04T22:15:40.168587: step 288, loss 0.506149.
Train: 2018-08-04T22:15:43.700210: step 289, loss 0.586014.
Train: 2018-08-04T22:15:47.216206: step 290, loss 0.599343.
Test: 2018-08-04T22:16:02.155283: step 290, loss 0.548288.
Train: 2018-08-04T22:16:05.686906: step 291, loss 0.563436.
Train: 2018-08-04T22:16:09.234155: step 292, loss 0.570951.
Train: 2018-08-04T22:16:12.734524: step 293, loss 0.563197.
Train: 2018-08-04T22:16:16.281774: step 294, loss 0.511018.
Train: 2018-08-04T22:16:19.797770: step 295, loss 0.54832.
Train: 2018-08-04T22:16:23.313767: step 296, loss 0.520383.
Train: 2018-08-04T22:16:26.892269: step 297, loss 0.583415.
Train: 2018-08-04T22:16:30.455145: step 298, loss 0.435181.
Train: 2018-08-04T22:16:33.986768: step 299, loss 0.564941.
Train: 2018-08-04T22:16:37.518391: step 300, loss 0.557529.
Test: 2018-08-04T22:16:52.488721: step 300, loss 0.549029.
Train: 2018-08-04T22:16:58.301835: step 301, loss 0.478095.
Train: 2018-08-04T22:17:00.020767: step 302, loss 0.585491.
Train: 2018-08-04T22:17:03.552389: step 303, loss 0.500966.
Train: 2018-08-04T22:17:07.099638: step 304, loss 0.578108.
Train: 2018-08-04T22:17:10.600008: step 305, loss 0.571739.
Train: 2018-08-04T22:17:14.131631: step 306, loss 0.537032.
Train: 2018-08-04T22:17:17.647627: step 307, loss 0.655144.
Train: 2018-08-04T22:17:21.147996: step 308, loss 0.573243.
Train: 2018-08-04T22:17:24.695246: step 309, loss 0.531439.
Train: 2018-08-04T22:17:28.226868: step 310, loss 0.64975.
Test: 2018-08-04T22:17:43.009679: step 310, loss 0.547123.
Train: 2018-08-04T22:17:46.510048: step 311, loss 0.518823.
Train: 2018-08-04T22:17:50.041672: step 312, loss 0.615696.
Train: 2018-08-04T22:17:53.542041: step 313, loss 0.572015.
Train: 2018-08-04T22:17:57.073663: step 314, loss 0.588844.
Train: 2018-08-04T22:18:00.636540: step 315, loss 0.667403.
Train: 2018-08-04T22:18:04.168162: step 316, loss 0.483686.
Train: 2018-08-04T22:18:07.699785: step 317, loss 0.520584.
Train: 2018-08-04T22:18:11.200155: step 318, loss 0.575487.
Train: 2018-08-04T22:18:14.716151: step 319, loss 0.556058.
Train: 2018-08-04T22:18:18.232147: step 320, loss 0.520066.
Test: 2018-08-04T22:18:33.124344: step 320, loss 0.548318.
Train: 2018-08-04T22:18:36.671593: step 321, loss 0.548736.
Train: 2018-08-04T22:18:40.250097: step 322, loss 0.556533.
Train: 2018-08-04T22:18:43.797345: step 323, loss 0.532881.
Train: 2018-08-04T22:18:47.313342: step 324, loss 0.622507.
Train: 2018-08-04T22:18:50.829338: step 325, loss 0.592111.
Train: 2018-08-04T22:18:54.345334: step 326, loss 0.506157.
Train: 2018-08-04T22:18:57.923837: step 327, loss 0.529645.
Train: 2018-08-04T22:19:01.455459: step 328, loss 0.600698.
Train: 2018-08-04T22:19:04.955829: step 329, loss 0.568981.
Train: 2018-08-04T22:19:08.503078: step 330, loss 0.577575.
Test: 2018-08-04T22:19:23.426529: step 330, loss 0.548887.
Train: 2018-08-04T22:19:27.083165: step 331, loss 0.620271.
Train: 2018-08-04T22:19:30.599161: step 332, loss 0.54275.
Train: 2018-08-04T22:19:34.146411: step 333, loss 0.545076.
Train: 2018-08-04T22:19:37.662406: step 334, loss 0.536826.
Train: 2018-08-04T22:19:41.194029: step 335, loss 0.522995.
Train: 2018-08-04T22:19:44.710025: step 336, loss 0.531186.
Train: 2018-08-04T22:19:48.210395: step 337, loss 0.605105.
Train: 2018-08-04T22:19:51.726391: step 338, loss 0.626195.
Train: 2018-08-04T22:19:55.242387: step 339, loss 0.591284.
Train: 2018-08-04T22:19:58.774010: step 340, loss 0.579146.
Test: 2018-08-04T22:20:13.713087: step 340, loss 0.548578.
Train: 2018-08-04T22:20:17.229083: step 341, loss 0.620653.
Train: 2018-08-04T22:20:20.745079: step 342, loss 0.604124.
Train: 2018-08-04T22:20:24.261075: step 343, loss 0.542205.
Train: 2018-08-04T22:20:27.777071: step 344, loss 0.563623.
Train: 2018-08-04T22:20:31.308694: step 345, loss 0.552964.
Train: 2018-08-04T22:20:34.840317: step 346, loss 0.484628.
Train: 2018-08-04T22:20:38.340686: step 347, loss 0.532252.
Train: 2018-08-04T22:20:41.919200: step 348, loss 0.528881.
Train: 2018-08-04T22:20:45.450812: step 349, loss 0.523581.
Train: 2018-08-04T22:20:48.951182: step 350, loss 0.625786.
Test: 2018-08-04T22:21:03.905886: step 350, loss 0.548729.
Train: 2018-08-04T22:21:07.437508: step 351, loss 0.503923.
Train: 2018-08-04T22:21:10.984757: step 352, loss 0.565134.
Train: 2018-08-04T22:21:14.500753: step 353, loss 0.486977.
Train: 2018-08-04T22:21:18.016750: step 354, loss 0.580337.
Train: 2018-08-04T22:21:21.532746: step 355, loss 0.559905.
Train: 2018-08-04T22:21:25.064368: step 356, loss 0.501034.
Train: 2018-08-04T22:21:28.595991: step 357, loss 0.54525.
Train: 2018-08-04T22:21:32.111988: step 358, loss 0.520279.
Train: 2018-08-04T22:21:35.643610: step 359, loss 0.479659.
Train: 2018-08-04T22:21:39.175233: step 360, loss 0.629909.
Test: 2018-08-04T22:21:54.067433: step 360, loss 0.548173.
Train: 2018-08-04T22:21:57.599053: step 361, loss 0.570487.
Train: 2018-08-04T22:22:01.161929: step 362, loss 0.548031.
Train: 2018-08-04T22:22:04.693552: step 363, loss 0.501679.
Train: 2018-08-04T22:22:08.225175: step 364, loss 0.567758.
Train: 2018-08-04T22:22:11.788051: step 365, loss 0.498228.
Train: 2018-08-04T22:22:15.304047: step 366, loss 0.513526.
Train: 2018-08-04T22:22:18.866923: step 367, loss 0.45788.
Train: 2018-08-04T22:22:22.382919: step 368, loss 0.473821.
Train: 2018-08-04T22:22:25.898915: step 369, loss 0.525328.
Train: 2018-08-04T22:22:29.399285: step 370, loss 0.555689.
Test: 2018-08-04T22:22:44.291481: step 370, loss 0.548867.
Train: 2018-08-04T22:22:47.823105: step 371, loss 0.628988.
Train: 2018-08-04T22:22:51.370354: step 372, loss 0.540342.
Train: 2018-08-04T22:22:54.901976: step 373, loss 0.516327.
Train: 2018-08-04T22:22:58.402346: step 374, loss 0.59381.
Train: 2018-08-04T22:23:01.902716: step 375, loss 0.630005.
Train: 2018-08-04T22:23:05.449965: step 376, loss 0.500309.
Train: 2018-08-04T22:23:08.965961: step 377, loss 0.489801.
Train: 2018-08-04T22:23:12.497584: step 378, loss 0.588357.
Train: 2018-08-04T22:23:15.997954: step 379, loss 0.522899.
Train: 2018-08-04T22:23:19.513950: step 380, loss 0.643556.
Test: 2018-08-04T22:23:34.374895: step 380, loss 0.547662.
Train: 2018-08-04T22:23:37.875263: step 381, loss 0.528098.
Train: 2018-08-04T22:23:41.422512: step 382, loss 0.551179.
Train: 2018-08-04T22:23:44.954135: step 383, loss 0.535376.
Train: 2018-08-04T22:23:48.438878: step 384, loss 0.588059.
Train: 2018-08-04T22:23:51.986127: step 385, loss 0.548684.
Train: 2018-08-04T22:23:55.470870: step 386, loss 0.50146.
Train: 2018-08-04T22:23:58.986866: step 387, loss 0.561517.
Train: 2018-08-04T22:24:02.549742: step 388, loss 0.465068.
Train: 2018-08-04T22:24:06.096992: step 389, loss 0.576651.
Train: 2018-08-04T22:24:09.612988: step 390, loss 0.614376.
Test: 2018-08-04T22:24:24.505186: step 390, loss 0.546825.
Train: 2018-08-04T22:24:28.036808: step 391, loss 0.568728.
Train: 2018-08-04T22:24:31.584057: step 392, loss 0.538644.
Train: 2018-08-04T22:24:35.084427: step 393, loss 0.705009.
Train: 2018-08-04T22:24:38.631677: step 394, loss 0.512879.
Train: 2018-08-04T22:24:42.178926: step 395, loss 0.519083.
Train: 2018-08-04T22:24:45.726175: step 396, loss 0.526528.
Train: 2018-08-04T22:24:49.226545: step 397, loss 0.563567.
Train: 2018-08-04T22:24:52.773794: step 398, loss 0.55018.
Train: 2018-08-04T22:24:56.274163: step 399, loss 0.628627.
Train: 2018-08-04T22:24:59.821413: step 400, loss 0.563452.
Test: 2018-08-04T22:25:14.697983: step 400, loss 0.548566.
Train: 2018-08-04T22:25:20.448590: step 401, loss 0.58354.
Train: 2018-08-04T22:25:23.948960: step 402, loss 0.536089.
Train: 2018-08-04T22:25:27.433703: step 403, loss 0.580886.
Train: 2018-08-04T22:25:30.934073: step 404, loss 0.60809.
Train: 2018-08-04T22:25:34.434442: step 405, loss 0.534222.
Train: 2018-08-04T22:25:37.966065: step 406, loss 0.592536.
Train: 2018-08-04T22:25:41.497687: step 407, loss 0.544611.
Train: 2018-08-04T22:25:45.013684: step 408, loss 0.609836.
Train: 2018-08-04T22:25:48.529680: step 409, loss 0.506364.
Train: 2018-08-04T22:25:52.014422: step 410, loss 0.543451.
Test: 2018-08-04T22:26:06.844113: step 410, loss 0.549752.
Train: 2018-08-04T22:26:10.360109: step 411, loss 0.567339.
Train: 2018-08-04T22:26:13.891732: step 412, loss 0.55599.
Train: 2018-08-04T22:26:17.438981: step 413, loss 0.629969.
Train: 2018-08-04T22:26:21.001857: step 414, loss 0.583248.
Train: 2018-08-04T22:26:24.486600: step 415, loss 0.564748.
Train: 2018-08-04T22:26:28.018223: step 416, loss 0.597038.
Train: 2018-08-04T22:26:31.518592: step 417, loss 0.648046.
Train: 2018-08-04T22:26:35.050215: step 418, loss 0.53787.
Train: 2018-08-04T22:26:38.550585: step 419, loss 0.561534.
Train: 2018-08-04T22:26:42.035328: step 420, loss 0.539704.
Test: 2018-08-04T22:26:56.896272: step 420, loss 0.548977.
Train: 2018-08-04T22:27:00.474775: step 421, loss 0.588274.
Train: 2018-08-04T22:27:03.990770: step 422, loss 0.526567.
Train: 2018-08-04T22:27:07.475513: step 423, loss 0.547424.
Train: 2018-08-04T22:27:11.007136: step 424, loss 0.578162.
Train: 2018-08-04T22:27:14.523132: step 425, loss 0.55627.
Train: 2018-08-04T22:27:18.023501: step 426, loss 0.587238.
Train: 2018-08-04T22:27:21.508244: step 427, loss 0.554694.
Train: 2018-08-04T22:27:24.992987: step 428, loss 0.524036.
Train: 2018-08-04T22:27:28.462103: step 429, loss 0.553458.
Train: 2018-08-04T22:27:31.978100: step 430, loss 0.554969.
Test: 2018-08-04T22:27:46.870298: step 430, loss 0.547884.
Train: 2018-08-04T22:27:50.370666: step 431, loss 0.529825.
Train: 2018-08-04T22:27:53.902289: step 432, loss 0.563425.
Train: 2018-08-04T22:27:57.387032: step 433, loss 0.598099.
Train: 2018-08-04T22:28:00.934281: step 434, loss 0.603534.
Train: 2018-08-04T22:28:04.403397: step 435, loss 0.556174.
Train: 2018-08-04T22:28:07.997527: step 436, loss 0.579822.
Train: 2018-08-04T22:28:11.529149: step 437, loss 0.591553.
Train: 2018-08-04T22:28:15.029519: step 438, loss 0.571972.
Train: 2018-08-04T22:28:18.545515: step 439, loss 0.562133.
Train: 2018-08-04T22:28:22.061511: step 440, loss 0.687588.
Test: 2018-08-04T22:28:36.906829: step 440, loss 0.550141.
Train: 2018-08-04T22:28:40.391571: step 441, loss 0.531577.
Train: 2018-08-04T22:28:43.954447: step 442, loss 0.593096.
Train: 2018-08-04T22:28:47.532950: step 443, loss 0.563644.
Train: 2018-08-04T22:28:51.017693: step 444, loss 0.537591.
Train: 2018-08-04T22:28:54.533689: step 445, loss 0.597459.
Train: 2018-08-04T22:28:58.018431: step 446, loss 0.579187.
Train: 2018-08-04T22:29:01.518802: step 447, loss 0.528544.
Train: 2018-08-04T22:29:05.175437: step 448, loss 0.644464.
Train: 2018-08-04T22:29:08.675806: step 449, loss 0.531952.
Train: 2018-08-04T22:29:12.176176: step 450, loss 0.625447.
Test: 2018-08-04T22:29:27.115254: step 450, loss 0.54821.
Train: 2018-08-04T22:29:30.631251: step 451, loss 0.564692.
Train: 2018-08-04T22:29:34.162872: step 452, loss 0.636845.
Train: 2018-08-04T22:29:35.881803: step 453, loss 0.49461.
Train: 2018-08-04T22:29:39.366547: step 454, loss 0.548067.
Train: 2018-08-04T22:29:42.913796: step 455, loss 0.574518.
Train: 2018-08-04T22:29:46.461045: step 456, loss 0.602149.
Train: 2018-08-04T22:29:49.992668: step 457, loss 0.539746.
Train: 2018-08-04T22:29:53.477411: step 458, loss 0.580743.
Train: 2018-08-04T22:29:56.993407: step 459, loss 0.532893.
Train: 2018-08-04T22:30:00.478150: step 460, loss 0.50964.
Test: 2018-08-04T22:30:15.370346: step 460, loss 0.548662.
Train: 2018-08-04T22:30:18.917606: step 461, loss 0.531933.
Train: 2018-08-04T22:30:22.433592: step 462, loss 0.515488.
Train: 2018-08-04T22:30:25.933962: step 463, loss 0.613773.
Train: 2018-08-04T22:30:29.418705: step 464, loss 0.472768.
Train: 2018-08-04T22:30:32.950328: step 465, loss 0.592375.
Train: 2018-08-04T22:30:36.450697: step 466, loss 0.601589.
Train: 2018-08-04T22:30:39.966693: step 467, loss 0.562376.
Train: 2018-08-04T22:30:43.529570: step 468, loss 0.544383.
Train: 2018-08-04T22:30:47.045579: step 469, loss 0.587312.
Train: 2018-08-04T22:30:50.530308: step 470, loss 0.596503.
Test: 2018-08-04T22:31:05.500638: step 470, loss 0.547535.
Train: 2018-08-04T22:31:09.032261: step 471, loss 0.590157.
Train: 2018-08-04T22:31:12.579511: step 472, loss 0.622189.
Train: 2018-08-04T22:31:16.126760: step 473, loss 0.596184.
Train: 2018-08-04T22:31:19.658384: step 474, loss 0.480548.
Train: 2018-08-04T22:31:23.158753: step 475, loss 0.527363.
Train: 2018-08-04T22:31:26.643495: step 476, loss 0.587169.
Train: 2018-08-04T22:31:30.159492: step 477, loss 0.487661.
Train: 2018-08-04T22:31:33.659861: step 478, loss 0.487617.
Train: 2018-08-04T22:31:37.207111: step 479, loss 0.562891.
Train: 2018-08-04T22:31:40.723107: step 480, loss 0.53761.
Test: 2018-08-04T22:31:55.552799: step 480, loss 0.547174.
Train: 2018-08-04T22:31:59.053167: step 481, loss 0.537667.
Train: 2018-08-04T22:32:02.569163: step 482, loss 0.562697.
Train: 2018-08-04T22:32:06.069532: step 483, loss 0.600018.
Train: 2018-08-04T22:32:09.585529: step 484, loss 0.581818.
Train: 2018-08-04T22:32:13.117151: step 485, loss 0.552631.
Train: 2018-08-04T22:32:16.680027: step 486, loss 0.561505.
Train: 2018-08-04T22:32:20.164770: step 487, loss 0.561398.
Train: 2018-08-04T22:32:23.665139: step 488, loss 0.555226.
Train: 2018-08-04T22:32:27.196762: step 489, loss 0.662321.
Train: 2018-08-04T22:32:30.697132: step 490, loss 0.611776.
Test: 2018-08-04T22:32:45.589331: step 490, loss 0.547436.
Train: 2018-08-04T22:32:49.120951: step 491, loss 0.552657.
Train: 2018-08-04T22:32:52.668201: step 492, loss 0.536723.
Train: 2018-08-04T22:32:56.152944: step 493, loss 0.567341.
Train: 2018-08-04T22:32:59.637687: step 494, loss 0.551964.
Train: 2018-08-04T22:33:03.200563: step 495, loss 0.547904.
Train: 2018-08-04T22:33:06.700932: step 496, loss 0.570079.
Train: 2018-08-04T22:33:10.248182: step 497, loss 0.52987.
Train: 2018-08-04T22:33:13.795431: step 498, loss 0.519445.
Train: 2018-08-04T22:33:17.295801: step 499, loss 0.550996.
Train: 2018-08-04T22:33:20.796170: step 500, loss 0.509483.
Test: 2018-08-04T22:33:35.641487: step 500, loss 0.547235.
Train: 2018-08-04T22:33:41.329588: step 501, loss 0.529174.
Train: 2018-08-04T22:33:44.829958: step 502, loss 0.522373.
Train: 2018-08-04T22:33:48.330327: step 503, loss 0.552249.
Train: 2018-08-04T22:33:51.830696: step 504, loss 0.545858.
Train: 2018-08-04T22:33:55.315439: step 505, loss 0.598398.
Train: 2018-08-04T22:33:58.815809: step 506, loss 0.639982.
Train: 2018-08-04T22:34:02.347431: step 507, loss 0.56547.
Train: 2018-08-04T22:34:05.816548: step 508, loss 0.572238.
Train: 2018-08-04T22:34:09.363797: step 509, loss 0.572817.
Train: 2018-08-04T22:34:12.864167: step 510, loss 0.535526.
Test: 2018-08-04T22:34:27.740739: step 510, loss 0.548851.
Train: 2018-08-04T22:34:31.241106: step 511, loss 0.561391.
Train: 2018-08-04T22:34:34.725849: step 512, loss 0.529379.
Train: 2018-08-04T22:34:38.226218: step 513, loss 0.570475.
Train: 2018-08-04T22:34:41.710961: step 514, loss 0.529046.
Train: 2018-08-04T22:34:45.258211: step 515, loss 0.606764.
Train: 2018-08-04T22:34:48.836714: step 516, loss 0.641802.
Train: 2018-08-04T22:34:52.321456: step 517, loss 0.606611.
Train: 2018-08-04T22:34:55.837453: step 518, loss 0.542925.
Train: 2018-08-04T22:34:59.353449: step 519, loss 0.537294.
Train: 2018-08-04T22:35:02.869445: step 520, loss 0.573145.
Test: 2018-08-04T22:35:17.792895: step 520, loss 0.547623.
Train: 2018-08-04T22:35:21.277639: step 521, loss 0.564447.
Train: 2018-08-04T22:35:24.840515: step 522, loss 0.55469.
Train: 2018-08-04T22:35:28.325257: step 523, loss 0.544257.
Train: 2018-08-04T22:35:31.856880: step 524, loss 0.585386.
Train: 2018-08-04T22:35:35.372876: step 525, loss 0.561453.
Train: 2018-08-04T22:35:38.857619: step 526, loss 0.621542.
Train: 2018-08-04T22:35:42.357988: step 527, loss 0.51368.
Train: 2018-08-04T22:35:45.873985: step 528, loss 0.546226.
Train: 2018-08-04T22:35:49.343101: step 529, loss 0.66286.
Train: 2018-08-04T22:35:52.890350: step 530, loss 0.58532.
Test: 2018-08-04T22:36:07.876308: step 530, loss 0.549068.
Train: 2018-08-04T22:36:11.392303: step 531, loss 0.60303.
Train: 2018-08-04T22:36:14.877045: step 532, loss 0.50331.
Train: 2018-08-04T22:36:18.377415: step 533, loss 0.563414.
Train: 2018-08-04T22:36:21.909039: step 534, loss 0.522042.
Train: 2018-08-04T22:36:25.409408: step 535, loss 0.593584.
Train: 2018-08-04T22:36:28.909777: step 536, loss 0.496852.
Train: 2018-08-04T22:36:32.378894: step 537, loss 0.571192.
Train: 2018-08-04T22:36:35.894889: step 538, loss 0.586618.
Train: 2018-08-04T22:36:39.410886: step 539, loss 0.588914.
Train: 2018-08-04T22:36:42.911255: step 540, loss 0.560738.
Test: 2018-08-04T22:36:57.772199: step 540, loss 0.549416.
Train: 2018-08-04T22:37:01.272568: step 541, loss 0.560504.
Train: 2018-08-04T22:37:04.741685: step 542, loss 0.553042.
Train: 2018-08-04T22:37:08.257680: step 543, loss 0.577246.
Train: 2018-08-04T22:37:11.742423: step 544, loss 0.627422.
Train: 2018-08-04T22:37:15.274046: step 545, loss 0.50413.
Train: 2018-08-04T22:37:18.774416: step 546, loss 0.638317.
Train: 2018-08-04T22:37:22.274786: step 547, loss 0.598477.
Train: 2018-08-04T22:37:25.790782: step 548, loss 0.56516.
Train: 2018-08-04T22:37:29.244271: step 549, loss 0.49085.
Train: 2018-08-04T22:37:32.729014: step 550, loss 0.563271.
Test: 2018-08-04T22:37:47.574332: step 550, loss 0.548992.
Train: 2018-08-04T22:37:51.090327: step 551, loss 0.512081.
Train: 2018-08-04T22:37:54.575070: step 552, loss 0.561373.
Train: 2018-08-04T22:37:58.091066: step 553, loss 0.562095.
Train: 2018-08-04T22:38:01.591436: step 554, loss 0.496604.
Train: 2018-08-04T22:38:05.091805: step 555, loss 0.552975.
Train: 2018-08-04T22:38:08.576548: step 556, loss 0.564276.
Train: 2018-08-04T22:38:12.076918: step 557, loss 0.511864.
Train: 2018-08-04T22:38:15.639793: step 558, loss 0.556989.
Train: 2018-08-04T22:38:19.171416: step 559, loss 0.568282.
Train: 2018-08-04T22:38:22.687412: step 560, loss 0.509356.
Test: 2018-08-04T22:38:37.563983: step 560, loss 0.547318.
Train: 2018-08-04T22:38:41.064352: step 561, loss 0.562385.
Train: 2018-08-04T22:38:44.549095: step 562, loss 0.51836.
Train: 2018-08-04T22:38:48.033838: step 563, loss 0.630345.
Train: 2018-08-04T22:38:51.596714: step 564, loss 0.57327.
Train: 2018-08-04T22:38:55.159590: step 565, loss 0.563686.
Train: 2018-08-04T22:38:58.659960: step 566, loss 0.617093.
Train: 2018-08-04T22:39:02.144702: step 567, loss 0.585853.
Train: 2018-08-04T22:39:05.613818: step 568, loss 0.616938.
Train: 2018-08-04T22:39:09.114188: step 569, loss 0.555887.
Train: 2018-08-04T22:39:12.630184: step 570, loss 0.615661.
Test: 2018-08-04T22:39:27.569263: step 570, loss 0.549294.
Train: 2018-08-04T22:39:31.132137: step 571, loss 0.535842.
Train: 2018-08-04T22:39:34.726267: step 572, loss 0.622565.
Train: 2018-08-04T22:39:38.242263: step 573, loss 0.544159.
Train: 2018-08-04T22:39:41.773886: step 574, loss 0.557997.
Train: 2018-08-04T22:39:45.274255: step 575, loss 0.530225.
Train: 2018-08-04T22:39:48.821505: step 576, loss 0.569343.
Train: 2018-08-04T22:39:52.368754: step 577, loss 0.632516.
Train: 2018-08-04T22:39:55.853497: step 578, loss 0.511824.
Train: 2018-08-04T22:39:59.338240: step 579, loss 0.538334.
Train: 2018-08-04T22:40:02.838609: step 580, loss 0.528839.
Test: 2018-08-04T22:40:17.730807: step 580, loss 0.548664.
Train: 2018-08-04T22:40:21.278055: step 581, loss 0.56443.
Train: 2018-08-04T22:40:24.840932: step 582, loss 0.546325.
Train: 2018-08-04T22:40:28.341301: step 583, loss 0.528116.
Train: 2018-08-04T22:40:31.888551: step 584, loss 0.57833.
Train: 2018-08-04T22:40:35.373294: step 585, loss 0.539409.
Train: 2018-08-04T22:40:38.842409: step 586, loss 0.612978.
Train: 2018-08-04T22:40:42.342779: step 587, loss 0.522062.
Train: 2018-08-04T22:40:45.843149: step 588, loss 0.545357.
Train: 2018-08-04T22:40:49.343518: step 589, loss 0.51228.
Train: 2018-08-04T22:40:52.890768: step 590, loss 0.486837.
Test: 2018-08-04T22:41:07.845471: step 590, loss 0.548361.
Train: 2018-08-04T22:41:11.330214: step 591, loss 0.561241.
Train: 2018-08-04T22:41:14.830583: step 592, loss 0.537057.
Train: 2018-08-04T22:41:18.315326: step 593, loss 0.519493.
Train: 2018-08-04T22:41:21.846949: step 594, loss 0.620279.
Train: 2018-08-04T22:41:25.378572: step 595, loss 0.538118.
Train: 2018-08-04T22:41:28.925821: step 596, loss 0.513519.
Train: 2018-08-04T22:41:32.394938: step 597, loss 0.603577.
Train: 2018-08-04T22:41:35.926561: step 598, loss 0.570405.
Train: 2018-08-04T22:41:39.442557: step 599, loss 0.643459.
Train: 2018-08-04T22:41:42.942926: step 600, loss 0.570017.
Test: 2018-08-04T22:41:57.882003: step 600, loss 0.54754.
Train: 2018-08-04T22:42:03.570103: step 601, loss 0.565212.
Train: 2018-08-04T22:42:07.054847: step 602, loss 0.703165.
Train: 2018-08-04T22:42:10.602096: step 603, loss 0.554191.
Train: 2018-08-04T22:42:12.289774: step 604, loss 0.560736.
Train: 2018-08-04T22:42:15.821396: step 605, loss 0.555063.
Train: 2018-08-04T22:42:19.337393: step 606, loss 0.555277.
Train: 2018-08-04T22:42:22.853389: step 607, loss 0.559664.
Train: 2018-08-04T22:42:26.322505: step 608, loss 0.529336.
Train: 2018-08-04T22:42:29.807248: step 609, loss 0.586278.
Train: 2018-08-04T22:42:33.323244: step 610, loss 0.64539.
Test: 2018-08-04T22:42:48.246695: step 610, loss 0.547905.
Train: 2018-08-04T22:42:51.700184: step 611, loss 0.58685.
Train: 2018-08-04T22:42:55.216180: step 612, loss 0.581885.
Train: 2018-08-04T22:42:58.810309: step 613, loss 0.562891.
Train: 2018-08-04T22:43:02.326305: step 614, loss 0.547989.
Train: 2018-08-04T22:43:05.842302: step 615, loss 0.560752.
Train: 2018-08-04T22:43:09.373925: step 616, loss 0.585413.
Train: 2018-08-04T22:43:12.858667: step 617, loss 0.564771.
Train: 2018-08-04T22:43:16.374664: step 618, loss 0.557.
Train: 2018-08-04T22:43:19.921913: step 619, loss 0.604143.
Train: 2018-08-04T22:43:23.469162: step 620, loss 0.593063.
Test: 2018-08-04T22:43:38.361359: step 620, loss 0.549049.
Train: 2018-08-04T22:43:41.846102: step 621, loss 0.570534.
Train: 2018-08-04T22:43:45.346472: step 622, loss 0.595633.
Train: 2018-08-04T22:43:48.971854: step 623, loss 0.610925.
Train: 2018-08-04T22:43:52.503477: step 624, loss 0.548051.
Train: 2018-08-04T22:43:56.035100: step 625, loss 0.564932.
Train: 2018-08-04T22:43:59.566722: step 626, loss 0.522313.
Train: 2018-08-04T22:44:03.082719: step 627, loss 0.554443.
Train: 2018-08-04T22:44:06.567462: step 628, loss 0.602395.
Train: 2018-08-04T22:44:10.052205: step 629, loss 0.53972.
Train: 2018-08-04T22:44:13.568201: step 630, loss 0.522526.
Test: 2018-08-04T22:44:28.476025: step 630, loss 0.549266.
Train: 2018-08-04T22:44:31.992021: step 631, loss 0.611611.
Train: 2018-08-04T22:44:35.539270: step 632, loss 0.611453.
Train: 2018-08-04T22:44:39.024013: step 633, loss 0.555302.
Train: 2018-08-04T22:44:42.524382: step 634, loss 0.578109.
Train: 2018-08-04T22:44:46.024751: step 635, loss 0.531298.
Train: 2018-08-04T22:44:49.556375: step 636, loss 0.538323.
Train: 2018-08-04T22:44:53.087997: step 637, loss 0.547171.
Train: 2018-08-04T22:44:56.666500: step 638, loss 0.579679.
Train: 2018-08-04T22:45:00.151243: step 639, loss 0.634774.
Train: 2018-08-04T22:45:03.635986: step 640, loss 0.588201.
Test: 2018-08-04T22:45:18.559436: step 640, loss 0.549908.
Train: 2018-08-04T22:45:22.028552: step 641, loss 0.610646.
Train: 2018-08-04T22:45:25.497669: step 642, loss 0.537908.
Train: 2018-08-04T22:45:29.013665: step 643, loss 0.593106.
Train: 2018-08-04T22:45:32.529661: step 644, loss 0.538043.
Train: 2018-08-04T22:45:35.998777: step 645, loss 0.588156.
Train: 2018-08-04T22:45:39.530400: step 646, loss 0.474054.
Train: 2018-08-04T22:45:42.999516: step 647, loss 0.54636.
Train: 2018-08-04T22:45:46.531139: step 648, loss 0.528385.
Train: 2018-08-04T22:45:50.047135: step 649, loss 0.489652.
Train: 2018-08-04T22:45:53.547504: step 650, loss 0.505584.
Test: 2018-08-04T22:46:08.486581: step 650, loss 0.548141.
Train: 2018-08-04T22:46:11.971324: step 651, loss 0.498152.
Train: 2018-08-04T22:46:15.471694: step 652, loss 0.544016.
Train: 2018-08-04T22:46:18.987690: step 653, loss 0.58161.
Train: 2018-08-04T22:46:22.503686: step 654, loss 0.614534.
Train: 2018-08-04T22:46:26.019682: step 655, loss 0.5905.
Train: 2018-08-04T22:46:29.551305: step 656, loss 0.606151.
Train: 2018-08-04T22:46:33.051674: step 657, loss 0.586114.
Train: 2018-08-04T22:46:36.536417: step 658, loss 0.6065.
Train: 2018-08-04T22:46:40.052414: step 659, loss 0.534987.
Train: 2018-08-04T22:46:43.599663: step 660, loss 0.526276.
Test: 2018-08-04T22:46:58.569993: step 660, loss 0.547505.
Train: 2018-08-04T22:47:02.117242: step 661, loss 0.57111.
Train: 2018-08-04T22:47:05.633239: step 662, loss 0.623547.
Train: 2018-08-04T22:47:09.117982: step 663, loss 0.571428.
Train: 2018-08-04T22:47:12.618351: step 664, loss 0.526367.
Train: 2018-08-04T22:47:16.118721: step 665, loss 0.526454.
Train: 2018-08-04T22:47:19.603464: step 666, loss 0.537266.
Train: 2018-08-04T22:47:23.150713: step 667, loss 0.530418.
Train: 2018-08-04T22:47:26.682335: step 668, loss 0.589906.
Train: 2018-08-04T22:47:30.213958: step 669, loss 0.52795.
Train: 2018-08-04T22:47:33.698701: step 670, loss 0.520166.
Test: 2018-08-04T22:47:48.637778: step 670, loss 0.547038.
Train: 2018-08-04T22:47:52.185027: step 671, loss 0.49449.
Train: 2018-08-04T22:47:55.763530: step 672, loss 0.607319.
Train: 2018-08-04T22:47:59.310779: step 673, loss 0.564258.
Train: 2018-08-04T22:48:02.842403: step 674, loss 0.500272.
Train: 2018-08-04T22:48:06.342772: step 675, loss 0.561693.
Train: 2018-08-04T22:48:09.811888: step 676, loss 0.545886.
Train: 2018-08-04T22:48:13.312258: step 677, loss 0.608238.
Train: 2018-08-04T22:48:16.812627: step 678, loss 0.606974.
Train: 2018-08-04T22:48:20.344250: step 679, loss 0.486425.
Train: 2018-08-04T22:48:23.922753: step 680, loss 0.535354.
Test: 2018-08-04T22:48:38.846203: step 680, loss 0.549118.
Train: 2018-08-04T22:48:42.330946: step 681, loss 0.580859.
Train: 2018-08-04T22:48:45.815689: step 682, loss 0.545218.
Train: 2018-08-04T22:48:49.362938: step 683, loss 0.587286.
Train: 2018-08-04T22:48:52.863308: step 684, loss 0.554018.
Train: 2018-08-04T22:48:56.379304: step 685, loss 0.511216.
Train: 2018-08-04T22:48:59.926553: step 686, loss 0.587451.
Train: 2018-08-04T22:49:03.426923: step 687, loss 0.600094.
Train: 2018-08-04T22:49:06.911665: step 688, loss 0.54506.
Train: 2018-08-04T22:49:10.427662: step 689, loss 0.57065.
Train: 2018-08-04T22:49:13.943658: step 690, loss 0.456622.
Test: 2018-08-04T22:49:28.929615: step 690, loss 0.547329.
Train: 2018-08-04T22:49:32.445611: step 691, loss 0.50049.
Train: 2018-08-04T22:49:36.008487: step 692, loss 0.596859.
Train: 2018-08-04T22:49:39.508856: step 693, loss 0.634857.
Train: 2018-08-04T22:49:43.024853: step 694, loss 0.597917.
Train: 2018-08-04T22:49:46.525223: step 695, loss 0.573419.
Train: 2018-08-04T22:49:50.009965: step 696, loss 0.630657.
Train: 2018-08-04T22:49:53.510334: step 697, loss 0.578963.
Train: 2018-08-04T22:49:57.073210: step 698, loss 0.536542.
Train: 2018-08-04T22:50:00.557953: step 699, loss 0.596448.
Train: 2018-08-04T22:50:04.027070: step 700, loss 0.590186.
Test: 2018-08-04T22:50:19.044279: step 700, loss 0.546853.
Train: 2018-08-04T22:50:24.763634: step 701, loss 0.529011.
Train: 2018-08-04T22:50:28.248376: step 702, loss 0.570916.
Train: 2018-08-04T22:50:31.795626: step 703, loss 0.545722.
Train: 2018-08-04T22:50:35.327249: step 704, loss 0.520468.
Train: 2018-08-04T22:50:38.827618: step 705, loss 0.554618.
Train: 2018-08-04T22:50:42.312361: step 706, loss 0.656531.
Train: 2018-08-04T22:50:45.797104: step 707, loss 0.545082.
Train: 2018-08-04T22:50:49.313100: step 708, loss 0.537787.
Train: 2018-08-04T22:50:52.829096: step 709, loss 0.586294.
Train: 2018-08-04T22:50:56.329466: step 710, loss 0.61209.
Test: 2018-08-04T22:51:11.252917: step 710, loss 0.548664.
Train: 2018-08-04T22:51:14.753285: step 711, loss 0.572459.
Train: 2018-08-04T22:51:18.284908: step 712, loss 0.638985.
Train: 2018-08-04T22:51:21.785278: step 713, loss 0.537887.
Train: 2018-08-04T22:51:25.316901: step 714, loss 0.579965.
Train: 2018-08-04T22:51:28.801643: step 715, loss 0.497096.
Train: 2018-08-04T22:51:32.364519: step 716, loss 0.612465.
Train: 2018-08-04T22:51:35.880515: step 717, loss 0.563126.
Train: 2018-08-04T22:51:39.365258: step 718, loss 0.505049.
Train: 2018-08-04T22:51:42.834374: step 719, loss 0.571312.
Train: 2018-08-04T22:51:46.334744: step 720, loss 0.530541.
Test: 2018-08-04T22:52:01.289447: step 720, loss 0.548921.
Train: 2018-08-04T22:52:04.774191: step 721, loss 0.571039.
Train: 2018-08-04T22:52:08.415200: step 722, loss 0.579035.
Train: 2018-08-04T22:52:11.993702: step 723, loss 0.578795.
Train: 2018-08-04T22:52:15.509699: step 724, loss 0.488011.
Train: 2018-08-04T22:52:19.025695: step 725, loss 0.505206.
Train: 2018-08-04T22:52:22.526064: step 726, loss 0.619615.
Train: 2018-08-04T22:52:26.026434: step 727, loss 0.587402.
Train: 2018-08-04T22:52:29.542430: step 728, loss 0.487368.
Train: 2018-08-04T22:52:33.058426: step 729, loss 0.561271.
Train: 2018-08-04T22:52:36.558796: step 730, loss 0.528905.
Test: 2018-08-04T22:52:51.466620: step 730, loss 0.54796.
Train: 2018-08-04T22:52:55.029495: step 731, loss 0.510894.
Train: 2018-08-04T22:52:58.561118: step 732, loss 0.486853.
Train: 2018-08-04T22:53:02.077114: step 733, loss 0.503647.
Train: 2018-08-04T22:53:05.655617: step 734, loss 0.552856.
Train: 2018-08-04T22:53:09.218493: step 735, loss 0.639167.
Train: 2018-08-04T22:53:12.703236: step 736, loss 0.622272.
Train: 2018-08-04T22:53:16.203606: step 737, loss 0.509826.
Train: 2018-08-04T22:53:19.735228: step 738, loss 0.527152.
Train: 2018-08-04T22:53:23.282478: step 739, loss 0.569864.
Train: 2018-08-04T22:53:26.798474: step 740, loss 0.536889.
Test: 2018-08-04T22:53:41.815685: step 740, loss 0.549299.
Train: 2018-08-04T22:53:45.300427: step 741, loss 0.581044.
Train: 2018-08-04T22:53:48.785170: step 742, loss 0.561981.
Train: 2018-08-04T22:53:52.269913: step 743, loss 0.564214.
Train: 2018-08-04T22:53:55.754655: step 744, loss 0.528178.
Train: 2018-08-04T22:53:59.286278: step 745, loss 0.553845.
Train: 2018-08-04T22:54:02.802274: step 746, loss 0.599867.
Train: 2018-08-04T22:54:06.349524: step 747, loss 0.528532.
Train: 2018-08-04T22:54:09.865520: step 748, loss 0.571975.
Train: 2018-08-04T22:54:13.365889: step 749, loss 0.570527.
Train: 2018-08-04T22:54:16.850632: step 750, loss 0.535937.
Test: 2018-08-04T22:54:31.836591: step 750, loss 0.54742.
Train: 2018-08-04T22:54:35.352585: step 751, loss 0.581155.
Train: 2018-08-04T22:54:38.884208: step 752, loss 0.49135.
Train: 2018-08-04T22:54:42.462711: step 753, loss 0.64114.
Train: 2018-08-04T22:54:45.931827: step 754, loss 0.598583.
Train: 2018-08-04T22:54:47.635132: step 755, loss 0.600531.
Train: 2018-08-04T22:54:51.135501: step 756, loss 0.615287.
Train: 2018-08-04T22:54:54.667124: step 757, loss 0.500131.
Train: 2018-08-04T22:54:58.183120: step 758, loss 0.561916.
Train: 2018-08-04T22:55:01.714743: step 759, loss 0.561518.
Train: 2018-08-04T22:55:05.215112: step 760, loss 0.562045.
Test: 2018-08-04T22:55:20.154189: step 760, loss 0.547031.
Train: 2018-08-04T22:55:23.654559: step 761, loss 0.597344.
Train: 2018-08-04T22:55:27.170555: step 762, loss 0.535852.
Train: 2018-08-04T22:55:30.686551: step 763, loss 0.597005.
Train: 2018-08-04T22:55:34.249427: step 764, loss 0.605075.
Train: 2018-08-04T22:55:37.781050: step 765, loss 0.578721.
Train: 2018-08-04T22:55:41.265793: step 766, loss 0.536281.
Train: 2018-08-04T22:55:44.781789: step 767, loss 0.546812.
Train: 2018-08-04T22:55:48.266532: step 768, loss 0.546312.
Train: 2018-08-04T22:55:51.782528: step 769, loss 0.562597.
Train: 2018-08-04T22:55:55.345404: step 770, loss 0.604659.
Test: 2018-08-04T22:56:10.253228: step 770, loss 0.548007.
Train: 2018-08-04T22:56:13.737971: step 771, loss 0.562585.
Train: 2018-08-04T22:56:17.300847: step 772, loss 0.486784.
Train: 2018-08-04T22:56:20.832470: step 773, loss 0.519146.
Train: 2018-08-04T22:56:24.364092: step 774, loss 0.537074.
Train: 2018-08-04T22:56:27.880089: step 775, loss 0.5796.
Train: 2018-08-04T22:56:31.380458: step 776, loss 0.562508.
Train: 2018-08-04T22:56:34.880827: step 777, loss 0.562154.
Train: 2018-08-04T22:56:38.381197: step 778, loss 0.606006.
Train: 2018-08-04T22:56:41.881566: step 779, loss 0.569523.
Train: 2018-08-04T22:56:45.428816: step 780, loss 0.579714.
Test: 2018-08-04T22:57:00.383520: step 780, loss 0.548144.
Train: 2018-08-04T22:57:03.915142: step 781, loss 0.561582.
Train: 2018-08-04T22:57:07.415512: step 782, loss 0.613327.
Train: 2018-08-04T22:57:10.931508: step 783, loss 0.553172.
Train: 2018-08-04T22:57:14.416251: step 784, loss 0.562343.
Train: 2018-08-04T22:57:17.916620: step 785, loss 0.570322.
Train: 2018-08-04T22:57:21.416990: step 786, loss 0.578717.
Train: 2018-08-04T22:57:24.964239: step 787, loss 0.579549.
Train: 2018-08-04T22:57:28.480236: step 788, loss 0.587667.
Train: 2018-08-04T22:57:31.996231: step 789, loss 0.613103.
Train: 2018-08-04T22:57:35.496601: step 790, loss 0.579409.
Test: 2018-08-04T22:57:50.420052: step 790, loss 0.547504.
Train: 2018-08-04T22:57:53.936047: step 791, loss 0.529659.
Train: 2018-08-04T22:57:57.436417: step 792, loss 0.553605.
Train: 2018-08-04T22:58:00.936786: step 793, loss 0.512938.
Train: 2018-08-04T22:58:04.437156: step 794, loss 0.54661.
Train: 2018-08-04T22:58:08.015659: step 795, loss 0.587721.
Train: 2018-08-04T22:58:11.562908: step 796, loss 0.545338.
Train: 2018-08-04T22:58:15.110158: step 797, loss 0.528722.
Train: 2018-08-04T22:58:18.626154: step 798, loss 0.545749.
Train: 2018-08-04T22:58:22.173403: step 799, loss 0.528409.
Train: 2018-08-04T22:58:25.673773: step 800, loss 0.572062.
Test: 2018-08-04T22:58:40.675356: step 800, loss 0.549278.
Train: 2018-08-04T22:58:46.519723: step 801, loss 0.57076.
Train: 2018-08-04T22:58:50.098226: step 802, loss 0.502839.
Train: 2018-08-04T22:58:53.567343: step 803, loss 0.638608.
Train: 2018-08-04T22:58:57.052085: step 804, loss 0.546322.
Train: 2018-08-04T22:59:00.599334: step 805, loss 0.595612.
Train: 2018-08-04T22:59:04.162210: step 806, loss 0.569452.
Train: 2018-08-04T22:59:07.678207: step 807, loss 0.570866.
Train: 2018-08-04T22:59:11.256709: step 808, loss 0.613392.
Train: 2018-08-04T22:59:14.741452: step 809, loss 0.468343.
Train: 2018-08-04T22:59:18.241821: step 810, loss 0.461258.
Test: 2018-08-04T22:59:33.368419: step 810, loss 0.548268.
Train: 2018-08-04T22:59:36.915667: step 811, loss 0.597194.
Train: 2018-08-04T22:59:40.416037: step 812, loss 0.553807.
Train: 2018-08-04T22:59:43.978913: step 813, loss 0.596073.
Train: 2018-08-04T22:59:47.510536: step 814, loss 0.614102.
Train: 2018-08-04T22:59:50.995279: step 815, loss 0.57158.
Train: 2018-08-04T22:59:54.511275: step 816, loss 0.587238.
Train: 2018-08-04T22:59:57.996018: step 817, loss 0.493744.
Train: 2018-08-04T23:00:01.480761: step 818, loss 0.570616.
Train: 2018-08-04T23:00:04.996757: step 819, loss 0.589018.
Train: 2018-08-04T23:00:08.512753: step 820, loss 0.578626.
Test: 2018-08-04T23:00:23.514337: step 820, loss 0.549385.
Train: 2018-08-04T23:00:27.045959: step 821, loss 0.630351.
Train: 2018-08-04T23:00:30.546329: step 822, loss 0.493399.
Train: 2018-08-04T23:00:34.062325: step 823, loss 0.585652.
Train: 2018-08-04T23:00:37.531441: step 824, loss 0.571323.
Train: 2018-08-04T23:00:41.109944: step 825, loss 0.503384.
Train: 2018-08-04T23:00:44.610314: step 826, loss 0.544999.
Train: 2018-08-04T23:00:48.126309: step 827, loss 0.503044.
Train: 2018-08-04T23:00:51.642306: step 828, loss 0.510228.
Train: 2018-08-04T23:00:55.158302: step 829, loss 0.561312.
Train: 2018-08-04T23:00:58.674298: step 830, loss 0.588472.
Test: 2018-08-04T23:01:13.629002: step 830, loss 0.546875.
Train: 2018-08-04T23:01:17.160624: step 831, loss 0.467805.
Train: 2018-08-04T23:01:20.723500: step 832, loss 0.501704.
Train: 2018-08-04T23:01:24.239496: step 833, loss 0.58018.
Train: 2018-08-04T23:01:27.724239: step 834, loss 0.615756.
Train: 2018-08-04T23:01:31.224609: step 835, loss 0.535864.
Train: 2018-08-04T23:01:34.724979: step 836, loss 0.510224.
Train: 2018-08-04T23:01:38.287855: step 837, loss 0.581589.
Train: 2018-08-04T23:01:41.835104: step 838, loss 0.608444.
Train: 2018-08-04T23:01:45.319847: step 839, loss 0.517971.
Train: 2018-08-04T23:01:48.835843: step 840, loss 0.53667.
Test: 2018-08-04T23:02:03.821800: step 840, loss 0.546807.
Train: 2018-08-04T23:02:07.337796: step 841, loss 0.6416.
Train: 2018-08-04T23:02:10.838165: step 842, loss 0.527615.
Train: 2018-08-04T23:02:14.338535: step 843, loss 0.650894.
Train: 2018-08-04T23:02:17.901411: step 844, loss 0.518673.
Train: 2018-08-04T23:02:21.401781: step 845, loss 0.561643.
Train: 2018-08-04T23:02:24.902150: step 846, loss 0.500416.
Train: 2018-08-04T23:02:28.386893: step 847, loss 0.616144.
Train: 2018-08-04T23:02:31.887262: step 848, loss 0.563488.
Train: 2018-08-04T23:02:35.387632: step 849, loss 0.553262.
Train: 2018-08-04T23:02:38.966135: step 850, loss 0.562382.
Test: 2018-08-04T23:02:53.936466: step 850, loss 0.547952.
Train: 2018-08-04T23:02:57.483714: step 851, loss 0.64145.
Train: 2018-08-04T23:03:00.984084: step 852, loss 0.483189.
Train: 2018-08-04T23:03:04.500080: step 853, loss 0.570913.
Train: 2018-08-04T23:03:08.000450: step 854, loss 0.552764.
Train: 2018-08-04T23:03:11.547699: step 855, loss 0.596942.
Train: 2018-08-04T23:03:15.126202: step 856, loss 0.553879.
Train: 2018-08-04T23:03:18.626571: step 857, loss 0.563183.
Train: 2018-08-04T23:03:22.126941: step 858, loss 0.493556.
Train: 2018-08-04T23:03:25.689817: step 859, loss 0.569418.
Train: 2018-08-04T23:03:29.205813: step 860, loss 0.596154.
Test: 2018-08-04T23:03:44.254276: step 860, loss 0.548111.
Train: 2018-08-04T23:03:47.770272: step 861, loss 0.527616.
Train: 2018-08-04T23:03:51.333149: step 862, loss 0.545123.
Train: 2018-08-04T23:03:54.849145: step 863, loss 0.674226.
Train: 2018-08-04T23:03:58.365141: step 864, loss 0.58042.
Train: 2018-08-04T23:04:01.881138: step 865, loss 0.5708.
Train: 2018-08-04T23:04:05.381506: step 866, loss 0.699856.
Train: 2018-08-04T23:04:08.897502: step 867, loss 0.502511.
Train: 2018-08-04T23:04:12.444752: step 868, loss 0.536805.
Train: 2018-08-04T23:04:15.913868: step 869, loss 0.630163.
Train: 2018-08-04T23:04:19.429864: step 870, loss 0.663724.
Test: 2018-08-04T23:04:34.368945: step 870, loss 0.548257.
Train: 2018-08-04T23:04:37.869311: step 871, loss 0.537698.
Train: 2018-08-04T23:04:41.385307: step 872, loss 0.479118.
Train: 2018-08-04T23:04:44.854424: step 873, loss 0.611711.
Train: 2018-08-04T23:04:48.479806: step 874, loss 0.512912.
Train: 2018-08-04T23:04:53.042788: step 875, loss 0.522528.
Train: 2018-08-04T23:04:57.480756: step 876, loss 0.586877.
Train: 2018-08-04T23:05:01.434306: step 877, loss 0.521146.
Train: 2018-08-04T23:05:04.950298: step 878, loss 0.529922.
Train: 2018-08-04T23:05:08.481917: step 879, loss 0.512322.
Train: 2018-08-04T23:05:12.044794: step 880, loss 0.512858.
Test: 2018-08-04T23:05:27.124510: step 880, loss 0.549649.
Train: 2018-08-04T23:05:30.624880: step 881, loss 0.546153.
Train: 2018-08-04T23:05:34.140875: step 882, loss 0.511926.
Train: 2018-08-04T23:05:37.609992: step 883, loss 0.578779.
Train: 2018-08-04T23:05:41.110362: step 884, loss 0.528349.
Train: 2018-08-04T23:05:44.595104: step 885, loss 0.59568.
Train: 2018-08-04T23:05:48.111101: step 886, loss 0.672791.
Train: 2018-08-04T23:05:51.658350: step 887, loss 0.536948.
Train: 2018-08-04T23:05:55.143093: step 888, loss 0.572757.
Train: 2018-08-04T23:05:58.659089: step 889, loss 0.613699.
Train: 2018-08-04T23:06:02.175085: step 890, loss 0.579449.
Test: 2018-08-04T23:06:17.192295: step 890, loss 0.548279.
Train: 2018-08-04T23:06:20.708291: step 891, loss 0.519692.
Train: 2018-08-04T23:06:24.239914: step 892, loss 0.579579.
Train: 2018-08-04T23:06:27.755910: step 893, loss 0.56137.
Train: 2018-08-04T23:06:31.225027: step 894, loss 0.587753.
Train: 2018-08-04T23:06:34.725396: step 895, loss 0.562246.
Train: 2018-08-04T23:06:38.225767: step 896, loss 0.476679.
Train: 2018-08-04T23:06:41.741761: step 897, loss 0.528739.
Train: 2018-08-04T23:06:45.257758: step 898, loss 0.587995.
Train: 2018-08-04T23:06:48.773754: step 899, loss 0.520405.
Train: 2018-08-04T23:06:52.289750: step 900, loss 0.570748.
Test: 2018-08-04T23:07:07.275706: step 900, loss 0.548382.
Train: 2018-08-04T23:07:12.948181: step 901, loss 0.597209.
Train: 2018-08-04T23:07:16.432924: step 902, loss 0.5274.
Train: 2018-08-04T23:07:19.917666: step 903, loss 0.579917.
Train: 2018-08-04T23:07:23.433662: step 904, loss 0.613501.
Train: 2018-08-04T23:07:26.934032: step 905, loss 0.527869.
Train: 2018-08-04T23:07:28.637337: step 906, loss 0.543626.
Train: 2018-08-04T23:07:32.137707: step 907, loss 0.648817.
Train: 2018-08-04T23:07:35.653702: step 908, loss 0.562054.
Train: 2018-08-04T23:07:39.107192: step 909, loss 0.605694.
Train: 2018-08-04T23:07:42.623188: step 910, loss 0.5191.
Test: 2018-08-04T23:07:57.546639: step 910, loss 0.548644.
Train: 2018-08-04T23:08:01.062635: step 911, loss 0.579714.
Train: 2018-08-04T23:08:04.563004: step 912, loss 0.579306.
Train: 2018-08-04T23:08:08.047747: step 913, loss 0.59698.
Train: 2018-08-04T23:08:11.532490: step 914, loss 0.503356.
Train: 2018-08-04T23:08:15.032859: step 915, loss 0.612803.
Train: 2018-08-04T23:08:18.548855: step 916, loss 0.553923.
Train: 2018-08-04T23:08:22.064852: step 917, loss 0.596415.
Train: 2018-08-04T23:08:25.565221: step 918, loss 0.537264.
Train: 2018-08-04T23:08:29.065590: step 919, loss 0.477862.
Train: 2018-08-04T23:08:32.550333: step 920, loss 0.520204.
Test: 2018-08-04T23:08:47.473784: step 920, loss 0.548237.
Train: 2018-08-04T23:08:50.942900: step 921, loss 0.478698.
Train: 2018-08-04T23:08:54.552656: step 922, loss 0.622916.
Train: 2018-08-04T23:08:58.084278: step 923, loss 0.570156.
Train: 2018-08-04T23:09:01.584648: step 924, loss 0.656836.
Train: 2018-08-04T23:09:05.069391: step 925, loss 0.527695.
Train: 2018-08-04T23:09:08.554134: step 926, loss 0.579426.
Train: 2018-08-04T23:09:12.038877: step 927, loss 0.579171.
Train: 2018-08-04T23:09:15.617379: step 928, loss 0.621485.
Train: 2018-08-04T23:09:19.149007: step 929, loss 0.637974.
Train: 2018-08-04T23:09:22.633745: step 930, loss 0.588391.
Test: 2018-08-04T23:09:37.572823: step 930, loss 0.54754.
Train: 2018-08-04T23:09:41.088818: step 931, loss 0.571096.
Train: 2018-08-04T23:09:44.589188: step 932, loss 0.587976.
Train: 2018-08-04T23:09:48.073931: step 933, loss 0.504314.
Train: 2018-08-04T23:09:51.589926: step 934, loss 0.528972.
Train: 2018-08-04T23:09:55.121549: step 935, loss 0.579195.
Train: 2018-08-04T23:09:58.621919: step 936, loss 0.480291.
Train: 2018-08-04T23:10:02.122289: step 937, loss 0.579771.
Train: 2018-08-04T23:10:05.653911: step 938, loss 0.586314.
Train: 2018-08-04T23:10:09.138654: step 939, loss 0.512508.
Train: 2018-08-04T23:10:12.639023: step 940, loss 0.579425.
Test: 2018-08-04T23:10:27.593727: step 940, loss 0.547884.
Train: 2018-08-04T23:10:31.109723: step 941, loss 0.59571.
Train: 2018-08-04T23:10:34.625720: step 942, loss 0.612329.
Train: 2018-08-04T23:10:38.126089: step 943, loss 0.561891.
Train: 2018-08-04T23:10:41.610832: step 944, loss 0.45333.
Train: 2018-08-04T23:10:45.079948: step 945, loss 0.553785.
Train: 2018-08-04T23:10:48.564691: step 946, loss 0.57088.
Train: 2018-08-04T23:10:52.080687: step 947, loss 0.536751.
Train: 2018-08-04T23:10:55.612310: step 948, loss 0.562211.
Train: 2018-08-04T23:10:59.128306: step 949, loss 0.604971.
Train: 2018-08-04T23:11:02.628676: step 950, loss 0.494871.
Test: 2018-08-04T23:11:17.599005: step 950, loss 0.548177.
Train: 2018-08-04T23:11:21.115002: step 951, loss 0.623527.
Train: 2018-08-04T23:11:24.599745: step 952, loss 0.520198.
Train: 2018-08-04T23:11:28.131368: step 953, loss 0.519919.
Train: 2018-08-04T23:11:31.662990: step 954, loss 0.621017.
Train: 2018-08-04T23:11:35.163360: step 955, loss 0.562283.
Train: 2018-08-04T23:11:38.648103: step 956, loss 0.528498.
Train: 2018-08-04T23:11:42.132845: step 957, loss 0.613741.
Train: 2018-08-04T23:11:45.601962: step 958, loss 0.595232.
Train: 2018-08-04T23:11:49.149211: step 959, loss 0.639716.
Train: 2018-08-04T23:11:52.665207: step 960, loss 0.494796.
Test: 2018-08-04T23:12:07.635538: step 960, loss 0.548681.
Train: 2018-08-04T23:12:11.151534: step 961, loss 0.587764.
Train: 2018-08-04T23:12:14.620650: step 962, loss 0.63926.
Train: 2018-08-04T23:12:18.152272: step 963, loss 0.553437.
Train: 2018-08-04T23:12:21.637015: step 964, loss 0.529427.
Train: 2018-08-04T23:12:25.137385: step 965, loss 0.511294.
Train: 2018-08-04T23:12:28.669007: step 966, loss 0.553818.
Train: 2018-08-04T23:12:32.122498: step 967, loss 0.613568.
Train: 2018-08-04T23:12:35.638493: step 968, loss 0.553931.
Train: 2018-08-04T23:12:39.123236: step 969, loss 0.545341.
Train: 2018-08-04T23:12:42.592354: step 970, loss 0.469205.
Test: 2018-08-04T23:12:57.578309: step 970, loss 0.547622.
Train: 2018-08-04T23:13:01.125558: step 971, loss 0.612954.
Train: 2018-08-04T23:13:04.657181: step 972, loss 0.51978.
Train: 2018-08-04T23:13:08.188804: step 973, loss 0.562487.
Train: 2018-08-04T23:13:11.673547: step 974, loss 0.630274.
Train: 2018-08-04T23:13:15.158290: step 975, loss 0.538066.
Train: 2018-08-04T23:13:18.658659: step 976, loss 0.571041.
Train: 2018-08-04T23:13:22.174656: step 977, loss 0.537589.
Train: 2018-08-04T23:13:25.706279: step 978, loss 0.638358.
Train: 2018-08-04T23:13:29.206648: step 979, loss 0.570971.
Train: 2018-08-04T23:13:32.785151: step 980, loss 0.587835.
Test: 2018-08-04T23:13:47.849240: step 980, loss 0.547992.
Train: 2018-08-04T23:13:51.349610: step 981, loss 0.544971.
Train: 2018-08-04T23:13:54.881233: step 982, loss 0.57936.
Train: 2018-08-04T23:13:58.444109: step 983, loss 0.536964.
Train: 2018-08-04T23:14:01.928852: step 984, loss 0.554022.
Train: 2018-08-04T23:14:05.444848: step 985, loss 0.570552.
Train: 2018-08-04T23:14:08.929591: step 986, loss 0.519884.
Train: 2018-08-04T23:14:12.445588: step 987, loss 0.578953.
Train: 2018-08-04T23:14:15.961583: step 988, loss 0.51937.
Train: 2018-08-04T23:14:19.602593: step 989, loss 0.579592.
Train: 2018-08-04T23:14:23.134216: step 990, loss 0.553954.
Test: 2018-08-04T23:14:38.120174: step 990, loss 0.548942.
Train: 2018-08-04T23:14:41.651795: step 991, loss 0.529314.
Train: 2018-08-04T23:14:45.152165: step 992, loss 0.587866.
Train: 2018-08-04T23:14:48.652534: step 993, loss 0.562447.
Train: 2018-08-04T23:14:52.137277: step 994, loss 0.476413.
Train: 2018-08-04T23:14:55.622020: step 995, loss 0.459819.
Train: 2018-08-04T23:14:59.200523: step 996, loss 0.596472.
Train: 2018-08-04T23:15:02.716519: step 997, loss 0.510198.
Train: 2018-08-04T23:15:06.248141: step 998, loss 0.535929.
Train: 2018-08-04T23:15:09.764137: step 999, loss 0.509871.
Train: 2018-08-04T23:15:13.233254: step 1000, loss 0.595482.
Test: 2018-08-04T23:15:28.266091: step 1000, loss 0.547719.
Train: 2018-08-04T23:15:34.063578: step 1001, loss 0.545461.
Train: 2018-08-04T23:15:37.563947: step 1002, loss 0.606123.
Train: 2018-08-04T23:15:41.079943: step 1003, loss 0.473567.
Train: 2018-08-04T23:15:44.595939: step 1004, loss 0.606127.
Train: 2018-08-04T23:15:48.065056: step 1005, loss 0.553543.
Train: 2018-08-04T23:15:51.565425: step 1006, loss 0.527119.
Train: 2018-08-04T23:15:55.050168: step 1007, loss 0.589844.
Train: 2018-08-04T23:15:58.628670: step 1008, loss 0.48103.
Train: 2018-08-04T23:16:02.175920: step 1009, loss 0.553392.
Train: 2018-08-04T23:16:05.645036: step 1010, loss 0.634702.
Test: 2018-08-04T23:16:20.646620: step 1010, loss 0.547354.
Train: 2018-08-04T23:16:24.131362: step 1011, loss 0.553959.
Train: 2018-08-04T23:16:27.678612: step 1012, loss 0.562596.
Train: 2018-08-04T23:16:31.163355: step 1013, loss 0.554473.
Train: 2018-08-04T23:16:34.663725: step 1014, loss 0.56356.
Train: 2018-08-04T23:16:38.164094: step 1015, loss 0.515648.
Train: 2018-08-04T23:16:41.664463: step 1016, loss 0.553649.
Train: 2018-08-04T23:16:45.133579: step 1017, loss 0.5451.
Train: 2018-08-04T23:16:48.618322: step 1018, loss 0.56229.
Train: 2018-08-04T23:16:52.103065: step 1019, loss 0.527076.
Train: 2018-08-04T23:16:55.603435: step 1020, loss 0.63664.
Test: 2018-08-04T23:17:10.620646: step 1020, loss 0.546948.
Train: 2018-08-04T23:17:14.136641: step 1021, loss 0.49772.
Train: 2018-08-04T23:17:17.652637: step 1022, loss 0.652941.
Train: 2018-08-04T23:17:21.137380: step 1023, loss 0.563105.
Train: 2018-08-04T23:17:24.637750: step 1024, loss 0.527868.
Train: 2018-08-04T23:17:28.153745: step 1025, loss 0.582825.
Train: 2018-08-04T23:17:31.669742: step 1026, loss 0.596737.
Train: 2018-08-04T23:17:35.185738: step 1027, loss 0.589519.
Train: 2018-08-04T23:17:38.654854: step 1028, loss 0.597459.
Train: 2018-08-04T23:17:42.186477: step 1029, loss 0.543921.
Train: 2018-08-04T23:17:45.639967: step 1030, loss 0.528052.
Test: 2018-08-04T23:18:00.594670: step 1030, loss 0.548392.
Train: 2018-08-04T23:18:04.095040: step 1031, loss 0.536216.
Train: 2018-08-04T23:18:07.626663: step 1032, loss 0.562275.
Train: 2018-08-04T23:18:11.142659: step 1033, loss 0.528366.
Train: 2018-08-04T23:18:14.627401: step 1034, loss 0.528144.
Train: 2018-08-04T23:18:18.127771: step 1035, loss 0.579767.
Train: 2018-08-04T23:18:21.628140: step 1036, loss 0.579996.
Train: 2018-08-04T23:18:25.128510: step 1037, loss 0.553454.
Train: 2018-08-04T23:18:28.660133: step 1038, loss 0.605396.
Train: 2018-08-04T23:18:32.191755: step 1039, loss 0.578728.
Train: 2018-08-04T23:18:35.692125: step 1040, loss 0.595696.
Test: 2018-08-04T23:18:50.646828: step 1040, loss 0.547983.
Train: 2018-08-04T23:18:54.162824: step 1041, loss 0.605988.
Train: 2018-08-04T23:18:57.663194: step 1042, loss 0.604851.
Train: 2018-08-04T23:19:01.163564: step 1043, loss 0.543969.
Train: 2018-08-04T23:19:04.695186: step 1044, loss 0.519773.
Train: 2018-08-04T23:19:08.211183: step 1045, loss 0.563667.
Train: 2018-08-04T23:19:11.695925: step 1046, loss 0.579084.
Train: 2018-08-04T23:19:15.180668: step 1047, loss 0.50296.
Train: 2018-08-04T23:19:18.712292: step 1048, loss 0.495121.
Train: 2018-08-04T23:19:22.212661: step 1049, loss 0.579043.
Train: 2018-08-04T23:19:25.713030: step 1050, loss 0.570103.
Test: 2018-08-04T23:19:40.714615: step 1050, loss 0.548763.
Train: 2018-08-04T23:19:44.246236: step 1051, loss 0.571039.
Train: 2018-08-04T23:19:47.762232: step 1052, loss 0.604583.
Train: 2018-08-04T23:19:51.231349: step 1053, loss 0.621567.
Train: 2018-08-04T23:19:54.716092: step 1054, loss 0.494241.
Train: 2018-08-04T23:19:58.216461: step 1055, loss 0.596494.
Train: 2018-08-04T23:20:01.732457: step 1056, loss 0.544668.
Train: 2018-08-04T23:20:03.435762: step 1057, loss 0.507973.
Train: 2018-08-04T23:20:06.936131: step 1058, loss 0.570009.
Train: 2018-08-04T23:20:10.436501: step 1059, loss 0.570526.
Train: 2018-08-04T23:20:13.921244: step 1060, loss 0.545948.
Test: 2018-08-04T23:20:28.969707: step 1060, loss 0.546372.
Train: 2018-08-04T23:20:32.470077: step 1061, loss 0.595751.
Train: 2018-08-04T23:20:36.017326: step 1062, loss 0.536515.
Train: 2018-08-04T23:20:39.517696: step 1063, loss 0.579795.
Train: 2018-08-04T23:20:43.002439: step 1064, loss 0.52859.
Train: 2018-08-04T23:20:46.502808: step 1065, loss 0.56979.
Train: 2018-08-04T23:20:49.987551: step 1066, loss 0.580225.
Train: 2018-08-04T23:20:53.487920: step 1067, loss 0.57167.
Train: 2018-08-04T23:20:57.003916: step 1068, loss 0.537299.
Train: 2018-08-04T23:21:00.535540: step 1069, loss 0.493927.
Train: 2018-08-04T23:21:04.035909: step 1070, loss 0.59618.
Test: 2018-08-04T23:21:19.084373: step 1070, loss 0.548793.
Train: 2018-08-04T23:21:22.584742: step 1071, loss 0.579367.
Train: 2018-08-04T23:21:26.131992: step 1072, loss 0.596313.
Train: 2018-08-04T23:21:29.632361: step 1073, loss 0.579684.
Train: 2018-08-04T23:21:33.148357: step 1074, loss 0.467793.
Train: 2018-08-04T23:21:36.664354: step 1075, loss 0.570945.
Train: 2018-08-04T23:21:40.164723: step 1076, loss 0.571698.
Train: 2018-08-04T23:21:43.665093: step 1077, loss 0.501407.
Train: 2018-08-04T23:21:47.165462: step 1078, loss 0.570952.
Train: 2018-08-04T23:21:50.665831: step 1079, loss 0.630924.
Train: 2018-08-04T23:21:54.166201: step 1080, loss 0.545732.
Test: 2018-08-04T23:22:09.230290: step 1080, loss 0.547309.
Train: 2018-08-04T23:22:12.793167: step 1081, loss 0.535224.
Train: 2018-08-04T23:22:16.309163: step 1082, loss 0.56261.
Train: 2018-08-04T23:22:19.825159: step 1083, loss 0.587892.
Train: 2018-08-04T23:22:23.325528: step 1084, loss 0.615106.
Train: 2018-08-04T23:22:26.825898: step 1085, loss 0.510441.
Train: 2018-08-04T23:22:30.341894: step 1086, loss 0.554429.
Train: 2018-08-04T23:22:33.873517: step 1087, loss 0.552618.
Train: 2018-08-04T23:22:37.358260: step 1088, loss 0.510853.
Train: 2018-08-04T23:22:40.843003: step 1089, loss 0.536239.
Train: 2018-08-04T23:22:44.374625: step 1090, loss 0.553309.
Test: 2018-08-04T23:22:59.329328: step 1090, loss 0.546646.
Train: 2018-08-04T23:23:02.845325: step 1091, loss 0.587808.
Train: 2018-08-04T23:23:06.345695: step 1092, loss 0.501662.
Train: 2018-08-04T23:23:09.877318: step 1093, loss 0.517726.
Train: 2018-08-04T23:23:13.393313: step 1094, loss 0.588586.
Train: 2018-08-04T23:23:16.893684: step 1095, loss 0.544519.
Train: 2018-08-04T23:23:20.378426: step 1096, loss 0.607457.
Train: 2018-08-04T23:23:23.863169: step 1097, loss 0.529233.
Train: 2018-08-04T23:23:27.410418: step 1098, loss 0.570529.
Train: 2018-08-04T23:23:30.926414: step 1099, loss 0.561532.
Train: 2018-08-04T23:23:34.458038: step 1100, loss 0.684472.
Test: 2018-08-04T23:23:49.459622: step 1100, loss 0.547937.
Train: 2018-08-04T23:23:55.897800: step 1101, loss 0.518282.
Train: 2018-08-04T23:23:59.663823: step 1102, loss 0.545069.
Train: 2018-08-04T23:24:03.257953: step 1103, loss 0.553105.
Train: 2018-08-04T23:24:06.930215: step 1104, loss 0.631465.
Train: 2018-08-04T23:24:10.539971: step 1105, loss 0.491017.
Train: 2018-08-04T23:24:14.102847: step 1106, loss 0.57021.
Train: 2018-08-04T23:24:17.618843: step 1107, loss 0.58091.
Train: 2018-08-04T23:24:21.134839: step 1108, loss 0.613092.
Train: 2018-08-04T23:24:24.635209: step 1109, loss 0.58953.
Train: 2018-08-04T23:24:28.119952: step 1110, loss 0.605628.
Test: 2018-08-04T23:24:43.137162: step 1110, loss 0.547925.
Train: 2018-08-04T23:24:46.684411: step 1111, loss 0.588187.
Train: 2018-08-04T23:24:50.216034: step 1112, loss 0.546573.
Train: 2018-08-04T23:24:53.716404: step 1113, loss 0.571971.
Train: 2018-08-04T23:24:57.232400: step 1114, loss 0.58792.
Train: 2018-08-04T23:25:00.764023: step 1115, loss 0.571607.
Train: 2018-08-04T23:25:04.326899: step 1116, loss 0.562644.
Train: 2018-08-04T23:25:07.874148: step 1117, loss 0.648546.
Train: 2018-08-04T23:25:11.421398: step 1118, loss 0.586769.
Train: 2018-08-04T23:25:14.921767: step 1119, loss 0.560957.
Train: 2018-08-04T23:25:18.406510: step 1120, loss 0.595691.
Test: 2018-08-04T23:25:33.361214: step 1120, loss 0.549448.
Train: 2018-08-04T23:25:36.877210: step 1121, loss 0.495694.
Train: 2018-08-04T23:25:40.408832: step 1122, loss 0.572166.
Train: 2018-08-04T23:25:43.956081: step 1123, loss 0.538969.
Train: 2018-08-04T23:25:47.472078: step 1124, loss 0.52955.
Train: 2018-08-04T23:25:50.988074: step 1125, loss 0.602838.
Train: 2018-08-04T23:25:54.472817: step 1126, loss 0.546355.
Train: 2018-08-04T23:25:57.973187: step 1127, loss 0.603398.
Train: 2018-08-04T23:26:01.489183: step 1128, loss 0.505087.
Train: 2018-08-04T23:26:05.020805: step 1129, loss 0.545868.
Train: 2018-08-04T23:26:08.552428: step 1130, loss 0.505277.
Test: 2018-08-04T23:26:23.679025: step 1130, loss 0.547995.
Train: 2018-08-04T23:26:27.195021: step 1131, loss 0.488354.
Train: 2018-08-04T23:26:30.711017: step 1132, loss 0.496371.
Train: 2018-08-04T23:26:34.195760: step 1133, loss 0.512259.
Train: 2018-08-04T23:26:37.680503: step 1134, loss 0.512547.
Train: 2018-08-04T23:26:41.196499: step 1135, loss 0.563263.
Train: 2018-08-04T23:26:44.728122: step 1136, loss 0.527795.
Train: 2018-08-04T23:26:48.212865: step 1137, loss 0.604492.
Train: 2018-08-04T23:26:51.713234: step 1138, loss 0.5791.
Train: 2018-08-04T23:26:55.213604: step 1139, loss 0.527628.
Train: 2018-08-04T23:26:58.698346: step 1140, loss 0.537033.
Test: 2018-08-04T23:27:13.699929: step 1140, loss 0.548606.
Train: 2018-08-04T23:27:17.262806: step 1141, loss 0.466289.
Train: 2018-08-04T23:27:20.763175: step 1142, loss 0.597054.
Train: 2018-08-04T23:27:24.263545: step 1143, loss 0.544931.
Train: 2018-08-04T23:27:27.779541: step 1144, loss 0.5618.
Train: 2018-08-04T23:27:31.279911: step 1145, loss 0.490875.
Train: 2018-08-04T23:27:34.764654: step 1146, loss 0.607104.
Train: 2018-08-04T23:27:38.280650: step 1147, loss 0.553147.
Train: 2018-08-04T23:27:41.827899: step 1148, loss 0.57261.
Train: 2018-08-04T23:27:45.343896: step 1149, loss 0.59805.
Train: 2018-08-04T23:27:48.844265: step 1150, loss 0.583178.
Test: 2018-08-04T23:28:03.830222: step 1150, loss 0.550054.
Train: 2018-08-04T23:28:07.471231: step 1151, loss 0.598344.
Train: 2018-08-04T23:28:10.955974: step 1152, loss 0.53753.
Train: 2018-08-04T23:28:14.487596: step 1153, loss 0.688479.
Train: 2018-08-04T23:28:18.050473: step 1154, loss 0.624779.
Train: 2018-08-04T23:28:21.566469: step 1155, loss 0.535469.
Train: 2018-08-04T23:28:25.066838: step 1156, loss 0.633303.
Train: 2018-08-04T23:28:28.567208: step 1157, loss 0.562088.
Train: 2018-08-04T23:28:32.051951: step 1158, loss 0.502754.
Train: 2018-08-04T23:28:35.567947: step 1159, loss 0.510521.
Train: 2018-08-04T23:28:39.115196: step 1160, loss 0.605736.
Test: 2018-08-04T23:28:54.273047: step 1160, loss 0.547687.
Train: 2018-08-04T23:28:57.757789: step 1161, loss 0.588016.
Train: 2018-08-04T23:29:01.242532: step 1162, loss 0.553966.
Train: 2018-08-04T23:29:04.774155: step 1163, loss 0.579287.
Train: 2018-08-04T23:29:08.274524: step 1164, loss 0.588332.
Train: 2018-08-04T23:29:11.790520: step 1165, loss 0.527709.
Train: 2018-08-04T23:29:15.322143: step 1166, loss 0.493638.
Train: 2018-08-04T23:29:18.822513: step 1167, loss 0.519756.
Train: 2018-08-04T23:29:22.354136: step 1168, loss 0.570237.
Train: 2018-08-04T23:29:25.854505: step 1169, loss 0.501995.
Train: 2018-08-04T23:29:29.354874: step 1170, loss 0.604669.
Test: 2018-08-04T23:29:44.325205: step 1170, loss 0.54806.
Train: 2018-08-04T23:29:47.856827: step 1171, loss 0.613894.
Train: 2018-08-04T23:29:51.419703: step 1172, loss 0.562859.
Train: 2018-08-04T23:29:54.920073: step 1173, loss 0.647284.
Train: 2018-08-04T23:29:58.436069: step 1174, loss 0.571205.
Train: 2018-08-04T23:30:01.920812: step 1175, loss 0.528173.
Train: 2018-08-04T23:30:05.421182: step 1176, loss 0.630119.
Train: 2018-08-04T23:30:08.905925: step 1177, loss 0.612583.
Train: 2018-08-04T23:30:12.437547: step 1178, loss 0.51204.
Train: 2018-08-04T23:30:15.937917: step 1179, loss 0.578948.
Train: 2018-08-04T23:30:19.453913: step 1180, loss 0.521278.
Test: 2018-08-04T23:30:34.424243: step 1180, loss 0.548755.
Train: 2018-08-04T23:30:37.908986: step 1181, loss 0.545188.
Train: 2018-08-04T23:30:41.409355: step 1182, loss 0.553553.
Train: 2018-08-04T23:30:44.925352: step 1183, loss 0.587405.
Train: 2018-08-04T23:30:48.456974: step 1184, loss 0.536926.
Train: 2018-08-04T23:30:51.957344: step 1185, loss 0.536768.
Train: 2018-08-04T23:30:55.473340: step 1186, loss 0.604779.
Train: 2018-08-04T23:30:58.989336: step 1187, loss 0.536557.
Train: 2018-08-04T23:31:02.520959: step 1188, loss 0.486018.
Train: 2018-08-04T23:31:06.005702: step 1189, loss 0.545366.
Train: 2018-08-04T23:31:09.537324: step 1190, loss 0.49526.
Test: 2018-08-04T23:31:24.476401: step 1190, loss 0.547378.
Train: 2018-08-04T23:31:27.992398: step 1191, loss 0.511187.
Train: 2018-08-04T23:31:31.539647: step 1192, loss 0.630457.
Train: 2018-08-04T23:31:35.055644: step 1193, loss 0.545137.
Train: 2018-08-04T23:31:38.524760: step 1194, loss 0.553527.
Train: 2018-08-04T23:31:42.009503: step 1195, loss 0.596777.
Train: 2018-08-04T23:31:45.509872: step 1196, loss 0.561395.
Train: 2018-08-04T23:31:49.041495: step 1197, loss 0.570012.
Train: 2018-08-04T23:31:52.573117: step 1198, loss 0.640674.
Train: 2018-08-04T23:31:56.042234: step 1199, loss 0.570998.
Train: 2018-08-04T23:31:59.558229: step 1200, loss 0.562093.
Test: 2018-08-04T23:32:14.622321: step 1200, loss 0.547907.
Train: 2018-08-04T23:32:20.279167: step 1201, loss 0.59684.
Train: 2018-08-04T23:32:23.795163: step 1202, loss 0.562078.
Train: 2018-08-04T23:32:27.311160: step 1203, loss 0.605092.
Train: 2018-08-04T23:32:30.795902: step 1204, loss 0.46649.
Train: 2018-08-04T23:32:34.265018: step 1205, loss 0.501614.
Train: 2018-08-04T23:32:37.749761: step 1206, loss 0.631706.
Train: 2018-08-04T23:32:41.250131: step 1207, loss 0.596955.
Train: 2018-08-04T23:32:42.969062: step 1208, loss 0.544608.
Train: 2018-08-04T23:32:46.500685: step 1209, loss 0.578841.
Train: 2018-08-04T23:32:49.985427: step 1210, loss 0.553981.
Test: 2018-08-04T23:33:04.971384: step 1210, loss 0.547005.
Train: 2018-08-04T23:33:08.456127: step 1211, loss 0.605518.
Train: 2018-08-04T23:33:11.925244: step 1212, loss 0.596756.
Train: 2018-08-04T23:33:15.441240: step 1213, loss 0.536944.
Train: 2018-08-04T23:33:18.972863: step 1214, loss 0.595972.
Train: 2018-08-04T23:33:22.504486: step 1215, loss 0.519694.
Train: 2018-08-04T23:33:26.036108: step 1216, loss 0.552331.
Train: 2018-08-04T23:33:29.536478: step 1217, loss 0.484506.
Train: 2018-08-04T23:33:33.068101: step 1218, loss 0.503113.
Train: 2018-08-04T23:33:36.584096: step 1219, loss 0.509853.
Train: 2018-08-04T23:33:40.115720: step 1220, loss 0.561509.
Test: 2018-08-04T23:33:55.117304: step 1220, loss 0.547154.
Train: 2018-08-04T23:33:58.602046: step 1221, loss 0.605601.
Train: 2018-08-04T23:34:02.118042: step 1222, loss 0.578476.
Train: 2018-08-04T23:34:05.634038: step 1223, loss 0.535121.
Train: 2018-08-04T23:34:09.118781: step 1224, loss 0.588079.
Train: 2018-08-04T23:34:12.634777: step 1225, loss 0.595432.
Train: 2018-08-04T23:34:16.150773: step 1226, loss 0.535565.
Train: 2018-08-04T23:34:19.682396: step 1227, loss 0.600548.
Train: 2018-08-04T23:34:23.182766: step 1228, loss 0.554753.
Train: 2018-08-04T23:34:26.667508: step 1229, loss 0.525344.
Train: 2018-08-04T23:34:30.167878: step 1230, loss 0.544672.
Test: 2018-08-04T23:34:45.153834: step 1230, loss 0.547412.
Train: 2018-08-04T23:34:48.685458: step 1231, loss 0.561186.
Train: 2018-08-04T23:34:52.170200: step 1232, loss 0.561436.
Train: 2018-08-04T23:34:55.670570: step 1233, loss 0.535944.
Train: 2018-08-04T23:34:59.170939: step 1234, loss 0.553464.
Train: 2018-08-04T23:35:02.686936: step 1235, loss 0.606656.
Train: 2018-08-04T23:35:06.187305: step 1236, loss 0.483777.
Train: 2018-08-04T23:35:09.672047: step 1237, loss 0.572128.
Train: 2018-08-04T23:35:13.188044: step 1238, loss 0.500483.
Train: 2018-08-04T23:35:16.750921: step 1239, loss 0.51722.
Train: 2018-08-04T23:35:20.235663: step 1240, loss 0.519279.
Test: 2018-08-04T23:35:35.252873: step 1240, loss 0.54949.
Train: 2018-08-04T23:35:38.784496: step 1241, loss 0.545628.
Train: 2018-08-04T23:35:42.269239: step 1242, loss 0.53503.
Train: 2018-08-04T23:35:45.753982: step 1243, loss 0.581382.
Train: 2018-08-04T23:35:49.254351: step 1244, loss 0.518806.
Train: 2018-08-04T23:35:52.817227: step 1245, loss 0.582115.
Train: 2018-08-04T23:35:56.333223: step 1246, loss 0.570479.
Train: 2018-08-04T23:35:59.817966: step 1247, loss 0.573963.
Train: 2018-08-04T23:36:03.474602: step 1248, loss 0.607431.
Train: 2018-08-04T23:36:06.974971: step 1249, loss 0.529384.
Train: 2018-08-04T23:36:10.459714: step 1250, loss 0.554352.
Test: 2018-08-04T23:36:25.461298: step 1250, loss 0.547593.
Train: 2018-08-04T23:36:29.024174: step 1251, loss 0.573168.
Train: 2018-08-04T23:36:32.555797: step 1252, loss 0.489378.
Train: 2018-08-04T23:36:36.024913: step 1253, loss 0.517289.
Train: 2018-08-04T23:36:39.525283: step 1254, loss 0.536567.
Train: 2018-08-04T23:36:43.025652: step 1255, loss 0.625311.
Train: 2018-08-04T23:36:46.510395: step 1256, loss 0.564653.
Train: 2018-08-04T23:36:50.073271: step 1257, loss 0.581569.
Train: 2018-08-04T23:36:53.589267: step 1258, loss 0.500865.
Train: 2018-08-04T23:36:57.089637: step 1259, loss 0.54465.
Train: 2018-08-04T23:37:00.590006: step 1260, loss 0.588621.
Test: 2018-08-04T23:37:15.622843: step 1260, loss 0.548045.
Train: 2018-08-04T23:37:19.123213: step 1261, loss 0.587079.
Train: 2018-08-04T23:37:22.623582: step 1262, loss 0.53572.
Train: 2018-08-04T23:37:26.155205: step 1263, loss 0.528943.
Train: 2018-08-04T23:37:29.671201: step 1264, loss 0.581257.
Train: 2018-08-04T23:37:33.171570: step 1265, loss 0.517275.
Train: 2018-08-04T23:37:36.718820: step 1266, loss 0.571585.
Train: 2018-08-04T23:37:40.203563: step 1267, loss 0.546524.
Train: 2018-08-04T23:37:43.703932: step 1268, loss 0.578758.
Train: 2018-08-04T23:37:47.235555: step 1269, loss 0.546076.
Train: 2018-08-04T23:37:50.767177: step 1270, loss 0.596059.
Test: 2018-08-04T23:38:05.721882: step 1270, loss 0.547713.
Train: 2018-08-04T23:38:09.253504: step 1271, loss 0.543407.
Train: 2018-08-04T23:38:12.753873: step 1272, loss 0.607441.
Train: 2018-08-04T23:38:16.269870: step 1273, loss 0.589878.
Train: 2018-08-04T23:38:19.770239: step 1274, loss 0.589364.
Train: 2018-08-04T23:38:23.301863: step 1275, loss 0.622992.
Train: 2018-08-04T23:38:26.849111: step 1276, loss 0.5976.
Train: 2018-08-04T23:38:30.333855: step 1277, loss 0.561604.
Train: 2018-08-04T23:38:33.834224: step 1278, loss 0.527612.
Train: 2018-08-04T23:38:37.318967: step 1279, loss 0.586897.
Train: 2018-08-04T23:38:40.834963: step 1280, loss 0.587947.
Test: 2018-08-04T23:38:55.836546: step 1280, loss 0.548944.
Train: 2018-08-04T23:38:59.368169: step 1281, loss 0.587528.
Train: 2018-08-04T23:39:02.899792: step 1282, loss 0.612803.
Train: 2018-08-04T23:39:06.415788: step 1283, loss 0.511689.
Train: 2018-08-04T23:39:09.916157: step 1284, loss 0.537249.
Train: 2018-08-04T23:39:13.400900: step 1285, loss 0.503707.
Train: 2018-08-04T23:39:16.885643: step 1286, loss 0.521373.
Train: 2018-08-04T23:39:20.432893: step 1287, loss 0.571129.
Train: 2018-08-04T23:39:23.948889: step 1288, loss 0.587492.
Train: 2018-08-04T23:39:27.464885: step 1289, loss 0.520511.
Train: 2018-08-04T23:39:30.949628: step 1290, loss 0.570703.
Test: 2018-08-04T23:39:45.951212: step 1290, loss 0.548726.
Train: 2018-08-04T23:39:49.482834: step 1291, loss 0.562223.
Train: 2018-08-04T23:39:52.983203: step 1292, loss 0.520114.
Train: 2018-08-04T23:39:56.483573: step 1293, loss 0.54549.
Train: 2018-08-04T23:39:59.999570: step 1294, loss 0.519846.
Train: 2018-08-04T23:40:03.531192: step 1295, loss 0.51991.
Train: 2018-08-04T23:40:07.031561: step 1296, loss 0.54587.
Train: 2018-08-04T23:40:10.578811: step 1297, loss 0.537082.
Train: 2018-08-04T23:40:14.094807: step 1298, loss 0.587623.
Train: 2018-08-04T23:40:17.595177: step 1299, loss 0.56231.
Train: 2018-08-04T23:40:21.079919: step 1300, loss 0.555305.
Test: 2018-08-04T23:40:36.081504: step 1300, loss 0.549695.
Train: 2018-08-04T23:40:41.785230: step 1301, loss 0.562224.
Train: 2018-08-04T23:40:45.285600: step 1302, loss 0.589032.
Train: 2018-08-04T23:40:48.754716: step 1303, loss 0.518763.
Train: 2018-08-04T23:40:52.270712: step 1304, loss 0.631761.
Train: 2018-08-04T23:40:55.802335: step 1305, loss 0.51066.
Train: 2018-08-04T23:40:59.302705: step 1306, loss 0.596152.
Train: 2018-08-04T23:41:02.803074: step 1307, loss 0.58915.
Train: 2018-08-04T23:41:06.334697: step 1308, loss 0.561989.
Train: 2018-08-04T23:41:09.819440: step 1309, loss 0.579956.
Train: 2018-08-04T23:41:13.304182: step 1310, loss 0.502653.
Test: 2018-08-04T23:41:28.290138: step 1310, loss 0.547463.
Train: 2018-08-04T23:41:31.790508: step 1311, loss 0.58857.
Train: 2018-08-04T23:41:35.322131: step 1312, loss 0.571317.
Train: 2018-08-04T23:41:38.869381: step 1313, loss 0.589102.
Train: 2018-08-04T23:41:42.401003: step 1314, loss 0.605691.
Train: 2018-08-04T23:41:45.870120: step 1315, loss 0.631996.
Train: 2018-08-04T23:41:49.401742: step 1316, loss 0.553567.
Train: 2018-08-04T23:41:52.886485: step 1317, loss 0.570983.
Train: 2018-08-04T23:41:56.449362: step 1318, loss 0.519207.
Train: 2018-08-04T23:41:59.934104: step 1319, loss 0.528491.
Train: 2018-08-04T23:42:03.434474: step 1320, loss 0.579455.
Test: 2018-08-04T23:42:18.451685: step 1320, loss 0.547957.
Train: 2018-08-04T23:42:21.967680: step 1321, loss 0.554129.
Train: 2018-08-04T23:42:25.483676: step 1322, loss 0.596722.
Train: 2018-08-04T23:42:28.984046: step 1323, loss 0.51988.
Train: 2018-08-04T23:42:32.515669: step 1324, loss 0.571253.
Train: 2018-08-04T23:42:36.016038: step 1325, loss 0.587641.
Train: 2018-08-04T23:42:39.500781: step 1326, loss 0.494329.
Train: 2018-08-04T23:42:42.985524: step 1327, loss 0.587509.
Train: 2018-08-04T23:42:46.485893: step 1328, loss 0.553723.
Train: 2018-08-04T23:42:49.986263: step 1329, loss 0.52043.
Train: 2018-08-04T23:42:53.486632: step 1330, loss 0.638218.
Test: 2018-08-04T23:43:08.503843: step 1330, loss 0.548203.
Train: 2018-08-04T23:43:12.004212: step 1331, loss 0.613169.
Train: 2018-08-04T23:43:15.520208: step 1332, loss 0.60447.
Train: 2018-08-04T23:43:19.020577: step 1333, loss 0.562211.
Train: 2018-08-04T23:43:22.520947: step 1334, loss 0.612551.
Train: 2018-08-04T23:43:26.005690: step 1335, loss 0.621287.
Train: 2018-08-04T23:43:29.521686: step 1336, loss 0.612444.
Train: 2018-08-04T23:43:33.068936: step 1337, loss 0.570696.
Train: 2018-08-04T23:43:36.600558: step 1338, loss 0.562562.
Train: 2018-08-04T23:43:40.132181: step 1339, loss 0.553916.
Train: 2018-08-04T23:43:43.616924: step 1340, loss 0.488547.
Test: 2018-08-04T23:43:58.681014: step 1340, loss 0.547803.
Train: 2018-08-04T23:44:02.165757: step 1341, loss 0.595214.
Train: 2018-08-04T23:44:05.713006: step 1342, loss 0.521276.
Train: 2018-08-04T23:44:09.229002: step 1343, loss 0.562276.
Train: 2018-08-04T23:44:12.744999: step 1344, loss 0.603252.
Train: 2018-08-04T23:44:16.260995: step 1345, loss 0.571183.
Train: 2018-08-04T23:44:19.823871: step 1346, loss 0.570493.
Train: 2018-08-04T23:44:23.339867: step 1347, loss 0.587188.
Train: 2018-08-04T23:44:26.887116: step 1348, loss 0.545708.
Train: 2018-08-04T23:44:30.387486: step 1349, loss 0.546296.
Train: 2018-08-04T23:44:33.887855: step 1350, loss 0.620743.
Test: 2018-08-04T23:44:48.826933: step 1350, loss 0.548994.
Train: 2018-08-04T23:44:52.405435: step 1351, loss 0.562432.
Train: 2018-08-04T23:44:55.905805: step 1352, loss 0.513136.
Train: 2018-08-04T23:44:59.406174: step 1353, loss 0.603894.
Train: 2018-08-04T23:45:02.906543: step 1354, loss 0.562324.
Train: 2018-08-04T23:45:06.469420: step 1355, loss 0.529453.
Train: 2018-08-04T23:45:09.969789: step 1356, loss 0.554011.
Train: 2018-08-04T23:45:13.454532: step 1357, loss 0.554203.
Train: 2018-08-04T23:45:16.939275: step 1358, loss 0.578391.
Train: 2018-08-04T23:45:18.642579: step 1359, loss 0.633753.
Train: 2018-08-04T23:45:22.127323: step 1360, loss 0.529797.
Test: 2018-08-04T23:45:37.097654: step 1360, loss 0.548645.
Train: 2018-08-04T23:45:40.691782: step 1361, loss 0.595435.
Train: 2018-08-04T23:45:44.192152: step 1362, loss 0.553864.
Train: 2018-08-04T23:45:47.692521: step 1363, loss 0.554004.
Train: 2018-08-04T23:45:51.177264: step 1364, loss 0.521288.
Train: 2018-08-04T23:45:54.708887: step 1365, loss 0.570325.
Train: 2018-08-04T23:45:58.178003: step 1366, loss 0.562281.
Train: 2018-08-04T23:46:01.709626: step 1367, loss 0.587339.
Train: 2018-08-04T23:46:05.209995: step 1368, loss 0.621146.
Train: 2018-08-04T23:46:08.694738: step 1369, loss 0.629111.
Train: 2018-08-04T23:46:12.195108: step 1370, loss 0.620966.
Test: 2018-08-04T23:46:27.196691: step 1370, loss 0.548991.
Train: 2018-08-04T23:46:30.697060: step 1371, loss 0.603687.
Train: 2018-08-04T23:46:34.291190: step 1372, loss 0.554145.
Train: 2018-08-04T23:46:37.807186: step 1373, loss 0.538159.
Train: 2018-08-04T23:46:41.338809: step 1374, loss 0.496625.
Train: 2018-08-04T23:46:44.854805: step 1375, loss 0.48058.
Train: 2018-08-04T23:46:48.323922: step 1376, loss 0.546106.
Train: 2018-08-04T23:46:51.902424: step 1377, loss 0.620178.
Train: 2018-08-04T23:46:55.387167: step 1378, loss 0.629023.
Train: 2018-08-04T23:46:58.903163: step 1379, loss 0.545716.
Train: 2018-08-04T23:47:02.450412: step 1380, loss 0.545445.
Test: 2018-08-04T23:47:17.451996: step 1380, loss 0.549401.
Train: 2018-08-04T23:47:20.952365: step 1381, loss 0.545712.
Train: 2018-08-04T23:47:24.437108: step 1382, loss 0.604446.
Train: 2018-08-04T23:47:27.953104: step 1383, loss 0.57874.
Train: 2018-08-04T23:47:31.453475: step 1384, loss 0.637182.
Train: 2018-08-04T23:47:34.969470: step 1385, loss 0.504168.
Train: 2018-08-04T23:47:38.485466: step 1386, loss 0.579173.
Train: 2018-08-04T23:47:41.970209: step 1387, loss 0.53718.
Train: 2018-08-04T23:47:45.486205: step 1388, loss 0.562689.
Train: 2018-08-04T23:47:48.986575: step 1389, loss 0.579196.
Train: 2018-08-04T23:47:52.455691: step 1390, loss 0.595181.
Test: 2018-08-04T23:48:07.441647: step 1390, loss 0.549939.
Train: 2018-08-04T23:48:10.973271: step 1391, loss 0.562333.
Train: 2018-08-04T23:48:14.520520: step 1392, loss 0.629016.
Train: 2018-08-04T23:48:18.020889: step 1393, loss 0.53728.
Train: 2018-08-04T23:48:21.521259: step 1394, loss 0.537827.
Train: 2018-08-04T23:48:25.021628: step 1395, loss 0.554202.
Train: 2018-08-04T23:48:28.521998: step 1396, loss 0.546176.
Train: 2018-08-04T23:48:32.053621: step 1397, loss 0.512875.
Train: 2018-08-04T23:48:35.585243: step 1398, loss 0.595654.
Train: 2018-08-04T23:48:39.069987: step 1399, loss 0.545269.
Train: 2018-08-04T23:48:42.554729: step 1400, loss 0.55388.
Test: 2018-08-04T23:48:57.571940: step 1400, loss 0.547679.
Train: 2018-08-04T23:49:03.291293: step 1401, loss 0.528817.
Train: 2018-08-04T23:49:06.791662: step 1402, loss 0.511935.
Train: 2018-08-04T23:49:10.276406: step 1403, loss 0.546168.
Train: 2018-08-04T23:49:13.823655: step 1404, loss 0.553726.
Train: 2018-08-04T23:49:17.308398: step 1405, loss 0.536716.
Train: 2018-08-04T23:49:20.808767: step 1406, loss 0.545297.
Train: 2018-08-04T23:49:24.277883: step 1407, loss 0.486169.
Train: 2018-08-04T23:49:27.762626: step 1408, loss 0.536021.
Train: 2018-08-04T23:49:31.247370: step 1409, loss 0.570585.
Train: 2018-08-04T23:49:34.747739: step 1410, loss 0.580299.
Test: 2018-08-04T23:49:49.671189: step 1410, loss 0.548014.
Train: 2018-08-04T23:49:53.374705: step 1411, loss 0.51819.
Train: 2018-08-04T23:49:56.890701: step 1412, loss 0.475321.
Train: 2018-08-04T23:50:00.406697: step 1413, loss 0.53542.
Train: 2018-08-04T23:50:03.907067: step 1414, loss 0.580129.
Train: 2018-08-04T23:50:07.423063: step 1415, loss 0.517494.
Train: 2018-08-04T23:50:10.939059: step 1416, loss 0.517389.
Train: 2018-08-04T23:50:14.501935: step 1417, loss 0.579757.
Train: 2018-08-04T23:50:18.002310: step 1418, loss 0.534883.
Train: 2018-08-04T23:50:21.487047: step 1419, loss 0.626626.
Train: 2018-08-04T23:50:24.971790: step 1420, loss 0.57167.
Test: 2018-08-04T23:50:39.926493: step 1420, loss 0.549009.
Train: 2018-08-04T23:50:43.411237: step 1421, loss 0.515454.
Train: 2018-08-04T23:50:46.974112: step 1422, loss 0.60676.
Train: 2018-08-04T23:50:50.505736: step 1423, loss 0.563752.
Train: 2018-08-04T23:50:54.006105: step 1424, loss 0.571509.
Train: 2018-08-04T23:50:57.568981: step 1425, loss 0.570898.
Train: 2018-08-04T23:51:01.053724: step 1426, loss 0.535065.
Train: 2018-08-04T23:51:04.538467: step 1427, loss 0.545319.
Train: 2018-08-04T23:51:08.085716: step 1428, loss 0.571954.
Train: 2018-08-04T23:51:11.617339: step 1429, loss 0.572152.
Train: 2018-08-04T23:51:15.102082: step 1430, loss 0.527207.
Test: 2018-08-04T23:51:30.056787: step 1430, loss 0.547967.
Train: 2018-08-04T23:51:33.541528: step 1431, loss 0.453061.
Train: 2018-08-04T23:51:37.026271: step 1432, loss 0.544176.
Train: 2018-08-04T23:51:40.526641: step 1433, loss 0.499675.
Train: 2018-08-04T23:51:44.058264: step 1434, loss 0.546121.
Train: 2018-08-04T23:51:47.558633: step 1435, loss 0.552683.
Train: 2018-08-04T23:51:51.090255: step 1436, loss 0.499986.
Train: 2018-08-04T23:51:54.574999: step 1437, loss 0.608559.
Train: 2018-08-04T23:51:58.059741: step 1438, loss 0.499038.
Train: 2018-08-04T23:52:01.575738: step 1439, loss 0.648048.
Train: 2018-08-04T23:52:05.107361: step 1440, loss 0.469119.
Test: 2018-08-04T23:52:20.093318: step 1440, loss 0.548812.
Train: 2018-08-04T23:52:23.609313: step 1441, loss 0.572172.
Train: 2018-08-04T23:52:27.125309: step 1442, loss 0.6175.
Train: 2018-08-04T23:52:30.625679: step 1443, loss 0.597251.
Train: 2018-08-04T23:52:34.172928: step 1444, loss 0.58357.
Train: 2018-08-04T23:52:37.673298: step 1445, loss 0.572771.
Train: 2018-08-04T23:52:41.204921: step 1446, loss 0.517106.
Train: 2018-08-04T23:52:44.736544: step 1447, loss 0.534685.
Train: 2018-08-04T23:52:48.315046: step 1448, loss 0.589642.
Train: 2018-08-04T23:52:51.799790: step 1449, loss 0.562455.
Train: 2018-08-04T23:52:55.284532: step 1450, loss 0.625594.
Test: 2018-08-04T23:53:10.286117: step 1450, loss 0.54635.
Train: 2018-08-04T23:53:13.786485: step 1451, loss 0.589407.
Train: 2018-08-04T23:53:17.318108: step 1452, loss 0.491293.
Train: 2018-08-04T23:53:20.834104: step 1453, loss 0.616646.
Train: 2018-08-04T23:53:24.303220: step 1454, loss 0.561631.
Train: 2018-08-04T23:53:27.819217: step 1455, loss 0.632015.
Train: 2018-08-04T23:53:31.335213: step 1456, loss 0.553595.
Train: 2018-08-04T23:53:34.866835: step 1457, loss 0.457575.
Train: 2018-08-04T23:53:38.414084: step 1458, loss 0.562359.
Train: 2018-08-04T23:53:41.961334: step 1459, loss 0.614329.
Train: 2018-08-04T23:53:45.461704: step 1460, loss 0.55321.
Test: 2018-08-04T23:54:00.494541: step 1460, loss 0.548067.
Train: 2018-08-04T23:54:03.994910: step 1461, loss 0.56247.
Train: 2018-08-04T23:54:07.495279: step 1462, loss 0.510257.
Train: 2018-08-04T23:54:11.026902: step 1463, loss 0.571153.
Train: 2018-08-04T23:54:14.605405: step 1464, loss 0.543926.
Train: 2018-08-04T23:54:18.121401: step 1465, loss 0.596777.
Train: 2018-08-04T23:54:21.637398: step 1466, loss 0.562246.
Train: 2018-08-04T23:54:25.122140: step 1467, loss 0.545411.
Train: 2018-08-04T23:54:28.606884: step 1468, loss 0.5871.
Train: 2018-08-04T23:54:32.107252: step 1469, loss 0.606204.
Train: 2018-08-04T23:54:35.638875: step 1470, loss 0.536253.
Test: 2018-08-04T23:54:50.640458: step 1470, loss 0.548622.
Train: 2018-08-04T23:54:54.156455: step 1471, loss 0.553741.
Train: 2018-08-04T23:54:57.672451: step 1472, loss 0.546282.
Train: 2018-08-04T23:55:01.157194: step 1473, loss 0.596874.
Train: 2018-08-04T23:55:04.626310: step 1474, loss 0.579122.
Train: 2018-08-04T23:55:08.157933: step 1475, loss 0.65679.
Train: 2018-08-04T23:55:11.642675: step 1476, loss 0.614211.
Train: 2018-08-04T23:55:15.158672: step 1477, loss 0.587458.
Train: 2018-08-04T23:55:18.659042: step 1478, loss 0.562561.
Train: 2018-08-04T23:55:22.143784: step 1479, loss 0.520088.
Train: 2018-08-04T23:55:25.644153: step 1480, loss 0.478951.
Test: 2018-08-04T23:55:40.614483: step 1480, loss 0.547725.
Train: 2018-08-04T23:55:44.114853: step 1481, loss 0.562109.
Train: 2018-08-04T23:55:47.615223: step 1482, loss 0.579011.
Train: 2018-08-04T23:55:51.131219: step 1483, loss 0.653761.
Train: 2018-08-04T23:55:54.647216: step 1484, loss 0.537434.
Train: 2018-08-04T23:55:58.131958: step 1485, loss 0.637142.
Train: 2018-08-04T23:56:01.616701: step 1486, loss 0.520492.
Train: 2018-08-04T23:56:05.132697: step 1487, loss 0.570435.
Train: 2018-08-04T23:56:08.633066: step 1488, loss 0.545933.
Train: 2018-08-04T23:56:12.149062: step 1489, loss 0.662347.
Train: 2018-08-04T23:56:15.680685: step 1490, loss 0.554008.
Test: 2018-08-04T23:56:30.619762: step 1490, loss 0.547491.
Train: 2018-08-04T23:56:34.135759: step 1491, loss 0.563059.
Train: 2018-08-04T23:56:37.589248: step 1492, loss 0.570533.
Train: 2018-08-04T23:56:41.089617: step 1493, loss 0.570985.
Train: 2018-08-04T23:56:44.589987: step 1494, loss 0.635615.
Train: 2018-08-04T23:56:48.105983: step 1495, loss 0.51369.
Train: 2018-08-04T23:56:51.621979: step 1496, loss 0.562762.
Train: 2018-08-04T23:56:55.091096: step 1497, loss 0.504747.
Train: 2018-08-04T23:56:58.607092: step 1498, loss 0.512932.
Train: 2018-08-04T23:57:02.107461: step 1499, loss 0.554687.
Train: 2018-08-04T23:57:05.592204: step 1500, loss 0.578325.
Test: 2018-08-04T23:57:20.562534: step 1500, loss 0.548244.
Train: 2018-08-04T23:57:26.250635: step 1501, loss 0.61248.
Train: 2018-08-04T23:57:29.719751: step 1502, loss 0.537682.
Train: 2018-08-04T23:57:33.220120: step 1503, loss 0.56218.
Train: 2018-08-04T23:57:36.704863: step 1504, loss 0.62757.
Train: 2018-08-04T23:57:40.205233: step 1505, loss 0.487965.
Train: 2018-08-04T23:57:43.705602: step 1506, loss 0.537133.
Train: 2018-08-04T23:57:47.237225: step 1507, loss 0.595417.
Train: 2018-08-04T23:57:50.768848: step 1508, loss 0.537822.
Train: 2018-08-04T23:57:54.269217: step 1509, loss 0.57782.
Train: 2018-08-04T23:57:55.972522: step 1510, loss 0.654404.
Test: 2018-08-04T23:58:10.989732: step 1510, loss 0.548247.
Train: 2018-08-04T23:58:14.490102: step 1511, loss 0.52908.
Train: 2018-08-04T23:58:17.959218: step 1512, loss 0.629373.
Train: 2018-08-04T23:58:21.459587: step 1513, loss 0.47864.
Train: 2018-08-04T23:58:24.991210: step 1514, loss 0.58786.
Train: 2018-08-04T23:58:28.460326: step 1515, loss 0.562557.
Train: 2018-08-04T23:58:31.960696: step 1516, loss 0.595191.
Train: 2018-08-04T23:58:35.461066: step 1517, loss 0.486885.
Train: 2018-08-04T23:58:38.961435: step 1518, loss 0.588308.
Train: 2018-08-04T23:58:42.493058: step 1519, loss 0.545313.
Train: 2018-08-04T23:58:46.055934: step 1520, loss 0.604603.
Test: 2018-08-04T23:59:00.995013: step 1520, loss 0.54697.
Train: 2018-08-04T23:59:04.479754: step 1521, loss 0.596051.
Train: 2018-08-04T23:59:07.995750: step 1522, loss 0.504889.
Train: 2018-08-04T23:59:11.542999: step 1523, loss 0.562506.
Train: 2018-08-04T23:59:15.090249: step 1524, loss 0.697404.
Train: 2018-08-04T23:59:18.668751: step 1525, loss 0.569565.
Train: 2018-08-04T23:59:22.184747: step 1526, loss 0.562205.
Train: 2018-08-04T23:59:25.700744: step 1527, loss 0.580008.
Train: 2018-08-04T23:59:29.201113: step 1528, loss 0.570801.
Train: 2018-08-04T23:59:32.670229: step 1529, loss 0.537722.
Train: 2018-08-04T23:59:36.186226: step 1530, loss 0.570853.
Test: 2018-08-04T23:59:51.109675: step 1530, loss 0.548941.
Train: 2018-08-04T23:59:54.656925: step 1531, loss 0.595724.
Train: 2018-08-04T23:59:58.141668: step 1532, loss 0.537389.
Train: 2018-08-05T00:00:01.626411: step 1533, loss 0.538121.
Train: 2018-08-05T00:00:05.158034: step 1534, loss 0.587556.
Train: 2018-08-05T00:00:08.705283: step 1535, loss 0.620135.
Train: 2018-08-05T00:00:12.190026: step 1536, loss 0.563477.
Train: 2018-08-05T00:00:15.690396: step 1537, loss 0.562453.
Train: 2018-08-05T00:00:19.206392: step 1538, loss 0.488316.
Train: 2018-08-05T00:00:22.722388: step 1539, loss 0.586675.
Train: 2018-08-05T00:00:26.222757: step 1540, loss 0.504472.
Test: 2018-08-05T00:00:41.130581: step 1540, loss 0.548824.
Train: 2018-08-05T00:00:44.599697: step 1541, loss 0.537837.
Train: 2018-08-05T00:00:48.084440: step 1542, loss 0.462689.
Train: 2018-08-05T00:00:51.631689: step 1543, loss 0.554644.
Train: 2018-08-05T00:00:55.163312: step 1544, loss 0.578231.
Train: 2018-08-05T00:00:58.694935: step 1545, loss 0.621301.
Train: 2018-08-05T00:01:02.195304: step 1546, loss 0.588729.
Train: 2018-08-05T00:01:05.664420: step 1547, loss 0.570761.
Train: 2018-08-05T00:01:09.180417: step 1548, loss 0.587903.
Train: 2018-08-05T00:01:12.712039: step 1549, loss 0.528369.
Train: 2018-08-05T00:01:16.196782: step 1550, loss 0.620873.
Test: 2018-08-05T00:01:31.151489: step 1550, loss 0.54782.
Train: 2018-08-05T00:01:34.651855: step 1551, loss 0.579345.
Train: 2018-08-05T00:01:38.136599: step 1552, loss 0.553866.
Train: 2018-08-05T00:01:41.621341: step 1553, loss 0.545672.
Train: 2018-08-05T00:01:45.121710: step 1554, loss 0.621232.
Train: 2018-08-05T00:01:48.590827: step 1555, loss 0.554522.
Train: 2018-08-05T00:01:52.106823: step 1556, loss 0.604145.
Train: 2018-08-05T00:01:55.622820: step 1557, loss 0.544923.
Train: 2018-08-05T00:01:59.091935: step 1558, loss 0.578925.
Train: 2018-08-05T00:02:02.576678: step 1559, loss 0.63851.
Train: 2018-08-05T00:02:06.061421: step 1560, loss 0.554723.
Test: 2018-08-05T00:02:21.000498: step 1560, loss 0.547834.
Train: 2018-08-05T00:02:24.516494: step 1561, loss 0.487124.
Train: 2018-08-05T00:02:28.032490: step 1562, loss 0.545698.
Train: 2018-08-05T00:02:31.579740: step 1563, loss 0.504156.
Train: 2018-08-05T00:02:35.080109: step 1564, loss 0.495839.
Train: 2018-08-05T00:02:38.580479: step 1565, loss 0.512542.
Train: 2018-08-05T00:02:42.049595: step 1566, loss 0.554298.
Train: 2018-08-05T00:02:45.534338: step 1567, loss 0.545691.
Train: 2018-08-05T00:02:49.065961: step 1568, loss 0.55455.
Train: 2018-08-05T00:02:52.550703: step 1569, loss 0.544687.
Train: 2018-08-05T00:02:56.051073: step 1570, loss 0.493949.
Test: 2018-08-05T00:03:10.990149: step 1570, loss 0.546375.
Train: 2018-08-05T00:03:14.490519: step 1571, loss 0.519458.
Train: 2018-08-05T00:03:18.022142: step 1572, loss 0.554246.
Train: 2018-08-05T00:03:21.538138: step 1573, loss 0.519376.
Train: 2018-08-05T00:03:25.038508: step 1574, loss 0.535964.
Train: 2018-08-05T00:03:28.601384: step 1575, loss 0.55415.
Train: 2018-08-05T00:03:32.117380: step 1576, loss 0.588726.
Train: 2018-08-05T00:03:35.617749: step 1577, loss 0.553047.
Train: 2018-08-05T00:03:39.118119: step 1578, loss 0.598318.
Train: 2018-08-05T00:03:42.634115: step 1579, loss 0.589426.
Train: 2018-08-05T00:03:46.134485: step 1580, loss 0.571329.
Test: 2018-08-05T00:04:01.120441: step 1580, loss 0.548043.
Train: 2018-08-05T00:04:04.620811: step 1581, loss 0.535846.
Train: 2018-08-05T00:04:08.105554: step 1582, loss 0.642361.
Train: 2018-08-05T00:04:11.652803: step 1583, loss 0.526913.
Train: 2018-08-05T00:04:15.137546: step 1584, loss 0.544349.
Train: 2018-08-05T00:04:18.653542: step 1585, loss 0.57111.
Train: 2018-08-05T00:04:22.169538: step 1586, loss 0.526953.
Train: 2018-08-05T00:04:25.669908: step 1587, loss 0.60703.
Train: 2018-08-05T00:04:29.154651: step 1588, loss 0.509324.
Train: 2018-08-05T00:04:32.608140: step 1589, loss 0.509069.
Train: 2018-08-05T00:04:36.108510: step 1590, loss 0.55358.
Test: 2018-08-05T00:04:51.000706: step 1590, loss 0.548672.
Train: 2018-08-05T00:04:54.532329: step 1591, loss 0.553292.
Train: 2018-08-05T00:04:58.017072: step 1592, loss 0.562419.
Train: 2018-08-05T00:05:01.533068: step 1593, loss 0.607688.
Train: 2018-08-05T00:05:05.033438: step 1594, loss 0.473838.
Train: 2018-08-05T00:05:08.549434: step 1595, loss 0.615999.
Train: 2018-08-05T00:05:12.112310: step 1596, loss 0.482525.
Train: 2018-08-05T00:05:15.612680: step 1597, loss 0.509408.
Train: 2018-08-05T00:05:19.128676: step 1598, loss 0.571006.
Train: 2018-08-05T00:05:22.629045: step 1599, loss 0.598445.
Train: 2018-08-05T00:05:26.113788: step 1600, loss 0.598736.
Test: 2018-08-05T00:05:41.068491: step 1600, loss 0.548034.
Train: 2018-08-05T00:05:46.772219: step 1601, loss 0.58032.
Train: 2018-08-05T00:05:50.256962: step 1602, loss 0.616128.
Train: 2018-08-05T00:05:53.741705: step 1603, loss 0.52659.
Train: 2018-08-05T00:05:57.242074: step 1604, loss 0.579671.
Train: 2018-08-05T00:06:00.773697: step 1605, loss 0.527482.
Train: 2018-08-05T00:06:04.289693: step 1606, loss 0.51021.
Train: 2018-08-05T00:06:07.790062: step 1607, loss 0.509411.
Train: 2018-08-05T00:06:11.290432: step 1608, loss 0.62412.
Train: 2018-08-05T00:06:14.790802: step 1609, loss 0.615289.
Train: 2018-08-05T00:06:18.259918: step 1610, loss 0.544909.
Test: 2018-08-05T00:06:33.230248: step 1610, loss 0.54733.
Train: 2018-08-05T00:06:36.746244: step 1611, loss 0.56243.
Train: 2018-08-05T00:06:40.246613: step 1612, loss 0.57169.
Train: 2018-08-05T00:06:43.762610: step 1613, loss 0.536585.
Train: 2018-08-05T00:06:47.247352: step 1614, loss 0.571603.
Train: 2018-08-05T00:06:50.732095: step 1615, loss 0.519005.
Train: 2018-08-05T00:06:54.216838: step 1616, loss 0.667245.
Train: 2018-08-05T00:06:57.748461: step 1617, loss 0.597051.
Train: 2018-08-05T00:07:01.248830: step 1618, loss 0.60566.
Train: 2018-08-05T00:07:04.749200: step 1619, loss 0.553545.
Train: 2018-08-05T00:07:08.233943: step 1620, loss 0.64808.
Test: 2018-08-05T00:07:23.173020: step 1620, loss 0.547863.
Train: 2018-08-05T00:07:26.689016: step 1621, loss 0.562293.
Train: 2018-08-05T00:07:30.189386: step 1622, loss 0.553884.
Train: 2018-08-05T00:07:33.705381: step 1623, loss 0.604687.
Train: 2018-08-05T00:07:37.221378: step 1624, loss 0.646798.
Train: 2018-08-05T00:07:40.706121: step 1625, loss 0.495219.
Train: 2018-08-05T00:07:44.190864: step 1626, loss 0.56239.
Train: 2018-08-05T00:07:47.675606: step 1627, loss 0.570794.
Train: 2018-08-05T00:07:51.175976: step 1628, loss 0.562458.
Train: 2018-08-05T00:07:54.738852: step 1629, loss 0.562523.
Train: 2018-08-05T00:07:58.239221: step 1630, loss 0.496277.
Test: 2018-08-05T00:08:13.193927: step 1630, loss 0.548861.
Train: 2018-08-05T00:08:16.694294: step 1631, loss 0.579036.
Train: 2018-08-05T00:08:20.194664: step 1632, loss 0.636905.
Train: 2018-08-05T00:08:23.726287: step 1633, loss 0.529505.
Train: 2018-08-05T00:08:27.226656: step 1634, loss 0.579005.
Train: 2018-08-05T00:08:30.758279: step 1635, loss 0.60371.
Train: 2018-08-05T00:08:34.274275: step 1636, loss 0.603651.
Train: 2018-08-05T00:08:37.774644: step 1637, loss 0.603585.
Train: 2018-08-05T00:08:41.259387: step 1638, loss 0.554396.
Train: 2018-08-05T00:08:44.759757: step 1639, loss 0.546265.
Train: 2018-08-05T00:08:48.260126: step 1640, loss 0.546298.
Test: 2018-08-05T00:09:03.230457: step 1640, loss 0.5475.
Train: 2018-08-05T00:09:06.746453: step 1641, loss 0.546311.
Train: 2018-08-05T00:09:10.293702: step 1642, loss 0.554463.
Train: 2018-08-05T00:09:13.794072: step 1643, loss 0.603411.
Train: 2018-08-05T00:09:17.294441: step 1644, loss 0.472879.
Train: 2018-08-05T00:09:20.810437: step 1645, loss 0.578942.
Train: 2018-08-05T00:09:24.342060: step 1646, loss 0.538012.
Train: 2018-08-05T00:09:27.858056: step 1647, loss 0.59538.
Train: 2018-08-05T00:09:31.374057: step 1648, loss 0.603634.
Train: 2018-08-05T00:09:34.858796: step 1649, loss 0.611876.
Train: 2018-08-05T00:09:38.359165: step 1650, loss 0.496775.
Test: 2018-08-05T00:09:53.345122: step 1650, loss 0.549917.
Train: 2018-08-05T00:09:56.845491: step 1651, loss 0.546064.
Train: 2018-08-05T00:10:00.361488: step 1652, loss 0.537776.
Train: 2018-08-05T00:10:03.846230: step 1653, loss 0.562492.
Train: 2018-08-05T00:10:07.362226: step 1654, loss 0.570757.
Train: 2018-08-05T00:10:10.893849: step 1655, loss 0.537568.
Train: 2018-08-05T00:10:14.378592: step 1656, loss 0.537491.
Train: 2018-08-05T00:10:17.894588: step 1657, loss 0.529061.
Train: 2018-08-05T00:10:21.379331: step 1658, loss 0.562405.
Train: 2018-08-05T00:10:24.895327: step 1659, loss 0.503623.
Train: 2018-08-05T00:10:28.395696: step 1660, loss 0.579231.
Test: 2018-08-05T00:10:43.381654: step 1660, loss 0.548622.
Train: 2018-08-05T00:10:45.084968: step 1661, loss 0.544314.
Train: 2018-08-05T00:10:48.585328: step 1662, loss 0.51991.
Train: 2018-08-05T00:10:52.085698: step 1663, loss 0.553819.
Train: 2018-08-05T00:10:55.570440: step 1664, loss 0.519564.
Train: 2018-08-05T00:10:59.055183: step 1665, loss 0.588108.
Train: 2018-08-05T00:11:02.586806: step 1666, loss 0.588195.
Train: 2018-08-05T00:11:06.102802: step 1667, loss 0.527781.
Train: 2018-08-05T00:11:09.603171: step 1668, loss 0.52769.
Train: 2018-08-05T00:11:13.103541: step 1669, loss 0.536281.
Train: 2018-08-05T00:11:16.603911: step 1670, loss 0.58852.
Test: 2018-08-05T00:11:31.511733: step 1670, loss 0.547979.
Train: 2018-08-05T00:11:35.027730: step 1671, loss 0.527417.
Train: 2018-08-05T00:11:38.559353: step 1672, loss 0.58867.
Train: 2018-08-05T00:11:42.075349: step 1673, loss 0.518507.
Train: 2018-08-05T00:11:45.575719: step 1674, loss 0.562405.
Train: 2018-08-05T00:11:49.076088: step 1675, loss 0.52716.
Train: 2018-08-05T00:11:52.560831: step 1676, loss 0.615437.
Train: 2018-08-05T00:11:56.076827: step 1677, loss 0.571279.
Train: 2018-08-05T00:11:59.624077: step 1678, loss 0.580132.
Train: 2018-08-05T00:12:03.155699: step 1679, loss 0.615508.
Train: 2018-08-05T00:12:06.640442: step 1680, loss 0.588931.
Test: 2018-08-05T00:12:21.704532: step 1680, loss 0.547896.
Train: 2018-08-05T00:12:25.204902: step 1681, loss 0.509515.
Train: 2018-08-05T00:12:28.689645: step 1682, loss 0.553604.
Train: 2018-08-05T00:12:32.205641: step 1683, loss 0.641652.
Train: 2018-08-05T00:12:35.737263: step 1684, loss 0.615105.
Train: 2018-08-05T00:12:39.253259: step 1685, loss 0.536112.
Train: 2018-08-05T00:12:42.738002: step 1686, loss 0.597308.
Train: 2018-08-05T00:12:46.238372: step 1687, loss 0.544943.
Train: 2018-08-05T00:12:49.738741: step 1688, loss 0.597094.
Train: 2018-08-05T00:12:53.239111: step 1689, loss 0.62296.
Train: 2018-08-05T00:12:56.755107: step 1690, loss 0.51059.
Test: 2018-08-05T00:13:11.725438: step 1690, loss 0.548366.
Train: 2018-08-05T00:13:15.257060: step 1691, loss 0.502122.
Train: 2018-08-05T00:13:18.726177: step 1692, loss 0.58811.
Train: 2018-08-05T00:13:22.210919: step 1693, loss 0.519446.
Train: 2018-08-05T00:13:25.773795: step 1694, loss 0.57948.
Train: 2018-08-05T00:13:29.274165: step 1695, loss 0.536641.
Train: 2018-08-05T00:13:32.852667: step 1696, loss 0.545211.
Train: 2018-08-05T00:13:36.368664: step 1697, loss 0.493835.
Train: 2018-08-05T00:13:39.900286: step 1698, loss 0.528035.
Train: 2018-08-05T00:13:43.416283: step 1699, loss 0.553743.
Train: 2018-08-05T00:13:46.994786: step 1700, loss 0.579553.
Test: 2018-08-05T00:14:01.949490: step 1700, loss 0.547147.
Train: 2018-08-05T00:14:07.715722: step 1701, loss 0.536476.
Train: 2018-08-05T00:14:11.247345: step 1702, loss 0.588244.
Train: 2018-08-05T00:14:14.778968: step 1703, loss 0.553697.
Train: 2018-08-05T00:14:18.294964: step 1704, loss 0.553691.
Train: 2018-08-05T00:14:21.764081: step 1705, loss 0.458425.
Train: 2018-08-05T00:14:25.248824: step 1706, loss 0.605777.
Train: 2018-08-05T00:14:28.749193: step 1707, loss 0.518858.
Train: 2018-08-05T00:14:32.233936: step 1708, loss 0.536205.
Train: 2018-08-05T00:14:35.749932: step 1709, loss 0.597337.
Train: 2018-08-05T00:14:39.234675: step 1710, loss 0.597393.
Test: 2018-08-05T00:14:54.205004: step 1710, loss 0.547756.
Train: 2018-08-05T00:14:57.752254: step 1711, loss 0.597411.
Train: 2018-08-05T00:15:01.236997: step 1712, loss 0.571134.
Train: 2018-08-05T00:15:04.737366: step 1713, loss 0.588622.
Train: 2018-08-05T00:15:08.253363: step 1714, loss 0.606061.
Train: 2018-08-05T00:15:11.800612: step 1715, loss 0.579803.
Train: 2018-08-05T00:15:15.316608: step 1716, loss 0.562356.
Train: 2018-08-05T00:15:18.832604: step 1717, loss 0.597073.
Train: 2018-08-05T00:15:22.379854: step 1718, loss 0.484429.
Train: 2018-08-05T00:15:25.880223: step 1719, loss 0.527746.
Train: 2018-08-05T00:15:29.380593: step 1720, loss 0.570988.
Test: 2018-08-05T00:15:44.382177: step 1720, loss 0.54711.
Train: 2018-08-05T00:15:47.929426: step 1721, loss 0.501837.
Train: 2018-08-05T00:15:51.492302: step 1722, loss 0.519091.
Train: 2018-08-05T00:15:54.977045: step 1723, loss 0.519029.
Train: 2018-08-05T00:15:58.493041: step 1724, loss 0.571032.
Train: 2018-08-05T00:16:01.977784: step 1725, loss 0.562355.
Train: 2018-08-05T00:16:05.478153: step 1726, loss 0.571069.
Train: 2018-08-05T00:16:08.994150: step 1727, loss 0.640828.
Train: 2018-08-05T00:16:12.510146: step 1728, loss 0.510089.
Train: 2018-08-05T00:16:16.010515: step 1729, loss 0.632077.
Train: 2018-08-05T00:16:19.510885: step 1730, loss 0.614579.
Test: 2018-08-05T00:16:34.496843: step 1730, loss 0.547847.
Train: 2018-08-05T00:16:37.981585: step 1731, loss 0.605772.
Train: 2018-08-05T00:16:41.513207: step 1732, loss 0.596977.
Train: 2018-08-05T00:16:45.029203: step 1733, loss 0.588224.
Train: 2018-08-05T00:16:48.592079: step 1734, loss 0.588125.
Train: 2018-08-05T00:16:52.076822: step 1735, loss 0.545209.
Train: 2018-08-05T00:16:55.592818: step 1736, loss 0.519659.
Train: 2018-08-05T00:16:59.108814: step 1737, loss 0.545303.
Train: 2018-08-05T00:17:02.593557: step 1738, loss 0.621892.
Train: 2018-08-05T00:17:06.125180: step 1739, loss 0.536893.
Train: 2018-08-05T00:17:09.641176: step 1740, loss 0.596238.
Test: 2018-08-05T00:17:24.720893: step 1740, loss 0.548245.
Train: 2018-08-05T00:17:28.236889: step 1741, loss 0.553909.
Train: 2018-08-05T00:17:31.721632: step 1742, loss 0.537056.
Train: 2018-08-05T00:17:35.222002: step 1743, loss 0.528656.
Train: 2018-08-05T00:17:38.706744: step 1744, loss 0.553944.
Train: 2018-08-05T00:17:42.222741: step 1745, loss 0.579229.
Train: 2018-08-05T00:17:45.738737: step 1746, loss 0.520235.
Train: 2018-08-05T00:17:49.223480: step 1747, loss 0.562369.
Train: 2018-08-05T00:17:52.833236: step 1748, loss 0.444196.
Train: 2018-08-05T00:17:56.364858: step 1749, loss 0.604697.
Train: 2018-08-05T00:17:59.865228: step 1750, loss 0.570835.
Test: 2018-08-05T00:18:14.804304: step 1750, loss 0.546975.
Train: 2018-08-05T00:18:18.335928: step 1751, loss 0.545345.
Train: 2018-08-05T00:18:21.851924: step 1752, loss 0.579375.
Train: 2018-08-05T00:18:25.336667: step 1753, loss 0.562339.
Train: 2018-08-05T00:18:28.837036: step 1754, loss 0.519643.
Train: 2018-08-05T00:18:32.337406: step 1755, loss 0.579448.
Train: 2018-08-05T00:18:35.806522: step 1756, loss 0.562335.
Train: 2018-08-05T00:18:39.322518: step 1757, loss 0.553756.
Train: 2018-08-05T00:18:42.854141: step 1758, loss 0.596697.
Train: 2018-08-05T00:18:46.338884: step 1759, loss 0.502175.
Train: 2018-08-05T00:18:49.839253: step 1760, loss 0.50208.
Test: 2018-08-05T00:19:04.793957: step 1760, loss 0.547925.
Train: 2018-08-05T00:19:08.294326: step 1761, loss 0.562339.
Train: 2018-08-05T00:19:11.810322: step 1762, loss 0.545041.
Train: 2018-08-05T00:19:15.341945: step 1763, loss 0.58836.
Train: 2018-08-05T00:19:18.873568: step 1764, loss 0.623142.
Train: 2018-08-05T00:19:22.389564: step 1765, loss 0.544981.
Train: 2018-08-05T00:19:25.889933: step 1766, loss 0.518913.
Train: 2018-08-05T00:19:29.374676: step 1767, loss 0.623237.
Train: 2018-08-05T00:19:32.875046: step 1768, loss 0.562354.
Train: 2018-08-05T00:19:36.375416: step 1769, loss 0.544971.
Train: 2018-08-05T00:19:39.891412: step 1770, loss 0.579734.
Test: 2018-08-05T00:19:54.814862: step 1770, loss 0.546445.
Train: 2018-08-05T00:19:58.346485: step 1771, loss 0.58841.
Train: 2018-08-05T00:20:01.862481: step 1772, loss 0.553671.
Train: 2018-08-05T00:20:05.331597: step 1773, loss 0.536338.
Train: 2018-08-05T00:20:08.831966: step 1774, loss 0.588346.
Train: 2018-08-05T00:20:12.316709: step 1775, loss 0.60564.
Train: 2018-08-05T00:20:15.832705: step 1776, loss 0.562341.
Train: 2018-08-05T00:20:19.348702: step 1777, loss 0.588228.
Train: 2018-08-05T00:20:22.880324: step 1778, loss 0.553724.
Train: 2018-08-05T00:20:26.380694: step 1779, loss 0.519348.
Train: 2018-08-05T00:20:29.881063: step 1780, loss 0.553743.
Test: 2018-08-05T00:20:44.913901: step 1780, loss 0.547993.
Train: 2018-08-05T00:20:48.414270: step 1781, loss 0.536572.
Train: 2018-08-05T00:20:51.930266: step 1782, loss 0.631041.
Train: 2018-08-05T00:20:55.446262: step 1783, loss 0.639522.
Train: 2018-08-05T00:20:58.931004: step 1784, loss 0.562336.
Train: 2018-08-05T00:21:02.431375: step 1785, loss 0.613517.
Train: 2018-08-05T00:21:05.931743: step 1786, loss 0.545344.
Train: 2018-08-05T00:21:09.416487: step 1787, loss 0.553875.
Train: 2018-08-05T00:21:12.932483: step 1788, loss 0.5539.
Train: 2018-08-05T00:21:16.448479: step 1789, loss 0.562365.
Train: 2018-08-05T00:21:19.964475: step 1790, loss 0.503358.
Test: 2018-08-05T00:21:34.919180: step 1790, loss 0.548293.
Train: 2018-08-05T00:21:38.419548: step 1791, loss 0.638239.
Train: 2018-08-05T00:21:41.951171: step 1792, loss 0.545546.
Train: 2018-08-05T00:21:45.451540: step 1793, loss 0.596011.
Train: 2018-08-05T00:21:49.014417: step 1794, loss 0.595965.
Train: 2018-08-05T00:21:52.514786: step 1795, loss 0.520519.
Train: 2018-08-05T00:21:55.999529: step 1796, loss 0.579143.
Train: 2018-08-05T00:21:59.499898: step 1797, loss 0.462091.
Train: 2018-08-05T00:22:03.031521: step 1798, loss 0.520548.
Train: 2018-08-05T00:22:06.500637: step 1799, loss 0.621121.
Train: 2018-08-05T00:22:10.016633: step 1800, loss 0.51201.
Test: 2018-08-05T00:22:24.986964: step 1800, loss 0.547556.
Train: 2018-08-05T00:22:30.690691: step 1801, loss 0.579204.
Train: 2018-08-05T00:22:34.175434: step 1802, loss 0.545526.
Train: 2018-08-05T00:22:37.660177: step 1803, loss 0.612997.
Train: 2018-08-05T00:22:41.144920: step 1804, loss 0.604577.
Train: 2018-08-05T00:22:44.598409: step 1805, loss 0.545489.
Train: 2018-08-05T00:22:48.130032: step 1806, loss 0.570806.
Train: 2018-08-05T00:22:51.646028: step 1807, loss 0.528613.
Train: 2018-08-05T00:22:55.130770: step 1808, loss 0.596142.
Train: 2018-08-05T00:22:58.631140: step 1809, loss 0.553921.
Train: 2018-08-05T00:23:02.100257: step 1810, loss 0.494797.
Test: 2018-08-05T00:23:17.086213: step 1810, loss 0.548035.
Train: 2018-08-05T00:23:20.586583: step 1811, loss 0.579278.
Train: 2018-08-05T00:23:22.305514: step 1812, loss 0.526216.
Train: 2018-08-05T00:23:25.821510: step 1813, loss 0.596293.
Train: 2018-08-05T00:23:29.337507: step 1814, loss 0.58783.
Train: 2018-08-05T00:23:32.869129: step 1815, loss 0.545351.
Train: 2018-08-05T00:23:36.369499: step 1816, loss 0.553841.
Train: 2018-08-05T00:23:39.885495: step 1817, loss 0.545321.
Train: 2018-08-05T00:23:43.417118: step 1818, loss 0.562341.
Train: 2018-08-05T00:23:46.933114: step 1819, loss 0.587928.
Train: 2018-08-05T00:23:50.433483: step 1820, loss 0.485543.
Test: 2018-08-05T00:24:05.388186: step 1820, loss 0.547861.
Train: 2018-08-05T00:24:08.888556: step 1821, loss 0.656384.
Train: 2018-08-05T00:24:12.451432: step 1822, loss 0.587981.
Train: 2018-08-05T00:24:15.951802: step 1823, loss 0.545254.
Train: 2018-08-05T00:24:19.483425: step 1824, loss 0.562338.
Train: 2018-08-05T00:24:23.030674: step 1825, loss 0.519654.
Train: 2018-08-05T00:24:26.515417: step 1826, loss 0.60505.
Train: 2018-08-05T00:24:30.031413: step 1827, loss 0.47694.
Train: 2018-08-05T00:24:33.516156: step 1828, loss 0.6051.
Train: 2018-08-05T00:24:37.032152: step 1829, loss 0.613675.
Train: 2018-08-05T00:24:40.532522: step 1830, loss 0.485379.
Test: 2018-08-05T00:24:55.424718: step 1830, loss 0.547844.
Train: 2018-08-05T00:24:58.971968: step 1831, loss 0.596573.
Train: 2018-08-05T00:25:02.487964: step 1832, loss 0.579457.
Train: 2018-08-05T00:25:05.988334: step 1833, loss 0.545218.
Train: 2018-08-05T00:25:09.519957: step 1834, loss 0.570896.
Train: 2018-08-05T00:25:13.051579: step 1835, loss 0.596574.
Train: 2018-08-05T00:25:16.536322: step 1836, loss 0.587995.
Train: 2018-08-05T00:25:20.052318: step 1837, loss 0.545253.
Train: 2018-08-05T00:25:23.568314: step 1838, loss 0.587946.
Train: 2018-08-05T00:25:27.068684: step 1839, loss 0.54529.
Train: 2018-08-05T00:25:30.553427: step 1840, loss 0.553822.
Test: 2018-08-05T00:25:45.539383: step 1840, loss 0.548318.
Train: 2018-08-05T00:25:49.055380: step 1841, loss 0.519768.
Train: 2018-08-05T00:25:52.571376: step 1842, loss 0.545303.
Train: 2018-08-05T00:25:56.087372: step 1843, loss 0.62202.
Train: 2018-08-05T00:25:59.587742: step 1844, loss 0.536777.
Train: 2018-08-05T00:26:03.088111: step 1845, loss 0.562341.
Train: 2018-08-05T00:26:06.604107: step 1846, loss 0.536773.
Train: 2018-08-05T00:26:10.104477: step 1847, loss 0.570867.
Train: 2018-08-05T00:26:13.636100: step 1848, loss 0.587932.
Train: 2018-08-05T00:26:17.167723: step 1849, loss 0.596456.
Train: 2018-08-05T00:26:20.668092: step 1850, loss 0.519732.
Test: 2018-08-05T00:26:35.638422: step 1850, loss 0.547715.
Train: 2018-08-05T00:26:39.201298: step 1851, loss 0.545295.
Train: 2018-08-05T00:26:42.717294: step 1852, loss 0.613499.
Train: 2018-08-05T00:26:46.202037: step 1853, loss 0.55382.
Train: 2018-08-05T00:26:49.718033: step 1854, loss 0.553824.
Train: 2018-08-05T00:26:53.218403: step 1855, loss 0.58789.
Train: 2018-08-05T00:26:56.734399: step 1856, loss 0.562343.
Train: 2018-08-05T00:27:00.234768: step 1857, loss 0.613375.
Train: 2018-08-05T00:27:03.735138: step 1858, loss 0.494417.
Train: 2018-08-05T00:27:07.204254: step 1859, loss 0.604806.
Train: 2018-08-05T00:27:10.688997: step 1860, loss 0.545381.
Test: 2018-08-05T00:27:25.643700: step 1860, loss 0.548382.
Train: 2018-08-05T00:27:29.222203: step 1861, loss 0.536908.
Train: 2018-08-05T00:27:32.722573: step 1862, loss 0.570833.
Train: 2018-08-05T00:27:36.238570: step 1863, loss 0.596285.
Train: 2018-08-05T00:27:39.770192: step 1864, loss 0.494526.
Train: 2018-08-05T00:27:43.270561: step 1865, loss 0.587807.
Train: 2018-08-05T00:27:46.770931: step 1866, loss 0.519907.
Train: 2018-08-05T00:27:50.318180: step 1867, loss 0.545348.
Train: 2018-08-05T00:27:53.834176: step 1868, loss 0.494254.
Train: 2018-08-05T00:27:57.334546: step 1869, loss 0.613548.
Train: 2018-08-05T00:28:00.834915: step 1870, loss 0.536696.
Test: 2018-08-05T00:28:15.867752: step 1870, loss 0.548036.
Train: 2018-08-05T00:28:19.368122: step 1871, loss 0.562336.
Train: 2018-08-05T00:28:22.899745: step 1872, loss 0.545183.
Train: 2018-08-05T00:28:26.415741: step 1873, loss 0.570927.
Train: 2018-08-05T00:28:29.978617: step 1874, loss 0.545129.
Train: 2018-08-05T00:28:33.478986: step 1875, loss 0.562337.
Train: 2018-08-05T00:28:36.979355: step 1876, loss 0.588225.
Train: 2018-08-05T00:28:40.479725: step 1877, loss 0.484628.
Train: 2018-08-05T00:28:43.964468: step 1878, loss 0.579648.
Train: 2018-08-05T00:28:47.558597: step 1879, loss 0.484352.
Train: 2018-08-05T00:28:51.090220: step 1880, loss 0.605807.
Test: 2018-08-05T00:29:06.013672: step 1880, loss 0.54782.
Train: 2018-08-05T00:29:09.514040: step 1881, loss 0.605882.
Train: 2018-08-05T00:29:13.030036: step 1882, loss 0.562359.
Train: 2018-08-05T00:29:16.546032: step 1883, loss 0.553649.
Train: 2018-08-05T00:29:20.046402: step 1884, loss 0.614653.
Train: 2018-08-05T00:29:23.593651: step 1885, loss 0.605899.
Train: 2018-08-05T00:29:27.140900: step 1886, loss 0.597126.
Train: 2018-08-05T00:29:30.688150: step 1887, loss 0.588365.
Train: 2018-08-05T00:29:34.188519: step 1888, loss 0.545043.
Train: 2018-08-05T00:29:37.657636: step 1889, loss 0.622751.
Train: 2018-08-05T00:29:41.173632: step 1890, loss 0.519324.
Test: 2018-08-05T00:29:56.128336: step 1890, loss 0.548196.
Train: 2018-08-05T00:29:59.659958: step 1891, loss 0.656764.
Train: 2018-08-05T00:30:03.175955: step 1892, loss 0.51958.
Train: 2018-08-05T00:30:06.723203: step 1893, loss 0.587926.
Train: 2018-08-05T00:30:10.207947: step 1894, loss 0.519822.
Train: 2018-08-05T00:30:13.708316: step 1895, loss 0.545368.
Train: 2018-08-05T00:30:17.208685: step 1896, loss 0.553871.
Train: 2018-08-05T00:30:20.693429: step 1897, loss 0.562353.
Train: 2018-08-05T00:30:24.225051: step 1898, loss 0.54542.
Train: 2018-08-05T00:30:27.756685: step 1899, loss 0.545424.
Train: 2018-08-05T00:30:31.225790: step 1900, loss 0.545419.
Test: 2018-08-05T00:30:46.164868: step 1900, loss 0.546448.
Train: 2018-08-05T00:30:51.806088: step 1901, loss 0.528462.
Train: 2018-08-05T00:30:55.290830: step 1902, loss 0.51993.
Train: 2018-08-05T00:30:58.775573: step 1903, loss 0.562345.
Train: 2018-08-05T00:31:02.291570: step 1904, loss 0.57086.
Train: 2018-08-05T00:31:05.823192: step 1905, loss 0.519682.
Train: 2018-08-05T00:31:09.323562: step 1906, loss 0.579439.
Train: 2018-08-05T00:31:12.823931: step 1907, loss 0.528073.
Train: 2018-08-05T00:31:16.355554: step 1908, loss 0.55375.
Train: 2018-08-05T00:31:19.824670: step 1909, loss 0.579542.
Train: 2018-08-05T00:31:23.356293: step 1910, loss 0.545103.
Test: 2018-08-05T00:31:38.342250: step 1910, loss 0.547725.
Train: 2018-08-05T00:31:41.858246: step 1911, loss 0.562339.
Train: 2018-08-05T00:31:45.374242: step 1912, loss 0.596918.
Train: 2018-08-05T00:31:48.905865: step 1913, loss 0.562342.
Train: 2018-08-05T00:31:52.453114: step 1914, loss 0.55369.
Train: 2018-08-05T00:31:55.969110: step 1915, loss 0.571001.
Train: 2018-08-05T00:31:59.500733: step 1916, loss 0.536367.
Train: 2018-08-05T00:32:02.985476: step 1917, loss 0.58834.
Train: 2018-08-05T00:32:06.501472: step 1918, loss 0.510357.
Train: 2018-08-05T00:32:10.017469: step 1919, loss 0.623061.
Train: 2018-08-05T00:32:13.502211: step 1920, loss 0.649042.
Test: 2018-08-05T00:32:28.456914: step 1920, loss 0.549887.
Train: 2018-08-05T00:32:31.988538: step 1921, loss 0.519092.
Train: 2018-08-05T00:32:35.645174: step 1922, loss 0.579619.
Train: 2018-08-05T00:32:39.161170: step 1923, loss 0.622725.
Train: 2018-08-05T00:32:42.677166: step 1924, loss 0.502109.
Train: 2018-08-05T00:32:46.161909: step 1925, loss 0.536554.
Train: 2018-08-05T00:32:49.646652: step 1926, loss 0.570924.
Train: 2018-08-05T00:32:53.131395: step 1927, loss 0.588084.
Train: 2018-08-05T00:32:56.663017: step 1928, loss 0.528045.
Train: 2018-08-05T00:33:00.194640: step 1929, loss 0.596613.
Train: 2018-08-05T00:33:03.710636: step 1930, loss 0.553776.
Test: 2018-08-05T00:33:18.727847: step 1930, loss 0.548052.
Train: 2018-08-05T00:33:22.243842: step 1931, loss 0.536677.
Train: 2018-08-05T00:33:25.744212: step 1932, loss 0.613646.
Train: 2018-08-05T00:33:29.260208: step 1933, loss 0.58796.
Train: 2018-08-05T00:33:32.791831: step 1934, loss 0.485603.
Train: 2018-08-05T00:33:36.307827: step 1935, loss 0.553812.
Train: 2018-08-05T00:33:39.839450: step 1936, loss 0.51116.
Train: 2018-08-05T00:33:43.386699: step 1937, loss 0.51963.
Train: 2018-08-05T00:33:46.887069: step 1938, loss 0.562336.
Train: 2018-08-05T00:33:50.403065: step 1939, loss 0.639519.
Train: 2018-08-05T00:33:53.919061: step 1940, loss 0.570912.
Test: 2018-08-05T00:34:08.905020: step 1940, loss 0.548409.
Train: 2018-08-05T00:34:12.467894: step 1941, loss 0.476583.
Train: 2018-08-05T00:34:15.999517: step 1942, loss 0.553746.
Train: 2018-08-05T00:34:19.499887: step 1943, loss 0.553732.
Train: 2018-08-05T00:34:22.984629: step 1944, loss 0.545102.
Train: 2018-08-05T00:34:26.484998: step 1945, loss 0.579604.
Train: 2018-08-05T00:34:30.000995: step 1946, loss 0.62284.
Train: 2018-08-05T00:34:33.532618: step 1947, loss 0.527778.
Train: 2018-08-05T00:34:37.048614: step 1948, loss 0.51912.
Train: 2018-08-05T00:34:40.548983: step 1949, loss 0.588307.
Train: 2018-08-05T00:34:44.033726: step 1950, loss 0.588321.
Test: 2018-08-05T00:34:59.035312: step 1950, loss 0.547884.
Train: 2018-08-05T00:35:02.566932: step 1951, loss 0.579659.
Train: 2018-08-05T00:35:06.082930: step 1952, loss 0.579648.
Train: 2018-08-05T00:35:09.630178: step 1953, loss 0.553696.
Train: 2018-08-05T00:35:13.130548: step 1954, loss 0.648724.
Train: 2018-08-05T00:35:16.724677: step 1955, loss 0.562337.
Train: 2018-08-05T00:35:20.209420: step 1956, loss 0.502157.
Train: 2018-08-05T00:35:23.725416: step 1957, loss 0.527976.
Train: 2018-08-05T00:35:27.210159: step 1958, loss 0.570924.
Train: 2018-08-05T00:35:30.726155: step 1959, loss 0.502228.
Train: 2018-08-05T00:35:34.226525: step 1960, loss 0.510764.
Test: 2018-08-05T00:35:49.212482: step 1960, loss 0.547953.
Train: 2018-08-05T00:35:52.697224: step 1961, loss 0.596784.
Train: 2018-08-05T00:35:56.197593: step 1962, loss 0.536476.
Train: 2018-08-05T00:35:57.916525: step 1963, loss 0.617586.
Train: 2018-08-05T00:36:01.432521: step 1964, loss 0.579604.
Train: 2018-08-05T00:36:04.917264: step 1965, loss 0.562338.
Train: 2018-08-05T00:36:08.448887: step 1966, loss 0.519207.
Train: 2018-08-05T00:36:11.964883: step 1967, loss 0.553708.
Train: 2018-08-05T00:36:15.465253: step 1968, loss 0.562339.
Train: 2018-08-05T00:36:18.981249: step 1969, loss 0.484586.
Train: 2018-08-05T00:36:22.481618: step 1970, loss 0.527717.
Test: 2018-08-05T00:36:37.420694: step 1970, loss 0.546858.
Train: 2018-08-05T00:36:40.967944: step 1971, loss 0.588381.
Train: 2018-08-05T00:36:44.499567: step 1972, loss 0.544971.
Train: 2018-08-05T00:36:48.046817: step 1973, loss 0.57977.
Train: 2018-08-05T00:36:51.547186: step 1974, loss 0.527501.
Train: 2018-08-05T00:36:55.016302: step 1975, loss 0.501266.
Train: 2018-08-05T00:36:58.547925: step 1976, loss 0.501121.
Train: 2018-08-05T00:37:02.079548: step 1977, loss 0.6063.
Train: 2018-08-05T00:37:05.626797: step 1978, loss 0.483214.
Train: 2018-08-05T00:37:09.127167: step 1979, loss 0.571255.
Train: 2018-08-05T00:37:12.643163: step 1980, loss 0.588998.
Test: 2018-08-05T00:37:27.629122: step 1980, loss 0.547854.
Train: 2018-08-05T00:37:31.176369: step 1981, loss 0.535859.
Train: 2018-08-05T00:37:34.676739: step 1982, loss 0.544707.
Train: 2018-08-05T00:37:38.192735: step 1983, loss 0.54469.
Train: 2018-08-05T00:37:41.708731: step 1984, loss 0.624902.
Train: 2018-08-05T00:37:45.177848: step 1985, loss 0.482261.
Train: 2018-08-05T00:37:48.678217: step 1986, loss 0.562518.
Train: 2018-08-05T00:37:52.178587: step 1987, loss 0.562529.
Train: 2018-08-05T00:37:55.678956: step 1988, loss 0.62517.
Train: 2018-08-05T00:37:59.194951: step 1989, loss 0.571472.
Train: 2018-08-05T00:38:02.679695: step 1990, loss 0.669709.
Test: 2018-08-05T00:38:17.775039: step 1990, loss 0.548442.
Train: 2018-08-05T00:38:21.275408: step 1991, loss 0.580299.
Train: 2018-08-05T00:38:24.822657: step 1992, loss 0.562464.
Train: 2018-08-05T00:38:28.323027: step 1993, loss 0.624348.
Train: 2018-08-05T00:38:31.807769: step 1994, loss 0.562411.
Train: 2018-08-05T00:38:35.370646: step 1995, loss 0.544849.
Train: 2018-08-05T00:38:38.886642: step 1996, loss 0.544894.
Train: 2018-08-05T00:38:42.387011: step 1997, loss 0.562361.
Train: 2018-08-05T00:38:45.918634: step 1998, loss 0.553662.
Train: 2018-08-05T00:38:49.419004: step 1999, loss 0.597026.
Train: 2018-08-05T00:38:52.903747: step 2000, loss 0.622854.
Test: 2018-08-05T00:39:07.858450: step 2000, loss 0.548154.
Train: 2018-08-05T00:39:13.624684: step 2001, loss 0.493451.
Train: 2018-08-05T00:39:17.125053: step 2002, loss 0.579519.
Train: 2018-08-05T00:39:20.641049: step 2003, loss 0.510903.
Train: 2018-08-05T00:39:24.125792: step 2004, loss 0.596588.
Train: 2018-08-05T00:39:27.626161: step 2005, loss 0.587983.
Train: 2018-08-05T00:39:31.142158: step 2006, loss 0.553808.
Train: 2018-08-05T00:39:34.642527: step 2007, loss 0.553825.
Train: 2018-08-05T00:39:38.158524: step 2008, loss 0.4943.
Train: 2018-08-05T00:39:41.674520: step 2009, loss 0.553836.
Train: 2018-08-05T00:39:45.143636: step 2010, loss 0.579367.
Test: 2018-08-05T00:40:00.082713: step 2010, loss 0.547734.
Train: 2018-08-05T00:40:03.629962: step 2011, loss 0.55383.
Train: 2018-08-05T00:40:07.130332: step 2012, loss 0.494224.
Train: 2018-08-05T00:40:10.630702: step 2013, loss 0.596457.
Train: 2018-08-05T00:40:14.131071: step 2014, loss 0.528193.
Train: 2018-08-05T00:40:17.615813: step 2015, loss 0.579434.
Train: 2018-08-05T00:40:21.131810: step 2016, loss 0.613676.
Train: 2018-08-05T00:40:24.632180: step 2017, loss 0.57089.
Train: 2018-08-05T00:40:28.132549: step 2018, loss 0.502481.
Train: 2018-08-05T00:40:31.632918: step 2019, loss 0.579453.
Train: 2018-08-05T00:40:35.148915: step 2020, loss 0.476715.
Test: 2018-08-05T00:40:50.134871: step 2020, loss 0.548597.
Train: 2018-08-05T00:40:53.650867: step 2021, loss 0.536593.
Train: 2018-08-05T00:40:57.166863: step 2022, loss 0.527927.
Train: 2018-08-05T00:41:00.682860: step 2023, loss 0.562338.
Train: 2018-08-05T00:41:04.167603: step 2024, loss 0.475847.
Train: 2018-08-05T00:41:07.699225: step 2025, loss 0.510246.
Train: 2018-08-05T00:41:11.199595: step 2026, loss 0.527469.
Train: 2018-08-05T00:41:14.715591: step 2027, loss 0.579919.
Train: 2018-08-05T00:41:18.231587: step 2028, loss 0.588802.
Train: 2018-08-05T00:41:21.716330: step 2029, loss 0.544781.
Train: 2018-08-05T00:41:25.232326: step 2030, loss 0.624365.
Test: 2018-08-05T00:41:40.187030: step 2030, loss 0.546029.
Train: 2018-08-05T00:41:43.687399: step 2031, loss 0.535887.
Train: 2018-08-05T00:41:47.219022: step 2032, loss 0.518127.
Train: 2018-08-05T00:41:50.750645: step 2033, loss 0.553584.
Train: 2018-08-05T00:41:54.251014: step 2034, loss 0.482425.
Train: 2018-08-05T00:41:57.751384: step 2035, loss 0.526822.
Train: 2018-08-05T00:42:01.267380: step 2036, loss 0.580435.
Train: 2018-08-05T00:42:04.752122: step 2037, loss 0.59843.
Train: 2018-08-05T00:42:08.252492: step 2038, loss 0.50871.
Train: 2018-08-05T00:42:11.784115: step 2039, loss 0.58957.
Train: 2018-08-05T00:42:15.300111: step 2040, loss 0.616592.
Test: 2018-08-05T00:42:30.270442: step 2040, loss 0.547571.
Train: 2018-08-05T00:42:33.911451: step 2041, loss 0.571584.
Train: 2018-08-05T00:42:37.411820: step 2042, loss 0.571571.
Train: 2018-08-05T00:42:40.912190: step 2043, loss 0.562578.
Train: 2018-08-05T00:42:44.459439: step 2044, loss 0.526706.
Train: 2018-08-05T00:42:47.959809: step 2045, loss 0.60732.
Train: 2018-08-05T00:42:51.475805: step 2046, loss 0.589344.
Train: 2018-08-05T00:42:54.976174: step 2047, loss 0.58034.
Train: 2018-08-05T00:42:58.492170: step 2048, loss 0.58027.
Train: 2018-08-05T00:43:01.992540: step 2049, loss 0.535856.
Train: 2018-08-05T00:43:05.524163: step 2050, loss 0.473971.
Test: 2018-08-05T00:43:20.619508: step 2050, loss 0.546042.
Train: 2018-08-05T00:43:24.135502: step 2051, loss 0.544753.
Train: 2018-08-05T00:43:27.635872: step 2052, loss 0.712726.
Train: 2018-08-05T00:43:31.120614: step 2053, loss 0.659332.
Train: 2018-08-05T00:43:34.652237: step 2054, loss 0.623746.
Train: 2018-08-05T00:43:38.168233: step 2055, loss 0.553648.
Train: 2018-08-05T00:43:41.699856: step 2056, loss 0.562346.
Train: 2018-08-05T00:43:45.200226: step 2057, loss 0.639956.
Train: 2018-08-05T00:43:48.763102: step 2058, loss 0.58805.
Train: 2018-08-05T00:43:52.247845: step 2059, loss 0.553821.
Train: 2018-08-05T00:43:55.763841: step 2060, loss 0.545404.
Test: 2018-08-05T00:44:10.749797: step 2060, loss 0.550025.
Train: 2018-08-05T00:44:14.265794: step 2061, loss 0.579242.
Train: 2018-08-05T00:44:17.844296: step 2062, loss 0.545584.
Train: 2018-08-05T00:44:21.375919: step 2063, loss 0.579148.
Train: 2018-08-05T00:44:24.860662: step 2064, loss 0.562423.
Train: 2018-08-05T00:44:28.361032: step 2065, loss 0.595719.
Train: 2018-08-05T00:44:31.861401: step 2066, loss 0.529299.
Train: 2018-08-05T00:44:35.361771: step 2067, loss 0.61213.
Train: 2018-08-05T00:44:38.877767: step 2068, loss 0.570756.
Train: 2018-08-05T00:44:42.378136: step 2069, loss 0.562528.
Train: 2018-08-05T00:44:45.894132: step 2070, loss 0.587184.
Test: 2018-08-05T00:45:00.864463: step 2070, loss 0.547755.
Train: 2018-08-05T00:45:04.411713: step 2071, loss 0.521604.
Train: 2018-08-05T00:45:07.927721: step 2072, loss 0.570765.
Train: 2018-08-05T00:45:11.428078: step 2073, loss 0.54623.
Train: 2018-08-05T00:45:14.944074: step 2074, loss 0.587122.
Train: 2018-08-05T00:45:18.491323: step 2075, loss 0.562594.
Train: 2018-08-05T00:45:21.991693: step 2076, loss 0.587112.
Train: 2018-08-05T00:45:25.507689: step 2077, loss 0.595272.
Train: 2018-08-05T00:45:29.008059: step 2078, loss 0.595249.
Train: 2018-08-05T00:45:32.539681: step 2079, loss 0.546336.
Train: 2018-08-05T00:45:36.024424: step 2080, loss 0.473088.
Test: 2018-08-05T00:45:51.057262: step 2080, loss 0.549563.
Train: 2018-08-05T00:45:54.573257: step 2081, loss 0.611548.
Train: 2018-08-05T00:45:58.104880: step 2082, loss 0.570772.
Train: 2018-08-05T00:46:01.620876: step 2083, loss 0.5871.
Train: 2018-08-05T00:46:05.089992: step 2084, loss 0.546273.
Train: 2018-08-05T00:46:08.574735: step 2085, loss 0.554424.
Train: 2018-08-05T00:46:12.075105: step 2086, loss 0.497137.
Train: 2018-08-05T00:46:15.606727: step 2087, loss 0.660997.
Train: 2018-08-05T00:46:19.153977: step 2088, loss 0.513317.
Train: 2018-08-05T00:46:22.638720: step 2089, loss 0.505.
Train: 2018-08-05T00:46:26.123463: step 2090, loss 0.504802.
Test: 2018-08-05T00:46:41.078167: step 2090, loss 0.549223.
Train: 2018-08-05T00:46:44.578536: step 2091, loss 0.504527.
Train: 2018-08-05T00:46:48.063278: step 2092, loss 0.595727.
Train: 2018-08-05T00:46:51.610528: step 2093, loss 0.587481.
Train: 2018-08-05T00:46:55.142151: step 2094, loss 0.579161.
Train: 2018-08-05T00:46:58.642520: step 2095, loss 0.503562.
Train: 2018-08-05T00:47:02.142890: step 2096, loss 0.638275.
Train: 2018-08-05T00:47:05.658886: step 2097, loss 0.570811.
Train: 2018-08-05T00:47:09.190509: step 2098, loss 0.570818.
Train: 2018-08-05T00:47:12.706505: step 2099, loss 0.528477.
Train: 2018-08-05T00:47:16.238127: step 2100, loss 0.596287.
Test: 2018-08-05T00:47:31.224085: step 2100, loss 0.549143.
Train: 2018-08-05T00:47:37.068452: step 2101, loss 0.613299.
Train: 2018-08-05T00:47:40.568821: step 2102, loss 0.621775.
Train: 2018-08-05T00:47:44.069190: step 2103, loss 0.587783.
Train: 2018-08-05T00:47:47.585186: step 2104, loss 0.570819.
Train: 2018-08-05T00:47:51.069930: step 2105, loss 0.477907.
Train: 2018-08-05T00:47:54.632806: step 2106, loss 0.562363.
Train: 2018-08-05T00:47:58.133175: step 2107, loss 0.587713.
Train: 2018-08-05T00:48:01.633545: step 2108, loss 0.486331.
Train: 2018-08-05T00:48:05.133914: step 2109, loss 0.579279.
Train: 2018-08-05T00:48:08.634284: step 2110, loss 0.553886.
Test: 2018-08-05T00:48:23.667122: step 2110, loss 0.547997.
Train: 2018-08-05T00:48:27.183117: step 2111, loss 0.511481.
Train: 2018-08-05T00:48:30.761619: step 2112, loss 0.545353.
Train: 2018-08-05T00:48:34.261989: step 2113, loss 0.638987.
Train: 2018-08-05T00:48:35.965293: step 2114, loss 0.580517.
Train: 2018-08-05T00:48:39.465663: step 2115, loss 0.562341.
Train: 2018-08-05T00:48:42.981659: step 2116, loss 0.553821.
Train: 2018-08-05T00:48:46.497656: step 2117, loss 0.604952.
Train: 2018-08-05T00:48:50.060531: step 2118, loss 0.536793.
Train: 2018-08-05T00:48:53.560901: step 2119, loss 0.528279.
Train: 2018-08-05T00:48:57.076897: step 2120, loss 0.562341.
Test: 2018-08-05T00:49:12.047228: step 2120, loss 0.547511.
Train: 2018-08-05T00:49:15.563223: step 2121, loss 0.53676.
Train: 2018-08-05T00:49:19.110473: step 2122, loss 0.511122.
Train: 2018-08-05T00:49:22.626469: step 2123, loss 0.622213.
Train: 2018-08-05T00:49:26.158092: step 2124, loss 0.562336.
Train: 2018-08-05T00:49:29.658461: step 2125, loss 0.510955.
Train: 2018-08-05T00:49:33.143204: step 2126, loss 0.579488.
Train: 2018-08-05T00:49:36.659201: step 2127, loss 0.57092.
Train: 2018-08-05T00:49:40.128317: step 2128, loss 0.562335.
Train: 2018-08-05T00:49:43.644313: step 2129, loss 0.59672.
Train: 2018-08-05T00:49:47.160309: step 2130, loss 0.570929.
Test: 2018-08-05T00:50:02.177519: step 2130, loss 0.547197.
Train: 2018-08-05T00:50:05.709142: step 2131, loss 0.519381.
Train: 2018-08-05T00:50:09.209511: step 2132, loss 0.579525.
Train: 2018-08-05T00:50:12.694254: step 2133, loss 0.519357.
Train: 2018-08-05T00:50:16.210250: step 2134, loss 0.553732.
Train: 2018-08-05T00:50:19.741873: step 2135, loss 0.5365.
Train: 2018-08-05T00:50:23.257869: step 2136, loss 0.519218.
Train: 2018-08-05T00:50:26.820745: step 2137, loss 0.666043.
Train: 2018-08-05T00:50:30.321114: step 2138, loss 0.545062.
Train: 2018-08-05T00:50:33.821485: step 2139, loss 0.605534.
Train: 2018-08-05T00:50:37.321854: step 2140, loss 0.545077.
Test: 2018-08-05T00:50:52.292184: step 2140, loss 0.547932.
Train: 2018-08-05T00:50:55.808180: step 2141, loss 0.570964.
Train: 2018-08-05T00:50:59.402310: step 2142, loss 0.527858.
Train: 2018-08-05T00:51:02.933932: step 2143, loss 0.510618.
Train: 2018-08-05T00:51:06.434302: step 2144, loss 0.648627.
Train: 2018-08-05T00:51:09.934671: step 2145, loss 0.66579.
Train: 2018-08-05T00:51:13.435040: step 2146, loss 0.579525.
Train: 2018-08-05T00:51:16.935410: step 2147, loss 0.58804.
Train: 2018-08-05T00:51:20.482659: step 2148, loss 0.596496.
Train: 2018-08-05T00:51:23.998656: step 2149, loss 0.63891.
Train: 2018-08-05T00:51:27.499025: step 2150, loss 0.545427.
Test: 2018-08-05T00:51:42.516236: step 2150, loss 0.547517.
Train: 2018-08-05T00:51:46.047858: step 2151, loss 0.503364.
Train: 2018-08-05T00:51:49.563854: step 2152, loss 0.654885.
Train: 2018-08-05T00:51:53.126730: step 2153, loss 0.587527.
Train: 2018-08-05T00:51:56.642726: step 2154, loss 0.537406.
Train: 2018-08-05T00:52:00.174349: step 2155, loss 0.653904.
Train: 2018-08-05T00:52:03.674719: step 2156, loss 0.620408.
Train: 2018-08-05T00:52:07.190715: step 2157, loss 0.570758.
Train: 2018-08-05T00:52:10.706711: step 2158, loss 0.603525.
Train: 2018-08-05T00:52:14.207081: step 2159, loss 0.587072.
Train: 2018-08-05T00:52:17.754330: step 2160, loss 0.505937.
Test: 2018-08-05T00:52:32.755913: step 2160, loss 0.549967.
Train: 2018-08-05T00:52:36.287537: step 2161, loss 0.554639.
Train: 2018-08-05T00:52:39.819159: step 2162, loss 0.603078.
Train: 2018-08-05T00:52:43.319529: step 2163, loss 0.522566.
Train: 2018-08-05T00:52:46.819898: step 2164, loss 0.562802.
Train: 2018-08-05T00:52:50.320268: step 2165, loss 0.635074.
Train: 2018-08-05T00:52:53.851890: step 2166, loss 0.562841.
Train: 2018-08-05T00:52:57.352260: step 2167, loss 0.506862.
Train: 2018-08-05T00:53:00.852629: step 2168, loss 0.538852.
Train: 2018-08-05T00:53:04.337372: step 2169, loss 0.666993.
Train: 2018-08-05T00:53:07.822115: step 2170, loss 0.610873.
Test: 2018-08-05T00:53:22.886205: step 2170, loss 0.548627.
Train: 2018-08-05T00:53:26.417828: step 2171, loss 0.546911.
Train: 2018-08-05T00:53:29.965078: step 2172, loss 0.562901.
Train: 2018-08-05T00:53:33.512327: step 2173, loss 0.52303.
Train: 2018-08-05T00:53:36.997070: step 2174, loss 0.562896.
Train: 2018-08-05T00:53:40.575572: step 2175, loss 0.610826.
Train: 2018-08-05T00:53:44.075942: step 2176, loss 0.578862.
Train: 2018-08-05T00:53:47.576312: step 2177, loss 0.514919.
Train: 2018-08-05T00:53:51.107935: step 2178, loss 0.522826.
Train: 2018-08-05T00:53:54.655184: step 2179, loss 0.570842.
Train: 2018-08-05T00:53:58.155553: step 2180, loss 0.530592.
Test: 2018-08-05T00:54:13.297777: step 2180, loss 0.547952.
Train: 2018-08-05T00:54:16.829399: step 2181, loss 0.570811.
Train: 2018-08-05T00:54:20.407902: step 2182, loss 0.506007.
Train: 2018-08-05T00:54:23.939525: step 2183, loss 0.538245.
Train: 2018-08-05T00:54:27.439894: step 2184, loss 0.50539.
Train: 2018-08-05T00:54:30.971517: step 2185, loss 0.570759.
Train: 2018-08-05T00:54:34.487513: step 2186, loss 0.595541.
Train: 2018-08-05T00:54:37.987883: step 2187, loss 0.537579.
Train: 2018-08-05T00:54:41.503879: step 2188, loss 0.587425.
Train: 2018-08-05T00:54:44.988622: step 2189, loss 0.646002.
Train: 2018-08-05T00:54:48.504618: step 2190, loss 0.562405.
Test: 2018-08-05T00:55:03.474948: step 2190, loss 0.54763.
Train: 2018-08-05T00:55:07.006571: step 2191, loss 0.503732.
Train: 2018-08-05T00:55:10.506941: step 2192, loss 0.562385.
Train: 2018-08-05T00:55:14.022937: step 2193, loss 0.562375.
Train: 2018-08-05T00:55:17.507680: step 2194, loss 0.570806.
Train: 2018-08-05T00:55:21.101809: step 2195, loss 0.553904.
Train: 2018-08-05T00:55:24.633432: step 2196, loss 0.545411.
Train: 2018-08-05T00:55:28.149428: step 2197, loss 0.570837.
Train: 2018-08-05T00:55:31.712304: step 2198, loss 0.630374.
Train: 2018-08-05T00:55:35.197047: step 2199, loss 0.570849.
Train: 2018-08-05T00:55:38.697417: step 2200, loss 0.55384.
Test: 2018-08-05T00:55:53.714628: step 2200, loss 0.548531.
Train: 2018-08-05T00:55:59.371474: step 2201, loss 0.528323.
Train: 2018-08-05T00:56:02.856216: step 2202, loss 0.536804.
Train: 2018-08-05T00:56:06.356586: step 2203, loss 0.494147.
Train: 2018-08-05T00:56:09.888209: step 2204, loss 0.570884.
Train: 2018-08-05T00:56:13.451085: step 2205, loss 0.545204.
Train: 2018-08-05T00:56:16.935828: step 2206, loss 0.605263.
Train: 2018-08-05T00:56:20.436197: step 2207, loss 0.527953.
Train: 2018-08-05T00:56:23.920940: step 2208, loss 0.605389.
Train: 2018-08-05T00:56:27.468189: step 2209, loss 0.553721.
Train: 2018-08-05T00:56:30.999812: step 2210, loss 0.570959.
Test: 2018-08-05T00:56:45.970142: step 2210, loss 0.54833.
Train: 2018-08-05T00:56:49.486139: step 2211, loss 0.536462.
Train: 2018-08-05T00:56:52.970882: step 2212, loss 0.588236.
Train: 2018-08-05T00:56:56.471251: step 2213, loss 0.605509.
Train: 2018-08-05T00:56:59.971621: step 2214, loss 0.614101.
Train: 2018-08-05T00:57:03.503243: step 2215, loss 0.596781.
Train: 2018-08-05T00:57:07.050493: step 2216, loss 0.579516.
Train: 2018-08-05T00:57:10.550862: step 2217, loss 0.605177.
Train: 2018-08-05T00:57:14.035605: step 2218, loss 0.587959.
Train: 2018-08-05T00:57:17.535974: step 2219, loss 0.570854.
Train: 2018-08-05T00:57:21.036344: step 2220, loss 0.56235.
Test: 2018-08-05T00:57:36.037928: step 2220, loss 0.548818.
Train: 2018-08-05T00:57:39.616430: step 2221, loss 0.503156.
Train: 2018-08-05T00:57:43.163680: step 2222, loss 0.596149.
Train: 2018-08-05T00:57:46.679676: step 2223, loss 0.562371.
Train: 2018-08-05T00:57:50.164419: step 2224, loss 0.537128.
Train: 2018-08-05T00:57:53.664788: step 2225, loss 0.5792.
Train: 2018-08-05T00:57:57.149531: step 2226, loss 0.570786.
Train: 2018-08-05T00:58:00.665527: step 2227, loss 0.528827.
Train: 2018-08-05T00:58:04.181523: step 2228, loss 0.528832.
Train: 2018-08-05T00:58:07.744400: step 2229, loss 0.512014.
Train: 2018-08-05T00:58:11.307276: step 2230, loss 0.604436.
Test: 2018-08-05T00:58:26.621392: step 2230, loss 0.548125.
Train: 2018-08-05T00:58:30.137388: step 2231, loss 0.570795.
Train: 2018-08-05T00:58:33.637757: step 2232, loss 0.596065.
Train: 2018-08-05T00:58:37.153754: step 2233, loss 0.528689.
Train: 2018-08-05T00:58:40.669750: step 2234, loss 0.570799.
Train: 2018-08-05T00:58:44.185746: step 2235, loss 0.638252.
Train: 2018-08-05T00:58:47.748622: step 2236, loss 0.579218.
Train: 2018-08-05T00:58:51.233365: step 2237, loss 0.53715.
Train: 2018-08-05T00:58:54.733735: step 2238, loss 0.46152.
Train: 2018-08-05T00:58:58.249731: step 2239, loss 0.528695.
Train: 2018-08-05T00:59:01.796980: step 2240, loss 0.545486.
Test: 2018-08-05T00:59:16.814191: step 2240, loss 0.54881.
Train: 2018-08-05T00:59:20.392693: step 2241, loss 0.477739.
Train: 2018-08-05T00:59:23.908689: step 2242, loss 0.545352.
Train: 2018-08-05T00:59:27.409059: step 2243, loss 0.545274.
Train: 2018-08-05T00:59:30.909428: step 2244, loss 0.60517.
Train: 2018-08-05T00:59:34.441051: step 2245, loss 0.588106.
Train: 2018-08-05T00:59:37.988300: step 2246, loss 0.467665.
Train: 2018-08-05T00:59:41.488670: step 2247, loss 0.501877.
Train: 2018-08-05T00:59:45.004666: step 2248, loss 0.475589.
Train: 2018-08-05T00:59:48.505035: step 2249, loss 0.562366.
Train: 2018-08-05T00:59:51.989778: step 2250, loss 0.615.
Test: 2018-08-05T01:00:06.991363: step 2250, loss 0.547915.
Train: 2018-08-05T01:00:10.538611: step 2251, loss 0.58.
Train: 2018-08-05T01:00:14.038981: step 2252, loss 0.47423.
Train: 2018-08-05T01:00:17.586230: step 2253, loss 0.456216.
Train: 2018-08-05T01:00:21.086600: step 2254, loss 0.535789.
Train: 2018-08-05T01:00:24.618222: step 2255, loss 0.598314.
Train: 2018-08-05T01:00:28.118592: step 2256, loss 0.580527.
Train: 2018-08-05T01:00:31.618962: step 2257, loss 0.562602.
Train: 2018-08-05T01:00:35.150584: step 2258, loss 0.526542.
Train: 2018-08-05T01:00:38.666581: step 2259, loss 0.562657.
Train: 2018-08-05T01:00:42.166950: step 2260, loss 0.562681.
Test: 2018-08-05T01:00:57.121655: step 2260, loss 0.547339.
Train: 2018-08-05T01:01:00.637649: step 2261, loss 0.553627.
Train: 2018-08-05T01:01:04.138019: step 2262, loss 0.535465.
Train: 2018-08-05T01:01:07.654016: step 2263, loss 0.480876.
Train: 2018-08-05T01:01:11.201265: step 2264, loss 0.553651.
Train: 2018-08-05T01:01:12.920196: step 2265, loss 0.543309.
Train: 2018-08-05T01:01:16.436193: step 2266, loss 0.553675.
Train: 2018-08-05T01:01:19.952189: step 2267, loss 0.517018.
Train: 2018-08-05T01:01:23.452558: step 2268, loss 0.599619.
Train: 2018-08-05T01:01:26.937301: step 2269, loss 0.526133.
Train: 2018-08-05T01:01:30.468924: step 2270, loss 0.618108.
Test: 2018-08-05T01:01:45.501762: step 2270, loss 0.547123.
Train: 2018-08-05T01:01:49.002130: step 2271, loss 0.526123.
Train: 2018-08-05T01:01:52.533753: step 2272, loss 0.507736.
Train: 2018-08-05T01:01:56.065376: step 2273, loss 0.507713.
Train: 2018-08-05T01:01:59.565745: step 2274, loss 0.608987.
Train: 2018-08-05T01:02:03.034861: step 2275, loss 0.581351.
Train: 2018-08-05T01:02:06.582111: step 2276, loss 0.608932.
Train: 2018-08-05T01:02:10.098107: step 2277, loss 0.599629.
Train: 2018-08-05T01:02:13.614103: step 2278, loss 0.572003.
Train: 2018-08-05T01:02:17.130100: step 2279, loss 0.517125.
Train: 2018-08-05T01:02:20.630469: step 2280, loss 0.544535.
Test: 2018-08-05T01:02:35.632052: step 2280, loss 0.549219.
Train: 2018-08-05T01:02:39.163675: step 2281, loss 0.489952.
Train: 2018-08-05T01:02:42.648418: step 2282, loss 0.462709.
Train: 2018-08-05T01:02:46.211294: step 2283, loss 0.517233.
Train: 2018-08-05T01:02:49.696037: step 2284, loss 0.498954.
Train: 2018-08-05T01:02:53.212033: step 2285, loss 0.5628.
Train: 2018-08-05T01:02:56.712403: step 2286, loss 0.53537.
Train: 2018-08-05T01:03:00.212772: step 2287, loss 0.544518.
Train: 2018-08-05T01:03:03.744395: step 2288, loss 0.516969.
Train: 2018-08-05T01:03:07.260391: step 2289, loss 0.59051.
Train: 2018-08-05T01:03:10.776387: step 2290, loss 0.544511.
Test: 2018-08-05T01:03:25.777970: step 2290, loss 0.547763.
Train: 2018-08-05T01:03:29.293967: step 2291, loss 0.609015.
Train: 2018-08-05T01:03:32.809963: step 2292, loss 0.553721.
Train: 2018-08-05T01:03:36.325959: step 2293, loss 0.553716.
Train: 2018-08-05T01:03:39.857582: step 2294, loss 0.526116.
Train: 2018-08-05T01:03:43.420457: step 2295, loss 0.535317.
Train: 2018-08-05T01:03:46.936454: step 2296, loss 0.562902.
Train: 2018-08-05T01:03:50.468077: step 2297, loss 0.618037.
Train: 2018-08-05T01:03:53.984073: step 2298, loss 0.581214.
Train: 2018-08-05T01:03:57.484445: step 2299, loss 0.535368.
Train: 2018-08-05T01:04:01.016065: step 2300, loss 0.562801.
Test: 2018-08-05T01:04:16.002022: step 2300, loss 0.546702.
Train: 2018-08-05T01:04:21.799509: step 2301, loss 0.581011.
Train: 2018-08-05T01:04:25.315505: step 2302, loss 0.608224.
Train: 2018-08-05T01:04:28.800248: step 2303, loss 0.481086.
Train: 2018-08-05T01:04:32.316244: step 2304, loss 0.562669.
Train: 2018-08-05T01:04:35.785360: step 2305, loss 0.571681.
Train: 2018-08-05T01:04:39.285730: step 2306, loss 0.544588.
Train: 2018-08-05T01:04:42.817353: step 2307, loss 0.607601.
Train: 2018-08-05T01:04:46.333349: step 2308, loss 0.634376.
Train: 2018-08-05T01:04:49.833718: step 2309, loss 0.508893.
Train: 2018-08-05T01:04:53.334088: step 2310, loss 0.642726.
Test: 2018-08-05T01:05:08.398177: step 2310, loss 0.547847.
Train: 2018-08-05T01:05:11.929801: step 2311, loss 0.642343.
Train: 2018-08-05T01:05:15.477050: step 2312, loss 0.615375.
Train: 2018-08-05T01:05:19.024300: step 2313, loss 0.562388.
Train: 2018-08-05T01:05:22.618429: step 2314, loss 0.553645.
Train: 2018-08-05T01:05:26.165679: step 2315, loss 0.562347.
Train: 2018-08-05T01:05:29.681675: step 2316, loss 0.510554.
Train: 2018-08-05T01:05:33.166417: step 2317, loss 0.545131.
Train: 2018-08-05T01:05:36.651161: step 2318, loss 0.485122.
Train: 2018-08-05T01:05:40.198410: step 2319, loss 0.553763.
Train: 2018-08-05T01:05:43.730032: step 2320, loss 0.493796.
Test: 2018-08-05T01:05:58.700362: step 2320, loss 0.547224.
Train: 2018-08-05T01:06:02.231985: step 2321, loss 0.562335.
Train: 2018-08-05T01:06:05.747982: step 2322, loss 0.510842.
Train: 2018-08-05T01:06:09.248351: step 2323, loss 0.562335.
Train: 2018-08-05T01:06:12.748721: step 2324, loss 0.50206.
Train: 2018-08-05T01:06:16.327223: step 2325, loss 0.605506.
Train: 2018-08-05T01:06:19.858846: step 2326, loss 0.510467.
Train: 2018-08-05T01:06:23.343589: step 2327, loss 0.510354.
Train: 2018-08-05T01:06:26.843959: step 2328, loss 0.614503.
Train: 2018-08-05T01:06:30.313075: step 2329, loss 0.588474.
Train: 2018-08-05T01:06:33.844698: step 2330, loss 0.553648.
Test: 2018-08-05T01:06:48.830654: step 2330, loss 0.548805.
Train: 2018-08-05T01:06:52.377904: step 2331, loss 0.588521.
Train: 2018-08-05T01:06:55.893900: step 2332, loss 0.597244.
Train: 2018-08-05T01:06:59.394269: step 2333, loss 0.579788.
Train: 2018-08-05T01:07:02.894639: step 2334, loss 0.605877.
Train: 2018-08-05T01:07:06.395008: step 2335, loss 0.518918.
Train: 2018-08-05T01:07:09.911005: step 2336, loss 0.666487.
Train: 2018-08-05T01:07:13.411374: step 2337, loss 0.527737.
Train: 2018-08-05T01:07:16.911744: step 2338, loss 0.605502.
Train: 2018-08-05T01:07:20.412113: step 2339, loss 0.562336.
Train: 2018-08-05T01:07:23.928109: step 2340, loss 0.536579.
Test: 2018-08-05T01:07:38.960946: step 2340, loss 0.547827.
Train: 2018-08-05T01:07:42.508195: step 2341, loss 0.562335.
Train: 2018-08-05T01:07:46.008565: step 2342, loss 0.545227.
Train: 2018-08-05T01:07:49.540188: step 2343, loss 0.579426.
Train: 2018-08-05T01:07:53.071811: step 2344, loss 0.459953.
Train: 2018-08-05T01:07:56.572180: step 2345, loss 0.48548.
Train: 2018-08-05T01:08:00.072549: step 2346, loss 0.545215.
Train: 2018-08-05T01:08:03.572919: step 2347, loss 0.562335.
Train: 2018-08-05T01:08:08.292167: step 2348, loss 0.562335.
Train: 2018-08-05T01:08:11.808163: step 2349, loss 0.588192.
Train: 2018-08-05T01:08:15.308533: step 2350, loss 0.622738.
Test: 2018-08-05T01:08:30.294490: step 2350, loss 0.548129.
Train: 2018-08-05T01:08:33.794859: step 2351, loss 0.458819.
Train: 2018-08-05T01:08:37.295229: step 2352, loss 0.596912.
Train: 2018-08-05T01:08:40.795598: step 2353, loss 0.579644.
Train: 2018-08-05T01:08:44.295968: step 2354, loss 0.570997.
Train: 2018-08-05T01:08:47.811964: step 2355, loss 0.519065.
Train: 2018-08-05T01:08:51.359213: step 2356, loss 0.553681.
Train: 2018-08-05T01:08:54.875209: step 2357, loss 0.571019.
Train: 2018-08-05T01:08:58.359952: step 2358, loss 0.562349.
Train: 2018-08-05T01:09:01.860322: step 2359, loss 0.518943.
Train: 2018-08-05T01:09:05.360691: step 2360, loss 0.597121.
Test: 2018-08-05T01:09:20.346650: step 2360, loss 0.548834.
Train: 2018-08-05T01:09:23.878271: step 2361, loss 0.562354.
Train: 2018-08-05T01:09:27.409894: step 2362, loss 0.527569.
Train: 2018-08-05T01:09:30.925890: step 2363, loss 0.536247.
Train: 2018-08-05T01:09:34.410633: step 2364, loss 0.544934.
Train: 2018-08-05T01:09:37.895375: step 2365, loss 0.579816.
Train: 2018-08-05T01:09:41.411372: step 2366, loss 0.606025.
Train: 2018-08-05T01:09:44.896114: step 2367, loss 0.579824.
Train: 2018-08-05T01:09:48.427737: step 2368, loss 0.649578.
Train: 2018-08-05T01:09:51.943734: step 2369, loss 0.69284.
Train: 2018-08-05T01:09:55.428476: step 2370, loss 0.588308.
Test: 2018-08-05T01:10:10.430061: step 2370, loss 0.548154.
Train: 2018-08-05T01:10:13.961683: step 2371, loss 0.562336.
Train: 2018-08-05T01:10:17.508932: step 2372, loss 0.528057.
Train: 2018-08-05T01:10:21.024928: step 2373, loss 0.528182.
Train: 2018-08-05T01:10:24.572178: step 2374, loss 0.621966.
Train: 2018-08-05T01:10:28.056921: step 2375, loss 0.613276.
Train: 2018-08-05T01:10:31.588543: step 2376, loss 0.596169.
Train: 2018-08-05T01:10:35.104539: step 2377, loss 0.553965.
Train: 2018-08-05T01:10:38.604909: step 2378, loss 0.554016.
Train: 2018-08-05T01:10:42.120905: step 2379, loss 0.495575.
Train: 2018-08-05T01:10:45.652529: step 2380, loss 0.512358.
Test: 2018-08-05T01:11:00.591604: step 2380, loss 0.54791.
Train: 2018-08-05T01:11:04.091975: step 2381, loss 0.554078.
Train: 2018-08-05T01:11:07.592344: step 2382, loss 0.579116.
Train: 2018-08-05T01:11:11.108340: step 2383, loss 0.537376.
Train: 2018-08-05T01:11:14.639962: step 2384, loss 0.529.
Train: 2018-08-05T01:11:18.155972: step 2385, loss 0.646071.
Train: 2018-08-05T01:11:21.671955: step 2386, loss 0.587498.
Train: 2018-08-05T01:11:25.203578: step 2387, loss 0.629262.
Train: 2018-08-05T01:11:28.703947: step 2388, loss 0.545751.
Train: 2018-08-05T01:11:32.204317: step 2389, loss 0.604069.
Train: 2018-08-05T01:11:35.704686: step 2390, loss 0.579069.
Test: 2018-08-05T01:11:50.690644: step 2390, loss 0.548046.
Train: 2018-08-05T01:11:54.191013: step 2391, loss 0.595632.
Train: 2018-08-05T01:11:57.707009: step 2392, loss 0.521132.
Train: 2018-08-05T01:12:01.285512: step 2393, loss 0.488147.
Train: 2018-08-05T01:12:04.801508: step 2394, loss 0.554222.
Train: 2018-08-05T01:12:08.317503: step 2395, loss 0.628688.
Train: 2018-08-05T01:12:11.802246: step 2396, loss 0.504575.
Train: 2018-08-05T01:12:15.286990: step 2397, loss 0.529348.
Train: 2018-08-05T01:12:18.865492: step 2398, loss 0.512678.
Train: 2018-08-05T01:12:22.397115: step 2399, loss 0.587405.
Train: 2018-08-05T01:12:25.897485: step 2400, loss 0.504046.
Test: 2018-08-05T01:12:41.008457: step 2400, loss 0.548045.
Train: 2018-08-05T01:12:46.821569: step 2401, loss 0.595879.
Train: 2018-08-05T01:12:50.321938: step 2402, loss 0.570782.
Train: 2018-08-05T01:12:53.806681: step 2403, loss 0.621222.
Train: 2018-08-05T01:12:57.322677: step 2404, loss 0.570791.
Train: 2018-08-05T01:13:00.854300: step 2405, loss 0.621282.
Train: 2018-08-05T01:13:04.370295: step 2406, loss 0.553974.
Train: 2018-08-05T01:13:07.933172: step 2407, loss 0.596001.
Train: 2018-08-05T01:13:11.433541: step 2408, loss 0.562389.
Train: 2018-08-05T01:13:14.933911: step 2409, loss 0.587557.
Train: 2018-08-05T01:13:18.465534: step 2410, loss 0.554023.
Test: 2018-08-05T01:13:33.545250: step 2410, loss 0.548811.
Train: 2018-08-05T01:13:37.076873: step 2411, loss 0.587515.
Train: 2018-08-05T01:13:40.608496: step 2412, loss 0.545692.
Train: 2018-08-05T01:13:44.140119: step 2413, loss 0.579124.
Train: 2018-08-05T01:13:47.702995: step 2414, loss 0.587463.
Train: 2018-08-05T01:13:51.187737: step 2415, loss 0.554091.
Train: 2018-08-05T01:13:52.906669: step 2416, loss 0.473571.
Train: 2018-08-05T01:13:56.438292: step 2417, loss 0.595794.
Train: 2018-08-05T01:13:59.923035: step 2418, loss 0.645896.
Train: 2018-08-05T01:14:03.407777: step 2419, loss 0.562428.
Train: 2018-08-05T01:14:06.908147: step 2420, loss 0.520785.
Test: 2018-08-05T01:14:21.987864: step 2420, loss 0.547751.
Train: 2018-08-05T01:14:25.503860: step 2421, loss 0.645747.
Train: 2018-08-05T01:14:29.035483: step 2422, loss 0.570761.
Train: 2018-08-05T01:14:32.551479: step 2423, loss 0.537534.
Train: 2018-08-05T01:14:36.083102: step 2424, loss 0.520952.
Train: 2018-08-05T01:14:39.567845: step 2425, loss 0.620591.
Train: 2018-08-05T01:14:43.052587: step 2426, loss 0.570758.
Train: 2018-08-05T01:14:46.584210: step 2427, loss 0.520997.
Train: 2018-08-05T01:14:50.084580: step 2428, loss 0.645434.
Train: 2018-08-05T01:14:53.584949: step 2429, loss 0.595616.
Train: 2018-08-05T01:14:57.116572: step 2430, loss 0.545942.
Test: 2018-08-05T01:15:12.149410: step 2430, loss 0.547173.
Train: 2018-08-05T01:15:15.681031: step 2431, loss 0.587282.
Train: 2018-08-05T01:15:19.212654: step 2432, loss 0.636767.
Train: 2018-08-05T01:15:22.759904: step 2433, loss 0.521386.
Train: 2018-08-05T01:15:26.260273: step 2434, loss 0.537889.
Train: 2018-08-05T01:15:29.807523: step 2435, loss 0.554333.
Train: 2018-08-05T01:15:33.370399: step 2436, loss 0.554334.
Train: 2018-08-05T01:15:36.870768: step 2437, loss 0.603622.
Train: 2018-08-05T01:15:40.402392: step 2438, loss 0.578972.
Train: 2018-08-05T01:15:43.918387: step 2439, loss 0.554346.
Train: 2018-08-05T01:15:47.465637: step 2440, loss 0.570761.
Test: 2018-08-05T01:16:02.435967: step 2440, loss 0.548665.
Train: 2018-08-05T01:16:05.967590: step 2441, loss 0.455893.
Train: 2018-08-05T01:16:09.514839: step 2442, loss 0.620111.
Train: 2018-08-05T01:16:13.030835: step 2443, loss 0.546053.
Train: 2018-08-05T01:16:16.546831: step 2444, loss 0.463537.
Train: 2018-08-05T01:16:20.047201: step 2445, loss 0.562478.
Train: 2018-08-05T01:16:23.547571: step 2446, loss 0.587377.
Train: 2018-08-05T01:16:27.110447: step 2447, loss 0.520775.
Train: 2018-08-05T01:16:30.673323: step 2448, loss 0.545689.
Train: 2018-08-05T01:16:34.173692: step 2449, loss 0.637914.
Train: 2018-08-05T01:16:37.720941: step 2450, loss 0.596006.
Test: 2018-08-05T01:16:52.706898: step 2450, loss 0.547942.
Train: 2018-08-05T01:16:56.222895: step 2451, loss 0.486664.
Train: 2018-08-05T01:16:59.707638: step 2452, loss 0.570803.
Train: 2018-08-05T01:17:03.254887: step 2453, loss 0.520104.
Train: 2018-08-05T01:17:06.802136: step 2454, loss 0.604731.
Train: 2018-08-05T01:17:10.302506: step 2455, loss 0.553858.
Train: 2018-08-05T01:17:13.802876: step 2456, loss 0.519821.
Train: 2018-08-05T01:17:17.303245: step 2457, loss 0.511186.
Train: 2018-08-05T01:17:20.803614: step 2458, loss 0.562336.
Train: 2018-08-05T01:17:24.366490: step 2459, loss 0.536598.
Train: 2018-08-05T01:17:27.898113: step 2460, loss 0.519305.
Test: 2018-08-05T01:17:42.930952: step 2460, loss 0.548709.
Train: 2018-08-05T01:17:46.493826: step 2461, loss 0.458683.
Train: 2018-08-05T01:17:49.962943: step 2462, loss 0.588404.
Train: 2018-08-05T01:17:53.478939: step 2463, loss 0.588527.
Train: 2018-08-05T01:17:56.979308: step 2464, loss 0.54488.
Train: 2018-08-05T01:18:00.495304: step 2465, loss 0.571167.
Train: 2018-08-05T01:18:04.026927: step 2466, loss 0.632778.
Train: 2018-08-05T01:18:07.527296: step 2467, loss 0.580013.
Train: 2018-08-05T01:18:11.027666: step 2468, loss 0.562409.
Train: 2018-08-05T01:18:14.512408: step 2469, loss 0.606423.
Train: 2018-08-05T01:18:18.028405: step 2470, loss 0.588784.
Test: 2018-08-05T01:18:33.029988: step 2470, loss 0.548136.
Train: 2018-08-05T01:18:36.577237: step 2471, loss 0.553615.
Train: 2018-08-05T01:18:40.186994: step 2472, loss 0.55362.
Train: 2018-08-05T01:18:43.702990: step 2473, loss 0.553625.
Train: 2018-08-05T01:18:47.187733: step 2474, loss 0.588618.
Train: 2018-08-05T01:18:50.672476: step 2475, loss 0.571103.
Train: 2018-08-05T01:18:54.157218: step 2476, loss 0.518771.
Train: 2018-08-05T01:18:57.657588: step 2477, loss 0.588496.
Train: 2018-08-05T01:19:01.189211: step 2478, loss 0.597158.
Train: 2018-08-05T01:19:04.705207: step 2479, loss 0.571034.
Train: 2018-08-05T01:19:08.205577: step 2480, loss 0.588343.
Test: 2018-08-05T01:19:23.207161: step 2480, loss 0.548501.
Train: 2018-08-05T01:19:26.754409: step 2481, loss 0.605564.
Train: 2018-08-05T01:19:30.254779: step 2482, loss 0.588189.
Train: 2018-08-05T01:19:33.739522: step 2483, loss 0.545159.
Train: 2018-08-05T01:19:37.271145: step 2484, loss 0.562336.
Train: 2018-08-05T01:19:40.802767: step 2485, loss 0.511082.
Train: 2018-08-05T01:19:44.287511: step 2486, loss 0.613534.
Train: 2018-08-05T01:19:47.772253: step 2487, loss 0.562342.
Train: 2018-08-05T01:19:51.288249: step 2488, loss 0.62183.
Train: 2018-08-05T01:19:54.804245: step 2489, loss 0.511519.
Train: 2018-08-05T01:19:58.304615: step 2490, loss 0.54544.
Test: 2018-08-05T01:20:13.306199: step 2490, loss 0.547665.
Train: 2018-08-05T01:20:16.853448: step 2491, loss 0.528559.
Train: 2018-08-05T01:20:20.416324: step 2492, loss 0.537013.
Train: 2018-08-05T01:20:23.916693: step 2493, loss 0.579268.
Train: 2018-08-05T01:20:27.432689: step 2494, loss 0.553905.
Train: 2018-08-05T01:20:30.917433: step 2495, loss 0.553902.
Train: 2018-08-05T01:20:34.464682: step 2496, loss 0.562357.
Train: 2018-08-05T01:20:37.980678: step 2497, loss 0.604688.
Train: 2018-08-05T01:20:41.512301: step 2498, loss 0.528505.
Train: 2018-08-05T01:20:44.981417: step 2499, loss 0.520023.
Train: 2018-08-05T01:20:48.481786: step 2500, loss 0.579307.
Test: 2018-08-05T01:21:03.577132: step 2500, loss 0.546421.
Train: 2018-08-05T01:21:09.233977: step 2501, loss 0.49447.
Train: 2018-08-05T01:21:12.765600: step 2502, loss 0.604865.
Train: 2018-08-05T01:21:16.297222: step 2503, loss 0.630452.
Train: 2018-08-05T01:21:19.797592: step 2504, loss 0.545323.
Train: 2018-08-05T01:21:23.313588: step 2505, loss 0.553833.
Train: 2018-08-05T01:21:26.829585: step 2506, loss 0.528295.
Train: 2018-08-05T01:21:30.314327: step 2507, loss 0.5879.
Train: 2018-08-05T01:21:33.845950: step 2508, loss 0.587907.
Train: 2018-08-05T01:21:37.377573: step 2509, loss 0.570861.
Train: 2018-08-05T01:21:40.862316: step 2510, loss 0.460145.
Test: 2018-08-05T01:21:55.848273: step 2510, loss 0.549268.
Train: 2018-08-05T01:21:59.333016: step 2511, loss 0.570871.
Train: 2018-08-05T01:22:02.849011: step 2512, loss 0.58797.
Train: 2018-08-05T01:22:06.365008: step 2513, loss 0.622191.
Train: 2018-08-05T01:22:09.912257: step 2514, loss 0.579428.
Train: 2018-08-05T01:22:13.443880: step 2515, loss 0.605028.
Train: 2018-08-05T01:22:16.959876: step 2516, loss 0.553817.
Train: 2018-08-05T01:22:20.460245: step 2517, loss 0.50276.
Train: 2018-08-05T01:22:23.960615: step 2518, loss 0.494244.
Train: 2018-08-05T01:22:27.460985: step 2519, loss 0.56234.
Train: 2018-08-05T01:22:30.992607: step 2520, loss 0.519653.
Test: 2018-08-05T01:22:45.962938: step 2520, loss 0.547851.
Train: 2018-08-05T01:22:49.510187: step 2521, loss 0.588002.
Train: 2018-08-05T01:22:53.026183: step 2522, loss 0.639438.
Train: 2018-08-05T01:22:56.526553: step 2523, loss 0.545209.
Train: 2018-08-05T01:23:00.026922: step 2524, loss 0.579462.
Train: 2018-08-05T01:23:03.542918: step 2525, loss 0.536656.
Train: 2018-08-05T01:23:07.074541: step 2526, loss 0.596582.
Train: 2018-08-05T01:23:10.574911: step 2527, loss 0.588006.
Train: 2018-08-05T01:23:14.044026: step 2528, loss 0.570884.
Train: 2018-08-05T01:23:17.544396: step 2529, loss 0.570876.
Train: 2018-08-05T01:23:21.044766: step 2530, loss 0.630556.
Test: 2018-08-05T01:23:36.046349: step 2530, loss 0.546573.
Train: 2018-08-05T01:23:39.562345: step 2531, loss 0.519817.
Train: 2018-08-05T01:23:43.125222: step 2532, loss 0.553853.
Train: 2018-08-05T01:23:46.672471: step 2533, loss 0.638715.
Train: 2018-08-05T01:23:50.157214: step 2534, loss 0.54543.
Train: 2018-08-05T01:23:53.673210: step 2535, loss 0.53702.
Train: 2018-08-05T01:23:57.189206: step 2536, loss 0.486422.
Train: 2018-08-05T01:24:00.658323: step 2537, loss 0.579253.
Train: 2018-08-05T01:24:04.205572: step 2538, loss 0.646835.
Train: 2018-08-05T01:24:07.752821: step 2539, loss 0.604542.
Train: 2018-08-05T01:24:11.284444: step 2540, loss 0.562378.
Test: 2018-08-05T01:24:26.286027: step 2540, loss 0.546811.
Train: 2018-08-05T01:24:29.848904: step 2541, loss 0.595987.
Train: 2018-08-05T01:24:33.364900: step 2542, loss 0.520497.
Train: 2018-08-05T01:24:36.880896: step 2543, loss 0.512179.
Train: 2018-08-05T01:24:40.396892: step 2544, loss 0.528914.
Train: 2018-08-05T01:24:43.959768: step 2545, loss 0.528875.
Train: 2018-08-05T01:24:47.507018: step 2546, loss 0.537204.
Train: 2018-08-05T01:24:51.023014: step 2547, loss 0.596033.
Train: 2018-08-05T01:24:54.523384: step 2548, loss 0.5371.
Train: 2018-08-05T01:24:58.023753: step 2549, loss 0.570806.
Train: 2018-08-05T01:25:01.539749: step 2550, loss 0.587717.
Test: 2018-08-05T01:25:16.510081: step 2550, loss 0.548427.
Train: 2018-08-05T01:25:20.088582: step 2551, loss 0.562359.
Train: 2018-08-05T01:25:23.651458: step 2552, loss 0.579287.
Train: 2018-08-05T01:25:27.198707: step 2553, loss 0.553886.
Train: 2018-08-05T01:25:30.699077: step 2554, loss 0.494566.
Train: 2018-08-05T01:25:34.199446: step 2555, loss 0.511405.
Train: 2018-08-05T01:25:37.684189: step 2556, loss 0.562342.
Train: 2018-08-05T01:25:41.215812: step 2557, loss 0.570876.
Train: 2018-08-05T01:25:44.778688: step 2558, loss 0.493885.
Train: 2018-08-05T01:25:48.263431: step 2559, loss 0.57092.
Train: 2018-08-05T01:25:51.763800: step 2560, loss 0.519287.
Test: 2018-08-05T01:26:06.734133: step 2560, loss 0.547713.
Train: 2018-08-05T01:26:10.250127: step 2561, loss 0.536422.
Train: 2018-08-05T01:26:13.750496: step 2562, loss 0.501659.
Train: 2018-08-05T01:26:17.297746: step 2563, loss 0.536236.
Train: 2018-08-05T01:26:20.829369: step 2564, loss 0.53614.
Train: 2018-08-05T01:26:24.345364: step 2565, loss 0.562396.
Train: 2018-08-05T01:26:27.876987: step 2566, loss 0.562418.
Train: 2018-08-05T01:26:29.564666: step 2567, loss 0.581306.
Train: 2018-08-05T01:26:33.065035: step 2568, loss 0.633371.
Train: 2018-08-05T01:26:36.596658: step 2569, loss 0.624543.
Train: 2018-08-05T01:26:40.112654: step 2570, loss 0.615617.
Test: 2018-08-05T01:26:55.114237: step 2570, loss 0.547467.
Train: 2018-08-05T01:26:58.645860: step 2571, loss 0.527068.
Train: 2018-08-05T01:27:02.193110: step 2572, loss 0.57126.
Train: 2018-08-05T01:27:05.677853: step 2573, loss 0.597688.
Train: 2018-08-05T01:27:09.209476: step 2574, loss 0.571203.
Train: 2018-08-05T01:27:12.772351: step 2575, loss 0.527284.
Train: 2018-08-05T01:27:16.335228: step 2576, loss 0.588676.
Train: 2018-08-05T01:27:19.835597: step 2577, loss 0.649834.
Train: 2018-08-05T01:27:23.351594: step 2578, loss 0.501367.
Train: 2018-08-05T01:27:26.867589: step 2579, loss 0.518883.
Train: 2018-08-05T01:27:30.367959: step 2580, loss 0.527615.
Test: 2018-08-05T01:27:45.510183: step 2580, loss 0.548652.
Train: 2018-08-05T01:27:49.088685: step 2581, loss 0.553669.
Train: 2018-08-05T01:27:52.589055: step 2582, loss 0.527638.
Train: 2018-08-05T01:27:56.089424: step 2583, loss 0.649158.
Train: 2018-08-05T01:27:59.589793: step 2584, loss 0.588349.
Train: 2018-08-05T01:28:03.090163: step 2585, loss 0.52774.
Train: 2018-08-05T01:28:06.637412: step 2586, loss 0.527777.
Train: 2018-08-05T01:28:10.200289: step 2587, loss 0.596889.
Train: 2018-08-05T01:28:13.700658: step 2588, loss 0.527826.
Train: 2018-08-05T01:28:17.201028: step 2589, loss 0.458835.
Train: 2018-08-05T01:28:20.701397: step 2590, loss 0.493213.
Test: 2018-08-05T01:28:35.812367: step 2590, loss 0.547473.
Train: 2018-08-05T01:28:39.328363: step 2591, loss 0.536345.
Train: 2018-08-05T01:28:42.828733: step 2592, loss 0.55366.
Train: 2018-08-05T01:28:46.391609: step 2593, loss 0.588522.
Train: 2018-08-05T01:28:49.923231: step 2594, loss 0.588582.
Train: 2018-08-05T01:28:53.407975: step 2595, loss 0.544882.
Train: 2018-08-05T01:28:56.939598: step 2596, loss 0.597414.
Train: 2018-08-05T01:29:00.424340: step 2597, loss 0.544862.
Train: 2018-08-05T01:29:03.955963: step 2598, loss 0.597447.
Train: 2018-08-05T01:29:07.487586: step 2599, loss 0.54486.
Train: 2018-08-05T01:29:11.066088: step 2600, loss 0.536099.
Test: 2018-08-05T01:29:26.098926: step 2600, loss 0.548354.
Train: 2018-08-05T01:29:31.849532: step 2601, loss 0.544857.
Train: 2018-08-05T01:29:35.334275: step 2602, loss 0.54485.
Train: 2018-08-05T01:29:38.834645: step 2603, loss 0.606266.
Train: 2018-08-05T01:29:42.335014: step 2604, loss 0.615018.
Train: 2018-08-05T01:29:45.866637: step 2605, loss 0.649963.
Train: 2018-08-05T01:29:49.367006: step 2606, loss 0.501262.
Train: 2018-08-05T01:29:52.867376: step 2607, loss 0.614643.
Train: 2018-08-05T01:29:56.367745: step 2608, loss 0.571042.
Train: 2018-08-05T01:29:59.899368: step 2609, loss 0.51902.
Train: 2018-08-05T01:30:03.384111: step 2610, loss 0.51909.
Test: 2018-08-05T01:30:18.354442: step 2610, loss 0.549098.
Train: 2018-08-05T01:30:21.901691: step 2611, loss 0.57963.
Train: 2018-08-05T01:30:25.433314: step 2612, loss 0.545068.
Train: 2018-08-05T01:30:28.949310: step 2613, loss 0.484669.
Train: 2018-08-05T01:30:32.449679: step 2614, loss 0.570978.
Train: 2018-08-05T01:30:35.934422: step 2615, loss 0.527764.
Train: 2018-08-05T01:30:39.434791: step 2616, loss 0.510413.
Train: 2018-08-05T01:30:42.935161: step 2617, loss 0.527654.
Train: 2018-08-05T01:30:46.466784: step 2618, loss 0.57105.
Train: 2018-08-05T01:30:49.982780: step 2619, loss 0.649492.
Train: 2018-08-05T01:30:53.498776: step 2620, loss 0.562361.
Test: 2018-08-05T01:31:08.515987: step 2620, loss 0.549015.
Train: 2018-08-05T01:31:12.000729: step 2621, loss 0.492669.
Train: 2018-08-05T01:31:15.532352: step 2622, loss 0.544921.
Train: 2018-08-05T01:31:19.048348: step 2623, loss 0.509969.
Train: 2018-08-05T01:31:22.579971: step 2624, loss 0.588635.
Train: 2018-08-05T01:31:26.111593: step 2625, loss 0.553621.
Train: 2018-08-05T01:31:29.611963: step 2626, loss 0.597487.
Train: 2018-08-05T01:31:33.112333: step 2627, loss 0.606273.
Train: 2018-08-05T01:31:36.628329: step 2628, loss 0.623774.
Train: 2018-08-05T01:31:40.113072: step 2629, loss 0.597383.
Train: 2018-08-05T01:31:43.644694: step 2630, loss 0.536184.
Test: 2018-08-05T01:31:58.646278: step 2630, loss 0.547612.
Train: 2018-08-05T01:32:02.146647: step 2631, loss 0.57107.
Train: 2018-08-05T01:32:05.647017: step 2632, loss 0.484123.
Train: 2018-08-05T01:32:09.131760: step 2633, loss 0.553663.
Train: 2018-08-05T01:32:12.632129: step 2634, loss 0.53629.
Train: 2018-08-05T01:32:16.132499: step 2635, loss 0.59711.
Train: 2018-08-05T01:32:19.679748: step 2636, loss 0.605773.
Train: 2018-08-05T01:32:23.195744: step 2637, loss 0.588362.
Train: 2018-08-05T01:32:26.711741: step 2638, loss 0.588307.
Train: 2018-08-05T01:32:30.212110: step 2639, loss 0.562339.
Train: 2018-08-05T01:32:33.696853: step 2640, loss 0.519259.
Test: 2018-08-05T01:32:48.714064: step 2640, loss 0.547765.
Train: 2018-08-05T01:32:52.261312: step 2641, loss 0.605365.
Train: 2018-08-05T01:32:55.839815: step 2642, loss 0.502211.
Train: 2018-08-05T01:32:59.355811: step 2643, loss 0.527996.
Train: 2018-08-05T01:33:02.903061: step 2644, loss 0.622442.
Train: 2018-08-05T01:33:06.403430: step 2645, loss 0.605222.
Train: 2018-08-05T01:33:09.903800: step 2646, loss 0.622264.
Train: 2018-08-05T01:33:13.404169: step 2647, loss 0.511125.
Train: 2018-08-05T01:33:16.967046: step 2648, loss 0.553819.
Train: 2018-08-05T01:33:20.529921: step 2649, loss 0.613409.
Train: 2018-08-05T01:33:23.999037: step 2650, loss 0.553855.
Test: 2018-08-05T01:33:39.156888: step 2650, loss 0.54761.
Train: 2018-08-05T01:33:42.704137: step 2651, loss 0.562352.
Train: 2018-08-05T01:33:46.204507: step 2652, loss 0.613139.
Train: 2018-08-05T01:33:49.720503: step 2653, loss 0.528595.
Train: 2018-08-05T01:33:53.252126: step 2654, loss 0.604523.
Train: 2018-08-05T01:33:56.783748: step 2655, loss 0.56238.
Train: 2018-08-05T01:34:00.284118: step 2656, loss 0.528803.
Train: 2018-08-05T01:34:03.784488: step 2657, loss 0.495281.
Train: 2018-08-05T01:34:07.284857: step 2658, loss 0.562389.
Train: 2018-08-05T01:34:10.816480: step 2659, loss 0.545582.
Train: 2018-08-05T01:34:14.348103: step 2660, loss 0.511911.
Test: 2018-08-05T01:34:29.365314: step 2660, loss 0.547516.
Train: 2018-08-05T01:34:32.912562: step 2661, loss 0.612952.
Train: 2018-08-05T01:34:36.444185: step 2662, loss 0.621434.
Train: 2018-08-05T01:34:39.928927: step 2663, loss 0.612978.
Train: 2018-08-05T01:34:43.460550: step 2664, loss 0.587644.
Train: 2018-08-05T01:34:46.945293: step 2665, loss 0.553973.
Train: 2018-08-05T01:34:50.445662: step 2666, loss 0.637965.
Train: 2018-08-05T01:34:53.977286: step 2667, loss 0.528905.
Train: 2018-08-05T01:34:57.508908: step 2668, loss 0.579132.
Train: 2018-08-05T01:35:01.024905: step 2669, loss 0.587459.
Train: 2018-08-05T01:35:04.525274: step 2670, loss 0.545777.
Test: 2018-08-05T01:35:19.542485: step 2670, loss 0.547978.
Train: 2018-08-05T01:35:23.074107: step 2671, loss 0.562444.
Train: 2018-08-05T01:35:26.715116: step 2672, loss 0.595685.
Train: 2018-08-05T01:35:30.262366: step 2673, loss 0.520986.
Train: 2018-08-05T01:35:33.793988: step 2674, loss 0.537588.
Train: 2018-08-05T01:35:37.278731: step 2675, loss 0.512689.
Train: 2018-08-05T01:35:40.810354: step 2676, loss 0.595687.
Train: 2018-08-05T01:35:44.326350: step 2677, loss 0.604024.
Train: 2018-08-05T01:35:47.826720: step 2678, loss 0.55413.
Train: 2018-08-05T01:35:51.389596: step 2679, loss 0.579079.
Train: 2018-08-05T01:35:54.952472: step 2680, loss 0.587396.
Test: 2018-08-05T01:36:09.969681: step 2680, loss 0.548562.
Train: 2018-08-05T01:36:13.485678: step 2681, loss 0.604016.
Train: 2018-08-05T01:36:17.001675: step 2682, loss 0.520936.
Train: 2018-08-05T01:36:20.517671: step 2683, loss 0.512632.
Train: 2018-08-05T01:36:24.018040: step 2684, loss 0.554132.
Train: 2018-08-05T01:36:27.565289: step 2685, loss 0.487501.
Train: 2018-08-05T01:36:31.081286: step 2686, loss 0.612529.
Train: 2018-08-05T01:36:34.581655: step 2687, loss 0.629341.
Train: 2018-08-05T01:36:38.082025: step 2688, loss 0.570774.
Train: 2018-08-05T01:36:41.582394: step 2689, loss 0.545667.
Train: 2018-08-05T01:36:45.082764: step 2690, loss 0.445167.
Test: 2018-08-05T01:37:00.099975: step 2690, loss 0.547003.
Train: 2018-08-05T01:37:03.647223: step 2691, loss 0.604389.
Train: 2018-08-05T01:37:07.147593: step 2692, loss 0.570795.
Train: 2018-08-05T01:37:10.663589: step 2693, loss 0.604528.
Train: 2018-08-05T01:37:14.179585: step 2694, loss 0.570805.
Train: 2018-08-05T01:37:17.679955: step 2695, loss 0.587689.
Train: 2018-08-05T01:37:21.227204: step 2696, loss 0.520164.
Train: 2018-08-05T01:37:24.774453: step 2697, loss 0.503226.
Train: 2018-08-05T01:37:28.415463: step 2698, loss 0.511558.
Train: 2018-08-05T01:37:31.915832: step 2699, loss 0.528379.
Train: 2018-08-05T01:37:35.400575: step 2700, loss 0.562341.
Test: 2018-08-05T01:37:50.433413: step 2700, loss 0.547666.
Train: 2018-08-05T01:37:56.137139: step 2701, loss 0.630727.
Train: 2018-08-05T01:37:59.621882: step 2702, loss 0.510972.
Train: 2018-08-05T01:38:03.169131: step 2703, loss 0.588076.
Train: 2018-08-05T01:38:06.685127: step 2704, loss 0.519369.
Train: 2018-08-05T01:38:10.169870: step 2705, loss 0.588173.
Train: 2018-08-05T01:38:13.654613: step 2706, loss 0.614085.
Train: 2018-08-05T01:38:17.170609: step 2707, loss 0.527835.
Train: 2018-08-05T01:38:20.639725: step 2708, loss 0.493277.
Train: 2018-08-05T01:38:24.171348: step 2709, loss 0.53639.
Train: 2018-08-05T01:38:27.718598: step 2710, loss 0.466963.
Test: 2018-08-05T01:38:42.657675: step 2710, loss 0.548019.
Train: 2018-08-05T01:38:46.142417: step 2711, loss 0.501417.
Train: 2018-08-05T01:38:49.627160: step 2712, loss 0.606112.
Train: 2018-08-05T01:38:53.143156: step 2713, loss 0.606269.
Train: 2018-08-05T01:38:56.643526: step 2714, loss 0.536025.
Train: 2018-08-05T01:39:00.159522: step 2715, loss 0.50955.
Train: 2018-08-05T01:39:03.706771: step 2716, loss 0.606609.
Train: 2018-08-05T01:39:07.207141: step 2717, loss 0.606684.
Train: 2018-08-05T01:39:08.910446: step 2718, loss 0.449155.
Train: 2018-08-05T01:39:12.410815: step 2719, loss 0.553591.
Train: 2018-08-05T01:39:15.895558: step 2720, loss 0.580258.
Test: 2018-08-05T01:39:30.912770: step 2720, loss 0.547624.
Train: 2018-08-05T01:39:34.397511: step 2721, loss 0.580293.
Train: 2018-08-05T01:39:37.944761: step 2722, loss 0.517959.
Train: 2018-08-05T01:39:41.476383: step 2723, loss 0.553588.
Train: 2018-08-05T01:39:44.976753: step 2724, loss 0.562517.
Train: 2018-08-05T01:39:48.524002: step 2725, loss 0.625072.
Train: 2018-08-05T01:39:52.008746: step 2726, loss 0.598234.
Train: 2018-08-05T01:39:55.540368: step 2727, loss 0.580334.
Train: 2018-08-05T01:39:59.040738: step 2728, loss 0.571385.
Train: 2018-08-05T01:40:02.572360: step 2729, loss 0.562469.
Train: 2018-08-05T01:40:06.057103: step 2730, loss 0.580174.
Test: 2018-08-05T01:40:21.027434: step 2730, loss 0.54808.
Train: 2018-08-05T01:40:24.527803: step 2731, loss 0.597793.
Train: 2018-08-05T01:40:28.043799: step 2732, loss 0.527165.
Train: 2018-08-05T01:40:31.559795: step 2733, loss 0.615164.
Train: 2018-08-05T01:40:35.107044: step 2734, loss 0.623746.
Train: 2018-08-05T01:40:38.623041: step 2735, loss 0.579825.
Train: 2018-08-05T01:40:42.139037: step 2736, loss 0.675344.
Train: 2018-08-05T01:40:45.639406: step 2737, loss 0.52779.
Train: 2018-08-05T01:40:49.171029: step 2738, loss 0.613902.
Train: 2018-08-05T01:40:52.702653: step 2739, loss 0.553791.
Train: 2018-08-05T01:40:56.218648: step 2740, loss 0.545336.
Test: 2018-08-05T01:41:11.313993: step 2740, loss 0.548014.
Train: 2018-08-05T01:41:14.829988: step 2741, loss 0.520005.
Train: 2018-08-05T01:41:18.330357: step 2742, loss 0.461002.
Train: 2018-08-05T01:41:21.830727: step 2743, loss 0.553919.
Train: 2018-08-05T01:41:25.331096: step 2744, loss 0.520137.
Train: 2018-08-05T01:41:28.847092: step 2745, loss 0.545453.
Train: 2018-08-05T01:41:32.363088: step 2746, loss 0.613146.
Train: 2018-08-05T01:41:35.910338: step 2747, loss 0.570822.
Train: 2018-08-05T01:41:39.457587: step 2748, loss 0.570822.
Train: 2018-08-05T01:41:42.989210: step 2749, loss 0.596214.
Train: 2018-08-05T01:41:46.489580: step 2750, loss 0.604646.
Test: 2018-08-05T01:42:01.506791: step 2750, loss 0.547291.
Train: 2018-08-05T01:42:04.991533: step 2751, loss 0.545478.
Train: 2018-08-05T01:42:08.538782: step 2752, loss 0.655152.
Train: 2018-08-05T01:42:12.070405: step 2753, loss 0.562381.
Train: 2018-08-05T01:42:15.570774: step 2754, loss 0.554003.
Train: 2018-08-05T01:42:19.055517: step 2755, loss 0.554031.
Train: 2018-08-05T01:42:22.602767: step 2756, loss 0.512255.
Train: 2018-08-05T01:42:26.103136: step 2757, loss 0.570771.
Train: 2018-08-05T01:42:29.650385: step 2758, loss 0.587482.
Train: 2018-08-05T01:42:33.197635: step 2759, loss 0.503966.
Train: 2018-08-05T01:42:36.682378: step 2760, loss 0.478844.
Test: 2018-08-05T01:42:51.730842: step 2760, loss 0.548405.
Train: 2018-08-05T01:42:55.262464: step 2761, loss 0.579157.
Train: 2018-08-05T01:42:58.762833: step 2762, loss 0.512007.
Train: 2018-08-05T01:43:02.247577: step 2763, loss 0.646601.
Train: 2018-08-05T01:43:05.794826: step 2764, loss 0.520215.
Train: 2018-08-05T01:43:09.357702: step 2765, loss 0.562364.
Train: 2018-08-05T01:43:12.873698: step 2766, loss 0.51159.
Train: 2018-08-05T01:43:16.389694: step 2767, loss 0.553866.
Train: 2018-08-05T01:43:19.890064: step 2768, loss 0.613382.
Train: 2018-08-05T01:43:23.437314: step 2769, loss 0.528273.
Train: 2018-08-05T01:43:26.937683: step 2770, loss 0.562339.
Test: 2018-08-05T01:43:42.048653: step 2770, loss 0.547668.
Train: 2018-08-05T01:43:45.580276: step 2771, loss 0.51105.
Train: 2018-08-05T01:43:49.190032: step 2772, loss 0.613756.
Train: 2018-08-05T01:43:52.690401: step 2773, loss 0.519431.
Train: 2018-08-05T01:43:56.253277: step 2774, loss 0.579531.
Train: 2018-08-05T01:43:59.784900: step 2775, loss 0.588165.
Train: 2018-08-05T01:44:03.269643: step 2776, loss 0.519259.
Train: 2018-08-05T01:44:06.770012: step 2777, loss 0.519197.
Train: 2018-08-05T01:44:10.332889: step 2778, loss 0.588281.
Train: 2018-08-05T01:44:13.864511: step 2779, loss 0.501736.
Train: 2018-08-05T01:44:17.364881: step 2780, loss 0.484239.
Test: 2018-08-05T01:44:32.460224: step 2780, loss 0.547412.
Train: 2018-08-05T01:44:35.976220: step 2781, loss 0.553649.
Train: 2018-08-05T01:44:39.476590: step 2782, loss 0.571113.
Train: 2018-08-05T01:44:43.008212: step 2783, loss 0.553621.
Train: 2018-08-05T01:44:46.508582: step 2784, loss 0.571183.
Train: 2018-08-05T01:44:50.040205: step 2785, loss 0.544803.
Train: 2018-08-05T01:44:53.524948: step 2786, loss 0.57124.
Train: 2018-08-05T01:44:57.025317: step 2787, loss 0.56243.
Train: 2018-08-05T01:45:00.556940: step 2788, loss 0.615492.
Train: 2018-08-05T01:45:04.088563: step 2789, loss 0.491712.
Train: 2018-08-05T01:45:07.620186: step 2790, loss 0.518194.
Test: 2018-08-05T01:45:22.684277: step 2790, loss 0.547448.
Train: 2018-08-05T01:45:26.215898: step 2791, loss 0.544727.
Train: 2018-08-05T01:45:29.810028: step 2792, loss 0.54471.
Train: 2018-08-05T01:45:33.326024: step 2793, loss 0.526907.
Train: 2018-08-05T01:45:36.810768: step 2794, loss 0.535765.
Train: 2018-08-05T01:45:40.373643: step 2795, loss 0.473214.
Train: 2018-08-05T01:45:43.905266: step 2796, loss 0.490865.
Train: 2018-08-05T01:45:47.421262: step 2797, loss 0.544601.
Train: 2018-08-05T01:45:50.906005: step 2798, loss 0.580711.
Train: 2018-08-05T01:45:54.390747: step 2799, loss 0.56268.
Train: 2018-08-05T01:45:57.906743: step 2800, loss 0.54455.
Test: 2018-08-05T01:46:12.861447: step 2800, loss 0.545449.
Train: 2018-08-05T01:46:18.690188: step 2801, loss 0.608241.
Train: 2018-08-05T01:46:22.206184: step 2802, loss 0.499002.
Train: 2018-08-05T01:46:25.706553: step 2803, loss 0.608384.
Train: 2018-08-05T01:46:29.206923: step 2804, loss 0.590153.
Train: 2018-08-05T01:46:32.707292: step 2805, loss 0.535414.
Train: 2018-08-05T01:46:36.207662: step 2806, loss 0.644817.
Train: 2018-08-05T01:46:39.723658: step 2807, loss 0.599128.
Train: 2018-08-05T01:46:43.224028: step 2808, loss 0.598986.
Train: 2018-08-05T01:46:46.724397: step 2809, loss 0.553613.
Train: 2018-08-05T01:46:50.224766: step 2810, loss 0.49954.
Test: 2018-08-05T01:47:05.288857: step 2810, loss 0.547572.
Train: 2018-08-05T01:47:08.804853: step 2811, loss 0.535613.
Train: 2018-08-05T01:47:12.352102: step 2812, loss 0.616445.
Train: 2018-08-05T01:47:15.868098: step 2813, loss 0.544636.
Train: 2018-08-05T01:47:19.415347: step 2814, loss 0.544654.
Train: 2018-08-05T01:47:22.931344: step 2815, loss 0.616013.
Train: 2018-08-05T01:47:26.431713: step 2816, loss 0.553589.
Train: 2018-08-05T01:47:29.947709: step 2817, loss 0.606797.
Train: 2018-08-05T01:47:33.463705: step 2818, loss 0.465228.
Train: 2018-08-05T01:47:36.979702: step 2819, loss 0.597724.
Train: 2018-08-05T01:47:40.511325: step 2820, loss 0.580024.
Test: 2018-08-05T01:47:55.544162: step 2820, loss 0.54813.
Train: 2018-08-05T01:47:59.075784: step 2821, loss 0.553612.
Train: 2018-08-05T01:48:02.607407: step 2822, loss 0.536086.
Train: 2018-08-05T01:48:06.107776: step 2823, loss 0.60615.
Train: 2018-08-05T01:48:09.592519: step 2824, loss 0.579837.
Train: 2018-08-05T01:48:13.108516: step 2825, loss 0.483961.
Train: 2018-08-05T01:48:16.640138: step 2826, loss 0.61458.
Train: 2018-08-05T01:48:20.124881: step 2827, loss 0.544977.
Train: 2018-08-05T01:48:23.656503: step 2828, loss 0.527648.
Train: 2018-08-05T01:48:27.188127: step 2829, loss 0.631699.
Train: 2018-08-05T01:48:30.657243: step 2830, loss 0.527738.
Test: 2018-08-05T01:48:45.690081: step 2830, loss 0.547114.
Train: 2018-08-05T01:48:49.190450: step 2831, loss 0.57098.
Train: 2018-08-05T01:48:52.706445: step 2832, loss 0.519194.
Train: 2018-08-05T01:48:56.238068: step 2833, loss 0.51921.
Train: 2018-08-05T01:48:59.800944: step 2834, loss 0.579598.
Train: 2018-08-05T01:49:03.316940: step 2835, loss 0.562338.
Train: 2018-08-05T01:49:06.864190: step 2836, loss 0.58823.
Train: 2018-08-05T01:49:10.364559: step 2837, loss 0.596839.
Train: 2018-08-05T01:49:13.896182: step 2838, loss 0.52788.
Train: 2018-08-05T01:49:17.412178: step 2839, loss 0.579556.
Train: 2018-08-05T01:49:20.912548: step 2840, loss 0.588143.
Test: 2018-08-05T01:49:35.976637: step 2840, loss 0.547394.
Train: 2018-08-05T01:49:39.492634: step 2841, loss 0.588108.
Train: 2018-08-05T01:49:42.993004: step 2842, loss 0.545183.
Train: 2018-08-05T01:49:46.493373: step 2843, loss 0.510945.
Train: 2018-08-05T01:49:50.024996: step 2844, loss 0.545207.
Train: 2018-08-05T01:49:53.587872: step 2845, loss 0.622303.
Train: 2018-08-05T01:49:57.088241: step 2846, loss 0.570894.
Train: 2018-08-05T01:50:00.604238: step 2847, loss 0.519594.
Train: 2018-08-05T01:50:04.151487: step 2848, loss 0.545242.
Train: 2018-08-05T01:50:07.651856: step 2849, loss 0.562337.
Train: 2018-08-05T01:50:11.183480: step 2850, loss 0.605096.
Test: 2018-08-05T01:50:26.185063: step 2850, loss 0.547081.
Train: 2018-08-05T01:50:29.732312: step 2851, loss 0.59652.
Train: 2018-08-05T01:50:33.248308: step 2852, loss 0.536736.
Train: 2018-08-05T01:50:36.764305: step 2853, loss 0.528227.
Train: 2018-08-05T01:50:40.249047: step 2854, loss 0.622044.
Train: 2018-08-05T01:50:43.765044: step 2855, loss 0.57938.
Train: 2018-08-05T01:50:47.296667: step 2856, loss 0.553836.
Train: 2018-08-05T01:50:50.828289: step 2857, loss 0.562346.
Train: 2018-08-05T01:50:54.328659: step 2858, loss 0.502916.
Train: 2018-08-05T01:50:57.829028: step 2859, loss 0.596324.
Train: 2018-08-05T01:51:01.360651: step 2860, loss 0.477431.
Test: 2018-08-05T01:51:16.409115: step 2860, loss 0.548532.
Train: 2018-08-05T01:51:19.909484: step 2861, loss 0.613375.
Train: 2018-08-05T01:51:23.425480: step 2862, loss 0.630409.
Train: 2018-08-05T01:51:26.925849: step 2863, loss 0.545349.
Train: 2018-08-05T01:51:30.441846: step 2864, loss 0.570841.
Train: 2018-08-05T01:51:33.926588: step 2865, loss 0.562349.
Train: 2018-08-05T01:51:37.426958: step 2866, loss 0.562351.
Train: 2018-08-05T01:51:40.942954: step 2867, loss 0.613217.
Train: 2018-08-05T01:51:44.474577: step 2868, loss 0.545428.
Train: 2018-08-05T01:51:46.177882: step 2869, loss 0.634519.
Train: 2018-08-05T01:51:49.693878: step 2870, loss 0.511753.
Test: 2018-08-05T01:52:04.742342: step 2870, loss 0.549656.
Train: 2018-08-05T01:52:08.258338: step 2871, loss 0.562372.
Train: 2018-08-05T01:52:11.805587: step 2872, loss 0.469752.
Train: 2018-08-05T01:52:15.305956: step 2873, loss 0.56237.
Train: 2018-08-05T01:52:18.837579: step 2874, loss 0.562365.
Train: 2018-08-05T01:52:22.353575: step 2875, loss 0.528553.
Train: 2018-08-05T01:52:25.853945: step 2876, loss 0.604694.
Train: 2018-08-05T01:52:29.354314: step 2877, loss 0.545405.
Train: 2018-08-05T01:52:32.870311: step 2878, loss 0.519932.
Train: 2018-08-05T01:52:36.386306: step 2879, loss 0.596346.
Train: 2018-08-05T01:52:39.902302: step 2880, loss 0.596379.
Test: 2018-08-05T01:52:54.872636: step 2880, loss 0.547933.
Train: 2018-08-05T01:52:58.451136: step 2881, loss 0.570854.
Train: 2018-08-05T01:53:01.998385: step 2882, loss 0.511277.
Train: 2018-08-05T01:53:05.498755: step 2883, loss 0.579382.
Train: 2018-08-05T01:53:09.030377: step 2884, loss 0.579392.
Train: 2018-08-05T01:53:12.530747: step 2885, loss 0.528226.
Train: 2018-08-05T01:53:16.046743: step 2886, loss 0.528193.
Train: 2018-08-05T01:53:19.562739: step 2887, loss 0.690585.
Train: 2018-08-05T01:53:23.125615: step 2888, loss 0.596493.
Train: 2018-08-05T01:53:26.610358: step 2889, loss 0.604953.
Train: 2018-08-05T01:53:30.126354: step 2890, loss 0.587846.
Test: 2018-08-05T01:53:45.284206: step 2890, loss 0.548392.
Train: 2018-08-05T01:53:48.768947: step 2891, loss 0.579304.
Train: 2018-08-05T01:53:52.284943: step 2892, loss 0.621519.
Train: 2018-08-05T01:53:55.832193: step 2893, loss 0.59605.
Train: 2018-08-05T01:53:59.348189: step 2894, loss 0.528864.
Train: 2018-08-05T01:54:02.848559: step 2895, loss 0.554054.
Train: 2018-08-05T01:54:06.364555: step 2896, loss 0.487379.
Train: 2018-08-05T01:54:09.880551: step 2897, loss 0.545758.
Train: 2018-08-05T01:54:13.443427: step 2898, loss 0.570765.
Train: 2018-08-05T01:54:16.959423: step 2899, loss 0.470711.
Train: 2018-08-05T01:54:20.506672: step 2900, loss 0.554058.
Test: 2018-08-05T01:54:35.523883: step 2900, loss 0.548413.
Train: 2018-08-05T01:54:41.180730: step 2901, loss 0.604279.
Train: 2018-08-05T01:54:44.681099: step 2902, loss 0.528852.
Train: 2018-08-05T01:54:48.181469: step 2903, loss 0.528778.
Train: 2018-08-05T01:54:51.666212: step 2904, loss 0.579221.
Train: 2018-08-05T01:54:55.182208: step 2905, loss 0.562366.
Train: 2018-08-05T01:54:58.698204: step 2906, loss 0.553904.
Train: 2018-08-05T01:55:02.214200: step 2907, loss 0.553883.
Train: 2018-08-05T01:55:05.745822: step 2908, loss 0.528399.
Train: 2018-08-05T01:55:09.277445: step 2909, loss 0.562343.
Train: 2018-08-05T01:55:12.777815: step 2910, loss 0.536757.
Test: 2018-08-05T01:55:27.779398: step 2910, loss 0.548255.
Train: 2018-08-05T01:55:31.420408: step 2911, loss 0.570886.
Train: 2018-08-05T01:55:34.920777: step 2912, loss 0.562335.
Train: 2018-08-05T01:55:38.421147: step 2913, loss 0.570919.
Train: 2018-08-05T01:55:41.952770: step 2914, loss 0.545143.
Train: 2018-08-05T01:55:45.453139: step 2915, loss 0.631221.
Train: 2018-08-05T01:55:48.937882: step 2916, loss 0.614.
Train: 2018-08-05T01:55:52.453879: step 2917, loss 0.476324.
Train: 2018-08-05T01:55:55.969875: step 2918, loss 0.553728.
Train: 2018-08-05T01:55:59.501497: step 2919, loss 0.527877.
Train: 2018-08-05T01:56:03.033120: step 2920, loss 0.536456.
Test: 2018-08-05T01:56:18.081584: step 2920, loss 0.547509.
Train: 2018-08-05T01:56:21.581953: step 2921, loss 0.536413.
Train: 2018-08-05T01:56:25.113576: step 2922, loss 0.545024.
Train: 2018-08-05T01:56:28.629572: step 2923, loss 0.536313.
Train: 2018-08-05T01:56:32.145568: step 2924, loss 0.571055.
Train: 2018-08-05T01:56:35.677191: step 2925, loss 0.54493.
Train: 2018-08-05T01:56:39.177560: step 2926, loss 0.571102.
Train: 2018-08-05T01:56:42.693556: step 2927, loss 0.55363.
Train: 2018-08-05T01:56:46.240806: step 2928, loss 0.509835.
Train: 2018-08-05T01:56:49.788055: step 2929, loss 0.553616.
Train: 2018-08-05T01:56:53.319678: step 2930, loss 0.579992.
Test: 2018-08-05T01:57:08.290009: step 2930, loss 0.547502.
Train: 2018-08-05T01:57:11.774751: step 2931, loss 0.571217.
Train: 2018-08-05T01:57:15.275120: step 2932, loss 0.624111.
Train: 2018-08-05T01:57:18.806744: step 2933, loss 0.544796.
Train: 2018-08-05T01:57:22.307113: step 2934, loss 0.56241.
Train: 2018-08-05T01:57:25.854363: step 2935, loss 0.668021.
Train: 2018-08-05T01:57:29.385985: step 2936, loss 0.527282.
Train: 2018-08-05T01:57:32.886355: step 2937, loss 0.614953.
Train: 2018-08-05T01:57:36.386724: step 2938, loss 0.536161.
Train: 2018-08-05T01:57:39.933974: step 2939, loss 0.562363.
Train: 2018-08-05T01:57:43.418716: step 2940, loss 0.527555.
Test: 2018-08-05T01:57:58.404674: step 2940, loss 0.54684.
Train: 2018-08-05T01:58:01.936296: step 2941, loss 0.501524.
Train: 2018-08-05T01:58:05.452292: step 2942, loss 0.640568.
Train: 2018-08-05T01:58:08.983915: step 2943, loss 0.510289.
Train: 2018-08-05T01:58:12.484285: step 2944, loss 0.588365.
Train: 2018-08-05T01:58:16.000281: step 2945, loss 0.493036.
Train: 2018-08-05T01:58:19.485024: step 2946, loss 0.605683.
Train: 2018-08-05T01:58:23.032273: step 2947, loss 0.536357.
Train: 2018-08-05T01:58:26.595149: step 2948, loss 0.536358.
Train: 2018-08-05T01:58:30.111145: step 2949, loss 0.562345.
Train: 2018-08-05T01:58:33.642768: step 2950, loss 0.666367.
Test: 2018-08-05T01:58:48.753739: step 2950, loss 0.549686.
Train: 2018-08-05T01:58:52.269734: step 2951, loss 0.536387.
Train: 2018-08-05T01:58:55.770104: step 2952, loss 0.570981.
Train: 2018-08-05T01:58:59.348606: step 2953, loss 0.562338.
Train: 2018-08-05T01:59:02.880230: step 2954, loss 0.588189.
Train: 2018-08-05T01:59:06.411852: step 2955, loss 0.648359.
Train: 2018-08-05T01:59:09.927848: step 2956, loss 0.579481.
Train: 2018-08-05T01:59:13.428217: step 2957, loss 0.553794.
Train: 2018-08-05T01:59:16.928587: step 2958, loss 0.587896.
Train: 2018-08-05T01:59:20.475837: step 2959, loss 0.579329.
Train: 2018-08-05T01:59:24.007459: step 2960, loss 0.486187.
Test: 2018-08-05T01:59:39.118430: step 2960, loss 0.54766.
Train: 2018-08-05T01:59:42.634426: step 2961, loss 0.629985.
Train: 2018-08-05T01:59:46.134795: step 2962, loss 0.520212.
Train: 2018-08-05T01:59:49.635164: step 2963, loss 0.604479.
Train: 2018-08-05T01:59:53.151160: step 2964, loss 0.528769.
Train: 2018-08-05T01:59:56.651530: step 2965, loss 0.553994.
Train: 2018-08-05T02:00:00.230033: step 2966, loss 0.562392.
Train: 2018-08-05T02:00:03.777282: step 2967, loss 0.562395.
Train: 2018-08-05T02:00:07.277652: step 2968, loss 0.57078.
Train: 2018-08-05T02:00:10.793648: step 2969, loss 0.604295.
Train: 2018-08-05T02:00:14.309644: step 2970, loss 0.478715.
Test: 2018-08-05T02:00:29.311228: step 2970, loss 0.547833.
Train: 2018-08-05T02:00:32.858477: step 2971, loss 0.570777.
Train: 2018-08-05T02:00:36.421353: step 2972, loss 0.52887.
Train: 2018-08-05T02:00:39.984229: step 2973, loss 0.537208.
Train: 2018-08-05T02:00:43.500225: step 2974, loss 0.562381.
Train: 2018-08-05T02:00:47.000595: step 2975, loss 0.520246.
Train: 2018-08-05T02:00:50.500964: step 2976, loss 0.520126.
Train: 2018-08-05T02:00:54.001334: step 2977, loss 0.579303.
Train: 2018-08-05T02:00:57.532957: step 2978, loss 0.494364.
Train: 2018-08-05T02:01:01.048953: step 2979, loss 0.596461.
Train: 2018-08-05T02:01:04.549323: step 2980, loss 0.579444.
Test: 2018-08-05T02:01:19.535282: step 2980, loss 0.549796.
Train: 2018-08-05T02:01:23.035649: step 2981, loss 0.562335.
Train: 2018-08-05T02:01:26.551645: step 2982, loss 0.596685.
Train: 2018-08-05T02:01:30.036388: step 2983, loss 0.527954.
Train: 2018-08-05T02:01:33.599264: step 2984, loss 0.519295.
Train: 2018-08-05T02:01:37.177767: step 2985, loss 0.58822.
Train: 2018-08-05T02:01:40.693763: step 2986, loss 0.493225.
Train: 2018-08-05T02:01:44.225385: step 2987, loss 0.475723.
Train: 2018-08-05T02:01:47.741382: step 2988, loss 0.623238.
Train: 2018-08-05T02:01:51.241751: step 2989, loss 0.605952.
Train: 2018-08-05T02:01:54.773374: step 2990, loss 0.623459.
Test: 2018-08-05T02:02:09.806210: step 2990, loss 0.549.
Train: 2018-08-05T02:02:13.353460: step 2991, loss 0.562366.
Train: 2018-08-05T02:02:16.869456: step 2992, loss 0.51004.
Train: 2018-08-05T02:02:20.354199: step 2993, loss 0.65835.
Train: 2018-08-05T02:02:23.870195: step 2994, loss 0.605923.
Train: 2018-08-05T02:02:27.401818: step 2995, loss 0.510202.
Train: 2018-08-05T02:02:30.949068: step 2996, loss 0.527622.
Train: 2018-08-05T02:02:34.480691: step 2997, loss 0.562349.
Train: 2018-08-05T02:02:38.012313: step 2998, loss 0.544998.
Train: 2018-08-05T02:02:41.512683: step 2999, loss 0.536326.
Train: 2018-08-05T02:02:45.013052: step 3000, loss 0.588379.
Test: 2018-08-05T02:03:00.077142: step 3000, loss 0.54826.
Train: 2018-08-05T02:03:05.905882: step 3001, loss 0.597045.
Train: 2018-08-05T02:03:09.421879: step 3002, loss 0.588341.
Train: 2018-08-05T02:03:12.937875: step 3003, loss 0.510431.
Train: 2018-08-05T02:03:16.438244: step 3004, loss 0.553693.
Train: 2018-08-05T02:03:19.938614: step 3005, loss 0.61422.
Train: 2018-08-05T02:03:23.438983: step 3006, loss 0.5278.
Train: 2018-08-05T02:03:26.939353: step 3007, loss 0.605489.
Train: 2018-08-05T02:03:30.470976: step 3008, loss 0.545102.
Train: 2018-08-05T02:03:34.065105: step 3009, loss 0.588164.
Train: 2018-08-05T02:03:37.565475: step 3010, loss 0.596722.
Test: 2018-08-05T02:03:52.598311: step 3010, loss 0.548205.
Train: 2018-08-05T02:03:56.114307: step 3011, loss 0.510863.
Train: 2018-08-05T02:03:59.614677: step 3012, loss 0.579479.
Train: 2018-08-05T02:04:03.115046: step 3013, loss 0.588022.
Train: 2018-08-05T02:04:06.599790: step 3014, loss 0.622179.
Train: 2018-08-05T02:04:10.147039: step 3015, loss 0.536762.
Train: 2018-08-05T02:04:13.741168: step 3016, loss 0.613403.
Train: 2018-08-05T02:04:17.241538: step 3017, loss 0.53689.
Train: 2018-08-05T02:04:20.757534: step 3018, loss 0.562354.
Train: 2018-08-05T02:04:24.257903: step 3019, loss 0.553904.
Train: 2018-08-05T02:04:25.976835: step 3020, loss 0.562364.
Test: 2018-08-05T02:04:41.025299: step 3020, loss 0.54964.
Train: 2018-08-05T02:04:44.556921: step 3021, loss 0.511755.
Train: 2018-08-05T02:04:48.104171: step 3022, loss 0.50331.
Train: 2018-08-05T02:04:51.635793: step 3023, loss 0.494767.
Train: 2018-08-05T02:04:55.120537: step 3024, loss 0.536933.
Train: 2018-08-05T02:04:58.683412: step 3025, loss 0.562345.
Train: 2018-08-05T02:05:02.183782: step 3026, loss 0.511194.
Train: 2018-08-05T02:05:05.731031: step 3027, loss 0.493892.
Train: 2018-08-05T02:05:09.247027: step 3028, loss 0.562335.
Train: 2018-08-05T02:05:12.747397: step 3029, loss 0.510547.
Train: 2018-08-05T02:05:16.247766: step 3030, loss 0.562347.
Test: 2018-08-05T02:05:31.296230: step 3030, loss 0.54621.
Train: 2018-08-05T02:05:34.859106: step 3031, loss 0.597198.
Train: 2018-08-05T02:05:38.375102: step 3032, loss 0.606048.
Train: 2018-08-05T02:05:41.891098: step 3033, loss 0.632378.
Train: 2018-08-05T02:05:45.438348: step 3034, loss 0.544878.
Train: 2018-08-05T02:05:48.938717: step 3035, loss 0.553627.
Train: 2018-08-05T02:05:52.423460: step 3036, loss 0.579887.
Train: 2018-08-05T02:05:55.939456: step 3037, loss 0.562378.
Train: 2018-08-05T02:05:59.424199: step 3038, loss 0.597371.
Train: 2018-08-05T02:06:02.971448: step 3039, loss 0.536156.
Train: 2018-08-05T02:06:06.518698: step 3040, loss 0.527436.
Test: 2018-08-05T02:06:21.520281: step 3040, loss 0.547784.
Train: 2018-08-05T02:06:25.036277: step 3041, loss 0.614773.
Train: 2018-08-05T02:06:28.552274: step 3042, loss 0.588538.
Train: 2018-08-05T02:06:32.021390: step 3043, loss 0.666876.
Train: 2018-08-05T02:06:35.521760: step 3044, loss 0.544994.
Train: 2018-08-05T02:06:39.022129: step 3045, loss 0.553691.
Train: 2018-08-05T02:06:42.538125: step 3046, loss 0.605477.
Train: 2018-08-05T02:06:46.069747: step 3047, loss 0.484945.
Train: 2018-08-05T02:06:49.663877: step 3048, loss 0.545162.
Train: 2018-08-05T02:06:53.179873: step 3049, loss 0.605227.
Train: 2018-08-05T02:06:56.711496: step 3050, loss 0.562336.
Test: 2018-08-05T02:07:11.666201: step 3050, loss 0.548256.
Train: 2018-08-05T02:07:15.197822: step 3051, loss 0.511042.
Train: 2018-08-05T02:07:18.698192: step 3052, loss 0.562337.
Train: 2018-08-05T02:07:22.198562: step 3053, loss 0.493995.
Train: 2018-08-05T02:07:25.730184: step 3054, loss 0.545231.
Train: 2018-08-05T02:07:29.214927: step 3055, loss 0.553771.
Train: 2018-08-05T02:07:32.699670: step 3056, loss 0.605216.
Train: 2018-08-05T02:07:36.231292: step 3057, loss 0.493702.
Train: 2018-08-05T02:07:39.747289: step 3058, loss 0.562335.
Train: 2018-08-05T02:07:43.247658: step 3059, loss 0.639798.
Train: 2018-08-05T02:07:46.763655: step 3060, loss 0.562336.
Test: 2018-08-05T02:08:01.749611: step 3060, loss 0.548564.
Train: 2018-08-05T02:08:05.265607: step 3061, loss 0.588141.
Train: 2018-08-05T02:08:08.781604: step 3062, loss 0.49358.
Train: 2018-08-05T02:08:12.297599: step 3063, loss 0.553736.
Train: 2018-08-05T02:08:15.844849: step 3064, loss 0.622579.
Train: 2018-08-05T02:08:19.376472: step 3065, loss 0.545134.
Train: 2018-08-05T02:08:22.892468: step 3066, loss 0.596729.
Train: 2018-08-05T02:08:26.424091: step 3067, loss 0.613874.
Train: 2018-08-05T02:08:29.924461: step 3068, loss 0.528044.
Train: 2018-08-05T02:08:33.440456: step 3069, loss 0.49383.
Train: 2018-08-05T02:08:36.972079: step 3070, loss 0.510933.
Test: 2018-08-05T02:08:51.989289: step 3070, loss 0.547414.
Train: 2018-08-05T02:08:55.489659: step 3071, loss 0.579494.
Train: 2018-08-05T02:08:59.005655: step 3072, loss 0.54516.
Train: 2018-08-05T02:09:02.521651: step 3073, loss 0.631116.
Train: 2018-08-05T02:09:06.006395: step 3074, loss 0.562335.
Train: 2018-08-05T02:09:09.538017: step 3075, loss 0.682601.
Train: 2018-08-05T02:09:13.069640: step 3076, loss 0.588032.
Train: 2018-08-05T02:09:16.585636: step 3077, loss 0.536724.
Train: 2018-08-05T02:09:20.117259: step 3078, loss 0.536787.
Train: 2018-08-05T02:09:23.633256: step 3079, loss 0.570849.
Train: 2018-08-05T02:09:27.180504: step 3080, loss 0.587822.
Test: 2018-08-05T02:09:42.197714: step 3080, loss 0.548979.
Train: 2018-08-05T02:09:45.729337: step 3081, loss 0.553878.
Train: 2018-08-05T02:09:49.276587: step 3082, loss 0.553895.
Train: 2018-08-05T02:09:52.808209: step 3083, loss 0.511646.
Train: 2018-08-05T02:09:56.355459: step 3084, loss 0.537.
Train: 2018-08-05T02:09:59.855828: step 3085, loss 0.520062.
Train: 2018-08-05T02:10:03.340571: step 3086, loss 0.511517.
Train: 2018-08-05T02:10:06.856567: step 3087, loss 0.570841.
Train: 2018-08-05T02:10:10.403817: step 3088, loss 0.528292.
Train: 2018-08-05T02:10:13.935440: step 3089, loss 0.545268.
Train: 2018-08-05T02:10:17.451436: step 3090, loss 0.510983.
Test: 2018-08-05T02:10:32.531152: step 3090, loss 0.546804.
Train: 2018-08-05T02:10:36.094029: step 3091, loss 0.476443.
Train: 2018-08-05T02:10:39.594398: step 3092, loss 0.588234.
Train: 2018-08-05T02:10:43.126021: step 3093, loss 0.55368.
Train: 2018-08-05T02:10:46.657644: step 3094, loss 0.562354.
Train: 2018-08-05T02:10:50.189266: step 3095, loss 0.64087.
Train: 2018-08-05T02:10:53.689636: step 3096, loss 0.518708.
Train: 2018-08-05T02:10:57.205632: step 3097, loss 0.614861.
Train: 2018-08-05T02:11:00.721628: step 3098, loss 0.606133.
Train: 2018-08-05T02:11:04.237625: step 3099, loss 0.509903.
Train: 2018-08-05T02:11:07.784874: step 3100, loss 0.509885.
Test: 2018-08-05T02:11:22.770830: step 3100, loss 0.547753.
Train: 2018-08-05T02:11:28.615198: step 3101, loss 0.509824.
Train: 2018-08-05T02:11:32.115567: step 3102, loss 0.544837.
Train: 2018-08-05T02:11:35.631563: step 3103, loss 0.579998.
Train: 2018-08-05T02:11:39.131933: step 3104, loss 0.527178.
Train: 2018-08-05T02:11:42.632302: step 3105, loss 0.597724.
Train: 2018-08-05T02:11:46.163925: step 3106, loss 0.650746.
Train: 2018-08-05T02:11:49.664295: step 3107, loss 0.580063.
Train: 2018-08-05T02:11:53.180291: step 3108, loss 0.571218.
Train: 2018-08-05T02:11:56.680660: step 3109, loss 0.509654.
Train: 2018-08-05T02:12:00.196657: step 3110, loss 0.518471.
Test: 2018-08-05T02:12:15.198240: step 3110, loss 0.546308.
Train: 2018-08-05T02:12:18.667357: step 3111, loss 0.588758.
Train: 2018-08-05T02:12:22.198979: step 3112, loss 0.588741.
Train: 2018-08-05T02:12:25.777482: step 3113, loss 0.509759.
Train: 2018-08-05T02:12:29.277851: step 3114, loss 0.623785.
Train: 2018-08-05T02:12:32.809474: step 3115, loss 0.448525.
Train: 2018-08-05T02:12:36.309843: step 3116, loss 0.571152.
Train: 2018-08-05T02:12:39.825840: step 3117, loss 0.500999.
Train: 2018-08-05T02:12:43.388715: step 3118, loss 0.59753.
Train: 2018-08-05T02:12:46.904712: step 3119, loss 0.553611.
Train: 2018-08-05T02:12:50.405081: step 3120, loss 0.579988.
Test: 2018-08-05T02:13:05.422291: step 3120, loss 0.547313.
Train: 2018-08-05T02:13:09.000794: step 3121, loss 0.694295.
Train: 2018-08-05T02:13:12.516791: step 3122, loss 0.641304.
Train: 2018-08-05T02:13:16.032786: step 3123, loss 0.588564.
Train: 2018-08-05T02:13:19.564409: step 3124, loss 0.597126.
Train: 2018-08-05T02:13:23.064779: step 3125, loss 0.553691.
Train: 2018-08-05T02:13:26.596401: step 3126, loss 0.527877.
Train: 2018-08-05T02:13:30.128024: step 3127, loss 0.553747.
Train: 2018-08-05T02:13:33.690900: step 3128, loss 0.579465.
Train: 2018-08-05T02:13:37.253777: step 3129, loss 0.519634.
Train: 2018-08-05T02:13:40.816653: step 3130, loss 0.622027.
Test: 2018-08-05T02:13:55.990129: step 3130, loss 0.54736.
Train: 2018-08-05T02:13:59.537378: step 3131, loss 0.587855.
Train: 2018-08-05T02:14:03.053375: step 3132, loss 0.57083.
Train: 2018-08-05T02:14:06.584998: step 3133, loss 0.587723.
Train: 2018-08-05T02:14:10.116620: step 3134, loss 0.528659.
Train: 2018-08-05T02:14:13.648243: step 3135, loss 0.56238.
Train: 2018-08-05T02:14:17.164239: step 3136, loss 0.545594.
Train: 2018-08-05T02:14:20.711489: step 3137, loss 0.554006.
Train: 2018-08-05T02:14:24.243112: step 3138, loss 0.537252.
Train: 2018-08-05T02:14:27.759108: step 3139, loss 0.612688.
Train: 2018-08-05T02:14:31.243851: step 3140, loss 0.537283.
Test: 2018-08-05T02:14:46.292314: step 3140, loss 0.549002.
Train: 2018-08-05T02:14:49.808310: step 3141, loss 0.562404.
Train: 2018-08-05T02:14:53.308680: step 3142, loss 0.554035.
Train: 2018-08-05T02:14:56.871556: step 3143, loss 0.595888.
Train: 2018-08-05T02:15:00.387552: step 3144, loss 0.57914.
Train: 2018-08-05T02:15:03.919175: step 3145, loss 0.520609.
Train: 2018-08-05T02:15:07.403917: step 3146, loss 0.520591.
Train: 2018-08-05T02:15:10.904287: step 3147, loss 0.512152.
Train: 2018-08-05T02:15:14.451537: step 3148, loss 0.562389.
Train: 2018-08-05T02:15:18.014413: step 3149, loss 0.478238.
Train: 2018-08-05T02:15:21.561662: step 3150, loss 0.553916.
Test: 2018-08-05T02:15:36.672632: step 3150, loss 0.548387.
Train: 2018-08-05T02:15:40.266762: step 3151, loss 0.596267.
Train: 2018-08-05T02:15:43.814011: step 3152, loss 0.579348.
Train: 2018-08-05T02:15:47.298754: step 3153, loss 0.494191.
Train: 2018-08-05T02:15:50.814750: step 3154, loss 0.622166.
Train: 2018-08-05T02:15:54.330746: step 3155, loss 0.613704.
Train: 2018-08-05T02:15:57.862369: step 3156, loss 0.545205.
Train: 2018-08-05T02:16:01.362739: step 3157, loss 0.553764.
Train: 2018-08-05T02:16:04.878735: step 3158, loss 0.570913.
Train: 2018-08-05T02:16:08.425984: step 3159, loss 0.579501.
Train: 2018-08-05T02:16:11.957607: step 3160, loss 0.605253.
Test: 2018-08-05T02:16:26.959190: step 3160, loss 0.547814.
Train: 2018-08-05T02:16:30.522066: step 3161, loss 0.596641.
Train: 2018-08-05T02:16:34.038063: step 3162, loss 0.476699.
Train: 2018-08-05T02:16:37.538432: step 3163, loss 0.588037.
Train: 2018-08-05T02:16:41.070055: step 3164, loss 0.588033.
Train: 2018-08-05T02:16:44.586051: step 3165, loss 0.588016.
Train: 2018-08-05T02:16:48.070794: step 3166, loss 0.596536.
Train: 2018-08-05T02:16:51.649297: step 3167, loss 0.587942.
Train: 2018-08-05T02:16:55.165293: step 3168, loss 0.579374.
Train: 2018-08-05T02:16:58.681289: step 3169, loss 0.570843.
Train: 2018-08-05T02:17:02.150405: step 3170, loss 0.579307.
Test: 2018-08-05T02:17:17.151988: step 3170, loss 0.54804.
Train: 2018-08-05T02:17:18.855294: step 3171, loss 0.616486.
Train: 2018-08-05T02:17:22.371290: step 3172, loss 0.62138.
Train: 2018-08-05T02:17:25.902912: step 3173, loss 0.5456.
Train: 2018-08-05T02:17:29.450162: step 3174, loss 0.545672.
Train: 2018-08-05T02:17:33.013038: step 3175, loss 0.487307.
Train: 2018-08-05T02:17:36.513407: step 3176, loss 0.529057.
Train: 2018-08-05T02:17:40.029403: step 3177, loss 0.570768.
Train: 2018-08-05T02:17:43.529773: step 3178, loss 0.662595.
Train: 2018-08-05T02:17:47.045769: step 3179, loss 0.562432.
Train: 2018-08-05T02:17:50.577392: step 3180, loss 0.595721.
Test: 2018-08-05T02:18:05.594602: step 3180, loss 0.547825.
Train: 2018-08-05T02:18:09.282491: step 3181, loss 0.512638.
Train: 2018-08-05T02:18:12.814114: step 3182, loss 0.537565.
Train: 2018-08-05T02:18:16.330111: step 3183, loss 0.570759.
Train: 2018-08-05T02:18:19.861733: step 3184, loss 0.520953.
Train: 2018-08-05T02:18:23.408983: step 3185, loss 0.545828.
Train: 2018-08-05T02:18:26.987485: step 3186, loss 0.495849.
Train: 2018-08-05T02:18:30.534735: step 3187, loss 0.537373.
Train: 2018-08-05T02:18:34.097611: step 3188, loss 0.595908.
Train: 2018-08-05T02:18:37.613607: step 3189, loss 0.587579.
Train: 2018-08-05T02:18:41.129603: step 3190, loss 0.511916.
Test: 2018-08-05T02:18:56.162440: step 3190, loss 0.54945.
Train: 2018-08-05T02:18:59.662809: step 3191, loss 0.545503.
Train: 2018-08-05T02:19:03.210059: step 3192, loss 0.630016.
Train: 2018-08-05T02:19:06.757308: step 3193, loss 0.570822.
Train: 2018-08-05T02:19:10.242051: step 3194, loss 0.536932.
Train: 2018-08-05T02:19:13.758047: step 3195, loss 0.621746.
Train: 2018-08-05T02:19:17.289670: step 3196, loss 0.604775.
Train: 2018-08-05T02:19:20.805666: step 3197, loss 0.579307.
Train: 2018-08-05T02:19:24.368542: step 3198, loss 0.570823.
Train: 2018-08-05T02:19:27.884538: step 3199, loss 0.579275.
Train: 2018-08-05T02:19:31.384908: step 3200, loss 0.604594.
Test: 2018-08-05T02:19:46.386491: step 3200, loss 0.548102.
Train: 2018-08-05T02:19:52.152725: step 3201, loss 0.579228.
Train: 2018-08-05T02:19:55.637468: step 3202, loss 0.570791.
Train: 2018-08-05T02:19:59.184717: step 3203, loss 0.562391.
Train: 2018-08-05T02:20:02.685087: step 3204, loss 0.570777.
Train: 2018-08-05T02:20:06.216710: step 3205, loss 0.554048.
Train: 2018-08-05T02:20:09.701452: step 3206, loss 0.554066.
Train: 2018-08-05T02:20:13.201822: step 3207, loss 0.554078.
Train: 2018-08-05T02:20:16.702192: step 3208, loss 0.537404.
Train: 2018-08-05T02:20:20.202561: step 3209, loss 0.570767.
Train: 2018-08-05T02:20:23.734184: step 3210, loss 0.562423.
Test: 2018-08-05T02:20:38.751395: step 3210, loss 0.548867.
Train: 2018-08-05T02:20:42.283017: step 3211, loss 0.512346.
Train: 2018-08-05T02:20:45.799013: step 3212, loss 0.562413.
Train: 2018-08-05T02:20:49.299383: step 3213, loss 0.595884.
Train: 2018-08-05T02:20:52.831005: step 3214, loss 0.520526.
Train: 2018-08-05T02:20:56.347001: step 3215, loss 0.52884.
Train: 2018-08-05T02:20:59.878625: step 3216, loss 0.57079.
Train: 2018-08-05T02:21:03.410247: step 3217, loss 0.562374.
Train: 2018-08-05T02:21:06.910617: step 3218, loss 0.59612.
Train: 2018-08-05T02:21:10.410986: step 3219, loss 0.553918.
Train: 2018-08-05T02:21:13.942609: step 3220, loss 0.54545.
Test: 2018-08-05T02:21:28.928568: step 3220, loss 0.547437.
Train: 2018-08-05T02:21:32.444562: step 3221, loss 0.579289.
Train: 2018-08-05T02:21:36.007438: step 3222, loss 0.519982.
Train: 2018-08-05T02:21:39.539060: step 3223, loss 0.630262.
Train: 2018-08-05T02:21:43.039430: step 3224, loss 0.579329.
Train: 2018-08-05T02:21:46.508547: step 3225, loss 0.545371.
Train: 2018-08-05T02:21:50.024542: step 3226, loss 0.485934.
Train: 2018-08-05T02:21:53.540538: step 3227, loss 0.519812.
Train: 2018-08-05T02:21:57.087788: step 3228, loss 0.553811.
Train: 2018-08-05T02:22:00.650664: step 3229, loss 0.570887.
Train: 2018-08-05T02:22:04.151034: step 3230, loss 0.536631.
Test: 2018-08-05T02:22:19.215124: step 3230, loss 0.547794.
Train: 2018-08-05T02:22:22.731120: step 3231, loss 0.493627.
Train: 2018-08-05T02:22:26.215863: step 3232, loss 0.510622.
Train: 2018-08-05T02:22:29.731859: step 3233, loss 0.579655.
Train: 2018-08-05T02:22:33.294735: step 3234, loss 0.562351.
Train: 2018-08-05T02:22:36.841985: step 3235, loss 0.56236.
Train: 2018-08-05T02:22:40.342354: step 3236, loss 0.675898.
Train: 2018-08-05T02:22:43.842724: step 3237, loss 0.571101.
Train: 2018-08-05T02:22:47.327466: step 3238, loss 0.562367.
Train: 2018-08-05T02:22:50.843463: step 3239, loss 0.640883.
Train: 2018-08-05T02:22:54.375085: step 3240, loss 0.571064.
Test: 2018-08-05T02:23:09.407923: step 3240, loss 0.549044.
Train: 2018-08-05T02:23:12.939545: step 3241, loss 0.527606.
Train: 2018-08-05T02:23:16.471168: step 3242, loss 0.588372.
Train: 2018-08-05T02:23:19.987164: step 3243, loss 0.545026.
Train: 2018-08-05T02:23:23.487534: step 3244, loss 0.553694.
Train: 2018-08-05T02:23:26.987903: step 3245, loss 0.579615.
Train: 2018-08-05T02:23:30.503900: step 3246, loss 0.674469.
Train: 2018-08-05T02:23:34.066775: step 3247, loss 0.579523.
Train: 2018-08-05T02:23:37.629652: step 3248, loss 0.528087.
Train: 2018-08-05T02:23:41.145648: step 3249, loss 0.545259.
Train: 2018-08-05T02:23:44.692897: step 3250, loss 0.604953.
Test: 2018-08-05T02:23:59.725734: step 3250, loss 0.548542.
Train: 2018-08-05T02:24:03.241730: step 3251, loss 0.664337.
Train: 2018-08-05T02:24:06.726473: step 3252, loss 0.528518.
Train: 2018-08-05T02:24:10.273722: step 3253, loss 0.579233.
Train: 2018-08-05T02:24:13.836598: step 3254, loss 0.545579.
Train: 2018-08-05T02:24:17.368221: step 3255, loss 0.537255.
Train: 2018-08-05T02:24:20.868590: step 3256, loss 0.612608.
Train: 2018-08-05T02:24:24.384587: step 3257, loss 0.545731.
Train: 2018-08-05T02:24:27.884956: step 3258, loss 0.579094.
Train: 2018-08-05T02:24:31.432206: step 3259, loss 0.562446.
Train: 2018-08-05T02:24:34.963828: step 3260, loss 0.554156.
Test: 2018-08-05T02:24:50.012292: step 3260, loss 0.548999.
Train: 2018-08-05T02:24:53.512661: step 3261, loss 0.537589.
Train: 2018-08-05T02:24:57.028658: step 3262, loss 0.529308.
Train: 2018-08-05T02:25:00.529027: step 3263, loss 0.587349.
Train: 2018-08-05T02:25:04.029396: step 3264, loss 0.587351.
Train: 2018-08-05T02:25:07.561019: step 3265, loss 0.579052.
Train: 2018-08-05T02:25:11.123895: step 3266, loss 0.562467.
Train: 2018-08-05T02:25:14.624265: step 3267, loss 0.496168.
Train: 2018-08-05T02:25:18.140261: step 3268, loss 0.587357.
Train: 2018-08-05T02:25:21.640630: step 3269, loss 0.545841.
Train: 2018-08-05T02:25:25.125374: step 3270, loss 0.446014.
Test: 2018-08-05T02:25:40.298850: step 3270, loss 0.548477.
Train: 2018-08-05T02:25:43.846099: step 3271, loss 0.637557.
Train: 2018-08-05T02:25:47.408975: step 3272, loss 0.528951.
Train: 2018-08-05T02:25:50.924972: step 3273, loss 0.512081.
Train: 2018-08-05T02:25:54.425341: step 3274, loss 0.579208.
Train: 2018-08-05T02:25:57.941338: step 3275, loss 0.579244.
Train: 2018-08-05T02:26:01.457334: step 3276, loss 0.570817.
Train: 2018-08-05T02:26:04.988956: step 3277, loss 0.562353.
Train: 2018-08-05T02:26:08.520579: step 3278, loss 0.51991.
Train: 2018-08-05T02:26:12.020948: step 3279, loss 0.502782.
Train: 2018-08-05T02:26:15.521318: step 3280, loss 0.536722.
Test: 2018-08-05T02:26:30.538530: step 3280, loss 0.547233.
Train: 2018-08-05T02:26:34.054524: step 3281, loss 0.519484.
Train: 2018-08-05T02:26:37.586147: step 3282, loss 0.52791.
Train: 2018-08-05T02:26:41.117770: step 3283, loss 0.59692.
Train: 2018-08-05T02:26:44.649392: step 3284, loss 0.588365.
Train: 2018-08-05T02:26:48.149762: step 3285, loss 0.571045.
Train: 2018-08-05T02:26:51.665758: step 3286, loss 0.544943.
Train: 2018-08-05T02:26:55.166128: step 3287, loss 0.510022.
Train: 2018-08-05T02:26:58.682124: step 3288, loss 0.562376.
Train: 2018-08-05T02:27:02.213747: step 3289, loss 0.571153.
Train: 2018-08-05T02:27:05.729743: step 3290, loss 0.588741.
Test: 2018-08-05T02:27:20.731328: step 3290, loss 0.548935.
Train: 2018-08-05T02:27:24.231696: step 3291, loss 0.606348.
Train: 2018-08-05T02:27:27.732065: step 3292, loss 0.615124.
Train: 2018-08-05T02:27:31.263688: step 3293, loss 0.527291.
Train: 2018-08-05T02:27:34.779685: step 3294, loss 0.527312.
Train: 2018-08-05T02:27:38.311307: step 3295, loss 0.562388.
Train: 2018-08-05T02:27:41.842930: step 3296, loss 0.650061.
Train: 2018-08-05T02:27:45.374553: step 3297, loss 0.536129.
Train: 2018-08-05T02:27:48.968682: step 3298, loss 0.562371.
Train: 2018-08-05T02:27:52.515931: step 3299, loss 0.544916.
Train: 2018-08-05T02:27:56.000674: step 3300, loss 0.527498.
Test: 2018-08-05T02:28:11.080392: step 3300, loss 0.547809.
Train: 2018-08-05T02:28:16.799745: step 3301, loss 0.562361.
Train: 2018-08-05T02:28:20.346994: step 3302, loss 0.492676.
Train: 2018-08-05T02:28:23.847364: step 3303, loss 0.60596.
Train: 2018-08-05T02:28:27.363360: step 3304, loss 0.597238.
Train: 2018-08-05T02:28:30.863730: step 3305, loss 0.605915.
Train: 2018-08-05T02:28:34.379726: step 3306, loss 0.527573.
Train: 2018-08-05T02:28:37.911348: step 3307, loss 0.579725.
Train: 2018-08-05T02:28:41.442971: step 3308, loss 0.6144.
Train: 2018-08-05T02:28:44.943341: step 3309, loss 0.579653.
Train: 2018-08-05T02:28:48.521847: step 3310, loss 0.562339.
Test: 2018-08-05T02:29:03.476547: step 3310, loss 0.547358.
Train: 2018-08-05T02:29:06.992543: step 3311, loss 0.596786.
Train: 2018-08-05T02:29:10.524166: step 3312, loss 0.536572.
Train: 2018-08-05T02:29:14.040162: step 3313, loss 0.588044.
Train: 2018-08-05T02:29:17.540532: step 3314, loss 0.528142.
Train: 2018-08-05T02:29:21.056528: step 3315, loss 0.570874.
Train: 2018-08-05T02:29:24.572524: step 3316, loss 0.519726.
Train: 2018-08-05T02:29:28.072893: step 3317, loss 0.536784.
Train: 2018-08-05T02:29:31.588889: step 3318, loss 0.460096.
Train: 2018-08-05T02:29:35.058006: step 3319, loss 0.562338.
Train: 2018-08-05T02:29:38.636509: step 3320, loss 0.536661.
Test: 2018-08-05T02:29:53.731851: step 3320, loss 0.548994.
Train: 2018-08-05T02:29:57.232221: step 3321, loss 0.570914.
Train: 2018-08-05T02:29:59.498086: step 3322, loss 0.599012.
Train: 2018-08-05T02:30:02.982829: step 3323, loss 0.562336.
Train: 2018-08-05T02:30:06.545705: step 3324, loss 0.570948.
Train: 2018-08-05T02:30:10.030447: step 3325, loss 0.493408.
Train: 2018-08-05T02:30:13.546443: step 3326, loss 0.536442.
Train: 2018-08-05T02:30:17.109320: step 3327, loss 0.536389.
Train: 2018-08-05T02:30:20.625316: step 3328, loss 0.649068.
Train: 2018-08-05T02:30:24.125685: step 3329, loss 0.536323.
Train: 2018-08-05T02:30:27.657308: step 3330, loss 0.623118.
Test: 2018-08-05T02:30:42.721398: step 3330, loss 0.548059.
Train: 2018-08-05T02:30:46.253021: step 3331, loss 0.536322.
Train: 2018-08-05T02:30:49.800271: step 3332, loss 0.571022.
Train: 2018-08-05T02:30:53.331893: step 3333, loss 0.501651.
Train: 2018-08-05T02:30:56.832263: step 3334, loss 0.640452.
Train: 2018-08-05T02:31:00.317006: step 3335, loss 0.631706.
Train: 2018-08-05T02:31:03.817375: step 3336, loss 0.553692.
Train: 2018-08-05T02:31:07.348998: step 3337, loss 0.519178.
Train: 2018-08-05T02:31:10.864994: step 3338, loss 0.622705.
Train: 2018-08-05T02:31:14.427870: step 3339, loss 0.622571.
Train: 2018-08-05T02:31:17.928240: step 3340, loss 0.630951.
Test: 2018-08-05T02:31:32.929823: step 3340, loss 0.547291.
Train: 2018-08-05T02:31:36.461446: step 3341, loss 0.553799.
Train: 2018-08-05T02:31:40.024322: step 3342, loss 0.494292.
Train: 2018-08-05T02:31:43.524692: step 3343, loss 0.536879.
Train: 2018-08-05T02:31:47.056314: step 3344, loss 0.58779.
Train: 2018-08-05T02:31:50.556684: step 3345, loss 0.587754.
Train: 2018-08-05T02:31:54.057053: step 3346, loss 0.503215.
Train: 2018-08-05T02:31:57.588683: step 3347, loss 0.537026.
Train: 2018-08-05T02:32:01.135925: step 3348, loss 0.562363.
Train: 2018-08-05T02:32:04.636295: step 3349, loss 0.596163.
Train: 2018-08-05T02:32:08.199171: step 3350, loss 0.587703.
Test: 2018-08-05T02:32:23.185129: step 3350, loss 0.548468.
Train: 2018-08-05T02:32:26.701124: step 3351, loss 0.54549.
Train: 2018-08-05T02:32:30.217120: step 3352, loss 0.486451.
Train: 2018-08-05T02:32:33.733116: step 3353, loss 0.553917.
Train: 2018-08-05T02:32:37.264739: step 3354, loss 0.54544.
Train: 2018-08-05T02:32:40.796362: step 3355, loss 0.545406.
Train: 2018-08-05T02:32:44.327985: step 3356, loss 0.536877.
Train: 2018-08-05T02:32:47.859607: step 3357, loss 0.536813.
Train: 2018-08-05T02:32:51.359977: step 3358, loss 0.536741.
Train: 2018-08-05T02:32:54.860347: step 3359, loss 0.553778.
Train: 2018-08-05T02:32:58.360716: step 3360, loss 0.579499.
Test: 2018-08-05T02:33:13.362300: step 3360, loss 0.547378.
Train: 2018-08-05T02:33:16.893922: step 3361, loss 0.553735.
Train: 2018-08-05T02:33:20.425545: step 3362, loss 0.605427.
Train: 2018-08-05T02:33:23.957168: step 3363, loss 0.570964.
Train: 2018-08-05T02:33:27.473164: step 3364, loss 0.553708.
Train: 2018-08-05T02:33:30.973534: step 3365, loss 0.562339.
Train: 2018-08-05T02:33:34.567663: step 3366, loss 0.579622.
Train: 2018-08-05T02:33:38.114912: step 3367, loss 0.56234.
Train: 2018-08-05T02:33:41.662162: step 3368, loss 0.588265.
Train: 2018-08-05T02:33:45.209411: step 3369, loss 0.674611.
Train: 2018-08-05T02:33:48.756661: step 3370, loss 0.588169.
Test: 2018-08-05T02:34:03.914511: step 3370, loss 0.547803.
Train: 2018-08-05T02:34:07.430507: step 3371, loss 0.570918.
Train: 2018-08-05T02:34:11.009010: step 3372, loss 0.545224.
Train: 2018-08-05T02:34:14.525006: step 3373, loss 0.553804.
Train: 2018-08-05T02:34:18.041002: step 3374, loss 0.587893.
Train: 2018-08-05T02:34:21.572625: step 3375, loss 0.587837.
Train: 2018-08-05T02:34:25.088621: step 3376, loss 0.587775.
Train: 2018-08-05T02:34:28.588990: step 3377, loss 0.57926.
Train: 2018-08-05T02:34:32.104986: step 3378, loss 0.579221.
Train: 2018-08-05T02:34:35.605356: step 3379, loss 0.629575.
Train: 2018-08-05T02:34:39.136979: step 3380, loss 0.503862.
Test: 2018-08-05T02:34:54.201069: step 3380, loss 0.54791.
Train: 2018-08-05T02:34:57.732692: step 3381, loss 0.629179.
Train: 2018-08-05T02:35:01.279941: step 3382, loss 0.55413.
Train: 2018-08-05T02:35:04.795937: step 3383, loss 0.504419.
Train: 2018-08-05T02:35:08.296307: step 3384, loss 0.50449.
Train: 2018-08-05T02:35:11.812303: step 3385, loss 0.529319.
Train: 2018-08-05T02:35:15.312673: step 3386, loss 0.48777.
Train: 2018-08-05T02:35:18.844295: step 3387, loss 0.54579.
Train: 2018-08-05T02:35:22.375918: step 3388, loss 0.570769.
Train: 2018-08-05T02:35:25.876287: step 3389, loss 0.587526.
Train: 2018-08-05T02:35:29.392284: step 3390, loss 0.520431.
Test: 2018-08-05T02:35:44.472001: step 3390, loss 0.548516.
Train: 2018-08-05T02:35:48.003623: step 3391, loss 0.553962.
Train: 2018-08-05T02:35:51.503993: step 3392, loss 0.553926.
Train: 2018-08-05T02:35:55.004362: step 3393, loss 0.630065.
Train: 2018-08-05T02:35:58.551612: step 3394, loss 0.486101.
Train: 2018-08-05T02:36:02.051981: step 3395, loss 0.596327.
Train: 2018-08-05T02:36:05.567977: step 3396, loss 0.579362.
Train: 2018-08-05T02:36:09.068347: step 3397, loss 0.596417.
Train: 2018-08-05T02:36:12.599970: step 3398, loss 0.579384.
Train: 2018-08-05T02:36:16.162845: step 3399, loss 0.562341.
Train: 2018-08-05T02:36:19.710095: step 3400, loss 0.519739.
Test: 2018-08-05T02:36:34.774185: step 3400, loss 0.547705.
Train: 2018-08-05T02:36:40.477913: step 3401, loss 0.587923.
Train: 2018-08-05T02:36:43.962655: step 3402, loss 0.604989.
Train: 2018-08-05T02:36:47.463025: step 3403, loss 0.545292.
Train: 2018-08-05T02:36:50.947768: step 3404, loss 0.60495.
Train: 2018-08-05T02:36:54.463764: step 3405, loss 0.536806.
Train: 2018-08-05T02:36:58.011013: step 3406, loss 0.596377.
Train: 2018-08-05T02:37:01.511383: step 3407, loss 0.528351.
Train: 2018-08-05T02:37:05.043006: step 3408, loss 0.519864.
Train: 2018-08-05T02:37:08.527748: step 3409, loss 0.57935.
Train: 2018-08-05T02:37:12.059371: step 3410, loss 0.545334.
Test: 2018-08-05T02:37:27.201594: step 3410, loss 0.548717.
Train: 2018-08-05T02:37:30.748844: step 3411, loss 0.536812.
Train: 2018-08-05T02:37:34.296094: step 3412, loss 0.587902.
Train: 2018-08-05T02:37:37.812089: step 3413, loss 0.604962.
Train: 2018-08-05T02:37:41.328086: step 3414, loss 0.579382.
Train: 2018-08-05T02:37:44.828455: step 3415, loss 0.621939.
Train: 2018-08-05T02:37:48.328824: step 3416, loss 0.587837.
Train: 2018-08-05T02:37:51.829194: step 3417, loss 0.604738.
Train: 2018-08-05T02:37:55.360817: step 3418, loss 0.596169.
Train: 2018-08-05T02:37:58.845559: step 3419, loss 0.596068.
Train: 2018-08-05T02:38:02.330303: step 3420, loss 0.587567.
Test: 2018-08-05T02:38:17.394393: step 3420, loss 0.547681.
Train: 2018-08-05T02:38:20.926015: step 3421, loss 0.545692.
Train: 2018-08-05T02:38:24.457638: step 3422, loss 0.545761.
Train: 2018-08-05T02:38:27.989261: step 3423, loss 0.545813.
Train: 2018-08-05T02:38:31.474004: step 3424, loss 0.59567.
Train: 2018-08-05T02:38:34.958746: step 3425, loss 0.554182.
Train: 2018-08-05T02:38:38.459117: step 3426, loss 0.570757.
Train: 2018-08-05T02:38:42.021992: step 3427, loss 0.537698.
Train: 2018-08-05T02:38:45.522362: step 3428, loss 0.636843.
Train: 2018-08-05T02:38:49.053984: step 3429, loss 0.546023.
Train: 2018-08-05T02:38:52.569981: step 3430, loss 0.52135.
Test: 2018-08-05T02:39:07.602818: step 3430, loss 0.548201.
Train: 2018-08-05T02:39:11.134440: step 3431, loss 0.554288.
Train: 2018-08-05T02:39:14.634810: step 3432, loss 0.60371.
Train: 2018-08-05T02:39:18.182059: step 3433, loss 0.587227.
Train: 2018-08-05T02:39:21.666802: step 3434, loss 0.578986.
Train: 2018-08-05T02:39:25.167172: step 3435, loss 0.529652.
Train: 2018-08-05T02:39:28.683168: step 3436, loss 0.595426.
Train: 2018-08-05T02:39:32.230417: step 3437, loss 0.570759.
Train: 2018-08-05T02:39:35.762040: step 3438, loss 0.570759.
Train: 2018-08-05T02:39:39.262409: step 3439, loss 0.521482.
Train: 2018-08-05T02:39:42.778406: step 3440, loss 0.546099.
Test: 2018-08-05T02:39:57.779989: step 3440, loss 0.549728.
Train: 2018-08-05T02:40:01.311612: step 3441, loss 0.546065.
Train: 2018-08-05T02:40:04.843234: step 3442, loss 0.521285.
Train: 2018-08-05T02:40:08.421737: step 3443, loss 0.612095.
Train: 2018-08-05T02:40:11.922107: step 3444, loss 0.512799.
Train: 2018-08-05T02:40:15.406850: step 3445, loss 0.529253.
Train: 2018-08-05T02:40:18.907220: step 3446, loss 0.579091.
Train: 2018-08-05T02:40:22.407589: step 3447, loss 0.554069.
Train: 2018-08-05T02:40:25.970465: step 3448, loss 0.512169.
Train: 2018-08-05T02:40:29.502088: step 3449, loss 0.528774.
Train: 2018-08-05T02:40:33.018084: step 3450, loss 0.545493.
Test: 2018-08-05T02:40:48.113427: step 3450, loss 0.548596.
Train: 2018-08-05T02:40:51.676303: step 3451, loss 0.545411.
Train: 2018-08-05T02:40:55.176673: step 3452, loss 0.545332.
Train: 2018-08-05T02:40:58.692669: step 3453, loss 0.528175.
Train: 2018-08-05T02:41:02.224291: step 3454, loss 0.588069.
Train: 2018-08-05T02:41:05.755915: step 3455, loss 0.50209.
Train: 2018-08-05T02:41:09.256284: step 3456, loss 0.614199.
Train: 2018-08-05T02:41:12.803534: step 3457, loss 0.55368.
Train: 2018-08-05T02:41:16.303903: step 3458, loss 0.553664.
Train: 2018-08-05T02:41:19.804272: step 3459, loss 0.536236.
Train: 2018-08-05T02:41:23.320269: step 3460, loss 0.571098.
Test: 2018-08-05T02:41:38.368732: step 3460, loss 0.54656.
Train: 2018-08-05T02:41:41.869102: step 3461, loss 0.536135.
Train: 2018-08-05T02:41:45.385098: step 3462, loss 0.50102.
Train: 2018-08-05T02:41:48.916721: step 3463, loss 0.588785.
Train: 2018-08-05T02:41:52.432717: step 3464, loss 0.483101.
Train: 2018-08-05T02:41:55.964339: step 3465, loss 0.562438.
Train: 2018-08-05T02:41:59.480336: step 3466, loss 0.535856.
Train: 2018-08-05T02:42:03.027585: step 3467, loss 0.518014.
Train: 2018-08-05T02:42:06.527955: step 3468, loss 0.58928.
Train: 2018-08-05T02:42:10.043950: step 3469, loss 0.517818.
Train: 2018-08-05T02:42:13.544320: step 3470, loss 0.517725.
Test: 2018-08-05T02:42:28.717797: step 3470, loss 0.546122.
Train: 2018-08-05T02:42:32.218167: step 3471, loss 0.553598.
Train: 2018-08-05T02:42:35.906056: step 3472, loss 0.517532.
Train: 2018-08-05T02:42:37.624987: step 3473, loss 0.620551.
Train: 2018-08-05T02:42:41.140984: step 3474, loss 0.607964.
Train: 2018-08-05T02:42:44.641353: step 3475, loss 0.580793.
Train: 2018-08-05T02:42:48.141722: step 3476, loss 0.571722.
Train: 2018-08-05T02:42:51.657719: step 3477, loss 0.490312.
Train: 2018-08-05T02:42:55.189341: step 3478, loss 0.580751.
Train: 2018-08-05T02:42:58.705338: step 3479, loss 0.472238.
Train: 2018-08-05T02:43:02.252587: step 3480, loss 0.562669.
Test: 2018-08-05T02:43:17.301051: step 3480, loss 0.54901.
Train: 2018-08-05T02:43:20.863926: step 3481, loss 0.517387.
Train: 2018-08-05T02:43:24.395549: step 3482, loss 0.499207.
Train: 2018-08-05T02:43:27.895919: step 3483, loss 0.535458.
Train: 2018-08-05T02:43:31.427541: step 3484, loss 0.489893.
Train: 2018-08-05T02:43:34.974791: step 3485, loss 0.498856.
Train: 2018-08-05T02:43:38.553294: step 3486, loss 0.562852.
Train: 2018-08-05T02:43:42.084917: step 3487, loss 0.53532.
Train: 2018-08-05T02:43:45.616539: step 3488, loss 0.516855.
Train: 2018-08-05T02:43:49.179415: step 3489, loss 0.646211.
Train: 2018-08-05T02:43:52.726665: step 3490, loss 0.590772.
Test: 2018-08-05T02:44:07.696996: step 3490, loss 0.548412.
Train: 2018-08-05T02:44:11.244244: step 3491, loss 0.535258.
Train: 2018-08-05T02:44:14.807120: step 3492, loss 0.58151.
Train: 2018-08-05T02:44:18.401250: step 3493, loss 0.544509.
Train: 2018-08-05T02:44:21.948499: step 3494, loss 0.562983.
Train: 2018-08-05T02:44:25.433242: step 3495, loss 0.58142.
Train: 2018-08-05T02:44:28.949238: step 3496, loss 0.553724.
Train: 2018-08-05T02:44:32.543368: step 3497, loss 0.581305.
Train: 2018-08-05T02:44:36.090617: step 3498, loss 0.507803.
Train: 2018-08-05T02:44:39.669120: step 3499, loss 0.526185.
Train: 2018-08-05T02:44:43.185116: step 3500, loss 0.4804.
Test: 2018-08-05T02:44:58.171073: step 3500, loss 0.547121.
Train: 2018-08-05T02:45:03.843547: step 3501, loss 0.627004.
Train: 2018-08-05T02:45:07.343916: step 3502, loss 0.581143.
Train: 2018-08-05T02:45:10.891166: step 3503, loss 0.544526.
Train: 2018-08-05T02:45:14.438415: step 3504, loss 0.617551.
Train: 2018-08-05T02:45:17.907531: step 3505, loss 0.508125.
Train: 2018-08-05T02:45:21.407901: step 3506, loss 0.617256.
Train: 2018-08-05T02:45:24.892644: step 3507, loss 0.562686.
Train: 2018-08-05T02:45:28.377386: step 3508, loss 0.490348.
Train: 2018-08-05T02:45:31.924636: step 3509, loss 0.553607.
Train: 2018-08-05T02:45:35.440632: step 3510, loss 0.499527.
Test: 2018-08-05T02:45:50.489096: step 3510, loss 0.547358.
Train: 2018-08-05T02:45:54.036345: step 3511, loss 0.535582.
Train: 2018-08-05T02:45:57.536714: step 3512, loss 0.625691.
Train: 2018-08-05T02:46:01.005831: step 3513, loss 0.589591.
Train: 2018-08-05T02:46:04.553080: step 3514, loss 0.670328.
Train: 2018-08-05T02:46:08.100329: step 3515, loss 0.553589.
Train: 2018-08-05T02:46:11.616326: step 3516, loss 0.58031.
Train: 2018-08-05T02:46:15.116695: step 3517, loss 0.544719.
Train: 2018-08-05T02:46:18.632691: step 3518, loss 0.553596.
Train: 2018-08-05T02:46:22.148687: step 3519, loss 0.615294.
Train: 2018-08-05T02:46:25.649057: step 3520, loss 0.553615.
Test: 2018-08-05T02:46:40.666266: step 3520, loss 0.54938.
Train: 2018-08-05T02:46:44.213517: step 3521, loss 0.571121.
Train: 2018-08-05T02:46:47.760766: step 3522, loss 0.579791.
Train: 2018-08-05T02:46:51.276762: step 3523, loss 0.52762.
Train: 2018-08-05T02:46:54.808385: step 3524, loss 0.553685.
Train: 2018-08-05T02:46:58.386888: step 3525, loss 0.501864.
Train: 2018-08-05T02:47:01.918511: step 3526, loss 0.510547.
Train: 2018-08-05T02:47:05.465760: step 3527, loss 0.510532.
Train: 2018-08-05T02:47:08.981756: step 3528, loss 0.501822.
Train: 2018-08-05T02:47:12.482126: step 3529, loss 0.545013.
Train: 2018-08-05T02:47:15.982495: step 3530, loss 0.553664.
Test: 2018-08-05T02:47:30.984079: step 3530, loss 0.546214.
Train: 2018-08-05T02:47:34.500075: step 3531, loss 0.536237.
Train: 2018-08-05T02:47:38.016071: step 3532, loss 0.501265.
Train: 2018-08-05T02:47:41.563320: step 3533, loss 0.5799.
Train: 2018-08-05T02:47:45.141823: step 3534, loss 0.579958.
Train: 2018-08-05T02:47:48.673446: step 3535, loss 0.571202.
Train: 2018-08-05T02:47:52.173815: step 3536, loss 0.606456.
Train: 2018-08-05T02:47:55.674185: step 3537, loss 0.597653.
Train: 2018-08-05T02:47:59.174554: step 3538, loss 0.544803.
Train: 2018-08-05T02:48:02.721804: step 3539, loss 0.571205.
Train: 2018-08-05T02:48:06.269053: step 3540, loss 0.536025.
Test: 2018-08-05T02:48:21.317518: step 3540, loss 0.547316.
Train: 2018-08-05T02:48:24.849140: step 3541, loss 0.597561.
Train: 2018-08-05T02:48:28.333882: step 3542, loss 0.544834.
Train: 2018-08-05T02:48:31.865505: step 3543, loss 0.544843.
Train: 2018-08-05T02:48:35.365874: step 3544, loss 0.465924.
Train: 2018-08-05T02:48:38.881871: step 3545, loss 0.62387.
Train: 2018-08-05T02:48:42.444747: step 3546, loss 0.59752.
Train: 2018-08-05T02:48:45.991996: step 3547, loss 0.571163.
Train: 2018-08-05T02:48:49.586126: step 3548, loss 0.588672.
Train: 2018-08-05T02:48:53.086495: step 3549, loss 0.571124.
Train: 2018-08-05T02:48:56.602491: step 3550, loss 0.501249.
Test: 2018-08-05T02:49:11.666582: step 3550, loss 0.546587.
Train: 2018-08-05T02:49:15.182578: step 3551, loss 0.544913.
Train: 2018-08-05T02:49:18.698574: step 3552, loss 0.605992.
Train: 2018-08-05T02:49:22.230197: step 3553, loss 0.579791.
Train: 2018-08-05T02:49:25.730566: step 3554, loss 0.536252.
Train: 2018-08-05T02:49:29.277816: step 3555, loss 0.544967.
Train: 2018-08-05T02:49:32.809438: step 3556, loss 0.492845.
Train: 2018-08-05T02:49:36.294182: step 3557, loss 0.605834.
Train: 2018-08-05T02:49:39.841431: step 3558, loss 0.55366.
Train: 2018-08-05T02:49:43.388680: step 3559, loss 0.536274.
Train: 2018-08-05T02:49:46.873423: step 3560, loss 0.623229.
Test: 2018-08-05T02:50:01.937515: step 3560, loss 0.545843.
Train: 2018-08-05T02:50:05.437883: step 3561, loss 0.553664.
Train: 2018-08-05T02:50:08.953879: step 3562, loss 0.544989.
Train: 2018-08-05T02:50:12.485501: step 3563, loss 0.597052.
Train: 2018-08-05T02:50:16.032751: step 3564, loss 0.501688.
Train: 2018-08-05T02:50:19.564374: step 3565, loss 0.605677.
Train: 2018-08-05T02:50:23.064743: step 3566, loss 0.588319.
Train: 2018-08-05T02:50:26.580739: step 3567, loss 0.657448.
Train: 2018-08-05T02:50:30.081109: step 3568, loss 0.596805.
Train: 2018-08-05T02:50:33.581479: step 3569, loss 0.553751.
Train: 2018-08-05T02:50:37.113101: step 3570, loss 0.588004.
Test: 2018-08-05T02:50:52.114684: step 3570, loss 0.546924.
Train: 2018-08-05T02:50:55.661934: step 3571, loss 0.613493.
Train: 2018-08-05T02:50:59.193557: step 3572, loss 0.494438.
Train: 2018-08-05T02:51:02.709553: step 3573, loss 0.511542.
Train: 2018-08-05T02:51:06.225549: step 3574, loss 0.511595.
Train: 2018-08-05T02:51:09.725919: step 3575, loss 0.613135.
Train: 2018-08-05T02:51:13.273168: step 3576, loss 0.587728.
Train: 2018-08-05T02:51:16.804791: step 3577, loss 0.579255.
Train: 2018-08-05T02:51:20.320787: step 3578, loss 0.587669.
Train: 2018-08-05T02:51:23.836783: step 3579, loss 0.579213.
Train: 2018-08-05T02:51:27.352779: step 3580, loss 0.553984.
Test: 2018-08-05T02:51:42.369989: step 3580, loss 0.547996.
Train: 2018-08-05T02:51:45.870359: step 3581, loss 0.520447.
Train: 2018-08-05T02:51:49.417608: step 3582, loss 0.537234.
Train: 2018-08-05T02:51:52.933605: step 3583, loss 0.545613.
Train: 2018-08-05T02:51:56.480854: step 3584, loss 0.646356.
Train: 2018-08-05T02:51:59.996850: step 3585, loss 0.587557.
Train: 2018-08-05T02:52:03.512846: step 3586, loss 0.587528.
Train: 2018-08-05T02:52:07.013216: step 3587, loss 0.554051.
Train: 2018-08-05T02:52:10.560465: step 3588, loss 0.52902.
Train: 2018-08-05T02:52:14.076462: step 3589, loss 0.570768.
Train: 2018-08-05T02:52:17.576831: step 3590, loss 0.587455.
Test: 2018-08-05T02:52:32.640922: step 3590, loss 0.547928.
Train: 2018-08-05T02:52:36.156917: step 3591, loss 0.545753.
Train: 2018-08-05T02:52:39.672914: step 3592, loss 0.587437.
Train: 2018-08-05T02:52:43.204536: step 3593, loss 0.562433.
Train: 2018-08-05T02:52:46.736159: step 3594, loss 0.654032.
Train: 2018-08-05T02:52:50.267781: step 3595, loss 0.57076.
Train: 2018-08-05T02:52:53.768151: step 3596, loss 0.595624.
Train: 2018-08-05T02:52:57.284147: step 3597, loss 0.579024.
Train: 2018-08-05T02:53:00.800143: step 3598, loss 0.603741.
Train: 2018-08-05T02:53:04.284886: step 3599, loss 0.496775.
Train: 2018-08-05T02:53:07.894642: step 3600, loss 0.578972.
Test: 2018-08-05T02:53:22.880601: step 3600, loss 0.549427.
Train: 2018-08-05T02:53:28.678086: step 3601, loss 0.562559.
Train: 2018-08-05T02:53:32.194083: step 3602, loss 0.562567.
Train: 2018-08-05T02:53:35.725704: step 3603, loss 0.587146.
Train: 2018-08-05T02:53:39.257328: step 3604, loss 0.554398.
Train: 2018-08-05T02:53:42.757697: step 3605, loss 0.546223.
Train: 2018-08-05T02:53:46.273694: step 3606, loss 0.562582.
Train: 2018-08-05T02:53:49.805316: step 3607, loss 0.562578.
Train: 2018-08-05T02:53:53.305686: step 3608, loss 0.546187.
Train: 2018-08-05T02:53:56.821682: step 3609, loss 0.56256.
Train: 2018-08-05T02:54:00.306425: step 3610, loss 0.505063.
Test: 2018-08-05T02:54:15.354889: step 3610, loss 0.548582.
Train: 2018-08-05T02:54:18.902138: step 3611, loss 0.554289.
Train: 2018-08-05T02:54:22.418134: step 3612, loss 0.579013.
Train: 2018-08-05T02:54:25.934129: step 3613, loss 0.579033.
Train: 2018-08-05T02:54:29.497006: step 3614, loss 0.579049.
Train: 2018-08-05T02:54:32.997376: step 3615, loss 0.487732.
Train: 2018-08-05T02:54:36.575879: step 3616, loss 0.537449.
Train: 2018-08-05T02:54:40.060621: step 3617, loss 0.637634.
Train: 2018-08-05T02:54:43.576617: step 3618, loss 0.587516.
Train: 2018-08-05T02:54:47.123867: step 3619, loss 0.554021.
Train: 2018-08-05T02:54:50.655489: step 3620, loss 0.570781.
Test: 2018-08-05T02:55:05.750833: step 3620, loss 0.548369.
Train: 2018-08-05T02:55:09.266829: step 3621, loss 0.637943.
Train: 2018-08-05T02:55:12.798452: step 3622, loss 0.512066.
Train: 2018-08-05T02:55:16.298821: step 3623, loss 0.654703.
Train: 2018-08-05T02:55:18.002126: step 3624, loss 0.473025.
Train: 2018-08-05T02:55:21.518122: step 3625, loss 0.503702.
Train: 2018-08-05T02:55:25.049745: step 3626, loss 0.579189.
Train: 2018-08-05T02:55:28.581368: step 3627, loss 0.604453.
Train: 2018-08-05T02:55:32.112991: step 3628, loss 0.579215.
Train: 2018-08-05T02:55:35.613360: step 3629, loss 0.579217.
Train: 2018-08-05T02:55:39.129357: step 3630, loss 0.570795.
Test: 2018-08-05T02:55:54.271580: step 3630, loss 0.549096.
Train: 2018-08-05T02:55:57.865710: step 3631, loss 0.579211.
Train: 2018-08-05T02:56:01.397332: step 3632, loss 0.553968.
Train: 2018-08-05T02:56:04.913328: step 3633, loss 0.604432.
Train: 2018-08-05T02:56:08.444951: step 3634, loss 0.562386.
Train: 2018-08-05T02:56:11.929694: step 3635, loss 0.56239.
Train: 2018-08-05T02:56:15.430063: step 3636, loss 0.495293.
Train: 2018-08-05T02:56:18.961686: step 3637, loss 0.570785.
Train: 2018-08-05T02:56:22.477682: step 3638, loss 0.545581.
Train: 2018-08-05T02:56:25.962425: step 3639, loss 0.545557.
Train: 2018-08-05T02:56:29.462795: step 3640, loss 0.604494.
Test: 2018-08-05T02:56:44.511259: step 3640, loss 0.548297.
Train: 2018-08-05T02:56:48.027254: step 3641, loss 0.579227.
Train: 2018-08-05T02:56:51.527624: step 3642, loss 0.537086.
Train: 2018-08-05T02:56:55.090500: step 3643, loss 0.520197.
Train: 2018-08-05T02:56:58.606496: step 3644, loss 0.511673.
Train: 2018-08-05T02:57:02.091239: step 3645, loss 0.562354.
Train: 2018-08-05T02:57:05.607235: step 3646, loss 0.587822.
Train: 2018-08-05T02:57:09.123231: step 3647, loss 0.570849.
Train: 2018-08-05T02:57:12.639227: step 3648, loss 0.519759.
Train: 2018-08-05T02:57:16.186476: step 3649, loss 0.570873.
Train: 2018-08-05T02:57:19.718099: step 3650, loss 0.502491.
Test: 2018-08-05T02:57:34.704056: step 3650, loss 0.547424.
Train: 2018-08-05T02:57:38.220053: step 3651, loss 0.579482.
Train: 2018-08-05T02:57:41.751675: step 3652, loss 0.476413.
Train: 2018-08-05T02:57:45.252045: step 3653, loss 0.501966.
Train: 2018-08-05T02:57:48.768041: step 3654, loss 0.614331.
Train: 2018-08-05T02:57:52.299664: step 3655, loss 0.614493.
Train: 2018-08-05T02:57:55.815661: step 3656, loss 0.588465.
Train: 2018-08-05T02:57:59.331656: step 3657, loss 0.536233.
Train: 2018-08-05T02:58:02.832026: step 3658, loss 0.597235.
Train: 2018-08-05T02:58:06.348022: step 3659, loss 0.571082.
Train: 2018-08-05T02:58:09.910898: step 3660, loss 0.579798.
Test: 2018-08-05T02:58:24.943734: step 3660, loss 0.548613.
Train: 2018-08-05T02:58:28.522237: step 3661, loss 0.579785.
Train: 2018-08-05T02:58:32.053860: step 3662, loss 0.579763.
Train: 2018-08-05T02:58:35.538603: step 3663, loss 0.423296.
Train: 2018-08-05T02:58:39.038973: step 3664, loss 0.605882.
Train: 2018-08-05T02:58:42.539342: step 3665, loss 0.510107.
Train: 2018-08-05T02:58:46.070965: step 3666, loss 0.553643.
Train: 2018-08-05T02:58:49.696347: step 3667, loss 0.52744.
Train: 2018-08-05T02:58:53.196717: step 3668, loss 0.579873.
Train: 2018-08-05T02:58:56.697086: step 3669, loss 0.553624.
Train: 2018-08-05T02:59:00.197456: step 3670, loss 0.623759.
Test: 2018-08-05T02:59:15.183412: step 3670, loss 0.548152.
Train: 2018-08-05T02:59:18.730663: step 3671, loss 0.623731.
Train: 2018-08-05T02:59:22.262285: step 3672, loss 0.614866.
Train: 2018-08-05T02:59:25.778281: step 3673, loss 0.571089.
Train: 2018-08-05T02:59:29.309903: step 3674, loss 0.571055.
Train: 2018-08-05T02:59:32.857153: step 3675, loss 0.605721.
Train: 2018-08-05T02:59:36.388776: step 3676, loss 0.570984.
Train: 2018-08-05T02:59:39.889146: step 3677, loss 0.536494.
Train: 2018-08-05T02:59:43.436395: step 3678, loss 0.510784.
Train: 2018-08-05T02:59:46.952392: step 3679, loss 0.648145.
Train: 2018-08-05T02:59:50.499641: step 3680, loss 0.570891.
Test: 2018-08-05T03:00:05.532479: step 3680, loss 0.547504.
Train: 2018-08-05T03:00:09.048474: step 3681, loss 0.511158.
Train: 2018-08-05T03:00:12.548843: step 3682, loss 0.596411.
Train: 2018-08-05T03:00:16.017960: step 3683, loss 0.587844.
Train: 2018-08-05T03:00:19.533955: step 3684, loss 0.630182.
Train: 2018-08-05T03:00:23.034325: step 3685, loss 0.528572.
Train: 2018-08-05T03:00:26.597201: step 3686, loss 0.545519.
Train: 2018-08-05T03:00:30.113197: step 3687, loss 0.604438.
Train: 2018-08-05T03:00:33.629193: step 3688, loss 0.503659.
Train: 2018-08-05T03:00:37.129563: step 3689, loss 0.57078.
Train: 2018-08-05T03:00:40.661185: step 3690, loss 0.570777.
Test: 2018-08-05T03:00:55.725276: step 3690, loss 0.548044.
Train: 2018-08-05T03:00:59.272526: step 3691, loss 0.554037.
Train: 2018-08-05T03:01:02.819780: step 3692, loss 0.554044.
Train: 2018-08-05T03:01:06.351397: step 3693, loss 0.503866.
Train: 2018-08-05T03:01:09.851767: step 3694, loss 0.604273.
Train: 2018-08-05T03:01:13.352136: step 3695, loss 0.554024.
Train: 2018-08-05T03:01:16.883759: step 3696, loss 0.512108.
Train: 2018-08-05T03:01:20.446635: step 3697, loss 0.604369.
Train: 2018-08-05T03:01:24.009511: step 3698, loss 0.486768.
Train: 2018-08-05T03:01:27.556761: step 3699, loss 0.545531.
Train: 2018-08-05T03:01:31.041504: step 3700, loss 0.562365.
Test: 2018-08-05T03:01:46.105594: step 3700, loss 0.547246.
Train: 2018-08-05T03:01:51.809321: step 3701, loss 0.570821.
Train: 2018-08-05T03:01:55.309690: step 3702, loss 0.570832.
Train: 2018-08-05T03:01:58.825687: step 3703, loss 0.621813.
Train: 2018-08-05T03:02:02.341683: step 3704, loss 0.545352.
Train: 2018-08-05T03:02:05.873305: step 3705, loss 0.545341.
Train: 2018-08-05T03:02:09.373675: step 3706, loss 0.570853.
Train: 2018-08-05T03:02:12.874044: step 3707, loss 0.587887.
Train: 2018-08-05T03:02:16.358787: step 3708, loss 0.579373.
Train: 2018-08-05T03:02:19.874783: step 3709, loss 0.562343.
Train: 2018-08-05T03:02:23.422033: step 3710, loss 0.630424.
Test: 2018-08-05T03:02:38.454869: step 3710, loss 0.549528.
Train: 2018-08-05T03:02:42.002119: step 3711, loss 0.553851.
Train: 2018-08-05T03:02:45.502489: step 3712, loss 0.553866.
Train: 2018-08-05T03:02:49.034112: step 3713, loss 0.553877.
Train: 2018-08-05T03:02:52.581361: step 3714, loss 0.621642.
Train: 2018-08-05T03:02:56.112984: step 3715, loss 0.596175.
Train: 2018-08-05T03:02:59.597727: step 3716, loss 0.494903.
Train: 2018-08-05T03:03:03.144976: step 3717, loss 0.629798.
Train: 2018-08-05T03:03:06.645345: step 3718, loss 0.604438.
Train: 2018-08-05T03:03:10.130088: step 3719, loss 0.444944.
Train: 2018-08-05T03:03:13.661711: step 3720, loss 0.646312.
Test: 2018-08-05T03:03:28.694548: step 3720, loss 0.547826.
Train: 2018-08-05T03:03:32.226171: step 3721, loss 0.554019.
Train: 2018-08-05T03:03:35.867181: step 3722, loss 0.520552.
Train: 2018-08-05T03:03:39.414430: step 3723, loss 0.562404.
Train: 2018-08-05T03:03:42.946052: step 3724, loss 0.562403.
Train: 2018-08-05T03:03:46.462048: step 3725, loss 0.520528.
Train: 2018-08-05T03:03:49.962418: step 3726, loss 0.512082.
Train: 2018-08-05T03:03:53.478414: step 3727, loss 0.579194.
Train: 2018-08-05T03:03:56.994411: step 3728, loss 0.604478.
Train: 2018-08-05T03:04:00.541659: step 3729, loss 0.494956.
Train: 2018-08-05T03:04:04.073282: step 3730, loss 0.604594.
Test: 2018-08-05T03:04:19.121746: step 3730, loss 0.548044.
Train: 2018-08-05T03:04:22.622115: step 3731, loss 0.553904.
Train: 2018-08-05T03:04:26.122485: step 3732, loss 0.553889.
Train: 2018-08-05T03:04:29.622855: step 3733, loss 0.45214.
Train: 2018-08-05T03:04:33.123224: step 3734, loss 0.562343.
Train: 2018-08-05T03:04:36.686100: step 3735, loss 0.596487.
Train: 2018-08-05T03:04:40.217723: step 3736, loss 0.519554.
Train: 2018-08-05T03:04:43.749345: step 3737, loss 0.605242.
Train: 2018-08-05T03:04:47.249715: step 3738, loss 0.605313.
Train: 2018-08-05T03:04:50.781338: step 3739, loss 0.622538.
Train: 2018-08-05T03:04:54.281707: step 3740, loss 0.493589.
Test: 2018-08-05T03:05:09.330171: step 3740, loss 0.547578.
Train: 2018-08-05T03:05:12.861793: step 3741, loss 0.579534.
Train: 2018-08-05T03:05:16.377790: step 3742, loss 0.536532.
Train: 2018-08-05T03:05:19.925039: step 3743, loss 0.527907.
Train: 2018-08-05T03:05:23.409782: step 3744, loss 0.570956.
Train: 2018-08-05T03:05:26.925778: step 3745, loss 0.527829.
Train: 2018-08-05T03:05:30.441774: step 3746, loss 0.56234.
Train: 2018-08-05T03:05:34.035904: step 3747, loss 0.63156.
Train: 2018-08-05T03:05:37.614407: step 3748, loss 0.536393.
Train: 2018-08-05T03:05:41.083523: step 3749, loss 0.519086.
Train: 2018-08-05T03:05:44.599519: step 3750, loss 0.614305.
Test: 2018-08-05T03:05:59.976141: step 3750, loss 0.548083.
Train: 2018-08-05T03:06:03.507765: step 3751, loss 0.571002.
Train: 2018-08-05T03:06:06.992508: step 3752, loss 0.570997.
Train: 2018-08-05T03:06:10.555383: step 3753, loss 0.562341.
Train: 2018-08-05T03:06:14.133887: step 3754, loss 0.605552.
Train: 2018-08-05T03:06:17.634256: step 3755, loss 0.553709.
Train: 2018-08-05T03:06:21.150252: step 3756, loss 0.510631.
Train: 2018-08-05T03:06:24.650621: step 3757, loss 0.562337.
Train: 2018-08-05T03:06:28.166618: step 3758, loss 0.527874.
Train: 2018-08-05T03:06:31.682614: step 3759, loss 0.605439.
Train: 2018-08-05T03:06:35.261116: step 3760, loss 0.55372.
Test: 2018-08-05T03:06:50.403340: step 3760, loss 0.546959.
Train: 2018-08-05T03:06:53.919336: step 3761, loss 0.510652.
Train: 2018-08-05T03:06:57.404079: step 3762, loss 0.562337.
Train: 2018-08-05T03:07:00.904448: step 3763, loss 0.493322.
Train: 2018-08-05T03:07:04.467325: step 3764, loss 0.562341.
Train: 2018-08-05T03:07:08.014575: step 3765, loss 0.571004.
Train: 2018-08-05T03:07:11.608703: step 3766, loss 0.666404.
Train: 2018-08-05T03:07:15.124700: step 3767, loss 0.553682.
Train: 2018-08-05T03:07:18.625069: step 3768, loss 0.536378.
Train: 2018-08-05T03:07:22.156692: step 3769, loss 0.484471.
Train: 2018-08-05T03:07:25.657062: step 3770, loss 0.536353.
Test: 2018-08-05T03:07:40.783658: step 3770, loss 0.547056.
Train: 2018-08-05T03:07:44.330907: step 3771, loss 0.562349.
Train: 2018-08-05T03:07:47.893783: step 3772, loss 0.562353.
Train: 2018-08-05T03:07:51.409780: step 3773, loss 0.562356.
Train: 2018-08-05T03:07:54.957029: step 3774, loss 0.55365.
Train: 2018-08-05T03:07:56.675961: step 3775, loss 0.636756.
Train: 2018-08-05T03:08:00.191956: step 3776, loss 0.64076.
Train: 2018-08-05T03:08:03.723579: step 3777, loss 0.562353.
Train: 2018-08-05T03:08:07.270829: step 3778, loss 0.545006.
Train: 2018-08-05T03:08:10.786825: step 3779, loss 0.562343.
Train: 2018-08-05T03:08:14.302821: step 3780, loss 0.527778.
Test: 2018-08-05T03:08:29.351285: step 3780, loss 0.547722.
Train: 2018-08-05T03:08:32.914161: step 3781, loss 0.562339.
Train: 2018-08-05T03:08:36.492664: step 3782, loss 0.596843.
Train: 2018-08-05T03:08:40.071166: step 3783, loss 0.562336.
Train: 2018-08-05T03:08:43.602789: step 3784, loss 0.502126.
Train: 2018-08-05T03:08:47.103159: step 3785, loss 0.613943.
Train: 2018-08-05T03:08:50.603528: step 3786, loss 0.502195.
Train: 2018-08-05T03:08:54.150777: step 3787, loss 0.570928.
Train: 2018-08-05T03:08:57.635521: step 3788, loss 0.527962.
Train: 2018-08-05T03:09:01.182770: step 3789, loss 0.519339.
Train: 2018-08-05T03:09:04.698766: step 3790, loss 0.527887.
Test: 2018-08-05T03:09:19.700349: step 3790, loss 0.54733.
Train: 2018-08-05T03:09:23.200719: step 3791, loss 0.562338.
Train: 2018-08-05T03:09:26.701089: step 3792, loss 0.54505.
Train: 2018-08-05T03:09:30.217085: step 3793, loss 0.553683.
Train: 2018-08-05T03:09:33.717454: step 3794, loss 0.562349.
Train: 2018-08-05T03:09:37.217823: step 3795, loss 0.562353.
Train: 2018-08-05T03:09:40.749446: step 3796, loss 0.544951.
Train: 2018-08-05T03:09:44.327949: step 3797, loss 0.597224.
Train: 2018-08-05T03:09:47.875199: step 3798, loss 0.483885.
Train: 2018-08-05T03:09:51.375568: step 3799, loss 0.59732.
Train: 2018-08-05T03:09:54.907195: step 3800, loss 0.597356.
Test: 2018-08-05T03:10:09.924402: step 3800, loss 0.548978.
Train: 2018-08-05T03:10:15.565622: step 3801, loss 0.544885.
Train: 2018-08-05T03:10:19.097245: step 3802, loss 0.597366.
Train: 2018-08-05T03:10:22.628867: step 3803, loss 0.54489.
Train: 2018-08-05T03:10:26.160490: step 3804, loss 0.597329.
Train: 2018-08-05T03:10:29.660859: step 3805, loss 0.649661.
Train: 2018-08-05T03:10:33.145602: step 3806, loss 0.571061.
Train: 2018-08-05T03:10:36.677225: step 3807, loss 0.579705.
Train: 2018-08-05T03:10:40.255728: step 3808, loss 0.605599.
Train: 2018-08-05T03:10:43.787350: step 3809, loss 0.458911.
Train: 2018-08-05T03:10:47.303346: step 3810, loss 0.639809.
Test: 2018-08-05T03:11:02.289303: step 3810, loss 0.547801.
Train: 2018-08-05T03:11:05.836553: step 3811, loss 0.553751.
Train: 2018-08-05T03:11:09.399429: step 3812, loss 0.545208.
Train: 2018-08-05T03:11:12.884172: step 3813, loss 0.485403.
Train: 2018-08-05T03:11:16.478302: step 3814, loss 0.536691.
Train: 2018-08-05T03:11:20.009925: step 3815, loss 0.630765.
Train: 2018-08-05T03:11:23.510293: step 3816, loss 0.451246.
Train: 2018-08-05T03:11:27.073170: step 3817, loss 0.562336.
Train: 2018-08-05T03:11:30.589166: step 3818, loss 0.562335.
Train: 2018-08-05T03:11:34.089535: step 3819, loss 0.49369.
Train: 2018-08-05T03:11:37.636785: step 3820, loss 0.527926.
Test: 2018-08-05T03:11:52.653995: step 3820, loss 0.548525.
Train: 2018-08-05T03:11:56.185617: step 3821, loss 0.683129.
Train: 2018-08-05T03:11:59.717240: step 3822, loss 0.55371.
Train: 2018-08-05T03:12:03.233237: step 3823, loss 0.562338.
Train: 2018-08-05T03:12:06.796113: step 3824, loss 0.536446.
Train: 2018-08-05T03:12:10.296482: step 3825, loss 0.570975.
Train: 2018-08-05T03:12:13.828105: step 3826, loss 0.519147.
Train: 2018-08-05T03:12:17.390981: step 3827, loss 0.5191.
Train: 2018-08-05T03:12:20.875724: step 3828, loss 0.571009.
Train: 2018-08-05T03:12:24.376094: step 3829, loss 0.492935.
Train: 2018-08-05T03:12:27.876463: step 3830, loss 0.501459.
Test: 2018-08-05T03:12:42.924927: step 3830, loss 0.547788.
Train: 2018-08-05T03:12:46.456549: step 3831, loss 0.58856.
Train: 2018-08-05T03:12:49.972545: step 3832, loss 0.527369.
Train: 2018-08-05T03:12:53.535421: step 3833, loss 0.544837.
Train: 2018-08-05T03:12:57.020164: step 3834, loss 0.64163.
Train: 2018-08-05T03:13:00.551787: step 3835, loss 0.562413.
Train: 2018-08-05T03:13:04.067783: step 3836, loss 0.580044.
Train: 2018-08-05T03:13:07.661913: step 3837, loss 0.606485.
Train: 2018-08-05T03:13:11.177909: step 3838, loss 0.544801.
Train: 2018-08-05T03:13:14.725158: step 3839, loss 0.632787.
Train: 2018-08-05T03:13:18.225528: step 3840, loss 0.632616.
Test: 2018-08-05T03:13:33.336498: step 3840, loss 0.54797.
Train: 2018-08-05T03:13:36.883748: step 3841, loss 0.571122.
Train: 2018-08-05T03:13:40.415370: step 3842, loss 0.597224.
Train: 2018-08-05T03:13:43.931366: step 3843, loss 0.518947.
Train: 2018-08-05T03:13:47.556749: step 3844, loss 0.536375.
Train: 2018-08-05T03:13:51.072745: step 3845, loss 0.570978.
Train: 2018-08-05T03:13:54.619994: step 3846, loss 0.553717.
Train: 2018-08-05T03:13:58.151617: step 3847, loss 0.484889.
Train: 2018-08-05T03:14:01.761373: step 3848, loss 0.64839.
Train: 2018-08-05T03:14:05.292996: step 3849, loss 0.502208.
Train: 2018-08-05T03:14:08.808992: step 3850, loss 0.579507.
Test: 2018-08-05T03:14:23.904336: step 3850, loss 0.547414.
Train: 2018-08-05T03:14:27.435959: step 3851, loss 0.553755.
Train: 2018-08-05T03:14:30.998834: step 3852, loss 0.579485.
Train: 2018-08-05T03:14:34.530457: step 3853, loss 0.630877.
Train: 2018-08-05T03:14:38.046454: step 3854, loss 0.605079.
Train: 2018-08-05T03:14:41.562449: step 3855, loss 0.622002.
Train: 2018-08-05T03:14:45.047193: step 3856, loss 0.579327.
Train: 2018-08-05T03:14:48.641322: step 3857, loss 0.545447.
Train: 2018-08-05T03:14:52.219824: step 3858, loss 0.58766.
Train: 2018-08-05T03:14:55.720194: step 3859, loss 0.595993.
Train: 2018-08-05T03:14:59.220564: step 3860, loss 0.587517.
Test: 2018-08-05T03:15:14.362787: step 3860, loss 0.549843.
Train: 2018-08-05T03:15:17.894410: step 3861, loss 0.537408.
Train: 2018-08-05T03:15:21.410406: step 3862, loss 0.454328.
Train: 2018-08-05T03:15:24.942028: step 3863, loss 0.637303.
Train: 2018-08-05T03:15:28.458025: step 3864, loss 0.496009.
Train: 2018-08-05T03:15:31.974021: step 3865, loss 0.587377.
Train: 2018-08-05T03:15:35.490017: step 3866, loss 0.545838.
Train: 2018-08-05T03:15:39.006013: step 3867, loss 0.545828.
Train: 2018-08-05T03:15:42.568889: step 3868, loss 0.61235.
Train: 2018-08-05T03:15:46.084886: step 3869, loss 0.570761.
Train: 2018-08-05T03:15:49.647762: step 3870, loss 0.504251.
Test: 2018-08-05T03:16:04.727478: step 3870, loss 0.549112.
Train: 2018-08-05T03:16:08.259101: step 3871, loss 0.520819.
Train: 2018-08-05T03:16:11.806351: step 3872, loss 0.512373.
Train: 2018-08-05T03:16:15.322346: step 3873, loss 0.604248.
Train: 2018-08-05T03:16:18.807089: step 3874, loss 0.512084.
Train: 2018-08-05T03:16:22.323086: step 3875, loss 0.570791.
Train: 2018-08-05T03:16:25.870335: step 3876, loss 0.629829.
Train: 2018-08-05T03:16:29.386332: step 3877, loss 0.570806.
Train: 2018-08-05T03:16:32.886701: step 3878, loss 0.596147.
Train: 2018-08-05T03:16:36.402697: step 3879, loss 0.62148.
Train: 2018-08-05T03:16:39.903066: step 3880, loss 0.562369.
Test: 2018-08-05T03:16:54.982784: step 3880, loss 0.547916.
Train: 2018-08-05T03:16:58.530033: step 3881, loss 0.587648.
Train: 2018-08-05T03:17:02.046029: step 3882, loss 0.579204.
Train: 2018-08-05T03:17:05.546398: step 3883, loss 0.537193.
Train: 2018-08-05T03:17:09.062395: step 3884, loss 0.554001.
Train: 2018-08-05T03:17:12.562764: step 3885, loss 0.562394.
Train: 2018-08-05T03:17:16.078761: step 3886, loss 0.57078.
Train: 2018-08-05T03:17:19.641636: step 3887, loss 0.453455.
Train: 2018-08-05T03:17:23.157633: step 3888, loss 0.553989.
Train: 2018-08-05T03:17:26.673628: step 3889, loss 0.570794.
Train: 2018-08-05T03:17:30.173998: step 3890, loss 0.579234.
Test: 2018-08-05T03:17:45.175582: step 3890, loss 0.548069.
Train: 2018-08-05T03:17:48.707204: step 3891, loss 0.587696.
Train: 2018-08-05T03:17:52.207574: step 3892, loss 0.553913.
Train: 2018-08-05T03:17:55.739196: step 3893, loss 0.545445.
Train: 2018-08-05T03:17:59.270819: step 3894, loss 0.545421.
Train: 2018-08-05T03:18:02.818069: step 3895, loss 0.511472.
Train: 2018-08-05T03:18:06.334065: step 3896, loss 0.647356.
Train: 2018-08-05T03:18:09.850061: step 3897, loss 0.596362.
Train: 2018-08-05T03:18:13.397310: step 3898, loss 0.545342.
Train: 2018-08-05T03:18:16.913307: step 3899, loss 0.460319.
Train: 2018-08-05T03:18:20.460556: step 3900, loss 0.570862.
Test: 2018-08-05T03:18:35.477766: step 3900, loss 0.548079.
Train: 2018-08-05T03:18:41.259627: step 3901, loss 0.570876.
Train: 2018-08-05T03:18:44.775622: step 3902, loss 0.511033.
Train: 2018-08-05T03:18:48.275992: step 3903, loss 0.605193.
Train: 2018-08-05T03:18:51.776362: step 3904, loss 0.5795.
Train: 2018-08-05T03:18:55.261105: step 3905, loss 0.579513.
Train: 2018-08-05T03:18:58.792728: step 3906, loss 0.579517.
Train: 2018-08-05T03:19:02.339977: step 3907, loss 0.605285.
Train: 2018-08-05T03:19:05.871600: step 3908, loss 0.545173.
Train: 2018-08-05T03:19:09.403222: step 3909, loss 0.613788.
Train: 2018-08-05T03:19:12.903592: step 3910, loss 0.579458.
Test: 2018-08-05T03:19:28.061444: step 3910, loss 0.54846.
Train: 2018-08-05T03:19:31.577438: step 3911, loss 0.519613.
Train: 2018-08-05T03:19:35.124688: step 3912, loss 0.519648.
Train: 2018-08-05T03:19:38.640684: step 3913, loss 0.545258.
Train: 2018-08-05T03:19:42.203560: step 3914, loss 0.519615.
Train: 2018-08-05T03:19:45.703929: step 3915, loss 0.485328.
Train: 2018-08-05T03:19:49.266805: step 3916, loss 0.570916.
Train: 2018-08-05T03:19:52.782801: step 3917, loss 0.63115.
Train: 2018-08-05T03:19:56.361304: step 3918, loss 0.527906.
Train: 2018-08-05T03:19:59.892927: step 3919, loss 0.605429.
Train: 2018-08-05T03:20:03.440177: step 3920, loss 0.648538.
Test: 2018-08-05T03:20:18.519895: step 3920, loss 0.548558.
Train: 2018-08-05T03:20:22.035889: step 3921, loss 0.622575.
Train: 2018-08-05T03:20:25.567513: step 3922, loss 0.553754.
Train: 2018-08-05T03:20:29.083509: step 3923, loss 0.49385.
Train: 2018-08-05T03:20:32.662011: step 3924, loss 0.502454.
Train: 2018-08-05T03:20:36.209261: step 3925, loss 0.468176.
Train: 2018-08-05T03:20:37.928192: step 3926, loss 0.617259.
Train: 2018-08-05T03:20:41.428562: step 3927, loss 0.502196.
Train: 2018-08-05T03:20:44.944557: step 3928, loss 0.545115.
Train: 2018-08-05T03:20:48.491807: step 3929, loss 0.501925.
Train: 2018-08-05T03:20:52.023429: step 3930, loss 0.59698.
Test: 2018-08-05T03:21:07.040640: step 3930, loss 0.547656.
Train: 2018-08-05T03:21:10.603516: step 3931, loss 0.59706.
Train: 2018-08-05T03:21:14.119513: step 3932, loss 0.562352.
Train: 2018-08-05T03:21:17.651135: step 3933, loss 0.562355.
Train: 2018-08-05T03:21:21.198385: step 3934, loss 0.588465.
Train: 2018-08-05T03:21:24.730007: step 3935, loss 0.658094.
Train: 2018-08-05T03:21:28.246004: step 3936, loss 0.518921.
Train: 2018-08-05T03:21:31.793253: step 3937, loss 0.562349.
Train: 2018-08-05T03:21:35.293622: step 3938, loss 0.562346.
Train: 2018-08-05T03:21:38.809618: step 3939, loss 0.674934.
Train: 2018-08-05T03:21:42.325614: step 3940, loss 0.66593.
Test: 2018-08-05T03:21:57.374079: step 3940, loss 0.547201.
Train: 2018-08-05T03:22:00.890074: step 3941, loss 0.53657.
Train: 2018-08-05T03:22:04.421697: step 3942, loss 0.6051.
Train: 2018-08-05T03:22:07.953320: step 3943, loss 0.545316.
Train: 2018-08-05T03:22:11.500569: step 3944, loss 0.613234.
Train: 2018-08-05T03:22:15.000939: step 3945, loss 0.587692.
Train: 2018-08-05T03:22:18.579441: step 3946, loss 0.553981.
Train: 2018-08-05T03:22:22.126691: step 3947, loss 0.537292.
Train: 2018-08-05T03:22:25.673941: step 3948, loss 0.595808.
Train: 2018-08-05T03:22:29.236816: step 3949, loss 0.562442.
Train: 2018-08-05T03:22:32.737186: step 3950, loss 0.537571.
Test: 2018-08-05T03:22:47.801278: step 3950, loss 0.549978.
Train: 2018-08-05T03:22:51.332899: step 3951, loss 0.570757.
Train: 2018-08-05T03:22:54.848895: step 3952, loss 0.579024.
Train: 2018-08-05T03:22:58.396144: step 3953, loss 0.595518.
Train: 2018-08-05T03:23:01.912140: step 3954, loss 0.488395.
Train: 2018-08-05T03:23:05.475017: step 3955, loss 0.554286.
Train: 2018-08-05T03:23:09.022267: step 3956, loss 0.620187.
Train: 2018-08-05T03:23:12.538262: step 3957, loss 0.620146.
Train: 2018-08-05T03:23:16.069885: step 3958, loss 0.595406.
Train: 2018-08-05T03:23:19.554628: step 3959, loss 0.488794.
Train: 2018-08-05T03:23:23.054997: step 3960, loss 0.636327.
Test: 2018-08-05T03:23:38.119090: step 3960, loss 0.550049.
Train: 2018-08-05T03:23:41.728844: step 3961, loss 0.538038.
Train: 2018-08-05T03:23:45.291720: step 3962, loss 0.570767.
Train: 2018-08-05T03:23:48.807716: step 3963, loss 0.676997.
Train: 2018-08-05T03:23:52.308086: step 3964, loss 0.570777.
Train: 2018-08-05T03:23:55.808455: step 3965, loss 0.505781.
Train: 2018-08-05T03:23:59.340078: step 3966, loss 0.457123.
Train: 2018-08-05T03:24:02.902954: step 3967, loss 0.595186.
Train: 2018-08-05T03:24:06.450203: step 3968, loss 0.546341.
Train: 2018-08-05T03:24:09.997452: step 3969, loss 0.497335.
Train: 2018-08-05T03:24:13.497822: step 3970, loss 0.497085.
Test: 2018-08-05T03:24:28.577539: step 3970, loss 0.54804.
Train: 2018-08-05T03:24:32.093535: step 3971, loss 0.570758.
Train: 2018-08-05T03:24:35.625158: step 3972, loss 0.554237.
Train: 2018-08-05T03:24:39.188034: step 3973, loss 0.537585.
Train: 2018-08-05T03:24:42.735283: step 3974, loss 0.579093.
Train: 2018-08-05T03:24:46.220026: step 3975, loss 0.545691.
Train: 2018-08-05T03:24:49.736023: step 3976, loss 0.587566.
Train: 2018-08-05T03:24:53.236392: step 3977, loss 0.511883.
Train: 2018-08-05T03:24:56.752388: step 3978, loss 0.53702.
Train: 2018-08-05T03:25:00.315264: step 3979, loss 0.536907.
Train: 2018-08-05T03:25:03.862513: step 3980, loss 0.519759.
Test: 2018-08-05T03:25:18.910977: step 3980, loss 0.546077.
Train: 2018-08-05T03:25:22.442600: step 3981, loss 0.511.
Train: 2018-08-05T03:25:25.942969: step 3982, loss 0.588138.
Train: 2018-08-05T03:25:29.443339: step 3983, loss 0.527795.
Train: 2018-08-05T03:25:32.974961: step 3984, loss 0.588368.
Train: 2018-08-05T03:25:36.506584: step 3985, loss 0.544953.
Train: 2018-08-05T03:25:40.038207: step 3986, loss 0.527448.
Train: 2018-08-05T03:25:43.554203: step 3987, loss 0.553623.
Train: 2018-08-05T03:25:47.038946: step 3988, loss 0.527246.
Train: 2018-08-05T03:25:50.586195: step 3989, loss 0.571238.
Train: 2018-08-05T03:25:54.149072: step 3990, loss 0.580123.
Test: 2018-08-05T03:26:09.166282: step 3990, loss 0.545616.
Train: 2018-08-05T03:26:12.729158: step 3991, loss 0.597892.
Train: 2018-08-05T03:26:16.229528: step 3992, loss 0.553591.
Train: 2018-08-05T03:26:19.745524: step 3993, loss 0.535841.
Train: 2018-08-05T03:26:23.292773: step 3994, loss 0.535821.
Train: 2018-08-05T03:26:26.824396: step 3995, loss 0.571379.
Train: 2018-08-05T03:26:30.340392: step 3996, loss 0.615906.
Train: 2018-08-05T03:26:33.887642: step 3997, loss 0.615874.
Train: 2018-08-05T03:26:37.450517: step 3998, loss 0.491411.
Train: 2018-08-05T03:26:40.966513: step 3999, loss 0.571349.
Train: 2018-08-05T03:26:44.466883: step 4000, loss 0.562464.
Test: 2018-08-05T03:26:59.546602: step 4000, loss 0.547445.
Train: 2018-08-05T03:27:05.265954: step 4001, loss 0.509256.
Train: 2018-08-05T03:27:08.797576: step 4002, loss 0.482642.
Train: 2018-08-05T03:27:12.313573: step 4003, loss 0.464765.
Train: 2018-08-05T03:27:15.860822: step 4004, loss 0.580319.
Train: 2018-08-05T03:27:19.361191: step 4005, loss 0.473216.
Train: 2018-08-05T03:27:22.877188: step 4006, loss 0.508782.
Train: 2018-08-05T03:27:26.377558: step 4007, loss 0.580592.
Train: 2018-08-05T03:27:29.877927: step 4008, loss 0.553607.
Train: 2018-08-05T03:27:33.378296: step 4009, loss 0.653146.
Train: 2018-08-05T03:27:36.909919: step 4010, loss 0.589823.
Test: 2018-08-05T03:27:51.927129: step 4010, loss 0.54797.
Train: 2018-08-05T03:27:55.536885: step 4011, loss 0.535521.
Train: 2018-08-05T03:27:59.052881: step 4012, loss 0.526477.
Train: 2018-08-05T03:28:02.584504: step 4013, loss 0.526471.
Train: 2018-08-05T03:28:06.084874: step 4014, loss 0.580782.
Train: 2018-08-05T03:28:09.632123: step 4015, loss 0.580782.
Train: 2018-08-05T03:28:13.195000: step 4016, loss 0.535519.
Train: 2018-08-05T03:28:16.773502: step 4017, loss 0.535523.
Train: 2018-08-05T03:28:20.242618: step 4018, loss 0.571706.
Train: 2018-08-05T03:28:23.742988: step 4019, loss 0.517448.
Train: 2018-08-05T03:28:27.274610: step 4020, loss 0.5717.
Test: 2018-08-05T03:28:42.323075: step 4020, loss 0.547555.
Train: 2018-08-05T03:28:45.917203: step 4021, loss 0.580734.
Train: 2018-08-05T03:28:49.526960: step 4022, loss 0.589741.
Train: 2018-08-05T03:28:53.074208: step 4023, loss 0.51753.
Train: 2018-08-05T03:28:56.637085: step 4024, loss 0.58965.
Train: 2018-08-05T03:29:00.137454: step 4025, loss 0.526604.
Train: 2018-08-05T03:29:03.669078: step 4026, loss 0.61653.
Train: 2018-08-05T03:29:07.185073: step 4027, loss 0.53565.
Train: 2018-08-05T03:29:10.747949: step 4028, loss 0.535676.
Train: 2018-08-05T03:29:14.263946: step 4029, loss 0.643067.
Train: 2018-08-05T03:29:17.858075: step 4030, loss 0.589279.
Test: 2018-08-05T03:29:32.890911: step 4030, loss 0.548038.
Train: 2018-08-05T03:29:36.406908: step 4031, loss 0.562482.
Train: 2018-08-05T03:29:39.922904: step 4032, loss 0.526993.
Train: 2018-08-05T03:29:43.454527: step 4033, loss 0.509365.
Train: 2018-08-05T03:29:47.017403: step 4034, loss 0.562432.
Train: 2018-08-05T03:29:50.564652: step 4035, loss 0.5536.
Train: 2018-08-05T03:29:54.111902: step 4036, loss 0.571232.
Train: 2018-08-05T03:29:57.596645: step 4037, loss 0.527195.
Train: 2018-08-05T03:30:01.128267: step 4038, loss 0.571204.
Train: 2018-08-05T03:30:04.659890: step 4039, loss 0.615144.
Train: 2018-08-05T03:30:08.222766: step 4040, loss 0.571163.
Test: 2018-08-05T03:30:23.271231: step 4040, loss 0.547154.
Train: 2018-08-05T03:30:26.787227: step 4041, loss 0.553625.
Train: 2018-08-05T03:30:30.303222: step 4042, loss 0.553633.
Train: 2018-08-05T03:30:33.819219: step 4043, loss 0.614715.
Train: 2018-08-05T03:30:37.366467: step 4044, loss 0.518846.
Train: 2018-08-05T03:30:40.866837: step 4045, loss 0.536286.
Train: 2018-08-05T03:30:44.445340: step 4046, loss 0.623114.
Train: 2018-08-05T03:30:47.945709: step 4047, loss 0.571006.
Train: 2018-08-05T03:30:51.492958: step 4048, loss 0.579626.
Train: 2018-08-05T03:30:54.993328: step 4049, loss 0.52785.
Train: 2018-08-05T03:30:58.540578: step 4050, loss 0.639815.
Test: 2018-08-05T03:31:13.573414: step 4050, loss 0.548001.
Train: 2018-08-05T03:31:17.105038: step 4051, loss 0.519421.
Train: 2018-08-05T03:31:20.636660: step 4052, loss 0.510933.
Train: 2018-08-05T03:31:24.183910: step 4053, loss 0.553774.
Train: 2018-08-05T03:31:27.684279: step 4054, loss 0.630806.
Train: 2018-08-05T03:31:31.184648: step 4055, loss 0.553794.
Train: 2018-08-05T03:31:34.669391: step 4056, loss 0.57087.
Train: 2018-08-05T03:31:38.201014: step 4057, loss 0.553823.
Train: 2018-08-05T03:31:41.732637: step 4058, loss 0.528308.
Train: 2018-08-05T03:31:45.311140: step 4059, loss 0.655916.
Train: 2018-08-05T03:31:48.827136: step 4060, loss 0.528399.
Test: 2018-08-05T03:32:03.859972: step 4060, loss 0.547415.
Train: 2018-08-05T03:32:07.391595: step 4061, loss 0.553876.
Train: 2018-08-05T03:32:10.907592: step 4062, loss 0.647045.
Train: 2018-08-05T03:32:14.454841: step 4063, loss 0.604598.
Train: 2018-08-05T03:32:17.986464: step 4064, loss 0.545537.
Train: 2018-08-05T03:32:21.564966: step 4065, loss 0.570786.
Train: 2018-08-05T03:32:25.096589: step 4066, loss 0.595915.
Train: 2018-08-05T03:32:28.596959: step 4067, loss 0.545703.
Train: 2018-08-05T03:32:32.081702: step 4068, loss 0.654155.
Train: 2018-08-05T03:32:35.597698: step 4069, loss 0.579067.
Train: 2018-08-05T03:32:39.113694: step 4070, loss 0.612144.
Test: 2018-08-05T03:32:54.177784: step 4070, loss 0.54837.
Train: 2018-08-05T03:32:57.693780: step 4071, loss 0.578999.
Train: 2018-08-05T03:33:01.256656: step 4072, loss 0.455835.
Train: 2018-08-05T03:33:04.772653: step 4073, loss 0.529758.
Train: 2018-08-05T03:33:08.288649: step 4074, loss 0.56256.
Train: 2018-08-05T03:33:11.851525: step 4075, loss 0.521542.
Train: 2018-08-05T03:33:15.367521: step 4076, loss 0.472187.
Train: 2018-08-05T03:33:17.102079: step 4077, loss 0.668024.
Train: 2018-08-05T03:33:20.633702: step 4078, loss 0.562506.
Train: 2018-08-05T03:33:24.118445: step 4079, loss 0.579015.
Train: 2018-08-05T03:33:27.618814: step 4080, loss 0.554229.
Test: 2018-08-05T03:33:42.682904: step 4080, loss 0.550194.
Train: 2018-08-05T03:33:46.245780: step 4081, loss 0.554213.
Train: 2018-08-05T03:33:49.761776: step 4082, loss 0.58732.
Train: 2018-08-05T03:33:53.293400: step 4083, loss 0.53761.
Train: 2018-08-05T03:33:56.840649: step 4084, loss 0.554163.
Train: 2018-08-05T03:34:00.356645: step 4085, loss 0.62062.
Train: 2018-08-05T03:34:03.872641: step 4086, loss 0.562449.
Train: 2018-08-05T03:34:07.404264: step 4087, loss 0.520877.
Train: 2018-08-05T03:34:10.935887: step 4088, loss 0.537463.
Train: 2018-08-05T03:34:14.530016: step 4089, loss 0.537405.
Train: 2018-08-05T03:34:18.061639: step 4090, loss 0.495533.
Test: 2018-08-05T03:34:33.156983: step 4090, loss 0.547027.
Train: 2018-08-05T03:34:36.672980: step 4091, loss 0.486876.
Train: 2018-08-05T03:34:40.173348: step 4092, loss 0.545504.
Train: 2018-08-05T03:34:43.658091: step 4093, loss 0.604726.
Train: 2018-08-05T03:34:47.174087: step 4094, loss 0.55384.
Train: 2018-08-05T03:34:50.721337: step 4095, loss 0.553805.
Train: 2018-08-05T03:34:54.268586: step 4096, loss 0.613701.
Train: 2018-08-05T03:34:57.753329: step 4097, loss 0.553759.
Train: 2018-08-05T03:35:01.300578: step 4098, loss 0.65684.
Train: 2018-08-05T03:35:04.800948: step 4099, loss 0.57951.
Train: 2018-08-05T03:35:08.332570: step 4100, loss 0.622403.
Test: 2018-08-05T03:35:23.365408: step 4100, loss 0.547638.
Train: 2018-08-05T03:35:29.194148: step 4101, loss 0.485257.
Train: 2018-08-05T03:35:32.741397: step 4102, loss 0.553773.
Train: 2018-08-05T03:35:36.241767: step 4103, loss 0.60515.
Train: 2018-08-05T03:35:39.757762: step 4104, loss 0.570891.
Train: 2018-08-05T03:35:43.273759: step 4105, loss 0.536697.
Train: 2018-08-05T03:35:46.774128: step 4106, loss 0.54525.
Train: 2018-08-05T03:35:50.305751: step 4107, loss 0.528163.
Train: 2018-08-05T03:35:53.821747: step 4108, loss 0.528136.
Train: 2018-08-05T03:35:57.400250: step 4109, loss 0.570897.
Train: 2018-08-05T03:36:00.916246: step 4110, loss 0.545194.
Test: 2018-08-05T03:36:15.933457: step 4110, loss 0.547805.
Train: 2018-08-05T03:36:19.480706: step 4111, loss 0.528008.
Train: 2018-08-05T03:36:22.996702: step 4112, loss 0.562335.
Train: 2018-08-05T03:36:26.512698: step 4113, loss 0.48483.
Train: 2018-08-05T03:36:30.075574: step 4114, loss 0.614167.
Train: 2018-08-05T03:36:33.560317: step 4115, loss 0.562342.
Train: 2018-08-05T03:36:37.123193: step 4116, loss 0.571008.
Train: 2018-08-05T03:36:40.639189: step 4117, loss 0.571019.
Train: 2018-08-05T03:36:44.139559: step 4118, loss 0.562349.
Train: 2018-08-05T03:36:47.671181: step 4119, loss 0.614441.
Train: 2018-08-05T03:36:51.202804: step 4120, loss 0.510293.
Test: 2018-08-05T03:37:06.188761: step 4120, loss 0.547855.
Train: 2018-08-05T03:37:09.720384: step 4121, loss 0.588386.
Train: 2018-08-05T03:37:13.220754: step 4122, loss 0.51029.
Train: 2018-08-05T03:37:16.721123: step 4123, loss 0.588398.
Train: 2018-08-05T03:37:20.205866: step 4124, loss 0.544984.
Train: 2018-08-05T03:37:23.706235: step 4125, loss 0.527608.
Train: 2018-08-05T03:37:27.269111: step 4126, loss 0.605823.
Train: 2018-08-05T03:37:30.785107: step 4127, loss 0.571046.
Train: 2018-08-05T03:37:34.285477: step 4128, loss 0.57973.
Train: 2018-08-05T03:37:37.785846: step 4129, loss 0.527622.
Train: 2018-08-05T03:37:41.286216: step 4130, loss 0.597075.
Test: 2018-08-05T03:37:56.319052: step 4130, loss 0.547662.
Train: 2018-08-05T03:37:59.835049: step 4131, loss 0.588369.
Train: 2018-08-05T03:38:03.413551: step 4132, loss 0.53636.
Train: 2018-08-05T03:38:06.945174: step 4133, loss 0.501759.
Train: 2018-08-05T03:38:10.492424: step 4134, loss 0.501731.
Train: 2018-08-05T03:38:14.039673: step 4135, loss 0.709787.
Train: 2018-08-05T03:38:17.586923: step 4136, loss 0.519051.
Train: 2018-08-05T03:38:21.087292: step 4137, loss 0.519078.
Train: 2018-08-05T03:38:24.618915: step 4138, loss 0.48445.
Train: 2018-08-05T03:38:28.166164: step 4139, loss 0.623036.
Train: 2018-08-05T03:38:31.682161: step 4140, loss 0.501643.
Test: 2018-08-05T03:38:46.746251: step 4140, loss 0.547448.
Train: 2018-08-05T03:38:50.277873: step 4141, loss 0.562351.
Train: 2018-08-05T03:38:53.840749: step 4142, loss 0.536271.
Train: 2018-08-05T03:38:57.372372: step 4143, loss 0.588481.
Train: 2018-08-05T03:39:00.903995: step 4144, loss 0.571075.
Train: 2018-08-05T03:39:04.435618: step 4145, loss 0.605947.
Train: 2018-08-05T03:39:07.935988: step 4146, loss 0.588494.
Train: 2018-08-05T03:39:11.451984: step 4147, loss 0.510151.
Train: 2018-08-05T03:39:14.983606: step 4148, loss 0.562356.
Train: 2018-08-05T03:39:18.499603: step 4149, loss 0.544959.
Train: 2018-08-05T03:39:22.078105: step 4150, loss 0.510161.
Test: 2018-08-05T03:39:37.110942: step 4150, loss 0.546011.
Train: 2018-08-05T03:39:40.642565: step 4151, loss 0.544942.
Train: 2018-08-05T03:39:44.142934: step 4152, loss 0.579801.
Train: 2018-08-05T03:39:47.705811: step 4153, loss 0.579815.
Train: 2018-08-05T03:39:51.237433: step 4154, loss 0.61472.
Train: 2018-08-05T03:39:54.753429: step 4155, loss 0.544929.
Train: 2018-08-05T03:39:58.269425: step 4156, loss 0.536229.
Train: 2018-08-05T03:40:01.769795: step 4157, loss 0.536233.
Train: 2018-08-05T03:40:05.254538: step 4158, loss 0.605915.
Train: 2018-08-05T03:40:08.754907: step 4159, loss 0.579767.
Train: 2018-08-05T03:40:12.270903: step 4160, loss 0.544964.
Test: 2018-08-05T03:40:27.350621: step 4160, loss 0.547442.
Train: 2018-08-05T03:40:30.866616: step 4161, loss 0.649238.
Train: 2018-08-05T03:40:34.413866: step 4162, loss 0.562346.
Train: 2018-08-05T03:40:37.945489: step 4163, loss 0.553695.
Train: 2018-08-05T03:40:41.477121: step 4164, loss 0.553709.
Train: 2018-08-05T03:40:44.993108: step 4165, loss 0.657097.
Train: 2018-08-05T03:40:48.509104: step 4166, loss 0.519413.
Train: 2018-08-05T03:40:52.025100: step 4167, loss 0.562335.
Train: 2018-08-05T03:40:55.572349: step 4168, loss 0.511058.
Train: 2018-08-05T03:40:59.119599: step 4169, loss 0.587957.
Train: 2018-08-05T03:41:02.666848: step 4170, loss 0.639098.
Test: 2018-08-05T03:41:17.746566: step 4170, loss 0.548923.
Train: 2018-08-05T03:41:21.293815: step 4171, loss 0.63889.
Train: 2018-08-05T03:41:24.825437: step 4172, loss 0.579295.
Train: 2018-08-05T03:41:28.341434: step 4173, loss 0.570804.
Train: 2018-08-05T03:41:31.857430: step 4174, loss 0.587596.
Train: 2018-08-05T03:41:35.404679: step 4175, loss 0.53729.
Train: 2018-08-05T03:41:38.905049: step 4176, loss 0.537379.
Train: 2018-08-05T03:41:42.421045: step 4177, loss 0.562433.
Train: 2018-08-05T03:41:45.921414: step 4178, loss 0.61235.
Train: 2018-08-05T03:41:49.468664: step 4179, loss 0.570758.
Train: 2018-08-05T03:41:53.000286: step 4180, loss 0.512806.
Test: 2018-08-05T03:42:08.001869: step 4180, loss 0.549049.
Train: 2018-08-05T03:42:11.564746: step 4181, loss 0.595573.
Train: 2018-08-05T03:42:15.111996: step 4182, loss 0.603803.
Train: 2018-08-05T03:42:18.627991: step 4183, loss 0.570757.
Train: 2018-08-05T03:42:22.143988: step 4184, loss 0.521369.
Train: 2018-08-05T03:42:25.706864: step 4185, loss 0.653034.
Train: 2018-08-05T03:42:29.207233: step 4186, loss 0.595386.
Train: 2018-08-05T03:42:32.723230: step 4187, loss 0.578952.
Train: 2018-08-05T03:42:36.286106: step 4188, loss 0.497275.
Train: 2018-08-05T03:42:39.833355: step 4189, loss 0.554449.
Train: 2018-08-05T03:42:43.333725: step 4190, loss 0.578933.
Test: 2018-08-05T03:42:58.366561: step 4190, loss 0.5488.
Train: 2018-08-05T03:43:01.882557: step 4191, loss 0.546296.
Train: 2018-08-05T03:43:05.398553: step 4192, loss 0.578933.
Train: 2018-08-05T03:43:08.977056: step 4193, loss 0.513626.
Train: 2018-08-05T03:43:12.508679: step 4194, loss 0.505359.
Train: 2018-08-05T03:43:16.024676: step 4195, loss 0.619961.
Train: 2018-08-05T03:43:19.525045: step 4196, loss 0.554336.
Train: 2018-08-05T03:43:23.041041: step 4197, loss 0.554306.
Train: 2018-08-05T03:43:26.572663: step 4198, loss 0.562515.
Train: 2018-08-05T03:43:30.104286: step 4199, loss 0.579014.
Train: 2018-08-05T03:43:33.667163: step 4200, loss 0.628646.
Test: 2018-08-05T03:43:48.825012: step 4200, loss 0.548104.
Train: 2018-08-05T03:43:54.559993: step 4201, loss 0.521137.
Train: 2018-08-05T03:43:58.029110: step 4202, loss 0.628709.
Train: 2018-08-05T03:44:01.576359: step 4203, loss 0.521101.
Train: 2018-08-05T03:44:05.123609: step 4204, loss 0.570757.
Train: 2018-08-05T03:44:08.670857: step 4205, loss 0.554182.
Train: 2018-08-05T03:44:12.249360: step 4206, loss 0.587349.
Train: 2018-08-05T03:44:15.765356: step 4207, loss 0.56246.
Train: 2018-08-05T03:44:19.328233: step 4208, loss 0.529245.
Train: 2018-08-05T03:44:22.828602: step 4209, loss 0.520878.
Train: 2018-08-05T03:44:26.313346: step 4210, loss 0.595762.
Test: 2018-08-05T03:44:41.314931: step 4210, loss 0.549064.
Train: 2018-08-05T03:44:44.909058: step 4211, loss 0.545734.
Train: 2018-08-05T03:44:48.518814: step 4212, loss 0.528977.
Train: 2018-08-05T03:44:52.128570: step 4213, loss 0.579157.
Train: 2018-08-05T03:44:55.691446: step 4214, loss 0.5456.
Train: 2018-08-05T03:44:59.223069: step 4215, loss 0.545554.
Train: 2018-08-05T03:45:02.754691: step 4216, loss 0.579235.
Train: 2018-08-05T03:45:06.255061: step 4217, loss 0.486329.
Train: 2018-08-05T03:45:09.786684: step 4218, loss 0.570829.
Train: 2018-08-05T03:45:13.287054: step 4219, loss 0.579346.
Train: 2018-08-05T03:45:16.818676: step 4220, loss 0.596418.
Test: 2018-08-05T03:45:31.835887: step 4220, loss 0.548879.
Train: 2018-08-05T03:45:35.367509: step 4221, loss 0.536749.
Train: 2018-08-05T03:45:38.899132: step 4222, loss 0.493982.
Train: 2018-08-05T03:45:42.446381: step 4223, loss 0.553765.
Train: 2018-08-05T03:45:45.978004: step 4224, loss 0.536553.
Train: 2018-08-05T03:45:49.509627: step 4225, loss 0.476136.
Train: 2018-08-05T03:45:53.041250: step 4226, loss 0.475756.
Train: 2018-08-05T03:45:56.557246: step 4227, loss 0.640734.
Train: 2018-08-05T03:45:58.276178: step 4228, loss 0.65558.
Train: 2018-08-05T03:46:01.885933: step 4229, loss 0.571124.
Train: 2018-08-05T03:46:05.401930: step 4230, loss 0.56238.
Test: 2018-08-05T03:46:20.466019: step 4230, loss 0.548965.
Train: 2018-08-05T03:46:24.013269: step 4231, loss 0.562382.
Train: 2018-08-05T03:46:27.513638: step 4232, loss 0.614948.
Train: 2018-08-05T03:46:31.076515: step 4233, loss 0.571132.
Train: 2018-08-05T03:46:34.561258: step 4234, loss 0.571118.
Train: 2018-08-05T03:46:38.092880: step 4235, loss 0.501243.
Train: 2018-08-05T03:46:41.671383: step 4236, loss 0.544905.
Train: 2018-08-05T03:46:45.203006: step 4237, loss 0.536167.
Train: 2018-08-05T03:46:48.719002: step 4238, loss 0.632291.
Train: 2018-08-05T03:46:52.234998: step 4239, loss 0.562369.
Train: 2018-08-05T03:46:55.766621: step 4240, loss 0.510027.
Test: 2018-08-05T03:47:10.783832: step 4240, loss 0.549603.
Train: 2018-08-05T03:47:14.299827: step 4241, loss 0.536194.
Train: 2018-08-05T03:47:17.831450: step 4242, loss 0.597281.
Train: 2018-08-05T03:47:21.409953: step 4243, loss 0.553641.
Train: 2018-08-05T03:47:24.957202: step 4244, loss 0.553642.
Train: 2018-08-05T03:47:28.535705: step 4245, loss 0.579808.
Train: 2018-08-05T03:47:32.020448: step 4246, loss 0.588513.
Train: 2018-08-05T03:47:35.520817: step 4247, loss 0.536238.
Train: 2018-08-05T03:47:39.068067: step 4248, loss 0.631969.
Train: 2018-08-05T03:47:42.615316: step 4249, loss 0.518932.
Train: 2018-08-05T03:47:46.146939: step 4250, loss 0.588372.
Test: 2018-08-05T03:48:01.195402: step 4250, loss 0.548478.
Train: 2018-08-05T03:48:04.695772: step 4251, loss 0.588328.
Train: 2018-08-05T03:48:08.227394: step 4252, loss 0.614204.
Train: 2018-08-05T03:48:11.759017: step 4253, loss 0.502008.
Train: 2018-08-05T03:48:15.306267: step 4254, loss 0.527912.
Train: 2018-08-05T03:48:18.837889: step 4255, loss 0.519332.
Train: 2018-08-05T03:48:22.369513: step 4256, loss 0.622559.
Train: 2018-08-05T03:48:25.869882: step 4257, loss 0.545146.
Train: 2018-08-05T03:48:29.385878: step 4258, loss 0.519387.
Train: 2018-08-05T03:48:32.933127: step 4259, loss 0.502187.
Train: 2018-08-05T03:48:36.496004: step 4260, loss 0.536519.
Test: 2018-08-05T03:48:51.513215: step 4260, loss 0.548732.
Train: 2018-08-05T03:48:55.060463: step 4261, loss 0.596825.
Train: 2018-08-05T03:48:58.560833: step 4262, loss 0.553708.
Train: 2018-08-05T03:49:02.061202: step 4263, loss 0.570978.
Train: 2018-08-05T03:49:05.561572: step 4264, loss 0.519122.
Train: 2018-08-05T03:49:09.077568: step 4265, loss 0.570999.
Train: 2018-08-05T03:49:12.656070: step 4266, loss 0.57101.
Train: 2018-08-05T03:49:16.172067: step 4267, loss 0.536335.
Train: 2018-08-05T03:49:19.688063: step 4268, loss 0.571029.
Train: 2018-08-05T03:49:23.204059: step 4269, loss 0.597094.
Train: 2018-08-05T03:49:26.720055: step 4270, loss 0.579719.
Test: 2018-08-05T03:49:41.877905: step 4270, loss 0.547055.
Train: 2018-08-05T03:49:45.393901: step 4271, loss 0.579708.
Train: 2018-08-05T03:49:48.972404: step 4272, loss 0.510322.
Train: 2018-08-05T03:49:52.535280: step 4273, loss 0.545003.
Train: 2018-08-05T03:49:56.035650: step 4274, loss 0.553673.
Train: 2018-08-05T03:49:59.551646: step 4275, loss 0.475555.
Train: 2018-08-05T03:50:03.083268: step 4276, loss 0.579751.
Train: 2018-08-05T03:50:06.630518: step 4277, loss 0.518807.
Train: 2018-08-05T03:50:10.146514: step 4278, loss 0.553638.
Train: 2018-08-05T03:50:13.678137: step 4279, loss 0.571122.
Train: 2018-08-05T03:50:17.194133: step 4280, loss 0.58866.
Test: 2018-08-05T03:50:32.258226: step 4280, loss 0.546738.
Train: 2018-08-05T03:50:35.758593: step 4281, loss 0.562386.
Train: 2018-08-05T03:50:39.258963: step 4282, loss 0.579927.
Train: 2018-08-05T03:50:42.790585: step 4283, loss 0.518543.
Train: 2018-08-05T03:50:46.337834: step 4284, loss 0.562392.
Train: 2018-08-05T03:50:49.885084: step 4285, loss 0.544834.
Train: 2018-08-05T03:50:53.401080: step 4286, loss 0.57997.
Train: 2018-08-05T03:50:56.917076: step 4287, loss 0.58876.
Train: 2018-08-05T03:51:00.433072: step 4288, loss 0.597525.
Train: 2018-08-05T03:51:03.933442: step 4289, loss 0.500996.
Train: 2018-08-05T03:51:07.465065: step 4290, loss 0.492231.
Test: 2018-08-05T03:51:22.544781: step 4290, loss 0.548135.
Train: 2018-08-05T03:51:26.107658: step 4291, loss 0.597516.
Train: 2018-08-05T03:51:29.608027: step 4292, loss 0.518484.
Train: 2018-08-05T03:51:33.124023: step 4293, loss 0.474491.
Train: 2018-08-05T03:51:36.640019: step 4294, loss 0.553603.
Train: 2018-08-05T03:51:40.187269: step 4295, loss 0.633097.
Train: 2018-08-05T03:51:43.734518: step 4296, loss 0.535922.
Train: 2018-08-05T03:51:47.250515: step 4297, loss 0.571284.
Train: 2018-08-05T03:51:50.829017: step 4298, loss 0.49166.
Train: 2018-08-05T03:51:54.360640: step 4299, loss 0.553592.
Train: 2018-08-05T03:51:57.861009: step 4300, loss 0.500344.
Test: 2018-08-05T03:52:12.925100: step 4300, loss 0.546604.
Train: 2018-08-05T03:52:18.738213: step 4301, loss 0.580273.
Train: 2018-08-05T03:52:22.238583: step 4302, loss 0.526862.
Train: 2018-08-05T03:52:25.770205: step 4303, loss 0.562514.
Train: 2018-08-05T03:52:29.301828: step 4304, loss 0.589344.
Train: 2018-08-05T03:52:32.864704: step 4305, loss 0.589366.
Train: 2018-08-05T03:52:36.396327: step 4306, loss 0.607244.
Train: 2018-08-05T03:52:39.865444: step 4307, loss 0.517864.
Train: 2018-08-05T03:52:43.412693: step 4308, loss 0.571442.
Train: 2018-08-05T03:52:46.913062: step 4309, loss 0.464393.
Train: 2018-08-05T03:52:50.429058: step 4310, loss 0.589302.
Test: 2018-08-05T03:53:05.493149: step 4310, loss 0.548016.
Train: 2018-08-05T03:53:09.024771: step 4311, loss 0.544659.
Train: 2018-08-05T03:53:12.540768: step 4312, loss 0.625046.
Train: 2018-08-05T03:53:16.088017: step 4313, loss 0.589275.
Train: 2018-08-05T03:53:19.604013: step 4314, loss 0.580305.
Train: 2018-08-05T03:53:23.151262: step 4315, loss 0.598021.
Train: 2018-08-05T03:53:26.667259: step 4316, loss 0.624481.
Train: 2018-08-05T03:53:30.167628: step 4317, loss 0.518295.
Train: 2018-08-05T03:53:33.714878: step 4318, loss 0.553607.
Train: 2018-08-05T03:53:37.230874: step 4319, loss 0.606285.
Train: 2018-08-05T03:53:40.778123: step 4320, loss 0.501133.
Test: 2018-08-05T03:53:55.810959: step 4320, loss 0.547584.
Train: 2018-08-05T03:53:59.373836: step 4321, loss 0.562369.
Train: 2018-08-05T03:54:02.921086: step 4322, loss 0.544928.
Train: 2018-08-05T03:54:06.437082: step 4323, loss 0.492714.
Train: 2018-08-05T03:54:09.999958: step 4324, loss 0.562358.
Train: 2018-08-05T03:54:13.562834: step 4325, loss 0.588478.
Train: 2018-08-05T03:54:17.063203: step 4326, loss 0.640667.
Train: 2018-08-05T03:54:20.641707: step 4327, loss 0.553668.
Train: 2018-08-05T03:54:24.173329: step 4328, loss 0.527686.
Train: 2018-08-05T03:54:27.689325: step 4329, loss 0.553688.
Train: 2018-08-05T03:54:31.189694: step 4330, loss 0.519103.
Test: 2018-08-05T03:54:46.363171: step 4330, loss 0.54551.
Train: 2018-08-05T03:54:49.894794: step 4331, loss 0.579638.
Train: 2018-08-05T03:54:53.426417: step 4332, loss 0.553696.
Train: 2018-08-05T03:54:56.958040: step 4333, loss 0.579626.
Train: 2018-08-05T03:55:00.536542: step 4334, loss 0.605528.
Train: 2018-08-05T03:55:04.052538: step 4335, loss 0.62271.
Train: 2018-08-05T03:55:07.568534: step 4336, loss 0.579538.
Train: 2018-08-05T03:55:11.084531: step 4337, loss 0.528026.
Train: 2018-08-05T03:55:14.600527: step 4338, loss 0.553774.
Train: 2018-08-05T03:55:18.163403: step 4339, loss 0.562337.
Train: 2018-08-05T03:55:21.695025: step 4340, loss 0.570875.
Test: 2018-08-05T03:55:36.727862: step 4340, loss 0.549672.
Train: 2018-08-05T03:55:40.259486: step 4341, loss 0.536764.
Train: 2018-08-05T03:55:43.759855: step 4342, loss 0.562341.
Train: 2018-08-05T03:55:47.307105: step 4343, loss 0.587884.
Train: 2018-08-05T03:55:50.807474: step 4344, loss 0.587858.
Train: 2018-08-05T03:55:54.354723: step 4345, loss 0.562348.
Train: 2018-08-05T03:55:57.886346: step 4346, loss 0.562351.
Train: 2018-08-05T03:56:01.386715: step 4347, loss 0.545414.
Train: 2018-08-05T03:56:04.949592: step 4348, loss 0.621608.
Train: 2018-08-05T03:56:08.481215: step 4349, loss 0.477874.
Train: 2018-08-05T03:56:11.981584: step 4350, loss 0.553911.
Test: 2018-08-05T03:56:27.045674: step 4350, loss 0.547267.
Train: 2018-08-05T03:56:30.608550: step 4351, loss 0.56236.
Train: 2018-08-05T03:56:34.140173: step 4352, loss 0.545442.
Train: 2018-08-05T03:56:37.687423: step 4353, loss 0.604684.
Train: 2018-08-05T03:56:41.187792: step 4354, loss 0.562357.
Train: 2018-08-05T03:56:44.750668: step 4355, loss 0.638529.
Train: 2018-08-05T03:56:48.251037: step 4356, loss 0.545464.
Train: 2018-08-05T03:56:51.767034: step 4357, loss 0.537047.
Train: 2018-08-05T03:56:55.329909: step 4358, loss 0.562368.
Train: 2018-08-05T03:56:58.845906: step 4359, loss 0.520199.
Train: 2018-08-05T03:57:02.330649: step 4360, loss 0.638328.
Test: 2018-08-05T03:57:17.441619: step 4360, loss 0.548482.
Train: 2018-08-05T03:57:20.988868: step 4361, loss 0.545507.
Train: 2018-08-05T03:57:24.520491: step 4362, loss 0.562372.
Train: 2018-08-05T03:57:28.036487: step 4363, loss 0.596076.
Train: 2018-08-05T03:57:31.568111: step 4364, loss 0.528707.
Train: 2018-08-05T03:57:35.084107: step 4365, loss 0.612879.
Train: 2018-08-05T03:57:38.646982: step 4366, loss 0.553975.
Train: 2018-08-05T03:57:42.162978: step 4367, loss 0.595991.
Train: 2018-08-05T03:57:45.663348: step 4368, loss 0.646295.
Train: 2018-08-05T03:57:49.194971: step 4369, loss 0.528947.
Train: 2018-08-05T03:57:52.726594: step 4370, loss 0.520669.
Test: 2018-08-05T03:58:07.821937: step 4370, loss 0.549062.
Train: 2018-08-05T03:58:11.353560: step 4371, loss 0.570767.
Train: 2018-08-05T03:58:14.900809: step 4372, loss 0.629143.
Train: 2018-08-05T03:58:18.432431: step 4373, loss 0.579086.
Train: 2018-08-05T03:58:21.948428: step 4374, loss 0.512613.
Train: 2018-08-05T03:58:25.480051: step 4375, loss 0.520946.
Train: 2018-08-05T03:58:29.011673: step 4376, loss 0.670444.
Train: 2018-08-05T03:58:32.558922: step 4377, loss 0.545881.
Train: 2018-08-05T03:58:36.121799: step 4378, loss 0.496208.
Train: 2018-08-05T03:58:37.825104: step 4379, loss 0.633201.
Train: 2018-08-05T03:58:41.341100: step 4380, loss 0.537629.
Test: 2018-08-05T03:58:56.420818: step 4380, loss 0.548454.
Train: 2018-08-05T03:58:59.968066: step 4381, loss 0.570757.
Train: 2018-08-05T03:59:03.530945: step 4382, loss 0.50451.
Train: 2018-08-05T03:59:07.062565: step 4383, loss 0.570758.
Train: 2018-08-05T03:59:10.609815: step 4384, loss 0.537549.
Train: 2018-08-05T03:59:14.110184: step 4385, loss 0.570761.
Train: 2018-08-05T03:59:17.626180: step 4386, loss 0.520782.
Train: 2018-08-05T03:59:21.142177: step 4387, loss 0.58747.
Train: 2018-08-05T03:59:24.720679: step 4388, loss 0.537312.
Train: 2018-08-05T03:59:28.252302: step 4389, loss 0.629468.
Train: 2018-08-05T03:59:31.783927: step 4390, loss 0.554004.
Test: 2018-08-05T03:59:46.848015: step 4390, loss 0.54798.
Train: 2018-08-05T03:59:50.364011: step 4391, loss 0.587577.
Train: 2018-08-05T03:59:53.895634: step 4392, loss 0.587582.
Train: 2018-08-05T03:59:57.380377: step 4393, loss 0.520409.
Train: 2018-08-05T04:00:00.912000: step 4394, loss 0.553982.
Train: 2018-08-05T04:00:04.459249: step 4395, loss 0.621256.
Train: 2018-08-05T04:00:07.975245: step 4396, loss 0.57079.
Train: 2018-08-05T04:00:11.506868: step 4397, loss 0.621214.
Train: 2018-08-05T04:00:15.038491: step 4398, loss 0.520438.
Train: 2018-08-05T04:00:18.538867: step 4399, loss 0.562394.
Train: 2018-08-05T04:00:22.117363: step 4400, loss 0.486935.
Test: 2018-08-05T04:00:37.134574: step 4400, loss 0.548365.
Train: 2018-08-05T04:00:42.822674: step 4401, loss 0.520403.
Train: 2018-08-05T04:00:46.323043: step 4402, loss 0.570794.
Train: 2018-08-05T04:00:49.854665: step 4403, loss 0.579237.
Train: 2018-08-05T04:00:53.355035: step 4404, loss 0.562364.
Train: 2018-08-05T04:00:56.886658: step 4405, loss 0.604648.
Train: 2018-08-05T04:01:00.402654: step 4406, loss 0.553897.
Train: 2018-08-05T04:01:03.965530: step 4407, loss 0.579287.
Train: 2018-08-05T04:01:07.450273: step 4408, loss 0.52002.
Train: 2018-08-05T04:01:10.966269: step 4409, loss 0.553876.
Train: 2018-08-05T04:01:14.466639: step 4410, loss 0.596297.
Test: 2018-08-05T04:01:29.499476: step 4410, loss 0.547778.
Train: 2018-08-05T04:01:33.031099: step 4411, loss 0.638762.
Train: 2018-08-05T04:01:36.562721: step 4412, loss 0.613229.
Train: 2018-08-05T04:01:40.109970: step 4413, loss 0.503133.
Train: 2018-08-05T04:01:43.657220: step 4414, loss 0.570815.
Train: 2018-08-05T04:01:47.141963: step 4415, loss 0.553914.
Train: 2018-08-05T04:01:50.642333: step 4416, loss 0.528584.
Train: 2018-08-05T04:01:54.173955: step 4417, loss 0.604606.
Train: 2018-08-05T04:01:57.674325: step 4418, loss 0.621475.
Train: 2018-08-05T04:02:01.237201: step 4419, loss 0.604521.
Train: 2018-08-05T04:02:04.753197: step 4420, loss 0.562381.
Test: 2018-08-05T04:02:19.832913: step 4420, loss 0.547024.
Train: 2018-08-05T04:02:23.364537: step 4421, loss 0.528822.
Train: 2018-08-05T04:02:26.911786: step 4422, loss 0.562396.
Train: 2018-08-05T04:02:30.427783: step 4423, loss 0.528898.
Train: 2018-08-05T04:02:33.943778: step 4424, loss 0.512141.
Train: 2018-08-05T04:02:37.475401: step 4425, loss 0.570781.
Train: 2018-08-05T04:02:41.022651: step 4426, loss 0.579181.
Train: 2018-08-05T04:02:44.523020: step 4427, loss 0.637999.
Train: 2018-08-05T04:02:48.007763: step 4428, loss 0.545605.
Train: 2018-08-05T04:02:51.523759: step 4429, loss 0.663062.
Train: 2018-08-05T04:02:55.071008: step 4430, loss 0.495465.
Test: 2018-08-05T04:03:10.197606: step 4430, loss 0.548061.
Train: 2018-08-05T04:03:13.729228: step 4431, loss 0.637665.
Train: 2018-08-05T04:03:17.245224: step 4432, loss 0.612482.
Train: 2018-08-05T04:03:20.745593: step 4433, loss 0.53749.
Train: 2018-08-05T04:03:24.261590: step 4434, loss 0.612264.
Train: 2018-08-05T04:03:27.777586: step 4435, loss 0.587312.
Train: 2018-08-05T04:03:31.293582: step 4436, loss 0.562503.
Train: 2018-08-05T04:03:34.903338: step 4437, loss 0.570757.
Train: 2018-08-05T04:03:38.419335: step 4438, loss 0.562546.
Train: 2018-08-05T04:03:41.982210: step 4439, loss 0.562564.
Train: 2018-08-05T04:03:45.498206: step 4440, loss 0.587138.
Test: 2018-08-05T04:04:00.671684: step 4440, loss 0.548571.
Train: 2018-08-05T04:04:04.234559: step 4441, loss 0.562596.
Train: 2018-08-05T04:04:07.734929: step 4442, loss 0.611581.
Train: 2018-08-05T04:04:11.360312: step 4443, loss 0.530053.
Train: 2018-08-05T04:04:14.938814: step 4444, loss 0.60333.
Train: 2018-08-05T04:04:18.454810: step 4445, loss 0.554535.
Train: 2018-08-05T04:04:21.970806: step 4446, loss 0.546436.
Train: 2018-08-05T04:04:25.486803: step 4447, loss 0.635717.
Train: 2018-08-05T04:04:29.034052: step 4448, loss 0.505975.
Train: 2018-08-05T04:04:32.612554: step 4449, loss 0.530278.
Train: 2018-08-05T04:04:36.128551: step 4450, loss 0.595129.
Test: 2018-08-05T04:04:51.192640: step 4450, loss 0.550424.
Train: 2018-08-05T04:04:54.724264: step 4451, loss 0.554555.
Train: 2018-08-05T04:04:58.240260: step 4452, loss 0.562661.
Train: 2018-08-05T04:05:01.771883: step 4453, loss 0.554516.
Train: 2018-08-05T04:05:05.272252: step 4454, loss 0.546344.
Train: 2018-08-05T04:05:08.819502: step 4455, loss 0.554453.
Train: 2018-08-05T04:05:12.366751: step 4456, loss 0.497175.
Train: 2018-08-05T04:05:15.945254: step 4457, loss 0.578967.
Train: 2018-08-05T04:05:19.429996: step 4458, loss 0.620149.
Train: 2018-08-05T04:05:22.945992: step 4459, loss 0.521285.
Train: 2018-08-05T04:05:26.477616: step 4460, loss 0.579023.
Test: 2018-08-05T04:05:41.619839: step 4460, loss 0.548257.
Train: 2018-08-05T04:05:45.151462: step 4461, loss 0.595608.
Train: 2018-08-05T04:05:48.667458: step 4462, loss 0.504403.
Train: 2018-08-05T04:05:52.167827: step 4463, loss 0.537497.
Train: 2018-08-05T04:05:55.683824: step 4464, loss 0.570766.
Train: 2018-08-05T04:05:59.199820: step 4465, loss 0.604226.
Train: 2018-08-05T04:06:02.715816: step 4466, loss 0.604282.
Train: 2018-08-05T04:06:06.325572: step 4467, loss 0.503732.
Train: 2018-08-05T04:06:09.919702: step 4468, loss 0.553992.
Train: 2018-08-05T04:06:13.435698: step 4469, loss 0.478246.
Train: 2018-08-05T04:06:16.982947: step 4470, loss 0.461032.
Test: 2018-08-05T04:06:32.062664: step 4470, loss 0.547775.
Train: 2018-08-05T04:06:35.672420: step 4471, loss 0.562348.
Train: 2018-08-05T04:06:39.266549: step 4472, loss 0.54527.
Train: 2018-08-05T04:06:42.798172: step 4473, loss 0.519457.
Train: 2018-08-05T04:06:46.329794: step 4474, loss 0.562337.
Train: 2018-08-05T04:06:49.845791: step 4475, loss 0.484397.
Train: 2018-08-05T04:06:53.346160: step 4476, loss 0.56236.
Train: 2018-08-05T04:06:56.862156: step 4477, loss 0.544871.
Train: 2018-08-05T04:07:00.378152: step 4478, loss 0.588791.
Train: 2018-08-05T04:07:03.909775: step 4479, loss 0.588907.
Train: 2018-08-05T04:07:07.425772: step 4480, loss 0.588991.
Test: 2018-08-05T04:07:22.583622: step 4480, loss 0.548469.
Train: 2018-08-05T04:07:26.099617: step 4481, loss 0.642223.
Train: 2018-08-05T04:07:29.615614: step 4482, loss 0.500433.
Train: 2018-08-05T04:07:33.131610: step 4483, loss 0.589057.
Train: 2018-08-05T04:07:36.663233: step 4484, loss 0.500395.
Train: 2018-08-05T04:07:40.179228: step 4485, loss 0.580215.
Train: 2018-08-05T04:07:43.710852: step 4486, loss 0.651252.
Train: 2018-08-05T04:07:47.242474: step 4487, loss 0.553592.
Train: 2018-08-05T04:07:50.727217: step 4488, loss 0.52704.
Train: 2018-08-05T04:07:54.258840: step 4489, loss 0.500528.
Train: 2018-08-05T04:07:57.774836: step 4490, loss 0.473963.
Test: 2018-08-05T04:08:12.776422: step 4490, loss 0.548467.
Train: 2018-08-05T04:08:16.386176: step 4491, loss 0.571321.
Train: 2018-08-05T04:08:19.917799: step 4492, loss 0.642363.
Train: 2018-08-05T04:08:23.449421: step 4493, loss 0.571336.
Train: 2018-08-05T04:08:26.981044: step 4494, loss 0.571322.
Train: 2018-08-05T04:08:30.528302: step 4495, loss 0.553593.
Train: 2018-08-05T04:08:34.044290: step 4496, loss 0.580136.
Train: 2018-08-05T04:08:37.544659: step 4497, loss 0.650778.
Train: 2018-08-05T04:08:41.091909: step 4498, loss 0.61525.
Train: 2018-08-05T04:08:44.623531: step 4499, loss 0.562389.
Train: 2018-08-05T04:08:48.123901: step 4500, loss 0.544897.
Test: 2018-08-05T04:09:03.219245: step 4500, loss 0.548616.
Train: 2018-08-05T04:09:08.860465: step 4501, loss 0.510105.
Train: 2018-08-05T04:09:12.345208: step 4502, loss 0.492814.
Train: 2018-08-05T04:09:15.876830: step 4503, loss 0.544974.
Train: 2018-08-05T04:09:19.424080: step 4504, loss 0.553664.
Train: 2018-08-05T04:09:22.955703: step 4505, loss 0.553663.
Train: 2018-08-05T04:09:26.471699: step 4506, loss 0.597113.
Train: 2018-08-05T04:09:29.987695: step 4507, loss 0.579719.
Train: 2018-08-05T04:09:33.503691: step 4508, loss 0.536323.
Train: 2018-08-05T04:09:37.019687: step 4509, loss 0.571017.
Train: 2018-08-05T04:09:40.551310: step 4510, loss 0.588339.
Test: 2018-08-05T04:09:55.584147: step 4510, loss 0.548687.
Train: 2018-08-05T04:09:59.100143: step 4511, loss 0.475804.
Train: 2018-08-05T04:10:02.616139: step 4512, loss 0.60564.
Train: 2018-08-05T04:10:06.147762: step 4513, loss 0.579654.
Train: 2018-08-05T04:10:09.679385: step 4514, loss 0.519097.
Train: 2018-08-05T04:10:13.195381: step 4515, loss 0.553692.
Train: 2018-08-05T04:10:16.711377: step 4516, loss 0.545038.
Train: 2018-08-05T04:10:20.227373: step 4517, loss 0.553687.
Train: 2018-08-05T04:10:23.712116: step 4518, loss 0.571006.
Train: 2018-08-05T04:10:27.212486: step 4519, loss 0.596999.
Train: 2018-08-05T04:10:30.728482: step 4520, loss 0.545026.
Test: 2018-08-05T04:10:45.792573: step 4520, loss 0.547886.
Train: 2018-08-05T04:10:49.339821: step 4521, loss 0.605628.
Train: 2018-08-05T04:10:52.887071: step 4522, loss 0.519107.
Train: 2018-08-05T04:10:56.403067: step 4523, loss 0.553696.
Train: 2018-08-05T04:10:59.981569: step 4524, loss 0.501828.
Train: 2018-08-05T04:11:03.497566: step 4525, loss 0.579652.
Train: 2018-08-05T04:11:07.029189: step 4526, loss 0.484405.
Train: 2018-08-05T04:11:10.592064: step 4527, loss 0.562349.
Train: 2018-08-05T04:11:14.108061: step 4528, loss 0.518885.
Train: 2018-08-05T04:11:17.686564: step 4529, loss 0.518786.
Train: 2018-08-05T04:11:19.405495: step 4530, loss 0.655612.
Test: 2018-08-05T04:11:34.407079: step 4530, loss 0.548171.
Train: 2018-08-05T04:11:37.954328: step 4531, loss 0.536135.
Train: 2018-08-05T04:11:41.470325: step 4532, loss 0.641183.
Train: 2018-08-05T04:11:45.001947: step 4533, loss 0.527382.
Train: 2018-08-05T04:11:48.502316: step 4534, loss 0.553629.
Train: 2018-08-05T04:11:52.049566: step 4535, loss 0.501147.
Train: 2018-08-05T04:11:55.659322: step 4536, loss 0.536111.
Train: 2018-08-05T04:11:59.206572: step 4537, loss 0.562388.
Train: 2018-08-05T04:12:02.738194: step 4538, loss 0.579951.
Train: 2018-08-05T04:12:06.238564: step 4539, loss 0.54483.
Train: 2018-08-05T04:12:09.738933: step 4540, loss 0.500876.
Test: 2018-08-05T04:12:24.896784: step 4540, loss 0.547301.
Train: 2018-08-05T04:12:28.459659: step 4541, loss 0.553606.
Train: 2018-08-05T04:12:31.975655: step 4542, loss 0.553601.
Train: 2018-08-05T04:12:35.522904: step 4543, loss 0.527105.
Train: 2018-08-05T04:12:39.023275: step 4544, loss 0.597831.
Train: 2018-08-05T04:12:42.570524: step 4545, loss 0.52703.
Train: 2018-08-05T04:12:46.086520: step 4546, loss 0.526995.
Train: 2018-08-05T04:12:49.665023: step 4547, loss 0.54471.
Train: 2018-08-05T04:12:53.227899: step 4548, loss 0.562484.
Train: 2018-08-05T04:12:56.806402: step 4549, loss 0.607025.
Train: 2018-08-05T04:13:00.353651: step 4550, loss 0.526868.
Test: 2018-08-05T04:13:15.433368: step 4550, loss 0.547822.
Train: 2018-08-05T04:13:18.949364: step 4551, loss 0.500117.
Train: 2018-08-05T04:13:22.465360: step 4552, loss 0.562514.
Train: 2018-08-05T04:13:25.996983: step 4553, loss 0.535716.
Train: 2018-08-05T04:13:29.575486: step 4554, loss 0.544642.
Train: 2018-08-05T04:13:33.185242: step 4555, loss 0.607357.
Train: 2018-08-05T04:13:36.716864: step 4556, loss 0.625288.
Train: 2018-08-05T04:13:40.279741: step 4557, loss 0.57149.
Train: 2018-08-05T04:13:43.811363: step 4558, loss 0.553589.
Train: 2018-08-05T04:13:47.389866: step 4559, loss 0.589278.
Train: 2018-08-05T04:13:50.921489: step 4560, loss 0.589204.
Test: 2018-08-05T04:14:05.969952: step 4560, loss 0.548865.
Train: 2018-08-05T04:14:09.517202: step 4561, loss 0.571352.
Train: 2018-08-05T04:14:13.064451: step 4562, loss 0.624448.
Train: 2018-08-05T04:14:16.627327: step 4563, loss 0.588891.
Train: 2018-08-05T04:14:20.205831: step 4564, loss 0.553612.
Train: 2018-08-05T04:14:23.737453: step 4565, loss 0.588639.
Train: 2018-08-05T04:14:27.269076: step 4566, loss 0.510054.
Train: 2018-08-05T04:14:30.785072: step 4567, loss 0.562354.
Train: 2018-08-05T04:14:34.301068: step 4568, loss 0.527654.
Train: 2018-08-05T04:14:37.832690: step 4569, loss 0.475749.
Train: 2018-08-05T04:14:41.333060: step 4570, loss 0.510371.
Test: 2018-08-05T04:14:56.428403: step 4570, loss 0.548062.
Train: 2018-08-05T04:14:59.975653: step 4571, loss 0.484287.
Train: 2018-08-05T04:15:03.538529: step 4572, loss 0.571052.
Train: 2018-08-05T04:15:07.117032: step 4573, loss 0.632088.
Train: 2018-08-05T04:15:10.648655: step 4574, loss 0.562363.
Train: 2018-08-05T04:15:14.149024: step 4575, loss 0.571086.
Train: 2018-08-05T04:15:17.649394: step 4576, loss 0.658298.
Train: 2018-08-05T04:15:21.181016: step 4577, loss 0.57106.
Train: 2018-08-05T04:15:24.728266: step 4578, loss 0.562351.
Train: 2018-08-05T04:15:28.259888: step 4579, loss 0.545013.
Train: 2018-08-05T04:15:31.791512: step 4580, loss 0.570995.
Test: 2018-08-05T04:15:46.933735: step 4580, loss 0.548112.
Train: 2018-08-05T04:15:50.434104: step 4581, loss 0.527786.
Train: 2018-08-05T04:15:53.950100: step 4582, loss 0.562339.
Train: 2018-08-05T04:15:57.497350: step 4583, loss 0.562338.
Train: 2018-08-05T04:16:01.060226: step 4584, loss 0.639898.
Train: 2018-08-05T04:16:04.591848: step 4585, loss 0.484958.
Train: 2018-08-05T04:16:08.092219: step 4586, loss 0.579521.
Train: 2018-08-05T04:16:11.701974: step 4587, loss 0.579506.
Train: 2018-08-05T04:16:15.217970: step 4588, loss 0.579486.
Train: 2018-08-05T04:16:18.733966: step 4589, loss 0.54521.
Train: 2018-08-05T04:16:22.234336: step 4590, loss 0.511007.
Test: 2018-08-05T04:16:37.267172: step 4590, loss 0.547258.
Train: 2018-08-05T04:16:40.814423: step 4591, loss 0.536665.
Train: 2018-08-05T04:16:44.314792: step 4592, loss 0.545209.
Train: 2018-08-05T04:16:47.830788: step 4593, loss 0.596623.
Train: 2018-08-05T04:16:51.362411: step 4594, loss 0.545188.
Train: 2018-08-05T04:16:54.894034: step 4595, loss 0.54518.
Train: 2018-08-05T04:16:58.425656: step 4596, loss 0.545166.
Train: 2018-08-05T04:17:01.941653: step 4597, loss 0.562335.
Train: 2018-08-05T04:17:05.473275: step 4598, loss 0.596741.
Train: 2018-08-05T04:17:08.973645: step 4599, loss 0.63115.
Train: 2018-08-05T04:17:12.474014: step 4600, loss 0.579514.
Test: 2018-08-05T04:17:27.522477: step 4600, loss 0.548409.
Train: 2018-08-05T04:17:33.382471: step 4601, loss 0.493737.
Train: 2018-08-05T04:17:36.945347: step 4602, loss 0.588057.
Train: 2018-08-05T04:17:40.461344: step 4603, loss 0.579472.
Train: 2018-08-05T04:17:43.992966: step 4604, loss 0.570896.
Train: 2018-08-05T04:17:47.508962: step 4605, loss 0.622193.
Train: 2018-08-05T04:17:51.009333: step 4606, loss 0.587934.
Train: 2018-08-05T04:17:54.509701: step 4607, loss 0.62191.
Train: 2018-08-05T04:17:58.041325: step 4608, loss 0.570829.
Train: 2018-08-05T04:18:01.635454: step 4609, loss 0.604613.
Train: 2018-08-05T04:18:05.135823: step 4610, loss 0.621291.
Test: 2018-08-05T04:18:20.293673: step 4610, loss 0.548415.
Train: 2018-08-05T04:18:23.794043: step 4611, loss 0.495408.
Train: 2018-08-05T04:18:27.341293: step 4612, loss 0.520653.
Train: 2018-08-05T04:18:30.841672: step 4613, loss 0.61247.
Train: 2018-08-05T04:18:34.373285: step 4614, loss 0.61237.
Train: 2018-08-05T04:18:37.967414: step 4615, loss 0.595646.
Train: 2018-08-05T04:18:41.483410: step 4616, loss 0.463276.
Train: 2018-08-05T04:18:44.983780: step 4617, loss 0.512919.
Train: 2018-08-05T04:18:48.499776: step 4618, loss 0.554219.
Train: 2018-08-05T04:18:52.000145: step 4619, loss 0.529373.
Train: 2018-08-05T04:18:55.547395: step 4620, loss 0.537591.
Test: 2018-08-05T04:19:10.595858: step 4620, loss 0.550099.
Train: 2018-08-05T04:19:14.174361: step 4621, loss 0.57076.
Train: 2018-08-05T04:19:17.690358: step 4622, loss 0.537451.
Train: 2018-08-05T04:19:21.253233: step 4623, loss 0.470578.
Train: 2018-08-05T04:19:24.769229: step 4624, loss 0.63787.
Train: 2018-08-05T04:19:28.269599: step 4625, loss 0.629632.
Train: 2018-08-05T04:19:31.816848: step 4626, loss 0.528731.
Train: 2018-08-05T04:19:35.348471: step 4627, loss 0.545524.
Train: 2018-08-05T04:19:38.864468: step 4628, loss 0.537049.
Train: 2018-08-05T04:19:42.396090: step 4629, loss 0.545443.
Train: 2018-08-05T04:19:45.927713: step 4630, loss 0.604741.
Test: 2018-08-05T04:20:00.944924: step 4630, loss 0.547977.
Train: 2018-08-05T04:20:04.476546: step 4631, loss 0.61328.
Train: 2018-08-05T04:20:08.008168: step 4632, loss 0.613282.
Train: 2018-08-05T04:20:11.539791: step 4633, loss 0.553871.
Train: 2018-08-05T04:20:15.024534: step 4634, loss 0.570827.
Train: 2018-08-05T04:20:18.571783: step 4635, loss 0.59623.
Train: 2018-08-05T04:20:22.072153: step 4636, loss 0.562361.
Train: 2018-08-05T04:20:25.619402: step 4637, loss 0.503243.
Train: 2018-08-05T04:20:29.182278: step 4638, loss 0.638412.
Train: 2018-08-05T04:20:32.713901: step 4639, loss 0.59611.
Train: 2018-08-05T04:20:36.214271: step 4640, loss 0.553957.
Test: 2018-08-05T04:20:51.293988: step 4640, loss 0.547763.
Train: 2018-08-05T04:20:54.809984: step 4641, loss 0.520351.
Train: 2018-08-05T04:20:58.341606: step 4642, loss 0.621209.
Train: 2018-08-05T04:21:01.873230: step 4643, loss 0.486876.
Train: 2018-08-05T04:21:05.357972: step 4644, loss 0.553996.
Train: 2018-08-05T04:21:08.889595: step 4645, loss 0.486797.
Train: 2018-08-05T04:21:12.421218: step 4646, loss 0.579213.
Train: 2018-08-05T04:21:15.937214: step 4647, loss 0.612968.
Train: 2018-08-05T04:21:19.484474: step 4648, loss 0.528617.
Train: 2018-08-05T04:21:23.000460: step 4649, loss 0.537018.
Train: 2018-08-05T04:21:26.516455: step 4650, loss 0.562357.
Test: 2018-08-05T04:21:41.564918: step 4650, loss 0.54722.
Train: 2018-08-05T04:21:45.174675: step 4651, loss 0.579306.
Train: 2018-08-05T04:21:48.690671: step 4652, loss 0.613268.
Train: 2018-08-05T04:21:52.237921: step 4653, loss 0.51992.
Train: 2018-08-05T04:21:55.769543: step 4654, loss 0.528374.
Train: 2018-08-05T04:21:59.285540: step 4655, loss 0.638903.
Train: 2018-08-05T04:22:02.817163: step 4656, loss 0.596363.
Train: 2018-08-05T04:22:06.380039: step 4657, loss 0.536856.
Train: 2018-08-05T04:22:09.958541: step 4658, loss 0.545358.
Train: 2018-08-05T04:22:13.443284: step 4659, loss 0.655802.
Train: 2018-08-05T04:22:16.990534: step 4660, loss 0.647153.
Test: 2018-08-05T04:22:32.054625: step 4660, loss 0.54747.
Train: 2018-08-05T04:22:35.554993: step 4661, loss 0.511657.
Train: 2018-08-05T04:22:39.086616: step 4662, loss 0.596105.
Train: 2018-08-05T04:22:42.633866: step 4663, loss 0.486659.
Train: 2018-08-05T04:22:46.165488: step 4664, loss 0.587609.
Train: 2018-08-05T04:22:49.712737: step 4665, loss 0.562386.
Train: 2018-08-05T04:22:53.181854: step 4666, loss 0.57918.
Train: 2018-08-05T04:22:56.697850: step 4667, loss 0.537235.
Train: 2018-08-05T04:23:00.229472: step 4668, loss 0.58755.
Train: 2018-08-05T04:23:03.792349: step 4669, loss 0.595913.
Train: 2018-08-05T04:23:07.339598: step 4670, loss 0.545668.
Test: 2018-08-05T04:23:22.466195: step 4670, loss 0.549793.
Train: 2018-08-05T04:23:25.982191: step 4671, loss 0.579129.
Train: 2018-08-05T04:23:29.513814: step 4672, loss 0.512307.
Train: 2018-08-05T04:23:33.029810: step 4673, loss 0.487192.
Train: 2018-08-05T04:23:36.545806: step 4674, loss 0.537262.
Train: 2018-08-05T04:23:40.155562: step 4675, loss 0.520386.
Train: 2018-08-05T04:23:43.734065: step 4676, loss 0.486516.
Train: 2018-08-05T04:23:47.250061: step 4677, loss 0.604702.
Train: 2018-08-05T04:23:50.781684: step 4678, loss 0.587836.
Train: 2018-08-05T04:23:54.328934: step 4679, loss 0.502709.
Train: 2018-08-05T04:23:57.844930: step 4680, loss 0.587988.
Test: 2018-08-05T04:24:13.034032: step 4680, loss 0.547033.
Train: 2018-08-05T04:24:14.799845: step 4681, loss 0.489202.
Train: 2018-08-05T04:24:18.378347: step 4682, loss 0.536511.
Train: 2018-08-05T04:24:21.909970: step 4683, loss 0.571003.
Train: 2018-08-05T04:24:25.441593: step 4684, loss 0.553693.
Train: 2018-08-05T04:24:28.988842: step 4685, loss 0.527607.
Train: 2018-08-05T04:24:32.504838: step 4686, loss 0.47512.
Train: 2018-08-05T04:24:36.036461: step 4687, loss 0.536085.
Train: 2018-08-05T04:24:39.583710: step 4688, loss 0.553589.
Train: 2018-08-05T04:24:43.099706: step 4689, loss 0.465072.
Train: 2018-08-05T04:24:46.615703: step 4690, loss 0.562462.
Test: 2018-08-05T04:25:01.773553: step 4690, loss 0.54759.
Train: 2018-08-05T04:25:05.273923: step 4691, loss 0.580586.
Train: 2018-08-05T04:25:08.805545: step 4692, loss 0.526519.
Train: 2018-08-05T04:25:12.321541: step 4693, loss 0.571769.
Train: 2018-08-05T04:25:15.853164: step 4694, loss 0.54451.
Train: 2018-08-05T04:25:19.353534: step 4695, loss 0.508013.
Train: 2018-08-05T04:25:22.869530: step 4696, loss 0.599119.
Train: 2018-08-05T04:25:26.369899: step 4697, loss 0.535058.
Train: 2018-08-05T04:25:29.901522: step 4698, loss 0.599612.
Train: 2018-08-05T04:25:33.433145: step 4699, loss 0.544729.
Train: 2018-08-05T04:25:36.964768: step 4700, loss 0.535717.
Test: 2018-08-05T04:25:52.028860: step 4700, loss 0.548398.
Train: 2018-08-05T04:25:57.748211: step 4701, loss 0.544571.
Train: 2018-08-05T04:26:01.264208: step 4702, loss 0.60911.
Train: 2018-08-05T04:26:04.764577: step 4703, loss 0.553603.
Train: 2018-08-05T04:26:08.264947: step 4704, loss 0.608509.
Train: 2018-08-05T04:26:11.796569: step 4705, loss 0.553516.
Train: 2018-08-05T04:26:15.328192: step 4706, loss 0.571901.
Train: 2018-08-05T04:26:18.859815: step 4707, loss 0.517294.
Train: 2018-08-05T04:26:22.375811: step 4708, loss 0.644171.
Train: 2018-08-05T04:26:25.891807: step 4709, loss 0.56265.
Train: 2018-08-05T04:26:29.392177: step 4710, loss 0.571609.
Test: 2018-08-05T04:26:44.378133: step 4710, loss 0.549026.
Train: 2018-08-05T04:26:47.925383: step 4711, loss 0.589491.
Train: 2018-08-05T04:26:51.488259: step 4712, loss 0.651955.
Train: 2018-08-05T04:26:55.066761: step 4713, loss 0.535796.
Train: 2018-08-05T04:26:58.582758: step 4714, loss 0.56245.
Train: 2018-08-05T04:27:02.083128: step 4715, loss 0.509494.
Train: 2018-08-05T04:27:05.599124: step 4716, loss 0.492032.
Train: 2018-08-05T04:27:09.099493: step 4717, loss 0.544827.
Train: 2018-08-05T04:27:12.646743: step 4718, loss 0.579948.
Train: 2018-08-05T04:27:16.193992: step 4719, loss 0.667582.
Train: 2018-08-05T04:27:19.678735: step 4720, loss 0.509953.
Test: 2018-08-05T04:27:34.680319: step 4720, loss 0.548004.
Train: 2018-08-05T04:27:38.180688: step 4721, loss 0.492621.
Train: 2018-08-05T04:27:41.759191: step 4722, loss 0.544936.
Train: 2018-08-05T04:27:45.275187: step 4723, loss 0.60591.
Train: 2018-08-05T04:27:48.822436: step 4724, loss 0.588454.
Train: 2018-08-05T04:27:52.354060: step 4725, loss 0.553666.
Train: 2018-08-05T04:27:55.838802: step 4726, loss 0.57969.
Train: 2018-08-05T04:27:59.401678: step 4727, loss 0.570999.
Train: 2018-08-05T04:28:02.933301: step 4728, loss 0.579621.
Train: 2018-08-05T04:28:06.464924: step 4729, loss 0.579582.
Train: 2018-08-05T04:28:10.074680: step 4730, loss 0.536527.
Test: 2018-08-05T04:28:25.123143: step 4730, loss 0.548188.
Train: 2018-08-05T04:28:28.654767: step 4731, loss 0.493619.
Train: 2018-08-05T04:28:32.186389: step 4732, loss 0.502207.
Train: 2018-08-05T04:28:35.718011: step 4733, loss 0.502131.
Train: 2018-08-05T04:28:39.265261: step 4734, loss 0.5882.
Train: 2018-08-05T04:28:42.781257: step 4735, loss 0.570973.
Train: 2018-08-05T04:28:46.391013: step 4736, loss 0.588273.
Train: 2018-08-05T04:28:49.938263: step 4737, loss 0.588285.
Train: 2018-08-05T04:28:53.438632: step 4738, loss 0.588279.
Train: 2018-08-05T04:28:57.001508: step 4739, loss 0.527783.
Train: 2018-08-05T04:29:00.501877: step 4740, loss 0.56234.
Test: 2018-08-05T04:29:15.534717: step 4740, loss 0.547516.
Train: 2018-08-05T04:29:19.097590: step 4741, loss 0.588253.
Train: 2018-08-05T04:29:22.644840: step 4742, loss 0.553707.
Train: 2018-08-05T04:29:26.176463: step 4743, loss 0.579593.
Train: 2018-08-05T04:29:29.676832: step 4744, loss 0.588198.
Train: 2018-08-05T04:29:33.192828: step 4745, loss 0.545119.
Train: 2018-08-05T04:29:36.693198: step 4746, loss 0.596739.
Train: 2018-08-05T04:29:40.193567: step 4747, loss 0.545161.
Train: 2018-08-05T04:29:43.756443: step 4748, loss 0.545179.
Train: 2018-08-05T04:29:47.288066: step 4749, loss 0.485183.
Train: 2018-08-05T04:29:50.850942: step 4750, loss 0.579498.
Test: 2018-08-05T04:30:05.899406: step 4750, loss 0.546809.
Train: 2018-08-05T04:30:09.399775: step 4751, loss 0.673955.
Train: 2018-08-05T04:30:12.915771: step 4752, loss 0.596615.
Train: 2018-08-05T04:30:16.478648: step 4753, loss 0.596532.
Train: 2018-08-05T04:30:20.072777: step 4754, loss 0.570864.
Train: 2018-08-05T04:30:23.620027: step 4755, loss 0.46885.
Train: 2018-08-05T04:30:27.136023: step 4756, loss 0.596328.
Train: 2018-08-05T04:30:30.652019: step 4757, loss 0.57932.
Train: 2018-08-05T04:30:34.152388: step 4758, loss 0.536933.
Train: 2018-08-05T04:30:37.699637: step 4759, loss 0.520017.
Train: 2018-08-05T04:30:41.215634: step 4760, loss 0.596237.
Test: 2018-08-05T04:30:56.248472: step 4760, loss 0.548019.
Train: 2018-08-05T04:30:59.811346: step 4761, loss 0.54542.
Train: 2018-08-05T04:31:03.342969: step 4762, loss 0.494611.
Train: 2018-08-05T04:31:06.843339: step 4763, loss 0.596276.
Train: 2018-08-05T04:31:10.359335: step 4764, loss 0.5284.
Train: 2018-08-05T04:31:13.890958: step 4765, loss 0.570845.
Train: 2018-08-05T04:31:17.391327: step 4766, loss 0.528309.
Train: 2018-08-05T04:31:20.891697: step 4767, loss 0.519724.
Train: 2018-08-05T04:31:24.407693: step 4768, loss 0.562337.
Train: 2018-08-05T04:31:27.923689: step 4769, loss 0.605153.
Train: 2018-08-05T04:31:31.424059: step 4770, loss 0.528045.
Test: 2018-08-05T04:31:46.503775: step 4770, loss 0.548192.
Train: 2018-08-05T04:31:50.051025: step 4771, loss 0.510813.
Train: 2018-08-05T04:31:53.582648: step 4772, loss 0.579554.
Train: 2018-08-05T04:31:57.129897: step 4773, loss 0.570963.
Train: 2018-08-05T04:32:00.677146: step 4774, loss 0.545063.
Train: 2018-08-05T04:32:04.193142: step 4775, loss 0.484473.
Train: 2018-08-05T04:32:07.693513: step 4776, loss 0.527635.
Train: 2018-08-05T04:32:11.193882: step 4777, loss 0.544944.
Train: 2018-08-05T04:32:14.709879: step 4778, loss 0.606051.
Train: 2018-08-05T04:32:18.288380: step 4779, loss 0.606144.
Train: 2018-08-05T04:32:21.820003: step 4780, loss 0.632457.
Test: 2018-08-05T04:32:36.837215: step 4780, loss 0.547763.
Train: 2018-08-05T04:32:40.321957: step 4781, loss 0.536124.
Train: 2018-08-05T04:32:43.853579: step 4782, loss 0.474896.
Train: 2018-08-05T04:32:47.369575: step 4783, loss 0.658739.
Train: 2018-08-05T04:32:50.869945: step 4784, loss 0.544875.
Train: 2018-08-05T04:32:54.432821: step 4785, loss 0.614858.
Train: 2018-08-05T04:32:57.995697: step 4786, loss 0.501244.
Train: 2018-08-05T04:33:01.496067: step 4787, loss 0.571096.
Train: 2018-08-05T04:33:05.043316: step 4788, loss 0.562365.
Train: 2018-08-05T04:33:08.543686: step 4789, loss 0.597233.
Train: 2018-08-05T04:33:12.075308: step 4790, loss 0.579768.
Test: 2018-08-05T04:33:27.123771: step 4790, loss 0.547839.
Train: 2018-08-05T04:33:30.671021: step 4791, loss 0.562353.
Train: 2018-08-05T04:33:34.312030: step 4792, loss 0.597052.
Train: 2018-08-05T04:33:37.890533: step 4793, loss 0.527718.
Train: 2018-08-05T04:33:41.422156: step 4794, loss 0.51912.
Train: 2018-08-05T04:33:44.985032: step 4795, loss 0.657386.
Train: 2018-08-05T04:33:48.501028: step 4796, loss 0.501999.
Train: 2018-08-05T04:33:52.063905: step 4797, loss 0.622617.
Train: 2018-08-05T04:33:55.626780: step 4798, loss 0.553742.
Train: 2018-08-05T04:33:59.142777: step 4799, loss 0.622377.
Train: 2018-08-05T04:34:02.658773: step 4800, loss 0.562336.
Test: 2018-08-05T04:34:17.769743: step 4800, loss 0.547898.
Train: 2018-08-05T04:34:23.535976: step 4801, loss 0.562339.
Train: 2018-08-05T04:34:27.036346: step 4802, loss 0.596381.
Train: 2018-08-05T04:34:30.567969: step 4803, loss 0.553864.
Train: 2018-08-05T04:34:34.099592: step 4804, loss 0.579286.
Train: 2018-08-05T04:34:37.615587: step 4805, loss 0.537032.
Train: 2018-08-05T04:34:41.115957: step 4806, loss 0.54551.
Train: 2018-08-05T04:34:44.600700: step 4807, loss 0.579219.
Train: 2018-08-05T04:34:48.116696: step 4808, loss 0.596026.
Train: 2018-08-05T04:34:51.663946: step 4809, loss 0.604371.
Train: 2018-08-05T04:34:55.226822: step 4810, loss 0.478642.
Test: 2018-08-05T04:35:10.259659: step 4810, loss 0.548225.
Train: 2018-08-05T04:35:13.775655: step 4811, loss 0.64614.
Train: 2018-08-05T04:35:17.260397: step 4812, loss 0.595842.
Train: 2018-08-05T04:35:20.807647: step 4813, loss 0.570765.
Train: 2018-08-05T04:35:24.323643: step 4814, loss 0.495901.
Train: 2018-08-05T04:35:27.855266: step 4815, loss 0.562446.
Train: 2018-08-05T04:35:31.386889: step 4816, loss 0.604009.
Train: 2018-08-05T04:35:34.887258: step 4817, loss 0.512633.
Train: 2018-08-05T04:35:38.387628: step 4818, loss 0.562453.
Train: 2018-08-05T04:35:41.887997: step 4819, loss 0.58738.
Train: 2018-08-05T04:35:45.419620: step 4820, loss 0.495973.
Test: 2018-08-05T04:36:00.421204: step 4820, loss 0.549497.
Train: 2018-08-05T04:36:03.952826: step 4821, loss 0.570762.
Train: 2018-08-05T04:36:07.500076: step 4822, loss 0.545762.
Train: 2018-08-05T04:36:11.031699: step 4823, loss 0.579117.
Train: 2018-08-05T04:36:14.578948: step 4824, loss 0.554053.
Train: 2018-08-05T04:36:18.173077: step 4825, loss 0.528918.
Train: 2018-08-05T04:36:21.689074: step 4826, loss 0.579172.
Train: 2018-08-05T04:36:25.220696: step 4827, loss 0.612804.
Train: 2018-08-05T04:36:28.752320: step 4828, loss 0.553976.
Train: 2018-08-05T04:36:32.268315: step 4829, loss 0.553967.
Train: 2018-08-05T04:36:35.784312: step 4830, loss 0.621312.
Test: 2018-08-05T04:36:50.832775: step 4830, loss 0.547549.
Train: 2018-08-05T04:36:54.380024: step 4831, loss 0.545547.
Train: 2018-08-05T04:36:56.098956: step 4832, loss 0.652145.
Train: 2018-08-05T04:36:59.630579: step 4833, loss 0.503588.
Train: 2018-08-05T04:37:03.162201: step 4834, loss 0.512006.
Train: 2018-08-05T04:37:06.693824: step 4835, loss 0.562384.
Train: 2018-08-05T04:37:10.209820: step 4836, loss 0.553968.
Train: 2018-08-05T04:37:13.725816: step 4837, loss 0.562375.
Train: 2018-08-05T04:37:17.257439: step 4838, loss 0.545512.
Train: 2018-08-05T04:37:20.773435: step 4839, loss 0.477958.
Train: 2018-08-05T04:37:24.305058: step 4840, loss 0.621625.
Test: 2018-08-05T04:37:39.353522: step 4840, loss 0.547799.
Train: 2018-08-05T04:37:42.869518: step 4841, loss 0.596271.
Train: 2018-08-05T04:37:46.369887: step 4842, loss 0.604779.
Train: 2018-08-05T04:37:49.885883: step 4843, loss 0.519932.
Train: 2018-08-05T04:37:53.448759: step 4844, loss 0.647243.
Train: 2018-08-05T04:37:56.980382: step 4845, loss 0.621708.
Train: 2018-08-05T04:38:00.512005: step 4846, loss 0.587738.
Train: 2018-08-05T04:38:04.043628: step 4847, loss 0.537053.
Train: 2018-08-05T04:38:07.559624: step 4848, loss 0.570798.
Train: 2018-08-05T04:38:11.106873: step 4849, loss 0.562381.
Train: 2018-08-05T04:38:14.622870: step 4850, loss 0.579183.
Test: 2018-08-05T04:38:29.671334: step 4850, loss 0.547622.
Train: 2018-08-05T04:38:33.187329: step 4851, loss 0.503707.
Train: 2018-08-05T04:38:36.718952: step 4852, loss 0.453407.
Train: 2018-08-05T04:38:40.250575: step 4853, loss 0.503553.
Train: 2018-08-05T04:38:43.766571: step 4854, loss 0.503329.
Train: 2018-08-05T04:38:47.266940: step 4855, loss 0.528463.
Train: 2018-08-05T04:38:50.829817: step 4856, loss 0.485721.
Train: 2018-08-05T04:38:54.345813: step 4857, loss 0.639415.
Train: 2018-08-05T04:38:57.861809: step 4858, loss 0.596719.
Train: 2018-08-05T04:39:01.424685: step 4859, loss 0.553719.
Train: 2018-08-05T04:39:04.956308: step 4860, loss 0.56234.
Test: 2018-08-05T04:39:19.989144: step 4860, loss 0.548881.
Train: 2018-08-05T04:39:23.552021: step 4861, loss 0.622946.
Train: 2018-08-05T04:39:27.052390: step 4862, loss 0.527694.
Train: 2018-08-05T04:39:30.584013: step 4863, loss 0.649078.
Train: 2018-08-05T04:39:34.100009: step 4864, loss 0.631675.
Train: 2018-08-05T04:39:37.662885: step 4865, loss 0.588283.
Train: 2018-08-05T04:39:41.163255: step 4866, loss 0.570962.
Train: 2018-08-05T04:39:44.679251: step 4867, loss 0.588144.
Train: 2018-08-05T04:39:48.195247: step 4868, loss 0.613803.
Train: 2018-08-05T04:39:51.695617: step 4869, loss 0.528152.
Train: 2018-08-05T04:39:55.227239: step 4870, loss 0.545293.
Test: 2018-08-05T04:40:10.322583: step 4870, loss 0.54892.
Train: 2018-08-05T04:40:13.838579: step 4871, loss 0.536824.
Train: 2018-08-05T04:40:17.385828: step 4872, loss 0.630315.
Train: 2018-08-05T04:40:20.901824: step 4873, loss 0.579302.
Train: 2018-08-05T04:40:24.433447: step 4874, loss 0.520101.
Train: 2018-08-05T04:40:27.980697: step 4875, loss 0.553926.
Train: 2018-08-05T04:40:31.527946: step 4876, loss 0.511782.
Train: 2018-08-05T04:40:35.075195: step 4877, loss 0.587669.
Train: 2018-08-05T04:40:38.575566: step 4878, loss 0.537078.
Train: 2018-08-05T04:40:42.154068: step 4879, loss 0.5202.
Train: 2018-08-05T04:40:45.701317: step 4880, loss 0.545475.
Test: 2018-08-05T04:41:00.749780: step 4880, loss 0.547455.
Train: 2018-08-05T04:41:04.297030: step 4881, loss 0.536985.
Train: 2018-08-05T04:41:07.828653: step 4882, loss 0.511502.
Train: 2018-08-05T04:41:11.375903: step 4883, loss 0.638845.
Train: 2018-08-05T04:41:14.876272: step 4884, loss 0.579361.
Train: 2018-08-05T04:41:18.361015: step 4885, loss 0.536802.
Train: 2018-08-05T04:41:21.892637: step 4886, loss 0.52825.
Train: 2018-08-05T04:41:25.408633: step 4887, loss 0.562338.
Train: 2018-08-05T04:41:28.955883: step 4888, loss 0.622187.
Train: 2018-08-05T04:41:32.487506: step 4889, loss 0.545235.
Train: 2018-08-05T04:41:36.034755: step 4890, loss 0.60511.
Test: 2018-08-05T04:41:51.145725: step 4890, loss 0.548844.
Train: 2018-08-05T04:41:54.692975: step 4891, loss 0.493934.
Train: 2018-08-05T04:41:58.193345: step 4892, loss 0.53666.
Train: 2018-08-05T04:42:01.740594: step 4893, loss 0.536623.
Train: 2018-08-05T04:42:05.272216: step 4894, loss 0.631023.
Train: 2018-08-05T04:42:08.803839: step 4895, loss 0.527986.
Train: 2018-08-05T04:42:12.382342: step 4896, loss 0.519364.
Train: 2018-08-05T04:42:15.929591: step 4897, loss 0.579552.
Train: 2018-08-05T04:42:19.461215: step 4898, loss 0.562337.
Train: 2018-08-05T04:42:22.992837: step 4899, loss 0.545088.
Train: 2018-08-05T04:42:26.493206: step 4900, loss 0.588241.
Test: 2018-08-05T04:42:41.588552: step 4900, loss 0.548113.
Train: 2018-08-05T04:42:47.339157: step 4901, loss 0.588253.
Train: 2018-08-05T04:42:50.855153: step 4902, loss 0.527796.
Train: 2018-08-05T04:42:54.355523: step 4903, loss 0.57098.
Train: 2018-08-05T04:42:57.871519: step 4904, loss 0.545058.
Train: 2018-08-05T04:43:01.371888: step 4905, loss 0.596922.
Train: 2018-08-05T04:43:04.872258: step 4906, loss 0.622835.
Train: 2018-08-05T04:43:08.450761: step 4907, loss 0.570966.
Train: 2018-08-05T04:43:11.966757: step 4908, loss 0.536496.
Train: 2018-08-05T04:43:15.482753: step 4909, loss 0.562336.
Train: 2018-08-05T04:43:18.998749: step 4910, loss 0.562335.
Test: 2018-08-05T04:43:34.140973: step 4910, loss 0.547004.
Train: 2018-08-05T04:43:37.656968: step 4911, loss 0.553747.
Train: 2018-08-05T04:43:41.204218: step 4912, loss 0.485094.
Train: 2018-08-05T04:43:44.735841: step 4913, loss 0.545153.
Train: 2018-08-05T04:43:48.345597: step 4914, loss 0.570937.
Train: 2018-08-05T04:43:51.892846: step 4915, loss 0.579554.
Train: 2018-08-05T04:43:55.377589: step 4916, loss 0.605399.
Train: 2018-08-05T04:43:58.893585: step 4917, loss 0.622589.
Train: 2018-08-05T04:44:02.409581: step 4918, loss 0.527968.
Train: 2018-08-05T04:44:05.972458: step 4919, loss 0.536584.
Train: 2018-08-05T04:44:09.519707: step 4920, loss 0.605238.
Test: 2018-08-05T04:44:24.677557: step 4920, loss 0.54822.
Train: 2018-08-05T04:44:28.302940: step 4921, loss 0.519485.
Train: 2018-08-05T04:44:31.834562: step 4922, loss 0.622312.
Train: 2018-08-05T04:44:35.319305: step 4923, loss 0.647889.
Train: 2018-08-05T04:44:38.819675: step 4924, loss 0.536755.
Train: 2018-08-05T04:44:42.366924: step 4925, loss 0.511292.
Train: 2018-08-05T04:44:45.929800: step 4926, loss 0.502843.
Train: 2018-08-05T04:44:49.430170: step 4927, loss 0.630376.
Train: 2018-08-05T04:44:52.946166: step 4928, loss 0.596327.
Train: 2018-08-05T04:44:56.462162: step 4929, loss 0.545389.
Train: 2018-08-05T04:44:59.978158: step 4930, loss 0.570826.
Test: 2018-08-05T04:45:15.010996: step 4930, loss 0.547446.
Train: 2018-08-05T04:45:18.558245: step 4931, loss 0.494662.
Train: 2018-08-05T04:45:22.089867: step 4932, loss 0.562356.
Train: 2018-08-05T04:45:25.621490: step 4933, loss 0.545414.
Train: 2018-08-05T04:45:29.121860: step 4934, loss 0.528444.
Train: 2018-08-05T04:45:32.637856: step 4935, loss 0.519898.
Train: 2018-08-05T04:45:36.138225: step 4936, loss 0.545324.
Train: 2018-08-05T04:45:39.654222: step 4937, loss 0.536748.
Train: 2018-08-05T04:45:43.170218: step 4938, loss 0.656428.
Train: 2018-08-05T04:45:46.701841: step 4939, loss 0.553778.
Train: 2018-08-05T04:45:50.249090: step 4940, loss 0.493834.
Test: 2018-08-05T04:46:05.391313: step 4940, loss 0.548994.
Train: 2018-08-05T04:46:08.907309: step 4941, loss 0.579494.
Train: 2018-08-05T04:46:12.438932: step 4942, loss 0.605291.
Train: 2018-08-05T04:46:15.970555: step 4943, loss 0.502178.
Train: 2018-08-05T04:46:19.564684: step 4944, loss 0.553729.
Train: 2018-08-05T04:46:23.065054: step 4945, loss 0.614055.
Train: 2018-08-05T04:46:26.596676: step 4946, loss 0.657174.
Train: 2018-08-05T04:46:30.097046: step 4947, loss 0.45046.
Train: 2018-08-05T04:46:33.644296: step 4948, loss 0.58817.
Train: 2018-08-05T04:46:37.144665: step 4949, loss 0.588171.
Train: 2018-08-05T04:46:40.691914: step 4950, loss 0.613975.
Test: 2018-08-05T04:46:55.849764: step 4950, loss 0.54838.
Train: 2018-08-05T04:46:59.381387: step 4951, loss 0.656853.
Train: 2018-08-05T04:47:02.897383: step 4952, loss 0.510959.
Train: 2018-08-05T04:47:06.413379: step 4953, loss 0.528157.
Train: 2018-08-05T04:47:09.913749: step 4954, loss 0.519662.
Train: 2018-08-05T04:47:13.429746: step 4955, loss 0.596476.
Train: 2018-08-05T04:47:16.992621: step 4956, loss 0.477069.
Train: 2018-08-05T04:47:20.524244: step 4957, loss 0.562338.
Train: 2018-08-05T04:47:24.024614: step 4958, loss 0.58797.
Train: 2018-08-05T04:47:27.556236: step 4959, loss 0.502509.
Train: 2018-08-05T04:47:31.056606: step 4960, loss 0.536656.
Test: 2018-08-05T04:47:46.151950: step 4960, loss 0.547815.
Train: 2018-08-05T04:47:49.683572: step 4961, loss 0.630945.
Train: 2018-08-05T04:47:53.262075: step 4962, loss 0.5366.
Train: 2018-08-05T04:47:56.809324: step 4963, loss 0.562335.
Train: 2018-08-05T04:48:00.325320: step 4964, loss 0.596696.
Train: 2018-08-05T04:48:03.856943: step 4965, loss 0.570924.
Train: 2018-08-05T04:48:07.357313: step 4966, loss 0.545163.
Train: 2018-08-05T04:48:10.873309: step 4967, loss 0.622436.
Train: 2018-08-05T04:48:14.389305: step 4968, loss 0.63951.
Train: 2018-08-05T04:48:17.999061: step 4969, loss 0.519581.
Train: 2018-08-05T04:48:21.499430: step 4970, loss 0.545263.
Test: 2018-08-05T04:48:36.532268: step 4970, loss 0.548294.
Train: 2018-08-05T04:48:40.048263: step 4971, loss 0.545284.
Train: 2018-08-05T04:48:43.689273: step 4972, loss 0.519727.
Train: 2018-08-05T04:48:47.220896: step 4973, loss 0.639071.
Train: 2018-08-05T04:48:50.783771: step 4974, loss 0.545313.
Train: 2018-08-05T04:48:54.299768: step 4975, loss 0.621898.
Train: 2018-08-05T04:48:57.862644: step 4976, loss 0.545366.
Train: 2018-08-05T04:49:01.347387: step 4977, loss 0.519956.
Train: 2018-08-05T04:49:04.863383: step 4978, loss 0.587782.
Train: 2018-08-05T04:49:08.379379: step 4979, loss 0.570824.
Train: 2018-08-05T04:49:11.911002: step 4980, loss 0.528507.
Test: 2018-08-05T04:49:27.037598: step 4980, loss 0.547834.
Train: 2018-08-05T04:49:30.600475: step 4981, loss 0.553895.
Train: 2018-08-05T04:49:34.147724: step 4982, loss 0.562357.
Train: 2018-08-05T04:49:35.866655: step 4983, loss 0.472046.
Train: 2018-08-05T04:49:39.382652: step 4984, loss 0.511439.
Train: 2018-08-05T04:49:42.914275: step 4985, loss 0.54532.
Train: 2018-08-05T04:49:46.477150: step 4986, loss 0.605029.
Train: 2018-08-05T04:49:50.040027: step 4987, loss 0.579445.
Train: 2018-08-05T04:49:53.587276: step 4988, loss 0.588034.
Train: 2018-08-05T04:49:57.103272: step 4989, loss 0.58805.
Train: 2018-08-05T04:50:00.666148: step 4990, loss 0.588051.
Test: 2018-08-05T04:50:15.730239: step 4990, loss 0.548421.
Train: 2018-08-05T04:50:19.230608: step 4991, loss 0.605174.
Train: 2018-08-05T04:50:22.777857: step 4992, loss 0.562336.
Train: 2018-08-05T04:50:26.325107: step 4993, loss 0.519612.
Train: 2018-08-05T04:50:29.856730: step 4994, loss 0.553794.
Train: 2018-08-05T04:50:33.372725: step 4995, loss 0.493996.
Train: 2018-08-05T04:50:36.888722: step 4996, loss 0.647884.
Train: 2018-08-05T04:50:40.373465: step 4997, loss 0.570887.
Train: 2018-08-05T04:50:43.936341: step 4998, loss 0.656324.
Train: 2018-08-05T04:50:47.467963: step 4999, loss 0.587907.
Train: 2018-08-05T04:50:50.999587: step 5000, loss 0.553848.
Test: 2018-08-05T04:51:06.048050: step 5000, loss 0.547414.
Train: 2018-08-05T04:51:11.704897: step 5001, loss 0.596262.
Train: 2018-08-05T04:51:15.220893: step 5002, loss 0.553908.
Train: 2018-08-05T04:51:18.721263: step 5003, loss 0.58767.
Train: 2018-08-05T04:51:22.268512: step 5004, loss 0.503499.
Train: 2018-08-05T04:51:25.800135: step 5005, loss 0.562385.
Train: 2018-08-05T04:51:29.316131: step 5006, loss 0.553992.
Train: 2018-08-05T04:51:32.816501: step 5007, loss 0.612745.
Train: 2018-08-05T04:51:36.332497: step 5008, loss 0.579159.
Train: 2018-08-05T04:51:39.864119: step 5009, loss 0.520572.
Train: 2018-08-05T04:51:43.364489: step 5010, loss 0.512228.
Test: 2018-08-05T04:51:58.366073: step 5010, loss 0.548425.
Train: 2018-08-05T04:52:01.913322: step 5011, loss 0.512181.
Train: 2018-08-05T04:52:05.491824: step 5012, loss 0.52046.
Train: 2018-08-05T04:52:08.992194: step 5013, loss 0.537151.
Train: 2018-08-05T04:52:12.508190: step 5014, loss 0.587676.
Train: 2018-08-05T04:52:16.024186: step 5015, loss 0.520083.
Train: 2018-08-05T04:52:19.524556: step 5016, loss 0.55387.
Train: 2018-08-05T04:52:23.087436: step 5017, loss 0.4943.
Train: 2018-08-05T04:52:26.681562: step 5018, loss 0.605039.
Train: 2018-08-05T04:52:30.181931: step 5019, loss 0.59659.
Train: 2018-08-05T04:52:33.713554: step 5020, loss 0.570914.
Test: 2018-08-05T04:52:48.793271: step 5020, loss 0.548186.
Train: 2018-08-05T04:52:52.309267: step 5021, loss 0.588106.
Train: 2018-08-05T04:52:55.840889: step 5022, loss 0.536547.
Train: 2018-08-05T04:52:59.372512: step 5023, loss 0.562336.
Train: 2018-08-05T04:53:02.919762: step 5024, loss 0.57095.
Train: 2018-08-05T04:53:06.451384: step 5025, loss 0.502004.
Train: 2018-08-05T04:53:09.967381: step 5026, loss 0.536436.
Train: 2018-08-05T04:53:13.483377: step 5027, loss 0.640212.
Train: 2018-08-05T04:53:17.015000: step 5028, loss 0.562343.
Train: 2018-08-05T04:53:20.577876: step 5029, loss 0.501758.
Train: 2018-08-05T04:53:24.109499: step 5030, loss 0.59701.
Test: 2018-08-05T04:53:39.236096: step 5030, loss 0.546671.
Train: 2018-08-05T04:53:42.783345: step 5031, loss 0.553677.
Train: 2018-08-05T04:53:46.283714: step 5032, loss 0.527655.
Train: 2018-08-05T04:53:49.799711: step 5033, loss 0.588396.
Train: 2018-08-05T04:53:53.315706: step 5034, loss 0.544981.
Train: 2018-08-05T04:53:56.831703: step 5035, loss 0.588422.
Train: 2018-08-05T04:54:00.425832: step 5036, loss 0.657928.
Train: 2018-08-05T04:54:03.926202: step 5037, loss 0.527666.
Train: 2018-08-05T04:54:07.457825: step 5038, loss 0.640275.
Train: 2018-08-05T04:54:11.020700: step 5039, loss 0.596877.
Train: 2018-08-05T04:54:14.521070: step 5040, loss 0.579548.
Test: 2018-08-05T04:54:29.632041: step 5040, loss 0.547419.
Train: 2018-08-05T04:54:33.194916: step 5041, loss 0.545181.
Train: 2018-08-05T04:54:36.773418: step 5042, loss 0.545229.
Train: 2018-08-05T04:54:40.289415: step 5043, loss 0.553803.
Train: 2018-08-05T04:54:43.774158: step 5044, loss 0.596424.
Train: 2018-08-05T04:54:47.305780: step 5045, loss 0.553844.
Train: 2018-08-05T04:54:50.821777: step 5046, loss 0.570835.
Train: 2018-08-05T04:54:54.353400: step 5047, loss 0.536944.
Train: 2018-08-05T04:54:57.900649: step 5048, loss 0.528512.
Train: 2018-08-05T04:55:01.432272: step 5049, loss 0.570818.
Train: 2018-08-05T04:55:04.948268: step 5050, loss 0.503158.
Test: 2018-08-05T04:55:20.012359: step 5050, loss 0.548996.
Train: 2018-08-05T04:55:23.528354: step 5051, loss 0.562356.
Train: 2018-08-05T04:55:27.059977: step 5052, loss 0.562353.
Train: 2018-08-05T04:55:30.622853: step 5053, loss 0.536903.
Train: 2018-08-05T04:55:34.154475: step 5054, loss 0.596325.
Train: 2018-08-05T04:55:37.686098: step 5055, loss 0.468854.
Train: 2018-08-05T04:55:41.186468: step 5056, loss 0.562341.
Train: 2018-08-05T04:55:44.702464: step 5057, loss 0.545257.
Train: 2018-08-05T04:55:48.218461: step 5058, loss 0.502411.
Train: 2018-08-05T04:55:51.734457: step 5059, loss 0.545155.
Train: 2018-08-05T04:55:55.266079: step 5060, loss 0.553718.
Test: 2018-08-05T04:56:10.345796: step 5060, loss 0.548697.
Train: 2018-08-05T04:56:13.877419: step 5061, loss 0.536402.
Train: 2018-08-05T04:56:17.393415: step 5062, loss 0.571023.
Train: 2018-08-05T04:56:20.956291: step 5063, loss 0.544958.
Train: 2018-08-05T04:56:24.456661: step 5064, loss 0.54492.
Train: 2018-08-05T04:56:27.988283: step 5065, loss 0.59736.
Train: 2018-08-05T04:56:31.551160: step 5066, loss 0.527343.
Train: 2018-08-05T04:56:35.114036: step 5067, loss 0.527282.
Train: 2018-08-05T04:56:38.630032: step 5068, loss 0.46562.
Train: 2018-08-05T04:56:42.177281: step 5069, loss 0.53593.
Train: 2018-08-05T04:56:45.708904: step 5070, loss 0.580194.
Test: 2018-08-05T04:57:00.741742: step 5070, loss 0.547424.
Train: 2018-08-05T04:57:04.304617: step 5071, loss 0.526906.
Train: 2018-08-05T04:57:07.945626: step 5072, loss 0.580356.
Train: 2018-08-05T04:57:11.492876: step 5073, loss 0.580418.
Train: 2018-08-05T04:57:14.993245: step 5074, loss 0.598373.
Train: 2018-08-05T04:57:18.509241: step 5075, loss 0.517751.
Train: 2018-08-05T04:57:22.009611: step 5076, loss 0.580501.
Train: 2018-08-05T04:57:25.556860: step 5077, loss 0.535648.
Train: 2018-08-05T04:57:29.119736: step 5078, loss 0.463814.
Train: 2018-08-05T04:57:32.698239: step 5079, loss 0.535603.
Train: 2018-08-05T04:57:36.214235: step 5080, loss 0.58066.
Test: 2018-08-05T04:57:51.325206: step 5080, loss 0.547558.
Train: 2018-08-05T04:57:54.856828: step 5081, loss 0.643922.
Train: 2018-08-05T04:57:58.372824: step 5082, loss 0.526528.
Train: 2018-08-05T04:58:01.888820: step 5083, loss 0.607766.
Train: 2018-08-05T04:58:05.404816: step 5084, loss 0.571636.
Train: 2018-08-05T04:58:09.077079: step 5085, loss 0.571607.
Train: 2018-08-05T04:58:12.639955: step 5086, loss 0.616509.
Train: 2018-08-05T04:58:16.280964: step 5087, loss 0.598402.
Train: 2018-08-05T04:58:19.796961: step 5088, loss 0.535726.
Train: 2018-08-05T04:58:23.312957: step 5089, loss 0.562494.
Train: 2018-08-05T04:58:26.860207: step 5090, loss 0.651286.
Test: 2018-08-05T04:58:41.877417: step 5090, loss 0.54706.
Train: 2018-08-05T04:58:45.409039: step 5091, loss 0.527068.
Train: 2018-08-05T04:58:49.050049: step 5092, loss 0.562415.
Train: 2018-08-05T04:58:52.597298: step 5093, loss 0.632664.
Train: 2018-08-05T04:58:56.097667: step 5094, loss 0.623584.
Train: 2018-08-05T04:58:59.598037: step 5095, loss 0.579749.
Train: 2018-08-05T04:59:03.160913: step 5096, loss 0.614246.
Train: 2018-08-05T04:59:06.708163: step 5097, loss 0.605331.
Train: 2018-08-05T04:59:10.286665: step 5098, loss 0.587972.
Train: 2018-08-05T04:59:13.802661: step 5099, loss 0.519892.
Train: 2018-08-05T04:59:17.303031: step 5100, loss 0.537009.
Test: 2018-08-05T04:59:32.335868: step 5100, loss 0.548896.
Train: 2018-08-05T04:59:38.133355: step 5101, loss 0.545536.
Train: 2018-08-05T04:59:41.711857: step 5102, loss 0.537203.
Train: 2018-08-05T04:59:45.243480: step 5103, loss 0.562399.
Train: 2018-08-05T04:59:48.790730: step 5104, loss 0.554044.
Train: 2018-08-05T04:59:52.306726: step 5105, loss 0.545708.
Train: 2018-08-05T04:59:55.838349: step 5106, loss 0.537374.
Train: 2018-08-05T04:59:59.338719: step 5107, loss 0.620863.
Train: 2018-08-05T05:00:02.901594: step 5108, loss 0.587446.
Train: 2018-08-05T05:00:06.433217: step 5109, loss 0.562435.
Train: 2018-08-05T05:00:09.996093: step 5110, loss 0.53749.
Test: 2018-08-05T05:00:25.107063: step 5110, loss 0.548177.
Train: 2018-08-05T05:00:28.638687: step 5111, loss 0.529187.
Train: 2018-08-05T05:00:32.139055: step 5112, loss 0.49589.
Train: 2018-08-05T05:00:35.701931: step 5113, loss 0.579102.
Train: 2018-08-05T05:00:39.217928: step 5114, loss 0.554068.
Train: 2018-08-05T05:00:42.749551: step 5115, loss 0.562408.
Train: 2018-08-05T05:00:46.265547: step 5116, loss 0.595914.
Train: 2018-08-05T05:00:49.812796: step 5117, loss 0.595939.
Train: 2018-08-05T05:00:53.328793: step 5118, loss 0.554007.
Train: 2018-08-05T05:00:56.844789: step 5119, loss 0.595953.
Train: 2018-08-05T05:01:00.376411: step 5120, loss 0.528844.
Test: 2018-08-05T05:01:15.456128: step 5120, loss 0.547217.
Train: 2018-08-05T05:01:18.987751: step 5121, loss 0.587567.
Train: 2018-08-05T05:01:22.519373: step 5122, loss 0.562391.
Train: 2018-08-05T05:01:26.019743: step 5123, loss 0.478459.
Train: 2018-08-05T05:01:29.551366: step 5124, loss 0.612841.
Train: 2018-08-05T05:01:33.082988: step 5125, loss 0.553961.
Train: 2018-08-05T05:01:36.614611: step 5126, loss 0.52025.
Train: 2018-08-05T05:01:40.130607: step 5127, loss 0.553926.
Train: 2018-08-05T05:01:43.646604: step 5128, loss 0.587729.
Train: 2018-08-05T05:01:47.162600: step 5129, loss 0.536956.
Train: 2018-08-05T05:01:50.678596: step 5130, loss 0.596274.
Test: 2018-08-05T05:02:05.711435: step 5130, loss 0.547197.
Train: 2018-08-05T05:02:09.305563: step 5131, loss 0.579324.
Train: 2018-08-05T05:02:12.852812: step 5132, loss 0.570839.
Train: 2018-08-05T05:02:16.384434: step 5133, loss 0.553855.
Train: 2018-08-05T05:02:18.103366: step 5134, loss 0.453605.
Train: 2018-08-05T05:02:21.634988: step 5135, loss 0.511231.
Train: 2018-08-05T05:02:25.135358: step 5136, loss 0.519594.
Train: 2018-08-05T05:02:28.682608: step 5137, loss 0.562335.
Train: 2018-08-05T05:02:32.198604: step 5138, loss 0.596794.
Train: 2018-08-05T05:02:35.761480: step 5139, loss 0.510525.
Train: 2018-08-05T05:02:39.293103: step 5140, loss 0.519026.
Test: 2018-08-05T05:02:54.404073: step 5140, loss 0.547431.
Train: 2018-08-05T05:02:57.920069: step 5141, loss 0.588443.
Train: 2018-08-05T05:03:01.467318: step 5142, loss 0.562364.
Train: 2018-08-05T05:03:05.014568: step 5143, loss 0.544892.
Train: 2018-08-05T05:03:08.561817: step 5144, loss 0.544861.
Train: 2018-08-05T05:03:12.093440: step 5145, loss 0.667778.
Train: 2018-08-05T05:03:15.625063: step 5146, loss 0.553614.
Train: 2018-08-05T05:03:19.187939: step 5147, loss 0.536055.
Train: 2018-08-05T05:03:22.766442: step 5148, loss 0.588744.
Train: 2018-08-05T05:03:26.282438: step 5149, loss 0.62385.
Train: 2018-08-05T05:03:29.814061: step 5150, loss 0.614972.
Test: 2018-08-05T05:03:45.065670: step 5150, loss 0.547776.
Train: 2018-08-05T05:03:48.659800: step 5151, loss 0.597335.
Train: 2018-08-05T05:03:52.222676: step 5152, loss 0.544937.
Train: 2018-08-05T05:03:55.769925: step 5153, loss 0.553664.
Train: 2018-08-05T05:03:59.285922: step 5154, loss 0.536342.
Train: 2018-08-05T05:04:02.848798: step 5155, loss 0.570996.
Train: 2018-08-05T05:04:06.364794: step 5156, loss 0.63145.
Train: 2018-08-05T05:04:09.974550: step 5157, loss 0.588173.
Train: 2018-08-05T05:04:13.568679: step 5158, loss 0.562335.
Train: 2018-08-05T05:04:17.162808: step 5159, loss 0.502434.
Train: 2018-08-05T05:04:20.694431: step 5160, loss 0.511068.
Test: 2018-08-05T05:04:35.758523: step 5160, loss 0.548071.
Train: 2018-08-05T05:04:39.337024: step 5161, loss 0.57088.
Train: 2018-08-05T05:04:42.868647: step 5162, loss 0.528181.
Train: 2018-08-05T05:04:46.431523: step 5163, loss 0.6563.
Train: 2018-08-05T05:04:49.947519: step 5164, loss 0.622034.
Train: 2018-08-05T05:04:53.463516: step 5165, loss 0.502817.
Train: 2018-08-05T05:04:56.979512: step 5166, loss 0.536867.
Train: 2018-08-05T05:05:00.495508: step 5167, loss 0.553859.
Train: 2018-08-05T05:05:03.995877: step 5168, loss 0.511428.
Train: 2018-08-05T05:05:07.543127: step 5169, loss 0.528369.
Train: 2018-08-05T05:05:11.106003: step 5170, loss 0.545328.
Test: 2018-08-05T05:05:26.154467: step 5170, loss 0.547518.
Train: 2018-08-05T05:05:29.717342: step 5171, loss 0.468589.
Train: 2018-08-05T05:05:33.280218: step 5172, loss 0.622211.
Train: 2018-08-05T05:05:36.796215: step 5173, loss 0.570906.
Train: 2018-08-05T05:05:40.327837: step 5174, loss 0.528.
Train: 2018-08-05T05:05:43.890714: step 5175, loss 0.579539.
Train: 2018-08-05T05:05:47.422336: step 5176, loss 0.622637.
Train: 2018-08-05T05:05:50.953959: step 5177, loss 0.536492.
Train: 2018-08-05T05:05:54.438702: step 5178, loss 0.605433.
Train: 2018-08-05T05:05:58.017205: step 5179, loss 0.570952.
Train: 2018-08-05T05:06:01.564454: step 5180, loss 0.48485.
Test: 2018-08-05T05:06:16.659798: step 5180, loss 0.547547.
Train: 2018-08-05T05:06:20.207047: step 5181, loss 0.6399.
Train: 2018-08-05T05:06:23.832430: step 5182, loss 0.553725.
Train: 2018-08-05T05:06:27.379679: step 5183, loss 0.570943.
Train: 2018-08-05T05:06:30.911302: step 5184, loss 0.596738.
Train: 2018-08-05T05:06:34.411671: step 5185, loss 0.605278.
Train: 2018-08-05T05:06:37.943295: step 5186, loss 0.553765.
Train: 2018-08-05T05:06:41.506170: step 5187, loss 0.587999.
Train: 2018-08-05T05:06:45.131553: step 5188, loss 0.570874.
Train: 2018-08-05T05:06:48.663175: step 5189, loss 0.587893.
Train: 2018-08-05T05:06:52.179172: step 5190, loss 0.536858.
Test: 2018-08-05T05:07:07.243264: step 5190, loss 0.548576.
Train: 2018-08-05T05:07:10.790511: step 5191, loss 0.536905.
Train: 2018-08-05T05:07:14.322134: step 5192, loss 0.528457.
Train: 2018-08-05T05:07:17.853757: step 5193, loss 0.545406.
Train: 2018-08-05T05:07:21.447887: step 5194, loss 0.562352.
Train: 2018-08-05T05:07:25.010762: step 5195, loss 0.469078.
Train: 2018-08-05T05:07:28.526759: step 5196, loss 0.604842.
Train: 2018-08-05T05:07:32.058381: step 5197, loss 0.570853.
Train: 2018-08-05T05:07:35.590004: step 5198, loss 0.553824.
Train: 2018-08-05T05:07:39.106000: step 5199, loss 0.56234.
Train: 2018-08-05T05:07:42.653250: step 5200, loss 0.570873.
Test: 2018-08-05T05:07:57.764223: step 5200, loss 0.548075.
Train: 2018-08-05T05:08:03.467947: step 5201, loss 0.570878.
Train: 2018-08-05T05:08:06.999570: step 5202, loss 0.570881.
Train: 2018-08-05T05:08:10.531192: step 5203, loss 0.596518.
Train: 2018-08-05T05:08:14.015936: step 5204, loss 0.528176.
Train: 2018-08-05T05:08:17.563185: step 5205, loss 0.502542.
Train: 2018-08-05T05:08:21.063555: step 5206, loss 0.562336.
Train: 2018-08-05T05:08:24.626430: step 5207, loss 0.553769.
Train: 2018-08-05T05:08:28.142427: step 5208, loss 0.596645.
Train: 2018-08-05T05:08:31.689676: step 5209, loss 0.502265.
Train: 2018-08-05T05:08:35.236925: step 5210, loss 0.545144.
Test: 2018-08-05T05:08:50.254135: step 5210, loss 0.548351.
Train: 2018-08-05T05:08:53.770132: step 5211, loss 0.545113.
Train: 2018-08-05T05:08:57.301755: step 5212, loss 0.545082.
Train: 2018-08-05T05:09:00.833377: step 5213, loss 0.527757.
Train: 2018-08-05T05:09:04.333747: step 5214, loss 0.579681.
Train: 2018-08-05T05:09:07.849743: step 5215, loss 0.518932.
Train: 2018-08-05T05:09:11.365739: step 5216, loss 0.588473.
Train: 2018-08-05T05:09:14.881735: step 5217, loss 0.597238.
Train: 2018-08-05T05:09:18.444611: step 5218, loss 0.588537.
Train: 2018-08-05T05:09:21.976234: step 5219, loss 0.518749.
Train: 2018-08-05T05:09:25.492230: step 5220, loss 0.53618.
Test: 2018-08-05T05:09:40.540694: step 5220, loss 0.547577.
Train: 2018-08-05T05:09:44.087943: step 5221, loss 0.553633.
Train: 2018-08-05T05:09:47.603940: step 5222, loss 0.597367.
Train: 2018-08-05T05:09:51.119936: step 5223, loss 0.553628.
Train: 2018-08-05T05:09:54.667185: step 5224, loss 0.579877.
Train: 2018-08-05T05:09:58.214434: step 5225, loss 0.553629.
Train: 2018-08-05T05:10:01.730430: step 5226, loss 0.623587.
Train: 2018-08-05T05:10:05.230800: step 5227, loss 0.553638.
Train: 2018-08-05T05:10:08.793676: step 5228, loss 0.536207.
Train: 2018-08-05T05:10:12.309673: step 5229, loss 0.5188.
Train: 2018-08-05T05:10:15.903802: step 5230, loss 0.579786.
Test: 2018-08-05T05:10:30.952265: step 5230, loss 0.549017.
Train: 2018-08-05T05:10:34.483888: step 5231, loss 0.579779.
Train: 2018-08-05T05:10:38.015511: step 5232, loss 0.544951.
Train: 2018-08-05T05:10:41.531507: step 5233, loss 0.562356.
Train: 2018-08-05T05:10:45.047503: step 5234, loss 0.527575.
Train: 2018-08-05T05:10:48.579126: step 5235, loss 0.571051.
Train: 2018-08-05T05:10:52.110748: step 5236, loss 0.518877.
Train: 2018-08-05T05:10:55.611118: step 5237, loss 0.57976.
Train: 2018-08-05T05:10:59.127114: step 5238, loss 0.54495.
Train: 2018-08-05T05:11:02.643110: step 5239, loss 0.544944.
Train: 2018-08-05T05:11:06.127853: step 5240, loss 0.544934.
Test: 2018-08-05T05:11:21.223196: step 5240, loss 0.5478.
Train: 2018-08-05T05:11:24.770446: step 5241, loss 0.579806.
Train: 2018-08-05T05:11:28.286442: step 5242, loss 0.475128.
Train: 2018-08-05T05:11:31.864945: step 5243, loss 0.606078.
Train: 2018-08-05T05:11:35.380941: step 5244, loss 0.597368.
Train: 2018-08-05T05:11:38.896938: step 5245, loss 0.641095.
Train: 2018-08-05T05:11:42.397307: step 5246, loss 0.571099.
Train: 2018-08-05T05:11:45.897676: step 5247, loss 0.562361.
Train: 2018-08-05T05:11:49.491806: step 5248, loss 0.588446.
Train: 2018-08-05T05:11:53.054682: step 5249, loss 0.553672.
Train: 2018-08-05T05:11:56.601931: step 5250, loss 0.562344.
Test: 2018-08-05T05:12:11.697275: step 5250, loss 0.548104.
Train: 2018-08-05T05:12:15.244524: step 5251, loss 0.484549.
Train: 2018-08-05T05:12:18.776147: step 5252, loss 0.475914.
Train: 2018-08-05T05:12:22.292143: step 5253, loss 0.519058.
Train: 2018-08-05T05:12:25.808139: step 5254, loss 0.597058.
Train: 2018-08-05T05:12:29.339762: step 5255, loss 0.536286.
Train: 2018-08-05T05:12:32.887011: step 5256, loss 0.527546.
Train: 2018-08-05T05:12:36.403008: step 5257, loss 0.579806.
Train: 2018-08-05T05:12:39.903377: step 5258, loss 0.553636.
Train: 2018-08-05T05:12:43.403746: step 5259, loss 0.536141.
Train: 2018-08-05T05:12:46.904116: step 5260, loss 0.623696.
Test: 2018-08-05T05:13:01.952580: step 5260, loss 0.54856.
Train: 2018-08-05T05:13:05.484202: step 5261, loss 0.641222.
Train: 2018-08-05T05:13:09.109585: step 5262, loss 0.632339.
Train: 2018-08-05T05:13:12.641208: step 5263, loss 0.58852.
Train: 2018-08-05T05:13:16.172831: step 5264, loss 0.640557.
Train: 2018-08-05T05:13:19.751333: step 5265, loss 0.562341.
Train: 2018-08-05T05:13:23.236076: step 5266, loss 0.605388.
Train: 2018-08-05T05:13:26.830206: step 5267, loss 0.53663.
Train: 2018-08-05T05:13:30.393082: step 5268, loss 0.596478.
Train: 2018-08-05T05:13:34.018464: step 5269, loss 0.587842.
Train: 2018-08-05T05:13:37.581341: step 5270, loss 0.528512.
Test: 2018-08-05T05:13:52.739191: step 5270, loss 0.547894.
Train: 2018-08-05T05:13:56.317693: step 5271, loss 0.545499.
Train: 2018-08-05T05:13:59.880569: step 5272, loss 0.511894.
Train: 2018-08-05T05:14:03.443445: step 5273, loss 0.596006.
Train: 2018-08-05T05:14:06.975068: step 5274, loss 0.579176.
Train: 2018-08-05T05:14:10.553571: step 5275, loss 0.503753.
Train: 2018-08-05T05:14:14.085194: step 5276, loss 0.570777.
Train: 2018-08-05T05:14:17.616817: step 5277, loss 0.562402.
Train: 2018-08-05T05:14:21.148439: step 5278, loss 0.612647.
Train: 2018-08-05T05:14:24.711315: step 5279, loss 0.554044.
Train: 2018-08-05T05:14:28.242938: step 5280, loss 0.570771.
Test: 2018-08-05T05:14:43.369537: step 5280, loss 0.548659.
Train: 2018-08-05T05:14:46.885531: step 5281, loss 0.545712.
Train: 2018-08-05T05:14:50.417154: step 5282, loss 0.554067.
Train: 2018-08-05T05:14:53.948777: step 5283, loss 0.570769.
Train: 2018-08-05T05:14:57.464773: step 5284, loss 0.554064.
Train: 2018-08-05T05:14:59.199331: step 5285, loss 0.437639.
Train: 2018-08-05T05:15:02.762207: step 5286, loss 0.579162.
Train: 2018-08-05T05:15:06.293830: step 5287, loss 0.579191.
Train: 2018-08-05T05:15:09.809826: step 5288, loss 0.503426.
Train: 2018-08-05T05:15:13.325822: step 5289, loss 0.579251.
Train: 2018-08-05T05:15:16.826191: step 5290, loss 0.61316.
Test: 2018-08-05T05:15:31.843402: step 5290, loss 0.546995.
Train: 2018-08-05T05:15:35.390651: step 5291, loss 0.545423.
Train: 2018-08-05T05:15:38.969154: step 5292, loss 0.545271.
Train: 2018-08-05T05:15:42.547657: step 5293, loss 0.621949.
Train: 2018-08-05T05:15:46.079279: step 5294, loss 0.545119.
Train: 2018-08-05T05:15:49.595276: step 5295, loss 0.545717.
Train: 2018-08-05T05:15:53.095645: step 5296, loss 0.605779.
Train: 2018-08-05T05:15:56.658521: step 5297, loss 0.528241.
Train: 2018-08-05T05:16:00.174517: step 5298, loss 0.528108.
Train: 2018-08-05T05:16:03.690513: step 5299, loss 0.562327.
Train: 2018-08-05T05:16:07.175256: step 5300, loss 0.57944.
Test: 2018-08-05T05:16:22.286226: step 5300, loss 0.547071.
Train: 2018-08-05T05:16:28.005580: step 5301, loss 0.596541.
Train: 2018-08-05T05:16:31.505950: step 5302, loss 0.536688.
Train: 2018-08-05T05:16:35.068829: step 5303, loss 0.639313.
Train: 2018-08-05T05:16:38.631702: step 5304, loss 0.553796.
Train: 2018-08-05T05:16:42.132071: step 5305, loss 0.579406.
Train: 2018-08-05T05:16:45.648067: step 5306, loss 0.621996.
Train: 2018-08-05T05:16:49.148437: step 5307, loss 0.545344.
Train: 2018-08-05T05:16:52.648806: step 5308, loss 0.621758.
Train: 2018-08-05T05:16:56.227309: step 5309, loss 0.52005.
Train: 2018-08-05T05:16:59.743306: step 5310, loss 0.553917.
Test: 2018-08-05T05:17:14.916782: step 5310, loss 0.547304.
Train: 2018-08-05T05:17:18.464031: step 5311, loss 0.570804.
Train: 2018-08-05T05:17:21.995655: step 5312, loss 0.553945.
Train: 2018-08-05T05:17:25.527277: step 5313, loss 0.587637.
Train: 2018-08-05T05:17:29.043273: step 5314, loss 0.562381.
Train: 2018-08-05T05:17:32.574896: step 5315, loss 0.62119.
Train: 2018-08-05T05:17:36.122145: step 5316, loss 0.604308.
Train: 2018-08-05T05:17:39.669395: step 5317, loss 0.537337.
Train: 2018-08-05T05:17:43.154138: step 5318, loss 0.579109.
Train: 2018-08-05T05:17:46.654507: step 5319, loss 0.520805.
Train: 2018-08-05T05:17:50.170504: step 5320, loss 0.5458.
Test: 2018-08-05T05:18:05.234594: step 5320, loss 0.548931.
Train: 2018-08-05T05:18:08.781843: step 5321, loss 0.562442.
Train: 2018-08-05T05:18:12.516612: step 5322, loss 0.554123.
Train: 2018-08-05T05:18:16.063862: step 5323, loss 0.620696.
Train: 2018-08-05T05:18:19.579858: step 5324, loss 0.554131.
Train: 2018-08-05T05:18:23.095854: step 5325, loss 0.579071.
Train: 2018-08-05T05:18:26.674356: step 5326, loss 0.6372.
Train: 2018-08-05T05:18:30.174726: step 5327, loss 0.521036.
Train: 2018-08-05T05:18:33.721975: step 5328, loss 0.521079.
Train: 2018-08-05T05:18:37.284852: step 5329, loss 0.54591.
Train: 2018-08-05T05:18:40.800848: step 5330, loss 0.562468.
Test: 2018-08-05T05:18:55.864940: step 5330, loss 0.548224.
Train: 2018-08-05T05:18:59.380934: step 5331, loss 0.645426.
Train: 2018-08-05T05:19:02.896930: step 5332, loss 0.6122.
Train: 2018-08-05T05:19:06.459806: step 5333, loss 0.512845.
Train: 2018-08-05T05:19:09.991430: step 5334, loss 0.504597.
Train: 2018-08-05T05:19:13.538678: step 5335, loss 0.620433.
Train: 2018-08-05T05:19:17.054675: step 5336, loss 0.570757.
Train: 2018-08-05T05:19:20.633177: step 5337, loss 0.504541.
Train: 2018-08-05T05:19:24.133547: step 5338, loss 0.487877.
Train: 2018-08-05T05:19:27.649543: step 5339, loss 0.604014.
Train: 2018-08-05T05:19:31.259299: step 5340, loss 0.520788.
Test: 2018-08-05T05:19:46.370269: step 5340, loss 0.547698.
Train: 2018-08-05T05:19:49.948772: step 5341, loss 0.570769.
Train: 2018-08-05T05:19:53.449141: step 5342, loss 0.545658.
Train: 2018-08-05T05:19:56.965138: step 5343, loss 0.528806.
Train: 2018-08-05T05:20:00.496760: step 5344, loss 0.553951.
Train: 2018-08-05T05:20:03.997130: step 5345, loss 0.55391.
Train: 2018-08-05T05:20:07.560006: step 5346, loss 0.528432.
Train: 2018-08-05T05:20:11.122882: step 5347, loss 0.587855.
Train: 2018-08-05T05:20:14.763891: step 5348, loss 0.639193.
Train: 2018-08-05T05:20:18.264261: step 5349, loss 0.570915.
Train: 2018-08-05T05:20:21.811510: step 5350, loss 0.553813.
Test: 2018-08-05T05:20:36.953736: step 5350, loss 0.548272.
Train: 2018-08-05T05:20:40.485356: step 5351, loss 0.587961.
Train: 2018-08-05T05:20:44.032606: step 5352, loss 0.579415.
Train: 2018-08-05T05:20:47.579855: step 5353, loss 0.528191.
Train: 2018-08-05T05:20:51.080225: step 5354, loss 0.570878.
Train: 2018-08-05T05:20:54.580594: step 5355, loss 0.528173.
Train: 2018-08-05T05:20:58.096590: step 5356, loss 0.562337.
Train: 2018-08-05T05:21:01.628213: step 5357, loss 0.485339.
Train: 2018-08-05T05:21:05.222343: step 5358, loss 0.57091.
Train: 2018-08-05T05:21:08.753965: step 5359, loss 0.502194.
Train: 2018-08-05T05:21:12.285588: step 5360, loss 0.614039.
Test: 2018-08-05T05:21:27.349679: step 5360, loss 0.549316.
Train: 2018-08-05T05:21:30.881301: step 5361, loss 0.545078.
Train: 2018-08-05T05:21:34.428551: step 5362, loss 0.562341.
Train: 2018-08-05T05:21:37.944547: step 5363, loss 0.562343.
Train: 2018-08-05T05:21:41.460543: step 5364, loss 0.527675.
Train: 2018-08-05T05:21:45.054672: step 5365, loss 0.614448.
Train: 2018-08-05T05:21:48.555042: step 5366, loss 0.553665.
Train: 2018-08-05T05:21:52.086665: step 5367, loss 0.597115.
Train: 2018-08-05T05:21:55.602661: step 5368, loss 0.527603.
Train: 2018-08-05T05:21:59.103030: step 5369, loss 0.63187.
Train: 2018-08-05T05:22:02.619026: step 5370, loss 0.588385.
Test: 2018-08-05T05:22:17.729997: step 5370, loss 0.548874.
Train: 2018-08-05T05:22:21.277246: step 5371, loss 0.588335.
Train: 2018-08-05T05:22:24.871375: step 5372, loss 0.545053.
Train: 2018-08-05T05:22:28.371745: step 5373, loss 0.588226.
Train: 2018-08-05T05:22:31.934621: step 5374, loss 0.562336.
Train: 2018-08-05T05:22:35.450617: step 5375, loss 0.450611.
Train: 2018-08-05T05:22:38.966613: step 5376, loss 0.605331.
Train: 2018-08-05T05:22:42.482609: step 5377, loss 0.519358.
Train: 2018-08-05T05:22:46.045485: step 5378, loss 0.553735.
Train: 2018-08-05T05:22:49.639614: step 5379, loss 0.639781.
Train: 2018-08-05T05:22:53.155611: step 5380, loss 0.545145.
Test: 2018-08-05T05:23:08.422850: step 5380, loss 0.546607.
Train: 2018-08-05T05:23:11.938844: step 5381, loss 0.61387.
Train: 2018-08-05T05:23:15.439213: step 5382, loss 0.570909.
Train: 2018-08-05T05:23:19.048969: step 5383, loss 0.545218.
Train: 2018-08-05T05:23:22.596219: step 5384, loss 0.545241.
Train: 2018-08-05T05:23:26.127841: step 5385, loss 0.57942.
Train: 2018-08-05T05:23:29.675091: step 5386, loss 0.587936.
Train: 2018-08-05T05:23:33.222340: step 5387, loss 0.502706.
Train: 2018-08-05T05:23:36.722710: step 5388, loss 0.545304.
Train: 2018-08-05T05:23:40.301213: step 5389, loss 0.621992.
Train: 2018-08-05T05:23:43.848462: step 5390, loss 0.50275.
Test: 2018-08-05T05:23:58.943807: step 5390, loss 0.547726.
Train: 2018-08-05T05:24:02.491055: step 5391, loss 0.570858.
Train: 2018-08-05T05:24:06.007051: step 5392, loss 0.528267.
Train: 2018-08-05T05:24:09.523047: step 5393, loss 0.553813.
Train: 2018-08-05T05:24:13.117176: step 5394, loss 0.605015.
Train: 2018-08-05T05:24:16.726933: step 5395, loss 0.519662.
Train: 2018-08-05T05:24:20.305436: step 5396, loss 0.536708.
Train: 2018-08-05T05:24:23.821432: step 5397, loss 0.579446.
Train: 2018-08-05T05:24:27.353054: step 5398, loss 0.528086.
Train: 2018-08-05T05:24:30.853424: step 5399, loss 0.622362.
Train: 2018-08-05T05:24:34.369419: step 5400, loss 0.553759.
Test: 2018-08-05T05:24:49.511643: step 5400, loss 0.547615.
Train: 2018-08-05T05:24:55.293504: step 5401, loss 0.588067.
Train: 2018-08-05T05:24:58.809500: step 5402, loss 0.605205.
Train: 2018-08-05T05:25:02.372376: step 5403, loss 0.579461.
Train: 2018-08-05T05:25:05.903998: step 5404, loss 0.545238.
Train: 2018-08-05T05:25:09.419995: step 5405, loss 0.459849.
Train: 2018-08-05T05:25:12.935991: step 5406, loss 0.57944.
Train: 2018-08-05T05:25:16.467614: step 5407, loss 0.536661.
Train: 2018-08-05T05:25:20.030490: step 5408, loss 0.536629.
Train: 2018-08-05T05:25:23.562113: step 5409, loss 0.476506.
Train: 2018-08-05T05:25:27.062482: step 5410, loss 0.527892.
Test: 2018-08-05T05:25:42.079693: step 5410, loss 0.548305.
Train: 2018-08-05T05:25:45.595688: step 5411, loss 0.527771.
Train: 2018-08-05T05:25:49.096058: step 5412, loss 0.553672.
Train: 2018-08-05T05:25:52.643307: step 5413, loss 0.579775.
Train: 2018-08-05T05:25:56.174930: step 5414, loss 0.52744.
Train: 2018-08-05T05:25:59.690927: step 5415, loss 0.544863.
Train: 2018-08-05T05:26:03.191296: step 5416, loss 0.571185.
Train: 2018-08-05T05:26:06.707292: step 5417, loss 0.53599.
Train: 2018-08-05T05:26:10.254541: step 5418, loss 0.553598.
Train: 2018-08-05T05:26:13.801791: step 5419, loss 0.544743.
Train: 2018-08-05T05:26:17.380293: step 5420, loss 0.562461.
Test: 2018-08-05T05:26:32.428756: step 5420, loss 0.547839.
Train: 2018-08-05T05:26:36.007259: step 5421, loss 0.500267.
Train: 2018-08-05T05:26:39.554510: step 5422, loss 0.57141.
Train: 2018-08-05T05:26:43.101759: step 5423, loss 0.562518.
Train: 2018-08-05T05:26:46.664635: step 5424, loss 0.562534.
Train: 2018-08-05T05:26:50.211884: step 5425, loss 0.526725.
Train: 2018-08-05T05:26:53.790387: step 5426, loss 0.535653.
Train: 2018-08-05T05:26:57.306383: step 5427, loss 0.526638.
Train: 2018-08-05T05:27:00.822379: step 5428, loss 0.535593.
Train: 2018-08-05T05:27:04.354002: step 5429, loss 0.634814.
Train: 2018-08-05T05:27:07.885624: step 5430, loss 0.571659.
Test: 2018-08-05T05:27:23.043476: step 5430, loss 0.546937.
Train: 2018-08-05T05:27:26.590724: step 5431, loss 0.571651.
Train: 2018-08-05T05:27:30.122347: step 5432, loss 0.59869.
Train: 2018-08-05T05:27:33.638343: step 5433, loss 0.571605.
Train: 2018-08-05T05:27:37.154339: step 5434, loss 0.553597.
Train: 2018-08-05T05:27:40.654709: step 5435, loss 0.562564.
Train: 2018-08-05T05:27:42.373640: step 5436, loss 0.638964.
Train: 2018-08-05T05:27:45.874010: step 5437, loss 0.482186.
Train: 2018-08-05T05:27:49.421259: step 5438, loss 0.598142.
Train: 2018-08-05T05:27:52.952882: step 5439, loss 0.51803.
Train: 2018-08-05T05:27:56.453251: step 5440, loss 0.500329.
Test: 2018-08-05T05:28:11.517341: step 5440, loss 0.547643.
Train: 2018-08-05T05:28:15.064591: step 5441, loss 0.55359.
Train: 2018-08-05T05:28:18.611840: step 5442, loss 0.55359.
Train: 2018-08-05T05:28:22.127836: step 5443, loss 0.580204.
Train: 2018-08-05T05:28:25.675086: step 5444, loss 0.624508.
Train: 2018-08-05T05:28:29.237962: step 5445, loss 0.51821.
Train: 2018-08-05T05:28:32.769585: step 5446, loss 0.544761.
Train: 2018-08-05T05:28:36.285581: step 5447, loss 0.553599.
Train: 2018-08-05T05:28:39.785950: step 5448, loss 0.562422.
Train: 2018-08-05T05:28:43.286320: step 5449, loss 0.509529.
Train: 2018-08-05T05:28:46.896076: step 5450, loss 0.597684.
Test: 2018-08-05T05:29:01.928913: step 5450, loss 0.548309.
Train: 2018-08-05T05:29:05.476162: step 5451, loss 0.597654.
Train: 2018-08-05T05:29:09.023412: step 5452, loss 0.623982.
Train: 2018-08-05T05:29:12.539408: step 5453, loss 0.544845.
Train: 2018-08-05T05:29:16.086657: step 5454, loss 0.597389.
Train: 2018-08-05T05:29:19.618280: step 5455, loss 0.588549.
Train: 2018-08-05T05:29:23.165529: step 5456, loss 0.579752.
Train: 2018-08-05T05:29:26.712779: step 5457, loss 0.571014.
Train: 2018-08-05T05:29:30.244402: step 5458, loss 0.553692.
Train: 2018-08-05T05:29:33.760398: step 5459, loss 0.536514.
Train: 2018-08-05T05:29:37.276394: step 5460, loss 0.50216.
Test: 2018-08-05T05:29:52.309232: step 5460, loss 0.547394.
Train: 2018-08-05T05:29:55.872107: step 5461, loss 0.57951.
Train: 2018-08-05T05:29:59.434983: step 5462, loss 0.57094.
Train: 2018-08-05T05:30:02.982233: step 5463, loss 0.562335.
Train: 2018-08-05T05:30:06.482602: step 5464, loss 0.61375.
Train: 2018-08-05T05:30:10.014225: step 5465, loss 0.511043.
Train: 2018-08-05T05:30:13.530221: step 5466, loss 0.511081.
Train: 2018-08-05T05:30:17.046217: step 5467, loss 0.639258.
Train: 2018-08-05T05:30:20.609093: step 5468, loss 0.570874.
Train: 2018-08-05T05:30:24.125090: step 5469, loss 0.536766.
Train: 2018-08-05T05:30:27.641086: step 5470, loss 0.536783.
Test: 2018-08-05T05:30:42.658296: step 5470, loss 0.547917.
Train: 2018-08-05T05:30:46.221172: step 5471, loss 0.536782.
Train: 2018-08-05T05:30:49.784048: step 5472, loss 0.630536.
Train: 2018-08-05T05:30:53.315671: step 5473, loss 0.579374.
Train: 2018-08-05T05:30:56.831667: step 5474, loss 0.579356.
Train: 2018-08-05T05:31:00.410170: step 5475, loss 0.451933.
Train: 2018-08-05T05:31:03.941792: step 5476, loss 0.553842.
Train: 2018-08-05T05:31:07.442162: step 5477, loss 0.519775.
Train: 2018-08-05T05:31:10.958158: step 5478, loss 0.57087.
Train: 2018-08-05T05:31:14.489781: step 5479, loss 0.596518.
Train: 2018-08-05T05:31:18.037030: step 5480, loss 0.587991.
Test: 2018-08-05T05:31:33.210507: step 5480, loss 0.548841.
Train: 2018-08-05T05:31:36.742130: step 5481, loss 0.579441.
Train: 2018-08-05T05:31:40.242499: step 5482, loss 0.545237.
Train: 2018-08-05T05:31:43.805376: step 5483, loss 0.570887.
Train: 2018-08-05T05:31:47.336998: step 5484, loss 0.587987.
Train: 2018-08-05T05:31:50.868621: step 5485, loss 0.596515.
Train: 2018-08-05T05:31:54.431497: step 5486, loss 0.477009.
Train: 2018-08-05T05:31:57.978747: step 5487, loss 0.562338.
Train: 2018-08-05T05:32:01.510369: step 5488, loss 0.596512.
Train: 2018-08-05T05:32:05.041992: step 5489, loss 0.562338.
Train: 2018-08-05T05:32:08.557988: step 5490, loss 0.579418.
Test: 2018-08-05T05:32:23.653332: step 5490, loss 0.547691.
Train: 2018-08-05T05:32:27.200581: step 5491, loss 0.536732.
Train: 2018-08-05T05:32:30.747831: step 5492, loss 0.53673.
Train: 2018-08-05T05:32:34.295080: step 5493, loss 0.622127.
Train: 2018-08-05T05:32:37.811076: step 5494, loss 0.519664.
Train: 2018-08-05T05:32:41.342699: step 5495, loss 0.502578.
Train: 2018-08-05T05:32:44.858695: step 5496, loss 0.553786.
Train: 2018-08-05T05:32:48.390318: step 5497, loss 0.639408.
Train: 2018-08-05T05:32:51.937568: step 5498, loss 0.66507.
Train: 2018-08-05T05:32:55.453563: step 5499, loss 0.528178.
Train: 2018-08-05T05:32:59.000813: step 5500, loss 0.545286.
Test: 2018-08-05T05:33:14.033650: step 5500, loss 0.54733.
Train: 2018-08-05T05:33:19.815510: step 5501, loss 0.639009.
Train: 2018-08-05T05:33:23.315879: step 5502, loss 0.57934.
Train: 2018-08-05T05:33:26.878756: step 5503, loss 0.604725.
Train: 2018-08-05T05:33:30.379125: step 5504, loss 0.553917.
Train: 2018-08-05T05:33:33.973254: step 5505, loss 0.52868.
Train: 2018-08-05T05:33:37.520504: step 5506, loss 0.58761.
Train: 2018-08-05T05:33:41.099007: step 5507, loss 0.545605.
Train: 2018-08-05T05:33:44.615003: step 5508, loss 0.562396.
Train: 2018-08-05T05:33:48.130999: step 5509, loss 0.604267.
Train: 2018-08-05T05:33:51.693875: step 5510, loss 0.587486.
Test: 2018-08-05T05:34:06.695459: step 5510, loss 0.54965.
Train: 2018-08-05T05:34:10.289588: step 5511, loss 0.520726.
Train: 2018-08-05T05:34:13.899344: step 5512, loss 0.529104.
Train: 2018-08-05T05:34:17.462220: step 5513, loss 0.504095.
Train: 2018-08-05T05:34:21.040723: step 5514, loss 0.470606.
Train: 2018-08-05T05:34:24.556719: step 5515, loss 0.545647.
Train: 2018-08-05T05:34:28.088342: step 5516, loss 0.52875.
Train: 2018-08-05T05:34:31.619965: step 5517, loss 0.553922.
Train: 2018-08-05T05:34:35.167214: step 5518, loss 0.519971.
Train: 2018-08-05T05:34:38.714463: step 5519, loss 0.587886.
Train: 2018-08-05T05:34:42.230460: step 5520, loss 0.502525.
Test: 2018-08-05T05:34:57.325803: step 5520, loss 0.549187.
Train: 2018-08-05T05:35:00.873052: step 5521, loss 0.5795.
Train: 2018-08-05T05:35:04.404675: step 5522, loss 0.493431.
Train: 2018-08-05T05:35:07.983178: step 5523, loss 0.570996.
Train: 2018-08-05T05:35:11.514801: step 5524, loss 0.588413.
Train: 2018-08-05T05:35:15.046423: step 5525, loss 0.571072.
Train: 2018-08-05T05:35:18.593673: step 5526, loss 0.518709.
Train: 2018-08-05T05:35:22.125296: step 5527, loss 0.614923.
Train: 2018-08-05T05:35:25.641292: step 5528, loss 0.562388.
Train: 2018-08-05T05:35:29.235422: step 5529, loss 0.597508.
Train: 2018-08-05T05:35:32.767045: step 5530, loss 0.553614.
Test: 2018-08-05T05:35:47.862388: step 5530, loss 0.547932.
Train: 2018-08-05T05:35:51.378383: step 5531, loss 0.553614.
Train: 2018-08-05T05:35:54.878753: step 5532, loss 0.518482.
Train: 2018-08-05T05:35:58.394749: step 5533, loss 0.588774.
Train: 2018-08-05T05:36:01.910745: step 5534, loss 0.606364.
Train: 2018-08-05T05:36:05.473621: step 5535, loss 0.544828.
Train: 2018-08-05T05:36:09.005244: step 5536, loss 0.641408.
Train: 2018-08-05T05:36:12.568120: step 5537, loss 0.59742.
Train: 2018-08-05T05:36:16.099743: step 5538, loss 0.527436.
Train: 2018-08-05T05:36:19.662619: step 5539, loss 0.562362.
Train: 2018-08-05T05:36:23.194242: step 5540, loss 0.501471.
Test: 2018-08-05T05:36:38.352093: step 5540, loss 0.547233.
Train: 2018-08-05T05:36:41.899341: step 5541, loss 0.571049.
Train: 2018-08-05T05:36:45.446591: step 5542, loss 0.492845.
Train: 2018-08-05T05:36:48.978214: step 5543, loss 0.510187.
Train: 2018-08-05T05:36:52.494210: step 5544, loss 0.597194.
Train: 2018-08-05T05:36:56.072713: step 5545, loss 0.605932.
Train: 2018-08-05T05:36:59.604336: step 5546, loss 0.632043.
Train: 2018-08-05T05:37:03.135958: step 5547, loss 0.544967.
Train: 2018-08-05T05:37:06.683208: step 5548, loss 0.588392.
Train: 2018-08-05T05:37:10.199204: step 5549, loss 0.631655.
Train: 2018-08-05T05:37:13.683947: step 5550, loss 0.545069.
Test: 2018-08-05T05:37:28.763664: step 5550, loss 0.549143.
Train: 2018-08-05T05:37:32.310913: step 5551, loss 0.588173.
Train: 2018-08-05T05:37:35.842536: step 5552, loss 0.579508.
Train: 2018-08-05T05:37:39.358532: step 5553, loss 0.528097.
Train: 2018-08-05T05:37:42.890155: step 5554, loss 0.596505.
Train: 2018-08-05T05:37:46.499911: step 5555, loss 0.545302.
Train: 2018-08-05T05:37:50.015907: step 5556, loss 0.519832.
Train: 2018-08-05T05:37:53.547529: step 5557, loss 0.460401.
Train: 2018-08-05T05:37:57.063526: step 5558, loss 0.604885.
Train: 2018-08-05T05:38:00.595149: step 5559, loss 0.655976.
Train: 2018-08-05T05:38:04.173651: step 5560, loss 0.570844.
Test: 2018-08-05T05:38:19.347127: step 5560, loss 0.54818.
Train: 2018-08-05T05:38:22.863124: step 5561, loss 0.570834.
Train: 2018-08-05T05:38:26.379120: step 5562, loss 0.460708.
Train: 2018-08-05T05:38:29.910743: step 5563, loss 0.553875.
Train: 2018-08-05T05:38:33.411112: step 5564, loss 0.545379.
Train: 2018-08-05T05:38:37.036495: step 5565, loss 0.579339.
Train: 2018-08-05T05:38:40.599372: step 5566, loss 0.553842.
Train: 2018-08-05T05:38:44.177875: step 5567, loss 0.553833.
Train: 2018-08-05T05:38:47.709497: step 5568, loss 0.494188.
Train: 2018-08-05T05:38:51.225493: step 5569, loss 0.587957.
Train: 2018-08-05T05:38:54.725863: step 5570, loss 0.528123.
Test: 2018-08-05T05:39:09.852459: step 5570, loss 0.548217.
Train: 2018-08-05T05:39:13.384082: step 5571, loss 0.596623.
Train: 2018-08-05T05:39:16.962585: step 5572, loss 0.562335.
Train: 2018-08-05T05:39:20.525461: step 5573, loss 0.639656.
Train: 2018-08-05T05:39:24.041457: step 5574, loss 0.613844.
Train: 2018-08-05T05:39:27.573080: step 5575, loss 0.648029.
Train: 2018-08-05T05:39:31.089076: step 5576, loss 0.605036.
Train: 2018-08-05T05:39:34.620699: step 5577, loss 0.46879.
Train: 2018-08-05T05:39:38.167948: step 5578, loss 0.536874.
Train: 2018-08-05T05:39:41.699571: step 5579, loss 0.596285.
Train: 2018-08-05T05:39:45.246820: step 5580, loss 0.570825.
Test: 2018-08-05T05:40:00.373419: step 5580, loss 0.548622.
Train: 2018-08-05T05:40:03.905040: step 5581, loss 0.536983.
Train: 2018-08-05T05:40:07.452289: step 5582, loss 0.553909.
Train: 2018-08-05T05:40:10.983912: step 5583, loss 0.613054.
Train: 2018-08-05T05:40:14.531162: step 5584, loss 0.511751.
Train: 2018-08-05T05:40:18.109664: step 5585, loss 0.562369.
Train: 2018-08-05T05:40:21.625661: step 5586, loss 0.511762.
Train: 2018-08-05T05:40:23.344592: step 5587, loss 0.544351.
Train: 2018-08-05T05:40:26.844962: step 5588, loss 0.604641.
Train: 2018-08-05T05:40:30.360958: step 5589, loss 0.587738.
Train: 2018-08-05T05:40:33.892580: step 5590, loss 0.579276.
Test: 2018-08-05T05:40:49.003551: step 5590, loss 0.548629.
Train: 2018-08-05T05:40:52.566427: step 5591, loss 0.54545.
Train: 2018-08-05T05:40:56.113676: step 5592, loss 0.587726.
Train: 2018-08-05T05:40:59.645299: step 5593, loss 0.511655.
Train: 2018-08-05T05:41:03.161295: step 5594, loss 0.52853.
Train: 2018-08-05T05:41:06.739798: step 5595, loss 0.511538.
Train: 2018-08-05T05:41:10.302674: step 5596, loss 0.502916.
Train: 2018-08-05T05:41:13.849923: step 5597, loss 0.511219.
Train: 2018-08-05T05:41:17.412800: step 5598, loss 0.545222.
Train: 2018-08-05T05:41:20.913169: step 5599, loss 0.596706.
Train: 2018-08-05T05:41:24.413538: step 5600, loss 0.588193.
Test: 2018-08-05T05:41:39.493255: step 5600, loss 0.548114.
Train: 2018-08-05T05:41:45.478262: step 5601, loss 0.519156.
Train: 2018-08-05T05:41:49.025512: step 5602, loss 0.553684.
Train: 2018-08-05T05:41:52.604014: step 5603, loss 0.605764.
Train: 2018-08-05T05:41:56.120010: step 5604, loss 0.605824.
Train: 2018-08-05T05:41:59.636006: step 5605, loss 0.536267.
Train: 2018-08-05T05:42:03.152003: step 5606, loss 0.51885.
Train: 2018-08-05T05:42:06.699252: step 5607, loss 0.510082.
Train: 2018-08-05T05:42:10.230875: step 5608, loss 0.632231.
Train: 2018-08-05T05:42:13.762497: step 5609, loss 0.544898.
Train: 2018-08-05T05:42:17.262867: step 5610, loss 0.571116.
Test: 2018-08-05T05:42:32.358211: step 5610, loss 0.548374.
Train: 2018-08-05T05:42:35.921087: step 5611, loss 0.58861.
Train: 2018-08-05T05:42:39.421456: step 5612, loss 0.579856.
Train: 2018-08-05T05:42:42.937452: step 5613, loss 0.536168.
Train: 2018-08-05T05:42:46.453448: step 5614, loss 0.518712.
Train: 2018-08-05T05:42:50.000698: step 5615, loss 0.562371.
Train: 2018-08-05T05:42:53.547947: step 5616, loss 0.509938.
Train: 2018-08-05T05:42:57.095196: step 5617, loss 0.579878.
Train: 2018-08-05T05:43:00.642446: step 5618, loss 0.649943.
Train: 2018-08-05T05:43:04.127189: step 5619, loss 0.597354.
Train: 2018-08-05T05:43:07.705691: step 5620, loss 0.536186.
Test: 2018-08-05T05:43:22.754156: step 5620, loss 0.547606.
Train: 2018-08-05T05:43:26.301404: step 5621, loss 0.605936.
Train: 2018-08-05T05:43:29.864281: step 5622, loss 0.579745.
Train: 2018-08-05T05:43:33.411530: step 5623, loss 0.588369.
Train: 2018-08-05T05:43:36.927526: step 5624, loss 0.519097.
Train: 2018-08-05T05:43:40.443522: step 5625, loss 0.562339.
Train: 2018-08-05T05:43:43.975145: step 5626, loss 0.545097.
Train: 2018-08-05T05:43:47.631782: step 5627, loss 0.519286.
Train: 2018-08-05T05:43:51.179032: step 5628, loss 0.562336.
Train: 2018-08-05T05:43:54.695041: step 5629, loss 0.484871.
Train: 2018-08-05T05:43:58.211023: step 5630, loss 0.614055.
Test: 2018-08-05T05:44:13.275113: step 5630, loss 0.547741.
Train: 2018-08-05T05:44:16.853616: step 5631, loss 0.519232.
Train: 2018-08-05T05:44:20.400865: step 5632, loss 0.605487.
Train: 2018-08-05T05:44:23.948114: step 5633, loss 0.579597.
Train: 2018-08-05T05:44:27.464111: step 5634, loss 0.536462.
Train: 2018-08-05T05:44:30.964481: step 5635, loss 0.57959.
Train: 2018-08-05T05:44:34.464850: step 5636, loss 0.605452.
Train: 2018-08-05T05:44:37.965220: step 5637, loss 0.545112.
Train: 2018-08-05T05:44:41.481215: step 5638, loss 0.60536.
Train: 2018-08-05T05:44:45.012838: step 5639, loss 0.536565.
Train: 2018-08-05T05:44:48.544461: step 5640, loss 0.553753.
Test: 2018-08-05T05:45:03.655432: step 5640, loss 0.546633.
Train: 2018-08-05T05:45:07.171428: step 5641, loss 0.51946.
Train: 2018-08-05T05:45:10.703050: step 5642, loss 0.545181.
Train: 2018-08-05T05:45:14.219046: step 5643, loss 0.528009.
Train: 2018-08-05T05:45:17.735042: step 5644, loss 0.485008.
Train: 2018-08-05T05:45:21.297918: step 5645, loss 0.4848.
Train: 2018-08-05T05:45:24.829541: step 5646, loss 0.588292.
Train: 2018-08-05T05:45:28.361164: step 5647, loss 0.518968.
Train: 2018-08-05T05:45:31.877160: step 5648, loss 0.605892.
Train: 2018-08-05T05:45:35.393156: step 5649, loss 0.614719.
Train: 2018-08-05T05:45:38.909153: step 5650, loss 0.623494.
Test: 2018-08-05T05:45:53.973245: step 5650, loss 0.547592.
Train: 2018-08-05T05:45:57.520492: step 5651, loss 0.51001.
Train: 2018-08-05T05:46:01.067741: step 5652, loss 0.544909.
Train: 2018-08-05T05:46:04.568111: step 5653, loss 0.5187.
Train: 2018-08-05T05:46:08.068481: step 5654, loss 0.588612.
Train: 2018-08-05T05:46:11.568850: step 5655, loss 0.579878.
Train: 2018-08-05T05:46:15.084846: step 5656, loss 0.579877.
Train: 2018-08-05T05:46:18.632096: step 5657, loss 0.606104.
Train: 2018-08-05T05:46:22.163719: step 5658, loss 0.527438.
Train: 2018-08-05T05:46:25.695341: step 5659, loss 0.597273.
Train: 2018-08-05T05:46:29.211337: step 5660, loss 0.579789.
Test: 2018-08-05T05:46:44.337934: step 5660, loss 0.547028.
Train: 2018-08-05T05:46:47.838303: step 5661, loss 0.623244.
Train: 2018-08-05T05:46:51.338673: step 5662, loss 0.527657.
Train: 2018-08-05T05:46:54.885922: step 5663, loss 0.510413.
Train: 2018-08-05T05:46:58.386292: step 5664, loss 0.501805.
Train: 2018-08-05T05:47:01.933542: step 5665, loss 0.579647.
Train: 2018-08-05T05:47:05.465164: step 5666, loss 0.570995.
Train: 2018-08-05T05:47:08.965534: step 5667, loss 0.493137.
Train: 2018-08-05T05:47:12.481530: step 5668, loss 0.605649.
Train: 2018-08-05T05:47:16.013153: step 5669, loss 0.596991.
Train: 2018-08-05T05:47:19.544775: step 5670, loss 0.536377.
Test: 2018-08-05T05:47:34.608865: step 5670, loss 0.54789.
Train: 2018-08-05T05:47:38.140488: step 5671, loss 0.536381.
Train: 2018-08-05T05:47:41.687738: step 5672, loss 0.562343.
Train: 2018-08-05T05:47:45.203734: step 5673, loss 0.536367.
Train: 2018-08-05T05:47:48.719730: step 5674, loss 0.58834.
Train: 2018-08-05T05:47:52.220100: step 5675, loss 0.519021.
Train: 2018-08-05T05:47:55.751722: step 5676, loss 0.536331.
Train: 2018-08-05T05:47:59.267719: step 5677, loss 0.56235.
Train: 2018-08-05T05:48:02.799341: step 5678, loss 0.605811.
Train: 2018-08-05T05:48:06.299711: step 5679, loss 0.571044.
Train: 2018-08-05T05:48:09.815707: step 5680, loss 0.562352.
Test: 2018-08-05T05:48:24.926676: step 5680, loss 0.548446.
Train: 2018-08-05T05:48:28.442673: step 5681, loss 0.484189.
Train: 2018-08-05T05:48:31.974296: step 5682, loss 0.536269.
Train: 2018-08-05T05:48:35.521545: step 5683, loss 0.579776.
Train: 2018-08-05T05:48:39.053169: step 5684, loss 0.553646.
Train: 2018-08-05T05:48:42.569164: step 5685, loss 0.57109.
Train: 2018-08-05T05:48:46.147667: step 5686, loss 0.579827.
Train: 2018-08-05T05:48:49.663663: step 5687, loss 0.562368.
Train: 2018-08-05T05:48:53.210913: step 5688, loss 0.571096.
Train: 2018-08-05T05:48:56.805042: step 5689, loss 0.571091.
Train: 2018-08-05T05:49:00.305412: step 5690, loss 0.588523.
Test: 2018-08-05T05:49:15.338248: step 5690, loss 0.547614.
Train: 2018-08-05T05:49:18.901125: step 5691, loss 0.562359.
Train: 2018-08-05T05:49:22.417120: step 5692, loss 0.536259.
Train: 2018-08-05T05:49:25.948743: step 5693, loss 0.518886.
Train: 2018-08-05T05:49:29.495993: step 5694, loss 0.588442.
Train: 2018-08-05T05:49:33.058869: step 5695, loss 0.518891.
Train: 2018-08-05T05:49:36.606118: step 5696, loss 0.544961.
Train: 2018-08-05T05:49:40.122114: step 5697, loss 0.605871.
Train: 2018-08-05T05:49:43.669364: step 5698, loss 0.510159.
Train: 2018-08-05T05:49:47.185360: step 5699, loss 0.553652.
Train: 2018-08-05T05:49:50.748236: step 5700, loss 0.544937.
Test: 2018-08-05T05:50:05.765447: step 5700, loss 0.5474.
Train: 2018-08-05T05:50:11.500427: step 5701, loss 0.544925.
Train: 2018-08-05T05:50:15.016423: step 5702, loss 0.571094.
Train: 2018-08-05T05:50:18.594925: step 5703, loss 0.562368.
Train: 2018-08-05T05:50:22.079668: step 5704, loss 0.544895.
Train: 2018-08-05T05:50:25.611291: step 5705, loss 0.588609.
Train: 2018-08-05T05:50:29.189794: step 5706, loss 0.571115.
Train: 2018-08-05T05:50:32.737044: step 5707, loss 0.571116.
Train: 2018-08-05T05:50:36.268666: step 5708, loss 0.588579.
Train: 2018-08-05T05:50:39.800289: step 5709, loss 0.527464.
Train: 2018-08-05T05:50:43.316285: step 5710, loss 0.597283.
Test: 2018-08-05T05:50:58.364751: step 5710, loss 0.549617.
Train: 2018-08-05T05:51:01.896371: step 5711, loss 0.518803.
Train: 2018-08-05T05:51:05.412368: step 5712, loss 0.605904.
Train: 2018-08-05T05:51:08.990870: step 5713, loss 0.54496.
Train: 2018-08-05T05:51:12.506866: step 5714, loss 0.579707.
Train: 2018-08-05T05:51:16.022862: step 5715, loss 0.562333.
Train: 2018-08-05T05:51:19.507606: step 5716, loss 0.510451.
Train: 2018-08-05T05:51:23.039228: step 5717, loss 0.588217.
Train: 2018-08-05T05:51:26.570851: step 5718, loss 0.553479.
Train: 2018-08-05T05:51:30.118101: step 5719, loss 0.528122.
Train: 2018-08-05T05:51:33.618470: step 5720, loss 0.562181.
Test: 2018-08-05T05:51:48.666935: step 5720, loss 0.548788.
Train: 2018-08-05T05:51:52.167303: step 5721, loss 0.639222.
Train: 2018-08-05T05:51:55.714553: step 5722, loss 0.60568.
Train: 2018-08-05T05:51:59.230549: step 5723, loss 0.623622.
Train: 2018-08-05T05:52:02.762171: step 5724, loss 0.565204.
Train: 2018-08-05T05:52:06.309421: step 5725, loss 0.580041.
Train: 2018-08-05T05:52:09.841044: step 5726, loss 0.630781.
Train: 2018-08-05T05:52:13.372666: step 5727, loss 0.570691.
Train: 2018-08-05T05:52:16.873036: step 5728, loss 0.570831.
Train: 2018-08-05T05:52:20.404659: step 5729, loss 0.553976.
Train: 2018-08-05T05:52:23.936281: step 5730, loss 0.503431.
Test: 2018-08-05T05:52:39.094131: step 5730, loss 0.549874.
Train: 2018-08-05T05:52:42.657008: step 5731, loss 0.579201.
Train: 2018-08-05T05:52:46.219884: step 5732, loss 0.595998.
Train: 2018-08-05T05:52:49.767133: step 5733, loss 0.579167.
Train: 2018-08-05T05:52:53.251876: step 5734, loss 0.637722.
Train: 2018-08-05T05:52:56.783499: step 5735, loss 0.562427.
Train: 2018-08-05T05:53:00.299495: step 5736, loss 0.545821.
Train: 2018-08-05T05:53:03.846744: step 5737, loss 0.612229.
Train: 2018-08-05T05:53:05.565675: step 5738, loss 0.474285.
Train: 2018-08-05T05:53:09.066046: step 5739, loss 0.496375.
Train: 2018-08-05T05:53:12.582042: step 5740, loss 0.570756.
Test: 2018-08-05T05:53:27.724264: step 5740, loss 0.549401.
Train: 2018-08-05T05:53:31.240261: step 5741, loss 0.537622.
Train: 2018-08-05T05:53:34.818764: step 5742, loss 0.537566.
Train: 2018-08-05T05:53:38.366013: step 5743, loss 0.554128.
Train: 2018-08-05T05:53:41.960142: step 5744, loss 0.545756.
Train: 2018-08-05T05:53:45.460512: step 5745, loss 0.512264.
Train: 2018-08-05T05:53:48.960882: step 5746, loss 0.512064.
Train: 2018-08-05T05:53:52.492505: step 5747, loss 0.545522.
Train: 2018-08-05T05:53:56.055380: step 5748, loss 0.604673.
Train: 2018-08-05T05:53:59.587003: step 5749, loss 0.494434.
Train: 2018-08-05T05:54:03.134253: step 5750, loss 0.528237.
Test: 2018-08-05T05:54:18.323358: step 5750, loss 0.547439.
Train: 2018-08-05T05:54:21.854978: step 5751, loss 0.579466.
Train: 2018-08-05T05:54:25.386601: step 5752, loss 0.562335.
Train: 2018-08-05T05:54:28.902598: step 5753, loss 0.614093.
Train: 2018-08-05T05:54:32.434220: step 5754, loss 0.588265.
Train: 2018-08-05T05:54:35.981470: step 5755, loss 0.631544.
Train: 2018-08-05T05:54:39.528719: step 5756, loss 0.562341.
Train: 2018-08-05T05:54:43.060342: step 5757, loss 0.605537.
Train: 2018-08-05T05:54:46.576338: step 5758, loss 0.57959.
Train: 2018-08-05T05:54:50.092335: step 5759, loss 0.657049.
Train: 2018-08-05T05:54:53.608330: step 5760, loss 0.562335.
Test: 2018-08-05T05:55:08.641167: step 5760, loss 0.547666.
Train: 2018-08-05T05:55:12.219670: step 5761, loss 0.562337.
Train: 2018-08-05T05:55:15.766919: step 5762, loss 0.596432.
Train: 2018-08-05T05:55:19.345422: step 5763, loss 0.57084.
Train: 2018-08-05T05:55:22.845791: step 5764, loss 0.553892.
Train: 2018-08-05T05:55:26.346161: step 5765, loss 0.570807.
Train: 2018-08-05T05:55:29.893411: step 5766, loss 0.596051.
Train: 2018-08-05T05:55:33.440660: step 5767, loss 0.562391.
Train: 2018-08-05T05:55:37.019163: step 5768, loss 0.604256.
Train: 2018-08-05T05:55:40.519532: step 5769, loss 0.579109.
Train: 2018-08-05T05:55:44.051155: step 5770, loss 0.570761.
Test: 2018-08-05T05:55:59.193380: step 5770, loss 0.548239.
Train: 2018-08-05T05:56:02.725001: step 5771, loss 0.562467.
Train: 2018-08-05T05:56:06.334757: step 5772, loss 0.49633.
Train: 2018-08-05T05:56:09.850754: step 5773, loss 0.471579.
Train: 2018-08-05T05:56:13.413630: step 5774, loss 0.554198.
Train: 2018-08-05T05:56:16.929625: step 5775, loss 0.587349.
Train: 2018-08-05T05:56:20.461248: step 5776, loss 0.628903.
Train: 2018-08-05T05:56:23.945992: step 5777, loss 0.487716.
Train: 2018-08-05T05:56:27.461987: step 5778, loss 0.53749.
Train: 2018-08-05T05:56:30.993610: step 5779, loss 0.53742.
Train: 2018-08-05T05:56:34.556486: step 5780, loss 0.554053.
Test: 2018-08-05T05:56:49.683084: step 5780, loss 0.547628.
Train: 2018-08-05T05:56:53.214706: step 5781, loss 0.486962.
Train: 2018-08-05T05:56:56.746329: step 5782, loss 0.579212.
Train: 2018-08-05T05:57:00.262325: step 5783, loss 0.562364.
Train: 2018-08-05T05:57:03.793947: step 5784, loss 0.638614.
Train: 2018-08-05T05:57:07.325571: step 5785, loss 0.570834.
Train: 2018-08-05T05:57:10.872820: step 5786, loss 0.494418.
Train: 2018-08-05T05:57:14.388816: step 5787, loss 0.587876.
Train: 2018-08-05T05:57:17.920439: step 5788, loss 0.570865.
Train: 2018-08-05T05:57:21.467688: step 5789, loss 0.511128.
Train: 2018-08-05T05:57:24.968058: step 5790, loss 0.553782.
Test: 2018-08-05T05:57:40.016521: step 5790, loss 0.547623.
Train: 2018-08-05T05:57:43.579397: step 5791, loss 0.570908.
Train: 2018-08-05T05:57:47.142273: step 5792, loss 0.519397.
Train: 2018-08-05T05:57:50.689523: step 5793, loss 0.510681.
Train: 2018-08-05T05:57:54.189893: step 5794, loss 0.64008.
Train: 2018-08-05T05:57:57.705888: step 5795, loss 0.588288.
Train: 2018-08-05T05:58:01.206258: step 5796, loss 0.570994.
Train: 2018-08-05T05:58:04.753508: step 5797, loss 0.657534.
Train: 2018-08-05T05:58:08.394517: step 5798, loss 0.536428.
Train: 2018-08-05T05:58:11.926139: step 5799, loss 0.527832.
Train: 2018-08-05T05:58:15.426509: step 5800, loss 0.467487.
Test: 2018-08-05T05:58:30.490599: step 5800, loss 0.546723.
Train: 2018-08-05T05:58:36.350592: step 5801, loss 0.519156.
Train: 2018-08-05T05:58:39.850962: step 5802, loss 0.536374.
Train: 2018-08-05T05:58:43.351332: step 5803, loss 0.631778.
Train: 2018-08-05T05:58:46.945461: step 5804, loss 0.501556.
Train: 2018-08-05T05:58:50.477084: step 5805, loss 0.571058.
Train: 2018-08-05T05:58:53.993080: step 5806, loss 0.51008.
Train: 2018-08-05T05:58:57.509076: step 5807, loss 0.553636.
Train: 2018-08-05T05:59:01.025073: step 5808, loss 0.501118.
Train: 2018-08-05T05:59:04.587948: step 5809, loss 0.500947.
Train: 2018-08-05T05:59:08.135198: step 5810, loss 0.562415.
Test: 2018-08-05T05:59:23.277421: step 5810, loss 0.547468.
Train: 2018-08-05T05:59:26.824671: step 5811, loss 0.50939.
Train: 2018-08-05T05:59:30.340667: step 5812, loss 0.571341.
Train: 2018-08-05T05:59:33.872290: step 5813, loss 0.571394.
Train: 2018-08-05T05:59:37.372659: step 5814, loss 0.562513.
Train: 2018-08-05T05:59:40.873029: step 5815, loss 0.580416.
Train: 2018-08-05T05:59:44.451531: step 5816, loss 0.571496.
Train: 2018-08-05T05:59:48.014407: step 5817, loss 0.526715.
Train: 2018-08-05T05:59:51.577283: step 5818, loss 0.51772.
Train: 2018-08-05T05:59:55.093280: step 5819, loss 0.53563.
Train: 2018-08-05T05:59:58.656155: step 5820, loss 0.526604.
Test: 2018-08-05T06:00:13.673367: step 5820, loss 0.548393.
Train: 2018-08-05T06:00:17.220616: step 5821, loss 0.670825.
Train: 2018-08-05T06:00:20.814745: step 5822, loss 0.526567.
Train: 2018-08-05T06:00:24.393248: step 5823, loss 0.580638.
Train: 2018-08-05T06:00:27.924870: step 5824, loss 0.616641.
Train: 2018-08-05T06:00:31.409613: step 5825, loss 0.580562.
Train: 2018-08-05T06:00:34.925610: step 5826, loss 0.544625.
Train: 2018-08-05T06:00:38.441605: step 5827, loss 0.51779.
Train: 2018-08-05T06:00:41.973228: step 5828, loss 0.553589.
Train: 2018-08-05T06:00:45.504851: step 5829, loss 0.625036.
Train: 2018-08-05T06:00:49.020848: step 5830, loss 0.52686.
Test: 2018-08-05T06:01:04.147446: step 5830, loss 0.54681.
Train: 2018-08-05T06:01:07.663440: step 5831, loss 0.606957.
Train: 2018-08-05T06:01:11.179436: step 5832, loss 0.562461.
Train: 2018-08-05T06:01:14.711059: step 5833, loss 0.60669.
Train: 2018-08-05T06:01:18.258308: step 5834, loss 0.597698.
Train: 2018-08-05T06:01:21.805558: step 5835, loss 0.500921.
Train: 2018-08-05T06:01:25.337181: step 5836, loss 0.536091.
Train: 2018-08-05T06:01:28.853177: step 5837, loss 0.649871.
Train: 2018-08-05T06:01:32.400426: step 5838, loss 0.571082.
Train: 2018-08-05T06:01:35.932049: step 5839, loss 0.562353.
Train: 2018-08-05T06:01:39.463672: step 5840, loss 0.501717.
Test: 2018-08-05T06:01:54.543389: step 5840, loss 0.548299.
Train: 2018-08-05T06:01:58.043758: step 5841, loss 0.562341.
Train: 2018-08-05T06:02:01.544128: step 5842, loss 0.527807.
Train: 2018-08-05T06:02:05.060124: step 5843, loss 0.536458.
Train: 2018-08-05T06:02:08.576120: step 5844, loss 0.622715.
Train: 2018-08-05T06:02:12.092116: step 5845, loss 0.579561.
Train: 2018-08-05T06:02:15.670619: step 5846, loss 0.519348.
Train: 2018-08-05T06:02:19.186615: step 5847, loss 0.510785.
Train: 2018-08-05T06:02:22.718238: step 5848, loss 0.519358.
Train: 2018-08-05T06:02:26.265487: step 5849, loss 0.596764.
Train: 2018-08-05T06:02:29.781483: step 5850, loss 0.605387.
Test: 2018-08-05T06:02:44.876829: step 5850, loss 0.547172.
Train: 2018-08-05T06:02:48.408450: step 5851, loss 0.570941.
Train: 2018-08-05T06:02:51.971326: step 5852, loss 0.562335.
Train: 2018-08-05T06:02:55.534202: step 5853, loss 0.58811.
Train: 2018-08-05T06:02:59.050198: step 5854, loss 0.53659.
Train: 2018-08-05T06:03:02.566194: step 5855, loss 0.613793.
Train: 2018-08-05T06:03:06.113443: step 5856, loss 0.570897.
Train: 2018-08-05T06:03:09.629440: step 5857, loss 0.545237.
Train: 2018-08-05T06:03:13.145436: step 5858, loss 0.562332.
Train: 2018-08-05T06:03:16.755192: step 5859, loss 0.664657.
Train: 2018-08-05T06:03:20.302442: step 5860, loss 0.630308.
Test: 2018-08-05T06:03:35.382158: step 5860, loss 0.548044.
Train: 2018-08-05T06:03:38.898154: step 5861, loss 0.621579.
Train: 2018-08-05T06:03:42.445404: step 5862, loss 0.579167.
Train: 2018-08-05T06:03:46.023907: step 5863, loss 0.537583.
Train: 2018-08-05T06:03:49.539903: step 5864, loss 0.570515.
Train: 2018-08-05T06:03:53.071525: step 5865, loss 0.529075.
Train: 2018-08-05T06:03:56.665655: step 5866, loss 0.678113.
Train: 2018-08-05T06:04:00.197278: step 5867, loss 0.521117.
Train: 2018-08-05T06:04:03.713274: step 5868, loss 0.546626.
Train: 2018-08-05T06:04:07.229270: step 5869, loss 0.522395.
Train: 2018-08-05T06:04:10.760893: step 5870, loss 0.512989.
Test: 2018-08-05T06:04:25.887490: step 5870, loss 0.548355.
Train: 2018-08-05T06:04:29.434739: step 5871, loss 0.57121.
Train: 2018-08-05T06:04:32.997615: step 5872, loss 0.55403.
Train: 2018-08-05T06:04:36.513611: step 5873, loss 0.619543.
Train: 2018-08-05T06:04:40.060861: step 5874, loss 0.594802.
Train: 2018-08-05T06:04:43.592483: step 5875, loss 0.554581.
Train: 2018-08-05T06:04:47.108479: step 5876, loss 0.586406.
Train: 2018-08-05T06:04:50.686982: step 5877, loss 0.51366.
Train: 2018-08-05T06:04:54.281111: step 5878, loss 0.539105.
Train: 2018-08-05T06:04:57.797107: step 5879, loss 0.627845.
Train: 2018-08-05T06:05:01.344357: step 5880, loss 0.61242.
Test: 2018-08-05T06:05:16.455328: step 5880, loss 0.548389.
Train: 2018-08-05T06:05:20.018203: step 5881, loss 0.506593.
Train: 2018-08-05T06:05:23.549826: step 5882, loss 0.587329.
Train: 2018-08-05T06:05:27.097075: step 5883, loss 0.530265.
Train: 2018-08-05T06:05:30.628698: step 5884, loss 0.513014.
Train: 2018-08-05T06:05:34.144694: step 5885, loss 0.603816.
Train: 2018-08-05T06:05:37.645064: step 5886, loss 0.537517.
Train: 2018-08-05T06:05:41.207940: step 5887, loss 0.579089.
Train: 2018-08-05T06:05:44.723936: step 5888, loss 0.529031.
Train: 2018-08-05T06:05:46.458494: step 5889, loss 0.562407.
Train: 2018-08-05T06:05:49.974490: step 5890, loss 0.579167.
Test: 2018-08-05T06:06:05.038581: step 5890, loss 0.548162.
Train: 2018-08-05T06:06:08.554577: step 5891, loss 0.646397.
Train: 2018-08-05T06:06:12.101826: step 5892, loss 0.528791.
Train: 2018-08-05T06:06:15.649075: step 5893, loss 0.49515.
Train: 2018-08-05T06:06:19.165071: step 5894, loss 0.494997.
Train: 2018-08-05T06:06:22.727949: step 5895, loss 0.65533.
Train: 2018-08-05T06:06:26.306450: step 5896, loss 0.553897.
Train: 2018-08-05T06:06:29.806820: step 5897, loss 0.596241.
Train: 2018-08-05T06:06:33.322816: step 5898, loss 0.579303.
Train: 2018-08-05T06:06:36.838812: step 5899, loss 0.562353.
Train: 2018-08-05T06:06:40.370435: step 5900, loss 0.562353.
Test: 2018-08-05T06:06:55.731432: step 5900, loss 0.547025.
Train: 2018-08-05T06:07:01.482039: step 5901, loss 0.655595.
Train: 2018-08-05T06:07:05.013661: step 5902, loss 0.579279.
Train: 2018-08-05T06:07:08.529657: step 5903, loss 0.511709.
Train: 2018-08-05T06:07:12.061280: step 5904, loss 0.629863.
Train: 2018-08-05T06:07:15.561649: step 5905, loss 0.579215.
Train: 2018-08-05T06:07:19.046393: step 5906, loss 0.537182.
Train: 2018-08-05T06:07:22.624895: step 5907, loss 0.587563.
Train: 2018-08-05T06:07:26.140891: step 5908, loss 0.487015.
Train: 2018-08-05T06:07:29.703768: step 5909, loss 0.554021.
Train: 2018-08-05T06:07:33.188511: step 5910, loss 0.537246.
Test: 2018-08-05T06:07:48.252600: step 5910, loss 0.547794.
Train: 2018-08-05T06:07:51.815477: step 5911, loss 0.537212.
Train: 2018-08-05T06:07:55.331473: step 5912, loss 0.520347.
Train: 2018-08-05T06:07:58.863095: step 5913, loss 0.587657.
Train: 2018-08-05T06:08:02.394718: step 5914, loss 0.528592.
Train: 2018-08-05T06:08:05.910715: step 5915, loss 0.562357.
Train: 2018-08-05T06:08:09.426711: step 5916, loss 0.587797.
Train: 2018-08-05T06:08:12.973960: step 5917, loss 0.672774.
Train: 2018-08-05T06:08:16.489956: step 5918, loss 0.57932.
Train: 2018-08-05T06:08:20.052833: step 5919, loss 0.579301.
Train: 2018-08-05T06:08:23.615708: step 5920, loss 0.58774.
Test: 2018-08-05T06:08:38.695426: step 5920, loss 0.548844.
Train: 2018-08-05T06:08:42.211421: step 5921, loss 0.528586.
Train: 2018-08-05T06:08:45.868057: step 5922, loss 0.596116.
Train: 2018-08-05T06:08:49.399680: step 5923, loss 0.511828.
Train: 2018-08-05T06:08:52.946930: step 5924, loss 0.562374.
Train: 2018-08-05T06:08:56.462926: step 5925, loss 0.562375.
Train: 2018-08-05T06:09:00.025802: step 5926, loss 0.604485.
Train: 2018-08-05T06:09:03.557424: step 5927, loss 0.604451.
Train: 2018-08-05T06:09:07.073421: step 5928, loss 0.545585.
Train: 2018-08-05T06:09:10.651923: step 5929, loss 0.537216.
Train: 2018-08-05T06:09:14.183547: step 5930, loss 0.545614.
Test: 2018-08-05T06:09:29.247636: step 5930, loss 0.548571.
Train: 2018-08-05T06:09:32.763632: step 5931, loss 0.528827.
Train: 2018-08-05T06:09:36.357762: step 5932, loss 0.570786.
Train: 2018-08-05T06:09:39.889385: step 5933, loss 0.49513.
Train: 2018-08-05T06:09:43.436634: step 5934, loss 0.553947.
Train: 2018-08-05T06:09:46.952630: step 5935, loss 0.553919.
Train: 2018-08-05T06:09:50.468627: step 5936, loss 0.452305.
Train: 2018-08-05T06:09:53.984623: step 5937, loss 0.57085.
Train: 2018-08-05T06:09:57.531872: step 5938, loss 0.553801.
Train: 2018-08-05T06:10:01.079122: step 5939, loss 0.64803.
Train: 2018-08-05T06:10:04.579491: step 5940, loss 0.528009.
Test: 2018-08-05T06:10:19.752967: step 5940, loss 0.548173.
Train: 2018-08-05T06:10:23.315844: step 5941, loss 0.588131.
Train: 2018-08-05T06:10:26.847466: step 5942, loss 0.570945.
Train: 2018-08-05T06:10:30.347836: step 5943, loss 0.510636.
Train: 2018-08-05T06:10:33.910713: step 5944, loss 0.588237.
Train: 2018-08-05T06:10:37.473588: step 5945, loss 0.570982.
Train: 2018-08-05T06:10:40.989584: step 5946, loss 0.553693.
Train: 2018-08-05T06:10:44.568088: step 5947, loss 0.605615.
Train: 2018-08-05T06:10:48.130963: step 5948, loss 0.570994.
Train: 2018-08-05T06:10:51.646959: step 5949, loss 0.545046.
Train: 2018-08-05T06:10:55.194209: step 5950, loss 0.588279.
Test: 2018-08-05T06:11:10.289553: step 5950, loss 0.5499.
Train: 2018-08-05T06:11:13.805548: step 5951, loss 0.56234.
Train: 2018-08-05T06:11:17.305918: step 5952, loss 0.484641.
Train: 2018-08-05T06:11:20.837541: step 5953, loss 0.622828.
Train: 2018-08-05T06:11:24.369164: step 5954, loss 0.562339.
Train: 2018-08-05T06:11:27.916413: step 5955, loss 0.562339.
Train: 2018-08-05T06:11:31.463662: step 5956, loss 0.562338.
Train: 2018-08-05T06:11:34.995285: step 5957, loss 0.562337.
Train: 2018-08-05T06:11:38.511281: step 5958, loss 0.502003.
Train: 2018-08-05T06:11:42.011651: step 5959, loss 0.657222.
Train: 2018-08-05T06:11:45.558900: step 5960, loss 0.519266.
Test: 2018-08-05T06:12:00.669870: step 5960, loss 0.548352.
Train: 2018-08-05T06:12:04.201493: step 5961, loss 0.588169.
Train: 2018-08-05T06:12:07.764369: step 5962, loss 0.622557.
Train: 2018-08-05T06:12:11.311619: step 5963, loss 0.493658.
Train: 2018-08-05T06:12:14.827615: step 5964, loss 0.570915.
Train: 2018-08-05T06:12:18.343611: step 5965, loss 0.485162.
Train: 2018-08-05T06:12:21.890860: step 5966, loss 0.622422.
Train: 2018-08-05T06:12:25.438110: step 5967, loss 0.545174.
Train: 2018-08-05T06:12:28.985359: step 5968, loss 0.639562.
Train: 2018-08-05T06:12:32.485729: step 5969, loss 0.562335.
Train: 2018-08-05T06:12:36.017352: step 5970, loss 0.579443.
Test: 2018-08-05T06:12:51.081443: step 5970, loss 0.546506.
Train: 2018-08-05T06:12:54.659944: step 5971, loss 0.5538.
Train: 2018-08-05T06:12:58.191567: step 5972, loss 0.587919.
Train: 2018-08-05T06:13:01.707564: step 5973, loss 0.553832.
Train: 2018-08-05T06:13:05.270439: step 5974, loss 0.553847.
Train: 2018-08-05T06:13:08.880195: step 5975, loss 0.562348.
Train: 2018-08-05T06:13:12.396192: step 5976, loss 0.553869.
Train: 2018-08-05T06:13:15.927814: step 5977, loss 0.553876.
Train: 2018-08-05T06:13:19.475064: step 5978, loss 0.621667.
Train: 2018-08-05T06:13:23.053567: step 5979, loss 0.57928.
Train: 2018-08-05T06:13:26.569563: step 5980, loss 0.545474.
Test: 2018-08-05T06:13:41.727412: step 5980, loss 0.548281.
Train: 2018-08-05T06:13:45.274662: step 5981, loss 0.604545.
Train: 2018-08-05T06:13:48.853165: step 5982, loss 0.469765.
Train: 2018-08-05T06:13:52.369161: step 5983, loss 0.520262.
Train: 2018-08-05T06:13:55.885157: step 5984, loss 0.612974.
Train: 2018-08-05T06:13:59.448033: step 5985, loss 0.587675.
Train: 2018-08-05T06:14:02.917150: step 5986, loss 0.570802.
Train: 2018-08-05T06:14:06.464400: step 5987, loss 0.553943.
Train: 2018-08-05T06:14:10.011648: step 5988, loss 0.537089.
Train: 2018-08-05T06:14:13.558898: step 5989, loss 0.56237.
Train: 2018-08-05T06:14:17.090521: step 5990, loss 0.629856.
Test: 2018-08-05T06:14:32.138986: step 5990, loss 0.549073.
Train: 2018-08-05T06:14:35.670607: step 5991, loss 0.553944.
Train: 2018-08-05T06:14:39.186603: step 5992, loss 0.553952.
Train: 2018-08-05T06:14:42.733853: step 5993, loss 0.612894.
Train: 2018-08-05T06:14:46.312355: step 5994, loss 0.579198.
Train: 2018-08-05T06:14:49.828351: step 5995, loss 0.520411.
Train: 2018-08-05T06:14:53.328721: step 5996, loss 0.545605.
Train: 2018-08-05T06:14:56.875970: step 5997, loss 0.579178.
Train: 2018-08-05T06:15:00.407593: step 5998, loss 0.612747.
Train: 2018-08-05T06:15:03.986096: step 5999, loss 0.595928.
Train: 2018-08-05T06:15:07.517719: step 6000, loss 0.570774.
Test: 2018-08-05T06:15:22.644315: step 6000, loss 0.548655.
Train: 2018-08-05T06:15:28.285536: step 6001, loss 0.612542.
Train: 2018-08-05T06:15:31.801532: step 6002, loss 0.579097.
Train: 2018-08-05T06:15:35.301902: step 6003, loss 0.545823.
Train: 2018-08-05T06:15:38.817898: step 6004, loss 0.55416.
Train: 2018-08-05T06:15:42.427654: step 6005, loss 0.496181.
Train: 2018-08-05T06:15:45.959276: step 6006, loss 0.56246.
Train: 2018-08-05T06:15:49.490899: step 6007, loss 0.620519.
Train: 2018-08-05T06:15:52.991269: step 6008, loss 0.537615.
Train: 2018-08-05T06:15:56.507265: step 6009, loss 0.521081.
Train: 2018-08-05T06:16:00.023261: step 6010, loss 0.520957.
Test: 2018-08-05T06:16:15.056098: step 6010, loss 0.548734.
Train: 2018-08-05T06:16:18.618974: step 6011, loss 0.56244.
Train: 2018-08-05T06:16:22.166224: step 6012, loss 0.520728.
Train: 2018-08-05T06:16:25.744726: step 6013, loss 0.637689.
Train: 2018-08-05T06:16:29.276349: step 6014, loss 0.621016.
Train: 2018-08-05T06:16:32.839225: step 6015, loss 0.512177.
Train: 2018-08-05T06:16:36.370858: step 6016, loss 0.587537.
Train: 2018-08-05T06:16:39.886844: step 6017, loss 0.537247.
Train: 2018-08-05T06:16:43.480974: step 6018, loss 0.512038.
Train: 2018-08-05T06:16:47.059476: step 6019, loss 0.553973.
Train: 2018-08-05T06:16:50.591099: step 6020, loss 0.528663.
Test: 2018-08-05T06:17:05.748950: step 6020, loss 0.547853.
Train: 2018-08-05T06:17:09.280572: step 6021, loss 0.545443.
Train: 2018-08-05T06:17:12.827821: step 6022, loss 0.596254.
Train: 2018-08-05T06:17:16.359444: step 6023, loss 0.51136.
Train: 2018-08-05T06:17:19.891066: step 6024, loss 0.519719.
Train: 2018-08-05T06:17:23.422690: step 6025, loss 0.562299.
Train: 2018-08-05T06:17:26.923059: step 6026, loss 0.571076.
Train: 2018-08-05T06:17:30.485935: step 6027, loss 0.519111.
Train: 2018-08-05T06:17:33.986305: step 6028, loss 0.545205.
Train: 2018-08-05T06:17:37.580434: step 6029, loss 0.484198.
Train: 2018-08-05T06:17:41.112057: step 6030, loss 0.492087.
Test: 2018-08-05T06:17:56.254281: step 6030, loss 0.548048.
Train: 2018-08-05T06:17:59.832783: step 6031, loss 0.589677.
Train: 2018-08-05T06:18:03.348779: step 6032, loss 0.535525.
Train: 2018-08-05T06:18:06.896028: step 6033, loss 0.572903.
Train: 2018-08-05T06:18:10.427651: step 6034, loss 0.607815.
Train: 2018-08-05T06:18:13.959274: step 6035, loss 0.453968.
Train: 2018-08-05T06:18:17.490897: step 6036, loss 0.579845.
Train: 2018-08-05T06:18:21.006893: step 6037, loss 0.581097.
Train: 2018-08-05T06:18:24.538516: step 6038, loss 0.551582.
Train: 2018-08-05T06:18:28.070138: step 6039, loss 0.58203.
Train: 2018-08-05T06:18:29.773444: step 6040, loss 0.386632.
Test: 2018-08-05T06:18:44.868786: step 6040, loss 0.546749.
Train: 2018-08-05T06:18:48.416036: step 6041, loss 0.621286.
Train: 2018-08-05T06:18:51.932032: step 6042, loss 0.57423.
Train: 2018-08-05T06:18:55.463655: step 6043, loss 0.591845.
Train: 2018-08-05T06:18:58.995278: step 6044, loss 0.574817.
Train: 2018-08-05T06:19:02.511274: step 6045, loss 0.62781.
Train: 2018-08-05T06:19:06.042897: step 6046, loss 0.645138.
Train: 2018-08-05T06:19:09.574520: step 6047, loss 0.652031.
Train: 2018-08-05T06:19:13.153022: step 6048, loss 0.535861.
Train: 2018-08-05T06:19:16.684645: step 6049, loss 0.52722.
Train: 2018-08-05T06:19:20.231895: step 6050, loss 0.579881.
Test: 2018-08-05T06:19:35.342864: step 6050, loss 0.547961.
Train: 2018-08-05T06:19:38.905741: step 6051, loss 0.597407.
Train: 2018-08-05T06:19:42.421737: step 6052, loss 0.509941.
Train: 2018-08-05T06:19:45.953359: step 6053, loss 0.527566.
Train: 2018-08-05T06:19:49.500609: step 6054, loss 0.50993.
Train: 2018-08-05T06:19:53.047859: step 6055, loss 0.527709.
Train: 2018-08-05T06:19:56.563855: step 6056, loss 0.509667.
Train: 2018-08-05T06:20:00.142358: step 6057, loss 0.571683.
Train: 2018-08-05T06:20:03.705234: step 6058, loss 0.569924.
Train: 2018-08-05T06:20:07.221230: step 6059, loss 0.572556.
Train: 2018-08-05T06:20:10.815359: step 6060, loss 0.490911.
Test: 2018-08-05T06:20:25.879450: step 6060, loss 0.547134.
Train: 2018-08-05T06:20:29.395445: step 6061, loss 0.579775.
Train: 2018-08-05T06:20:32.942695: step 6062, loss 0.544576.
Train: 2018-08-05T06:20:36.458691: step 6063, loss 0.482464.
Train: 2018-08-05T06:20:39.974687: step 6064, loss 0.55801.
Train: 2018-08-05T06:20:43.506310: step 6065, loss 0.603692.
Train: 2018-08-05T06:20:47.053560: step 6066, loss 0.551022.
Train: 2018-08-05T06:20:50.600809: step 6067, loss 0.550999.
Train: 2018-08-05T06:20:54.101179: step 6068, loss 0.622962.
Train: 2018-08-05T06:20:57.617175: step 6069, loss 0.580389.
Train: 2018-08-05T06:21:01.164424: step 6070, loss 0.569544.
Test: 2018-08-05T06:21:16.306648: step 6070, loss 0.546458.
Train: 2018-08-05T06:21:19.838270: step 6071, loss 0.519142.
Train: 2018-08-05T06:21:23.401146: step 6072, loss 0.59089.
Train: 2018-08-05T06:21:26.948396: step 6073, loss 0.544491.
Train: 2018-08-05T06:21:30.495645: step 6074, loss 0.615528.
Train: 2018-08-05T06:21:33.996015: step 6075, loss 0.579455.
Train: 2018-08-05T06:21:37.527637: step 6076, loss 0.640386.
Train: 2018-08-05T06:21:41.043633: step 6077, loss 0.545256.
Train: 2018-08-05T06:21:44.590883: step 6078, loss 0.604865.
Train: 2018-08-05T06:21:48.153759: step 6079, loss 0.494755.
Train: 2018-08-05T06:21:51.685382: step 6080, loss 0.537127.
Test: 2018-08-05T06:22:06.905738: step 6080, loss 0.549124.
Train: 2018-08-05T06:22:10.452988: step 6081, loss 0.587489.
Train: 2018-08-05T06:22:13.984611: step 6082, loss 0.528938.
Train: 2018-08-05T06:22:17.547490: step 6083, loss 0.612777.
Train: 2018-08-05T06:22:21.079110: step 6084, loss 0.58744.
Train: 2018-08-05T06:22:24.626360: step 6085, loss 0.554065.
Train: 2018-08-05T06:22:28.142355: step 6086, loss 0.579066.
Train: 2018-08-05T06:22:31.642725: step 6087, loss 0.579072.
Train: 2018-08-05T06:22:35.189974: step 6088, loss 0.520865.
Train: 2018-08-05T06:22:38.752850: step 6089, loss 0.487504.
Train: 2018-08-05T06:22:42.268846: step 6090, loss 0.612517.
Test: 2018-08-05T06:22:57.379816: step 6090, loss 0.547699.
Train: 2018-08-05T06:23:00.958319: step 6091, loss 0.4956.
Train: 2018-08-05T06:23:04.505569: step 6092, loss 0.595878.
Train: 2018-08-05T06:23:08.021565: step 6093, loss 0.562401.
Train: 2018-08-05T06:23:11.537561: step 6094, loss 0.637862.
Train: 2018-08-05T06:23:15.069183: step 6095, loss 0.562399.
Train: 2018-08-05T06:23:18.600807: step 6096, loss 0.503775.
Train: 2018-08-05T06:23:22.194936: step 6097, loss 0.612696.
Train: 2018-08-05T06:23:25.726558: step 6098, loss 0.629451.
Train: 2018-08-05T06:23:29.242555: step 6099, loss 0.562406.
Train: 2018-08-05T06:23:32.821058: step 6100, loss 0.545699.
Test: 2018-08-05T06:23:47.900775: step 6100, loss 0.548854.
Train: 2018-08-05T06:23:53.760768: step 6101, loss 0.570769.
Train: 2018-08-05T06:23:57.308017: step 6102, loss 0.595804.
Train: 2018-08-05T06:24:00.886520: step 6103, loss 0.5791.
Train: 2018-08-05T06:24:04.433770: step 6104, loss 0.570762.
Train: 2018-08-05T06:24:07.918513: step 6105, loss 0.57076.
Train: 2018-08-05T06:24:11.465762: step 6106, loss 0.570759.
Train: 2018-08-05T06:24:14.997385: step 6107, loss 0.554175.
Train: 2018-08-05T06:24:18.529007: step 6108, loss 0.587328.
Train: 2018-08-05T06:24:22.076257: step 6109, loss 0.554204.
Train: 2018-08-05T06:24:25.639133: step 6110, loss 0.587298.
Test: 2018-08-05T06:24:40.843864: step 6110, loss 0.548313.
Train: 2018-08-05T06:24:44.406739: step 6111, loss 0.579019.
Train: 2018-08-05T06:24:47.953988: step 6112, loss 0.521234.
Train: 2018-08-05T06:24:51.485611: step 6113, loss 0.488208.
Train: 2018-08-05T06:24:55.001608: step 6114, loss 0.521128.
Train: 2018-08-05T06:24:58.548857: step 6115, loss 0.554167.
Train: 2018-08-05T06:25:02.127360: step 6116, loss 0.545802.
Train: 2018-08-05T06:25:05.658982: step 6117, loss 0.554075.
Train: 2018-08-05T06:25:09.190605: step 6118, loss 0.537287.
Train: 2018-08-05T06:25:12.722228: step 6119, loss 0.612794.
Train: 2018-08-05T06:25:16.253851: step 6120, loss 0.596051.
Test: 2018-08-05T06:25:31.333569: step 6120, loss 0.547519.
Train: 2018-08-05T06:25:34.943324: step 6121, loss 0.553943.
Train: 2018-08-05T06:25:38.568706: step 6122, loss 0.537048.
Train: 2018-08-05T06:25:42.131582: step 6123, loss 0.613089.
Train: 2018-08-05T06:25:45.631952: step 6124, loss 0.553899.
Train: 2018-08-05T06:25:49.210455: step 6125, loss 0.503098.
Train: 2018-08-05T06:25:52.710824: step 6126, loss 0.579315.
Train: 2018-08-05T06:25:56.258073: step 6127, loss 0.570842.
Train: 2018-08-05T06:25:59.789696: step 6128, loss 0.562344.
Train: 2018-08-05T06:26:03.305692: step 6129, loss 0.468697.
Train: 2018-08-05T06:26:06.884195: step 6130, loss 0.587954.
Test: 2018-08-05T06:26:21.963912: step 6130, loss 0.54706.
Train: 2018-08-05T06:26:25.511161: step 6131, loss 0.502438.
Train: 2018-08-05T06:26:29.058411: step 6132, loss 0.579503.
Train: 2018-08-05T06:26:32.590034: step 6133, loss 0.55373.
Train: 2018-08-05T06:26:36.137283: step 6134, loss 0.588215.
Train: 2018-08-05T06:26:39.653279: step 6135, loss 0.596895.
Train: 2018-08-05T06:26:43.184902: step 6136, loss 0.493189.
Train: 2018-08-05T06:26:46.763405: step 6137, loss 0.631629.
Train: 2018-08-05T06:26:50.341908: step 6138, loss 0.596994.
Train: 2018-08-05T06:26:53.873530: step 6139, loss 0.588314.
Train: 2018-08-05T06:26:57.436406: step 6140, loss 0.570988.
Test: 2018-08-05T06:27:12.531750: step 6140, loss 0.547917.
Train: 2018-08-05T06:27:16.063373: step 6141, loss 0.570975.
Train: 2018-08-05T06:27:19.626249: step 6142, loss 0.510597.
Train: 2018-08-05T06:27:23.189125: step 6143, loss 0.510609.
Train: 2018-08-05T06:27:26.705121: step 6144, loss 0.553709.
Train: 2018-08-05T06:27:30.267997: step 6145, loss 0.657341.
Train: 2018-08-05T06:27:33.830874: step 6146, loss 0.493332.
Train: 2018-08-05T06:27:37.378123: step 6147, loss 0.484685.
Train: 2018-08-05T06:27:40.940999: step 6148, loss 0.588273.
Train: 2018-08-05T06:27:44.488248: step 6149, loss 0.467159.
Train: 2018-08-05T06:27:48.004244: step 6150, loss 0.571027.
Test: 2018-08-05T06:28:03.099588: step 6150, loss 0.547428.
Train: 2018-08-05T06:28:06.678091: step 6151, loss 0.562355.
Train: 2018-08-05T06:28:10.256593: step 6152, loss 0.571077.
Train: 2018-08-05T06:28:13.772589: step 6153, loss 0.527453.
Train: 2018-08-05T06:28:17.335465: step 6154, loss 0.562376.
Train: 2018-08-05T06:28:20.835835: step 6155, loss 0.553623.
Train: 2018-08-05T06:28:24.398711: step 6156, loss 0.579939.
Train: 2018-08-05T06:28:27.930334: step 6157, loss 0.579959.
Train: 2018-08-05T06:28:31.493210: step 6158, loss 0.544828.
Train: 2018-08-05T06:28:34.977953: step 6159, loss 0.553611.
Train: 2018-08-05T06:28:38.493949: step 6160, loss 0.694305.
Test: 2018-08-05T06:28:53.604922: step 6160, loss 0.548749.
Train: 2018-08-05T06:28:57.120915: step 6161, loss 0.518527.
Train: 2018-08-05T06:29:00.636911: step 6162, loss 0.553623.
Train: 2018-08-05T06:29:04.215414: step 6163, loss 0.553628.
Train: 2018-08-05T06:29:07.762664: step 6164, loss 0.597339.
Train: 2018-08-05T06:29:11.294286: step 6165, loss 0.510009.
Train: 2018-08-05T06:29:14.810283: step 6166, loss 0.562364.
Train: 2018-08-05T06:29:18.357532: step 6167, loss 0.536216.
Train: 2018-08-05T06:29:21.873528: step 6168, loss 0.492647.
Train: 2018-08-05T06:29:25.405151: step 6169, loss 0.527466.
Train: 2018-08-05T06:29:28.952400: step 6170, loss 0.536152.
Test: 2018-08-05T06:29:44.047745: step 6170, loss 0.548966.
Train: 2018-08-05T06:29:47.548113: step 6171, loss 0.518595.
Train: 2018-08-05T06:29:51.110990: step 6172, loss 0.509716.
Train: 2018-08-05T06:29:54.642612: step 6173, loss 0.580029.
Train: 2018-08-05T06:29:58.174235: step 6174, loss 0.571257.
Train: 2018-08-05T06:30:01.705858: step 6175, loss 0.527059.
Train: 2018-08-05T06:30:05.268734: step 6176, loss 0.60678.
Train: 2018-08-05T06:30:08.831610: step 6177, loss 0.482611.
Train: 2018-08-05T06:30:12.347606: step 6178, loss 0.562481.
Train: 2018-08-05T06:30:15.879229: step 6179, loss 0.598128.
Train: 2018-08-05T06:30:19.395225: step 6180, loss 0.64273.
Test: 2018-08-05T06:30:34.521821: step 6180, loss 0.546802.
Train: 2018-08-05T06:30:38.069071: step 6181, loss 0.535781.
Train: 2018-08-05T06:30:41.600694: step 6182, loss 0.491306.
Train: 2018-08-05T06:30:45.210449: step 6183, loss 0.544685.
Train: 2018-08-05T06:30:48.742073: step 6184, loss 0.553588.
Train: 2018-08-05T06:30:52.242442: step 6185, loss 0.553588.
Train: 2018-08-05T06:30:55.758439: step 6186, loss 0.651689.
Train: 2018-08-05T06:30:59.290061: step 6187, loss 0.50016.
Train: 2018-08-05T06:31:02.884191: step 6188, loss 0.580294.
Train: 2018-08-05T06:31:06.431440: step 6189, loss 0.606954.
Train: 2018-08-05T06:31:09.947436: step 6190, loss 0.606856.
Test: 2018-08-05T06:31:24.980274: step 6190, loss 0.546642.
Train: 2018-08-05T06:31:26.730458: step 6191, loss 0.581336.
Train: 2018-08-05T06:31:30.262081: step 6192, loss 0.544771.
Train: 2018-08-05T06:31:33.762450: step 6193, loss 0.500783.
Train: 2018-08-05T06:31:37.309699: step 6194, loss 0.53602.
Train: 2018-08-05T06:31:40.856949: step 6195, loss 0.597553.
Train: 2018-08-05T06:31:44.372945: step 6196, loss 0.597494.
Train: 2018-08-05T06:31:47.920195: step 6197, loss 0.571138.
Train: 2018-08-05T06:31:51.467444: step 6198, loss 0.606057.
Train: 2018-08-05T06:31:54.952187: step 6199, loss 0.440412.
Train: 2018-08-05T06:31:58.530690: step 6200, loss 0.579774.
Test: 2018-08-05T06:32:13.594780: step 6200, loss 0.547624.
Train: 2018-08-05T06:32:19.314133: step 6201, loss 0.562356.
Train: 2018-08-05T06:32:22.830129: step 6202, loss 0.579744.
Train: 2018-08-05T06:32:26.330499: step 6203, loss 0.527613.
Train: 2018-08-05T06:32:29.846495: step 6204, loss 0.544986.
Train: 2018-08-05T06:32:33.331238: step 6205, loss 0.588396.
Train: 2018-08-05T06:32:36.847234: step 6206, loss 0.605726.
Train: 2018-08-05T06:32:40.394484: step 6207, loss 0.579668.
Train: 2018-08-05T06:32:43.910479: step 6208, loss 0.527758.
Train: 2018-08-05T06:32:47.426476: step 6209, loss 0.545066.
Train: 2018-08-05T06:32:50.942472: step 6210, loss 0.527812.
Test: 2018-08-05T06:33:06.084697: step 6210, loss 0.546133.
Train: 2018-08-05T06:33:09.631945: step 6211, loss 0.510542.
Train: 2018-08-05T06:33:13.210448: step 6212, loss 0.65741.
Train: 2018-08-05T06:33:16.726443: step 6213, loss 0.588241.
Train: 2018-08-05T06:33:20.258067: step 6214, loss 0.588199.
Train: 2018-08-05T06:33:23.758436: step 6215, loss 0.553732.
Train: 2018-08-05T06:33:27.274433: step 6216, loss 0.656824.
Train: 2018-08-05T06:33:30.837308: step 6217, loss 0.570896.
Train: 2018-08-05T06:33:34.400185: step 6218, loss 0.528215.
Train: 2018-08-05T06:33:37.994314: step 6219, loss 0.604902.
Train: 2018-08-05T06:33:41.525936: step 6220, loss 0.553863.
Test: 2018-08-05T06:33:56.590026: step 6220, loss 0.547829.
Train: 2018-08-05T06:34:00.137276: step 6221, loss 0.570821.
Train: 2018-08-05T06:34:03.668899: step 6222, loss 0.570809.
Train: 2018-08-05T06:34:07.200522: step 6223, loss 0.553947.
Train: 2018-08-05T06:34:10.732144: step 6224, loss 0.55397.
Train: 2018-08-05T06:34:14.341901: step 6225, loss 0.595983.
Train: 2018-08-05T06:34:17.873523: step 6226, loss 0.537247.
Train: 2018-08-05T06:34:21.420773: step 6227, loss 0.579151.
Train: 2018-08-05T06:34:24.968022: step 6228, loss 0.595864.
Train: 2018-08-05T06:34:28.499645: step 6229, loss 0.595814.
Train: 2018-08-05T06:34:32.031268: step 6230, loss 0.57911.
Test: 2018-08-05T06:34:47.157866: step 6230, loss 0.547994.
Train: 2018-08-05T06:34:50.705114: step 6231, loss 0.579072.
Train: 2018-08-05T06:34:54.283617: step 6232, loss 0.521004.
Train: 2018-08-05T06:34:57.815239: step 6233, loss 0.579041.
Train: 2018-08-05T06:35:01.331236: step 6234, loss 0.570756.
Train: 2018-08-05T06:35:04.909738: step 6235, loss 0.603822.
Train: 2018-08-05T06:35:08.410109: step 6236, loss 0.562504.
Train: 2018-08-05T06:35:11.972984: step 6237, loss 0.513072.
Train: 2018-08-05T06:35:15.488980: step 6238, loss 0.628446.
Train: 2018-08-05T06:35:19.083109: step 6239, loss 0.578988.
Train: 2018-08-05T06:35:22.583479: step 6240, loss 0.595417.
Test: 2018-08-05T06:35:37.741330: step 6240, loss 0.548476.
Train: 2018-08-05T06:35:41.351085: step 6241, loss 0.611785.
Train: 2018-08-05T06:35:44.867081: step 6242, loss 0.578949.
Train: 2018-08-05T06:35:48.414331: step 6243, loss 0.521793.
Train: 2018-08-05T06:35:51.945953: step 6244, loss 0.538156.
Train: 2018-08-05T06:35:55.446323: step 6245, loss 0.619694.
Train: 2018-08-05T06:35:59.009199: step 6246, loss 0.578922.
Train: 2018-08-05T06:36:02.540821: step 6247, loss 0.521986.
Train: 2018-08-05T06:36:06.072444: step 6248, loss 0.481316.
Train: 2018-08-05T06:36:09.635321: step 6249, loss 0.562623.
Train: 2018-08-05T06:36:13.182570: step 6250, loss 0.595279.
Test: 2018-08-05T06:36:28.387300: step 6250, loss 0.548731.
Train: 2018-08-05T06:36:31.887670: step 6251, loss 0.529856.
Train: 2018-08-05T06:36:35.403666: step 6252, loss 0.55436.
Train: 2018-08-05T06:36:38.904035: step 6253, loss 0.652969.
Train: 2018-08-05T06:36:42.435658: step 6254, loss 0.578982.
Train: 2018-08-05T06:36:46.029787: step 6255, loss 0.537862.
Train: 2018-08-05T06:36:49.561410: step 6256, loss 0.578988.
Train: 2018-08-05T06:36:53.093033: step 6257, loss 0.595463.
Train: 2018-08-05T06:36:56.624656: step 6258, loss 0.529585.
Train: 2018-08-05T06:37:00.156278: step 6259, loss 0.628448.
Train: 2018-08-05T06:37:03.719154: step 6260, loss 0.504862.
Test: 2018-08-05T06:37:18.783245: step 6260, loss 0.549311.
Train: 2018-08-05T06:37:22.314868: step 6261, loss 0.546021.
Train: 2018-08-05T06:37:25.893370: step 6262, loss 0.603785.
Train: 2018-08-05T06:37:29.409366: step 6263, loss 0.545972.
Train: 2018-08-05T06:37:32.925363: step 6264, loss 0.554217.
Train: 2018-08-05T06:37:36.472612: step 6265, loss 0.537636.
Train: 2018-08-05T06:37:40.004235: step 6266, loss 0.537576.
Train: 2018-08-05T06:37:43.551484: step 6267, loss 0.55413.
Train: 2018-08-05T06:37:47.129987: step 6268, loss 0.52075.
Train: 2018-08-05T06:37:50.661610: step 6269, loss 0.595862.
Train: 2018-08-05T06:37:54.161979: step 6270, loss 0.637838.
Test: 2018-08-05T06:38:09.257324: step 6270, loss 0.549547.
Train: 2018-08-05T06:38:12.788946: step 6271, loss 0.595939.
Train: 2018-08-05T06:38:16.383075: step 6272, loss 0.545628.
Train: 2018-08-05T06:38:19.945951: step 6273, loss 0.554009.
Train: 2018-08-05T06:38:23.477574: step 6274, loss 0.654683.
Train: 2018-08-05T06:38:26.977943: step 6275, loss 0.587531.
Train: 2018-08-05T06:38:30.509566: step 6276, loss 0.537327.
Train: 2018-08-05T06:38:34.025562: step 6277, loss 0.529005.
Train: 2018-08-05T06:38:37.541558: step 6278, loss 0.595827.
Train: 2018-08-05T06:38:41.057554: step 6279, loss 0.545727.
Train: 2018-08-05T06:38:44.589177: step 6280, loss 0.462273.
Test: 2018-08-05T06:38:59.731401: step 6280, loss 0.548246.
Train: 2018-08-05T06:39:03.325530: step 6281, loss 0.612596.
Train: 2018-08-05T06:39:06.841526: step 6282, loss 0.55403.
Train: 2018-08-05T06:39:10.357522: step 6283, loss 0.48695.
Train: 2018-08-05T06:39:13.904772: step 6284, loss 0.553976.
Train: 2018-08-05T06:39:17.452021: step 6285, loss 0.596092.
Train: 2018-08-05T06:39:20.983644: step 6286, loss 0.469459.
Train: 2018-08-05T06:39:24.530894: step 6287, loss 0.511486.
Train: 2018-08-05T06:39:28.078143: step 6288, loss 0.528281.
Train: 2018-08-05T06:39:31.609765: step 6289, loss 0.630791.
Train: 2018-08-05T06:39:35.157015: step 6290, loss 0.528012.
Test: 2018-08-05T06:39:50.221106: step 6290, loss 0.547563.
Train: 2018-08-05T06:39:53.737101: step 6291, loss 0.527903.
Train: 2018-08-05T06:39:57.268724: step 6292, loss 0.570979.
Train: 2018-08-05T06:40:00.831600: step 6293, loss 0.60567.
Train: 2018-08-05T06:40:04.316343: step 6294, loss 0.510272.
Train: 2018-08-05T06:40:07.863593: step 6295, loss 0.588461.
Train: 2018-08-05T06:40:11.410842: step 6296, loss 0.518783.
Train: 2018-08-05T06:40:14.942465: step 6297, loss 0.466278.
Train: 2018-08-05T06:40:18.520968: step 6298, loss 0.606237.
Train: 2018-08-05T06:40:22.036964: step 6299, loss 0.588776.
Train: 2018-08-05T06:40:25.552961: step 6300, loss 0.535995.
Test: 2018-08-05T06:40:40.773317: step 6300, loss 0.547893.
Train: 2018-08-05T06:40:46.430164: step 6301, loss 0.500676.
Train: 2018-08-05T06:40:49.946160: step 6302, loss 0.597818.
Train: 2018-08-05T06:40:53.430903: step 6303, loss 0.571309.
Train: 2018-08-05T06:40:56.993779: step 6304, loss 0.509255.
Train: 2018-08-05T06:41:00.525402: step 6305, loss 0.598006.
Train: 2018-08-05T06:41:04.041398: step 6306, loss 0.526919.
Train: 2018-08-05T06:41:07.557394: step 6307, loss 0.509087.
Train: 2018-08-05T06:41:11.089017: step 6308, loss 0.509001.
Train: 2018-08-05T06:41:14.589386: step 6309, loss 0.651933.
Train: 2018-08-05T06:41:18.199142: step 6310, loss 0.571476.
Test: 2018-08-05T06:41:33.310113: step 6310, loss 0.548215.
Train: 2018-08-05T06:41:36.872988: step 6311, loss 0.517819.
Train: 2018-08-05T06:41:40.357731: step 6312, loss 0.598329.
Train: 2018-08-05T06:41:43.873727: step 6313, loss 0.598311.
Train: 2018-08-05T06:41:47.374097: step 6314, loss 0.544656.
Train: 2018-08-05T06:41:50.890093: step 6315, loss 0.526817.
Train: 2018-08-05T06:41:54.421716: step 6316, loss 0.508988.
Train: 2018-08-05T06:41:57.968965: step 6317, loss 0.553588.
Train: 2018-08-05T06:42:01.484961: step 6318, loss 0.642866.
Train: 2018-08-05T06:42:05.016584: step 6319, loss 0.500099.
Train: 2018-08-05T06:42:08.563833: step 6320, loss 0.580326.
Test: 2018-08-05T06:42:23.643551: step 6320, loss 0.547006.
Train: 2018-08-05T06:42:27.159547: step 6321, loss 0.553588.
Train: 2018-08-05T06:42:30.769302: step 6322, loss 0.589181.
Train: 2018-08-05T06:42:34.316552: step 6323, loss 0.562465.
Train: 2018-08-05T06:42:37.816922: step 6324, loss 0.535832.
Train: 2018-08-05T06:42:41.379797: step 6325, loss 0.518118.
Train: 2018-08-05T06:42:44.911420: step 6326, loss 0.562325.
Train: 2018-08-05T06:42:48.458669: step 6327, loss 0.606888.
Train: 2018-08-05T06:42:51.990292: step 6328, loss 0.500935.
Train: 2018-08-05T06:42:55.537542: step 6329, loss 0.570974.
Train: 2018-08-05T06:42:59.037911: step 6330, loss 0.624386.
Test: 2018-08-05T06:43:14.195761: step 6330, loss 0.547748.
Train: 2018-08-05T06:43:17.774264: step 6331, loss 0.52669.
Train: 2018-08-05T06:43:21.290261: step 6332, loss 0.55384.
Train: 2018-08-05T06:43:24.806257: step 6333, loss 0.545107.
Train: 2018-08-05T06:43:28.384759: step 6334, loss 0.562018.
Train: 2018-08-05T06:43:31.963262: step 6335, loss 0.527288.
Train: 2018-08-05T06:43:35.526138: step 6336, loss 0.553516.
Train: 2018-08-05T06:43:39.073387: step 6337, loss 0.605274.
Train: 2018-08-05T06:43:42.620637: step 6338, loss 0.597607.
Train: 2018-08-05T06:43:46.214766: step 6339, loss 0.546962.
Train: 2018-08-05T06:43:49.746389: step 6340, loss 0.502193.
Test: 2018-08-05T06:44:04.919866: step 6340, loss 0.546686.
Train: 2018-08-05T06:44:08.420235: step 6341, loss 0.54505.
Train: 2018-08-05T06:44:10.123540: step 6342, loss 0.580752.
Train: 2018-08-05T06:44:13.670790: step 6343, loss 0.492456.
Train: 2018-08-05T06:44:17.218039: step 6344, loss 0.553604.
Train: 2018-08-05T06:44:20.780915: step 6345, loss 0.562211.
Train: 2018-08-05T06:44:24.343791: step 6346, loss 0.571221.
Train: 2018-08-05T06:44:27.891041: step 6347, loss 0.571272.
Train: 2018-08-05T06:44:31.469544: step 6348, loss 0.553608.
Train: 2018-08-05T06:44:34.985540: step 6349, loss 0.527116.
Train: 2018-08-05T06:44:38.501536: step 6350, loss 0.53592.
Test: 2018-08-05T06:44:53.549999: step 6350, loss 0.546037.
Train: 2018-08-05T06:44:57.050369: step 6351, loss 0.500513.
Train: 2018-08-05T06:45:00.566365: step 6352, loss 0.633373.
Train: 2018-08-05T06:45:04.144867: step 6353, loss 0.571324.
Train: 2018-08-05T06:45:07.692117: step 6354, loss 0.642238.
Train: 2018-08-05T06:45:11.223740: step 6355, loss 0.571289.
Train: 2018-08-05T06:45:14.739736: step 6356, loss 0.580085.
Train: 2018-08-05T06:45:18.240105: step 6357, loss 0.535989.
Train: 2018-08-05T06:45:21.740475: step 6358, loss 0.65032.
Train: 2018-08-05T06:45:25.287725: step 6359, loss 0.579905.
Train: 2018-08-05T06:45:28.850600: step 6360, loss 0.649653.
Test: 2018-08-05T06:45:43.992824: step 6360, loss 0.547449.
Train: 2018-08-05T06:45:47.555699: step 6361, loss 0.56235.
Train: 2018-08-05T06:45:51.071696: step 6362, loss 0.631469.
Train: 2018-08-05T06:45:54.603319: step 6363, loss 0.656831.
Train: 2018-08-05T06:45:58.119315: step 6364, loss 0.545285.
Train: 2018-08-05T06:46:01.682191: step 6365, loss 0.503035.
Train: 2018-08-05T06:46:05.291948: step 6366, loss 0.545493.
Train: 2018-08-05T06:46:08.792316: step 6367, loss 0.562382.
Train: 2018-08-05T06:46:12.339566: step 6368, loss 0.554016.
Train: 2018-08-05T06:46:15.855562: step 6369, loss 0.545678.
Train: 2018-08-05T06:46:19.371559: step 6370, loss 0.562421.
Test: 2018-08-05T06:46:34.545036: step 6370, loss 0.549114.
Train: 2018-08-05T06:46:38.107911: step 6371, loss 0.57911.
Train: 2018-08-05T06:46:41.670787: step 6372, loss 0.537595.
Train: 2018-08-05T06:46:45.218037: step 6373, loss 0.529186.
Train: 2018-08-05T06:46:48.734033: step 6374, loss 0.579146.
Train: 2018-08-05T06:46:52.250029: step 6375, loss 0.537442.
Train: 2018-08-05T06:46:55.828532: step 6376, loss 0.620884.
Train: 2018-08-05T06:46:59.391408: step 6377, loss 0.570762.
Train: 2018-08-05T06:47:02.938658: step 6378, loss 0.512592.
Train: 2018-08-05T06:47:06.454653: step 6379, loss 0.504237.
Train: 2018-08-05T06:47:09.986276: step 6380, loss 0.579097.
Test: 2018-08-05T06:47:25.081619: step 6380, loss 0.548675.
Train: 2018-08-05T06:47:28.613243: step 6381, loss 0.512344.
Train: 2018-08-05T06:47:32.129239: step 6382, loss 0.495456.
Train: 2018-08-05T06:47:35.660861: step 6383, loss 0.570788.
Train: 2018-08-05T06:47:39.223737: step 6384, loss 0.604529.
Train: 2018-08-05T06:47:42.755360: step 6385, loss 0.562362.
Train: 2018-08-05T06:47:46.302609: step 6386, loss 0.562355.
Train: 2018-08-05T06:47:49.818605: step 6387, loss 0.587803.
Train: 2018-08-05T06:47:53.334603: step 6388, loss 0.545358.
Train: 2018-08-05T06:47:56.881851: step 6389, loss 0.519809.
Train: 2018-08-05T06:48:00.429101: step 6390, loss 0.604971.
Test: 2018-08-05T06:48:15.540070: step 6390, loss 0.546708.
Train: 2018-08-05T06:48:19.134200: step 6391, loss 0.605017.
Train: 2018-08-05T06:48:22.665824: step 6392, loss 0.579411.
Train: 2018-08-05T06:48:26.166192: step 6393, loss 0.511135.
Train: 2018-08-05T06:48:29.729069: step 6394, loss 0.553797.
Train: 2018-08-05T06:48:33.291944: step 6395, loss 0.485395.
Train: 2018-08-05T06:48:36.839194: step 6396, loss 0.570905.
Train: 2018-08-05T06:48:40.339564: step 6397, loss 0.62245.
Train: 2018-08-05T06:48:44.011826: step 6398, loss 0.588112.
Train: 2018-08-05T06:48:47.496569: step 6399, loss 0.579518.
Train: 2018-08-05T06:48:51.059445: step 6400, loss 0.562335.
Test: 2018-08-05T06:49:06.217298: step 6400, loss 0.547408.
Train: 2018-08-05T06:49:11.952276: step 6401, loss 0.51942.
Train: 2018-08-05T06:49:15.515151: step 6402, loss 0.510814.
Train: 2018-08-05T06:49:19.031148: step 6403, loss 0.622533.
Train: 2018-08-05T06:49:22.531517: step 6404, loss 0.536536.
Train: 2018-08-05T06:49:26.047513: step 6405, loss 0.519314.
Train: 2018-08-05T06:49:29.641643: step 6406, loss 0.536489.
Train: 2018-08-05T06:49:33.204519: step 6407, loss 0.527815.
Train: 2018-08-05T06:49:36.783022: step 6408, loss 0.579642.
Train: 2018-08-05T06:49:40.314645: step 6409, loss 0.562345.
Train: 2018-08-05T06:49:43.846267: step 6410, loss 0.597049.
Test: 2018-08-05T06:49:58.910358: step 6410, loss 0.548254.
Train: 2018-08-05T06:50:02.426354: step 6411, loss 0.544992.
Train: 2018-08-05T06:50:05.911097: step 6412, loss 0.536298.
Train: 2018-08-05T06:50:09.442719: step 6413, loss 0.579741.
Train: 2018-08-05T06:50:13.021222: step 6414, loss 0.562355.
Train: 2018-08-05T06:50:16.584099: step 6415, loss 0.579758.
Train: 2018-08-05T06:50:20.146974: step 6416, loss 0.579755.
Train: 2018-08-05T06:50:23.662971: step 6417, loss 0.55366.
Train: 2018-08-05T06:50:27.178967: step 6418, loss 0.562353.
Train: 2018-08-05T06:50:30.726216: step 6419, loss 0.562352.
Train: 2018-08-05T06:50:34.304719: step 6420, loss 0.484211.
Test: 2018-08-05T06:50:49.368809: step 6420, loss 0.546637.
Train: 2018-08-05T06:50:52.947312: step 6421, loss 0.553661.
Train: 2018-08-05T06:50:56.478934: step 6422, loss 0.553655.
Train: 2018-08-05T06:50:59.994930: step 6423, loss 0.623333.
Train: 2018-08-05T06:51:03.526553: step 6424, loss 0.571065.
Train: 2018-08-05T06:51:07.089430: step 6425, loss 0.536253.
Train: 2018-08-05T06:51:10.636679: step 6426, loss 0.640657.
Train: 2018-08-05T06:51:14.183928: step 6427, loss 0.51893.
Train: 2018-08-05T06:51:17.715551: step 6428, loss 0.579702.
Train: 2018-08-05T06:51:21.215921: step 6429, loss 0.562345.
Train: 2018-08-05T06:51:24.778797: step 6430, loss 0.571.
Test: 2018-08-05T06:51:39.874141: step 6430, loss 0.547504.
Train: 2018-08-05T06:51:43.421390: step 6431, loss 0.588277.
Train: 2018-08-05T06:51:46.968639: step 6432, loss 0.527817.
Train: 2018-08-05T06:51:50.547141: step 6433, loss 0.579582.
Train: 2018-08-05T06:51:54.110018: step 6434, loss 0.493444.
Train: 2018-08-05T06:51:57.626014: step 6435, loss 0.562337.
Train: 2018-08-05T06:52:01.188890: step 6436, loss 0.588186.
Train: 2018-08-05T06:52:04.736140: step 6437, loss 0.562337.
Train: 2018-08-05T06:52:08.299016: step 6438, loss 0.579556.
Train: 2018-08-05T06:52:11.893145: step 6439, loss 0.596748.
Train: 2018-08-05T06:52:15.393515: step 6440, loss 0.570925.
Test: 2018-08-05T06:52:30.473231: step 6440, loss 0.546825.
Train: 2018-08-05T06:52:33.973600: step 6441, loss 0.605223.
Train: 2018-08-05T06:52:37.489597: step 6442, loss 0.656468.
Train: 2018-08-05T06:52:41.021220: step 6443, loss 0.587906.
Train: 2018-08-05T06:52:44.568469: step 6444, loss 0.587806.
Train: 2018-08-05T06:52:48.115718: step 6445, loss 0.520124.
Train: 2018-08-05T06:52:51.631715: step 6446, loss 0.604489.
Train: 2018-08-05T06:52:55.163337: step 6447, loss 0.629531.
Train: 2018-08-05T06:52:58.726214: step 6448, loss 0.595828.
Train: 2018-08-05T06:53:02.257836: step 6449, loss 0.529197.
Train: 2018-08-05T06:53:05.789459: step 6450, loss 0.479641.
Test: 2018-08-05T06:53:20.962937: step 6450, loss 0.548473.
Train: 2018-08-05T06:53:24.525812: step 6451, loss 0.579031.
Train: 2018-08-05T06:53:28.041808: step 6452, loss 0.562491.
Train: 2018-08-05T06:53:31.557804: step 6453, loss 0.570756.
Train: 2018-08-05T06:53:35.136307: step 6454, loss 0.546007.
Train: 2018-08-05T06:53:38.667929: step 6455, loss 0.570756.
Train: 2018-08-05T06:53:42.262059: step 6456, loss 0.603741.
Train: 2018-08-05T06:53:45.793682: step 6457, loss 0.562519.
Train: 2018-08-05T06:53:49.309678: step 6458, loss 0.537828.
Train: 2018-08-05T06:53:52.825675: step 6459, loss 0.513123.
Train: 2018-08-05T06:53:56.404177: step 6460, loss 0.504794.
Test: 2018-08-05T06:54:11.593280: step 6460, loss 0.548678.
Train: 2018-08-05T06:54:15.093651: step 6461, loss 0.6121.
Train: 2018-08-05T06:54:18.656526: step 6462, loss 0.52935.
Train: 2018-08-05T06:54:22.203775: step 6463, loss 0.537556.
Train: 2018-08-05T06:54:25.735398: step 6464, loss 0.487525.
Train: 2018-08-05T06:54:29.235768: step 6465, loss 0.537332.
Train: 2018-08-05T06:54:32.783017: step 6466, loss 0.595979.
Train: 2018-08-05T06:54:36.283386: step 6467, loss 0.52024.
Train: 2018-08-05T06:54:39.799382: step 6468, loss 0.528515.
Train: 2018-08-05T06:54:43.346632: step 6469, loss 0.528352.
Train: 2018-08-05T06:54:46.893881: step 6470, loss 0.553799.
Test: 2018-08-05T06:55:01.973599: step 6470, loss 0.548406.
Train: 2018-08-05T06:55:05.552101: step 6471, loss 0.605218.
Train: 2018-08-05T06:55:09.208737: step 6472, loss 0.588143.
Train: 2018-08-05T06:55:12.771614: step 6473, loss 0.501991.
Train: 2018-08-05T06:55:16.334489: step 6474, loss 0.622877.
Train: 2018-08-05T06:55:19.881739: step 6475, loss 0.545023.
Train: 2018-08-05T06:55:23.460242: step 6476, loss 0.501624.
Train: 2018-08-05T06:55:27.007491: step 6477, loss 0.571053.
Train: 2018-08-05T06:55:30.554741: step 6478, loss 0.553646.
Train: 2018-08-05T06:55:34.070736: step 6479, loss 0.4925.
Train: 2018-08-05T06:55:37.602359: step 6480, loss 0.536101.
Test: 2018-08-05T06:55:52.728957: step 6480, loss 0.547924.
Train: 2018-08-05T06:55:56.323086: step 6481, loss 0.606346.
Train: 2018-08-05T06:55:59.885961: step 6482, loss 0.5448.
Train: 2018-08-05T06:56:03.433211: step 6483, loss 0.580066.
Train: 2018-08-05T06:56:06.949208: step 6484, loss 0.588927.
Train: 2018-08-05T06:56:10.480830: step 6485, loss 0.518256.
Train: 2018-08-05T06:56:13.996826: step 6486, loss 0.615507.
Train: 2018-08-05T06:56:17.544076: step 6487, loss 0.606647.
Train: 2018-08-05T06:56:21.106951: step 6488, loss 0.562428.
Train: 2018-08-05T06:56:24.638574: step 6489, loss 0.580055.
Train: 2018-08-05T06:56:28.138944: step 6490, loss 0.491988.
Test: 2018-08-05T06:56:43.249914: step 6490, loss 0.547508.
Train: 2018-08-05T06:56:46.765910: step 6491, loss 0.588807.
Train: 2018-08-05T06:56:50.344412: step 6492, loss 0.536026.
Train: 2018-08-05T06:56:52.078971: step 6493, loss 0.468661.
Train: 2018-08-05T06:56:55.626220: step 6494, loss 0.580007.
Train: 2018-08-05T06:56:59.204723: step 6495, loss 0.553605.
Train: 2018-08-05T06:57:02.720719: step 6496, loss 0.580041.
Train: 2018-08-05T06:57:06.221089: step 6497, loss 0.580044.
Train: 2018-08-05T06:57:09.768338: step 6498, loss 0.588844.
Train: 2018-08-05T06:57:13.299961: step 6499, loss 0.474399.
Train: 2018-08-05T06:57:16.862837: step 6500, loss 0.491956.
Test: 2018-08-05T06:57:31.973809: step 6500, loss 0.548296.
Train: 2018-08-05T06:57:37.677534: step 6501, loss 0.571248.
Train: 2018-08-05T06:57:41.193530: step 6502, loss 0.562433.
Train: 2018-08-05T06:57:44.693900: step 6503, loss 0.527058.
Train: 2018-08-05T06:57:48.209896: step 6504, loss 0.633325.
Train: 2018-08-05T06:57:51.772772: step 6505, loss 0.633306.
Train: 2018-08-05T06:57:55.288768: step 6506, loss 0.553596.
Train: 2018-08-05T06:57:58.789138: step 6507, loss 0.641864.
Train: 2018-08-05T06:58:02.336387: step 6508, loss 0.527214.
Train: 2018-08-05T06:58:05.868010: step 6509, loss 0.562393.
Train: 2018-08-05T06:58:09.399633: step 6510, loss 0.527351.
Test: 2018-08-05T06:58:24.510602: step 6510, loss 0.548576.
Train: 2018-08-05T06:58:28.104732: step 6511, loss 0.439945.
Train: 2018-08-05T06:58:31.667608: step 6512, loss 0.536116.
Train: 2018-08-05T06:58:35.183604: step 6513, loss 0.51855.
Train: 2018-08-05T06:58:38.730853: step 6514, loss 0.579968.
Train: 2018-08-05T06:58:42.262476: step 6515, loss 0.48323.
Train: 2018-08-05T06:58:45.778473: step 6516, loss 0.677092.
Train: 2018-08-05T06:58:49.372602: step 6517, loss 0.491859.
Train: 2018-08-05T06:58:52.904225: step 6518, loss 0.562429.
Train: 2018-08-05T06:58:56.420221: step 6519, loss 0.588952.
Train: 2018-08-05T06:58:59.936217: step 6520, loss 0.562436.
Test: 2018-08-05T06:59:15.047188: step 6520, loss 0.546655.
Train: 2018-08-05T06:59:18.578810: step 6521, loss 0.615475.
Train: 2018-08-05T06:59:22.141686: step 6522, loss 0.571256.
Train: 2018-08-05T06:59:25.688936: step 6523, loss 0.641757.
Train: 2018-08-05T06:59:29.251812: step 6524, loss 0.588764.
Train: 2018-08-05T06:59:32.783435: step 6525, loss 0.483561.
Train: 2018-08-05T06:59:36.315057: step 6526, loss 0.597349.
Train: 2018-08-05T06:59:39.831065: step 6527, loss 0.571089.
Train: 2018-08-05T06:59:43.347050: step 6528, loss 0.588465.
Train: 2018-08-05T06:59:46.894299: step 6529, loss 0.518955.
Train: 2018-08-05T06:59:50.472802: step 6530, loss 0.553681.
Test: 2018-08-05T07:00:05.552519: step 6530, loss 0.547494.
Train: 2018-08-05T07:00:09.115395: step 6531, loss 0.493125.
Train: 2018-08-05T07:00:12.647017: step 6532, loss 0.570996.
Train: 2018-08-05T07:00:16.178640: step 6533, loss 0.596951.
Train: 2018-08-05T07:00:19.694636: step 6534, loss 0.536408.
Train: 2018-08-05T07:00:23.210633: step 6535, loss 0.510492.
Train: 2018-08-05T07:00:26.773509: step 6536, loss 0.570989.
Train: 2018-08-05T07:00:30.320758: step 6537, loss 0.570993.
Train: 2018-08-05T07:00:33.836754: step 6538, loss 0.545037.
Train: 2018-08-05T07:00:37.337124: step 6539, loss 0.545031.
Train: 2018-08-05T07:00:40.900000: step 6540, loss 0.622978.
Test: 2018-08-05T07:00:56.182866: step 6540, loss 0.548286.
Train: 2018-08-05T07:00:59.745739: step 6541, loss 0.579654.
Train: 2018-08-05T07:01:03.277362: step 6542, loss 0.570987.
Train: 2018-08-05T07:01:06.840238: step 6543, loss 0.614151.
Train: 2018-08-05T07:01:10.324981: step 6544, loss 0.553721.
Train: 2018-08-05T07:01:13.872230: step 6545, loss 0.562335.
Train: 2018-08-05T07:01:17.419480: step 6546, loss 0.64818.
Train: 2018-08-05T07:01:20.919850: step 6547, loss 0.528116.
Train: 2018-08-05T07:01:24.498352: step 6548, loss 0.51113.
Train: 2018-08-05T07:01:28.045601: step 6549, loss 0.477081.
Train: 2018-08-05T07:01:31.577225: step 6550, loss 0.553805.
Test: 2018-08-05T07:01:46.703822: step 6550, loss 0.547285.
Train: 2018-08-05T07:01:50.235444: step 6551, loss 0.622134.
Train: 2018-08-05T07:01:53.735813: step 6552, loss 0.562338.
Train: 2018-08-05T07:01:57.298689: step 6553, loss 0.596483.
Train: 2018-08-05T07:02:00.861565: step 6554, loss 0.468539.
Train: 2018-08-05T07:02:04.455695: step 6555, loss 0.553802.
Train: 2018-08-05T07:02:07.971691: step 6556, loss 0.545244.
Train: 2018-08-05T07:02:11.487687: step 6557, loss 0.630804.
Train: 2018-08-05T07:02:15.066189: step 6558, loss 0.519552.
Train: 2018-08-05T07:02:18.597812: step 6559, loss 0.519521.
Train: 2018-08-05T07:02:22.160689: step 6560, loss 0.579487.
Test: 2018-08-05T07:02:37.271659: step 6560, loss 0.549183.
Train: 2018-08-05T07:02:40.803282: step 6561, loss 0.57092.
Train: 2018-08-05T07:02:44.334904: step 6562, loss 0.570926.
Train: 2018-08-05T07:02:47.866527: step 6563, loss 0.510771.
Train: 2018-08-05T07:02:51.398150: step 6564, loss 0.588155.
Train: 2018-08-05T07:02:54.914146: step 6565, loss 0.553724.
Train: 2018-08-05T07:02:58.461396: step 6566, loss 0.657147.
Train: 2018-08-05T07:03:01.993018: step 6567, loss 0.596765.
Train: 2018-08-05T07:03:05.509014: step 6568, loss 0.631057.
Train: 2018-08-05T07:03:09.025010: step 6569, loss 0.570898.
Train: 2018-08-05T07:03:12.509753: step 6570, loss 0.536734.
Test: 2018-08-05T07:03:27.636350: step 6570, loss 0.548513.
Train: 2018-08-05T07:03:31.183600: step 6571, loss 0.681556.
Train: 2018-08-05T07:03:34.855862: step 6572, loss 0.562353.
Train: 2018-08-05T07:03:38.387485: step 6573, loss 0.553925.
Train: 2018-08-05T07:03:41.934735: step 6574, loss 0.596025.
Train: 2018-08-05T07:03:45.466357: step 6575, loss 0.587535.
Train: 2018-08-05T07:03:48.982353: step 6576, loss 0.554076.
Train: 2018-08-05T07:03:52.560857: step 6577, loss 0.570761.
Train: 2018-08-05T07:03:56.123732: step 6578, loss 0.562465.
Train: 2018-08-05T07:03:59.702235: step 6579, loss 0.628662.
Train: 2018-08-05T07:04:03.265111: step 6580, loss 0.595481.
Test: 2018-08-05T07:04:18.516722: step 6580, loss 0.549974.
Train: 2018-08-05T07:04:22.032717: step 6581, loss 0.521504.
Train: 2018-08-05T07:04:25.579967: step 6582, loss 0.570764.
Train: 2018-08-05T07:04:29.127216: step 6583, loss 0.521734.
Train: 2018-08-05T07:04:32.705732: step 6584, loss 0.57077.
Train: 2018-08-05T07:04:36.237342: step 6585, loss 0.538127.
Train: 2018-08-05T07:04:39.753338: step 6586, loss 0.529957.
Train: 2018-08-05T07:04:43.269334: step 6587, loss 0.570768.
Train: 2018-08-05T07:04:46.785330: step 6588, loss 0.578948.
Train: 2018-08-05T07:04:50.285700: step 6589, loss 0.587141.
Train: 2018-08-05T07:04:53.817322: step 6590, loss 0.521616.
Test: 2018-08-05T07:05:09.053307: step 6590, loss 0.548478.
Train: 2018-08-05T07:05:12.600555: step 6591, loss 0.603578.
Train: 2018-08-05T07:05:16.179058: step 6592, loss 0.595386.
Train: 2018-08-05T07:05:19.695054: step 6593, loss 0.554355.
Train: 2018-08-05T07:05:23.226677: step 6594, loss 0.57076.
Train: 2018-08-05T07:05:26.742673: step 6595, loss 0.595402.
Train: 2018-08-05T07:05:30.274295: step 6596, loss 0.587184.
Train: 2018-08-05T07:05:33.821545: step 6597, loss 0.587174.
Train: 2018-08-05T07:05:37.431301: step 6598, loss 0.546165.
Train: 2018-08-05T07:05:40.962924: step 6599, loss 0.595353.
Train: 2018-08-05T07:05:44.494546: step 6600, loss 0.521623.
Test: 2018-08-05T07:05:59.558636: step 6600, loss 0.547942.
Train: 2018-08-05T07:06:05.371750: step 6601, loss 0.562569.
Train: 2018-08-05T07:06:08.887747: step 6602, loss 0.562563.
Train: 2018-08-05T07:06:12.403743: step 6603, loss 0.570761.
Train: 2018-08-05T07:06:15.950992: step 6604, loss 0.521496.
Train: 2018-08-05T07:06:19.451361: step 6605, loss 0.488506.
Train: 2018-08-05T07:06:22.982984: step 6606, loss 0.595518.
Train: 2018-08-05T07:06:26.530233: step 6607, loss 0.636957.
Train: 2018-08-05T07:06:30.077483: step 6608, loss 0.570757.
Train: 2018-08-05T07:06:33.640360: step 6609, loss 0.562471.
Train: 2018-08-05T07:06:37.187609: step 6610, loss 0.504422.
Test: 2018-08-05T07:06:52.204818: step 6610, loss 0.548956.
Train: 2018-08-05T07:06:55.752068: step 6611, loss 0.554141.
Train: 2018-08-05T07:06:59.252438: step 6612, loss 0.54578.
Train: 2018-08-05T07:07:02.799688: step 6613, loss 0.537374.
Train: 2018-08-05T07:07:06.331310: step 6614, loss 0.495419.
Train: 2018-08-05T07:07:09.878560: step 6615, loss 0.579199.
Train: 2018-08-05T07:07:13.441436: step 6616, loss 0.537055.
Train: 2018-08-05T07:07:16.988685: step 6617, loss 0.51154.
Train: 2018-08-05T07:07:20.473428: step 6618, loss 0.587866.
Train: 2018-08-05T07:07:24.005051: step 6619, loss 0.519645.
Train: 2018-08-05T07:07:27.521047: step 6620, loss 0.596653.
Test: 2018-08-05T07:07:42.725777: step 6620, loss 0.546985.
Train: 2018-08-05T07:07:46.273027: step 6621, loss 0.570944.
Train: 2018-08-05T07:07:49.898409: step 6622, loss 0.62262.
Train: 2018-08-05T07:07:53.383153: step 6623, loss 0.528026.
Train: 2018-08-05T07:07:56.914775: step 6624, loss 0.596928.
Train: 2018-08-05T07:08:00.446398: step 6625, loss 0.596908.
Train: 2018-08-05T07:08:04.009273: step 6626, loss 0.501877.
Train: 2018-08-05T07:08:07.587776: step 6627, loss 0.527759.
Train: 2018-08-05T07:08:11.166279: step 6628, loss 0.510393.
Train: 2018-08-05T07:08:14.651022: step 6629, loss 0.55367.
Train: 2018-08-05T07:08:18.198272: step 6630, loss 0.484071.
Test: 2018-08-05T07:08:33.309241: step 6630, loss 0.547588.
Train: 2018-08-05T07:08:36.950251: step 6631, loss 0.544909.
Train: 2018-08-05T07:08:40.481874: step 6632, loss 0.501069.
Train: 2018-08-05T07:08:44.013496: step 6633, loss 0.536017.
Train: 2018-08-05T07:08:47.560746: step 6634, loss 0.535932.
Train: 2018-08-05T07:08:51.076742: step 6635, loss 0.509246.
Train: 2018-08-05T07:08:54.592738: step 6636, loss 0.571407.
Train: 2018-08-05T07:08:58.124361: step 6637, loss 0.535704.
Train: 2018-08-05T07:09:01.671610: step 6638, loss 0.59847.
Train: 2018-08-05T07:09:05.218860: step 6639, loss 0.625568.
Train: 2018-08-05T07:09:08.766110: step 6640, loss 0.544598.
Test: 2018-08-05T07:09:23.923959: step 6640, loss 0.546529.
Train: 2018-08-05T07:09:27.424329: step 6641, loss 0.517571.
Train: 2018-08-05T07:09:30.971578: step 6642, loss 0.607718.
Train: 2018-08-05T07:09:34.503201: step 6643, loss 0.571642.
Train: 2018-08-05T07:09:36.222133: step 6644, loss 0.562618.
Train: 2018-08-05T07:09:39.785009: step 6645, loss 0.571618.
Train: 2018-08-05T07:09:43.394764: step 6646, loss 0.616592.
Train: 2018-08-05T07:09:46.926387: step 6647, loss 0.607468.
Train: 2018-08-05T07:09:50.458010: step 6648, loss 0.455132.
Train: 2018-08-05T07:09:53.989633: step 6649, loss 0.553589.
Train: 2018-08-05T07:09:57.536883: step 6650, loss 0.562524.
Test: 2018-08-05T07:10:12.663479: step 6650, loss 0.546786.
Train: 2018-08-05T07:10:16.210729: step 6651, loss 0.553588.
Train: 2018-08-05T07:10:19.773604: step 6652, loss 0.535754.
Train: 2018-08-05T07:10:23.305227: step 6653, loss 0.615977.
Train: 2018-08-05T07:10:26.836850: step 6654, loss 0.64256.
Train: 2018-08-05T07:10:30.368473: step 6655, loss 0.491519.
Train: 2018-08-05T07:10:33.884469: step 6656, loss 0.597852.
Train: 2018-08-05T07:10:37.416092: step 6657, loss 0.535939.
Train: 2018-08-05T07:10:40.932088: step 6658, loss 0.518351.
Train: 2018-08-05T07:10:44.494965: step 6659, loss 0.553605.
Train: 2018-08-05T07:10:48.026587: step 6660, loss 0.58.
Test: 2018-08-05T07:11:03.137556: step 6660, loss 0.547117.
Train: 2018-08-05T07:11:06.669180: step 6661, loss 0.632691.
Train: 2018-08-05T07:11:10.216429: step 6662, loss 0.623726.
Train: 2018-08-05T07:11:13.732425: step 6663, loss 0.49253.
Train: 2018-08-05T07:11:17.295301: step 6664, loss 0.510092.
Train: 2018-08-05T07:11:20.842551: step 6665, loss 0.588467.
Train: 2018-08-05T07:11:24.358547: step 6666, loss 0.571043.
Train: 2018-08-05T07:11:27.890170: step 6667, loss 0.597057.
Train: 2018-08-05T07:11:31.421793: step 6668, loss 0.571001.
Train: 2018-08-05T07:11:34.969042: step 6669, loss 0.545063.
Train: 2018-08-05T07:11:38.516292: step 6670, loss 0.588207.
Test: 2018-08-05T07:11:53.736649: step 6670, loss 0.546578.
Train: 2018-08-05T07:11:57.252644: step 6671, loss 0.57094.
Train: 2018-08-05T07:12:00.799894: step 6672, loss 0.536577.
Train: 2018-08-05T07:12:04.362770: step 6673, loss 0.656649.
Train: 2018-08-05T07:12:07.878766: step 6674, loss 0.562336.
Train: 2018-08-05T07:12:11.379136: step 6675, loss 0.570859.
Train: 2018-08-05T07:12:14.957638: step 6676, loss 0.570847.
Train: 2018-08-05T07:12:18.489261: step 6677, loss 0.570827.
Train: 2018-08-05T07:12:22.020884: step 6678, loss 0.62151.
Train: 2018-08-05T07:12:25.552507: step 6679, loss 0.587631.
Train: 2018-08-05T07:12:29.099756: step 6680, loss 0.595941.
Test: 2018-08-05T07:12:44.210727: step 6680, loss 0.548663.
Train: 2018-08-05T07:12:47.742349: step 6681, loss 0.554071.
Train: 2018-08-05T07:12:51.273972: step 6682, loss 0.545793.
Train: 2018-08-05T07:12:54.821221: step 6683, loss 0.620573.
Train: 2018-08-05T07:12:58.305964: step 6684, loss 0.512844.
Train: 2018-08-05T07:13:01.821960: step 6685, loss 0.562498.
Train: 2018-08-05T07:13:05.400463: step 6686, loss 0.636719.
Train: 2018-08-05T07:13:08.932086: step 6687, loss 0.595423.
Train: 2018-08-05T07:13:12.510588: step 6688, loss 0.529788.
Train: 2018-08-05T07:13:16.042211: step 6689, loss 0.611663.
Train: 2018-08-05T07:13:19.558208: step 6690, loss 0.570773.
Test: 2018-08-05T07:13:34.872324: step 6690, loss 0.548301.
Train: 2018-08-05T07:13:38.450827: step 6691, loss 0.554503.
Train: 2018-08-05T07:13:41.998076: step 6692, loss 0.513915.
Train: 2018-08-05T07:13:45.529699: step 6693, loss 0.481429.
Train: 2018-08-05T07:13:49.170708: step 6694, loss 0.546358.
Train: 2018-08-05T07:13:52.749211: step 6695, loss 0.562611.
Train: 2018-08-05T07:13:56.265207: step 6696, loss 0.61167.
Train: 2018-08-05T07:13:59.812456: step 6697, loss 0.537999.
Train: 2018-08-05T07:14:03.375332: step 6698, loss 0.562554.
Train: 2018-08-05T07:14:06.906955: step 6699, loss 0.521421.
Train: 2018-08-05T07:14:10.469832: step 6700, loss 0.61199.
Test: 2018-08-05T07:14:25.721442: step 6700, loss 0.548701.
Train: 2018-08-05T07:14:31.409542: step 6701, loss 0.537716.
Train: 2018-08-05T07:14:34.972418: step 6702, loss 0.537644.
Train: 2018-08-05T07:14:38.504041: step 6703, loss 0.562458.
Train: 2018-08-05T07:14:42.004410: step 6704, loss 0.487545.
Train: 2018-08-05T07:14:45.520406: step 6705, loss 0.520635.
Train: 2018-08-05T07:14:49.067656: step 6706, loss 0.553993.
Train: 2018-08-05T07:14:52.614905: step 6707, loss 0.478033.
Train: 2018-08-05T07:14:56.130901: step 6708, loss 0.56235.
Train: 2018-08-05T07:14:59.615644: step 6709, loss 0.570869.
Train: 2018-08-05T07:15:03.162893: step 6710, loss 0.536628.
Test: 2018-08-05T07:15:18.258237: step 6710, loss 0.548751.
Train: 2018-08-05T07:15:21.852366: step 6711, loss 0.545118.
Train: 2018-08-05T07:15:25.337110: step 6712, loss 0.536397.
Train: 2018-08-05T07:15:28.868732: step 6713, loss 0.53629.
Train: 2018-08-05T07:15:32.415981: step 6714, loss 0.562367.
Train: 2018-08-05T07:15:35.947605: step 6715, loss 0.518578.
Train: 2018-08-05T07:15:39.479227: step 6716, loss 0.597603.
Train: 2018-08-05T07:15:43.073357: step 6717, loss 0.624204.
Train: 2018-08-05T07:15:46.589352: step 6718, loss 0.562433.
Train: 2018-08-05T07:15:50.152229: step 6719, loss 0.482828.
Train: 2018-08-05T07:15:53.746358: step 6720, loss 0.562458.
Test: 2018-08-05T07:16:08.872955: step 6720, loss 0.548046.
Train: 2018-08-05T07:16:12.420204: step 6721, loss 0.562473.
Train: 2018-08-05T07:16:16.045587: step 6722, loss 0.544691.
Train: 2018-08-05T07:16:19.577210: step 6723, loss 0.517945.
Train: 2018-08-05T07:16:23.108833: step 6724, loss 0.544659.
Train: 2018-08-05T07:16:26.671709: step 6725, loss 0.544643.
Train: 2018-08-05T07:16:30.218958: step 6726, loss 0.652198.
Train: 2018-08-05T07:16:33.750581: step 6727, loss 0.54463.
Train: 2018-08-05T07:16:37.297830: step 6728, loss 0.571513.
Train: 2018-08-05T07:16:40.798200: step 6729, loss 0.589416.
Train: 2018-08-05T07:16:44.329823: step 6730, loss 0.499917.
Test: 2018-08-05T07:16:59.440793: step 6730, loss 0.548008.
Train: 2018-08-05T07:17:03.034933: step 6731, loss 0.508865.
Train: 2018-08-05T07:17:06.613425: step 6732, loss 0.571494.
Train: 2018-08-05T07:17:10.129421: step 6733, loss 0.464047.
Train: 2018-08-05T07:17:13.692298: step 6734, loss 0.562566.
Train: 2018-08-05T07:17:17.208293: step 6735, loss 0.571568.
Train: 2018-08-05T07:17:20.724290: step 6736, loss 0.49963.
Train: 2018-08-05T07:17:24.318419: step 6737, loss 0.643717.
Train: 2018-08-05T07:17:27.881295: step 6738, loss 0.589642.
Train: 2018-08-05T07:17:31.412918: step 6739, loss 0.562602.
Train: 2018-08-05T07:17:34.928914: step 6740, loss 0.508638.
Test: 2018-08-05T07:17:50.055511: step 6740, loss 0.546745.
Train: 2018-08-05T07:17:53.571507: step 6741, loss 0.625521.
Train: 2018-08-05T07:17:57.118756: step 6742, loss 0.499738.
Train: 2018-08-05T07:18:00.666005: step 6743, loss 0.589481.
Train: 2018-08-05T07:18:04.228882: step 6744, loss 0.508785.
Train: 2018-08-05T07:18:07.776131: step 6745, loss 0.616306.
Train: 2018-08-05T07:18:11.276501: step 6746, loss 0.499915.
Train: 2018-08-05T07:18:14.776870: step 6747, loss 0.571475.
Train: 2018-08-05T07:18:18.324119: step 6748, loss 0.535716.
Train: 2018-08-05T07:18:21.886996: step 6749, loss 0.526787.
Train: 2018-08-05T07:18:25.418619: step 6750, loss 0.571461.
Test: 2018-08-05T07:18:40.545215: step 6750, loss 0.548219.
Train: 2018-08-05T07:18:44.076838: step 6751, loss 0.633995.
Train: 2018-08-05T07:18:47.577208: step 6752, loss 0.58926.
Train: 2018-08-05T07:18:51.155710: step 6753, loss 0.580279.
Train: 2018-08-05T07:18:54.687333: step 6754, loss 0.55359.
Train: 2018-08-05T07:18:58.265836: step 6755, loss 0.544742.
Train: 2018-08-05T07:19:01.797459: step 6756, loss 0.482926.
Train: 2018-08-05T07:19:05.313455: step 6757, loss 0.633074.
Train: 2018-08-05T07:19:08.829451: step 6758, loss 0.624113.
Train: 2018-08-05T07:19:12.329820: step 6759, loss 0.562398.
Train: 2018-08-05T07:19:15.877070: step 6760, loss 0.527342.
Test: 2018-08-05T07:19:30.988040: step 6760, loss 0.547572.
Train: 2018-08-05T07:19:34.550916: step 6761, loss 0.597342.
Train: 2018-08-05T07:19:38.113792: step 6762, loss 0.501334.
Train: 2018-08-05T07:19:41.629788: step 6763, loss 0.605896.
Train: 2018-08-05T07:19:45.192664: step 6764, loss 0.518909.
Train: 2018-08-05T07:19:48.677407: step 6765, loss 0.562349.
Train: 2018-08-05T07:19:52.193404: step 6766, loss 0.545007.
Train: 2018-08-05T07:19:55.756279: step 6767, loss 0.5017.
Train: 2018-08-05T07:19:59.287902: step 6768, loss 0.545009.
Train: 2018-08-05T07:20:02.819525: step 6769, loss 0.579699.
Train: 2018-08-05T07:20:06.366775: step 6770, loss 0.536315.
Test: 2018-08-05T07:20:21.524625: step 6770, loss 0.548847.
Train: 2018-08-05T07:20:25.071874: step 6771, loss 0.518929.
Train: 2018-08-05T07:20:28.603497: step 6772, loss 0.579749.
Train: 2018-08-05T07:20:32.119493: step 6773, loss 0.571062.
Train: 2018-08-05T07:20:35.666742: step 6774, loss 0.658162.
Train: 2018-08-05T07:20:39.229618: step 6775, loss 0.58844.
Train: 2018-08-05T07:20:42.761241: step 6776, loss 0.649119.
Train: 2018-08-05T07:20:46.292864: step 6777, loss 0.562341.
Train: 2018-08-05T07:20:49.808860: step 6778, loss 0.545108.
Train: 2018-08-05T07:20:53.356109: step 6779, loss 0.579515.
Train: 2018-08-05T07:20:56.918986: step 6780, loss 0.579465.
Test: 2018-08-05T07:21:11.998702: step 6780, loss 0.548667.
Train: 2018-08-05T07:21:15.561578: step 6781, loss 0.54526.
Train: 2018-08-05T07:21:19.077575: step 6782, loss 0.553823.
Train: 2018-08-05T07:21:22.640451: step 6783, loss 0.536839.
Train: 2018-08-05T07:21:26.140820: step 6784, loss 0.545363.
Train: 2018-08-05T07:21:29.688070: step 6785, loss 0.638727.
Train: 2018-08-05T07:21:33.235319: step 6786, loss 0.630095.
Train: 2018-08-05T07:21:36.813822: step 6787, loss 0.612996.
Train: 2018-08-05T07:21:40.329818: step 6788, loss 0.553982.
Train: 2018-08-05T07:21:43.861441: step 6789, loss 0.545656.
Train: 2018-08-05T07:21:47.377437: step 6790, loss 0.537367.
Test: 2018-08-05T07:22:02.441528: step 6790, loss 0.548508.
Train: 2018-08-05T07:22:06.020030: step 6791, loss 0.562429.
Train: 2018-08-05T07:22:09.567279: step 6792, loss 0.529145.
Train: 2018-08-05T07:22:13.114529: step 6793, loss 0.529163.
Train: 2018-08-05T07:22:16.646151: step 6794, loss 0.554115.
Train: 2018-08-05T07:22:18.365083: step 6795, loss 0.562434.
Train: 2018-08-05T07:22:21.865453: step 6796, loss 0.612448.
Train: 2018-08-05T07:22:25.428328: step 6797, loss 0.495758.
Train: 2018-08-05T07:22:29.038085: step 6798, loss 0.579113.
Train: 2018-08-05T07:22:32.585334: step 6799, loss 0.637597.
Train: 2018-08-05T07:22:36.148210: step 6800, loss 0.512336.
Test: 2018-08-05T07:22:51.274807: step 6800, loss 0.54693.
Train: 2018-08-05T07:22:56.931654: step 6801, loss 0.687701.
Train: 2018-08-05T07:23:00.447650: step 6802, loss 0.545762.
Train: 2018-08-05T07:23:03.963646: step 6803, loss 0.554118.
Train: 2018-08-05T07:23:07.542149: step 6804, loss 0.57076.
Train: 2018-08-05T07:23:11.136278: step 6805, loss 0.545843.
Train: 2018-08-05T07:23:14.636648: step 6806, loss 0.587364.
Train: 2018-08-05T07:23:18.152644: step 6807, loss 0.579054.
Train: 2018-08-05T07:23:21.668640: step 6808, loss 0.504449.
Train: 2018-08-05T07:23:25.200263: step 6809, loss 0.545876.
Train: 2018-08-05T07:23:28.763139: step 6810, loss 0.603972.
Test: 2018-08-05T07:23:43.952243: step 6810, loss 0.548011.
Train: 2018-08-05T07:23:47.499492: step 6811, loss 0.587369.
Train: 2018-08-05T07:23:51.015488: step 6812, loss 0.462825.
Train: 2018-08-05T07:23:54.547111: step 6813, loss 0.579082.
Train: 2018-08-05T07:23:58.078734: step 6814, loss 0.56243.
Train: 2018-08-05T07:24:01.610356: step 6815, loss 0.570768.
Train: 2018-08-05T07:24:05.173233: step 6816, loss 0.595852.
Train: 2018-08-05T07:24:08.798615: step 6817, loss 0.6126.
Train: 2018-08-05T07:24:12.330238: step 6818, loss 0.579134.
Train: 2018-08-05T07:24:15.924367: step 6819, loss 0.562415.
Train: 2018-08-05T07:24:19.487244: step 6820, loss 0.57912.
Test: 2018-08-05T07:24:34.629469: step 6820, loss 0.54983.
Train: 2018-08-05T07:24:38.161090: step 6821, loss 0.520697.
Train: 2018-08-05T07:24:41.755219: step 6822, loss 0.554072.
Train: 2018-08-05T07:24:45.302468: step 6823, loss 0.487232.
Train: 2018-08-05T07:24:48.818464: step 6824, loss 0.57915.
Train: 2018-08-05T07:24:52.334460: step 6825, loss 0.48689.
Train: 2018-08-05T07:24:55.834830: step 6826, loss 0.469778.
Train: 2018-08-05T07:24:59.366453: step 6827, loss 0.596205.
Train: 2018-08-05T07:25:02.898076: step 6828, loss 0.485889.
Train: 2018-08-05T07:25:06.507832: step 6829, loss 0.51964.
Train: 2018-08-05T07:25:10.039454: step 6830, loss 0.562335.
Test: 2018-08-05T07:25:25.087918: step 6830, loss 0.549514.
Train: 2018-08-05T07:25:28.635167: step 6831, loss 0.570969.
Train: 2018-08-05T07:25:32.213671: step 6832, loss 0.579679.
Train: 2018-08-05T07:25:35.760920: step 6833, loss 0.544963.
Train: 2018-08-05T07:25:39.276916: step 6834, loss 0.544917.
Train: 2018-08-05T07:25:42.808539: step 6835, loss 0.597388.
Train: 2018-08-05T07:25:46.324535: step 6836, loss 0.527308.
Train: 2018-08-05T07:25:49.840531: step 6837, loss 0.527236.
Train: 2018-08-05T07:25:53.419034: step 6838, loss 0.580048.
Train: 2018-08-05T07:25:56.950657: step 6839, loss 0.518267.
Train: 2018-08-05T07:26:00.513532: step 6840, loss 0.518174.
Test: 2018-08-05T07:26:15.671383: step 6840, loss 0.549274.
Train: 2018-08-05T07:26:19.249886: step 6841, loss 0.491424.
Train: 2018-08-05T07:26:22.797135: step 6842, loss 0.553588.
Train: 2018-08-05T07:26:26.297504: step 6843, loss 0.571482.
Train: 2018-08-05T07:26:29.813500: step 6844, loss 0.535653.
Train: 2018-08-05T07:26:33.313870: step 6845, loss 0.634547.
Train: 2018-08-05T07:26:36.861119: step 6846, loss 0.5536.
Train: 2018-08-05T07:26:40.408369: step 6847, loss 0.535587.
Train: 2018-08-05T07:26:44.096258: step 6848, loss 0.589664.
Train: 2018-08-05T07:26:47.612254: step 6849, loss 0.580649.
Train: 2018-08-05T07:26:51.175130: step 6850, loss 0.616668.
Test: 2018-08-05T07:27:06.379860: step 6850, loss 0.547158.
Train: 2018-08-05T07:27:09.911483: step 6851, loss 0.598561.
Train: 2018-08-05T07:27:13.411853: step 6852, loss 0.598439.
Train: 2018-08-05T07:27:16.974729: step 6853, loss 0.562529.
Train: 2018-08-05T07:27:20.568858: step 6854, loss 0.544677.
Train: 2018-08-05T07:27:24.084854: step 6855, loss 0.598024.
Train: 2018-08-05T07:27:27.585224: step 6856, loss 0.509306.
Train: 2018-08-05T07:27:31.116846: step 6857, loss 0.624303.
Train: 2018-08-05T07:27:34.695349: step 6858, loss 0.527178.
Train: 2018-08-05T07:27:38.289479: step 6859, loss 0.562399.
Train: 2018-08-05T07:27:41.821101: step 6860, loss 0.597453.
Test: 2018-08-05T07:27:56.994578: step 6860, loss 0.547775.
Train: 2018-08-05T07:28:00.541828: step 6861, loss 0.614817.
Train: 2018-08-05T07:28:04.057824: step 6862, loss 0.562358.
Train: 2018-08-05T07:28:07.651954: step 6863, loss 0.5797.
Train: 2018-08-05T07:28:11.167949: step 6864, loss 0.553697.
Train: 2018-08-05T07:28:14.715199: step 6865, loss 0.519256.
Train: 2018-08-05T07:28:18.293701: step 6866, loss 0.639723.
Train: 2018-08-05T07:28:21.825324: step 6867, loss 0.605178.
Train: 2018-08-05T07:28:25.325693: step 6868, loss 0.647673.
Train: 2018-08-05T07:28:28.857316: step 6869, loss 0.528405.
Train: 2018-08-05T07:28:32.373313: step 6870, loss 0.553912.
Test: 2018-08-05T07:28:47.624922: step 6870, loss 0.547345.
Train: 2018-08-05T07:28:51.250305: step 6871, loss 0.646574.
Train: 2018-08-05T07:28:54.844435: step 6872, loss 0.587532.
Train: 2018-08-05T07:28:58.344804: step 6873, loss 0.570765.
Train: 2018-08-05T07:29:01.923307: step 6874, loss 0.579055.
Train: 2018-08-05T07:29:05.454930: step 6875, loss 0.545976.
Train: 2018-08-05T07:29:09.002179: step 6876, loss 0.653073.
Train: 2018-08-05T07:29:12.549429: step 6877, loss 0.578954.
Train: 2018-08-05T07:29:16.127931: step 6878, loss 0.578926.
Train: 2018-08-05T07:29:19.706434: step 6879, loss 0.58702.
Train: 2018-08-05T07:29:23.206804: step 6880, loss 0.522325.
Test: 2018-08-05T07:29:38.255266: step 6880, loss 0.549673.
Train: 2018-08-05T07:29:41.771263: step 6881, loss 0.482172.
Train: 2018-08-05T07:29:45.287259: step 6882, loss 0.586936.
Train: 2018-08-05T07:29:48.834509: step 6883, loss 0.546661.
Train: 2018-08-05T07:29:52.428638: step 6884, loss 0.586935.
Train: 2018-08-05T07:29:55.960260: step 6885, loss 0.53054.
Train: 2018-08-05T07:29:59.491884: step 6886, loss 0.538557.
Train: 2018-08-05T07:30:03.007880: step 6887, loss 0.554649.
Train: 2018-08-05T07:30:06.508250: step 6888, loss 0.538412.
Train: 2018-08-05T07:30:10.086752: step 6889, loss 0.562669.
Train: 2018-08-05T07:30:13.618375: step 6890, loss 0.538215.
Test: 2018-08-05T07:30:28.744972: step 6890, loss 0.548398.
Train: 2018-08-05T07:30:32.276594: step 6891, loss 0.529933.
Train: 2018-08-05T07:30:35.792591: step 6892, loss 0.578961.
Train: 2018-08-05T07:30:39.324214: step 6893, loss 0.504944.
Train: 2018-08-05T07:30:42.840210: step 6894, loss 0.537701.
Train: 2018-08-05T07:30:46.371832: step 6895, loss 0.570759.
Train: 2018-08-05T07:30:49.903455: step 6896, loss 0.570766.
Train: 2018-08-05T07:30:53.466331: step 6897, loss 0.503821.
Train: 2018-08-05T07:30:57.122967: step 6898, loss 0.528746.
Train: 2018-08-05T07:31:00.717097: step 6899, loss 0.604619.
Train: 2018-08-05T07:31:04.233093: step 6900, loss 0.570833.
Test: 2018-08-05T07:31:19.312810: step 6900, loss 0.546957.
Train: 2018-08-05T07:31:25.063416: step 6901, loss 0.528307.
Train: 2018-08-05T07:31:28.595039: step 6902, loss 0.553798.
Train: 2018-08-05T07:31:32.173542: step 6903, loss 0.562335.
Train: 2018-08-05T07:31:35.673912: step 6904, loss 0.484989.
Train: 2018-08-05T07:31:39.205535: step 6905, loss 0.640019.
Train: 2018-08-05T07:31:42.721530: step 6906, loss 0.553692.
Train: 2018-08-05T07:31:46.268780: step 6907, loss 0.597017.
Train: 2018-08-05T07:31:49.816029: step 6908, loss 0.518963.
Train: 2018-08-05T07:31:53.363279: step 6909, loss 0.614509.
Train: 2018-08-05T07:31:56.910528: step 6910, loss 0.588443.
Test: 2018-08-05T07:32:11.990246: step 6910, loss 0.547634.
Train: 2018-08-05T07:32:15.521868: step 6911, loss 0.579742.
Train: 2018-08-05T07:32:19.037864: step 6912, loss 0.579729.
Train: 2018-08-05T07:32:22.585113: step 6913, loss 0.571028.
Train: 2018-08-05T07:32:26.147989: step 6914, loss 0.527671.
Train: 2018-08-05T07:32:29.742119: step 6915, loss 0.605671.
Train: 2018-08-05T07:32:33.242488: step 6916, loss 0.631569.
Train: 2018-08-05T07:32:36.820991: step 6917, loss 0.501932.
Train: 2018-08-05T07:32:40.305734: step 6918, loss 0.450295.
Train: 2018-08-05T07:32:43.837357: step 6919, loss 0.484678.
Train: 2018-08-05T07:32:47.400233: step 6920, loss 0.50178.
Test: 2018-08-05T07:33:02.511203: step 6920, loss 0.54725.
Train: 2018-08-05T07:33:06.042826: step 6921, loss 0.614446.
Train: 2018-08-05T07:33:09.621329: step 6922, loss 0.544955.
Train: 2018-08-05T07:33:13.152951: step 6923, loss 0.51877.
Train: 2018-08-05T07:33:16.715835: step 6924, loss 0.509921.
Train: 2018-08-05T07:33:20.263077: step 6925, loss 0.588704.
Train: 2018-08-05T07:33:23.810326: step 6926, loss 0.536025.
Train: 2018-08-05T07:33:27.388829: step 6927, loss 0.50953.
Train: 2018-08-05T07:33:30.904825: step 6928, loss 0.562438.
Train: 2018-08-05T07:33:34.436448: step 6929, loss 0.518126.
Train: 2018-08-05T07:33:37.968071: step 6930, loss 0.633633.
Test: 2018-08-05T07:33:53.125921: step 6930, loss 0.547827.
Train: 2018-08-05T07:33:56.704423: step 6931, loss 0.43783.
Train: 2018-08-05T07:34:00.236046: step 6932, loss 0.616128.
Train: 2018-08-05T07:34:03.798922: step 6933, loss 0.55359.
Train: 2018-08-05T07:34:07.314919: step 6934, loss 0.553592.
Train: 2018-08-05T07:34:10.846541: step 6935, loss 0.607441.
Train: 2018-08-05T07:34:14.393791: step 6936, loss 0.517694.
Train: 2018-08-05T07:34:17.956667: step 6937, loss 0.553595.
Train: 2018-08-05T07:34:21.519543: step 6938, loss 0.616501.
Train: 2018-08-05T07:34:25.051166: step 6939, loss 0.589513.
Train: 2018-08-05T07:34:28.551535: step 6940, loss 0.580492.
Test: 2018-08-05T07:34:43.615625: step 6940, loss 0.546975.
Train: 2018-08-05T07:34:47.162874: step 6941, loss 0.535691.
Train: 2018-08-05T07:34:50.694497: step 6942, loss 0.598274.
Train: 2018-08-05T07:34:54.194867: step 6943, loss 0.651682.
Train: 2018-08-05T07:34:57.757743: step 6944, loss 0.535824.
Train: 2018-08-05T07:35:01.320619: step 6945, loss 0.624429.
Train: 2018-08-05T07:35:03.039551: step 6946, loss 0.581227.
Train: 2018-08-05T07:35:06.555547: step 6947, loss 0.606282.
Train: 2018-08-05T07:35:10.118423: step 6948, loss 0.588572.
Train: 2018-08-05T07:35:13.681299: step 6949, loss 0.588419.
Train: 2018-08-05T07:35:17.244175: step 6950, loss 0.596912.
Test: 2018-08-05T07:35:32.480159: step 6950, loss 0.547586.
Train: 2018-08-05T07:35:35.996155: step 6951, loss 0.605309.
Train: 2018-08-05T07:35:39.559031: step 6952, loss 0.519621.
Train: 2018-08-05T07:35:43.090653: step 6953, loss 0.621878.
Train: 2018-08-05T07:35:46.606650: step 6954, loss 0.494683.
Train: 2018-08-05T07:35:50.107019: step 6955, loss 0.545507.
Train: 2018-08-05T07:35:53.638642: step 6956, loss 0.553971.
Train: 2018-08-05T07:35:57.185891: step 6957, loss 0.537212.
Train: 2018-08-05T07:36:00.748767: step 6958, loss 0.587545.
Train: 2018-08-05T07:36:04.264763: step 6959, loss 0.612622.
Train: 2018-08-05T07:36:07.827639: step 6960, loss 0.537372.
Test: 2018-08-05T07:36:22.922984: step 6960, loss 0.549081.
Train: 2018-08-05T07:36:26.470232: step 6961, loss 0.520746.
Train: 2018-08-05T07:36:29.986229: step 6962, loss 0.520761.
Train: 2018-08-05T07:36:33.517852: step 6963, loss 0.562426.
Train: 2018-08-05T07:36:37.127613: step 6964, loss 0.587462.
Train: 2018-08-05T07:36:40.706110: step 6965, loss 0.562419.
Train: 2018-08-05T07:36:44.206480: step 6966, loss 0.595827.
Train: 2018-08-05T07:36:47.738102: step 6967, loss 0.537369.
Train: 2018-08-05T07:36:51.254098: step 6968, loss 0.595829.
Train: 2018-08-05T07:36:54.816974: step 6969, loss 0.570769.
Train: 2018-08-05T07:36:58.332971: step 6970, loss 0.604159.
Test: 2018-08-05T07:37:13.522075: step 6970, loss 0.548502.
Train: 2018-08-05T07:37:17.022444: step 6971, loss 0.545751.
Train: 2018-08-05T07:37:20.585320: step 6972, loss 0.579098.
Train: 2018-08-05T07:37:24.101316: step 6973, loss 0.604074.
Train: 2018-08-05T07:37:27.632939: step 6974, loss 0.529184.
Train: 2018-08-05T07:37:31.227068: step 6975, loss 0.587383.
Train: 2018-08-05T07:37:34.789944: step 6976, loss 0.562455.
Train: 2018-08-05T07:37:38.305940: step 6977, loss 0.562459.
Train: 2018-08-05T07:37:41.837563: step 6978, loss 0.612238.
Train: 2018-08-05T07:37:45.353559: step 6979, loss 0.587327.
Train: 2018-08-05T07:37:48.900809: step 6980, loss 0.504588.
Test: 2018-08-05T07:38:04.043033: step 6980, loss 0.549243.
Train: 2018-08-05T07:38:07.590281: step 6981, loss 0.52113.
Train: 2018-08-05T07:38:11.153158: step 6982, loss 0.545916.
Train: 2018-08-05T07:38:14.669154: step 6983, loss 0.554172.
Train: 2018-08-05T07:38:18.200777: step 6984, loss 0.537532.
Train: 2018-08-05T07:38:21.748026: step 6985, loss 0.512485.
Train: 2018-08-05T07:38:25.295275: step 6986, loss 0.529007.
Train: 2018-08-05T07:38:28.905032: step 6987, loss 0.57078.
Train: 2018-08-05T07:38:32.452281: step 6988, loss 0.596027.
Train: 2018-08-05T07:38:35.952650: step 6989, loss 0.528647.
Train: 2018-08-05T07:38:39.484273: step 6990, loss 0.57927.
Test: 2018-08-05T07:38:54.657752: step 6990, loss 0.548786.
Train: 2018-08-05T07:38:58.205000: step 6991, loss 0.553879.
Train: 2018-08-05T07:39:01.705369: step 6992, loss 0.536869.
Train: 2018-08-05T07:39:05.252618: step 6993, loss 0.511249.
Train: 2018-08-05T07:39:08.815494: step 6994, loss 0.579423.
Train: 2018-08-05T07:39:12.315864: step 6995, loss 0.665153.
Train: 2018-08-05T07:39:15.847486: step 6996, loss 0.630934.
Train: 2018-08-05T07:39:19.410362: step 6997, loss 0.53666.
Train: 2018-08-05T07:39:22.988866: step 6998, loss 0.639311.
Train: 2018-08-05T07:39:26.567369: step 6999, loss 0.519669.
Train: 2018-08-05T07:39:30.145871: step 7000, loss 0.613488.
Test: 2018-08-05T07:39:45.241215: step 7000, loss 0.547941.
Train: 2018-08-05T07:39:50.913688: step 7001, loss 0.485781.
Train: 2018-08-05T07:39:54.429684: step 7002, loss 0.57085.
Train: 2018-08-05T07:39:57.945680: step 7003, loss 0.61336.
Train: 2018-08-05T07:40:01.430423: step 7004, loss 0.553857.
Train: 2018-08-05T07:40:04.977672: step 7005, loss 0.579314.
Train: 2018-08-05T07:40:08.540549: step 7006, loss 0.553883.
Train: 2018-08-05T07:40:12.056545: step 7007, loss 0.587746.
Train: 2018-08-05T07:40:15.556914: step 7008, loss 0.587715.
Train: 2018-08-05T07:40:19.088537: step 7009, loss 0.520182.
Train: 2018-08-05T07:40:22.620160: step 7010, loss 0.545507.
Test: 2018-08-05T07:40:37.715505: step 7010, loss 0.549455.
Train: 2018-08-05T07:40:41.247126: step 7011, loss 0.579232.
Train: 2018-08-05T07:40:44.825629: step 7012, loss 0.587665.
Train: 2018-08-05T07:40:48.325999: step 7013, loss 0.570788.
Train: 2018-08-05T07:40:51.826368: step 7014, loss 0.579228.
Train: 2018-08-05T07:40:55.357991: step 7015, loss 0.545576.
Train: 2018-08-05T07:40:58.858360: step 7016, loss 0.570786.
Train: 2018-08-05T07:41:02.389983: step 7017, loss 0.512014.
Train: 2018-08-05T07:41:05.984112: step 7018, loss 0.553984.
Train: 2018-08-05T07:41:09.515735: step 7019, loss 0.528746.
Train: 2018-08-05T07:41:13.047358: step 7020, loss 0.596065.
Test: 2018-08-05T07:41:28.173955: step 7020, loss 0.549264.
Train: 2018-08-05T07:41:31.736831: step 7021, loss 0.562371.
Train: 2018-08-05T07:41:35.284080: step 7022, loss 0.553933.
Train: 2018-08-05T07:41:38.800077: step 7023, loss 0.553922.
Train: 2018-08-05T07:41:42.378579: step 7024, loss 0.629978.
Train: 2018-08-05T07:41:45.972708: step 7025, loss 0.570811.
Train: 2018-08-05T07:41:49.535584: step 7026, loss 0.537035.
Train: 2018-08-05T07:41:53.051581: step 7027, loss 0.55392.
Train: 2018-08-05T07:41:56.583204: step 7028, loss 0.537022.
Train: 2018-08-05T07:42:00.114826: step 7029, loss 0.56236.
Train: 2018-08-05T07:42:03.646449: step 7030, loss 0.49466.
Test: 2018-08-05T07:42:18.741793: step 7030, loss 0.548576.
Train: 2018-08-05T07:42:22.242162: step 7031, loss 0.553869.
Train: 2018-08-05T07:42:25.773785: step 7032, loss 0.52834.
Train: 2018-08-05T07:42:29.321034: step 7033, loss 0.587915.
Train: 2018-08-05T07:42:32.821404: step 7034, loss 0.536711.
Train: 2018-08-05T07:42:36.353027: step 7035, loss 0.562335.
Train: 2018-08-05T07:42:39.884650: step 7036, loss 0.570914.
Train: 2018-08-05T07:42:43.416272: step 7037, loss 0.605309.
Train: 2018-08-05T07:42:46.979148: step 7038, loss 0.545137.
Train: 2018-08-05T07:42:50.526398: step 7039, loss 0.570936.
Train: 2018-08-05T07:42:54.026767: step 7040, loss 0.570948.
Test: 2018-08-05T07:43:09.200245: step 7040, loss 0.546959.
Train: 2018-08-05T07:43:12.700614: step 7041, loss 0.553719.
Train: 2018-08-05T07:43:16.294743: step 7042, loss 0.570962.
Train: 2018-08-05T07:43:19.841992: step 7043, loss 0.588194.
Train: 2018-08-05T07:43:23.389242: step 7044, loss 0.631252.
Train: 2018-08-05T07:43:26.905238: step 7045, loss 0.527963.
Train: 2018-08-05T07:43:30.436860: step 7046, loss 0.467901.
Train: 2018-08-05T07:43:33.999737: step 7047, loss 0.57093.
Train: 2018-08-05T07:43:37.593866: step 7048, loss 0.579537.
Train: 2018-08-05T07:43:41.172369: step 7049, loss 0.484903.
Train: 2018-08-05T07:43:44.703992: step 7050, loss 0.570945.
Test: 2018-08-05T07:43:59.877468: step 7050, loss 0.547915.
Train: 2018-08-05T07:44:03.424718: step 7051, loss 0.596899.
Train: 2018-08-05T07:44:06.925087: step 7052, loss 0.553721.
Train: 2018-08-05T07:44:10.456710: step 7053, loss 0.579637.
Train: 2018-08-05T07:44:14.035213: step 7054, loss 0.510491.
Train: 2018-08-05T07:44:17.644969: step 7055, loss 0.519053.
Train: 2018-08-05T07:44:21.192218: step 7056, loss 0.631781.
Train: 2018-08-05T07:44:24.708214: step 7057, loss 0.518997.
Train: 2018-08-05T07:44:28.239838: step 7058, loss 0.536314.
Train: 2018-08-05T07:44:31.755833: step 7059, loss 0.631875.
Train: 2018-08-05T07:44:35.318709: step 7060, loss 0.53631.
Test: 2018-08-05T07:44:50.507814: step 7060, loss 0.548048.
Train: 2018-08-05T07:44:54.055062: step 7061, loss 0.562351.
Train: 2018-08-05T07:44:57.586685: step 7062, loss 0.553669.
Train: 2018-08-05T07:45:01.118308: step 7063, loss 0.657861.
Train: 2018-08-05T07:45:04.696810: step 7064, loss 0.536384.
Train: 2018-08-05T07:45:08.259687: step 7065, loss 0.510379.
Train: 2018-08-05T07:45:11.806936: step 7066, loss 0.562323.
Train: 2018-08-05T07:45:15.354186: step 7067, loss 0.588204.
Train: 2018-08-05T07:45:18.885808: step 7068, loss 0.510516.
Train: 2018-08-05T07:45:22.401805: step 7069, loss 0.562421.
Train: 2018-08-05T07:45:25.917800: step 7070, loss 0.536287.
Test: 2018-08-05T07:45:41.028771: step 7070, loss 0.546739.
Train: 2018-08-05T07:45:44.638526: step 7071, loss 0.57133.
Train: 2018-08-05T07:45:48.201403: step 7072, loss 0.458649.
Train: 2018-08-05T07:45:51.733026: step 7073, loss 0.501603.
Train: 2018-08-05T07:45:55.280275: step 7074, loss 0.597172.
Train: 2018-08-05T07:45:58.796271: step 7075, loss 0.536183.
Train: 2018-08-05T07:46:02.343521: step 7076, loss 0.501086.
Train: 2018-08-05T07:46:05.843890: step 7077, loss 0.588755.
Train: 2018-08-05T07:46:09.391139: step 7078, loss 0.580024.
Train: 2018-08-05T07:46:12.954016: step 7079, loss 0.615343.
Train: 2018-08-05T07:46:16.485638: step 7080, loss 0.606533.
Test: 2018-08-05T07:46:31.565355: step 7080, loss 0.547696.
Train: 2018-08-05T07:46:35.112605: step 7081, loss 0.632933.
Train: 2018-08-05T07:46:38.691107: step 7082, loss 0.544815.
Train: 2018-08-05T07:46:42.253984: step 7083, loss 0.606277.
Train: 2018-08-05T07:46:45.816860: step 7084, loss 0.457346.
Train: 2018-08-05T07:46:49.364109: step 7085, loss 0.544879.
Train: 2018-08-05T07:46:52.942611: step 7086, loss 0.536134.
Train: 2018-08-05T07:46:56.427354: step 7087, loss 0.614877.
Train: 2018-08-05T07:47:00.005857: step 7088, loss 0.571115.
Train: 2018-08-05T07:47:03.537480: step 7089, loss 0.553637.
Train: 2018-08-05T07:47:07.053477: step 7090, loss 0.562365.
Test: 2018-08-05T07:47:22.180073: step 7090, loss 0.547005.
Train: 2018-08-05T07:47:25.742949: step 7091, loss 0.605936.
Train: 2018-08-05T07:47:29.274572: step 7092, loss 0.518862.
Train: 2018-08-05T07:47:32.821822: step 7093, loss 0.501516.
Train: 2018-08-05T07:47:36.369071: step 7094, loss 0.527576.
Train: 2018-08-05T07:47:39.885067: step 7095, loss 0.597165.
Train: 2018-08-05T07:47:43.416690: step 7096, loss 0.562338.
Train: 2018-08-05T07:47:45.151247: step 7097, loss 0.488135.
Train: 2018-08-05T07:47:48.745377: step 7098, loss 0.510093.
Train: 2018-08-05T07:47:52.261373: step 7099, loss 0.588628.
Train: 2018-08-05T07:47:55.777369: step 7100, loss 0.571105.
Test: 2018-08-05T07:48:10.950846: step 7100, loss 0.548364.
Train: 2018-08-05T07:48:16.982733: step 7101, loss 0.667443.
Train: 2018-08-05T07:48:20.483102: step 7102, loss 0.553636.
Train: 2018-08-05T07:48:24.045978: step 7103, loss 0.492508.
Train: 2018-08-05T07:48:27.655735: step 7104, loss 0.597313.
Train: 2018-08-05T07:48:31.171731: step 7105, loss 0.536177.
Train: 2018-08-05T07:48:34.703353: step 7106, loss 0.544907.
Train: 2018-08-05T07:48:38.219350: step 7107, loss 0.466315.
Train: 2018-08-05T07:48:41.735346: step 7108, loss 0.606137.
Train: 2018-08-05T07:48:45.282595: step 7109, loss 0.53611.
Train: 2018-08-05T07:48:48.845471: step 7110, loss 0.623785.
Test: 2018-08-05T07:49:03.972070: step 7110, loss 0.54674.
Train: 2018-08-05T07:49:07.550571: step 7111, loss 0.571135.
Train: 2018-08-05T07:49:11.113447: step 7112, loss 0.527348.
Train: 2018-08-05T07:49:14.613816: step 7113, loss 0.56238.
Train: 2018-08-05T07:49:18.192319: step 7114, loss 0.509853.
Train: 2018-08-05T07:49:21.802075: step 7115, loss 0.544858.
Train: 2018-08-05T07:49:25.396205: step 7116, loss 0.588714.
Train: 2018-08-05T07:49:28.912201: step 7117, loss 0.536069.
Train: 2018-08-05T07:49:32.443823: step 7118, loss 0.632629.
Train: 2018-08-05T07:49:35.959820: step 7119, loss 0.544849.
Train: 2018-08-05T07:49:39.491442: step 7120, loss 0.509798.
Test: 2018-08-05T07:49:54.586785: step 7120, loss 0.547946.
Train: 2018-08-05T07:49:58.134036: step 7121, loss 0.562387.
Train: 2018-08-05T07:50:01.712538: step 7122, loss 0.571157.
Train: 2018-08-05T07:50:05.259787: step 7123, loss 0.571155.
Train: 2018-08-05T07:50:08.791410: step 7124, loss 0.553622.
Train: 2018-08-05T07:50:12.323034: step 7125, loss 0.588667.
Train: 2018-08-05T07:50:15.823403: step 7126, loss 0.588637.
Train: 2018-08-05T07:50:19.339398: step 7127, loss 0.509937.
Train: 2018-08-05T07:50:22.917901: step 7128, loss 0.562371.
Train: 2018-08-05T07:50:26.465151: step 7129, loss 0.579832.
Train: 2018-08-05T07:50:29.965520: step 7130, loss 0.59726.
Test: 2018-08-05T07:50:45.029612: step 7130, loss 0.547814.
Train: 2018-08-05T07:50:48.608113: step 7131, loss 0.518812.
Train: 2018-08-05T07:50:52.155362: step 7132, loss 0.54495.
Train: 2018-08-05T07:50:55.655732: step 7133, loss 0.562356.
Train: 2018-08-05T07:50:59.187355: step 7134, loss 0.588448.
Train: 2018-08-05T07:51:02.781484: step 7135, loss 0.571042.
Train: 2018-08-05T07:51:06.297481: step 7136, loss 0.605744.
Train: 2018-08-05T07:51:09.860356: step 7137, loss 0.588328.
Train: 2018-08-05T07:51:13.376353: step 7138, loss 0.49322.
Train: 2018-08-05T07:51:16.923602: step 7139, loss 0.493274.
Train: 2018-08-05T07:51:20.502105: step 7140, loss 0.519145.
Test: 2018-08-05T07:51:35.691211: step 7140, loss 0.547893.
Train: 2018-08-05T07:51:39.238458: step 7141, loss 0.545039.
Train: 2018-08-05T07:51:42.816960: step 7142, loss 0.519017.
Train: 2018-08-05T07:51:46.332957: step 7143, loss 0.649207.
Train: 2018-08-05T07:51:49.848953: step 7144, loss 0.544978.
Train: 2018-08-05T07:51:53.349322: step 7145, loss 0.605806.
Train: 2018-08-05T07:51:56.912198: step 7146, loss 0.54498.
Train: 2018-08-05T07:52:00.443821: step 7147, loss 0.588399.
Train: 2018-08-05T07:52:03.975444: step 7148, loss 0.553673.
Train: 2018-08-05T07:52:07.491440: step 7149, loss 0.527668.
Train: 2018-08-05T07:52:11.023063: step 7150, loss 0.518996.
Test: 2018-08-05T07:52:26.259047: step 7150, loss 0.548456.
Train: 2018-08-05T07:52:29.806295: step 7151, loss 0.562349.
Train: 2018-08-05T07:52:33.353545: step 7152, loss 0.56235.
Train: 2018-08-05T07:52:36.963301: step 7153, loss 0.571041.
Train: 2018-08-05T07:52:40.479297: step 7154, loss 0.536281.
Train: 2018-08-05T07:52:44.010920: step 7155, loss 0.536265.
Train: 2018-08-05T07:52:47.573796: step 7156, loss 0.649414.
Train: 2018-08-05T07:52:51.074165: step 7157, loss 0.510171.
Train: 2018-08-05T07:52:54.652668: step 7158, loss 0.562355.
Train: 2018-08-05T07:52:58.262424: step 7159, loss 0.536259.
Train: 2018-08-05T07:53:01.825301: step 7160, loss 0.65809.
Test: 2018-08-05T07:53:16.905017: step 7160, loss 0.547241.
Train: 2018-08-05T07:53:20.483520: step 7161, loss 0.544975.
Train: 2018-08-05T07:53:23.999516: step 7162, loss 0.501602.
Train: 2018-08-05T07:53:27.531139: step 7163, loss 0.562349.
Train: 2018-08-05T07:53:31.062762: step 7164, loss 0.579707.
Train: 2018-08-05T07:53:34.641264: step 7165, loss 0.510295.
Train: 2018-08-05T07:53:38.204141: step 7166, loss 0.571032.
Train: 2018-08-05T07:53:41.767016: step 7167, loss 0.51893.
Train: 2018-08-05T07:53:45.314266: step 7168, loss 0.527584.
Train: 2018-08-05T07:53:48.861515: step 7169, loss 0.536234.
Train: 2018-08-05T07:53:52.393138: step 7170, loss 0.475106.
Test: 2018-08-05T07:54:07.582245: step 7170, loss 0.54695.
Train: 2018-08-05T07:54:11.160744: step 7171, loss 0.606135.
Train: 2018-08-05T07:54:14.770500: step 7172, loss 0.606277.
Train: 2018-08-05T07:54:18.349003: step 7173, loss 0.544792.
Train: 2018-08-05T07:54:21.880626: step 7174, loss 0.650328.
Train: 2018-08-05T07:54:25.443503: step 7175, loss 0.650298.
Train: 2018-08-05T07:54:28.990751: step 7176, loss 0.588682.
Train: 2018-08-05T07:54:32.553627: step 7177, loss 0.571079.
Train: 2018-08-05T07:54:36.100877: step 7178, loss 0.605852.
Train: 2018-08-05T07:54:39.648127: step 7179, loss 0.571004.
Train: 2018-08-05T07:54:43.226629: step 7180, loss 0.536457.
Test: 2018-08-05T07:54:58.478240: step 7180, loss 0.547376.
Train: 2018-08-05T07:55:02.025489: step 7181, loss 0.58814.
Train: 2018-08-05T07:55:05.541484: step 7182, loss 0.62235.
Train: 2018-08-05T07:55:09.151241: step 7183, loss 0.49404.
Train: 2018-08-05T07:55:12.682864: step 7184, loss 0.638996.
Train: 2018-08-05T07:55:16.214486: step 7185, loss 0.545379.
Train: 2018-08-05T07:55:19.746109: step 7186, loss 0.562358.
Train: 2018-08-05T07:55:23.246479: step 7187, loss 0.604552.
Train: 2018-08-05T07:55:26.809354: step 7188, loss 0.52034.
Train: 2018-08-05T07:55:30.356604: step 7189, loss 0.495244.
Train: 2018-08-05T07:55:33.888227: step 7190, loss 0.528819.
Test: 2018-08-05T07:55:49.014826: step 7190, loss 0.548167.
Train: 2018-08-05T07:55:52.608953: step 7191, loss 0.579192.
Train: 2018-08-05T07:55:56.124949: step 7192, loss 0.537176.
Train: 2018-08-05T07:55:59.672198: step 7193, loss 0.56237.
Train: 2018-08-05T07:56:03.203821: step 7194, loss 0.503381.
Train: 2018-08-05T07:56:06.797951: step 7195, loss 0.613094.
Train: 2018-08-05T07:56:10.329573: step 7196, loss 0.579228.
Train: 2018-08-05T07:56:13.861196: step 7197, loss 0.477655.
Train: 2018-08-05T07:56:17.392820: step 7198, loss 0.647524.
Train: 2018-08-05T07:56:20.908815: step 7199, loss 0.562634.
Train: 2018-08-05T07:56:24.456065: step 7200, loss 0.579229.
Test: 2018-08-05T07:56:39.551409: step 7200, loss 0.548567.
Train: 2018-08-05T07:56:45.348895: step 7201, loss 0.511444.
Train: 2018-08-05T07:56:48.864891: step 7202, loss 0.545414.
Train: 2018-08-05T07:56:52.380887: step 7203, loss 0.570911.
Train: 2018-08-05T07:56:55.881257: step 7204, loss 0.519743.
Train: 2018-08-05T07:56:59.412880: step 7205, loss 0.570823.
Train: 2018-08-05T07:57:02.928876: step 7206, loss 0.553815.
Train: 2018-08-05T07:57:06.507379: step 7207, loss 0.545244.
Train: 2018-08-05T07:57:10.085881: step 7208, loss 0.553653.
Train: 2018-08-05T07:57:13.601877: step 7209, loss 0.65669.
Train: 2018-08-05T07:57:17.149128: step 7210, loss 0.613938.
Test: 2018-08-05T07:57:32.275724: step 7210, loss 0.548587.
Train: 2018-08-05T07:57:35.791720: step 7211, loss 0.468014.
Train: 2018-08-05T07:57:39.307716: step 7212, loss 0.570708.
Train: 2018-08-05T07:57:42.854965: step 7213, loss 0.554269.
Train: 2018-08-05T07:57:46.449095: step 7214, loss 0.55318.
Train: 2018-08-05T07:57:49.996344: step 7215, loss 0.614217.
Train: 2018-08-05T07:57:53.527967: step 7216, loss 0.588454.
Train: 2018-08-05T07:57:57.028337: step 7217, loss 0.57964.
Train: 2018-08-05T07:58:00.559960: step 7218, loss 0.579785.
Train: 2018-08-05T07:58:04.122835: step 7219, loss 0.54525.
Train: 2018-08-05T07:58:07.716965: step 7220, loss 0.596299.
Test: 2018-08-05T07:58:22.781055: step 7220, loss 0.548036.
Train: 2018-08-05T07:58:26.312677: step 7221, loss 0.545461.
Train: 2018-08-05T07:58:29.844300: step 7222, loss 0.621284.
Train: 2018-08-05T07:58:33.375923: step 7223, loss 0.554198.
Train: 2018-08-05T07:58:36.891919: step 7224, loss 0.529314.
Train: 2018-08-05T07:58:40.423542: step 7225, loss 0.60434.
Train: 2018-08-05T07:58:43.970792: step 7226, loss 0.579092.
Train: 2018-08-05T07:58:47.564921: step 7227, loss 0.570606.
Train: 2018-08-05T07:58:51.080917: step 7228, loss 0.529115.
Train: 2018-08-05T07:58:54.643793: step 7229, loss 0.520543.
Train: 2018-08-05T07:58:58.206669: step 7230, loss 0.578822.
Test: 2018-08-05T07:59:13.380145: step 7230, loss 0.547674.
Train: 2018-08-05T07:59:16.958649: step 7231, loss 0.570775.
Train: 2018-08-05T07:59:20.537151: step 7232, loss 0.554141.
Train: 2018-08-05T07:59:24.100028: step 7233, loss 0.603871.
Train: 2018-08-05T07:59:27.616024: step 7234, loss 0.55434.
Train: 2018-08-05T07:59:31.132020: step 7235, loss 0.562772.
Train: 2018-08-05T07:59:34.648016: step 7236, loss 0.629337.
Train: 2018-08-05T07:59:38.195265: step 7237, loss 0.562678.
Train: 2018-08-05T07:59:41.789395: step 7238, loss 0.662239.
Train: 2018-08-05T07:59:45.352271: step 7239, loss 0.579126.
Train: 2018-08-05T07:59:48.868267: step 7240, loss 0.57074.
Test: 2018-08-05T08:00:04.072998: step 7240, loss 0.549085.
Train: 2018-08-05T08:00:07.588993: step 7241, loss 0.644528.
Train: 2018-08-05T08:00:11.120616: step 7242, loss 0.57877.
Train: 2018-08-05T08:00:14.683492: step 7243, loss 0.48358.
Train: 2018-08-05T08:00:18.215115: step 7244, loss 0.434704.
Train: 2018-08-05T08:00:21.762365: step 7245, loss 0.513763.
Train: 2018-08-05T08:00:25.325241: step 7246, loss 0.505435.
Train: 2018-08-05T08:00:28.856863: step 7247, loss 0.504489.
Train: 2018-08-05T08:00:30.607048: step 7248, loss 0.562435.
Train: 2018-08-05T08:00:34.154298: step 7249, loss 0.612757.
Train: 2018-08-05T08:00:37.717173: step 7250, loss 0.629768.
Test: 2018-08-05T08:00:52.796890: step 7250, loss 0.548079.
Train: 2018-08-05T08:00:56.344139: step 7251, loss 0.53705.
Train: 2018-08-05T08:00:59.875763: step 7252, loss 0.545444.
Train: 2018-08-05T08:01:03.438639: step 7253, loss 0.562352.
Train: 2018-08-05T08:01:06.985888: step 7254, loss 0.536859.
Train: 2018-08-05T08:01:10.501884: step 7255, loss 0.562342.
Train: 2018-08-05T08:01:14.080387: step 7256, loss 0.519655.
Train: 2018-08-05T08:01:17.627637: step 7257, loss 0.605146.
Train: 2018-08-05T08:01:21.174886: step 7258, loss 0.562335.
Train: 2018-08-05T08:01:24.659629: step 7259, loss 0.588105.
Train: 2018-08-05T08:01:28.222505: step 7260, loss 0.613916.
Test: 2018-08-05T08:01:43.349102: step 7260, loss 0.546994.
Train: 2018-08-05T08:01:46.911977: step 7261, loss 0.527961.
Train: 2018-08-05T08:01:50.474854: step 7262, loss 0.622509.
Train: 2018-08-05T08:01:54.053356: step 7263, loss 0.57951.
Train: 2018-08-05T08:01:57.600606: step 7264, loss 0.553758.
Train: 2018-08-05T08:02:01.147855: step 7265, loss 0.536632.
Train: 2018-08-05T08:02:04.898251: step 7266, loss 0.502383.
Train: 2018-08-05T08:02:08.523634: step 7267, loss 0.605198.
Train: 2018-08-05T08:02:12.258403: step 7268, loss 0.613765.
Train: 2018-08-05T08:02:16.008799: step 7269, loss 0.553774.
Train: 2018-08-05T08:02:19.524795: step 7270, loss 0.562336.
Test: 2018-08-05T08:02:34.682645: step 7270, loss 0.548064.
Train: 2018-08-05T08:02:38.214268: step 7271, loss 0.570883.
Train: 2018-08-05T08:02:41.777144: step 7272, loss 0.596491.
Train: 2018-08-05T08:02:45.308767: step 7273, loss 0.54529.
Train: 2018-08-05T08:02:48.856016: step 7274, loss 0.54531.
Train: 2018-08-05T08:02:52.434519: step 7275, loss 0.579365.
Train: 2018-08-05T08:02:55.950515: step 7276, loss 0.545337.
Train: 2018-08-05T08:02:59.435258: step 7277, loss 0.613348.
Train: 2018-08-05T08:03:02.982507: step 7278, loss 0.638742.
Train: 2018-08-05T08:03:06.498504: step 7279, loss 0.562357.
Train: 2018-08-05T08:03:10.030127: step 7280, loss 0.570807.
Test: 2018-08-05T08:03:25.109844: step 7280, loss 0.548119.
Train: 2018-08-05T08:03:28.657092: step 7281, loss 0.520273.
Train: 2018-08-05T08:03:32.173089: step 7282, loss 0.579201.
Train: 2018-08-05T08:03:35.798471: step 7283, loss 0.553989.
Train: 2018-08-05T08:03:39.330095: step 7284, loss 0.495274.
Train: 2018-08-05T08:03:42.892970: step 7285, loss 0.570785.
Train: 2018-08-05T08:03:46.424593: step 7286, loss 0.579186.
Train: 2018-08-05T08:03:49.971843: step 7287, loss 0.587589.
Train: 2018-08-05T08:03:53.503465: step 7288, loss 0.503601.
Train: 2018-08-05T08:03:57.035088: step 7289, loss 0.51194.
Train: 2018-08-05T08:04:00.566711: step 7290, loss 0.570799.
Test: 2018-08-05T08:04:15.818322: step 7290, loss 0.549047.
Train: 2018-08-05T08:04:19.412451: step 7291, loss 0.570807.
Train: 2018-08-05T08:04:22.928446: step 7292, loss 0.520093.
Train: 2018-08-05T08:04:26.491323: step 7293, loss 0.477621.
Train: 2018-08-05T08:04:30.069825: step 7294, loss 0.604879.
Train: 2018-08-05T08:04:33.632701: step 7295, loss 0.613517.
Train: 2018-08-05T08:04:37.164324: step 7296, loss 0.511095.
Train: 2018-08-05T08:04:40.727200: step 7297, loss 0.553776.
Train: 2018-08-05T08:04:44.258823: step 7298, loss 0.52802.
Train: 2018-08-05T08:04:47.852952: step 7299, loss 0.519327.
Train: 2018-08-05T08:04:51.368949: step 7300, loss 0.510558.
Test: 2018-08-05T08:05:06.573678: step 7300, loss 0.548673.
Train: 2018-08-05T08:05:12.449298: step 7301, loss 0.553681.
Train: 2018-08-05T08:05:15.996548: step 7302, loss 0.527571.
Train: 2018-08-05T08:05:19.543798: step 7303, loss 0.597289.
Train: 2018-08-05T08:05:23.028541: step 7304, loss 0.562372.
Train: 2018-08-05T08:05:26.560163: step 7305, loss 0.544854.
Train: 2018-08-05T08:05:30.154292: step 7306, loss 0.553611.
Train: 2018-08-05T08:05:33.654662: step 7307, loss 0.544785.
Train: 2018-08-05T08:05:37.186285: step 7308, loss 0.53593.
Train: 2018-08-05T08:05:40.717907: step 7309, loss 0.589005.
Train: 2018-08-05T08:05:44.218277: step 7310, loss 0.562459.
Test: 2018-08-05T08:05:59.297996: step 7310, loss 0.547032.
Train: 2018-08-05T08:06:02.876497: step 7311, loss 0.660072.
Train: 2018-08-05T08:06:06.501879: step 7312, loss 0.562458.
Train: 2018-08-05T08:06:10.002249: step 7313, loss 0.580142.
Train: 2018-08-05T08:06:13.549499: step 7314, loss 0.606605.
Train: 2018-08-05T08:06:17.112375: step 7315, loss 0.624099.
Train: 2018-08-05T08:06:20.659624: step 7316, loss 0.518497.
Train: 2018-08-05T08:06:24.222500: step 7317, loss 0.623678.
Train: 2018-08-05T08:06:27.754123: step 7318, loss 0.579814.
Train: 2018-08-05T08:06:31.285746: step 7319, loss 0.492824.
Train: 2018-08-05T08:06:34.801742: step 7320, loss 0.57102.
Test: 2018-08-05T08:06:49.897085: step 7320, loss 0.546295.
Train: 2018-08-05T08:06:53.459961: step 7321, loss 0.640233.
Train: 2018-08-05T08:06:57.022837: step 7322, loss 0.501975.
Train: 2018-08-05T08:07:00.601340: step 7323, loss 0.570941.
Train: 2018-08-05T08:07:04.148590: step 7324, loss 0.545159.
Train: 2018-08-05T08:07:07.695839: step 7325, loss 0.605209.
Train: 2018-08-05T08:07:11.258715: step 7326, loss 0.511006.
Train: 2018-08-05T08:07:14.852844: step 7327, loss 0.519605.
Train: 2018-08-05T08:07:18.400094: step 7328, loss 0.570884.
Train: 2018-08-05T08:07:21.947343: step 7329, loss 0.639243.
Train: 2018-08-05T08:07:25.463339: step 7330, loss 0.690284.
Test: 2018-08-05T08:07:40.574309: step 7330, loss 0.547383.
Train: 2018-08-05T08:07:44.105932: step 7331, loss 0.57084.
Train: 2018-08-05T08:07:47.684435: step 7332, loss 0.477783.
Train: 2018-08-05T08:07:51.200431: step 7333, loss 0.621461.
Train: 2018-08-05T08:07:54.716428: step 7334, loss 0.587631.
Train: 2018-08-05T08:07:58.294931: step 7335, loss 0.579175.
Train: 2018-08-05T08:08:01.842180: step 7336, loss 0.512209.
Train: 2018-08-05T08:08:05.342549: step 7337, loss 0.512296.
Train: 2018-08-05T08:08:08.905425: step 7338, loss 0.512305.
Train: 2018-08-05T08:08:12.468301: step 7339, loss 0.55405.
Train: 2018-08-05T08:08:15.968671: step 7340, loss 0.520548.
Test: 2018-08-05T08:08:31.048387: step 7340, loss 0.546644.
Train: 2018-08-05T08:08:34.595638: step 7341, loss 0.503668.
Train: 2018-08-05T08:08:38.158514: step 7342, loss 0.587629.
Train: 2018-08-05T08:08:41.674510: step 7343, loss 0.629871.
Train: 2018-08-05T08:08:45.253012: step 7344, loss 0.579254.
Train: 2018-08-05T08:08:48.831515: step 7345, loss 0.57926.
Train: 2018-08-05T08:08:52.394391: step 7346, loss 0.562363.
Train: 2018-08-05T08:08:55.957267: step 7347, loss 0.511664.
Train: 2018-08-05T08:08:59.520143: step 7348, loss 0.536978.
Train: 2018-08-05T08:09:03.036139: step 7349, loss 0.494555.
Train: 2018-08-05T08:09:06.567762: step 7350, loss 0.63035.
Test: 2018-08-05T08:09:21.631852: step 7350, loss 0.54852.
Train: 2018-08-05T08:09:25.179101: step 7351, loss 0.519786.
Train: 2018-08-05T08:09:28.710724: step 7352, loss 0.519693.
Train: 2018-08-05T08:09:32.242347: step 7353, loss 0.562336.
Train: 2018-08-05T08:09:35.805223: step 7354, loss 0.605207.
Train: 2018-08-05T08:09:39.352473: step 7355, loss 0.527992.
Train: 2018-08-05T08:09:42.884095: step 7356, loss 0.510725.
Train: 2018-08-05T08:09:46.400091: step 7357, loss 0.527837.
Train: 2018-08-05T08:09:49.931714: step 7358, loss 0.579647.
Train: 2018-08-05T08:09:53.447711: step 7359, loss 0.588366.
Train: 2018-08-05T08:09:57.010587: step 7360, loss 0.562351.
Test: 2018-08-05T08:10:12.152810: step 7360, loss 0.54783.
Train: 2018-08-05T08:10:15.684433: step 7361, loss 0.54496.
Train: 2018-08-05T08:10:19.216055: step 7362, loss 0.55365.
Train: 2018-08-05T08:10:22.778931: step 7363, loss 0.579808.
Train: 2018-08-05T08:10:26.310554: step 7364, loss 0.518725.
Train: 2018-08-05T08:10:29.842177: step 7365, loss 0.614822.
Train: 2018-08-05T08:10:33.405053: step 7366, loss 0.562374.
Train: 2018-08-05T08:10:36.921050: step 7367, loss 0.588603.
Train: 2018-08-05T08:10:40.483925: step 7368, loss 0.544897.
Train: 2018-08-05T08:10:44.015548: step 7369, loss 0.527431.
Train: 2018-08-05T08:10:47.547171: step 7370, loss 0.562371.
Test: 2018-08-05T08:11:02.642515: step 7370, loss 0.54818.
Train: 2018-08-05T08:11:06.174138: step 7371, loss 0.579851.
Train: 2018-08-05T08:11:09.768267: step 7372, loss 0.553635.
Train: 2018-08-05T08:11:13.331143: step 7373, loss 0.649715.
Train: 2018-08-05T08:11:16.862766: step 7374, loss 0.623377.
Train: 2018-08-05T08:11:20.378761: step 7375, loss 0.458093.
Train: 2018-08-05T08:11:23.926011: step 7376, loss 0.527626.
Train: 2018-08-05T08:11:27.426381: step 7377, loss 0.579709.
Train: 2018-08-05T08:11:31.020510: step 7378, loss 0.527647.
Train: 2018-08-05T08:11:34.552133: step 7379, loss 0.605731.
Train: 2018-08-05T08:11:38.068129: step 7380, loss 0.536339.
Test: 2018-08-05T08:11:53.100966: step 7380, loss 0.547872.
Train: 2018-08-05T08:11:56.663842: step 7381, loss 0.527679.
Train: 2018-08-05T08:12:00.242345: step 7382, loss 0.553677.
Train: 2018-08-05T08:12:03.789594: step 7383, loss 0.588369.
Train: 2018-08-05T08:12:07.336844: step 7384, loss 0.518986.
Train: 2018-08-05T08:12:10.930973: step 7385, loss 0.579704.
Train: 2018-08-05T08:12:14.478222: step 7386, loss 0.597064.
Train: 2018-08-05T08:12:17.994219: step 7387, loss 0.545001.
Train: 2018-08-05T08:12:21.541468: step 7388, loss 0.571017.
Train: 2018-08-05T08:12:25.104344: step 7389, loss 0.562345.
Train: 2018-08-05T08:12:28.651594: step 7390, loss 0.596987.
Test: 2018-08-05T08:12:43.762566: step 7390, loss 0.547498.
Train: 2018-08-05T08:12:47.278560: step 7391, loss 0.501796.
Train: 2018-08-05T08:12:50.825809: step 7392, loss 0.553692.
Train: 2018-08-05T08:12:54.357432: step 7393, loss 0.579644.
Train: 2018-08-05T08:12:57.889054: step 7394, loss 0.562342.
Train: 2018-08-05T08:13:01.389424: step 7395, loss 0.640152.
Train: 2018-08-05T08:13:04.936674: step 7396, loss 0.55371.
Train: 2018-08-05T08:13:08.515177: step 7397, loss 0.510651.
Train: 2018-08-05T08:13:12.046799: step 7398, loss 0.519284.
Train: 2018-08-05T08:13:13.781357: step 7399, loss 0.488826.
Train: 2018-08-05T08:13:17.297354: step 7400, loss 0.640018.
Test: 2018-08-05T08:13:32.470830: step 7400, loss 0.548122.
Train: 2018-08-05T08:13:38.221437: step 7401, loss 0.657284.
Train: 2018-08-05T08:13:41.799940: step 7402, loss 0.519265.
Train: 2018-08-05T08:13:45.347190: step 7403, loss 0.613973.
Train: 2018-08-05T08:13:48.941319: step 7404, loss 0.579513.
Train: 2018-08-05T08:13:52.504195: step 7405, loss 0.49377.
Train: 2018-08-05T08:13:56.035818: step 7406, loss 0.605166.
Train: 2018-08-05T08:13:59.551814: step 7407, loss 0.451131.
Train: 2018-08-05T08:14:03.083436: step 7408, loss 0.605153.
Train: 2018-08-05T08:14:06.615059: step 7409, loss 0.588028.
Train: 2018-08-05T08:14:10.162309: step 7410, loss 0.622255.
Test: 2018-08-05T08:14:25.304533: step 7410, loss 0.54846.
Train: 2018-08-05T08:14:28.851781: step 7411, loss 0.570882.
Train: 2018-08-05T08:14:32.367778: step 7412, loss 0.562339.
Train: 2018-08-05T08:14:35.930653: step 7413, loss 0.511241.
Train: 2018-08-05T08:14:39.446650: step 7414, loss 0.485715.
Train: 2018-08-05T08:14:43.025153: step 7415, loss 0.587917.
Train: 2018-08-05T08:14:46.556775: step 7416, loss 0.553807.
Train: 2018-08-05T08:14:50.119652: step 7417, loss 0.545261.
Train: 2018-08-05T08:14:53.651275: step 7418, loss 0.528145.
Train: 2018-08-05T08:14:57.182897: step 7419, loss 0.656523.
Train: 2018-08-05T08:15:00.714520: step 7420, loss 0.562336.
Test: 2018-08-05T08:15:15.762986: step 7420, loss 0.547262.
Train: 2018-08-05T08:15:19.325860: step 7421, loss 0.570891.
Train: 2018-08-05T08:15:22.951242: step 7422, loss 0.536688.
Train: 2018-08-05T08:15:26.482865: step 7423, loss 0.528138.
Train: 2018-08-05T08:15:30.030114: step 7424, loss 0.476778.
Train: 2018-08-05T08:15:33.577364: step 7425, loss 0.588065.
Train: 2018-08-05T08:15:37.077734: step 7426, loss 0.570926.
Train: 2018-08-05T08:15:40.656236: step 7427, loss 0.613945.
Train: 2018-08-05T08:15:44.219112: step 7428, loss 0.588141.
Train: 2018-08-05T08:15:47.750736: step 7429, loss 0.570931.
Train: 2018-08-05T08:15:51.282358: step 7430, loss 0.596686.
Test: 2018-08-05T08:16:06.362076: step 7430, loss 0.547816.
Train: 2018-08-05T08:16:09.987457: step 7431, loss 0.528044.
Train: 2018-08-05T08:16:13.519080: step 7432, loss 0.536602.
Train: 2018-08-05T08:16:17.113209: step 7433, loss 0.579472.
Train: 2018-08-05T08:16:20.660459: step 7434, loss 0.587948.
Train: 2018-08-05T08:16:24.160829: step 7435, loss 0.468652.
Train: 2018-08-05T08:16:27.692452: step 7436, loss 0.493979.
Train: 2018-08-05T08:16:31.255327: step 7437, loss 0.639673.
Train: 2018-08-05T08:16:34.755697: step 7438, loss 0.562345.
Train: 2018-08-05T08:16:38.302946: step 7439, loss 0.579546.
Train: 2018-08-05T08:16:41.850196: step 7440, loss 0.639796.
Test: 2018-08-05T08:16:57.008046: step 7440, loss 0.54838.
Train: 2018-08-05T08:17:00.539669: step 7441, loss 0.527965.
Train: 2018-08-05T08:17:04.102544: step 7442, loss 0.545163.
Train: 2018-08-05T08:17:07.618541: step 7443, loss 0.596668.
Train: 2018-08-05T08:17:11.181417: step 7444, loss 0.519468.
Train: 2018-08-05T08:17:14.806800: step 7445, loss 0.528044.
Train: 2018-08-05T08:17:18.369676: step 7446, loss 0.553757.
Train: 2018-08-05T08:17:21.885672: step 7447, loss 0.622427.
Train: 2018-08-05T08:17:25.432921: step 7448, loss 0.622389.
Train: 2018-08-05T08:17:28.948917: step 7449, loss 0.545209.
Train: 2018-08-05T08:17:32.496166: step 7450, loss 0.570888.
Test: 2018-08-05T08:17:47.591511: step 7450, loss 0.547682.
Train: 2018-08-05T08:17:51.138760: step 7451, loss 0.596498.
Train: 2018-08-05T08:17:54.670382: step 7452, loss 0.536771.
Train: 2018-08-05T08:17:58.186378: step 7453, loss 0.596394.
Train: 2018-08-05T08:18:01.733628: step 7454, loss 0.511362.
Train: 2018-08-05T08:18:05.233998: step 7455, loss 0.519883.
Train: 2018-08-05T08:18:08.796874: step 7456, loss 0.562346.
Train: 2018-08-05T08:18:12.422256: step 7457, loss 0.528343.
Train: 2018-08-05T08:18:15.969506: step 7458, loss 0.613405.
Train: 2018-08-05T08:18:19.485502: step 7459, loss 0.655953.
Train: 2018-08-05T08:18:23.001498: step 7460, loss 0.519884.
Test: 2018-08-05T08:18:38.128095: step 7460, loss 0.547398.
Train: 2018-08-05T08:18:41.644091: step 7461, loss 0.604775.
Train: 2018-08-05T08:18:45.144461: step 7462, loss 0.570825.
Train: 2018-08-05T08:18:48.722963: step 7463, loss 0.621557.
Train: 2018-08-05T08:18:52.270213: step 7464, loss 0.596104.
Train: 2018-08-05T08:18:55.817462: step 7465, loss 0.604418.
Train: 2018-08-05T08:18:59.333459: step 7466, loss 0.579153.
Train: 2018-08-05T08:19:02.865081: step 7467, loss 0.612497.
Train: 2018-08-05T08:19:06.412331: step 7468, loss 0.529208.
Train: 2018-08-05T08:19:10.006460: step 7469, loss 0.521044.
Train: 2018-08-05T08:19:13.616216: step 7470, loss 0.562484.
Test: 2018-08-05T08:19:28.930332: step 7470, loss 0.547366.
Train: 2018-08-05T08:19:32.524462: step 7471, loss 0.678158.
Train: 2018-08-05T08:19:36.056085: step 7472, loss 0.554294.
Train: 2018-08-05T08:19:39.587707: step 7473, loss 0.529721.
Train: 2018-08-05T08:19:43.088077: step 7474, loss 0.595346.
Train: 2018-08-05T08:19:46.604073: step 7475, loss 0.56259.
Train: 2018-08-05T08:19:50.182576: step 7476, loss 0.529952.
Train: 2018-08-05T08:19:53.714198: step 7477, loss 0.570772.
Train: 2018-08-05T08:19:57.308328: step 7478, loss 0.611555.
Train: 2018-08-05T08:20:00.886831: step 7479, loss 0.538193.
Train: 2018-08-05T08:20:04.402827: step 7480, loss 0.554492.
Test: 2018-08-05T08:20:19.576305: step 7480, loss 0.549408.
Train: 2018-08-05T08:20:23.107926: step 7481, loss 0.587065.
Train: 2018-08-05T08:20:26.655176: step 7482, loss 0.505644.
Train: 2018-08-05T08:20:30.218052: step 7483, loss 0.635997.
Train: 2018-08-05T08:20:33.734048: step 7484, loss 0.546323.
Train: 2018-08-05T08:20:37.250044: step 7485, loss 0.595235.
Train: 2018-08-05T08:20:40.750414: step 7486, loss 0.489255.
Train: 2018-08-05T08:20:44.328916: step 7487, loss 0.562604.
Train: 2018-08-05T08:20:47.891792: step 7488, loss 0.562585.
Train: 2018-08-05T08:20:51.470295: step 7489, loss 0.521587.
Train: 2018-08-05T08:20:55.001918: step 7490, loss 0.546102.
Test: 2018-08-05T08:21:10.097263: step 7490, loss 0.548175.
Train: 2018-08-05T08:21:13.644511: step 7491, loss 0.595489.
Train: 2018-08-05T08:21:17.160507: step 7492, loss 0.504661.
Train: 2018-08-05T08:21:20.707756: step 7493, loss 0.587338.
Train: 2018-08-05T08:21:24.270633: step 7494, loss 0.512575.
Train: 2018-08-05T08:21:27.817882: step 7495, loss 0.529055.
Train: 2018-08-05T08:21:31.333878: step 7496, loss 0.612661.
Train: 2018-08-05T08:21:34.849875: step 7497, loss 0.562387.
Train: 2018-08-05T08:21:38.381497: step 7498, loss 0.503437.
Train: 2018-08-05T08:21:41.897493: step 7499, loss 0.528563.
Train: 2018-08-05T08:21:45.475996: step 7500, loss 0.545383.
Test: 2018-08-05T08:22:00.680727: step 7500, loss 0.547921.
Train: 2018-08-05T08:22:06.337574: step 7501, loss 0.545307.
Train: 2018-08-05T08:22:09.884823: step 7502, loss 0.502471.
Train: 2018-08-05T08:22:13.463325: step 7503, loss 0.570912.
Train: 2018-08-05T08:22:17.010575: step 7504, loss 0.596897.
Train: 2018-08-05T08:22:20.526571: step 7505, loss 0.570999.
Train: 2018-08-05T08:22:24.058194: step 7506, loss 0.536301.
Train: 2018-08-05T08:22:27.621070: step 7507, loss 0.536327.
Train: 2018-08-05T08:22:31.152693: step 7508, loss 0.518752.
Train: 2018-08-05T08:22:34.684315: step 7509, loss 0.501123.
Train: 2018-08-05T08:22:38.231565: step 7510, loss 0.51842.
Test: 2018-08-05T08:22:53.326910: step 7510, loss 0.547477.
Train: 2018-08-05T08:22:56.842904: step 7511, loss 0.571273.
Train: 2018-08-05T08:23:00.405780: step 7512, loss 0.580101.
Train: 2018-08-05T08:23:03.937404: step 7513, loss 0.615694.
Train: 2018-08-05T08:23:07.469026: step 7514, loss 0.598016.
Train: 2018-08-05T08:23:11.000649: step 7515, loss 0.58035.
Train: 2018-08-05T08:23:14.563525: step 7516, loss 0.571461.
Train: 2018-08-05T08:23:18.079521: step 7517, loss 0.61584.
Train: 2018-08-05T08:23:21.673651: step 7518, loss 0.571246.
Train: 2018-08-05T08:23:25.220900: step 7519, loss 0.562458.
Train: 2018-08-05T08:23:28.752523: step 7520, loss 0.571313.
Test: 2018-08-05T08:23:43.988506: step 7520, loss 0.548707.
Train: 2018-08-05T08:23:47.535755: step 7521, loss 0.5273.
Train: 2018-08-05T08:23:51.114258: step 7522, loss 0.606352.
Train: 2018-08-05T08:23:54.645881: step 7523, loss 0.562406.
Train: 2018-08-05T08:23:58.224384: step 7524, loss 0.492369.
Train: 2018-08-05T08:24:01.802886: step 7525, loss 0.597354.
Train: 2018-08-05T08:24:05.334509: step 7526, loss 0.579833.
Train: 2018-08-05T08:24:08.897385: step 7527, loss 0.579795.
Train: 2018-08-05T08:24:12.460261: step 7528, loss 0.579753.
Train: 2018-08-05T08:24:16.007511: step 7529, loss 0.562349.
Train: 2018-08-05T08:24:19.601640: step 7530, loss 0.545024.
Test: 2018-08-05T08:24:34.665732: step 7530, loss 0.549693.
Train: 2018-08-05T08:24:38.212980: step 7531, loss 0.553695.
Train: 2018-08-05T08:24:41.744602: step 7532, loss 0.545071.
Train: 2018-08-05T08:24:45.260599: step 7533, loss 0.553712.
Train: 2018-08-05T08:24:48.776595: step 7534, loss 0.536478.
Train: 2018-08-05T08:24:52.308218: step 7535, loss 0.5451.
Train: 2018-08-05T08:24:55.871094: step 7536, loss 0.493376.
Train: 2018-08-05T08:24:59.418343: step 7537, loss 0.510537.
Train: 2018-08-05T08:25:02.981219: step 7538, loss 0.510414.
Train: 2018-08-05T08:25:06.497215: step 7539, loss 0.579715.
Train: 2018-08-05T08:25:10.060092: step 7540, loss 0.562357.
Test: 2018-08-05T08:25:25.217942: step 7540, loss 0.547597.
Train: 2018-08-05T08:25:28.749564: step 7541, loss 0.492589.
Train: 2018-08-05T08:25:32.281187: step 7542, loss 0.474877.
Train: 2018-08-05T08:25:35.844063: step 7543, loss 0.509662.
Train: 2018-08-05T08:25:39.438193: step 7544, loss 0.571265.
Train: 2018-08-05T08:25:42.969816: step 7545, loss 0.553591.
Train: 2018-08-05T08:25:46.517065: step 7546, loss 0.517976.
Train: 2018-08-05T08:25:50.048688: step 7547, loss 0.669793.
Train: 2018-08-05T08:25:53.658444: step 7548, loss 0.54464.
Train: 2018-08-05T08:25:57.268199: step 7549, loss 0.616323.
Train: 2018-08-05T08:25:58.971504: step 7550, loss 0.486089.
Test: 2018-08-05T08:26:14.113728: step 7550, loss 0.54655.
Train: 2018-08-05T08:26:17.692231: step 7551, loss 0.661244.
Train: 2018-08-05T08:26:21.223854: step 7552, loss 0.571514.
Train: 2018-08-05T08:26:24.771103: step 7553, loss 0.55359.
Train: 2018-08-05T08:26:28.287099: step 7554, loss 0.544652.
Train: 2018-08-05T08:26:31.881228: step 7555, loss 0.562516.
Train: 2018-08-05T08:26:35.459731: step 7556, loss 0.500082.
Train: 2018-08-05T08:26:38.975727: step 7557, loss 0.580341.
Train: 2018-08-05T08:26:42.491723: step 7558, loss 0.624889.
Train: 2018-08-05T08:26:46.007720: step 7559, loss 0.482425.
Train: 2018-08-05T08:26:49.539342: step 7560, loss 0.518019.
Test: 2018-08-05T08:27:04.665940: step 7560, loss 0.546399.
Train: 2018-08-05T08:27:08.260068: step 7561, loss 0.482424.
Train: 2018-08-05T08:27:11.791691: step 7562, loss 0.491213.
Train: 2018-08-05T08:27:15.323314: step 7563, loss 0.544654.
Train: 2018-08-05T08:27:18.901817: step 7564, loss 0.589423.
Train: 2018-08-05T08:27:22.464693: step 7565, loss 0.598454.
Train: 2018-08-05T08:27:26.011942: step 7566, loss 0.580524.
Train: 2018-08-05T08:27:29.543565: step 7567, loss 0.58052.
Train: 2018-08-05T08:27:33.090815: step 7568, loss 0.598439.
Train: 2018-08-05T08:27:36.653691: step 7569, loss 0.51777.
Train: 2018-08-05T08:27:40.185314: step 7570, loss 0.55359.
Test: 2018-08-05T08:27:55.280657: step 7570, loss 0.547186.
Train: 2018-08-05T08:27:58.812279: step 7571, loss 0.562532.
Train: 2018-08-05T08:28:02.359529: step 7572, loss 0.598261.
Train: 2018-08-05T08:28:05.922405: step 7573, loss 0.571427.
Train: 2018-08-05T08:28:09.516534: step 7574, loss 0.562491.
Train: 2018-08-05T08:28:13.032531: step 7575, loss 0.49139.
Train: 2018-08-05T08:28:16.564154: step 7576, loss 0.55359.
Train: 2018-08-05T08:28:20.111403: step 7577, loss 0.615729.
Train: 2018-08-05T08:28:23.611773: step 7578, loss 0.580179.
Train: 2018-08-05T08:28:27.143395: step 7579, loss 0.571284.
Train: 2018-08-05T08:28:30.690645: step 7580, loss 0.535948.
Test: 2018-08-05T08:28:45.801615: step 7580, loss 0.54709.
Train: 2018-08-05T08:28:49.380118: step 7581, loss 0.544792.
Train: 2018-08-05T08:28:52.927367: step 7582, loss 0.58881.
Train: 2018-08-05T08:28:56.427737: step 7583, loss 0.571183.
Train: 2018-08-05T08:28:59.943732: step 7584, loss 0.623768.
Train: 2018-08-05T08:29:03.490982: step 7585, loss 0.579856.
Train: 2018-08-05T08:29:07.085112: step 7586, loss 0.536222.
Train: 2018-08-05T08:29:10.647987: step 7587, loss 0.501514.
Train: 2018-08-05T08:29:14.163984: step 7588, loss 0.579714.
Train: 2018-08-05T08:29:17.726860: step 7589, loss 0.597025.
Train: 2018-08-05T08:29:21.305362: step 7590, loss 0.493129.
Test: 2018-08-05T08:29:36.447586: step 7590, loss 0.548497.
Train: 2018-08-05T08:29:40.026089: step 7591, loss 0.570989.
Train: 2018-08-05T08:29:43.604592: step 7592, loss 0.545057.
Train: 2018-08-05T08:29:47.167468: step 7593, loss 0.605533.
Train: 2018-08-05T08:29:50.745970: step 7594, loss 0.476058.
Train: 2018-08-05T08:29:54.293220: step 7595, loss 0.657307.
Train: 2018-08-05T08:29:57.824842: step 7596, loss 0.570962.
Train: 2018-08-05T08:30:01.340839: step 7597, loss 0.553701.
Train: 2018-08-05T08:30:04.934968: step 7598, loss 0.579509.
Train: 2018-08-05T08:30:08.435338: step 7599, loss 0.519498.
Train: 2018-08-05T08:30:11.982587: step 7600, loss 0.613777.
Test: 2018-08-05T08:30:27.109186: step 7600, loss 0.548837.
Train: 2018-08-05T08:30:32.969177: step 7601, loss 0.553853.
Train: 2018-08-05T08:30:36.469547: step 7602, loss 0.494013.
Train: 2018-08-05T08:30:39.969916: step 7603, loss 0.570882.
Train: 2018-08-05T08:30:43.548419: step 7604, loss 0.493852.
Train: 2018-08-05T08:30:47.080042: step 7605, loss 0.570915.
Train: 2018-08-05T08:30:50.611664: step 7606, loss 0.588104.
Train: 2018-08-05T08:30:54.158914: step 7607, loss 0.588106.
Train: 2018-08-05T08:30:57.690536: step 7608, loss 0.622452.
Train: 2018-08-05T08:31:01.206533: step 7609, loss 0.545186.
Train: 2018-08-05T08:31:04.738156: step 7610, loss 0.493798.
Test: 2018-08-05T08:31:19.817873: step 7610, loss 0.547821.
Train: 2018-08-05T08:31:23.349496: step 7611, loss 0.648059.
Train: 2018-08-05T08:31:26.912371: step 7612, loss 0.579458.
Train: 2018-08-05T08:31:30.443994: step 7613, loss 0.63927.
Train: 2018-08-05T08:31:33.959991: step 7614, loss 0.579385.
Train: 2018-08-05T08:31:37.475986: step 7615, loss 0.51137.
Train: 2018-08-05T08:31:41.054489: step 7616, loss 0.553869.
Train: 2018-08-05T08:31:44.601739: step 7617, loss 0.553883.
Train: 2018-08-05T08:31:48.117735: step 7618, loss 0.494653.
Train: 2018-08-05T08:31:51.727491: step 7619, loss 0.536949.
Train: 2018-08-05T08:31:55.259113: step 7620, loss 0.545394.
Test: 2018-08-05T08:32:10.416964: step 7620, loss 0.547972.
Train: 2018-08-05T08:32:13.979840: step 7621, loss 0.587821.
Train: 2018-08-05T08:32:17.573969: step 7622, loss 0.54535.
Train: 2018-08-05T08:32:21.121219: step 7623, loss 0.562344.
Train: 2018-08-05T08:32:24.684095: step 7624, loss 0.613436.
Train: 2018-08-05T08:32:28.184464: step 7625, loss 0.451657.
Train: 2018-08-05T08:32:31.716087: step 7626, loss 0.477007.
Train: 2018-08-05T08:32:35.263336: step 7627, loss 0.605164.
Train: 2018-08-05T08:32:38.794959: step 7628, loss 0.631031.
Train: 2018-08-05T08:32:42.342209: step 7629, loss 0.562335.
Train: 2018-08-05T08:32:45.858205: step 7630, loss 0.631129.
Test: 2018-08-05T08:33:01.062935: step 7630, loss 0.546009.
Train: 2018-08-05T08:33:04.625811: step 7631, loss 0.502193.
Train: 2018-08-05T08:33:08.157434: step 7632, loss 0.519358.
Train: 2018-08-05T08:33:11.689056: step 7633, loss 0.62258.
Train: 2018-08-05T08:33:15.220679: step 7634, loss 0.60536.
Train: 2018-08-05T08:33:18.783555: step 7635, loss 0.579526.
Train: 2018-08-05T08:33:22.346432: step 7636, loss 0.527999.
Train: 2018-08-05T08:33:25.909308: step 7637, loss 0.536597.
Train: 2018-08-05T08:33:29.425304: step 7638, loss 0.588074.
Train: 2018-08-05T08:33:33.035060: step 7639, loss 0.579485.
Train: 2018-08-05T08:33:36.597936: step 7640, loss 0.519497.
Test: 2018-08-05T08:33:51.740159: step 7640, loss 0.548222.
Train: 2018-08-05T08:33:55.287409: step 7641, loss 0.553767.
Train: 2018-08-05T08:33:58.834658: step 7642, loss 0.536623.
Train: 2018-08-05T08:34:02.350654: step 7643, loss 0.502294.
Train: 2018-08-05T08:34:05.882277: step 7644, loss 0.562335.
Train: 2018-08-05T08:34:09.398273: step 7645, loss 0.588161.
Train: 2018-08-05T08:34:13.023656: step 7646, loss 0.570953.
Train: 2018-08-05T08:34:16.602158: step 7647, loss 0.570959.
Train: 2018-08-05T08:34:20.149408: step 7648, loss 0.519214.
Train: 2018-08-05T08:34:23.696658: step 7649, loss 0.579608.
Train: 2018-08-05T08:34:27.212654: step 7650, loss 0.57098.
Test: 2018-08-05T08:34:42.386132: step 7650, loss 0.54711.
Train: 2018-08-05T08:34:45.964633: step 7651, loss 0.588269.
Train: 2018-08-05T08:34:49.480629: step 7652, loss 0.588261.
Train: 2018-08-05T08:34:53.043505: step 7653, loss 0.553706.
Train: 2018-08-05T08:34:56.543874: step 7654, loss 0.657236.
Train: 2018-08-05T08:35:00.075497: step 7655, loss 0.493499.
Train: 2018-08-05T08:35:03.622747: step 7656, loss 0.553739.
Train: 2018-08-05T08:35:07.154369: step 7657, loss 0.596696.
Train: 2018-08-05T08:35:10.701619: step 7658, loss 0.579491.
Train: 2018-08-05T08:35:14.233242: step 7659, loss 0.622284.
Train: 2018-08-05T08:35:17.780492: step 7660, loss 0.545257.
Test: 2018-08-05T08:35:32.953968: step 7660, loss 0.547715.
Train: 2018-08-05T08:35:36.469964: step 7661, loss 0.604952.
Train: 2018-08-05T08:35:40.001587: step 7662, loss 0.647329.
Train: 2018-08-05T08:35:43.501957: step 7663, loss 0.528512.
Train: 2018-08-05T08:35:47.033580: step 7664, loss 0.486456.
Train: 2018-08-05T08:35:50.580829: step 7665, loss 0.520247.
Train: 2018-08-05T08:35:54.174958: step 7666, loss 0.545524.
Train: 2018-08-05T08:35:57.737834: step 7667, loss 0.604513.
Train: 2018-08-05T08:36:01.269457: step 7668, loss 0.579222.
Train: 2018-08-05T08:36:04.785453: step 7669, loss 0.520289.
Train: 2018-08-05T08:36:08.301449: step 7670, loss 0.528695.
Test: 2018-08-05T08:36:23.428047: step 7670, loss 0.548488.
Train: 2018-08-05T08:36:27.006549: step 7671, loss 0.604518.
Train: 2018-08-05T08:36:30.631931: step 7672, loss 0.52865.
Train: 2018-08-05T08:36:34.194808: step 7673, loss 0.596119.
Train: 2018-08-05T08:36:37.710804: step 7674, loss 0.528611.
Train: 2018-08-05T08:36:41.226800: step 7675, loss 0.638383.
Train: 2018-08-05T08:36:44.742796: step 7676, loss 0.579246.
Train: 2018-08-05T08:36:48.305672: step 7677, loss 0.520213.
Train: 2018-08-05T08:36:51.852922: step 7678, loss 0.56237.
Train: 2018-08-05T08:36:55.384545: step 7679, loss 0.553936.
Train: 2018-08-05T08:36:58.900541: step 7680, loss 0.486444.
Test: 2018-08-05T08:37:14.027138: step 7680, loss 0.546882.
Train: 2018-08-05T08:37:17.621266: step 7681, loss 0.613079.
Train: 2018-08-05T08:37:21.168516: step 7682, loss 0.545439.
Train: 2018-08-05T08:37:24.731393: step 7683, loss 0.587762.
Train: 2018-08-05T08:37:28.278642: step 7684, loss 0.596245.
Train: 2018-08-05T08:37:31.810264: step 7685, loss 0.545413.
Train: 2018-08-05T08:37:35.326260: step 7686, loss 0.63013.
Train: 2018-08-05T08:37:38.857883: step 7687, loss 0.528513.
Train: 2018-08-05T08:37:42.436386: step 7688, loss 0.613107.
Train: 2018-08-05T08:37:45.999262: step 7689, loss 0.511685.
Train: 2018-08-05T08:37:49.609018: step 7690, loss 0.596146.
Test: 2018-08-05T08:38:04.751241: step 7690, loss 0.548662.
Train: 2018-08-05T08:38:08.267237: step 7691, loss 0.511733.
Train: 2018-08-05T08:38:11.798860: step 7692, loss 0.503265.
Train: 2018-08-05T08:38:15.330483: step 7693, loss 0.59619.
Train: 2018-08-05T08:38:18.877732: step 7694, loss 0.536961.
Train: 2018-08-05T08:38:22.471863: step 7695, loss 0.562352.
Train: 2018-08-05T08:38:25.987858: step 7696, loss 0.570837.
Train: 2018-08-05T08:38:29.503854: step 7697, loss 0.596331.
Train: 2018-08-05T08:38:33.066730: step 7698, loss 0.460372.
Train: 2018-08-05T08:38:36.582727: step 7699, loss 0.587896.
Train: 2018-08-05T08:38:40.098723: step 7700, loss 0.519681.
Test: 2018-08-05T08:38:55.194066: step 7700, loss 0.546873.
Train: 2018-08-05T08:38:59.100728: step 7701, loss 0.544093.
Train: 2018-08-05T08:39:02.647978: step 7702, loss 0.579481.
Train: 2018-08-05T08:39:06.179601: step 7703, loss 0.588102.
Train: 2018-08-05T08:39:09.742477: step 7704, loss 0.562335.
Train: 2018-08-05T08:39:13.242847: step 7705, loss 0.553729.
Train: 2018-08-05T08:39:16.758843: step 7706, loss 0.536491.
Train: 2018-08-05T08:39:20.337345: step 7707, loss 0.562338.
Train: 2018-08-05T08:39:23.868969: step 7708, loss 0.614164.
Train: 2018-08-05T08:39:27.400591: step 7709, loss 0.519154.
Train: 2018-08-05T08:39:30.900961: step 7710, loss 0.545053.
Test: 2018-08-05T08:39:46.152571: step 7710, loss 0.54829.
Train: 2018-08-05T08:39:49.715446: step 7711, loss 0.536385.
Train: 2018-08-05T08:39:53.215816: step 7712, loss 0.562344.
Train: 2018-08-05T08:39:56.763066: step 7713, loss 0.518973.
Train: 2018-08-05T08:40:00.310315: step 7714, loss 0.597118.
Train: 2018-08-05T08:40:03.826311: step 7715, loss 0.597155.
Train: 2018-08-05T08:40:07.357934: step 7716, loss 0.623249.
Train: 2018-08-05T08:40:10.905183: step 7717, loss 0.492852.
Train: 2018-08-05T08:40:14.405552: step 7718, loss 0.544977.
Train: 2018-08-05T08:40:17.968429: step 7719, loss 0.527586.
Train: 2018-08-05T08:40:21.484425: step 7720, loss 0.614559.
Test: 2018-08-05T08:40:36.611022: step 7720, loss 0.548229.
Train: 2018-08-05T08:40:40.142644: step 7721, loss 0.553657.
Train: 2018-08-05T08:40:43.658641: step 7722, loss 0.518872.
Train: 2018-08-05T08:40:47.237143: step 7723, loss 0.588464.
Train: 2018-08-05T08:40:50.753139: step 7724, loss 0.544952.
Train: 2018-08-05T08:40:54.300396: step 7725, loss 0.562357.
Train: 2018-08-05T08:40:57.847638: step 7726, loss 0.544947.
Train: 2018-08-05T08:41:01.394888: step 7727, loss 0.571068.
Train: 2018-08-05T08:41:04.942137: step 7728, loss 0.579778.
Train: 2018-08-05T08:41:08.458133: step 7729, loss 0.562358.
Train: 2018-08-05T08:41:11.958503: step 7730, loss 0.57976.
Test: 2018-08-05T08:41:27.116353: step 7730, loss 0.547633.
Train: 2018-08-05T08:41:30.632349: step 7731, loss 0.631911.
Train: 2018-08-05T08:41:34.242105: step 7732, loss 0.614396.
Train: 2018-08-05T08:41:37.773728: step 7733, loss 0.614223.
Train: 2018-08-05T08:41:41.336604: step 7734, loss 0.49344.
Train: 2018-08-05T08:41:44.836973: step 7735, loss 0.588113.
Train: 2018-08-05T08:41:48.384223: step 7736, loss 0.562335.
Train: 2018-08-05T08:41:51.915845: step 7737, loss 0.613637.
Train: 2018-08-05T08:41:55.494349: step 7738, loss 0.536773.
Train: 2018-08-05T08:41:59.088478: step 7739, loss 0.553842.
Train: 2018-08-05T08:42:02.666981: step 7740, loss 0.536892.
Test: 2018-08-05T08:42:17.762325: step 7740, loss 0.548977.
Train: 2018-08-05T08:42:21.325200: step 7741, loss 0.587781.
Train: 2018-08-05T08:42:24.856823: step 7742, loss 0.57082.
Train: 2018-08-05T08:42:28.404072: step 7743, loss 0.545463.
Train: 2018-08-05T08:42:31.935695: step 7744, loss 0.596132.
Train: 2018-08-05T08:42:35.545451: step 7745, loss 0.494945.
Train: 2018-08-05T08:42:39.077074: step 7746, loss 0.570801.
Train: 2018-08-05T08:42:42.593070: step 7747, loss 0.638236.
Train: 2018-08-05T08:42:46.234079: step 7748, loss 0.43614.
Train: 2018-08-05T08:42:49.781329: step 7749, loss 0.60451.
Train: 2018-08-05T08:42:53.359831: step 7750, loss 0.596094.
Test: 2018-08-05T08:43:08.470802: step 7750, loss 0.547714.
Train: 2018-08-05T08:43:12.033678: step 7751, loss 0.612941.
Train: 2018-08-05T08:43:15.549674: step 7752, loss 0.579211.
Train: 2018-08-05T08:43:19.096923: step 7753, loss 0.545575.
Train: 2018-08-05T08:43:22.612919: step 7754, loss 0.545595.
Train: 2018-08-05T08:43:26.160169: step 7755, loss 0.587571.
Train: 2018-08-05T08:43:29.723045: step 7756, loss 0.562394.
Train: 2018-08-05T08:43:33.317174: step 7757, loss 0.579161.
Train: 2018-08-05T08:43:36.817545: step 7758, loss 0.545653.
Train: 2018-08-05T08:43:40.396047: step 7759, loss 0.520544.
Train: 2018-08-05T08:43:43.958922: step 7760, loss 0.554021.
Test: 2018-08-05T08:43:59.210532: step 7760, loss 0.54781.
Train: 2018-08-05T08:44:02.804662: step 7761, loss 0.537236.
Train: 2018-08-05T08:44:06.351912: step 7762, loss 0.503595.
Train: 2018-08-05T08:44:09.899161: step 7763, loss 0.58764.
Train: 2018-08-05T08:44:13.446411: step 7764, loss 0.570805.
Train: 2018-08-05T08:44:17.009287: step 7765, loss 0.562361.
Train: 2018-08-05T08:44:20.556536: step 7766, loss 0.562356.
Train: 2018-08-05T08:44:24.072532: step 7767, loss 0.587782.
Train: 2018-08-05T08:44:27.619782: step 7768, loss 0.579316.
Train: 2018-08-05T08:44:31.198284: step 7769, loss 0.502949.
Train: 2018-08-05T08:44:34.714280: step 7770, loss 0.511348.
Test: 2018-08-05T08:44:49.809624: step 7770, loss 0.547521.
Train: 2018-08-05T08:44:53.356873: step 7771, loss 0.519734.
Train: 2018-08-05T08:44:56.951003: step 7772, loss 0.587984.
Train: 2018-08-05T08:45:00.466999: step 7773, loss 0.630887.
Train: 2018-08-05T08:45:04.014248: step 7774, loss 0.545186.
Train: 2018-08-05T08:45:07.608377: step 7775, loss 0.519424.
Train: 2018-08-05T08:45:11.155627: step 7776, loss 0.527948.
Train: 2018-08-05T08:45:14.655996: step 7777, loss 0.545105.
Train: 2018-08-05T08:45:18.156366: step 7778, loss 0.683242.
Train: 2018-08-05T08:45:21.672362: step 7779, loss 0.562339.
Train: 2018-08-05T08:45:25.219612: step 7780, loss 0.605474.
Test: 2018-08-05T08:45:40.268075: step 7780, loss 0.54775.
Train: 2018-08-05T08:45:43.846578: step 7781, loss 0.553721.
Train: 2018-08-05T08:45:47.409454: step 7782, loss 0.55373.
Train: 2018-08-05T08:45:50.941076: step 7783, loss 0.510748.
Train: 2018-08-05T08:45:54.472699: step 7784, loss 0.519335.
Train: 2018-08-05T08:45:58.051203: step 7785, loss 0.536507.
Train: 2018-08-05T08:46:01.567198: step 7786, loss 0.579582.
Train: 2018-08-05T08:46:05.161328: step 7787, loss 0.527816.
Train: 2018-08-05T08:46:08.771084: step 7788, loss 0.562341.
Train: 2018-08-05T08:46:12.318333: step 7789, loss 0.536378.
Train: 2018-08-05T08:46:15.834329: step 7790, loss 0.579684.
Test: 2018-08-05T08:46:31.054686: step 7790, loss 0.549654.
Train: 2018-08-05T08:46:34.570682: step 7791, loss 0.544993.
Train: 2018-08-05T08:46:38.149185: step 7792, loss 0.623174.
Train: 2018-08-05T08:46:41.743316: step 7793, loss 0.544978.
Train: 2018-08-05T08:46:45.306190: step 7794, loss 0.605788.
Train: 2018-08-05T08:46:48.822187: step 7795, loss 0.588386.
Train: 2018-08-05T08:46:52.338183: step 7796, loss 0.519014.
Train: 2018-08-05T08:46:55.901059: step 7797, loss 0.562344.
Train: 2018-08-05T08:46:59.432681: step 7798, loss 0.62295.
Train: 2018-08-05T08:47:02.995557: step 7799, loss 0.56234.
Train: 2018-08-05T08:47:06.574060: step 7800, loss 0.570966.
Test: 2018-08-05T08:47:21.747538: step 7800, loss 0.548546.
Train: 2018-08-05T08:47:27.388758: step 7801, loss 0.510657.
Train: 2018-08-05T08:47:30.920380: step 7802, loss 0.648424.
Train: 2018-08-05T08:47:34.436377: step 7803, loss 0.527979.
Train: 2018-08-05T08:47:37.967999: step 7804, loss 0.588066.
Train: 2018-08-05T08:47:41.530875: step 7805, loss 0.493845.
Train: 2018-08-05T08:47:45.046871: step 7806, loss 0.596574.
Train: 2018-08-05T08:47:48.547241: step 7807, loss 0.519578.
Train: 2018-08-05T08:47:52.110117: step 7808, loss 0.562336.
Train: 2018-08-05T08:47:55.641740: step 7809, loss 0.553783.
Train: 2018-08-05T08:47:59.173363: step 7810, loss 0.553781.
Test: 2018-08-05T08:48:14.284332: step 7810, loss 0.548437.
Train: 2018-08-05T08:48:17.831582: step 7811, loss 0.510988.
Train: 2018-08-05T08:48:21.394458: step 7812, loss 0.545195.
Train: 2018-08-05T08:48:24.941708: step 7813, loss 0.536583.
Train: 2018-08-05T08:48:28.504584: step 7814, loss 0.545133.
Train: 2018-08-05T08:48:32.083086: step 7815, loss 0.545099.
Train: 2018-08-05T08:48:35.630337: step 7816, loss 0.553702.
Train: 2018-08-05T08:48:39.193212: step 7817, loss 0.510405.
Train: 2018-08-05T08:48:42.771715: step 7818, loss 0.588394.
Train: 2018-08-05T08:48:46.287711: step 7819, loss 0.562356.
Train: 2018-08-05T08:48:49.834961: step 7820, loss 0.605929.
Test: 2018-08-05T08:49:04.961559: step 7820, loss 0.546398.
Train: 2018-08-05T08:49:08.493180: step 7821, loss 0.553645.
Train: 2018-08-05T08:49:12.040430: step 7822, loss 0.597258.
Train: 2018-08-05T08:49:15.603306: step 7823, loss 0.510042.
Train: 2018-08-05T08:49:19.181808: step 7824, loss 0.632178.
Train: 2018-08-05T08:49:22.697805: step 7825, loss 0.623392.
Train: 2018-08-05T08:49:26.229428: step 7826, loss 0.536258.
Train: 2018-08-05T08:49:29.761050: step 7827, loss 0.553665.
Train: 2018-08-05T08:49:33.277046: step 7828, loss 0.544998.
Train: 2018-08-05T08:49:36.839922: step 7829, loss 0.588349.
Train: 2018-08-05T08:49:40.371545: step 7830, loss 0.553687.
Test: 2018-08-05T08:49:55.435635: step 7830, loss 0.54631.
Train: 2018-08-05T08:49:59.014138: step 7831, loss 0.60557.
Train: 2018-08-05T08:50:02.545761: step 7832, loss 0.562338.
Train: 2018-08-05T08:50:06.061757: step 7833, loss 0.47621.
Train: 2018-08-05T08:50:09.562127: step 7834, loss 0.614018.
Train: 2018-08-05T08:50:13.125003: step 7835, loss 0.553731.
Train: 2018-08-05T08:50:16.672252: step 7836, loss 0.613922.
Train: 2018-08-05T08:50:20.203874: step 7837, loss 0.570917.
Train: 2018-08-05T08:50:23.719871: step 7838, loss 0.468108.
Train: 2018-08-05T08:50:27.282747: step 7839, loss 0.596611.
Train: 2018-08-05T08:50:30.783116: step 7840, loss 0.5709.
Test: 2018-08-05T08:50:45.847206: step 7840, loss 0.549026.
Train: 2018-08-05T08:50:49.347576: step 7841, loss 0.459621.
Train: 2018-08-05T08:50:52.926079: step 7842, loss 0.562335.
Train: 2018-08-05T08:50:56.504581: step 7843, loss 0.631021.
Train: 2018-08-05T08:51:00.083084: step 7844, loss 0.502246.
Train: 2018-08-05T08:51:03.599080: step 7845, loss 0.536555.
Train: 2018-08-05T08:51:07.130703: step 7846, loss 0.527912.
Train: 2018-08-05T08:51:10.677953: step 7847, loss 0.536467.
Train: 2018-08-05T08:51:14.209576: step 7848, loss 0.536411.
Train: 2018-08-05T08:51:17.788078: step 7849, loss 0.53635.
Train: 2018-08-05T08:51:21.304074: step 7850, loss 0.571041.
Test: 2018-08-05T08:51:36.415044: step 7850, loss 0.546813.
Train: 2018-08-05T08:51:39.993547: step 7851, loss 0.614609.
Train: 2018-08-05T08:51:41.728105: step 7852, loss 0.692514.
Train: 2018-08-05T08:51:45.259728: step 7853, loss 0.510168.
Train: 2018-08-05T08:51:48.838231: step 7854, loss 0.536281.
Train: 2018-08-05T08:51:52.369854: step 7855, loss 0.623167.
Train: 2018-08-05T08:51:55.917103: step 7856, loss 0.614391.
Train: 2018-08-05T08:51:59.448725: step 7857, loss 0.47583.
Train: 2018-08-05T08:52:02.964722: step 7858, loss 0.570987.
Train: 2018-08-05T08:52:06.527599: step 7859, loss 0.536423.
Train: 2018-08-05T08:52:10.106101: step 7860, loss 0.562339.
Test: 2018-08-05T08:52:25.232697: step 7860, loss 0.54772.
Train: 2018-08-05T08:52:28.795574: step 7861, loss 0.605513.
Train: 2018-08-05T08:52:32.342823: step 7862, loss 0.588211.
Train: 2018-08-05T08:52:35.858819: step 7863, loss 0.536506.
Train: 2018-08-05T08:52:39.406069: step 7864, loss 0.562334.
Train: 2018-08-05T08:52:42.906438: step 7865, loss 0.54515.
Train: 2018-08-05T08:52:46.469314: step 7866, loss 0.579512.
Train: 2018-08-05T08:52:50.047817: step 7867, loss 0.579502.
Train: 2018-08-05T08:52:53.579440: step 7868, loss 0.53662.
Train: 2018-08-05T08:52:57.126690: step 7869, loss 0.562339.
Train: 2018-08-05T08:53:00.658312: step 7870, loss 0.562331.
Test: 2018-08-05T08:53:15.863044: step 7870, loss 0.549224.
Train: 2018-08-05T08:53:19.394665: step 7871, loss 0.613684.
Train: 2018-08-05T08:53:22.973167: step 7872, loss 0.562344.
Train: 2018-08-05T08:53:26.536044: step 7873, loss 0.570874.
Train: 2018-08-05T08:53:30.114546: step 7874, loss 0.596425.
Train: 2018-08-05T08:53:33.677422: step 7875, loss 0.51983.
Train: 2018-08-05T08:53:37.255925: step 7876, loss 0.528372.
Train: 2018-08-05T08:53:40.803175: step 7877, loss 0.579336.
Train: 2018-08-05T08:53:44.366051: step 7878, loss 0.604826.
Train: 2018-08-05T08:53:47.913300: step 7879, loss 0.562353.
Train: 2018-08-05T08:53:51.444923: step 7880, loss 0.570829.
Test: 2018-08-05T08:54:06.587147: step 7880, loss 0.548801.
Train: 2018-08-05T08:54:10.118769: step 7881, loss 0.562356.
Train: 2018-08-05T08:54:13.666018: step 7882, loss 0.545442.
Train: 2018-08-05T08:54:17.275774: step 7883, loss 0.528542.
Train: 2018-08-05T08:54:20.838650: step 7884, loss 0.536985.
Train: 2018-08-05T08:54:24.385900: step 7885, loss 0.59622.
Train: 2018-08-05T08:54:27.948776: step 7886, loss 0.570823.
Train: 2018-08-05T08:54:31.464773: step 7887, loss 0.570823.
Train: 2018-08-05T08:54:35.043275: step 7888, loss 0.570822.
Train: 2018-08-05T08:54:38.543646: step 7889, loss 0.587747.
Train: 2018-08-05T08:54:42.090894: step 7890, loss 0.536989.
Test: 2018-08-05T08:54:57.248743: step 7890, loss 0.547848.
Train: 2018-08-05T08:55:00.795994: step 7891, loss 0.528535.
Train: 2018-08-05T08:55:04.311990: step 7892, loss 0.672365.
Train: 2018-08-05T08:55:07.874866: step 7893, loss 0.587706.
Train: 2018-08-05T08:55:11.390862: step 7894, loss 0.511789.
Train: 2018-08-05T08:55:14.938111: step 7895, loss 0.562371.
Train: 2018-08-05T08:55:18.516614: step 7896, loss 0.528685.
Train: 2018-08-05T08:55:22.079490: step 7897, loss 0.570817.
Train: 2018-08-05T08:55:25.642366: step 7898, loss 0.587655.
Train: 2018-08-05T08:55:29.173989: step 7899, loss 0.56234.
Train: 2018-08-05T08:55:32.705612: step 7900, loss 0.545397.
Test: 2018-08-05T08:55:47.832208: step 7900, loss 0.548031.
Train: 2018-08-05T08:55:53.676575: step 7901, loss 0.587519.
Train: 2018-08-05T08:55:57.239452: step 7902, loss 0.6301.
Train: 2018-08-05T08:56:00.817954: step 7903, loss 0.563218.
Train: 2018-08-05T08:56:04.349577: step 7904, loss 0.655388.
Train: 2018-08-05T08:56:07.849947: step 7905, loss 0.520702.
Train: 2018-08-05T08:56:11.350316: step 7906, loss 0.620835.
Train: 2018-08-05T08:56:14.897565: step 7907, loss 0.604034.
Train: 2018-08-05T08:56:18.429188: step 7908, loss 0.512739.
Train: 2018-08-05T08:56:22.007691: step 7909, loss 0.562484.
Train: 2018-08-05T08:56:25.539314: step 7910, loss 0.587274.
Test: 2018-08-05T08:56:40.619031: step 7910, loss 0.548555.
Train: 2018-08-05T08:56:44.166280: step 7911, loss 0.537782.
Train: 2018-08-05T08:56:47.729156: step 7912, loss 0.529577.
Train: 2018-08-05T08:56:51.245152: step 7913, loss 0.56252.
Train: 2018-08-05T08:56:54.808028: step 7914, loss 0.554278.
Train: 2018-08-05T08:56:58.386531: step 7915, loss 0.546023.
Train: 2018-08-05T08:57:01.918154: step 7916, loss 0.620277.
Train: 2018-08-05T08:57:05.434150: step 7917, loss 0.554252.
Train: 2018-08-05T08:57:08.981400: step 7918, loss 0.628534.
Train: 2018-08-05T08:57:12.513022: step 7919, loss 0.620223.
Train: 2018-08-05T08:57:16.060272: step 7920, loss 0.537853.
Test: 2018-08-05T08:57:31.296255: step 7920, loss 0.549011.
Train: 2018-08-05T08:57:34.874758: step 7921, loss 0.521463.
Train: 2018-08-05T08:57:38.453260: step 7922, loss 0.472163.
Train: 2018-08-05T08:57:42.016136: step 7923, loss 0.628405.
Train: 2018-08-05T08:57:45.516506: step 7924, loss 0.496583.
Train: 2018-08-05T08:57:49.079382: step 7925, loss 0.562495.
Train: 2018-08-05T08:57:52.626632: step 7926, loss 0.595596.
Train: 2018-08-05T08:57:56.173881: step 7927, loss 0.6288.
Train: 2018-08-05T08:57:59.752384: step 7928, loss 0.545884.
Train: 2018-08-05T08:58:03.284006: step 7929, loss 0.570758.
Train: 2018-08-05T08:58:06.956270: step 7930, loss 0.620547.
Test: 2018-08-05T08:58:22.739186: step 7930, loss 0.548238.
Train: 2018-08-05T08:58:26.239555: step 7931, loss 0.487846.
Train: 2018-08-05T08:58:29.771177: step 7932, loss 0.554158.
Train: 2018-08-05T08:58:33.318427: step 7933, loss 0.545828.
Train: 2018-08-05T08:58:36.881303: step 7934, loss 0.520814.
Train: 2018-08-05T08:58:40.397299: step 7935, loss 0.545729.
Train: 2018-08-05T08:58:43.960175: step 7936, loss 0.579144.
Train: 2018-08-05T08:58:47.569931: step 7937, loss 0.595946.
Train: 2018-08-05T08:58:51.085928: step 7938, loss 0.579186.
Train: 2018-08-05T08:58:54.664430: step 7939, loss 0.553975.
Train: 2018-08-05T08:58:58.211679: step 7940, loss 0.553962.
Test: 2018-08-05T08:59:13.353903: step 7940, loss 0.548493.
Train: 2018-08-05T08:59:16.869899: step 7941, loss 0.553946.
Train: 2018-08-05T08:59:20.432775: step 7942, loss 0.520177.
Train: 2018-08-05T08:59:23.933145: step 7943, loss 0.528533.
Train: 2018-08-05T08:59:27.480394: step 7944, loss 0.570831.
Train: 2018-08-05T08:59:31.012017: step 7945, loss 0.553846.
Train: 2018-08-05T08:59:34.543640: step 7946, loss 0.587899.
Train: 2018-08-05T08:59:38.075262: step 7947, loss 0.545274.
Train: 2018-08-05T08:59:41.622512: step 7948, loss 0.562336.
Train: 2018-08-05T08:59:45.154135: step 7949, loss 0.519534.
Train: 2018-08-05T08:59:48.685757: step 7950, loss 0.579494.
Test: 2018-08-05T09:00:03.859234: step 7950, loss 0.547785.
Train: 2018-08-05T09:00:07.468990: step 7951, loss 0.648274.
Train: 2018-08-05T09:00:11.016240: step 7952, loss 0.510807.
Train: 2018-08-05T09:00:14.547863: step 7953, loss 0.588113.
Train: 2018-08-05T09:00:18.063859: step 7954, loss 0.562325.
Train: 2018-08-05T09:00:21.642362: step 7955, loss 0.510764.
Train: 2018-08-05T09:00:25.220865: step 7956, loss 0.510784.
Train: 2018-08-05T09:00:28.768114: step 7957, loss 0.56235.
Train: 2018-08-05T09:00:32.330990: step 7958, loss 0.588278.
Train: 2018-08-05T09:00:35.831359: step 7959, loss 0.562341.
Train: 2018-08-05T09:00:39.362982: step 7960, loss 0.545013.
Test: 2018-08-05T09:00:54.614593: step 7960, loss 0.547066.
Train: 2018-08-05T09:00:58.161841: step 7961, loss 0.492956.
Train: 2018-08-05T09:01:01.709091: step 7962, loss 0.518895.
Train: 2018-08-05T09:01:05.240714: step 7963, loss 0.553537.
Train: 2018-08-05T09:01:08.772337: step 7964, loss 0.518386.
Train: 2018-08-05T09:01:12.303959: step 7965, loss 0.526597.
Train: 2018-08-05T09:01:15.882462: step 7966, loss 0.589417.
Train: 2018-08-05T09:01:19.414085: step 7967, loss 0.545352.
Train: 2018-08-05T09:01:22.976960: step 7968, loss 0.554125.
Train: 2018-08-05T09:01:26.524210: step 7969, loss 0.516775.
Train: 2018-08-05T09:01:30.040206: step 7970, loss 0.553384.
Test: 2018-08-05T09:01:45.135552: step 7970, loss 0.547752.
Train: 2018-08-05T09:01:48.682799: step 7971, loss 0.553323.
Train: 2018-08-05T09:01:52.292556: step 7972, loss 0.547152.
Train: 2018-08-05T09:01:55.824178: step 7973, loss 0.5364.
Train: 2018-08-05T09:01:59.402681: step 7974, loss 0.58177.
Train: 2018-08-05T09:02:02.918678: step 7975, loss 0.644318.
Train: 2018-08-05T09:02:06.481553: step 7976, loss 0.616927.
Train: 2018-08-05T09:02:10.044429: step 7977, loss 0.580322.
Train: 2018-08-05T09:02:13.544799: step 7978, loss 0.57136.
Train: 2018-08-05T09:02:17.060795: step 7979, loss 0.597849.
Train: 2018-08-05T09:02:20.608044: step 7980, loss 0.483043.
Test: 2018-08-05T09:02:35.781521: step 7980, loss 0.546895.
Train: 2018-08-05T09:02:39.328771: step 7981, loss 0.56241.
Train: 2018-08-05T09:02:42.891647: step 7982, loss 0.448131.
Train: 2018-08-05T09:02:46.501402: step 7983, loss 0.650363.
Train: 2018-08-05T09:02:50.048652: step 7984, loss 0.553613.
Train: 2018-08-05T09:02:53.580275: step 7985, loss 0.606262.
Train: 2018-08-05T09:02:57.111897: step 7986, loss 0.553625.
Train: 2018-08-05T09:03:00.643520: step 7987, loss 0.51867.
Train: 2018-08-05T09:03:04.206396: step 7988, loss 0.527437.
Train: 2018-08-05T09:03:07.784899: step 7989, loss 0.536173.
Train: 2018-08-05T09:03:11.300896: step 7990, loss 0.553635.
Test: 2018-08-05T09:03:26.443119: step 7990, loss 0.546974.
Train: 2018-08-05T09:03:30.005995: step 7991, loss 0.527421.
Train: 2018-08-05T09:03:33.568871: step 7992, loss 0.597358.
Train: 2018-08-05T09:03:37.084867: step 7993, loss 0.483661.
Train: 2018-08-05T09:03:40.725876: step 7994, loss 0.615145.
Train: 2018-08-05T09:03:44.335632: step 7995, loss 0.562516.
Train: 2018-08-05T09:03:47.867255: step 7996, loss 0.571158.
Train: 2018-08-05T09:03:51.430131: step 7997, loss 0.544862.
Train: 2018-08-05T09:03:55.071141: step 7998, loss 0.571138.
Train: 2018-08-05T09:03:58.618390: step 7999, loss 0.641161.
Train: 2018-08-05T09:04:02.181266: step 8000, loss 0.579842.
Test: 2018-08-05T09:04:17.354744: step 8000, loss 0.548208.
Train: 2018-08-05T09:04:23.183483: step 8001, loss 0.518784.
Train: 2018-08-05T09:04:26.668226: step 8002, loss 0.562357.
Train: 2018-08-05T09:04:28.387157: step 8003, loss 0.48817.
Train: 2018-08-05T09:04:31.918780: step 8004, loss 0.536266.
Train: 2018-08-05T09:04:35.434776: step 8005, loss 0.57106.
Train: 2018-08-05T09:04:38.997652: step 8006, loss 0.614596.
Train: 2018-08-05T09:04:42.529275: step 8007, loss 0.588453.
Train: 2018-08-05T09:04:46.029645: step 8008, loss 0.562352.
Train: 2018-08-05T09:04:49.545641: step 8009, loss 0.562348.
Train: 2018-08-05T09:04:53.124143: step 8010, loss 0.60567.
Test: 2018-08-05T09:05:08.266367: step 8010, loss 0.547104.
Train: 2018-08-05T09:05:11.829243: step 8011, loss 0.545048.
Train: 2018-08-05T09:05:15.376492: step 8012, loss 0.510542.
Train: 2018-08-05T09:05:18.908115: step 8013, loss 0.570967.
Train: 2018-08-05T09:05:22.486618: step 8014, loss 0.562338.
Train: 2018-08-05T09:05:26.049494: step 8015, loss 0.588194.
Train: 2018-08-05T09:05:29.581117: step 8016, loss 0.639819.
Train: 2018-08-05T09:05:33.159620: step 8017, loss 0.553749.
Train: 2018-08-05T09:05:36.738122: step 8018, loss 0.485237.
Train: 2018-08-05T09:05:40.316625: step 8019, loss 0.536647.
Train: 2018-08-05T09:05:43.816994: step 8020, loss 0.553771.
Test: 2018-08-05T09:05:59.021726: step 8020, loss 0.548422.
Train: 2018-08-05T09:06:02.537721: step 8021, loss 0.60517.
Train: 2018-08-05T09:06:06.100597: step 8022, loss 0.510968.
Train: 2018-08-05T09:06:09.616593: step 8023, loss 0.55377.
Train: 2018-08-05T09:06:13.163842: step 8024, loss 0.528052.
Train: 2018-08-05T09:06:16.773598: step 8025, loss 0.528008.
Train: 2018-08-05T09:06:20.305222: step 8026, loss 0.605323.
Train: 2018-08-05T09:06:23.821217: step 8027, loss 0.588147.
Train: 2018-08-05T09:06:27.368467: step 8028, loss 0.527919.
Train: 2018-08-05T09:06:30.884464: step 8029, loss 0.562336.
Train: 2018-08-05T09:06:34.447339: step 8030, loss 0.614032.
Test: 2018-08-05T09:06:49.573935: step 8030, loss 0.546964.
Train: 2018-08-05T09:06:53.121185: step 8031, loss 0.553725.
Train: 2018-08-05T09:06:56.699688: step 8032, loss 0.553728.
Train: 2018-08-05T09:07:00.215684: step 8033, loss 0.55373.
Train: 2018-08-05T09:07:03.778560: step 8034, loss 0.527913.
Train: 2018-08-05T09:07:07.278930: step 8035, loss 0.570948.
Train: 2018-08-05T09:07:10.794926: step 8036, loss 0.519263.
Train: 2018-08-05T09:07:14.357802: step 8037, loss 0.553713.
Train: 2018-08-05T09:07:17.889425: step 8038, loss 0.622788.
Train: 2018-08-05T09:07:21.452301: step 8039, loss 0.570972.
Train: 2018-08-05T09:07:25.062057: step 8040, loss 0.545079.
Test: 2018-08-05T09:07:40.141774: step 8040, loss 0.548325.
Train: 2018-08-05T09:07:43.704650: step 8041, loss 0.570967.
Train: 2018-08-05T09:07:47.251899: step 8042, loss 0.545086.
Train: 2018-08-05T09:07:50.861655: step 8043, loss 0.562338.
Train: 2018-08-05T09:07:54.393278: step 8044, loss 0.510581.
Train: 2018-08-05T09:07:57.971781: step 8045, loss 0.588244.
Train: 2018-08-05T09:08:01.550283: step 8046, loss 0.562339.
Train: 2018-08-05T09:08:05.066279: step 8047, loss 0.614178.
Train: 2018-08-05T09:08:08.597902: step 8048, loss 0.57097.
Train: 2018-08-05T09:08:12.160779: step 8049, loss 0.579581.
Train: 2018-08-05T09:08:15.692401: step 8050, loss 0.562336.
Test: 2018-08-05T09:08:30.944012: step 8050, loss 0.548964.
Train: 2018-08-05T09:08:34.460007: step 8051, loss 0.596731.
Train: 2018-08-05T09:08:38.007257: step 8052, loss 0.562335.
Train: 2018-08-05T09:08:41.538880: step 8053, loss 0.528068.
Train: 2018-08-05T09:08:45.054876: step 8054, loss 0.553777.
Train: 2018-08-05T09:08:48.633379: step 8055, loss 0.61366.
Train: 2018-08-05T09:08:52.165001: step 8056, loss 0.570877.
Train: 2018-08-05T09:08:55.696624: step 8057, loss 0.536765.
Train: 2018-08-05T09:08:59.228247: step 8058, loss 0.604925.
Train: 2018-08-05T09:09:02.775496: step 8059, loss 0.621855.
Train: 2018-08-05T09:09:06.353999: step 8060, loss 0.587783.
Test: 2018-08-05T09:09:21.511848: step 8060, loss 0.548639.
Train: 2018-08-05T09:09:25.059099: step 8061, loss 0.503211.
Train: 2018-08-05T09:09:28.621975: step 8062, loss 0.545492.
Train: 2018-08-05T09:09:32.169224: step 8063, loss 0.579231.
Train: 2018-08-05T09:09:35.685222: step 8064, loss 0.48659.
Train: 2018-08-05T09:09:39.201216: step 8065, loss 0.59608.
Train: 2018-08-05T09:09:42.748466: step 8066, loss 0.59608.
Train: 2018-08-05T09:09:46.311342: step 8067, loss 0.52027.
Train: 2018-08-05T09:09:49.921098: step 8068, loss 0.545525.
Train: 2018-08-05T09:09:53.452721: step 8069, loss 0.570801.
Train: 2018-08-05T09:09:57.046850: step 8070, loss 0.537061.
Test: 2018-08-05T09:10:12.173447: step 8070, loss 0.548065.
Train: 2018-08-05T09:10:15.720696: step 8071, loss 0.537028.
Train: 2018-08-05T09:10:19.392959: step 8072, loss 0.587737.
Train: 2018-08-05T09:10:22.924582: step 8073, loss 0.536953.
Train: 2018-08-05T09:10:26.456205: step 8074, loss 0.562351.
Train: 2018-08-05T09:10:30.003454: step 8075, loss 0.63028.
Train: 2018-08-05T09:10:33.519450: step 8076, loss 0.562348.
Train: 2018-08-05T09:10:37.019820: step 8077, loss 0.511423.
Train: 2018-08-05T09:10:40.551443: step 8078, loss 0.570843.
Train: 2018-08-05T09:10:44.083065: step 8079, loss 0.587851.
Train: 2018-08-05T09:10:47.692821: step 8080, loss 0.485819.
Test: 2018-08-05T09:11:02.850674: step 8080, loss 0.547723.
Train: 2018-08-05T09:11:06.351041: step 8081, loss 0.596413.
Train: 2018-08-05T09:11:09.867037: step 8082, loss 0.630544.
Train: 2018-08-05T09:11:13.398660: step 8083, loss 0.545302.
Train: 2018-08-05T09:11:16.914656: step 8084, loss 0.545307.
Train: 2018-08-05T09:11:20.446278: step 8085, loss 0.485671.
Train: 2018-08-05T09:11:24.009155: step 8086, loss 0.587942.
Train: 2018-08-05T09:11:27.540777: step 8087, loss 0.553793.
Train: 2018-08-05T09:11:31.056774: step 8088, loss 0.528121.
Train: 2018-08-05T09:11:34.572770: step 8089, loss 0.545197.
Train: 2018-08-05T09:11:38.104393: step 8090, loss 0.579506.
Test: 2018-08-05T09:11:53.246616: step 8090, loss 0.547185.
Train: 2018-08-05T09:11:56.778239: step 8091, loss 0.570933.
Train: 2018-08-05T09:12:00.309862: step 8092, loss 0.502094.
Train: 2018-08-05T09:12:03.872738: step 8093, loss 0.510591.
Train: 2018-08-05T09:12:07.404360: step 8094, loss 0.51044.
Train: 2018-08-05T09:12:10.920356: step 8095, loss 0.544985.
Train: 2018-08-05T09:12:14.451980: step 8096, loss 0.544933.
Train: 2018-08-05T09:12:17.999228: step 8097, loss 0.597353.
Train: 2018-08-05T09:12:21.562105: step 8098, loss 0.606209.
Train: 2018-08-05T09:12:25.109355: step 8099, loss 0.562391.
Train: 2018-08-05T09:12:28.640977: step 8100, loss 0.553614.
Test: 2018-08-05T09:12:43.751947: step 8100, loss 0.547723.
Train: 2018-08-05T09:12:49.783834: step 8101, loss 0.483311.
Train: 2018-08-05T09:12:53.346710: step 8102, loss 0.535991.
Train: 2018-08-05T09:12:56.909586: step 8103, loss 0.491807.
Train: 2018-08-05T09:13:00.534969: step 8104, loss 0.606736.
Train: 2018-08-05T09:13:04.144725: step 8105, loss 0.597964.
Train: 2018-08-05T09:13:07.770108: step 8106, loss 0.526942.
Train: 2018-08-05T09:13:11.348610: step 8107, loss 0.553589.
Train: 2018-08-05T09:13:14.895860: step 8108, loss 0.607014.
Train: 2018-08-05T09:13:18.443109: step 8109, loss 0.526877.
Train: 2018-08-05T09:13:22.037239: step 8110, loss 0.58922.
Test: 2018-08-05T09:13:37.210716: step 8110, loss 0.546596.
Train: 2018-08-05T09:13:40.882978: step 8111, loss 0.580303.
Train: 2018-08-05T09:13:44.430227: step 8112, loss 0.615871.
Train: 2018-08-05T09:13:48.071242: step 8113, loss 0.526951.
Train: 2018-08-05T09:13:51.649740: step 8114, loss 0.491515.
Train: 2018-08-05T09:13:55.212615: step 8115, loss 0.597934.
Train: 2018-08-05T09:13:58.806744: step 8116, loss 0.589037.
Train: 2018-08-05T09:14:02.353994: step 8117, loss 0.509353.
Train: 2018-08-05T09:14:05.885617: step 8118, loss 0.571284.
Train: 2018-08-05T09:14:09.448493: step 8119, loss 0.53592.
Train: 2018-08-05T09:14:13.042623: step 8120, loss 0.659628.
Test: 2018-08-05T09:14:28.294233: step 8120, loss 0.548507.
Train: 2018-08-05T09:14:31.857109: step 8121, loss 0.544788.
Train: 2018-08-05T09:14:35.482492: step 8122, loss 0.606394.
Train: 2018-08-05T09:14:39.076621: step 8123, loss 0.536071.
Train: 2018-08-05T09:14:42.592617: step 8124, loss 0.597398.
Train: 2018-08-05T09:14:46.139866: step 8125, loss 0.579829.
Train: 2018-08-05T09:14:49.671489: step 8126, loss 0.579766.
Train: 2018-08-05T09:14:53.203112: step 8127, loss 0.571026.
Train: 2018-08-05T09:14:56.765988: step 8128, loss 0.51909.
Train: 2018-08-05T09:15:00.375744: step 8129, loss 0.536438.
Train: 2018-08-05T09:15:03.907367: step 8130, loss 0.501978.
Test: 2018-08-05T09:15:19.065217: step 8130, loss 0.548134.
Train: 2018-08-05T09:15:22.628093: step 8131, loss 0.588209.
Train: 2018-08-05T09:15:26.190969: step 8132, loss 0.493381.
Train: 2018-08-05T09:15:29.722592: step 8133, loss 0.562338.
Train: 2018-08-05T09:15:33.254215: step 8134, loss 0.545069.
Train: 2018-08-05T09:15:36.848344: step 8135, loss 0.562341.
Train: 2018-08-05T09:15:40.442473: step 8136, loss 0.579646.
Train: 2018-08-05T09:15:43.958469: step 8137, loss 0.562343.
Train: 2018-08-05T09:15:47.474466: step 8138, loss 0.545029.
Train: 2018-08-05T09:15:51.021715: step 8139, loss 0.640304.
Train: 2018-08-05T09:15:54.522085: step 8140, loss 0.545039.
Test: 2018-08-05T09:16:09.726815: step 8140, loss 0.548501.
Train: 2018-08-05T09:16:13.242811: step 8141, loss 0.622852.
Train: 2018-08-05T09:16:16.774433: step 8142, loss 0.622723.
Train: 2018-08-05T09:16:20.274803: step 8143, loss 0.588131.
Train: 2018-08-05T09:16:23.790799: step 8144, loss 0.605178.
Train: 2018-08-05T09:16:27.338049: step 8145, loss 0.562339.
Train: 2018-08-05T09:16:30.885298: step 8146, loss 0.511334.
Train: 2018-08-05T09:16:34.448174: step 8147, loss 0.579316.
Train: 2018-08-05T09:16:37.995423: step 8148, loss 0.545431.
Train: 2018-08-05T09:16:41.527047: step 8149, loss 0.520118.
Train: 2018-08-05T09:16:45.089922: step 8150, loss 0.570809.
Test: 2018-08-05T09:17:00.247773: step 8150, loss 0.548078.
Train: 2018-08-05T09:17:03.810649: step 8151, loss 0.596124.
Train: 2018-08-05T09:17:07.326645: step 8152, loss 0.553943.
Train: 2018-08-05T09:17:10.920775: step 8153, loss 0.537111.
Train: 2018-08-05T09:17:12.655332: step 8154, loss 0.580339.
Train: 2018-08-05T09:17:16.218209: step 8155, loss 0.51188.
Train: 2018-08-05T09:17:19.796711: step 8156, loss 0.587643.
Train: 2018-08-05T09:17:23.312707: step 8157, loss 0.604497.
Train: 2018-08-05T09:17:26.891210: step 8158, loss 0.604471.
Train: 2018-08-05T09:17:30.438459: step 8159, loss 0.646445.
Train: 2018-08-05T09:17:34.032588: step 8160, loss 0.595917.
Test: 2018-08-05T09:17:49.143566: step 8160, loss 0.548666.
Train: 2018-08-05T09:17:52.768942: step 8161, loss 0.529019.
Train: 2018-08-05T09:17:56.347444: step 8162, loss 0.579094.
Train: 2018-08-05T09:17:59.894693: step 8163, loss 0.562449.
Train: 2018-08-05T09:18:03.426316: step 8164, loss 0.545874.
Train: 2018-08-05T09:18:07.036072: step 8165, loss 0.579041.
Train: 2018-08-05T09:18:10.567695: step 8166, loss 0.479756.
Train: 2018-08-05T09:18:14.114945: step 8167, loss 0.570757.
Train: 2018-08-05T09:18:17.646567: step 8168, loss 0.562471.
Train: 2018-08-05T09:18:21.193817: step 8169, loss 0.57905.
Train: 2018-08-05T09:18:24.741067: step 8170, loss 0.603945.
Test: 2018-08-05T09:18:39.883290: step 8170, loss 0.548231.
Train: 2018-08-05T09:18:43.414913: step 8171, loss 0.562464.
Train: 2018-08-05T09:18:47.009042: step 8172, loss 0.554174.
Train: 2018-08-05T09:18:50.525038: step 8173, loss 0.529291.
Train: 2018-08-05T09:18:54.087914: step 8174, loss 0.620572.
Train: 2018-08-05T09:18:57.603910: step 8175, loss 0.537558.
Train: 2018-08-05T09:19:01.151160: step 8176, loss 0.562455.
Train: 2018-08-05T09:19:04.682783: step 8177, loss 0.545832.
Train: 2018-08-05T09:19:08.230032: step 8178, loss 0.554125.
Train: 2018-08-05T09:19:11.746028: step 8179, loss 0.570763.
Train: 2018-08-05T09:19:15.262025: step 8180, loss 0.537416.
Test: 2018-08-05T09:19:30.498008: step 8180, loss 0.548086.
Train: 2018-08-05T09:19:34.014004: step 8181, loss 0.612526.
Train: 2018-08-05T09:19:37.545627: step 8182, loss 0.57077.
Train: 2018-08-05T09:19:41.092876: step 8183, loss 0.528982.
Train: 2018-08-05T09:19:44.624499: step 8184, loss 0.570774.
Train: 2018-08-05T09:19:48.187375: step 8185, loss 0.570777.
Train: 2018-08-05T09:19:51.750251: step 8186, loss 0.554015.
Train: 2018-08-05T09:19:55.250620: step 8187, loss 0.537221.
Train: 2018-08-05T09:19:58.813497: step 8188, loss 0.595997.
Train: 2018-08-05T09:20:02.407626: step 8189, loss 0.587609.
Train: 2018-08-05T09:20:05.986128: step 8190, loss 0.4951.
Test: 2018-08-05T09:20:21.097099: step 8190, loss 0.54714.
Train: 2018-08-05T09:20:24.644349: step 8191, loss 0.587647.
Train: 2018-08-05T09:20:28.191597: step 8192, loss 0.553936.
Train: 2018-08-05T09:20:31.691967: step 8193, loss 0.562365.
Train: 2018-08-05T09:20:35.223590: step 8194, loss 0.553908.
Train: 2018-08-05T09:20:38.755213: step 8195, loss 0.596211.
Train: 2018-08-05T09:20:42.380595: step 8196, loss 0.536955.
Train: 2018-08-05T09:20:45.896592: step 8197, loss 0.536927.
Train: 2018-08-05T09:20:49.459468: step 8198, loss 0.579324.
Train: 2018-08-05T09:20:52.975465: step 8199, loss 0.596331.
Train: 2018-08-05T09:20:56.507087: step 8200, loss 0.587839.
Test: 2018-08-05T09:21:11.633685: step 8200, loss 0.548552.
Train: 2018-08-05T09:21:17.399917: step 8201, loss 0.511383.
Train: 2018-08-05T09:21:20.915914: step 8202, loss 0.570846.
Train: 2018-08-05T09:21:24.478790: step 8203, loss 0.545333.
Train: 2018-08-05T09:21:27.994785: step 8204, loss 0.494238.
Train: 2018-08-05T09:21:31.526409: step 8205, loss 0.553806.
Train: 2018-08-05T09:21:35.058031: step 8206, loss 0.570889.
Train: 2018-08-05T09:21:38.558401: step 8207, loss 0.528063.
Train: 2018-08-05T09:21:42.105650: step 8208, loss 0.536569.
Train: 2018-08-05T09:21:45.715406: step 8209, loss 0.48483.
Train: 2018-08-05T09:21:49.278282: step 8210, loss 0.570988.
Test: 2018-08-05T09:22:04.529892: step 8210, loss 0.547859.
Train: 2018-08-05T09:22:08.061515: step 8211, loss 0.562348.
Train: 2018-08-05T09:22:11.561884: step 8212, loss 0.501446.
Train: 2018-08-05T09:22:15.077880: step 8213, loss 0.571105.
Train: 2018-08-05T09:22:18.609503: step 8214, loss 0.553622.
Train: 2018-08-05T09:22:22.234887: step 8215, loss 0.562399.
Train: 2018-08-05T09:22:25.766508: step 8216, loss 0.606461.
Train: 2018-08-05T09:22:29.266878: step 8217, loss 0.544781.
Train: 2018-08-05T09:22:32.798501: step 8218, loss 0.553598.
Train: 2018-08-05T09:22:36.314497: step 8219, loss 0.571276.
Train: 2018-08-05T09:22:39.908627: step 8220, loss 0.500518.
Test: 2018-08-05T09:22:55.097733: step 8220, loss 0.548266.
Train: 2018-08-05T09:22:58.644980: step 8221, loss 0.562453.
Train: 2018-08-05T09:23:02.223482: step 8222, loss 0.544717.
Train: 2018-08-05T09:23:05.770731: step 8223, loss 0.633557.
Train: 2018-08-05T09:23:09.333608: step 8224, loss 0.544708.
Train: 2018-08-05T09:23:12.849603: step 8225, loss 0.54471.
Train: 2018-08-05T09:23:16.396853: step 8226, loss 0.57135.
Train: 2018-08-05T09:23:20.006609: step 8227, loss 0.562467.
Train: 2018-08-05T09:23:23.522606: step 8228, loss 0.580207.
Train: 2018-08-05T09:23:27.038602: step 8229, loss 0.597907.
Train: 2018-08-05T09:23:30.585851: step 8230, loss 0.588982.
Test: 2018-08-05T09:23:45.774955: step 8230, loss 0.547482.
Train: 2018-08-05T09:23:49.290951: step 8231, loss 0.491819.
Train: 2018-08-05T09:23:52.869453: step 8232, loss 0.588875.
Train: 2018-08-05T09:23:56.401076: step 8233, loss 0.474356.
Train: 2018-08-05T09:23:59.948325: step 8234, loss 0.58003.
Train: 2018-08-05T09:24:03.479948: step 8235, loss 0.624061.
Train: 2018-08-05T09:24:06.995945: step 8236, loss 0.615159.
Train: 2018-08-05T09:24:10.558820: step 8237, loss 0.53608.
Train: 2018-08-05T09:24:14.152949: step 8238, loss 0.562378.
Train: 2018-08-05T09:24:17.715826: step 8239, loss 0.527432.
Train: 2018-08-05T09:24:21.294328: step 8240, loss 0.562366.
Test: 2018-08-05T09:24:36.420928: step 8240, loss 0.547606.
Train: 2018-08-05T09:24:39.936922: step 8241, loss 0.605938.
Train: 2018-08-05T09:24:43.484171: step 8242, loss 0.47538.
Train: 2018-08-05T09:24:47.015794: step 8243, loss 0.562355.
Train: 2018-08-05T09:24:50.609923: step 8244, loss 0.544963.
Train: 2018-08-05T09:24:54.172799: step 8245, loss 0.571052.
Train: 2018-08-05T09:24:57.720048: step 8246, loss 0.501485.
Train: 2018-08-05T09:25:01.267298: step 8247, loss 0.649407.
Train: 2018-08-05T09:25:04.814548: step 8248, loss 0.536266.
Train: 2018-08-05T09:25:08.346170: step 8249, loss 0.553661.
Train: 2018-08-05T09:25:11.924673: step 8250, loss 0.527591.
Test: 2018-08-05T09:25:27.098150: step 8250, loss 0.548235.
Train: 2018-08-05T09:25:30.770413: step 8251, loss 0.527578.
Train: 2018-08-05T09:25:34.317662: step 8252, loss 0.527546.
Train: 2018-08-05T09:25:37.880538: step 8253, loss 0.553646.
Train: 2018-08-05T09:25:41.412161: step 8254, loss 0.544909.
Train: 2018-08-05T09:25:44.943784: step 8255, loss 0.544888.
Train: 2018-08-05T09:25:48.475406: step 8256, loss 0.562382.
Train: 2018-08-05T09:25:52.022656: step 8257, loss 0.597469.
Train: 2018-08-05T09:25:55.538652: step 8258, loss 0.571164.
Train: 2018-08-05T09:25:59.070275: step 8259, loss 0.492201.
Train: 2018-08-05T09:26:02.617524: step 8260, loss 0.527255.
Test: 2018-08-05T09:26:17.728494: step 8260, loss 0.548722.
Train: 2018-08-05T09:26:21.228864: step 8261, loss 0.606416.
Train: 2018-08-05T09:26:24.791740: step 8262, loss 0.553605.
Train: 2018-08-05T09:26:28.323366: step 8263, loss 0.509546.
Train: 2018-08-05T09:26:31.870612: step 8264, loss 0.641841.
Train: 2018-08-05T09:26:35.386608: step 8265, loss 0.659435.
Train: 2018-08-05T09:26:38.918231: step 8266, loss 0.615182.
Train: 2018-08-05T09:26:42.481107: step 8267, loss 0.544857.
Train: 2018-08-05T09:26:46.059610: step 8268, loss 0.57111.
Train: 2018-08-05T09:26:49.606860: step 8269, loss 0.614628.
Train: 2018-08-05T09:26:53.154109: step 8270, loss 0.544995.
Test: 2018-08-05T09:27:08.311958: step 8270, loss 0.5475.
Train: 2018-08-05T09:27:11.906088: step 8271, loss 0.536396.
Train: 2018-08-05T09:27:15.453342: step 8272, loss 0.536456.
Train: 2018-08-05T09:27:18.969334: step 8273, loss 0.570949.
Train: 2018-08-05T09:27:22.500957: step 8274, loss 0.570932.
Train: 2018-08-05T09:27:26.079459: step 8275, loss 0.502265.
Train: 2018-08-05T09:27:29.673589: step 8276, loss 0.562335.
Train: 2018-08-05T09:27:33.189585: step 8277, loss 0.579486.
Train: 2018-08-05T09:27:36.705581: step 8278, loss 0.553766.
Train: 2018-08-05T09:27:40.252830: step 8279, loss 0.545205.
Train: 2018-08-05T09:27:43.815706: step 8280, loss 0.493817.
Test: 2018-08-05T09:27:58.957930: step 8280, loss 0.548208.
Train: 2018-08-05T09:28:02.552059: step 8281, loss 0.528027.
Train: 2018-08-05T09:28:06.068056: step 8282, loss 0.536552.
Train: 2018-08-05T09:28:09.646558: step 8283, loss 0.631253.
Train: 2018-08-05T09:28:13.178181: step 8284, loss 0.562337.
Train: 2018-08-05T09:28:16.694177: step 8285, loss 0.622701.
Train: 2018-08-05T09:28:20.257053: step 8286, loss 0.553721.
Train: 2018-08-05T09:28:23.819930: step 8287, loss 0.605381.
Train: 2018-08-05T09:28:27.382805: step 8288, loss 0.57093.
Train: 2018-08-05T09:28:30.945681: step 8289, loss 0.605238.
Train: 2018-08-05T09:28:34.446051: step 8290, loss 0.579455.
Test: 2018-08-05T09:28:49.619528: step 8290, loss 0.547491.
Train: 2018-08-05T09:28:53.135524: step 8291, loss 0.613559.
Train: 2018-08-05T09:28:56.698400: step 8292, loss 0.613385.
Train: 2018-08-05T09:29:00.245649: step 8293, loss 0.562354.
Train: 2018-08-05T09:29:03.855405: step 8294, loss 0.562367.
Train: 2018-08-05T09:29:07.340148: step 8295, loss 0.537154.
Train: 2018-08-05T09:29:10.856144: step 8296, loss 0.545616.
Train: 2018-08-05T09:29:14.356514: step 8297, loss 0.612648.
Train: 2018-08-05T09:29:17.888137: step 8298, loss 0.545712.
Train: 2018-08-05T09:29:21.451013: step 8299, loss 0.587439.
Train: 2018-08-05T09:29:24.967009: step 8300, loss 0.545804.
Test: 2018-08-05T09:29:40.046725: step 8300, loss 0.549534.
Train: 2018-08-05T09:29:45.734826: step 8301, loss 0.545837.
Train: 2018-08-05T09:29:49.250822: step 8302, loss 0.554156.
Train: 2018-08-05T09:29:52.751192: step 8303, loss 0.545863.
Train: 2018-08-05T09:29:56.267188: step 8304, loss 0.537556.
Train: 2018-08-05T09:29:58.001746: step 8305, loss 0.527001.
Train: 2018-08-05T09:30:01.548995: step 8306, loss 0.495849.
Train: 2018-08-05T09:30:05.049365: step 8307, loss 0.545715.
Train: 2018-08-05T09:30:08.596614: step 8308, loss 0.545637.
Train: 2018-08-05T09:30:12.112610: step 8309, loss 0.570791.
Train: 2018-08-05T09:30:15.581727: step 8310, loss 0.545494.
Test: 2018-08-05T09:30:30.630193: step 8310, loss 0.548026.
Train: 2018-08-05T09:30:34.224319: step 8311, loss 0.596214.
Train: 2018-08-05T09:30:37.755943: step 8312, loss 0.621731.
Train: 2018-08-05T09:30:41.256312: step 8313, loss 0.596303.
Train: 2018-08-05T09:30:44.756681: step 8314, loss 0.604785.
Train: 2018-08-05T09:30:48.272677: step 8315, loss 0.553874.
Train: 2018-08-05T09:30:51.757420: step 8316, loss 0.596243.
Train: 2018-08-05T09:30:55.289043: step 8317, loss 0.545437.
Train: 2018-08-05T09:30:58.820666: step 8318, loss 0.562361.
Train: 2018-08-05T09:31:02.367915: step 8319, loss 0.638393.
Train: 2018-08-05T09:31:05.868285: step 8320, loss 0.553943.
Test: 2018-08-05T09:31:20.869869: step 8320, loss 0.548716.
Train: 2018-08-05T09:31:24.385864: step 8321, loss 0.570792.
Train: 2018-08-05T09:31:27.901861: step 8322, loss 0.545591.
Train: 2018-08-05T09:31:31.417856: step 8323, loss 0.453334.
Train: 2018-08-05T09:31:34.933853: step 8324, loss 0.486762.
Train: 2018-08-05T09:31:38.465476: step 8325, loss 0.579231.
Train: 2018-08-05T09:31:42.012725: step 8326, loss 0.537006.
Train: 2018-08-05T09:31:45.528721: step 8327, loss 0.477584.
Train: 2018-08-05T09:31:49.029091: step 8328, loss 0.562342.
Train: 2018-08-05T09:31:52.529460: step 8329, loss 0.630744.
Train: 2018-08-05T09:31:56.061083: step 8330, loss 0.570905.
Test: 2018-08-05T09:32:11.078294: step 8330, loss 0.546615.
Train: 2018-08-05T09:32:14.594289: step 8331, loss 0.55375.
Train: 2018-08-05T09:32:18.110286: step 8332, loss 0.553736.
Train: 2018-08-05T09:32:21.610655: step 8333, loss 0.545108.
Train: 2018-08-05T09:32:25.157904: step 8334, loss 0.579599.
Train: 2018-08-05T09:32:28.658274: step 8335, loss 0.545058.
Train: 2018-08-05T09:32:32.205523: step 8336, loss 0.501768.
Train: 2018-08-05T09:32:35.752773: step 8337, loss 0.449568.
Train: 2018-08-05T09:32:39.284396: step 8338, loss 0.571077.
Train: 2018-08-05T09:32:42.784765: step 8339, loss 0.53613.
Train: 2018-08-05T09:32:46.285135: step 8340, loss 0.553613.
Test: 2018-08-05T09:33:01.317972: step 8340, loss 0.548914.
Train: 2018-08-05T09:33:04.896474: step 8341, loss 0.553603.
Train: 2018-08-05T09:33:08.459351: step 8342, loss 0.580121.
Train: 2018-08-05T09:33:11.990973: step 8343, loss 0.562454.
Train: 2018-08-05T09:33:15.569476: step 8344, loss 0.464795.
Train: 2018-08-05T09:33:19.116725: step 8345, loss 0.509035.
Train: 2018-08-05T09:33:22.663975: step 8346, loss 0.562537.
Train: 2018-08-05T09:33:26.179971: step 8347, loss 0.625411.
Train: 2018-08-05T09:33:29.727220: step 8348, loss 0.598552.
Train: 2018-08-05T09:33:33.321350: step 8349, loss 0.508625.
Train: 2018-08-05T09:33:36.852972: step 8350, loss 0.562607.
Test: 2018-08-05T09:33:51.885809: step 8350, loss 0.546319.
Train: 2018-08-05T09:33:55.401805: step 8351, loss 0.616695.
Train: 2018-08-05T09:33:58.933428: step 8352, loss 0.580625.
Train: 2018-08-05T09:34:02.496304: step 8353, loss 0.526607.
Train: 2018-08-05T09:34:06.059181: step 8354, loss 0.56259.
Train: 2018-08-05T09:34:09.590803: step 8355, loss 0.52664.
Train: 2018-08-05T09:34:13.106800: step 8356, loss 0.607497.
Train: 2018-08-05T09:34:16.622796: step 8357, loss 0.598451.
Train: 2018-08-05T09:34:20.201298: step 8358, loss 0.67892.
Train: 2018-08-05T09:34:23.686041: step 8359, loss 0.544676.
Train: 2018-08-05T09:34:27.233290: step 8360, loss 0.500329.
Test: 2018-08-05T09:34:42.203621: step 8360, loss 0.546641.
Train: 2018-08-05T09:34:45.719617: step 8361, loss 0.571303.
Train: 2018-08-05T09:34:49.219987: step 8362, loss 0.465278.
Train: 2018-08-05T09:34:52.735982: step 8363, loss 0.562426.
Train: 2018-08-05T09:34:56.236352: step 8364, loss 0.606535.
Train: 2018-08-05T09:34:59.752348: step 8365, loss 0.553604.
Train: 2018-08-05T09:35:03.283971: step 8366, loss 0.5712.
Train: 2018-08-05T09:35:06.846847: step 8367, loss 0.518488.
Train: 2018-08-05T09:35:10.378470: step 8368, loss 0.62382.
Train: 2018-08-05T09:35:13.910093: step 8369, loss 0.509835.
Train: 2018-08-05T09:35:17.426089: step 8370, loss 0.588629.
Test: 2018-08-05T09:35:32.443301: step 8370, loss 0.548986.
Train: 2018-08-05T09:35:35.990548: step 8371, loss 0.536158.
Train: 2018-08-05T09:35:39.553425: step 8372, loss 0.571099.
Train: 2018-08-05T09:35:43.085047: step 8373, loss 0.501317.
Train: 2018-08-05T09:35:46.601043: step 8374, loss 0.562365.
Train: 2018-08-05T09:35:50.070160: step 8375, loss 0.562365.
Train: 2018-08-05T09:35:53.617409: step 8376, loss 0.588537.
Train: 2018-08-05T09:35:57.117779: step 8377, loss 0.597234.
Train: 2018-08-05T09:36:00.618149: step 8378, loss 0.562358.
Train: 2018-08-05T09:36:04.181024: step 8379, loss 0.544968.
Train: 2018-08-05T09:36:07.681394: step 8380, loss 0.605773.
Test: 2018-08-05T09:36:22.682979: step 8380, loss 0.547271.
Train: 2018-08-05T09:36:26.198973: step 8381, loss 0.527674.
Train: 2018-08-05T09:36:29.714969: step 8382, loss 0.571003.
Train: 2018-08-05T09:36:33.246592: step 8383, loss 0.579639.
Train: 2018-08-05T09:36:36.762588: step 8384, loss 0.553704.
Train: 2018-08-05T09:36:40.262958: step 8385, loss 0.596837.
Train: 2018-08-05T09:36:43.794581: step 8386, loss 0.553728.
Train: 2018-08-05T09:36:47.326204: step 8387, loss 0.476388.
Train: 2018-08-05T09:36:50.826573: step 8388, loss 0.613925.
Train: 2018-08-05T09:36:54.342569: step 8389, loss 0.58811.
Train: 2018-08-05T09:36:57.842938: step 8390, loss 0.613818.
Test: 2018-08-05T09:37:12.891402: step 8390, loss 0.548433.
Train: 2018-08-05T09:37:16.407398: step 8391, loss 0.579457.
Train: 2018-08-05T09:37:19.939021: step 8392, loss 0.528181.
Train: 2018-08-05T09:37:23.439391: step 8393, loss 0.53676.
Train: 2018-08-05T09:37:26.939760: step 8394, loss 0.494181.
Train: 2018-08-05T09:37:30.487009: step 8395, loss 0.570867.
Train: 2018-08-05T09:37:33.987379: step 8396, loss 0.562339.
Train: 2018-08-05T09:37:37.519002: step 8397, loss 0.613556.
Train: 2018-08-05T09:37:41.081878: step 8398, loss 0.562339.
Train: 2018-08-05T09:37:44.550994: step 8399, loss 0.587916.
Train: 2018-08-05T09:37:48.051364: step 8400, loss 0.553826.
Test: 2018-08-05T09:38:02.990443: step 8400, loss 0.547544.
Train: 2018-08-05T09:38:08.756674: step 8401, loss 0.502778.
Train: 2018-08-05T09:38:12.257044: step 8402, loss 0.596401.
Train: 2018-08-05T09:38:15.819919: step 8403, loss 0.570855.
Train: 2018-08-05T09:38:19.335916: step 8404, loss 0.511281.
Train: 2018-08-05T09:38:22.851912: step 8405, loss 0.536789.
Train: 2018-08-05T09:38:26.336655: step 8406, loss 0.545281.
Train: 2018-08-05T09:38:29.837024: step 8407, loss 0.545252.
Train: 2018-08-05T09:38:33.368648: step 8408, loss 0.605127.
Train: 2018-08-05T09:38:36.900270: step 8409, loss 0.476697.
Train: 2018-08-05T09:38:40.416266: step 8410, loss 0.58809.
Test: 2018-08-05T09:38:55.339718: step 8410, loss 0.548568.
Train: 2018-08-05T09:38:58.840086: step 8411, loss 0.605331.
Train: 2018-08-05T09:39:02.309202: step 8412, loss 0.4677.
Train: 2018-08-05T09:39:05.872078: step 8413, loss 0.562338.
Train: 2018-08-05T09:39:09.372448: step 8414, loss 0.579627.
Train: 2018-08-05T09:39:12.888444: step 8415, loss 0.553687.
Train: 2018-08-05T09:39:16.435693: step 8416, loss 0.519002.
Train: 2018-08-05T09:39:19.936063: step 8417, loss 0.571039.
Train: 2018-08-05T09:39:23.420806: step 8418, loss 0.553654.
Train: 2018-08-05T09:39:26.936802: step 8419, loss 0.544929.
Train: 2018-08-05T09:39:30.468424: step 8420, loss 0.562369.
Test: 2018-08-05T09:39:45.470008: step 8420, loss 0.547973.
Train: 2018-08-05T09:39:48.970377: step 8421, loss 0.509912.
Train: 2018-08-05T09:39:52.517627: step 8422, loss 0.658785.
Train: 2018-08-05T09:39:56.002370: step 8423, loss 0.623723.
Train: 2018-08-05T09:39:59.487113: step 8424, loss 0.52738.
Train: 2018-08-05T09:40:02.987482: step 8425, loss 0.571117.
Train: 2018-08-05T09:40:06.472226: step 8426, loss 0.588573.
Train: 2018-08-05T09:40:10.019474: step 8427, loss 0.597247.
Train: 2018-08-05T09:40:13.504217: step 8428, loss 0.510149.
Train: 2018-08-05T09:40:17.035840: step 8429, loss 0.666663.
Train: 2018-08-05T09:40:20.520583: step 8430, loss 0.657656.
Test: 2018-08-05T09:40:35.537794: step 8430, loss 0.547344.
Train: 2018-08-05T09:40:39.100669: step 8431, loss 0.562337.
Train: 2018-08-05T09:40:42.585412: step 8432, loss 0.58808.
Train: 2018-08-05T09:40:46.132661: step 8433, loss 0.587961.
Train: 2018-08-05T09:40:49.617404: step 8434, loss 0.587845.
Train: 2018-08-05T09:40:53.149027: step 8435, loss 0.562359.
Train: 2018-08-05T09:40:56.633770: step 8436, loss 0.579217.
Train: 2018-08-05T09:41:00.118513: step 8437, loss 0.545627.
Train: 2018-08-05T09:41:03.572002: step 8438, loss 0.512279.
Train: 2018-08-05T09:41:07.134878: step 8439, loss 0.587448.
Train: 2018-08-05T09:41:10.682128: step 8440, loss 0.554116.
Test: 2018-08-05T09:41:25.777473: step 8440, loss 0.549145.
Train: 2018-08-05T09:41:29.293467: step 8441, loss 0.55414.
Train: 2018-08-05T09:41:32.778210: step 8442, loss 0.562458.
Train: 2018-08-05T09:41:36.278580: step 8443, loss 0.603935.
Train: 2018-08-05T09:41:39.763323: step 8444, loss 0.529351.
Train: 2018-08-05T09:41:43.263692: step 8445, loss 0.537648.
Train: 2018-08-05T09:41:46.779688: step 8446, loss 0.545918.
Train: 2018-08-05T09:41:50.311311: step 8447, loss 0.512753.
Train: 2018-08-05T09:41:53.827307: step 8448, loss 0.512635.
Train: 2018-08-05T09:41:57.312050: step 8449, loss 0.670719.
Train: 2018-08-05T09:42:00.796793: step 8450, loss 0.545765.
Test: 2018-08-05T09:42:15.767124: step 8450, loss 0.549264.
Train: 2018-08-05T09:42:19.267493: step 8451, loss 0.579107.
Train: 2018-08-05T09:42:22.783489: step 8452, loss 0.620839.
Train: 2018-08-05T09:42:26.299485: step 8453, loss 0.595783.
Train: 2018-08-05T09:42:29.799854: step 8454, loss 0.579091.
Train: 2018-08-05T09:42:33.268971: step 8455, loss 0.554129.
Train: 2018-08-05T09:42:34.972275: step 8456, loss 0.580175.
Train: 2018-08-05T09:42:38.441392: step 8457, loss 0.520969.
Train: 2018-08-05T09:42:41.941761: step 8458, loss 0.579057.
Train: 2018-08-05T09:42:45.442131: step 8459, loss 0.545866.
Train: 2018-08-05T09:42:48.911247: step 8460, loss 0.570759.
Test: 2018-08-05T09:43:03.834696: step 8460, loss 0.54859.
Train: 2018-08-05T09:43:07.413200: step 8461, loss 0.537548.
Train: 2018-08-05T09:43:10.929196: step 8462, loss 0.529206.
Train: 2018-08-05T09:43:14.398312: step 8463, loss 0.570763.
Train: 2018-08-05T09:43:17.914308: step 8464, loss 0.554088.
Train: 2018-08-05T09:43:21.414678: step 8465, loss 0.637592.
Train: 2018-08-05T09:43:24.899421: step 8466, loss 0.65429.
Train: 2018-08-05T09:43:28.399790: step 8467, loss 0.545761.
Train: 2018-08-05T09:43:31.884533: step 8468, loss 0.579085.
Train: 2018-08-05T09:43:35.416155: step 8469, loss 0.554139.
Train: 2018-08-05T09:43:38.932152: step 8470, loss 0.620571.
Test: 2018-08-05T09:43:53.871228: step 8470, loss 0.548447.
Train: 2018-08-05T09:43:57.355972: step 8471, loss 0.562473.
Train: 2018-08-05T09:44:00.856341: step 8472, loss 0.628643.
Train: 2018-08-05T09:44:04.356711: step 8473, loss 0.595491.
Train: 2018-08-05T09:44:07.872707: step 8474, loss 0.537889.
Train: 2018-08-05T09:44:11.373077: step 8475, loss 0.529765.
Train: 2018-08-05T09:44:14.920325: step 8476, loss 0.54619.
Train: 2018-08-05T09:44:18.436322: step 8477, loss 0.570764.
Train: 2018-08-05T09:44:21.936691: step 8478, loss 0.52983.
Train: 2018-08-05T09:44:25.405808: step 8479, loss 0.48883.
Train: 2018-08-05T09:44:28.937430: step 8480, loss 0.521467.
Test: 2018-08-05T09:44:43.829627: step 8480, loss 0.549312.
Train: 2018-08-05T09:44:47.361250: step 8481, loss 0.587246.
Train: 2018-08-05T09:44:50.861620: step 8482, loss 0.59556.
Train: 2018-08-05T09:44:54.393243: step 8483, loss 0.612176.
Train: 2018-08-05T09:44:57.909238: step 8484, loss 0.570758.
Train: 2018-08-05T09:45:01.409608: step 8485, loss 0.570758.
Train: 2018-08-05T09:45:04.863098: step 8486, loss 0.579057.
Train: 2018-08-05T09:45:08.363467: step 8487, loss 0.529258.
Train: 2018-08-05T09:45:11.863837: step 8488, loss 0.628924.
Train: 2018-08-05T09:45:15.395460: step 8489, loss 0.554148.
Train: 2018-08-05T09:45:18.880202: step 8490, loss 0.529234.
Test: 2018-08-05T09:45:33.756773: step 8490, loss 0.548183.
Train: 2018-08-05T09:45:37.319648: step 8491, loss 0.67882.
Train: 2018-08-05T09:45:40.835645: step 8492, loss 0.554163.
Train: 2018-08-05T09:45:44.382895: step 8493, loss 0.595618.
Train: 2018-08-05T09:45:47.867637: step 8494, loss 0.595572.
Train: 2018-08-05T09:45:51.414886: step 8495, loss 0.562503.
Train: 2018-08-05T09:45:54.899629: step 8496, loss 0.562518.
Train: 2018-08-05T09:45:58.368745: step 8497, loss 0.570758.
Train: 2018-08-05T09:46:01.869115: step 8498, loss 0.595406.
Train: 2018-08-05T09:46:05.338231: step 8499, loss 0.636369.
Train: 2018-08-05T09:46:08.838601: step 8500, loss 0.570768.
Test: 2018-08-05T09:46:23.683917: step 8500, loss 0.548446.
Train: 2018-08-05T09:46:29.450151: step 8501, loss 0.54632.
Train: 2018-08-05T09:46:33.106788: step 8502, loss 0.578917.
Train: 2018-08-05T09:46:36.709187: step 8503, loss 0.554546.
Train: 2018-08-05T09:46:40.187109: step 8504, loss 0.603233.
Train: 2018-08-05T09:46:43.688340: step 8505, loss 0.62747.
Train: 2018-08-05T09:46:47.172061: step 8506, loss 0.530457.
Train: 2018-08-05T09:46:50.688057: step 8507, loss 0.490235.
Train: 2018-08-05T09:46:54.235307: step 8508, loss 0.578881.
Train: 2018-08-05T09:46:57.751303: step 8509, loss 0.538543.
Train: 2018-08-05T09:47:01.220419: step 8510, loss 0.481944.
Test: 2018-08-05T09:47:16.206375: step 8510, loss 0.548775.
Train: 2018-08-05T09:47:19.753626: step 8511, loss 0.55458.
Train: 2018-08-05T09:47:23.222741: step 8512, loss 0.513831.
Train: 2018-08-05T09:47:26.738738: step 8513, loss 0.546247.
Train: 2018-08-05T09:47:30.317241: step 8514, loss 0.628248.
Train: 2018-08-05T09:47:33.864490: step 8515, loss 0.546048.
Train: 2018-08-05T09:47:37.364859: step 8516, loss 0.60381.
Train: 2018-08-05T09:47:40.896482: step 8517, loss 0.603877.
Train: 2018-08-05T09:47:44.381225: step 8518, loss 0.545892.
Train: 2018-08-05T09:47:47.928475: step 8519, loss 0.537557.
Train: 2018-08-05T09:47:51.444471: step 8520, loss 0.537492.
Test: 2018-08-05T09:48:06.414802: step 8520, loss 0.547733.
Train: 2018-08-05T09:48:09.915171: step 8521, loss 0.562427.
Train: 2018-08-05T09:48:13.431166: step 8522, loss 0.545695.
Train: 2018-08-05T09:48:16.978416: step 8523, loss 0.48697.
Train: 2018-08-05T09:48:20.494413: step 8524, loss 0.545546.
Train: 2018-08-05T09:48:24.010408: step 8525, loss 0.55391.
Train: 2018-08-05T09:48:27.573285: step 8526, loss 0.570835.
Train: 2018-08-05T09:48:31.058027: step 8527, loss 0.553827.
Train: 2018-08-05T09:48:34.527144: step 8528, loss 0.545251.
Train: 2018-08-05T09:48:38.011886: step 8529, loss 0.553764.
Train: 2018-08-05T09:48:41.496629: step 8530, loss 0.47635.
Test: 2018-08-05T09:48:56.404452: step 8530, loss 0.548708.
Train: 2018-08-05T09:48:59.920449: step 8531, loss 0.588256.
Train: 2018-08-05T09:49:03.452072: step 8532, loss 0.571016.
Train: 2018-08-05T09:49:06.968068: step 8533, loss 0.544964.
Train: 2018-08-05T09:49:10.484064: step 8534, loss 0.597245.
Train: 2018-08-05T09:49:13.968806: step 8535, loss 0.579842.
Train: 2018-08-05T09:49:17.484803: step 8536, loss 0.55363.
Train: 2018-08-05T09:49:21.000799: step 8537, loss 0.56238.
Train: 2018-08-05T09:49:24.501169: step 8538, loss 0.597433.
Train: 2018-08-05T09:49:28.032792: step 8539, loss 0.518578.
Train: 2018-08-05T09:49:31.517534: step 8540, loss 0.536085.
Test: 2018-08-05T09:49:46.378479: step 8540, loss 0.548139.
Train: 2018-08-05T09:49:49.863221: step 8541, loss 0.51851.
Train: 2018-08-05T09:49:53.379217: step 8542, loss 0.597568.
Train: 2018-08-05T09:49:56.879586: step 8543, loss 0.562405.
Train: 2018-08-05T09:50:00.442462: step 8544, loss 0.588813.
Train: 2018-08-05T09:50:03.895952: step 8545, loss 0.500813.
Train: 2018-08-05T09:50:07.380695: step 8546, loss 0.553605.
Train: 2018-08-05T09:50:10.912318: step 8547, loss 0.588859.
Train: 2018-08-05T09:50:14.397061: step 8548, loss 0.553602.
Train: 2018-08-05T09:50:17.928683: step 8549, loss 0.597676.
Train: 2018-08-05T09:50:21.475933: step 8550, loss 0.544797.
Test: 2018-08-05T09:50:36.399385: step 8550, loss 0.548316.
Train: 2018-08-05T09:50:39.899752: step 8551, loss 0.624026.
Train: 2018-08-05T09:50:43.368869: step 8552, loss 0.5624.
Train: 2018-08-05T09:50:46.837985: step 8553, loss 0.474716.
Train: 2018-08-05T09:50:50.322728: step 8554, loss 0.544849.
Train: 2018-08-05T09:50:53.838724: step 8555, loss 0.56239.
Train: 2018-08-05T09:50:57.339093: step 8556, loss 0.579936.
Train: 2018-08-05T09:51:00.792583: step 8557, loss 0.562388.
Train: 2018-08-05T09:51:04.308579: step 8558, loss 0.58868.
Train: 2018-08-05T09:51:07.808949: step 8559, loss 0.56238.
Train: 2018-08-05T09:51:11.309318: step 8560, loss 0.579865.
Test: 2018-08-05T09:51:26.154634: step 8560, loss 0.547586.
Train: 2018-08-05T09:51:29.733138: step 8561, loss 0.527443.
Train: 2018-08-05T09:51:33.280387: step 8562, loss 0.510016.
Train: 2018-08-05T09:51:36.796383: step 8563, loss 0.579823.
Train: 2018-08-05T09:51:40.312380: step 8564, loss 0.518734.
Train: 2018-08-05T09:51:43.812749: step 8565, loss 0.52744.
Train: 2018-08-05T09:51:47.344372: step 8566, loss 0.606089.
Train: 2018-08-05T09:51:50.829115: step 8567, loss 0.597348.
Train: 2018-08-05T09:51:54.313857: step 8568, loss 0.562371.
Train: 2018-08-05T09:51:57.861107: step 8569, loss 0.492535.
Train: 2018-08-05T09:52:01.330223: step 8570, loss 0.527428.
Test: 2018-08-05T09:52:16.222421: step 8570, loss 0.546761.
Train: 2018-08-05T09:52:19.754043: step 8571, loss 0.59736.
Train: 2018-08-05T09:52:23.285666: step 8572, loss 0.527385.
Train: 2018-08-05T09:52:26.754782: step 8573, loss 0.518602.
Train: 2018-08-05T09:52:30.348911: step 8574, loss 0.588682.
Train: 2018-08-05T09:52:33.833654: step 8575, loss 0.632688.
Train: 2018-08-05T09:52:37.302770: step 8576, loss 0.536099.
Train: 2018-08-05T09:52:40.850020: step 8577, loss 0.571128.
Train: 2018-08-05T09:52:44.303510: step 8578, loss 0.553669.
Train: 2018-08-05T09:52:47.788252: step 8579, loss 0.492431.
Train: 2018-08-05T09:52:51.319875: step 8580, loss 0.579892.
Test: 2018-08-05T09:53:06.165192: step 8580, loss 0.547957.
Train: 2018-08-05T09:53:09.806202: step 8581, loss 0.518593.
Train: 2018-08-05T09:53:13.275318: step 8582, loss 0.597453.
Train: 2018-08-05T09:53:16.791314: step 8583, loss 0.597455.
Train: 2018-08-05T09:53:20.276057: step 8584, loss 0.606182.
Train: 2018-08-05T09:53:23.776426: step 8585, loss 0.588607.
Train: 2018-08-05T09:53:27.292423: step 8586, loss 0.553641.
Train: 2018-08-05T09:53:30.792792: step 8587, loss 0.571065.
Train: 2018-08-05T09:53:34.308787: step 8588, loss 0.51022.
Train: 2018-08-05T09:53:37.809157: step 8589, loss 0.605757.
Train: 2018-08-05T09:53:41.356407: step 8590, loss 0.536348.
Test: 2018-08-05T09:53:56.201724: step 8590, loss 0.549082.
Train: 2018-08-05T09:53:59.717721: step 8591, loss 0.510407.
Train: 2018-08-05T09:54:03.233716: step 8592, loss 0.588317.
Train: 2018-08-05T09:54:06.765339: step 8593, loss 0.562341.
Train: 2018-08-05T09:54:10.250082: step 8594, loss 0.527754.
Train: 2018-08-05T09:54:13.766078: step 8595, loss 0.52776.
Train: 2018-08-05T09:54:17.297701: step 8596, loss 0.545033.
Train: 2018-08-05T09:54:20.798070: step 8597, loss 0.579673.
Train: 2018-08-05T09:54:24.329693: step 8598, loss 0.553678.
Train: 2018-08-05T09:54:27.845689: step 8599, loss 0.631733.
Train: 2018-08-05T09:54:31.330432: step 8600, loss 0.527689.
Test: 2018-08-05T09:54:46.207003: step 8600, loss 0.546484.
Train: 2018-08-05T09:54:52.004490: step 8601, loss 0.59699.
Train: 2018-08-05T09:54:55.489232: step 8602, loss 0.519082.
Train: 2018-08-05T09:54:58.958348: step 8603, loss 0.501784.
Train: 2018-08-05T09:55:02.443091: step 8604, loss 0.648955.
Train: 2018-08-05T09:55:05.974714: step 8605, loss 0.519076.
Train: 2018-08-05T09:55:09.475084: step 8606, loss 0.536382.
Train: 2018-08-05T09:55:11.162762: step 8607, loss 0.469987.
Train: 2018-08-05T09:55:14.647504: step 8608, loss 0.54499.
Train: 2018-08-05T09:55:18.147874: step 8609, loss 0.579757.
Train: 2018-08-05T09:55:21.663870: step 8610, loss 0.571078.
Test: 2018-08-05T09:55:36.634201: step 8610, loss 0.547591.
Train: 2018-08-05T09:55:40.165823: step 8611, loss 0.571094.
Train: 2018-08-05T09:55:43.650566: step 8612, loss 0.501229.
Train: 2018-08-05T09:55:47.119682: step 8613, loss 0.614887.
Train: 2018-08-05T09:55:50.620051: step 8614, loss 0.509844.
Train: 2018-08-05T09:55:54.120421: step 8615, loss 0.54485.
Train: 2018-08-05T09:55:57.573911: step 8616, loss 0.571178.
Train: 2018-08-05T09:56:01.058653: step 8617, loss 0.579985.
Train: 2018-08-05T09:56:04.527770: step 8618, loss 0.588791.
Train: 2018-08-05T09:56:08.043766: step 8619, loss 0.606368.
Train: 2018-08-05T09:56:11.512882: step 8620, loss 0.606302.
Test: 2018-08-05T09:56:26.311320: step 8620, loss 0.54634.
Train: 2018-08-05T09:56:29.827315: step 8621, loss 0.562384.
Train: 2018-08-05T09:56:33.327685: step 8622, loss 0.597343.
Train: 2018-08-05T09:56:36.843682: step 8623, loss 0.53621.
Train: 2018-08-05T09:56:40.390930: step 8624, loss 0.562356.
Train: 2018-08-05T09:56:43.875674: step 8625, loss 0.57103.
Train: 2018-08-05T09:56:47.360416: step 8626, loss 0.596935.
Train: 2018-08-05T09:56:50.829532: step 8627, loss 0.553731.
Train: 2018-08-05T09:56:54.345528: step 8628, loss 0.571043.
Train: 2018-08-05T09:56:57.845898: step 8629, loss 0.485437.
Train: 2018-08-05T09:57:01.361894: step 8630, loss 0.55382.
Test: 2018-08-05T09:57:16.285344: step 8630, loss 0.546997.
Train: 2018-08-05T09:57:19.785714: step 8631, loss 0.545153.
Train: 2018-08-05T09:57:23.286083: step 8632, loss 0.493545.
Train: 2018-08-05T09:57:26.755199: step 8633, loss 0.605521.
Train: 2018-08-05T09:57:30.239942: step 8634, loss 0.579579.
Train: 2018-08-05T09:57:33.818445: step 8635, loss 0.553715.
Train: 2018-08-05T09:57:37.365695: step 8636, loss 0.614046.
Train: 2018-08-05T09:57:40.897317: step 8637, loss 0.614034.
Train: 2018-08-05T09:57:44.350807: step 8638, loss 0.63113.
Train: 2018-08-05T09:57:47.835550: step 8639, loss 0.536636.
Train: 2018-08-05T09:57:51.320292: step 8640, loss 0.528172.
Test: 2018-08-05T09:58:06.165611: step 8640, loss 0.548295.
Train: 2018-08-05T09:58:09.759739: step 8641, loss 0.502648.
Train: 2018-08-05T09:58:13.306989: step 8642, loss 0.553813.
Train: 2018-08-05T09:58:16.838611: step 8643, loss 0.553809.
Train: 2018-08-05T09:58:20.307728: step 8644, loss 0.528219.
Train: 2018-08-05T09:58:23.823724: step 8645, loss 0.57942.
Train: 2018-08-05T09:58:27.308466: step 8646, loss 0.570879.
Train: 2018-08-05T09:58:30.824462: step 8647, loss 0.579451.
Train: 2018-08-05T09:58:34.762378: step 8648, loss 0.553789.
Train: 2018-08-05T09:58:38.434641: step 8649, loss 0.493962.
Train: 2018-08-05T09:58:42.028770: step 8650, loss 0.622259.
Test: 2018-08-05T09:58:59.046192: step 8650, loss 0.547446.
Train: 2018-08-05T09:59:02.780961: step 8651, loss 0.579458.
Train: 2018-08-05T09:59:06.497049: step 8652, loss 0.59657.
Train: 2018-08-05T09:59:10.177045: step 8653, loss 0.519589.
Train: 2018-08-05T09:59:13.989947: step 8654, loss 0.536688.
Train: 2018-08-05T09:59:17.709090: step 8655, loss 0.622216.
Train: 2018-08-05T09:59:21.412608: step 8656, loss 0.536696.
Train: 2018-08-05T09:59:25.444282: step 8657, loss 0.570883.
Train: 2018-08-05T09:59:29.038411: step 8658, loss 0.511083.
Train: 2018-08-05T09:59:32.648167: step 8659, loss 0.587984.
Train: 2018-08-05T09:59:36.351684: step 8660, loss 0.587988.
Test: 2018-08-05T09:59:52.056468: step 8660, loss 0.54767.
Train: 2018-08-05T09:59:55.728729: step 8661, loss 0.511056.
Train: 2018-08-05T09:59:59.479124: step 8662, loss 0.468252.
Train: 2018-08-05T10:00:03.729574: step 8663, loss 0.502299.
Train: 2018-08-05T10:00:07.511223: step 8664, loss 0.622598.
Train: 2018-08-05T10:00:11.324125: step 8665, loss 0.570964.
Train: 2018-08-05T10:00:15.090148: step 8666, loss 0.527781.
Train: 2018-08-05T10:00:18.809290: step 8667, loss 0.527712.
Train: 2018-08-05T10:00:22.559686: step 8668, loss 0.501589.
Train: 2018-08-05T10:00:26.496999: step 8669, loss 0.544939.
Train: 2018-08-05T10:00:30.309902: step 8670, loss 0.518669.
Test: 2018-08-05T10:00:47.371705: step 8670, loss 0.547535.
Train: 2018-08-05T10:00:51.340460: step 8671, loss 0.63259.
Train: 2018-08-05T10:00:56.149834: step 8672, loss 0.632732.
Train: 2018-08-05T10:01:00.470319: step 8673, loss 0.659112.
Train: 2018-08-05T10:01:04.645791: step 8674, loss 0.571164.
Train: 2018-08-05T10:01:08.569837: step 8675, loss 0.536116.
Train: 2018-08-05T10:01:12.507753: step 8676, loss 0.553635.
Train: 2018-08-05T10:01:16.182333: step 8677, loss 0.553638.
Train: 2018-08-05T10:01:19.838969: step 8678, loss 0.55365.
Train: 2018-08-05T10:01:23.448725: step 8679, loss 0.518815.
Train: 2018-08-05T10:01:27.136616: step 8680, loss 0.57107.
Test: 2018-08-05T10:01:42.409297: step 8680, loss 0.548418.
Train: 2018-08-05T10:01:46.144075: step 8681, loss 0.544944.
Train: 2018-08-05T10:01:49.753822: step 8682, loss 0.579774.
Train: 2018-08-05T10:01:53.504221: step 8683, loss 0.579764.
Train: 2018-08-05T10:01:57.238987: step 8684, loss 0.518876.
Train: 2018-08-05T10:02:00.968789: step 8685, loss 0.614531.
Train: 2018-08-05T10:02:04.594171: step 8686, loss 0.510236.
Train: 2018-08-05T10:02:08.203927: step 8687, loss 0.60578.
Train: 2018-08-05T10:02:11.766804: step 8688, loss 0.527642.
Train: 2018-08-05T10:02:15.360933: step 8689, loss 0.501627.
Train: 2018-08-05T10:02:18.970689: step 8690, loss 0.579716.
Test: 2018-08-05T10:02:34.144166: step 8690, loss 0.547244.
Train: 2018-08-05T10:02:37.769548: step 8691, loss 0.579725.
Train: 2018-08-05T10:02:41.473064: step 8692, loss 0.510235.
Train: 2018-08-05T10:02:45.065278: step 8693, loss 0.588437.
Train: 2018-08-05T10:02:48.696704: step 8694, loss 0.605837.
Train: 2018-08-05T10:02:52.330364: step 8695, loss 0.544974.
Train: 2018-08-05T10:02:56.012152: step 8696, loss 0.588407.
Train: 2018-08-05T10:02:59.648820: step 8697, loss 0.518968.
Train: 2018-08-05T10:03:03.275461: step 8698, loss 0.6144.
Train: 2018-08-05T10:03:06.855980: step 8699, loss 0.622994.
Train: 2018-08-05T10:03:10.443518: step 8700, loss 0.51913.
Test: 2018-08-05T10:03:25.725143: step 8700, loss 0.548323.
Train: 2018-08-05T10:03:31.653905: step 8701, loss 0.570968.
Train: 2018-08-05T10:03:35.325666: step 8702, loss 0.55372.
Train: 2018-08-05T10:03:38.968350: step 8703, loss 0.510699.
Train: 2018-08-05T10:03:42.589979: step 8704, loss 0.519306.
Train: 2018-08-05T10:03:46.206593: step 8705, loss 0.614018.
Train: 2018-08-05T10:03:49.829224: step 8706, loss 0.639829.
Train: 2018-08-05T10:03:53.489956: step 8707, loss 0.553743.
Train: 2018-08-05T10:03:57.204832: step 8708, loss 0.545179.
Train: 2018-08-05T10:04:00.848519: step 8709, loss 0.62231.
Train: 2018-08-05T10:04:04.449090: step 8710, loss 0.519603.
Test: 2018-08-05T10:04:19.744754: step 8710, loss 0.548278.
Train: 2018-08-05T10:04:23.337304: step 8711, loss 0.545266.
Train: 2018-08-05T10:04:26.907796: step 8712, loss 0.570869.
Train: 2018-08-05T10:04:30.510374: step 8713, loss 0.647568.
Train: 2018-08-05T10:04:34.146039: step 8714, loss 0.545345.
Train: 2018-08-05T10:04:37.764659: step 8715, loss 0.613249.
Train: 2018-08-05T10:04:41.356207: step 8716, loss 0.570817.
Train: 2018-08-05T10:04:44.947755: step 8717, loss 0.612979.
Train: 2018-08-05T10:04:48.536295: step 8718, loss 0.511955.
Train: 2018-08-05T10:04:52.196025: step 8719, loss 0.528842.
Train: 2018-08-05T10:04:55.852746: step 8720, loss 0.570778.
Test: 2018-08-05T10:05:11.011044: step 8720, loss 0.548615.
Train: 2018-08-05T10:05:14.615627: step 8721, loss 0.612635.
Train: 2018-08-05T10:05:18.266332: step 8722, loss 0.562415.
Train: 2018-08-05T10:05:21.887960: step 8723, loss 0.520713.
Train: 2018-08-05T10:05:25.529642: step 8724, loss 0.579105.
Train: 2018-08-05T10:05:29.123195: step 8725, loss 0.554096.
Train: 2018-08-05T10:05:32.753847: step 8726, loss 0.587429.
Train: 2018-08-05T10:05:36.415581: step 8727, loss 0.47917.
Train: 2018-08-05T10:05:40.001114: step 8728, loss 0.554088.
Train: 2018-08-05T10:05:43.578624: step 8729, loss 0.587475.
Train: 2018-08-05T10:05:47.185212: step 8730, loss 0.528967.
Test: 2018-08-05T10:06:02.750593: step 8730, loss 0.549762.
Train: 2018-08-05T10:06:06.494546: step 8731, loss 0.587529.
Train: 2018-08-05T10:06:10.138233: step 8732, loss 0.503697.
Train: 2018-08-05T10:06:13.749834: step 8733, loss 0.596007.
Train: 2018-08-05T10:06:17.383494: step 8734, loss 0.60447.
Train: 2018-08-05T10:06:20.966018: step 8735, loss 0.528684.
Train: 2018-08-05T10:06:24.625747: step 8736, loss 0.553936.
Train: 2018-08-05T10:06:28.211279: step 8737, loss 0.570809.
Train: 2018-08-05T10:06:31.811853: step 8738, loss 0.56236.
Train: 2018-08-05T10:06:35.396381: step 8739, loss 0.570819.
Train: 2018-08-05T10:06:38.995950: step 8740, loss 0.596238.
Test: 2018-08-05T10:06:54.237469: step 8740, loss 0.549183.
Train: 2018-08-05T10:06:57.866116: step 8741, loss 0.520002.
Train: 2018-08-05T10:07:02.215437: step 8742, loss 0.528434.
Train: 2018-08-05T10:07:06.176328: step 8743, loss 0.587818.
Train: 2018-08-05T10:07:10.289000: step 8744, loss 0.545347.
Train: 2018-08-05T10:07:14.344342: step 8745, loss 0.596424.
Train: 2018-08-05T10:07:18.259981: step 8746, loss 0.536777.
Train: 2018-08-05T10:07:22.122447: step 8747, loss 0.553805.
Train: 2018-08-05T10:07:25.924125: step 8748, loss 0.519659.
Train: 2018-08-05T10:07:29.709152: step 8749, loss 0.545194.
Train: 2018-08-05T10:07:33.567748: step 8750, loss 0.579514.
Test: 2018-08-05T10:07:50.565350: step 8750, loss 0.548569.
Train: 2018-08-05T10:07:54.507431: step 8751, loss 0.536622.
Train: 2018-08-05T10:07:58.223309: step 8752, loss 0.545105.
Train: 2018-08-05T10:08:01.977972: step 8753, loss 0.527898.
Train: 2018-08-05T10:08:05.664731: step 8754, loss 0.588352.
Train: 2018-08-05T10:08:09.442883: step 8755, loss 0.52767.
Train: 2018-08-05T10:08:13.064511: step 8756, loss 0.605859.
Train: 2018-08-05T10:08:16.732261: step 8757, loss 0.483973.
Train: 2018-08-05T10:08:18.519012: step 8758, loss 0.69238.
Train: 2018-08-05T10:08:22.144650: step 8759, loss 0.545361.
Train: 2018-08-05T10:08:25.753243: step 8760, loss 0.553743.
Test: 2018-08-05T10:08:41.260850: step 8760, loss 0.548046.
Train: 2018-08-05T10:08:45.047905: step 8761, loss 0.588359.
Train: 2018-08-05T10:08:48.806898: step 8762, loss 0.545001.
Train: 2018-08-05T10:08:52.785103: step 8763, loss 0.545011.
Train: 2018-08-05T10:08:56.635837: step 8764, loss 0.519017.
Train: 2018-08-05T10:09:00.306595: step 8765, loss 0.466956.
Train: 2018-08-05T10:09:03.942261: step 8766, loss 0.623216.
Train: 2018-08-05T10:09:07.603995: step 8767, loss 0.579763.
Train: 2018-08-05T10:09:11.209591: step 8768, loss 0.562358.
Train: 2018-08-05T10:09:14.843241: step 8769, loss 0.518816.
Train: 2018-08-05T10:09:18.552100: step 8770, loss 0.579799.
Test: 2018-08-05T10:09:33.779582: step 8770, loss 0.547999.
Train: 2018-08-05T10:09:37.413242: step 8771, loss 0.640863.
Train: 2018-08-05T10:09:41.028855: step 8772, loss 0.544939.
Train: 2018-08-05T10:09:44.640456: step 8773, loss 0.53625.
Train: 2018-08-05T10:09:48.250052: step 8774, loss 0.501464.
Train: 2018-08-05T10:09:51.866667: step 8775, loss 0.623303.
Train: 2018-08-05T10:09:55.533415: step 8776, loss 0.562356.
Train: 2018-08-05T10:09:59.194146: step 8777, loss 0.57105.
Train: 2018-08-05T10:10:03.224862: step 8778, loss 0.536288.
Train: 2018-08-05T10:10:06.836464: step 8779, loss 0.544981.
Train: 2018-08-05T10:10:10.450070: step 8780, loss 0.588406.
Test: 2018-08-05T10:10:25.750747: step 8780, loss 0.546655.
Train: 2018-08-05T10:10:29.370369: step 8781, loss 0.562349.
Train: 2018-08-05T10:10:33.062184: step 8782, loss 0.597044.
Train: 2018-08-05T10:10:36.673786: step 8783, loss 0.622977.
Train: 2018-08-05T10:10:40.269344: step 8784, loss 0.545062.
Train: 2018-08-05T10:10:43.920050: step 8785, loss 0.527854.
Train: 2018-08-05T10:10:47.572760: step 8786, loss 0.588168.
Train: 2018-08-05T10:10:51.221461: step 8787, loss 0.502162.
Train: 2018-08-05T10:10:54.836069: step 8788, loss 0.579524.
Train: 2018-08-05T10:10:58.396535: step 8789, loss 0.545157.
Train: 2018-08-05T10:11:02.051251: step 8790, loss 0.502223.
Test: 2018-08-05T10:11:17.596581: step 8790, loss 0.548967.
Train: 2018-08-05T10:11:21.230238: step 8791, loss 0.57953.
Train: 2018-08-05T10:11:24.813765: step 8792, loss 0.57954.
Train: 2018-08-05T10:11:28.400300: step 8793, loss 0.553732.
Train: 2018-08-05T10:11:32.038973: step 8794, loss 0.519309.
Train: 2018-08-05T10:11:35.656590: step 8795, loss 0.588182.
Train: 2018-08-05T10:11:39.265183: step 8796, loss 0.588193.
Train: 2018-08-05T10:11:42.867761: step 8797, loss 0.57957.
Train: 2018-08-05T10:11:46.461314: step 8798, loss 0.648447.
Train: 2018-08-05T10:11:50.066900: step 8799, loss 0.545156.
Train: 2018-08-05T10:11:53.675493: step 8800, loss 0.622346.
Test: 2018-08-05T10:12:09.022292: step 8800, loss 0.549047.
Train: 2018-08-05T10:12:14.965091: step 8801, loss 0.562337.
Train: 2018-08-05T10:12:18.604767: step 8802, loss 0.579386.
Train: 2018-08-05T10:12:22.196315: step 8803, loss 0.54535.
Train: 2018-08-05T10:12:25.773826: step 8804, loss 0.486036.
Train: 2018-08-05T10:12:29.366377: step 8805, loss 0.630173.
Train: 2018-08-05T10:12:32.993018: step 8806, loss 0.57082.
Train: 2018-08-05T10:12:36.604619: step 8807, loss 0.486321.
Train: 2018-08-05T10:12:40.206194: step 8808, loss 0.545459.
Train: 2018-08-05T10:12:43.824814: step 8809, loss 0.553902.
Train: 2018-08-05T10:12:47.400319: step 8810, loss 0.63007.
Test: 2018-08-05T10:13:02.635823: step 8810, loss 0.54726.
Train: 2018-08-05T10:13:06.229376: step 8811, loss 0.579275.
Train: 2018-08-05T10:13:09.884092: step 8812, loss 0.587711.
Train: 2018-08-05T10:13:13.511736: step 8813, loss 0.503303.
Train: 2018-08-05T10:13:17.161439: step 8814, loss 0.604561.
Train: 2018-08-05T10:13:20.812144: step 8815, loss 0.671988.
Train: 2018-08-05T10:13:24.387650: step 8816, loss 0.570789.
Train: 2018-08-05T10:13:28.008282: step 8817, loss 0.528874.
Train: 2018-08-05T10:13:31.692068: step 8818, loss 0.595871.
Train: 2018-08-05T10:13:35.373856: step 8819, loss 0.529034.
Train: 2018-08-05T10:13:39.027570: step 8820, loss 0.579102.
Test: 2018-08-05T10:13:54.443553: step 8820, loss 0.547765.
Train: 2018-08-05T10:13:58.034098: step 8821, loss 0.604067.
Train: 2018-08-05T10:14:01.639683: step 8822, loss 0.57076.
Train: 2018-08-05T10:14:05.268331: step 8823, loss 0.570758.
Train: 2018-08-05T10:14:08.920038: step 8824, loss 0.529364.
Train: 2018-08-05T10:14:12.566733: step 8825, loss 0.512843.
Train: 2018-08-05T10:14:16.158281: step 8826, loss 0.595596.
Train: 2018-08-05T10:14:19.774895: step 8827, loss 0.587317.
Train: 2018-08-05T10:14:23.354411: step 8828, loss 0.579033.
Train: 2018-08-05T10:14:26.953981: step 8829, loss 0.479762.
Train: 2018-08-05T10:14:30.578617: step 8830, loss 0.62047.
Test: 2018-08-05T10:14:45.742931: step 8830, loss 0.54882.
Train: 2018-08-05T10:14:49.338490: step 8831, loss 0.56247.
Train: 2018-08-05T10:14:52.967136: step 8832, loss 0.620499.
Train: 2018-08-05T10:14:56.565703: step 8833, loss 0.479645.
Train: 2018-08-05T10:15:00.187331: step 8834, loss 0.52929.
Train: 2018-08-05T10:15:03.772863: step 8835, loss 0.579071.
Train: 2018-08-05T10:15:07.389478: step 8836, loss 0.595737.
Train: 2018-08-05T10:15:11.051213: step 8837, loss 0.629084.
Train: 2018-08-05T10:15:14.641758: step 8838, loss 0.562437.
Train: 2018-08-05T10:15:18.225285: step 8839, loss 0.537476.
Train: 2018-08-05T10:15:21.801792: step 8840, loss 0.58741.
Test: 2018-08-05T10:15:37.173659: step 8840, loss 0.5495.
Train: 2018-08-05T10:15:40.823361: step 8841, loss 0.479221.
Train: 2018-08-05T10:15:44.444990: step 8842, loss 0.529077.
Train: 2018-08-05T10:15:48.203982: step 8843, loss 0.562412.
Train: 2018-08-05T10:15:51.794528: step 8844, loss 0.562399.
Train: 2018-08-05T10:15:55.421169: step 8845, loss 0.553989.
Train: 2018-08-05T10:15:59.036781: step 8846, loss 0.587631.
Train: 2018-08-05T10:16:02.663422: step 8847, loss 0.596096.
Train: 2018-08-05T10:16:06.287056: step 8848, loss 0.562368.
Train: 2018-08-05T10:16:09.986892: step 8849, loss 0.570808.
Train: 2018-08-05T10:16:13.576434: step 8850, loss 0.621496.
Test: 2018-08-05T10:16:28.757794: step 8850, loss 0.549047.
Train: 2018-08-05T10:16:32.385438: step 8851, loss 0.63833.
Train: 2018-08-05T10:16:36.017093: step 8852, loss 0.553956.
Train: 2018-08-05T10:16:39.612651: step 8853, loss 0.579192.
Train: 2018-08-05T10:16:43.268370: step 8854, loss 0.570781.
Train: 2018-08-05T10:16:46.905038: step 8855, loss 0.503813.
Train: 2018-08-05T10:16:50.525663: step 8856, loss 0.612613.
Train: 2018-08-05T10:16:54.110193: step 8857, loss 0.52063.
Train: 2018-08-05T10:16:57.693719: step 8858, loss 0.503925.
Train: 2018-08-05T10:17:01.344425: step 8859, loss 0.595875.
Train: 2018-08-05T10:17:04.925946: step 8860, loss 0.495431.
Test: 2018-08-05T10:17:20.130367: step 8860, loss 0.547416.
Train: 2018-08-05T10:17:23.758011: step 8861, loss 0.570782.
Train: 2018-08-05T10:17:27.387660: step 8862, loss 0.545574.
Train: 2018-08-05T10:17:31.000265: step 8863, loss 0.503418.
Train: 2018-08-05T10:17:34.584794: step 8864, loss 0.537012.
Train: 2018-08-05T10:17:38.196395: step 8865, loss 0.570832.
Train: 2018-08-05T10:17:41.864145: step 8866, loss 0.596369.
Train: 2018-08-05T10:17:45.479758: step 8867, loss 0.511201.
Train: 2018-08-05T10:17:49.142495: step 8868, loss 0.587982.
Train: 2018-08-05T10:17:52.750086: step 8869, loss 0.553769.
Train: 2018-08-05T10:17:56.393772: step 8870, loss 0.536585.
Test: 2018-08-05T10:18:11.741574: step 8870, loss 0.547175.
Train: 2018-08-05T10:18:15.364205: step 8871, loss 0.622562.
Train: 2018-08-05T10:18:19.007892: step 8872, loss 0.691491.
Train: 2018-08-05T10:18:22.649573: step 8873, loss 0.527968.
Train: 2018-08-05T10:18:26.339382: step 8874, loss 0.536592.
Train: 2018-08-05T10:18:29.936946: step 8875, loss 0.553759.
Train: 2018-08-05T10:18:33.529497: step 8876, loss 0.459457.
Train: 2018-08-05T10:18:37.119040: step 8877, loss 0.605283.
Train: 2018-08-05T10:18:40.689532: step 8878, loss 0.631104.
Train: 2018-08-05T10:18:44.331213: step 8879, loss 0.613868.
Train: 2018-08-05T10:18:47.971892: step 8880, loss 0.493757.
Test: 2018-08-05T10:19:03.250510: step 8880, loss 0.545655.
Train: 2018-08-05T10:19:07.005492: step 8881, loss 0.536625.
Train: 2018-08-05T10:19:10.867760: step 8882, loss 0.545189.
Train: 2018-08-05T10:19:14.460311: step 8883, loss 0.596648.
Train: 2018-08-05T10:19:18.073917: step 8884, loss 0.613795.
Train: 2018-08-05T10:19:21.712591: step 8885, loss 0.562335.
Train: 2018-08-05T10:19:25.347253: step 8886, loss 0.511007.
Train: 2018-08-05T10:19:28.973895: step 8887, loss 0.476785.
Train: 2018-08-05T10:19:32.566446: step 8888, loss 0.52805.
Train: 2018-08-05T10:19:36.167018: step 8889, loss 0.553743.
Train: 2018-08-05T10:19:39.748539: step 8890, loss 0.605397.
Test: 2018-08-05T10:19:55.010111: step 8890, loss 0.54774.
Train: 2018-08-05T10:19:58.616700: step 8891, loss 0.665799.
Train: 2018-08-05T10:20:02.422818: step 8892, loss 0.484839.
Train: 2018-08-05T10:20:06.007348: step 8893, loss 0.579566.
Train: 2018-08-05T10:20:09.586863: step 8894, loss 0.527876.
Train: 2018-08-05T10:20:13.197462: step 8895, loss 0.536473.
Train: 2018-08-05T10:20:16.769960: step 8896, loss 0.536445.
Train: 2018-08-05T10:20:20.440719: step 8897, loss 0.553696.
Train: 2018-08-05T10:20:24.055328: step 8898, loss 0.545028.
Train: 2018-08-05T10:20:27.665926: step 8899, loss 0.605707.
Train: 2018-08-05T10:20:31.312621: step 8900, loss 0.605729.
Test: 2018-08-05T10:20:46.565170: step 8900, loss 0.548464.
Train: 2018-08-05T10:20:52.335510: step 8901, loss 0.492976.
Train: 2018-08-05T10:20:55.933074: step 8902, loss 0.588386.
Train: 2018-08-05T10:20:59.594809: step 8903, loss 0.57103.
Train: 2018-08-05T10:21:03.205407: step 8904, loss 0.571029.
Train: 2018-08-05T10:21:06.784923: step 8905, loss 0.544995.
Train: 2018-08-05T10:21:10.358424: step 8906, loss 0.553672.
Train: 2018-08-05T10:21:13.919892: step 8907, loss 0.484259.
Train: 2018-08-05T10:21:17.514448: step 8908, loss 0.510207.
Train: 2018-08-05T10:21:19.280142: step 8909, loss 0.413665.
Train: 2018-08-05T10:21:22.871690: step 8910, loss 0.597416.
Test: 2018-08-05T10:21:38.013945: step 8910, loss 0.546909.
Train: 2018-08-05T10:21:41.618528: step 8911, loss 0.571194.
Train: 2018-08-05T10:21:45.223111: step 8912, loss 0.650617.
Train: 2018-08-05T10:21:48.802627: step 8913, loss 0.544772.
Train: 2018-08-05T10:21:52.376127: step 8914, loss 0.615446.
Train: 2018-08-05T10:21:55.978704: step 8915, loss 0.641919.
Train: 2018-08-05T10:21:59.582284: step 8916, loss 0.518349.
Train: 2018-08-05T10:22:03.189875: step 8917, loss 0.544803.
Train: 2018-08-05T10:22:06.821530: step 8918, loss 0.492039.
Train: 2018-08-05T10:22:10.392022: step 8919, loss 0.553607.
Train: 2018-08-05T10:22:13.982567: step 8920, loss 0.571216.
Test: 2018-08-05T10:22:29.100759: step 8920, loss 0.548515.
Train: 2018-08-05T10:22:32.716371: step 8921, loss 0.571218.
Train: 2018-08-05T10:22:36.367076: step 8922, loss 0.597629.
Train: 2018-08-05T10:22:40.001739: step 8923, loss 0.536018.
Train: 2018-08-05T10:22:43.582257: step 8924, loss 0.632725.
Train: 2018-08-05T10:22:47.175811: step 8925, loss 0.606251.
Train: 2018-08-05T10:22:50.779391: step 8926, loss 0.579869.
Train: 2018-08-05T10:22:54.349883: step 8927, loss 0.571083.
Train: 2018-08-05T10:22:57.969505: step 8928, loss 0.571046.
Train: 2018-08-05T10:23:01.594142: step 8929, loss 0.51035.
Train: 2018-08-05T10:23:05.175663: step 8930, loss 0.501786.
Test: 2018-08-05T10:23:20.496393: step 8930, loss 0.548098.
Train: 2018-08-05T10:23:24.086939: step 8931, loss 0.631524.
Train: 2018-08-05T10:23:27.663446: step 8932, loss 0.562339.
Train: 2018-08-05T10:23:31.292093: step 8933, loss 0.562337.
Train: 2018-08-05T10:23:34.964857: step 8934, loss 0.553732.
Train: 2018-08-05T10:23:38.625589: step 8935, loss 0.502189.
Train: 2018-08-05T10:23:42.285318: step 8936, loss 0.519369.
Train: 2018-08-05T10:23:45.907949: step 8937, loss 0.648352.
Train: 2018-08-05T10:23:49.507518: step 8938, loss 0.476404.
Train: 2018-08-05T10:23:53.124133: step 8939, loss 0.553734.
Train: 2018-08-05T10:23:56.774838: step 8940, loss 0.51068.
Test: 2018-08-05T10:24:11.999312: step 8940, loss 0.547336.
Train: 2018-08-05T10:24:15.626956: step 8941, loss 0.476077.
Train: 2018-08-05T10:24:19.249587: step 8942, loss 0.527718.
Train: 2018-08-05T10:24:22.829103: step 8943, loss 0.536284.
Train: 2018-08-05T10:24:26.489835: step 8944, loss 0.649599.
Train: 2018-08-05T10:24:30.058322: step 8945, loss 0.614794.
Train: 2018-08-05T10:24:33.660899: step 8946, loss 0.649761.
Train: 2018-08-05T10:24:37.269493: step 8947, loss 0.544918.
Train: 2018-08-05T10:24:40.889115: step 8948, loss 0.501374.
Train: 2018-08-05T10:24:44.486679: step 8949, loss 0.536225.
Train: 2018-08-05T10:24:48.071209: step 8950, loss 0.544933.
Test: 2018-08-05T10:25:03.313731: step 8950, loss 0.547598.
Train: 2018-08-05T10:25:06.917311: step 8951, loss 0.510033.
Train: 2018-08-05T10:25:10.543952: step 8952, loss 0.588578.
Train: 2018-08-05T10:25:14.170593: step 8953, loss 0.579859.
Train: 2018-08-05T10:25:17.743091: step 8954, loss 0.597353.
Train: 2018-08-05T10:25:21.330628: step 8955, loss 0.553633.
Train: 2018-08-05T10:25:24.931200: step 8956, loss 0.501229.
Train: 2018-08-05T10:25:28.564860: step 8957, loss 0.483708.
Train: 2018-08-05T10:25:32.193507: step 8958, loss 0.544863.
Train: 2018-08-05T10:25:35.810122: step 8959, loss 0.509723.
Train: 2018-08-05T10:25:39.420721: step 8960, loss 0.536001.
Test: 2018-08-05T10:25:54.693323: step 8960, loss 0.547274.
Train: 2018-08-05T10:25:58.287879: step 8961, loss 0.571251.
Train: 2018-08-05T10:26:01.918530: step 8962, loss 0.518195.
Train: 2018-08-05T10:26:05.502057: step 8963, loss 0.500332.
Train: 2018-08-05T10:26:09.119675: step 8964, loss 0.544676.
Train: 2018-08-05T10:26:12.737292: step 8965, loss 0.51785.
Train: 2018-08-05T10:26:16.321821: step 8966, loss 0.535656.
Train: 2018-08-05T10:26:19.914372: step 8967, loss 0.616652.
Train: 2018-08-05T10:26:23.490880: step 8968, loss 0.544571.
Train: 2018-08-05T10:26:27.065383: step 8969, loss 0.571683.
Train: 2018-08-05T10:26:30.721102: step 8970, loss 0.607909.
Test: 2018-08-05T10:26:45.925522: step 8970, loss 0.546095.
Train: 2018-08-05T10:26:49.642403: step 8971, loss 0.562648.
Train: 2018-08-05T10:26:53.268042: step 8972, loss 0.535502.
Train: 2018-08-05T10:26:56.920753: step 8973, loss 0.616953.
Train: 2018-08-05T10:27:00.541378: step 8974, loss 0.562646.
Train: 2018-08-05T10:27:04.144958: step 8975, loss 0.562627.
Train: 2018-08-05T10:27:07.761573: step 8976, loss 0.481572.
Train: 2018-08-05T10:27:11.638881: step 8977, loss 0.544597.
Train: 2018-08-05T10:27:15.345735: step 8978, loss 0.54459.
Train: 2018-08-05T10:27:18.974382: step 8979, loss 0.5176.
Train: 2018-08-05T10:27:22.650154: step 8980, loss 0.544565.
Test: 2018-08-05T10:27:37.835524: step 8980, loss 0.547769.
Train: 2018-08-05T10:27:41.492245: step 8981, loss 0.652813.
Train: 2018-08-05T10:27:45.119889: step 8982, loss 0.607635.
Train: 2018-08-05T10:27:48.777613: step 8983, loss 0.643442.
Train: 2018-08-05T10:27:52.399241: step 8984, loss 0.571473.
Train: 2018-08-05T10:27:56.044933: step 8985, loss 0.517944.
Train: 2018-08-05T10:27:59.642497: step 8986, loss 0.571362.
Train: 2018-08-05T10:28:03.267134: step 8987, loss 0.562452.
Train: 2018-08-05T10:28:06.878735: step 8988, loss 0.580099.
Train: 2018-08-05T10:28:10.642741: step 8989, loss 0.474348.
Train: 2018-08-05T10:28:14.262364: step 8990, loss 0.553608.
Test: 2018-08-05T10:28:29.578080: step 8990, loss 0.547117.
Train: 2018-08-05T10:28:33.185671: step 8991, loss 0.606335.
Train: 2018-08-05T10:28:36.807304: step 8992, loss 0.544848.
Train: 2018-08-05T10:28:40.428927: step 8993, loss 0.632436.
Train: 2018-08-05T10:28:44.032507: step 8994, loss 0.579831.
Train: 2018-08-05T10:28:47.677196: step 8995, loss 0.562357.
Train: 2018-08-05T10:28:51.360990: step 8996, loss 0.579706.
Train: 2018-08-05T10:28:54.949530: step 8997, loss 0.536387.
Train: 2018-08-05T10:28:58.578177: step 8998, loss 0.527811.
Train: 2018-08-05T10:29:02.163709: step 8999, loss 0.639917.
Train: 2018-08-05T10:29:05.810403: step 9000, loss 0.502173.
Test: 2018-08-05T10:29:20.965694: step 9000, loss 0.548989.
Train: 2018-08-05T10:29:26.909495: step 9001, loss 0.502257.
Train: 2018-08-05T10:29:30.787644: step 9002, loss 0.519422.
Train: 2018-08-05T10:29:35.039112: step 9003, loss 0.527974.
Train: 2018-08-05T10:29:38.752303: step 9004, loss 0.527923.
Train: 2018-08-05T10:29:42.462575: step 9005, loss 0.579579.
Train: 2018-08-05T10:29:46.316429: step 9006, loss 0.579605.
Train: 2018-08-05T10:29:50.526691: step 9007, loss 0.536418.
Train: 2018-08-05T10:29:54.402213: step 9008, loss 0.579645.
Train: 2018-08-05T10:29:58.279954: step 9009, loss 0.588317.
Train: 2018-08-05T10:30:02.125220: step 9010, loss 0.579659.
Test: 2018-08-05T10:30:18.430405: step 9010, loss 0.548089.
Train: 2018-08-05T10:30:22.147802: step 9011, loss 0.510417.
Train: 2018-08-05T10:30:25.930190: step 9012, loss 0.493063.
Train: 2018-08-05T10:30:29.626015: step 9013, loss 0.649122.
Train: 2018-08-05T10:30:33.317666: step 9014, loss 0.484262.
Train: 2018-08-05T10:30:37.025523: step 9015, loss 0.649241.
Train: 2018-08-05T10:30:40.863727: step 9016, loss 0.56235.
Train: 2018-08-05T10:30:44.536490: step 9017, loss 0.571027.
Train: 2018-08-05T10:30:48.219281: step 9018, loss 0.571018.
Train: 2018-08-05T10:30:51.873997: step 9019, loss 0.536357.
Train: 2018-08-05T10:30:55.586868: step 9020, loss 0.588321.
Test: 2018-08-05T10:31:11.081705: step 9020, loss 0.548692.
Train: 2018-08-05T10:31:14.768146: step 9021, loss 0.493138.
Train: 2018-08-05T10:31:18.429880: step 9022, loss 0.596962.
Train: 2018-08-05T10:31:22.135636: step 9023, loss 0.640208.
Train: 2018-08-05T10:31:25.817188: step 9024, loss 0.510533.
Train: 2018-08-05T10:31:29.663413: step 9025, loss 0.579593.
Train: 2018-08-05T10:31:33.377287: step 9026, loss 0.570955.
Train: 2018-08-05T10:31:37.014958: step 9027, loss 0.579551.
Train: 2018-08-05T10:31:40.683306: step 9028, loss 0.596713.
Train: 2018-08-05T10:31:44.352060: step 9029, loss 0.605215.
Train: 2018-08-05T10:31:47.992739: step 9030, loss 0.528131.
Test: 2018-08-05T10:32:03.461100: step 9030, loss 0.547101.
Train: 2018-08-05T10:32:07.189364: step 9031, loss 0.57941.
Train: 2018-08-05T10:32:10.888360: step 9032, loss 0.57086.
Train: 2018-08-05T10:32:14.655548: step 9033, loss 0.587848.
Train: 2018-08-05T10:32:18.379448: step 9034, loss 0.545389.
Train: 2018-08-05T10:32:22.057226: step 9035, loss 0.579288.
Train: 2018-08-05T10:32:25.786139: step 9036, loss 0.50321.
Train: 2018-08-05T10:32:29.449879: step 9037, loss 0.520126.
Train: 2018-08-05T10:32:33.135186: step 9038, loss 0.511642.
Train: 2018-08-05T10:32:36.839681: step 9039, loss 0.503078.
Train: 2018-08-05T10:32:40.614717: step 9040, loss 0.519881.
Test: 2018-08-05T10:32:56.012485: step 9040, loss 0.548301.
Train: 2018-08-05T10:32:59.666198: step 9041, loss 0.570864.
Train: 2018-08-05T10:33:03.446133: step 9042, loss 0.536687.
Train: 2018-08-05T10:33:07.120411: step 9043, loss 0.570915.
Train: 2018-08-05T10:33:10.849324: step 9044, loss 0.596742.
Train: 2018-08-05T10:33:14.546152: step 9045, loss 0.5968.
Train: 2018-08-05T10:33:18.269050: step 9046, loss 0.545093.
Train: 2018-08-05T10:33:22.053215: step 9047, loss 0.614123.
Train: 2018-08-05T10:33:25.867355: step 9048, loss 0.562338.
Train: 2018-08-05T10:33:29.677290: step 9049, loss 0.605467.
Train: 2018-08-05T10:33:33.534100: step 9050, loss 0.562337.
Test: 2018-08-05T10:33:49.475767: step 9050, loss 0.547964.
Train: 2018-08-05T10:33:53.243784: step 9051, loss 0.674205.
Train: 2018-08-05T10:33:56.974702: step 9052, loss 0.579486.
Train: 2018-08-05T10:34:00.689049: step 9053, loss 0.570883.
Train: 2018-08-05T10:34:04.385877: step 9054, loss 0.502725.
Train: 2018-08-05T10:34:08.039148: step 9055, loss 0.587851.
Train: 2018-08-05T10:34:11.925449: step 9056, loss 0.570834.
Train: 2018-08-05T10:34:15.832586: step 9057, loss 0.553889.
Train: 2018-08-05T10:34:19.609687: step 9058, loss 0.646892.
Train: 2018-08-05T10:34:23.429165: step 9059, loss 0.562373.
Train: 2018-08-05T10:34:25.191851: step 9060, loss 0.562386.
Test: 2018-08-05T10:34:41.130088: step 9060, loss 0.548983.
Train: 2018-08-05T10:34:45.031461: step 9061, loss 0.604297.
Train: 2018-08-05T10:34:48.777149: step 9062, loss 0.520648.
Train: 2018-08-05T10:34:52.429860: step 9063, loss 0.537416.
Train: 2018-08-05T10:34:56.180786: step 9064, loss 0.612417.
Train: 2018-08-05T10:34:59.791385: step 9065, loss 0.537498.
Train: 2018-08-05T10:35:03.388949: step 9066, loss 0.595679.
Train: 2018-08-05T10:35:07.002313: step 9067, loss 0.595642.
Train: 2018-08-05T10:35:10.673919: step 9068, loss 0.504568.
Train: 2018-08-05T10:35:14.572283: step 9069, loss 0.56248.
Train: 2018-08-05T10:35:18.210956: step 9070, loss 0.545928.
Test: 2018-08-05T10:35:33.565777: step 9070, loss 0.547887.
Train: 2018-08-05T10:35:37.347831: step 9071, loss 0.661835.
Train: 2018-08-05T10:35:41.226141: step 9072, loss 0.529428.
Train: 2018-08-05T10:35:45.096431: step 9073, loss 0.471632.
Train: 2018-08-05T10:35:48.989781: step 9074, loss 0.587305.
Train: 2018-08-05T10:35:52.860070: step 9075, loss 0.55419.
Train: 2018-08-05T10:35:56.707298: step 9076, loss 0.537579.
Train: 2018-08-05T10:36:00.560542: step 9077, loss 0.662176.
Train: 2018-08-05T10:36:04.297476: step 9078, loss 0.471074.
Train: 2018-08-05T10:36:08.083541: step 9079, loss 0.545795.
Train: 2018-08-05T10:36:11.742268: step 9080, loss 0.562426.
Test: 2018-08-05T10:36:27.013867: step 9080, loss 0.55038.
Train: 2018-08-05T10:36:30.620455: step 9081, loss 0.595843.
Train: 2018-08-05T10:36:34.203982: step 9082, loss 0.562407.
Train: 2018-08-05T10:36:37.828618: step 9083, loss 0.487004.
Train: 2018-08-05T10:36:41.414150: step 9084, loss 0.59599.
Train: 2018-08-05T10:36:45.032770: step 9085, loss 0.537127.
Train: 2018-08-05T10:36:48.607272: step 9086, loss 0.528623.
Train: 2018-08-05T10:36:52.222884: step 9087, loss 0.511594.
Train: 2018-08-05T10:36:55.793377: step 9088, loss 0.545363.
Train: 2018-08-05T10:36:59.380914: step 9089, loss 0.587913.
Train: 2018-08-05T10:37:02.946393: step 9090, loss 0.502499.
Test: 2018-08-05T10:37:18.209971: step 9090, loss 0.548794.
Train: 2018-08-05T10:37:21.849647: step 9091, loss 0.596659.
Train: 2018-08-05T10:37:25.435179: step 9092, loss 0.648373.
Train: 2018-08-05T10:37:29.042769: step 9093, loss 0.596769.
Train: 2018-08-05T10:37:32.672419: step 9094, loss 0.49349.
Train: 2018-08-05T10:37:36.270986: step 9095, loss 0.648493.
Train: 2018-08-05T10:37:39.887600: step 9096, loss 0.631204.
Train: 2018-08-05T10:37:43.531287: step 9097, loss 0.588103.
Train: 2018-08-05T10:37:47.195027: step 9098, loss 0.553768.
Train: 2018-08-05T10:37:50.849744: step 9099, loss 0.553788.
Train: 2018-08-05T10:37:54.526518: step 9100, loss 0.570872.
Test: 2018-08-05T10:38:09.759013: step 9100, loss 0.548312.
Train: 2018-08-05T10:38:16.856882: step 9101, loss 0.519753.
Train: 2018-08-05T10:38:20.454446: step 9102, loss 0.528295.
Train: 2018-08-05T10:38:24.027947: step 9103, loss 0.587882.
Train: 2018-08-05T10:38:27.602449: step 9104, loss 0.545324.
Train: 2018-08-05T10:38:31.212045: step 9105, loss 0.63042.
Train: 2018-08-05T10:38:34.803594: step 9106, loss 0.51986.
Train: 2018-08-05T10:38:38.392133: step 9107, loss 0.6303.
Train: 2018-08-05T10:38:41.981676: step 9108, loss 0.630184.
Train: 2018-08-05T10:38:45.598291: step 9109, loss 0.511641.
Train: 2018-08-05T10:38:49.194852: step 9110, loss 0.520167.
Test: 2018-08-05T10:39:04.288980: step 9110, loss 0.549056.
Train: 2018-08-05T10:39:07.910608: step 9111, loss 0.621421.
Train: 2018-08-05T10:39:11.543265: step 9112, loss 0.562375.
Train: 2018-08-05T10:39:15.120776: step 9113, loss 0.53715.
Train: 2018-08-05T10:39:18.713327: step 9114, loss 0.52036.
Train: 2018-08-05T10:39:22.302869: step 9115, loss 0.562382.
Train: 2018-08-05T10:39:25.918481: step 9116, loss 0.469844.
Train: 2018-08-05T10:39:29.537101: step 9117, loss 0.58767.
Train: 2018-08-05T10:39:33.143689: step 9118, loss 0.553914.
Train: 2018-08-05T10:39:36.732230: step 9119, loss 0.520036.
Train: 2018-08-05T10:39:40.312748: step 9120, loss 0.545377.
Test: 2018-08-05T10:39:55.495110: step 9120, loss 0.550287.
Train: 2018-08-05T10:39:59.123757: step 9121, loss 0.6134.
Train: 2018-08-05T10:40:02.826601: step 9122, loss 0.604946.
Train: 2018-08-05T10:40:06.429178: step 9123, loss 0.545293.
Train: 2018-08-05T10:40:10.060833: step 9124, loss 0.639105.
Train: 2018-08-05T10:40:13.655389: step 9125, loss 0.528258.
Train: 2018-08-05T10:40:17.224878: step 9126, loss 0.562341.
Train: 2018-08-05T10:40:20.849514: step 9127, loss 0.630482.
Train: 2018-08-05T10:40:24.411985: step 9128, loss 0.536833.
Train: 2018-08-05T10:40:28.033613: step 9129, loss 0.562346.
Train: 2018-08-05T10:40:31.659252: step 9130, loss 0.570838.
Test: 2018-08-05T10:40:46.848633: step 9130, loss 0.54916.
Train: 2018-08-05T10:40:50.447199: step 9131, loss 0.494489.
Train: 2018-08-05T10:40:54.059803: step 9132, loss 0.604793.
Train: 2018-08-05T10:40:57.637314: step 9133, loss 0.58781.
Train: 2018-08-05T10:41:01.216830: step 9134, loss 0.486023.
Train: 2018-08-05T10:41:04.849487: step 9135, loss 0.50292.
Train: 2018-08-05T10:41:08.474123: step 9136, loss 0.460238.
Train: 2018-08-05T10:41:12.069683: step 9137, loss 0.502519.
Train: 2018-08-05T10:41:15.657220: step 9138, loss 0.545137.
Train: 2018-08-05T10:41:19.244757: step 9139, loss 0.571038.
Train: 2018-08-05T10:41:22.825276: step 9140, loss 0.579772.
Test: 2018-08-05T10:41:37.992598: step 9140, loss 0.548229.
Train: 2018-08-05T10:41:41.610215: step 9141, loss 0.527617.
Train: 2018-08-05T10:41:45.233849: step 9142, loss 0.64089.
Train: 2018-08-05T10:41:48.850463: step 9143, loss 0.492477.
Train: 2018-08-05T10:41:52.457051: step 9144, loss 0.501064.
Train: 2018-08-05T10:41:56.055618: step 9145, loss 0.606352.
Train: 2018-08-05T10:41:59.647166: step 9146, loss 0.465517.
Train: 2018-08-05T10:42:03.245733: step 9147, loss 0.580119.
Train: 2018-08-05T10:42:06.853324: step 9148, loss 0.59792.
Train: 2018-08-05T10:42:10.478962: step 9149, loss 0.61575.
Train: 2018-08-05T10:42:14.071513: step 9150, loss 0.509179.
Test: 2018-08-05T10:42:29.269918: step 9150, loss 0.548654.
Train: 2018-08-05T10:42:32.901572: step 9151, loss 0.571372.
Train: 2018-08-05T10:42:36.507158: step 9152, loss 0.553588.
Train: 2018-08-05T10:42:40.081660: step 9153, loss 0.526884.
Train: 2018-08-05T10:42:43.679224: step 9154, loss 0.544678.
Train: 2018-08-05T10:42:47.287818: step 9155, loss 0.589265.
Train: 2018-08-05T10:42:50.873350: step 9156, loss 0.589272.
Train: 2018-08-05T10:42:54.447853: step 9157, loss 0.562504.
Train: 2018-08-05T10:42:58.043411: step 9158, loss 0.544678.
Train: 2018-08-05T10:43:01.630949: step 9159, loss 0.60702.
Train: 2018-08-05T10:43:05.262603: step 9160, loss 0.535805.
Test: 2018-08-05T10:43:20.591355: step 9160, loss 0.548251.
Train: 2018-08-05T10:43:24.179895: step 9161, loss 0.491409.
Train: 2018-08-05T10:43:27.786483: step 9162, loss 0.589131.
Train: 2018-08-05T10:43:31.380036: step 9163, loss 0.535826.
Train: 2018-08-05T10:43:34.994645: step 9164, loss 0.509184.
Train: 2018-08-05T10:43:38.638332: step 9165, loss 0.606918.
Train: 2018-08-05T10:43:42.370254: step 9166, loss 0.606901.
Train: 2018-08-05T10:43:46.055049: step 9167, loss 0.544717.
Train: 2018-08-05T10:43:49.708762: step 9168, loss 0.571319.
Train: 2018-08-05T10:43:53.305324: step 9169, loss 0.544741.
Train: 2018-08-05T10:43:56.903891: step 9170, loss 0.606655.
Test: 2018-08-05T10:44:12.233645: step 9170, loss 0.548498.
Train: 2018-08-05T10:44:15.820179: step 9171, loss 0.588902.
Train: 2018-08-05T10:44:19.458853: step 9172, loss 0.51839.
Train: 2018-08-05T10:44:23.114571: step 9173, loss 0.553594.
Train: 2018-08-05T10:44:26.758262: step 9174, loss 0.562496.
Train: 2018-08-05T10:44:30.334766: step 9175, loss 0.536152.
Train: 2018-08-05T10:44:33.915285: step 9176, loss 0.536199.
Train: 2018-08-05T10:44:37.544934: step 9177, loss 0.571074.
Train: 2018-08-05T10:44:41.183607: step 9178, loss 0.579972.
Train: 2018-08-05T10:44:44.848832: step 9179, loss 0.536159.
Train: 2018-08-05T10:44:48.513575: step 9180, loss 0.632193.
Test: 2018-08-05T10:45:03.829291: step 9180, loss 0.546813.
Train: 2018-08-05T10:45:07.473980: step 9181, loss 0.588408.
Train: 2018-08-05T10:45:11.039459: step 9182, loss 0.527731.
Train: 2018-08-05T10:45:14.616970: step 9183, loss 0.605741.
Train: 2018-08-05T10:45:18.232582: step 9184, loss 0.571252.
Train: 2018-08-05T10:45:21.838167: step 9185, loss 0.52831.
Train: 2018-08-05T10:45:25.452777: step 9186, loss 0.55391.
Train: 2018-08-05T10:45:29.125541: step 9187, loss 0.588269.
Train: 2018-08-05T10:45:32.742156: step 9188, loss 0.545108.
Train: 2018-08-05T10:45:36.330695: step 9189, loss 0.596781.
Train: 2018-08-05T10:45:39.937284: step 9190, loss 0.562335.
Test: 2018-08-05T10:45:55.083550: step 9190, loss 0.548002.
Train: 2018-08-05T10:45:58.688132: step 9191, loss 0.536588.
Train: 2018-08-05T10:46:02.274667: step 9192, loss 0.613782.
Train: 2018-08-05T10:46:05.862205: step 9193, loss 0.519548.
Train: 2018-08-05T10:46:09.435705: step 9194, loss 0.587988.
Train: 2018-08-05T10:46:13.031263: step 9195, loss 0.536718.
Train: 2018-08-05T10:46:16.641862: step 9196, loss 0.647674.
Train: 2018-08-05T10:46:20.242434: step 9197, loss 0.511268.
Train: 2018-08-05T10:46:23.833982: step 9198, loss 0.570849.
Train: 2018-08-05T10:46:27.450597: step 9199, loss 0.621808.
Train: 2018-08-05T10:46:31.029110: step 9200, loss 0.528455.
Test: 2018-08-05T10:46:46.192422: step 9200, loss 0.547443.
Train: 2018-08-05T10:46:52.046986: step 9201, loss 0.587747.
Train: 2018-08-05T10:46:55.673627: step 9202, loss 0.537014.
Train: 2018-08-05T10:46:59.294253: step 9203, loss 0.53704.
Train: 2018-08-05T10:47:02.879785: step 9204, loss 0.596127.
Train: 2018-08-05T10:47:06.448271: step 9205, loss 0.511772.
Train: 2018-08-05T10:47:10.015755: step 9206, loss 0.57924.
Train: 2018-08-05T10:47:13.584242: step 9207, loss 0.604551.
Train: 2018-08-05T10:47:17.400387: step 9208, loss 0.57923.
Train: 2018-08-05T10:47:21.011989: step 9209, loss 0.604479.
Train: 2018-08-05T10:47:24.668710: step 9210, loss 0.5876.
Test: 2018-08-05T10:47:39.771861: step 9210, loss 0.548193.
Train: 2018-08-05T10:47:41.544574: step 9211, loss 0.526607.
Train: 2018-08-05T10:47:45.185253: step 9212, loss 0.562399.
Train: 2018-08-05T10:47:48.780811: step 9213, loss 0.537288.
Train: 2018-08-05T10:47:52.385394: step 9214, loss 0.537291.
Train: 2018-08-05T10:47:56.015044: step 9215, loss 0.545649.
Train: 2018-08-05T10:47:59.639680: step 9216, loss 0.663007.
Train: 2018-08-05T10:48:03.232230: step 9217, loss 0.537282.
Train: 2018-08-05T10:48:06.818765: step 9218, loss 0.570775.
Train: 2018-08-05T10:48:10.390260: step 9219, loss 0.57914.
Train: 2018-08-05T10:48:13.962757: step 9220, loss 0.554054.
Test: 2018-08-05T10:48:29.162165: step 9220, loss 0.548268.
Train: 2018-08-05T10:48:32.818886: step 9221, loss 0.562415.
Train: 2018-08-05T10:48:36.440514: step 9222, loss 0.529001.
Train: 2018-08-05T10:48:40.023038: step 9223, loss 0.620926.
Train: 2018-08-05T10:48:43.624613: step 9224, loss 0.554062.
Train: 2018-08-05T10:48:47.258273: step 9225, loss 0.604175.
Train: 2018-08-05T10:48:50.835783: step 9226, loss 0.504026.
Train: 2018-08-05T10:48:54.461422: step 9227, loss 0.579114.
Train: 2018-08-05T10:48:58.087061: step 9228, loss 0.512334.
Train: 2018-08-05T10:49:01.688635: step 9229, loss 0.612567.
Train: 2018-08-05T10:49:05.263138: step 9230, loss 0.629297.
Test: 2018-08-05T10:49:20.390354: step 9230, loss 0.548284.
Train: 2018-08-05T10:49:24.104227: step 9231, loss 0.562403.
Train: 2018-08-05T10:49:27.831135: step 9232, loss 0.545804.
Train: 2018-08-05T10:49:31.460785: step 9233, loss 0.5124.
Train: 2018-08-05T10:49:35.167639: step 9234, loss 0.637544.
Train: 2018-08-05T10:49:39.104104: step 9235, loss 0.620814.
Train: 2018-08-05T10:49:42.770852: step 9236, loss 0.529137.
Train: 2018-08-05T10:49:46.366410: step 9237, loss 0.595724.
Train: 2018-08-05T10:49:50.236700: step 9238, loss 0.537534.
Train: 2018-08-05T10:49:53.901442: step 9239, loss 0.545853.
Train: 2018-08-05T10:49:57.518057: step 9240, loss 0.512648.
Test: 2018-08-05T10:50:12.811398: step 9240, loss 0.548756.
Train: 2018-08-05T10:50:16.528279: step 9241, loss 0.545822.
Train: 2018-08-05T10:50:20.119828: step 9242, loss 0.604071.
Train: 2018-08-05T10:50:23.723407: step 9243, loss 0.562431.
Train: 2018-08-05T10:50:27.353057: step 9244, loss 0.587444.
Train: 2018-08-05T10:50:30.970674: step 9245, loss 0.662515.
Train: 2018-08-05T10:50:34.613358: step 9246, loss 0.554114.
Train: 2018-08-05T10:50:38.183850: step 9247, loss 0.587386.
Train: 2018-08-05T10:50:41.770385: step 9248, loss 0.620547.
Train: 2018-08-05T10:50:45.358925: step 9249, loss 0.454901.
Train: 2018-08-05T10:50:48.963508: step 9250, loss 0.570756.
Test: 2018-08-05T10:51:04.122809: step 9250, loss 0.548846.
Train: 2018-08-05T10:51:07.756468: step 9251, loss 0.587311.
Train: 2018-08-05T10:51:11.376091: step 9252, loss 0.562482.
Train: 2018-08-05T10:51:14.962626: step 9253, loss 0.55421.
Train: 2018-08-05T10:51:18.540137: step 9254, loss 0.51283.
Train: 2018-08-05T10:51:22.125669: step 9255, loss 0.570757.
Train: 2018-08-05T10:51:25.699169: step 9256, loss 0.570758.
Train: 2018-08-05T10:51:29.331826: step 9257, loss 0.579067.
Train: 2018-08-05T10:51:32.991556: step 9258, loss 0.579075.
Train: 2018-08-05T10:51:36.585109: step 9259, loss 0.487586.
Train: 2018-08-05T10:51:40.167633: step 9260, loss 0.537421.
Test: 2018-08-05T10:51:55.360022: step 9260, loss 0.547492.
Train: 2018-08-05T10:51:58.948562: step 9261, loss 0.579129.
Train: 2018-08-05T10:52:02.613304: step 9262, loss 0.537272.
Train: 2018-08-05T10:52:06.508661: step 9263, loss 0.570786.
Train: 2018-08-05T10:52:10.349872: step 9264, loss 0.553962.
Train: 2018-08-05T10:52:13.938412: step 9265, loss 0.553934.
Train: 2018-08-05T10:52:17.527955: step 9266, loss 0.613085.
Train: 2018-08-05T10:52:21.105465: step 9267, loss 0.562357.
Train: 2018-08-05T10:52:24.690997: step 9268, loss 0.562355.
Train: 2018-08-05T10:52:28.270514: step 9269, loss 0.587783.
Train: 2018-08-05T10:52:31.870084: step 9270, loss 0.511477.
Test: 2018-08-05T10:52:47.235955: step 9270, loss 0.546411.
Train: 2018-08-05T10:52:50.883652: step 9271, loss 0.570841.
Train: 2018-08-05T10:52:54.475200: step 9272, loss 0.511344.
Train: 2018-08-05T10:52:58.092818: step 9273, loss 0.587904.
Train: 2018-08-05T10:53:01.704419: step 9274, loss 0.459996.
Train: 2018-08-05T10:53:05.340084: step 9275, loss 0.59658.
Train: 2018-08-05T10:53:08.929627: step 9276, loss 0.570913.
Train: 2018-08-05T10:53:12.661548: step 9277, loss 0.502171.
Train: 2018-08-05T10:53:16.267134: step 9278, loss 0.519216.
Train: 2018-08-05T10:53:19.952933: step 9279, loss 0.553687.
Train: 2018-08-05T10:53:23.619680: step 9280, loss 0.562354.
Test: 2018-08-05T10:53:38.979515: step 9280, loss 0.547589.
Train: 2018-08-05T10:53:42.626209: step 9281, loss 0.553705.
Train: 2018-08-05T10:53:46.273906: step 9282, loss 0.580153.
Train: 2018-08-05T10:53:49.912580: step 9283, loss 0.579847.
Train: 2018-08-05T10:53:53.508138: step 9284, loss 0.571189.
Train: 2018-08-05T10:53:57.253094: step 9285, loss 0.501077.
Train: 2018-08-05T10:54:00.828600: step 9286, loss 0.544841.
Train: 2018-08-05T10:54:04.418142: step 9287, loss 0.562402.
Train: 2018-08-05T10:54:08.089904: step 9288, loss 0.535997.
Train: 2018-08-05T10:54:11.696492: step 9289, loss 0.518321.
Train: 2018-08-05T10:54:15.291048: step 9290, loss 0.571277.
Test: 2018-08-05T10:54:30.643863: step 9290, loss 0.548474.
Train: 2018-08-05T10:54:34.232403: step 9291, loss 0.624442.
Train: 2018-08-05T10:54:37.820943: step 9292, loss 0.571308.
Train: 2018-08-05T10:54:41.426529: step 9293, loss 0.589015.
Train: 2018-08-05T10:54:45.067207: step 9294, loss 0.473965.
Train: 2018-08-05T10:54:48.670787: step 9295, loss 0.589012.
Train: 2018-08-05T10:54:52.264340: step 9296, loss 0.447334.
Train: 2018-08-05T10:54:55.853883: step 9297, loss 0.544716.
Train: 2018-08-05T10:54:59.465485: step 9298, loss 0.589159.
Train: 2018-08-05T10:55:03.049011: step 9299, loss 0.535782.
Train: 2018-08-05T10:55:06.671642: step 9300, loss 0.500097.
Test: 2018-08-05T10:55:21.850996: step 9300, loss 0.548836.
Train: 2018-08-05T10:55:27.698541: step 9301, loss 0.535718.
Train: 2018-08-05T10:55:31.320169: step 9302, loss 0.526722.
Train: 2018-08-05T10:55:34.895675: step 9303, loss 0.634412.
Train: 2018-08-05T10:55:38.474189: step 9304, loss 0.58954.
Train: 2018-08-05T10:55:42.038664: step 9305, loss 0.571565.
Train: 2018-08-05T10:55:45.670320: step 9306, loss 0.526656.
Train: 2018-08-05T10:55:49.297963: step 9307, loss 0.571553.
Train: 2018-08-05T10:55:52.873469: step 9308, loss 0.580523.
Train: 2018-08-05T10:55:56.451982: step 9309, loss 0.535671.
Train: 2018-08-05T10:56:00.040522: step 9310, loss 0.598396.
Test: 2018-08-05T10:56:15.173754: step 9310, loss 0.546771.
Train: 2018-08-05T10:56:18.769312: step 9311, loss 0.580427.
Train: 2018-08-05T10:56:22.402972: step 9312, loss 0.526814.
Train: 2018-08-05T10:56:26.004547: step 9313, loss 0.553592.
Train: 2018-08-05T10:56:29.625172: step 9314, loss 0.464499.
Train: 2018-08-05T10:56:33.213712: step 9315, loss 0.598179.
Train: 2018-08-05T10:56:36.802253: step 9316, loss 0.615995.
Train: 2018-08-05T10:56:40.391795: step 9317, loss 0.589196.
Train: 2018-08-05T10:56:43.998383: step 9318, loss 0.606891.
Train: 2018-08-05T10:56:47.643073: step 9319, loss 0.52702.
Train: 2018-08-05T10:56:51.245650: step 9320, loss 0.474047.
Test: 2018-08-05T10:57:06.443052: step 9320, loss 0.548286.
Train: 2018-08-05T10:57:10.083731: step 9321, loss 0.580105.
Train: 2018-08-05T10:57:13.668260: step 9322, loss 0.553599.
Train: 2018-08-05T10:57:17.289888: step 9323, loss 0.562423.
Train: 2018-08-05T10:57:20.908508: step 9324, loss 0.535969.
Train: 2018-08-05T10:57:24.536152: step 9325, loss 0.553603.
Train: 2018-08-05T10:57:28.158783: step 9326, loss 0.553603.
Train: 2018-08-05T10:57:31.759355: step 9327, loss 0.553602.
Train: 2018-08-05T10:57:35.465207: step 9328, loss 0.465529.
Train: 2018-08-05T10:57:39.074803: step 9329, loss 0.641825.
Train: 2018-08-05T10:57:42.693423: step 9330, loss 0.65062.
Test: 2018-08-05T10:57:57.933940: step 9330, loss 0.547508.
Train: 2018-08-05T10:58:01.566597: step 9331, loss 0.571216.
Train: 2018-08-05T10:58:05.177195: step 9332, loss 0.518498.
Train: 2018-08-05T10:58:08.835922: step 9333, loss 0.518545.
Train: 2018-08-05T10:58:12.430478: step 9334, loss 0.632528.
Train: 2018-08-05T10:58:16.036064: step 9335, loss 0.588609.
Train: 2018-08-05T10:58:19.649670: step 9336, loss 0.614734.
Train: 2018-08-05T10:58:23.241218: step 9337, loss 0.605788.
Train: 2018-08-05T10:58:27.087444: step 9338, loss 0.596922.
Train: 2018-08-05T10:58:30.934671: step 9339, loss 0.519344.
Train: 2018-08-05T10:58:34.868128: step 9340, loss 0.579444.
Test: 2018-08-05T10:58:50.139728: step 9340, loss 0.547667.
Train: 2018-08-05T10:58:53.729270: step 9341, loss 0.485864.
Train: 2018-08-05T10:58:57.321821: step 9342, loss 0.648698.
Train: 2018-08-05T10:59:00.969518: step 9343, loss 0.570823.
Train: 2018-08-05T10:59:04.643285: step 9344, loss 0.485807.
Train: 2018-08-05T10:59:08.403780: step 9345, loss 0.553937.
Train: 2018-08-05T10:59:11.990315: step 9346, loss 0.621868.
Train: 2018-08-05T10:59:15.593895: step 9347, loss 0.536901.
Train: 2018-08-05T10:59:19.196472: step 9348, loss 0.587754.
Train: 2018-08-05T10:59:22.837151: step 9349, loss 0.520237.
Train: 2018-08-05T10:59:26.476827: step 9350, loss 0.58773.
Test: 2018-08-05T10:59:41.654176: step 9350, loss 0.548643.
Train: 2018-08-05T10:59:45.273799: step 9351, loss 0.511696.
Train: 2018-08-05T10:59:48.888408: step 9352, loss 0.638454.
Train: 2018-08-05T10:59:52.492991: step 9353, loss 0.579256.
Train: 2018-08-05T10:59:56.076517: step 9354, loss 0.486461.
Train: 2018-08-05T10:59:59.721207: step 9355, loss 0.553928.
Train: 2018-08-05T11:00:03.592499: step 9356, loss 0.494804.
Train: 2018-08-05T11:00:07.147951: step 9357, loss 0.553861.
Train: 2018-08-05T11:00:10.669312: step 9358, loss 0.655812.
Train: 2018-08-05T11:00:14.167612: step 9359, loss 0.621825.
Train: 2018-08-05T11:00:17.683960: step 9360, loss 0.562353.
Test: 2018-08-05T11:00:32.534440: step 9360, loss 0.547265.
Train: 2018-08-05T11:00:36.093903: step 9361, loss 0.553904.
Train: 2018-08-05T11:00:37.819490: step 9362, loss 0.652482.
Train: 2018-08-05T11:00:41.324809: step 9363, loss 0.638208.
Train: 2018-08-05T11:00:44.861211: step 9364, loss 0.528822.
Train: 2018-08-05T11:00:48.355500: step 9365, loss 0.595881.
Train: 2018-08-05T11:00:51.862825: step 9366, loss 0.479001.
Train: 2018-08-05T11:00:55.395215: step 9367, loss 0.587435.
Train: 2018-08-05T11:00:58.912566: step 9368, loss 0.620711.
Train: 2018-08-05T11:01:02.413874: step 9369, loss 0.554148.
Train: 2018-08-05T11:01:05.899139: step 9370, loss 0.496138.
Test: 2018-08-05T11:01:20.852894: step 9370, loss 0.549573.
Train: 2018-08-05T11:01:24.353199: step 9371, loss 0.5293.
Train: 2018-08-05T11:01:27.849494: step 9372, loss 0.529261.
Train: 2018-08-05T11:01:31.353810: step 9373, loss 0.61233.
Train: 2018-08-05T11:01:34.881188: step 9374, loss 0.554124.
Train: 2018-08-05T11:01:38.368459: step 9375, loss 0.579089.
Train: 2018-08-05T11:01:41.862748: step 9376, loss 0.545774.
Train: 2018-08-05T11:01:45.355032: step 9377, loss 0.579103.
Train: 2018-08-05T11:01:48.858346: step 9378, loss 0.595794.
Train: 2018-08-05T11:01:52.432849: step 9379, loss 0.554083.
Train: 2018-08-05T11:01:55.933154: step 9380, loss 0.562423.
Test: 2018-08-05T11:02:10.785639: step 9380, loss 0.548867.
Train: 2018-08-05T11:02:14.286947: step 9381, loss 0.570768.
Train: 2018-08-05T11:02:17.782239: step 9382, loss 0.579116.
Train: 2018-08-05T11:02:21.284550: step 9383, loss 0.512339.
Train: 2018-08-05T11:02:24.775832: step 9384, loss 0.554057.
Train: 2018-08-05T11:02:28.291177: step 9385, loss 0.520568.
Train: 2018-08-05T11:02:31.796496: step 9386, loss 0.604326.
Train: 2018-08-05T11:02:35.276748: step 9387, loss 0.545597.
Train: 2018-08-05T11:02:38.735944: step 9388, loss 0.562382.
Train: 2018-08-05T11:02:42.243269: step 9389, loss 0.495012.
Train: 2018-08-05T11:02:45.730539: step 9390, loss 0.562365.
Test: 2018-08-05T11:03:00.595057: step 9390, loss 0.548607.
Train: 2018-08-05T11:03:04.118423: step 9391, loss 0.55389.
Train: 2018-08-05T11:03:07.695934: step 9392, loss 0.647224.
Train: 2018-08-05T11:03:11.214287: step 9393, loss 0.553856.
Train: 2018-08-05T11:03:14.680502: step 9394, loss 0.57934.
Train: 2018-08-05T11:03:18.165768: step 9395, loss 0.604839.
Train: 2018-08-05T11:03:21.692143: step 9396, loss 0.587809.
Train: 2018-08-05T11:03:25.186433: step 9397, loss 0.460795.
Train: 2018-08-05T11:03:28.707794: step 9398, loss 0.553878.
Train: 2018-08-05T11:03:32.213112: step 9399, loss 0.53683.
Train: 2018-08-05T11:03:35.703391: step 9400, loss 0.502706.
Test: 2018-08-05T11:03:50.610020: step 9400, loss 0.548855.
Train: 2018-08-05T11:03:56.329225: step 9401, loss 0.613599.
Train: 2018-08-05T11:03:59.846575: step 9402, loss 0.579448.
Train: 2018-08-05T11:04:03.330838: step 9403, loss 0.519515.
Train: 2018-08-05T11:04:06.862226: step 9404, loss 0.562335.
Train: 2018-08-05T11:04:10.402639: step 9405, loss 0.536559.
Train: 2018-08-05T11:04:13.878880: step 9406, loss 0.553728.
Train: 2018-08-05T11:04:17.394226: step 9407, loss 0.553713.
Train: 2018-08-05T11:04:20.885507: step 9408, loss 0.545061.
Train: 2018-08-05T11:04:24.355732: step 9409, loss 0.631593.
Train: 2018-08-05T11:04:27.865062: step 9410, loss 0.510395.
Test: 2018-08-05T11:04:42.710528: step 9410, loss 0.54767.
Train: 2018-08-05T11:04:46.192786: step 9411, loss 0.59702.
Train: 2018-08-05T11:04:49.706126: step 9412, loss 0.58836.
Train: 2018-08-05T11:04:53.196405: step 9413, loss 0.536342.
Train: 2018-08-05T11:04:56.716764: step 9414, loss 0.536339.
Train: 2018-08-05T11:05:00.211054: step 9415, loss 0.623068.
Train: 2018-08-05T11:05:03.733417: step 9416, loss 0.579681.
Train: 2018-08-05T11:05:07.247760: step 9417, loss 0.553686.
Train: 2018-08-05T11:05:10.731021: step 9418, loss 0.510444.
Train: 2018-08-05T11:05:14.239347: step 9419, loss 0.579644.
Train: 2018-08-05T11:05:17.713584: step 9420, loss 0.57964.
Test: 2018-08-05T11:05:32.610186: step 9420, loss 0.548502.
Train: 2018-08-05T11:05:36.139569: step 9421, loss 0.545054.
Train: 2018-08-05T11:05:39.662935: step 9422, loss 0.596904.
Train: 2018-08-05T11:05:43.187305: step 9423, loss 0.519179.
Train: 2018-08-05T11:05:46.654522: step 9424, loss 0.57097.
Train: 2018-08-05T11:05:50.139788: step 9425, loss 0.55371.
Train: 2018-08-05T11:05:53.631069: step 9426, loss 0.639984.
Train: 2018-08-05T11:05:57.112324: step 9427, loss 0.519278.
Train: 2018-08-05T11:06:00.652736: step 9428, loss 0.5021.
Train: 2018-08-05T11:06:04.197159: step 9429, loss 0.476238.
Train: 2018-08-05T11:06:07.676408: step 9430, loss 0.519192.
Test: 2018-08-05T11:06:22.499816: step 9430, loss 0.548885.
Train: 2018-08-05T11:06:25.994106: step 9431, loss 0.605615.
Train: 2018-08-05T11:06:29.494411: step 9432, loss 0.553675.
Train: 2018-08-05T11:06:32.969650: step 9433, loss 0.527625.
Train: 2018-08-05T11:06:36.482990: step 9434, loss 0.614569.
Train: 2018-08-05T11:06:39.997333: step 9435, loss 0.527536.
Train: 2018-08-05T11:06:43.494630: step 9436, loss 0.553646.
Train: 2018-08-05T11:06:46.992931: step 9437, loss 0.571092.
Train: 2018-08-05T11:06:50.496244: step 9438, loss 0.553636.
Train: 2018-08-05T11:06:53.993542: step 9439, loss 0.597331.
Train: 2018-08-05T11:06:57.511895: step 9440, loss 0.509941.
Test: 2018-08-05T11:07:12.396466: step 9440, loss 0.548172.
Train: 2018-08-05T11:07:15.912814: step 9441, loss 0.55363.
Train: 2018-08-05T11:07:19.400084: step 9442, loss 0.571132.
Train: 2018-08-05T11:07:22.889361: step 9443, loss 0.536111.
Train: 2018-08-05T11:07:26.374626: step 9444, loss 0.571149.
Train: 2018-08-05T11:07:29.860894: step 9445, loss 0.501013.
Train: 2018-08-05T11:07:33.390277: step 9446, loss 0.650205.
Train: 2018-08-05T11:07:36.898604: step 9447, loss 0.536064.
Train: 2018-08-05T11:07:40.384872: step 9448, loss 0.553617.
Train: 2018-08-05T11:07:43.889188: step 9449, loss 0.536067.
Train: 2018-08-05T11:07:47.374454: step 9450, loss 0.562393.
Test: 2018-08-05T11:08:02.519717: step 9450, loss 0.548135.
Train: 2018-08-05T11:08:06.043084: step 9451, loss 0.606294.
Train: 2018-08-05T11:08:09.550408: step 9452, loss 0.553617.
Train: 2018-08-05T11:08:13.077786: step 9453, loss 0.588685.
Train: 2018-08-05T11:08:16.606166: step 9454, loss 0.597399.
Train: 2018-08-05T11:08:20.100455: step 9455, loss 0.623532.
Train: 2018-08-05T11:08:23.608782: step 9456, loss 0.632035.
Train: 2018-08-05T11:08:27.094048: step 9457, loss 0.51032.
Train: 2018-08-05T11:08:30.589340: step 9458, loss 0.570986.
Train: 2018-08-05T11:08:34.096664: step 9459, loss 0.493375.
Train: 2018-08-05T11:08:37.601983: step 9460, loss 0.536507.
Test: 2018-08-05T11:08:52.432409: step 9460, loss 0.547767.
Train: 2018-08-05T11:08:55.974826: step 9461, loss 0.562336.
Train: 2018-08-05T11:08:59.456081: step 9462, loss 0.579534.
Train: 2018-08-05T11:09:02.967416: step 9463, loss 0.570926.
Train: 2018-08-05T11:09:06.486772: step 9464, loss 0.648163.
Train: 2018-08-05T11:09:10.015152: step 9465, loss 0.53666.
Train: 2018-08-05T11:09:13.574615: step 9466, loss 0.528171.
Train: 2018-08-05T11:09:17.064894: step 9467, loss 0.613535.
Train: 2018-08-05T11:09:20.559184: step 9468, loss 0.579374.
Train: 2018-08-05T11:09:24.091574: step 9469, loss 0.562345.
Train: 2018-08-05T11:09:27.581853: step 9470, loss 0.528432.
Test: 2018-08-05T11:09:42.450381: step 9470, loss 0.548012.
Train: 2018-08-05T11:09:45.981769: step 9471, loss 0.596239.
Train: 2018-08-05T11:09:49.515162: step 9472, loss 0.553902.
Train: 2018-08-05T11:09:53.002433: step 9473, loss 0.596151.
Train: 2018-08-05T11:09:56.488701: step 9474, loss 0.545506.
Train: 2018-08-05T11:10:00.105316: step 9475, loss 0.537117.
Train: 2018-08-05T11:10:03.931488: step 9476, loss 0.486607.
Train: 2018-08-05T11:10:07.709532: step 9477, loss 0.570801.
Train: 2018-08-05T11:10:11.703460: step 9478, loss 0.562367.
Train: 2018-08-05T11:10:15.578762: step 9479, loss 0.545466.
Train: 2018-08-05T11:10:19.217436: step 9480, loss 0.579278.
Test: 2018-08-05T11:10:34.441910: step 9480, loss 0.547435.
Train: 2018-08-05T11:10:38.101639: step 9481, loss 0.553888.
Train: 2018-08-05T11:10:41.715246: step 9482, loss 0.562353.
Train: 2018-08-05T11:10:45.349908: step 9483, loss 0.60477.
Train: 2018-08-05T11:10:49.022672: step 9484, loss 0.56235.
Train: 2018-08-05T11:10:52.615223: step 9485, loss 0.536898.
Train: 2018-08-05T11:10:56.245875: step 9486, loss 0.587815.
Train: 2018-08-05T11:10:59.909615: step 9487, loss 0.579326.
Train: 2018-08-05T11:11:03.505174: step 9488, loss 0.596293.
Train: 2018-08-05T11:11:07.108754: step 9489, loss 0.528443.
Train: 2018-08-05T11:11:10.733390: step 9490, loss 0.621686.
Test: 2018-08-05T11:11:25.902717: step 9490, loss 0.547831.
Train: 2018-08-05T11:11:29.500281: step 9491, loss 0.604679.
Train: 2018-08-05T11:11:33.088821: step 9492, loss 0.545471.
Train: 2018-08-05T11:11:36.670343: step 9493, loss 0.58767.
Train: 2018-08-05T11:11:40.249859: step 9494, loss 0.46978.
Train: 2018-08-05T11:11:43.873492: step 9495, loss 0.528688.
Train: 2018-08-05T11:11:47.485094: step 9496, loss 0.638258.
Train: 2018-08-05T11:11:51.096695: step 9497, loss 0.553944.
Train: 2018-08-05T11:11:54.685235: step 9498, loss 0.545521.
Train: 2018-08-05T11:11:58.288815: step 9499, loss 0.520233.
Train: 2018-08-05T11:12:01.880363: step 9500, loss 0.587681.
Test: 2018-08-05T11:12:17.197082: step 9500, loss 0.548459.
Train: 2018-08-05T11:12:23.021567: step 9501, loss 0.621468.
Train: 2018-08-05T11:12:26.655227: step 9502, loss 0.537059.
Train: 2018-08-05T11:12:30.249783: step 9503, loss 0.579241.
Train: 2018-08-05T11:12:33.844339: step 9504, loss 0.655141.
Train: 2018-08-05T11:12:37.431876: step 9505, loss 0.604451.
Train: 2018-08-05T11:12:41.016406: step 9506, loss 0.587562.
Train: 2018-08-05T11:12:44.617981: step 9507, loss 0.579136.
Train: 2018-08-05T11:12:48.228579: step 9508, loss 0.487388.
Train: 2018-08-05T11:12:51.808095: step 9509, loss 0.537445.
Train: 2018-08-05T11:12:55.378587: step 9510, loss 0.512467.
Test: 2018-08-05T11:13:10.499787: step 9510, loss 0.547929.
Train: 2018-08-05T11:13:14.127431: step 9511, loss 0.570765.
Train: 2018-08-05T11:13:17.709955: step 9512, loss 0.537389.
Train: 2018-08-05T11:13:19.463617: step 9513, loss 0.41979.
Train: 2018-08-05T11:13:23.076221: step 9514, loss 0.562389.
Train: 2018-08-05T11:13:26.701859: step 9515, loss 0.587663.
Train: 2018-08-05T11:13:30.288394: step 9516, loss 0.5539.
Train: 2018-08-05T11:13:33.977201: step 9517, loss 0.596292.
Train: 2018-08-05T11:13:37.606850: step 9518, loss 0.553841.
Train: 2018-08-05T11:13:41.279614: step 9519, loss 0.545297.
Train: 2018-08-05T11:13:44.906256: step 9520, loss 0.613586.
Test: 2018-08-05T11:14:00.132735: step 9520, loss 0.548058.
Train: 2018-08-05T11:14:03.766395: step 9521, loss 0.519589.
Train: 2018-08-05T11:14:07.397047: step 9522, loss 0.58803.
Train: 2018-08-05T11:14:11.072819: step 9523, loss 0.579482.
Train: 2018-08-05T11:14:14.680410: step 9524, loss 0.536601.
Train: 2018-08-05T11:14:18.294016: step 9525, loss 0.485054.
Train: 2018-08-05T11:14:21.965778: step 9526, loss 0.493469.
Train: 2018-08-05T11:14:25.562339: step 9527, loss 0.57098.
Train: 2018-08-05T11:14:29.205023: step 9528, loss 0.623009.
Train: 2018-08-05T11:14:32.789552: step 9529, loss 0.492927.
Train: 2018-08-05T11:14:36.421207: step 9530, loss 0.510152.
Test: 2018-08-05T11:14:51.615601: step 9530, loss 0.547989.
Train: 2018-08-05T11:14:55.301478: step 9531, loss 0.614751.
Train: 2018-08-05T11:14:59.004405: step 9532, loss 0.579868.
Train: 2018-08-05T11:15:02.655206: step 9533, loss 0.553625.
Train: 2018-08-05T11:15:06.257381: step 9534, loss 0.55362.
Train: 2018-08-05T11:15:09.907657: step 9535, loss 0.500965.
Train: 2018-08-05T11:15:13.502276: step 9536, loss 0.588782.
Train: 2018-08-05T11:15:17.087893: step 9537, loss 0.544802.
Train: 2018-08-05T11:15:20.706604: step 9538, loss 0.562417.
Train: 2018-08-05T11:15:24.340335: step 9539, loss 0.52713.
Train: 2018-08-05T11:15:27.952523: step 9540, loss 0.571269.
Test: 2018-08-05T11:15:43.296525: step 9540, loss 0.547872.
Train: 2018-08-05T11:15:46.937792: step 9541, loss 0.527061.
Train: 2018-08-05T11:15:50.557478: step 9542, loss 0.70416.
Train: 2018-08-05T11:15:54.140575: step 9543, loss 0.58012.
Train: 2018-08-05T11:15:57.754271: step 9544, loss 0.597716.
Train: 2018-08-05T11:16:01.380480: step 9545, loss 0.597602.
Train: 2018-08-05T11:16:04.950545: step 9546, loss 0.483462.
Train: 2018-08-05T11:16:08.562740: step 9547, loss 0.518599.
Train: 2018-08-05T11:16:12.165386: step 9548, loss 0.623637.
Train: 2018-08-05T11:16:15.758515: step 9549, loss 0.518698.
Train: 2018-08-05T11:16:19.391248: step 9550, loss 0.640904.
Test: 2018-08-05T11:16:34.639000: step 9550, loss 0.548021.
Train: 2018-08-05T11:16:38.264748: step 9551, loss 0.553653.
Train: 2018-08-05T11:16:41.993738: step 9552, loss 0.510242.
Train: 2018-08-05T11:16:45.593393: step 9553, loss 0.5797.
Train: 2018-08-05T11:16:49.179002: step 9554, loss 0.562345.
Train: 2018-08-05T11:16:52.767631: step 9555, loss 0.510422.
Train: 2018-08-05T11:16:56.467550: step 9556, loss 0.562342.
Train: 2018-08-05T11:17:00.107324: step 9557, loss 0.53639.
Train: 2018-08-05T11:17:03.919561: step 9558, loss 0.545036.
Train: 2018-08-05T11:17:07.610472: step 9559, loss 0.622951.
Train: 2018-08-05T11:17:11.239216: step 9560, loss 0.562342.
Test: 2018-08-05T11:17:26.390233: step 9560, loss 0.547904.
Train: 2018-08-05T11:17:29.993376: step 9561, loss 0.519119.
Train: 2018-08-05T11:17:33.870253: step 9562, loss 0.501823.
Train: 2018-08-05T11:17:37.665935: step 9563, loss 0.59697.
Train: 2018-08-05T11:17:41.302668: step 9564, loss 0.545024.
Train: 2018-08-05T11:17:44.952925: step 9565, loss 0.623006.
Train: 2018-08-05T11:17:48.572122: step 9566, loss 0.536366.
Train: 2018-08-05T11:17:52.194832: step 9567, loss 0.536371.
Train: 2018-08-05T11:17:56.052184: step 9568, loss 0.519043.
Train: 2018-08-05T11:17:59.834855: step 9569, loss 0.640373.
Train: 2018-08-05T11:18:03.483128: step 9570, loss 0.579672.
Test: 2018-08-05T11:18:18.734846: step 9570, loss 0.547093.
Train: 2018-08-05T11:18:22.361559: step 9571, loss 0.59696.
Train: 2018-08-05T11:18:25.986763: step 9572, loss 0.605536.
Train: 2018-08-05T11:18:29.576386: step 9573, loss 0.527867.
Train: 2018-08-05T11:18:33.216652: step 9574, loss 0.536525.
Train: 2018-08-05T11:18:36.857418: step 9575, loss 0.459189.
Train: 2018-08-05T11:18:40.464080: step 9576, loss 0.476267.
Train: 2018-08-05T11:18:44.271826: step 9577, loss 0.562344.
Train: 2018-08-05T11:18:48.008365: step 9578, loss 0.570987.
Train: 2018-08-05T11:18:51.655656: step 9579, loss 0.579721.
Train: 2018-08-05T11:18:55.249293: step 9580, loss 0.588437.
Test: 2018-08-05T11:19:10.478988: step 9580, loss 0.547234.
Train: 2018-08-05T11:19:14.096689: step 9581, loss 0.631881.
Train: 2018-08-05T11:19:17.762527: step 9582, loss 0.562351.
Train: 2018-08-05T11:19:21.360186: step 9583, loss 0.605704.
Train: 2018-08-05T11:19:24.946792: step 9584, loss 0.545021.
Train: 2018-08-05T11:19:28.541438: step 9585, loss 0.484644.
Train: 2018-08-05T11:19:32.249382: step 9586, loss 0.467438.
Train: 2018-08-05T11:19:35.924257: step 9587, loss 0.692412.
Train: 2018-08-05T11:19:39.699873: step 9588, loss 0.553682.
Train: 2018-08-05T11:19:43.284459: step 9589, loss 0.493079.
Train: 2018-08-05T11:19:46.897650: step 9590, loss 0.545014.
Test: 2018-08-05T11:20:02.508854: step 9590, loss 0.547061.
Train: 2018-08-05T11:20:06.127557: step 9591, loss 0.640421.
Train: 2018-08-05T11:20:09.712169: step 9592, loss 0.527672.
Train: 2018-08-05T11:20:13.339869: step 9593, loss 0.562346.
Train: 2018-08-05T11:20:17.017741: step 9594, loss 0.605685.
Train: 2018-08-05T11:20:20.631428: step 9595, loss 0.58832.
Train: 2018-08-05T11:20:24.215031: step 9596, loss 0.54505.
Train: 2018-08-05T11:20:27.842758: step 9597, loss 0.527794.
Train: 2018-08-05T11:20:31.479473: step 9598, loss 0.596873.
Train: 2018-08-05T11:20:35.116202: step 9599, loss 0.562338.
Train: 2018-08-05T11:20:38.787071: step 9600, loss 0.49341.
Test: 2018-08-05T11:20:54.024241: step 9600, loss 0.547941.
Train: 2018-08-05T11:20:59.891953: step 9601, loss 0.596817.
Train: 2018-08-05T11:21:03.489589: step 9602, loss 0.553719.
Train: 2018-08-05T11:21:07.062648: step 9603, loss 0.553721.
Train: 2018-08-05T11:21:10.653277: step 9604, loss 0.588184.
Train: 2018-08-05T11:21:14.416369: step 9605, loss 0.605391.
Train: 2018-08-05T11:21:18.108307: step 9606, loss 0.527942.
Train: 2018-08-05T11:21:21.748582: step 9607, loss 0.536557.
Train: 2018-08-05T11:21:25.354756: step 9608, loss 0.527966.
Train: 2018-08-05T11:21:28.932848: step 9609, loss 0.613922.
Train: 2018-08-05T11:21:32.501429: step 9610, loss 0.527953.
Test: 2018-08-05T11:21:47.831350: step 9610, loss 0.547993.
Train: 2018-08-05T11:21:51.526250: step 9611, loss 0.588109.
Train: 2018-08-05T11:21:55.277320: step 9612, loss 0.493709.
Train: 2018-08-05T11:21:59.049405: step 9613, loss 0.579524.
Train: 2018-08-05T11:22:02.818999: step 9614, loss 0.553726.
Train: 2018-08-05T11:22:06.392063: step 9615, loss 0.519284.
Train: 2018-08-05T11:22:09.975695: step 9616, loss 0.476103.
Train: 2018-08-05T11:22:13.591861: step 9617, loss 0.692123.
Train: 2018-08-05T11:22:17.202525: step 9618, loss 0.527742.
Train: 2018-08-05T11:22:20.778621: step 9619, loss 0.536377.
Train: 2018-08-05T11:22:24.360750: step 9620, loss 0.579674.
Test: 2018-08-05T11:22:39.577412: step 9620, loss 0.547868.
Train: 2018-08-05T11:22:43.169543: step 9621, loss 0.61436.
Train: 2018-08-05T11:22:46.739621: step 9622, loss 0.622987.
Train: 2018-08-05T11:22:50.362817: step 9623, loss 0.553695.
Train: 2018-08-05T11:22:54.015603: step 9624, loss 0.536441.
Train: 2018-08-05T11:22:57.609255: step 9625, loss 0.553713.
Train: 2018-08-05T11:23:01.195870: step 9626, loss 0.579572.
Train: 2018-08-05T11:23:04.815065: step 9627, loss 0.519294.
Train: 2018-08-05T11:23:08.411687: step 9628, loss 0.596766.
Train: 2018-08-05T11:23:12.053422: step 9629, loss 0.553735.
Train: 2018-08-05T11:23:15.676604: step 9630, loss 0.605309.
Test: 2018-08-05T11:23:30.885249: step 9630, loss 0.549385.
Train: 2018-08-05T11:23:34.518503: step 9631, loss 0.553753.
Train: 2018-08-05T11:23:38.116647: step 9632, loss 0.58805.
Train: 2018-08-05T11:23:41.813527: step 9633, loss 0.536663.
Train: 2018-08-05T11:23:45.391112: step 9634, loss 0.605084.
Train: 2018-08-05T11:23:49.011838: step 9635, loss 0.605011.
Train: 2018-08-05T11:23:52.635040: step 9636, loss 0.502712.
Train: 2018-08-05T11:23:56.335468: step 9637, loss 0.613584.
Train: 2018-08-05T11:24:00.074479: step 9638, loss 0.553827.
Train: 2018-08-05T11:24:03.763359: step 9639, loss 0.579429.
Train: 2018-08-05T11:24:07.363505: step 9640, loss 0.52837.
Test: 2018-08-05T11:24:22.627831: step 9640, loss 0.548604.
Train: 2018-08-05T11:24:26.209451: step 9641, loss 0.579242.
Train: 2018-08-05T11:24:29.835196: step 9642, loss 0.587892.
Train: 2018-08-05T11:24:33.456402: step 9643, loss 0.528605.
Train: 2018-08-05T11:24:37.035976: step 9644, loss 0.562315.
Train: 2018-08-05T11:24:40.626610: step 9645, loss 0.553954.
Train: 2018-08-05T11:24:44.218229: step 9646, loss 0.587662.
Train: 2018-08-05T11:24:47.814868: step 9647, loss 0.520236.
Train: 2018-08-05T11:24:51.414523: step 9648, loss 0.570778.
Train: 2018-08-05T11:24:55.018215: step 9649, loss 0.579188.
Train: 2018-08-05T11:24:58.621878: step 9650, loss 0.621502.
Test: 2018-08-05T11:25:13.816485: step 9650, loss 0.547918.
Train: 2018-08-05T11:25:17.408122: step 9651, loss 0.57919.
Train: 2018-08-05T11:25:21.019806: step 9652, loss 0.520301.
Train: 2018-08-05T11:25:24.601402: step 9653, loss 0.587537.
Train: 2018-08-05T11:25:28.254191: step 9654, loss 0.612774.
Train: 2018-08-05T11:25:31.887943: step 9655, loss 0.512082.
Train: 2018-08-05T11:25:35.496630: step 9656, loss 0.528807.
Train: 2018-08-05T11:25:39.080235: step 9657, loss 0.486517.
Train: 2018-08-05T11:25:42.715989: step 9658, loss 0.595758.
Train: 2018-08-05T11:25:46.324682: step 9659, loss 0.537118.
Train: 2018-08-05T11:25:49.962421: step 9660, loss 0.57857.
Test: 2018-08-05T11:26:05.105337: step 9660, loss 0.547933.
Train: 2018-08-05T11:26:08.924088: step 9661, loss 0.510226.
Train: 2018-08-05T11:26:12.556833: step 9662, loss 0.580209.
Train: 2018-08-05T11:26:16.141432: step 9663, loss 0.589064.
Train: 2018-08-05T11:26:17.917192: step 9664, loss 0.622293.
Train: 2018-08-05T11:26:21.509828: step 9665, loss 0.553428.
Train: 2018-08-05T11:26:25.295995: step 9666, loss 0.534673.
Train: 2018-08-05T11:26:28.964834: step 9667, loss 0.572813.
Train: 2018-08-05T11:26:32.618624: step 9668, loss 0.553856.
Train: 2018-08-05T11:26:36.211254: step 9669, loss 0.484319.
Train: 2018-08-05T11:26:39.999406: step 9670, loss 0.546085.
Test: 2018-08-05T11:26:55.282279: step 9670, loss 0.548711.
Train: 2018-08-05T11:26:58.925030: step 9671, loss 0.48474.
Train: 2018-08-05T11:27:02.549730: step 9672, loss 0.570283.
Train: 2018-08-05T11:27:06.174448: step 9673, loss 0.571659.
Train: 2018-08-05T11:27:09.759551: step 9674, loss 0.552216.
Train: 2018-08-05T11:27:13.365197: step 9675, loss 0.54585.
Train: 2018-08-05T11:27:16.970350: step 9676, loss 0.579164.
Train: 2018-08-05T11:27:20.550445: step 9677, loss 0.546645.
Train: 2018-08-05T11:27:24.188692: step 9678, loss 0.561126.
Train: 2018-08-05T11:27:27.780323: step 9679, loss 0.49161.
Train: 2018-08-05T11:27:31.406033: step 9680, loss 0.587642.
Test: 2018-08-05T11:27:46.572595: step 9680, loss 0.548659.
Train: 2018-08-05T11:27:50.154197: step 9681, loss 0.547107.
Train: 2018-08-05T11:27:53.742308: step 9682, loss 0.517605.
Train: 2018-08-05T11:27:57.306849: step 9683, loss 0.537507.
Train: 2018-08-05T11:28:00.986201: step 9684, loss 0.509804.
Train: 2018-08-05T11:28:04.616956: step 9685, loss 0.554209.
Train: 2018-08-05T11:28:08.319401: step 9686, loss 0.582497.
Train: 2018-08-05T11:28:11.914570: step 9687, loss 0.616716.
Train: 2018-08-05T11:28:15.515274: step 9688, loss 0.563185.
Train: 2018-08-05T11:28:19.143992: step 9689, loss 0.580865.
Train: 2018-08-05T11:28:22.788743: step 9690, loss 0.562917.
Test: 2018-08-05T11:28:37.973309: step 9690, loss 0.547341.
Train: 2018-08-05T11:28:41.676234: step 9691, loss 0.545156.
Train: 2018-08-05T11:28:45.270390: step 9692, loss 0.562316.
Train: 2018-08-05T11:28:48.979851: step 9693, loss 0.536092.
Train: 2018-08-05T11:28:52.603071: step 9694, loss 0.536235.
Train: 2018-08-05T11:28:56.221271: step 9695, loss 0.553654.
Train: 2018-08-05T11:28:59.822921: step 9696, loss 0.614635.
Train: 2018-08-05T11:29:03.421558: step 9697, loss 0.57977.
Train: 2018-08-05T11:29:07.015681: step 9698, loss 0.57105.
Train: 2018-08-05T11:29:10.607803: step 9699, loss 0.571036.
Train: 2018-08-05T11:29:14.186401: step 9700, loss 0.527658.
Test: 2018-08-05T11:29:29.410563: step 9700, loss 0.547672.
Train: 2018-08-05T11:29:35.228121: step 9701, loss 0.562346.
Train: 2018-08-05T11:29:38.959629: step 9702, loss 0.614312.
Train: 2018-08-05T11:29:42.561319: step 9703, loss 0.527756.
Train: 2018-08-05T11:29:46.186030: step 9704, loss 0.536424.
Train: 2018-08-05T11:29:49.778660: step 9705, loss 0.579611.
Train: 2018-08-05T11:29:53.392332: step 9706, loss 0.579597.
Train: 2018-08-05T11:29:56.982946: step 9707, loss 0.570957.
Train: 2018-08-05T11:30:00.653851: step 9708, loss 0.553726.
Train: 2018-08-05T11:30:04.416956: step 9709, loss 0.545131.
Train: 2018-08-05T11:30:08.013082: step 9710, loss 0.553737.
Test: 2018-08-05T11:30:23.293915: step 9710, loss 0.548178.
Train: 2018-08-05T11:30:26.974271: step 9711, loss 0.510763.
Train: 2018-08-05T11:30:30.699273: step 9712, loss 0.57954.
Train: 2018-08-05T11:30:34.292916: step 9713, loss 0.536521.
Train: 2018-08-05T11:30:37.882044: step 9714, loss 0.545113.
Train: 2018-08-05T11:30:41.528305: step 9715, loss 0.519235.
Train: 2018-08-05T11:30:45.184098: step 9716, loss 0.588247.
Train: 2018-08-05T11:30:48.925641: step 9717, loss 0.570985.
Train: 2018-08-05T11:30:52.537811: step 9718, loss 0.52774.
Train: 2018-08-05T11:30:56.149500: step 9719, loss 0.527699.
Train: 2018-08-05T11:30:59.739125: step 9720, loss 0.544995.
Test: 2018-08-05T11:31:15.025851: step 9720, loss 0.547435.
Train: 2018-08-05T11:31:18.709251: step 9721, loss 0.55366.
Train: 2018-08-05T11:31:22.359544: step 9722, loss 0.510106.
Train: 2018-08-05T11:31:25.966225: step 9723, loss 0.536174.
Train: 2018-08-05T11:31:29.556842: step 9724, loss 0.614916.
Train: 2018-08-05T11:31:33.154477: step 9725, loss 0.614989.
Train: 2018-08-05T11:31:36.744147: step 9726, loss 0.57992.
Train: 2018-08-05T11:31:40.379872: step 9727, loss 0.536098.
Train: 2018-08-05T11:31:43.984046: step 9728, loss 0.59743.
Train: 2018-08-05T11:31:47.588706: step 9729, loss 0.553626.
Train: 2018-08-05T11:31:51.176837: step 9730, loss 0.518641.
Test: 2018-08-05T11:32:06.604513: step 9730, loss 0.54817.
Train: 2018-08-05T11:32:10.399690: step 9731, loss 0.492392.
Train: 2018-08-05T11:32:14.016370: step 9732, loss 0.553623.
Train: 2018-08-05T11:32:17.631555: step 9733, loss 0.579934.
Train: 2018-08-05T11:32:21.305897: step 9734, loss 0.676514.
Train: 2018-08-05T11:32:24.889006: step 9735, loss 0.597434.
Train: 2018-08-05T11:32:28.466089: step 9736, loss 0.579854.
Train: 2018-08-05T11:32:32.103329: step 9737, loss 0.518778.
Train: 2018-08-05T11:32:35.691953: step 9738, loss 0.588465.
Train: 2018-08-05T11:32:39.315674: step 9739, loss 0.562351.
Train: 2018-08-05T11:32:43.129894: step 9740, loss 0.579679.
Test: 2018-08-05T11:32:58.440277: step 9740, loss 0.547899.
Train: 2018-08-05T11:33:02.064484: step 9741, loss 0.579636.
Train: 2018-08-05T11:33:05.681178: step 9742, loss 0.51058.
Train: 2018-08-05T11:33:09.267770: step 9743, loss 0.502022.
Train: 2018-08-05T11:33:12.840866: step 9744, loss 0.545101.
Train: 2018-08-05T11:33:16.451028: step 9745, loss 0.553715.
Train: 2018-08-05T11:33:20.100316: step 9746, loss 0.510575.
Train: 2018-08-05T11:33:23.694453: step 9747, loss 0.553699.
Train: 2018-08-05T11:33:27.292099: step 9748, loss 0.588304.
Train: 2018-08-05T11:33:30.877730: step 9749, loss 0.536363.
Train: 2018-08-05T11:33:34.520485: step 9750, loss 0.553676.
Test: 2018-08-05T11:33:49.792376: step 9750, loss 0.547253.
Train: 2018-08-05T11:33:53.442173: step 9751, loss 0.597072.
Train: 2018-08-05T11:33:57.096976: step 9752, loss 0.579715.
Train: 2018-08-05T11:34:00.693610: step 9753, loss 0.492905.
Train: 2018-08-05T11:34:04.273200: step 9754, loss 0.579734.
Train: 2018-08-05T11:34:07.880373: step 9755, loss 0.51018.
Train: 2018-08-05T11:34:11.531649: step 9756, loss 0.588487.
Train: 2018-08-05T11:34:15.141331: step 9757, loss 0.544931.
Train: 2018-08-05T11:34:18.777571: step 9758, loss 0.562365.
Train: 2018-08-05T11:34:22.360654: step 9759, loss 0.562368.
Train: 2018-08-05T11:34:25.994387: step 9760, loss 0.431348.
Test: 2018-08-05T11:34:41.334230: step 9760, loss 0.548356.
Train: 2018-08-05T11:34:44.920350: step 9761, loss 0.483528.
Train: 2018-08-05T11:34:48.542572: step 9762, loss 0.685595.
Train: 2018-08-05T11:34:52.133675: step 9763, loss 0.491938.
Train: 2018-08-05T11:34:55.727786: step 9764, loss 0.580088.
Train: 2018-08-05T11:34:59.338936: step 9765, loss 0.650876.
Train: 2018-08-05T11:35:02.945602: step 9766, loss 0.51824.
Train: 2018-08-05T11:35:06.544271: step 9767, loss 0.58012.
Train: 2018-08-05T11:35:10.133903: step 9768, loss 0.580112.
Train: 2018-08-05T11:35:13.738097: step 9769, loss 0.562429.
Train: 2018-08-05T11:35:17.348265: step 9770, loss 0.562424.
Test: 2018-08-05T11:35:32.792512: step 9770, loss 0.547898.
Train: 2018-08-05T11:35:36.400648: step 9771, loss 0.518339.
Train: 2018-08-05T11:35:40.000289: step 9772, loss 0.624121.
Train: 2018-08-05T11:35:43.598917: step 9773, loss 0.509598.
Train: 2018-08-05T11:35:47.207078: step 9774, loss 0.527213.
Train: 2018-08-05T11:35:50.833814: step 9775, loss 0.571207.
Train: 2018-08-05T11:35:54.450492: step 9776, loss 0.5976.
Train: 2018-08-05T11:35:58.095286: step 9777, loss 0.57998.
Train: 2018-08-05T11:36:01.705975: step 9778, loss 0.553615.
Train: 2018-08-05T11:36:05.308632: step 9779, loss 0.544853.
Train: 2018-08-05T11:36:08.916300: step 9780, loss 0.579901.
Test: 2018-08-05T11:36:24.707429: step 9780, loss 0.547766.
Train: 2018-08-05T11:36:28.337162: step 9781, loss 0.501138.
Train: 2018-08-05T11:36:31.974918: step 9782, loss 0.623617.
Train: 2018-08-05T11:36:35.608166: step 9783, loss 0.588581.
Train: 2018-08-05T11:36:39.198804: step 9784, loss 0.510042.
Train: 2018-08-05T11:36:42.798962: step 9785, loss 0.605932.
Train: 2018-08-05T11:36:46.384582: step 9786, loss 0.518857.
Train: 2018-08-05T11:36:49.961675: step 9787, loss 0.614518.
Train: 2018-08-05T11:36:53.652107: step 9788, loss 0.484241.
Train: 2018-08-05T11:36:57.684951: step 9789, loss 0.631775.
Train: 2018-08-05T11:37:01.519744: step 9790, loss 0.57101.
Test: 2018-08-05T11:37:16.712845: step 9790, loss 0.5469.
Train: 2018-08-05T11:37:20.333076: step 9791, loss 0.562342.
Train: 2018-08-05T11:37:23.933726: step 9792, loss 0.596887.
Train: 2018-08-05T11:37:27.557433: step 9793, loss 0.562337.
Train: 2018-08-05T11:37:31.208197: step 9794, loss 0.65694.
Train: 2018-08-05T11:37:34.877048: step 9795, loss 0.570903.
Train: 2018-08-05T11:37:38.594016: step 9796, loss 0.622089.
Train: 2018-08-05T11:37:42.209727: step 9797, loss 0.596332.
Train: 2018-08-05T11:37:45.818390: step 9798, loss 0.553905.
Train: 2018-08-05T11:37:49.405530: step 9799, loss 0.629733.
Train: 2018-08-05T11:37:53.064866: step 9800, loss 0.64616.
Test: 2018-08-05T11:38:08.367211: step 9800, loss 0.547585.
Train: 2018-08-05T11:38:14.101048: step 9801, loss 0.545797.
Train: 2018-08-05T11:38:17.715747: step 9802, loss 0.570757.
Train: 2018-08-05T11:38:21.368543: step 9803, loss 0.546047.
Train: 2018-08-05T11:38:24.981245: step 9804, loss 0.587171.
Train: 2018-08-05T11:38:28.574876: step 9805, loss 0.529902.
Train: 2018-08-05T11:38:32.202626: step 9806, loss 0.58708.
Train: 2018-08-05T11:38:35.814813: step 9807, loss 0.595177.
Train: 2018-08-05T11:38:39.407969: step 9808, loss 0.595117.
Train: 2018-08-05T11:38:43.158028: step 9809, loss 0.6678.
Train: 2018-08-05T11:38:46.806317: step 9810, loss 0.554747.
Test: 2018-08-05T11:39:02.030980: step 9810, loss 0.551314.
Train: 2018-08-05T11:39:05.671745: step 9811, loss 0.578865.
Train: 2018-08-05T11:39:09.328056: step 9812, loss 0.538963.
Train: 2018-08-05T11:39:12.968812: step 9813, loss 0.547019.
Train: 2018-08-05T11:39:16.675808: step 9814, loss 0.594755.
Train: 2018-08-05T11:39:18.629067: step 9815, loss 0.546062.
Train: 2018-08-05T11:39:22.422249: step 9816, loss 0.610573.
Train: 2018-08-05T11:39:26.077060: step 9817, loss 0.555109.
Train: 2018-08-05T11:39:29.747929: step 9818, loss 0.547216.
Train: 2018-08-05T11:39:33.376657: step 9819, loss 0.642159.
Train: 2018-08-05T11:39:36.972328: step 9820, loss 0.57886.
Test: 2018-08-05T11:39:52.962540: step 9820, loss 0.549726.
Train: 2018-08-05T11:39:56.558167: step 9821, loss 0.578866.
Train: 2018-08-05T11:40:00.173860: step 9822, loss 0.602505.
Train: 2018-08-05T11:40:03.885293: step 9823, loss 0.539537.
Train: 2018-08-05T11:40:07.497972: step 9824, loss 0.555282.
Train: 2018-08-05T11:40:11.094118: step 9825, loss 0.547414.
Train: 2018-08-05T11:40:14.734887: step 9826, loss 0.555252.
Train: 2018-08-05T11:40:18.382665: step 9827, loss 0.531563.
Train: 2018-08-05T11:40:22.004856: step 9828, loss 0.539342.
Train: 2018-08-05T11:40:25.632585: step 9829, loss 0.626438.
Train: 2018-08-05T11:40:29.226224: step 9830, loss 0.578858.
Test: 2018-08-05T11:40:44.430857: step 9830, loss 0.549293.
Train: 2018-08-05T11:40:48.068613: step 9831, loss 0.578859.
Train: 2018-08-05T11:40:51.695357: step 9832, loss 0.570892.
Train: 2018-08-05T11:40:55.323105: step 9833, loss 0.586838.
Train: 2018-08-05T11:40:58.910701: step 9834, loss 0.586847.
Train: 2018-08-05T11:41:02.525380: step 9835, loss 0.56288.
Train: 2018-08-05T11:41:06.185236: step 9836, loss 0.522874.
Train: 2018-08-05T11:41:09.813943: step 9837, loss 0.634976.
Train: 2018-08-05T11:41:13.440137: step 9838, loss 0.53877.
Train: 2018-08-05T11:41:17.036288: step 9839, loss 0.56281.
Train: 2018-08-05T11:41:20.714652: step 9840, loss 0.570832.
Test: 2018-08-05T11:41:36.108281: step 9840, loss 0.548391.
Train: 2018-08-05T11:41:39.756077: step 9841, loss 0.522497.
Train: 2018-08-05T11:41:43.376766: step 9842, loss 0.538509.
Train: 2018-08-05T11:41:47.056646: step 9843, loss 0.643706.
Train: 2018-08-05T11:41:50.727985: step 9844, loss 0.570792.
Train: 2018-08-05T11:41:54.324622: step 9845, loss 0.570787.
Train: 2018-08-05T11:41:57.923788: step 9846, loss 0.513882.
Train: 2018-08-05T11:42:01.511908: step 9847, loss 0.603369.
Train: 2018-08-05T11:42:05.099513: step 9848, loss 0.587091.
Train: 2018-08-05T11:42:08.720694: step 9849, loss 0.587107.
Train: 2018-08-05T11:42:12.357448: step 9850, loss 0.480893.
Test: 2018-08-05T11:42:27.789141: step 9850, loss 0.548514.
Train: 2018-08-05T11:42:31.290518: step 9851, loss 0.578954.
Train: 2018-08-05T11:42:34.823987: step 9852, loss 0.505086.
Train: 2018-08-05T11:42:38.321360: step 9853, loss 0.554282.
Train: 2018-08-05T11:42:41.840788: step 9854, loss 0.537691.
Train: 2018-08-05T11:42:45.379275: step 9855, loss 0.56246.
Train: 2018-08-05T11:42:48.904221: step 9856, loss 0.595746.
Train: 2018-08-05T11:42:52.397065: step 9857, loss 0.579117.
Train: 2018-08-05T11:42:55.978155: step 9858, loss 0.562407.
Train: 2018-08-05T11:42:59.460969: step 9859, loss 0.604309.
Train: 2018-08-05T11:43:02.967353: step 9860, loss 0.629512.
Test: 2018-08-05T11:43:17.948902: step 9860, loss 0.547041.
Train: 2018-08-05T11:43:21.583134: step 9861, loss 0.512086.
Train: 2018-08-05T11:43:25.140671: step 9862, loss 0.579173.
Train: 2018-08-05T11:43:28.644546: step 9863, loss 0.595963.
Train: 2018-08-05T11:43:32.159452: step 9864, loss 0.562392.
Train: 2018-08-05T11:43:35.702957: step 9865, loss 0.520455.
Train: 2018-08-05T11:43:39.193299: step 9866, loss 0.579179.
Train: 2018-08-05T11:43:42.739815: step 9867, loss 0.503602.
Train: 2018-08-05T11:43:46.256760: step 9868, loss 0.545554.
Train: 2018-08-05T11:43:49.827835: step 9869, loss 0.562371.
Train: 2018-08-05T11:43:53.316696: step 9870, loss 0.562364.
Test: 2018-08-05T11:44:08.241463: step 9870, loss 0.547644.
Train: 2018-08-05T11:44:11.772415: step 9871, loss 0.562358.
Train: 2018-08-05T11:44:15.297851: step 9872, loss 0.553879.
Train: 2018-08-05T11:44:18.857395: step 9873, loss 0.477462.
Train: 2018-08-05T11:44:22.441006: step 9874, loss 0.485685.
Train: 2018-08-05T11:44:25.937374: step 9875, loss 0.673584.
Train: 2018-08-05T11:44:29.442265: step 9876, loss 0.493751.
Train: 2018-08-05T11:44:32.917076: step 9877, loss 0.562335.
Train: 2018-08-05T11:44:36.442514: step 9878, loss 0.510601.
Train: 2018-08-05T11:44:39.974979: step 9879, loss 0.501771.
Train: 2018-08-05T11:44:43.486366: step 9880, loss 0.562361.
Test: 2018-08-05T11:44:58.457336: step 9880, loss 0.548391.
Train: 2018-08-05T11:45:01.975265: step 9881, loss 0.501244.
Train: 2018-08-05T11:45:05.478160: step 9882, loss 0.492166.
Train: 2018-08-05T11:45:08.981037: step 9883, loss 0.624835.
Train: 2018-08-05T11:45:12.505475: step 9884, loss 0.456226.
Train: 2018-08-05T11:45:16.010870: step 9885, loss 0.589233.
Train: 2018-08-05T11:45:19.539854: step 9886, loss 0.553602.
Train: 2018-08-05T11:45:23.097382: step 9887, loss 0.544616.
Train: 2018-08-05T11:45:26.632855: step 9888, loss 0.437087.
Train: 2018-08-05T11:45:30.164808: step 9889, loss 0.6257.
Train: 2018-08-05T11:45:33.640608: step 9890, loss 0.544552.
Test: 2018-08-05T11:45:48.503220: step 9890, loss 0.546923.
Train: 2018-08-05T11:45:51.990054: step 9891, loss 0.562703.
Train: 2018-08-05T11:45:55.514509: step 9892, loss 0.608157.
Train: 2018-08-05T11:45:59.080064: step 9893, loss 0.526336.
Train: 2018-08-05T11:46:02.580446: step 9894, loss 0.571959.
Train: 2018-08-05T11:46:06.089842: step 9895, loss 0.535477.
Train: 2018-08-05T11:46:09.581199: step 9896, loss 0.553654.
Train: 2018-08-05T11:46:13.107644: step 9897, loss 0.553654.
Train: 2018-08-05T11:46:16.673191: step 9898, loss 0.535416.
Train: 2018-08-05T11:46:20.179589: step 9899, loss 0.535391.
Train: 2018-08-05T11:46:23.712053: step 9900, loss 0.562808.
Test: 2018-08-05T11:46:38.762143: step 9900, loss 0.546279.
Train: 2018-08-05T11:46:44.561665: step 9901, loss 0.608526.
Train: 2018-08-05T11:46:48.084110: step 9902, loss 0.571932.
Train: 2018-08-05T11:46:51.577467: step 9903, loss 0.526286.
Train: 2018-08-05T11:46:55.107927: step 9904, loss 0.562759.
Train: 2018-08-05T11:46:58.610318: step 9905, loss 0.571851.
Train: 2018-08-05T11:47:02.146785: step 9906, loss 0.526344.
Train: 2018-08-05T11:47:05.623095: step 9907, loss 0.562738.
Train: 2018-08-05T11:47:09.124494: step 9908, loss 0.526264.
Train: 2018-08-05T11:47:12.591265: step 9909, loss 0.517488.
Train: 2018-08-05T11:47:16.108169: step 9910, loss 0.498903.
Test: 2018-08-05T11:47:31.005466: step 9910, loss 0.546528.
Train: 2018-08-05T11:47:34.499819: step 9911, loss 0.562982.
Train: 2018-08-05T11:47:37.994692: step 9912, loss 0.626476.
Train: 2018-08-05T11:47:41.481034: step 9913, loss 0.562737.
Train: 2018-08-05T11:47:44.994940: step 9914, loss 0.535448.
Train: 2018-08-05T11:47:48.514366: step 9915, loss 0.544597.
Train: 2018-08-05T11:47:52.044328: step 9916, loss 0.5449.
Train: 2018-08-05T11:47:55.541694: step 9917, loss 0.598823.
Train: 2018-08-05T11:47:59.043066: step 9918, loss 0.607744.
Train: 2018-08-05T11:48:02.543467: step 9919, loss 0.598837.
Train: 2018-08-05T11:48:06.087956: step 9920, loss 0.499556.
Test: 2018-08-05T11:48:21.004282: step 9920, loss 0.548407.
Train: 2018-08-05T11:48:24.517193: step 9921, loss 0.580626.
Train: 2018-08-05T11:48:28.079733: step 9922, loss 0.508797.
Train: 2018-08-05T11:48:31.581117: step 9923, loss 0.544634.
Train: 2018-08-05T11:48:35.064452: step 9924, loss 0.607284.
Train: 2018-08-05T11:48:38.560832: step 9925, loss 0.589325.
Train: 2018-08-05T11:48:42.073240: step 9926, loss 0.589241.
Train: 2018-08-05T11:48:45.538534: step 9927, loss 0.598025.
Train: 2018-08-05T11:48:49.078520: step 9928, loss 0.597872.
Train: 2018-08-05T11:48:52.588933: step 9929, loss 0.518321.
Train: 2018-08-05T11:48:56.082286: step 9930, loss 0.536021.
Test: 2018-08-05T11:49:10.975580: step 9930, loss 0.549555.
Train: 2018-08-05T11:49:14.516061: step 9931, loss 0.51852.
Train: 2018-08-05T11:49:18.074092: step 9932, loss 0.527332.
Train: 2018-08-05T11:49:21.613563: step 9933, loss 0.623689.
Train: 2018-08-05T11:49:25.152045: step 9934, loss 0.588599.
Train: 2018-08-05T11:49:28.691520: step 9935, loss 0.553643.
Train: 2018-08-05T11:49:32.185845: step 9936, loss 0.527543.
Train: 2018-08-05T11:49:35.701261: step 9937, loss 0.736215.
Train: 2018-08-05T11:49:39.186592: step 9938, loss 0.55369.
Train: 2018-08-05T11:49:42.704519: step 9939, loss 0.579568.
Train: 2018-08-05T11:49:46.223946: step 9940, loss 0.570914.
Test: 2018-08-05T11:50:01.137277: step 9940, loss 0.547869.
Train: 2018-08-05T11:50:04.650683: step 9941, loss 0.536702.
Train: 2018-08-05T11:50:08.150552: step 9942, loss 0.57086.
Train: 2018-08-05T11:50:11.656432: step 9943, loss 0.681263.
Train: 2018-08-05T11:50:15.159829: step 9944, loss 0.570812.
Train: 2018-08-05T11:50:18.651184: step 9945, loss 0.545563.
Train: 2018-08-05T11:50:22.158083: step 9946, loss 0.595905.
Train: 2018-08-05T11:50:25.772263: step 9947, loss 0.562425.
Train: 2018-08-05T11:50:29.252582: step 9948, loss 0.57076.
Train: 2018-08-05T11:50:32.743462: step 9949, loss 0.537629.
Train: 2018-08-05T11:50:36.258361: step 9950, loss 0.554231.
Test: 2018-08-05T11:50:51.158640: step 9950, loss 0.548733.
Train: 2018-08-05T11:50:54.669047: step 9951, loss 0.546012.
Train: 2018-08-05T11:50:58.198003: step 9952, loss 0.562517.
Train: 2018-08-05T11:51:01.717941: step 9953, loss 0.546057.
Train: 2018-08-05T11:51:05.226837: step 9954, loss 0.562525.
Train: 2018-08-05T11:51:08.725211: step 9955, loss 0.620155.
Train: 2018-08-05T11:51:12.218576: step 9956, loss 0.504967.
Train: 2018-08-05T11:51:15.698918: step 9957, loss 0.587215.
Train: 2018-08-05T11:51:19.235393: step 9958, loss 0.628362.
Train: 2018-08-05T11:51:22.738792: step 9959, loss 0.554321.
Train: 2018-08-05T11:51:26.261222: step 9960, loss 0.562547.
Test: 2018-08-05T11:51:41.157996: step 9960, loss 0.54922.
Train: 2018-08-05T11:51:44.668908: step 9961, loss 0.603596.
Train: 2018-08-05T11:51:48.153247: step 9962, loss 0.537965.
Train: 2018-08-05T11:51:51.653119: step 9963, loss 0.488795.
Train: 2018-08-05T11:51:55.155014: step 9964, loss 0.587181.
Train: 2018-08-05T11:51:58.686474: step 9965, loss 0.521442.
Train: 2018-08-05T11:52:00.407104: step 9966, loss 0.56252.
Train: 2018-08-05T11:52:03.895457: step 9967, loss 0.579011.
Train: 2018-08-05T11:52:07.379289: step 9968, loss 0.661705.
Train: 2018-08-05T11:52:10.860610: step 9969, loss 0.570756.
Train: 2018-08-05T11:52:14.359484: step 9970, loss 0.570756.
Test: 2018-08-05T11:52:29.281272: step 9970, loss 0.548718.
Train: 2018-08-05T11:52:32.802717: step 9971, loss 0.653296.
Train: 2018-08-05T11:52:36.313131: step 9972, loss 0.570757.
Train: 2018-08-05T11:52:39.796456: step 9973, loss 0.521462.
Train: 2018-08-05T11:52:43.280774: step 9974, loss 0.537922.
Train: 2018-08-05T11:52:46.787161: step 9975, loss 0.58718.
Train: 2018-08-05T11:52:50.266500: step 9976, loss 0.587174.
Train: 2018-08-05T11:52:53.790940: step 9977, loss 0.496951.
Train: 2018-08-05T11:52:57.307861: step 9978, loss 0.55434.
Train: 2018-08-05T11:53:00.828807: step 9979, loss 0.504987.
Train: 2018-08-05T11:53:04.314138: step 9980, loss 0.529536.
Test: 2018-08-05T11:53:19.231046: step 9980, loss 0.549621.
Train: 2018-08-05T11:53:22.740441: step 9981, loss 0.521123.
Train: 2018-08-05T11:53:26.233290: step 9982, loss 0.504308.
Train: 2018-08-05T11:53:29.727143: step 9983, loss 0.537371.
Train: 2018-08-05T11:53:33.259110: step 9984, loss 0.621145.
Train: 2018-08-05T11:53:36.736927: step 9985, loss 0.60449.
Train: 2018-08-05T11:53:40.249343: step 9986, loss 0.587693.
Train: 2018-08-05T11:53:43.721640: step 9987, loss 0.56236.
Train: 2018-08-05T11:53:47.228034: step 9988, loss 0.604695.
Train: 2018-08-05T11:53:50.736429: step 9989, loss 0.630127.
Train: 2018-08-05T11:53:54.246833: step 9990, loss 0.511581.
Test: 2018-08-05T11:54:09.101047: step 9990, loss 0.547831.
Train: 2018-08-05T11:54:12.639026: step 9991, loss 0.553892.
Train: 2018-08-05T11:54:16.125381: step 9992, loss 0.596227.
Train: 2018-08-05T11:54:19.625270: step 9993, loss 0.511566.
Train: 2018-08-05T11:54:23.135180: step 9994, loss 0.545409.
Train: 2018-08-05T11:54:26.645085: step 9995, loss 0.570833.
Train: 2018-08-05T11:54:30.159989: step 9996, loss 0.51141.
Train: 2018-08-05T11:54:33.642311: step 9997, loss 0.6389.
Train: 2018-08-05T11:54:37.163744: step 9998, loss 0.570851.
Train: 2018-08-05T11:54:40.655110: step 9999, loss 0.562344.
Train: 2018-08-05T11:54:44.143456: step 10000, loss 0.511307.
Test: 2018-08-05T11:54:58.996579: step 10000, loss 0.547337.
Train: 2018-08-05T11:55:04.679282: step 10001, loss 0.536796.
Train: 2018-08-05T11:55:08.210752: step 10002, loss 0.630566.
Train: 2018-08-05T11:55:11.711140: step 10003, loss 0.545284.
Train: 2018-08-05T11:55:15.246617: step 10004, loss 0.5794.
Train: 2018-08-05T11:55:18.713895: step 10005, loss 0.596457.
Train: 2018-08-05T11:55:22.241841: step 10006, loss 0.545296.
Train: 2018-08-05T11:55:25.758766: step 10007, loss 0.545302.
Train: 2018-08-05T11:55:29.293737: step 10008, loss 0.536781.
Train: 2018-08-05T11:55:32.814675: step 10009, loss 0.52824.
Train: 2018-08-05T11:55:36.323067: step 10010, loss 0.570874.
Test: 2018-08-05T11:55:51.134164: step 10010, loss 0.548461.
Train: 2018-08-05T11:55:54.601939: step 10011, loss 0.562337.
Train: 2018-08-05T11:55:58.081263: step 10012, loss 0.553784.
Train: 2018-08-05T11:56:01.575621: step 10013, loss 0.596576.
Train: 2018-08-05T11:56:05.090525: step 10014, loss 0.528091.
Train: 2018-08-05T11:56:08.650564: step 10015, loss 0.476653.
Train: 2018-08-05T11:56:12.124870: step 10016, loss 0.596695.
Train: 2018-08-05T11:56:15.643290: step 10017, loss 0.613955.
Train: 2018-08-05T11:56:19.133656: step 10018, loss 0.596757.
Train: 2018-08-05T11:56:22.614981: step 10019, loss 0.562335.
Train: 2018-08-05T11:56:26.145439: step 10020, loss 0.553739.
Test: 2018-08-05T11:56:41.098887: step 10020, loss 0.547786.
Train: 2018-08-05T11:56:44.626850: step 10021, loss 0.493587.
Train: 2018-08-05T11:56:48.125221: step 10022, loss 0.536526.
Train: 2018-08-05T11:56:51.594004: step 10023, loss 0.639885.
Train: 2018-08-05T11:56:55.086359: step 10024, loss 0.588177.
Train: 2018-08-05T11:56:58.599271: step 10025, loss 0.596761.
Train: 2018-08-05T11:57:02.121696: step 10026, loss 0.545149.
Train: 2018-08-05T11:57:05.662183: step 10027, loss 0.596671.
Train: 2018-08-05T11:57:09.190642: step 10028, loss 0.476642.
Train: 2018-08-05T11:57:12.660936: step 10029, loss 0.630907.
Train: 2018-08-05T11:57:16.139261: step 10030, loss 0.579457.
Test: 2018-08-05T11:57:31.000418: step 10030, loss 0.546684.
Train: 2018-08-05T11:57:34.468190: step 10031, loss 0.545241.
Train: 2018-08-05T11:57:38.002152: step 10032, loss 0.545258.
Train: 2018-08-05T11:57:41.516087: step 10033, loss 0.579406.
Train: 2018-08-05T11:57:44.995393: step 10034, loss 0.494124.
Train: 2018-08-05T11:57:48.494770: step 10035, loss 0.622074.
Train: 2018-08-05T11:57:51.991155: step 10036, loss 0.562341.
Train: 2018-08-05T11:57:55.477489: step 10037, loss 0.536769.
Train: 2018-08-05T11:57:58.989900: step 10038, loss 0.545289.
Train: 2018-08-05T11:58:02.510332: step 10039, loss 0.673199.
Train: 2018-08-05T11:58:05.995661: step 10040, loss 0.519789.
Test: 2018-08-05T11:58:21.046312: step 10040, loss 0.548145.
Train: 2018-08-05T11:58:24.568239: step 10041, loss 0.511321.
Train: 2018-08-05T11:58:28.048563: step 10042, loss 0.553833.
Train: 2018-08-05T11:58:31.562984: step 10043, loss 0.638925.
Train: 2018-08-05T11:58:35.138541: step 10044, loss 0.562353.
Train: 2018-08-05T11:58:38.650943: step 10045, loss 0.553857.
Train: 2018-08-05T11:58:42.150320: step 10046, loss 0.519995.
Train: 2018-08-05T11:58:45.654694: step 10047, loss 0.502968.
Train: 2018-08-05T11:58:49.237802: step 10048, loss 0.621841.
Train: 2018-08-05T11:58:52.745201: step 10049, loss 0.604858.
Train: 2018-08-05T11:58:56.260636: step 10050, loss 0.587837.
Test: 2018-08-05T11:59:11.092728: step 10050, loss 0.546613.
Train: 2018-08-05T11:59:14.616674: step 10051, loss 0.604783.
Train: 2018-08-05T11:59:18.132094: step 10052, loss 0.528474.
Train: 2018-08-05T11:59:21.651536: step 10053, loss 0.545434.
Train: 2018-08-05T11:59:25.129849: step 10054, loss 0.520072.
Train: 2018-08-05T11:59:28.617212: step 10055, loss 0.579281.
Train: 2018-08-05T11:59:32.173227: step 10056, loss 0.553894.
Train: 2018-08-05T11:59:35.686638: step 10057, loss 0.520025.
Train: 2018-08-05T11:59:39.154437: step 10058, loss 0.596265.
Train: 2018-08-05T11:59:42.639766: step 10059, loss 0.528425.
Train: 2018-08-05T11:59:46.130619: step 10060, loss 0.59631.
Test: 2018-08-05T12:00:00.975260: step 10060, loss 0.549333.
Train: 2018-08-05T12:00:04.539810: step 10061, loss 0.570835.
Train: 2018-08-05T12:00:08.073790: step 10062, loss 0.570835.
Train: 2018-08-05T12:00:11.573174: step 10063, loss 0.562372.
Train: 2018-08-05T12:00:15.068528: step 10064, loss 0.570856.
Train: 2018-08-05T12:00:18.533795: step 10065, loss 0.553854.
Train: 2018-08-05T12:00:22.059216: step 10066, loss 0.477382.
Train: 2018-08-05T12:00:25.562598: step 10067, loss 0.519799.
Train: 2018-08-05T12:00:29.078516: step 10068, loss 0.553808.
Train: 2018-08-05T12:00:32.591946: step 10069, loss 0.528099.
Train: 2018-08-05T12:00:36.089804: step 10070, loss 0.55376.
Test: 2018-08-05T12:00:50.954954: step 10070, loss 0.548155.
Train: 2018-08-05T12:00:54.446814: step 10071, loss 0.622633.
Train: 2018-08-05T12:00:57.957221: step 10072, loss 0.588228.
Train: 2018-08-05T12:01:01.488675: step 10073, loss 0.527853.
Train: 2018-08-05T12:01:05.002097: step 10074, loss 0.579601.
Train: 2018-08-05T12:01:08.502490: step 10075, loss 0.562339.
Train: 2018-08-05T12:01:12.034452: step 10076, loss 0.605546.
Train: 2018-08-05T12:01:15.537842: step 10077, loss 0.553703.
Train: 2018-08-05T12:01:19.053778: step 10078, loss 0.545071.
Train: 2018-08-05T12:01:22.525076: step 10079, loss 0.631413.
Train: 2018-08-05T12:01:26.039484: step 10080, loss 0.570959.
Test: 2018-08-05T12:01:40.861576: step 10080, loss 0.54796.
Train: 2018-08-05T12:01:44.440666: step 10081, loss 0.570944.
Train: 2018-08-05T12:01:47.947569: step 10082, loss 0.562335.
Train: 2018-08-05T12:01:51.448446: step 10083, loss 0.528012.
Train: 2018-08-05T12:01:54.972890: step 10084, loss 0.545185.
Train: 2018-08-05T12:01:58.482295: step 10085, loss 0.553762.
Train: 2018-08-05T12:02:01.960632: step 10086, loss 0.613771.
Train: 2018-08-05T12:02:05.521189: step 10087, loss 0.562336.
Train: 2018-08-05T12:02:09.038107: step 10088, loss 0.630764.
Train: 2018-08-05T12:02:12.517939: step 10089, loss 0.553806.
Train: 2018-08-05T12:02:16.007273: step 10090, loss 0.570857.
Test: 2018-08-05T12:02:30.809806: step 10090, loss 0.54737.
Train: 2018-08-05T12:02:34.317204: step 10091, loss 0.536851.
Train: 2018-08-05T12:02:37.803544: step 10092, loss 0.562349.
Train: 2018-08-05T12:02:41.340036: step 10093, loss 0.621705.
Train: 2018-08-05T12:02:44.836408: step 10094, loss 0.520057.
Train: 2018-08-05T12:02:48.349319: step 10095, loss 0.587716.
Train: 2018-08-05T12:02:51.873251: step 10096, loss 0.537049.
Train: 2018-08-05T12:02:55.343542: step 10097, loss 0.494901.
Train: 2018-08-05T12:02:58.824872: step 10098, loss 0.621455.
Train: 2018-08-05T12:03:02.335276: step 10099, loss 0.520178.
Train: 2018-08-05T12:03:05.891815: step 10100, loss 0.545479.
Test: 2018-08-05T12:03:20.731925: step 10100, loss 0.549998.
Train: 2018-08-05T12:03:26.485294: step 10101, loss 0.570813.
Train: 2018-08-05T12:03:29.980143: step 10102, loss 0.528528.
Train: 2018-08-05T12:03:33.489021: step 10103, loss 0.545414.
Train: 2018-08-05T12:03:36.952296: step 10104, loss 0.502955.
Train: 2018-08-05T12:03:40.453676: step 10105, loss 0.545325.
Train: 2018-08-05T12:03:43.932979: step 10106, loss 0.579407.
Train: 2018-08-05T12:03:47.419312: step 10107, loss 0.60511.
Train: 2018-08-05T12:03:50.890098: step 10108, loss 0.519514.
Train: 2018-08-05T12:03:54.372933: step 10109, loss 0.562333.
Train: 2018-08-05T12:03:57.845257: step 10110, loss 0.553743.
Test: 2018-08-05T12:04:12.646307: step 10110, loss 0.548947.
Train: 2018-08-05T12:04:16.195324: step 10111, loss 0.493458.
Train: 2018-08-05T12:04:19.735315: step 10112, loss 0.510529.
Train: 2018-08-05T12:04:23.224656: step 10113, loss 0.518981.
Train: 2018-08-05T12:04:26.697964: step 10114, loss 0.571071.
Train: 2018-08-05T12:04:30.205862: step 10115, loss 0.50124.
Train: 2018-08-05T12:04:33.708737: step 10116, loss 0.641393.
Train: 2018-08-05T12:04:35.425347: step 10117, loss 0.524816.
Train: 2018-08-05T12:04:38.913687: step 10118, loss 0.500539.
Train: 2018-08-05T12:04:42.392003: step 10119, loss 0.624864.
Train: 2018-08-05T12:04:45.878846: step 10120, loss 0.589109.
Test: 2018-08-05T12:05:00.647850: step 10120, loss 0.54866.
Train: 2018-08-05T12:05:04.129675: step 10121, loss 0.500338.
Train: 2018-08-05T12:05:07.605980: step 10122, loss 0.580221.
Train: 2018-08-05T12:05:11.125923: step 10123, loss 0.607037.
Train: 2018-08-05T12:05:14.626798: step 10124, loss 0.624691.
Train: 2018-08-05T12:05:18.129172: step 10125, loss 0.544733.
Train: 2018-08-05T12:05:21.621546: step 10126, loss 0.52705.
Train: 2018-08-05T12:05:25.109390: step 10127, loss 0.535911.
Train: 2018-08-05T12:05:28.644354: step 10128, loss 0.535915.
Train: 2018-08-05T12:05:32.188854: step 10129, loss 0.518228.
Train: 2018-08-05T12:05:35.719313: step 10130, loss 0.62439.
Test: 2018-08-05T12:05:50.677697: step 10130, loss 0.548076.
Train: 2018-08-05T12:05:54.170556: step 10131, loss 0.500531.
Train: 2018-08-05T12:05:57.667909: step 10132, loss 0.500503.
Train: 2018-08-05T12:06:01.152224: step 10133, loss 0.597902.
Train: 2018-08-05T12:06:04.633556: step 10134, loss 0.597923.
Train: 2018-08-05T12:06:08.141939: step 10135, loss 0.580179.
Train: 2018-08-05T12:06:11.678414: step 10136, loss 0.535885.
Train: 2018-08-05T12:06:15.178786: step 10137, loss 0.624394.
Train: 2018-08-05T12:06:18.674147: step 10138, loss 0.588932.
Train: 2018-08-05T12:06:22.155475: step 10139, loss 0.562416.
Train: 2018-08-05T12:06:25.655854: step 10140, loss 0.571195.
Test: 2018-08-05T12:06:40.436803: step 10140, loss 0.54774.
Train: 2018-08-05T12:06:43.953731: step 10141, loss 0.571162.
Train: 2018-08-05T12:06:47.482692: step 10142, loss 0.527376.
Train: 2018-08-05T12:06:51.012142: step 10143, loss 0.571107.
Train: 2018-08-05T12:06:54.501999: step 10144, loss 0.571086.
Train: 2018-08-05T12:06:58.053522: step 10145, loss 0.614588.
Train: 2018-08-05T12:07:01.571442: step 10146, loss 0.58839.
Train: 2018-08-05T12:07:05.087869: step 10147, loss 0.536385.
Train: 2018-08-05T12:07:08.595269: step 10148, loss 0.493284.
Train: 2018-08-05T12:07:12.114719: step 10149, loss 0.622714.
Train: 2018-08-05T12:07:15.604059: step 10150, loss 0.674238.
Test: 2018-08-05T12:07:30.421128: step 10150, loss 0.548217.
Train: 2018-08-05T12:07:33.914985: step 10151, loss 0.665195.
Train: 2018-08-05T12:07:37.432423: step 10152, loss 0.604942.
Train: 2018-08-05T12:07:40.928290: step 10153, loss 0.503093.
Train: 2018-08-05T12:07:44.457768: step 10154, loss 0.520228.
Train: 2018-08-05T12:07:47.953633: step 10155, loss 0.545578.
Train: 2018-08-05T12:07:51.434954: step 10156, loss 0.579165.
Train: 2018-08-05T12:07:54.947355: step 10157, loss 0.570773.
Train: 2018-08-05T12:07:58.426680: step 10158, loss 0.570768.
Train: 2018-08-05T12:08:01.943110: step 10159, loss 0.495797.
Train: 2018-08-05T12:08:05.475610: step 10160, loss 0.554108.
Test: 2018-08-05T12:08:20.442037: step 10160, loss 0.547758.
Train: 2018-08-05T12:08:23.933381: step 10161, loss 0.537448.
Train: 2018-08-05T12:08:27.446282: step 10162, loss 0.554094.
Train: 2018-08-05T12:08:30.983753: step 10163, loss 0.529048.
Train: 2018-08-05T12:08:34.458067: step 10164, loss 0.528979.
Train: 2018-08-05T12:08:37.989520: step 10165, loss 0.545641.
Train: 2018-08-05T12:08:41.510964: step 10166, loss 0.545583.
Train: 2018-08-05T12:08:45.029903: step 10167, loss 0.562373.
Train: 2018-08-05T12:08:48.509737: step 10168, loss 0.520127.
Train: 2018-08-05T12:08:52.031166: step 10169, loss 0.60473.
Train: 2018-08-05T12:08:55.493432: step 10170, loss 0.579334.
Test: 2018-08-05T12:09:10.327062: step 10170, loss 0.548334.
Train: 2018-08-05T12:09:13.825424: step 10171, loss 0.494294.
Train: 2018-08-05T12:09:17.328808: step 10172, loss 0.630585.
Train: 2018-08-05T12:09:20.891368: step 10173, loss 0.553798.
Train: 2018-08-05T12:09:24.421323: step 10174, loss 0.553786.
Train: 2018-08-05T12:09:27.915660: step 10175, loss 0.60514.
Train: 2018-08-05T12:09:31.447593: step 10176, loss 0.562336.
Train: 2018-08-05T12:09:34.943449: step 10177, loss 0.630846.
Train: 2018-08-05T12:09:38.498478: step 10178, loss 0.536679.
Train: 2018-08-05T12:09:42.026931: step 10179, loss 0.519603.
Train: 2018-08-05T12:09:45.496226: step 10180, loss 0.553787.
Test: 2018-08-05T12:10:00.306215: step 10180, loss 0.547067.
Train: 2018-08-05T12:10:03.802072: step 10181, loss 0.553783.
Train: 2018-08-05T12:10:07.310450: step 10182, loss 0.502428.
Train: 2018-08-05T12:10:10.782755: step 10183, loss 0.545188.
Train: 2018-08-05T12:10:14.333274: step 10184, loss 0.527973.
Train: 2018-08-05T12:10:17.842681: step 10185, loss 0.519278.
Train: 2018-08-05T12:10:21.332036: step 10186, loss 0.60553.
Train: 2018-08-05T12:10:24.892063: step 10187, loss 0.588303.
Train: 2018-08-05T12:10:28.404475: step 10188, loss 0.493046.
Train: 2018-08-05T12:10:31.872744: step 10189, loss 0.60576.
Train: 2018-08-05T12:10:35.436288: step 10190, loss 0.553662.
Test: 2018-08-05T12:10:50.259354: step 10190, loss 0.548427.
Train: 2018-08-05T12:10:53.748677: step 10191, loss 0.588457.
Train: 2018-08-05T12:10:57.311728: step 10192, loss 0.562358.
Train: 2018-08-05T12:11:00.792061: step 10193, loss 0.649406.
Train: 2018-08-05T12:11:04.270378: step 10194, loss 0.579733.
Train: 2018-08-05T12:11:07.767771: step 10195, loss 0.553674.
Train: 2018-08-05T12:11:11.251115: step 10196, loss 0.510398.
Train: 2018-08-05T12:11:14.763531: step 10197, loss 0.55369.
Train: 2018-08-05T12:11:18.259897: step 10198, loss 0.614246.
Train: 2018-08-05T12:11:21.727188: step 10199, loss 0.553701.
Train: 2018-08-05T12:11:25.204508: step 10200, loss 0.562339.
Test: 2018-08-05T12:11:39.964920: step 10200, loss 0.548539.
Train: 2018-08-05T12:11:45.619028: step 10201, loss 0.493391.
Train: 2018-08-05T12:11:49.129431: step 10202, loss 0.458878.
Train: 2018-08-05T12:11:52.689972: step 10203, loss 0.562346.
Train: 2018-08-05T12:11:56.181841: step 10204, loss 0.519033.
Train: 2018-08-05T12:11:59.654642: step 10205, loss 0.623171.
Train: 2018-08-05T12:12:03.136968: step 10206, loss 0.579749.
Train: 2018-08-05T12:12:06.621306: step 10207, loss 0.536253.
Train: 2018-08-05T12:12:10.109657: step 10208, loss 0.614617.
Train: 2018-08-05T12:12:13.612028: step 10209, loss 0.466581.
Train: 2018-08-05T12:12:17.103389: step 10210, loss 0.52748.
Test: 2018-08-05T12:12:31.918469: step 10210, loss 0.547576.
Train: 2018-08-05T12:12:35.422349: step 10211, loss 0.501198.
Train: 2018-08-05T12:12:38.907706: step 10212, loss 0.53609.
Train: 2018-08-05T12:12:42.390038: step 10213, loss 0.562402.
Train: 2018-08-05T12:12:45.869359: step 10214, loss 0.571234.
Train: 2018-08-05T12:12:49.428905: step 10215, loss 0.588933.
Train: 2018-08-05T12:12:52.932287: step 10216, loss 0.562439.
Train: 2018-08-05T12:12:56.443713: step 10217, loss 0.527039.
Train: 2018-08-05T12:12:59.953126: step 10218, loss 0.527002.
Train: 2018-08-05T12:13:03.430432: step 10219, loss 0.535833.
Train: 2018-08-05T12:13:06.941325: step 10220, loss 0.589168.
Test: 2018-08-05T12:13:22.026027: step 10220, loss 0.549058.
Train: 2018-08-05T12:13:25.554473: step 10221, loss 0.580267.
Train: 2018-08-05T12:13:29.066888: step 10222, loss 0.669342.
Train: 2018-08-05T12:13:32.577292: step 10223, loss 0.518058.
Train: 2018-08-05T12:13:36.073656: step 10224, loss 0.57131.
Train: 2018-08-05T12:13:39.575017: step 10225, loss 0.588861.
Train: 2018-08-05T12:13:43.093940: step 10226, loss 0.518573.
Train: 2018-08-05T12:13:46.638426: step 10227, loss 0.536307.
Train: 2018-08-05T12:13:50.234565: step 10228, loss 0.535796.
Train: 2018-08-05T12:13:53.712886: step 10229, loss 0.491832.
Train: 2018-08-05T12:13:57.186200: step 10230, loss 0.55361.
Test: 2018-08-05T12:14:11.977661: step 10230, loss 0.548067.
Train: 2018-08-05T12:14:15.500097: step 10231, loss 0.562445.
Train: 2018-08-05T12:14:19.028543: step 10232, loss 0.615697.
Train: 2018-08-05T12:14:22.551972: step 10233, loss 0.544734.
Train: 2018-08-05T12:14:26.044327: step 10234, loss 0.544736.
Train: 2018-08-05T12:14:29.519134: step 10235, loss 0.589015.
Train: 2018-08-05T12:14:32.981929: step 10236, loss 0.642063.
Train: 2018-08-05T12:14:36.457241: step 10237, loss 0.615348.
Train: 2018-08-05T12:14:39.929538: step 10238, loss 0.562353.
Train: 2018-08-05T12:14:43.470514: step 10239, loss 0.562474.
Train: 2018-08-05T12:14:46.979434: step 10240, loss 0.640991.
Test: 2018-08-05T12:15:01.907282: step 10240, loss 0.547241.
Train: 2018-08-05T12:15:05.409647: step 10241, loss 0.492889.
Train: 2018-08-05T12:15:08.950143: step 10242, loss 0.536371.
Train: 2018-08-05T12:15:12.458545: step 10243, loss 0.622931.
Train: 2018-08-05T12:15:15.962935: step 10244, loss 0.588235.
Train: 2018-08-05T12:15:19.481348: step 10245, loss 0.553758.
Train: 2018-08-05T12:15:22.988742: step 10246, loss 0.63094.
Train: 2018-08-05T12:15:26.497657: step 10247, loss 0.587934.
Train: 2018-08-05T12:15:29.991532: step 10248, loss 0.562236.
Train: 2018-08-05T12:15:33.459817: step 10249, loss 0.553819.
Train: 2018-08-05T12:15:36.968203: step 10250, loss 0.570575.
Test: 2018-08-05T12:15:51.879542: step 10250, loss 0.549149.
Train: 2018-08-05T12:15:55.378418: step 10251, loss 0.52916.
Train: 2018-08-05T12:15:58.922916: step 10252, loss 0.562463.
Train: 2018-08-05T12:16:02.405251: step 10253, loss 0.569621.
Train: 2018-08-05T12:16:05.897604: step 10254, loss 0.539541.
Train: 2018-08-05T12:16:09.366903: step 10255, loss 0.579369.
Train: 2018-08-05T12:16:12.850246: step 10256, loss 0.538095.
Train: 2018-08-05T12:16:16.354624: step 10257, loss 0.512559.
Train: 2018-08-05T12:16:19.887104: step 10258, loss 0.503969.
Train: 2018-08-05T12:16:23.375443: step 10259, loss 0.553848.
Train: 2018-08-05T12:16:26.847739: step 10260, loss 0.655091.
Test: 2018-08-05T12:16:41.693928: step 10260, loss 0.546524.
Train: 2018-08-05T12:16:45.222398: step 10261, loss 0.553941.
Train: 2018-08-05T12:16:48.737790: step 10262, loss 0.579235.
Train: 2018-08-05T12:16:52.273766: step 10263, loss 0.579381.
Train: 2018-08-05T12:16:55.784696: step 10264, loss 0.545295.
Train: 2018-08-05T12:16:59.302132: step 10265, loss 0.604751.
Train: 2018-08-05T12:17:02.787477: step 10266, loss 0.503153.
Train: 2018-08-05T12:17:06.276837: step 10267, loss 0.587788.
Train: 2018-08-05T12:17:07.987449: step 10268, loss 0.490157.
Train: 2018-08-05T12:17:11.485835: step 10269, loss 0.536948.
Train: 2018-08-05T12:17:14.999249: step 10270, loss 0.545377.
Test: 2018-08-05T12:17:29.865921: step 10270, loss 0.549105.
Train: 2018-08-05T12:17:33.370324: step 10271, loss 0.545317.
Train: 2018-08-05T12:17:36.894265: step 10272, loss 0.579408.
Train: 2018-08-05T12:17:40.382113: step 10273, loss 0.553786.
Train: 2018-08-05T12:17:43.869451: step 10274, loss 0.570902.
Train: 2018-08-05T12:17:47.344760: step 10275, loss 0.579495.
Train: 2018-08-05T12:17:50.882743: step 10276, loss 0.596689.
Train: 2018-08-05T12:17:54.411205: step 10277, loss 0.562335.
Train: 2018-08-05T12:17:57.910067: step 10278, loss 0.493615.
Train: 2018-08-05T12:18:01.430990: step 10279, loss 0.579541.
Train: 2018-08-05T12:18:04.913331: step 10280, loss 0.665667.
Test: 2018-08-05T12:18:20.051219: step 10280, loss 0.546787.
Train: 2018-08-05T12:18:23.554578: step 10281, loss 0.639727.
Train: 2018-08-05T12:18:27.094063: step 10282, loss 0.553761.
Train: 2018-08-05T12:18:30.602454: step 10283, loss 0.622206.
Train: 2018-08-05T12:18:34.098818: step 10284, loss 0.630522.
Train: 2018-08-05T12:18:37.610227: step 10285, loss 0.494484.
Train: 2018-08-05T12:18:41.102566: step 10286, loss 0.60466.
Train: 2018-08-05T12:18:44.607949: step 10287, loss 0.553937.
Train: 2018-08-05T12:18:48.161970: step 10288, loss 0.5792.
Train: 2018-08-05T12:18:51.665865: step 10289, loss 0.52885.
Train: 2018-08-05T12:18:55.194314: step 10290, loss 0.520542.
Test: 2018-08-05T12:19:10.067516: step 10290, loss 0.548623.
Train: 2018-08-05T12:19:13.582935: step 10291, loss 0.470354.
Train: 2018-08-05T12:19:17.121400: step 10292, loss 0.57078.
Train: 2018-08-05T12:19:20.649851: step 10293, loss 0.612759.
Train: 2018-08-05T12:19:24.162253: step 10294, loss 0.545593.
Train: 2018-08-05T12:19:27.683180: step 10295, loss 0.528772.
Train: 2018-08-05T12:19:31.202101: step 10296, loss 0.537132.
Train: 2018-08-05T12:19:34.698465: step 10297, loss 0.51178.
Train: 2018-08-05T12:19:38.193845: step 10298, loss 0.503167.
Train: 2018-08-05T12:19:41.680196: step 10299, loss 0.570838.
Train: 2018-08-05T12:19:45.182075: step 10300, loss 0.545305.
Test: 2018-08-05T12:20:00.080859: step 10300, loss 0.547275.
Train: 2018-08-05T12:20:05.866836: step 10301, loss 0.536695.
Train: 2018-08-05T12:20:09.379247: step 10302, loss 0.570913.
Train: 2018-08-05T12:20:12.867085: step 10303, loss 0.57094.
Train: 2018-08-05T12:20:16.344890: step 10304, loss 0.519209.
Train: 2018-08-05T12:20:19.834216: step 10305, loss 0.614257.
Train: 2018-08-05T12:20:23.337616: step 10306, loss 0.571012.
Train: 2018-08-05T12:20:26.900154: step 10307, loss 0.536317.
Train: 2018-08-05T12:20:30.385984: step 10308, loss 0.562353.
Train: 2018-08-05T12:20:33.921940: step 10309, loss 0.571059.
Train: 2018-08-05T12:20:37.391214: step 10310, loss 0.56236.
Test: 2018-08-05T12:20:52.204258: step 10310, loss 0.548207.
Train: 2018-08-05T12:20:55.682076: step 10311, loss 0.501348.
Train: 2018-08-05T12:20:59.201010: step 10312, loss 0.544905.
Train: 2018-08-05T12:21:02.712919: step 10313, loss 0.571125.
Train: 2018-08-05T12:21:06.184720: step 10314, loss 0.562383.
Train: 2018-08-05T12:21:09.681574: step 10315, loss 0.553618.
Train: 2018-08-05T12:21:13.172913: step 10316, loss 0.588734.
Train: 2018-08-05T12:21:16.645215: step 10317, loss 0.536049.
Train: 2018-08-05T12:21:20.152622: step 10318, loss 0.579976.
Train: 2018-08-05T12:21:23.668048: step 10319, loss 0.527244.
Train: 2018-08-05T12:21:27.165930: step 10320, loss 0.588789.
Test: 2018-08-05T12:21:42.049200: step 10320, loss 0.548121.
Train: 2018-08-05T12:21:45.571638: step 10321, loss 0.588786.
Train: 2018-08-05T12:21:49.053967: step 10322, loss 0.588762.
Train: 2018-08-05T12:21:52.541299: step 10323, loss 0.606271.
Train: 2018-08-05T12:21:56.044193: step 10324, loss 0.501087.
Train: 2018-08-05T12:21:59.548071: step 10325, loss 0.527383.
Train: 2018-08-05T12:22:03.101576: step 10326, loss 0.501149.
Train: 2018-08-05T12:22:06.579395: step 10327, loss 0.606156.
Train: 2018-08-05T12:22:10.120892: step 10328, loss 0.553626.
Train: 2018-08-05T12:22:13.591199: step 10329, loss 0.527366.
Train: 2018-08-05T12:22:17.087577: step 10330, loss 0.536108.
Test: 2018-08-05T12:22:31.923685: step 10330, loss 0.547545.
Train: 2018-08-05T12:22:35.446620: step 10331, loss 0.536089.
Train: 2018-08-05T12:22:38.944999: step 10332, loss 0.606274.
Train: 2018-08-05T12:22:42.443368: step 10333, loss 0.650163.
Train: 2018-08-05T12:22:45.951767: step 10334, loss 0.52734.
Train: 2018-08-05T12:22:49.470197: step 10335, loss 0.658642.
Train: 2018-08-05T12:22:52.951528: step 10336, loss 0.562366.
Train: 2018-08-05T12:22:56.492022: step 10337, loss 0.518859.
Train: 2018-08-05T12:23:00.047541: step 10338, loss 0.614454.
Train: 2018-08-05T12:23:03.552440: step 10339, loss 0.562344.
Train: 2018-08-05T12:23:07.079890: step 10340, loss 0.527789.
Test: 2018-08-05T12:23:21.977654: step 10340, loss 0.547936.
Train: 2018-08-05T12:23:25.472506: step 10341, loss 0.605454.
Train: 2018-08-05T12:23:28.956838: step 10342, loss 0.536529.
Train: 2018-08-05T12:23:32.478753: step 10343, loss 0.536572.
Train: 2018-08-05T12:23:36.003199: step 10344, loss 0.553756.
Train: 2018-08-05T12:23:39.490519: step 10345, loss 0.588053.
Train: 2018-08-05T12:23:42.996907: step 10346, loss 0.510964.
Train: 2018-08-05T12:23:46.478232: step 10347, loss 0.528089.
Train: 2018-08-05T12:23:49.988633: step 10348, loss 0.528065.
Train: 2018-08-05T12:23:53.521091: step 10349, loss 0.528019.
Train: 2018-08-05T12:23:57.029496: step 10350, loss 0.579526.
Test: 2018-08-05T12:24:11.870133: step 10350, loss 0.547962.
Train: 2018-08-05T12:24:15.371519: step 10351, loss 0.596763.
Train: 2018-08-05T12:24:18.909004: step 10352, loss 0.545115.
Train: 2018-08-05T12:24:22.401374: step 10353, loss 0.579569.
Train: 2018-08-05T12:24:25.895747: step 10354, loss 0.527864.
Train: 2018-08-05T12:24:29.412170: step 10355, loss 0.579589.
Train: 2018-08-05T12:24:32.909552: step 10356, loss 0.648629.
Train: 2018-08-05T12:24:36.384867: step 10357, loss 0.536489.
Train: 2018-08-05T12:24:39.846150: step 10358, loss 0.596769.
Train: 2018-08-05T12:24:43.368595: step 10359, loss 0.519361.
Train: 2018-08-05T12:24:46.850951: step 10360, loss 0.588106.
Test: 2018-08-05T12:25:01.834383: step 10360, loss 0.547806.
Train: 2018-08-05T12:25:05.356315: step 10361, loss 0.588079.
Train: 2018-08-05T12:25:08.875240: step 10362, loss 0.553767.
Train: 2018-08-05T12:25:12.363082: step 10363, loss 0.510989.
Train: 2018-08-05T12:25:15.906077: step 10364, loss 0.622241.
Train: 2018-08-05T12:25:19.397941: step 10365, loss 0.52815.
Train: 2018-08-05T12:25:22.855186: step 10366, loss 0.528166.
Train: 2018-08-05T12:25:26.417741: step 10367, loss 0.502516.
Train: 2018-08-05T12:25:29.932152: step 10368, loss 0.562336.
Train: 2018-08-05T12:25:33.401454: step 10369, loss 0.553764.
Train: 2018-08-05T12:25:36.878282: step 10370, loss 0.596671.
Test: 2018-08-05T12:25:51.736366: step 10370, loss 0.548189.
Train: 2018-08-05T12:25:55.257295: step 10371, loss 0.579512.
Train: 2018-08-05T12:25:58.751661: step 10372, loss 0.61387.
Train: 2018-08-05T12:26:02.251030: step 10373, loss 0.588075.
Train: 2018-08-05T12:26:05.764949: step 10374, loss 0.588037.
Train: 2018-08-05T12:26:09.322962: step 10375, loss 0.536685.
Train: 2018-08-05T12:26:12.797272: step 10376, loss 0.528177.
Train: 2018-08-05T12:26:16.302666: step 10377, loss 0.570875.
Train: 2018-08-05T12:26:19.802037: step 10378, loss 0.587937.
Train: 2018-08-05T12:26:23.298431: step 10379, loss 0.56234.
Train: 2018-08-05T12:26:26.875007: step 10380, loss 0.511242.
Test: 2018-08-05T12:26:41.768789: step 10380, loss 0.546937.
Train: 2018-08-05T12:26:45.277189: step 10381, loss 0.477149.
Train: 2018-08-05T12:26:48.799642: step 10382, loss 0.528188.
Train: 2018-08-05T12:26:52.288998: step 10383, loss 0.613698.
Train: 2018-08-05T12:26:55.759284: step 10384, loss 0.562335.
Train: 2018-08-05T12:26:59.254653: step 10385, loss 0.588077.
Train: 2018-08-05T12:27:02.752024: step 10386, loss 0.596673.
Train: 2018-08-05T12:27:06.293512: step 10387, loss 0.562335.
Train: 2018-08-05T12:27:09.763812: step 10388, loss 0.570913.
Train: 2018-08-05T12:27:13.256173: step 10389, loss 0.570909.
Train: 2018-08-05T12:27:16.745509: step 10390, loss 0.562335.
Test: 2018-08-05T12:27:31.509874: step 10390, loss 0.547838.
Train: 2018-08-05T12:27:34.973132: step 10391, loss 0.545211.
Train: 2018-08-05T12:27:38.502085: step 10392, loss 0.562336.
Train: 2018-08-05T12:27:41.999444: step 10393, loss 0.519541.
Train: 2018-08-05T12:27:45.493786: step 10394, loss 0.579466.
Train: 2018-08-05T12:27:48.971111: step 10395, loss 0.536633.
Train: 2018-08-05T12:27:52.465464: step 10396, loss 0.579483.
Train: 2018-08-05T12:27:55.935747: step 10397, loss 0.579489.
Train: 2018-08-05T12:27:59.464196: step 10398, loss 0.605214.
Train: 2018-08-05T12:28:02.975607: step 10399, loss 0.60517.
Train: 2018-08-05T12:28:06.463952: step 10400, loss 0.511032.
Test: 2018-08-05T12:28:21.404296: step 10400, loss 0.548459.
Train: 2018-08-05T12:28:27.070948: step 10401, loss 0.519609.
Train: 2018-08-05T12:28:30.552290: step 10402, loss 0.553788.
Train: 2018-08-05T12:28:34.023597: step 10403, loss 0.605103.
Train: 2018-08-05T12:28:37.575122: step 10404, loss 0.528139.
Train: 2018-08-05T12:28:41.112592: step 10405, loss 0.605097.
Train: 2018-08-05T12:28:44.595898: step 10406, loss 0.570884.
Train: 2018-08-05T12:28:48.154431: step 10407, loss 0.545258.
Train: 2018-08-05T12:28:51.694903: step 10408, loss 0.613561.
Train: 2018-08-05T12:28:55.153181: step 10409, loss 0.553815.
Train: 2018-08-05T12:28:58.675631: step 10410, loss 0.536794.
Test: 2018-08-05T12:29:13.444563: step 10410, loss 0.548714.
Train: 2018-08-05T12:29:16.964991: step 10411, loss 0.55383.
Train: 2018-08-05T12:29:20.460844: step 10412, loss 0.579365.
Train: 2018-08-05T12:29:24.025893: step 10413, loss 0.553837.
Train: 2018-08-05T12:29:27.513731: step 10414, loss 0.545335.
Train: 2018-08-05T12:29:30.998551: step 10415, loss 0.545332.
Train: 2018-08-05T12:29:34.497919: step 10416, loss 0.613408.
Train: 2018-08-05T12:29:38.040432: step 10417, loss 0.570849.
Train: 2018-08-05T12:29:41.534800: step 10418, loss 0.553846.
Train: 2018-08-05T12:29:43.235870: step 10419, loss 0.489849.
Train: 2018-08-05T12:29:46.724224: step 10420, loss 0.502797.
Test: 2018-08-05T12:30:01.532290: step 10420, loss 0.547311.
Train: 2018-08-05T12:30:05.120379: step 10421, loss 0.57086.
Train: 2018-08-05T12:30:08.673896: step 10422, loss 0.511018.
Train: 2018-08-05T12:30:12.179289: step 10423, loss 0.639509.
Train: 2018-08-05T12:30:15.662627: step 10424, loss 0.450474.
Train: 2018-08-05T12:30:19.134942: step 10425, loss 0.50167.
Train: 2018-08-05T12:30:22.614277: step 10426, loss 0.580372.
Train: 2018-08-05T12:30:26.117142: step 10427, loss 0.579151.
Train: 2018-08-05T12:30:29.623037: step 10428, loss 0.561125.
Train: 2018-08-05T12:30:33.195603: step 10429, loss 0.597358.
Train: 2018-08-05T12:30:36.685452: step 10430, loss 0.608368.
Test: 2018-08-05T12:30:51.487514: step 10430, loss 0.546728.
Train: 2018-08-05T12:30:54.977365: step 10431, loss 0.581126.
Train: 2018-08-05T12:30:58.437139: step 10432, loss 0.607432.
Train: 2018-08-05T12:31:01.933526: step 10433, loss 0.509745.
Train: 2018-08-05T12:31:05.444926: step 10434, loss 0.493041.
Train: 2018-08-05T12:31:08.953823: step 10435, loss 0.571242.
Train: 2018-08-05T12:31:12.429138: step 10436, loss 0.562068.
Train: 2018-08-05T12:31:15.908962: step 10437, loss 0.475214.
Train: 2018-08-05T12:31:19.398316: step 10438, loss 0.553425.
Train: 2018-08-05T12:31:22.894684: step 10439, loss 0.553581.
Train: 2018-08-05T12:31:26.406090: step 10440, loss 0.457525.
Test: 2018-08-05T12:31:41.198088: step 10440, loss 0.547128.
Train: 2018-08-05T12:31:44.710496: step 10441, loss 0.536202.
Train: 2018-08-05T12:31:48.206884: step 10442, loss 0.588795.
Train: 2018-08-05T12:31:51.696232: step 10443, loss 0.527032.
Train: 2018-08-05T12:31:55.219682: step 10444, loss 0.571396.
Train: 2018-08-05T12:31:58.712536: step 10445, loss 0.491865.
Train: 2018-08-05T12:32:02.215422: step 10446, loss 0.50904.
Train: 2018-08-05T12:32:05.712796: step 10447, loss 0.562526.
Train: 2018-08-05T12:32:09.204126: step 10448, loss 0.544904.
Train: 2018-08-05T12:32:12.696978: step 10449, loss 0.571069.
Train: 2018-08-05T12:32:16.181816: step 10450, loss 0.580899.
Test: 2018-08-05T12:32:31.088542: step 10450, loss 0.549427.
Train: 2018-08-05T12:32:34.585920: step 10451, loss 0.4902.
Train: 2018-08-05T12:32:38.095322: step 10452, loss 0.616688.
Train: 2018-08-05T12:32:41.595695: step 10453, loss 0.545041.
Train: 2018-08-05T12:32:45.123140: step 10454, loss 0.544468.
Train: 2018-08-05T12:32:48.610492: step 10455, loss 0.554368.
Train: 2018-08-05T12:32:52.085807: step 10456, loss 0.507785.
Train: 2018-08-05T12:32:55.589199: step 10457, loss 0.562932.
Train: 2018-08-05T12:32:59.097617: step 10458, loss 0.581287.
Train: 2018-08-05T12:33:02.626082: step 10459, loss 0.535147.
Train: 2018-08-05T12:33:06.224230: step 10460, loss 0.517268.
Test: 2018-08-05T12:33:21.100922: step 10460, loss 0.547336.
Train: 2018-08-05T12:33:24.589767: step 10461, loss 0.598908.
Train: 2018-08-05T12:33:28.104679: step 10462, loss 0.5264.
Train: 2018-08-05T12:33:31.585009: step 10463, loss 0.535434.
Train: 2018-08-05T12:33:35.081364: step 10464, loss 0.635846.
Train: 2018-08-05T12:33:38.589272: step 10465, loss 0.47209.
Train: 2018-08-05T12:33:42.104193: step 10466, loss 0.562593.
Train: 2018-08-05T12:33:45.584014: step 10467, loss 0.526474.
Train: 2018-08-05T12:33:49.090890: step 10468, loss 0.490238.
Train: 2018-08-05T12:33:52.623354: step 10469, loss 0.499209.
Train: 2018-08-05T12:33:56.109687: step 10470, loss 0.644451.
Test: 2018-08-05T12:34:10.992892: step 10470, loss 0.546917.
Train: 2018-08-05T12:34:14.540394: step 10471, loss 0.58089.
Train: 2018-08-05T12:34:18.063352: step 10472, loss 0.599074.
Train: 2018-08-05T12:34:21.550698: step 10473, loss 0.544592.
Train: 2018-08-05T12:34:25.034046: step 10474, loss 0.553656.
Train: 2018-08-05T12:34:28.522396: step 10475, loss 0.607878.
Train: 2018-08-05T12:34:32.006721: step 10476, loss 0.544597.
Train: 2018-08-05T12:34:35.523140: step 10477, loss 0.544595.
Train: 2018-08-05T12:34:39.050598: step 10478, loss 0.535621.
Train: 2018-08-05T12:34:42.562003: step 10479, loss 0.508705.
Train: 2018-08-05T12:34:46.037316: step 10480, loss 0.616424.
Test: 2018-08-05T12:35:00.919608: step 10480, loss 0.546555.
Train: 2018-08-05T12:35:04.433538: step 10481, loss 0.535668.
Train: 2018-08-05T12:35:07.924389: step 10482, loss 0.508828.
Train: 2018-08-05T12:35:11.443313: step 10483, loss 0.61625.
Train: 2018-08-05T12:35:14.961754: step 10484, loss 0.589345.
Train: 2018-08-05T12:35:18.453127: step 10485, loss 0.544667.
Train: 2018-08-05T12:35:21.951002: step 10486, loss 0.669366.
Train: 2018-08-05T12:35:25.509518: step 10487, loss 0.59795.
Train: 2018-08-05T12:35:28.996361: step 10488, loss 0.553597.
Train: 2018-08-05T12:35:32.521808: step 10489, loss 0.553607.
Train: 2018-08-05T12:35:36.046766: step 10490, loss 0.562387.
Test: 2018-08-05T12:35:50.922452: step 10490, loss 0.547778.
Train: 2018-08-05T12:35:54.430852: step 10491, loss 0.588589.
Train: 2018-08-05T12:35:57.968339: step 10492, loss 0.597187.
Train: 2018-08-05T12:36:01.463210: step 10493, loss 0.545004.
Train: 2018-08-05T12:36:04.953070: step 10494, loss 0.553699.
Train: 2018-08-05T12:36:08.462489: step 10495, loss 0.519257.
Train: 2018-08-05T12:36:11.977904: step 10496, loss 0.605337.
Train: 2018-08-05T12:36:15.503340: step 10497, loss 0.545183.
Train: 2018-08-05T12:36:18.995683: step 10498, loss 0.51099.
Train: 2018-08-05T12:36:22.549701: step 10499, loss 0.605112.
Train: 2018-08-05T12:36:26.044064: step 10500, loss 0.468353.
Test: 2018-08-05T12:36:40.908272: step 10500, loss 0.547653.
Train: 2018-08-05T12:36:46.584950: step 10501, loss 0.545237.
Train: 2018-08-05T12:36:50.128463: step 10502, loss 0.587959.
Train: 2018-08-05T12:36:53.637867: step 10503, loss 0.596883.
Train: 2018-08-05T12:36:57.167321: step 10504, loss 0.613903.
Train: 2018-08-05T12:37:00.660192: step 10505, loss 0.562306.
Train: 2018-08-05T12:37:04.114943: step 10506, loss 0.596446.
Train: 2018-08-05T12:37:07.647903: step 10507, loss 0.553834.
Train: 2018-08-05T12:37:11.157823: step 10508, loss 0.613317.
Train: 2018-08-05T12:37:14.659199: step 10509, loss 0.621665.
Train: 2018-08-05T12:37:18.154565: step 10510, loss 0.570808.
Test: 2018-08-05T12:37:33.129047: step 10510, loss 0.549102.
Train: 2018-08-05T12:37:36.622904: step 10511, loss 0.520309.
Train: 2018-08-05T12:37:40.132302: step 10512, loss 0.629563.
Train: 2018-08-05T12:37:43.673779: step 10513, loss 0.537297.
Train: 2018-08-05T12:37:47.178163: step 10514, loss 0.587469.
Train: 2018-08-05T12:37:50.680048: step 10515, loss 0.61241.
Train: 2018-08-05T12:37:54.163371: step 10516, loss 0.587362.
Train: 2018-08-05T12:37:57.714873: step 10517, loss 0.57903.
Train: 2018-08-05T12:38:01.205706: step 10518, loss 0.611987.
Train: 2018-08-05T12:38:04.702548: step 10519, loss 0.546119.
Train: 2018-08-05T12:38:08.228495: step 10520, loss 0.52982.
Test: 2018-08-05T12:38:23.241023: step 10520, loss 0.549128.
Train: 2018-08-05T12:38:26.746906: step 10521, loss 0.595292.
Train: 2018-08-05T12:38:30.260324: step 10522, loss 0.587088.
Train: 2018-08-05T12:38:33.777243: step 10523, loss 0.538223.
Train: 2018-08-05T12:38:37.271094: step 10524, loss 0.554525.
Train: 2018-08-05T12:38:40.778489: step 10525, loss 0.513916.
Train: 2018-08-05T12:38:44.286908: step 10526, loss 0.562652.
Train: 2018-08-05T12:38:47.815372: step 10527, loss 0.521943.
Train: 2018-08-05T12:38:51.303713: step 10528, loss 0.570773.
Train: 2018-08-05T12:38:54.779544: step 10529, loss 0.587114.
Train: 2018-08-05T12:38:58.266891: step 10530, loss 0.562581.
Test: 2018-08-05T12:39:13.137594: step 10530, loss 0.548313.
Train: 2018-08-05T12:39:16.670568: step 10531, loss 0.537979.
Train: 2018-08-05T12:39:20.215573: step 10532, loss 0.620039.
Train: 2018-08-05T12:39:23.770589: step 10533, loss 0.570759.
Train: 2018-08-05T12:39:27.253427: step 10534, loss 0.496736.
Train: 2018-08-05T12:39:30.745262: step 10535, loss 0.521293.
Train: 2018-08-05T12:39:34.285732: step 10536, loss 0.512863.
Train: 2018-08-05T12:39:37.814193: step 10537, loss 0.496011.
Train: 2018-08-05T12:39:41.351675: step 10538, loss 0.570769.
Train: 2018-08-05T12:39:44.862594: step 10539, loss 0.537221.
Train: 2018-08-05T12:39:48.397565: step 10540, loss 0.553938.
Test: 2018-08-05T12:40:03.357101: step 10540, loss 0.548401.
Train: 2018-08-05T12:40:06.864997: step 10541, loss 0.596239.
Train: 2018-08-05T12:40:10.388431: step 10542, loss 0.528342.
Train: 2018-08-05T12:40:13.887793: step 10543, loss 0.570873.
Train: 2018-08-05T12:40:17.408228: step 10544, loss 0.562336.
Train: 2018-08-05T12:40:20.957740: step 10545, loss 0.570922.
Train: 2018-08-05T12:40:24.452090: step 10546, loss 0.622594.
Train: 2018-08-05T12:40:27.986567: step 10547, loss 0.579568.
Train: 2018-08-05T12:40:31.471905: step 10548, loss 0.631287.
Train: 2018-08-05T12:40:34.963250: step 10549, loss 0.579553.
Train: 2018-08-05T12:40:38.483688: step 10550, loss 0.570931.
Test: 2018-08-05T12:40:53.375986: step 10550, loss 0.548595.
Train: 2018-08-05T12:40:56.867851: step 10551, loss 0.656737.
Train: 2018-08-05T12:41:00.422903: step 10552, loss 0.622202.
Train: 2018-08-05T12:41:03.912250: step 10553, loss 0.528285.
Train: 2018-08-05T12:41:07.394075: step 10554, loss 0.596296.
Train: 2018-08-05T12:41:10.907978: step 10555, loss 0.553904.
Train: 2018-08-05T12:41:14.481547: step 10556, loss 0.553942.
Train: 2018-08-05T12:41:18.009508: step 10557, loss 0.478299.
Train: 2018-08-05T12:41:21.512401: step 10558, loss 0.621223.
Train: 2018-08-05T12:41:25.175720: step 10559, loss 0.629534.
Train: 2018-08-05T12:41:28.883653: step 10560, loss 0.537297.
Test: 2018-08-05T12:41:44.608062: step 10560, loss 0.548272.
Train: 2018-08-05T12:41:48.309987: step 10561, loss 0.57077.
Train: 2018-08-05T12:41:51.845441: step 10562, loss 0.687516.
Train: 2018-08-05T12:41:55.363849: step 10563, loss 0.545849.
Train: 2018-08-05T12:41:58.866239: step 10564, loss 0.537655.
Train: 2018-08-05T12:42:02.353589: step 10565, loss 0.529475.
Train: 2018-08-05T12:42:05.878031: step 10566, loss 0.570756.
Train: 2018-08-05T12:42:09.360343: step 10567, loss 0.570757.
Train: 2018-08-05T12:42:12.880786: step 10568, loss 0.603678.
Train: 2018-08-05T12:42:16.421266: step 10569, loss 0.52146.
Train: 2018-08-05T12:42:18.129880: step 10570, loss 0.667683.
Test: 2018-08-05T12:42:33.036570: step 10570, loss 0.548511.
Train: 2018-08-05T12:42:36.537951: step 10571, loss 0.562571.
Train: 2018-08-05T12:42:40.031803: step 10572, loss 0.587119.
Train: 2018-08-05T12:42:43.520656: step 10573, loss 0.587087.
Train: 2018-08-05T12:42:47.033073: step 10574, loss 0.530092.
Train: 2018-08-05T12:42:50.582593: step 10575, loss 0.611426.
Train: 2018-08-05T12:42:54.076953: step 10576, loss 0.587016.
Train: 2018-08-05T12:42:57.629470: step 10577, loss 0.603182.
Train: 2018-08-05T12:43:01.180477: step 10578, loss 0.546585.
Train: 2018-08-05T12:43:04.688365: step 10579, loss 0.651435.
Train: 2018-08-05T12:43:08.318083: step 10580, loss 0.562802.
Test: 2018-08-05T12:43:23.285522: step 10580, loss 0.549827.
Train: 2018-08-05T12:43:26.805950: step 10581, loss 0.546814.
Train: 2018-08-05T12:43:30.328395: step 10582, loss 0.514871.
Train: 2018-08-05T12:43:33.824259: step 10583, loss 0.610858.
Train: 2018-08-05T12:43:37.313598: step 10584, loss 0.594845.
Train: 2018-08-05T12:43:40.812974: step 10585, loss 0.554915.
Train: 2018-08-05T12:43:44.299326: step 10586, loss 0.538975.
Train: 2018-08-05T12:43:47.842822: step 10587, loss 0.594821.
Train: 2018-08-05T12:43:51.393324: step 10588, loss 0.538961.
Train: 2018-08-05T12:43:54.874145: step 10589, loss 0.530939.
Train: 2018-08-05T12:43:58.392061: step 10590, loss 0.570861.
Test: 2018-08-05T12:44:13.245212: step 10590, loss 0.550365.
Train: 2018-08-05T12:44:16.770154: step 10591, loss 0.554815.
Train: 2018-08-05T12:44:20.344718: step 10592, loss 0.635117.
Train: 2018-08-05T12:44:23.871171: step 10593, loss 0.530637.
Train: 2018-08-05T12:44:27.389088: step 10594, loss 0.562773.
Train: 2018-08-05T12:44:30.887925: step 10595, loss 0.498227.
Train: 2018-08-05T12:44:34.376262: step 10596, loss 0.554615.
Train: 2018-08-05T12:44:37.844570: step 10597, loss 0.465212.
Train: 2018-08-05T12:44:41.343435: step 10598, loss 0.562602.
Train: 2018-08-05T12:44:44.885927: step 10599, loss 0.537915.
Train: 2018-08-05T12:44:48.408371: step 10600, loss 0.579013.
Test: 2018-08-05T12:45:03.344207: step 10600, loss 0.548225.
Train: 2018-08-05T12:45:09.145719: step 10601, loss 0.562462.
Train: 2018-08-05T12:45:12.654625: step 10602, loss 0.487438.
Train: 2018-08-05T12:45:16.135960: step 10603, loss 0.570779.
Train: 2018-08-05T12:45:19.620286: step 10604, loss 0.503412.
Train: 2018-08-05T12:45:23.142719: step 10605, loss 0.587772.
Train: 2018-08-05T12:45:26.688221: step 10606, loss 0.562342.
Train: 2018-08-05T12:45:30.175547: step 10607, loss 0.536687.
Train: 2018-08-05T12:45:33.651857: step 10608, loss 0.527985.
Train: 2018-08-05T12:45:37.143723: step 10609, loss 0.536456.
Train: 2018-08-05T12:45:40.624539: step 10610, loss 0.571013.
Test: 2018-08-05T12:45:55.471695: step 10610, loss 0.547424.
Train: 2018-08-05T12:45:59.010171: step 10611, loss 0.51885.
Train: 2018-08-05T12:46:02.516073: step 10612, loss 0.571111.
Train: 2018-08-05T12:46:06.029981: step 10613, loss 0.536077.
Train: 2018-08-05T12:46:09.531868: step 10614, loss 0.641629.
Train: 2018-08-05T12:46:13.037263: step 10615, loss 0.535971.
Train: 2018-08-05T12:46:16.507057: step 10616, loss 0.553598.
Train: 2018-08-05T12:46:20.089158: step 10617, loss 0.544751.
Train: 2018-08-05T12:46:23.603578: step 10618, loss 0.527017.
Train: 2018-08-05T12:46:27.099941: step 10619, loss 0.544714.
Train: 2018-08-05T12:46:30.588290: step 10620, loss 0.553588.
Test: 2018-08-05T12:46:45.421872: step 10620, loss 0.548029.
Train: 2018-08-05T12:46:48.933290: step 10621, loss 0.598133.
Train: 2018-08-05T12:46:52.424139: step 10622, loss 0.544673.
Train: 2018-08-05T12:46:55.943565: step 10623, loss 0.571431.
Train: 2018-08-05T12:46:59.539192: step 10624, loss 0.544664.
Train: 2018-08-05T12:47:03.038051: step 10625, loss 0.544662.
Train: 2018-08-05T12:47:06.530901: step 10626, loss 0.580383.
Train: 2018-08-05T12:47:10.008217: step 10627, loss 0.571449.
Train: 2018-08-05T12:47:13.532671: step 10628, loss 0.598218.
Train: 2018-08-05T12:47:17.072679: step 10629, loss 0.60707.
Train: 2018-08-05T12:47:20.609646: step 10630, loss 0.624731.
Test: 2018-08-05T12:47:35.469304: step 10630, loss 0.548878.
Train: 2018-08-05T12:47:38.967173: step 10631, loss 0.53587.
Train: 2018-08-05T12:47:42.459538: step 10632, loss 0.544761.
Train: 2018-08-05T12:47:45.952402: step 10633, loss 0.509527.
Train: 2018-08-05T12:47:49.450268: step 10634, loss 0.527193.
Train: 2018-08-05T12:47:52.981739: step 10635, loss 0.553607.
Train: 2018-08-05T12:47:56.504166: step 10636, loss 0.465649.
Train: 2018-08-05T12:47:59.993508: step 10637, loss 0.676925.
Train: 2018-08-05T12:48:03.521964: step 10638, loss 0.597599.
Train: 2018-08-05T12:48:07.011313: step 10639, loss 0.553614.
Train: 2018-08-05T12:48:10.538762: step 10640, loss 0.614985.
Test: 2018-08-05T12:48:25.423496: step 10640, loss 0.547371.
Train: 2018-08-05T12:48:29.046192: step 10641, loss 0.492435.
Train: 2018-08-05T12:48:32.641827: step 10642, loss 0.571102.
Train: 2018-08-05T12:48:36.139181: step 10643, loss 0.597248.
Train: 2018-08-05T12:48:39.641568: step 10644, loss 0.527543.
Train: 2018-08-05T12:48:43.146950: step 10645, loss 0.544967.
Train: 2018-08-05T12:48:46.636286: step 10646, loss 0.631842.
Train: 2018-08-05T12:48:50.186802: step 10647, loss 0.614347.
Train: 2018-08-05T12:48:53.755366: step 10648, loss 0.501866.
Train: 2018-08-05T12:48:57.240726: step 10649, loss 0.553713.
Train: 2018-08-05T12:49:00.753650: step 10650, loss 0.579562.
Test: 2018-08-05T12:49:15.717629: step 10650, loss 0.547381.
Train: 2018-08-05T12:49:19.299742: step 10651, loss 0.60533.
Train: 2018-08-05T12:49:22.833201: step 10652, loss 0.528023.
Train: 2018-08-05T12:49:26.316536: step 10653, loss 0.570901.
Train: 2018-08-05T12:49:29.849003: step 10654, loss 0.562336.
Train: 2018-08-05T12:49:33.372452: step 10655, loss 0.553797.
Train: 2018-08-05T12:49:36.884359: step 10656, loss 0.502621.
Train: 2018-08-05T12:49:40.409324: step 10657, loss 0.5026.
Train: 2018-08-05T12:49:43.913202: step 10658, loss 0.596528.
Train: 2018-08-05T12:49:47.437622: step 10659, loss 0.57089.
Train: 2018-08-05T12:49:50.952039: step 10660, loss 0.536664.
Test: 2018-08-05T12:50:05.933002: step 10660, loss 0.548228.
Train: 2018-08-05T12:50:09.443399: step 10661, loss 0.622292.
Train: 2018-08-05T12:50:12.937764: step 10662, loss 0.656509.
Train: 2018-08-05T12:50:16.437155: step 10663, loss 0.502557.
Train: 2018-08-05T12:50:19.933533: step 10664, loss 0.579403.
Train: 2018-08-05T12:50:23.437417: step 10665, loss 0.562341.
Train: 2018-08-05T12:50:26.973399: step 10666, loss 0.545317.
Train: 2018-08-05T12:50:30.539443: step 10667, loss 0.511298.
Train: 2018-08-05T12:50:34.064406: step 10668, loss 0.630444.
Train: 2018-08-05T12:50:37.598873: step 10669, loss 0.50281.
Train: 2018-08-05T12:50:41.105771: step 10670, loss 0.536817.
Test: 2018-08-05T12:50:55.970912: step 10670, loss 0.546744.
Train: 2018-08-05T12:50:59.466788: step 10671, loss 0.426061.
Train: 2018-08-05T12:51:02.985212: step 10672, loss 0.579438.
Train: 2018-08-05T12:51:06.513164: step 10673, loss 0.528026.
Train: 2018-08-05T12:51:10.008515: step 10674, loss 0.596761.
Train: 2018-08-05T12:51:13.514903: step 10675, loss 0.527824.
Train: 2018-08-05T12:51:17.010265: step 10676, loss 0.510447.
Train: 2018-08-05T12:51:20.508114: step 10677, loss 0.518948.
Train: 2018-08-05T12:51:24.070154: step 10678, loss 0.606008.
Train: 2018-08-05T12:51:27.602604: step 10679, loss 0.553635.
Train: 2018-08-05T12:51:31.087928: step 10680, loss 0.606178.
Test: 2018-08-05T12:51:45.953169: step 10680, loss 0.547339.
Train: 2018-08-05T12:51:49.473588: step 10681, loss 0.553619.
Train: 2018-08-05T12:51:52.990002: step 10682, loss 0.615062.
Train: 2018-08-05T12:51:56.478843: step 10683, loss 0.54484.
Train: 2018-08-05T12:51:59.995256: step 10684, loss 0.54484.
Train: 2018-08-05T12:52:03.537232: step 10685, loss 0.597503.
Train: 2018-08-05T12:52:07.050632: step 10686, loss 0.56239.
Train: 2018-08-05T12:52:10.608152: step 10687, loss 0.623746.
Train: 2018-08-05T12:52:14.073920: step 10688, loss 0.553629.
Train: 2018-08-05T12:52:17.566283: step 10689, loss 0.597299.
Train: 2018-08-05T12:52:21.121821: step 10690, loss 0.544937.
Test: 2018-08-05T12:52:36.018535: step 10690, loss 0.547433.
Train: 2018-08-05T12:52:39.536463: step 10691, loss 0.527576.
Train: 2018-08-05T12:52:43.027318: step 10692, loss 0.657887.
Train: 2018-08-05T12:52:46.532705: step 10693, loss 0.648929.
Train: 2018-08-05T12:52:50.054130: step 10694, loss 0.588192.
Train: 2018-08-05T12:52:53.551991: step 10695, loss 0.528026.
Train: 2018-08-05T12:52:57.063393: step 10696, loss 0.587976.
Train: 2018-08-05T12:53:00.592346: step 10697, loss 0.562342.
Train: 2018-08-05T12:53:04.115766: step 10698, loss 0.511445.
Train: 2018-08-05T12:53:07.609630: step 10699, loss 0.562355.
Train: 2018-08-05T12:53:11.109497: step 10700, loss 0.579267.
Test: 2018-08-05T12:53:26.194740: step 10700, loss 0.548084.
Train: 2018-08-05T12:53:31.872927: step 10701, loss 0.596115.
Train: 2018-08-05T12:53:35.414418: step 10702, loss 0.587628.
Train: 2018-08-05T12:53:38.917812: step 10703, loss 0.671517.
Train: 2018-08-05T12:53:42.421208: step 10704, loss 0.554061.
Train: 2018-08-05T12:53:45.925593: step 10705, loss 0.520833.
Train: 2018-08-05T12:53:49.407935: step 10706, loss 0.562458.
Train: 2018-08-05T12:53:52.902295: step 10707, loss 0.6453.
Train: 2018-08-05T12:53:56.397656: step 10708, loss 0.529495.
Train: 2018-08-05T12:53:59.906053: step 10709, loss 0.595455.
Train: 2018-08-05T12:54:03.399420: step 10710, loss 0.50508.
Test: 2018-08-05T12:54:18.334797: step 10710, loss 0.548294.
Train: 2018-08-05T12:54:21.865764: step 10711, loss 0.537952.
Train: 2018-08-05T12:54:25.367155: step 10712, loss 0.521551.
Train: 2018-08-05T12:54:28.872553: step 10713, loss 0.529707.
Train: 2018-08-05T12:54:32.369907: step 10714, loss 0.578985.
Train: 2018-08-05T12:54:35.879816: step 10715, loss 0.570757.
Train: 2018-08-05T12:54:39.404761: step 10716, loss 0.61201.
Train: 2018-08-05T12:54:42.891099: step 10717, loss 0.521238.
Train: 2018-08-05T12:54:46.392469: step 10718, loss 0.47985.
Train: 2018-08-05T12:54:49.911887: step 10719, loss 0.52101.
Train: 2018-08-05T12:54:53.431317: step 10720, loss 0.579087.
Test: 2018-08-05T12:55:08.380204: step 10720, loss 0.547505.
Train: 2018-08-05T12:55:10.095311: step 10721, loss 0.598057.
Train: 2018-08-05T12:55:13.595207: step 10722, loss 0.520542.
Train: 2018-08-05T12:55:17.114121: step 10723, loss 0.54559.
Train: 2018-08-05T12:55:20.647081: step 10724, loss 0.604505.
Train: 2018-08-05T12:55:24.133909: step 10725, loss 0.579253.
Train: 2018-08-05T12:55:27.661351: step 10726, loss 0.587727.
Train: 2018-08-05T12:55:31.191814: step 10727, loss 0.553894.
Train: 2018-08-05T12:55:34.707229: step 10728, loss 0.562354.
Train: 2018-08-05T12:55:38.254742: step 10729, loss 0.443639.
Train: 2018-08-05T12:55:41.745089: step 10730, loss 0.494273.
Test: 2018-08-05T12:55:56.670463: step 10730, loss 0.548258.
Train: 2018-08-05T12:56:00.237521: step 10731, loss 0.647815.
Train: 2018-08-05T12:56:03.733884: step 10732, loss 0.596604.
Train: 2018-08-05T12:56:07.277387: step 10733, loss 0.656696.
Train: 2018-08-05T12:56:10.825390: step 10734, loss 0.536622.
Train: 2018-08-05T12:56:14.328257: step 10735, loss 0.588042.
Train: 2018-08-05T12:56:17.830637: step 10736, loss 0.596584.
Train: 2018-08-05T12:56:21.319490: step 10737, loss 0.562337.
Train: 2018-08-05T12:56:24.836387: step 10738, loss 0.596492.
Train: 2018-08-05T12:56:28.369845: step 10739, loss 0.51973.
Train: 2018-08-05T12:56:31.907836: step 10740, loss 0.596403.
Test: 2018-08-05T12:56:46.862800: step 10740, loss 0.547753.
Train: 2018-08-05T12:56:50.396778: step 10741, loss 0.51983.
Train: 2018-08-05T12:56:53.941784: step 10742, loss 0.502847.
Train: 2018-08-05T12:56:57.428632: step 10743, loss 0.562343.
Train: 2018-08-05T12:57:00.938537: step 10744, loss 0.562343.
Train: 2018-08-05T12:57:04.480027: step 10745, loss 0.553818.
Train: 2018-08-05T12:57:07.998947: step 10746, loss 0.639114.
Train: 2018-08-05T12:57:11.507335: step 10747, loss 0.604959.
Train: 2018-08-05T12:57:15.005707: step 10748, loss 0.570853.
Train: 2018-08-05T12:57:18.504077: step 10749, loss 0.536859.
Train: 2018-08-05T12:57:22.061109: step 10750, loss 0.579325.
Test: 2018-08-05T12:57:37.025574: step 10750, loss 0.548778.
Train: 2018-08-05T12:57:40.526944: step 10751, loss 0.553873.
Train: 2018-08-05T12:57:44.096509: step 10752, loss 0.596239.
Train: 2018-08-05T12:57:47.593385: step 10753, loss 0.604654.
Train: 2018-08-05T12:57:51.094264: step 10754, loss 0.503282.
Train: 2018-08-05T12:57:54.602667: step 10755, loss 0.553933.
Train: 2018-08-05T12:57:58.098023: step 10756, loss 0.553937.
Train: 2018-08-05T12:58:01.627475: step 10757, loss 0.587667.
Train: 2018-08-05T12:58:05.177993: step 10758, loss 0.604513.
Train: 2018-08-05T12:58:08.781145: step 10759, loss 0.612878.
Train: 2018-08-05T12:58:12.364731: step 10760, loss 0.579181.
Test: 2018-08-05T12:58:27.697616: step 10760, loss 0.548411.
Train: 2018-08-05T12:58:31.281194: step 10761, loss 0.52889.
Train: 2018-08-05T12:58:34.780070: step 10762, loss 0.528956.
Train: 2018-08-05T12:58:38.286964: step 10763, loss 0.587554.
Train: 2018-08-05T12:58:41.820928: step 10764, loss 0.595903.
Train: 2018-08-05T12:58:45.326321: step 10765, loss 0.512336.
Train: 2018-08-05T12:58:48.890865: step 10766, loss 0.55407.
Train: 2018-08-05T12:58:52.416802: step 10767, loss 0.579122.
Train: 2018-08-05T12:58:55.921698: step 10768, loss 0.629239.
Train: 2018-08-05T12:58:59.458165: step 10769, loss 0.5374.
Train: 2018-08-05T12:59:02.970570: step 10770, loss 0.52908.
Test: 2018-08-05T12:59:17.887382: step 10770, loss 0.549265.
Train: 2018-08-05T12:59:21.422363: step 10771, loss 0.562426.
Train: 2018-08-05T12:59:24.921255: step 10772, loss 0.604144.
Train: 2018-08-05T12:59:28.446710: step 10773, loss 0.587448.
Train: 2018-08-05T12:59:31.939071: step 10774, loss 0.545762.
Train: 2018-08-05T12:59:35.477553: step 10775, loss 0.5541.
Train: 2018-08-05T12:59:39.026065: step 10776, loss 0.545766.
Train: 2018-08-05T12:59:42.535472: step 10777, loss 0.604116.
Train: 2018-08-05T12:59:46.069460: step 10778, loss 0.56243.
Train: 2018-08-05T12:59:49.582369: step 10779, loss 0.545763.
Train: 2018-08-05T12:59:53.096267: step 10780, loss 0.537417.
Test: 2018-08-05T13:00:08.144437: step 10780, loss 0.5481.
Train: 2018-08-05T13:00:11.643802: step 10781, loss 0.570767.
Train: 2018-08-05T13:00:15.167250: step 10782, loss 0.595826.
Train: 2018-08-05T13:00:18.696692: step 10783, loss 0.587476.
Train: 2018-08-05T13:00:22.182526: step 10784, loss 0.570769.
Train: 2018-08-05T13:00:25.700465: step 10785, loss 0.529034.
Train: 2018-08-05T13:00:29.228915: step 10786, loss 0.57912.
Train: 2018-08-05T13:00:32.723792: step 10787, loss 0.579122.
Train: 2018-08-05T13:00:36.280809: step 10788, loss 0.512308.
Train: 2018-08-05T13:00:39.812258: step 10789, loss 0.537328.
Train: 2018-08-05T13:00:43.307133: step 10790, loss 0.545651.
Test: 2018-08-05T13:00:58.308669: step 10790, loss 0.547217.
Train: 2018-08-05T13:01:01.833620: step 10791, loss 0.553999.
Train: 2018-08-05T13:01:05.351044: step 10792, loss 0.58761.
Train: 2018-08-05T13:01:08.866472: step 10793, loss 0.520271.
Train: 2018-08-05T13:01:12.408960: step 10794, loss 0.579246.
Train: 2018-08-05T13:01:15.939419: step 10795, loss 0.511636.
Train: 2018-08-05T13:01:19.443294: step 10796, loss 0.528445.
Train: 2018-08-05T13:01:22.934135: step 10797, loss 0.536833.
Train: 2018-08-05T13:01:26.460571: step 10798, loss 0.545273.
Train: 2018-08-05T13:01:29.977988: step 10799, loss 0.553773.
Train: 2018-08-05T13:01:33.526496: step 10800, loss 0.545154.
Test: 2018-08-05T13:01:48.495955: step 10800, loss 0.549133.
Train: 2018-08-05T13:01:54.179189: step 10801, loss 0.527858.
Train: 2018-08-05T13:01:57.705631: step 10802, loss 0.579659.
Train: 2018-08-05T13:02:01.221560: step 10803, loss 0.553675.
Train: 2018-08-05T13:02:04.833739: step 10804, loss 0.54496.
Train: 2018-08-05T13:02:08.424365: step 10805, loss 0.553644.
Train: 2018-08-05T13:02:12.053575: step 10806, loss 0.579857.
Train: 2018-08-05T13:02:15.720369: step 10807, loss 0.571128.
Train: 2018-08-05T13:02:19.274902: step 10808, loss 0.53613.
Train: 2018-08-05T13:02:22.762754: step 10809, loss 0.588763.
Train: 2018-08-05T13:02:26.265151: step 10810, loss 0.579987.
Test: 2018-08-05T13:02:41.295280: step 10810, loss 0.548126.
Train: 2018-08-05T13:02:44.778634: step 10811, loss 0.544822.
Train: 2018-08-05T13:02:48.303088: step 10812, loss 0.580027.
Train: 2018-08-05T13:02:51.822524: step 10813, loss 0.500854.
Train: 2018-08-05T13:02:55.305357: step 10814, loss 0.650408.
Train: 2018-08-05T13:02:58.799215: step 10815, loss 0.500875.
Train: 2018-08-05T13:03:02.284586: step 10816, loss 0.571191.
Train: 2018-08-05T13:03:05.794491: step 10817, loss 0.615132.
Train: 2018-08-05T13:03:09.328456: step 10818, loss 0.579945.
Train: 2018-08-05T13:03:12.910043: step 10819, loss 0.509817.
Train: 2018-08-05T13:03:16.404407: step 10820, loss 0.597404.
Test: 2018-08-05T13:03:31.351297: step 10820, loss 0.547369.
Train: 2018-08-05T13:03:34.852678: step 10821, loss 0.50117.
Train: 2018-08-05T13:03:38.348055: step 10822, loss 0.597344.
Train: 2018-08-05T13:03:41.847945: step 10823, loss 0.579839.
Train: 2018-08-05T13:03:45.407986: step 10824, loss 0.518751.
Train: 2018-08-05T13:03:48.926412: step 10825, loss 0.588522.
Train: 2018-08-05T13:03:52.445838: step 10826, loss 0.579782.
Train: 2018-08-05T13:03:55.970272: step 10827, loss 0.562356.
Train: 2018-08-05T13:03:59.483712: step 10828, loss 0.492851.
Train: 2018-08-05T13:04:02.989084: step 10829, loss 0.605799.
Train: 2018-08-05T13:04:06.531587: step 10830, loss 0.458165.
Test: 2018-08-05T13:04:21.432913: step 10830, loss 0.549235.
Train: 2018-08-05T13:04:25.016021: step 10831, loss 0.588436.
Train: 2018-08-05T13:04:28.562522: step 10832, loss 0.61455.
Train: 2018-08-05T13:04:32.066922: step 10833, loss 0.597128.
Train: 2018-08-05T13:04:35.609418: step 10834, loss 0.518943.
Train: 2018-08-05T13:04:39.097259: step 10835, loss 0.631769.
Train: 2018-08-05T13:04:42.593131: step 10836, loss 0.571005.
Train: 2018-08-05T13:04:46.154168: step 10837, loss 0.579628.
Train: 2018-08-05T13:04:49.647007: step 10838, loss 0.570962.
Train: 2018-08-05T13:04:53.142372: step 10839, loss 0.502099.
Train: 2018-08-05T13:04:56.661797: step 10840, loss 0.579533.
Test: 2018-08-05T13:05:11.570056: step 10840, loss 0.548188.
Train: 2018-08-05T13:05:15.071938: step 10841, loss 0.579513.
Train: 2018-08-05T13:05:18.606407: step 10842, loss 0.562335.
Train: 2018-08-05T13:05:22.125825: step 10843, loss 0.579468.
Train: 2018-08-05T13:05:25.657809: step 10844, loss 0.51957.
Train: 2018-08-05T13:05:29.146655: step 10845, loss 0.562337.
Train: 2018-08-05T13:05:32.651041: step 10846, loss 0.502517.
Train: 2018-08-05T13:05:36.159959: step 10847, loss 0.579443.
Train: 2018-08-05T13:05:39.681877: step 10848, loss 0.519549.
Train: 2018-08-05T13:05:43.202313: step 10849, loss 0.596609.
Train: 2018-08-05T13:05:46.685647: step 10850, loss 0.562335.
Test: 2018-08-05T13:06:01.591435: step 10850, loss 0.547816.
Train: 2018-08-05T13:06:05.089820: step 10851, loss 0.502309.
Train: 2018-08-05T13:06:08.623299: step 10852, loss 0.562335.
Train: 2018-08-05T13:06:12.175821: step 10853, loss 0.519331.
Train: 2018-08-05T13:06:15.673675: step 10854, loss 0.596814.
Train: 2018-08-05T13:06:19.175063: step 10855, loss 0.631372.
Train: 2018-08-05T13:06:22.715564: step 10856, loss 0.562338.
Train: 2018-08-05T13:06:26.198909: step 10857, loss 0.570958.
Train: 2018-08-05T13:06:29.738405: step 10858, loss 0.527877.
Train: 2018-08-05T13:06:33.225745: step 10859, loss 0.55372.
Train: 2018-08-05T13:06:36.727127: step 10860, loss 0.614044.
Test: 2018-08-05T13:06:51.735145: step 10860, loss 0.547957.
Train: 2018-08-05T13:06:55.317239: step 10861, loss 0.49346.
Train: 2018-08-05T13:06:58.837663: step 10862, loss 0.63987.
Train: 2018-08-05T13:07:02.347064: step 10863, loss 0.545126.
Train: 2018-08-05T13:07:05.858471: step 10864, loss 0.476354.
Train: 2018-08-05T13:07:09.343800: step 10865, loss 0.510661.
Train: 2018-08-05T13:07:12.865227: step 10866, loss 0.519174.
Train: 2018-08-05T13:07:16.388661: step 10867, loss 0.544461.
Train: 2018-08-05T13:07:19.912092: step 10868, loss 0.527319.
Train: 2018-08-05T13:07:23.414481: step 10869, loss 0.625886.
Train: 2018-08-05T13:07:26.915348: step 10870, loss 0.570446.
Test: 2018-08-05T13:07:41.972498: step 10870, loss 0.549688.
Train: 2018-08-05T13:07:45.515498: step 10871, loss 0.597207.
Train: 2018-08-05T13:07:47.227123: step 10872, loss 0.485192.
Train: 2018-08-05T13:07:50.765617: step 10873, loss 0.589293.
Train: 2018-08-05T13:07:54.288052: step 10874, loss 0.553623.
Train: 2018-08-05T13:07:57.801458: step 10875, loss 0.535131.
Train: 2018-08-05T13:08:01.303851: step 10876, loss 0.634445.
Train: 2018-08-05T13:08:04.878413: step 10877, loss 0.563667.
Train: 2018-08-05T13:08:08.383808: step 10878, loss 0.608147.
Train: 2018-08-05T13:08:11.954395: step 10879, loss 0.624323.
Train: 2018-08-05T13:08:15.481856: step 10880, loss 0.5807.
Test: 2018-08-05T13:08:30.445785: step 10880, loss 0.548612.
Train: 2018-08-05T13:08:34.054447: step 10881, loss 0.622815.
Train: 2018-08-05T13:08:37.559839: step 10882, loss 0.596394.
Train: 2018-08-05T13:08:41.073750: step 10883, loss 0.579635.
Train: 2018-08-05T13:08:44.607718: step 10884, loss 0.545289.
Train: 2018-08-05T13:08:48.167257: step 10885, loss 0.520009.
Train: 2018-08-05T13:08:51.704242: step 10886, loss 0.553922.
Train: 2018-08-05T13:08:55.187094: step 10887, loss 0.494975.
Train: 2018-08-05T13:08:58.680433: step 10888, loss 0.570796.
Train: 2018-08-05T13:09:02.179800: step 10889, loss 0.503462.
Train: 2018-08-05T13:09:05.687206: step 10890, loss 0.604494.
Test: 2018-08-05T13:09:20.632049: step 10890, loss 0.5485.
Train: 2018-08-05T13:09:24.206606: step 10891, loss 0.62134.
Train: 2018-08-05T13:09:27.768651: step 10892, loss 0.495077.
Train: 2018-08-05T13:09:31.276546: step 10893, loss 0.511881.
Train: 2018-08-05T13:09:34.773412: step 10894, loss 0.545513.
Train: 2018-08-05T13:09:38.262757: step 10895, loss 0.638368.
Train: 2018-08-05T13:09:41.767149: step 10896, loss 0.503249.
Train: 2018-08-05T13:09:45.287580: step 10897, loss 0.553903.
Train: 2018-08-05T13:09:48.829052: step 10898, loss 0.655516.
Train: 2018-08-05T13:09:52.320394: step 10899, loss 0.646991.
Train: 2018-08-05T13:09:55.848832: step 10900, loss 0.511708.
Test: 2018-08-05T13:10:10.744094: step 10900, loss 0.548089.
Train: 2018-08-05T13:10:16.407725: step 10901, loss 0.570803.
Train: 2018-08-05T13:10:19.920137: step 10902, loss 0.58765.
Train: 2018-08-05T13:10:23.450096: step 10903, loss 0.629687.
Train: 2018-08-05T13:10:27.014158: step 10904, loss 0.570783.
Train: 2018-08-05T13:10:30.510520: step 10905, loss 0.570775.
Train: 2018-08-05T13:10:33.988828: step 10906, loss 0.55407.
Train: 2018-08-05T13:10:37.510766: step 10907, loss 0.570764.
Train: 2018-08-05T13:10:40.995109: step 10908, loss 0.520846.
Train: 2018-08-05T13:10:44.513038: step 10909, loss 0.595716.
Train: 2018-08-05T13:10:48.055037: step 10910, loss 0.570747.
Test: 2018-08-05T13:11:02.981892: step 10910, loss 0.548013.
Train: 2018-08-05T13:11:06.477275: step 10911, loss 0.496011.
Train: 2018-08-05T13:11:09.980674: step 10912, loss 0.587421.
Train: 2018-08-05T13:11:13.494088: step 10913, loss 0.545609.
Train: 2018-08-05T13:11:16.969405: step 10914, loss 0.603672.
Train: 2018-08-05T13:11:20.506870: step 10915, loss 0.494617.
Train: 2018-08-05T13:11:24.084443: step 10916, loss 0.536811.
Train: 2018-08-05T13:11:27.595858: step 10917, loss 0.54272.
Train: 2018-08-05T13:11:31.105276: step 10918, loss 0.562047.
Train: 2018-08-05T13:11:34.607636: step 10919, loss 0.606746.
Train: 2018-08-05T13:11:38.098494: step 10920, loss 0.536408.
Test: 2018-08-05T13:11:53.081036: step 10920, loss 0.547585.
Train: 2018-08-05T13:11:56.619036: step 10921, loss 0.537281.
Train: 2018-08-05T13:12:00.138468: step 10922, loss 0.563089.
Train: 2018-08-05T13:12:03.661409: step 10923, loss 0.519199.
Train: 2018-08-05T13:12:07.171804: step 10924, loss 0.536664.
Train: 2018-08-05T13:12:10.688707: step 10925, loss 0.545436.
Train: 2018-08-05T13:12:14.178062: step 10926, loss 0.55361.
Train: 2018-08-05T13:12:17.682976: step 10927, loss 0.486443.
Train: 2018-08-05T13:12:21.209922: step 10928, loss 0.561506.
Train: 2018-08-05T13:12:24.749415: step 10929, loss 0.562129.
Train: 2018-08-05T13:12:28.230233: step 10930, loss 0.587806.
Test: 2018-08-05T13:12:43.112480: step 10930, loss 0.549247.
Train: 2018-08-05T13:12:46.640937: step 10931, loss 0.562264.
Train: 2018-08-05T13:12:50.179415: step 10932, loss 0.534631.
Train: 2018-08-05T13:12:53.670779: step 10933, loss 0.509853.
Train: 2018-08-05T13:12:57.196220: step 10934, loss 0.52782.
Train: 2018-08-05T13:13:00.760763: step 10935, loss 0.537139.
Train: 2018-08-05T13:13:04.263650: step 10936, loss 0.56277.
Train: 2018-08-05T13:13:07.812645: step 10937, loss 0.517781.
Train: 2018-08-05T13:13:11.298980: step 10938, loss 0.482102.
Train: 2018-08-05T13:13:14.827952: step 10939, loss 0.594656.
Train: 2018-08-05T13:13:18.365921: step 10940, loss 0.57358.
Test: 2018-08-05T13:13:33.327893: step 10940, loss 0.547753.
Train: 2018-08-05T13:13:36.831280: step 10941, loss 0.62561.
Train: 2018-08-05T13:13:40.375774: step 10942, loss 0.560824.
Train: 2018-08-05T13:13:43.912755: step 10943, loss 0.535334.
Train: 2018-08-05T13:13:47.445208: step 10944, loss 0.579591.
Train: 2018-08-05T13:13:51.029796: step 10945, loss 0.606844.
Train: 2018-08-05T13:13:54.550744: step 10946, loss 0.554065.
Train: 2018-08-05T13:13:58.073686: step 10947, loss 0.517737.
Train: 2018-08-05T13:14:01.612670: step 10948, loss 0.517957.
Train: 2018-08-05T13:14:05.132600: step 10949, loss 0.626278.
Train: 2018-08-05T13:14:08.634981: step 10950, loss 0.48227.
Test: 2018-08-05T13:14:23.613946: step 10950, loss 0.548459.
Train: 2018-08-05T13:14:27.146911: step 10951, loss 0.563184.
Train: 2018-08-05T13:14:30.712460: step 10952, loss 0.527354.
Train: 2018-08-05T13:14:34.241926: step 10953, loss 0.606824.
Train: 2018-08-05T13:14:37.737287: step 10954, loss 0.545052.
Train: 2018-08-05T13:14:41.229140: step 10955, loss 0.483181.
Train: 2018-08-05T13:14:44.751064: step 10956, loss 0.544857.
Train: 2018-08-05T13:14:48.251948: step 10957, loss 0.571114.
Train: 2018-08-05T13:14:51.772885: step 10958, loss 0.562345.
Train: 2018-08-05T13:14:55.293323: step 10959, loss 0.571295.
Train: 2018-08-05T13:14:58.819768: step 10960, loss 0.633037.
Test: 2018-08-05T13:15:13.761638: step 10960, loss 0.547097.
Train: 2018-08-05T13:15:17.300118: step 10961, loss 0.536008.
Train: 2018-08-05T13:15:20.826578: step 10962, loss 0.57997.
Train: 2018-08-05T13:15:24.345999: step 10963, loss 0.60632.
Train: 2018-08-05T13:15:27.933134: step 10964, loss 0.658753.
Train: 2018-08-05T13:15:31.465595: step 10965, loss 0.52747.
Train: 2018-08-05T13:15:34.966492: step 10966, loss 0.597142.
Train: 2018-08-05T13:15:38.490432: step 10967, loss 0.571011.
Train: 2018-08-05T13:15:42.033420: step 10968, loss 0.545069.
Train: 2018-08-05T13:15:45.542315: step 10969, loss 0.553726.
Train: 2018-08-05T13:15:49.066762: step 10970, loss 0.510799.
Test: 2018-08-05T13:16:04.025162: step 10970, loss 0.547216.
Train: 2018-08-05T13:16:07.542089: step 10971, loss 0.536596.
Train: 2018-08-05T13:16:11.072564: step 10972, loss 0.639515.
Train: 2018-08-05T13:16:14.579971: step 10973, loss 0.553779.
Train: 2018-08-05T13:16:18.073320: step 10974, loss 0.528167.
Train: 2018-08-05T13:16:21.602797: step 10975, loss 0.622086.
Train: 2018-08-05T13:16:25.107207: step 10976, loss 0.536787.
Train: 2018-08-05T13:16:28.608592: step 10977, loss 0.528315.
Train: 2018-08-05T13:16:32.157106: step 10978, loss 0.511321.
Train: 2018-08-05T13:16:35.652475: step 10979, loss 0.587875.
Train: 2018-08-05T13:16:39.130792: step 10980, loss 0.587879.
Test: 2018-08-05T13:16:54.150863: step 10980, loss 0.548721.
Train: 2018-08-05T13:16:57.676827: step 10981, loss 0.5198.
Train: 2018-08-05T13:17:01.191231: step 10982, loss 0.536801.
Train: 2018-08-05T13:17:04.779842: step 10983, loss 0.630525.
Train: 2018-08-05T13:17:08.297262: step 10984, loss 0.57086.
Train: 2018-08-05T13:17:11.767582: step 10985, loss 0.519774.
Train: 2018-08-05T13:17:15.269967: step 10986, loss 0.613444.
Train: 2018-08-05T13:17:18.779379: step 10987, loss 0.553832.
Train: 2018-08-05T13:17:22.277747: step 10988, loss 0.511304.
Train: 2018-08-05T13:17:25.780133: step 10989, loss 0.579368.
Train: 2018-08-05T13:17:29.303599: step 10990, loss 0.536797.
Test: 2018-08-05T13:17:44.261559: step 10990, loss 0.549481.
Train: 2018-08-05T13:17:47.766942: step 10991, loss 0.570863.
Train: 2018-08-05T13:17:51.280348: step 10992, loss 0.528233.
Train: 2018-08-05T13:17:54.799785: step 10993, loss 0.519651.
Train: 2018-08-05T13:17:58.294149: step 10994, loss 0.605111.
Train: 2018-08-05T13:18:01.865723: step 10995, loss 0.519522.
Train: 2018-08-05T13:18:05.391664: step 10996, loss 0.605223.
Train: 2018-08-05T13:18:08.910115: step 10997, loss 0.596666.
Train: 2018-08-05T13:18:12.426011: step 10998, loss 0.579497.
Train: 2018-08-05T13:18:15.941445: step 10999, loss 0.579487.
Train: 2018-08-05T13:18:19.444838: step 11000, loss 0.545199.
Test: 2018-08-05T13:18:34.407283: step 11000, loss 0.547245.
Train: 2018-08-05T13:18:40.091495: step 11001, loss 0.596591.
Train: 2018-08-05T13:18:43.616438: step 11002, loss 0.596551.
Train: 2018-08-05T13:18:47.083721: step 11003, loss 0.664795.
Train: 2018-08-05T13:18:50.557528: step 11004, loss 0.545334.
Train: 2018-08-05T13:18:54.042383: step 11005, loss 0.553873.
Train: 2018-08-05T13:18:57.514675: step 11006, loss 0.528532.
Train: 2018-08-05T13:19:01.032083: step 11007, loss 0.646804.
Train: 2018-08-05T13:19:04.547498: step 11008, loss 0.553961.
Train: 2018-08-05T13:19:08.050898: step 11009, loss 0.553995.
Train: 2018-08-05T13:19:11.533747: step 11010, loss 0.495383.
Test: 2018-08-05T13:19:26.479621: step 11010, loss 0.54803.
Train: 2018-08-05T13:19:30.024629: step 11011, loss 0.562402.
Train: 2018-08-05T13:19:33.525998: step 11012, loss 0.579149.
Train: 2018-08-05T13:19:37.124630: step 11013, loss 0.554035.
Train: 2018-08-05T13:19:40.684667: step 11014, loss 0.545668.
Train: 2018-08-05T13:19:44.193563: step 11015, loss 0.537287.
Train: 2018-08-05T13:19:47.705981: step 11016, loss 0.554017.
Train: 2018-08-05T13:19:51.203353: step 11017, loss 0.595955.
Train: 2018-08-05T13:19:54.689692: step 11018, loss 0.545602.
Train: 2018-08-05T13:19:58.221159: step 11019, loss 0.545584.
Train: 2018-08-05T13:20:01.764650: step 11020, loss 0.596025.
Test: 2018-08-05T13:20:16.665478: step 11020, loss 0.548713.
Train: 2018-08-05T13:20:20.182908: step 11021, loss 0.503478.
Train: 2018-08-05T13:20:23.723396: step 11022, loss 0.562371.
Train: 2018-08-05T13:20:25.464092: step 11023, loss 0.50833.
Train: 2018-08-05T13:20:28.965481: step 11024, loss 0.587754.
Train: 2018-08-05T13:20:32.485416: step 11025, loss 0.613245.
Train: 2018-08-05T13:20:36.019893: step 11026, loss 0.536886.
Train: 2018-08-05T13:20:39.557351: step 11027, loss 0.536853.
Train: 2018-08-05T13:20:43.061239: step 11028, loss 0.596389.
Train: 2018-08-05T13:20:46.574659: step 11029, loss 0.630484.
Train: 2018-08-05T13:20:50.121162: step 11030, loss 0.570854.
Test: 2018-08-05T13:21:05.070578: step 11030, loss 0.547165.
Train: 2018-08-05T13:21:08.584482: step 11031, loss 0.579351.
Train: 2018-08-05T13:21:12.154042: step 11032, loss 0.553854.
Train: 2018-08-05T13:21:15.666950: step 11033, loss 0.562349.
Train: 2018-08-05T13:21:19.177357: step 11034, loss 0.60475.
Train: 2018-08-05T13:21:22.665198: step 11035, loss 0.553889.
Train: 2018-08-05T13:21:26.162062: step 11036, loss 0.604642.
Train: 2018-08-05T13:21:29.688511: step 11037, loss 0.562366.
Train: 2018-08-05T13:21:33.207445: step 11038, loss 0.638204.
Train: 2018-08-05T13:21:36.705315: step 11039, loss 0.520392.
Train: 2018-08-05T13:21:40.225770: step 11040, loss 0.554013.
Test: 2018-08-05T13:21:55.126043: step 11040, loss 0.548614.
Train: 2018-08-05T13:21:58.661023: step 11041, loss 0.545659.
Train: 2018-08-05T13:22:02.167426: step 11042, loss 0.528945.
Train: 2018-08-05T13:22:05.657781: step 11043, loss 0.595875.
Train: 2018-08-05T13:22:09.176221: step 11044, loss 0.47878.
Train: 2018-08-05T13:22:12.706667: step 11045, loss 0.587529.
Train: 2018-08-05T13:22:16.220072: step 11046, loss 0.528862.
Train: 2018-08-05T13:22:19.728974: step 11047, loss 0.528797.
Train: 2018-08-05T13:22:23.241874: step 11048, loss 0.511873.
Train: 2018-08-05T13:22:26.754290: step 11049, loss 0.604591.
Train: 2018-08-05T13:22:30.282754: step 11050, loss 0.545431.
Test: 2018-08-05T13:22:45.325943: step 11050, loss 0.548185.
Train: 2018-08-05T13:22:48.836348: step 11051, loss 0.562351.
Train: 2018-08-05T13:22:52.367799: step 11052, loss 0.485846.
Train: 2018-08-05T13:22:55.878705: step 11053, loss 0.57087.
Train: 2018-08-05T13:22:59.402148: step 11054, loss 0.562336.
Train: 2018-08-05T13:23:02.921581: step 11055, loss 0.596652.
Train: 2018-08-05T13:23:06.501168: step 11056, loss 0.553741.
Train: 2018-08-05T13:23:10.059197: step 11057, loss 0.484866.
Train: 2018-08-05T13:23:13.559077: step 11058, loss 0.562339.
Train: 2018-08-05T13:23:17.097554: step 11059, loss 0.562344.
Train: 2018-08-05T13:23:20.625500: step 11060, loss 0.544992.
Test: 2018-08-05T13:23:35.594458: step 11060, loss 0.546825.
Train: 2018-08-05T13:23:39.092818: step 11061, loss 0.553656.
Train: 2018-08-05T13:23:42.668899: step 11062, loss 0.597242.
Train: 2018-08-05T13:23:46.201360: step 11063, loss 0.536178.
Train: 2018-08-05T13:23:49.702228: step 11064, loss 0.614834.
Train: 2018-08-05T13:23:53.235191: step 11065, loss 0.562375.
Train: 2018-08-05T13:23:56.749603: step 11066, loss 0.597355.
Train: 2018-08-05T13:24:00.240958: step 11067, loss 0.623537.
Train: 2018-08-05T13:24:03.802482: step 11068, loss 0.527486.
Train: 2018-08-05T13:24:07.342457: step 11069, loss 0.536235.
Train: 2018-08-05T13:24:10.847344: step 11070, loss 0.536251.
Test: 2018-08-05T13:24:25.782221: step 11070, loss 0.547225.
Train: 2018-08-05T13:24:29.308680: step 11071, loss 0.597157.
Train: 2018-08-05T13:24:32.810564: step 11072, loss 0.54497.
Train: 2018-08-05T13:24:36.335502: step 11073, loss 0.631842.
Train: 2018-08-05T13:24:39.879997: step 11074, loss 0.536342.
Train: 2018-08-05T13:24:43.406446: step 11075, loss 0.501752.
Train: 2018-08-05T13:24:46.906831: step 11076, loss 0.527722.
Train: 2018-08-05T13:24:50.411729: step 11077, loss 0.536362.
Train: 2018-08-05T13:24:53.934172: step 11078, loss 0.527668.
Train: 2018-08-05T13:24:57.442074: step 11079, loss 0.649186.
Train: 2018-08-05T13:25:01.017144: step 11080, loss 0.597066.
Test: 2018-08-05T13:25:15.965070: step 11080, loss 0.54767.
Train: 2018-08-05T13:25:19.498518: step 11081, loss 0.571015.
Train: 2018-08-05T13:25:23.006414: step 11082, loss 0.493092.
Train: 2018-08-05T13:25:26.534371: step 11083, loss 0.527712.
Train: 2018-08-05T13:25:30.033756: step 11084, loss 0.58834.
Train: 2018-08-05T13:25:33.559202: step 11085, loss 0.519017.
Train: 2018-08-05T13:25:37.131286: step 11086, loss 0.536326.
Train: 2018-08-05T13:25:40.687814: step 11087, loss 0.562351.
Train: 2018-08-05T13:25:44.197219: step 11088, loss 0.553659.
Train: 2018-08-05T13:25:47.720642: step 11089, loss 0.544949.
Train: 2018-08-05T13:25:51.217500: step 11090, loss 0.562362.
Test: 2018-08-05T13:26:06.268673: step 11090, loss 0.547193.
Train: 2018-08-05T13:26:09.784575: step 11091, loss 0.614709.
Train: 2018-08-05T13:26:13.317060: step 11092, loss 0.527478.
Train: 2018-08-05T13:26:16.861058: step 11093, loss 0.579815.
Train: 2018-08-05T13:26:20.358902: step 11094, loss 0.614707.
Train: 2018-08-05T13:26:23.883346: step 11095, loss 0.536224.
Train: 2018-08-05T13:26:27.382734: step 11096, loss 0.527536.
Train: 2018-08-05T13:26:30.887115: step 11097, loss 0.632.
Train: 2018-08-05T13:26:34.428630: step 11098, loss 0.649268.
Train: 2018-08-05T13:26:38.007700: step 11099, loss 0.553682.
Train: 2018-08-05T13:26:41.521608: step 11100, loss 0.545065.
Test: 2018-08-05T13:26:56.453938: step 11100, loss 0.549135.
Train: 2018-08-05T13:27:02.236390: step 11101, loss 0.562337.
Train: 2018-08-05T13:27:05.752289: step 11102, loss 0.570934.
Train: 2018-08-05T13:27:09.227605: step 11103, loss 0.579495.
Train: 2018-08-05T13:27:12.749053: step 11104, loss 0.528095.
Train: 2018-08-05T13:27:16.277497: step 11105, loss 0.536691.
Train: 2018-08-05T13:27:19.821492: step 11106, loss 0.579423.
Train: 2018-08-05T13:27:23.310333: step 11107, loss 0.570873.
Train: 2018-08-05T13:27:26.812696: step 11108, loss 0.570865.
Train: 2018-08-05T13:27:30.348663: step 11109, loss 0.528282.
Train: 2018-08-05T13:27:33.868094: step 11110, loss 0.596393.
Test: 2018-08-05T13:27:48.757376: step 11110, loss 0.549316.
Train: 2018-08-05T13:27:52.304877: step 11111, loss 0.528327.
Train: 2018-08-05T13:27:55.824313: step 11112, loss 0.519828.
Train: 2018-08-05T13:27:59.362294: step 11113, loss 0.570854.
Train: 2018-08-05T13:28:02.886239: step 11114, loss 0.60492.
Train: 2018-08-05T13:28:06.402653: step 11115, loss 0.579367.
Train: 2018-08-05T13:28:09.914060: step 11116, loss 0.536825.
Train: 2018-08-05T13:28:13.462571: step 11117, loss 0.545333.
Train: 2018-08-05T13:28:16.997033: step 11118, loss 0.494274.
Train: 2018-08-05T13:28:20.569610: step 11119, loss 0.477098.
Train: 2018-08-05T13:28:24.094051: step 11120, loss 0.570891.
Test: 2018-08-05T13:28:39.135253: step 11120, loss 0.547809.
Train: 2018-08-05T13:28:42.642643: step 11121, loss 0.605233.
Train: 2018-08-05T13:28:46.159584: step 11122, loss 0.527958.
Train: 2018-08-05T13:28:49.791321: step 11123, loss 0.536496.
Train: 2018-08-05T13:28:53.317771: step 11124, loss 0.57961.
Train: 2018-08-05T13:28:56.840210: step 11125, loss 0.562342.
Train: 2018-08-05T13:29:00.385706: step 11126, loss 0.57101.
Train: 2018-08-05T13:29:03.911140: step 11127, loss 0.510297.
Train: 2018-08-05T13:29:07.448613: step 11128, loss 0.544967.
Train: 2018-08-05T13:29:10.972051: step 11129, loss 0.56236.
Train: 2018-08-05T13:29:14.568678: step 11130, loss 0.544911.
Test: 2018-08-05T13:29:29.533108: step 11130, loss 0.545959.
Train: 2018-08-05T13:29:33.082604: step 11131, loss 0.571119.
Train: 2018-08-05T13:29:36.622562: step 11132, loss 0.553624.
Train: 2018-08-05T13:29:40.116906: step 11133, loss 0.562388.
Train: 2018-08-05T13:29:43.630336: step 11134, loss 0.562393.
Train: 2018-08-05T13:29:47.163295: step 11135, loss 0.562397.
Train: 2018-08-05T13:29:50.693246: step 11136, loss 0.492077.
Train: 2018-08-05T13:29:54.177584: step 11137, loss 0.580025.
Train: 2018-08-05T13:29:57.679965: step 11138, loss 0.606504.
Train: 2018-08-05T13:30:01.180339: step 11139, loss 0.597686.
Train: 2018-08-05T13:30:04.781483: step 11140, loss 0.571222.
Test: 2018-08-05T13:30:19.778036: step 11140, loss 0.548319.
Train: 2018-08-05T13:30:23.321539: step 11141, loss 0.518413.
Train: 2018-08-05T13:30:26.863014: step 11142, loss 0.536016.
Train: 2018-08-05T13:30:30.397483: step 11143, loss 0.597596.
Train: 2018-08-05T13:30:33.945981: step 11144, loss 0.571192.
Train: 2018-08-05T13:30:37.466424: step 11145, loss 0.659003.
Train: 2018-08-05T13:30:40.976330: step 11146, loss 0.509846.
Train: 2018-08-05T13:30:44.502792: step 11147, loss 0.579854.
Train: 2018-08-05T13:30:48.108955: step 11148, loss 0.527476.
Train: 2018-08-05T13:30:51.618368: step 11149, loss 0.588495.
Train: 2018-08-05T13:30:55.143312: step 11150, loss 0.518874.
Test: 2018-08-05T13:31:10.166390: step 11150, loss 0.54924.
Train: 2018-08-05T13:31:13.713872: step 11151, loss 0.614489.
Train: 2018-08-05T13:31:17.275420: step 11152, loss 0.57102.
Train: 2018-08-05T13:31:20.804875: step 11153, loss 0.61428.
Train: 2018-08-05T13:31:24.337332: step 11154, loss 0.527816.
Train: 2018-08-05T13:31:27.893346: step 11155, loss 0.579563.
Train: 2018-08-05T13:31:31.404756: step 11156, loss 0.588119.
Train: 2018-08-05T13:31:34.910637: step 11157, loss 0.605198.
Train: 2018-08-05T13:31:38.415026: step 11158, loss 0.553793.
Train: 2018-08-05T13:31:42.002629: step 11159, loss 0.545299.
Train: 2018-08-05T13:31:45.549148: step 11160, loss 0.596355.
Test: 2018-08-05T13:32:00.481061: step 11160, loss 0.548384.
Train: 2018-08-05T13:32:04.019551: step 11161, loss 0.579311.
Train: 2018-08-05T13:32:07.554020: step 11162, loss 0.545446.
Train: 2018-08-05T13:32:11.056412: step 11163, loss 0.570806.
Train: 2018-08-05T13:32:14.572845: step 11164, loss 0.604489.
Train: 2018-08-05T13:32:18.107811: step 11165, loss 0.570787.
Train: 2018-08-05T13:32:21.644781: step 11166, loss 0.512123.
Train: 2018-08-05T13:32:25.153671: step 11167, loss 0.562405.
Train: 2018-08-05T13:32:28.661573: step 11168, loss 0.528954.
Train: 2018-08-05T13:32:32.150410: step 11169, loss 0.537314.
Train: 2018-08-05T13:32:35.684885: step 11170, loss 0.604262.
Test: 2018-08-05T13:32:50.759627: step 11170, loss 0.549581.
Train: 2018-08-05T13:32:54.298625: step 11171, loss 0.562405.
Train: 2018-08-05T13:32:57.849153: step 11172, loss 0.528924.
Train: 2018-08-05T13:33:01.342495: step 11173, loss 0.5624.
Train: 2018-08-05T13:33:03.062111: step 11174, loss 0.455076.
Train: 2018-08-05T13:33:06.556471: step 11175, loss 0.579204.
Train: 2018-08-05T13:33:10.064863: step 11176, loss 0.511761.
Train: 2018-08-05T13:33:13.595327: step 11177, loss 0.545426.
Train: 2018-08-05T13:33:17.116737: step 11178, loss 0.647305.
Train: 2018-08-05T13:33:20.686797: step 11179, loss 0.604882.
Train: 2018-08-05T13:33:24.216756: step 11180, loss 0.536811.
Test: 2018-08-05T13:33:39.211782: step 11180, loss 0.547527.
Train: 2018-08-05T13:33:42.732738: step 11181, loss 0.579378.
Train: 2018-08-05T13:33:46.238137: step 11182, loss 0.519732.
Train: 2018-08-05T13:33:49.758063: step 11183, loss 0.630604.
Train: 2018-08-05T13:33:53.280537: step 11184, loss 0.630584.
Train: 2018-08-05T13:33:56.801974: step 11185, loss 0.536796.
Train: 2018-08-05T13:34:00.294322: step 11186, loss 0.570851.
Train: 2018-08-05T13:34:03.843843: step 11187, loss 0.536854.
Train: 2018-08-05T13:34:07.362273: step 11188, loss 0.553853.
Train: 2018-08-05T13:34:10.853612: step 11189, loss 0.553855.
Train: 2018-08-05T13:34:14.386584: step 11190, loss 0.54536.
Test: 2018-08-05T13:34:29.351026: step 11190, loss 0.548154.
Train: 2018-08-05T13:34:32.902558: step 11191, loss 0.502861.
Train: 2018-08-05T13:34:36.418984: step 11192, loss 0.545316.
Train: 2018-08-05T13:34:39.934392: step 11193, loss 0.604989.
Train: 2018-08-05T13:34:43.439803: step 11194, loss 0.528189.
Train: 2018-08-05T13:34:46.974279: step 11195, loss 0.493935.
Train: 2018-08-05T13:34:50.507759: step 11196, loss 0.665232.
Train: 2018-08-05T13:34:54.052259: step 11197, loss 0.605221.
Train: 2018-08-05T13:34:57.540615: step 11198, loss 0.562335.
Train: 2018-08-05T13:35:01.067062: step 11199, loss 0.605165.
Train: 2018-08-05T13:35:04.570427: step 11200, loss 0.519572.
Test: 2018-08-05T13:35:19.540441: step 11200, loss 0.548453.
Train: 2018-08-05T13:35:25.260734: step 11201, loss 0.53669.
Train: 2018-08-05T13:35:28.836816: step 11202, loss 0.519585.
Train: 2018-08-05T13:35:32.347214: step 11203, loss 0.553776.
Train: 2018-08-05T13:35:35.860121: step 11204, loss 0.61375.
Train: 2018-08-05T13:35:39.391564: step 11205, loss 0.570903.
Train: 2018-08-05T13:35:42.897955: step 11206, loss 0.545205.
Train: 2018-08-05T13:35:46.405859: step 11207, loss 0.536638.
Train: 2018-08-05T13:35:49.939324: step 11208, loss 0.493766.
Train: 2018-08-05T13:35:53.484320: step 11209, loss 0.545158.
Train: 2018-08-05T13:35:56.974663: step 11210, loss 0.605372.
Test: 2018-08-05T13:36:11.868475: step 11210, loss 0.548543.
Train: 2018-08-05T13:36:15.368375: step 11211, loss 0.553721.
Train: 2018-08-05T13:36:18.889305: step 11212, loss 0.536464.
Train: 2018-08-05T13:36:22.413752: step 11213, loss 0.553703.
Train: 2018-08-05T13:36:25.956752: step 11214, loss 0.519097.
Train: 2018-08-05T13:36:29.496246: step 11215, loss 0.571013.
Train: 2018-08-05T13:36:33.004639: step 11216, loss 0.544987.
Train: 2018-08-05T13:36:36.506519: step 11217, loss 0.536264.
Train: 2018-08-05T13:36:40.000379: step 11218, loss 0.605937.
Train: 2018-08-05T13:36:43.509767: step 11219, loss 0.640865.
Train: 2018-08-05T13:36:47.070313: step 11220, loss 0.553648.
Test: 2018-08-05T13:37:02.039273: step 11220, loss 0.548219.
Train: 2018-08-05T13:37:05.606823: step 11221, loss 0.605888.
Train: 2018-08-05T13:37:09.112723: step 11222, loss 0.562353.
Train: 2018-08-05T13:37:12.599065: step 11223, loss 0.553672.
Train: 2018-08-05T13:37:16.100451: step 11224, loss 0.553681.
Train: 2018-08-05T13:37:19.597817: step 11225, loss 0.510421.
Train: 2018-08-05T13:37:23.126288: step 11226, loss 0.562343.
Train: 2018-08-05T13:37:26.719915: step 11227, loss 0.579649.
Train: 2018-08-05T13:37:30.241866: step 11228, loss 0.622883.
Train: 2018-08-05T13:37:33.755284: step 11229, loss 0.640036.
Train: 2018-08-05T13:37:37.240612: step 11230, loss 0.476296.
Test: 2018-08-05T13:37:52.187517: step 11230, loss 0.547192.
Train: 2018-08-05T13:37:55.705945: step 11231, loss 0.613897.
Train: 2018-08-05T13:37:59.244432: step 11232, loss 0.553761.
Train: 2018-08-05T13:38:02.772899: step 11233, loss 0.48531.
Train: 2018-08-05T13:38:06.287329: step 11234, loss 0.528102.
Train: 2018-08-05T13:38:09.819784: step 11235, loss 0.613724.
Train: 2018-08-05T13:38:13.343721: step 11236, loss 0.528093.
Train: 2018-08-05T13:38:16.866152: step 11237, loss 0.553772.
Train: 2018-08-05T13:38:20.405612: step 11238, loss 0.613739.
Train: 2018-08-05T13:38:23.937072: step 11239, loss 0.553775.
Train: 2018-08-05T13:38:27.475558: step 11240, loss 0.562336.
Test: 2018-08-05T13:38:42.543724: step 11240, loss 0.548052.
Train: 2018-08-05T13:38:46.124320: step 11241, loss 0.596548.
Train: 2018-08-05T13:38:49.637218: step 11242, loss 0.596509.
Train: 2018-08-05T13:38:53.154145: step 11243, loss 0.570867.
Train: 2018-08-05T13:38:56.702134: step 11244, loss 0.596391.
Train: 2018-08-05T13:39:00.228061: step 11245, loss 0.52838.
Train: 2018-08-05T13:39:03.786602: step 11246, loss 0.511469.
Train: 2018-08-05T13:39:07.296509: step 11247, loss 0.545392.
Train: 2018-08-05T13:39:10.841514: step 11248, loss 0.536902.
Train: 2018-08-05T13:39:14.332851: step 11249, loss 0.545366.
Train: 2018-08-05T13:39:17.889882: step 11250, loss 0.502836.
Test: 2018-08-05T13:39:32.993179: step 11250, loss 0.548304.
Train: 2018-08-05T13:39:36.572773: step 11251, loss 0.630519.
Train: 2018-08-05T13:39:40.134317: step 11252, loss 0.579396.
Train: 2018-08-05T13:39:43.629703: step 11253, loss 0.511156.
Train: 2018-08-05T13:39:47.139607: step 11254, loss 0.511084.
Train: 2018-08-05T13:39:50.630475: step 11255, loss 0.545211.
Train: 2018-08-05T13:39:54.152903: step 11256, loss 0.588086.
Train: 2018-08-05T13:39:57.737488: step 11257, loss 0.493553.
Train: 2018-08-05T13:40:01.324609: step 11258, loss 0.614076.
Train: 2018-08-05T13:40:04.842535: step 11259, loss 0.545068.
Train: 2018-08-05T13:40:08.395027: step 11260, loss 0.588291.
Test: 2018-08-05T13:40:23.411145: step 11260, loss 0.548085.
Train: 2018-08-05T13:40:27.055380: step 11261, loss 0.596972.
Train: 2018-08-05T13:40:30.562776: step 11262, loss 0.467121.
Train: 2018-08-05T13:40:34.159416: step 11263, loss 0.605714.
Train: 2018-08-05T13:40:37.710936: step 11264, loss 0.536311.
Train: 2018-08-05T13:40:41.225349: step 11265, loss 0.553663.
Train: 2018-08-05T13:40:44.736758: step 11266, loss 0.484065.
Train: 2018-08-05T13:40:48.278233: step 11267, loss 0.579807.
Train: 2018-08-05T13:40:51.777602: step 11268, loss 0.509948.
Train: 2018-08-05T13:40:55.317084: step 11269, loss 0.649986.
Train: 2018-08-05T13:40:58.849551: step 11270, loss 0.562385.
Test: 2018-08-05T13:41:13.794509: step 11270, loss 0.548553.
Train: 2018-08-05T13:41:17.355559: step 11271, loss 0.536088.
Train: 2018-08-05T13:41:20.866469: step 11272, loss 0.544846.
Train: 2018-08-05T13:41:24.522270: step 11273, loss 0.5185.
Train: 2018-08-05T13:41:28.253271: step 11274, loss 0.518443.
Train: 2018-08-05T13:41:32.056452: step 11275, loss 0.49193.
Train: 2018-08-05T13:41:35.833560: step 11276, loss 0.509403.
Train: 2018-08-05T13:41:39.554533: step 11277, loss 0.606824.
Train: 2018-08-05T13:41:43.400844: step 11278, loss 0.491339.
Train: 2018-08-05T13:41:47.187016: step 11279, loss 0.553588.
Train: 2018-08-05T13:41:50.825745: step 11280, loss 0.580436.
Test: 2018-08-05T13:42:05.916021: step 11280, loss 0.546552.
Train: 2018-08-05T13:42:09.490599: step 11281, loss 0.535658.
Train: 2018-08-05T13:42:13.000521: step 11282, loss 0.553596.
Train: 2018-08-05T13:42:16.522462: step 11283, loss 0.526591.
Train: 2018-08-05T13:42:20.039899: step 11284, loss 0.562629.
Train: 2018-08-05T13:42:23.553321: step 11285, loss 0.616875.
Train: 2018-08-05T13:42:27.059722: step 11286, loss 0.562651.
Train: 2018-08-05T13:42:30.582156: step 11287, loss 0.544574.
Train: 2018-08-05T13:42:34.123658: step 11288, loss 0.562649.
Train: 2018-08-05T13:42:37.639065: step 11289, loss 0.544576.
Train: 2018-08-05T13:42:41.138438: step 11290, loss 0.571676.
Test: 2018-08-05T13:42:56.082843: step 11290, loss 0.54839.
Train: 2018-08-05T13:42:59.588236: step 11291, loss 0.607773.
Train: 2018-08-05T13:43:03.101657: step 11292, loss 0.643724.
Train: 2018-08-05T13:43:06.652177: step 11293, loss 0.553595.
Train: 2018-08-05T13:43:10.342047: step 11294, loss 0.598357.
Train: 2018-08-05T13:43:13.886542: step 11295, loss 0.508991.
Train: 2018-08-05T13:43:17.409468: step 11296, loss 0.535795.
Train: 2018-08-05T13:43:20.927900: step 11297, loss 0.518072.
Train: 2018-08-05T13:43:24.461366: step 11298, loss 0.553591.
Train: 2018-08-05T13:43:27.985812: step 11299, loss 0.518145.
Train: 2018-08-05T13:43:31.513273: step 11300, loss 0.553592.
Test: 2018-08-05T13:43:46.440567: step 11300, loss 0.547655.
Train: 2018-08-05T13:43:52.112245: step 11301, loss 0.668769.
Train: 2018-08-05T13:43:55.638692: step 11302, loss 0.571272.
Train: 2018-08-05T13:43:59.190199: step 11303, loss 0.562418.
Train: 2018-08-05T13:44:02.694592: step 11304, loss 0.55361.
Train: 2018-08-05T13:44:06.203996: step 11305, loss 0.606263.
Train: 2018-08-05T13:44:09.736965: step 11306, loss 0.562377.
Train: 2018-08-05T13:44:13.281446: step 11307, loss 0.544919.
Train: 2018-08-05T13:44:16.776808: step 11308, loss 0.588465.
Train: 2018-08-05T13:44:20.339863: step 11309, loss 0.562346.
Train: 2018-08-05T13:44:23.870313: step 11310, loss 0.571019.
Test: 2018-08-05T13:44:38.993676: step 11310, loss 0.546724.
Train: 2018-08-05T13:44:42.540695: step 11311, loss 0.510523.
Train: 2018-08-05T13:44:46.076170: step 11312, loss 0.588219.
Train: 2018-08-05T13:44:49.617164: step 11313, loss 0.613992.
Train: 2018-08-05T13:44:53.180219: step 11314, loss 0.579506.
Train: 2018-08-05T13:44:56.737728: step 11315, loss 0.570897.
Train: 2018-08-05T13:45:00.249154: step 11316, loss 0.622099.
Train: 2018-08-05T13:45:03.762564: step 11317, loss 0.579354.
Train: 2018-08-05T13:45:07.291007: step 11318, loss 0.503049.
Train: 2018-08-05T13:45:10.796399: step 11319, loss 0.579269.
Train: 2018-08-05T13:45:14.302773: step 11320, loss 0.511754.
Test: 2018-08-05T13:45:29.324918: step 11320, loss 0.549652.
Train: 2018-08-05T13:45:32.962668: step 11321, loss 0.57923.
Train: 2018-08-05T13:45:36.510170: step 11322, loss 0.562376.
Train: 2018-08-05T13:45:40.044122: step 11323, loss 0.587619.
Train: 2018-08-05T13:45:43.587110: step 11324, loss 0.64641.
Train: 2018-08-05T13:45:45.311761: step 11325, loss 0.633887.
Train: 2018-08-05T13:45:48.825174: step 11326, loss 0.487347.
Train: 2018-08-05T13:45:52.350630: step 11327, loss 0.562437.
Train: 2018-08-05T13:45:55.891125: step 11328, loss 0.562449.
Train: 2018-08-05T13:45:59.417080: step 11329, loss 0.529256.
Train: 2018-08-05T13:46:02.984626: step 11330, loss 0.579057.
Test: 2018-08-05T13:46:17.933012: step 11330, loss 0.549185.
Train: 2018-08-05T13:46:21.445919: step 11331, loss 0.562464.
Train: 2018-08-05T13:46:24.967360: step 11332, loss 0.554175.
Train: 2018-08-05T13:46:28.469756: step 11333, loss 0.587341.
Train: 2018-08-05T13:46:31.967130: step 11334, loss 0.603912.
Train: 2018-08-05T13:46:35.467504: step 11335, loss 0.570757.
Train: 2018-08-05T13:46:39.003995: step 11336, loss 0.587296.
Train: 2018-08-05T13:46:42.536462: step 11337, loss 0.537723.
Train: 2018-08-05T13:46:46.046854: step 11338, loss 0.554248.
Train: 2018-08-05T13:46:49.560263: step 11339, loss 0.521235.
Train: 2018-08-05T13:46:53.068668: step 11340, loss 0.570756.
Test: 2018-08-05T13:47:08.045635: step 11340, loss 0.549817.
Train: 2018-08-05T13:47:11.600147: step 11341, loss 0.562486.
Train: 2018-08-05T13:47:15.156706: step 11342, loss 0.521087.
Train: 2018-08-05T13:47:18.711240: step 11343, loss 0.562463.
Train: 2018-08-05T13:47:22.209613: step 11344, loss 0.504268.
Train: 2018-08-05T13:47:25.726040: step 11345, loss 0.587443.
Train: 2018-08-05T13:47:29.258495: step 11346, loss 0.604209.
Train: 2018-08-05T13:47:32.776426: step 11347, loss 0.587517.
Train: 2018-08-05T13:47:36.314405: step 11348, loss 0.579154.
Train: 2018-08-05T13:47:39.882972: step 11349, loss 0.545638.
Train: 2018-08-05T13:47:43.410422: step 11350, loss 0.478523.
Test: 2018-08-05T13:47:58.351818: step 11350, loss 0.547948.
Train: 2018-08-05T13:48:01.851671: step 11351, loss 0.51192.
Train: 2018-08-05T13:48:05.407689: step 11352, loss 0.537042.
Train: 2018-08-05T13:48:08.916092: step 11353, loss 0.570828.
Train: 2018-08-05T13:48:12.491176: step 11354, loss 0.570848.
Train: 2018-08-05T13:48:16.045194: step 11355, loss 0.56234.
Train: 2018-08-05T13:48:19.612238: step 11356, loss 0.49394.
Train: 2018-08-05T13:48:23.140702: step 11357, loss 0.493677.
Train: 2018-08-05T13:48:26.701240: step 11358, loss 0.553713.
Train: 2018-08-05T13:48:30.203618: step 11359, loss 0.527692.
Train: 2018-08-05T13:48:33.728040: step 11360, loss 0.510132.
Test: 2018-08-05T13:48:48.774215: step 11360, loss 0.547765.
Train: 2018-08-05T13:48:52.292135: step 11361, loss 0.562378.
Train: 2018-08-05T13:48:55.800044: step 11362, loss 0.57998.
Train: 2018-08-05T13:48:59.315978: step 11363, loss 0.544779.
Train: 2018-08-05T13:49:02.847938: step 11364, loss 0.53589.
Train: 2018-08-05T13:49:06.356335: step 11365, loss 0.562472.
Train: 2018-08-05T13:49:09.863723: step 11366, loss 0.54468.
Train: 2018-08-05T13:49:13.381162: step 11367, loss 0.535723.
Train: 2018-08-05T13:49:16.953738: step 11368, loss 0.616297.
Train: 2018-08-05T13:49:20.528310: step 11369, loss 0.616374.
Train: 2018-08-05T13:49:24.042721: step 11370, loss 0.517726.
Test: 2018-08-05T13:49:39.001189: step 11370, loss 0.548202.
Train: 2018-08-05T13:49:42.542158: step 11371, loss 0.598446.
Train: 2018-08-05T13:49:46.070100: step 11372, loss 0.58049.
Train: 2018-08-05T13:49:49.617087: step 11373, loss 0.544635.
Train: 2018-08-05T13:49:53.149543: step 11374, loss 0.616227.
Train: 2018-08-05T13:49:56.706561: step 11375, loss 0.625025.
Train: 2018-08-05T13:50:00.208944: step 11376, loss 0.571388.
Train: 2018-08-05T13:50:03.717843: step 11377, loss 0.544722.
Train: 2018-08-05T13:50:07.254824: step 11378, loss 0.535908.
Train: 2018-08-05T13:50:10.801331: step 11379, loss 0.58007.
Train: 2018-08-05T13:50:14.348837: step 11380, loss 0.536005.
Test: 2018-08-05T13:50:29.345847: step 11380, loss 0.548535.
Train: 2018-08-05T13:50:32.936967: step 11381, loss 0.52726.
Train: 2018-08-05T13:50:36.464425: step 11382, loss 0.615037.
Train: 2018-08-05T13:50:39.972300: step 11383, loss 0.579889.
Train: 2018-08-05T13:50:43.516285: step 11384, loss 0.562369.
Train: 2018-08-05T13:50:47.055770: step 11385, loss 0.553649.
Train: 2018-08-05T13:50:50.589730: step 11386, loss 0.510192.
Train: 2018-08-05T13:50:54.099136: step 11387, loss 0.562351.
Train: 2018-08-05T13:50:57.610557: step 11388, loss 0.597065.
Train: 2018-08-05T13:51:01.134002: step 11389, loss 0.605672.
Train: 2018-08-05T13:51:04.649413: step 11390, loss 0.62285.
Test: 2018-08-05T13:51:19.595336: step 11390, loss 0.549934.
Train: 2018-08-05T13:51:23.140841: step 11391, loss 0.596789.
Train: 2018-08-05T13:51:26.694848: step 11392, loss 0.519442.
Train: 2018-08-05T13:51:30.194729: step 11393, loss 0.562336.
Train: 2018-08-05T13:51:33.732182: step 11394, loss 0.54527.
Train: 2018-08-05T13:51:37.260641: step 11395, loss 0.545304.
Train: 2018-08-05T13:51:40.764534: step 11396, loss 0.638913.
Train: 2018-08-05T13:51:44.302520: step 11397, loss 0.570834.
Train: 2018-08-05T13:51:47.845517: step 11398, loss 0.520055.
Train: 2018-08-05T13:51:51.375000: step 11399, loss 0.613052.
Train: 2018-08-05T13:51:54.887912: step 11400, loss 0.553944.
Test: 2018-08-05T13:52:09.916990: step 11400, loss 0.548719.
Train: 2018-08-05T13:52:15.652316: step 11401, loss 0.528733.
Train: 2018-08-05T13:52:19.202839: step 11402, loss 0.587597.
Train: 2018-08-05T13:52:22.781924: step 11403, loss 0.461668.
Train: 2018-08-05T13:52:26.308885: step 11404, loss 0.612808.
Train: 2018-08-05T13:52:29.845876: step 11405, loss 0.596001.
Train: 2018-08-05T13:52:33.350779: step 11406, loss 0.61278.
Train: 2018-08-05T13:52:36.898292: step 11407, loss 0.503702.
Train: 2018-08-05T13:52:40.416214: step 11408, loss 0.62947.
Train: 2018-08-05T13:52:43.942162: step 11409, loss 0.528917.
Train: 2018-08-05T13:52:47.460598: step 11410, loss 0.503833.
Test: 2018-08-05T13:53:02.374926: step 11410, loss 0.549376.
Train: 2018-08-05T13:53:05.904379: step 11411, loss 0.646163.
Train: 2018-08-05T13:53:09.451897: step 11412, loss 0.612617.
Train: 2018-08-05T13:53:12.959290: step 11413, loss 0.520652.
Train: 2018-08-05T13:53:16.493756: step 11414, loss 0.554073.
Train: 2018-08-05T13:53:20.020226: step 11415, loss 0.520694.
Train: 2018-08-05T13:53:23.595325: step 11416, loss 0.554064.
Train: 2018-08-05T13:53:27.120289: step 11417, loss 0.554049.
Train: 2018-08-05T13:53:30.640221: step 11418, loss 0.570776.
Train: 2018-08-05T13:53:34.205765: step 11419, loss 0.587541.
Train: 2018-08-05T13:53:37.711659: step 11420, loss 0.537241.
Test: 2018-08-05T13:53:52.686658: step 11420, loss 0.547985.
Train: 2018-08-05T13:53:56.246193: step 11421, loss 0.56239.
Train: 2018-08-05T13:53:59.806722: step 11422, loss 0.537176.
Train: 2018-08-05T13:54:03.316123: step 11423, loss 0.495047.
Train: 2018-08-05T13:54:06.816486: step 11424, loss 0.53704.
Train: 2018-08-05T13:54:10.318883: step 11425, loss 0.579295.
Train: 2018-08-05T13:54:13.842312: step 11426, loss 0.562347.
Train: 2018-08-05T13:54:17.384824: step 11427, loss 0.570856.
Train: 2018-08-05T13:54:20.969417: step 11428, loss 0.587932.
Train: 2018-08-05T13:54:24.472294: step 11429, loss 0.511089.
Train: 2018-08-05T13:54:27.984721: step 11430, loss 0.62226.
Test: 2018-08-05T13:54:42.925087: step 11430, loss 0.548226.
Train: 2018-08-05T13:54:46.477114: step 11431, loss 0.553769.
Train: 2018-08-05T13:54:49.972492: step 11432, loss 0.536615.
Train: 2018-08-05T13:54:53.495949: step 11433, loss 0.596671.
Train: 2018-08-05T13:54:57.026413: step 11434, loss 0.622443.
Train: 2018-08-05T13:55:00.571923: step 11435, loss 0.553757.
Train: 2018-08-05T13:55:04.078315: step 11436, loss 0.579478.
Train: 2018-08-05T13:55:07.584708: step 11437, loss 0.596584.
Train: 2018-08-05T13:55:11.111168: step 11438, loss 0.536696.
Train: 2018-08-05T13:55:14.612536: step 11439, loss 0.545261.
Train: 2018-08-05T13:55:18.146994: step 11440, loss 0.502603.
Test: 2018-08-05T13:55:33.185667: step 11440, loss 0.54945.
Train: 2018-08-05T13:55:36.730172: step 11441, loss 0.545256.
Train: 2018-08-05T13:55:40.239580: step 11442, loss 0.570887.
Train: 2018-08-05T13:55:43.733937: step 11443, loss 0.562336.
Train: 2018-08-05T13:55:47.259379: step 11444, loss 0.52808.
Train: 2018-08-05T13:55:50.769787: step 11445, loss 0.588063.
Train: 2018-08-05T13:55:54.372944: step 11446, loss 0.605243.
Train: 2018-08-05T13:55:57.982137: step 11447, loss 0.5366.
Train: 2018-08-05T13:56:01.490028: step 11448, loss 0.553755.
Train: 2018-08-05T13:56:05.003944: step 11449, loss 0.579502.
Train: 2018-08-05T13:56:08.507826: step 11450, loss 0.51084.
Test: 2018-08-05T13:56:23.448740: step 11450, loss 0.547987.
Train: 2018-08-05T13:56:26.967665: step 11451, loss 0.536561.
Train: 2018-08-05T13:56:30.533220: step 11452, loss 0.562336.
Train: 2018-08-05T13:56:34.090765: step 11453, loss 0.519262.
Train: 2018-08-05T13:56:37.615226: step 11454, loss 0.562339.
Train: 2018-08-05T13:56:41.155712: step 11455, loss 0.657468.
Train: 2018-08-05T13:56:44.671657: step 11456, loss 0.553697.
Train: 2018-08-05T13:56:48.190092: step 11457, loss 0.519139.
Train: 2018-08-05T13:56:51.723066: step 11458, loss 0.640147.
Train: 2018-08-05T13:56:55.270564: step 11459, loss 0.562339.
Train: 2018-08-05T13:56:58.758910: step 11460, loss 0.553713.
Test: 2018-08-05T13:57:13.765956: step 11460, loss 0.548343.
Train: 2018-08-05T13:57:17.299435: step 11461, loss 0.545104.
Train: 2018-08-05T13:57:20.836901: step 11462, loss 0.570948.
Train: 2018-08-05T13:57:24.344785: step 11463, loss 0.613972.
Train: 2018-08-05T13:57:27.889780: step 11464, loss 0.596698.
Train: 2018-08-05T13:57:31.434273: step 11465, loss 0.536625.
Train: 2018-08-05T13:57:34.927148: step 11466, loss 0.62223.
Train: 2018-08-05T13:57:38.437028: step 11467, loss 0.562339.
Train: 2018-08-05T13:57:41.962977: step 11468, loss 0.519788.
Train: 2018-08-05T13:57:45.479411: step 11469, loss 0.502847.
Train: 2018-08-05T13:57:48.998343: step 11470, loss 0.553844.
Test: 2018-08-05T13:58:03.928710: step 11470, loss 0.547554.
Train: 2018-08-05T13:58:07.499261: step 11471, loss 0.596363.
Train: 2018-08-05T13:58:11.045764: step 11472, loss 0.66436.
Train: 2018-08-05T13:58:14.569219: step 11473, loss 0.519958.
Train: 2018-08-05T13:58:18.123740: step 11474, loss 0.570823.
Train: 2018-08-05T13:58:21.641162: step 11475, loss 0.604637.
Train: 2018-08-05T13:58:23.379837: step 11476, loss 0.52637.
Train: 2018-08-05T13:58:26.916320: step 11477, loss 0.545516.
Train: 2018-08-05T13:58:30.431256: step 11478, loss 0.570798.
Train: 2018-08-05T13:58:33.981267: step 11479, loss 0.537121.
Train: 2018-08-05T13:58:37.494671: step 11480, loss 0.511858.
Test: 2018-08-05T13:58:52.646063: step 11480, loss 0.547126.
Train: 2018-08-05T13:58:56.140920: step 11481, loss 0.545509.
Train: 2018-08-05T13:58:59.665854: step 11482, loss 0.494807.
Train: 2018-08-05T13:59:03.283538: step 11483, loss 0.503061.
Train: 2018-08-05T13:59:06.847593: step 11484, loss 0.536828.
Train: 2018-08-05T13:59:10.391583: step 11485, loss 0.553796.
Train: 2018-08-05T13:59:13.914528: step 11486, loss 0.622371.
Train: 2018-08-05T13:59:17.436980: step 11487, loss 0.596721.
Train: 2018-08-05T13:59:21.015080: step 11488, loss 0.519303.
Train: 2018-08-05T13:59:24.558077: step 11489, loss 0.570961.
Train: 2018-08-05T13:59:28.097565: step 11490, loss 0.545064.
Test: 2018-08-05T13:59:43.161240: step 11490, loss 0.547693.
Train: 2018-08-05T13:59:46.715265: step 11491, loss 0.614258.
Train: 2018-08-05T13:59:50.255751: step 11492, loss 0.63159.
Train: 2018-08-05T13:59:53.791237: step 11493, loss 0.519115.
Train: 2018-08-05T13:59:57.288606: step 11494, loss 0.553697.
Train: 2018-08-05T14:00:00.843150: step 11495, loss 0.579625.
Train: 2018-08-05T14:00:04.487897: step 11496, loss 0.56234.
Train: 2018-08-05T14:00:08.016847: step 11497, loss 0.579606.
Train: 2018-08-05T14:00:11.572861: step 11498, loss 0.570964.
Train: 2018-08-05T14:00:15.083266: step 11499, loss 0.484786.
Train: 2018-08-05T14:00:18.589669: step 11500, loss 0.545092.
Test: 2018-08-05T14:00:33.619830: step 11500, loss 0.548321.
Train: 2018-08-05T14:00:39.337128: step 11501, loss 0.562338.
Train: 2018-08-05T14:00:42.863580: step 11502, loss 0.510512.
Train: 2018-08-05T14:00:46.384027: step 11503, loss 0.536382.
Train: 2018-08-05T14:00:49.911472: step 11504, loss 0.518989.
Train: 2018-08-05T14:00:53.464988: step 11505, loss 0.562354.
Train: 2018-08-05T14:00:56.988436: step 11506, loss 0.571078.
Train: 2018-08-05T14:01:00.546965: step 11507, loss 0.614763.
Train: 2018-08-05T14:01:04.083439: step 11508, loss 0.527426.
Train: 2018-08-05T14:01:07.590338: step 11509, loss 0.50116.
Train: 2018-08-05T14:01:11.129304: step 11510, loss 0.614965.
Test: 2018-08-05T14:01:26.204540: step 11510, loss 0.549156.
Train: 2018-08-05T14:01:29.697895: step 11511, loss 0.606233.
Train: 2018-08-05T14:01:33.227343: step 11512, loss 0.553621.
Train: 2018-08-05T14:01:36.764816: step 11513, loss 0.536099.
Train: 2018-08-05T14:01:40.374483: step 11514, loss 0.614958.
Train: 2018-08-05T14:01:43.923989: step 11515, loss 0.52737.
Train: 2018-08-05T14:01:47.422377: step 11516, loss 0.606119.
Train: 2018-08-05T14:01:50.920751: step 11517, loss 0.544899.
Train: 2018-08-05T14:01:54.424146: step 11518, loss 0.562367.
Train: 2018-08-05T14:01:57.957598: step 11519, loss 0.518774.
Train: 2018-08-05T14:02:01.470998: step 11520, loss 0.527495.
Test: 2018-08-05T14:02:16.504120: step 11520, loss 0.547397.
Train: 2018-08-05T14:02:20.030576: step 11521, loss 0.544921.
Train: 2018-08-05T14:02:23.533966: step 11522, loss 0.60601.
Train: 2018-08-05T14:02:27.064431: step 11523, loss 0.544915.
Train: 2018-08-05T14:02:30.558801: step 11524, loss 0.562366.
Train: 2018-08-05T14:02:34.102789: step 11525, loss 0.579812.
Train: 2018-08-05T14:02:37.683899: step 11526, loss 0.536209.
Train: 2018-08-05T14:02:41.217871: step 11527, loss 0.510061.
Train: 2018-08-05T14:02:44.731289: step 11528, loss 0.527466.
Train: 2018-08-05T14:02:48.246187: step 11529, loss 0.562372.
Train: 2018-08-05T14:02:51.755594: step 11530, loss 0.509887.
Test: 2018-08-05T14:03:06.710991: step 11530, loss 0.548755.
Train: 2018-08-05T14:03:10.251980: step 11531, loss 0.55362.
Train: 2018-08-05T14:03:13.781444: step 11532, loss 0.456996.
Train: 2018-08-05T14:03:17.305883: step 11533, loss 0.588869.
Train: 2018-08-05T14:03:20.826309: step 11534, loss 0.62432.
Train: 2018-08-05T14:03:24.348737: step 11535, loss 0.518198.
Train: 2018-08-05T14:03:27.849101: step 11536, loss 0.624498.
Train: 2018-08-05T14:03:31.419653: step 11537, loss 0.527002.
Train: 2018-08-05T14:03:34.945105: step 11538, loss 0.500384.
Train: 2018-08-05T14:03:38.476587: step 11539, loss 0.580235.
Train: 2018-08-05T14:03:41.960915: step 11540, loss 0.589147.
Test: 2018-08-05T14:03:56.895785: step 11540, loss 0.547018.
Train: 2018-08-05T14:04:00.411208: step 11541, loss 0.598037.
Train: 2018-08-05T14:04:03.932140: step 11542, loss 0.624643.
Train: 2018-08-05T14:04:07.467596: step 11543, loss 0.597899.
Train: 2018-08-05T14:04:10.981021: step 11544, loss 0.500587.
Train: 2018-08-05T14:04:14.513484: step 11545, loss 0.571243.
Train: 2018-08-05T14:04:18.034929: step 11546, loss 0.48316.
Train: 2018-08-05T14:04:21.536811: step 11547, loss 0.571215.
Train: 2018-08-05T14:04:25.049711: step 11548, loss 0.588812.
Train: 2018-08-05T14:04:28.573156: step 11549, loss 0.650321.
Train: 2018-08-05T14:04:32.138713: step 11550, loss 0.588684.
Test: 2018-08-05T14:04:47.057043: step 11550, loss 0.545769.
Train: 2018-08-05T14:04:50.585521: step 11551, loss 0.527424.
Train: 2018-08-05T14:04:54.127015: step 11552, loss 0.597225.
Train: 2018-08-05T14:04:57.631398: step 11553, loss 0.588421.
Train: 2018-08-05T14:05:01.128779: step 11554, loss 0.65761.
Train: 2018-08-05T14:05:04.691825: step 11555, loss 0.570953.
Train: 2018-08-05T14:05:08.209245: step 11556, loss 0.639498.
Train: 2018-08-05T14:05:11.701585: step 11557, loss 0.545301.
Train: 2018-08-05T14:05:15.181900: step 11558, loss 0.579305.
Train: 2018-08-05T14:05:18.658210: step 11559, loss 0.545498.
Train: 2018-08-05T14:05:22.135023: step 11560, loss 0.604372.
Test: 2018-08-05T14:05:37.062886: step 11560, loss 0.547501.
Train: 2018-08-05T14:05:40.611886: step 11561, loss 0.587533.
Train: 2018-08-05T14:05:44.149373: step 11562, loss 0.604064.
Train: 2018-08-05T14:05:47.685835: step 11563, loss 0.579031.
Train: 2018-08-05T14:05:51.197256: step 11564, loss 0.529614.
Train: 2018-08-05T14:05:54.716693: step 11565, loss 0.52969.
Train: 2018-08-05T14:05:58.215072: step 11566, loss 0.546171.
Train: 2018-08-05T14:06:01.712440: step 11567, loss 0.480706.
Train: 2018-08-05T14:06:05.259951: step 11568, loss 0.554371.
Train: 2018-08-05T14:06:08.794425: step 11569, loss 0.546138.
Train: 2018-08-05T14:06:12.280784: step 11570, loss 0.578981.
Test: 2018-08-05T14:06:27.225656: step 11570, loss 0.549718.
Train: 2018-08-05T14:06:30.716997: step 11571, loss 0.488412.
Train: 2018-08-05T14:06:34.215370: step 11572, loss 0.579018.
Train: 2018-08-05T14:06:37.726770: step 11573, loss 0.512772.
Train: 2018-08-05T14:06:41.238185: step 11574, loss 0.545817.
Train: 2018-08-05T14:06:44.788713: step 11575, loss 0.579115.
Train: 2018-08-05T14:06:48.293101: step 11576, loss 0.562402.
Train: 2018-08-05T14:06:51.857666: step 11577, loss 0.604381.
Train: 2018-08-05T14:06:55.418206: step 11578, loss 0.511896.
Train: 2018-08-05T14:06:58.948686: step 11579, loss 0.604556.
Train: 2018-08-05T14:07:02.487173: step 11580, loss 0.646878.
Test: 2018-08-05T14:07:17.416517: step 11580, loss 0.548837.
Train: 2018-08-05T14:07:20.944976: step 11581, loss 0.528569.
Train: 2018-08-05T14:07:24.476940: step 11582, loss 0.537004.
Train: 2018-08-05T14:07:27.979815: step 11583, loss 0.57928.
Train: 2018-08-05T14:07:31.471663: step 11584, loss 0.638554.
Train: 2018-08-05T14:07:34.967014: step 11585, loss 0.536989.
Train: 2018-08-05T14:07:38.506504: step 11586, loss 0.621536.
Train: 2018-08-05T14:07:42.041984: step 11587, loss 0.587686.
Train: 2018-08-05T14:07:45.578469: step 11588, loss 0.511837.
Train: 2018-08-05T14:07:49.134497: step 11589, loss 0.570795.
Train: 2018-08-05T14:07:52.636879: step 11590, loss 0.59603.
Test: 2018-08-05T14:08:07.661962: step 11590, loss 0.548354.
Train: 2018-08-05T14:08:11.148299: step 11591, loss 0.579189.
Train: 2018-08-05T14:08:14.680762: step 11592, loss 0.545613.
Train: 2018-08-05T14:08:18.240298: step 11593, loss 0.47857.
Train: 2018-08-05T14:08:21.793812: step 11594, loss 0.595963.
Train: 2018-08-05T14:08:25.299190: step 11595, loss 0.595977.
Train: 2018-08-05T14:08:28.796566: step 11596, loss 0.5456.
Train: 2018-08-05T14:08:32.382160: step 11597, loss 0.562388.
Train: 2018-08-05T14:08:36.333137: step 11598, loss 0.595983.
Train: 2018-08-05T14:08:40.323297: step 11599, loss 0.537202.
Train: 2018-08-05T14:08:44.319010: step 11600, loss 0.56239.
Test: 2018-08-05T14:09:00.584835: step 11600, loss 0.548357.
Train: 2018-08-05T14:09:06.558319: step 11601, loss 0.570787.
Train: 2018-08-05T14:09:10.502925: step 11602, loss 0.680015.
Train: 2018-08-05T14:09:14.178766: step 11603, loss 0.537253.
Train: 2018-08-05T14:09:17.865646: step 11604, loss 0.579143.
Train: 2018-08-05T14:09:21.562558: step 11605, loss 0.503931.
Train: 2018-08-05T14:09:25.267478: step 11606, loss 0.528997.
Train: 2018-08-05T14:09:28.860617: step 11607, loss 0.579134.
Train: 2018-08-05T14:09:32.463757: step 11608, loss 0.579139.
Train: 2018-08-05T14:09:36.075422: step 11609, loss 0.579142.
Train: 2018-08-05T14:09:39.708151: step 11610, loss 0.570774.
Test: 2018-08-05T14:09:54.944340: step 11610, loss 0.549211.
Train: 2018-08-05T14:09:58.607673: step 11611, loss 0.528954.
Train: 2018-08-05T14:10:02.363766: step 11612, loss 0.503817.
Train: 2018-08-05T14:10:05.946861: step 11613, loss 0.554003.
Train: 2018-08-05T14:10:09.537986: step 11614, loss 0.604407.
Train: 2018-08-05T14:10:13.224861: step 11615, loss 0.537145.
Train: 2018-08-05T14:10:16.862599: step 11616, loss 0.570822.
Train: 2018-08-05T14:10:20.690360: step 11617, loss 0.477901.
Train: 2018-08-05T14:10:24.301042: step 11618, loss 0.647147.
Train: 2018-08-05T14:10:27.913717: step 11619, loss 0.553945.
Train: 2018-08-05T14:10:31.523391: step 11620, loss 0.562259.
Test: 2018-08-05T14:10:47.347592: step 11620, loss 0.54834.
Train: 2018-08-05T14:10:51.019925: step 11621, loss 0.536816.
Train: 2018-08-05T14:10:54.680723: step 11622, loss 0.562492.
Train: 2018-08-05T14:10:58.344543: step 11623, loss 0.596503.
Train: 2018-08-05T14:11:01.937674: step 11624, loss 0.613502.
Train: 2018-08-05T14:11:05.537839: step 11625, loss 0.579362.
Train: 2018-08-05T14:11:09.167045: step 11626, loss 0.51984.
Train: 2018-08-05T14:11:10.967913: step 11627, loss 0.526073.
Train: 2018-08-05T14:11:14.647775: step 11628, loss 0.545326.
Train: 2018-08-05T14:11:18.264464: step 11629, loss 0.562342.
Train: 2018-08-05T14:11:21.908236: step 11630, loss 0.570867.
Test: 2018-08-05T14:11:37.094244: step 11630, loss 0.547497.
Train: 2018-08-05T14:11:40.687380: step 11631, loss 0.570877.
Train: 2018-08-05T14:11:44.339676: step 11632, loss 0.511111.
Train: 2018-08-05T14:11:47.993974: step 11633, loss 0.545222.
Train: 2018-08-05T14:11:51.597641: step 11634, loss 0.53664.
Train: 2018-08-05T14:11:55.237385: step 11635, loss 0.484916.
Train: 2018-08-05T14:11:58.912241: step 11636, loss 0.614321.
Train: 2018-08-05T14:12:02.560018: step 11637, loss 0.614707.
Train: 2018-08-05T14:12:06.192796: step 11638, loss 0.553688.
Train: 2018-08-05T14:12:09.841594: step 11639, loss 0.570993.
Train: 2018-08-05T14:12:13.455284: step 11640, loss 0.553692.
Test: 2018-08-05T14:12:28.774224: step 11640, loss 0.547492.
Train: 2018-08-05T14:12:32.577398: step 11641, loss 0.605611.
Train: 2018-08-05T14:12:36.254251: step 11642, loss 0.57099.
Train: 2018-08-05T14:12:39.876470: step 11643, loss 0.570982.
Train: 2018-08-05T14:12:43.634067: step 11644, loss 0.579606.
Train: 2018-08-05T14:12:47.260786: step 11645, loss 0.605449.
Train: 2018-08-05T14:12:50.865449: step 11646, loss 0.639769.
Train: 2018-08-05T14:12:54.514224: step 11647, loss 0.536618.
Train: 2018-08-05T14:12:58.149993: step 11648, loss 0.605079.
Train: 2018-08-05T14:13:01.725589: step 11649, loss 0.613454.
Train: 2018-08-05T14:13:05.419484: step 11650, loss 0.553867.
Test: 2018-08-05T14:13:20.769962: step 11650, loss 0.548053.
Train: 2018-08-05T14:13:24.470364: step 11651, loss 0.613066.
Train: 2018-08-05T14:13:28.159253: step 11652, loss 0.554007.
Train: 2018-08-05T14:13:31.900272: step 11653, loss 0.595948.
Train: 2018-08-05T14:13:35.610228: step 11654, loss 0.537364.
Train: 2018-08-05T14:13:39.337717: step 11655, loss 0.504151.
Train: 2018-08-05T14:13:43.013046: step 11656, loss 0.604029.
Train: 2018-08-05T14:13:46.741526: step 11657, loss 0.570759.
Train: 2018-08-05T14:13:50.444957: step 11658, loss 0.56247.
Train: 2018-08-05T14:13:54.097733: step 11659, loss 0.620408.
Train: 2018-08-05T14:13:57.761579: step 11660, loss 0.562502.
Test: 2018-08-05T14:14:13.410852: step 11660, loss 0.548383.
Train: 2018-08-05T14:14:17.084682: step 11661, loss 0.562519.
Train: 2018-08-05T14:14:20.785093: step 11662, loss 0.488521.
Train: 2018-08-05T14:14:24.383246: step 11663, loss 0.562529.
Train: 2018-08-05T14:14:27.985897: step 11664, loss 0.587222.
Train: 2018-08-05T14:14:31.680796: step 11665, loss 0.546032.
Train: 2018-08-05T14:14:35.306033: step 11666, loss 0.529468.
Train: 2018-08-05T14:14:38.916199: step 11667, loss 0.562367.
Train: 2018-08-05T14:14:42.520837: step 11668, loss 0.54594.
Train: 2018-08-05T14:14:46.121501: step 11669, loss 0.536776.
Train: 2018-08-05T14:14:49.708639: step 11670, loss 0.588859.
Test: 2018-08-05T14:15:04.950408: step 11670, loss 0.548171.
Train: 2018-08-05T14:15:08.601699: step 11671, loss 0.526624.
Train: 2018-08-05T14:15:12.246475: step 11672, loss 0.5551.
Train: 2018-08-05T14:15:15.856158: step 11673, loss 0.512493.
Train: 2018-08-05T14:15:19.498921: step 11674, loss 0.520926.
Train: 2018-08-05T14:15:23.141703: step 11675, loss 0.570091.
Train: 2018-08-05T14:15:26.861704: step 11676, loss 0.573007.
Train: 2018-08-05T14:15:30.505498: step 11677, loss 0.663332.
Train: 2018-08-05T14:15:34.548347: step 11678, loss 0.535881.
Train: 2018-08-05T14:15:38.375623: step 11679, loss 0.588937.
Train: 2018-08-05T14:15:42.260501: step 11680, loss 0.545189.
Test: 2018-08-05T14:15:57.653599: step 11680, loss 0.547642.
Train: 2018-08-05T14:16:01.267783: step 11681, loss 0.562826.
Train: 2018-08-05T14:16:04.864436: step 11682, loss 0.536771.
Train: 2018-08-05T14:16:08.466584: step 11683, loss 0.484892.
Train: 2018-08-05T14:16:12.069729: step 11684, loss 0.55336.
Train: 2018-08-05T14:16:15.946153: step 11685, loss 0.631308.
Train: 2018-08-05T14:16:19.630053: step 11686, loss 0.537264.
Train: 2018-08-05T14:16:23.217205: step 11687, loss 0.485234.
Train: 2018-08-05T14:16:26.792824: step 11688, loss 0.519522.
Train: 2018-08-05T14:16:30.401006: step 11689, loss 0.519479.
Train: 2018-08-05T14:16:34.074861: step 11690, loss 0.466938.
Test: 2018-08-05T14:16:49.457934: step 11690, loss 0.548296.
Train: 2018-08-05T14:16:53.063602: step 11691, loss 0.580724.
Train: 2018-08-05T14:16:56.680307: step 11692, loss 0.597728.
Train: 2018-08-05T14:17:00.292022: step 11693, loss 0.527063.
Train: 2018-08-05T14:17:03.913248: step 11694, loss 0.56983.
Train: 2018-08-05T14:17:07.517414: step 11695, loss 0.52623.
Train: 2018-08-05T14:17:11.209316: step 11696, loss 0.536075.
Train: 2018-08-05T14:17:14.906259: step 11697, loss 0.57227.
Train: 2018-08-05T14:17:18.585122: step 11698, loss 0.546961.
Train: 2018-08-05T14:17:22.217846: step 11699, loss 0.49012.
Train: 2018-08-05T14:17:25.834553: step 11700, loss 0.635013.
Test: 2018-08-05T14:17:41.341006: step 11700, loss 0.547136.
Train: 2018-08-05T14:17:47.167585: step 11701, loss 0.598799.
Train: 2018-08-05T14:17:50.845453: step 11702, loss 0.564951.
Train: 2018-08-05T14:17:54.510293: step 11703, loss 0.553492.
Train: 2018-08-05T14:17:58.136526: step 11704, loss 0.599209.
Train: 2018-08-05T14:18:01.739689: step 11705, loss 0.588492.
Train: 2018-08-05T14:18:05.325314: step 11706, loss 0.525734.
Train: 2018-08-05T14:18:08.914933: step 11707, loss 0.599263.
Train: 2018-08-05T14:18:12.527615: step 11708, loss 0.589407.
Train: 2018-08-05T14:18:16.145306: step 11709, loss 0.562929.
Train: 2018-08-05T14:18:19.744458: step 11710, loss 0.579612.
Test: 2018-08-05T14:18:35.134089: step 11710, loss 0.548879.
Train: 2018-08-05T14:18:38.741271: step 11711, loss 0.527647.
Train: 2018-08-05T14:18:42.363990: step 11712, loss 0.562381.
Train: 2018-08-05T14:18:45.964649: step 11713, loss 0.51931.
Train: 2018-08-05T14:18:49.705693: step 11714, loss 0.50216.
Train: 2018-08-05T14:18:53.448728: step 11715, loss 0.562335.
Train: 2018-08-05T14:18:57.063431: step 11716, loss 0.631172.
Train: 2018-08-05T14:19:00.685631: step 11717, loss 0.510768.
Train: 2018-08-05T14:19:04.292772: step 11718, loss 0.527956.
Train: 2018-08-05T14:19:07.873369: step 11719, loss 0.674154.
Train: 2018-08-05T14:19:11.500090: step 11720, loss 0.613851.
Test: 2018-08-05T14:19:26.639974: step 11720, loss 0.548036.
Train: 2018-08-05T14:19:30.262192: step 11721, loss 0.545211.
Train: 2018-08-05T14:19:33.949565: step 11722, loss 0.562337.
Train: 2018-08-05T14:19:37.700680: step 11723, loss 0.613503.
Train: 2018-08-05T14:19:41.375017: step 11724, loss 0.48582.
Train: 2018-08-05T14:19:45.009752: step 11725, loss 0.570842.
Train: 2018-08-05T14:19:48.694108: step 11726, loss 0.553862.
Train: 2018-08-05T14:19:52.539925: step 11727, loss 0.562351.
Train: 2018-08-05T14:19:56.197742: step 11728, loss 0.579307.
Train: 2018-08-05T14:19:59.826479: step 11729, loss 0.587766.
Train: 2018-08-05T14:20:03.588586: step 11730, loss 0.630035.
Test: 2018-08-05T14:20:19.185751: step 11730, loss 0.549054.
Train: 2018-08-05T14:20:23.053108: step 11731, loss 0.537056.
Train: 2018-08-05T14:20:27.040780: step 11732, loss 0.587642.
Train: 2018-08-05T14:20:31.038492: step 11733, loss 0.545574.
Train: 2018-08-05T14:20:34.802579: step 11734, loss 0.58757.
Train: 2018-08-05T14:20:38.764647: step 11735, loss 0.570778.
Train: 2018-08-05T14:20:42.676119: step 11736, loss 0.528952.
Train: 2018-08-05T14:20:46.433546: step 11737, loss 0.520618.
Train: 2018-08-05T14:20:50.332032: step 11738, loss 0.545683.
Train: 2018-08-05T14:20:54.061054: step 11739, loss 0.554029.
Train: 2018-08-05T14:20:57.815615: step 11740, loss 0.495327.
Test: 2018-08-05T14:21:13.344494: step 11740, loss 0.547938.
Train: 2018-08-05T14:21:17.145810: step 11741, loss 0.545525.
Train: 2018-08-05T14:21:20.836175: step 11742, loss 0.494571.
Train: 2018-08-05T14:21:24.437814: step 11743, loss 0.578986.
Train: 2018-08-05T14:21:28.114419: step 11744, loss 0.588914.
Train: 2018-08-05T14:21:31.772223: step 11745, loss 0.545808.
Train: 2018-08-05T14:21:35.473168: step 11746, loss 0.579652.
Train: 2018-08-05T14:21:39.206722: step 11747, loss 0.614251.
Train: 2018-08-05T14:21:42.881079: step 11748, loss 0.535347.
Train: 2018-08-05T14:21:46.529834: step 11749, loss 0.605506.
Train: 2018-08-05T14:21:50.208744: step 11750, loss 0.562095.
Test: 2018-08-05T14:22:05.836224: step 11750, loss 0.548492.
Train: 2018-08-05T14:22:09.559237: step 11751, loss 0.553762.
Train: 2018-08-05T14:22:13.334110: step 11752, loss 0.614416.
Train: 2018-08-05T14:22:17.165193: step 11753, loss 0.579026.
Train: 2018-08-05T14:22:20.845546: step 11754, loss 0.578803.
Train: 2018-08-05T14:22:24.700077: step 11755, loss 0.570928.
Train: 2018-08-05T14:22:28.352384: step 11756, loss 0.604547.
Train: 2018-08-05T14:22:32.046289: step 11757, loss 0.545108.
Train: 2018-08-05T14:22:35.672014: step 11758, loss 0.605094.
Train: 2018-08-05T14:22:39.266670: step 11759, loss 0.596928.
Train: 2018-08-05T14:22:42.874842: step 11760, loss 0.63098.
Test: 2018-08-05T14:22:58.332560: step 11760, loss 0.548164.
Train: 2018-08-05T14:23:02.012921: step 11761, loss 0.520391.
Train: 2018-08-05T14:23:05.608548: step 11762, loss 0.545805.
Train: 2018-08-05T14:23:09.257831: step 11763, loss 0.637426.
Train: 2018-08-05T14:23:12.944696: step 11764, loss 0.545927.
Train: 2018-08-05T14:23:16.569422: step 11765, loss 0.554248.
Train: 2018-08-05T14:23:20.184120: step 11766, loss 0.488391.
Train: 2018-08-05T14:23:23.804820: step 11767, loss 0.546046.
Train: 2018-08-05T14:23:27.455588: step 11768, loss 0.513062.
Train: 2018-08-05T14:23:31.091339: step 11769, loss 0.537724.
Train: 2018-08-05T14:23:34.760667: step 11770, loss 0.529364.
Test: 2018-08-05T14:23:50.069503: step 11770, loss 0.548394.
Train: 2018-08-05T14:23:53.724795: step 11771, loss 0.653806.
Train: 2018-08-05T14:23:57.368580: step 11772, loss 0.554137.
Train: 2018-08-05T14:24:00.969235: step 11773, loss 0.479234.
Train: 2018-08-05T14:24:04.576914: step 11774, loss 0.520691.
Train: 2018-08-05T14:24:08.213659: step 11775, loss 0.6378.
Train: 2018-08-05T14:24:11.893036: step 11776, loss 0.52882.
Train: 2018-08-05T14:24:15.500717: step 11777, loss 0.511899.
Train: 2018-08-05T14:24:17.252450: step 11778, loss 0.580375.
Train: 2018-08-05T14:24:20.857594: step 11779, loss 0.562356.
Train: 2018-08-05T14:24:24.454216: step 11780, loss 0.553863.
Test: 2018-08-05T14:24:39.676364: step 11780, loss 0.548528.
Train: 2018-08-05T14:24:43.303582: step 11781, loss 0.528316.
Train: 2018-08-05T14:24:46.956376: step 11782, loss 0.613533.
Train: 2018-08-05T14:24:50.555039: step 11783, loss 0.545246.
Train: 2018-08-05T14:24:54.154173: step 11784, loss 0.570896.
Train: 2018-08-05T14:24:57.751323: step 11785, loss 0.639486.
Train: 2018-08-05T14:25:01.366005: step 11786, loss 0.510926.
Train: 2018-08-05T14:25:05.000766: step 11787, loss 0.519464.
Train: 2018-08-05T14:25:08.653553: step 11788, loss 0.562335.
Train: 2018-08-05T14:25:12.256211: step 11789, loss 0.622528.
Train: 2018-08-05T14:25:15.904006: step 11790, loss 0.484956.
Test: 2018-08-05T14:25:31.174774: step 11790, loss 0.548748.
Train: 2018-08-05T14:25:34.823541: step 11791, loss 0.493449.
Train: 2018-08-05T14:25:38.499426: step 11792, loss 0.562339.
Train: 2018-08-05T14:25:42.157254: step 11793, loss 0.579656.
Train: 2018-08-05T14:25:45.927376: step 11794, loss 0.545003.
Train: 2018-08-05T14:25:49.576678: step 11795, loss 0.544975.
Train: 2018-08-05T14:25:53.187876: step 11796, loss 0.536241.
Train: 2018-08-05T14:25:56.955003: step 11797, loss 0.597268.
Train: 2018-08-05T14:26:00.853445: step 11798, loss 0.649727.
Train: 2018-08-05T14:26:04.739874: step 11799, loss 0.536184.
Train: 2018-08-05T14:26:08.404718: step 11800, loss 0.527467.
Test: 2018-08-05T14:26:23.679535: step 11800, loss 0.547591.
Train: 2018-08-05T14:26:29.919219: step 11801, loss 0.527458.
Train: 2018-08-05T14:26:33.616108: step 11802, loss 0.47502.
Train: 2018-08-05T14:26:37.219782: step 11803, loss 0.544867.
Train: 2018-08-05T14:26:40.830986: step 11804, loss 0.571173.
Train: 2018-08-05T14:26:44.409067: step 11805, loss 0.509626.
Train: 2018-08-05T14:26:48.047853: step 11806, loss 0.580061.
Train: 2018-08-05T14:26:51.662557: step 11807, loss 0.527086.
Train: 2018-08-05T14:26:55.259207: step 11808, loss 0.597878.
Train: 2018-08-05T14:26:58.836782: step 11809, loss 0.571325.
Train: 2018-08-05T14:27:02.430888: step 11810, loss 0.562463.
Test: 2018-08-05T14:27:17.628472: step 11810, loss 0.548459.
Train: 2018-08-05T14:27:21.263743: step 11811, loss 0.571343.
Train: 2018-08-05T14:27:24.918556: step 11812, loss 0.491456.
Train: 2018-08-05T14:27:28.555310: step 11813, loss 0.642464.
Train: 2018-08-05T14:27:32.178523: step 11814, loss 0.491421.
Train: 2018-08-05T14:27:35.773169: step 11815, loss 0.60691.
Train: 2018-08-05T14:27:39.370822: step 11816, loss 0.571354.
Train: 2018-08-05T14:27:42.974502: step 11817, loss 0.500342.
Train: 2018-08-05T14:27:46.631321: step 11818, loss 0.580222.
Train: 2018-08-05T14:27:50.305692: step 11819, loss 0.55359.
Train: 2018-08-05T14:27:53.946946: step 11820, loss 0.544717.
Test: 2018-08-05T14:28:09.187160: step 11820, loss 0.548054.
Train: 2018-08-05T14:28:12.828443: step 11821, loss 0.544718.
Train: 2018-08-05T14:28:16.548422: step 11822, loss 0.571338.
Train: 2018-08-05T14:28:20.180665: step 11823, loss 0.615691.
Train: 2018-08-05T14:28:23.833948: step 11824, loss 0.553592.
Train: 2018-08-05T14:28:27.484749: step 11825, loss 0.509363.
Train: 2018-08-05T14:28:31.101445: step 11826, loss 0.527064.
Train: 2018-08-05T14:28:34.695590: step 11827, loss 0.535902.
Train: 2018-08-05T14:28:38.297261: step 11828, loss 0.500482.
Train: 2018-08-05T14:28:41.868861: step 11829, loss 0.597924.
Train: 2018-08-05T14:28:45.503128: step 11830, loss 0.544719.
Test: 2018-08-05T14:29:00.840069: step 11830, loss 0.548662.
Train: 2018-08-05T14:29:04.486827: step 11831, loss 0.589103.
Train: 2018-08-05T14:29:08.098528: step 11832, loss 0.562467.
Train: 2018-08-05T14:29:11.704178: step 11833, loss 0.526965.
Train: 2018-08-05T14:29:15.297831: step 11834, loss 0.491447.
Train: 2018-08-05T14:29:18.956146: step 11835, loss 0.56248.
Train: 2018-08-05T14:29:22.579856: step 11836, loss 0.571391.
Train: 2018-08-05T14:29:26.198064: step 11837, loss 0.482328.
Train: 2018-08-05T14:29:29.797221: step 11838, loss 0.598216.
Train: 2018-08-05T14:29:33.405901: step 11839, loss 0.589323.
Train: 2018-08-05T14:29:36.996541: step 11840, loss 0.598258.
Test: 2018-08-05T14:29:52.394094: step 11840, loss 0.547402.
Train: 2018-08-05T14:29:56.087971: step 11841, loss 0.464331.
Train: 2018-08-05T14:29:59.750335: step 11842, loss 0.526786.
Train: 2018-08-05T14:30:03.635762: step 11843, loss 0.535697.
Train: 2018-08-05T14:30:07.238949: step 11844, loss 0.562552.
Train: 2018-08-05T14:30:10.856143: step 11845, loss 0.553593.
Train: 2018-08-05T14:30:14.537984: step 11846, loss 0.544616.
Train: 2018-08-05T14:30:18.148176: step 11847, loss 0.580561.
Train: 2018-08-05T14:30:21.805994: step 11848, loss 0.571579.
Train: 2018-08-05T14:30:25.443753: step 11849, loss 0.51764.
Train: 2018-08-05T14:30:29.068969: step 11850, loss 0.60756.
Test: 2018-08-05T14:30:44.321226: step 11850, loss 0.547368.
Train: 2018-08-05T14:30:47.987065: step 11851, loss 0.616509.
Train: 2018-08-05T14:30:51.612788: step 11852, loss 0.517714.
Train: 2018-08-05T14:30:55.241524: step 11853, loss 0.517752.
Train: 2018-08-05T14:30:58.865250: step 11854, loss 0.562548.
Train: 2018-08-05T14:31:02.496500: step 11855, loss 0.669973.
Train: 2018-08-05T14:31:06.123744: step 11856, loss 0.642857.
Train: 2018-08-05T14:31:09.729414: step 11857, loss 0.615802.
Train: 2018-08-05T14:31:13.335081: step 11858, loss 0.553596.
Train: 2018-08-05T14:31:16.938236: step 11859, loss 0.544809.
Train: 2018-08-05T14:31:20.566462: step 11860, loss 0.588673.
Test: 2018-08-05T14:31:35.785661: step 11860, loss 0.548196.
Train: 2018-08-05T14:31:39.429437: step 11861, loss 0.58854.
Train: 2018-08-05T14:31:43.040125: step 11862, loss 0.518928.
Train: 2018-08-05T14:31:46.659823: step 11863, loss 0.622933.
Train: 2018-08-05T14:31:50.257477: step 11864, loss 0.536483.
Train: 2018-08-05T14:31:53.886214: step 11865, loss 0.562335.
Train: 2018-08-05T14:31:57.514922: step 11866, loss 0.553774.
Train: 2018-08-05T14:32:01.133609: step 11867, loss 0.630648.
Train: 2018-08-05T14:32:04.731268: step 11868, loss 0.596366.
Train: 2018-08-05T14:32:08.359987: step 11869, loss 0.562355.
Train: 2018-08-05T14:32:11.976667: step 11870, loss 0.570805.
Test: 2018-08-05T14:32:27.207868: step 11870, loss 0.54815.
Train: 2018-08-05T14:32:30.801014: step 11871, loss 0.587603.
Train: 2018-08-05T14:32:34.426728: step 11872, loss 0.5289.
Train: 2018-08-05T14:32:38.086561: step 11873, loss 0.545707.
Train: 2018-08-05T14:32:41.690714: step 11874, loss 0.529068.
Train: 2018-08-05T14:32:45.291840: step 11875, loss 0.587432.
Train: 2018-08-05T14:32:48.893504: step 11876, loss 0.570763.
Train: 2018-08-05T14:32:52.494170: step 11877, loss 0.554129.
Train: 2018-08-05T14:32:56.111366: step 11878, loss 0.66218.
Train: 2018-08-05T14:32:59.765645: step 11879, loss 0.537605.
Train: 2018-08-05T14:33:03.391354: step 11880, loss 0.570756.
Test: 2018-08-05T14:33:18.686144: step 11880, loss 0.54908.
Train: 2018-08-05T14:33:22.314902: step 11881, loss 0.545975.
Train: 2018-08-05T14:33:25.906022: step 11882, loss 0.579009.
Train: 2018-08-05T14:33:29.517721: step 11883, loss 0.620225.
Train: 2018-08-05T14:33:33.201608: step 11884, loss 0.578985.
Train: 2018-08-05T14:33:36.916546: step 11885, loss 0.59539.
Train: 2018-08-05T14:33:40.598922: step 11886, loss 0.595333.
Train: 2018-08-05T14:33:44.268775: step 11887, loss 0.61977.
Train: 2018-08-05T14:33:47.882444: step 11888, loss 0.578917.
Train: 2018-08-05T14:33:51.482088: step 11889, loss 0.619442.
Train: 2018-08-05T14:33:55.090790: step 11890, loss 0.586958.
Test: 2018-08-05T14:34:10.350004: step 11890, loss 0.54844.
Train: 2018-08-05T14:34:13.999301: step 11891, loss 0.611033.
Train: 2018-08-05T14:34:17.615021: step 11892, loss 0.594869.
Train: 2018-08-05T14:34:21.252261: step 11893, loss 0.546999.
Train: 2018-08-05T14:34:24.862920: step 11894, loss 0.618546.
Train: 2018-08-05T14:34:28.447020: step 11895, loss 0.555148.
Train: 2018-08-05T14:34:32.073755: step 11896, loss 0.539468.
Train: 2018-08-05T14:34:35.742595: step 11897, loss 0.523812.
Train: 2018-08-05T14:34:39.376352: step 11898, loss 0.476638.
Train: 2018-08-05T14:34:42.973987: step 11899, loss 0.523677.
Train: 2018-08-05T14:34:46.574633: step 11900, loss 0.55512.
Test: 2018-08-05T14:35:01.839403: step 11900, loss 0.54897.
Train: 2018-08-05T14:35:07.641926: step 11901, loss 0.555025.
Train: 2018-08-05T14:35:11.231567: step 11902, loss 0.538974.
Train: 2018-08-05T14:35:14.911446: step 11903, loss 0.594895.
Train: 2018-08-05T14:35:18.579773: step 11904, loss 0.538655.
Train: 2018-08-05T14:35:22.188960: step 11905, loss 0.514263.
Train: 2018-08-05T14:35:25.834739: step 11906, loss 0.61951.
Train: 2018-08-05T14:35:29.440425: step 11907, loss 0.595224.
Train: 2018-08-05T14:35:33.031059: step 11908, loss 0.497226.
Train: 2018-08-05T14:35:36.681865: step 11909, loss 0.595378.
Train: 2018-08-05T14:35:40.284514: step 11910, loss 0.562526.
Test: 2018-08-05T14:35:55.525743: step 11910, loss 0.54909.
Train: 2018-08-05T14:35:59.148460: step 11911, loss 0.579013.
Train: 2018-08-05T14:36:02.763694: step 11912, loss 0.620421.
Train: 2018-08-05T14:36:06.387915: step 11913, loss 0.504468.
Train: 2018-08-05T14:36:10.070754: step 11914, loss 0.537534.
Train: 2018-08-05T14:36:13.716519: step 11915, loss 0.554102.
Train: 2018-08-05T14:36:17.343240: step 11916, loss 0.57077.
Train: 2018-08-05T14:36:20.924834: step 11917, loss 0.570777.
Train: 2018-08-05T14:36:24.527509: step 11918, loss 0.570784.
Train: 2018-08-05T14:36:28.131673: step 11919, loss 0.495094.
Train: 2018-08-05T14:36:31.718787: step 11920, loss 0.562367.
Test: 2018-08-05T14:36:47.029075: step 11920, loss 0.548611.
Train: 2018-08-05T14:36:50.690382: step 11921, loss 0.553893.
Train: 2018-08-05T14:36:54.401329: step 11922, loss 0.604792.
Train: 2018-08-05T14:36:58.007982: step 11923, loss 0.604861.
Train: 2018-08-05T14:37:01.642731: step 11924, loss 0.545327.
Train: 2018-08-05T14:37:05.266451: step 11925, loss 0.502727.
Train: 2018-08-05T14:37:08.867089: step 11926, loss 0.502593.
Train: 2018-08-05T14:37:12.513839: step 11927, loss 0.545209.
Train: 2018-08-05T14:37:16.135066: step 11928, loss 0.579518.
Train: 2018-08-05T14:37:17.899345: step 11929, loss 0.617463.
Train: 2018-08-05T14:37:21.487008: step 11930, loss 0.501979.
Test: 2018-08-05T14:37:36.719680: step 11930, loss 0.547112.
Train: 2018-08-05T14:37:40.320824: step 11931, loss 0.596907.
Train: 2018-08-05T14:37:43.915986: step 11932, loss 0.519083.
Train: 2018-08-05T14:37:47.562787: step 11933, loss 0.579683.
Train: 2018-08-05T14:37:51.194027: step 11934, loss 0.492912.
Train: 2018-08-05T14:37:54.810233: step 11935, loss 0.553655.
Train: 2018-08-05T14:37:58.415899: step 11936, loss 0.623423.
Train: 2018-08-05T14:38:02.009996: step 11937, loss 0.518722.
Train: 2018-08-05T14:38:05.620183: step 11938, loss 0.54489.
Train: 2018-08-05T14:38:09.230852: step 11939, loss 0.614916.
Train: 2018-08-05T14:38:12.871606: step 11940, loss 0.553624.
Test: 2018-08-05T14:38:28.170473: step 11940, loss 0.547552.
Train: 2018-08-05T14:38:31.815745: step 11941, loss 0.597421.
Train: 2018-08-05T14:38:35.433947: step 11942, loss 0.544872.
Train: 2018-08-05T14:38:39.076218: step 11943, loss 0.544876.
Train: 2018-08-05T14:38:42.678350: step 11944, loss 0.501124.
Train: 2018-08-05T14:38:46.299081: step 11945, loss 0.544863.
Train: 2018-08-05T14:38:49.945858: step 11946, loss 0.588703.
Train: 2018-08-05T14:38:53.557531: step 11947, loss 0.562392.
Train: 2018-08-05T14:38:57.210807: step 11948, loss 0.562393.
Train: 2018-08-05T14:39:00.918216: step 11949, loss 0.544837.
Train: 2018-08-05T14:39:04.504837: step 11950, loss 0.492147.
Test: 2018-08-05T14:39:19.739599: step 11950, loss 0.54731.
Train: 2018-08-05T14:39:23.353291: step 11951, loss 0.509633.
Train: 2018-08-05T14:39:26.984035: step 11952, loss 0.588868.
Train: 2018-08-05T14:39:30.604767: step 11953, loss 0.580086.
Train: 2018-08-05T14:39:34.203889: step 11954, loss 0.571269.
Train: 2018-08-05T14:39:37.792008: step 11955, loss 0.544757.
Train: 2018-08-05T14:39:41.392169: step 11956, loss 0.509376.
Train: 2018-08-05T14:39:45.018890: step 11957, loss 0.562449.
Train: 2018-08-05T14:39:48.679187: step 11958, loss 0.597919.
Train: 2018-08-05T14:39:52.409179: step 11959, loss 0.615654.
Train: 2018-08-05T14:39:56.102114: step 11960, loss 0.527028.
Test: 2018-08-05T14:40:12.460684: step 11960, loss 0.547256.
Train: 2018-08-05T14:40:16.375048: step 11961, loss 0.544744.
Train: 2018-08-05T14:40:20.109072: step 11962, loss 0.562442.
Train: 2018-08-05T14:40:23.983636: step 11963, loss 0.668561.
Train: 2018-08-05T14:40:27.768796: step 11964, loss 0.641789.
Train: 2018-08-05T14:40:31.489798: step 11965, loss 0.527271.
Train: 2018-08-05T14:40:35.178690: step 11966, loss 0.553627.
Train: 2018-08-05T14:40:38.835509: step 11967, loss 0.518733.
Train: 2018-08-05T14:40:42.490324: step 11968, loss 0.501384.
Train: 2018-08-05T14:40:46.197275: step 11969, loss 0.544944.
Train: 2018-08-05T14:40:49.890202: step 11970, loss 0.614594.
Test: 2018-08-05T14:41:05.363149: step 11970, loss 0.548834.
Train: 2018-08-05T14:41:09.169260: step 11971, loss 0.6493.
Train: 2018-08-05T14:41:12.895016: step 11972, loss 0.562346.
Train: 2018-08-05T14:41:16.569890: step 11973, loss 0.614195.
Train: 2018-08-05T14:41:20.254264: step 11974, loss 0.579555.
Train: 2018-08-05T14:41:23.984566: step 11975, loss 0.510875.
Train: 2018-08-05T14:41:27.921260: step 11976, loss 0.493886.
Train: 2018-08-05T14:41:31.902651: step 11977, loss 0.536684.
Train: 2018-08-05T14:41:35.779281: step 11978, loss 0.536684.
Train: 2018-08-05T14:41:39.758431: step 11979, loss 0.468228.
Train: 2018-08-05T14:41:43.702857: step 11980, loss 0.502296.
Test: 2018-08-05T14:41:59.478868: step 11980, loss 0.548555.
Train: 2018-08-05T14:42:03.172743: step 11981, loss 0.545121.
Train: 2018-08-05T14:42:06.856130: step 11982, loss 0.605529.
Train: 2018-08-05T14:42:10.573088: step 11983, loss 0.666224.
Train: 2018-08-05T14:42:14.553685: step 11984, loss 0.57965.
Train: 2018-08-05T14:42:18.433564: step 11985, loss 0.527751.
Train: 2018-08-05T14:42:22.154539: step 11986, loss 0.536399.
Train: 2018-08-05T14:42:25.843437: step 11987, loss 0.510431.
Train: 2018-08-05T14:42:29.568441: step 11988, loss 0.510356.
Train: 2018-08-05T14:42:33.317743: step 11989, loss 0.571037.
Train: 2018-08-05T14:42:37.058785: step 11990, loss 0.57976.
Test: 2018-08-05T14:42:52.728619: step 11990, loss 0.54761.
Train: 2018-08-05T14:42:56.477680: step 11991, loss 0.536223.
Train: 2018-08-05T14:43:00.170063: step 11992, loss 0.632174.
Train: 2018-08-05T14:43:03.930187: step 11993, loss 0.518744.
Train: 2018-08-05T14:43:07.727871: step 11994, loss 0.614748.
Train: 2018-08-05T14:43:11.510494: step 11995, loss 0.544917.
Train: 2018-08-05T14:43:15.219452: step 11996, loss 0.562364.
Train: 2018-08-05T14:43:18.898313: step 11997, loss 0.571081.
Train: 2018-08-05T14:43:22.652373: step 11998, loss 0.501378.
Train: 2018-08-05T14:43:26.392475: step 11999, loss 0.658267.
Train: 2018-08-05T14:43:30.104464: step 12000, loss 0.571063.
Test: 2018-08-05T14:43:45.840022: step 12000, loss 0.547439.
Train: 2018-08-05T14:43:51.875213: step 12001, loss 0.597114.
Train: 2018-08-05T14:43:55.608245: step 12002, loss 0.571019.
Train: 2018-08-05T14:43:59.360296: step 12003, loss 0.588303.
Train: 2018-08-05T14:44:03.011085: step 12004, loss 0.536443.
Train: 2018-08-05T14:44:06.722052: step 12005, loss 0.562337.
Train: 2018-08-05T14:44:10.398901: step 12006, loss 0.57954.
Train: 2018-08-05T14:44:14.175023: step 12007, loss 0.467888.
Train: 2018-08-05T14:44:18.018316: step 12008, loss 0.562335.
Train: 2018-08-05T14:44:21.698938: step 12009, loss 0.545159.
Train: 2018-08-05T14:44:25.377825: step 12010, loss 0.510785.
Test: 2018-08-05T14:44:40.917096: step 12010, loss 0.548166.
Train: 2018-08-05T14:44:44.624025: step 12011, loss 0.588145.
Train: 2018-08-05T14:44:48.300905: step 12012, loss 0.648425.
Train: 2018-08-05T14:44:52.048973: step 12013, loss 0.562335.
Train: 2018-08-05T14:44:55.720838: step 12014, loss 0.588106.
Train: 2018-08-05T14:44:59.402722: step 12015, loss 0.519445.
Train: 2018-08-05T14:45:03.165791: step 12016, loss 0.605203.
Train: 2018-08-05T14:45:06.852696: step 12017, loss 0.545211.
Train: 2018-08-05T14:45:10.545119: step 12018, loss 0.588001.
Train: 2018-08-05T14:45:14.266120: step 12019, loss 0.519617.
Train: 2018-08-05T14:45:17.987103: step 12020, loss 0.596503.
Test: 2018-08-05T14:45:33.713144: step 12020, loss 0.547498.
Train: 2018-08-05T14:45:37.406056: step 12021, loss 0.579405.
Train: 2018-08-05T14:45:41.162108: step 12022, loss 0.50268.
Train: 2018-08-05T14:45:44.881612: step 12023, loss 0.536769.
Train: 2018-08-05T14:45:48.598489: step 12024, loss 0.528223.
Train: 2018-08-05T14:45:52.406220: step 12025, loss 0.587956.
Train: 2018-08-05T14:45:56.130224: step 12026, loss 0.570882.
Train: 2018-08-05T14:45:59.810102: step 12027, loss 0.553789.
Train: 2018-08-05T14:46:03.496498: step 12028, loss 0.536681.
Train: 2018-08-05T14:46:07.147284: step 12029, loss 0.622256.
Train: 2018-08-05T14:46:10.878784: step 12030, loss 0.528108.
Test: 2018-08-05T14:46:26.698070: step 12030, loss 0.546857.
Train: 2018-08-05T14:46:30.498245: step 12031, loss 0.553775.
Train: 2018-08-05T14:46:34.244312: step 12032, loss 0.502385.
Train: 2018-08-05T14:46:37.956263: step 12033, loss 0.605224.
Train: 2018-08-05T14:46:41.691888: step 12034, loss 0.57952.
Train: 2018-08-05T14:46:45.366734: step 12035, loss 0.648177.
Train: 2018-08-05T14:46:49.043611: step 12036, loss 0.50235.
Train: 2018-08-05T14:46:52.779639: step 12037, loss 0.596608.
Train: 2018-08-05T14:46:56.486626: step 12038, loss 0.613698.
Train: 2018-08-05T14:47:00.147453: step 12039, loss 0.502525.
Train: 2018-08-05T14:47:03.865444: step 12040, loss 0.519635.
Test: 2018-08-05T14:47:19.379844: step 12040, loss 0.547872.
Train: 2018-08-05T14:47:23.037656: step 12041, loss 0.587969.
Train: 2018-08-05T14:47:26.791729: step 12042, loss 0.545252.
Train: 2018-08-05T14:47:30.520231: step 12043, loss 0.562337.
Train: 2018-08-05T14:47:34.261300: step 12044, loss 0.562337.
Train: 2018-08-05T14:47:37.923128: step 12045, loss 0.596526.
Train: 2018-08-05T14:47:41.618045: step 12046, loss 0.511082.
Train: 2018-08-05T14:47:45.281442: step 12047, loss 0.596525.
Train: 2018-08-05T14:47:48.978080: step 12048, loss 0.536701.
Train: 2018-08-05T14:47:52.768287: step 12049, loss 0.570885.
Train: 2018-08-05T14:47:56.486265: step 12050, loss 0.511044.
Test: 2018-08-05T14:48:11.926752: step 12050, loss 0.547255.
Train: 2018-08-05T14:48:15.743984: step 12051, loss 0.570894.
Train: 2018-08-05T14:48:19.470003: step 12052, loss 0.510944.
Train: 2018-08-05T14:48:23.199041: step 12053, loss 0.553755.
Train: 2018-08-05T14:48:26.879922: step 12054, loss 0.579524.
Train: 2018-08-05T14:48:30.590375: step 12055, loss 0.622568.
Train: 2018-08-05T14:48:34.349229: step 12056, loss 0.536526.
Train: 2018-08-05T14:48:38.089879: step 12057, loss 0.579548.
Train: 2018-08-05T14:48:41.795711: step 12058, loss 0.519309.
Train: 2018-08-05T14:48:45.466562: step 12059, loss 0.657063.
Train: 2018-08-05T14:48:49.136417: step 12060, loss 0.613941.
Test: 2018-08-05T14:49:04.769668: step 12060, loss 0.547606.
Train: 2018-08-05T14:49:08.452056: step 12061, loss 0.562335.
Train: 2018-08-05T14:49:12.148759: step 12062, loss 0.562335.
Train: 2018-08-05T14:49:15.806580: step 12063, loss 0.579438.
Train: 2018-08-05T14:49:19.517552: step 12064, loss 0.596474.
Train: 2018-08-05T14:49:23.212000: step 12065, loss 0.553829.
Train: 2018-08-05T14:49:26.965779: step 12066, loss 0.485881.
Train: 2018-08-05T14:49:30.653189: step 12067, loss 0.528371.
Train: 2018-08-05T14:49:34.335574: step 12068, loss 0.579363.
Train: 2018-08-05T14:49:38.022449: step 12069, loss 0.536831.
Train: 2018-08-05T14:49:41.671250: step 12070, loss 0.528322.
Test: 2018-08-05T14:49:57.238809: step 12070, loss 0.547905.
Train: 2018-08-05T14:50:01.054720: step 12071, loss 0.579438.
Train: 2018-08-05T14:50:04.836877: step 12072, loss 0.579338.
Train: 2018-08-05T14:50:08.502005: step 12073, loss 0.528221.
Train: 2018-08-05T14:50:12.184396: step 12074, loss 0.579389.
Train: 2018-08-05T14:50:15.970554: step 12075, loss 0.630627.
Train: 2018-08-05T14:50:19.633368: step 12076, loss 0.545142.
Train: 2018-08-05T14:50:23.337293: step 12077, loss 0.596447.
Train: 2018-08-05T14:50:27.003102: step 12078, loss 0.665231.
Train: 2018-08-05T14:50:30.712061: step 12079, loss 0.562605.
Train: 2018-08-05T14:50:32.550037: step 12080, loss 0.507765.
Test: 2018-08-05T14:50:48.025374: step 12080, loss 0.547972.
Train: 2018-08-05T14:50:51.697723: step 12081, loss 0.477319.
Train: 2018-08-05T14:50:55.403657: step 12082, loss 0.622016.
Train: 2018-08-05T14:50:59.070497: step 12083, loss 0.545467.
Train: 2018-08-05T14:51:02.816551: step 12084, loss 0.511292.
Train: 2018-08-05T14:51:06.531504: step 12085, loss 0.596113.
Train: 2018-08-05T14:51:10.229425: step 12086, loss 0.528557.
Train: 2018-08-05T14:51:13.977477: step 12087, loss 0.613354.
Train: 2018-08-05T14:51:17.671408: step 12088, loss 0.580052.
Train: 2018-08-05T14:51:21.384039: step 12089, loss 0.6138.
Train: 2018-08-05T14:51:25.068753: step 12090, loss 0.528486.
Test: 2018-08-05T14:51:40.606752: step 12090, loss 0.549011.
Train: 2018-08-05T14:51:44.278635: step 12091, loss 0.579226.
Train: 2018-08-05T14:51:48.005631: step 12092, loss 0.638329.
Train: 2018-08-05T14:51:51.843109: step 12093, loss 0.54557.
Train: 2018-08-05T14:51:55.518010: step 12094, loss 0.579172.
Train: 2018-08-05T14:51:59.207386: step 12095, loss 0.503784.
Train: 2018-08-05T14:52:02.857656: step 12096, loss 0.545662.
Train: 2018-08-05T14:52:06.496405: step 12097, loss 0.554032.
Train: 2018-08-05T14:52:10.234923: step 12098, loss 0.604275.
Train: 2018-08-05T14:52:13.955922: step 12099, loss 0.570775.
Train: 2018-08-05T14:52:17.713004: step 12100, loss 0.537307.
Test: 2018-08-05T14:52:33.351287: step 12100, loss 0.547082.
Train: 2018-08-05T14:52:39.479201: step 12101, loss 0.478726.
Train: 2018-08-05T14:52:43.144037: step 12102, loss 0.545627.
Train: 2018-08-05T14:52:46.844975: step 12103, loss 0.503566.
Train: 2018-08-05T14:52:50.541364: step 12104, loss 0.62981.
Train: 2018-08-05T14:52:54.274367: step 12105, loss 0.587695.
Train: 2018-08-05T14:52:57.936195: step 12106, loss 0.604618.
Train: 2018-08-05T14:53:01.670192: step 12107, loss 0.486294.
Train: 2018-08-05T14:53:05.410215: step 12108, loss 0.545425.
Train: 2018-08-05T14:53:09.145258: step 12109, loss 0.62172.
Train: 2018-08-05T14:53:12.856221: step 12110, loss 0.587806.
Test: 2018-08-05T14:53:28.795297: step 12110, loss 0.547983.
Train: 2018-08-05T14:53:32.493191: step 12111, loss 0.511438.
Train: 2018-08-05T14:53:36.184598: step 12112, loss 0.579335.
Train: 2018-08-05T14:53:39.897548: step 12113, loss 0.562346.
Train: 2018-08-05T14:53:43.603504: step 12114, loss 0.562345.
Train: 2018-08-05T14:53:47.303435: step 12115, loss 0.570852.
Train: 2018-08-05T14:53:51.072565: step 12116, loss 0.63894.
Train: 2018-08-05T14:53:54.854703: step 12117, loss 0.468841.
Train: 2018-08-05T14:53:58.569670: step 12118, loss 0.528316.
Train: 2018-08-05T14:54:02.219470: step 12119, loss 0.587898.
Train: 2018-08-05T14:54:05.866765: step 12120, loss 0.570865.
Test: 2018-08-05T14:54:21.525639: step 12120, loss 0.548291.
Train: 2018-08-05T14:54:25.237351: step 12121, loss 0.54528.
Train: 2018-08-05T14:54:28.920998: step 12122, loss 0.562338.
Train: 2018-08-05T14:54:32.617934: step 12123, loss 0.511082.
Train: 2018-08-05T14:54:36.324867: step 12124, loss 0.613676.
Train: 2018-08-05T14:54:40.094492: step 12125, loss 0.6137.
Train: 2018-08-05T14:54:43.746160: step 12126, loss 0.622226.
Train: 2018-08-05T14:54:47.443071: step 12127, loss 0.511092.
Train: 2018-08-05T14:54:51.072818: step 12128, loss 0.528193.
Train: 2018-08-05T14:54:54.771745: step 12129, loss 0.579414.
Train: 2018-08-05T14:54:58.506479: step 12130, loss 0.570875.
Test: 2018-08-05T14:55:14.156255: step 12130, loss 0.547497.
Train: 2018-08-05T14:55:17.835418: step 12131, loss 0.605007.
Train: 2018-08-05T14:55:21.498856: step 12132, loss 0.56234.
Train: 2018-08-05T14:55:25.260965: step 12133, loss 0.536796.
Train: 2018-08-05T14:55:29.068696: step 12134, loss 0.553831.
Train: 2018-08-05T14:55:32.785721: step 12135, loss 0.579364.
Train: 2018-08-05T14:55:36.475640: step 12136, loss 0.562344.
Train: 2018-08-05T14:55:40.170128: step 12137, loss 0.579351.
Train: 2018-08-05T14:55:43.837973: step 12138, loss 0.570843.
Train: 2018-08-05T14:55:47.549935: step 12139, loss 0.545367.
Train: 2018-08-05T14:55:51.296989: step 12140, loss 0.613275.
Test: 2018-08-05T14:56:06.719671: step 12140, loss 0.548195.
Train: 2018-08-05T14:56:10.413058: step 12141, loss 0.486062.
Train: 2018-08-05T14:56:14.185193: step 12142, loss 0.613232.
Train: 2018-08-05T14:56:17.937258: step 12143, loss 0.494555.
Train: 2018-08-05T14:56:21.655748: step 12144, loss 0.452088.
Train: 2018-08-05T14:56:25.342661: step 12145, loss 0.528316.
Train: 2018-08-05T14:56:29.008508: step 12146, loss 0.59648.
Train: 2018-08-05T14:56:32.728495: step 12147, loss 0.588002.
Train: 2018-08-05T14:56:36.423888: step 12148, loss 0.545197.
Train: 2018-08-05T14:56:40.164412: step 12149, loss 0.622423.
Train: 2018-08-05T14:56:43.824767: step 12150, loss 0.5881.
Test: 2018-08-05T14:56:59.340384: step 12150, loss 0.54859.
Train: 2018-08-05T14:57:03.117020: step 12151, loss 0.613845.
Train: 2018-08-05T14:57:06.786361: step 12152, loss 0.579481.
Train: 2018-08-05T14:57:10.513353: step 12153, loss 0.587998.
Train: 2018-08-05T14:57:14.254879: step 12154, loss 0.519777.
Train: 2018-08-05T14:57:17.978375: step 12155, loss 0.579416.
Train: 2018-08-05T14:57:21.656268: step 12156, loss 0.613497.
Train: 2018-08-05T14:57:25.373269: step 12157, loss 0.485729.
Train: 2018-08-05T14:57:29.172485: step 12158, loss 0.613403.
Train: 2018-08-05T14:57:32.879466: step 12159, loss 0.502882.
Train: 2018-08-05T14:57:36.579411: step 12160, loss 0.655874.
Test: 2018-08-05T14:57:52.216620: step 12160, loss 0.547781.
Train: 2018-08-05T14:57:55.871421: step 12161, loss 0.528394.
Train: 2018-08-05T14:57:59.540261: step 12162, loss 0.579315.
Train: 2018-08-05T14:58:03.217130: step 12163, loss 0.46915.
Train: 2018-08-05T14:58:06.936835: step 12164, loss 0.604755.
Train: 2018-08-05T14:58:10.634768: step 12165, loss 0.528428.
Train: 2018-08-05T14:58:14.426465: step 12166, loss 0.494454.
Train: 2018-08-05T14:58:18.182558: step 12167, loss 0.579359.
Train: 2018-08-05T14:58:21.841873: step 12168, loss 0.519758.
Train: 2018-08-05T14:58:25.564875: step 12169, loss 0.596482.
Train: 2018-08-05T14:58:29.287136: step 12170, loss 0.596523.
Test: 2018-08-05T14:58:44.745933: step 12170, loss 0.548843.
Train: 2018-08-05T14:58:48.529611: step 12171, loss 0.562342.
Train: 2018-08-05T14:58:52.229051: step 12172, loss 0.442578.
Train: 2018-08-05T14:58:55.925956: step 12173, loss 0.553758.
Train: 2018-08-05T14:58:59.614853: step 12174, loss 0.527937.
Train: 2018-08-05T14:59:03.338869: step 12175, loss 0.553713.
Train: 2018-08-05T14:59:07.111491: step 12176, loss 0.605585.
Train: 2018-08-05T14:59:10.855035: step 12177, loss 0.57967.
Train: 2018-08-05T14:59:14.587048: step 12178, loss 0.501646.
Train: 2018-08-05T14:59:18.232867: step 12179, loss 0.536284.
Train: 2018-08-05T14:59:21.869600: step 12180, loss 0.579801.
Test: 2018-08-05T14:59:37.508394: step 12180, loss 0.548802.
Train: 2018-08-05T14:59:41.234418: step 12181, loss 0.571087.
Train: 2018-08-05T14:59:44.957404: step 12182, loss 0.579835.
Train: 2018-08-05T14:59:48.783665: step 12183, loss 0.527419.
Train: 2018-08-05T14:59:52.591865: step 12184, loss 0.623612.
Train: 2018-08-05T14:59:56.292801: step 12185, loss 0.606105.
Train: 2018-08-05T14:59:59.940585: step 12186, loss 0.562371.
Train: 2018-08-05T15:00:03.829973: step 12187, loss 0.55364.
Train: 2018-08-05T15:00:07.479756: step 12188, loss 0.597234.
Train: 2018-08-05T15:00:11.151624: step 12189, loss 0.510134.
Train: 2018-08-05T15:00:14.912717: step 12190, loss 0.536258.
Test: 2018-08-05T15:00:30.509816: step 12190, loss 0.547227.
Train: 2018-08-05T15:00:34.169135: step 12191, loss 0.666739.
Train: 2018-08-05T15:00:37.877573: step 12192, loss 0.562349.
Train: 2018-08-05T15:00:41.595581: step 12193, loss 0.527693.
Train: 2018-08-05T15:00:45.276849: step 12194, loss 0.64887.
Train: 2018-08-05T15:00:48.948897: step 12195, loss 0.553709.
Train: 2018-08-05T15:00:52.703979: step 12196, loss 0.502079.
Train: 2018-08-05T15:00:56.396882: step 12197, loss 0.527938.
Train: 2018-08-05T15:01:00.030639: step 12198, loss 0.476368.
Train: 2018-08-05T15:01:03.805782: step 12199, loss 0.527902.
Train: 2018-08-05T15:01:07.483635: step 12200, loss 0.510592.
Test: 2018-08-05T15:01:23.001117: step 12200, loss 0.547104.
Train: 2018-08-05T15:01:29.011727: step 12201, loss 0.596927.
Train: 2018-08-05T15:01:32.708164: step 12202, loss 0.536362.
Train: 2018-08-05T15:01:36.449229: step 12203, loss 0.579702.
Train: 2018-08-05T15:01:40.081983: step 12204, loss 0.527599.
Train: 2018-08-05T15:01:43.771879: step 12205, loss 0.553654.
Train: 2018-08-05T15:01:47.471801: step 12206, loss 0.57108.
Train: 2018-08-05T15:01:51.299062: step 12207, loss 0.667108.
Train: 2018-08-05T15:01:55.306867: step 12208, loss 0.492603.
Train: 2018-08-05T15:01:59.067993: step 12209, loss 0.588535.
Train: 2018-08-05T15:02:02.858171: step 12210, loss 0.571085.
Test: 2018-08-05T15:02:18.594495: step 12210, loss 0.548206.
Train: 2018-08-05T15:02:22.290913: step 12211, loss 0.588512.
Train: 2018-08-05T15:02:25.993838: step 12212, loss 0.510115.
Train: 2018-08-05T15:02:29.701772: step 12213, loss 0.579774.
Train: 2018-08-05T15:02:33.479961: step 12214, loss 0.597172.
Train: 2018-08-05T15:02:37.366920: step 12215, loss 0.53627.
Train: 2018-08-05T15:02:41.176173: step 12216, loss 0.527598.
Train: 2018-08-05T15:02:44.889102: step 12217, loss 0.527591.
Train: 2018-08-05T15:02:48.610608: step 12218, loss 0.61454.
Train: 2018-08-05T15:02:52.287967: step 12219, loss 0.614504.
Train: 2018-08-05T15:02:56.014811: step 12220, loss 0.571025.
Test: 2018-08-05T15:03:11.793916: step 12220, loss 0.547279.
Train: 2018-08-05T15:03:15.471777: step 12221, loss 0.553682.
Train: 2018-08-05T15:03:19.178719: step 12222, loss 0.588295.
Train: 2018-08-05T15:03:22.903705: step 12223, loss 0.57961.
Train: 2018-08-05T15:03:26.636563: step 12224, loss 0.553718.
Train: 2018-08-05T15:03:30.334467: step 12225, loss 0.588149.
Train: 2018-08-05T15:03:34.051468: step 12226, loss 0.588096.
Train: 2018-08-05T15:03:37.796548: step 12227, loss 0.596602.
Train: 2018-08-05T15:03:41.561171: step 12228, loss 0.528166.
Train: 2018-08-05T15:03:45.301719: step 12229, loss 0.545286.
Train: 2018-08-05T15:03:48.993465: step 12230, loss 0.536796.
Test: 2018-08-05T15:04:04.629281: step 12230, loss 0.548328.
Train: 2018-08-05T15:04:06.393021: step 12231, loss 0.562343.
Train: 2018-08-05T15:04:10.194939: step 12232, loss 0.55384.
Train: 2018-08-05T15:04:13.995127: step 12233, loss 0.587849.
Train: 2018-08-05T15:04:17.765257: step 12234, loss 0.460416.
Train: 2018-08-05T15:04:21.459759: step 12235, loss 0.58786.
Train: 2018-08-05T15:04:25.130602: step 12236, loss 0.604898.
Train: 2018-08-05T15:04:28.796353: step 12237, loss 0.579362.
Train: 2018-08-05T15:04:32.538910: step 12238, loss 0.553839.
Train: 2018-08-05T15:04:36.330610: step 12239, loss 0.494323.
Train: 2018-08-05T15:04:40.093719: step 12240, loss 0.596392.
Test: 2018-08-05T15:04:55.577041: step 12240, loss 0.547925.
Train: 2018-08-05T15:04:59.286475: step 12241, loss 0.579372.
Train: 2018-08-05T15:05:03.060097: step 12242, loss 0.562342.
Train: 2018-08-05T15:05:06.731948: step 12243, loss 0.528282.
Train: 2018-08-05T15:05:10.400782: step 12244, loss 0.536778.
Train: 2018-08-05T15:05:14.120282: step 12245, loss 0.553809.
Train: 2018-08-05T15:05:17.828710: step 12246, loss 0.494014.
Train: 2018-08-05T15:05:21.534656: step 12247, loss 0.647945.
Train: 2018-08-05T15:05:25.283633: step 12248, loss 0.55377.
Train: 2018-08-05T15:05:29.047718: step 12249, loss 0.536625.
Train: 2018-08-05T15:05:32.757707: step 12250, loss 0.519442.
Test: 2018-08-05T15:05:48.192460: step 12250, loss 0.546995.
Train: 2018-08-05T15:05:51.961100: step 12251, loss 0.527962.
Train: 2018-08-05T15:05:55.681079: step 12252, loss 0.562336.
Train: 2018-08-05T15:05:59.387638: step 12253, loss 0.519196.
Train: 2018-08-05T15:06:03.074541: step 12254, loss 0.484492.
Train: 2018-08-05T15:06:06.779981: step 12255, loss 0.640485.
Train: 2018-08-05T15:06:10.471393: step 12256, loss 0.597142.
Train: 2018-08-05T15:06:14.247529: step 12257, loss 0.510136.
Train: 2018-08-05T15:06:18.029183: step 12258, loss 0.58852.
Train: 2018-08-05T15:06:21.780237: step 12259, loss 0.597271.
Train: 2018-08-05T15:06:25.514234: step 12260, loss 0.518734.
Test: 2018-08-05T15:06:41.086850: step 12260, loss 0.546377.
Train: 2018-08-05T15:06:44.837952: step 12261, loss 0.606035.
Train: 2018-08-05T15:06:48.558947: step 12262, loss 0.492519.
Train: 2018-08-05T15:06:52.227788: step 12263, loss 0.509933.
Train: 2018-08-05T15:06:55.956302: step 12264, loss 0.571136.
Train: 2018-08-05T15:06:59.606599: step 12265, loss 0.55362.
Train: 2018-08-05T15:07:03.356320: step 12266, loss 0.553615.
Train: 2018-08-05T15:07:07.034693: step 12267, loss 0.553611.
Train: 2018-08-05T15:07:10.698016: step 12268, loss 0.536012.
Train: 2018-08-05T15:07:14.426017: step 12269, loss 0.562413.
Train: 2018-08-05T15:07:18.131965: step 12270, loss 0.553601.
Test: 2018-08-05T15:07:33.676301: step 12270, loss 0.548293.
Train: 2018-08-05T15:07:37.376129: step 12271, loss 0.544771.
Train: 2018-08-05T15:07:41.051501: step 12272, loss 0.500576.
Train: 2018-08-05T15:07:44.730353: step 12273, loss 0.61557.
Train: 2018-08-05T15:07:48.385675: step 12274, loss 0.456141.
Train: 2018-08-05T15:07:52.190901: step 12275, loss 0.54471.
Train: 2018-08-05T15:07:55.918902: step 12276, loss 0.589187.
Train: 2018-08-05T15:07:59.583231: step 12277, loss 0.544676.
Train: 2018-08-05T15:08:03.247570: step 12278, loss 0.580359.
Train: 2018-08-05T15:08:06.952502: step 12279, loss 0.616099.
Train: 2018-08-05T15:08:10.678333: step 12280, loss 0.562514.
Test: 2018-08-05T15:08:26.461360: step 12280, loss 0.546585.
Train: 2018-08-05T15:08:30.329758: step 12281, loss 0.51791.
Train: 2018-08-05T15:08:34.438839: step 12282, loss 0.598187.
Train: 2018-08-05T15:08:38.219540: step 12283, loss 0.544676.
Train: 2018-08-05T15:08:41.965595: step 12284, loss 0.58031.
Train: 2018-08-05T15:08:45.774846: step 12285, loss 0.571385.
Train: 2018-08-05T15:08:49.672323: step 12286, loss 0.500269.
Train: 2018-08-05T15:08:53.567247: step 12287, loss 0.526934.
Train: 2018-08-05T15:08:57.392033: step 12288, loss 0.606916.
Train: 2018-08-05T15:09:01.050855: step 12289, loss 0.642405.
Train: 2018-08-05T15:09:04.744741: step 12290, loss 0.535869.
Test: 2018-08-05T15:09:20.670699: step 12290, loss 0.548277.
Train: 2018-08-05T15:09:24.341065: step 12291, loss 0.597827.
Train: 2018-08-05T15:09:28.034980: step 12292, loss 0.544773.
Train: 2018-08-05T15:09:31.733914: step 12293, loss 0.544795.
Train: 2018-08-05T15:09:35.431854: step 12294, loss 0.527223.
Train: 2018-08-05T15:09:39.073640: step 12295, loss 0.536036.
Train: 2018-08-05T15:09:42.764539: step 12296, loss 0.597532.
Train: 2018-08-05T15:09:46.387264: step 12297, loss 0.597485.
Train: 2018-08-05T15:09:49.986934: step 12298, loss 0.553624.
Train: 2018-08-05T15:09:53.582572: step 12299, loss 0.536143.
Train: 2018-08-05T15:09:57.451457: step 12300, loss 0.571105.
Test: 2018-08-05T15:10:12.937929: step 12300, loss 0.548197.
Train: 2018-08-05T15:10:18.717403: step 12301, loss 0.536193.
Train: 2018-08-05T15:10:22.316059: step 12302, loss 0.544926.
Train: 2018-08-05T15:10:25.937249: step 12303, loss 0.597221.
Train: 2018-08-05T15:10:29.554461: step 12304, loss 0.553653.
Train: 2018-08-05T15:10:33.169154: step 12305, loss 0.518867.
Train: 2018-08-05T15:10:36.810429: step 12306, loss 0.562355.
Train: 2018-08-05T15:10:40.436675: step 12307, loss 0.544963.
Train: 2018-08-05T15:10:44.061911: step 12308, loss 0.510174.
Train: 2018-08-05T15:10:47.665057: step 12309, loss 0.466597.
Train: 2018-08-05T15:10:51.272750: step 12310, loss 0.509996.
Test: 2018-08-05T15:11:06.551603: step 12310, loss 0.547757.
Train: 2018-08-05T15:11:10.179842: step 12311, loss 0.536111.
Train: 2018-08-05T15:11:13.838149: step 12312, loss 0.562398.
Train: 2018-08-05T15:11:17.486975: step 12313, loss 0.500739.
Train: 2018-08-05T15:11:21.134780: step 12314, loss 0.562437.
Train: 2018-08-05T15:11:24.756483: step 12315, loss 0.56246.
Train: 2018-08-05T15:11:28.350123: step 12316, loss 0.544697.
Train: 2018-08-05T15:11:31.969327: step 12317, loss 0.580327.
Train: 2018-08-05T15:11:35.776058: step 12318, loss 0.491092.
Train: 2018-08-05T15:11:39.659505: step 12319, loss 0.562542.
Train: 2018-08-05T15:11:43.309297: step 12320, loss 0.553593.
Test: 2018-08-05T15:11:58.569619: step 12320, loss 0.549436.
Train: 2018-08-05T15:12:02.186819: step 12321, loss 0.634483.
Train: 2018-08-05T15:12:05.804519: step 12322, loss 0.544608.
Train: 2018-08-05T15:12:09.399660: step 12323, loss 0.562589.
Train: 2018-08-05T15:12:13.021865: step 12324, loss 0.643512.
Train: 2018-08-05T15:12:16.650105: step 12325, loss 0.544618.
Train: 2018-08-05T15:12:20.244741: step 12326, loss 0.526699.
Train: 2018-08-05T15:12:23.838877: step 12327, loss 0.607334.
Train: 2018-08-05T15:12:27.439532: step 12328, loss 0.598298.
Train: 2018-08-05T15:12:31.053197: step 12329, loss 0.651711.
Train: 2018-08-05T15:12:34.698986: step 12330, loss 0.624669.
Test: 2018-08-05T15:12:49.975779: step 12330, loss 0.547874.
Train: 2018-08-05T15:12:53.591475: step 12331, loss 0.52707.
Train: 2018-08-05T15:12:57.192142: step 12332, loss 0.544797.
Train: 2018-08-05T15:13:00.788796: step 12333, loss 0.650172.
Train: 2018-08-05T15:13:04.397474: step 12334, loss 0.684677.
Train: 2018-08-05T15:13:08.000127: step 12335, loss 0.553671.
Train: 2018-08-05T15:13:11.645919: step 12336, loss 0.553711.
Train: 2018-08-05T15:13:15.274651: step 12337, loss 0.588078.
Train: 2018-08-05T15:13:18.870308: step 12338, loss 0.545267.
Train: 2018-08-05T15:13:22.502561: step 12339, loss 0.613332.
Train: 2018-08-05T15:13:26.097703: step 12340, loss 0.52854.
Test: 2018-08-05T15:13:41.470205: step 12340, loss 0.549471.
Train: 2018-08-05T15:13:45.109464: step 12341, loss 0.537106.
Train: 2018-08-05T15:13:48.809370: step 12342, loss 0.528793.
Train: 2018-08-05T15:13:52.470168: step 12343, loss 0.503715.
Train: 2018-08-05T15:13:56.091385: step 12344, loss 0.537259.
Train: 2018-08-05T15:13:59.711578: step 12345, loss 0.595925.
Train: 2018-08-05T15:14:03.320263: step 12346, loss 0.570778.
Train: 2018-08-05T15:14:06.997125: step 12347, loss 0.528901.
Train: 2018-08-05T15:14:10.632888: step 12348, loss 0.587534.
Train: 2018-08-05T15:14:14.266150: step 12349, loss 0.537268.
Train: 2018-08-05T15:14:17.876339: step 12350, loss 0.554016.
Test: 2018-08-05T15:14:33.094961: step 12350, loss 0.547613.
Train: 2018-08-05T15:14:36.704123: step 12351, loss 0.579169.
Train: 2018-08-05T15:14:40.314302: step 12352, loss 0.604347.
Train: 2018-08-05T15:14:43.930508: step 12353, loss 0.612719.
Train: 2018-08-05T15:14:47.561242: step 12354, loss 0.520519.
Train: 2018-08-05T15:14:51.186997: step 12355, loss 0.528905.
Train: 2018-08-05T15:14:54.799664: step 12356, loss 0.579157.
Train: 2018-08-05T15:14:58.390303: step 12357, loss 0.595921.
Train: 2018-08-05T15:15:02.002988: step 12358, loss 0.59591.
Train: 2018-08-05T15:15:05.615675: step 12359, loss 0.520558.
Train: 2018-08-05T15:15:09.221840: step 12360, loss 0.545665.
Test: 2018-08-05T15:15:24.495156: step 12360, loss 0.548031.
Train: 2018-08-05T15:15:28.176557: step 12361, loss 0.662894.
Train: 2018-08-05T15:15:31.815808: step 12362, loss 0.587496.
Train: 2018-08-05T15:15:35.451049: step 12363, loss 0.587462.
Train: 2018-08-05T15:15:39.058717: step 12364, loss 0.562434.
Train: 2018-08-05T15:15:42.826796: step 12365, loss 0.579077.
Train: 2018-08-05T15:15:46.431498: step 12366, loss 0.537553.
Train: 2018-08-05T15:15:50.063252: step 12367, loss 0.587345.
Train: 2018-08-05T15:15:53.714032: step 12368, loss 0.521057.
Train: 2018-08-05T15:15:57.334737: step 12369, loss 0.637016.
Train: 2018-08-05T15:16:00.927392: step 12370, loss 0.562487.
Test: 2018-08-05T15:16:16.257875: step 12370, loss 0.548132.
Train: 2018-08-05T15:16:19.883104: step 12371, loss 0.504679.
Train: 2018-08-05T15:16:23.515862: step 12372, loss 0.620327.
Train: 2018-08-05T15:16:27.169653: step 12373, loss 0.612031.
Train: 2018-08-05T15:16:30.812414: step 12374, loss 0.513066.
Train: 2018-08-05T15:16:34.393002: step 12375, loss 0.578996.
Train: 2018-08-05T15:16:37.987638: step 12376, loss 0.554286.
Train: 2018-08-05T15:16:41.594804: step 12377, loss 0.578992.
Train: 2018-08-05T15:16:45.221041: step 12378, loss 0.562524.
Train: 2018-08-05T15:16:48.852781: step 12379, loss 0.537825.
Train: 2018-08-05T15:16:52.481505: step 12380, loss 0.669618.
Test: 2018-08-05T15:17:07.753269: step 12380, loss 0.548226.
Train: 2018-08-05T15:17:11.390534: step 12381, loss 0.578984.
Train: 2018-08-05T15:17:13.151783: step 12382, loss 0.580068.
Train: 2018-08-05T15:17:16.761477: step 12383, loss 0.562561.
Train: 2018-08-05T15:17:20.337576: step 12384, loss 0.538003.
Train: 2018-08-05T15:17:23.972834: step 12385, loss 0.546204.
Train: 2018-08-05T15:17:27.611590: step 12386, loss 0.587141.
Train: 2018-08-05T15:17:31.209240: step 12387, loss 0.578952.
Train: 2018-08-05T15:17:34.790843: step 12388, loss 0.521659.
Train: 2018-08-05T15:17:38.402533: step 12389, loss 0.505237.
Train: 2018-08-05T15:17:42.007706: step 12390, loss 0.587177.
Test: 2018-08-05T15:17:57.280958: step 12390, loss 0.548998.
Train: 2018-08-05T15:18:00.936758: step 12391, loss 0.529654.
Train: 2018-08-05T15:18:04.565996: step 12392, loss 0.578996.
Train: 2018-08-05T15:18:08.192237: step 12393, loss 0.653303.
Train: 2018-08-05T15:18:11.799922: step 12394, loss 0.53774.
Train: 2018-08-05T15:18:15.394573: step 12395, loss 0.587274.
Train: 2018-08-05T15:18:19.007262: step 12396, loss 0.562496.
Train: 2018-08-05T15:18:22.659071: step 12397, loss 0.496392.
Train: 2018-08-05T15:18:26.328911: step 12398, loss 0.603867.
Train: 2018-08-05T15:18:29.940096: step 12399, loss 0.529333.
Train: 2018-08-05T15:18:33.561303: step 12400, loss 0.56246.
Test: 2018-08-05T15:18:48.849119: step 12400, loss 0.548759.
Train: 2018-08-05T15:18:54.687242: step 12401, loss 0.587383.
Train: 2018-08-05T15:18:58.277360: step 12402, loss 0.545801.
Train: 2018-08-05T15:19:01.909606: step 12403, loss 0.554101.
Train: 2018-08-05T15:19:05.530328: step 12404, loss 0.579112.
Train: 2018-08-05T15:19:09.135011: step 12405, loss 0.545708.
Train: 2018-08-05T15:19:12.787299: step 12406, loss 0.57914.
Train: 2018-08-05T15:19:16.387445: step 12407, loss 0.545651.
Train: 2018-08-05T15:19:20.004131: step 12408, loss 0.528848.
Train: 2018-08-05T15:19:23.631863: step 12409, loss 0.570788.
Train: 2018-08-05T15:19:27.265605: step 12410, loss 0.621302.
Test: 2018-08-05T15:19:42.532423: step 12410, loss 0.548311.
Train: 2018-08-05T15:19:46.168179: step 12411, loss 0.52869.
Train: 2018-08-05T15:19:49.766818: step 12412, loss 0.55394.
Train: 2018-08-05T15:19:53.353444: step 12413, loss 0.562366.
Train: 2018-08-05T15:19:56.945083: step 12414, loss 0.511659.
Train: 2018-08-05T15:20:00.644002: step 12415, loss 0.596228.
Train: 2018-08-05T15:20:04.369504: step 12416, loss 0.579309.
Train: 2018-08-05T15:20:07.970658: step 12417, loss 0.536893.
Train: 2018-08-05T15:20:11.571335: step 12418, loss 0.528361.
Train: 2018-08-05T15:20:15.163466: step 12419, loss 0.587878.
Train: 2018-08-05T15:20:18.769625: step 12420, loss 0.579384.
Test: 2018-08-05T15:20:34.058966: step 12420, loss 0.548098.
Train: 2018-08-05T15:20:37.694712: step 12421, loss 0.56234.
Train: 2018-08-05T15:20:41.347031: step 12422, loss 0.596472.
Train: 2018-08-05T15:20:44.976774: step 12423, loss 0.613534.
Train: 2018-08-05T15:20:48.589456: step 12424, loss 0.502678.
Train: 2018-08-05T15:20:52.190129: step 12425, loss 0.57939.
Train: 2018-08-05T15:20:55.779235: step 12426, loss 0.587911.
Train: 2018-08-05T15:20:59.378427: step 12427, loss 0.579378.
Train: 2018-08-05T15:21:03.005165: step 12428, loss 0.621917.
Train: 2018-08-05T15:21:06.636898: step 12429, loss 0.570841.
Train: 2018-08-05T15:21:10.247580: step 12430, loss 0.545397.
Test: 2018-08-05T15:21:25.489354: step 12430, loss 0.548217.
Train: 2018-08-05T15:21:29.108087: step 12431, loss 0.545424.
Train: 2018-08-05T15:21:32.710752: step 12432, loss 0.528524.
Train: 2018-08-05T15:21:36.314411: step 12433, loss 0.520068.
Train: 2018-08-05T15:21:39.954185: step 12434, loss 0.528495.
Train: 2018-08-05T15:21:43.660165: step 12435, loss 0.579308.
Train: 2018-08-05T15:21:47.268873: step 12436, loss 0.545376.
Train: 2018-08-05T15:21:50.881052: step 12437, loss 0.468874.
Train: 2018-08-05T15:21:54.476201: step 12438, loss 0.545294.
Train: 2018-08-05T15:21:58.077854: step 12439, loss 0.579436.
Train: 2018-08-05T15:22:01.717620: step 12440, loss 0.553765.
Test: 2018-08-05T15:22:16.972904: step 12440, loss 0.54878.
Train: 2018-08-05T15:22:20.589106: step 12441, loss 0.519386.
Train: 2018-08-05T15:22:24.195778: step 12442, loss 0.493418.
Train: 2018-08-05T15:22:27.813488: step 12443, loss 0.553693.
Train: 2018-08-05T15:22:31.419680: step 12444, loss 0.527632.
Train: 2018-08-05T15:22:35.028867: step 12445, loss 0.544933.
Train: 2018-08-05T15:22:38.668604: step 12446, loss 0.501166.
Train: 2018-08-05T15:22:42.329911: step 12447, loss 0.606306.
Train: 2018-08-05T15:22:45.953113: step 12448, loss 0.562415.
Train: 2018-08-05T15:22:49.565769: step 12449, loss 0.562445.
Train: 2018-08-05T15:22:53.185968: step 12450, loss 0.562433.
Test: 2018-08-05T15:23:08.465249: step 12450, loss 0.547242.
Train: 2018-08-05T15:23:12.086950: step 12451, loss 0.580168.
Train: 2018-08-05T15:23:15.705646: step 12452, loss 0.589115.
Train: 2018-08-05T15:23:19.352933: step 12453, loss 0.597974.
Train: 2018-08-05T15:23:22.959098: step 12454, loss 0.571338.
Train: 2018-08-05T15:23:26.566772: step 12455, loss 0.527.
Train: 2018-08-05T15:23:30.162925: step 12456, loss 0.589037.
Train: 2018-08-05T15:23:33.817220: step 12457, loss 0.668689.
Train: 2018-08-05T15:23:37.481056: step 12458, loss 0.58891.
Train: 2018-08-05T15:23:41.174954: step 12459, loss 0.553607.
Train: 2018-08-05T15:23:44.792656: step 12460, loss 0.474654.
Test: 2018-08-05T15:24:00.075468: step 12460, loss 0.547547.
Train: 2018-08-05T15:24:03.679162: step 12461, loss 0.544857.
Train: 2018-08-05T15:24:07.287845: step 12462, loss 0.562381.
Train: 2018-08-05T15:24:10.925596: step 12463, loss 0.527377.
Train: 2018-08-05T15:24:14.548282: step 12464, loss 0.579874.
Train: 2018-08-05T15:24:18.174016: step 12465, loss 0.597348.
Train: 2018-08-05T15:24:21.799730: step 12466, loss 0.562369.
Train: 2018-08-05T15:24:25.396370: step 12467, loss 0.51876.
Train: 2018-08-05T15:24:29.007047: step 12468, loss 0.553645.
Train: 2018-08-05T15:24:32.608700: step 12469, loss 0.579791.
Train: 2018-08-05T15:24:36.237425: step 12470, loss 0.614611.
Test: 2018-08-05T15:24:51.507759: step 12470, loss 0.547834.
Train: 2018-08-05T15:24:55.134997: step 12471, loss 0.605823.
Train: 2018-08-05T15:24:58.886551: step 12472, loss 0.475621.
Train: 2018-08-05T15:25:02.491708: step 12473, loss 0.562346.
Train: 2018-08-05T15:25:06.107388: step 12474, loss 0.545021.
Train: 2018-08-05T15:25:09.697513: step 12475, loss 0.510387.
Train: 2018-08-05T15:25:13.305680: step 12476, loss 0.562345.
Train: 2018-08-05T15:25:16.906340: step 12477, loss 0.501656.
Train: 2018-08-05T15:25:20.515030: step 12478, loss 0.571034.
Train: 2018-08-05T15:25:24.114170: step 12479, loss 0.588436.
Train: 2018-08-05T15:25:27.740385: step 12480, loss 0.562355.
Test: 2018-08-05T15:25:43.020222: step 12480, loss 0.548025.
Train: 2018-08-05T15:25:46.702107: step 12481, loss 0.51885.
Train: 2018-08-05T15:25:50.324829: step 12482, loss 0.544939.
Train: 2018-08-05T15:25:53.977619: step 12483, loss 0.579806.
Train: 2018-08-05T15:25:57.609869: step 12484, loss 0.501279.
Train: 2018-08-05T15:26:01.219044: step 12485, loss 0.466221.
Train: 2018-08-05T15:26:04.829739: step 12486, loss 0.518543.
Train: 2018-08-05T15:26:08.449941: step 12487, loss 0.553607.
Train: 2018-08-05T15:26:12.049602: step 12488, loss 0.588913.
Train: 2018-08-05T15:26:15.667806: step 12489, loss 0.615535.
Train: 2018-08-05T15:26:19.283496: step 12490, loss 0.589019.
Test: 2018-08-05T15:26:34.551735: step 12490, loss 0.546841.
Train: 2018-08-05T15:26:38.162902: step 12491, loss 0.60674.
Train: 2018-08-05T15:26:41.750034: step 12492, loss 0.615547.
Train: 2018-08-05T15:26:45.362219: step 12493, loss 0.571279.
Train: 2018-08-05T15:26:48.971886: step 12494, loss 0.553602.
Train: 2018-08-05T15:26:52.615668: step 12495, loss 0.553608.
Train: 2018-08-05T15:26:56.241410: step 12496, loss 0.571172.
Train: 2018-08-05T15:26:59.870135: step 12497, loss 0.483382.
Train: 2018-08-05T15:27:03.491827: step 12498, loss 0.536025.
Train: 2018-08-05T15:27:07.104509: step 12499, loss 0.623745.
Train: 2018-08-05T15:27:10.712679: step 12500, loss 0.651506.
Test: 2018-08-05T15:27:25.961413: step 12500, loss 0.54671.
Train: 2018-08-05T15:27:31.798530: step 12501, loss 0.606245.
Train: 2018-08-05T15:27:35.414223: step 12502, loss 0.545049.
Train: 2018-08-05T15:27:39.030931: step 12503, loss 0.623188.
Train: 2018-08-05T15:27:42.632595: step 12504, loss 0.553693.
Train: 2018-08-05T15:27:46.234260: step 12505, loss 0.502003.
Train: 2018-08-05T15:27:49.833912: step 12506, loss 0.588143.
Train: 2018-08-05T15:27:53.464148: step 12507, loss 0.562335.
Train: 2018-08-05T15:27:57.087864: step 12508, loss 0.588031.
Train: 2018-08-05T15:28:00.696545: step 12509, loss 0.587971.
Train: 2018-08-05T15:28:04.300217: step 12510, loss 0.502687.
Test: 2018-08-05T15:28:19.660759: step 12510, loss 0.547344.
Train: 2018-08-05T15:28:23.310045: step 12511, loss 0.553831.
Train: 2018-08-05T15:28:26.920743: step 12512, loss 0.570848.
Train: 2018-08-05T15:28:30.551480: step 12513, loss 0.596329.
Train: 2018-08-05T15:28:34.184229: step 12514, loss 0.613246.
Train: 2018-08-05T15:28:37.791910: step 12515, loss 0.60467.
Train: 2018-08-05T15:28:41.385556: step 12516, loss 0.55393.
Train: 2018-08-05T15:28:45.004243: step 12517, loss 0.545544.
Train: 2018-08-05T15:28:48.676084: step 12518, loss 0.646404.
Train: 2018-08-05T15:28:52.291793: step 12519, loss 0.554027.
Train: 2018-08-05T15:28:55.908489: step 12520, loss 0.570769.
Test: 2018-08-05T15:29:11.146128: step 12520, loss 0.547944.
Train: 2018-08-05T15:29:14.818971: step 12521, loss 0.54577.
Train: 2018-08-05T15:29:18.466259: step 12522, loss 0.512547.
Train: 2018-08-05T15:29:22.078979: step 12523, loss 0.612322.
Train: 2018-08-05T15:29:25.672103: step 12524, loss 0.554156.
Train: 2018-08-05T15:29:29.288296: step 12525, loss 0.504407.
Train: 2018-08-05T15:29:32.925045: step 12526, loss 0.56246.
Train: 2018-08-05T15:29:36.523202: step 12527, loss 0.496028.
Train: 2018-08-05T15:29:40.126349: step 12528, loss 0.629006.
Train: 2018-08-05T15:29:43.723980: step 12529, loss 0.562437.
Train: 2018-08-05T15:29:47.328637: step 12530, loss 0.587425.
Test: 2018-08-05T15:30:02.853627: step 12530, loss 0.5489.
Train: 2018-08-05T15:30:06.505394: step 12531, loss 0.587429.
Train: 2018-08-05T15:30:10.146161: step 12532, loss 0.587424.
Train: 2018-08-05T15:30:11.916914: step 12533, loss 0.580197.
Train: 2018-08-05T15:30:15.524573: step 12534, loss 0.529172.
Train: 2018-08-05T15:30:19.123205: step 12535, loss 0.612354.
Train: 2018-08-05T15:30:22.716314: step 12536, loss 0.637251.
Train: 2018-08-05T15:30:26.392656: step 12537, loss 0.537586.
Train: 2018-08-05T15:30:30.029412: step 12538, loss 0.521066.
Train: 2018-08-05T15:30:33.630566: step 12539, loss 0.636998.
Train: 2018-08-05T15:30:37.232223: step 12540, loss 0.545956.
Test: 2018-08-05T15:30:52.458903: step 12540, loss 0.548704.
Train: 2018-08-05T15:30:56.165837: step 12541, loss 0.529461.
Train: 2018-08-05T15:30:59.758960: step 12542, loss 0.570756.
Train: 2018-08-05T15:31:03.381160: step 12543, loss 0.521204.
Train: 2018-08-05T15:31:06.998859: step 12544, loss 0.562489.
Train: 2018-08-05T15:31:10.626584: step 12545, loss 0.554206.
Train: 2018-08-05T15:31:14.228227: step 12546, loss 0.603901.
Train: 2018-08-05T15:31:17.821858: step 12547, loss 0.579047.
Train: 2018-08-05T15:31:21.467630: step 12548, loss 0.521016.
Train: 2018-08-05T15:31:25.065280: step 12549, loss 0.554159.
Train: 2018-08-05T15:31:28.699531: step 12550, loss 0.537518.
Test: 2018-08-05T15:31:43.983306: step 12550, loss 0.548149.
Train: 2018-08-05T15:31:47.966557: step 12551, loss 0.570763.
Train: 2018-08-05T15:31:52.051614: step 12552, loss 0.595783.
Train: 2018-08-05T15:31:56.057423: step 12553, loss 0.537383.
Train: 2018-08-05T15:31:59.818530: step 12554, loss 0.612558.
Train: 2018-08-05T15:32:03.409661: step 12555, loss 0.587492.
Train: 2018-08-05T15:32:07.035890: step 12556, loss 0.520619.
Train: 2018-08-05T15:32:10.667118: step 12557, loss 0.512214.
Train: 2018-08-05T15:32:14.285346: step 12558, loss 0.57916.
Train: 2018-08-05T15:32:17.921107: step 12559, loss 0.470067.
Train: 2018-08-05T15:32:21.522766: step 12560, loss 0.511855.
Test: 2018-08-05T15:32:36.789536: step 12560, loss 0.548048.
Train: 2018-08-05T15:32:40.402234: step 12561, loss 0.62999.
Train: 2018-08-05T15:32:44.044006: step 12562, loss 0.579299.
Train: 2018-08-05T15:32:47.672237: step 12563, loss 0.587811.
Train: 2018-08-05T15:32:51.289407: step 12564, loss 0.528361.
Train: 2018-08-05T15:32:54.887057: step 12565, loss 0.570853.
Train: 2018-08-05T15:32:58.478705: step 12566, loss 0.528254.
Train: 2018-08-05T15:33:02.113445: step 12567, loss 0.639178.
Train: 2018-08-05T15:33:05.735151: step 12568, loss 0.587956.
Train: 2018-08-05T15:33:09.348869: step 12569, loss 0.519657.
Train: 2018-08-05T15:33:12.930506: step 12570, loss 0.545256.
Test: 2018-08-05T15:33:28.148193: step 12570, loss 0.547865.
Train: 2018-08-05T15:33:31.785949: step 12571, loss 0.553789.
Train: 2018-08-05T15:33:35.436742: step 12572, loss 0.596557.
Train: 2018-08-05T15:33:39.060458: step 12573, loss 0.510998.
Train: 2018-08-05T15:33:42.718261: step 12574, loss 0.605165.
Train: 2018-08-05T15:33:46.364028: step 12575, loss 0.622304.
Train: 2018-08-05T15:33:49.974699: step 12576, loss 0.58801.
Train: 2018-08-05T15:33:53.579396: step 12577, loss 0.605063.
Train: 2018-08-05T15:33:57.220172: step 12578, loss 0.53676.
Train: 2018-08-05T15:34:00.812314: step 12579, loss 0.570856.
Train: 2018-08-05T15:34:04.511240: step 12580, loss 0.613356.
Test: 2018-08-05T15:34:20.025184: step 12580, loss 0.548575.
Train: 2018-08-05T15:34:23.640871: step 12581, loss 0.604761.
Train: 2018-08-05T15:34:27.264062: step 12582, loss 0.570817.
Train: 2018-08-05T15:34:30.878209: step 12583, loss 0.60454.
Train: 2018-08-05T15:34:34.953143: step 12584, loss 0.520351.
Train: 2018-08-05T15:34:38.777934: step 12585, loss 0.646285.
Train: 2018-08-05T15:34:42.400662: step 12586, loss 0.503891.
Train: 2018-08-05T15:34:46.024388: step 12587, loss 0.579112.
Train: 2018-08-05T15:34:49.605991: step 12588, loss 0.512461.
Train: 2018-08-05T15:34:53.199096: step 12589, loss 0.529142.
Train: 2018-08-05T15:34:56.810295: step 12590, loss 0.562436.
Test: 2018-08-05T15:35:12.136261: step 12590, loss 0.548522.
Train: 2018-08-05T15:35:15.743935: step 12591, loss 0.512455.
Train: 2018-08-05T15:35:19.370691: step 12592, loss 0.645847.
Train: 2018-08-05T15:35:23.000913: step 12593, loss 0.537406.
Train: 2018-08-05T15:35:26.631619: step 12594, loss 0.520708.
Train: 2018-08-05T15:35:30.235256: step 12595, loss 0.554062.
Train: 2018-08-05T15:35:33.851498: step 12596, loss 0.595871.
Train: 2018-08-05T15:35:37.462189: step 12597, loss 0.595891.
Train: 2018-08-05T15:35:41.083905: step 12598, loss 0.54566.
Train: 2018-08-05T15:35:44.702617: step 12599, loss 0.595905.
Train: 2018-08-05T15:35:48.291734: step 12600, loss 0.562402.
Test: 2018-08-05T15:36:03.514398: step 12600, loss 0.54784.
Train: 2018-08-05T15:36:09.538530: step 12601, loss 0.570776.
Train: 2018-08-05T15:36:13.255990: step 12602, loss 0.520542.
Train: 2018-08-05T15:36:16.884719: step 12603, loss 0.528883.
Train: 2018-08-05T15:36:20.479378: step 12604, loss 0.512042.
Train: 2018-08-05T15:36:24.111104: step 12605, loss 0.520319.
Train: 2018-08-05T15:36:27.713749: step 12606, loss 0.54549.
Train: 2018-08-05T15:36:31.294347: step 12607, loss 0.511564.
Train: 2018-08-05T15:36:34.898516: step 12608, loss 0.553847.
Train: 2018-08-05T15:36:38.501691: step 12609, loss 0.5794.
Train: 2018-08-05T15:36:42.097821: step 12610, loss 0.570892.
Test: 2018-08-05T15:36:57.346114: step 12610, loss 0.549195.
Train: 2018-08-05T15:37:00.993892: step 12611, loss 0.570912.
Train: 2018-08-05T15:37:04.628147: step 12612, loss 0.596715.
Train: 2018-08-05T15:37:08.232322: step 12613, loss 0.536521.
Train: 2018-08-05T15:37:11.821960: step 12614, loss 0.596808.
Train: 2018-08-05T15:37:15.424101: step 12615, loss 0.545091.
Train: 2018-08-05T15:37:19.032764: step 12616, loss 0.588231.
Train: 2018-08-05T15:37:22.642950: step 12617, loss 0.588237.
Train: 2018-08-05T15:37:26.280695: step 12618, loss 0.596858.
Train: 2018-08-05T15:37:29.874318: step 12619, loss 0.605442.
Train: 2018-08-05T15:37:33.480999: step 12620, loss 0.527914.
Test: 2018-08-05T15:37:48.721716: step 12620, loss 0.54778.
Train: 2018-08-05T15:37:52.318359: step 12621, loss 0.613917.
Train: 2018-08-05T15:37:55.951100: step 12622, loss 0.536594.
Train: 2018-08-05T15:37:59.597882: step 12623, loss 0.528057.
Train: 2018-08-05T15:38:03.259700: step 12624, loss 0.502378.
Train: 2018-08-05T15:38:06.857347: step 12625, loss 0.53662.
Train: 2018-08-05T15:38:10.439935: step 12626, loss 0.579498.
Train: 2018-08-05T15:38:14.034585: step 12627, loss 0.596684.
Train: 2018-08-05T15:38:17.624207: step 12628, loss 0.596681.
Train: 2018-08-05T15:38:21.251945: step 12629, loss 0.605234.
Train: 2018-08-05T15:38:24.876672: step 12630, loss 0.562335.
Test: 2018-08-05T15:38:40.113818: step 12630, loss 0.549232.
Train: 2018-08-05T15:38:43.755073: step 12631, loss 0.57089.
Train: 2018-08-05T15:38:47.353734: step 12632, loss 0.622128.
Train: 2018-08-05T15:38:50.965422: step 12633, loss 0.536781.
Train: 2018-08-05T15:38:54.547032: step 12634, loss 0.502808.
Train: 2018-08-05T15:38:58.195831: step 12635, loss 0.596353.
Train: 2018-08-05T15:39:01.834589: step 12636, loss 0.570841.
Train: 2018-08-05T15:39:05.446281: step 12637, loss 0.621744.
Train: 2018-08-05T15:39:09.033909: step 12638, loss 0.562356.
Train: 2018-08-05T15:39:12.626038: step 12639, loss 0.613069.
Train: 2018-08-05T15:39:16.256261: step 12640, loss 0.545514.
Test: 2018-08-05T15:39:31.559111: step 12640, loss 0.547363.
Train: 2018-08-05T15:39:35.195858: step 12641, loss 0.486672.
Train: 2018-08-05T15:39:38.826579: step 12642, loss 0.562381.
Train: 2018-08-05T15:39:42.453306: step 12643, loss 0.511924.
Train: 2018-08-05T15:39:46.050447: step 12644, loss 0.486615.
Train: 2018-08-05T15:39:49.641549: step 12645, loss 0.486414.
Train: 2018-08-05T15:39:53.225669: step 12646, loss 0.579297.
Train: 2018-08-05T15:39:56.868428: step 12647, loss 0.553848.
Train: 2018-08-05T15:40:00.572368: step 12648, loss 0.519721.
Train: 2018-08-05T15:40:04.267282: step 12649, loss 0.545228.
Train: 2018-08-05T15:40:07.902036: step 12650, loss 0.639592.
Test: 2018-08-05T15:40:23.159821: step 12650, loss 0.547184.
Train: 2018-08-05T15:40:26.792589: step 12651, loss 0.553737.
Train: 2018-08-05T15:40:30.403270: step 12652, loss 0.52789.
Train: 2018-08-05T15:40:34.038021: step 12653, loss 0.501933.
Train: 2018-08-05T15:40:37.665753: step 12654, loss 0.605618.
Train: 2018-08-05T15:40:41.257903: step 12655, loss 0.571017.
Train: 2018-08-05T15:40:44.881639: step 12656, loss 0.579714.
Train: 2018-08-05T15:40:48.478769: step 12657, loss 0.510217.
Train: 2018-08-05T15:40:52.071402: step 12658, loss 0.54495.
Train: 2018-08-05T15:40:55.702637: step 12659, loss 0.571082.
Train: 2018-08-05T15:40:59.306800: step 12660, loss 0.623477.
Test: 2018-08-05T15:41:14.531017: step 12660, loss 0.54799.
Train: 2018-08-05T15:41:18.174282: step 12661, loss 0.553638.
Train: 2018-08-05T15:41:21.786961: step 12662, loss 0.579826.
Train: 2018-08-05T15:41:25.580116: step 12663, loss 0.571091.
Train: 2018-08-05T15:41:29.421402: step 12664, loss 0.6234.
Train: 2018-08-05T15:41:33.234650: step 12665, loss 0.54495.
Train: 2018-08-05T15:41:37.070423: step 12666, loss 0.579735.
Train: 2018-08-05T15:41:40.929261: step 12667, loss 0.51029.
Train: 2018-08-05T15:41:44.766529: step 12668, loss 0.571018.
Train: 2018-08-05T15:41:48.631860: step 12669, loss 0.597003.
Train: 2018-08-05T15:41:52.233525: step 12670, loss 0.47582.
Test: 2018-08-05T15:42:07.492277: step 12670, loss 0.547291.
Train: 2018-08-05T15:42:11.126017: step 12671, loss 0.527723.
Train: 2018-08-05T15:42:14.764770: step 12672, loss 0.536356.
Train: 2018-08-05T15:42:18.399526: step 12673, loss 0.527653.
Train: 2018-08-05T15:42:21.988161: step 12674, loss 0.57104.
Train: 2018-08-05T15:42:25.577775: step 12675, loss 0.536257.
Train: 2018-08-05T15:42:29.176449: step 12676, loss 0.553648.
Train: 2018-08-05T15:42:32.772084: step 12677, loss 0.492554.
Train: 2018-08-05T15:42:36.389785: step 12678, loss 0.562377.
Train: 2018-08-05T15:42:40.014501: step 12679, loss 0.571156.
Train: 2018-08-05T15:42:43.621183: step 12680, loss 0.51848.
Test: 2018-08-05T15:42:58.863370: step 12680, loss 0.547707.
Train: 2018-08-05T15:43:02.484615: step 12681, loss 0.59762.
Train: 2018-08-05T15:43:06.089766: step 12682, loss 0.606483.
Train: 2018-08-05T15:43:09.779651: step 12683, loss 0.650562.
Train: 2018-08-05T15:43:11.569464: step 12684, loss 0.449769.
Train: 2018-08-05T15:43:15.205221: step 12685, loss 0.606429.
Train: 2018-08-05T15:43:18.800381: step 12686, loss 0.597603.
Train: 2018-08-05T15:43:22.413548: step 12687, loss 0.465731.
Train: 2018-08-05T15:43:26.012197: step 12688, loss 0.518438.
Train: 2018-08-05T15:43:29.628902: step 12689, loss 0.606424.
Train: 2018-08-05T15:43:33.286715: step 12690, loss 0.483174.
Test: 2018-08-05T15:43:48.642737: step 12690, loss 0.546679.
Train: 2018-08-05T15:43:52.295534: step 12691, loss 0.606501.
Train: 2018-08-05T15:43:55.922260: step 12692, loss 0.56242.
Train: 2018-08-05T15:43:59.545971: step 12693, loss 0.624167.
Train: 2018-08-05T15:44:03.139620: step 12694, loss 0.562414.
Train: 2018-08-05T15:44:06.737774: step 12695, loss 0.492004.
Train: 2018-08-05T15:44:10.403103: step 12696, loss 0.527202.
Train: 2018-08-05T15:44:14.041864: step 12697, loss 0.588833.
Train: 2018-08-05T15:44:17.689660: step 12698, loss 0.597638.
Train: 2018-08-05T15:44:21.281295: step 12699, loss 0.667997.
Train: 2018-08-05T15:44:24.870437: step 12700, loss 0.509745.
Test: 2018-08-05T15:44:40.154313: step 12700, loss 0.547754.
Train: 2018-08-05T15:44:45.914213: step 12701, loss 0.579901.
Train: 2018-08-05T15:44:49.522881: step 12702, loss 0.5886.
Train: 2018-08-05T15:44:53.147139: step 12703, loss 0.518758.
Train: 2018-08-05T15:44:56.734773: step 12704, loss 0.579777.
Train: 2018-08-05T15:45:00.355986: step 12705, loss 0.571048.
Train: 2018-08-05T15:45:03.945614: step 12706, loss 0.579707.
Train: 2018-08-05T15:45:07.542262: step 12707, loss 0.519036.
Train: 2018-08-05T15:45:11.173001: step 12708, loss 0.605605.
Train: 2018-08-05T15:45:14.801746: step 12709, loss 0.545066.
Train: 2018-08-05T15:45:18.396379: step 12710, loss 0.553713.
Test: 2018-08-05T15:45:33.580445: step 12710, loss 0.548543.
Train: 2018-08-05T15:45:37.168549: step 12711, loss 0.596799.
Train: 2018-08-05T15:45:40.787238: step 12712, loss 0.553735.
Train: 2018-08-05T15:45:44.379873: step 12713, loss 0.545157.
Train: 2018-08-05T15:45:48.007595: step 12714, loss 0.510851.
Train: 2018-08-05T15:45:51.628305: step 12715, loss 0.570917.
Train: 2018-08-05T15:45:55.218904: step 12716, loss 0.656725.
Train: 2018-08-05T15:45:58.824060: step 12717, loss 0.493821.
Train: 2018-08-05T15:46:02.427223: step 12718, loss 0.545214.
Train: 2018-08-05T15:46:06.030886: step 12719, loss 0.622258.
Train: 2018-08-05T15:46:09.927494: step 12720, loss 0.553787.
Test: 2018-08-05T15:46:25.561710: step 12720, loss 0.546893.
Train: 2018-08-05T15:46:29.225027: step 12721, loss 0.596504.
Train: 2018-08-05T15:46:32.958026: step 12722, loss 0.570868.
Train: 2018-08-05T15:46:36.568697: step 12723, loss 0.562342.
Train: 2018-08-05T15:46:40.166339: step 12724, loss 0.579352.
Train: 2018-08-05T15:46:43.748466: step 12725, loss 0.562348.
Train: 2018-08-05T15:46:47.350632: step 12726, loss 0.638659.
Train: 2018-08-05T15:46:50.983382: step 12727, loss 0.663824.
Train: 2018-08-05T15:46:54.601090: step 12728, loss 0.562377.
Train: 2018-08-05T15:46:58.186192: step 12729, loss 0.562396.
Train: 2018-08-05T15:47:01.779830: step 12730, loss 0.58748.
Test: 2018-08-05T15:47:16.974445: step 12730, loss 0.547193.
Train: 2018-08-05T15:47:20.578571: step 12731, loss 0.545788.
Train: 2018-08-05T15:47:24.189238: step 12732, loss 0.603966.
Train: 2018-08-05T15:47:27.815963: step 12733, loss 0.670055.
Train: 2018-08-05T15:47:31.449693: step 12734, loss 0.570757.
Train: 2018-08-05T15:47:35.046355: step 12735, loss 0.53798.
Train: 2018-08-05T15:47:38.656015: step 12736, loss 0.603441.
Train: 2018-08-05T15:47:42.256667: step 12737, loss 0.51382.
Train: 2018-08-05T15:47:45.853303: step 12738, loss 0.587027.
Train: 2018-08-05T15:47:49.486045: step 12739, loss 0.497878.
Train: 2018-08-05T15:47:53.125822: step 12740, loss 0.506003.
Test: 2018-08-05T15:48:08.349999: step 12740, loss 0.548768.
Train: 2018-08-05T15:48:11.948138: step 12741, loss 0.505922.
Train: 2018-08-05T15:48:15.543285: step 12742, loss 0.48949.
Train: 2018-08-05T15:48:19.119384: step 12743, loss 0.554447.
Train: 2018-08-05T15:48:22.713029: step 12744, loss 0.497006.
Train: 2018-08-05T15:48:26.344788: step 12745, loss 0.529569.
Train: 2018-08-05T15:48:29.969512: step 12746, loss 0.537625.
Train: 2018-08-05T15:48:33.583193: step 12747, loss 0.637395.
Train: 2018-08-05T15:48:37.161799: step 12748, loss 0.545695.
Train: 2018-08-05T15:48:40.770484: step 12749, loss 0.545615.
Train: 2018-08-05T15:48:44.363630: step 12750, loss 0.570796.
Test: 2018-08-05T15:48:59.545273: step 12750, loss 0.548257.
Train: 2018-08-05T15:49:03.208624: step 12751, loss 0.537024.
Train: 2018-08-05T15:49:06.822331: step 12752, loss 0.519977.
Train: 2018-08-05T15:49:10.448058: step 12753, loss 0.562343.
Train: 2018-08-05T15:49:14.042700: step 12754, loss 0.528183.
Train: 2018-08-05T15:49:17.656864: step 12755, loss 0.553764.
Train: 2018-08-05T15:49:21.292590: step 12756, loss 0.588142.
Train: 2018-08-05T15:49:24.902759: step 12757, loss 0.545087.
Train: 2018-08-05T15:49:28.519952: step 12758, loss 0.545044.
Train: 2018-08-05T15:49:32.135156: step 12759, loss 0.458277.
Train: 2018-08-05T15:49:35.722270: step 12760, loss 0.527508.
Test: 2018-08-05T15:49:50.916324: step 12760, loss 0.548763.
Train: 2018-08-05T15:49:54.520999: step 12761, loss 0.527378.
Train: 2018-08-05T15:49:58.106614: step 12762, loss 0.553687.
Train: 2018-08-05T15:50:01.925993: step 12763, loss 0.527175.
Train: 2018-08-05T15:50:05.567754: step 12764, loss 0.63355.
Train: 2018-08-05T15:50:09.155363: step 12765, loss 0.509188.
Train: 2018-08-05T15:50:12.743020: step 12766, loss 0.500154.
Train: 2018-08-05T15:50:16.350711: step 12767, loss 0.598272.
Train: 2018-08-05T15:50:19.985459: step 12768, loss 0.553591.
Train: 2018-08-05T15:50:23.593140: step 12769, loss 0.589494.
Train: 2018-08-05T15:50:27.214871: step 12770, loss 0.553596.
Test: 2018-08-05T15:50:42.409497: step 12770, loss 0.547158.
Train: 2018-08-05T15:50:46.010128: step 12771, loss 0.661521.
Train: 2018-08-05T15:50:49.642881: step 12772, loss 0.58953.
Train: 2018-08-05T15:50:53.241066: step 12773, loss 0.598435.
Train: 2018-08-05T15:50:56.826682: step 12774, loss 0.508854.
Train: 2018-08-05T15:51:00.441388: step 12775, loss 0.571458.
Train: 2018-08-05T15:51:04.041533: step 12776, loss 0.526827.
Train: 2018-08-05T15:51:07.646703: step 12777, loss 0.500118.
Train: 2018-08-05T15:51:11.253415: step 12778, loss 0.580324.
Train: 2018-08-05T15:51:14.837030: step 12779, loss 0.54468.
Train: 2018-08-05T15:51:18.414118: step 12780, loss 0.544683.
Test: 2018-08-05T15:51:33.652317: step 12780, loss 0.547622.
Train: 2018-08-05T15:51:37.298101: step 12781, loss 0.509066.
Train: 2018-08-05T15:51:40.910774: step 12782, loss 0.544678.
Train: 2018-08-05T15:51:44.496392: step 12783, loss 0.526837.
Train: 2018-08-05T15:51:48.106077: step 12784, loss 0.598224.
Train: 2018-08-05T15:51:51.705729: step 12785, loss 0.660734.
Train: 2018-08-05T15:51:55.298875: step 12786, loss 0.598153.
Train: 2018-08-05T15:51:58.903552: step 12787, loss 0.651393.
Train: 2018-08-05T15:52:02.515729: step 12788, loss 0.624442.
Train: 2018-08-05T15:52:06.116389: step 12789, loss 0.562416.
Train: 2018-08-05T15:52:09.706015: step 12790, loss 0.606258.
Test: 2018-08-05T15:52:24.922261: step 12790, loss 0.546583.
Train: 2018-08-05T15:52:28.531410: step 12791, loss 0.623474.
Train: 2018-08-05T15:52:32.141101: step 12792, loss 0.588387.
Train: 2018-08-05T15:52:35.724705: step 12793, loss 0.536451.
Train: 2018-08-05T15:52:39.361467: step 12794, loss 0.459285.
Train: 2018-08-05T15:52:42.978168: step 12795, loss 0.519499.
Train: 2018-08-05T15:52:46.577831: step 12796, loss 0.673564.
Train: 2018-08-05T15:52:50.194509: step 12797, loss 0.52823.
Train: 2018-08-05T15:52:53.793171: step 12798, loss 0.596373.
Train: 2018-08-05T15:52:57.367757: step 12799, loss 0.570834.
Train: 2018-08-05T15:53:01.002005: step 12800, loss 0.545435.
Test: 2018-08-05T15:53:16.210141: step 12800, loss 0.548456.
Train: 2018-08-05T15:53:22.000110: step 12801, loss 0.55392.
Train: 2018-08-05T15:53:25.590709: step 12802, loss 0.511786.
Train: 2018-08-05T15:53:29.196348: step 12803, loss 0.596082.
Train: 2018-08-05T15:53:32.828079: step 12804, loss 0.537117.
Train: 2018-08-05T15:53:36.440762: step 12805, loss 0.61288.
Train: 2018-08-05T15:53:40.102592: step 12806, loss 0.587603.
Train: 2018-08-05T15:53:43.733337: step 12807, loss 0.56239.
Train: 2018-08-05T15:53:47.324973: step 12808, loss 0.587546.
Train: 2018-08-05T15:53:50.921109: step 12809, loss 0.620993.
Train: 2018-08-05T15:53:54.525270: step 12810, loss 0.503981.
Test: 2018-08-05T15:54:09.762445: step 12810, loss 0.548305.
Train: 2018-08-05T15:54:13.407711: step 12811, loss 0.604127.
Train: 2018-08-05T15:54:17.052481: step 12812, loss 0.604068.
Train: 2018-08-05T15:54:20.677694: step 12813, loss 0.562452.
Train: 2018-08-05T15:54:24.261810: step 12814, loss 0.595634.
Train: 2018-08-05T15:54:27.851937: step 12815, loss 0.496294.
Train: 2018-08-05T15:54:31.457604: step 12816, loss 0.661718.
Train: 2018-08-05T15:54:35.036189: step 12817, loss 0.496509.
Train: 2018-08-05T15:54:38.659438: step 12818, loss 0.579001.
Train: 2018-08-05T15:54:42.279114: step 12819, loss 0.513088.
Train: 2018-08-05T15:54:45.870239: step 12820, loss 0.546028.
Test: 2018-08-05T15:55:01.091422: step 12820, loss 0.548347.
Train: 2018-08-05T15:55:04.690584: step 12821, loss 0.513001.
Train: 2018-08-05T15:55:08.319323: step 12822, loss 0.521151.
Train: 2018-08-05T15:55:11.905939: step 12823, loss 0.537593.
Train: 2018-08-05T15:55:15.528656: step 12824, loss 0.570761.
Train: 2018-08-05T15:55:19.161381: step 12825, loss 0.529064.
Train: 2018-08-05T15:55:22.757024: step 12826, loss 0.554031.
Train: 2018-08-05T15:55:26.390772: step 12827, loss 0.545599.
Train: 2018-08-05T15:55:29.985428: step 12828, loss 0.495.
Train: 2018-08-05T15:55:33.588090: step 12829, loss 0.536974.
Train: 2018-08-05T15:55:37.201783: step 12830, loss 0.596326.
Test: 2018-08-05T15:55:52.418905: step 12830, loss 0.54889.
Train: 2018-08-05T15:55:56.021052: step 12831, loss 0.536748.
Train: 2018-08-05T15:55:59.603666: step 12832, loss 0.596526.
Train: 2018-08-05T15:56:03.196802: step 12833, loss 0.528022.
Train: 2018-08-05T15:56:06.788939: step 12834, loss 0.510595.
Train: 2018-08-05T15:56:08.534631: step 12835, loss 0.599263.
Train: 2018-08-05T15:56:12.148858: step 12836, loss 0.597274.
Train: 2018-08-05T15:56:15.772594: step 12837, loss 0.518929.
Train: 2018-08-05T15:56:19.591912: step 12838, loss 0.554389.
Train: 2018-08-05T15:56:23.213631: step 12839, loss 0.580003.
Train: 2018-08-05T15:56:26.810758: step 12840, loss 0.588764.
Test: 2018-08-05T15:56:42.026897: step 12840, loss 0.547593.
Train: 2018-08-05T15:56:45.669161: step 12841, loss 0.614826.
Train: 2018-08-05T15:56:49.287864: step 12842, loss 0.640685.
Train: 2018-08-05T15:56:52.923111: step 12843, loss 0.449458.
Train: 2018-08-05T15:56:56.530794: step 12844, loss 0.492872.
Train: 2018-08-05T15:57:00.134422: step 12845, loss 0.544962.
Train: 2018-08-05T15:57:03.719029: step 12846, loss 0.61461.
Train: 2018-08-05T15:57:07.320692: step 12847, loss 0.588491.
Train: 2018-08-05T15:57:10.947405: step 12848, loss 0.518823.
Train: 2018-08-05T15:57:14.562109: step 12849, loss 0.56236.
Train: 2018-08-05T15:57:18.194843: step 12850, loss 0.518797.
Test: 2018-08-05T15:57:33.408972: step 12850, loss 0.548.
Train: 2018-08-05T15:57:37.000125: step 12851, loss 0.623412.
Train: 2018-08-05T15:57:40.612315: step 12852, loss 0.527491.
Train: 2018-08-05T15:57:44.207938: step 12853, loss 0.483884.
Train: 2018-08-05T15:57:47.810093: step 12854, loss 0.571103.
Train: 2018-08-05T15:57:51.429799: step 12855, loss 0.62358.
Train: 2018-08-05T15:57:55.041489: step 12856, loss 0.492438.
Train: 2018-08-05T15:57:58.650164: step 12857, loss 0.457368.
Train: 2018-08-05T15:58:02.334058: step 12858, loss 0.527293.
Train: 2018-08-05T15:58:05.932705: step 12859, loss 0.527205.
Train: 2018-08-05T15:58:09.531852: step 12860, loss 0.571256.
Test: 2018-08-05T15:58:24.775136: step 12860, loss 0.547663.
Train: 2018-08-05T15:58:28.420920: step 12861, loss 0.518189.
Train: 2018-08-05T15:58:32.035641: step 12862, loss 0.500328.
Train: 2018-08-05T15:58:35.639793: step 12863, loss 0.598132.
Train: 2018-08-05T15:58:39.250966: step 12864, loss 0.616099.
Train: 2018-08-05T15:58:42.850097: step 12865, loss 0.526772.
Train: 2018-08-05T15:58:46.459748: step 12866, loss 0.562542.
Train: 2018-08-05T15:58:50.153687: step 12867, loss 0.580472.
Train: 2018-08-05T15:58:53.788429: step 12868, loss 0.562556.
Train: 2018-08-05T15:58:57.394098: step 12869, loss 0.526697.
Train: 2018-08-05T15:59:01.006792: step 12870, loss 0.589474.
Test: 2018-08-05T15:59:16.262021: step 12870, loss 0.546758.
Train: 2018-08-05T15:59:19.905799: step 12871, loss 0.580499.
Train: 2018-08-05T15:59:23.583675: step 12872, loss 0.571517.
Train: 2018-08-05T15:59:27.206407: step 12873, loss 0.625221.
Train: 2018-08-05T15:59:30.819593: step 12874, loss 0.642932.
Train: 2018-08-05T15:59:34.418736: step 12875, loss 0.562491.
Train: 2018-08-05T15:59:38.007869: step 12876, loss 0.535846.
Train: 2018-08-05T15:59:41.609030: step 12877, loss 0.57129.
Train: 2018-08-05T15:59:45.188640: step 12878, loss 0.491838.
Train: 2018-08-05T15:59:48.826896: step 12879, loss 0.500738.
Train: 2018-08-05T15:59:52.436074: step 12880, loss 0.562413.
Test: 2018-08-05T16:00:07.910428: step 12880, loss 0.547502.
Train: 2018-08-05T16:00:11.537150: step 12881, loss 0.624047.
Train: 2018-08-05T16:00:15.143812: step 12882, loss 0.509653.
Train: 2018-08-05T16:00:18.742958: step 12883, loss 0.606328.
Train: 2018-08-05T16:00:22.448906: step 12884, loss 0.553617.
Train: 2018-08-05T16:00:26.110756: step 12885, loss 0.553622.
Train: 2018-08-05T16:00:29.731459: step 12886, loss 0.553626.
Train: 2018-08-05T16:00:33.321083: step 12887, loss 0.509908.
Train: 2018-08-05T16:00:36.921734: step 12888, loss 0.623587.
Train: 2018-08-05T16:00:40.535412: step 12889, loss 0.60604.
Train: 2018-08-05T16:00:44.160095: step 12890, loss 0.544929.
Test: 2018-08-05T16:00:59.415822: step 12890, loss 0.547422.
Train: 2018-08-05T16:01:03.045545: step 12891, loss 0.640685.
Train: 2018-08-05T16:01:06.669243: step 12892, loss 0.588381.
Train: 2018-08-05T16:01:10.282433: step 12893, loss 0.570991.
Train: 2018-08-05T16:01:13.885568: step 12894, loss 0.674433.
Train: 2018-08-05T16:01:17.478202: step 12895, loss 0.519435.
Train: 2018-08-05T16:01:21.100411: step 12896, loss 0.49395.
Train: 2018-08-05T16:01:24.754732: step 12897, loss 0.502622.
Train: 2018-08-05T16:01:28.372927: step 12898, loss 0.596441.
Train: 2018-08-05T16:01:31.973576: step 12899, loss 0.536799.
Train: 2018-08-05T16:01:35.574207: step 12900, loss 0.519798.
Test: 2018-08-05T16:01:50.806909: step 12900, loss 0.548128.
Train: 2018-08-05T16:01:56.691670: step 12901, loss 0.630434.
Train: 2018-08-05T16:02:00.279791: step 12902, loss 0.511331.
Train: 2018-08-05T16:02:03.854882: step 12903, loss 0.587853.
Train: 2018-08-05T16:02:07.457039: step 12904, loss 0.570845.
Train: 2018-08-05T16:02:11.077763: step 12905, loss 0.596325.
Train: 2018-08-05T16:02:14.669365: step 12906, loss 0.519924.
Train: 2018-08-05T16:02:18.245935: step 12907, loss 0.553866.
Train: 2018-08-05T16:02:21.846571: step 12908, loss 0.587803.
Train: 2018-08-05T16:02:25.428206: step 12909, loss 0.536909.
Train: 2018-08-05T16:02:29.045411: step 12910, loss 0.655652.
Test: 2018-08-05T16:02:44.246053: step 12910, loss 0.547629.
Train: 2018-08-05T16:02:47.867277: step 12911, loss 0.562355.
Train: 2018-08-05T16:02:51.467927: step 12912, loss 0.56236.
Train: 2018-08-05T16:02:55.061556: step 12913, loss 0.52014.
Train: 2018-08-05T16:02:58.664226: step 12914, loss 0.57925.
Train: 2018-08-05T16:03:02.261369: step 12915, loss 0.59612.
Train: 2018-08-05T16:03:05.882081: step 12916, loss 0.5708.
Train: 2018-08-05T16:03:09.509813: step 12917, loss 0.629734.
Train: 2018-08-05T16:03:13.124527: step 12918, loss 0.570787.
Train: 2018-08-05T16:03:16.720168: step 12919, loss 0.562397.
Train: 2018-08-05T16:03:20.326341: step 12920, loss 0.545671.
Test: 2018-08-05T16:03:35.579572: step 12920, loss 0.548649.
Train: 2018-08-05T16:03:39.219321: step 12921, loss 0.570771.
Train: 2018-08-05T16:03:42.923244: step 12922, loss 0.587463.
Train: 2018-08-05T16:03:46.550963: step 12923, loss 0.62911.
Train: 2018-08-05T16:03:50.164664: step 12924, loss 0.637269.
Train: 2018-08-05T16:03:53.766317: step 12925, loss 0.446519.
Train: 2018-08-05T16:03:57.351429: step 12926, loss 0.529383.
Train: 2018-08-05T16:04:00.963112: step 12927, loss 0.537652.
Train: 2018-08-05T16:04:04.610393: step 12928, loss 0.595602.
Train: 2018-08-05T16:04:08.250161: step 12929, loss 0.504497.
Train: 2018-08-05T16:04:11.855826: step 12930, loss 0.479524.
Test: 2018-08-05T16:04:27.091051: step 12930, loss 0.548357.
Train: 2018-08-05T16:04:30.702734: step 12931, loss 0.554123.
Train: 2018-08-05T16:04:34.307915: step 12932, loss 0.512357.
Train: 2018-08-05T16:04:37.915107: step 12933, loss 0.461887.
Train: 2018-08-05T16:04:41.548376: step 12934, loss 0.520265.
Train: 2018-08-05T16:04:45.165070: step 12935, loss 0.51153.
Train: 2018-08-05T16:04:48.753190: step 12936, loss 0.61348.
Train: 2018-08-05T16:04:52.381444: step 12937, loss 0.596584.
Train: 2018-08-05T16:04:55.974058: step 12938, loss 0.545152.
Train: 2018-08-05T16:04:59.548630: step 12939, loss 0.545096.
Train: 2018-08-05T16:05:03.316738: step 12940, loss 0.536394.
Test: 2018-08-05T16:05:18.845679: step 12940, loss 0.547855.
Train: 2018-08-05T16:05:22.612790: step 12941, loss 0.501599.
Train: 2018-08-05T16:05:26.265071: step 12942, loss 0.544933.
Train: 2018-08-05T16:05:29.858690: step 12943, loss 0.658607.
Train: 2018-08-05T16:05:33.438285: step 12944, loss 0.597435.
Train: 2018-08-05T16:05:37.019886: step 12945, loss 0.483466.
Train: 2018-08-05T16:05:40.642611: step 12946, loss 0.571185.
Train: 2018-08-05T16:05:44.294432: step 12947, loss 0.571206.
Train: 2018-08-05T16:05:47.877541: step 12948, loss 0.632884.
Train: 2018-08-05T16:05:51.485705: step 12949, loss 0.45675.
Train: 2018-08-05T16:05:55.096894: step 12950, loss 0.491883.
Test: 2018-08-05T16:06:10.354684: step 12950, loss 0.548285.
Train: 2018-08-05T16:06:13.960378: step 12951, loss 0.588947.
Train: 2018-08-05T16:06:17.585091: step 12952, loss 0.544742.
Train: 2018-08-05T16:06:21.198785: step 12953, loss 0.562459.
Train: 2018-08-05T16:06:24.775402: step 12954, loss 0.57134.
Train: 2018-08-05T16:06:28.373545: step 12955, loss 0.642404.
Train: 2018-08-05T16:06:31.967182: step 12956, loss 0.562471.
Train: 2018-08-05T16:06:35.554816: step 12957, loss 0.580188.
Train: 2018-08-05T16:06:39.152475: step 12958, loss 0.5713.
Train: 2018-08-05T16:06:42.772672: step 12959, loss 0.553596.
Train: 2018-08-05T16:06:46.359812: step 12960, loss 0.659535.
Test: 2018-08-05T16:07:01.602013: step 12960, loss 0.548317.
Train: 2018-08-05T16:07:05.227754: step 12961, loss 0.571208.
Train: 2018-08-05T16:07:08.837948: step 12962, loss 0.500973.
Train: 2018-08-05T16:07:12.443634: step 12963, loss 0.597415.
Train: 2018-08-05T16:07:16.043779: step 12964, loss 0.544896.
Train: 2018-08-05T16:07:19.658477: step 12965, loss 0.562364.
Train: 2018-08-05T16:07:23.290206: step 12966, loss 0.562358.
Train: 2018-08-05T16:07:26.898890: step 12967, loss 0.562353.
Train: 2018-08-05T16:07:30.463952: step 12968, loss 0.657793.
Train: 2018-08-05T16:07:34.073665: step 12969, loss 0.527748.
Train: 2018-08-05T16:07:37.654764: step 12970, loss 0.55371.
Test: 2018-08-05T16:07:52.889943: step 12970, loss 0.547956.
Train: 2018-08-05T16:07:56.526176: step 12971, loss 0.579557.
Train: 2018-08-05T16:08:00.157941: step 12972, loss 0.553743.
Train: 2018-08-05T16:08:03.745553: step 12973, loss 0.502304.
Train: 2018-08-05T16:08:07.333166: step 12974, loss 0.52805.
Train: 2018-08-05T16:08:10.912757: step 12975, loss 0.57948.
Train: 2018-08-05T16:08:14.502366: step 12976, loss 0.528053.
Train: 2018-08-05T16:08:18.111044: step 12977, loss 0.639503.
Train: 2018-08-05T16:08:21.749794: step 12978, loss 0.562335.
Train: 2018-08-05T16:08:25.341943: step 12979, loss 0.588006.
Train: 2018-08-05T16:08:28.934576: step 12980, loss 0.605061.
Test: 2018-08-05T16:08:44.157183: step 12980, loss 0.547706.
Train: 2018-08-05T16:08:47.793439: step 12981, loss 0.587921.
Train: 2018-08-05T16:08:51.385066: step 12982, loss 0.596372.
Train: 2018-08-05T16:08:54.995735: step 12983, loss 0.570834.
Train: 2018-08-05T16:08:58.612421: step 12984, loss 0.604665.
Train: 2018-08-05T16:09:02.250174: step 12985, loss 0.511759.
Train: 2018-08-05T16:09:04.008400: step 12986, loss 0.508491.
Train: 2018-08-05T16:09:07.580505: step 12987, loss 0.537132.
Train: 2018-08-05T16:09:11.168635: step 12988, loss 0.537129.
Train: 2018-08-05T16:09:14.821446: step 12989, loss 0.604483.
Train: 2018-08-05T16:09:18.468223: step 12990, loss 0.697092.
Test: 2018-08-05T16:09:33.764055: step 12990, loss 0.548758.
Train: 2018-08-05T16:09:37.386770: step 12991, loss 0.621131.
Train: 2018-08-05T16:09:40.998454: step 12992, loss 0.520626.
Train: 2018-08-05T16:09:44.630195: step 12993, loss 0.554284.
Train: 2018-08-05T16:09:48.203774: step 12994, loss 0.637206.
Train: 2018-08-05T16:09:51.829498: step 12995, loss 0.587492.
Train: 2018-08-05T16:09:55.431165: step 12996, loss 0.628687.
Train: 2018-08-05T16:09:59.012760: step 12997, loss 0.554117.
Train: 2018-08-05T16:10:02.797921: step 12998, loss 0.636481.
Train: 2018-08-05T16:10:06.389031: step 12999, loss 0.586968.
Train: 2018-08-05T16:10:09.987149: step 13000, loss 0.530436.
Test: 2018-08-05T16:10:25.221423: step 13000, loss 0.548972.
Train: 2018-08-05T16:10:31.031469: step 13001, loss 0.4741.
Train: 2018-08-05T16:10:34.633121: step 13002, loss 0.546784.
Train: 2018-08-05T16:10:38.235790: step 13003, loss 0.530233.
Train: 2018-08-05T16:10:41.823397: step 13004, loss 0.489461.
Train: 2018-08-05T16:10:45.400005: step 13005, loss 0.538122.
Train: 2018-08-05T16:10:48.986605: step 13006, loss 0.554388.
Train: 2018-08-05T16:10:52.614321: step 13007, loss 0.578974.
Train: 2018-08-05T16:10:56.225993: step 13008, loss 0.578993.
Train: 2018-08-05T16:10:59.813122: step 13009, loss 0.545992.
Train: 2018-08-05T16:11:03.396262: step 13010, loss 0.537656.
Test: 2018-08-05T16:11:18.552700: step 13010, loss 0.549364.
Train: 2018-08-05T16:11:22.152853: step 13011, loss 0.628851.
Train: 2018-08-05T16:11:25.750018: step 13012, loss 0.554139.
Train: 2018-08-05T16:11:29.356668: step 13013, loss 0.520825.
Train: 2018-08-05T16:11:32.960327: step 13014, loss 0.562424.
Train: 2018-08-05T16:11:36.558483: step 13015, loss 0.554051.
Train: 2018-08-05T16:11:40.144124: step 13016, loss 0.570778.
Train: 2018-08-05T16:11:43.716218: step 13017, loss 0.512023.
Train: 2018-08-05T16:11:47.331920: step 13018, loss 0.58763.
Train: 2018-08-05T16:11:50.946634: step 13019, loss 0.596108.
Train: 2018-08-05T16:11:54.570348: step 13020, loss 0.57081.
Test: 2018-08-05T16:12:09.747866: step 13020, loss 0.549604.
Train: 2018-08-05T16:12:13.374120: step 13021, loss 0.621537.
Train: 2018-08-05T16:12:17.053989: step 13022, loss 0.553909.
Train: 2018-08-05T16:12:20.651644: step 13023, loss 0.59617.
Train: 2018-08-05T16:12:24.235260: step 13024, loss 0.511679.
Train: 2018-08-05T16:12:27.832890: step 13025, loss 0.629974.
Train: 2018-08-05T16:12:31.444573: step 13026, loss 0.579254.
Train: 2018-08-05T16:12:35.067291: step 13027, loss 0.596112.
Train: 2018-08-05T16:12:38.662946: step 13028, loss 0.596067.
Train: 2018-08-05T16:12:42.260593: step 13029, loss 0.612823.
Train: 2018-08-05T16:12:45.847694: step 13030, loss 0.637852.
Test: 2018-08-05T16:13:01.049283: step 13030, loss 0.548277.
Train: 2018-08-05T16:13:04.671987: step 13031, loss 0.545713.
Train: 2018-08-05T16:13:08.391966: step 13032, loss 0.595743.
Train: 2018-08-05T16:13:12.030223: step 13033, loss 0.603955.
Train: 2018-08-05T16:13:15.622334: step 13034, loss 0.529413.
Train: 2018-08-05T16:13:19.235518: step 13035, loss 0.537765.
Train: 2018-08-05T16:13:22.856725: step 13036, loss 0.554288.
Train: 2018-08-05T16:13:26.469401: step 13037, loss 0.496732.
Train: 2018-08-05T16:13:30.092109: step 13038, loss 0.529612.
Train: 2018-08-05T16:13:33.962467: step 13039, loss 0.570757.
Train: 2018-08-05T16:13:37.751577: step 13040, loss 0.579005.
Test: 2018-08-05T16:13:53.117678: step 13040, loss 0.549283.
Train: 2018-08-05T16:13:56.758446: step 13041, loss 0.595523.
Train: 2018-08-05T16:14:00.373152: step 13042, loss 0.595529.
Train: 2018-08-05T16:14:03.980823: step 13043, loss 0.529482.
Train: 2018-08-05T16:14:07.592513: step 13044, loss 0.52946.
Train: 2018-08-05T16:14:11.245327: step 13045, loss 0.562486.
Train: 2018-08-05T16:14:14.838440: step 13046, loss 0.479669.
Train: 2018-08-05T16:14:18.477679: step 13047, loss 0.537538.
Train: 2018-08-05T16:14:22.094382: step 13048, loss 0.612427.
Train: 2018-08-05T16:14:25.685013: step 13049, loss 0.587468.
Train: 2018-08-05T16:14:29.316758: step 13050, loss 0.512239.
Test: 2018-08-05T16:14:44.519817: step 13050, loss 0.548013.
Train: 2018-08-05T16:14:48.143037: step 13051, loss 0.612688.
Train: 2018-08-05T16:14:51.724116: step 13052, loss 0.562391.
Train: 2018-08-05T16:14:55.313226: step 13053, loss 0.595993.
Train: 2018-08-05T16:14:58.918926: step 13054, loss 0.562383.
Train: 2018-08-05T16:15:02.532624: step 13055, loss 0.50351.
Train: 2018-08-05T16:15:06.150327: step 13056, loss 0.511829.
Train: 2018-08-05T16:15:09.758013: step 13057, loss 0.621482.
Train: 2018-08-05T16:15:13.343630: step 13058, loss 0.570815.
Train: 2018-08-05T16:15:16.937274: step 13059, loss 0.528504.
Train: 2018-08-05T16:15:20.541934: step 13060, loss 0.528448.
Test: 2018-08-05T16:15:37.177491: step 13060, loss 0.547967.
Train: 2018-08-05T16:15:41.342315: step 13061, loss 0.562347.
Train: 2018-08-05T16:15:45.466453: step 13062, loss 0.579362.
Train: 2018-08-05T16:15:49.614130: step 13063, loss 0.604947.
Train: 2018-08-05T16:15:53.721939: step 13064, loss 0.519716.
Train: 2018-08-05T16:15:57.774050: step 13065, loss 0.562338.
Train: 2018-08-05T16:16:01.921698: step 13066, loss 0.596515.
Train: 2018-08-05T16:16:05.998720: step 13067, loss 0.49396.
Train: 2018-08-05T16:16:10.086266: step 13068, loss 0.493851.
Train: 2018-08-05T16:16:14.206846: step 13069, loss 0.570919.
Train: 2018-08-05T16:16:18.263066: step 13070, loss 0.519321.
Test: 2018-08-05T16:16:34.969319: step 13070, loss 0.547533.
Train: 2018-08-05T16:16:39.070920: step 13071, loss 0.510577.
Train: 2018-08-05T16:16:43.173455: step 13072, loss 0.588312.
Train: 2018-08-05T16:16:47.342716: step 13073, loss 0.571027.
Train: 2018-08-05T16:16:51.454972: step 13074, loss 0.553659.
Train: 2018-08-05T16:16:55.571068: step 13075, loss 0.562361.
Train: 2018-08-05T16:16:59.679757: step 13076, loss 0.562367.
Train: 2018-08-05T16:17:03.807956: step 13077, loss 0.536156.
Train: 2018-08-05T16:17:07.845827: step 13078, loss 0.579885.
Train: 2018-08-05T16:17:11.839580: step 13079, loss 0.58867.
Train: 2018-08-05T16:17:15.458283: step 13080, loss 0.597445.
Test: 2018-08-05T16:17:30.663914: step 13080, loss 0.54755.
Train: 2018-08-05T16:17:34.309701: step 13081, loss 0.588667.
Train: 2018-08-05T16:17:37.875257: step 13082, loss 0.597389.
Train: 2018-08-05T16:17:41.482415: step 13083, loss 0.544895.
Train: 2018-08-05T16:17:45.072067: step 13084, loss 0.614732.
Train: 2018-08-05T16:17:48.690746: step 13085, loss 0.501399.
Train: 2018-08-05T16:17:52.282386: step 13086, loss 0.640662.
Train: 2018-08-05T16:17:55.888538: step 13087, loss 0.518948.
Train: 2018-08-05T16:17:59.486167: step 13088, loss 0.597019.
Train: 2018-08-05T16:18:03.088846: step 13089, loss 0.588296.
Train: 2018-08-05T16:18:06.675481: step 13090, loss 0.536444.
Test: 2018-08-05T16:18:21.871110: step 13090, loss 0.548937.
Train: 2018-08-05T16:18:25.467259: step 13091, loss 0.562337.
Train: 2018-08-05T16:18:29.088462: step 13092, loss 0.596753.
Train: 2018-08-05T16:18:32.754808: step 13093, loss 0.622441.
Train: 2018-08-05T16:18:36.389561: step 13094, loss 0.528092.
Train: 2018-08-05T16:18:39.972156: step 13095, loss 0.502536.
Train: 2018-08-05T16:18:43.543728: step 13096, loss 0.545264.
Train: 2018-08-05T16:18:47.135856: step 13097, loss 0.579407.
Train: 2018-08-05T16:18:50.747044: step 13098, loss 0.562339.
Train: 2018-08-05T16:18:54.369747: step 13099, loss 0.553817.
Train: 2018-08-05T16:18:57.961873: step 13100, loss 0.502694.
Test: 2018-08-05T16:19:13.198079: step 13100, loss 0.546526.
Train: 2018-08-05T16:19:19.090852: step 13101, loss 0.570868.
Train: 2018-08-05T16:19:22.692508: step 13102, loss 0.596472.
Train: 2018-08-05T16:19:26.352322: step 13103, loss 0.570871.
Train: 2018-08-05T16:19:29.940456: step 13104, loss 0.570869.
Train: 2018-08-05T16:19:33.562687: step 13105, loss 0.587921.
Train: 2018-08-05T16:19:37.178383: step 13106, loss 0.596421.
Train: 2018-08-05T16:19:40.785058: step 13107, loss 0.562344.
Train: 2018-08-05T16:19:44.393730: step 13108, loss 0.536854.
Train: 2018-08-05T16:19:47.984354: step 13109, loss 0.536871.
Train: 2018-08-05T16:19:51.574986: step 13110, loss 0.630283.
Test: 2018-08-05T16:20:06.966146: step 13110, loss 0.547602.
Train: 2018-08-05T16:20:10.609933: step 13111, loss 0.536909.
Train: 2018-08-05T16:20:14.234644: step 13112, loss 0.503028.
Train: 2018-08-05T16:20:17.844318: step 13113, loss 0.545391.
Train: 2018-08-05T16:20:21.429925: step 13114, loss 0.579324.
Train: 2018-08-05T16:20:25.052648: step 13115, loss 0.579331.
Train: 2018-08-05T16:20:28.677344: step 13116, loss 0.485913.
Train: 2018-08-05T16:20:32.310079: step 13117, loss 0.494292.
Train: 2018-08-05T16:20:35.932793: step 13118, loss 0.519689.
Train: 2018-08-05T16:20:39.522938: step 13119, loss 0.553777.
Train: 2018-08-05T16:20:43.117078: step 13120, loss 0.459314.
Test: 2018-08-05T16:20:58.292172: step 13120, loss 0.548727.
Train: 2018-08-05T16:21:01.890321: step 13121, loss 0.648593.
Train: 2018-08-05T16:21:05.481949: step 13122, loss 0.501808.
Train: 2018-08-05T16:21:09.120704: step 13123, loss 0.597056.
Train: 2018-08-05T16:21:12.736396: step 13124, loss 0.544961.
Train: 2018-08-05T16:21:16.327526: step 13125, loss 0.475192.
Train: 2018-08-05T16:21:19.918657: step 13126, loss 0.588622.
Train: 2018-08-05T16:21:23.517309: step 13127, loss 0.588703.
Train: 2018-08-05T16:21:27.115966: step 13128, loss 0.492103.
Train: 2018-08-05T16:21:30.730691: step 13129, loss 0.518359.
Train: 2018-08-05T16:21:34.332366: step 13130, loss 0.624304.
Test: 2018-08-05T16:21:49.569177: step 13130, loss 0.547662.
Train: 2018-08-05T16:21:53.175841: step 13131, loss 0.500481.
Train: 2018-08-05T16:21:56.768503: step 13132, loss 0.562463.
Train: 2018-08-05T16:22:00.380180: step 13133, loss 0.580259.
Train: 2018-08-05T16:22:04.000881: step 13134, loss 0.517984.
Train: 2018-08-05T16:22:07.621583: step 13135, loss 0.598171.
Train: 2018-08-05T16:22:11.245276: step 13136, loss 0.517895.
Train: 2018-08-05T16:22:13.007019: step 13137, loss 0.581585.
Train: 2018-08-05T16:22:16.607150: step 13138, loss 0.616172.
Train: 2018-08-05T16:22:20.213316: step 13139, loss 0.580395.
Train: 2018-08-05T16:22:23.803419: step 13140, loss 0.464323.
Test: 2018-08-05T16:22:39.078276: step 13140, loss 0.547604.
Train: 2018-08-05T16:22:42.724548: step 13141, loss 0.598249.
Train: 2018-08-05T16:22:46.375331: step 13142, loss 0.625027.
Train: 2018-08-05T16:22:50.010069: step 13143, loss 0.509006.
Train: 2018-08-05T16:22:53.604213: step 13144, loss 0.624878.
Train: 2018-08-05T16:22:57.207352: step 13145, loss 0.642541.
Train: 2018-08-05T16:23:00.806494: step 13146, loss 0.509257.
Train: 2018-08-05T16:23:04.419676: step 13147, loss 0.580139.
Train: 2018-08-05T16:23:08.032363: step 13148, loss 0.535944.
Train: 2018-08-05T16:23:11.648571: step 13149, loss 0.597661.
Train: 2018-08-05T16:23:15.233179: step 13150, loss 0.562401.
Test: 2018-08-05T16:23:30.442323: step 13150, loss 0.548347.
Train: 2018-08-05T16:23:34.072531: step 13151, loss 0.544847.
Train: 2018-08-05T16:23:37.705265: step 13152, loss 0.536116.
Train: 2018-08-05T16:23:41.337007: step 13153, loss 0.562374.
Train: 2018-08-05T16:23:44.973739: step 13154, loss 0.518708.
Train: 2018-08-05T16:23:48.610990: step 13155, loss 0.518726.
Train: 2018-08-05T16:23:52.192620: step 13156, loss 0.544907.
Train: 2018-08-05T16:23:55.796785: step 13157, loss 0.501224.
Train: 2018-08-05T16:23:59.400446: step 13158, loss 0.46615.
Train: 2018-08-05T16:24:03.004124: step 13159, loss 0.553617.
Train: 2018-08-05T16:24:06.646888: step 13160, loss 0.606391.
Test: 2018-08-05T16:24:21.928665: step 13160, loss 0.546281.
Train: 2018-08-05T16:24:25.541848: step 13161, loss 0.668137.
Train: 2018-08-05T16:24:29.131964: step 13162, loss 0.553605.
Train: 2018-08-05T16:24:32.722611: step 13163, loss 0.562407.
Train: 2018-08-05T16:24:36.313242: step 13164, loss 0.536022.
Train: 2018-08-05T16:24:39.901849: step 13165, loss 0.536027.
Train: 2018-08-05T16:24:43.533584: step 13166, loss 0.536024.
Train: 2018-08-05T16:24:47.171334: step 13167, loss 0.544811.
Train: 2018-08-05T16:24:50.777015: step 13168, loss 0.571211.
Train: 2018-08-05T16:24:54.371651: step 13169, loss 0.615239.
Train: 2018-08-05T16:24:57.974302: step 13170, loss 0.536013.
Test: 2018-08-05T16:25:13.237121: step 13170, loss 0.547312.
Train: 2018-08-05T16:25:16.868353: step 13171, loss 0.536021.
Train: 2018-08-05T16:25:20.499562: step 13172, loss 0.527227.
Train: 2018-08-05T16:25:24.116269: step 13173, loss 0.606397.
Train: 2018-08-05T16:25:27.753020: step 13174, loss 0.571197.
Train: 2018-08-05T16:25:31.364709: step 13175, loss 0.615127.
Train: 2018-08-05T16:25:34.954305: step 13176, loss 0.606255.
Train: 2018-08-05T16:25:38.553463: step 13177, loss 0.509869.
Train: 2018-08-05T16:25:42.178184: step 13178, loss 0.641031.
Train: 2018-08-05T16:25:45.803928: step 13179, loss 0.562362.
Train: 2018-08-05T16:25:49.407572: step 13180, loss 0.571047.
Test: 2018-08-05T16:26:04.645263: step 13180, loss 0.547067.
Train: 2018-08-05T16:26:08.258961: step 13181, loss 0.484304.
Train: 2018-08-05T16:26:11.871667: step 13182, loss 0.614325.
Train: 2018-08-05T16:26:15.484358: step 13183, loss 0.46722.
Train: 2018-08-05T16:26:19.094033: step 13184, loss 0.553694.
Train: 2018-08-05T16:26:22.715739: step 13185, loss 0.605586.
Train: 2018-08-05T16:26:26.331427: step 13186, loss 0.553698.
Train: 2018-08-05T16:26:29.919057: step 13187, loss 0.562339.
Train: 2018-08-05T16:26:33.502668: step 13188, loss 0.553706.
Train: 2018-08-05T16:26:37.095796: step 13189, loss 0.570968.
Train: 2018-08-05T16:26:40.735066: step 13190, loss 0.519213.
Test: 2018-08-05T16:26:55.924213: step 13190, loss 0.54793.
Train: 2018-08-05T16:26:59.513824: step 13191, loss 0.579593.
Train: 2018-08-05T16:27:03.137547: step 13192, loss 0.510581.
Train: 2018-08-05T16:27:06.741215: step 13193, loss 0.570972.
Train: 2018-08-05T16:27:10.337847: step 13194, loss 0.588252.
Train: 2018-08-05T16:27:13.936500: step 13195, loss 0.622798.
Train: 2018-08-05T16:27:17.546705: step 13196, loss 0.501953.
Train: 2018-08-05T16:27:21.175428: step 13197, loss 0.605468.
Train: 2018-08-05T16:27:24.763039: step 13198, loss 0.648523.
Train: 2018-08-05T16:27:28.382721: step 13199, loss 0.605324.
Train: 2018-08-05T16:27:31.986369: step 13200, loss 0.58805.
Test: 2018-08-05T16:27:47.195495: step 13200, loss 0.548855.
Train: 2018-08-05T16:27:53.077728: step 13201, loss 0.587969.
Train: 2018-08-05T16:27:56.668375: step 13202, loss 0.570857.
Train: 2018-08-05T16:28:00.290096: step 13203, loss 0.553861.
Train: 2018-08-05T16:28:03.878205: step 13204, loss 0.596214.
Train: 2018-08-05T16:28:07.530489: step 13205, loss 0.570805.
Train: 2018-08-05T16:28:11.114100: step 13206, loss 0.553965.
Train: 2018-08-05T16:28:14.690681: step 13207, loss 0.621147.
Train: 2018-08-05T16:28:18.302362: step 13208, loss 0.537307.
Train: 2018-08-05T16:28:21.884969: step 13209, loss 0.587462.
Train: 2018-08-05T16:28:25.462575: step 13210, loss 0.529132.
Test: 2018-08-05T16:28:41.726663: step 13210, loss 0.548562.
Train: 2018-08-05T16:28:45.811132: step 13211, loss 0.72041.
Train: 2018-08-05T16:28:49.956433: step 13212, loss 0.521091.
Train: 2018-08-05T16:28:54.018911: step 13213, loss 0.471721.
Train: 2018-08-05T16:28:58.026197: step 13214, loss 0.546015.
Train: 2018-08-05T16:29:02.124255: step 13215, loss 0.579002.
Train: 2018-08-05T16:29:05.919942: step 13216, loss 0.50481.
Train: 2018-08-05T16:29:09.548683: step 13217, loss 0.579008.
Train: 2018-08-05T16:29:13.134284: step 13218, loss 0.521208.
Train: 2018-08-05T16:29:16.732935: step 13219, loss 0.504583.
Train: 2018-08-05T16:29:20.339605: step 13220, loss 0.595642.
Test: 2018-08-05T16:29:35.631422: step 13220, loss 0.548378.
Train: 2018-08-05T16:29:39.267197: step 13221, loss 0.587382.
Train: 2018-08-05T16:29:42.905958: step 13222, loss 0.52915.
Train: 2018-08-05T16:29:46.490062: step 13223, loss 0.52907.
Train: 2018-08-05T16:29:50.084198: step 13224, loss 0.537329.
Train: 2018-08-05T16:29:53.696389: step 13225, loss 0.545626.
Train: 2018-08-05T16:29:57.310068: step 13226, loss 0.545561.
Train: 2018-08-05T16:30:01.082198: step 13227, loss 0.545497.
Train: 2018-08-05T16:30:04.836776: step 13228, loss 0.545433.
Train: 2018-08-05T16:30:08.442948: step 13229, loss 0.53688.
Train: 2018-08-05T16:30:12.047613: step 13230, loss 0.596413.
Test: 2018-08-05T16:30:27.319912: step 13230, loss 0.547292.
Train: 2018-08-05T16:30:30.961183: step 13231, loss 0.519646.
Train: 2018-08-05T16:30:34.560837: step 13232, loss 0.562335.
Train: 2018-08-05T16:30:38.174026: step 13233, loss 0.536577.
Train: 2018-08-05T16:30:41.790704: step 13234, loss 0.545115.
Train: 2018-08-05T16:30:45.394361: step 13235, loss 0.536435.
Train: 2018-08-05T16:30:48.979988: step 13236, loss 0.605645.
Train: 2018-08-05T16:30:52.571625: step 13237, loss 0.571024.
Train: 2018-08-05T16:30:56.169259: step 13238, loss 0.544976.
Train: 2018-08-05T16:30:59.824042: step 13239, loss 0.562356.
Train: 2018-08-05T16:31:03.451786: step 13240, loss 0.597208.
Test: 2018-08-05T16:31:18.710510: step 13240, loss 0.547405.
Train: 2018-08-05T16:31:22.303142: step 13241, loss 0.536215.
Train: 2018-08-05T16:31:25.903272: step 13242, loss 0.571087.
Train: 2018-08-05T16:31:29.516451: step 13243, loss 0.579818.
Train: 2018-08-05T16:31:33.116105: step 13244, loss 0.623448.
Train: 2018-08-05T16:31:36.725778: step 13245, loss 0.536216.
Train: 2018-08-05T16:31:40.345485: step 13246, loss 0.510104.
Train: 2018-08-05T16:31:43.952162: step 13247, loss 0.571071.
Train: 2018-08-05T16:31:47.561833: step 13248, loss 0.588493.
Train: 2018-08-05T16:31:51.162987: step 13249, loss 0.562358.
Train: 2018-08-05T16:31:54.749118: step 13250, loss 0.571056.
Test: 2018-08-05T16:32:10.030975: step 13250, loss 0.546234.
Train: 2018-08-05T16:32:13.659713: step 13251, loss 0.571047.
Train: 2018-08-05T16:32:17.278422: step 13252, loss 0.597092.
Train: 2018-08-05T16:32:20.883092: step 13253, loss 0.536332.
Train: 2018-08-05T16:32:24.533882: step 13254, loss 0.562345.
Train: 2018-08-05T16:32:28.111468: step 13255, loss 0.527724.
Train: 2018-08-05T16:32:31.710625: step 13256, loss 0.674821.
Train: 2018-08-05T16:32:35.346880: step 13257, loss 0.553707.
Train: 2018-08-05T16:32:38.955078: step 13258, loss 0.579564.
Train: 2018-08-05T16:32:42.567770: step 13259, loss 0.605309.
Train: 2018-08-05T16:32:46.182464: step 13260, loss 0.528053.
Test: 2018-08-05T16:33:01.517968: step 13260, loss 0.548247.
Train: 2018-08-05T16:33:05.156245: step 13261, loss 0.57089.
Train: 2018-08-05T16:33:08.790990: step 13262, loss 0.5538.
Train: 2018-08-05T16:33:12.406695: step 13263, loss 0.528242.
Train: 2018-08-05T16:33:16.049469: step 13264, loss 0.536786.
Train: 2018-08-05T16:33:19.638592: step 13265, loss 0.545308.
Train: 2018-08-05T16:33:23.242748: step 13266, loss 0.604934.
Train: 2018-08-05T16:33:26.832886: step 13267, loss 0.528291.
Train: 2018-08-05T16:33:30.450082: step 13268, loss 0.596397.
Train: 2018-08-05T16:33:34.091869: step 13269, loss 0.528308.
Train: 2018-08-05T16:33:37.710587: step 13270, loss 0.553833.
Test: 2018-08-05T16:33:53.004354: step 13270, loss 0.546753.
Train: 2018-08-05T16:33:56.732846: step 13271, loss 0.655986.
Train: 2018-08-05T16:34:00.348547: step 13272, loss 0.553845.
Train: 2018-08-05T16:34:03.935157: step 13273, loss 0.58782.
Train: 2018-08-05T16:34:07.573925: step 13274, loss 0.562352.
Train: 2018-08-05T16:34:11.186622: step 13275, loss 0.570822.
Train: 2018-08-05T16:34:14.812863: step 13276, loss 0.486265.
Train: 2018-08-05T16:34:18.446608: step 13277, loss 0.52007.
Train: 2018-08-05T16:34:22.034231: step 13278, loss 0.51155.
Train: 2018-08-05T16:34:25.644917: step 13279, loss 0.52841.
Train: 2018-08-05T16:34:29.249082: step 13280, loss 0.536825.
Test: 2018-08-05T16:34:44.518840: step 13280, loss 0.546916.
Train: 2018-08-05T16:34:48.113979: step 13281, loss 0.562339.
Train: 2018-08-05T16:34:51.749210: step 13282, loss 0.58799.
Train: 2018-08-05T16:34:55.370921: step 13283, loss 0.596599.
Train: 2018-08-05T16:34:58.985602: step 13284, loss 0.52804.
Train: 2018-08-05T16:35:02.581236: step 13285, loss 0.570921.
Train: 2018-08-05T16:35:06.190398: step 13286, loss 0.562335.
Train: 2018-08-05T16:35:09.823159: step 13287, loss 0.510712.
Train: 2018-08-05T16:35:11.616495: step 13288, loss 0.562337.
Train: 2018-08-05T16:35:15.217675: step 13289, loss 0.596872.
Train: 2018-08-05T16:35:18.821347: step 13290, loss 0.622817.
Test: 2018-08-05T16:35:34.066534: step 13290, loss 0.548713.
Train: 2018-08-05T16:35:37.705796: step 13291, loss 0.579609.
Train: 2018-08-05T16:35:41.312475: step 13292, loss 0.588221.
Train: 2018-08-05T16:35:44.905609: step 13293, loss 0.493403.
Train: 2018-08-05T16:35:48.545883: step 13294, loss 0.527867.
Train: 2018-08-05T16:35:52.166592: step 13295, loss 0.562337.
Train: 2018-08-05T16:35:55.772759: step 13296, loss 0.605481.
Train: 2018-08-05T16:35:59.361863: step 13297, loss 0.59684.
Train: 2018-08-05T16:36:02.965011: step 13298, loss 0.605422.
Train: 2018-08-05T16:36:06.606763: step 13299, loss 0.62255.
Train: 2018-08-05T16:36:10.240486: step 13300, loss 0.622386.
Test: 2018-08-05T16:36:25.511743: step 13300, loss 0.548062.
Train: 2018-08-05T16:36:31.425094: step 13301, loss 0.536694.
Train: 2018-08-05T16:36:35.025747: step 13302, loss 0.596436.
Train: 2018-08-05T16:36:38.631453: step 13303, loss 0.570843.
Train: 2018-08-05T16:36:42.247135: step 13304, loss 0.553883.
Train: 2018-08-05T16:36:45.866869: step 13305, loss 0.655314.
Train: 2018-08-05T16:36:49.489595: step 13306, loss 0.553962.
Train: 2018-08-05T16:36:53.117323: step 13307, loss 0.562394.
Train: 2018-08-05T16:36:56.702934: step 13308, loss 0.528961.
Train: 2018-08-05T16:37:00.311090: step 13309, loss 0.579113.
Train: 2018-08-05T16:37:03.897180: step 13310, loss 0.604081.
Test: 2018-08-05T16:37:19.138347: step 13310, loss 0.548576.
Train: 2018-08-05T16:37:22.734488: step 13311, loss 0.579068.
Train: 2018-08-05T16:37:26.403340: step 13312, loss 0.554182.
Train: 2018-08-05T16:37:30.041112: step 13313, loss 0.504587.
Train: 2018-08-05T16:37:33.643250: step 13314, loss 0.521151.
Train: 2018-08-05T16:37:37.259955: step 13315, loss 0.51285.
Train: 2018-08-05T16:37:40.851079: step 13316, loss 0.52104.
Train: 2018-08-05T16:37:44.436202: step 13317, loss 0.562453.
Train: 2018-08-05T16:37:48.078975: step 13318, loss 0.529132.
Train: 2018-08-05T16:37:51.706705: step 13319, loss 0.562419.
Train: 2018-08-05T16:37:55.289808: step 13320, loss 0.537285.
Test: 2018-08-05T16:38:10.524548: step 13320, loss 0.547976.
Train: 2018-08-05T16:38:14.159820: step 13321, loss 0.604376.
Train: 2018-08-05T16:38:17.775532: step 13322, loss 0.646518.
Train: 2018-08-05T16:38:21.395758: step 13323, loss 0.596039.
Train: 2018-08-05T16:38:25.012447: step 13324, loss 0.528736.
Train: 2018-08-05T16:38:28.644195: step 13325, loss 0.58762.
Train: 2018-08-05T16:38:32.231802: step 13326, loss 0.596029.
Train: 2018-08-05T16:38:35.821404: step 13327, loss 0.520353.
Train: 2018-08-05T16:38:39.424043: step 13328, loss 0.52875.
Train: 2018-08-05T16:38:43.023700: step 13329, loss 0.545546.
Train: 2018-08-05T16:38:46.650449: step 13330, loss 0.61293.
Test: 2018-08-05T16:39:01.871654: step 13330, loss 0.548685.
Train: 2018-08-05T16:39:05.513424: step 13331, loss 0.621366.
Train: 2018-08-05T16:39:09.119584: step 13332, loss 0.537117.
Train: 2018-08-05T16:39:12.722759: step 13333, loss 0.537125.
Train: 2018-08-05T16:39:16.314391: step 13334, loss 0.587635.
Train: 2018-08-05T16:39:19.912041: step 13335, loss 0.528703.
Train: 2018-08-05T16:39:23.517666: step 13336, loss 0.612913.
Train: 2018-08-05T16:39:27.191524: step 13337, loss 0.587635.
Train: 2018-08-05T16:39:30.874400: step 13338, loss 0.528729.
Train: 2018-08-05T16:39:34.482102: step 13339, loss 0.520316.
Train: 2018-08-05T16:39:38.101812: step 13340, loss 0.511855.
Test: 2018-08-05T16:39:53.330492: step 13340, loss 0.548669.
Train: 2018-08-05T16:39:56.924131: step 13341, loss 0.545497.
Train: 2018-08-05T16:40:00.589454: step 13342, loss 0.545455.
Train: 2018-08-05T16:40:04.315479: step 13343, loss 0.596241.
Train: 2018-08-05T16:40:07.967269: step 13344, loss 0.528418.
Train: 2018-08-05T16:40:11.573967: step 13345, loss 0.613342.
Train: 2018-08-05T16:40:15.184639: step 13346, loss 0.545333.
Train: 2018-08-05T16:40:18.774234: step 13347, loss 0.553828.
Train: 2018-08-05T16:40:22.390415: step 13348, loss 0.570864.
Train: 2018-08-05T16:40:26.055741: step 13349, loss 0.639116.
Train: 2018-08-05T16:40:29.674954: step 13350, loss 0.587914.
Test: 2018-08-05T16:40:44.944221: step 13350, loss 0.549102.
Train: 2018-08-05T16:40:48.593001: step 13351, loss 0.638975.
Train: 2018-08-05T16:40:52.192147: step 13352, loss 0.613309.
Train: 2018-08-05T16:40:55.793296: step 13353, loss 0.579289.
Train: 2018-08-05T16:40:59.402994: step 13354, loss 0.579245.
Train: 2018-08-05T16:41:03.027710: step 13355, loss 0.553967.
Train: 2018-08-05T16:41:06.658434: step 13356, loss 0.554002.
Train: 2018-08-05T16:41:10.253067: step 13357, loss 0.554031.
Train: 2018-08-05T16:41:13.836677: step 13358, loss 0.570771.
Train: 2018-08-05T16:41:17.409762: step 13359, loss 0.43724.
Train: 2018-08-05T16:41:20.991841: step 13360, loss 0.562415.
Test: 2018-08-05T16:41:37.051642: step 13360, loss 0.548245.
Train: 2018-08-05T16:41:40.933053: step 13361, loss 0.554043.
Train: 2018-08-05T16:41:44.860575: step 13362, loss 0.579153.
Train: 2018-08-05T16:41:48.711935: step 13363, loss 0.528862.
Train: 2018-08-05T16:41:52.449954: step 13364, loss 0.553991.
Train: 2018-08-05T16:41:56.058625: step 13365, loss 0.654898.
Train: 2018-08-05T16:41:59.677317: step 13366, loss 0.545565.
Train: 2018-08-05T16:42:03.315070: step 13367, loss 0.478284.
Train: 2018-08-05T16:42:07.248667: step 13368, loss 0.537097.
Train: 2018-08-05T16:42:10.890429: step 13369, loss 0.621473.
Train: 2018-08-05T16:42:14.498106: step 13370, loss 0.570813.
Test: 2018-08-05T16:42:29.704223: step 13370, loss 0.548819.
Train: 2018-08-05T16:42:33.314389: step 13371, loss 0.630017.
Train: 2018-08-05T16:42:36.927582: step 13372, loss 0.596168.
Train: 2018-08-05T16:42:40.553814: step 13373, loss 0.570807.
Train: 2018-08-05T16:42:44.167511: step 13374, loss 0.612957.
Train: 2018-08-05T16:42:47.781719: step 13375, loss 0.562379.
Train: 2018-08-05T16:42:51.391908: step 13376, loss 0.587584.
Train: 2018-08-05T16:42:54.983518: step 13377, loss 0.512103.
Train: 2018-08-05T16:42:58.590667: step 13378, loss 0.503769.
Train: 2018-08-05T16:43:02.208390: step 13379, loss 0.528874.
Train: 2018-08-05T16:43:05.840121: step 13380, loss 0.537217.
Test: 2018-08-05T16:43:21.133920: step 13380, loss 0.548539.
Train: 2018-08-05T16:43:24.753124: step 13381, loss 0.562383.
Train: 2018-08-05T16:43:28.343257: step 13382, loss 0.612892.
Train: 2018-08-05T16:43:31.951931: step 13383, loss 0.629763.
Train: 2018-08-05T16:43:35.571655: step 13384, loss 0.596045.
Train: 2018-08-05T16:43:39.243517: step 13385, loss 0.596006.
Train: 2018-08-05T16:43:42.915892: step 13386, loss 0.503657.
Train: 2018-08-05T16:43:46.578201: step 13387, loss 0.495293.
Train: 2018-08-05T16:43:50.248010: step 13388, loss 0.562389.
Train: 2018-08-05T16:43:53.849675: step 13389, loss 0.545573.
Train: 2018-08-05T16:43:57.453839: step 13390, loss 0.495047.
Test: 2018-08-05T16:44:12.729078: step 13390, loss 0.547304.
Train: 2018-08-05T16:44:16.330733: step 13391, loss 0.486425.
Train: 2018-08-05T16:44:19.945423: step 13392, loss 0.511532.
Train: 2018-08-05T16:44:23.567117: step 13393, loss 0.587868.
Train: 2018-08-05T16:44:27.148214: step 13394, loss 0.442804.
Train: 2018-08-05T16:44:30.732830: step 13395, loss 0.545166.
Train: 2018-08-05T16:44:34.327980: step 13396, loss 0.579595.
Train: 2018-08-05T16:44:37.942673: step 13397, loss 0.527684.
Train: 2018-08-05T16:44:41.559370: step 13398, loss 0.457918.
Train: 2018-08-05T16:44:45.197157: step 13399, loss 0.553626.
Train: 2018-08-05T16:44:48.794811: step 13400, loss 0.580004.
Test: 2018-08-05T16:45:04.014480: step 13400, loss 0.547676.
Train: 2018-08-05T16:45:09.771374: step 13401, loss 0.562433.
Train: 2018-08-05T16:45:13.355475: step 13402, loss 0.571327.
Train: 2018-08-05T16:45:16.954121: step 13403, loss 0.526906.
Train: 2018-08-05T16:45:20.578824: step 13404, loss 0.624962.
Train: 2018-08-05T16:45:24.203044: step 13405, loss 0.634005.
Train: 2018-08-05T16:45:27.825759: step 13406, loss 0.633999.
Train: 2018-08-05T16:45:31.436450: step 13407, loss 0.544667.
Train: 2018-08-05T16:45:35.019059: step 13408, loss 0.500126.
Train: 2018-08-05T16:45:38.616705: step 13409, loss 0.598132.
Train: 2018-08-05T16:45:42.233397: step 13410, loss 0.580289.
Test: 2018-08-05T16:45:57.454234: step 13410, loss 0.546201.
Train: 2018-08-05T16:46:01.105527: step 13411, loss 0.544701.
Train: 2018-08-05T16:46:04.733236: step 13412, loss 0.500319.
Train: 2018-08-05T16:46:08.426140: step 13413, loss 0.571346.
Train: 2018-08-05T16:46:12.045870: step 13414, loss 0.615715.
Train: 2018-08-05T16:46:15.625459: step 13415, loss 0.535867.
Train: 2018-08-05T16:46:19.267226: step 13416, loss 0.535885.
Train: 2018-08-05T16:46:22.897964: step 13417, loss 0.553594.
Train: 2018-08-05T16:46:26.476597: step 13418, loss 0.580129.
Train: 2018-08-05T16:46:30.075248: step 13419, loss 0.54476.
Train: 2018-08-05T16:46:33.681928: step 13420, loss 0.562429.
Test: 2018-08-05T16:46:48.861982: step 13420, loss 0.548296.
Train: 2018-08-05T16:46:52.432024: step 13421, loss 0.562424.
Train: 2018-08-05T16:46:56.061798: step 13422, loss 0.615325.
Train: 2018-08-05T16:46:59.700051: step 13423, loss 0.500795.
Train: 2018-08-05T16:47:03.270111: step 13424, loss 0.518423.
Train: 2018-08-05T16:47:06.848215: step 13425, loss 0.588796.
Train: 2018-08-05T16:47:10.461382: step 13426, loss 0.518439.
Train: 2018-08-05T16:47:14.033957: step 13427, loss 0.553609.
Train: 2018-08-05T16:47:17.646666: step 13428, loss 0.474443.
Train: 2018-08-05T16:47:21.275390: step 13429, loss 0.562414.
Train: 2018-08-05T16:47:24.873049: step 13430, loss 0.518311.
Test: 2018-08-05T16:47:40.091683: step 13430, loss 0.54686.
Train: 2018-08-05T16:47:43.701366: step 13431, loss 0.562435.
Train: 2018-08-05T16:47:47.304019: step 13432, loss 0.509332.
Train: 2018-08-05T16:47:50.896146: step 13433, loss 0.544719.
Train: 2018-08-05T16:47:54.524384: step 13434, loss 0.526919.
Train: 2018-08-05T16:47:58.146098: step 13435, loss 0.553588.
Train: 2018-08-05T16:48:01.744757: step 13436, loss 0.651809.
Train: 2018-08-05T16:48:05.365468: step 13437, loss 0.491077.
Train: 2018-08-05T16:48:08.990195: step 13438, loss 0.634055.
Train: 2018-08-05T16:48:10.740917: step 13439, loss 0.562526.
Train: 2018-08-05T16:48:14.373674: step 13440, loss 0.517859.
Test: 2018-08-05T16:48:29.567301: step 13440, loss 0.547397.
Train: 2018-08-05T16:48:33.219107: step 13441, loss 0.544656.
Train: 2018-08-05T16:48:36.813261: step 13442, loss 0.634006.
Train: 2018-08-05T16:48:40.395363: step 13443, loss 0.571437.
Train: 2018-08-05T16:48:44.013071: step 13444, loss 0.491209.
Train: 2018-08-05T16:48:47.611707: step 13445, loss 0.615952.
Train: 2018-08-05T16:48:51.248465: step 13446, loss 0.500204.
Train: 2018-08-05T16:48:54.875189: step 13447, loss 0.580273.
Train: 2018-08-05T16:48:58.485864: step 13448, loss 0.526924.
Train: 2018-08-05T16:49:02.071501: step 13449, loss 0.544703.
Train: 2018-08-05T16:49:05.658606: step 13450, loss 0.52693.
Test: 2018-08-05T16:49:20.915388: step 13450, loss 0.546404.
Train: 2018-08-05T16:49:24.510547: step 13451, loss 0.58026.
Train: 2018-08-05T16:49:28.124247: step 13452, loss 0.615813.
Train: 2018-08-05T16:49:31.771549: step 13453, loss 0.615739.
Train: 2018-08-05T16:49:35.404780: step 13454, loss 0.562451.
Train: 2018-08-05T16:49:38.987401: step 13455, loss 0.465191.
Train: 2018-08-05T16:49:42.579559: step 13456, loss 0.61546.
Train: 2018-08-05T16:49:46.186738: step 13457, loss 0.535949.
Train: 2018-08-05T16:49:49.790407: step 13458, loss 0.544784.
Train: 2018-08-05T16:49:53.424154: step 13459, loss 0.562415.
Train: 2018-08-05T16:49:57.058906: step 13460, loss 0.535994.
Test: 2018-08-05T16:50:12.428487: step 13460, loss 0.548518.
Train: 2018-08-05T16:50:16.048698: step 13461, loss 0.615226.
Train: 2018-08-05T16:50:19.679441: step 13462, loss 0.562401.
Train: 2018-08-05T16:50:23.271093: step 13463, loss 0.641408.
Train: 2018-08-05T16:50:26.889298: step 13464, loss 0.56238.
Train: 2018-08-05T16:50:30.545617: step 13465, loss 0.518705.
Train: 2018-08-05T16:50:34.157290: step 13466, loss 0.553645.
Train: 2018-08-05T16:50:37.756943: step 13467, loss 0.60589.
Train: 2018-08-05T16:50:41.349585: step 13468, loss 0.553664.
Train: 2018-08-05T16:50:44.952217: step 13469, loss 0.588362.
Train: 2018-08-05T16:50:48.541332: step 13470, loss 0.45851.
Test: 2018-08-05T16:51:03.757496: step 13470, loss 0.547097.
Train: 2018-08-05T16:51:07.417815: step 13471, loss 0.588297.
Train: 2018-08-05T16:51:11.047537: step 13472, loss 0.519111.
Train: 2018-08-05T16:51:14.689328: step 13473, loss 0.545047.
Train: 2018-08-05T16:51:18.283448: step 13474, loss 0.614248.
Train: 2018-08-05T16:51:21.881590: step 13475, loss 0.562341.
Train: 2018-08-05T16:51:25.458183: step 13476, loss 0.58826.
Train: 2018-08-05T16:51:29.093946: step 13477, loss 0.61412.
Train: 2018-08-05T16:51:32.703619: step 13478, loss 0.493433.
Train: 2018-08-05T16:51:36.301261: step 13479, loss 0.570944.
Train: 2018-08-05T16:51:39.896916: step 13480, loss 0.613949.
Test: 2018-08-05T16:51:55.095996: step 13480, loss 0.549573.
Train: 2018-08-05T16:51:58.703163: step 13481, loss 0.545159.
Train: 2018-08-05T16:52:02.298811: step 13482, loss 0.570913.
Train: 2018-08-05T16:52:05.936567: step 13483, loss 0.528065.
Train: 2018-08-05T16:52:09.562299: step 13484, loss 0.62228.
Train: 2018-08-05T16:52:13.150428: step 13485, loss 0.493937.
Train: 2018-08-05T16:52:16.736531: step 13486, loss 0.562337.
Train: 2018-08-05T16:52:20.338181: step 13487, loss 0.528146.
Train: 2018-08-05T16:52:23.921291: step 13488, loss 0.562336.
Train: 2018-08-05T16:52:27.546532: step 13489, loss 0.562336.
Train: 2018-08-05T16:52:31.164217: step 13490, loss 0.596575.
Test: 2018-08-05T16:52:46.407935: step 13490, loss 0.547651.
Train: 2018-08-05T16:52:50.044681: step 13491, loss 0.528107.
Train: 2018-08-05T16:52:53.628315: step 13492, loss 0.536654.
Train: 2018-08-05T16:52:57.218961: step 13493, loss 0.596606.
Train: 2018-08-05T16:53:00.821617: step 13494, loss 0.605176.
Train: 2018-08-05T16:53:04.446320: step 13495, loss 0.562336.
Train: 2018-08-05T16:53:08.068007: step 13496, loss 0.519558.
Train: 2018-08-05T16:53:11.668637: step 13497, loss 0.502436.
Train: 2018-08-05T16:53:15.261270: step 13498, loss 0.528063.
Train: 2018-08-05T16:53:18.857914: step 13499, loss 0.536585.
Train: 2018-08-05T16:53:22.463083: step 13500, loss 0.484925.
Test: 2018-08-05T16:53:37.721846: step 13500, loss 0.548125.
Train: 2018-08-05T16:53:43.606610: step 13501, loss 0.614114.
Train: 2018-08-05T16:53:47.218305: step 13502, loss 0.553695.
Train: 2018-08-05T16:53:50.815946: step 13503, loss 0.588329.
Train: 2018-08-05T16:53:54.379527: step 13504, loss 0.597028.
Train: 2018-08-05T16:53:57.965140: step 13505, loss 0.640391.
Train: 2018-08-05T16:54:01.555781: step 13506, loss 0.536401.
Train: 2018-08-05T16:54:05.155423: step 13507, loss 0.527869.
Train: 2018-08-05T16:54:08.819265: step 13508, loss 0.648844.
Train: 2018-08-05T16:54:12.395848: step 13509, loss 0.648692.
Train: 2018-08-05T16:54:15.992480: step 13510, loss 0.510719.
Test: 2018-08-05T16:54:31.187527: step 13510, loss 0.547404.
Train: 2018-08-05T16:54:34.793707: step 13511, loss 0.553763.
Train: 2018-08-05T16:54:38.414404: step 13512, loss 0.545219.
Train: 2018-08-05T16:54:42.013552: step 13513, loss 0.682266.
Train: 2018-08-05T16:54:45.642788: step 13514, loss 0.59656.
Train: 2018-08-05T16:54:49.243477: step 13515, loss 0.570693.
Train: 2018-08-05T16:54:52.847153: step 13516, loss 0.579252.
Train: 2018-08-05T16:54:56.441781: step 13517, loss 0.562083.
Train: 2018-08-05T16:55:00.029894: step 13518, loss 0.588185.
Train: 2018-08-05T16:55:03.615011: step 13519, loss 0.621023.
Train: 2018-08-05T16:55:07.238238: step 13520, loss 0.562243.
Test: 2018-08-05T16:55:22.451864: step 13520, loss 0.54806.
Train: 2018-08-05T16:55:26.116721: step 13521, loss 0.529982.
Train: 2018-08-05T16:55:29.726415: step 13522, loss 0.595702.
Train: 2018-08-05T16:55:33.315048: step 13523, loss 0.56237.
Train: 2018-08-05T16:55:36.907667: step 13524, loss 0.603757.
Train: 2018-08-05T16:55:40.511322: step 13525, loss 0.537342.
Train: 2018-08-05T16:55:44.145053: step 13526, loss 0.53789.
Train: 2018-08-05T16:55:47.786821: step 13527, loss 0.56269.
Train: 2018-08-05T16:55:51.391490: step 13528, loss 0.562541.
Train: 2018-08-05T16:55:55.009703: step 13529, loss 0.521734.
Train: 2018-08-05T16:55:58.626880: step 13530, loss 0.571051.
Test: 2018-08-05T16:56:13.833459: step 13530, loss 0.549352.
Train: 2018-08-05T16:56:17.487254: step 13531, loss 0.554374.
Train: 2018-08-05T16:56:21.130037: step 13532, loss 0.620312.
Train: 2018-08-05T16:56:24.747215: step 13533, loss 0.545957.
Train: 2018-08-05T16:56:28.334832: step 13534, loss 0.595563.
Train: 2018-08-05T16:56:31.938023: step 13535, loss 0.521152.
Train: 2018-08-05T16:56:35.531640: step 13536, loss 0.529386.
Train: 2018-08-05T16:56:39.153349: step 13537, loss 0.56247.
Train: 2018-08-05T16:56:42.754997: step 13538, loss 0.520963.
Train: 2018-08-05T16:56:46.385248: step 13539, loss 0.512532.
Train: 2018-08-05T16:56:49.983398: step 13540, loss 0.662564.
Test: 2018-08-05T16:57:05.215657: step 13540, loss 0.548274.
Train: 2018-08-05T16:57:08.840859: step 13541, loss 0.554063.
Train: 2018-08-05T16:57:12.449048: step 13542, loss 0.56241.
Train: 2018-08-05T16:57:16.045185: step 13543, loss 0.570776.
Train: 2018-08-05T16:57:19.681455: step 13544, loss 0.562399.
Train: 2018-08-05T16:57:23.312684: step 13545, loss 0.503683.
Train: 2018-08-05T16:57:26.921863: step 13546, loss 0.503553.
Train: 2018-08-05T16:57:30.504966: step 13547, loss 0.537081.
Train: 2018-08-05T16:57:34.106638: step 13548, loss 0.536985.
Train: 2018-08-05T16:57:37.697256: step 13549, loss 0.596299.
Train: 2018-08-05T16:57:41.340032: step 13550, loss 0.485766.
Test: 2018-08-05T16:57:56.555165: step 13550, loss 0.548861.
Train: 2018-08-05T16:58:00.149306: step 13551, loss 0.579418.
Train: 2018-08-05T16:58:03.742956: step 13552, loss 0.605166.
Train: 2018-08-05T16:58:07.442868: step 13553, loss 0.528006.
Train: 2018-08-05T16:58:11.132747: step 13554, loss 0.467713.
Train: 2018-08-05T16:58:14.803598: step 13555, loss 0.570974.
Train: 2018-08-05T16:58:18.548605: step 13556, loss 0.622982.
Train: 2018-08-05T16:58:22.187368: step 13557, loss 0.440868.
Train: 2018-08-05T16:58:25.784017: step 13558, loss 0.492686.
Train: 2018-08-05T16:58:29.365135: step 13559, loss 0.492392.
Train: 2018-08-05T16:58:32.953269: step 13560, loss 0.588782.
Test: 2018-08-05T16:58:48.246137: step 13560, loss 0.54687.
Train: 2018-08-05T16:58:51.892901: step 13561, loss 0.500626.
Train: 2018-08-05T16:58:55.553724: step 13562, loss 0.553591.
Train: 2018-08-05T16:58:59.198501: step 13563, loss 0.500156.
Train: 2018-08-05T16:59:02.811686: step 13564, loss 0.643051.
Train: 2018-08-05T16:59:06.386283: step 13565, loss 0.652248.
Train: 2018-08-05T16:59:09.986929: step 13566, loss 0.535647.
Train: 2018-08-05T16:59:13.593586: step 13567, loss 0.508696.
Train: 2018-08-05T16:59:17.223314: step 13568, loss 0.616545.
Train: 2018-08-05T16:59:20.846028: step 13569, loss 0.562591.
Train: 2018-08-05T16:59:24.429645: step 13570, loss 0.508639.
Test: 2018-08-05T16:59:39.658374: step 13570, loss 0.550055.
Train: 2018-08-05T16:59:43.257526: step 13571, loss 0.553599.
Train: 2018-08-05T16:59:46.859169: step 13572, loss 0.5536.
Train: 2018-08-05T16:59:50.454820: step 13573, loss 0.544596.
Train: 2018-08-05T16:59:54.064489: step 13574, loss 0.616661.
Train: 2018-08-05T16:59:57.695256: step 13575, loss 0.607609.
Train: 2018-08-05T17:00:01.497971: step 13576, loss 0.571569.
Train: 2018-08-05T17:00:05.192898: step 13577, loss 0.526684.
Train: 2018-08-05T17:00:08.805587: step 13578, loss 0.607341.
Train: 2018-08-05T17:00:12.398234: step 13579, loss 0.616165.
Train: 2018-08-05T17:00:15.997876: step 13580, loss 0.526851.
Test: 2018-08-05T17:00:31.280163: step 13580, loss 0.546402.
Train: 2018-08-05T17:00:34.904873: step 13581, loss 0.589157.
Train: 2018-08-05T17:00:38.533074: step 13582, loss 0.544723.
Train: 2018-08-05T17:00:42.119683: step 13583, loss 0.606682.
Train: 2018-08-05T17:00:45.744401: step 13584, loss 0.544779.
Train: 2018-08-05T17:00:49.355085: step 13585, loss 0.492011.
Train: 2018-08-05T17:00:52.954730: step 13586, loss 0.579979.
Train: 2018-08-05T17:00:56.604553: step 13587, loss 0.588722.
Train: 2018-08-05T17:01:00.232774: step 13588, loss 0.571143.
Train: 2018-08-05T17:01:03.818397: step 13589, loss 0.606086.
Train: 2018-08-05T17:01:05.572150: step 13590, loss 0.580964.
Test: 2018-08-05T17:01:21.021428: step 13590, loss 0.546433.
Train: 2018-08-05T17:01:25.126076: step 13591, loss 0.544965.
Train: 2018-08-05T17:01:29.249231: step 13592, loss 0.536325.
Train: 2018-08-05T17:01:33.472185: step 13593, loss 0.683576.
Train: 2018-08-05T17:01:37.511116: step 13594, loss 0.536458.
Train: 2018-08-05T17:01:41.569618: step 13595, loss 0.579537.
Train: 2018-08-05T17:01:45.632128: step 13596, loss 0.562335.
Train: 2018-08-05T17:01:49.673603: step 13597, loss 0.596539.
Train: 2018-08-05T17:01:53.821596: step 13598, loss 0.570864.
Train: 2018-08-05T17:01:57.992372: step 13599, loss 0.485865.
Train: 2018-08-05T17:02:02.071343: step 13600, loss 0.502937.
Test: 2018-08-05T17:02:18.111858: step 13600, loss 0.549151.
Train: 2018-08-05T17:02:24.076326: step 13601, loss 0.545373.
Train: 2018-08-05T17:02:27.663448: step 13602, loss 0.638767.
Train: 2018-08-05T17:02:31.256095: step 13603, loss 0.579314.
Train: 2018-08-05T17:02:34.851737: step 13604, loss 0.579296.
Train: 2018-08-05T17:02:38.486480: step 13605, loss 0.545441.
Train: 2018-08-05T17:02:42.111215: step 13606, loss 0.562362.
Train: 2018-08-05T17:02:45.692853: step 13607, loss 0.587697.
Train: 2018-08-05T17:02:49.293498: step 13608, loss 0.5455.
Train: 2018-08-05T17:02:52.891147: step 13609, loss 0.579229.
Train: 2018-08-05T17:02:56.475750: step 13610, loss 0.579218.
Test: 2018-08-05T17:03:11.692926: step 13610, loss 0.548717.
Train: 2018-08-05T17:03:15.331157: step 13611, loss 0.59603.
Train: 2018-08-05T17:03:18.936833: step 13612, loss 0.595985.
Train: 2018-08-05T17:03:22.540509: step 13613, loss 0.54563.
Train: 2018-08-05T17:03:26.137652: step 13614, loss 0.54566.
Train: 2018-08-05T17:03:29.728773: step 13615, loss 0.579138.
Train: 2018-08-05T17:03:33.371564: step 13616, loss 0.570769.
Train: 2018-08-05T17:03:36.989266: step 13617, loss 0.587462.
Train: 2018-08-05T17:03:40.642056: step 13618, loss 0.545746.
Train: 2018-08-05T17:03:44.292835: step 13619, loss 0.545779.
Train: 2018-08-05T17:03:47.895504: step 13620, loss 0.554043.
Test: 2018-08-05T17:04:03.132191: step 13620, loss 0.548171.
Train: 2018-08-05T17:04:06.739875: step 13621, loss 0.52915.
Train: 2018-08-05T17:04:10.361581: step 13622, loss 0.529354.
Train: 2018-08-05T17:04:14.045448: step 13623, loss 0.50401.
Train: 2018-08-05T17:04:17.692729: step 13624, loss 0.579141.
Train: 2018-08-05T17:04:21.286341: step 13625, loss 0.637916.
Train: 2018-08-05T17:04:24.876957: step 13626, loss 0.604369.
Train: 2018-08-05T17:04:28.464092: step 13627, loss 0.537209.
Train: 2018-08-05T17:04:32.066761: step 13628, loss 0.545596.
Train: 2018-08-05T17:04:35.702494: step 13629, loss 0.579189.
Train: 2018-08-05T17:04:39.330207: step 13630, loss 0.453121.
Test: 2018-08-05T17:04:54.558342: step 13630, loss 0.549077.
Train: 2018-08-05T17:04:58.162509: step 13631, loss 0.629781.
Train: 2018-08-05T17:05:01.787230: step 13632, loss 0.579238.
Train: 2018-08-05T17:05:05.399393: step 13633, loss 0.579246.
Train: 2018-08-05T17:05:09.015083: step 13634, loss 0.528596.
Train: 2018-08-05T17:05:12.650311: step 13635, loss 0.503209.
Train: 2018-08-05T17:05:16.273013: step 13636, loss 0.528484.
Train: 2018-08-05T17:05:19.882692: step 13637, loss 0.689686.
Train: 2018-08-05T17:05:23.492863: step 13638, loss 0.494445.
Train: 2018-08-05T17:05:27.128137: step 13639, loss 0.58784.
Train: 2018-08-05T17:05:30.746835: step 13640, loss 0.596358.
Test: 2018-08-05T17:05:45.971030: step 13640, loss 0.546775.
Train: 2018-08-05T17:05:49.579221: step 13641, loss 0.570848.
Train: 2018-08-05T17:05:53.219979: step 13642, loss 0.570846.
Train: 2018-08-05T17:05:56.843691: step 13643, loss 0.485866.
Train: 2018-08-05T17:06:00.445335: step 13644, loss 0.494282.
Train: 2018-08-05T17:06:04.048485: step 13645, loss 0.604978.
Train: 2018-08-05T17:06:07.619556: step 13646, loss 0.494026.
Train: 2018-08-05T17:06:11.229235: step 13647, loss 0.579456.
Train: 2018-08-05T17:06:14.872988: step 13648, loss 0.545183.
Train: 2018-08-05T17:06:18.504725: step 13649, loss 0.588115.
Train: 2018-08-05T17:06:22.113406: step 13650, loss 0.596753.
Test: 2018-08-05T17:06:37.375141: step 13650, loss 0.549148.
Train: 2018-08-05T17:06:41.002854: step 13651, loss 0.545119.
Train: 2018-08-05T17:06:44.608493: step 13652, loss 0.570951.
Train: 2018-08-05T17:06:48.221177: step 13653, loss 0.588192.
Train: 2018-08-05T17:06:51.870954: step 13654, loss 0.562337.
Train: 2018-08-05T17:06:55.497673: step 13655, loss 0.579569.
Train: 2018-08-05T17:06:59.088777: step 13656, loss 0.545112.
Train: 2018-08-05T17:07:02.711457: step 13657, loss 0.553725.
Train: 2018-08-05T17:07:06.315621: step 13658, loss 0.450399.
Train: 2018-08-05T17:07:09.943862: step 13659, loss 0.562338.
Train: 2018-08-05T17:07:13.579116: step 13660, loss 0.527767.
Test: 2018-08-05T17:07:28.814315: step 13660, loss 0.547878.
Train: 2018-08-05T17:07:32.430493: step 13661, loss 0.50171.
Train: 2018-08-05T17:07:36.050194: step 13662, loss 0.614481.
Train: 2018-08-05T17:07:39.642822: step 13663, loss 0.527546.
Train: 2018-08-05T17:07:43.223416: step 13664, loss 0.544923.
Train: 2018-08-05T17:07:46.811011: step 13665, loss 0.527417.
Train: 2018-08-05T17:07:50.459794: step 13666, loss 0.571142.
Train: 2018-08-05T17:07:54.090534: step 13667, loss 0.623825.
Train: 2018-08-05T17:07:57.689688: step 13668, loss 0.544834.
Train: 2018-08-05T17:08:01.297868: step 13669, loss 0.544826.
Train: 2018-08-05T17:08:04.904552: step 13670, loss 0.562402.
Test: 2018-08-05T17:08:20.151255: step 13670, loss 0.548928.
Train: 2018-08-05T17:08:23.776992: step 13671, loss 0.536013.
Train: 2018-08-05T17:08:27.406720: step 13672, loss 0.571215.
Train: 2018-08-05T17:08:31.042489: step 13673, loss 0.500749.
Train: 2018-08-05T17:08:34.647170: step 13674, loss 0.60653.
Train: 2018-08-05T17:08:38.268884: step 13675, loss 0.509477.
Train: 2018-08-05T17:08:41.874533: step 13676, loss 0.544763.
Train: 2018-08-05T17:08:45.479215: step 13677, loss 0.659729.
Train: 2018-08-05T17:08:49.097947: step 13678, loss 0.597783.
Train: 2018-08-05T17:08:52.718629: step 13679, loss 0.580072.
Train: 2018-08-05T17:08:56.335805: step 13680, loss 0.571221.
Test: 2018-08-05T17:09:11.614182: step 13680, loss 0.548731.
Train: 2018-08-05T17:09:15.245913: step 13681, loss 0.579984.
Train: 2018-08-05T17:09:18.894690: step 13682, loss 0.588707.
Train: 2018-08-05T17:09:22.499354: step 13683, loss 0.536127.
Train: 2018-08-05T17:09:26.140602: step 13684, loss 0.58857.
Train: 2018-08-05T17:09:29.782856: step 13685, loss 0.544934.
Train: 2018-08-05T17:09:33.382505: step 13686, loss 0.536263.
Train: 2018-08-05T17:09:37.019264: step 13687, loss 0.657899.
Train: 2018-08-05T17:09:40.620930: step 13688, loss 0.510381.
Train: 2018-08-05T17:09:44.209555: step 13689, loss 0.648793.
Train: 2018-08-05T17:09:47.867395: step 13690, loss 0.545102.
Test: 2018-08-05T17:10:03.253511: step 13690, loss 0.547586.
Train: 2018-08-05T17:10:06.845144: step 13691, loss 0.527958.
Train: 2018-08-05T17:10:10.447313: step 13692, loss 0.459385.
Train: 2018-08-05T17:10:14.053492: step 13693, loss 0.553753.
Train: 2018-08-05T17:10:17.658182: step 13694, loss 0.588091.
Train: 2018-08-05T17:10:21.255832: step 13695, loss 0.596671.
Train: 2018-08-05T17:10:24.881568: step 13696, loss 0.630954.
Train: 2018-08-05T17:10:28.505273: step 13697, loss 0.528095.
Train: 2018-08-05T17:10:32.131537: step 13698, loss 0.476836.
Train: 2018-08-05T17:10:35.719654: step 13699, loss 0.664992.
Train: 2018-08-05T17:10:39.320288: step 13700, loss 0.613591.
Test: 2018-08-05T17:10:54.585488: step 13700, loss 0.547126.
Train: 2018-08-05T17:11:00.411583: step 13701, loss 0.528249.
Train: 2018-08-05T17:11:04.009222: step 13702, loss 0.570854.
Train: 2018-08-05T17:11:07.626932: step 13703, loss 0.604841.
Train: 2018-08-05T17:11:11.252159: step 13704, loss 0.545388.
Train: 2018-08-05T17:11:14.834284: step 13705, loss 0.545417.
Train: 2018-08-05T17:11:18.436937: step 13706, loss 0.61312.
Train: 2018-08-05T17:11:22.027584: step 13707, loss 0.570809.
Train: 2018-08-05T17:11:25.627241: step 13708, loss 0.503361.
Train: 2018-08-05T17:11:29.237889: step 13709, loss 0.511813.
Train: 2018-08-05T17:11:32.862612: step 13710, loss 0.56237.
Test: 2018-08-05T17:11:48.078719: step 13710, loss 0.547693.
Train: 2018-08-05T17:11:51.718007: step 13711, loss 0.612994.
Train: 2018-08-05T17:11:55.317664: step 13712, loss 0.587674.
Train: 2018-08-05T17:11:58.901262: step 13713, loss 0.579229.
Train: 2018-08-05T17:12:02.474346: step 13714, loss 0.612901.
Train: 2018-08-05T17:12:06.098551: step 13715, loss 0.545571.
Train: 2018-08-05T17:12:09.728778: step 13716, loss 0.528804.
Train: 2018-08-05T17:12:13.442776: step 13717, loss 0.579177.
Train: 2018-08-05T17:12:17.480839: step 13718, loss 0.595947.
Train: 2018-08-05T17:12:21.518836: step 13719, loss 0.528882.
Train: 2018-08-05T17:12:25.526157: step 13720, loss 0.587532.
Test: 2018-08-05T17:12:41.895802: step 13720, loss 0.549193.
Train: 2018-08-05T17:12:45.960265: step 13721, loss 0.537288.
Train: 2018-08-05T17:12:50.024757: step 13722, loss 0.528916.
Train: 2018-08-05T17:12:54.083755: step 13723, loss 0.554021.
Train: 2018-08-05T17:12:58.089586: step 13724, loss 0.478536.
Train: 2018-08-05T17:13:02.147517: step 13725, loss 0.545568.
Train: 2018-08-05T17:13:06.200461: step 13726, loss 0.562371.
Train: 2018-08-05T17:13:10.399375: step 13727, loss 0.528561.
Train: 2018-08-05T17:13:14.492173: step 13728, loss 0.545404.
Train: 2018-08-05T17:13:18.538109: step 13729, loss 0.562345.
Train: 2018-08-05T17:13:22.556008: step 13730, loss 0.52825.
Test: 2018-08-05T17:13:39.005537: step 13730, loss 0.546486.
Train: 2018-08-05T17:13:43.150725: step 13731, loss 0.613629.
Train: 2018-08-05T17:13:47.242786: step 13732, loss 0.588027.
Train: 2018-08-05T17:13:51.379936: step 13733, loss 0.485178.
Train: 2018-08-05T17:13:55.444973: step 13734, loss 0.519367.
Train: 2018-08-05T17:13:59.481841: step 13735, loss 0.545099.
Train: 2018-08-05T17:14:03.541383: step 13736, loss 0.536409.
Train: 2018-08-05T17:14:07.611424: step 13737, loss 0.588358.
Train: 2018-08-05T17:14:11.683133: step 13738, loss 0.597109.
Train: 2018-08-05T17:14:15.736581: step 13739, loss 0.597157.
Train: 2018-08-05T17:14:19.804560: step 13740, loss 0.579765.
Test: 2018-08-05T17:14:36.211587: step 13740, loss 0.549224.
Train: 2018-08-05T17:14:38.240622: step 13741, loss 0.580925.
Train: 2018-08-05T17:14:42.302593: step 13742, loss 0.527557.
Train: 2018-08-05T17:14:46.299380: step 13743, loss 0.553655.
Train: 2018-08-05T17:14:50.300193: step 13744, loss 0.57106.
Train: 2018-08-05T17:14:54.393719: step 13745, loss 0.562357.
Train: 2018-08-05T17:14:58.468182: step 13746, loss 0.571058.
Train: 2018-08-05T17:15:02.556211: step 13747, loss 0.536259.
Train: 2018-08-05T17:15:06.587442: step 13748, loss 0.597154.
Train: 2018-08-05T17:15:10.588493: step 13749, loss 0.562354.
Train: 2018-08-05T17:15:14.617403: step 13750, loss 0.57104.
Test: 2018-08-05T17:15:31.058474: step 13750, loss 0.547052.
Train: 2018-08-05T17:15:35.127024: step 13751, loss 0.544987.
Train: 2018-08-05T17:15:39.244721: step 13752, loss 0.544994.
Train: 2018-08-05T17:15:43.351266: step 13753, loss 0.571024.
Train: 2018-08-05T17:15:47.410740: step 13754, loss 0.553675.
Train: 2018-08-05T17:15:51.479703: step 13755, loss 0.527668.
Train: 2018-08-05T17:15:55.552726: step 13756, loss 0.623055.
Train: 2018-08-05T17:15:59.593968: step 13757, loss 0.545016.
Train: 2018-08-05T17:16:03.634849: step 13758, loss 0.579664.
Train: 2018-08-05T17:16:07.678743: step 13759, loss 0.614259.
Train: 2018-08-05T17:16:11.736178: step 13760, loss 0.501879.
Test: 2018-08-05T17:16:28.136300: step 13760, loss 0.547326.
Train: 2018-08-05T17:16:32.173231: step 13761, loss 0.536441.
Train: 2018-08-05T17:16:36.239254: step 13762, loss 0.579602.
Train: 2018-08-05T17:16:40.322550: step 13763, loss 0.570966.
Train: 2018-08-05T17:16:44.412541: step 13764, loss 0.588207.
Train: 2018-08-05T17:16:48.507266: step 13765, loss 0.52788.
Train: 2018-08-05T17:16:52.514636: step 13766, loss 0.51928.
Train: 2018-08-05T17:16:56.220583: step 13767, loss 0.665719.
Train: 2018-08-05T17:16:59.813223: step 13768, loss 0.648354.
Train: 2018-08-05T17:17:03.425928: step 13769, loss 0.545183.
Train: 2018-08-05T17:17:07.035607: step 13770, loss 0.562336.
Test: 2018-08-05T17:17:22.297359: step 13770, loss 0.548279.
Train: 2018-08-05T17:17:25.929100: step 13771, loss 0.528194.
Train: 2018-08-05T17:17:29.512715: step 13772, loss 0.502669.
Train: 2018-08-05T17:17:33.125394: step 13773, loss 0.56234.
Train: 2018-08-05T17:17:36.736065: step 13774, loss 0.587911.
Train: 2018-08-05T17:17:40.443507: step 13775, loss 0.49419.
Train: 2018-08-05T17:17:44.084779: step 13776, loss 0.587918.
Train: 2018-08-05T17:17:47.716531: step 13777, loss 0.613509.
Train: 2018-08-05T17:17:51.298134: step 13778, loss 0.630517.
Train: 2018-08-05T17:17:54.891764: step 13779, loss 0.587861.
Train: 2018-08-05T17:17:58.481878: step 13780, loss 0.519916.
Test: 2018-08-05T17:18:13.725575: step 13780, loss 0.54644.
Train: 2018-08-05T17:18:17.307690: step 13781, loss 0.553876.
Train: 2018-08-05T17:18:20.966522: step 13782, loss 0.62164.
Train: 2018-08-05T17:18:24.608298: step 13783, loss 0.613081.
Train: 2018-08-05T17:18:28.225999: step 13784, loss 0.503352.
Train: 2018-08-05T17:18:31.827640: step 13785, loss 0.520269.
Train: 2018-08-05T17:18:35.425796: step 13786, loss 0.511857.
Train: 2018-08-05T17:18:39.027466: step 13787, loss 0.570799.
Train: 2018-08-05T17:18:42.667241: step 13788, loss 0.570803.
Train: 2018-08-05T17:18:46.291962: step 13789, loss 0.579242.
Train: 2018-08-05T17:18:49.889626: step 13790, loss 0.537051.
Test: 2018-08-05T17:19:05.232193: step 13790, loss 0.547289.
Train: 2018-08-05T17:19:08.875453: step 13791, loss 0.621477.
Train: 2018-08-05T17:19:12.495150: step 13792, loss 0.596129.
Train: 2018-08-05T17:19:16.099809: step 13793, loss 0.587666.
Train: 2018-08-05T17:19:19.725031: step 13794, loss 0.520273.
Train: 2018-08-05T17:19:23.343222: step 13795, loss 0.638136.
Train: 2018-08-05T17:19:26.936360: step 13796, loss 0.503564.
Train: 2018-08-05T17:19:30.521954: step 13797, loss 0.637989.
Train: 2018-08-05T17:19:34.115079: step 13798, loss 0.537237.
Train: 2018-08-05T17:19:37.712220: step 13799, loss 0.528889.
Train: 2018-08-05T17:19:41.352970: step 13800, loss 0.512141.
Test: 2018-08-05T17:19:56.561098: step 13800, loss 0.5482.
Train: 2018-08-05T17:20:02.550625: step 13801, loss 0.537242.
Train: 2018-08-05T17:20:06.137239: step 13802, loss 0.553992.
Train: 2018-08-05T17:20:09.730866: step 13803, loss 0.520335.
Train: 2018-08-05T17:20:13.317473: step 13804, loss 0.528659.
Train: 2018-08-05T17:20:16.910080: step 13805, loss 0.646874.
Train: 2018-08-05T17:20:20.536823: step 13806, loss 0.587734.
Train: 2018-08-05T17:20:24.151521: step 13807, loss 0.536974.
Train: 2018-08-05T17:20:27.793276: step 13808, loss 0.570823.
Train: 2018-08-05T17:20:31.374882: step 13809, loss 0.536934.
Train: 2018-08-05T17:20:34.969499: step 13810, loss 0.579315.
Test: 2018-08-05T17:20:50.172090: step 13810, loss 0.548173.
Train: 2018-08-05T17:20:53.787775: step 13811, loss 0.553861.
Train: 2018-08-05T17:20:57.410486: step 13812, loss 0.485895.
Train: 2018-08-05T17:21:01.057268: step 13813, loss 0.579369.
Train: 2018-08-05T17:21:04.675963: step 13814, loss 0.56234.
Train: 2018-08-05T17:21:08.266119: step 13815, loss 0.65628.
Train: 2018-08-05T17:21:11.884331: step 13816, loss 0.528193.
Train: 2018-08-05T17:21:15.475457: step 13817, loss 0.536722.
Train: 2018-08-05T17:21:19.071109: step 13818, loss 0.562337.
Train: 2018-08-05T17:21:22.661744: step 13819, loss 0.536686.
Train: 2018-08-05T17:21:26.287452: step 13820, loss 0.545218.
Test: 2018-08-05T17:21:41.518679: step 13820, loss 0.547234.
Train: 2018-08-05T17:21:45.114842: step 13821, loss 0.579475.
Train: 2018-08-05T17:21:48.749584: step 13822, loss 0.467992.
Train: 2018-08-05T17:21:52.341194: step 13823, loss 0.579531.
Train: 2018-08-05T17:21:55.939834: step 13824, loss 0.562336.
Train: 2018-08-05T17:21:59.568566: step 13825, loss 0.665872.
Train: 2018-08-05T17:22:03.180766: step 13826, loss 0.562338.
Train: 2018-08-05T17:22:06.788917: step 13827, loss 0.51924.
Train: 2018-08-05T17:22:10.402609: step 13828, loss 0.519227.
Train: 2018-08-05T17:22:14.013772: step 13829, loss 0.545076.
Train: 2018-08-05T17:22:17.627987: step 13830, loss 0.56234.
Test: 2018-08-05T17:22:32.855682: step 13830, loss 0.547298.
Train: 2018-08-05T17:22:36.540550: step 13831, loss 0.53639.
Train: 2018-08-05T17:22:40.196358: step 13832, loss 0.588331.
Train: 2018-08-05T17:22:43.794019: step 13833, loss 0.58835.
Train: 2018-08-05T17:22:47.396176: step 13834, loss 0.527671.
Train: 2018-08-05T17:22:51.016404: step 13835, loss 0.579696.
Train: 2018-08-05T17:22:54.601018: step 13836, loss 0.562348.
Train: 2018-08-05T17:22:58.223199: step 13837, loss 0.527642.
Train: 2018-08-05T17:23:01.838891: step 13838, loss 0.640492.
Train: 2018-08-05T17:23:05.448072: step 13839, loss 0.518975.
Train: 2018-08-05T17:23:09.061733: step 13840, loss 0.553674.
Test: 2018-08-05T17:23:24.413214: step 13840, loss 0.548061.
Train: 2018-08-05T17:23:28.026896: step 13841, loss 0.588369.
Train: 2018-08-05T17:23:31.628537: step 13842, loss 0.519.
Train: 2018-08-05T17:23:35.308388: step 13843, loss 0.536333.
Train: 2018-08-05T17:23:38.964686: step 13844, loss 0.518964.
Train: 2018-08-05T17:23:42.607935: step 13845, loss 0.501535.
Train: 2018-08-05T17:23:46.207585: step 13846, loss 0.571066.
Train: 2018-08-05T17:23:49.834324: step 13847, loss 0.536197.
Train: 2018-08-05T17:23:53.414415: step 13848, loss 0.553633.
Train: 2018-08-05T17:23:57.049641: step 13849, loss 0.527359.
Train: 2018-08-05T17:24:00.680369: step 13850, loss 0.667683.
Test: 2018-08-05T17:24:15.971202: step 13850, loss 0.54794.
Train: 2018-08-05T17:24:19.590374: step 13851, loss 0.562391.
Train: 2018-08-05T17:24:23.190530: step 13852, loss 0.48344.
Train: 2018-08-05T17:24:26.779160: step 13853, loss 0.562396.
Train: 2018-08-05T17:24:30.385831: step 13854, loss 0.54482.
Train: 2018-08-05T17:24:34.030092: step 13855, loss 0.580004.
Train: 2018-08-05T17:24:37.658811: step 13856, loss 0.580015.
Train: 2018-08-05T17:24:41.254438: step 13857, loss 0.588818.
Train: 2018-08-05T17:24:44.857081: step 13858, loss 0.650381.
Train: 2018-08-05T17:24:48.476778: step 13859, loss 0.553615.
Train: 2018-08-05T17:24:52.084982: step 13860, loss 0.553623.
Test: 2018-08-05T17:25:07.361304: step 13860, loss 0.546764.
Train: 2018-08-05T17:25:11.391628: step 13861, loss 0.58861.
Train: 2018-08-05T17:25:15.128641: step 13862, loss 0.466379.
Train: 2018-08-05T17:25:18.851612: step 13863, loss 0.571089.
Train: 2018-08-05T17:25:22.536980: step 13864, loss 0.579802.
Train: 2018-08-05T17:25:26.253463: step 13865, loss 0.588495.
Train: 2018-08-05T17:25:29.907252: step 13866, loss 0.527556.
Train: 2018-08-05T17:25:33.661323: step 13867, loss 0.562354.
Train: 2018-08-05T17:25:37.312603: step 13868, loss 0.527598.
Train: 2018-08-05T17:25:40.906265: step 13869, loss 0.553664.
Train: 2018-08-05T17:25:44.525487: step 13870, loss 0.605795.
Test: 2018-08-05T17:25:59.835370: step 13870, loss 0.547252.
Train: 2018-08-05T17:26:03.435023: step 13871, loss 0.484216.
Train: 2018-08-05T17:26:07.066755: step 13872, loss 0.518917.
Train: 2018-08-05T17:26:10.684462: step 13873, loss 0.666732.
Train: 2018-08-05T17:26:14.336762: step 13874, loss 0.605807.
Train: 2018-08-05T17:26:17.934913: step 13875, loss 0.536318.
Train: 2018-08-05T17:26:21.553627: step 13876, loss 0.562346.
Train: 2018-08-05T17:26:25.136724: step 13877, loss 0.605638.
Train: 2018-08-05T17:26:28.732876: step 13878, loss 0.570984.
Train: 2018-08-05T17:26:32.341559: step 13879, loss 0.614106.
Train: 2018-08-05T17:26:35.972293: step 13880, loss 0.588151.
Test: 2018-08-05T17:26:51.231600: step 13880, loss 0.548598.
Train: 2018-08-05T17:26:54.852795: step 13881, loss 0.562335.
Train: 2018-08-05T17:26:58.458479: step 13882, loss 0.588009.
Train: 2018-08-05T17:27:02.059148: step 13883, loss 0.553806.
Train: 2018-08-05T17:27:05.654802: step 13884, loss 0.55383.
Train: 2018-08-05T17:27:09.269494: step 13885, loss 0.562347.
Train: 2018-08-05T17:27:12.905241: step 13886, loss 0.562351.
Train: 2018-08-05T17:27:16.536991: step 13887, loss 0.536956.
Train: 2018-08-05T17:27:20.157710: step 13888, loss 0.5539.
Train: 2018-08-05T17:27:23.748326: step 13889, loss 0.553907.
Train: 2018-08-05T17:27:27.357003: step 13890, loss 0.52856.
Test: 2018-08-05T17:27:42.598706: step 13890, loss 0.547853.
Train: 2018-08-05T17:27:46.239460: step 13891, loss 0.511637.
Train: 2018-08-05T17:27:48.030293: step 13892, loss 0.580416.
Train: 2018-08-05T17:27:51.659533: step 13893, loss 0.613193.
Train: 2018-08-05T17:27:55.281247: step 13894, loss 0.553881.
Train: 2018-08-05T17:27:58.893927: step 13895, loss 0.638611.
Train: 2018-08-05T17:28:02.519662: step 13896, loss 0.52005.
Train: 2018-08-05T17:28:06.159936: step 13897, loss 0.50315.
Train: 2018-08-05T17:28:09.825261: step 13898, loss 0.61315.
Train: 2018-08-05T17:28:13.460992: step 13899, loss 0.596213.
Train: 2018-08-05T17:28:17.078688: step 13900, loss 0.621563.
Test: 2018-08-05T17:28:32.348481: step 13900, loss 0.548072.
Train: 2018-08-05T17:28:38.201656: step 13901, loss 0.511711.
Train: 2018-08-05T17:28:41.795287: step 13902, loss 0.537055.
Train: 2018-08-05T17:28:45.405453: step 13903, loss 0.52018.
Train: 2018-08-05T17:28:49.127452: step 13904, loss 0.579255.
Train: 2018-08-05T17:28:52.744148: step 13905, loss 0.570812.
Train: 2018-08-05T17:28:56.325737: step 13906, loss 0.477844.
Train: 2018-08-05T17:28:59.922372: step 13907, loss 0.562355.
Train: 2018-08-05T17:29:03.512999: step 13908, loss 0.56235.
Train: 2018-08-05T17:29:07.115655: step 13909, loss 0.570844.
Train: 2018-08-05T17:29:10.767445: step 13910, loss 0.545323.
Test: 2018-08-05T17:29:25.994141: step 13910, loss 0.547518.
Train: 2018-08-05T17:29:29.613841: step 13911, loss 0.579386.
Train: 2018-08-05T17:29:33.230028: step 13912, loss 0.459951.
Train: 2018-08-05T17:29:36.956542: step 13913, loss 0.536664.
Train: 2018-08-05T17:29:40.591318: step 13914, loss 0.699674.
Train: 2018-08-05T17:29:44.217039: step 13915, loss 0.622431.
Train: 2018-08-05T17:29:47.851781: step 13916, loss 0.622371.
Train: 2018-08-05T17:29:51.493564: step 13917, loss 0.545217.
Train: 2018-08-05T17:29:55.074156: step 13918, loss 0.596521.
Train: 2018-08-05T17:29:58.663263: step 13919, loss 0.622041.
Train: 2018-08-05T17:30:02.488008: step 13920, loss 0.562345.
Test: 2018-08-05T17:30:17.808482: step 13920, loss 0.549749.
Train: 2018-08-05T17:30:21.469783: step 13921, loss 0.545389.
Train: 2018-08-05T17:30:25.138100: step 13922, loss 0.596212.
Train: 2018-08-05T17:30:28.776868: step 13923, loss 0.537036.
Train: 2018-08-05T17:30:32.392057: step 13924, loss 0.604516.
Train: 2018-08-05T17:30:35.995222: step 13925, loss 0.537152.
Train: 2018-08-05T17:30:39.593863: step 13926, loss 0.587579.
Train: 2018-08-05T17:30:43.187002: step 13927, loss 0.61269.
Train: 2018-08-05T17:30:46.806207: step 13928, loss 0.495521.
Train: 2018-08-05T17:30:50.431921: step 13929, loss 0.537353.
Train: 2018-08-05T17:30:54.035585: step 13930, loss 0.645944.
Test: 2018-08-05T17:31:09.308875: step 13930, loss 0.548118.
Train: 2018-08-05T17:31:12.926075: step 13931, loss 0.570766.
Train: 2018-08-05T17:31:16.528728: step 13932, loss 0.587413.
Train: 2018-08-05T17:31:20.127387: step 13933, loss 0.603999.
Train: 2018-08-05T17:31:23.749109: step 13934, loss 0.579047.
Train: 2018-08-05T17:31:27.386871: step 13935, loss 0.545946.
Train: 2018-08-05T17:31:30.990558: step 13936, loss 0.603786.
Train: 2018-08-05T17:31:34.605262: step 13937, loss 0.546052.
Train: 2018-08-05T17:31:38.212939: step 13938, loss 0.620121.
Train: 2018-08-05T17:31:41.807584: step 13939, loss 0.546136.
Train: 2018-08-05T17:31:45.407236: step 13940, loss 0.628128.
Test: 2018-08-05T17:32:00.669097: step 13940, loss 0.548003.
Train: 2018-08-05T17:32:04.279774: step 13941, loss 0.521726.
Train: 2018-08-05T17:32:07.916529: step 13942, loss 0.562608.
Train: 2018-08-05T17:32:11.521709: step 13943, loss 0.570774.
Train: 2018-08-05T17:32:15.130392: step 13944, loss 0.521888.
Train: 2018-08-05T17:32:18.737084: step 13945, loss 0.554475.
Train: 2018-08-05T17:32:22.320701: step 13946, loss 0.546307.
Train: 2018-08-05T17:32:25.969495: step 13947, loss 0.570771.
Train: 2018-08-05T17:32:29.620282: step 13948, loss 0.521731.
Train: 2018-08-05T17:32:33.217923: step 13949, loss 0.554386.
Train: 2018-08-05T17:32:36.821085: step 13950, loss 0.537935.
Test: 2018-08-05T17:32:52.050298: step 13950, loss 0.549737.
Train: 2018-08-05T17:32:55.676518: step 13951, loss 0.603668.
Train: 2018-08-05T17:32:59.271640: step 13952, loss 0.537793.
Train: 2018-08-05T17:33:02.919414: step 13953, loss 0.537722.
Train: 2018-08-05T17:33:06.546642: step 13954, loss 0.595596.
Train: 2018-08-05T17:33:10.155330: step 13955, loss 0.512697.
Train: 2018-08-05T17:33:13.772019: step 13956, loss 0.520859.
Train: 2018-08-05T17:33:17.362660: step 13957, loss 0.545732.
Train: 2018-08-05T17:33:20.948252: step 13958, loss 0.562402.
Train: 2018-08-05T17:33:24.580011: step 13959, loss 0.528782.
Train: 2018-08-05T17:33:28.198741: step 13960, loss 0.520214.
Test: 2018-08-05T17:33:43.554698: step 13960, loss 0.547243.
Train: 2018-08-05T17:33:47.181425: step 13961, loss 0.477698.
Train: 2018-08-05T17:33:50.799151: step 13962, loss 0.62192.
Train: 2018-08-05T17:33:54.394794: step 13963, loss 0.502542.
Train: 2018-08-05T17:33:57.994464: step 13964, loss 0.553755.
Train: 2018-08-05T17:34:01.633229: step 13965, loss 0.545107.
Train: 2018-08-05T17:34:05.243895: step 13966, loss 0.553693.
Train: 2018-08-05T17:34:08.886650: step 13967, loss 0.623114.
Train: 2018-08-05T17:34:12.509872: step 13968, loss 0.562355.
Train: 2018-08-05T17:34:16.103030: step 13969, loss 0.57979.
Train: 2018-08-05T17:34:19.716733: step 13970, loss 0.58854.
Test: 2018-08-05T17:34:34.979482: step 13970, loss 0.549798.
Train: 2018-08-05T17:34:38.615236: step 13971, loss 0.579826.
Train: 2018-08-05T17:34:42.277037: step 13972, loss 0.509989.
Train: 2018-08-05T17:34:45.879692: step 13973, loss 0.553634.
Train: 2018-08-05T17:34:49.471306: step 13974, loss 0.562375.
Train: 2018-08-05T17:34:53.072974: step 13975, loss 0.632389.
Train: 2018-08-05T17:34:56.680629: step 13976, loss 0.509906.
Train: 2018-08-05T17:35:00.311852: step 13977, loss 0.518644.
Train: 2018-08-05T17:35:03.949110: step 13978, loss 0.623655.
Train: 2018-08-05T17:35:07.544750: step 13979, loss 0.649875.
Train: 2018-08-05T17:35:11.158441: step 13980, loss 0.562369.
Test: 2018-08-05T17:35:26.434217: step 13980, loss 0.54841.
Train: 2018-08-05T17:35:30.080522: step 13981, loss 0.544933.
Train: 2018-08-05T17:35:33.703741: step 13982, loss 0.605859.
Train: 2018-08-05T17:35:37.510950: step 13983, loss 0.536307.
Train: 2018-08-05T17:35:41.156730: step 13984, loss 0.519013.
Train: 2018-08-05T17:35:44.767432: step 13985, loss 0.631624.
Train: 2018-08-05T17:35:48.370592: step 13986, loss 0.56234.
Train: 2018-08-05T17:35:51.995831: step 13987, loss 0.622722.
Train: 2018-08-05T17:35:55.624573: step 13988, loss 0.545132.
Train: 2018-08-05T17:35:59.250305: step 13989, loss 0.605246.
Train: 2018-08-05T17:36:02.892070: step 13990, loss 0.493877.
Test: 2018-08-05T17:36:18.114832: step 13990, loss 0.547671.
Train: 2018-08-05T17:36:21.727516: step 13991, loss 0.502515.
Train: 2018-08-05T17:36:25.326698: step 13992, loss 0.570883.
Train: 2018-08-05T17:36:28.949910: step 13993, loss 0.536706.
Train: 2018-08-05T17:36:32.557608: step 13994, loss 0.553791.
Train: 2018-08-05T17:36:36.210422: step 13995, loss 0.562337.
Train: 2018-08-05T17:36:39.835141: step 13996, loss 0.536678.
Train: 2018-08-05T17:36:43.470397: step 13997, loss 0.528098.
Train: 2018-08-05T17:36:47.076567: step 13998, loss 0.528051.
Train: 2018-08-05T17:36:50.673214: step 13999, loss 0.588096.
Train: 2018-08-05T17:36:54.288915: step 14000, loss 0.562335.
Test: 2018-08-05T17:37:09.536680: step 14000, loss 0.548162.
Train: 2018-08-05T17:37:15.371780: step 14001, loss 0.61397.
Train: 2018-08-05T17:37:18.965916: step 14002, loss 0.493497.
Train: 2018-08-05T17:37:22.559552: step 14003, loss 0.596796.
Train: 2018-08-05T17:37:26.128109: step 14004, loss 0.545101.
Train: 2018-08-05T17:37:29.735791: step 14005, loss 0.545091.
Train: 2018-08-05T17:37:33.324417: step 14006, loss 0.570969.
Train: 2018-08-05T17:37:36.930082: step 14007, loss 0.59688.
Train: 2018-08-05T17:37:40.527242: step 14008, loss 0.605507.
Train: 2018-08-05T17:37:44.120880: step 14009, loss 0.553713.
Train: 2018-08-05T17:37:47.731074: step 14010, loss 0.570954.
Test: 2018-08-05T17:38:02.972288: step 14010, loss 0.547561.
Train: 2018-08-05T17:38:06.609034: step 14011, loss 0.579555.
Train: 2018-08-05T17:38:10.216679: step 14012, loss 0.579534.
Train: 2018-08-05T17:38:13.839376: step 14013, loss 0.570922.
Train: 2018-08-05T17:38:17.483161: step 14014, loss 0.562335.
Train: 2018-08-05T17:38:21.126937: step 14015, loss 0.64796.
Train: 2018-08-05T17:38:24.722602: step 14016, loss 0.545262.
Train: 2018-08-05T17:38:28.334284: step 14017, loss 0.596417.
Train: 2018-08-05T17:38:31.935920: step 14018, loss 0.502874.
Train: 2018-08-05T17:38:35.547602: step 14019, loss 0.587806.
Train: 2018-08-05T17:38:39.187370: step 14020, loss 0.519994.
Test: 2018-08-05T17:38:54.422048: step 14020, loss 0.54763.
Train: 2018-08-05T17:38:58.038757: step 14021, loss 0.596224.
Train: 2018-08-05T17:39:01.673517: step 14022, loss 0.553902.
Train: 2018-08-05T17:39:05.256140: step 14023, loss 0.579263.
Train: 2018-08-05T17:39:08.872848: step 14024, loss 0.562366.
Train: 2018-08-05T17:39:12.464468: step 14025, loss 0.503329.
Train: 2018-08-05T17:39:16.110233: step 14026, loss 0.528616.
Train: 2018-08-05T17:39:19.737949: step 14027, loss 0.57081.
Train: 2018-08-05T17:39:23.325563: step 14028, loss 0.570815.
Train: 2018-08-05T17:39:26.921213: step 14029, loss 0.5116.
Train: 2018-08-05T17:39:30.528377: step 14030, loss 0.460673.
Test: 2018-08-05T17:39:45.894886: step 14030, loss 0.549711.
Train: 2018-08-05T17:39:49.494554: step 14031, loss 0.57935.
Train: 2018-08-05T17:39:53.141844: step 14032, loss 0.528235.
Train: 2018-08-05T17:39:56.784611: step 14033, loss 0.468252.
Train: 2018-08-05T17:40:00.401805: step 14034, loss 0.527965.
Train: 2018-08-05T17:40:04.168423: step 14035, loss 0.562339.
Train: 2018-08-05T17:40:07.906971: step 14036, loss 0.605694.
Train: 2018-08-05T17:40:11.575797: step 14037, loss 0.484102.
Train: 2018-08-05T17:40:15.208515: step 14038, loss 0.553639.
Train: 2018-08-05T17:40:18.848297: step 14039, loss 0.562383.
Train: 2018-08-05T17:40:22.450948: step 14040, loss 0.553611.
Test: 2018-08-05T17:40:37.708677: step 14040, loss 0.546277.
Train: 2018-08-05T17:40:41.433663: step 14041, loss 0.535976.
Train: 2018-08-05T17:40:45.035317: step 14042, loss 0.562435.
Train: 2018-08-05T17:40:46.800051: step 14043, loss 0.581355.
Train: 2018-08-05T17:40:50.429791: step 14044, loss 0.580217.
Train: 2018-08-05T17:40:54.037458: step 14045, loss 0.544704.
Train: 2018-08-05T17:40:57.660667: step 14046, loss 0.500219.
Train: 2018-08-05T17:41:01.248282: step 14047, loss 0.553588.
Train: 2018-08-05T17:41:04.857950: step 14048, loss 0.526807.
Train: 2018-08-05T17:41:08.461094: step 14049, loss 0.544645.
Train: 2018-08-05T17:41:12.080798: step 14050, loss 0.562553.
Test: 2018-08-05T17:41:27.603261: step 14050, loss 0.547993.
Train: 2018-08-05T17:41:31.473609: step 14051, loss 0.571544.
Train: 2018-08-05T17:41:35.294353: step 14052, loss 0.535628.
Train: 2018-08-05T17:41:39.145671: step 14053, loss 0.526616.
Train: 2018-08-05T17:41:43.088789: step 14054, loss 0.580622.
Train: 2018-08-05T17:41:47.048417: step 14055, loss 0.562617.
Train: 2018-08-05T17:41:50.761397: step 14056, loss 0.598693.
Train: 2018-08-05T17:41:54.470371: step 14057, loss 0.553604.
Train: 2018-08-05T17:41:58.172304: step 14058, loss 0.562612.
Train: 2018-08-05T17:42:01.781973: step 14059, loss 0.553601.
Train: 2018-08-05T17:42:05.379597: step 14060, loss 0.598588.
Test: 2018-08-05T17:42:20.665901: step 14060, loss 0.549023.
Train: 2018-08-05T17:42:24.261015: step 14061, loss 0.499687.
Train: 2018-08-05T17:42:27.847649: step 14062, loss 0.553595.
Train: 2018-08-05T17:42:31.447304: step 14063, loss 0.526662.
Train: 2018-08-05T17:42:35.054966: step 14064, loss 0.481772.
Train: 2018-08-05T17:42:38.649101: step 14065, loss 0.526631.
Train: 2018-08-05T17:42:42.240225: step 14066, loss 0.517591.
Train: 2018-08-05T17:42:45.861458: step 14067, loss 0.544586.
Train: 2018-08-05T17:42:49.479168: step 14068, loss 0.653005.
Train: 2018-08-05T17:42:53.099883: step 14069, loss 0.63492.
Train: 2018-08-05T17:42:56.735611: step 14070, loss 0.598704.
Test: 2018-08-05T17:43:12.068483: step 14070, loss 0.547569.
Train: 2018-08-05T17:43:15.722278: step 14071, loss 0.472609.
Train: 2018-08-05T17:43:19.355041: step 14072, loss 0.499645.
Train: 2018-08-05T17:43:22.982768: step 14073, loss 0.589574.
Train: 2018-08-05T17:43:26.581425: step 14074, loss 0.526629.
Train: 2018-08-05T17:43:30.222208: step 14075, loss 0.571575.
Train: 2018-08-05T17:43:33.903082: step 14076, loss 0.499684.
Train: 2018-08-05T17:43:37.536832: step 14077, loss 0.589557.
Train: 2018-08-05T17:43:41.201661: step 14078, loss 0.58056.
Train: 2018-08-05T17:43:44.815358: step 14079, loss 0.571558.
Train: 2018-08-05T17:43:48.478198: step 14080, loss 0.481817.
Test: 2018-08-05T17:44:03.816711: step 14080, loss 0.546961.
Train: 2018-08-05T17:44:07.487060: step 14081, loss 0.544619.
Train: 2018-08-05T17:44:11.147869: step 14082, loss 0.544617.
Train: 2018-08-05T17:44:14.764567: step 14083, loss 0.616465.
Train: 2018-08-05T17:44:18.394308: step 14084, loss 0.580517.
Train: 2018-08-05T17:44:22.001967: step 14085, loss 0.634265.
Train: 2018-08-05T17:44:25.620676: step 14086, loss 0.553589.
Train: 2018-08-05T17:44:29.265451: step 14087, loss 0.562508.
Train: 2018-08-05T17:44:32.886158: step 14088, loss 0.553588.
Train: 2018-08-05T17:44:36.511874: step 14089, loss 0.624639.
Train: 2018-08-05T17:44:40.123556: step 14090, loss 0.615567.
Test: 2018-08-05T17:44:55.429971: step 14090, loss 0.547489.
Train: 2018-08-05T17:44:59.061724: step 14091, loss 0.580057.
Train: 2018-08-05T17:45:02.675442: step 14092, loss 0.562396.
Train: 2018-08-05T17:45:06.343278: step 14093, loss 0.588629.
Train: 2018-08-05T17:45:09.979542: step 14094, loss 0.501352.
Train: 2018-08-05T17:45:13.598249: step 14095, loss 0.614514.
Train: 2018-08-05T17:45:17.220985: step 14096, loss 0.536352.
Train: 2018-08-05T17:45:20.872784: step 14097, loss 0.527772.
Train: 2018-08-05T17:45:24.472428: step 14098, loss 0.588219.
Train: 2018-08-05T17:45:28.121237: step 14099, loss 0.545119.
Train: 2018-08-05T17:45:31.761005: step 14100, loss 0.57093.
Test: 2018-08-05T17:45:47.051856: step 14100, loss 0.548202.
Train: 2018-08-05T17:45:52.952654: step 14101, loss 0.528012.
Train: 2018-08-05T17:45:56.561331: step 14102, loss 0.553762.
Train: 2018-08-05T17:46:00.269274: step 14103, loss 0.570903.
Train: 2018-08-05T17:46:03.854894: step 14104, loss 0.562336.
Train: 2018-08-05T17:46:07.490667: step 14105, loss 0.528113.
Train: 2018-08-05T17:46:11.129905: step 14106, loss 0.545224.
Train: 2018-08-05T17:46:14.731569: step 14107, loss 0.579454.
Train: 2018-08-05T17:46:18.333723: step 14108, loss 0.570895.
Train: 2018-08-05T17:46:21.936380: step 14109, loss 0.528107.
Train: 2018-08-05T17:46:25.549575: step 14110, loss 0.579458.
Test: 2018-08-05T17:46:40.783727: step 14110, loss 0.547051.
Train: 2018-08-05T17:46:44.428511: step 14111, loss 0.588021.
Train: 2018-08-05T17:46:48.081289: step 14112, loss 0.553778.
Train: 2018-08-05T17:46:51.688962: step 14113, loss 0.605117.
Train: 2018-08-05T17:46:55.294647: step 14114, loss 0.622168.
Train: 2018-08-05T17:46:58.890285: step 14115, loss 0.55381.
Train: 2018-08-05T17:47:02.504994: step 14116, loss 0.553827.
Train: 2018-08-05T17:47:06.141759: step 14117, loss 0.570848.
Train: 2018-08-05T17:47:09.785542: step 14118, loss 0.57933.
Train: 2018-08-05T17:47:13.405258: step 14119, loss 0.57083.
Train: 2018-08-05T17:47:17.008931: step 14120, loss 0.613146.
Test: 2018-08-05T17:47:32.283755: step 14120, loss 0.547677.
Train: 2018-08-05T17:47:35.896439: step 14121, loss 0.65526.
Train: 2018-08-05T17:47:39.539204: step 14122, loss 0.570792.
Train: 2018-08-05T17:47:43.132854: step 14123, loss 0.579161.
Train: 2018-08-05T17:47:46.740009: step 14124, loss 0.478896.
Train: 2018-08-05T17:47:50.370744: step 14125, loss 0.67085.
Train: 2018-08-05T17:47:53.981927: step 14126, loss 0.53751.
Train: 2018-08-05T17:47:57.595610: step 14127, loss 0.504417.
Train: 2018-08-05T17:48:01.199755: step 14128, loss 0.562472.
Train: 2018-08-05T17:48:04.822963: step 14129, loss 0.678401.
Train: 2018-08-05T17:48:08.453194: step 14130, loss 0.554242.
Test: 2018-08-05T17:48:23.700432: step 14130, loss 0.547051.
Train: 2018-08-05T17:48:27.332177: step 14131, loss 0.521323.
Train: 2018-08-05T17:48:30.947879: step 14132, loss 0.521372.
Train: 2018-08-05T17:48:34.547541: step 14133, loss 0.570758.
Train: 2018-08-05T17:48:38.141186: step 14134, loss 0.546059.
Train: 2018-08-05T17:48:41.739342: step 14135, loss 0.529568.
Train: 2018-08-05T17:48:45.375592: step 14136, loss 0.496515.
Train: 2018-08-05T17:48:49.014355: step 14137, loss 0.496312.
Train: 2018-08-05T17:48:52.605978: step 14138, loss 0.545846.
Train: 2018-08-05T17:48:56.191099: step 14139, loss 0.545753.
Train: 2018-08-05T17:48:59.802284: step 14140, loss 0.579146.
Test: 2018-08-05T17:49:15.070051: step 14140, loss 0.548747.
Train: 2018-08-05T17:49:18.698758: step 14141, loss 0.553988.
Train: 2018-08-05T17:49:22.348543: step 14142, loss 0.520241.
Train: 2018-08-05T17:49:25.974774: step 14143, loss 0.528525.
Train: 2018-08-05T17:49:29.593980: step 14144, loss 0.604813.
Train: 2018-08-05T17:49:33.179584: step 14145, loss 0.579377.
Train: 2018-08-05T17:49:36.778714: step 14146, loss 0.562338.
Train: 2018-08-05T17:49:40.410966: step 14147, loss 0.570891.
Train: 2018-08-05T17:49:44.045698: step 14148, loss 0.545198.
Train: 2018-08-05T17:49:47.704514: step 14149, loss 0.553751.
Train: 2018-08-05T17:49:51.298692: step 14150, loss 0.596733.
Test: 2018-08-05T17:50:06.714901: step 14150, loss 0.547763.
Train: 2018-08-05T17:50:10.340626: step 14151, loss 0.596764.
Train: 2018-08-05T17:50:13.930783: step 14152, loss 0.657025.
Train: 2018-08-05T17:50:17.545465: step 14153, loss 0.527961.
Train: 2018-08-05T17:50:21.176213: step 14154, loss 0.579506.
Train: 2018-08-05T17:50:24.806949: step 14155, loss 0.562335.
Train: 2018-08-05T17:50:28.424655: step 14156, loss 0.588034.
Train: 2018-08-05T17:50:32.030323: step 14157, loss 0.553783.
Train: 2018-08-05T17:50:35.641002: step 14158, loss 0.511077.
Train: 2018-08-05T17:50:39.243170: step 14159, loss 0.579423.
Train: 2018-08-05T17:50:42.863413: step 14160, loss 0.511102.
Test: 2018-08-05T17:50:58.093522: step 14160, loss 0.548264.
Train: 2018-08-05T17:51:01.803965: step 14161, loss 0.519614.
Train: 2018-08-05T17:51:05.409611: step 14162, loss 0.570892.
Train: 2018-08-05T17:51:09.018269: step 14163, loss 0.579466.
Train: 2018-08-05T17:51:12.618937: step 14164, loss 0.579476.
Train: 2018-08-05T17:51:16.242648: step 14165, loss 0.54519.
Train: 2018-08-05T17:51:19.865369: step 14166, loss 0.553758.
Train: 2018-08-05T17:51:23.465030: step 14167, loss 0.63957.
Train: 2018-08-05T17:51:27.083749: step 14168, loss 0.536615.
Train: 2018-08-05T17:51:30.685412: step 14169, loss 0.570905.
Train: 2018-08-05T17:51:34.274539: step 14170, loss 0.579466.
Test: 2018-08-05T17:51:49.534788: step 14170, loss 0.547649.
Train: 2018-08-05T17:51:53.160518: step 14171, loss 0.510985.
Train: 2018-08-05T17:51:56.796270: step 14172, loss 0.562336.
Train: 2018-08-05T17:52:00.676720: step 14173, loss 0.545211.
Train: 2018-08-05T17:52:04.702126: step 14174, loss 0.493805.
Train: 2018-08-05T17:52:08.719950: step 14175, loss 0.562335.
Train: 2018-08-05T17:52:12.781928: step 14176, loss 0.631092.
Train: 2018-08-05T17:52:16.847437: step 14177, loss 0.527956.
Train: 2018-08-05T17:52:20.975586: step 14178, loss 0.579537.
Train: 2018-08-05T17:52:25.283253: step 14179, loss 0.54513.
Train: 2018-08-05T17:52:29.420415: step 14180, loss 0.700048.
Test: 2018-08-05T17:52:45.970311: step 14180, loss 0.547596.
Train: 2018-08-05T17:52:50.092891: step 14181, loss 0.579512.
Train: 2018-08-05T17:52:54.181450: step 14182, loss 0.53663.
Train: 2018-08-05T17:52:58.276023: step 14183, loss 0.656437.
Train: 2018-08-05T17:53:02.354582: step 14184, loss 0.536759.
Train: 2018-08-05T17:53:06.478088: step 14185, loss 0.57085.
Train: 2018-08-05T17:53:10.549595: step 14186, loss 0.528406.
Train: 2018-08-05T17:53:14.623521: step 14187, loss 0.570827.
Train: 2018-08-05T17:53:18.266784: step 14188, loss 0.511588.
Train: 2018-08-05T17:53:21.909039: step 14189, loss 0.570818.
Train: 2018-08-05T17:53:25.562868: step 14190, loss 0.579272.
Test: 2018-08-05T17:53:41.002186: step 14190, loss 0.548831.
Train: 2018-08-05T17:53:44.733688: step 14191, loss 0.545459.
Train: 2018-08-05T17:53:48.379991: step 14192, loss 0.545463.
Train: 2018-08-05T17:53:52.019255: step 14193, loss 0.537007.
Train: 2018-08-05T17:53:53.806559: step 14194, loss 0.544316.
Train: 2018-08-05T17:53:57.463876: step 14195, loss 0.587756.
Train: 2018-08-05T17:54:01.075575: step 14196, loss 0.579296.
Train: 2018-08-05T17:54:04.710294: step 14197, loss 0.536937.
Train: 2018-08-05T17:54:08.375120: step 14198, loss 0.655607.
Train: 2018-08-05T17:54:12.022905: step 14199, loss 0.553888.
Train: 2018-08-05T17:54:15.648630: step 14200, loss 0.553898.
Test: 2018-08-05T17:54:30.985584: step 14200, loss 0.54785.
Train: 2018-08-05T17:54:36.836737: step 14201, loss 0.536994.
Train: 2018-08-05T17:54:40.433395: step 14202, loss 0.545449.
Train: 2018-08-05T17:54:44.105247: step 14203, loss 0.613111.
Train: 2018-08-05T17:54:47.742492: step 14204, loss 0.553907.
Train: 2018-08-05T17:54:51.330086: step 14205, loss 0.638419.
Train: 2018-08-05T17:54:54.958295: step 14206, loss 0.53706.
Train: 2018-08-05T17:54:58.572997: step 14207, loss 0.646646.
Train: 2018-08-05T17:55:02.186176: step 14208, loss 0.553977.
Train: 2018-08-05T17:55:05.843983: step 14209, loss 0.579171.
Train: 2018-08-05T17:55:09.481774: step 14210, loss 0.554033.
Test: 2018-08-05T17:55:24.782137: step 14210, loss 0.548264.
Train: 2018-08-05T17:55:28.445458: step 14211, loss 0.545699.
Train: 2018-08-05T17:55:32.050124: step 14212, loss 0.537374.
Train: 2018-08-05T17:55:35.658810: step 14213, loss 0.570768.
Train: 2018-08-05T17:55:39.256464: step 14214, loss 0.587452.
Train: 2018-08-05T17:55:42.902247: step 14215, loss 0.529084.
Train: 2018-08-05T17:55:46.532977: step 14216, loss 0.529079.
Train: 2018-08-05T17:55:50.119593: step 14217, loss 0.570767.
Train: 2018-08-05T17:55:53.739301: step 14218, loss 0.570769.
Train: 2018-08-05T17:55:57.332926: step 14219, loss 0.537349.
Train: 2018-08-05T17:56:00.964145: step 14220, loss 0.51222.
Test: 2018-08-05T17:56:16.200801: step 14220, loss 0.547433.
Train: 2018-08-05T17:56:19.839057: step 14221, loss 0.512103.
Train: 2018-08-05T17:56:23.474816: step 14222, loss 0.520346.
Train: 2018-08-05T17:56:27.051440: step 14223, loss 0.55393.
Train: 2018-08-05T17:56:30.662127: step 14224, loss 0.503092.
Train: 2018-08-05T17:56:34.272829: step 14225, loss 0.596354.
Train: 2018-08-05T17:56:37.892568: step 14226, loss 0.596457.
Train: 2018-08-05T17:56:41.499243: step 14227, loss 0.536692.
Train: 2018-08-05T17:56:45.125958: step 14228, loss 0.51092.
Train: 2018-08-05T17:56:48.742658: step 14229, loss 0.47637.
Train: 2018-08-05T17:56:52.349334: step 14230, loss 0.54507.
Test: 2018-08-05T17:57:07.594076: step 14230, loss 0.548663.
Train: 2018-08-05T17:57:11.189751: step 14231, loss 0.475631.
Train: 2018-08-05T17:57:14.798432: step 14232, loss 0.52749.
Train: 2018-08-05T17:57:18.455252: step 14233, loss 0.641269.
Train: 2018-08-05T17:57:22.069940: step 14234, loss 0.553609.
Train: 2018-08-05T17:57:25.658557: step 14235, loss 0.56242.
Train: 2018-08-05T17:57:29.278786: step 14236, loss 0.527072.
Train: 2018-08-05T17:57:32.874953: step 14237, loss 0.53586.
Train: 2018-08-05T17:57:36.475597: step 14238, loss 0.491358.
Train: 2018-08-05T17:57:40.077252: step 14239, loss 0.598198.
Train: 2018-08-05T17:57:43.706001: step 14240, loss 0.544646.
Test: 2018-08-05T17:57:58.967735: step 14240, loss 0.547379.
Train: 2018-08-05T17:58:02.613503: step 14241, loss 0.553592.
Train: 2018-08-05T17:58:06.577167: step 14242, loss 0.553595.
Train: 2018-08-05T17:58:10.257043: step 14243, loss 0.499617.
Train: 2018-08-05T17:58:13.957994: step 14244, loss 0.553605.
Train: 2018-08-05T17:58:17.652922: step 14245, loss 0.625916.
Train: 2018-08-05T17:58:21.283659: step 14246, loss 0.580743.
Train: 2018-08-05T17:58:24.946487: step 14247, loss 0.625957.
Train: 2018-08-05T17:58:28.547157: step 14248, loss 0.56264.
Train: 2018-08-05T17:58:32.155831: step 14249, loss 0.571639.
Train: 2018-08-05T17:58:35.759478: step 14250, loss 0.580606.
Test: 2018-08-05T17:58:51.106966: step 14250, loss 0.548196.
Train: 2018-08-05T17:58:54.750227: step 14251, loss 0.562579.
Train: 2018-08-05T17:58:58.399007: step 14252, loss 0.562557.
Train: 2018-08-05T17:59:02.028757: step 14253, loss 0.616197.
Train: 2018-08-05T17:59:05.622379: step 14254, loss 0.553585.
Train: 2018-08-05T17:59:09.222050: step 14255, loss 0.606699.
Train: 2018-08-05T17:59:12.818205: step 14256, loss 0.544765.
Train: 2018-08-05T17:59:16.446449: step 14257, loss 0.501218.
Train: 2018-08-05T17:59:20.100240: step 14258, loss 0.554227.
Train: 2018-08-05T17:59:23.732485: step 14259, loss 0.509568.
Train: 2018-08-05T17:59:27.316092: step 14260, loss 0.641643.
Test: 2018-08-05T17:59:42.590961: step 14260, loss 0.547936.
Train: 2018-08-05T17:59:46.209636: step 14261, loss 0.51843.
Train: 2018-08-05T17:59:49.868443: step 14262, loss 0.500964.
Train: 2018-08-05T17:59:53.479149: step 14263, loss 0.606296.
Train: 2018-08-05T17:59:57.135938: step 14264, loss 0.57994.
Train: 2018-08-05T18:00:00.891060: step 14265, loss 0.52733.
Train: 2018-08-05T18:00:04.654196: step 14266, loss 0.588657.
Train: 2018-08-05T18:00:08.294943: step 14267, loss 0.597369.
Train: 2018-08-05T18:00:11.915664: step 14268, loss 0.53618.
Train: 2018-08-05T18:00:15.522330: step 14269, loss 0.632126.
Train: 2018-08-05T18:00:19.172111: step 14270, loss 0.544897.
Test: 2018-08-05T18:00:34.461482: step 14270, loss 0.547508.
Train: 2018-08-05T18:00:38.145846: step 14271, loss 0.614304.
Train: 2018-08-05T18:00:41.746498: step 14272, loss 0.570523.
Train: 2018-08-05T18:00:45.349153: step 14273, loss 0.48505.
Train: 2018-08-05T18:00:48.948301: step 14274, loss 0.522381.
Train: 2018-08-05T18:00:52.543433: step 14275, loss 0.605804.
Train: 2018-08-05T18:00:56.187203: step 14276, loss 0.579839.
Train: 2018-08-05T18:00:59.814917: step 14277, loss 0.5618.
Train: 2018-08-05T18:01:03.388493: step 14278, loss 0.647741.
Train: 2018-08-05T18:01:06.992650: step 14279, loss 0.545359.
Train: 2018-08-05T18:01:10.613862: step 14280, loss 0.614089.
Test: 2018-08-05T18:01:25.886165: step 14280, loss 0.547134.
Train: 2018-08-05T18:01:29.546483: step 14281, loss 0.494238.
Train: 2018-08-05T18:01:33.181224: step 14282, loss 0.570847.
Train: 2018-08-05T18:01:36.812959: step 14283, loss 0.596365.
Train: 2018-08-05T18:01:40.424661: step 14284, loss 0.545321.
Train: 2018-08-05T18:01:44.024294: step 14285, loss 0.536849.
Train: 2018-08-05T18:01:47.639992: step 14286, loss 0.604844.
Train: 2018-08-05T18:01:51.230620: step 14287, loss 0.604795.
Train: 2018-08-05T18:01:54.879407: step 14288, loss 0.562354.
Train: 2018-08-05T18:01:58.514155: step 14289, loss 0.486236.
Train: 2018-08-05T18:02:02.113792: step 14290, loss 0.545443.
Test: 2018-08-05T18:02:17.371615: step 14290, loss 0.548617.
Train: 2018-08-05T18:02:20.978249: step 14291, loss 0.562358.
Train: 2018-08-05T18:02:24.736279: step 14292, loss 0.570821.
Train: 2018-08-05T18:02:28.344970: step 14293, loss 0.528495.
Train: 2018-08-05T18:02:31.986758: step 14294, loss 0.562354.
Train: 2018-08-05T18:02:35.605477: step 14295, loss 0.58779.
Train: 2018-08-05T18:02:39.191599: step 14296, loss 0.562351.
Train: 2018-08-05T18:02:42.788733: step 14297, loss 0.528413.
Train: 2018-08-05T18:02:46.407451: step 14298, loss 0.596316.
Train: 2018-08-05T18:02:50.026150: step 14299, loss 0.587829.
Train: 2018-08-05T18:02:53.660878: step 14300, loss 0.545365.
Test: 2018-08-05T18:03:08.880076: step 14300, loss 0.547383.
Train: 2018-08-05T18:03:14.675097: step 14301, loss 0.502903.
Train: 2018-08-05T18:03:18.292288: step 14302, loss 0.613362.
Train: 2018-08-05T18:03:21.892955: step 14303, loss 0.536832.
Train: 2018-08-05T18:03:25.491612: step 14304, loss 0.494265.
Train: 2018-08-05T18:03:29.077736: step 14305, loss 0.56234.
Train: 2018-08-05T18:03:32.692927: step 14306, loss 0.511092.
Train: 2018-08-05T18:03:36.310610: step 14307, loss 0.562336.
Train: 2018-08-05T18:03:39.969402: step 14308, loss 0.54517.
Train: 2018-08-05T18:03:43.612148: step 14309, loss 0.519324.
Train: 2018-08-05T18:03:47.201751: step 14310, loss 0.501947.
Test: 2018-08-05T18:04:02.427922: step 14310, loss 0.548082.
Train: 2018-08-05T18:04:06.064682: step 14311, loss 0.536366.
Train: 2018-08-05T18:04:09.690412: step 14312, loss 0.605811.
Train: 2018-08-05T18:04:13.352722: step 14313, loss 0.614639.
Train: 2018-08-05T18:04:16.993485: step 14314, loss 0.527473.
Train: 2018-08-05T18:04:20.602658: step 14315, loss 0.614793.
Train: 2018-08-05T18:04:24.199830: step 14316, loss 0.606074.
Train: 2018-08-05T18:04:27.832578: step 14317, loss 0.588576.
Train: 2018-08-05T18:04:31.427235: step 14318, loss 0.55364.
Train: 2018-08-05T18:04:35.077000: step 14319, loss 0.57108.
Train: 2018-08-05T18:04:38.683176: step 14320, loss 0.518819.
Test: 2018-08-05T18:04:53.949408: step 14320, loss 0.546015.
Train: 2018-08-05T18:04:57.568626: step 14321, loss 0.588475.
Train: 2018-08-05T18:05:01.175840: step 14322, loss 0.597151.
Train: 2018-08-05T18:05:04.762448: step 14323, loss 0.614469.
Train: 2018-08-05T18:05:08.358113: step 14324, loss 0.545014.
Train: 2018-08-05T18:05:11.986835: step 14325, loss 0.545042.
Train: 2018-08-05T18:05:15.623587: step 14326, loss 0.588252.
Train: 2018-08-05T18:05:19.225251: step 14327, loss 0.519228.
Train: 2018-08-05T18:05:22.827918: step 14328, loss 0.502035.
Train: 2018-08-05T18:05:26.456674: step 14329, loss 0.545102.
Train: 2018-08-05T18:05:30.052330: step 14330, loss 0.596827.
Test: 2018-08-05T18:05:45.468951: step 14330, loss 0.547741.
Train: 2018-08-05T18:05:49.180403: step 14331, loss 0.579579.
Train: 2018-08-05T18:05:52.863269: step 14332, loss 0.639886.
Train: 2018-08-05T18:05:56.607794: step 14333, loss 0.536537.
Train: 2018-08-05T18:06:00.263605: step 14334, loss 0.536571.
Train: 2018-08-05T18:06:03.904872: step 14335, loss 0.553753.
Train: 2018-08-05T18:06:07.558178: step 14336, loss 0.562335.
Train: 2018-08-05T18:06:11.290692: step 14337, loss 0.570908.
Train: 2018-08-05T18:06:14.999136: step 14338, loss 0.562335.
Train: 2018-08-05T18:06:18.603794: step 14339, loss 0.562335.
Train: 2018-08-05T18:06:22.181388: step 14340, loss 0.562336.
Test: 2018-08-05T18:06:37.445683: step 14340, loss 0.549034.
Train: 2018-08-05T18:06:41.083952: step 14341, loss 0.553782.
Train: 2018-08-05T18:06:44.705675: step 14342, loss 0.596544.
Train: 2018-08-05T18:06:48.354456: step 14343, loss 0.562337.
Train: 2018-08-05T18:06:51.982166: step 14344, loss 0.587949.
Train: 2018-08-05T18:06:53.737920: step 14345, loss 0.707853.
Train: 2018-08-05T18:06:57.345588: step 14346, loss 0.630288.
Train: 2018-08-05T18:07:00.951253: step 14347, loss 0.604618.
Train: 2018-08-05T18:07:04.554921: step 14348, loss 0.621235.
Train: 2018-08-05T18:07:08.184653: step 14349, loss 0.57913.
Train: 2018-08-05T18:07:11.810390: step 14350, loss 0.520882.
Test: 2018-08-05T18:07:27.068711: step 14350, loss 0.549221.
Train: 2018-08-05T18:07:30.653807: step 14351, loss 0.479679.
Train: 2018-08-05T18:07:34.285544: step 14352, loss 0.537696.
Train: 2018-08-05T18:07:37.880699: step 14353, loss 0.562499.
Train: 2018-08-05T18:07:41.500904: step 14354, loss 0.554253.
Train: 2018-08-05T18:07:45.116094: step 14355, loss 0.644997.
Train: 2018-08-05T18:07:48.727287: step 14356, loss 0.562524.
Train: 2018-08-05T18:07:52.326932: step 14357, loss 0.488543.
Train: 2018-08-05T18:07:55.992741: step 14358, loss 0.562534.
Train: 2018-08-05T18:07:59.601420: step 14359, loss 0.472013.
Train: 2018-08-05T18:08:03.208083: step 14360, loss 0.587255.
Test: 2018-08-05T18:08:18.501984: step 14360, loss 0.548499.
Train: 2018-08-05T18:08:22.137240: step 14361, loss 0.587286.
Train: 2018-08-05T18:08:25.765474: step 14362, loss 0.504552.
Train: 2018-08-05T18:08:29.402230: step 14363, loss 0.587351.
Train: 2018-08-05T18:08:33.010389: step 14364, loss 0.487642.
Train: 2018-08-05T18:08:36.601498: step 14365, loss 0.545748.
Train: 2018-08-05T18:08:40.219704: step 14366, loss 0.604246.
Train: 2018-08-05T18:08:43.827900: step 14367, loss 0.587556.
Train: 2018-08-05T18:08:47.470676: step 14368, loss 0.553985.
Train: 2018-08-05T18:08:51.290626: step 14369, loss 0.638119.
Train: 2018-08-05T18:08:54.904304: step 14370, loss 0.612877.
Test: 2018-08-05T18:09:10.178105: step 14370, loss 0.548724.
Train: 2018-08-05T18:09:13.805847: step 14371, loss 0.553972.
Train: 2018-08-05T18:09:17.414030: step 14372, loss 0.570789.
Train: 2018-08-05T18:09:21.033735: step 14373, loss 0.612789.
Train: 2018-08-05T18:09:24.652472: step 14374, loss 0.595947.
Train: 2018-08-05T18:09:28.280189: step 14375, loss 0.48705.
Train: 2018-08-05T18:09:31.873848: step 14376, loss 0.629377.
Train: 2018-08-05T18:09:35.535143: step 14377, loss 0.503889.
Train: 2018-08-05T18:09:39.130783: step 14378, loss 0.595854.
Train: 2018-08-05T18:09:42.713878: step 14379, loss 0.562414.
Train: 2018-08-05T18:09:46.342597: step 14380, loss 0.554063.
Test: 2018-08-05T18:10:01.539173: step 14380, loss 0.548468.
Train: 2018-08-05T18:10:05.065131: step 14381, loss 0.53736.
Train: 2018-08-05T18:10:08.579055: step 14382, loss 0.570771.
Train: 2018-08-05T18:10:12.099487: step 14383, loss 0.537331.
Train: 2018-08-05T18:10:15.632960: step 14384, loss 0.554038.
Train: 2018-08-05T18:10:19.156402: step 14385, loss 0.612665.
Train: 2018-08-05T18:10:22.696874: step 14386, loss 0.595912.
Train: 2018-08-05T18:10:26.266936: step 14387, loss 0.470291.
Train: 2018-08-05T18:10:29.779338: step 14388, loss 0.587553.
Train: 2018-08-05T18:10:33.285732: step 14389, loss 0.637932.
Train: 2018-08-05T18:10:36.824707: step 14390, loss 0.554005.
Test: 2018-08-05T18:10:51.795672: step 14390, loss 0.548777.
Train: 2018-08-05T18:10:55.299065: step 14391, loss 0.579166.
Train: 2018-08-05T18:10:58.878167: step 14392, loss 0.587541.
Train: 2018-08-05T18:11:02.399602: step 14393, loss 0.554029.
Train: 2018-08-05T18:11:05.920032: step 14394, loss 0.528931.
Train: 2018-08-05T18:11:09.429448: step 14395, loss 0.545663.
Train: 2018-08-05T18:11:12.955398: step 14396, loss 0.604282.
Train: 2018-08-05T18:11:16.490360: step 14397, loss 0.562402.
Train: 2018-08-05T18:11:20.023318: step 14398, loss 0.528906.
Train: 2018-08-05T18:11:23.548264: step 14399, loss 0.562398.
Train: 2018-08-05T18:11:27.061686: step 14400, loss 0.604324.
Test: 2018-08-05T18:11:42.048211: step 14400, loss 0.54762.
Train: 2018-08-05T18:11:47.784561: step 14401, loss 0.562395.
Train: 2018-08-05T18:11:51.287959: step 14402, loss 0.537242.
Train: 2018-08-05T18:11:54.782335: step 14403, loss 0.537225.
Train: 2018-08-05T18:11:58.305789: step 14404, loss 0.5036.
Train: 2018-08-05T18:12:01.848786: step 14405, loss 0.562377.
Train: 2018-08-05T18:12:05.336139: step 14406, loss 0.579237.
Train: 2018-08-05T18:12:08.843544: step 14407, loss 0.553916.
Train: 2018-08-05T18:12:12.341916: step 14408, loss 0.545436.
Train: 2018-08-05T18:12:15.837281: step 14409, loss 0.596258.
Train: 2018-08-05T18:12:19.376766: step 14410, loss 0.630229.
Test: 2018-08-05T18:12:34.312680: step 14410, loss 0.546819.
Train: 2018-08-05T18:12:37.832108: step 14411, loss 0.477535.
Train: 2018-08-05T18:12:41.326464: step 14412, loss 0.502898.
Train: 2018-08-05T18:12:44.837873: step 14413, loss 0.553832.
Train: 2018-08-05T18:12:48.348285: step 14414, loss 0.56234.
Train: 2018-08-05T18:12:51.842659: step 14415, loss 0.630747.
Train: 2018-08-05T18:12:55.365081: step 14416, loss 0.579446.
Train: 2018-08-05T18:12:58.907559: step 14417, loss 0.519554.
Train: 2018-08-05T18:13:02.429006: step 14418, loss 0.5709.
Train: 2018-08-05T18:13:05.970490: step 14419, loss 0.579476.
Train: 2018-08-05T18:13:09.504972: step 14420, loss 0.519473.
Test: 2018-08-05T18:13:24.506560: step 14420, loss 0.549189.
Train: 2018-08-05T18:13:28.011955: step 14421, loss 0.519428.
Train: 2018-08-05T18:13:31.520861: step 14422, loss 0.579527.
Train: 2018-08-05T18:13:35.056332: step 14423, loss 0.52791.
Train: 2018-08-05T18:13:38.607853: step 14424, loss 0.476126.
Train: 2018-08-05T18:13:42.113250: step 14425, loss 0.501806.
Train: 2018-08-05T18:13:45.641202: step 14426, loss 0.56235.
Train: 2018-08-05T18:13:49.273949: step 14427, loss 0.571071.
Train: 2018-08-05T18:13:52.809426: step 14428, loss 0.579841.
Train: 2018-08-05T18:13:56.332862: step 14429, loss 0.527365.
Train: 2018-08-05T18:13:59.860846: step 14430, loss 0.518518.
Test: 2018-08-05T18:14:14.827793: step 14430, loss 0.548116.
Train: 2018-08-05T18:14:18.372794: step 14431, loss 0.606406.
Train: 2018-08-05T18:14:21.894234: step 14432, loss 0.491904.
Train: 2018-08-05T18:14:25.402633: step 14433, loss 0.553596.
Train: 2018-08-05T18:14:28.925070: step 14434, loss 0.580166.
Train: 2018-08-05T18:14:32.459533: step 14435, loss 0.580208.
Train: 2018-08-05T18:14:35.968927: step 14436, loss 0.544708.
Train: 2018-08-05T18:14:39.488842: step 14437, loss 0.598042.
Train: 2018-08-05T18:14:42.984705: step 14438, loss 0.615831.
Train: 2018-08-05T18:14:46.476060: step 14439, loss 0.624653.
Train: 2018-08-05T18:14:49.984460: step 14440, loss 0.544728.
Test: 2018-08-05T18:15:04.971937: step 14440, loss 0.546851.
Train: 2018-08-05T18:15:08.518935: step 14441, loss 0.49166.
Train: 2018-08-05T18:15:12.068438: step 14442, loss 0.562436.
Train: 2018-08-05T18:15:15.568299: step 14443, loss 0.588962.
Train: 2018-08-05T18:15:19.084723: step 14444, loss 0.61538.
Train: 2018-08-05T18:15:22.587603: step 14445, loss 0.491913.
Train: 2018-08-05T18:15:26.136604: step 14446, loss 0.536016.
Train: 2018-08-05T18:15:29.679099: step 14447, loss 0.580086.
Train: 2018-08-05T18:15:33.216577: step 14448, loss 0.571313.
Train: 2018-08-05T18:15:36.727487: step 14449, loss 0.588766.
Train: 2018-08-05T18:15:40.307088: step 14450, loss 0.588696.
Test: 2018-08-05T18:15:55.303649: step 14450, loss 0.547964.
Train: 2018-08-05T18:15:58.826089: step 14451, loss 0.562378.
Train: 2018-08-05T18:16:02.341508: step 14452, loss 0.606045.
Train: 2018-08-05T18:16:05.880005: step 14453, loss 0.510088.
Train: 2018-08-05T18:16:09.417472: step 14454, loss 0.51016.
Train: 2018-08-05T18:16:12.938906: step 14455, loss 0.562354.
Train: 2018-08-05T18:16:16.414241: step 14456, loss 0.466747.
Train: 2018-08-05T18:16:19.933673: step 14457, loss 0.631978.
Train: 2018-08-05T18:16:23.426024: step 14458, loss 0.61456.
Train: 2018-08-05T18:16:26.962494: step 14459, loss 0.597112.
Train: 2018-08-05T18:16:30.488936: step 14460, loss 0.536326.
Test: 2018-08-05T18:16:45.442384: step 14460, loss 0.54688.
Train: 2018-08-05T18:16:48.966318: step 14461, loss 0.536356.
Train: 2018-08-05T18:16:52.463719: step 14462, loss 0.484432.
Train: 2018-08-05T18:16:55.958572: step 14463, loss 0.536354.
Train: 2018-08-05T18:16:59.451919: step 14464, loss 0.510309.
Train: 2018-08-05T18:17:02.984861: step 14465, loss 0.527594.
Train: 2018-08-05T18:17:06.541398: step 14466, loss 0.501393.
Train: 2018-08-05T18:17:10.048785: step 14467, loss 0.579844.
Train: 2018-08-05T18:17:13.583759: step 14468, loss 0.571138.
Train: 2018-08-05T18:17:17.101677: step 14469, loss 0.685214.
Train: 2018-08-05T18:17:20.609078: step 14470, loss 0.536086.
Test: 2018-08-05T18:17:35.578530: step 14470, loss 0.546941.
Train: 2018-08-05T18:17:39.122535: step 14471, loss 0.527326.
Train: 2018-08-05T18:17:42.636953: step 14472, loss 0.579922.
Train: 2018-08-05T18:17:46.159388: step 14473, loss 0.614981.
Train: 2018-08-05T18:17:49.653737: step 14474, loss 0.571135.
Train: 2018-08-05T18:17:53.184708: step 14475, loss 0.614833.
Train: 2018-08-05T18:17:56.672543: step 14476, loss 0.553642.
Train: 2018-08-05T18:18:00.195999: step 14477, loss 0.59718.
Train: 2018-08-05T18:18:03.716432: step 14478, loss 0.631818.
Train: 2018-08-05T18:18:07.227841: step 14479, loss 0.579646.
Train: 2018-08-05T18:18:10.732239: step 14480, loss 0.579577.
Test: 2018-08-05T18:18:25.706688: step 14480, loss 0.548783.
Train: 2018-08-05T18:18:29.205576: step 14481, loss 0.536571.
Train: 2018-08-05T18:18:32.700953: step 14482, loss 0.536645.
Train: 2018-08-05T18:18:36.227395: step 14483, loss 0.587973.
Train: 2018-08-05T18:18:39.747336: step 14484, loss 0.553815.
Train: 2018-08-05T18:18:43.252222: step 14485, loss 0.570852.
Train: 2018-08-05T18:18:46.773156: step 14486, loss 0.477435.
Train: 2018-08-05T18:18:50.275060: step 14487, loss 0.511405.
Train: 2018-08-05T18:18:53.779462: step 14488, loss 0.511355.
Train: 2018-08-05T18:18:57.312932: step 14489, loss 0.587885.
Train: 2018-08-05T18:19:00.843383: step 14490, loss 0.604962.
Test: 2018-08-05T18:19:15.819901: step 14490, loss 0.548689.
Train: 2018-08-05T18:19:19.429063: step 14491, loss 0.587919.
Train: 2018-08-05T18:19:22.950503: step 14492, loss 0.570864.
Train: 2018-08-05T18:19:26.465919: step 14493, loss 0.562341.
Train: 2018-08-05T18:19:29.966776: step 14494, loss 0.604931.
Train: 2018-08-05T18:19:33.484696: step 14495, loss 0.562344.
Train: 2018-08-05T18:19:35.230383: step 14496, loss 0.562347.
Train: 2018-08-05T18:19:38.755822: step 14497, loss 0.562348.
Train: 2018-08-05T18:19:42.279760: step 14498, loss 0.553864.
Train: 2018-08-05T18:19:45.793658: step 14499, loss 0.511462.
Train: 2018-08-05T18:19:49.279005: step 14500, loss 0.528404.
Test: 2018-08-05T18:20:04.393872: step 14500, loss 0.546983.
Train: 2018-08-05T18:20:10.086598: step 14501, loss 0.55385.
Train: 2018-08-05T18:20:13.613056: step 14502, loss 0.570851.
Train: 2018-08-05T18:20:17.145512: step 14503, loss 0.613436.
Train: 2018-08-05T18:20:20.640882: step 14504, loss 0.562342.
Train: 2018-08-05T18:20:24.126710: step 14505, loss 0.596399.
Train: 2018-08-05T18:20:27.654646: step 14506, loss 0.545327.
Train: 2018-08-05T18:20:31.155030: step 14507, loss 0.587861.
Train: 2018-08-05T18:20:34.696516: step 14508, loss 0.50285.
Train: 2018-08-05T18:20:38.214453: step 14509, loss 0.536834.
Train: 2018-08-05T18:20:41.698790: step 14510, loss 0.579367.
Test: 2018-08-05T18:20:56.659696: step 14510, loss 0.548707.
Train: 2018-08-05T18:21:00.179636: step 14511, loss 0.536792.
Train: 2018-08-05T18:21:03.695050: step 14512, loss 0.477088.
Train: 2018-08-05T18:21:07.206457: step 14513, loss 0.587978.
Train: 2018-08-05T18:21:10.743943: step 14514, loss 0.493836.
Train: 2018-08-05T18:21:14.273401: step 14515, loss 0.588097.
Train: 2018-08-05T18:21:17.775801: step 14516, loss 0.502098.
Train: 2018-08-05T18:21:21.296252: step 14517, loss 0.614126.
Train: 2018-08-05T18:21:24.842765: step 14518, loss 0.553695.
Train: 2018-08-05T18:21:28.331108: step 14519, loss 0.605641.
Train: 2018-08-05T18:21:31.859566: step 14520, loss 0.57101.
Test: 2018-08-05T18:21:46.781945: step 14520, loss 0.548071.
Train: 2018-08-05T18:21:50.333967: step 14521, loss 0.545012.
Train: 2018-08-05T18:21:53.837359: step 14522, loss 0.640389.
Train: 2018-08-05T18:21:57.330726: step 14523, loss 0.553682.
Train: 2018-08-05T18:22:00.842144: step 14524, loss 0.579652.
Train: 2018-08-05T18:22:04.358073: step 14525, loss 0.648788.
Train: 2018-08-05T18:22:07.874002: step 14526, loss 0.519231.
Train: 2018-08-05T18:22:11.399448: step 14527, loss 0.536516.
Train: 2018-08-05T18:22:14.911858: step 14528, loss 0.57953.
Train: 2018-08-05T18:22:18.427288: step 14529, loss 0.579507.
Train: 2018-08-05T18:22:21.933670: step 14530, loss 0.536616.
Test: 2018-08-05T18:22:36.852015: step 14530, loss 0.548622.
Train: 2018-08-05T18:22:40.374445: step 14531, loss 0.596596.
Train: 2018-08-05T18:22:43.881322: step 14532, loss 0.553784.
Train: 2018-08-05T18:22:47.423822: step 14533, loss 0.639217.
Train: 2018-08-05T18:22:50.908150: step 14534, loss 0.519739.
Train: 2018-08-05T18:22:54.399488: step 14535, loss 0.553836.
Train: 2018-08-05T18:22:57.915924: step 14536, loss 0.604836.
Train: 2018-08-05T18:23:01.422829: step 14537, loss 0.570833.
Train: 2018-08-05T18:23:04.940755: step 14538, loss 0.511552.
Train: 2018-08-05T18:23:08.456184: step 14539, loss 0.596206.
Train: 2018-08-05T18:23:11.956564: step 14540, loss 0.587719.
Test: 2018-08-05T18:23:26.874377: step 14540, loss 0.548077.
Train: 2018-08-05T18:23:30.382282: step 14541, loss 0.621445.
Train: 2018-08-05T18:23:33.925284: step 14542, loss 0.61289.
Train: 2018-08-05T18:23:37.403080: step 14543, loss 0.495253.
Train: 2018-08-05T18:23:40.915495: step 14544, loss 0.579159.
Train: 2018-08-05T18:23:44.443950: step 14545, loss 0.503838.
Train: 2018-08-05T18:23:47.944324: step 14546, loss 0.612604.
Train: 2018-08-05T18:23:51.442681: step 14547, loss 0.629273.
Train: 2018-08-05T18:23:54.939023: step 14548, loss 0.620801.
Train: 2018-08-05T18:23:58.423348: step 14549, loss 0.587388.
Train: 2018-08-05T18:24:01.920726: step 14550, loss 0.570757.
Test: 2018-08-05T18:24:16.832039: step 14550, loss 0.549072.
Train: 2018-08-05T18:24:20.433192: step 14551, loss 0.603811.
Train: 2018-08-05T18:24:23.952615: step 14552, loss 0.562521.
Train: 2018-08-05T18:24:27.469028: step 14553, loss 0.62004.
Train: 2018-08-05T18:24:30.967389: step 14554, loss 0.497109.
Train: 2018-08-05T18:24:34.471288: step 14555, loss 0.505405.
Train: 2018-08-05T18:24:37.965161: step 14556, loss 0.529921.
Train: 2018-08-05T18:24:41.492113: step 14557, loss 0.562591.
Train: 2018-08-05T18:24:45.044631: step 14558, loss 0.538031.
Train: 2018-08-05T18:24:48.533965: step 14559, loss 0.513389.
Train: 2018-08-05T18:24:52.042367: step 14560, loss 0.587194.
Test: 2018-08-05T18:25:06.971731: step 14560, loss 0.548773.
Train: 2018-08-05T18:25:10.457579: step 14561, loss 0.55429.
Train: 2018-08-05T18:25:13.970000: step 14562, loss 0.529501.
Train: 2018-08-05T18:25:17.509491: step 14563, loss 0.570757.
Train: 2018-08-05T18:25:21.065019: step 14564, loss 0.52929.
Train: 2018-08-05T18:25:24.569418: step 14565, loss 0.479259.
Train: 2018-08-05T18:25:28.114934: step 14566, loss 0.579126.
Train: 2018-08-05T18:25:31.633335: step 14567, loss 0.637878.
Train: 2018-08-05T18:25:35.187364: step 14568, loss 0.553981.
Train: 2018-08-05T18:25:38.733872: step 14569, loss 0.444494.
Train: 2018-08-05T18:25:42.258313: step 14570, loss 0.646912.
Test: 2018-08-05T18:25:57.227257: step 14570, loss 0.549178.
Train: 2018-08-05T18:26:00.741674: step 14571, loss 0.494568.
Train: 2018-08-05T18:26:04.254573: step 14572, loss 0.604849.
Train: 2018-08-05T18:26:07.771012: step 14573, loss 0.596416.
Train: 2018-08-05T18:26:11.274387: step 14574, loss 0.562339.
Train: 2018-08-05T18:26:14.801833: step 14575, loss 0.528182.
Train: 2018-08-05T18:26:18.311230: step 14576, loss 0.553783.
Train: 2018-08-05T18:26:21.842198: step 14577, loss 0.639442.
Train: 2018-08-05T18:26:25.346108: step 14578, loss 0.536633.
Train: 2018-08-05T18:26:28.825426: step 14579, loss 0.579478.
Train: 2018-08-05T18:26:32.314782: step 14580, loss 0.519473.
Test: 2018-08-05T18:26:47.256669: step 14580, loss 0.547413.
Train: 2018-08-05T18:26:50.820223: step 14581, loss 0.510856.
Train: 2018-08-05T18:26:54.353702: step 14582, loss 0.579525.
Train: 2018-08-05T18:26:57.872630: step 14583, loss 0.53652.
Train: 2018-08-05T18:27:01.374508: step 14584, loss 0.467533.
Train: 2018-08-05T18:27:04.880864: step 14585, loss 0.657441.
Train: 2018-08-05T18:27:08.368718: step 14586, loss 0.57965.
Train: 2018-08-05T18:27:11.906684: step 14587, loss 0.571002.
Train: 2018-08-05T18:27:15.416095: step 14588, loss 0.493063.
Train: 2018-08-05T18:27:18.913470: step 14589, loss 0.60571.
Train: 2018-08-05T18:27:22.406329: step 14590, loss 0.588376.
Test: 2018-08-05T18:27:37.365263: step 14590, loss 0.547461.
Train: 2018-08-05T18:27:40.881694: step 14591, loss 0.492952.
Train: 2018-08-05T18:27:44.448257: step 14592, loss 0.579718.
Train: 2018-08-05T18:27:47.986743: step 14593, loss 0.571041.
Train: 2018-08-05T18:27:51.508177: step 14594, loss 0.52759.
Train: 2018-08-05T18:27:55.026608: step 14595, loss 0.684125.
Train: 2018-08-05T18:27:58.544544: step 14596, loss 0.501563.
Train: 2018-08-05T18:28:02.047437: step 14597, loss 0.657836.
Train: 2018-08-05T18:28:05.561850: step 14598, loss 0.596991.
Train: 2018-08-05T18:28:09.137427: step 14599, loss 0.56234.
Train: 2018-08-05T18:28:12.657362: step 14600, loss 0.51925.
Test: 2018-08-05T18:28:27.624801: step 14600, loss 0.547965.
Train: 2018-08-05T18:28:33.420805: step 14601, loss 0.527916.
Train: 2018-08-05T18:28:36.915165: step 14602, loss 0.545137.
Train: 2018-08-05T18:28:40.403536: step 14603, loss 0.579527.
Train: 2018-08-05T18:28:43.901405: step 14604, loss 0.519383.
Train: 2018-08-05T18:28:47.412330: step 14605, loss 0.579519.
Train: 2018-08-05T18:28:51.006956: step 14606, loss 0.553744.
Train: 2018-08-05T18:28:54.490781: step 14607, loss 0.545154.
Train: 2018-08-05T18:28:57.986636: step 14608, loss 0.510779.
Train: 2018-08-05T18:29:01.466962: step 14609, loss 0.596747.
Train: 2018-08-05T18:29:04.972355: step 14610, loss 0.527911.
Test: 2018-08-05T18:29:19.908736: step 14610, loss 0.54676.
Train: 2018-08-05T18:29:23.442711: step 14611, loss 0.570951.
Train: 2018-08-05T18:29:26.975174: step 14612, loss 0.579578.
Train: 2018-08-05T18:29:30.459499: step 14613, loss 0.57096.
Train: 2018-08-05T18:29:33.938835: step 14614, loss 0.639939.
Train: 2018-08-05T18:29:37.447242: step 14615, loss 0.519288.
Train: 2018-08-05T18:29:40.965666: step 14616, loss 0.57094.
Train: 2018-08-05T18:29:44.504162: step 14617, loss 0.510743.
Train: 2018-08-05T18:29:48.012574: step 14618, loss 0.510728.
Train: 2018-08-05T18:29:51.502933: step 14619, loss 0.519278.
Train: 2018-08-05T18:29:55.066475: step 14620, loss 0.484689.
Test: 2018-08-05T18:30:10.074024: step 14620, loss 0.546893.
Train: 2018-08-05T18:30:13.582433: step 14621, loss 0.553688.
Train: 2018-08-05T18:30:17.080794: step 14622, loss 0.553669.
Train: 2018-08-05T18:30:20.580162: step 14623, loss 0.544943.
Train: 2018-08-05T18:30:24.108619: step 14624, loss 0.536177.
Train: 2018-08-05T18:30:27.648091: step 14625, loss 0.544742.
Train: 2018-08-05T18:30:31.186061: step 14626, loss 0.677195.
Train: 2018-08-05T18:30:34.687932: step 14627, loss 0.53568.
Train: 2018-08-05T18:30:38.191323: step 14628, loss 0.588777.
Train: 2018-08-05T18:30:41.693711: step 14629, loss 0.562914.
Train: 2018-08-05T18:30:45.212640: step 14630, loss 0.606553.
Test: 2018-08-05T18:31:00.123439: step 14630, loss 0.547337.
Train: 2018-08-05T18:31:03.627342: step 14631, loss 0.544867.
Train: 2018-08-05T18:31:07.125204: step 14632, loss 0.527353.
Train: 2018-08-05T18:31:10.643126: step 14633, loss 0.518607.
Train: 2018-08-05T18:31:14.153522: step 14634, loss 0.544865.
Train: 2018-08-05T18:31:17.622820: step 14635, loss 0.527328.
Train: 2018-08-05T18:31:21.164806: step 14636, loss 0.562391.
Train: 2018-08-05T18:31:24.687744: step 14637, loss 0.579959.
Train: 2018-08-05T18:31:28.173601: step 14638, loss 0.492119.
Train: 2018-08-05T18:31:31.690023: step 14639, loss 0.527215.
Train: 2018-08-05T18:31:35.207445: step 14640, loss 0.491907.
Test: 2018-08-05T18:31:50.158851: step 14640, loss 0.548081.
Train: 2018-08-05T18:31:53.707349: step 14641, loss 0.606624.
Train: 2018-08-05T18:31:57.363158: step 14642, loss 0.48278.
Train: 2018-08-05T18:32:00.872060: step 14643, loss 0.642339.
Train: 2018-08-05T18:32:04.352895: step 14644, loss 0.62463.
Train: 2018-08-05T18:32:07.862305: step 14645, loss 0.668786.
Train: 2018-08-05T18:32:11.365716: step 14646, loss 0.588812.
Train: 2018-08-05T18:32:13.075319: step 14647, loss 0.655283.
Train: 2018-08-05T18:32:16.605272: step 14648, loss 0.605971.
Train: 2018-08-05T18:32:20.136744: step 14649, loss 0.579545.
Train: 2018-08-05T18:32:23.658696: step 14650, loss 0.503048.
Test: 2018-08-05T18:32:38.578029: step 14650, loss 0.548254.
Train: 2018-08-05T18:32:42.083400: step 14651, loss 0.548118.
Train: 2018-08-05T18:32:45.619872: step 14652, loss 0.604655.
Train: 2018-08-05T18:32:49.143311: step 14653, loss 0.537388.
Train: 2018-08-05T18:32:52.673739: step 14654, loss 0.611946.
Train: 2018-08-05T18:32:56.193670: step 14655, loss 0.596531.
Train: 2018-08-05T18:32:59.704557: step 14656, loss 0.596026.
Train: 2018-08-05T18:33:03.198917: step 14657, loss 0.522339.
Train: 2018-08-05T18:33:06.698284: step 14658, loss 0.604293.
Train: 2018-08-05T18:33:10.194161: step 14659, loss 0.49624.
Train: 2018-08-05T18:33:13.708075: step 14660, loss 0.553768.
Test: 2018-08-05T18:33:28.671538: step 14660, loss 0.547978.
Train: 2018-08-05T18:33:32.183455: step 14661, loss 0.579394.
Train: 2018-08-05T18:33:35.678828: step 14662, loss 0.545502.
Train: 2018-08-05T18:33:39.169184: step 14663, loss 0.545523.
Train: 2018-08-05T18:33:42.687118: step 14664, loss 0.672495.
Train: 2018-08-05T18:33:46.186494: step 14665, loss 0.562246.
Train: 2018-08-05T18:33:49.745040: step 14666, loss 0.562387.
Train: 2018-08-05T18:33:53.286548: step 14667, loss 0.520207.
Train: 2018-08-05T18:33:56.811992: step 14668, loss 0.545433.
Train: 2018-08-05T18:34:00.335437: step 14669, loss 0.60467.
Train: 2018-08-05T18:34:03.843839: step 14670, loss 0.562355.
Test: 2018-08-05T18:34:18.780713: step 14670, loss 0.546481.
Train: 2018-08-05T18:34:22.327714: step 14671, loss 0.46932.
Train: 2018-08-05T18:34:25.858665: step 14672, loss 0.630107.
Train: 2018-08-05T18:34:29.408170: step 14673, loss 0.587758.
Train: 2018-08-05T18:34:32.915573: step 14674, loss 0.562357.
Train: 2018-08-05T18:34:36.442530: step 14675, loss 0.4524.
Train: 2018-08-05T18:34:39.953430: step 14676, loss 0.511517.
Train: 2018-08-05T18:34:43.446790: step 14677, loss 0.621807.
Train: 2018-08-05T18:34:46.979255: step 14678, loss 0.596361.
Train: 2018-08-05T18:34:50.518743: step 14679, loss 0.502789.
Train: 2018-08-05T18:34:54.031153: step 14680, loss 0.55382.
Test: 2018-08-05T18:35:08.956537: step 14680, loss 0.548087.
Train: 2018-08-05T18:35:12.451896: step 14681, loss 0.545271.
Train: 2018-08-05T18:35:15.946760: step 14682, loss 0.605079.
Train: 2018-08-05T18:35:19.456168: step 14683, loss 0.613663.
Train: 2018-08-05T18:35:22.966070: step 14684, loss 0.605095.
Train: 2018-08-05T18:35:26.512066: step 14685, loss 0.613592.
Train: 2018-08-05T18:35:30.031003: step 14686, loss 0.545289.
Train: 2018-08-05T18:35:33.524871: step 14687, loss 0.511262.
Train: 2018-08-05T18:35:37.042307: step 14688, loss 0.604896.
Train: 2018-08-05T18:35:40.538184: step 14689, loss 0.596351.
Train: 2018-08-05T18:35:44.052607: step 14690, loss 0.655715.
Test: 2018-08-05T18:35:58.989984: step 14690, loss 0.547644.
Train: 2018-08-05T18:36:02.527965: step 14691, loss 0.562358.
Train: 2018-08-05T18:36:06.065459: step 14692, loss 0.663607.
Train: 2018-08-05T18:36:09.569349: step 14693, loss 0.528792.
Train: 2018-08-05T18:36:13.064190: step 14694, loss 0.562404.
Train: 2018-08-05T18:36:16.573588: step 14695, loss 0.529034.
Train: 2018-08-05T18:36:20.065947: step 14696, loss 0.554102.
Train: 2018-08-05T18:36:23.608444: step 14697, loss 0.5874.
Train: 2018-08-05T18:36:27.140920: step 14698, loss 0.504316.
Train: 2018-08-05T18:36:30.624242: step 14699, loss 0.579063.
Train: 2018-08-05T18:36:34.156724: step 14700, loss 0.61226.
Test: 2018-08-05T18:36:49.095071: step 14700, loss 0.549195.
Train: 2018-08-05T18:36:54.744676: step 14701, loss 0.645367.
Train: 2018-08-05T18:36:58.217485: step 14702, loss 0.529416.
Train: 2018-08-05T18:37:01.743411: step 14703, loss 0.595521.
Train: 2018-08-05T18:37:05.266855: step 14704, loss 0.513084.
Train: 2018-08-05T18:37:08.780269: step 14705, loss 0.578991.
Train: 2018-08-05T18:37:12.268119: step 14706, loss 0.529617.
Train: 2018-08-05T18:37:15.754460: step 14707, loss 0.546069.
Train: 2018-08-05T18:37:19.255337: step 14708, loss 0.513111.
Train: 2018-08-05T18:37:22.821898: step 14709, loss 0.471761.
Train: 2018-08-05T18:37:26.331295: step 14710, loss 0.54592.
Test: 2018-08-05T18:37:41.274239: step 14710, loss 0.548192.
Train: 2018-08-05T18:37:44.762584: step 14711, loss 0.579069.
Train: 2018-08-05T18:37:48.280024: step 14712, loss 0.595768.
Train: 2018-08-05T18:37:51.787945: step 14713, loss 0.537359.
Train: 2018-08-05T18:37:55.312906: step 14714, loss 0.604273.
Train: 2018-08-05T18:37:58.838365: step 14715, loss 0.570781.
Train: 2018-08-05T18:38:02.340748: step 14716, loss 0.55399.
Train: 2018-08-05T18:38:05.823059: step 14717, loss 0.570791.
Train: 2018-08-05T18:38:09.334468: step 14718, loss 0.562376.
Train: 2018-08-05T18:38:12.828811: step 14719, loss 0.570801.
Train: 2018-08-05T18:38:16.321666: step 14720, loss 0.570805.
Test: 2018-08-05T18:38:31.261576: step 14720, loss 0.548654.
Train: 2018-08-05T18:38:34.798561: step 14721, loss 0.503266.
Train: 2018-08-05T18:38:38.321997: step 14722, loss 0.562359.
Train: 2018-08-05T18:38:41.829382: step 14723, loss 0.562354.
Train: 2018-08-05T18:38:45.312715: step 14724, loss 0.57932.
Train: 2018-08-05T18:38:48.814124: step 14725, loss 0.570841.
Train: 2018-08-05T18:38:52.311487: step 14726, loss 0.604852.
Train: 2018-08-05T18:38:55.838959: step 14727, loss 0.562345.
Train: 2018-08-05T18:38:59.353378: step 14728, loss 0.494339.
Train: 2018-08-05T18:39:02.868801: step 14729, loss 0.553831.
Train: 2018-08-05T18:39:06.349121: step 14730, loss 0.502679.
Test: 2018-08-05T18:39:21.309549: step 14730, loss 0.547086.
Train: 2018-08-05T18:39:24.840487: step 14731, loss 0.613596.
Train: 2018-08-05T18:39:28.349880: step 14732, loss 0.476809.
Train: 2018-08-05T18:39:31.872330: step 14733, loss 0.639511.
Train: 2018-08-05T18:39:35.392760: step 14734, loss 0.519421.
Train: 2018-08-05T18:39:38.872072: step 14735, loss 0.570931.
Train: 2018-08-05T18:39:42.365417: step 14736, loss 0.579549.
Train: 2018-08-05T18:39:45.870812: step 14737, loss 0.527884.
Train: 2018-08-05T18:39:49.360676: step 14738, loss 0.527841.
Train: 2018-08-05T18:39:52.872568: step 14739, loss 0.622814.
Train: 2018-08-05T18:39:56.442129: step 14740, loss 0.493199.
Test: 2018-08-05T18:40:11.394041: step 14740, loss 0.548684.
Train: 2018-08-05T18:40:14.894929: step 14741, loss 0.674875.
Train: 2018-08-05T18:40:18.433427: step 14742, loss 0.579642.
Train: 2018-08-05T18:40:21.937818: step 14743, loss 0.484566.
Train: 2018-08-05T18:40:25.464244: step 14744, loss 0.588278.
Train: 2018-08-05T18:40:28.957581: step 14745, loss 0.545051.
Train: 2018-08-05T18:40:32.467493: step 14746, loss 0.614219.
Train: 2018-08-05T18:40:36.001957: step 14747, loss 0.570979.
Train: 2018-08-05T18:40:39.517394: step 14748, loss 0.640013.
Train: 2018-08-05T18:40:43.022270: step 14749, loss 0.545116.
Train: 2018-08-05T18:40:46.518663: step 14750, loss 0.536552.
Test: 2018-08-05T18:41:01.441554: step 14750, loss 0.547801.
Train: 2018-08-05T18:41:04.935929: step 14751, loss 0.536583.
Train: 2018-08-05T18:41:08.468910: step 14752, loss 0.596649.
Train: 2018-08-05T18:41:12.002878: step 14753, loss 0.519495.
Train: 2018-08-05T18:41:15.500233: step 14754, loss 0.622293.
Train: 2018-08-05T18:41:19.009633: step 14755, loss 0.562336.
Train: 2018-08-05T18:41:22.526058: step 14756, loss 0.511085.
Train: 2018-08-05T18:41:26.250022: step 14757, loss 0.62212.
Train: 2018-08-05T18:41:29.993046: step 14758, loss 0.519694.
Train: 2018-08-05T18:41:33.712493: step 14759, loss 0.553814.
Train: 2018-08-05T18:41:37.434461: step 14760, loss 0.664635.
Test: 2018-08-05T18:41:53.016996: step 14760, loss 0.548332.
Train: 2018-08-05T18:41:56.568518: step 14761, loss 0.604878.
Train: 2018-08-05T18:42:00.070902: step 14762, loss 0.528413.
Train: 2018-08-05T18:42:03.591335: step 14763, loss 0.536946.
Train: 2018-08-05T18:42:07.109758: step 14764, loss 0.553897.
Train: 2018-08-05T18:42:10.648231: step 14765, loss 0.545451.
Train: 2018-08-05T18:42:14.156623: step 14766, loss 0.604622.
Train: 2018-08-05T18:42:17.640466: step 14767, loss 0.570808.
Train: 2018-08-05T18:42:21.173438: step 14768, loss 0.579238.
Train: 2018-08-05T18:42:24.689856: step 14769, loss 0.545525.
Train: 2018-08-05T18:42:28.232355: step 14770, loss 0.54554.
Test: 2018-08-05T18:42:43.175190: step 14770, loss 0.548322.
Train: 2018-08-05T18:42:46.706633: step 14771, loss 0.570794.
Train: 2018-08-05T18:42:50.222049: step 14772, loss 0.537139.
Train: 2018-08-05T18:42:53.729432: step 14773, loss 0.579209.
Train: 2018-08-05T18:42:57.234826: step 14774, loss 0.587624.
Train: 2018-08-05T18:43:00.753272: step 14775, loss 0.537146.
Train: 2018-08-05T18:43:04.279707: step 14776, loss 0.52873.
Train: 2018-08-05T18:43:07.864817: step 14777, loss 0.545537.
Train: 2018-08-05T18:43:11.373733: step 14778, loss 0.562371.
Train: 2018-08-05T18:43:14.882136: step 14779, loss 0.562367.
Train: 2018-08-05T18:43:18.371505: step 14780, loss 0.553916.
Test: 2018-08-05T18:43:33.352533: step 14780, loss 0.548626.
Train: 2018-08-05T18:43:36.861939: step 14781, loss 0.520077.
Train: 2018-08-05T18:43:40.391903: step 14782, loss 0.545409.
Train: 2018-08-05T18:43:43.927866: step 14783, loss 0.5199.
Train: 2018-08-05T18:43:47.430244: step 14784, loss 0.55383.
Train: 2018-08-05T18:43:51.011843: step 14785, loss 0.630612.
Train: 2018-08-05T18:43:54.532267: step 14786, loss 0.579423.
Train: 2018-08-05T18:43:58.033636: step 14787, loss 0.459762.
Train: 2018-08-05T18:44:01.544047: step 14788, loss 0.52806.
Train: 2018-08-05T18:44:05.088537: step 14789, loss 0.476404.
Train: 2018-08-05T18:44:08.594952: step 14790, loss 0.570966.
Test: 2018-08-05T18:44:23.573866: step 14790, loss 0.547286.
Train: 2018-08-05T18:44:27.102326: step 14791, loss 0.510393.
Train: 2018-08-05T18:44:30.641816: step 14792, loss 0.588433.
Train: 2018-08-05T18:44:34.148701: step 14793, loss 0.518765.
Train: 2018-08-05T18:44:37.674623: step 14794, loss 0.501131.
Train: 2018-08-05T18:44:41.201582: step 14795, loss 0.562397.
Train: 2018-08-05T18:44:44.701443: step 14796, loss 0.580051.
Train: 2018-08-05T18:44:48.190784: step 14797, loss 0.535915.
Train: 2018-08-05T18:44:49.892872: step 14798, loss 0.505721.
Train: 2018-08-05T18:44:53.399744: step 14799, loss 0.509117.
Train: 2018-08-05T18:44:56.914167: step 14800, loss 0.535733.
Test: 2018-08-05T18:45:11.855591: step 14800, loss 0.548619.
Train: 2018-08-05T18:45:17.615495: step 14801, loss 0.526712.
Train: 2018-08-05T18:45:21.162010: step 14802, loss 0.607556.
Train: 2018-08-05T18:45:24.665380: step 14803, loss 0.571631.
Train: 2018-08-05T18:45:28.188825: step 14804, loss 0.580696.
Train: 2018-08-05T18:45:31.692205: step 14805, loss 0.61688.
Train: 2018-08-05T18:45:35.190575: step 14806, loss 0.553611.
Train: 2018-08-05T18:45:38.698469: step 14807, loss 0.55361.
Train: 2018-08-05T18:45:42.231434: step 14808, loss 0.589729.
Train: 2018-08-05T18:45:45.721783: step 14809, loss 0.535564.
Train: 2018-08-05T18:45:49.201122: step 14810, loss 0.571634.
Test: 2018-08-05T18:46:04.125416: step 14810, loss 0.547567.
Train: 2018-08-05T18:46:07.644339: step 14811, loss 0.544595.
Train: 2018-08-05T18:46:11.163770: step 14812, loss 0.64359.
Train: 2018-08-05T18:46:14.703755: step 14813, loss 0.607464.
Train: 2018-08-05T18:46:18.238226: step 14814, loss 0.562541.
Train: 2018-08-05T18:46:21.737598: step 14815, loss 0.616059.
Train: 2018-08-05T18:46:25.230951: step 14816, loss 0.535807.
Train: 2018-08-05T18:46:28.706267: step 14817, loss 0.553592.
Train: 2018-08-05T18:46:32.244755: step 14818, loss 0.500568.
Train: 2018-08-05T18:46:35.776216: step 14819, loss 0.624192.
Train: 2018-08-05T18:46:39.287623: step 14820, loss 0.650413.
Test: 2018-08-05T18:46:54.203447: step 14820, loss 0.548151.
Train: 2018-08-05T18:46:57.728910: step 14821, loss 0.536092.
Train: 2018-08-05T18:47:01.252858: step 14822, loss 0.571106.
Train: 2018-08-05T18:47:04.755729: step 14823, loss 0.571065.
Train: 2018-08-05T18:47:08.260115: step 14824, loss 0.597065.
Train: 2018-08-05T18:47:11.795603: step 14825, loss 0.553693.
Train: 2018-08-05T18:47:15.304998: step 14826, loss 0.579579.
Train: 2018-08-05T18:47:18.807384: step 14827, loss 0.596709.
Train: 2018-08-05T18:47:22.289701: step 14828, loss 0.60515.
Train: 2018-08-05T18:47:25.789072: step 14829, loss 0.528226.
Train: 2018-08-05T18:47:29.289466: step 14830, loss 0.596356.
Test: 2018-08-05T18:47:44.188783: step 14830, loss 0.54859.
Train: 2018-08-05T18:47:47.734780: step 14831, loss 0.570828.
Train: 2018-08-05T18:47:51.256707: step 14832, loss 0.587707.
Train: 2018-08-05T18:47:54.775634: step 14833, loss 0.503433.
Train: 2018-08-05T18:47:58.265988: step 14834, loss 0.570789.
Train: 2018-08-05T18:48:01.757841: step 14835, loss 0.503651.
Train: 2018-08-05T18:48:05.251687: step 14836, loss 0.612727.
Train: 2018-08-05T18:48:08.777129: step 14837, loss 0.461848.
Train: 2018-08-05T18:48:12.295562: step 14838, loss 0.486905.
Train: 2018-08-05T18:48:15.819996: step 14839, loss 0.579198.
Train: 2018-08-05T18:48:19.316353: step 14840, loss 0.5371.
Test: 2018-08-05T18:48:34.258755: step 14840, loss 0.548069.
Train: 2018-08-05T18:48:37.765147: step 14841, loss 0.52859.
Train: 2018-08-05T18:48:41.397871: step 14842, loss 0.621625.
Train: 2018-08-05T18:48:44.928307: step 14843, loss 0.51148.
Train: 2018-08-05T18:48:48.451256: step 14844, loss 0.536853.
Train: 2018-08-05T18:48:51.959646: step 14845, loss 0.57086.
Train: 2018-08-05T18:48:55.465061: step 14846, loss 0.528191.
Train: 2018-08-05T18:48:58.970468: step 14847, loss 0.553777.
Train: 2018-08-05T18:49:02.475874: step 14848, loss 0.553756.
Train: 2018-08-05T18:49:05.991306: step 14849, loss 0.648327.
Train: 2018-08-05T18:49:09.514741: step 14850, loss 0.648366.
Test: 2018-08-05T18:49:24.445589: step 14850, loss 0.548776.
Train: 2018-08-05T18:49:27.983100: step 14851, loss 0.562335.
Train: 2018-08-05T18:49:31.501546: step 14852, loss 0.605249.
Train: 2018-08-05T18:49:34.996428: step 14853, loss 0.639438.
Train: 2018-08-05T18:49:38.498313: step 14854, loss 0.562338.
Train: 2018-08-05T18:49:42.024776: step 14855, loss 0.553825.
Train: 2018-08-05T18:49:45.557242: step 14856, loss 0.528362.
Train: 2018-08-05T18:49:49.041593: step 14857, loss 0.587801.
Train: 2018-08-05T18:49:52.562022: step 14858, loss 0.545418.
Train: 2018-08-05T18:49:56.074443: step 14859, loss 0.511612.
Train: 2018-08-05T18:49:59.647009: step 14860, loss 0.613101.
Test: 2018-08-05T18:50:14.601916: step 14860, loss 0.547865.
Train: 2018-08-05T18:50:18.125856: step 14861, loss 0.545467.
Train: 2018-08-05T18:50:21.670846: step 14862, loss 0.604582.
Train: 2018-08-05T18:50:25.214840: step 14863, loss 0.545504.
Train: 2018-08-05T18:50:28.720728: step 14864, loss 0.54552.
Train: 2018-08-05T18:50:32.218095: step 14865, loss 0.55395.
Train: 2018-08-05T18:50:35.725491: step 14866, loss 0.587644.
Train: 2018-08-05T18:50:39.232906: step 14867, loss 0.604471.
Train: 2018-08-05T18:50:42.783414: step 14868, loss 0.5792.
Train: 2018-08-05T18:50:46.319898: step 14869, loss 0.595979.
Train: 2018-08-05T18:50:49.808268: step 14870, loss 0.570779.
Test: 2018-08-05T18:51:04.740130: step 14870, loss 0.54843.
Train: 2018-08-05T18:51:08.246011: step 14871, loss 0.671196.
Train: 2018-08-05T18:51:11.760924: step 14872, loss 0.495719.
Train: 2018-08-05T18:51:15.256307: step 14873, loss 0.512499.
Train: 2018-08-05T18:51:18.804826: step 14874, loss 0.620675.
Train: 2018-08-05T18:51:22.326267: step 14875, loss 0.537534.
Train: 2018-08-05T18:51:25.824650: step 14876, loss 0.562459.
Train: 2018-08-05T18:51:29.340077: step 14877, loss 0.520986.
Train: 2018-08-05T18:51:32.854492: step 14878, loss 0.579058.
Train: 2018-08-05T18:51:36.355874: step 14879, loss 0.537556.
Train: 2018-08-05T18:51:39.868778: step 14880, loss 0.587374.
Test: 2018-08-05T18:51:54.819625: step 14880, loss 0.549719.
Train: 2018-08-05T18:51:58.367644: step 14881, loss 0.55414.
Train: 2018-08-05T18:52:01.874029: step 14882, loss 0.562446.
Train: 2018-08-05T18:52:05.382950: step 14883, loss 0.5458.
Train: 2018-08-05T18:52:08.877801: step 14884, loss 0.537448.
Train: 2018-08-05T18:52:12.377163: step 14885, loss 0.570767.
Train: 2018-08-05T18:52:15.937716: step 14886, loss 0.537359.
Train: 2018-08-05T18:52:19.456648: step 14887, loss 0.629345.
Train: 2018-08-05T18:52:22.966549: step 14888, loss 0.587515.
Train: 2018-08-05T18:52:26.452892: step 14889, loss 0.595883.
Train: 2018-08-05T18:52:29.974316: step 14890, loss 0.470405.
Test: 2018-08-05T18:52:44.952808: step 14890, loss 0.548222.
Train: 2018-08-05T18:52:48.476726: step 14891, loss 0.520527.
Train: 2018-08-05T18:52:52.079382: step 14892, loss 0.545606.
Train: 2018-08-05T18:52:55.616855: step 14893, loss 0.537146.
Train: 2018-08-05T18:52:59.116224: step 14894, loss 0.528636.
Train: 2018-08-05T18:53:02.628642: step 14895, loss 0.570818.
Train: 2018-08-05T18:53:06.113980: step 14896, loss 0.494503.
Train: 2018-08-05T18:53:09.612358: step 14897, loss 0.545318.
Train: 2018-08-05T18:53:13.160880: step 14898, loss 0.605052.
Train: 2018-08-05T18:53:16.682341: step 14899, loss 0.510952.
Train: 2018-08-05T18:53:20.177711: step 14900, loss 0.553738.
Test: 2018-08-05T18:53:35.145624: step 14900, loss 0.546753.
Train: 2018-08-05T18:53:40.961191: step 14901, loss 0.588201.
Train: 2018-08-05T18:53:44.455556: step 14902, loss 0.570981.
Train: 2018-08-05T18:53:47.958929: step 14903, loss 0.63155.
Train: 2018-08-05T18:53:51.493393: step 14904, loss 0.510437.
Train: 2018-08-05T18:53:55.024867: step 14905, loss 0.536364.
Train: 2018-08-05T18:53:58.524736: step 14906, loss 0.562347.
Train: 2018-08-05T18:54:02.032633: step 14907, loss 0.501573.
Train: 2018-08-05T18:54:05.533498: step 14908, loss 0.475349.
Train: 2018-08-05T18:54:09.030351: step 14909, loss 0.448876.
Train: 2018-08-05T18:54:12.542767: step 14910, loss 0.492201.
Test: 2018-08-05T18:54:27.503248: step 14910, loss 0.547891.
Train: 2018-08-05T18:54:31.037218: step 14911, loss 0.544777.
Train: 2018-08-05T18:54:34.523577: step 14912, loss 0.615671.
Train: 2018-08-05T18:54:38.011947: step 14913, loss 0.544688.
Train: 2018-08-05T18:54:41.522357: step 14914, loss 0.607167.
Train: 2018-08-05T18:54:45.018226: step 14915, loss 0.625174.
Train: 2018-08-05T18:54:48.529118: step 14916, loss 0.553591.
Train: 2018-08-05T18:54:52.174875: step 14917, loss 0.687947.
Train: 2018-08-05T18:54:55.671260: step 14918, loss 0.571469.
Train: 2018-08-05T18:54:59.175129: step 14919, loss 0.544667.
Train: 2018-08-05T18:55:02.672983: step 14920, loss 0.482342.
Test: 2018-08-05T18:55:17.627406: step 14920, loss 0.546803.
Train: 2018-08-05T18:55:21.126275: step 14921, loss 0.509075.
Train: 2018-08-05T18:55:24.659252: step 14922, loss 0.553588.
Train: 2018-08-05T18:55:28.221299: step 14923, loss 0.553588.
Train: 2018-08-05T18:55:31.759789: step 14924, loss 0.607062.
Train: 2018-08-05T18:55:35.284728: step 14925, loss 0.517963.
Train: 2018-08-05T18:55:38.811673: step 14926, loss 0.615932.
Train: 2018-08-05T18:55:42.324597: step 14927, loss 0.562485.
Train: 2018-08-05T18:55:45.825484: step 14928, loss 0.580247.
Train: 2018-08-05T18:55:49.369471: step 14929, loss 0.526973.
Train: 2018-08-05T18:55:52.880372: step 14930, loss 0.544727.
Test: 2018-08-05T18:56:07.839358: step 14930, loss 0.548064.
Train: 2018-08-05T18:56:11.352249: step 14931, loss 0.544734.
Train: 2018-08-05T18:56:14.852628: step 14932, loss 0.544738.
Train: 2018-08-05T18:56:18.348484: step 14933, loss 0.518184.
Train: 2018-08-05T18:56:21.846356: step 14934, loss 0.606729.
Train: 2018-08-05T18:56:25.389844: step 14935, loss 0.518189.
Train: 2018-08-05T18:56:28.912277: step 14936, loss 0.597854.
Train: 2018-08-05T18:56:32.414652: step 14937, loss 0.571287.
Train: 2018-08-05T18:56:35.903006: step 14938, loss 0.597788.
Train: 2018-08-05T18:56:39.403864: step 14939, loss 0.535951.
Train: 2018-08-05T18:56:42.910777: step 14940, loss 0.553602.
Test: 2018-08-05T18:56:57.843111: step 14940, loss 0.547907.
Train: 2018-08-05T18:57:01.404147: step 14941, loss 0.588831.
Train: 2018-08-05T18:57:04.963179: step 14942, loss 0.615165.
Train: 2018-08-05T18:57:08.466598: step 14943, loss 0.536072.
Train: 2018-08-05T18:57:11.975992: step 14944, loss 0.553624.
Train: 2018-08-05T18:57:15.489384: step 14945, loss 0.492427.
Train: 2018-08-05T18:57:18.993271: step 14946, loss 0.527408.
Train: 2018-08-05T18:57:22.533756: step 14947, loss 0.562374.
Train: 2018-08-05T18:57:26.056195: step 14948, loss 0.509898.
Train: 2018-08-05T18:57:27.778326: step 14949, loss 0.693155.
Train: 2018-08-05T18:57:31.266149: step 14950, loss 0.571114.
Test: 2018-08-05T18:57:46.220527: step 14950, loss 0.547391.
Train: 2018-08-05T18:57:49.727422: step 14951, loss 0.632176.
Train: 2018-08-05T18:57:53.227800: step 14952, loss 0.553655.
Train: 2018-08-05T18:57:56.775814: step 14953, loss 0.48423.
Train: 2018-08-05T18:58:00.295742: step 14954, loss 0.571019.
Train: 2018-08-05T18:58:03.790093: step 14955, loss 0.527691.
Train: 2018-08-05T18:58:07.351643: step 14956, loss 0.501721.
Train: 2018-08-05T18:58:10.842504: step 14957, loss 0.493012.
Train: 2018-08-05T18:58:14.358912: step 14958, loss 0.56235.
Train: 2018-08-05T18:58:17.878849: step 14959, loss 0.536265.
Train: 2018-08-05T18:58:21.404296: step 14960, loss 0.579786.
Test: 2018-08-05T18:58:36.338669: step 14960, loss 0.548599.
Train: 2018-08-05T18:58:39.859589: step 14961, loss 0.553642.
Train: 2018-08-05T18:58:43.366979: step 14962, loss 0.614773.
Train: 2018-08-05T18:58:46.869375: step 14963, loss 0.597307.
Train: 2018-08-05T18:58:50.467006: step 14964, loss 0.509998.
Train: 2018-08-05T18:58:54.021535: step 14965, loss 0.579828.
Train: 2018-08-05T18:58:57.551009: step 14966, loss 0.536182.
Train: 2018-08-05T18:59:01.067412: step 14967, loss 0.536178.
Train: 2018-08-05T18:59:04.573801: step 14968, loss 0.475021.
Train: 2018-08-05T18:59:08.100254: step 14969, loss 0.553627.
Train: 2018-08-05T18:59:11.601644: step 14970, loss 0.501011.
Test: 2018-08-05T18:59:26.556078: step 14970, loss 0.547112.
Train: 2018-08-05T18:59:30.109089: step 14971, loss 0.55361.
Train: 2018-08-05T18:59:33.646055: step 14972, loss 0.535979.
Train: 2018-08-05T18:59:37.144921: step 14973, loss 0.553597.
Train: 2018-08-05T18:59:40.650319: step 14974, loss 0.527032.
Train: 2018-08-05T18:59:44.147695: step 14975, loss 0.580217.
Train: 2018-08-05T18:59:47.653075: step 14976, loss 0.553589.
Train: 2018-08-05T18:59:51.174513: step 14977, loss 0.615925.
Train: 2018-08-05T18:59:54.705968: step 14978, loss 0.509048.
Train: 2018-08-05T18:59:58.216362: step 14979, loss 0.526836.
Train: 2018-08-05T19:00:01.760838: step 14980, loss 0.580377.
Test: 2018-08-05T19:00:16.800429: step 14980, loss 0.547601.
Train: 2018-08-05T19:00:20.308827: step 14981, loss 0.553589.
Train: 2018-08-05T19:00:23.836771: step 14982, loss 0.580414.
Train: 2018-08-05T19:00:27.382260: step 14983, loss 0.5983.
Train: 2018-08-05T19:00:30.889665: step 14984, loss 0.589331.
Train: 2018-08-05T19:00:34.423131: step 14985, loss 0.651749.
Train: 2018-08-05T19:00:37.927044: step 14986, loss 0.473509.
Train: 2018-08-05T19:00:41.428915: step 14987, loss 0.606909.
Train: 2018-08-05T19:00:44.937303: step 14988, loss 0.571327.
Train: 2018-08-05T19:00:48.435684: step 14989, loss 0.527047.
Train: 2018-08-05T19:00:51.972156: step 14990, loss 0.527089.
Test: 2018-08-05T19:01:06.936156: step 14990, loss 0.548293.
Train: 2018-08-05T19:01:10.446056: step 14991, loss 0.509458.
Train: 2018-08-05T19:01:13.970505: step 14992, loss 0.580084.
Train: 2018-08-05T19:01:17.467875: step 14993, loss 0.527126.
Train: 2018-08-05T19:01:20.972764: step 14994, loss 0.624202.
Train: 2018-08-05T19:01:24.471128: step 14995, loss 0.518341.
Train: 2018-08-05T19:01:27.991026: step 14996, loss 0.500732.
Train: 2018-08-05T19:01:31.516481: step 14997, loss 0.518332.
Train: 2018-08-05T19:01:35.011866: step 14998, loss 0.597738.
Train: 2018-08-05T19:01:38.503217: step 14999, loss 0.624243.
Train: 2018-08-05T19:01:41.989549: step 15000, loss 0.544778.
Test: 2018-08-05T19:01:56.973507: step 15000, loss 0.547085.
Train: 2018-08-05T19:02:02.623606: step 15001, loss 0.668214.
Train: 2018-08-05T19:02:06.142532: step 15002, loss 0.579989.
Train: 2018-08-05T19:02:09.645405: step 15003, loss 0.597464.
Train: 2018-08-05T19:02:13.145761: step 15004, loss 0.614816.
Train: 2018-08-05T19:02:16.662180: step 15005, loss 0.544947.
Train: 2018-08-05T19:02:20.165569: step 15006, loss 0.553672.
Train: 2018-08-05T19:02:23.657949: step 15007, loss 0.596946.
Train: 2018-08-05T19:02:27.213480: step 15008, loss 0.484739.
Train: 2018-08-05T19:02:30.731918: step 15009, loss 0.519296.
Train: 2018-08-05T19:02:34.227278: step 15010, loss 0.510724.
Test: 2018-08-05T19:02:49.171688: step 15010, loss 0.548164.
Train: 2018-08-05T19:02:52.663537: step 15011, loss 0.545127.
Train: 2018-08-05T19:02:56.163928: step 15012, loss 0.5279.
Train: 2018-08-05T19:02:59.670321: step 15013, loss 0.519245.
Train: 2018-08-05T19:03:03.208812: step 15014, loss 0.536439.
Train: 2018-08-05T19:03:06.749780: step 15015, loss 0.614245.
Train: 2018-08-05T19:03:10.257191: step 15016, loss 0.536372.
Train: 2018-08-05T19:03:13.762580: step 15017, loss 0.640348.
Train: 2018-08-05T19:03:17.302092: step 15018, loss 0.562345.
Train: 2018-08-05T19:03:20.802491: step 15019, loss 0.596978.
Train: 2018-08-05T19:03:24.330913: step 15020, loss 0.57099.
Test: 2018-08-05T19:03:39.234757: step 15020, loss 0.54851.
Train: 2018-08-05T19:03:42.747146: step 15021, loss 0.545064.
Train: 2018-08-05T19:03:46.253552: step 15022, loss 0.570969.
Train: 2018-08-05T19:03:49.751414: step 15023, loss 0.588204.
Train: 2018-08-05T19:03:53.256294: step 15024, loss 0.536505.
Train: 2018-08-05T19:03:56.782757: step 15025, loss 0.588145.
Train: 2018-08-05T19:04:00.295659: step 15026, loss 0.61389.
Train: 2018-08-05T19:04:03.811569: step 15027, loss 0.605207.
Train: 2018-08-05T19:04:07.309939: step 15028, loss 0.58799.
Train: 2018-08-05T19:04:10.849420: step 15029, loss 0.536762.
Train: 2018-08-05T19:04:14.336767: step 15030, loss 0.502791.
Test: 2018-08-05T19:04:29.272670: step 15030, loss 0.547757.
Train: 2018-08-05T19:04:32.820696: step 15031, loss 0.494341.
Train: 2018-08-05T19:04:36.332588: step 15032, loss 0.630388.
Train: 2018-08-05T19:04:39.865062: step 15033, loss 0.647337.
Train: 2018-08-05T19:04:43.366950: step 15034, loss 0.545392.
Train: 2018-08-05T19:04:46.870832: step 15035, loss 0.486167.
Train: 2018-08-05T19:04:50.376729: step 15036, loss 0.48617.
Train: 2018-08-05T19:04:53.883629: step 15037, loss 0.519964.
Train: 2018-08-05T19:04:57.431134: step 15038, loss 0.579338.
Train: 2018-08-05T19:05:00.946566: step 15039, loss 0.536814.
Train: 2018-08-05T19:05:04.452957: step 15040, loss 0.596445.
Test: 2018-08-05T19:05:19.370740: step 15040, loss 0.54769.
Train: 2018-08-05T19:05:22.889173: step 15041, loss 0.596482.
Train: 2018-08-05T19:05:26.457745: step 15042, loss 0.579416.
Train: 2018-08-05T19:05:29.990199: step 15043, loss 0.528182.
Train: 2018-08-05T19:05:33.495595: step 15044, loss 0.570882.
Train: 2018-08-05T19:05:37.026056: step 15045, loss 0.502499.
Train: 2018-08-05T19:05:40.535456: step 15046, loss 0.622263.
Train: 2018-08-05T19:05:44.029308: step 15047, loss 0.588024.
Train: 2018-08-05T19:05:47.523159: step 15048, loss 0.562336.
Train: 2018-08-05T19:05:51.012507: step 15049, loss 0.54522.
Train: 2018-08-05T19:05:54.548006: step 15050, loss 0.630802.
Test: 2018-08-05T19:06:09.471836: step 15050, loss 0.548062.
Train: 2018-08-05T19:06:13.012329: step 15051, loss 0.528146.
Train: 2018-08-05T19:06:16.540292: step 15052, loss 0.536707.
Train: 2018-08-05T19:06:20.039161: step 15053, loss 0.536705.
Train: 2018-08-05T19:06:23.546559: step 15054, loss 0.587982.
Train: 2018-08-05T19:06:27.048965: step 15055, loss 0.587981.
Train: 2018-08-05T19:06:30.555353: step 15056, loss 0.6136.
Train: 2018-08-05T19:06:34.066763: step 15057, loss 0.579402.
Train: 2018-08-05T19:06:37.607769: step 15058, loss 0.579378.
Train: 2018-08-05T19:06:41.105615: step 15059, loss 0.587854.
Train: 2018-08-05T19:06:44.615020: step 15060, loss 0.604778.
Test: 2018-08-05T19:06:59.580468: step 15060, loss 0.548223.
Train: 2018-08-05T19:07:03.119441: step 15061, loss 0.553894.
Train: 2018-08-05T19:07:06.622834: step 15062, loss 0.553919.
Train: 2018-08-05T19:07:10.179854: step 15063, loss 0.520221.
Train: 2018-08-05T19:07:13.711808: step 15064, loss 0.638191.
Train: 2018-08-05T19:07:17.226713: step 15065, loss 0.545569.
Train: 2018-08-05T19:07:20.721559: step 15066, loss 0.537206.
Train: 2018-08-05T19:07:24.221452: step 15067, loss 0.554005.
Train: 2018-08-05T19:07:27.850669: step 15068, loss 0.595934.
Train: 2018-08-05T19:07:31.394164: step 15069, loss 0.604283.
Train: 2018-08-05T19:07:34.915091: step 15070, loss 0.512232.
Test: 2018-08-05T19:07:49.901619: step 15070, loss 0.547103.
Train: 2018-08-05T19:07:53.413020: step 15071, loss 0.63765.
Train: 2018-08-05T19:07:56.924438: step 15072, loss 0.537386.
Train: 2018-08-05T19:08:00.418818: step 15073, loss 0.595777.
Train: 2018-08-05T19:08:03.918188: step 15074, loss 0.529136.
Train: 2018-08-05T19:08:07.446642: step 15075, loss 0.604046.
Train: 2018-08-05T19:08:10.985602: step 15076, loss 0.57076.
Train: 2018-08-05T19:08:14.490470: step 15077, loss 0.595665.
Train: 2018-08-05T19:08:17.992864: step 15078, loss 0.595625.
Train: 2018-08-05T19:08:21.509283: step 15079, loss 0.537663.
Train: 2018-08-05T19:08:25.017662: step 15080, loss 0.620341.
Test: 2018-08-05T19:08:39.954562: step 15080, loss 0.548735.
Train: 2018-08-05T19:08:43.481011: step 15081, loss 0.504777.
Train: 2018-08-05T19:08:47.033529: step 15082, loss 0.579.
Train: 2018-08-05T19:08:50.557982: step 15083, loss 0.562519.
Train: 2018-08-05T19:08:54.259401: step 15084, loss 0.562522.
Train: 2018-08-05T19:08:57.983864: step 15085, loss 0.554291.
Train: 2018-08-05T19:09:01.711838: step 15086, loss 0.570757.
Train: 2018-08-05T19:09:05.529045: step 15087, loss 0.504876.
Train: 2018-08-05T19:09:09.300153: step 15088, loss 0.554263.
Train: 2018-08-05T19:09:13.026650: step 15089, loss 0.545976.
Train: 2018-08-05T19:09:16.799252: step 15090, loss 0.554205.
Test: 2018-08-05T19:09:31.805775: step 15090, loss 0.547661.
Train: 2018-08-05T19:09:35.340244: step 15091, loss 0.537587.
Train: 2018-08-05T19:09:38.881729: step 15092, loss 0.529195.
Train: 2018-08-05T19:09:42.397144: step 15093, loss 0.545752.
Train: 2018-08-05T19:09:45.938129: step 15094, loss 0.604228.
Train: 2018-08-05T19:09:49.431992: step 15095, loss 0.545636.
Train: 2018-08-05T19:09:52.964456: step 15096, loss 0.621186.
Train: 2018-08-05T19:09:56.477855: step 15097, loss 0.528751.
Train: 2018-08-05T19:09:59.973217: step 15098, loss 0.528691.
Train: 2018-08-05T19:10:03.547775: step 15099, loss 0.562367.
Train: 2018-08-05T19:10:05.285446: step 15100, loss 0.652552.
Test: 2018-08-05T19:10:20.271953: step 15100, loss 0.547654.
