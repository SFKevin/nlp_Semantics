Train: 2018-08-01T22:51:59.645854: step 1, loss 1.31837.
Train: 2018-08-01T22:51:59.878233: step 2, loss 1.18434.
Train: 2018-08-01T22:52:00.076702: step 3, loss 1.08087.
Train: 2018-08-01T22:52:00.277141: step 4, loss 1.02789.
Train: 2018-08-01T22:52:00.482591: step 5, loss 1.0488.
Train: 2018-08-01T22:52:00.688066: step 6, loss 0.81688.
Train: 2018-08-01T22:52:00.887542: step 7, loss 0.75573.
Train: 2018-08-01T22:52:01.086976: step 8, loss 0.797118.
Train: 2018-08-01T22:52:01.299424: step 9, loss 0.692204.
Train: 2018-08-01T22:52:01.502863: step 10, loss 0.733917.
Test: 2018-08-01T22:52:02.316689: step 10, loss 0.679956.
Train: 2018-08-01T22:52:02.511168: step 11, loss 0.758238.
Train: 2018-08-01T22:52:02.700688: step 12, loss 0.630896.
Train: 2018-08-01T22:52:02.890183: step 13, loss 0.653673.
Train: 2018-08-01T22:52:03.090647: step 14, loss 0.653947.
Train: 2018-08-01T22:52:03.287119: step 15, loss 0.781667.
Train: 2018-08-01T22:52:03.479579: step 16, loss 0.596416.
Train: 2018-08-01T22:52:03.673087: step 17, loss 0.630859.
Train: 2018-08-01T22:52:03.859589: step 18, loss 0.637658.
Train: 2018-08-01T22:52:04.056064: step 19, loss 0.456393.
Train: 2018-08-01T22:52:04.254534: step 20, loss 0.684513.
Test: 2018-08-01T22:52:04.794093: step 20, loss 0.593232.
Train: 2018-08-01T22:52:04.982587: step 21, loss 0.66124.
Train: 2018-08-01T22:52:05.178065: step 22, loss 0.700135.
Train: 2018-08-01T22:52:05.367563: step 23, loss 0.610588.
Train: 2018-08-01T22:52:05.563038: step 24, loss 0.584599.
Train: 2018-08-01T22:52:05.757491: step 25, loss 0.620777.
Train: 2018-08-01T22:52:05.947982: step 26, loss 0.567775.
Train: 2018-08-01T22:52:06.136502: step 27, loss 0.608579.
Train: 2018-08-01T22:52:06.327965: step 28, loss 0.577886.
Train: 2018-08-01T22:52:06.522446: step 29, loss 0.664632.
Train: 2018-08-01T22:52:06.715927: step 30, loss 0.518539.
Test: 2018-08-01T22:52:07.249532: step 30, loss 0.573383.
Train: 2018-08-01T22:52:07.444978: step 31, loss 0.682482.
Train: 2018-08-01T22:52:07.645445: step 32, loss 0.579974.
Train: 2018-08-01T22:52:07.830972: step 33, loss 0.63264.
Train: 2018-08-01T22:52:08.021463: step 34, loss 2.85173.
Train: 2018-08-01T22:52:08.222927: step 35, loss 0.765367.
Train: 2018-08-01T22:52:08.417404: step 36, loss 0.61327.
Train: 2018-08-01T22:52:08.607898: step 37, loss 0.624537.
Train: 2018-08-01T22:52:08.798362: step 38, loss 0.518095.
Train: 2018-08-01T22:52:08.987904: step 39, loss 0.584565.
Train: 2018-08-01T22:52:09.187354: step 40, loss 0.628556.
Test: 2018-08-01T22:52:09.708955: step 40, loss 0.612129.
Train: 2018-08-01T22:52:09.898446: step 41, loss 0.644941.
Train: 2018-08-01T22:52:10.094923: step 42, loss 0.632844.
Train: 2018-08-01T22:52:10.287381: step 43, loss 0.6428.
Train: 2018-08-01T22:52:10.476873: step 44, loss 0.640958.
Train: 2018-08-01T22:52:10.667395: step 45, loss 0.680547.
Train: 2018-08-01T22:52:10.860848: step 46, loss 0.624562.
Train: 2018-08-01T22:52:11.052364: step 47, loss 0.665753.
Train: 2018-08-01T22:52:11.246840: step 48, loss 0.655842.
Train: 2018-08-01T22:52:11.442293: step 49, loss 0.746449.
Train: 2018-08-01T22:52:11.638792: step 50, loss 0.674775.
Test: 2018-08-01T22:52:12.177329: step 50, loss 0.641412.
Train: 2018-08-01T22:52:12.364852: step 51, loss 0.662433.
Train: 2018-08-01T22:52:12.558309: step 52, loss 0.590566.
Train: 2018-08-01T22:52:12.755782: step 53, loss 0.696111.
Train: 2018-08-01T22:52:12.951260: step 54, loss 0.613286.
Train: 2018-08-01T22:52:13.144767: step 55, loss 0.605359.
Train: 2018-08-01T22:52:13.335232: step 56, loss 0.721478.
Train: 2018-08-01T22:52:13.524727: step 57, loss 0.658348.
Train: 2018-08-01T22:52:13.728210: step 58, loss 0.609905.
Train: 2018-08-01T22:52:13.920699: step 59, loss 0.800892.
Train: 2018-08-01T22:52:14.122136: step 60, loss 0.720241.
Test: 2018-08-01T22:52:14.664680: step 60, loss 0.649517.
Train: 2018-08-01T22:52:14.859159: step 61, loss 0.629471.
Train: 2018-08-01T22:52:15.050647: step 62, loss 0.734789.
Train: 2018-08-01T22:52:15.245159: step 63, loss 0.751043.
Train: 2018-08-01T22:52:15.436621: step 64, loss 0.64244.
Train: 2018-08-01T22:52:15.626125: step 65, loss 0.602764.
Train: 2018-08-01T22:52:15.821617: step 66, loss 0.62529.
Train: 2018-08-01T22:52:16.019075: step 67, loss 0.753237.
Train: 2018-08-01T22:52:16.218530: step 68, loss 0.709051.
Train: 2018-08-01T22:52:16.411049: step 69, loss 0.741224.
Train: 2018-08-01T22:52:16.608488: step 70, loss 0.691886.
Test: 2018-08-01T22:52:17.152034: step 70, loss 0.651816.
Train: 2018-08-01T22:52:17.348511: step 71, loss 0.649734.
Train: 2018-08-01T22:52:17.543987: step 72, loss 0.640513.
Train: 2018-08-01T22:52:17.739465: step 73, loss 0.732063.
Train: 2018-08-01T22:52:17.940927: step 74, loss 0.731151.
Train: 2018-08-01T22:52:18.135432: step 75, loss 0.575368.
Train: 2018-08-01T22:52:18.334899: step 76, loss 0.615737.
Train: 2018-08-01T22:52:18.525367: step 77, loss 0.593545.
Train: 2018-08-01T22:52:18.725829: step 78, loss 0.675627.
Train: 2018-08-01T22:52:18.923333: step 79, loss 0.53414.
Train: 2018-08-01T22:52:19.112793: step 80, loss 0.681246.
Test: 2018-08-01T22:52:19.659357: step 80, loss 0.626954.
Train: 2018-08-01T22:52:19.853812: step 81, loss 0.721361.
Train: 2018-08-01T22:52:20.043332: step 82, loss 0.497552.
Train: 2018-08-01T22:52:20.231828: step 83, loss 0.583091.
Train: 2018-08-01T22:52:20.430298: step 84, loss 0.703487.
Train: 2018-08-01T22:52:20.620767: step 85, loss 0.583562.
Train: 2018-08-01T22:52:20.813249: step 86, loss 0.711122.
Train: 2018-08-01T22:52:21.012714: step 87, loss 0.503915.
Train: 2018-08-01T22:52:21.213179: step 88, loss 0.640947.
Train: 2018-08-01T22:52:21.403670: step 89, loss 0.517813.
Train: 2018-08-01T22:52:21.597152: step 90, loss 0.525284.
Test: 2018-08-01T22:52:22.147681: step 90, loss 0.618215.
Train: 2018-08-01T22:52:22.347176: step 91, loss 0.747349.
Train: 2018-08-01T22:52:22.543621: step 92, loss 0.666812.
Train: 2018-08-01T22:52:22.751069: step 93, loss 0.553194.
Train: 2018-08-01T22:52:22.945548: step 94, loss 0.608439.
Train: 2018-08-01T22:52:23.140054: step 95, loss 0.561905.
Train: 2018-08-01T22:52:23.342510: step 96, loss 0.55594.
Train: 2018-08-01T22:52:23.543981: step 97, loss 0.663161.
Train: 2018-08-01T22:52:23.737457: step 98, loss 0.729382.
Train: 2018-08-01T22:52:23.930913: step 99, loss 0.678455.
Train: 2018-08-01T22:52:24.120433: step 100, loss 0.578766.
Test: 2018-08-01T22:52:24.654977: step 100, loss 0.60971.
Train: 2018-08-01T22:52:25.446236: step 101, loss 0.564961.
Train: 2018-08-01T22:52:25.640750: step 102, loss 0.607618.
Train: 2018-08-01T22:52:25.832222: step 103, loss 0.642206.
Train: 2018-08-01T22:52:26.034689: step 104, loss 0.59084.
Train: 2018-08-01T22:52:26.237136: step 105, loss 0.619327.
Train: 2018-08-01T22:52:26.449558: step 106, loss 0.554207.
Train: 2018-08-01T22:52:26.641042: step 107, loss 0.601897.
Train: 2018-08-01T22:52:26.834544: step 108, loss 0.662383.
Train: 2018-08-01T22:52:27.038998: step 109, loss 0.748391.
Train: 2018-08-01T22:52:27.243458: step 110, loss 0.635457.
Test: 2018-08-01T22:52:27.766035: step 110, loss 0.603099.
Train: 2018-08-01T22:52:27.961512: step 111, loss 0.511049.
Train: 2018-08-01T22:52:28.159981: step 112, loss 0.590783.
Train: 2018-08-01T22:52:28.360446: step 113, loss 0.710654.
Train: 2018-08-01T22:52:28.579863: step 114, loss 0.587575.
Train: 2018-08-01T22:52:28.774341: step 115, loss 0.532769.
Train: 2018-08-01T22:52:28.972809: step 116, loss 0.575609.
Train: 2018-08-01T22:52:29.176270: step 117, loss 0.674646.
Train: 2018-08-01T22:52:29.371744: step 118, loss 0.616245.
Train: 2018-08-01T22:52:29.572208: step 119, loss 0.554671.
Train: 2018-08-01T22:52:29.771674: step 120, loss 0.551985.
Test: 2018-08-01T22:52:30.300261: step 120, loss 0.594871.
Train: 2018-08-01T22:52:30.498762: step 121, loss 0.631697.
Train: 2018-08-01T22:52:30.691215: step 122, loss 0.64443.
Train: 2018-08-01T22:52:30.877718: step 123, loss 0.65291.
Train: 2018-08-01T22:52:31.081199: step 124, loss 0.596056.
Train: 2018-08-01T22:52:31.281638: step 125, loss 0.530109.
Train: 2018-08-01T22:52:31.473171: step 126, loss 0.68105.
Train: 2018-08-01T22:52:31.666607: step 127, loss 0.550885.
Train: 2018-08-01T22:52:31.859119: step 128, loss 0.640516.
Train: 2018-08-01T22:52:32.052576: step 129, loss 0.560329.
Train: 2018-08-01T22:52:32.251071: step 130, loss 0.651178.
Test: 2018-08-01T22:52:32.780636: step 130, loss 0.589575.
Train: 2018-08-01T22:52:32.973116: step 131, loss 0.587605.
Train: 2018-08-01T22:52:33.170588: step 132, loss 0.656776.
Train: 2018-08-01T22:52:33.368059: step 133, loss 0.55735.
Train: 2018-08-01T22:52:33.556582: step 134, loss 0.666185.
Train: 2018-08-01T22:52:33.753055: step 135, loss 0.663822.
Train: 2018-08-01T22:52:33.947511: step 136, loss 0.690605.
Train: 2018-08-01T22:52:34.141991: step 137, loss 0.537086.
Train: 2018-08-01T22:52:34.337490: step 138, loss 0.709315.
Train: 2018-08-01T22:52:34.532945: step 139, loss 0.556457.
Train: 2018-08-01T22:52:34.732439: step 140, loss 0.54314.
Test: 2018-08-01T22:52:35.267981: step 140, loss 0.590368.
Train: 2018-08-01T22:52:35.465452: step 141, loss 0.621358.
Train: 2018-08-01T22:52:35.661927: step 142, loss 0.675634.
Train: 2018-08-01T22:52:35.853442: step 143, loss 0.570869.
Train: 2018-08-01T22:52:36.054877: step 144, loss 0.617212.
Train: 2018-08-01T22:52:36.254347: step 145, loss 0.635149.
Train: 2018-08-01T22:52:36.444834: step 146, loss 0.543223.
Train: 2018-08-01T22:52:36.635354: step 147, loss 0.616781.
Train: 2018-08-01T22:52:36.831800: step 148, loss 0.672619.
Train: 2018-08-01T22:52:37.019298: step 149, loss 0.575893.
Train: 2018-08-01T22:52:37.220760: step 150, loss 0.802842.
Test: 2018-08-01T22:52:37.758341: step 150, loss 0.585933.
Train: 2018-08-01T22:52:37.950834: step 151, loss 0.577243.
Train: 2018-08-01T22:52:38.149304: step 152, loss 0.723499.
Train: 2018-08-01T22:52:38.346781: step 153, loss 0.595113.
Train: 2018-08-01T22:52:38.547243: step 154, loss 0.639943.
Train: 2018-08-01T22:52:38.738736: step 155, loss 0.533141.
Train: 2018-08-01T22:52:38.929193: step 156, loss 0.510406.
Train: 2018-08-01T22:52:39.121689: step 157, loss 0.506778.
Train: 2018-08-01T22:52:39.328137: step 158, loss 0.645529.
Train: 2018-08-01T22:52:39.517621: step 159, loss 0.566316.
Train: 2018-08-01T22:52:39.706142: step 160, loss 0.564952.
Test: 2018-08-01T22:52:40.233706: step 160, loss 0.581062.
Train: 2018-08-01T22:52:40.431178: step 161, loss 0.700886.
Train: 2018-08-01T22:52:40.623664: step 162, loss 0.58582.
Train: 2018-08-01T22:52:40.817147: step 163, loss 0.585367.
Train: 2018-08-01T22:52:41.014617: step 164, loss 0.661072.
Train: 2018-08-01T22:52:41.210129: step 165, loss 0.714505.
Train: 2018-08-01T22:52:41.414550: step 166, loss 0.50695.
Train: 2018-08-01T22:52:41.603046: step 167, loss 0.548914.
Train: 2018-08-01T22:52:41.800517: step 168, loss 0.626254.
Train: 2018-08-01T22:52:41.993048: step 169, loss 0.484841.
Train: 2018-08-01T22:52:42.191472: step 170, loss 0.695217.
Test: 2018-08-01T22:52:42.735020: step 170, loss 0.577258.
Train: 2018-08-01T22:52:42.926508: step 171, loss 0.602556.
Train: 2018-08-01T22:52:43.116001: step 172, loss 0.522671.
Train: 2018-08-01T22:52:43.306492: step 173, loss 0.692502.
Train: 2018-08-01T22:52:43.501969: step 174, loss 0.57189.
Train: 2018-08-01T22:52:43.705427: step 175, loss 0.697103.
Train: 2018-08-01T22:52:43.899905: step 176, loss 0.506786.
Train: 2018-08-01T22:52:44.091419: step 177, loss 0.580012.
Train: 2018-08-01T22:52:44.289863: step 178, loss 0.567885.
Train: 2018-08-01T22:52:44.488337: step 179, loss 0.652371.
Train: 2018-08-01T22:52:44.678849: step 180, loss 0.548101.
Test: 2018-08-01T22:52:45.216386: step 180, loss 0.575177.
Train: 2018-08-01T22:52:45.415853: step 181, loss 0.648543.
Train: 2018-08-01T22:52:45.606343: step 182, loss 0.635721.
Train: 2018-08-01T22:52:45.798829: step 183, loss 0.591272.
Train: 2018-08-01T22:52:45.985361: step 184, loss 0.52707.
Train: 2018-08-01T22:52:46.184818: step 185, loss 0.692143.
Train: 2018-08-01T22:52:46.379277: step 186, loss 0.575294.
Train: 2018-08-01T22:52:46.566776: step 187, loss 0.709695.
Train: 2018-08-01T22:52:46.760259: step 188, loss 0.693053.
Train: 2018-08-01T22:52:46.951748: step 189, loss 0.500395.
Train: 2018-08-01T22:52:47.146253: step 190, loss 0.597514.
Test: 2018-08-01T22:52:47.674814: step 190, loss 0.575851.
Train: 2018-08-01T22:52:47.874298: step 191, loss 0.567495.
Train: 2018-08-01T22:52:48.068782: step 192, loss 0.584846.
Train: 2018-08-01T22:52:48.264269: step 193, loss 0.594544.
Train: 2018-08-01T22:52:48.458722: step 194, loss 0.594605.
Train: 2018-08-01T22:52:48.648240: step 195, loss 0.538406.
Train: 2018-08-01T22:52:48.849680: step 196, loss 0.60566.
Train: 2018-08-01T22:52:49.044153: step 197, loss 0.542167.
Train: 2018-08-01T22:52:49.238662: step 198, loss 0.695605.
Train: 2018-08-01T22:52:49.430122: step 199, loss 0.654889.
Train: 2018-08-01T22:52:49.630587: step 200, loss 0.527132.
Test: 2018-08-01T22:52:50.165164: step 200, loss 0.572598.
Train: 2018-08-01T22:52:50.949420: step 201, loss 0.589324.
Train: 2018-08-01T22:52:51.147864: step 202, loss 0.583077.
Train: 2018-08-01T22:52:51.338354: step 203, loss 0.521744.
Train: 2018-08-01T22:52:51.532834: step 204, loss 0.490584.
Train: 2018-08-01T22:52:51.733299: step 205, loss 0.551192.
Train: 2018-08-01T22:52:51.924787: step 206, loss 0.586043.
Train: 2018-08-01T22:52:52.114280: step 207, loss 0.637364.
Train: 2018-08-01T22:52:52.307770: step 208, loss 0.521997.
Train: 2018-08-01T22:52:52.503271: step 209, loss 0.604069.
Train: 2018-08-01T22:52:52.703735: step 210, loss 0.523458.
Test: 2018-08-01T22:52:53.240296: step 210, loss 0.569109.
Train: 2018-08-01T22:52:53.436771: step 211, loss 0.548416.
Train: 2018-08-01T22:52:53.629230: step 212, loss 0.601582.
Train: 2018-08-01T22:52:53.821716: step 213, loss 0.560812.
Train: 2018-08-01T22:52:54.012207: step 214, loss 0.611013.
Train: 2018-08-01T22:52:54.212670: step 215, loss 0.662089.
Train: 2018-08-01T22:52:54.406178: step 216, loss 0.499137.
Train: 2018-08-01T22:52:54.598640: step 217, loss 0.694018.
Train: 2018-08-01T22:52:54.792122: step 218, loss 0.657571.
Train: 2018-08-01T22:52:54.991588: step 219, loss 0.575831.
Train: 2018-08-01T22:52:55.182080: step 220, loss 0.58808.
Test: 2018-08-01T22:52:55.715652: step 220, loss 0.567995.
Train: 2018-08-01T22:52:55.925094: step 221, loss 0.61445.
Train: 2018-08-01T22:52:56.113615: step 222, loss 0.562058.
Train: 2018-08-01T22:52:56.319040: step 223, loss 0.649805.
Train: 2018-08-01T22:52:56.513519: step 224, loss 0.628805.
Train: 2018-08-01T22:52:56.713983: step 225, loss 0.576253.
Train: 2018-08-01T22:52:56.908465: step 226, loss 0.544194.
Train: 2018-08-01T22:52:57.096959: step 227, loss 0.682733.
Train: 2018-08-01T22:52:57.293435: step 228, loss 0.677591.
Train: 2018-08-01T22:52:57.489909: step 229, loss 0.632324.
Train: 2018-08-01T22:52:57.685406: step 230, loss 0.579019.
Test: 2018-08-01T22:52:58.215969: step 230, loss 0.571619.
Train: 2018-08-01T22:52:58.412443: step 231, loss 0.596746.
Train: 2018-08-01T22:52:58.613931: step 232, loss 0.576422.
Train: 2018-08-01T22:52:58.813372: step 233, loss 0.606721.
Train: 2018-08-01T22:52:59.009884: step 234, loss 0.630054.
Train: 2018-08-01T22:52:59.205324: step 235, loss 0.557007.
Train: 2018-08-01T22:52:59.399804: step 236, loss 0.649893.
Train: 2018-08-01T22:52:59.597276: step 237, loss 0.697307.
Train: 2018-08-01T22:52:59.794748: step 238, loss 0.62542.
Train: 2018-08-01T22:52:59.984241: step 239, loss 0.546275.
Train: 2018-08-01T22:53:00.178722: step 240, loss 0.583851.
Test: 2018-08-01T22:53:00.710328: step 240, loss 0.57292.
Train: 2018-08-01T22:53:00.900791: step 241, loss 0.582008.
Train: 2018-08-01T22:53:01.091314: step 242, loss 0.602341.
Train: 2018-08-01T22:53:01.283768: step 243, loss 0.649635.
Train: 2018-08-01T22:53:01.484258: step 244, loss 0.600613.
Train: 2018-08-01T22:53:01.680707: step 245, loss 0.47743.
Train: 2018-08-01T22:53:01.884193: step 246, loss 0.581655.
Train: 2018-08-01T22:53:02.080668: step 247, loss 0.629864.
Train: 2018-08-01T22:53:02.272153: step 248, loss 0.561267.
Train: 2018-08-01T22:53:02.464611: step 249, loss 0.549681.
Train: 2018-08-01T22:53:02.654104: step 250, loss 0.591062.
Test: 2018-08-01T22:53:03.195657: step 250, loss 0.566847.
Train: 2018-08-01T22:53:03.398116: step 251, loss 0.480464.
Train: 2018-08-01T22:53:03.586612: step 252, loss 0.569079.
Train: 2018-08-01T22:53:03.783113: step 253, loss 0.623507.
Train: 2018-08-01T22:53:03.975573: step 254, loss 0.663047.
Train: 2018-08-01T22:53:04.168084: step 255, loss 0.514009.
Train: 2018-08-01T22:53:04.363535: step 256, loss 0.526515.
Train: 2018-08-01T22:53:04.552031: step 257, loss 0.537407.
Train: 2018-08-01T22:53:04.738532: step 258, loss 0.453957.
Train: 2018-08-01T22:53:04.934035: step 259, loss 0.601829.
Train: 2018-08-01T22:53:05.116548: step 260, loss 0.533502.
Test: 2018-08-01T22:53:05.669044: step 260, loss 0.563558.
Train: 2018-08-01T22:53:05.862528: step 261, loss 0.496534.
Train: 2018-08-01T22:53:06.053051: step 262, loss 0.508409.
Train: 2018-08-01T22:53:06.241513: step 263, loss 0.572033.
Train: 2018-08-01T22:53:06.434997: step 264, loss 0.718887.
Train: 2018-08-01T22:53:06.620511: step 265, loss 0.556539.
Train: 2018-08-01T22:53:06.810014: step 266, loss 0.635184.
Train: 2018-08-01T22:53:07.007467: step 267, loss 0.55636.
Train: 2018-08-01T22:53:07.196992: step 268, loss 0.562753.
Train: 2018-08-01T22:53:07.388449: step 269, loss 0.573857.
Train: 2018-08-01T22:53:07.575965: step 270, loss 0.721779.
Test: 2018-08-01T22:53:08.105531: step 270, loss 0.562633.
Train: 2018-08-01T22:53:08.287046: step 271, loss 0.626097.
Train: 2018-08-01T22:53:08.470582: step 272, loss 0.535423.
Train: 2018-08-01T22:53:08.651073: step 273, loss 0.5202.
Train: 2018-08-01T22:53:08.836577: step 274, loss 0.604964.
Train: 2018-08-01T22:53:09.024075: step 275, loss 0.559635.
Train: 2018-08-01T22:53:09.210578: step 276, loss 0.673893.
Train: 2018-08-01T22:53:09.390129: step 277, loss 0.538177.
Train: 2018-08-01T22:53:09.570614: step 278, loss 0.566457.
Train: 2018-08-01T22:53:09.751132: step 279, loss 0.651234.
Train: 2018-08-01T22:53:09.928658: step 280, loss 0.572718.
Test: 2018-08-01T22:53:10.456247: step 280, loss 0.5639.
Train: 2018-08-01T22:53:10.639757: step 281, loss 0.663177.
Train: 2018-08-01T22:53:10.815288: step 282, loss 0.577377.
Train: 2018-08-01T22:53:10.994808: step 283, loss 0.590718.
Train: 2018-08-01T22:53:11.173357: step 284, loss 0.544942.
Train: 2018-08-01T22:53:11.345869: step 285, loss 0.552131.
Train: 2018-08-01T22:53:11.522397: step 286, loss 0.534823.
Train: 2018-08-01T22:53:11.696930: step 287, loss 0.550676.
Train: 2018-08-01T22:53:11.885427: step 288, loss 0.530796.
Train: 2018-08-01T22:53:12.061986: step 289, loss 0.468688.
Train: 2018-08-01T22:53:12.237512: step 290, loss 0.595776.
Test: 2018-08-01T22:53:12.764106: step 290, loss 0.563051.
Train: 2018-08-01T22:53:12.936617: step 291, loss 0.62939.
Train: 2018-08-01T22:53:13.114142: step 292, loss 0.553794.
Train: 2018-08-01T22:53:13.290696: step 293, loss 0.581432.
Train: 2018-08-01T22:53:13.462212: step 294, loss 0.514292.
Train: 2018-08-01T22:53:13.638773: step 295, loss 0.525279.
Train: 2018-08-01T22:53:13.811278: step 296, loss 0.51986.
Train: 2018-08-01T22:53:13.985812: step 297, loss 0.59254.
Train: 2018-08-01T22:53:14.155384: step 298, loss 0.618642.
Train: 2018-08-01T22:53:14.328894: step 299, loss 0.567913.
Train: 2018-08-01T22:53:14.498473: step 300, loss 0.623985.
Test: 2018-08-01T22:53:15.038997: step 300, loss 0.56032.
Train: 2018-08-01T22:53:15.779251: step 301, loss 0.482606.
Train: 2018-08-01T22:53:15.950817: step 302, loss 0.613467.
Train: 2018-08-01T22:53:16.118344: step 303, loss 0.555159.
Train: 2018-08-01T22:53:16.285922: step 304, loss 0.644153.
Train: 2018-08-01T22:53:16.448486: step 305, loss 0.490008.
Train: 2018-08-01T22:53:16.610055: step 306, loss 0.654287.
Train: 2018-08-01T22:53:16.776585: step 307, loss 0.570225.
Train: 2018-08-01T22:53:16.946131: step 308, loss 0.738747.
Train: 2018-08-01T22:53:17.115678: step 309, loss 0.636074.
Train: 2018-08-01T22:53:17.282232: step 310, loss 0.625795.
Test: 2018-08-01T22:53:17.808826: step 310, loss 0.56043.
Train: 2018-08-01T22:53:17.975408: step 311, loss 0.62023.
Train: 2018-08-01T22:53:18.144927: step 312, loss 0.615415.
Train: 2018-08-01T22:53:18.316493: step 313, loss 0.632111.
Train: 2018-08-01T22:53:18.483052: step 314, loss 0.504796.
Train: 2018-08-01T22:53:18.655586: step 315, loss 0.582834.
Train: 2018-08-01T22:53:18.829098: step 316, loss 0.561446.
Train: 2018-08-01T22:53:18.992660: step 317, loss 0.62136.
Train: 2018-08-01T22:53:19.171209: step 318, loss 0.547617.
Train: 2018-08-01T22:53:19.337738: step 319, loss 0.595179.
Train: 2018-08-01T22:53:19.502324: step 320, loss 0.593135.
Test: 2018-08-01T22:53:20.046842: step 320, loss 0.564088.
Train: 2018-08-01T22:53:20.213396: step 321, loss 0.576354.
Train: 2018-08-01T22:53:20.390923: step 322, loss 0.550162.
Train: 2018-08-01T22:53:20.552491: step 323, loss 0.593488.
Train: 2018-08-01T22:53:20.717079: step 324, loss 0.554689.
Train: 2018-08-01T22:53:20.876624: step 325, loss 0.603334.
Train: 2018-08-01T22:53:21.040219: step 326, loss 0.613926.
Train: 2018-08-01T22:53:21.206775: step 327, loss 0.636527.
Train: 2018-08-01T22:53:21.372299: step 328, loss 0.5468.
Train: 2018-08-01T22:53:21.537856: step 329, loss 0.607333.
Train: 2018-08-01T22:53:21.702417: step 330, loss 0.611674.
Test: 2018-08-01T22:53:22.247959: step 330, loss 0.562635.
Train: 2018-08-01T22:53:22.416508: step 331, loss 0.62416.
Train: 2018-08-01T22:53:22.580071: step 332, loss 0.655394.
Train: 2018-08-01T22:53:22.742636: step 333, loss 0.585714.
Train: 2018-08-01T22:53:22.906198: step 334, loss 0.514425.
Train: 2018-08-01T22:53:23.068790: step 335, loss 0.562849.
Train: 2018-08-01T22:53:23.238312: step 336, loss 0.620048.
Train: 2018-08-01T22:53:23.403868: step 337, loss 0.658984.
Train: 2018-08-01T22:53:23.562469: step 338, loss 0.628616.
Train: 2018-08-01T22:53:23.733986: step 339, loss 0.616632.
Train: 2018-08-01T22:53:23.898545: step 340, loss 0.516983.
Test: 2018-08-01T22:53:24.438110: step 340, loss 0.562556.
Train: 2018-08-01T22:53:24.604685: step 341, loss 0.556966.
Train: 2018-08-01T22:53:24.772211: step 342, loss 0.611461.
Train: 2018-08-01T22:53:24.934802: step 343, loss 0.583727.
Train: 2018-08-01T22:53:25.109309: step 344, loss 0.576508.
Train: 2018-08-01T22:53:25.277884: step 345, loss 0.533931.
Train: 2018-08-01T22:53:25.442471: step 346, loss 0.646496.
Train: 2018-08-01T22:53:25.618973: step 347, loss 0.500721.
Train: 2018-08-01T22:53:25.785553: step 348, loss 0.676215.
Train: 2018-08-01T22:53:25.945127: step 349, loss 0.544732.
Train: 2018-08-01T22:53:26.111694: step 350, loss 0.675632.
Test: 2018-08-01T22:53:26.642238: step 350, loss 0.560629.
Train: 2018-08-01T22:53:26.807795: step 351, loss 0.528237.
Train: 2018-08-01T22:53:26.978339: step 352, loss 0.554947.
Train: 2018-08-01T22:53:27.140933: step 353, loss 0.545084.
Train: 2018-08-01T22:53:27.309485: step 354, loss 0.547532.
Train: 2018-08-01T22:53:27.474015: step 355, loss 0.612706.
Train: 2018-08-01T22:53:27.632617: step 356, loss 0.590326.
Train: 2018-08-01T22:53:27.793193: step 357, loss 0.584522.
Train: 2018-08-01T22:53:27.963706: step 358, loss 0.523831.
Train: 2018-08-01T22:53:28.129263: step 359, loss 0.675471.
Train: 2018-08-01T22:53:28.295842: step 360, loss 0.493547.
Test: 2018-08-01T22:53:28.832411: step 360, loss 0.558356.
Train: 2018-08-01T22:53:29.010905: step 361, loss 0.45731.
Train: 2018-08-01T22:53:29.173471: step 362, loss 0.656024.
Train: 2018-08-01T22:53:29.341054: step 363, loss 0.65673.
Train: 2018-08-01T22:53:29.507602: step 364, loss 0.609004.
Train: 2018-08-01T22:53:29.664159: step 365, loss 0.484413.
Train: 2018-08-01T22:53:29.833706: step 366, loss 0.65452.
Train: 2018-08-01T22:53:29.997270: step 367, loss 0.547038.
Train: 2018-08-01T22:53:30.159859: step 368, loss 0.531518.
Train: 2018-08-01T22:53:30.321402: step 369, loss 0.512212.
Train: 2018-08-01T22:53:30.483001: step 370, loss 0.502484.
Test: 2018-08-01T22:53:31.017541: step 370, loss 0.55711.
Train: 2018-08-01T22:53:31.184096: step 371, loss 0.647723.
Train: 2018-08-01T22:53:31.346694: step 372, loss 0.62867.
Train: 2018-08-01T22:53:31.509280: step 373, loss 0.515457.
Train: 2018-08-01T22:53:31.674785: step 374, loss 0.478706.
Train: 2018-08-01T22:53:31.841340: step 375, loss 0.486411.
Train: 2018-08-01T22:53:32.018893: step 376, loss 0.487435.
Train: 2018-08-01T22:53:32.184452: step 377, loss 0.516708.
Train: 2018-08-01T22:53:32.349989: step 378, loss 0.594445.
Train: 2018-08-01T22:53:32.517557: step 379, loss 0.571531.
Train: 2018-08-01T22:53:32.687109: step 380, loss 0.590523.
Test: 2018-08-01T22:53:33.223677: step 380, loss 0.556436.
Train: 2018-08-01T22:53:33.397179: step 381, loss 0.587863.
Train: 2018-08-01T22:53:33.561765: step 382, loss 0.545237.
Train: 2018-08-01T22:53:33.720341: step 383, loss 0.643622.
Train: 2018-08-01T22:53:33.878892: step 384, loss 0.670001.
Train: 2018-08-01T22:53:34.043452: step 385, loss 0.739489.
Train: 2018-08-01T22:53:34.205044: step 386, loss 0.543664.
Train: 2018-08-01T22:53:34.372573: step 387, loss 0.591832.
Train: 2018-08-01T22:53:34.539127: step 388, loss 0.61629.
Train: 2018-08-01T22:53:34.700727: step 389, loss 0.604098.
Train: 2018-08-01T22:53:34.871239: step 390, loss 0.579866.
Test: 2018-08-01T22:53:35.409834: step 390, loss 0.557039.
Train: 2018-08-01T22:53:35.588322: step 391, loss 0.649712.
Train: 2018-08-01T22:53:35.753881: step 392, loss 0.438793.
Train: 2018-08-01T22:53:35.922430: step 393, loss 0.59102.
Train: 2018-08-01T22:53:36.092973: step 394, loss 0.67915.
Train: 2018-08-01T22:53:36.262523: step 395, loss 0.591914.
Train: 2018-08-01T22:53:36.427080: step 396, loss 0.560903.
Train: 2018-08-01T22:53:36.589647: step 397, loss 0.494006.
Train: 2018-08-01T22:53:36.758195: step 398, loss 0.508023.
Train: 2018-08-01T22:53:36.922755: step 399, loss 0.602633.
Train: 2018-08-01T22:53:37.090308: step 400, loss 0.579644.
Test: 2018-08-01T22:53:37.635849: step 400, loss 0.558558.
Train: 2018-08-01T22:53:38.391372: step 401, loss 0.550904.
Train: 2018-08-01T22:53:38.556932: step 402, loss 0.589083.
Train: 2018-08-01T22:53:38.730437: step 403, loss 0.630324.
Train: 2018-08-01T22:53:38.892031: step 404, loss 0.553877.
Train: 2018-08-01T22:53:39.057563: step 405, loss 0.550822.
Train: 2018-08-01T22:53:39.220154: step 406, loss 0.516708.
Train: 2018-08-01T22:53:39.383716: step 407, loss 0.604393.
Train: 2018-08-01T22:53:39.550277: step 408, loss 0.496569.
Train: 2018-08-01T22:53:39.718796: step 409, loss 0.534231.
Train: 2018-08-01T22:53:39.882383: step 410, loss 0.61687.
Test: 2018-08-01T22:53:40.422921: step 410, loss 0.556707.
Train: 2018-08-01T22:53:40.588499: step 411, loss 0.569512.
Train: 2018-08-01T22:53:40.754028: step 412, loss 0.523343.
Train: 2018-08-01T22:53:40.922602: step 413, loss 0.627571.
Train: 2018-08-01T22:53:41.090130: step 414, loss 0.572597.
Train: 2018-08-01T22:53:41.261671: step 415, loss 0.683017.
Train: 2018-08-01T22:53:41.425265: step 416, loss 0.695667.
Train: 2018-08-01T22:53:41.588823: step 417, loss 0.54491.
Train: 2018-08-01T22:53:41.753384: step 418, loss 0.612798.
Train: 2018-08-01T22:53:41.916945: step 419, loss 0.578468.
Train: 2018-08-01T22:53:42.081480: step 420, loss 0.64414.
Test: 2018-08-01T22:53:42.620040: step 420, loss 0.556668.
Train: 2018-08-01T22:53:42.788589: step 421, loss 0.469208.
Train: 2018-08-01T22:53:42.952153: step 422, loss 0.481113.
Train: 2018-08-01T22:53:43.112722: step 423, loss 0.551137.
Train: 2018-08-01T22:53:43.272323: step 424, loss 0.598957.
Train: 2018-08-01T22:53:43.433865: step 425, loss 0.530541.
Train: 2018-08-01T22:53:43.597460: step 426, loss 0.630876.
Train: 2018-08-01T22:53:43.765977: step 427, loss 0.617757.
Train: 2018-08-01T22:53:43.928543: step 428, loss 0.58499.
Train: 2018-08-01T22:53:44.093139: step 429, loss 0.59298.
Train: 2018-08-01T22:53:44.258659: step 430, loss 0.574091.
Test: 2018-08-01T22:53:44.807195: step 430, loss 0.556189.
Train: 2018-08-01T22:53:44.970755: step 431, loss 0.622456.
Train: 2018-08-01T22:53:45.142297: step 432, loss 0.614196.
Train: 2018-08-01T22:53:45.311853: step 433, loss 0.539823.
Train: 2018-08-01T22:53:45.475408: step 434, loss 0.591599.
Train: 2018-08-01T22:53:45.642984: step 435, loss 0.630601.
Train: 2018-08-01T22:53:45.803536: step 436, loss 0.555655.
Train: 2018-08-01T22:53:45.975072: step 437, loss 0.653241.
Train: 2018-08-01T22:53:46.143647: step 438, loss 0.502082.
Train: 2018-08-01T22:53:46.313199: step 439, loss 0.589574.
Train: 2018-08-01T22:53:46.488698: step 440, loss 0.507249.
Test: 2018-08-01T22:53:47.033243: step 440, loss 0.556806.
Train: 2018-08-01T22:53:47.202790: step 441, loss 0.592305.
Train: 2018-08-01T22:53:47.369375: step 442, loss 0.576245.
Train: 2018-08-01T22:53:47.529915: step 443, loss 0.559727.
Train: 2018-08-01T22:53:47.695473: step 444, loss 0.596785.
Train: 2018-08-01T22:53:47.860032: step 445, loss 0.641017.
Train: 2018-08-01T22:53:48.026587: step 446, loss 0.588947.
Train: 2018-08-01T22:53:48.190150: step 447, loss 0.614812.
Train: 2018-08-01T22:53:48.353739: step 448, loss 0.506959.
Train: 2018-08-01T22:53:48.517306: step 449, loss 0.626609.
Train: 2018-08-01T22:53:48.681860: step 450, loss 0.566175.
Test: 2018-08-01T22:53:49.226433: step 450, loss 0.556572.
Train: 2018-08-01T22:53:49.389974: step 451, loss 0.510678.
Train: 2018-08-01T22:53:49.557525: step 452, loss 0.6463.
Train: 2018-08-01T22:53:49.736017: step 453, loss 0.559934.
Train: 2018-08-01T22:53:49.900577: step 454, loss 0.575723.
Train: 2018-08-01T22:53:50.072119: step 455, loss 0.460465.
Train: 2018-08-01T22:53:50.238673: step 456, loss 0.596917.
Train: 2018-08-01T22:53:50.400242: step 457, loss 0.548553.
Train: 2018-08-01T22:53:50.571782: step 458, loss 0.549933.
Train: 2018-08-01T22:53:50.733352: step 459, loss 0.615256.
Train: 2018-08-01T22:53:50.903904: step 460, loss 0.549579.
Test: 2018-08-01T22:53:51.436477: step 460, loss 0.555081.
Train: 2018-08-01T22:53:51.595078: step 461, loss 0.629747.
Train: 2018-08-01T22:53:51.757644: step 462, loss 0.588857.
Train: 2018-08-01T22:53:51.936136: step 463, loss 0.456131.
Train: 2018-08-01T22:53:52.097704: step 464, loss 0.567582.
Train: 2018-08-01T22:53:52.272237: step 465, loss 0.547931.
Train: 2018-08-01T22:53:52.436798: step 466, loss 0.627927.
Train: 2018-08-01T22:53:52.605372: step 467, loss 0.557168.
Train: 2018-08-01T22:53:52.766941: step 468, loss 0.606799.
Train: 2018-08-01T22:53:52.928483: step 469, loss 0.576853.
Train: 2018-08-01T22:53:53.092072: step 470, loss 0.603948.
Test: 2018-08-01T22:53:53.613651: step 470, loss 0.554243.
Train: 2018-08-01T22:53:53.778212: step 471, loss 0.548748.
Train: 2018-08-01T22:53:53.939805: step 472, loss 0.404738.
Train: 2018-08-01T22:53:54.107332: step 473, loss 0.671852.
Train: 2018-08-01T22:53:54.267902: step 474, loss 0.543073.
Train: 2018-08-01T22:53:54.434457: step 475, loss 0.607581.
Train: 2018-08-01T22:53:54.646239: step 476, loss 0.602629.
Train: 2018-08-01T22:53:54.806810: step 477, loss 0.629711.
Train: 2018-08-01T22:53:54.980354: step 478, loss 0.484856.
Train: 2018-08-01T22:53:55.140912: step 479, loss 0.547637.
Train: 2018-08-01T22:53:55.306443: step 480, loss 0.579629.
Test: 2018-08-01T22:53:55.864950: step 480, loss 0.553863.
Train: 2018-08-01T22:53:56.080374: step 481, loss 0.667487.
Train: 2018-08-01T22:53:56.250926: step 482, loss 0.498925.
Train: 2018-08-01T22:53:56.412511: step 483, loss 0.641678.
Train: 2018-08-01T22:53:56.582057: step 484, loss 0.554344.
Train: 2018-08-01T22:53:56.756567: step 485, loss 0.473786.
Train: 2018-08-01T22:53:56.922124: step 486, loss 0.691357.
Train: 2018-08-01T22:53:57.085686: step 487, loss 0.499789.
Train: 2018-08-01T22:53:57.253239: step 488, loss 0.641643.
Train: 2018-08-01T22:53:57.421788: step 489, loss 0.558521.
Train: 2018-08-01T22:53:57.585351: step 490, loss 0.619398.
Test: 2018-08-01T22:53:58.119955: step 490, loss 0.554635.
Train: 2018-08-01T22:53:58.286507: step 491, loss 0.668298.
Train: 2018-08-01T22:53:58.451067: step 492, loss 0.56499.
Train: 2018-08-01T22:53:58.609612: step 493, loss 0.581726.
Train: 2018-08-01T22:53:58.779159: step 494, loss 0.567485.
Train: 2018-08-01T22:53:58.950729: step 495, loss 0.627131.
Train: 2018-08-01T22:53:59.118278: step 496, loss 0.491413.
Train: 2018-08-01T22:53:59.276860: step 497, loss 0.630777.
Train: 2018-08-01T22:53:59.443413: step 498, loss 0.526931.
Train: 2018-08-01T22:53:59.604951: step 499, loss 0.649889.
Train: 2018-08-01T22:53:59.777491: step 500, loss 0.531358.
Test: 2018-08-01T22:54:00.312093: step 500, loss 0.556188.
Train: 2018-08-01T22:54:01.094758: step 501, loss 0.63907.
Train: 2018-08-01T22:54:01.262327: step 502, loss 0.48031.
Train: 2018-08-01T22:54:01.423877: step 503, loss 0.58596.
Train: 2018-08-01T22:54:01.594421: step 504, loss 0.477737.
Train: 2018-08-01T22:54:01.759978: step 505, loss 0.595875.
Train: 2018-08-01T22:54:01.921573: step 506, loss 0.548826.
Train: 2018-08-01T22:54:02.086131: step 507, loss 0.503078.
Train: 2018-08-01T22:54:02.251664: step 508, loss 0.654493.
Train: 2018-08-01T22:54:02.412235: step 509, loss 0.540317.
Train: 2018-08-01T22:54:02.579807: step 510, loss 0.551634.
Test: 2018-08-01T22:54:03.117350: step 510, loss 0.554628.
Train: 2018-08-01T22:54:03.290886: step 511, loss 0.456574.
Train: 2018-08-01T22:54:03.463451: step 512, loss 0.460667.
Train: 2018-08-01T22:54:03.637958: step 513, loss 0.527733.
Train: 2018-08-01T22:54:03.802519: step 514, loss 0.67484.
Train: 2018-08-01T22:54:03.966082: step 515, loss 0.48471.
Train: 2018-08-01T22:54:04.133658: step 516, loss 0.513805.
Train: 2018-08-01T22:54:04.300195: step 517, loss 0.526209.
Train: 2018-08-01T22:54:04.467740: step 518, loss 0.59612.
Train: 2018-08-01T22:54:04.644294: step 519, loss 0.617642.
Train: 2018-08-01T22:54:04.811845: step 520, loss 0.571859.
Test: 2018-08-01T22:54:05.347402: step 520, loss 0.55283.
Train: 2018-08-01T22:54:05.509954: step 521, loss 0.552871.
Train: 2018-08-01T22:54:05.678504: step 522, loss 0.528934.
Train: 2018-08-01T22:54:05.850046: step 523, loss 0.478209.
Train: 2018-08-01T22:54:06.015628: step 524, loss 0.520125.
Train: 2018-08-01T22:54:06.208095: step 525, loss 0.513488.
Train: 2018-08-01T22:54:06.399582: step 526, loss 0.517724.
Train: 2018-08-01T22:54:06.563179: step 527, loss 0.668194.
Train: 2018-08-01T22:54:06.724756: step 528, loss 0.573574.
Train: 2018-08-01T22:54:06.885313: step 529, loss 0.535568.
Train: 2018-08-01T22:54:07.059810: step 530, loss 0.597483.
Test: 2018-08-01T22:54:07.603358: step 530, loss 0.552904.
Train: 2018-08-01T22:54:07.764939: step 531, loss 0.642579.
Train: 2018-08-01T22:54:07.931505: step 532, loss 0.506159.
Train: 2018-08-01T22:54:08.098061: step 533, loss 0.54324.
Train: 2018-08-01T22:54:08.265613: step 534, loss 0.538528.
Train: 2018-08-01T22:54:08.435159: step 535, loss 0.538663.
Train: 2018-08-01T22:54:08.598696: step 536, loss 0.542665.
Train: 2018-08-01T22:54:08.789188: step 537, loss 0.602586.
Train: 2018-08-01T22:54:08.965733: step 538, loss 0.514921.
Train: 2018-08-01T22:54:09.135288: step 539, loss 0.606492.
Train: 2018-08-01T22:54:09.298856: step 540, loss 0.628224.
Test: 2018-08-01T22:54:09.833396: step 540, loss 0.5525.
Train: 2018-08-01T22:54:09.998954: step 541, loss 0.563514.
Train: 2018-08-01T22:54:10.170495: step 542, loss 0.593622.
Train: 2018-08-01T22:54:10.365973: step 543, loss 0.586702.
Train: 2018-08-01T22:54:10.580400: step 544, loss 0.512633.
Train: 2018-08-01T22:54:10.819763: step 545, loss 0.453547.
Train: 2018-08-01T22:54:11.052139: step 546, loss 0.537212.
Train: 2018-08-01T22:54:11.211711: step 547, loss 0.619333.
Train: 2018-08-01T22:54:11.384250: step 548, loss 0.616939.
Train: 2018-08-01T22:54:11.568758: step 549, loss 0.560529.
Train: 2018-08-01T22:54:11.738304: step 550, loss 0.535733.
Test: 2018-08-01T22:54:12.276865: step 550, loss 0.55269.
Train: 2018-08-01T22:54:12.438463: step 551, loss 0.501642.
Train: 2018-08-01T22:54:12.606011: step 552, loss 0.596731.
Train: 2018-08-01T22:54:12.777552: step 553, loss 0.520572.
Train: 2018-08-01T22:54:12.944081: step 554, loss 0.577167.
Train: 2018-08-01T22:54:13.107643: step 555, loss 0.54482.
Train: 2018-08-01T22:54:13.279185: step 556, loss 0.60394.
Train: 2018-08-01T22:54:13.444761: step 557, loss 0.466413.
Train: 2018-08-01T22:54:13.612295: step 558, loss 0.567362.
Train: 2018-08-01T22:54:13.775881: step 559, loss 0.551379.
Train: 2018-08-01T22:54:13.941440: step 560, loss 0.584936.
Test: 2018-08-01T22:54:14.480973: step 560, loss 0.552379.
Train: 2018-08-01T22:54:14.661490: step 561, loss 0.494536.
Train: 2018-08-01T22:54:14.832068: step 562, loss 0.620317.
Train: 2018-08-01T22:54:15.002607: step 563, loss 0.529466.
Train: 2018-08-01T22:54:15.191073: step 564, loss 0.550334.
Train: 2018-08-01T22:54:15.373589: step 565, loss 0.549014.
Train: 2018-08-01T22:54:15.569065: step 566, loss 0.506899.
Train: 2018-08-01T22:54:15.806434: step 567, loss 0.470598.
Train: 2018-08-01T22:54:16.005898: step 568, loss 0.523306.
Train: 2018-08-01T22:54:16.179432: step 569, loss 0.580265.
Train: 2018-08-01T22:54:16.339005: step 570, loss 0.488729.
Test: 2018-08-01T22:54:16.877596: step 570, loss 0.552077.
Train: 2018-08-01T22:54:17.044121: step 571, loss 0.51286.
Train: 2018-08-01T22:54:17.209677: step 572, loss 0.595502.
Train: 2018-08-01T22:54:17.385209: step 573, loss 0.599489.
Train: 2018-08-01T22:54:17.548802: step 574, loss 0.528605.
Train: 2018-08-01T22:54:17.722307: step 575, loss 0.400444.
Train: 2018-08-01T22:54:17.888862: step 576, loss 0.576135.
Train: 2018-08-01T22:54:18.055417: step 577, loss 0.606601.
Train: 2018-08-01T22:54:18.217983: step 578, loss 0.59955.
Train: 2018-08-01T22:54:18.392540: step 579, loss 0.527248.
Train: 2018-08-01T22:54:18.568046: step 580, loss 0.516662.
Test: 2018-08-01T22:54:19.113589: step 580, loss 0.552318.
Train: 2018-08-01T22:54:19.282137: step 581, loss 0.608095.
Train: 2018-08-01T22:54:19.451684: step 582, loss 0.503952.
Train: 2018-08-01T22:54:19.614249: step 583, loss 0.630128.
Train: 2018-08-01T22:54:19.787786: step 584, loss 0.583519.
Train: 2018-08-01T22:54:19.948356: step 585, loss 0.5626.
Train: 2018-08-01T22:54:20.108927: step 586, loss 0.732828.
Train: 2018-08-01T22:54:20.275483: step 587, loss 0.512987.
Train: 2018-08-01T22:54:20.441064: step 588, loss 0.534899.
Train: 2018-08-01T22:54:20.604633: step 589, loss 0.524875.
Train: 2018-08-01T22:54:20.770188: step 590, loss 0.58429.
Test: 2018-08-01T22:54:21.315700: step 590, loss 0.551829.
Train: 2018-08-01T22:54:21.478292: step 591, loss 0.461527.
Train: 2018-08-01T22:54:21.643824: step 592, loss 0.572617.
Train: 2018-08-01T22:54:21.808439: step 593, loss 0.496337.
Train: 2018-08-01T22:54:21.975936: step 594, loss 0.442518.
Train: 2018-08-01T22:54:22.138534: step 595, loss 0.659676.
Train: 2018-08-01T22:54:22.306054: step 596, loss 0.568415.
Train: 2018-08-01T22:54:22.469645: step 597, loss 0.519655.
Train: 2018-08-01T22:54:22.633179: step 598, loss 0.547674.
Train: 2018-08-01T22:54:22.795745: step 599, loss 0.559814.
Train: 2018-08-01T22:54:22.960305: step 600, loss 0.536494.
Test: 2018-08-01T22:54:23.510865: step 600, loss 0.551733.
Train: 2018-08-01T22:54:24.317748: step 601, loss 0.52762.
Train: 2018-08-01T22:54:24.492272: step 602, loss 0.525294.
Train: 2018-08-01T22:54:24.665809: step 603, loss 0.568814.
Train: 2018-08-01T22:54:24.830344: step 604, loss 0.572457.
Train: 2018-08-01T22:54:24.999891: step 605, loss 0.654402.
Train: 2018-08-01T22:54:25.167466: step 606, loss 0.699875.
Train: 2018-08-01T22:54:25.333000: step 607, loss 0.566549.
Train: 2018-08-01T22:54:25.496562: step 608, loss 0.531895.
Train: 2018-08-01T22:54:25.659153: step 609, loss 0.487527.
Train: 2018-08-01T22:54:25.822691: step 610, loss 0.622787.
Test: 2018-08-01T22:54:26.373218: step 610, loss 0.551813.
Train: 2018-08-01T22:54:26.538776: step 611, loss 0.632563.
Train: 2018-08-01T22:54:26.700378: step 612, loss 0.621783.
Train: 2018-08-01T22:54:26.866900: step 613, loss 0.536711.
Train: 2018-08-01T22:54:27.034452: step 614, loss 0.549215.
Train: 2018-08-01T22:54:27.208985: step 615, loss 0.542653.
Train: 2018-08-01T22:54:27.371550: step 616, loss 0.554474.
Train: 2018-08-01T22:54:27.533119: step 617, loss 0.472384.
Train: 2018-08-01T22:54:27.699698: step 618, loss 0.496714.
Train: 2018-08-01T22:54:27.874228: step 619, loss 0.59834.
Train: 2018-08-01T22:54:28.042775: step 620, loss 0.704919.
Test: 2018-08-01T22:54:28.580319: step 620, loss 0.552314.
Train: 2018-08-01T22:54:28.745876: step 621, loss 0.489136.
Train: 2018-08-01T22:54:28.913429: step 622, loss 0.591143.
Train: 2018-08-01T22:54:29.080982: step 623, loss 0.58507.
Train: 2018-08-01T22:54:29.242575: step 624, loss 0.509351.
Train: 2018-08-01T22:54:29.405126: step 625, loss 0.564656.
Train: 2018-08-01T22:54:29.578660: step 626, loss 0.515515.
Train: 2018-08-01T22:54:29.739253: step 627, loss 0.494514.
Train: 2018-08-01T22:54:29.898794: step 628, loss 0.546566.
Train: 2018-08-01T22:54:30.059365: step 629, loss 0.527083.
Train: 2018-08-01T22:54:30.229948: step 630, loss 0.570813.
Test: 2018-08-01T22:54:30.763484: step 630, loss 0.551664.
Train: 2018-08-01T22:54:30.926059: step 631, loss 0.572131.
Train: 2018-08-01T22:54:31.099591: step 632, loss 0.532898.
Train: 2018-08-01T22:54:31.269130: step 633, loss 0.554594.
Train: 2018-08-01T22:54:31.426735: step 634, loss 0.567858.
Train: 2018-08-01T22:54:31.588278: step 635, loss 0.706102.
Train: 2018-08-01T22:54:31.749847: step 636, loss 0.618825.
Train: 2018-08-01T22:54:31.916401: step 637, loss 0.516752.
Train: 2018-08-01T22:54:32.086946: step 638, loss 0.525495.
Train: 2018-08-01T22:54:32.247547: step 639, loss 0.637018.
Train: 2018-08-01T22:54:32.406091: step 640, loss 0.582378.
Test: 2018-08-01T22:54:32.948642: step 640, loss 0.551386.
Train: 2018-08-01T22:54:33.118213: step 641, loss 0.573642.
Train: 2018-08-01T22:54:33.281776: step 642, loss 0.554632.
Train: 2018-08-01T22:54:33.441325: step 643, loss 0.624053.
Train: 2018-08-01T22:54:33.605884: step 644, loss 0.604039.
Train: 2018-08-01T22:54:33.772470: step 645, loss 0.500017.
Train: 2018-08-01T22:54:33.931016: step 646, loss 0.498958.
Train: 2018-08-01T22:54:34.092583: step 647, loss 0.47656.
Train: 2018-08-01T22:54:34.262164: step 648, loss 0.508876.
Train: 2018-08-01T22:54:34.427688: step 649, loss 0.559339.
Train: 2018-08-01T22:54:34.596237: step 650, loss 0.532138.
Test: 2018-08-01T22:54:35.126819: step 650, loss 0.55144.
Train: 2018-08-01T22:54:35.287390: step 651, loss 0.574298.
Train: 2018-08-01T22:54:35.456937: step 652, loss 0.548991.
Train: 2018-08-01T22:54:35.627480: step 653, loss 0.575241.
Train: 2018-08-01T22:54:35.795060: step 654, loss 0.602421.
Train: 2018-08-01T22:54:35.960621: step 655, loss 0.527516.
Train: 2018-08-01T22:54:36.132157: step 656, loss 0.664184.
Train: 2018-08-01T22:54:36.292701: step 657, loss 0.602542.
Train: 2018-08-01T22:54:36.466239: step 658, loss 0.529999.
Train: 2018-08-01T22:54:36.638777: step 659, loss 0.610802.
Train: 2018-08-01T22:54:36.802340: step 660, loss 0.588436.
Test: 2018-08-01T22:54:37.351879: step 660, loss 0.551323.
Train: 2018-08-01T22:54:37.520445: step 661, loss 0.465871.
Train: 2018-08-01T22:54:37.682986: step 662, loss 0.578772.
Train: 2018-08-01T22:54:37.852531: step 663, loss 0.595952.
Train: 2018-08-01T22:54:38.016121: step 664, loss 0.549296.
Train: 2018-08-01T22:54:38.187637: step 665, loss 0.609483.
Train: 2018-08-01T22:54:38.350202: step 666, loss 0.591465.
Train: 2018-08-01T22:54:38.508781: step 667, loss 0.541021.
Train: 2018-08-01T22:54:38.671343: step 668, loss 0.604839.
Train: 2018-08-01T22:54:38.835928: step 669, loss 0.495788.
Train: 2018-08-01T22:54:39.008455: step 670, loss 0.647691.
Test: 2018-08-01T22:54:39.550992: step 670, loss 0.551451.
Train: 2018-08-01T22:54:39.718545: step 671, loss 0.521096.
Train: 2018-08-01T22:54:39.885099: step 672, loss 0.452321.
Train: 2018-08-01T22:54:40.043688: step 673, loss 0.522631.
Train: 2018-08-01T22:54:40.212223: step 674, loss 0.496328.
Train: 2018-08-01T22:54:40.375786: step 675, loss 0.582387.
Train: 2018-08-01T22:54:40.532399: step 676, loss 0.55676.
Train: 2018-08-01T22:54:40.700951: step 677, loss 0.587199.
Train: 2018-08-01T22:54:40.871489: step 678, loss 0.562232.
Train: 2018-08-01T22:54:41.035025: step 679, loss 0.505683.
Train: 2018-08-01T22:54:41.202577: step 680, loss 0.55073.
Test: 2018-08-01T22:54:41.751111: step 680, loss 0.550974.
Train: 2018-08-01T22:54:41.931629: step 681, loss 0.455834.
Train: 2018-08-01T22:54:42.092215: step 682, loss 0.557678.
Train: 2018-08-01T22:54:42.256759: step 683, loss 0.589002.
Train: 2018-08-01T22:54:42.429297: step 684, loss 0.525679.
Train: 2018-08-01T22:54:42.606823: step 685, loss 0.565865.
Train: 2018-08-01T22:54:42.777368: step 686, loss 0.594391.
Train: 2018-08-01T22:54:42.952898: step 687, loss 0.522028.
Train: 2018-08-01T22:54:43.134422: step 688, loss 0.622873.
Train: 2018-08-01T22:54:43.299969: step 689, loss 0.562238.
Train: 2018-08-01T22:54:43.462566: step 690, loss 0.578179.
Test: 2018-08-01T22:54:44.002093: step 690, loss 0.55075.
Train: 2018-08-01T22:54:44.172637: step 691, loss 0.592667.
Train: 2018-08-01T22:54:44.341212: step 692, loss 0.53603.
Train: 2018-08-01T22:54:44.503752: step 693, loss 0.532554.
Train: 2018-08-01T22:54:44.667338: step 694, loss 0.531102.
Train: 2018-08-01T22:54:44.831909: step 695, loss 0.488907.
Train: 2018-08-01T22:54:44.999461: step 696, loss 0.606658.
Train: 2018-08-01T22:54:45.175956: step 697, loss 0.499018.
Train: 2018-08-01T22:54:45.351505: step 698, loss 0.523829.
Train: 2018-08-01T22:54:45.522043: step 699, loss 0.536479.
Train: 2018-08-01T22:54:45.690608: step 700, loss 0.57142.
Test: 2018-08-01T22:54:46.248089: step 700, loss 0.550665.
Train: 2018-08-01T22:54:47.055039: step 701, loss 0.530966.
Train: 2018-08-01T22:54:47.222559: step 702, loss 0.584319.
Train: 2018-08-01T22:54:47.393135: step 703, loss 0.650269.
Train: 2018-08-01T22:54:47.555720: step 704, loss 0.50739.
Train: 2018-08-01T22:54:47.724219: step 705, loss 0.487859.
Train: 2018-08-01T22:54:47.888809: step 706, loss 0.634776.
Train: 2018-08-01T22:54:48.052341: step 707, loss 0.572403.
Train: 2018-08-01T22:54:48.218896: step 708, loss 0.580298.
Train: 2018-08-01T22:54:48.381492: step 709, loss 0.532844.
Train: 2018-08-01T22:54:48.548017: step 710, loss 0.457024.
Test: 2018-08-01T22:54:49.084582: step 710, loss 0.55061.
Train: 2018-08-01T22:54:49.251137: step 711, loss 0.544443.
Train: 2018-08-01T22:54:49.416719: step 712, loss 0.520716.
Train: 2018-08-01T22:54:49.581284: step 713, loss 0.58347.
Train: 2018-08-01T22:54:49.747834: step 714, loss 0.516853.
Train: 2018-08-01T22:54:49.916393: step 715, loss 0.585468.
Train: 2018-08-01T22:54:50.080949: step 716, loss 0.513546.
Train: 2018-08-01T22:54:50.252488: step 717, loss 0.53811.
Train: 2018-08-01T22:54:50.422038: step 718, loss 0.516175.
Train: 2018-08-01T22:54:50.589585: step 719, loss 0.642046.
Train: 2018-08-01T22:54:50.753147: step 720, loss 0.591429.
Test: 2018-08-01T22:54:51.294707: step 720, loss 0.550512.
Train: 2018-08-01T22:54:51.490179: step 721, loss 0.594074.
Train: 2018-08-01T22:54:51.681640: step 722, loss 0.582638.
Train: 2018-08-01T22:54:51.846225: step 723, loss 0.687789.
Train: 2018-08-01T22:54:52.006771: step 724, loss 0.528425.
Train: 2018-08-01T22:54:52.172354: step 725, loss 0.587891.
Train: 2018-08-01T22:54:52.333921: step 726, loss 0.639775.
Train: 2018-08-01T22:54:52.497460: step 727, loss 0.561215.
Train: 2018-08-01T22:54:52.666041: step 728, loss 0.562185.
Train: 2018-08-01T22:54:52.849543: step 729, loss 0.634619.
Train: 2018-08-01T22:54:53.015074: step 730, loss 0.545909.
Test: 2018-08-01T22:54:53.545657: step 730, loss 0.55126.
Train: 2018-08-01T22:54:53.729191: step 731, loss 0.529636.
Train: 2018-08-01T22:54:53.906696: step 732, loss 0.588509.
Train: 2018-08-01T22:54:54.086243: step 733, loss 0.638218.
Train: 2018-08-01T22:54:54.261773: step 734, loss 0.519804.
Train: 2018-08-01T22:54:54.429294: step 735, loss 0.598841.
Train: 2018-08-01T22:54:54.607817: step 736, loss 0.614828.
Train: 2018-08-01T22:54:54.782375: step 737, loss 0.566683.
Train: 2018-08-01T22:54:54.943955: step 738, loss 0.597083.
Train: 2018-08-01T22:54:55.109507: step 739, loss 0.595866.
Train: 2018-08-01T22:54:55.273069: step 740, loss 0.58236.
Test: 2018-08-01T22:54:55.803646: step 740, loss 0.552639.
Train: 2018-08-01T22:54:55.979151: step 741, loss 0.550776.
Train: 2018-08-01T22:54:56.141742: step 742, loss 0.553741.
Train: 2018-08-01T22:54:56.313257: step 743, loss 0.498485.
Train: 2018-08-01T22:54:56.470836: step 744, loss 0.577487.
Train: 2018-08-01T22:54:56.636425: step 745, loss 0.546687.
Train: 2018-08-01T22:54:56.800998: step 746, loss 0.597356.
Train: 2018-08-01T22:54:56.971529: step 747, loss 0.522277.
Train: 2018-08-01T22:54:57.138081: step 748, loss 0.566049.
Train: 2018-08-01T22:54:57.305634: step 749, loss 0.528247.
Train: 2018-08-01T22:54:57.468171: step 750, loss 0.506509.
Test: 2018-08-01T22:54:57.999750: step 750, loss 0.551811.
Train: 2018-08-01T22:54:58.171318: step 751, loss 0.60135.
Train: 2018-08-01T22:54:58.339866: step 752, loss 0.527479.
Train: 2018-08-01T22:54:58.516394: step 753, loss 0.524043.
Train: 2018-08-01T22:54:58.680950: step 754, loss 0.748998.
Train: 2018-08-01T22:54:58.843495: step 755, loss 0.513815.
Train: 2018-08-01T22:54:59.003067: step 756, loss 0.579548.
Train: 2018-08-01T22:54:59.165658: step 757, loss 0.650175.
Train: 2018-08-01T22:54:59.331190: step 758, loss 0.581237.
Train: 2018-08-01T22:54:59.502760: step 759, loss 0.493801.
Train: 2018-08-01T22:54:59.670284: step 760, loss 0.551186.
Test: 2018-08-01T22:55:00.214828: step 760, loss 0.551104.
Train: 2018-08-01T22:55:00.378430: step 761, loss 0.662879.
Train: 2018-08-01T22:55:00.542005: step 762, loss 0.632215.
Train: 2018-08-01T22:55:00.704555: step 763, loss 0.51209.
Train: 2018-08-01T22:55:00.864117: step 764, loss 0.583697.
Train: 2018-08-01T22:55:01.024694: step 765, loss 0.577019.
Train: 2018-08-01T22:55:01.191247: step 766, loss 0.562737.
Train: 2018-08-01T22:55:01.354811: step 767, loss 0.502305.
Train: 2018-08-01T22:55:01.514380: step 768, loss 0.560383.
Train: 2018-08-01T22:55:01.679912: step 769, loss 0.468861.
Train: 2018-08-01T22:55:01.847517: step 770, loss 0.618247.
Test: 2018-08-01T22:55:02.388039: step 770, loss 0.550956.
Train: 2018-08-01T22:55:02.552619: step 771, loss 0.579133.
Train: 2018-08-01T22:55:02.716141: step 772, loss 0.560646.
Train: 2018-08-01T22:55:02.876749: step 773, loss 0.566639.
Train: 2018-08-01T22:55:03.037317: step 774, loss 0.628055.
Train: 2018-08-01T22:55:03.201871: step 775, loss 0.700228.
Train: 2018-08-01T22:55:03.362414: step 776, loss 0.632485.
Train: 2018-08-01T22:55:03.524007: step 777, loss 0.541055.
Train: 2018-08-01T22:55:03.691534: step 778, loss 0.565972.
Train: 2018-08-01T22:55:03.849112: step 779, loss 0.579393.
Train: 2018-08-01T22:55:04.012676: step 780, loss 0.561725.
Test: 2018-08-01T22:55:04.548243: step 780, loss 0.551269.
Train: 2018-08-01T22:55:04.713832: step 781, loss 0.60132.
Train: 2018-08-01T22:55:04.875369: step 782, loss 0.577928.
Train: 2018-08-01T22:55:05.037953: step 783, loss 0.63665.
Train: 2018-08-01T22:55:05.205511: step 784, loss 0.483943.
Train: 2018-08-01T22:55:05.367088: step 785, loss 0.551339.
Train: 2018-08-01T22:55:05.549594: step 786, loss 0.51621.
Train: 2018-08-01T22:55:05.748022: step 787, loss 0.468407.
Train: 2018-08-01T22:55:05.989377: step 788, loss 0.5888.
Train: 2018-08-01T22:55:06.247684: step 789, loss 0.602215.
Train: 2018-08-01T22:55:06.428201: step 790, loss 0.537028.
Test: 2018-08-01T22:55:06.980723: step 790, loss 0.551246.
Train: 2018-08-01T22:55:07.179192: step 791, loss 0.586286.
Train: 2018-08-01T22:55:07.359712: step 792, loss 0.581548.
Train: 2018-08-01T22:55:07.528261: step 793, loss 0.481593.
Train: 2018-08-01T22:55:07.700830: step 794, loss 0.669602.
Train: 2018-08-01T22:55:07.874339: step 795, loss 0.565911.
Train: 2018-08-01T22:55:08.078788: step 796, loss 0.564327.
Train: 2018-08-01T22:55:08.266288: step 797, loss 0.608171.
Train: 2018-08-01T22:55:08.450795: step 798, loss 0.629178.
Train: 2018-08-01T22:55:08.645306: step 799, loss 0.469846.
Train: 2018-08-01T22:55:08.839754: step 800, loss 0.544377.
Test: 2018-08-01T22:55:09.395268: step 800, loss 0.551028.
Train: 2018-08-01T22:55:10.229184: step 801, loss 0.547906.
Train: 2018-08-01T22:55:10.394750: step 802, loss 0.584914.
Train: 2018-08-01T22:55:10.556291: step 803, loss 0.564717.
Train: 2018-08-01T22:55:10.719884: step 804, loss 0.615908.
Train: 2018-08-01T22:55:10.889401: step 805, loss 0.546329.
Train: 2018-08-01T22:55:11.051967: step 806, loss 0.616485.
Train: 2018-08-01T22:55:11.218551: step 807, loss 0.546853.
Train: 2018-08-01T22:55:11.381087: step 808, loss 0.579996.
Train: 2018-08-01T22:55:11.550631: step 809, loss 0.439233.
Train: 2018-08-01T22:55:11.715218: step 810, loss 0.531222.
Test: 2018-08-01T22:55:12.245774: step 810, loss 0.550583.
Train: 2018-08-01T22:55:12.408341: step 811, loss 0.496886.
Train: 2018-08-01T22:55:12.569907: step 812, loss 0.567416.
Train: 2018-08-01T22:55:12.733501: step 813, loss 0.546243.
Train: 2018-08-01T22:55:12.897032: step 814, loss 0.672976.
Train: 2018-08-01T22:55:13.059626: step 815, loss 0.584151.
Train: 2018-08-01T22:55:13.223187: step 816, loss 0.508578.
Train: 2018-08-01T22:55:13.388749: step 817, loss 0.547837.
Train: 2018-08-01T22:55:13.559262: step 818, loss 0.565818.
Train: 2018-08-01T22:55:13.726859: step 819, loss 0.564614.
Train: 2018-08-01T22:55:13.894400: step 820, loss 0.524369.
Test: 2018-08-01T22:55:14.426944: step 820, loss 0.550038.
Train: 2018-08-01T22:55:14.602474: step 821, loss 0.496224.
Train: 2018-08-01T22:55:14.773018: step 822, loss 0.527458.
Train: 2018-08-01T22:55:14.939572: step 823, loss 0.549325.
Train: 2018-08-01T22:55:15.115104: step 824, loss 0.601445.
Train: 2018-08-01T22:55:15.300607: step 825, loss 0.545582.
Train: 2018-08-01T22:55:15.461179: step 826, loss 0.468853.
Train: 2018-08-01T22:55:15.645687: step 827, loss 0.717397.
Train: 2018-08-01T22:55:15.818224: step 828, loss 0.545171.
Train: 2018-08-01T22:55:15.979802: step 829, loss 0.566156.
Train: 2018-08-01T22:55:16.144376: step 830, loss 0.534302.
Test: 2018-08-01T22:55:16.698873: step 830, loss 0.549816.
Train: 2018-08-01T22:55:16.875407: step 831, loss 0.643129.
Train: 2018-08-01T22:55:17.052924: step 832, loss 0.598574.
Train: 2018-08-01T22:55:17.219507: step 833, loss 0.606016.
Train: 2018-08-01T22:55:17.382068: step 834, loss 0.73729.
Train: 2018-08-01T22:55:17.547625: step 835, loss 0.495115.
Train: 2018-08-01T22:55:17.710192: step 836, loss 0.585154.
Train: 2018-08-01T22:55:17.877718: step 837, loss 0.552065.
Train: 2018-08-01T22:55:18.040284: step 838, loss 0.444004.
Train: 2018-08-01T22:55:18.213821: step 839, loss 0.601313.
Train: 2018-08-01T22:55:18.374390: step 840, loss 0.584335.
Test: 2018-08-01T22:55:18.920962: step 840, loss 0.550511.
Train: 2018-08-01T22:55:19.083523: step 841, loss 0.580453.
Train: 2018-08-01T22:55:19.261020: step 842, loss 0.626333.
Train: 2018-08-01T22:55:19.420619: step 843, loss 0.548643.
Train: 2018-08-01T22:55:19.582163: step 844, loss 0.586102.
Train: 2018-08-01T22:55:19.752706: step 845, loss 0.563024.
Train: 2018-08-01T22:55:19.918264: step 846, loss 0.533358.
Train: 2018-08-01T22:55:20.077847: step 847, loss 0.594605.
Train: 2018-08-01T22:55:20.246393: step 848, loss 0.580072.
Train: 2018-08-01T22:55:20.408953: step 849, loss 0.517736.
Train: 2018-08-01T22:55:20.579495: step 850, loss 0.546721.
Test: 2018-08-01T22:55:21.120066: step 850, loss 0.551176.
Train: 2018-08-01T22:55:21.288600: step 851, loss 0.569939.
Train: 2018-08-01T22:55:21.459143: step 852, loss 0.526136.
Train: 2018-08-01T22:55:21.626696: step 853, loss 0.55152.
Train: 2018-08-01T22:55:21.790259: step 854, loss 0.551264.
Train: 2018-08-01T22:55:21.950863: step 855, loss 0.528327.
Train: 2018-08-01T22:55:22.110428: step 856, loss 0.58245.
Train: 2018-08-01T22:55:22.274993: step 857, loss 0.561145.
Train: 2018-08-01T22:55:22.439557: step 858, loss 0.731834.
Train: 2018-08-01T22:55:22.598130: step 859, loss 0.669204.
Train: 2018-08-01T22:55:22.762685: step 860, loss 0.518286.
Test: 2018-08-01T22:55:23.305210: step 860, loss 0.550802.
Train: 2018-08-01T22:55:23.469768: step 861, loss 0.583087.
Train: 2018-08-01T22:55:23.642336: step 862, loss 0.532773.
Train: 2018-08-01T22:55:23.804900: step 863, loss 0.583315.
Train: 2018-08-01T22:55:23.979407: step 864, loss 0.431313.
Train: 2018-08-01T22:55:24.140974: step 865, loss 0.597336.
Train: 2018-08-01T22:55:24.306532: step 866, loss 0.562614.
Train: 2018-08-01T22:55:24.474136: step 867, loss 0.516253.
Train: 2018-08-01T22:55:24.634681: step 868, loss 0.548999.
Train: 2018-08-01T22:55:24.794260: step 869, loss 0.653418.
Train: 2018-08-01T22:55:24.955825: step 870, loss 0.647278.
Test: 2018-08-01T22:55:25.481422: step 870, loss 0.550612.
Train: 2018-08-01T22:55:25.643984: step 871, loss 0.630203.
Train: 2018-08-01T22:55:25.811509: step 872, loss 0.564405.
Train: 2018-08-01T22:55:25.975072: step 873, loss 0.505575.
Train: 2018-08-01T22:55:26.135643: step 874, loss 0.465931.
Train: 2018-08-01T22:55:26.299207: step 875, loss 0.559187.
Train: 2018-08-01T22:55:26.462794: step 876, loss 0.533774.
Train: 2018-08-01T22:55:26.621375: step 877, loss 0.565437.
Train: 2018-08-01T22:55:26.785906: step 878, loss 0.681427.
Train: 2018-08-01T22:55:26.949492: step 879, loss 0.581926.
Train: 2018-08-01T22:55:27.121010: step 880, loss 0.534088.
Test: 2018-08-01T22:55:27.654595: step 880, loss 0.55055.
Train: 2018-08-01T22:55:27.818175: step 881, loss 0.563971.
Train: 2018-08-01T22:55:27.980711: step 882, loss 0.679525.
Train: 2018-08-01T22:55:28.142306: step 883, loss 0.562948.
Train: 2018-08-01T22:55:28.304874: step 884, loss 0.596657.
Train: 2018-08-01T22:55:28.467410: step 885, loss 0.497548.
Train: 2018-08-01T22:55:28.631995: step 886, loss 0.533389.
Train: 2018-08-01T22:55:28.794565: step 887, loss 0.565565.
Train: 2018-08-01T22:55:28.958123: step 888, loss 0.529656.
Train: 2018-08-01T22:55:29.119692: step 889, loss 0.567843.
Train: 2018-08-01T22:55:29.279270: step 890, loss 0.532823.
Test: 2018-08-01T22:55:29.818797: step 890, loss 0.550436.
Train: 2018-08-01T22:55:29.978401: step 891, loss 0.56784.
Train: 2018-08-01T22:55:30.140961: step 892, loss 0.596547.
Train: 2018-08-01T22:55:30.309484: step 893, loss 0.464766.
Train: 2018-08-01T22:55:30.475060: step 894, loss 0.615594.
Train: 2018-08-01T22:55:30.638633: step 895, loss 0.584897.
Train: 2018-08-01T22:55:30.801196: step 896, loss 0.565024.
Train: 2018-08-01T22:55:30.959748: step 897, loss 0.448943.
Train: 2018-08-01T22:55:31.124333: step 898, loss 0.512748.
Train: 2018-08-01T22:55:31.293853: step 899, loss 0.55004.
Train: 2018-08-01T22:55:31.461406: step 900, loss 0.578162.
Test: 2018-08-01T22:55:31.995976: step 900, loss 0.549808.
Train: 2018-08-01T22:55:32.811156: step 901, loss 0.628623.
Train: 2018-08-01T22:55:32.970725: step 902, loss 0.51414.
Train: 2018-08-01T22:55:33.147224: step 903, loss 0.66008.
Train: 2018-08-01T22:55:33.319781: step 904, loss 0.463972.
Train: 2018-08-01T22:55:33.482364: step 905, loss 0.563584.
Train: 2018-08-01T22:55:33.644895: step 906, loss 0.562488.
Train: 2018-08-01T22:55:33.804468: step 907, loss 0.493754.
Train: 2018-08-01T22:55:33.971022: step 908, loss 0.581299.
Train: 2018-08-01T22:55:34.138574: step 909, loss 0.581972.
Train: 2018-08-01T22:55:34.307125: step 910, loss 0.530341.
Test: 2018-08-01T22:55:34.841725: step 910, loss 0.549521.
Train: 2018-08-01T22:55:35.004261: step 911, loss 0.566252.
Train: 2018-08-01T22:55:35.169818: step 912, loss 0.623087.
Train: 2018-08-01T22:55:35.335406: step 913, loss 0.685284.
Train: 2018-08-01T22:55:35.498937: step 914, loss 0.494854.
Train: 2018-08-01T22:55:35.658512: step 915, loss 0.562363.
Train: 2018-08-01T22:55:35.821077: step 916, loss 0.546614.
Train: 2018-08-01T22:55:35.986668: step 917, loss 0.673213.
Train: 2018-08-01T22:55:36.152222: step 918, loss 0.557746.
Train: 2018-08-01T22:55:36.320772: step 919, loss 0.544175.
Train: 2018-08-01T22:55:36.479344: step 920, loss 0.545622.
Test: 2018-08-01T22:55:37.021867: step 920, loss 0.54968.
Train: 2018-08-01T22:55:37.185429: step 921, loss 0.460485.
Train: 2018-08-01T22:55:37.349989: step 922, loss 0.584022.
Train: 2018-08-01T22:55:37.519537: step 923, loss 0.511413.
Train: 2018-08-01T22:55:37.684097: step 924, loss 0.567826.
Train: 2018-08-01T22:55:37.844666: step 925, loss 0.531072.
Train: 2018-08-01T22:55:38.008261: step 926, loss 0.674261.
Train: 2018-08-01T22:55:38.178776: step 927, loss 0.631687.
Train: 2018-08-01T22:55:38.346326: step 928, loss 0.456395.
Train: 2018-08-01T22:55:38.515873: step 929, loss 0.597986.
Train: 2018-08-01T22:55:38.680433: step 930, loss 0.530218.
Test: 2018-08-01T22:55:39.222983: step 930, loss 0.549634.
Train: 2018-08-01T22:55:39.391532: step 931, loss 0.544042.
Train: 2018-08-01T22:55:39.557091: step 932, loss 0.389986.
Train: 2018-08-01T22:55:39.728631: step 933, loss 0.584362.
Train: 2018-08-01T22:55:39.890225: step 934, loss 0.608989.
Train: 2018-08-01T22:55:40.051812: step 935, loss 0.492225.
Train: 2018-08-01T22:55:40.216327: step 936, loss 0.52525.
Train: 2018-08-01T22:55:40.372940: step 937, loss 0.597686.
Train: 2018-08-01T22:55:40.534478: step 938, loss 0.474592.
Train: 2018-08-01T22:55:40.705021: step 939, loss 0.636044.
Train: 2018-08-01T22:55:40.876562: step 940, loss 0.565933.
Test: 2018-08-01T22:55:41.406146: step 940, loss 0.549349.
Train: 2018-08-01T22:55:41.573723: step 941, loss 0.529309.
Train: 2018-08-01T22:55:41.740279: step 942, loss 0.506408.
Train: 2018-08-01T22:55:41.910797: step 943, loss 0.548015.
Train: 2018-08-01T22:55:42.082338: step 944, loss 0.541426.
Train: 2018-08-01T22:55:42.253880: step 945, loss 0.582471.
Train: 2018-08-01T22:55:42.417468: step 946, loss 0.549284.
Train: 2018-08-01T22:55:42.578040: step 947, loss 0.619272.
Train: 2018-08-01T22:55:42.754542: step 948, loss 0.633811.
Train: 2018-08-01T22:55:42.916141: step 949, loss 0.475727.
Train: 2018-08-01T22:55:43.073719: step 950, loss 0.50959.
Test: 2018-08-01T22:55:43.613246: step 950, loss 0.549277.
Train: 2018-08-01T22:55:43.778832: step 951, loss 0.598391.
Train: 2018-08-01T22:55:43.942367: step 952, loss 0.584177.
Train: 2018-08-01T22:55:44.100949: step 953, loss 0.565879.
Train: 2018-08-01T22:55:44.263533: step 954, loss 0.511341.
Train: 2018-08-01T22:55:44.421117: step 955, loss 0.506098.
Train: 2018-08-01T22:55:44.581657: step 956, loss 0.47189.
Train: 2018-08-01T22:55:44.743225: step 957, loss 0.45709.
Train: 2018-08-01T22:55:44.909781: step 958, loss 0.470098.
Train: 2018-08-01T22:55:45.073374: step 959, loss 0.548862.
Train: 2018-08-01T22:55:45.244910: step 960, loss 0.544948.
Test: 2018-08-01T22:55:45.782447: step 960, loss 0.549301.
Train: 2018-08-01T22:55:45.946022: step 961, loss 0.453882.
Train: 2018-08-01T22:55:46.109573: step 962, loss 0.529822.
Train: 2018-08-01T22:55:46.281114: step 963, loss 0.546299.
Train: 2018-08-01T22:55:46.439715: step 964, loss 0.704572.
Train: 2018-08-01T22:55:46.604251: step 965, loss 0.503916.
Train: 2018-08-01T22:55:46.769807: step 966, loss 0.502008.
Train: 2018-08-01T22:55:46.930379: step 967, loss 0.665874.
Train: 2018-08-01T22:55:47.098953: step 968, loss 0.582415.
Train: 2018-08-01T22:55:47.272464: step 969, loss 0.548671.
Train: 2018-08-01T22:55:47.430068: step 970, loss 0.54479.
Test: 2018-08-01T22:55:47.975585: step 970, loss 0.549433.
Train: 2018-08-01T22:55:48.138153: step 971, loss 0.546183.
Train: 2018-08-01T22:55:48.299718: step 972, loss 0.54523.
Train: 2018-08-01T22:55:48.473253: step 973, loss 0.569186.
Train: 2018-08-01T22:55:48.635852: step 974, loss 0.605578.
Train: 2018-08-01T22:55:48.798386: step 975, loss 0.748638.
Train: 2018-08-01T22:55:48.963969: step 976, loss 0.564966.
Train: 2018-08-01T22:55:49.127516: step 977, loss 0.543471.
Train: 2018-08-01T22:55:49.294061: step 978, loss 0.579171.
Train: 2018-08-01T22:55:49.462610: step 979, loss 0.559392.
Train: 2018-08-01T22:55:49.628167: step 980, loss 0.652017.
Test: 2018-08-01T22:55:50.180697: step 980, loss 0.549328.
Train: 2018-08-01T22:55:50.341261: step 981, loss 0.513827.
Train: 2018-08-01T22:55:50.507815: step 982, loss 0.560816.
Train: 2018-08-01T22:55:50.666418: step 983, loss 0.527826.
Train: 2018-08-01T22:55:50.829955: step 984, loss 0.577486.
Train: 2018-08-01T22:55:50.996509: step 985, loss 0.529895.
Train: 2018-08-01T22:55:51.159074: step 986, loss 0.584002.
Train: 2018-08-01T22:55:51.321641: step 987, loss 0.634326.
Train: 2018-08-01T22:55:51.483207: step 988, loss 0.473578.
Train: 2018-08-01T22:55:51.647793: step 989, loss 0.565689.
Train: 2018-08-01T22:55:51.815352: step 990, loss 0.711538.
Test: 2018-08-01T22:55:52.365849: step 990, loss 0.54997.
Train: 2018-08-01T22:55:52.530409: step 991, loss 0.581184.
Train: 2018-08-01T22:55:52.694005: step 992, loss 0.652185.
Train: 2018-08-01T22:55:52.861548: step 993, loss 0.467035.
Train: 2018-08-01T22:55:53.032067: step 994, loss 0.562166.
Train: 2018-08-01T22:55:53.198624: step 995, loss 0.514162.
Train: 2018-08-01T22:55:53.361216: step 996, loss 0.530531.
Train: 2018-08-01T22:55:53.519788: step 997, loss 0.545013.
Train: 2018-08-01T22:55:53.680360: step 998, loss 0.465029.
Train: 2018-08-01T22:55:53.844920: step 999, loss 0.600468.
Train: 2018-08-01T22:55:54.007459: step 1000, loss 0.597348.
Test: 2018-08-01T22:55:54.546051: step 1000, loss 0.550102.
Train: 2018-08-01T22:55:55.331466: step 1001, loss 0.631382.
Train: 2018-08-01T22:55:55.515202: step 1002, loss 0.481059.
Train: 2018-08-01T22:55:55.686775: step 1003, loss 0.750218.
Train: 2018-08-01T22:55:55.857288: step 1004, loss 0.497571.
Train: 2018-08-01T22:55:56.030824: step 1005, loss 0.616159.
Train: 2018-08-01T22:55:56.196412: step 1006, loss 0.597967.
Train: 2018-08-01T22:55:56.365928: step 1007, loss 0.549005.
Train: 2018-08-01T22:55:56.522536: step 1008, loss 0.629515.
Train: 2018-08-01T22:55:56.692057: step 1009, loss 0.545854.
Train: 2018-08-01T22:55:56.856618: step 1010, loss 0.597731.
Test: 2018-08-01T22:55:57.396175: step 1010, loss 0.550531.
Train: 2018-08-01T22:55:57.587662: step 1011, loss 0.581517.
Train: 2018-08-01T22:55:57.752223: step 1012, loss 0.613709.
Train: 2018-08-01T22:55:57.916782: step 1013, loss 0.544921.
Train: 2018-08-01T22:55:58.082339: step 1014, loss 0.628488.
Train: 2018-08-01T22:55:58.256904: step 1015, loss 0.595598.
Train: 2018-08-01T22:55:58.424426: step 1016, loss 0.551884.
Train: 2018-08-01T22:55:58.590979: step 1017, loss 0.58047.
Train: 2018-08-01T22:55:58.751576: step 1018, loss 0.502253.
Train: 2018-08-01T22:55:58.921099: step 1019, loss 0.580394.
Train: 2018-08-01T22:55:59.088674: step 1020, loss 0.550018.
Test: 2018-08-01T22:55:59.618234: step 1020, loss 0.551096.
Train: 2018-08-01T22:55:59.788809: step 1021, loss 0.548105.
Train: 2018-08-01T22:55:59.961341: step 1022, loss 0.51847.
Train: 2018-08-01T22:56:00.137845: step 1023, loss 0.501915.
Train: 2018-08-01T22:56:00.305422: step 1024, loss 0.673385.
Train: 2018-08-01T22:56:00.471951: step 1025, loss 0.46646.
Train: 2018-08-01T22:56:00.639505: step 1026, loss 0.53368.
Train: 2018-08-01T22:56:00.814042: step 1027, loss 0.550014.
Train: 2018-08-01T22:56:00.977601: step 1028, loss 0.447796.
Train: 2018-08-01T22:56:01.149173: step 1029, loss 0.51582.
Train: 2018-08-01T22:56:01.313735: step 1030, loss 0.513513.
Test: 2018-08-01T22:56:01.872208: step 1030, loss 0.549617.
Train: 2018-08-01T22:56:02.049759: step 1031, loss 0.514127.
Train: 2018-08-01T22:56:02.229255: step 1032, loss 0.597197.
Train: 2018-08-01T22:56:02.395810: step 1033, loss 0.58185.
Train: 2018-08-01T22:56:02.558373: step 1034, loss 0.58448.
Train: 2018-08-01T22:56:02.724954: step 1035, loss 0.457692.
Train: 2018-08-01T22:56:02.889490: step 1036, loss 0.589668.
Train: 2018-08-01T22:56:03.068012: step 1037, loss 0.511287.
Train: 2018-08-01T22:56:03.238559: step 1038, loss 0.565855.
Train: 2018-08-01T22:56:03.411094: step 1039, loss 0.528699.
Train: 2018-08-01T22:56:03.579643: step 1040, loss 0.488826.
Test: 2018-08-01T22:56:04.116211: step 1040, loss 0.548976.
Train: 2018-08-01T22:56:04.273813: step 1041, loss 0.549344.
Train: 2018-08-01T22:56:04.451339: step 1042, loss 0.708223.
Train: 2018-08-01T22:56:04.614878: step 1043, loss 0.511825.
Train: 2018-08-01T22:56:04.776444: step 1044, loss 0.508467.
Train: 2018-08-01T22:56:04.937031: step 1045, loss 0.471052.
Train: 2018-08-01T22:56:05.102603: step 1046, loss 0.60054.
Train: 2018-08-01T22:56:05.272122: step 1047, loss 0.577677.
Train: 2018-08-01T22:56:05.442665: step 1048, loss 0.549957.
Train: 2018-08-01T22:56:05.614204: step 1049, loss 0.595964.
Train: 2018-08-01T22:56:05.782755: step 1050, loss 0.489951.
Test: 2018-08-01T22:56:06.312346: step 1050, loss 0.548992.
Train: 2018-08-01T22:56:06.482883: step 1051, loss 0.451869.
Train: 2018-08-01T22:56:06.654425: step 1052, loss 0.593266.
Train: 2018-08-01T22:56:06.828957: step 1053, loss 0.492899.
Train: 2018-08-01T22:56:06.988531: step 1054, loss 0.634217.
Train: 2018-08-01T22:56:07.154089: step 1055, loss 0.581262.
Train: 2018-08-01T22:56:07.322639: step 1056, loss 0.544918.
Train: 2018-08-01T22:56:07.491188: step 1057, loss 0.734475.
Train: 2018-08-01T22:56:07.658765: step 1058, loss 0.512275.
Train: 2018-08-01T22:56:07.820308: step 1059, loss 0.489946.
Train: 2018-08-01T22:56:07.996870: step 1060, loss 0.606459.
Test: 2018-08-01T22:56:08.533402: step 1060, loss 0.548944.
Train: 2018-08-01T22:56:08.704974: step 1061, loss 0.49134.
Train: 2018-08-01T22:56:08.867533: step 1062, loss 0.549976.
Train: 2018-08-01T22:56:09.039049: step 1063, loss 0.533401.
Train: 2018-08-01T22:56:09.200642: step 1064, loss 0.569877.
Train: 2018-08-01T22:56:09.372185: step 1065, loss 0.525244.
Train: 2018-08-01T22:56:09.532758: step 1066, loss 0.474735.
Train: 2018-08-01T22:56:09.698288: step 1067, loss 0.563446.
Train: 2018-08-01T22:56:09.857891: step 1068, loss 0.618842.
Train: 2018-08-01T22:56:10.034389: step 1069, loss 0.671215.
Train: 2018-08-01T22:56:10.204933: step 1070, loss 0.569176.
Test: 2018-08-01T22:56:10.748511: step 1070, loss 0.54892.
Train: 2018-08-01T22:56:10.915036: step 1071, loss 0.600181.
Train: 2018-08-01T22:56:11.106523: step 1072, loss 0.549478.
Train: 2018-08-01T22:56:11.288071: step 1073, loss 0.490364.
Train: 2018-08-01T22:56:11.481520: step 1074, loss 0.632452.
Train: 2018-08-01T22:56:11.680022: step 1075, loss 0.617688.
Train: 2018-08-01T22:56:11.865520: step 1076, loss 0.604119.
Train: 2018-08-01T22:56:12.029086: step 1077, loss 0.600803.
Train: 2018-08-01T22:56:12.217553: step 1078, loss 0.563766.
Train: 2018-08-01T22:56:12.387100: step 1079, loss 0.516226.
Train: 2018-08-01T22:56:12.665360: step 1080, loss 0.526919.
Test: 2018-08-01T22:56:13.278717: step 1080, loss 0.549548.
Train: 2018-08-01T22:56:13.515087: step 1081, loss 0.495642.
Train: 2018-08-01T22:56:13.708567: step 1082, loss 0.512138.
Train: 2018-08-01T22:56:13.893105: step 1083, loss 0.511524.
Train: 2018-08-01T22:56:14.056661: step 1084, loss 0.56255.
Train: 2018-08-01T22:56:14.241144: step 1085, loss 0.545787.
Train: 2018-08-01T22:56:14.408696: step 1086, loss 0.615851.
Train: 2018-08-01T22:56:14.589244: step 1087, loss 0.616642.
Train: 2018-08-01T22:56:14.758784: step 1088, loss 0.544242.
Train: 2018-08-01T22:56:14.923319: step 1089, loss 0.614236.
Train: 2018-08-01T22:56:15.087879: step 1090, loss 0.565546.
Test: 2018-08-01T22:56:15.630430: step 1090, loss 0.549484.
Train: 2018-08-01T22:56:15.797016: step 1091, loss 0.66673.
Train: 2018-08-01T22:56:15.968525: step 1092, loss 0.613583.
Train: 2018-08-01T22:56:16.163005: step 1093, loss 0.531135.
Train: 2018-08-01T22:56:16.352500: step 1094, loss 0.533155.
Train: 2018-08-01T22:56:16.533047: step 1095, loss 0.665615.
Train: 2018-08-01T22:56:16.698576: step 1096, loss 0.648499.
Train: 2018-08-01T22:56:16.857205: step 1097, loss 0.565023.
Train: 2018-08-01T22:56:17.017751: step 1098, loss 0.447302.
Train: 2018-08-01T22:56:17.183307: step 1099, loss 0.529407.
Train: 2018-08-01T22:56:17.354819: step 1100, loss 0.548194.
Test: 2018-08-01T22:56:17.885458: step 1100, loss 0.550158.
Train: 2018-08-01T22:56:18.650139: step 1101, loss 0.679509.
Train: 2018-08-01T22:56:18.825696: step 1102, loss 0.498349.
Train: 2018-08-01T22:56:18.988236: step 1103, loss 0.563247.
Train: 2018-08-01T22:56:19.149834: step 1104, loss 0.562082.
Train: 2018-08-01T22:56:19.313400: step 1105, loss 0.59768.
Train: 2018-08-01T22:56:19.478924: step 1106, loss 0.578119.
Train: 2018-08-01T22:56:19.646477: step 1107, loss 0.562901.
Train: 2018-08-01T22:56:19.810039: step 1108, loss 0.545557.
Train: 2018-08-01T22:56:19.969636: step 1109, loss 0.679549.
Train: 2018-08-01T22:56:20.130198: step 1110, loss 0.528633.
Test: 2018-08-01T22:56:20.672733: step 1110, loss 0.550178.
Train: 2018-08-01T22:56:20.836296: step 1111, loss 0.56566.
Train: 2018-08-01T22:56:21.005842: step 1112, loss 0.561345.
Train: 2018-08-01T22:56:21.170403: step 1113, loss 0.597838.
Train: 2018-08-01T22:56:21.337955: step 1114, loss 0.598296.
Train: 2018-08-01T22:56:21.502515: step 1115, loss 0.578767.
Train: 2018-08-01T22:56:21.671065: step 1116, loss 0.663557.
Train: 2018-08-01T22:56:21.837619: step 1117, loss 0.548483.
Train: 2018-08-01T22:56:22.000218: step 1118, loss 0.564091.
Train: 2018-08-01T22:56:22.163778: step 1119, loss 0.595303.
Train: 2018-08-01T22:56:22.339278: step 1120, loss 0.56373.
Test: 2018-08-01T22:56:22.872851: step 1120, loss 0.550479.
Train: 2018-08-01T22:56:23.033422: step 1121, loss 0.530835.
Train: 2018-08-01T22:56:23.201002: step 1122, loss 0.550291.
Train: 2018-08-01T22:56:23.367553: step 1123, loss 0.468362.
Train: 2018-08-01T22:56:23.533086: step 1124, loss 0.547031.
Train: 2018-08-01T22:56:23.694653: step 1125, loss 0.564184.
Train: 2018-08-01T22:56:23.861238: step 1126, loss 0.629613.
Train: 2018-08-01T22:56:24.022795: step 1127, loss 0.565612.
Train: 2018-08-01T22:56:24.183381: step 1128, loss 0.515421.
Train: 2018-08-01T22:56:24.352894: step 1129, loss 0.597404.
Train: 2018-08-01T22:56:24.516457: step 1130, loss 0.696739.
Test: 2018-08-01T22:56:25.059006: step 1130, loss 0.550038.
Train: 2018-08-01T22:56:25.222570: step 1131, loss 0.496014.
Train: 2018-08-01T22:56:25.395109: step 1132, loss 0.627125.
Train: 2018-08-01T22:56:25.554682: step 1133, loss 0.562529.
Train: 2018-08-01T22:56:25.720239: step 1134, loss 0.514493.
Train: 2018-08-01T22:56:25.882805: step 1135, loss 0.615488.
Train: 2018-08-01T22:56:26.050357: step 1136, loss 0.496947.
Train: 2018-08-01T22:56:26.210959: step 1137, loss 0.578788.
Train: 2018-08-01T22:56:26.373493: step 1138, loss 0.62908.
Train: 2018-08-01T22:56:26.543072: step 1139, loss 0.581756.
Train: 2018-08-01T22:56:26.704608: step 1140, loss 0.546351.
Test: 2018-08-01T22:56:27.244179: step 1140, loss 0.549884.
Train: 2018-08-01T22:56:27.413713: step 1141, loss 0.529439.
Train: 2018-08-01T22:56:27.581290: step 1142, loss 0.565671.
Train: 2018-08-01T22:56:27.747818: step 1143, loss 0.598386.
Train: 2018-08-01T22:56:27.904400: step 1144, loss 0.629176.
Train: 2018-08-01T22:56:28.065002: step 1145, loss 0.464161.
Train: 2018-08-01T22:56:28.230530: step 1146, loss 0.514113.
Train: 2018-08-01T22:56:28.396087: step 1147, loss 0.663405.
Train: 2018-08-01T22:56:28.558681: step 1148, loss 0.466057.
Train: 2018-08-01T22:56:28.731190: step 1149, loss 0.548945.
Train: 2018-08-01T22:56:28.906722: step 1150, loss 0.580231.
Test: 2018-08-01T22:56:29.442293: step 1150, loss 0.549525.
Train: 2018-08-01T22:56:29.603858: step 1151, loss 0.49592.
Train: 2018-08-01T22:56:29.769446: step 1152, loss 0.530129.
Train: 2018-08-01T22:56:29.936993: step 1153, loss 0.614023.
Train: 2018-08-01T22:56:30.111502: step 1154, loss 0.631113.
Train: 2018-08-01T22:56:30.278055: step 1155, loss 0.512459.
Train: 2018-08-01T22:56:30.438654: step 1156, loss 0.494825.
Train: 2018-08-01T22:56:30.599227: step 1157, loss 0.509768.
Train: 2018-08-01T22:56:30.761762: step 1158, loss 0.494095.
Train: 2018-08-01T22:56:30.925326: step 1159, loss 0.598735.
Train: 2018-08-01T22:56:31.083907: step 1160, loss 0.476042.
Test: 2018-08-01T22:56:31.608498: step 1160, loss 0.548894.
Train: 2018-08-01T22:56:31.769071: step 1161, loss 0.525786.
Train: 2018-08-01T22:56:31.928643: step 1162, loss 0.634162.
Train: 2018-08-01T22:56:32.097194: step 1163, loss 0.528607.
Train: 2018-08-01T22:56:32.266763: step 1164, loss 0.492162.
Train: 2018-08-01T22:56:32.433322: step 1165, loss 0.528718.
Train: 2018-08-01T22:56:32.595888: step 1166, loss 0.619507.
Train: 2018-08-01T22:56:32.765407: step 1167, loss 0.45456.
Train: 2018-08-01T22:56:32.929965: step 1168, loss 0.579312.
Train: 2018-08-01T22:56:33.097549: step 1169, loss 0.674111.
Train: 2018-08-01T22:56:33.263103: step 1170, loss 0.542838.
Test: 2018-08-01T22:56:33.800673: step 1170, loss 0.548651.
Train: 2018-08-01T22:56:33.977166: step 1171, loss 0.577698.
Train: 2018-08-01T22:56:34.143746: step 1172, loss 0.543838.
Train: 2018-08-01T22:56:34.303294: step 1173, loss 0.641293.
Train: 2018-08-01T22:56:34.464863: step 1174, loss 0.653878.
Train: 2018-08-01T22:56:34.633412: step 1175, loss 0.624157.
Train: 2018-08-01T22:56:34.798970: step 1176, loss 0.54594.
Train: 2018-08-01T22:56:34.958543: step 1177, loss 0.495208.
Train: 2018-08-01T22:56:35.135104: step 1178, loss 0.58264.
Train: 2018-08-01T22:56:35.297665: step 1179, loss 0.633847.
Train: 2018-08-01T22:56:35.463194: step 1180, loss 0.65261.
Test: 2018-08-01T22:56:36.009757: step 1180, loss 0.54884.
Train: 2018-08-01T22:56:36.172324: step 1181, loss 0.544886.
Train: 2018-08-01T22:56:36.337857: step 1182, loss 0.634325.
Train: 2018-08-01T22:56:36.496463: step 1183, loss 0.610514.
Train: 2018-08-01T22:56:36.655008: step 1184, loss 0.563071.
Train: 2018-08-01T22:56:36.817575: step 1185, loss 0.627926.
Train: 2018-08-01T22:56:36.983131: step 1186, loss 0.544644.
Train: 2018-08-01T22:56:37.147690: step 1187, loss 0.55894.
Train: 2018-08-01T22:56:37.314245: step 1188, loss 0.562419.
Train: 2018-08-01T22:56:37.490785: step 1189, loss 0.597699.
Train: 2018-08-01T22:56:37.666306: step 1190, loss 0.528518.
Test: 2018-08-01T22:56:38.217830: step 1190, loss 0.549765.
Train: 2018-08-01T22:56:38.379398: step 1191, loss 0.564745.
Train: 2018-08-01T22:56:38.543989: step 1192, loss 0.763968.
Train: 2018-08-01T22:56:38.708544: step 1193, loss 0.562254.
Train: 2018-08-01T22:56:38.872110: step 1194, loss 0.595605.
Train: 2018-08-01T22:56:39.033680: step 1195, loss 0.613778.
Train: 2018-08-01T22:56:39.201201: step 1196, loss 0.609565.
Train: 2018-08-01T22:56:39.364795: step 1197, loss 0.579326.
Train: 2018-08-01T22:56:39.530348: step 1198, loss 0.501597.
Train: 2018-08-01T22:56:39.708875: step 1199, loss 0.514157.
Train: 2018-08-01T22:56:39.874427: step 1200, loss 0.533303.
Test: 2018-08-01T22:56:40.422935: step 1200, loss 0.550905.
Train: 2018-08-01T22:56:41.189728: step 1201, loss 0.641232.
Train: 2018-08-01T22:56:41.355262: step 1202, loss 0.516096.
Train: 2018-08-01T22:56:41.517827: step 1203, loss 0.579273.
Train: 2018-08-01T22:56:41.678396: step 1204, loss 0.596071.
Train: 2018-08-01T22:56:41.852946: step 1205, loss 0.69116.
Train: 2018-08-01T22:56:42.023475: step 1206, loss 0.564033.
Train: 2018-08-01T22:56:42.281784: step 1207, loss 0.611755.
Train: 2018-08-01T22:56:42.565027: step 1208, loss 0.564323.
Train: 2018-08-01T22:56:42.762499: step 1209, loss 0.597882.
Train: 2018-08-01T22:56:42.948999: step 1210, loss 0.56377.
Test: 2018-08-01T22:56:43.496537: step 1210, loss 0.55121.
Train: 2018-08-01T22:56:43.799727: step 1211, loss 0.502388.
Train: 2018-08-01T22:56:44.064038: step 1212, loss 0.577337.
Train: 2018-08-01T22:56:44.297413: step 1213, loss 0.608417.
Train: 2018-08-01T22:56:44.470950: step 1214, loss 0.615167.
Train: 2018-08-01T22:56:44.631520: step 1215, loss 0.547443.
Train: 2018-08-01T22:56:44.801067: step 1216, loss 0.502997.
Train: 2018-08-01T22:56:44.969618: step 1217, loss 0.585114.
Train: 2018-08-01T22:56:45.146145: step 1218, loss 0.607431.
Train: 2018-08-01T22:56:45.320678: step 1219, loss 0.566875.
Train: 2018-08-01T22:56:45.538097: step 1220, loss 0.658916.
Test: 2018-08-01T22:56:46.090621: step 1220, loss 0.55097.
Train: 2018-08-01T22:56:46.263158: step 1221, loss 0.534959.
Train: 2018-08-01T22:56:46.436695: step 1222, loss 0.675786.
Train: 2018-08-01T22:56:46.615219: step 1223, loss 0.561191.
Train: 2018-08-01T22:56:46.807703: step 1224, loss 0.627687.
Train: 2018-08-01T22:56:47.004177: step 1225, loss 0.561191.
Train: 2018-08-01T22:56:47.199659: step 1226, loss 0.561017.
Train: 2018-08-01T22:56:47.403112: step 1227, loss 0.547513.
Train: 2018-08-01T22:56:47.591607: step 1228, loss 0.594733.
Train: 2018-08-01T22:56:47.776113: step 1229, loss 0.674583.
Train: 2018-08-01T22:56:47.970595: step 1230, loss 0.548655.
Test: 2018-08-01T22:56:48.507160: step 1230, loss 0.551293.
Train: 2018-08-01T22:56:48.724579: step 1231, loss 0.582843.
Train: 2018-08-01T22:56:48.926040: step 1232, loss 0.646011.
Train: 2018-08-01T22:56:49.095587: step 1233, loss 0.546921.
Train: 2018-08-01T22:56:49.264136: step 1234, loss 0.534034.
Train: 2018-08-01T22:56:49.441661: step 1235, loss 0.441352.
Train: 2018-08-01T22:56:49.624173: step 1236, loss 0.554395.
Train: 2018-08-01T22:56:49.792723: step 1237, loss 0.501658.
Train: 2018-08-01T22:56:49.970249: step 1238, loss 0.518728.
Train: 2018-08-01T22:56:50.151764: step 1239, loss 0.563347.
Train: 2018-08-01T22:56:50.328292: step 1240, loss 0.598497.
Test: 2018-08-01T22:56:50.854885: step 1240, loss 0.550584.
Train: 2018-08-01T22:56:51.020441: step 1241, loss 0.500491.
Train: 2018-08-01T22:56:51.215923: step 1242, loss 0.59646.
Train: 2018-08-01T22:56:51.396436: step 1243, loss 0.611143.
Train: 2018-08-01T22:56:51.582945: step 1244, loss 0.622835.
Train: 2018-08-01T22:56:51.762459: step 1245, loss 0.51481.
Train: 2018-08-01T22:56:51.933001: step 1246, loss 0.632637.
Train: 2018-08-01T22:56:52.097562: step 1247, loss 0.486782.
Train: 2018-08-01T22:56:52.289050: step 1248, loss 0.564552.
Train: 2018-08-01T22:56:52.469568: step 1249, loss 0.596066.
Train: 2018-08-01T22:56:52.637119: step 1250, loss 0.529827.
Test: 2018-08-01T22:56:53.173685: step 1250, loss 0.549687.
Train: 2018-08-01T22:56:53.343232: step 1251, loss 0.550457.
Train: 2018-08-01T22:56:53.510783: step 1252, loss 0.531971.
Train: 2018-08-01T22:56:53.682351: step 1253, loss 0.51136.
Train: 2018-08-01T22:56:53.851871: step 1254, loss 0.532534.
Train: 2018-08-01T22:56:54.012471: step 1255, loss 0.564384.
Train: 2018-08-01T22:56:54.180023: step 1256, loss 0.527052.
Train: 2018-08-01T22:56:54.386443: step 1257, loss 0.647951.
Train: 2018-08-01T22:56:54.568958: step 1258, loss 0.445991.
Train: 2018-08-01T22:56:54.756453: step 1259, loss 0.559634.
Train: 2018-08-01T22:56:54.919045: step 1260, loss 0.616286.
Test: 2018-08-01T22:56:55.456581: step 1260, loss 0.548915.
Train: 2018-08-01T22:56:55.627126: step 1261, loss 0.580774.
Train: 2018-08-01T22:56:55.793712: step 1262, loss 0.545146.
Train: 2018-08-01T22:56:55.959238: step 1263, loss 0.562703.
Train: 2018-08-01T22:56:56.137760: step 1264, loss 0.526356.
Train: 2018-08-01T22:56:56.296337: step 1265, loss 0.510865.
Train: 2018-08-01T22:56:56.471868: step 1266, loss 0.600085.
Train: 2018-08-01T22:56:56.650417: step 1267, loss 0.594351.
Train: 2018-08-01T22:56:56.816945: step 1268, loss 0.542934.
Train: 2018-08-01T22:56:56.978513: step 1269, loss 0.425614.
Train: 2018-08-01T22:56:57.139119: step 1270, loss 0.476822.
Test: 2018-08-01T22:56:57.681634: step 1270, loss 0.548559.
Train: 2018-08-01T22:56:57.844199: step 1271, loss 0.457715.
Train: 2018-08-01T22:56:58.026739: step 1272, loss 0.458602.
Train: 2018-08-01T22:56:58.201245: step 1273, loss 0.487497.
Train: 2018-08-01T22:56:58.363810: step 1274, loss 0.381093.
Train: 2018-08-01T22:56:58.522386: step 1275, loss 0.453462.
Train: 2018-08-01T22:56:58.685948: step 1276, loss 0.568805.
Train: 2018-08-01T22:56:58.853500: step 1277, loss 0.673968.
Train: 2018-08-01T22:56:59.025042: step 1278, loss 0.649384.
Train: 2018-08-01T22:56:59.194590: step 1279, loss 0.620029.
Train: 2018-08-01T22:56:59.361143: step 1280, loss 0.547595.
Test: 2018-08-01T22:56:59.895714: step 1280, loss 0.548759.
Train: 2018-08-01T22:57:00.061298: step 1281, loss 0.600365.
Train: 2018-08-01T22:57:00.233810: step 1282, loss 0.490268.
Train: 2018-08-01T22:57:00.393384: step 1283, loss 0.583348.
Train: 2018-08-01T22:57:00.553981: step 1284, loss 0.530912.
Train: 2018-08-01T22:57:00.715524: step 1285, loss 0.676933.
Train: 2018-08-01T22:57:00.881081: step 1286, loss 0.563452.
Train: 2018-08-01T22:57:01.042674: step 1287, loss 0.452691.
Train: 2018-08-01T22:57:01.211223: step 1288, loss 0.564995.
Train: 2018-08-01T22:57:01.373799: step 1289, loss 0.604695.
Train: 2018-08-01T22:57:01.535363: step 1290, loss 0.468485.
Test: 2018-08-01T22:57:02.059929: step 1290, loss 0.548513.
Train: 2018-08-01T22:57:02.224489: step 1291, loss 0.625752.
Train: 2018-08-01T22:57:02.385093: step 1292, loss 0.489698.
Train: 2018-08-01T22:57:02.558628: step 1293, loss 0.51156.
Train: 2018-08-01T22:57:02.725151: step 1294, loss 0.565718.
Train: 2018-08-01T22:57:02.886720: step 1295, loss 0.620496.
Train: 2018-08-01T22:57:03.050282: step 1296, loss 0.73007.
Train: 2018-08-01T22:57:03.207872: step 1297, loss 0.50974.
Train: 2018-08-01T22:57:03.369428: step 1298, loss 0.566538.
Train: 2018-08-01T22:57:03.533023: step 1299, loss 0.50703.
Train: 2018-08-01T22:57:03.694560: step 1300, loss 0.580712.
Test: 2018-08-01T22:57:04.224153: step 1300, loss 0.54845.
Train: 2018-08-01T22:57:04.990060: step 1301, loss 0.726821.
Train: 2018-08-01T22:57:05.159577: step 1302, loss 0.49058.
Train: 2018-08-01T22:57:05.330150: step 1303, loss 0.614989.
Train: 2018-08-01T22:57:05.492687: step 1304, loss 0.600994.
Train: 2018-08-01T22:57:05.656280: step 1305, loss 0.510197.
Train: 2018-08-01T22:57:05.818847: step 1306, loss 0.63252.
Train: 2018-08-01T22:57:05.984372: step 1307, loss 0.649783.
Train: 2018-08-01T22:57:06.143974: step 1308, loss 0.546605.
Train: 2018-08-01T22:57:06.303548: step 1309, loss 0.510696.
Train: 2018-08-01T22:57:06.477086: step 1310, loss 0.530264.
Test: 2018-08-01T22:57:07.007646: step 1310, loss 0.54907.
Train: 2018-08-01T22:57:07.179228: step 1311, loss 0.545599.
Train: 2018-08-01T22:57:07.339749: step 1312, loss 0.462445.
Train: 2018-08-01T22:57:07.499323: step 1313, loss 0.630498.
Train: 2018-08-01T22:57:07.659903: step 1314, loss 0.596811.
Train: 2018-08-01T22:57:07.824453: step 1315, loss 0.494084.
Train: 2018-08-01T22:57:07.985024: step 1316, loss 0.581914.
Train: 2018-08-01T22:57:08.147589: step 1317, loss 0.595945.
Train: 2018-08-01T22:57:08.307162: step 1318, loss 0.650161.
Train: 2018-08-01T22:57:08.467733: step 1319, loss 0.529815.
Train: 2018-08-01T22:57:08.635312: step 1320, loss 0.495743.
Test: 2018-08-01T22:57:09.171876: step 1320, loss 0.549246.
Train: 2018-08-01T22:57:09.335414: step 1321, loss 0.563192.
Train: 2018-08-01T22:57:09.495021: step 1322, loss 0.511891.
Train: 2018-08-01T22:57:09.666529: step 1323, loss 0.53013.
Train: 2018-08-01T22:57:09.832088: step 1324, loss 0.446992.
Train: 2018-08-01T22:57:09.996677: step 1325, loss 0.611888.
Train: 2018-08-01T22:57:10.160210: step 1326, loss 0.581154.
Train: 2018-08-01T22:57:10.332778: step 1327, loss 0.476488.
Train: 2018-08-01T22:57:10.498325: step 1328, loss 0.478732.
Train: 2018-08-01T22:57:10.660899: step 1329, loss 0.511037.
Train: 2018-08-01T22:57:10.827425: step 1330, loss 0.633369.
Test: 2018-08-01T22:57:11.371984: step 1330, loss 0.548696.
Train: 2018-08-01T22:57:11.541547: step 1331, loss 0.580937.
Train: 2018-08-01T22:57:11.705107: step 1332, loss 0.528626.
Train: 2018-08-01T22:57:11.864678: step 1333, loss 0.494651.
Train: 2018-08-01T22:57:12.030211: step 1334, loss 0.564677.
Train: 2018-08-01T22:57:12.202750: step 1335, loss 0.633516.
Train: 2018-08-01T22:57:12.367335: step 1336, loss 0.789905.
Train: 2018-08-01T22:57:12.536855: step 1337, loss 0.651887.
Train: 2018-08-01T22:57:12.700419: step 1338, loss 0.563154.
Train: 2018-08-01T22:57:12.871960: step 1339, loss 0.578876.
Train: 2018-08-01T22:57:13.034526: step 1340, loss 0.597803.
Test: 2018-08-01T22:57:13.574083: step 1340, loss 0.54875.
Train: 2018-08-01T22:57:13.735677: step 1341, loss 0.528833.
Train: 2018-08-01T22:57:13.899214: step 1342, loss 0.564687.
Train: 2018-08-01T22:57:14.062802: step 1343, loss 0.480198.
Train: 2018-08-01T22:57:14.226339: step 1344, loss 0.613889.
Train: 2018-08-01T22:57:14.386910: step 1345, loss 0.545958.
Train: 2018-08-01T22:57:14.556482: step 1346, loss 0.647846.
Train: 2018-08-01T22:57:14.718061: step 1347, loss 0.598454.
Train: 2018-08-01T22:57:14.879594: step 1348, loss 0.427888.
Train: 2018-08-01T22:57:15.046179: step 1349, loss 0.477773.
Train: 2018-08-01T22:57:15.207716: step 1350, loss 0.613786.
Test: 2018-08-01T22:57:15.741290: step 1350, loss 0.548993.
Train: 2018-08-01T22:57:15.901861: step 1351, loss 0.581718.
Train: 2018-08-01T22:57:16.063428: step 1352, loss 0.682123.
Train: 2018-08-01T22:57:16.225994: step 1353, loss 0.478873.
Train: 2018-08-01T22:57:16.385568: step 1354, loss 0.597352.
Train: 2018-08-01T22:57:16.548157: step 1355, loss 0.56489.
Train: 2018-08-01T22:57:16.710698: step 1356, loss 0.562115.
Train: 2018-08-01T22:57:16.884234: step 1357, loss 0.54615.
Train: 2018-08-01T22:57:17.048794: step 1358, loss 0.512162.
Train: 2018-08-01T22:57:17.213379: step 1359, loss 0.528356.
Train: 2018-08-01T22:57:17.376948: step 1360, loss 0.596787.
Test: 2018-08-01T22:57:17.915503: step 1360, loss 0.549024.
Train: 2018-08-01T22:57:18.078043: step 1361, loss 0.531781.
Train: 2018-08-01T22:57:18.239639: step 1362, loss 0.477762.
Train: 2018-08-01T22:57:18.411154: step 1363, loss 0.563154.
Train: 2018-08-01T22:57:18.582694: step 1364, loss 0.460713.
Train: 2018-08-01T22:57:18.745265: step 1365, loss 0.579589.
Train: 2018-08-01T22:57:18.908848: step 1366, loss 0.478323.
Train: 2018-08-01T22:57:19.075376: step 1367, loss 0.596855.
Train: 2018-08-01T22:57:19.262875: step 1368, loss 0.476082.
Train: 2018-08-01T22:57:19.426438: step 1369, loss 0.650148.
Train: 2018-08-01T22:57:19.600971: step 1370, loss 0.581127.
Test: 2018-08-01T22:57:20.127566: step 1370, loss 0.548513.
Train: 2018-08-01T22:57:20.331027: step 1371, loss 0.562798.
Train: 2018-08-01T22:57:20.518519: step 1372, loss 0.546903.
Train: 2018-08-01T22:57:20.679090: step 1373, loss 0.564597.
Train: 2018-08-01T22:57:20.857612: step 1374, loss 0.56425.
Train: 2018-08-01T22:57:21.022172: step 1375, loss 0.616612.
Train: 2018-08-01T22:57:21.186732: step 1376, loss 0.616668.
Train: 2018-08-01T22:57:21.350326: step 1377, loss 0.526862.
Train: 2018-08-01T22:57:21.513883: step 1378, loss 0.439702.
Train: 2018-08-01T22:57:21.717316: step 1379, loss 0.493336.
Train: 2018-08-01T22:57:21.888856: step 1380, loss 0.545889.
Test: 2018-08-01T22:57:22.432428: step 1380, loss 0.548411.
Train: 2018-08-01T22:57:22.594967: step 1381, loss 0.493915.
Train: 2018-08-01T22:57:22.763546: step 1382, loss 0.616217.
Train: 2018-08-01T22:57:22.936082: step 1383, loss 0.616337.
Train: 2018-08-01T22:57:23.103608: step 1384, loss 0.545706.
Train: 2018-08-01T22:57:23.264179: step 1385, loss 0.544231.
Train: 2018-08-01T22:57:23.425748: step 1386, loss 0.564572.
Train: 2018-08-01T22:57:23.585320: step 1387, loss 0.563513.
Train: 2018-08-01T22:57:23.747901: step 1388, loss 0.510284.
Train: 2018-08-01T22:57:23.918430: step 1389, loss 0.652299.
Train: 2018-08-01T22:57:24.083988: step 1390, loss 0.546458.
Test: 2018-08-01T22:57:24.617562: step 1390, loss 0.548354.
Train: 2018-08-01T22:57:24.800074: step 1391, loss 0.545643.
Train: 2018-08-01T22:57:24.961672: step 1392, loss 0.526825.
Train: 2018-08-01T22:57:25.141161: step 1393, loss 0.545543.
Train: 2018-08-01T22:57:25.304756: step 1394, loss 0.597103.
Train: 2018-08-01T22:57:25.472307: step 1395, loss 0.617358.
Train: 2018-08-01T22:57:25.636867: step 1396, loss 0.493579.
Train: 2018-08-01T22:57:25.797408: step 1397, loss 0.668269.
Train: 2018-08-01T22:57:25.964959: step 1398, loss 0.546729.
Train: 2018-08-01T22:57:26.138524: step 1399, loss 0.599684.
Train: 2018-08-01T22:57:26.314057: step 1400, loss 0.511849.
Test: 2018-08-01T22:57:26.873531: step 1400, loss 0.548438.
Train: 2018-08-01T22:57:27.692578: step 1401, loss 0.614987.
Train: 2018-08-01T22:57:27.868109: step 1402, loss 0.597228.
Train: 2018-08-01T22:57:28.028712: step 1403, loss 0.51122.
Train: 2018-08-01T22:57:28.200222: step 1404, loss 0.545224.
Train: 2018-08-01T22:57:28.388719: step 1405, loss 0.545791.
Train: 2018-08-01T22:57:28.571229: step 1406, loss 0.564062.
Train: 2018-08-01T22:57:28.733794: step 1407, loss 0.563767.
Train: 2018-08-01T22:57:28.896360: step 1408, loss 0.493404.
Train: 2018-08-01T22:57:29.081865: step 1409, loss 0.597904.
Train: 2018-08-01T22:57:29.261385: step 1410, loss 0.510969.
Test: 2018-08-01T22:57:29.790969: step 1410, loss 0.548545.
Train: 2018-08-01T22:57:29.957555: step 1411, loss 0.476331.
Train: 2018-08-01T22:57:30.137043: step 1412, loss 0.476215.
Train: 2018-08-01T22:57:30.301636: step 1413, loss 0.54713.
Train: 2018-08-01T22:57:30.464170: step 1414, loss 0.650684.
Train: 2018-08-01T22:57:30.653662: step 1415, loss 0.650416.
Train: 2018-08-01T22:57:30.885046: step 1416, loss 0.561573.
Train: 2018-08-01T22:57:31.165295: step 1417, loss 0.511084.
Train: 2018-08-01T22:57:31.369748: step 1418, loss 0.458271.
Train: 2018-08-01T22:57:31.524335: step 1419, loss 0.528512.
Train: 2018-08-01T22:57:31.688920: step 1420, loss 0.563568.
Test: 2018-08-01T22:57:32.227455: step 1420, loss 0.548356.
Train: 2018-08-01T22:57:32.391031: step 1421, loss 0.616692.
Train: 2018-08-01T22:57:32.553584: step 1422, loss 0.474072.
Train: 2018-08-01T22:57:32.716175: step 1423, loss 0.616097.
Train: 2018-08-01T22:57:32.879737: step 1424, loss 0.685945.
Train: 2018-08-01T22:57:33.046267: step 1425, loss 0.633392.
Train: 2018-08-01T22:57:33.213838: step 1426, loss 0.509777.
Train: 2018-08-01T22:57:33.374414: step 1427, loss 0.651304.
Train: 2018-08-01T22:57:33.548949: step 1428, loss 0.528098.
Train: 2018-08-01T22:57:33.711521: step 1429, loss 0.475847.
Train: 2018-08-01T22:57:33.875077: step 1430, loss 0.600329.
Test: 2018-08-01T22:57:34.424610: step 1430, loss 0.548407.
Train: 2018-08-01T22:57:34.590153: step 1431, loss 0.597003.
Train: 2018-08-01T22:57:34.761681: step 1432, loss 0.668538.
Train: 2018-08-01T22:57:34.925270: step 1433, loss 0.686704.
Train: 2018-08-01T22:57:35.089804: step 1434, loss 0.511302.
Train: 2018-08-01T22:57:35.259351: step 1435, loss 0.510934.
Train: 2018-08-01T22:57:35.421944: step 1436, loss 0.528264.
Train: 2018-08-01T22:57:35.587473: step 1437, loss 0.529323.
Train: 2018-08-01T22:57:35.751067: step 1438, loss 0.512426.
Train: 2018-08-01T22:57:35.917590: step 1439, loss 0.529117.
Train: 2018-08-01T22:57:36.080182: step 1440, loss 0.630706.
Test: 2018-08-01T22:57:36.622706: step 1440, loss 0.548707.
Train: 2018-08-01T22:57:36.788288: step 1441, loss 0.580816.
Train: 2018-08-01T22:57:36.948834: step 1442, loss 0.648925.
Train: 2018-08-01T22:57:37.111399: step 1443, loss 0.493824.
Train: 2018-08-01T22:57:37.280979: step 1444, loss 0.54542.
Train: 2018-08-01T22:57:37.443545: step 1445, loss 0.444881.
Train: 2018-08-01T22:57:37.624029: step 1446, loss 0.597396.
Train: 2018-08-01T22:57:37.791609: step 1447, loss 0.545523.
Train: 2018-08-01T22:57:37.961128: step 1448, loss 0.648747.
Train: 2018-08-01T22:57:38.128680: step 1449, loss 0.666224.
Train: 2018-08-01T22:57:38.309199: step 1450, loss 0.579648.
Test: 2018-08-01T22:57:38.848764: step 1450, loss 0.548817.
Train: 2018-08-01T22:57:39.017329: step 1451, loss 0.496856.
Train: 2018-08-01T22:57:39.181864: step 1452, loss 0.529465.
Train: 2018-08-01T22:57:39.346449: step 1453, loss 0.700039.
Train: 2018-08-01T22:57:39.515996: step 1454, loss 0.547937.
Train: 2018-08-01T22:57:39.680532: step 1455, loss 0.512271.
Train: 2018-08-01T22:57:39.859055: step 1456, loss 0.531038.
Train: 2018-08-01T22:57:40.023650: step 1457, loss 0.595379.
Train: 2018-08-01T22:57:40.193162: step 1458, loss 0.57961.
Train: 2018-08-01T22:57:40.353732: step 1459, loss 0.647394.
Train: 2018-08-01T22:57:40.519290: step 1460, loss 0.629291.
Test: 2018-08-01T22:57:41.056853: step 1460, loss 0.54917.
Train: 2018-08-01T22:57:41.228394: step 1461, loss 0.596635.
Train: 2018-08-01T22:57:41.403924: step 1462, loss 0.51307.
Train: 2018-08-01T22:57:41.580478: step 1463, loss 0.579984.
Train: 2018-08-01T22:57:41.747007: step 1464, loss 0.579642.
Train: 2018-08-01T22:57:41.910571: step 1465, loss 0.513359.
Train: 2018-08-01T22:57:42.072138: step 1466, loss 0.579778.
Train: 2018-08-01T22:57:42.236729: step 1467, loss 0.579751.
Train: 2018-08-01T22:57:42.395275: step 1468, loss 0.546838.
Train: 2018-08-01T22:57:42.556867: step 1469, loss 0.612878.
Train: 2018-08-01T22:57:42.716447: step 1470, loss 0.628928.
Test: 2018-08-01T22:57:43.266974: step 1470, loss 0.549489.
Train: 2018-08-01T22:57:43.433499: step 1471, loss 0.678249.
Train: 2018-08-01T22:57:43.595100: step 1472, loss 0.579331.
Train: 2018-08-01T22:57:43.759627: step 1473, loss 0.54714.
Train: 2018-08-01T22:57:43.929205: step 1474, loss 0.595779.
Train: 2018-08-01T22:57:44.088747: step 1475, loss 0.563221.
Train: 2018-08-01T22:57:44.248322: step 1476, loss 0.596219.
Train: 2018-08-01T22:57:44.407919: step 1477, loss 0.627831.
Train: 2018-08-01T22:57:44.570485: step 1478, loss 0.483347.
Train: 2018-08-01T22:57:44.733053: step 1479, loss 0.531131.
Train: 2018-08-01T22:57:44.899581: step 1480, loss 0.531021.
Test: 2018-08-01T22:57:45.437162: step 1480, loss 0.550001.
Train: 2018-08-01T22:57:45.598711: step 1481, loss 0.531392.
Train: 2018-08-01T22:57:45.759298: step 1482, loss 0.595307.
Train: 2018-08-01T22:57:45.923849: step 1483, loss 0.579831.
Train: 2018-08-01T22:57:46.090396: step 1484, loss 0.498965.
Train: 2018-08-01T22:57:46.251969: step 1485, loss 0.563079.
Train: 2018-08-01T22:57:46.416524: step 1486, loss 0.547328.
Train: 2018-08-01T22:57:46.587069: step 1487, loss 0.612097.
Train: 2018-08-01T22:57:46.749634: step 1488, loss 0.563733.
Train: 2018-08-01T22:57:46.907213: step 1489, loss 0.530841.
Train: 2018-08-01T22:57:47.076760: step 1490, loss 0.661195.
Test: 2018-08-01T22:57:47.621304: step 1490, loss 0.549569.
Train: 2018-08-01T22:57:47.785895: step 1491, loss 0.628492.
Train: 2018-08-01T22:57:47.950424: step 1492, loss 0.530616.
Train: 2018-08-01T22:57:48.124017: step 1493, loss 0.628816.
Train: 2018-08-01T22:57:48.289542: step 1494, loss 0.514423.
Train: 2018-08-01T22:57:48.453111: step 1495, loss 0.465378.
Train: 2018-08-01T22:57:48.617667: step 1496, loss 0.448747.
Train: 2018-08-01T22:57:48.776241: step 1497, loss 0.546317.
Train: 2018-08-01T22:57:48.936818: step 1498, loss 0.662164.
Train: 2018-08-01T22:57:49.100350: step 1499, loss 0.48034.
Train: 2018-08-01T22:57:49.260920: step 1500, loss 0.629585.
Test: 2018-08-01T22:57:49.806472: step 1500, loss 0.549155.
Train: 2018-08-01T22:57:50.571356: step 1501, loss 0.579697.
Train: 2018-08-01T22:57:50.733898: step 1502, loss 0.613011.
Train: 2018-08-01T22:57:50.895467: step 1503, loss 0.563361.
Train: 2018-08-01T22:57:51.070000: step 1504, loss 0.546547.
Train: 2018-08-01T22:57:51.241540: step 1505, loss 0.491557.
Train: 2018-08-01T22:57:51.408129: step 1506, loss 0.546465.
Train: 2018-08-01T22:57:51.573653: step 1507, loss 0.563178.
Train: 2018-08-01T22:57:51.733259: step 1508, loss 0.529278.
Train: 2018-08-01T22:57:51.903771: step 1509, loss 0.478632.
Train: 2018-08-01T22:57:52.064373: step 1510, loss 0.63057.
Test: 2018-08-01T22:57:52.592929: step 1510, loss 0.54875.
Train: 2018-08-01T22:57:52.752527: step 1511, loss 0.580081.
Train: 2018-08-01T22:57:52.911106: step 1512, loss 0.512274.
Train: 2018-08-01T22:57:53.076662: step 1513, loss 0.494422.
Train: 2018-08-01T22:57:53.242193: step 1514, loss 0.545549.
Train: 2018-08-01T22:57:53.409745: step 1515, loss 0.700861.
Train: 2018-08-01T22:57:53.573308: step 1516, loss 0.528644.
Train: 2018-08-01T22:57:53.735873: step 1517, loss 0.648795.
Train: 2018-08-01T22:57:53.897472: step 1518, loss 0.580014.
Train: 2018-08-01T22:57:54.058012: step 1519, loss 0.614889.
Train: 2018-08-01T22:57:54.224577: step 1520, loss 0.528461.
Test: 2018-08-01T22:57:54.756149: step 1520, loss 0.54857.
Train: 2018-08-01T22:57:54.923698: step 1521, loss 0.614257.
Train: 2018-08-01T22:57:55.086263: step 1522, loss 0.546062.
Train: 2018-08-01T22:57:55.251848: step 1523, loss 0.563236.
Train: 2018-08-01T22:57:55.409424: step 1524, loss 0.579477.
Train: 2018-08-01T22:57:55.632102: step 1525, loss 0.562624.
Train: 2018-08-01T22:57:55.796630: step 1526, loss 0.511602.
Train: 2018-08-01T22:57:55.960194: step 1527, loss 0.613938.
Train: 2018-08-01T22:57:56.135725: step 1528, loss 0.443333.
Train: 2018-08-01T22:57:56.296296: step 1529, loss 0.614294.
Train: 2018-08-01T22:57:56.475847: step 1530, loss 0.648993.
Test: 2018-08-01T22:57:57.028339: step 1530, loss 0.548638.
Train: 2018-08-01T22:57:57.196888: step 1531, loss 0.648011.
Train: 2018-08-01T22:57:57.366461: step 1532, loss 0.597001.
Train: 2018-08-01T22:57:57.534000: step 1533, loss 0.426745.
Train: 2018-08-01T22:57:57.700542: step 1534, loss 0.579847.
Train: 2018-08-01T22:57:57.865128: step 1535, loss 0.443848.
Train: 2018-08-01T22:57:58.029680: step 1536, loss 0.529309.
Train: 2018-08-01T22:57:58.216163: step 1537, loss 0.477728.
Train: 2018-08-01T22:57:58.374766: step 1538, loss 0.511671.
Train: 2018-08-01T22:57:58.534313: step 1539, loss 0.580743.
Train: 2018-08-01T22:57:58.698899: step 1540, loss 0.579369.
Test: 2018-08-01T22:57:59.237481: step 1540, loss 0.548463.
Train: 2018-08-01T22:57:59.471038: step 1541, loss 0.648569.
Train: 2018-08-01T22:57:59.640554: step 1542, loss 0.527893.
Train: 2018-08-01T22:57:59.807128: step 1543, loss 0.511643.
Train: 2018-08-01T22:57:59.974660: step 1544, loss 0.666894.
Train: 2018-08-01T22:58:00.147230: step 1545, loss 0.475292.
Train: 2018-08-01T22:58:00.341430: step 1546, loss 0.528513.
Train: 2018-08-01T22:58:00.511002: step 1547, loss 0.54541.
Train: 2018-08-01T22:58:00.673568: step 1548, loss 0.528265.
Train: 2018-08-01T22:58:00.839100: step 1549, loss 0.650983.
Train: 2018-08-01T22:58:01.041559: step 1550, loss 0.545565.
Test: 2018-08-01T22:58:01.574136: step 1550, loss 0.548291.
Train: 2018-08-01T22:58:01.847405: step 1551, loss 0.615179.
Train: 2018-08-01T22:58:02.054849: step 1552, loss 0.666699.
Train: 2018-08-01T22:58:02.221404: step 1553, loss 0.562551.
Train: 2018-08-01T22:58:02.390983: step 1554, loss 0.546614.
Train: 2018-08-01T22:58:02.554538: step 1555, loss 0.476492.
Train: 2018-08-01T22:58:02.726081: step 1556, loss 0.685217.
Train: 2018-08-01T22:58:02.891639: step 1557, loss 0.476184.
Train: 2018-08-01T22:58:03.067144: step 1558, loss 0.528597.
Train: 2018-08-01T22:58:03.275587: step 1559, loss 0.579796.
Train: 2018-08-01T22:58:03.505971: step 1560, loss 0.50996.
Test: 2018-08-01T22:58:04.044531: step 1560, loss 0.548371.
Train: 2018-08-01T22:58:04.214104: step 1561, loss 0.597671.
Train: 2018-08-01T22:58:04.385619: step 1562, loss 0.493656.
Train: 2018-08-01T22:58:04.558158: step 1563, loss 0.475181.
Train: 2018-08-01T22:58:04.718728: step 1564, loss 0.633382.
Train: 2018-08-01T22:58:04.892264: step 1565, loss 0.63291.
Train: 2018-08-01T22:58:05.054857: step 1566, loss 0.597623.
Train: 2018-08-01T22:58:05.216425: step 1567, loss 0.616749.
Train: 2018-08-01T22:58:05.391960: step 1568, loss 0.493122.
Train: 2018-08-01T22:58:05.560478: step 1569, loss 0.406231.
Train: 2018-08-01T22:58:05.720051: step 1570, loss 0.581772.
Test: 2018-08-01T22:58:06.269582: step 1570, loss 0.548331.
Train: 2018-08-01T22:58:06.430154: step 1571, loss 0.667372.
Train: 2018-08-01T22:58:06.592719: step 1572, loss 0.5284.
Train: 2018-08-01T22:58:06.769277: step 1573, loss 0.579756.
Train: 2018-08-01T22:58:06.933838: step 1574, loss 0.545788.
Train: 2018-08-01T22:58:07.098368: step 1575, loss 0.562536.
Train: 2018-08-01T22:58:07.268911: step 1576, loss 0.528551.
Train: 2018-08-01T22:58:07.435492: step 1577, loss 0.614796.
Train: 2018-08-01T22:58:07.603044: step 1578, loss 0.528617.
Train: 2018-08-01T22:58:07.769598: step 1579, loss 0.44062.
Train: 2018-08-01T22:58:07.936159: step 1580, loss 0.475644.
Test: 2018-08-01T22:58:08.470699: step 1580, loss 0.548291.
Train: 2018-08-01T22:58:08.640276: step 1581, loss 0.615505.
Train: 2018-08-01T22:58:08.803839: step 1582, loss 0.510264.
Train: 2018-08-01T22:58:08.966373: step 1583, loss 0.510293.
Train: 2018-08-01T22:58:09.134942: step 1584, loss 0.492412.
Train: 2018-08-01T22:58:09.296491: step 1585, loss 0.650822.
Train: 2018-08-01T22:58:09.462079: step 1586, loss 0.473779.
Train: 2018-08-01T22:58:09.623641: step 1587, loss 0.52823.
Train: 2018-08-01T22:58:09.791202: step 1588, loss 0.597669.
Train: 2018-08-01T22:58:09.952767: step 1589, loss 0.616122.
Train: 2018-08-01T22:58:10.114305: step 1590, loss 0.634467.
Test: 2018-08-01T22:58:10.648875: step 1590, loss 0.548108.
Train: 2018-08-01T22:58:10.813460: step 1591, loss 0.54505.
Train: 2018-08-01T22:58:10.975021: step 1592, loss 0.670816.
Train: 2018-08-01T22:58:11.146545: step 1593, loss 0.563792.
Train: 2018-08-01T22:58:11.309136: step 1594, loss 0.634139.
Train: 2018-08-01T22:58:11.470709: step 1595, loss 0.563114.
Train: 2018-08-01T22:58:11.636235: step 1596, loss 0.579602.
Train: 2018-08-01T22:58:11.798801: step 1597, loss 0.511015.
Train: 2018-08-01T22:58:11.956384: step 1598, loss 0.615838.
Train: 2018-08-01T22:58:12.119969: step 1599, loss 0.614984.
Train: 2018-08-01T22:58:12.290513: step 1600, loss 0.615114.
Test: 2018-08-01T22:58:12.823064: step 1600, loss 0.548314.
Train: 2018-08-01T22:58:13.610615: step 1601, loss 0.458825.
Train: 2018-08-01T22:58:13.775175: step 1602, loss 0.562857.
Train: 2018-08-01T22:58:13.934749: step 1603, loss 0.545203.
Train: 2018-08-01T22:58:14.106289: step 1604, loss 0.596961.
Train: 2018-08-01T22:58:14.264835: step 1605, loss 0.614129.
Train: 2018-08-01T22:58:14.432412: step 1606, loss 0.580552.
Train: 2018-08-01T22:58:14.596948: step 1607, loss 0.460344.
Train: 2018-08-01T22:58:14.761534: step 1608, loss 0.459004.
Train: 2018-08-01T22:58:14.931063: step 1609, loss 0.545753.
Train: 2018-08-01T22:58:15.093619: step 1610, loss 0.528026.
Test: 2018-08-01T22:58:15.632180: step 1610, loss 0.548403.
Train: 2018-08-01T22:58:15.795742: step 1611, loss 0.529301.
Train: 2018-08-01T22:58:15.957311: step 1612, loss 0.493662.
Train: 2018-08-01T22:58:16.124863: step 1613, loss 0.476414.
Train: 2018-08-01T22:58:16.287427: step 1614, loss 0.510161.
Train: 2018-08-01T22:58:16.459000: step 1615, loss 0.562724.
Train: 2018-08-01T22:58:16.633504: step 1616, loss 0.633809.
Train: 2018-08-01T22:58:16.808036: step 1617, loss 0.545197.
Train: 2018-08-01T22:58:16.986560: step 1618, loss 0.686088.
Train: 2018-08-01T22:58:17.153123: step 1619, loss 0.651492.
Train: 2018-08-01T22:58:17.316703: step 1620, loss 0.651553.
Test: 2018-08-01T22:58:17.848255: step 1620, loss 0.548206.
Train: 2018-08-01T22:58:18.020825: step 1621, loss 0.475934.
Train: 2018-08-01T22:58:18.180398: step 1622, loss 0.50937.
Train: 2018-08-01T22:58:18.342966: step 1623, loss 0.564037.
Train: 2018-08-01T22:58:18.505499: step 1624, loss 0.59733.
Train: 2018-08-01T22:58:18.673084: step 1625, loss 0.545909.
Train: 2018-08-01T22:58:18.837611: step 1626, loss 0.528079.
Train: 2018-08-01T22:58:19.019155: step 1627, loss 0.475173.
Train: 2018-08-01T22:58:19.180693: step 1628, loss 0.632707.
Train: 2018-08-01T22:58:19.344287: step 1629, loss 0.667674.
Train: 2018-08-01T22:58:19.517793: step 1630, loss 0.597989.
Test: 2018-08-01T22:58:20.053360: step 1630, loss 0.548289.
Train: 2018-08-01T22:58:20.222934: step 1631, loss 0.510893.
Train: 2018-08-01T22:58:20.391484: step 1632, loss 0.563383.
Train: 2018-08-01T22:58:20.565019: step 1633, loss 0.631438.
Train: 2018-08-01T22:58:20.729584: step 1634, loss 0.562152.
Train: 2018-08-01T22:58:20.889151: step 1635, loss 0.562784.
Train: 2018-08-01T22:58:21.053686: step 1636, loss 0.476318.
Train: 2018-08-01T22:58:21.216283: step 1637, loss 0.579906.
Train: 2018-08-01T22:58:21.384827: step 1638, loss 0.527871.
Train: 2018-08-01T22:58:21.547418: step 1639, loss 0.682949.
Train: 2018-08-01T22:58:21.708960: step 1640, loss 0.631265.
Test: 2018-08-01T22:58:22.252482: step 1640, loss 0.548459.
Train: 2018-08-01T22:58:22.421032: step 1641, loss 0.494226.
Train: 2018-08-01T22:58:22.592597: step 1642, loss 0.494999.
Train: 2018-08-01T22:58:22.760150: step 1643, loss 0.52803.
Train: 2018-08-01T22:58:22.923717: step 1644, loss 0.529417.
Train: 2018-08-01T22:58:23.085280: step 1645, loss 0.579018.
Train: 2018-08-01T22:58:23.253806: step 1646, loss 0.562692.
Train: 2018-08-01T22:58:23.416370: step 1647, loss 0.580337.
Train: 2018-08-01T22:58:23.577939: step 1648, loss 0.546924.
Train: 2018-08-01T22:58:23.735517: step 1649, loss 0.51122.
Train: 2018-08-01T22:58:23.896088: step 1650, loss 0.510271.
Test: 2018-08-01T22:58:24.438694: step 1650, loss 0.548378.
Train: 2018-08-01T22:58:24.601221: step 1651, loss 0.545459.
Train: 2018-08-01T22:58:24.761804: step 1652, loss 0.52772.
Train: 2018-08-01T22:58:24.924367: step 1653, loss 0.579679.
Train: 2018-08-01T22:58:25.096879: step 1654, loss 0.440778.
Train: 2018-08-01T22:58:25.261438: step 1655, loss 0.634221.
Train: 2018-08-01T22:58:25.423006: step 1656, loss 0.475592.
Train: 2018-08-01T22:58:25.592584: step 1657, loss 0.598106.
Train: 2018-08-01T22:58:25.761102: step 1658, loss 0.510187.
Train: 2018-08-01T22:58:25.924696: step 1659, loss 0.580447.
Train: 2018-08-01T22:58:26.090225: step 1660, loss 0.58048.
Test: 2018-08-01T22:58:26.638757: step 1660, loss 0.548118.
Train: 2018-08-01T22:58:26.808329: step 1661, loss 0.669821.
Train: 2018-08-01T22:58:26.974857: step 1662, loss 0.508451.
Train: 2018-08-01T22:58:27.135428: step 1663, loss 0.599067.
Train: 2018-08-01T22:58:27.299988: step 1664, loss 0.527524.
Train: 2018-08-01T22:58:27.471561: step 1665, loss 0.597839.
Train: 2018-08-01T22:58:27.637118: step 1666, loss 0.563931.
Train: 2018-08-01T22:58:27.797657: step 1667, loss 0.739662.
Train: 2018-08-01T22:58:27.962218: step 1668, loss 0.527545.
Train: 2018-08-01T22:58:28.131765: step 1669, loss 0.58036.
Train: 2018-08-01T22:58:28.297348: step 1670, loss 0.544653.
Test: 2018-08-01T22:58:28.837902: step 1670, loss 0.548249.
Train: 2018-08-01T22:58:29.002437: step 1671, loss 0.581151.
Train: 2018-08-01T22:58:29.172981: step 1672, loss 0.545532.
Train: 2018-08-01T22:58:29.340560: step 1673, loss 0.458443.
Train: 2018-08-01T22:58:29.503124: step 1674, loss 0.562563.
Train: 2018-08-01T22:58:29.665696: step 1675, loss 0.528025.
Train: 2018-08-01T22:58:29.825239: step 1676, loss 0.61496.
Train: 2018-08-01T22:58:29.983839: step 1677, loss 0.493956.
Train: 2018-08-01T22:58:30.149372: step 1678, loss 0.476057.
Train: 2018-08-01T22:58:30.325930: step 1679, loss 0.544821.
Train: 2018-08-01T22:58:30.492454: step 1680, loss 0.562058.
Test: 2018-08-01T22:58:31.037998: step 1680, loss 0.548229.
Train: 2018-08-01T22:58:31.206570: step 1681, loss 0.510278.
Train: 2018-08-01T22:58:31.371143: step 1682, loss 0.597068.
Train: 2018-08-01T22:58:31.528684: step 1683, loss 0.650527.
Train: 2018-08-01T22:58:31.695270: step 1684, loss 0.615446.
Train: 2018-08-01T22:58:31.856806: step 1685, loss 0.42348.
Train: 2018-08-01T22:58:32.019398: step 1686, loss 0.616499.
Train: 2018-08-01T22:58:32.179975: step 1687, loss 0.633209.
Train: 2018-08-01T22:58:32.346525: step 1688, loss 0.615023.
Train: 2018-08-01T22:58:32.515079: step 1689, loss 0.509615.
Train: 2018-08-01T22:58:32.686639: step 1690, loss 0.562693.
Test: 2018-08-01T22:58:33.224152: step 1690, loss 0.548223.
Train: 2018-08-01T22:58:33.385733: step 1691, loss 0.492693.
Train: 2018-08-01T22:58:33.556265: step 1692, loss 0.597976.
Train: 2018-08-01T22:58:33.713843: step 1693, loss 0.562535.
Train: 2018-08-01T22:58:33.882424: step 1694, loss 0.579923.
Train: 2018-08-01T22:58:34.043960: step 1695, loss 0.615884.
Train: 2018-08-01T22:58:34.213506: step 1696, loss 0.527385.
Train: 2018-08-01T22:58:34.374078: step 1697, loss 0.563011.
Train: 2018-08-01T22:58:34.540634: step 1698, loss 0.544853.
Train: 2018-08-01T22:58:34.702200: step 1699, loss 0.563366.
Train: 2018-08-01T22:58:34.861774: step 1700, loss 0.63247.
Test: 2018-08-01T22:58:35.394368: step 1700, loss 0.54829.
Train: 2018-08-01T22:58:36.141272: step 1701, loss 0.683864.
Train: 2018-08-01T22:58:36.306829: step 1702, loss 0.561376.
Train: 2018-08-01T22:58:36.468397: step 1703, loss 0.631663.
Train: 2018-08-01T22:58:36.630994: step 1704, loss 0.528452.
Train: 2018-08-01T22:58:36.794525: step 1705, loss 0.562764.
Train: 2018-08-01T22:58:36.964098: step 1706, loss 0.561795.
Train: 2018-08-01T22:58:37.127635: step 1707, loss 0.529511.
Train: 2018-08-01T22:58:37.290226: step 1708, loss 0.561489.
Train: 2018-08-01T22:58:37.459747: step 1709, loss 0.495338.
Train: 2018-08-01T22:58:37.622313: step 1710, loss 0.529132.
Test: 2018-08-01T22:58:38.163896: step 1710, loss 0.548573.
Train: 2018-08-01T22:58:38.334445: step 1711, loss 0.562688.
Train: 2018-08-01T22:58:38.505966: step 1712, loss 0.563032.
Train: 2018-08-01T22:58:38.668516: step 1713, loss 0.477941.
Train: 2018-08-01T22:58:38.828089: step 1714, loss 0.596379.
Train: 2018-08-01T22:58:38.985698: step 1715, loss 0.493976.
Train: 2018-08-01T22:58:39.152222: step 1716, loss 0.562035.
Train: 2018-08-01T22:58:39.313816: step 1717, loss 0.596378.
Train: 2018-08-01T22:58:39.479347: step 1718, loss 0.545414.
Train: 2018-08-01T22:58:39.640944: step 1719, loss 0.562399.
Train: 2018-08-01T22:58:39.802483: step 1720, loss 0.528678.
Test: 2018-08-01T22:58:40.344037: step 1720, loss 0.548308.
Train: 2018-08-01T22:58:40.517573: step 1721, loss 0.545545.
Train: 2018-08-01T22:58:40.684160: step 1722, loss 0.545046.
Train: 2018-08-01T22:58:40.854695: step 1723, loss 0.528058.
Train: 2018-08-01T22:58:41.017236: step 1724, loss 0.598451.
Train: 2018-08-01T22:58:41.179833: step 1725, loss 0.598512.
Train: 2018-08-01T22:58:41.345359: step 1726, loss 0.440145.
Train: 2018-08-01T22:58:41.511914: step 1727, loss 0.510122.
Train: 2018-08-01T22:58:41.675477: step 1728, loss 0.579601.
Train: 2018-08-01T22:58:41.836048: step 1729, loss 0.492079.
Train: 2018-08-01T22:58:41.998640: step 1730, loss 0.68575.
Test: 2018-08-01T22:58:42.538202: step 1730, loss 0.548061.
Train: 2018-08-01T22:58:42.698741: step 1731, loss 0.563821.
Train: 2018-08-01T22:58:42.861306: step 1732, loss 0.667677.
Train: 2018-08-01T22:58:43.027862: step 1733, loss 0.509755.
Train: 2018-08-01T22:58:43.200426: step 1734, loss 0.564179.
Train: 2018-08-01T22:58:43.360971: step 1735, loss 0.723185.
Train: 2018-08-01T22:58:43.528524: step 1736, loss 0.636436.
Train: 2018-08-01T22:58:43.691088: step 1737, loss 0.492064.
Train: 2018-08-01T22:58:43.848681: step 1738, loss 0.509733.
Train: 2018-08-01T22:58:44.015253: step 1739, loss 0.597592.
Train: 2018-08-01T22:58:44.181778: step 1740, loss 0.581732.
Test: 2018-08-01T22:58:44.720338: step 1740, loss 0.54826.
Train: 2018-08-01T22:58:44.887923: step 1741, loss 0.596022.
Train: 2018-08-01T22:58:45.052474: step 1742, loss 0.562848.
Train: 2018-08-01T22:58:45.213020: step 1743, loss 0.546309.
Train: 2018-08-01T22:58:45.382598: step 1744, loss 0.580182.
Train: 2018-08-01T22:58:45.548125: step 1745, loss 0.477883.
Train: 2018-08-01T22:58:45.710721: step 1746, loss 0.580629.
Train: 2018-08-01T22:58:45.877245: step 1747, loss 0.545285.
Train: 2018-08-01T22:58:46.037816: step 1748, loss 0.631416.
Train: 2018-08-01T22:58:46.202376: step 1749, loss 0.6485.
Train: 2018-08-01T22:58:46.360982: step 1750, loss 0.597697.
Test: 2018-08-01T22:58:46.896520: step 1750, loss 0.548582.
Train: 2018-08-01T22:58:47.061081: step 1751, loss 0.629202.
Train: 2018-08-01T22:58:47.224644: step 1752, loss 0.5801.
Train: 2018-08-01T22:58:47.388206: step 1753, loss 0.49516.
Train: 2018-08-01T22:58:47.552766: step 1754, loss 0.563073.
Train: 2018-08-01T22:58:47.722312: step 1755, loss 0.597065.
Train: 2018-08-01T22:58:47.885875: step 1756, loss 0.612969.
Train: 2018-08-01T22:58:48.050435: step 1757, loss 0.563833.
Train: 2018-08-01T22:58:48.214028: step 1758, loss 0.596415.
Train: 2018-08-01T22:58:48.378558: step 1759, loss 0.530236.
Train: 2018-08-01T22:58:48.543117: step 1760, loss 0.5467.
Test: 2018-08-01T22:58:49.083704: step 1760, loss 0.54912.
Train: 2018-08-01T22:58:49.254242: step 1761, loss 0.546822.
Train: 2018-08-01T22:58:49.417779: step 1762, loss 0.52983.
Train: 2018-08-01T22:58:49.580345: step 1763, loss 0.563063.
Train: 2018-08-01T22:58:49.746899: step 1764, loss 0.612843.
Train: 2018-08-01T22:58:49.907471: step 1765, loss 0.612221.
Train: 2018-08-01T22:58:50.074027: step 1766, loss 0.431256.
Train: 2018-08-01T22:58:50.237589: step 1767, loss 0.530151.
Train: 2018-08-01T22:58:50.398190: step 1768, loss 0.59569.
Train: 2018-08-01T22:58:50.567706: step 1769, loss 0.612543.
Train: 2018-08-01T22:58:50.741241: step 1770, loss 0.529747.
Test: 2018-08-01T22:58:51.266836: step 1770, loss 0.549037.
Train: 2018-08-01T22:58:51.428406: step 1771, loss 0.513094.
Train: 2018-08-01T22:58:51.592965: step 1772, loss 0.529701.
Train: 2018-08-01T22:58:51.763510: step 1773, loss 0.51245.
Train: 2018-08-01T22:58:51.924110: step 1774, loss 0.479683.
Train: 2018-08-01T22:58:52.088640: step 1775, loss 0.462117.
Train: 2018-08-01T22:58:52.251205: step 1776, loss 0.5464.
Train: 2018-08-01T22:58:52.414780: step 1777, loss 0.596716.
Train: 2018-08-01T22:58:52.573345: step 1778, loss 0.682354.
Train: 2018-08-01T22:58:52.741895: step 1779, loss 0.648487.
Train: 2018-08-01T22:58:52.908450: step 1780, loss 0.630918.
Test: 2018-08-01T22:58:53.454987: step 1780, loss 0.548474.
Train: 2018-08-01T22:58:53.622539: step 1781, loss 0.511477.
Train: 2018-08-01T22:58:53.786113: step 1782, loss 0.545474.
Train: 2018-08-01T22:58:53.950693: step 1783, loss 0.563253.
Train: 2018-08-01T22:58:54.117246: step 1784, loss 0.562743.
Train: 2018-08-01T22:58:54.280805: step 1785, loss 0.528681.
Train: 2018-08-01T22:58:54.444369: step 1786, loss 0.631122.
Train: 2018-08-01T22:58:54.606908: step 1787, loss 0.596428.
Train: 2018-08-01T22:58:54.774486: step 1788, loss 0.665385.
Train: 2018-08-01T22:58:54.936029: step 1789, loss 0.528957.
Train: 2018-08-01T22:58:55.102583: step 1790, loss 0.528881.
Test: 2018-08-01T22:58:55.638152: step 1790, loss 0.548523.
Train: 2018-08-01T22:58:55.799720: step 1791, loss 0.545384.
Train: 2018-08-01T22:58:55.963301: step 1792, loss 0.664351.
Train: 2018-08-01T22:58:56.137815: step 1793, loss 0.545495.
Train: 2018-08-01T22:58:56.299384: step 1794, loss 0.630654.
Train: 2018-08-01T22:58:56.464941: step 1795, loss 0.664364.
Train: 2018-08-01T22:58:56.636483: step 1796, loss 0.529096.
Train: 2018-08-01T22:58:56.796088: step 1797, loss 0.529302.
Train: 2018-08-01T22:58:56.959631: step 1798, loss 0.546179.
Train: 2018-08-01T22:58:57.120189: step 1799, loss 0.529254.
Train: 2018-08-01T22:58:57.285778: step 1800, loss 0.562699.
Test: 2018-08-01T22:58:57.814365: step 1800, loss 0.548814.
Train: 2018-08-01T22:58:58.616597: step 1801, loss 0.612714.
Train: 2018-08-01T22:58:58.780162: step 1802, loss 0.562778.
Train: 2018-08-01T22:58:58.944746: step 1803, loss 0.579523.
Train: 2018-08-01T22:58:59.102300: step 1804, loss 0.579425.
Train: 2018-08-01T22:58:59.268855: step 1805, loss 0.579759.
Train: 2018-08-01T22:58:59.434412: step 1806, loss 0.421075.
Train: 2018-08-01T22:58:59.610940: step 1807, loss 0.596133.
Train: 2018-08-01T22:58:59.772533: step 1808, loss 0.496146.
Train: 2018-08-01T22:58:59.950033: step 1809, loss 0.512683.
Train: 2018-08-01T22:59:00.122602: step 1810, loss 0.629529.
Test: 2018-08-01T22:59:00.664124: step 1810, loss 0.548733.
Train: 2018-08-01T22:59:00.827687: step 1811, loss 0.679797.
Train: 2018-08-01T22:59:00.991250: step 1812, loss 0.529457.
Train: 2018-08-01T22:59:01.153814: step 1813, loss 0.529409.
Train: 2018-08-01T22:59:01.319373: step 1814, loss 0.646802.
Train: 2018-08-01T22:59:01.480940: step 1815, loss 0.4453.
Train: 2018-08-01T22:59:01.650488: step 1816, loss 0.562809.
Train: 2018-08-01T22:59:01.811084: step 1817, loss 0.579456.
Train: 2018-08-01T22:59:01.976641: step 1818, loss 0.545802.
Train: 2018-08-01T22:59:02.149154: step 1819, loss 0.511937.
Train: 2018-08-01T22:59:02.312749: step 1820, loss 0.579555.
Test: 2018-08-01T22:59:02.850311: step 1820, loss 0.548563.
Train: 2018-08-01T22:59:03.013843: step 1821, loss 0.630329.
Train: 2018-08-01T22:59:03.185384: step 1822, loss 0.681212.
Train: 2018-08-01T22:59:03.352936: step 1823, loss 0.562943.
Train: 2018-08-01T22:59:03.512536: step 1824, loss 0.579775.
Train: 2018-08-01T22:59:03.673081: step 1825, loss 0.528861.
Train: 2018-08-01T22:59:03.845650: step 1826, loss 0.545886.
Train: 2018-08-01T22:59:04.006216: step 1827, loss 0.529113.
Train: 2018-08-01T22:59:04.173743: step 1828, loss 0.47827.
Train: 2018-08-01T22:59:04.340328: step 1829, loss 0.613305.
Train: 2018-08-01T22:59:04.502894: step 1830, loss 0.545745.
Test: 2018-08-01T22:59:05.033445: step 1830, loss 0.548557.
Train: 2018-08-01T22:59:05.210969: step 1831, loss 0.562341.
Train: 2018-08-01T22:59:05.374587: step 1832, loss 0.511785.
Train: 2018-08-01T22:59:05.541086: step 1833, loss 0.630421.
Train: 2018-08-01T22:59:05.702685: step 1834, loss 0.375866.
Train: 2018-08-01T22:59:05.861258: step 1835, loss 0.597008.
Train: 2018-08-01T22:59:06.032803: step 1836, loss 0.528791.
Train: 2018-08-01T22:59:06.201346: step 1837, loss 0.597088.
Train: 2018-08-01T22:59:06.358925: step 1838, loss 0.614505.
Train: 2018-08-01T22:59:06.520500: step 1839, loss 0.493481.
Train: 2018-08-01T22:59:06.685030: step 1840, loss 0.511046.
Test: 2018-08-01T22:59:07.218602: step 1840, loss 0.548228.
Train: 2018-08-01T22:59:07.384159: step 1841, loss 0.597307.
Train: 2018-08-01T22:59:07.545728: step 1842, loss 0.562691.
Train: 2018-08-01T22:59:07.704335: step 1843, loss 0.458047.
Train: 2018-08-01T22:59:07.865904: step 1844, loss 0.563108.
Train: 2018-08-01T22:59:08.030441: step 1845, loss 0.580671.
Train: 2018-08-01T22:59:08.191003: step 1846, loss 0.632486.
Train: 2018-08-01T22:59:08.355563: step 1847, loss 0.581149.
Train: 2018-08-01T22:59:08.514170: step 1848, loss 0.597941.
Train: 2018-08-01T22:59:08.675707: step 1849, loss 0.474744.
Train: 2018-08-01T22:59:08.840298: step 1850, loss 0.598026.
Test: 2018-08-01T22:59:09.364864: step 1850, loss 0.548077.
Train: 2018-08-01T22:59:09.531420: step 1851, loss 0.545191.
Train: 2018-08-01T22:59:09.710939: step 1852, loss 0.527848.
Train: 2018-08-01T22:59:09.871511: step 1853, loss 0.474806.
Train: 2018-08-01T22:59:10.033079: step 1854, loss 0.527725.
Train: 2018-08-01T22:59:10.200656: step 1855, loss 0.59848.
Train: 2018-08-01T22:59:10.368208: step 1856, loss 0.579725.
Train: 2018-08-01T22:59:10.537760: step 1857, loss 0.580238.
Train: 2018-08-01T22:59:10.695342: step 1858, loss 0.633394.
Train: 2018-08-01T22:59:10.857905: step 1859, loss 0.438889.
Train: 2018-08-01T22:59:11.023432: step 1860, loss 0.54493.
Test: 2018-08-01T22:59:11.554013: step 1860, loss 0.547986.
Train: 2018-08-01T22:59:11.720567: step 1861, loss 0.598645.
Train: 2018-08-01T22:59:11.881167: step 1862, loss 0.509649.
Train: 2018-08-01T22:59:12.040738: step 1863, loss 0.508872.
Train: 2018-08-01T22:59:12.201314: step 1864, loss 0.580248.
Train: 2018-08-01T22:59:12.368866: step 1865, loss 0.526461.
Train: 2018-08-01T22:59:12.532430: step 1866, loss 0.509485.
Train: 2018-08-01T22:59:12.699949: step 1867, loss 0.544494.
Train: 2018-08-01T22:59:12.862514: step 1868, loss 0.527832.
Train: 2018-08-01T22:59:13.027075: step 1869, loss 0.544643.
Train: 2018-08-01T22:59:13.195626: step 1870, loss 0.491477.
Test: 2018-08-01T22:59:13.732205: step 1870, loss 0.547878.
Train: 2018-08-01T22:59:13.902734: step 1871, loss 0.545899.
Train: 2018-08-01T22:59:14.065300: step 1872, loss 0.761151.
Train: 2018-08-01T22:59:14.226867: step 1873, loss 0.581597.
Train: 2018-08-01T22:59:14.393448: step 1874, loss 0.634499.
Train: 2018-08-01T22:59:14.557019: step 1875, loss 0.562594.
Train: 2018-08-01T22:59:14.720548: step 1876, loss 0.545229.
Train: 2018-08-01T22:59:14.879151: step 1877, loss 0.56289.
Train: 2018-08-01T22:59:15.042715: step 1878, loss 0.509023.
Train: 2018-08-01T22:59:15.209273: step 1879, loss 0.653434.
Train: 2018-08-01T22:59:15.376793: step 1880, loss 0.687278.
Test: 2018-08-01T22:59:15.899396: step 1880, loss 0.547963.
Train: 2018-08-01T22:59:16.063982: step 1881, loss 0.545771.
Train: 2018-08-01T22:59:16.239518: step 1882, loss 0.615864.
Train: 2018-08-01T22:59:16.401056: step 1883, loss 0.544934.
Train: 2018-08-01T22:59:16.564619: step 1884, loss 0.580377.
Train: 2018-08-01T22:59:16.729205: step 1885, loss 0.545606.
Train: 2018-08-01T22:59:16.887780: step 1886, loss 0.544977.
Train: 2018-08-01T22:59:17.052346: step 1887, loss 0.614908.
Train: 2018-08-01T22:59:17.231835: step 1888, loss 0.563114.
Train: 2018-08-01T22:59:17.397393: step 1889, loss 0.562886.
Train: 2018-08-01T22:59:17.560980: step 1890, loss 0.768219.
Test: 2018-08-01T22:59:18.087572: step 1890, loss 0.548392.
Train: 2018-08-01T22:59:18.258122: step 1891, loss 0.528412.
Train: 2018-08-01T22:59:18.430630: step 1892, loss 0.562839.
Train: 2018-08-01T22:59:18.597186: step 1893, loss 0.646851.
Train: 2018-08-01T22:59:18.764764: step 1894, loss 0.579355.
Train: 2018-08-01T22:59:18.934285: step 1895, loss 0.478869.
Train: 2018-08-01T22:59:19.106823: step 1896, loss 0.512397.
Train: 2018-08-01T22:59:19.274405: step 1897, loss 0.479363.
Train: 2018-08-01T22:59:19.433972: step 1898, loss 0.545911.
Train: 2018-08-01T22:59:19.596513: step 1899, loss 0.478988.
Train: 2018-08-01T22:59:19.759105: step 1900, loss 0.596121.
Test: 2018-08-01T22:59:20.293650: step 1900, loss 0.548708.
Train: 2018-08-01T22:59:21.149197: step 1901, loss 0.613354.
Train: 2018-08-01T22:59:21.313757: step 1902, loss 0.562622.
Train: 2018-08-01T22:59:21.479315: step 1903, loss 0.545998.
Train: 2018-08-01T22:59:21.646878: step 1904, loss 0.545826.
Train: 2018-08-01T22:59:21.821401: step 1905, loss 0.61304.
Train: 2018-08-01T22:59:21.974990: step 1906, loss 0.562636.
Train: 2018-08-01T22:59:22.138553: step 1907, loss 0.579564.
Train: 2018-08-01T22:59:22.299123: step 1908, loss 0.461933.
Train: 2018-08-01T22:59:22.461721: step 1909, loss 0.528864.
Train: 2018-08-01T22:59:22.622290: step 1910, loss 0.680984.
Test: 2018-08-01T22:59:23.174782: step 1910, loss 0.548552.
Train: 2018-08-01T22:59:23.357296: step 1911, loss 0.528429.
Train: 2018-08-01T22:59:23.521855: step 1912, loss 0.613645.
Train: 2018-08-01T22:59:23.684420: step 1913, loss 0.528614.
Train: 2018-08-01T22:59:23.844991: step 1914, loss 0.461097.
Train: 2018-08-01T22:59:24.002569: step 1915, loss 0.545808.
Train: 2018-08-01T22:59:24.178100: step 1916, loss 0.629937.
Train: 2018-08-01T22:59:24.341694: step 1917, loss 0.512053.
Train: 2018-08-01T22:59:24.508248: step 1918, loss 0.477393.
Train: 2018-08-01T22:59:24.670787: step 1919, loss 0.665016.
Train: 2018-08-01T22:59:24.843323: step 1920, loss 0.545182.
Test: 2018-08-01T22:59:25.389862: step 1920, loss 0.548299.
Train: 2018-08-01T22:59:25.550463: step 1921, loss 0.476969.
Train: 2018-08-01T22:59:25.716986: step 1922, loss 0.562624.
Train: 2018-08-01T22:59:25.881592: step 1923, loss 0.683325.
Train: 2018-08-01T22:59:26.046125: step 1924, loss 0.545817.
Train: 2018-08-01T22:59:26.216658: step 1925, loss 0.66628.
Train: 2018-08-01T22:59:26.386227: step 1926, loss 0.61436.
Train: 2018-08-01T22:59:26.549791: step 1927, loss 0.545081.
Train: 2018-08-01T22:59:26.717343: step 1928, loss 0.562128.
Train: 2018-08-01T22:59:26.881903: step 1929, loss 0.528419.
Train: 2018-08-01T22:59:27.045460: step 1930, loss 0.666062.
Test: 2018-08-01T22:59:27.588982: step 1930, loss 0.548352.
Train: 2018-08-01T22:59:27.755538: step 1931, loss 0.511806.
Train: 2018-08-01T22:59:27.923120: step 1932, loss 0.494671.
Train: 2018-08-01T22:59:28.086653: step 1933, loss 0.613098.
Train: 2018-08-01T22:59:28.262208: step 1934, loss 0.545737.
Train: 2018-08-01T22:59:28.426742: step 1935, loss 0.545858.
Train: 2018-08-01T22:59:28.599282: step 1936, loss 0.613253.
Train: 2018-08-01T22:59:28.763875: step 1937, loss 0.545737.
Train: 2018-08-01T22:59:28.930396: step 1938, loss 0.477802.
Train: 2018-08-01T22:59:29.087989: step 1939, loss 0.545703.
Train: 2018-08-01T22:59:29.255553: step 1940, loss 0.510904.
Test: 2018-08-01T22:59:29.792092: step 1940, loss 0.548345.
Train: 2018-08-01T22:59:29.961665: step 1941, loss 0.579408.
Train: 2018-08-01T22:59:30.127222: step 1942, loss 0.597055.
Train: 2018-08-01T22:59:30.301756: step 1943, loss 0.493753.
Train: 2018-08-01T22:59:30.465294: step 1944, loss 0.580132.
Train: 2018-08-01T22:59:30.634870: step 1945, loss 0.527022.
Train: 2018-08-01T22:59:30.803390: step 1946, loss 0.597384.
Train: 2018-08-01T22:59:30.962989: step 1947, loss 0.545759.
Train: 2018-08-01T22:59:31.128521: step 1948, loss 0.545639.
Train: 2018-08-01T22:59:31.293109: step 1949, loss 0.64959.
Train: 2018-08-01T22:59:31.463639: step 1950, loss 0.492743.
Test: 2018-08-01T22:59:32.004204: step 1950, loss 0.548151.
Train: 2018-08-01T22:59:32.168739: step 1951, loss 0.510696.
Train: 2018-08-01T22:59:32.336317: step 1952, loss 0.511407.
Train: 2018-08-01T22:59:32.504841: step 1953, loss 0.580382.
Train: 2018-08-01T22:59:32.670429: step 1954, loss 0.596264.
Train: 2018-08-01T22:59:32.836954: step 1955, loss 0.579788.
Train: 2018-08-01T22:59:33.004505: step 1956, loss 0.561577.
Train: 2018-08-01T22:59:33.169098: step 1957, loss 0.544768.
Train: 2018-08-01T22:59:33.334623: step 1958, loss 0.45735.
Train: 2018-08-01T22:59:33.496221: step 1959, loss 0.598196.
Train: 2018-08-01T22:59:33.660752: step 1960, loss 0.492308.
Test: 2018-08-01T22:59:34.205340: step 1960, loss 0.547969.
Train: 2018-08-01T22:59:34.374842: step 1961, loss 0.545177.
Train: 2018-08-01T22:59:34.542393: step 1962, loss 0.455081.
Train: 2018-08-01T22:59:34.709946: step 1963, loss 0.653637.
Train: 2018-08-01T22:59:34.873509: step 1964, loss 0.633847.
Train: 2018-08-01T22:59:35.063003: step 1965, loss 0.56205.
Train: 2018-08-01T22:59:35.231553: step 1966, loss 0.688044.
Train: 2018-08-01T22:59:35.437027: step 1967, loss 0.560502.
Train: 2018-08-01T22:59:35.658412: step 1968, loss 0.615609.
Train: 2018-08-01T22:59:35.892785: step 1969, loss 0.52671.
Train: 2018-08-01T22:59:36.070312: step 1970, loss 0.528823.
Test: 2018-08-01T22:59:36.618844: step 1970, loss 0.547968.
Train: 2018-08-01T22:59:36.782435: step 1971, loss 0.510594.
Train: 2018-08-01T22:59:36.949958: step 1972, loss 0.493043.
Train: 2018-08-01T22:59:37.116538: step 1973, loss 0.651091.
Train: 2018-08-01T22:59:37.287057: step 1974, loss 0.64888.
Train: 2018-08-01T22:59:37.452614: step 1975, loss 0.614339.
Train: 2018-08-01T22:59:37.614182: step 1976, loss 0.582296.
Train: 2018-08-01T22:59:37.778742: step 1977, loss 0.614348.
Train: 2018-08-01T22:59:37.951281: step 1978, loss 0.510033.
Train: 2018-08-01T22:59:38.120828: step 1979, loss 0.562741.
Train: 2018-08-01T22:59:38.281399: step 1980, loss 0.52867.
Test: 2018-08-01T22:59:38.817965: step 1980, loss 0.54821.
Train: 2018-08-01T22:59:38.985516: step 1981, loss 0.528592.
Train: 2018-08-01T22:59:39.150104: step 1982, loss 0.614054.
Train: 2018-08-01T22:59:39.312667: step 1983, loss 0.562264.
Train: 2018-08-01T22:59:39.484184: step 1984, loss 0.511782.
Train: 2018-08-01T22:59:39.643788: step 1985, loss 0.578166.
Train: 2018-08-01T22:59:39.805349: step 1986, loss 0.509821.
Train: 2018-08-01T22:59:39.969920: step 1987, loss 0.459949.
Train: 2018-08-01T22:59:40.137436: step 1988, loss 0.494106.
Train: 2018-08-01T22:59:40.297010: step 1989, loss 0.510202.
Train: 2018-08-01T22:59:40.459576: step 1990, loss 0.545872.
Test: 2018-08-01T22:59:40.993159: step 1990, loss 0.548088.
Train: 2018-08-01T22:59:41.156712: step 1991, loss 0.474552.
Train: 2018-08-01T22:59:41.317283: step 1992, loss 0.614043.
Train: 2018-08-01T22:59:41.482841: step 1993, loss 0.526247.
Train: 2018-08-01T22:59:41.640444: step 1994, loss 0.686499.
Train: 2018-08-01T22:59:41.804983: step 1995, loss 0.54275.
Train: 2018-08-01T22:59:41.972531: step 1996, loss 0.670697.
Train: 2018-08-01T22:59:42.140085: step 1997, loss 0.597334.
Train: 2018-08-01T22:59:42.297663: step 1998, loss 0.544814.
Train: 2018-08-01T22:59:42.462223: step 1999, loss 0.526448.
Train: 2018-08-01T22:59:42.623790: step 2000, loss 0.597712.
Test: 2018-08-01T22:59:43.157390: step 2000, loss 0.547935.
Train: 2018-08-01T22:59:43.939672: step 2001, loss 0.52502.
Train: 2018-08-01T22:59:44.102255: step 2002, loss 0.581813.
Train: 2018-08-01T22:59:44.270816: step 2003, loss 0.774906.
Train: 2018-08-01T22:59:44.432380: step 2004, loss 0.560926.
Train: 2018-08-01T22:59:44.594919: step 2005, loss 0.595602.
Train: 2018-08-01T22:59:44.756519: step 2006, loss 0.512175.
Train: 2018-08-01T22:59:44.918084: step 2007, loss 0.493129.
Train: 2018-08-01T22:59:45.080651: step 2008, loss 0.493292.
Train: 2018-08-01T22:59:45.247210: step 2009, loss 0.580804.
Train: 2018-08-01T22:59:45.418743: step 2010, loss 0.562304.
Test: 2018-08-01T22:59:45.964290: step 2010, loss 0.548129.
Train: 2018-08-01T22:59:46.127842: step 2011, loss 0.493907.
Train: 2018-08-01T22:59:46.293385: step 2012, loss 0.54523.
Train: 2018-08-01T22:59:46.455945: step 2013, loss 0.528519.
Train: 2018-08-01T22:59:46.616515: step 2014, loss 0.455413.
Train: 2018-08-01T22:59:46.780078: step 2015, loss 0.614546.
Train: 2018-08-01T22:59:46.948642: step 2016, loss 0.527704.
Train: 2018-08-01T22:59:47.115208: step 2017, loss 0.560236.
Train: 2018-08-01T22:59:47.281738: step 2018, loss 0.509804.
Train: 2018-08-01T22:59:47.438343: step 2019, loss 0.648137.
Train: 2018-08-01T22:59:47.601880: step 2020, loss 0.403762.
Test: 2018-08-01T22:59:48.151414: step 2020, loss 0.547869.
Train: 2018-08-01T22:59:48.314002: step 2021, loss 0.620512.
Train: 2018-08-01T22:59:48.482528: step 2022, loss 0.495093.
Train: 2018-08-01T22:59:48.648085: step 2023, loss 0.514683.
Train: 2018-08-01T22:59:48.809678: step 2024, loss 0.473983.
Train: 2018-08-01T22:59:48.986180: step 2025, loss 0.52545.
Train: 2018-08-01T22:59:49.154763: step 2026, loss 0.580741.
Train: 2018-08-01T22:59:49.315325: step 2027, loss 0.547952.
Train: 2018-08-01T22:59:49.475871: step 2028, loss 0.508756.
Train: 2018-08-01T22:59:49.637464: step 2029, loss 0.692875.
Train: 2018-08-01T22:59:49.800005: step 2030, loss 0.580852.
Test: 2018-08-01T22:59:50.343552: step 2030, loss 0.547786.
Train: 2018-08-01T22:59:50.507115: step 2031, loss 0.560083.
Train: 2018-08-01T22:59:50.680651: step 2032, loss 0.525637.
Train: 2018-08-01T22:59:50.843216: step 2033, loss 0.562891.
Train: 2018-08-01T22:59:51.011766: step 2034, loss 0.397672.
Train: 2018-08-01T22:59:51.185301: step 2035, loss 0.599832.
Train: 2018-08-01T22:59:51.359861: step 2036, loss 0.614632.
Train: 2018-08-01T22:59:51.525417: step 2037, loss 0.616633.
Train: 2018-08-01T22:59:51.686987: step 2038, loss 0.708417.
Train: 2018-08-01T22:59:51.847559: step 2039, loss 0.602488.
Train: 2018-08-01T22:59:52.011093: step 2040, loss 0.545359.
Test: 2018-08-01T22:59:52.547685: step 2040, loss 0.547827.
Train: 2018-08-01T22:59:52.714215: step 2041, loss 0.455499.
Train: 2018-08-01T22:59:52.873813: step 2042, loss 0.598406.
Train: 2018-08-01T22:59:53.035386: step 2043, loss 0.529572.
Train: 2018-08-01T22:59:53.197947: step 2044, loss 0.583144.
Train: 2018-08-01T22:59:53.374450: step 2045, loss 0.632847.
Train: 2018-08-01T22:59:53.539035: step 2046, loss 0.492461.
Train: 2018-08-01T22:59:53.701602: step 2047, loss 0.459647.
Train: 2018-08-01T22:59:53.867133: step 2048, loss 0.543098.
Train: 2018-08-01T22:59:54.032721: step 2049, loss 0.561889.
Train: 2018-08-01T22:59:54.193286: step 2050, loss 0.546261.
Test: 2018-08-01T22:59:54.722845: step 2050, loss 0.547978.
Train: 2018-08-01T22:59:54.883441: step 2051, loss 0.52765.
Train: 2018-08-01T22:59:55.042989: step 2052, loss 0.580886.
Train: 2018-08-01T22:59:55.204590: step 2053, loss 0.580009.
Train: 2018-08-01T22:59:55.368151: step 2054, loss 0.650539.
Train: 2018-08-01T22:59:55.536700: step 2055, loss 0.527402.
Train: 2018-08-01T22:59:55.775067: step 2056, loss 0.580829.
Train: 2018-08-01T22:59:55.937633: step 2057, loss 0.544629.
Train: 2018-08-01T22:59:56.107180: step 2058, loss 0.580575.
Train: 2018-08-01T22:59:56.281713: step 2059, loss 0.597163.
Train: 2018-08-01T22:59:56.451259: step 2060, loss 0.526731.
Test: 2018-08-01T22:59:56.994807: step 2060, loss 0.548102.
Train: 2018-08-01T22:59:57.174357: step 2061, loss 0.596825.
Train: 2018-08-01T22:59:57.351888: step 2062, loss 0.579313.
Train: 2018-08-01T22:59:57.518438: step 2063, loss 0.580326.
Train: 2018-08-01T22:59:57.683963: step 2064, loss 0.632998.
Train: 2018-08-01T22:59:57.856522: step 2065, loss 0.648113.
Train: 2018-08-01T22:59:58.021063: step 2066, loss 0.546023.
Train: 2018-08-01T22:59:58.187641: step 2067, loss 0.563332.
Train: 2018-08-01T22:59:58.355169: step 2068, loss 0.545678.
Train: 2018-08-01T22:59:58.515741: step 2069, loss 0.494593.
Train: 2018-08-01T22:59:58.698253: step 2070, loss 0.444223.
Test: 2018-08-01T22:59:59.232858: step 2070, loss 0.548421.
Train: 2018-08-01T22:59:59.397384: step 2071, loss 0.680854.
Train: 2018-08-01T22:59:59.563969: step 2072, loss 0.579752.
Train: 2018-08-01T22:59:59.740467: step 2073, loss 0.596619.
Train: 2018-08-01T22:59:59.905026: step 2074, loss 0.59584.
Train: 2018-08-01T23:00:00.082552: step 2075, loss 0.545115.
Train: 2018-08-01T23:00:00.247112: step 2076, loss 0.629566.
Train: 2018-08-01T23:00:00.405688: step 2077, loss 0.545611.
Train: 2018-08-01T23:00:00.574262: step 2078, loss 0.494911.
Train: 2018-08-01T23:00:00.734839: step 2079, loss 0.545583.
Train: 2018-08-01T23:00:00.893385: step 2080, loss 0.646142.
Test: 2018-08-01T23:00:01.429951: step 2080, loss 0.548601.
Train: 2018-08-01T23:00:01.631412: step 2081, loss 0.596358.
Train: 2018-08-01T23:00:01.821902: step 2082, loss 0.546004.
Train: 2018-08-01T23:00:01.993443: step 2083, loss 0.562875.
Train: 2018-08-01T23:00:02.174959: step 2084, loss 0.546126.
Train: 2018-08-01T23:00:02.359467: step 2085, loss 0.628582.
Train: 2018-08-01T23:00:02.520041: step 2086, loss 0.529197.
Train: 2018-08-01T23:00:02.689583: step 2087, loss 0.512309.
Train: 2018-08-01T23:00:02.864116: step 2088, loss 0.511873.
Train: 2018-08-01T23:00:03.025685: step 2089, loss 0.663119.
Train: 2018-08-01T23:00:03.183262: step 2090, loss 0.529202.
Test: 2018-08-01T23:00:03.727815: step 2090, loss 0.548609.
Train: 2018-08-01T23:00:03.903338: step 2091, loss 0.495341.
Train: 2018-08-01T23:00:04.072917: step 2092, loss 0.713583.
Train: 2018-08-01T23:00:04.240437: step 2093, loss 0.663414.
Train: 2018-08-01T23:00:04.406992: step 2094, loss 0.563032.
Train: 2018-08-01T23:00:04.575569: step 2095, loss 0.529021.
Train: 2018-08-01T23:00:04.735114: step 2096, loss 0.580345.
Train: 2018-08-01T23:00:04.903664: step 2097, loss 0.580087.
Train: 2018-08-01T23:00:05.066229: step 2098, loss 0.54611.
Train: 2018-08-01T23:00:05.230790: step 2099, loss 0.562893.
Train: 2018-08-01T23:00:05.395350: step 2100, loss 0.57968.
Test: 2018-08-01T23:00:05.928924: step 2100, loss 0.548831.
Train: 2018-08-01T23:00:06.708577: step 2101, loss 0.545773.
Train: 2018-08-01T23:00:06.876122: step 2102, loss 0.596698.
Train: 2018-08-01T23:00:07.040658: step 2103, loss 0.56228.
Train: 2018-08-01T23:00:07.211202: step 2104, loss 0.645846.
Train: 2018-08-01T23:00:07.374764: step 2105, loss 0.628781.
Train: 2018-08-01T23:00:07.548326: step 2106, loss 0.49658.
Train: 2018-08-01T23:00:07.728818: step 2107, loss 0.492567.
Train: 2018-08-01T23:00:07.912327: step 2108, loss 0.546529.
Train: 2018-08-01T23:00:08.086861: step 2109, loss 0.529509.
Train: 2018-08-01T23:00:08.258417: step 2110, loss 0.529167.
Test: 2018-08-01T23:00:08.788011: step 2110, loss 0.548894.
Train: 2018-08-01T23:00:08.947560: step 2111, loss 0.61195.
Train: 2018-08-01T23:00:09.124113: step 2112, loss 0.546303.
Train: 2018-08-01T23:00:09.281695: step 2113, loss 0.5301.
Train: 2018-08-01T23:00:09.454231: step 2114, loss 0.529626.
Train: 2018-08-01T23:00:09.623784: step 2115, loss 0.712477.
Train: 2018-08-01T23:00:09.794296: step 2116, loss 0.445162.
Train: 2018-08-01T23:00:09.972854: step 2117, loss 0.579689.
Train: 2018-08-01T23:00:10.138376: step 2118, loss 0.49604.
Train: 2018-08-01T23:00:10.311944: step 2119, loss 0.529353.
Train: 2018-08-01T23:00:10.479483: step 2120, loss 0.595924.
Test: 2018-08-01T23:00:11.026005: step 2120, loss 0.54849.
Train: 2018-08-01T23:00:11.196548: step 2121, loss 0.477775.
Train: 2018-08-01T23:00:11.359113: step 2122, loss 0.51204.
Train: 2018-08-01T23:00:11.522676: step 2123, loss 0.544887.
Train: 2018-08-01T23:00:11.705188: step 2124, loss 0.562761.
Train: 2018-08-01T23:00:11.881716: step 2125, loss 0.528326.
Train: 2018-08-01T23:00:12.050267: step 2126, loss 0.580153.
Train: 2018-08-01T23:00:12.217829: step 2127, loss 0.458655.
Train: 2018-08-01T23:00:12.381412: step 2128, loss 0.509251.
Train: 2018-08-01T23:00:12.557939: step 2129, loss 0.526267.
Train: 2018-08-01T23:00:12.719478: step 2130, loss 0.528619.
Test: 2018-08-01T23:00:13.268009: step 2130, loss 0.547879.
Train: 2018-08-01T23:00:13.434245: step 2131, loss 0.475483.
Train: 2018-08-01T23:00:13.597814: step 2132, loss 0.598331.
Train: 2018-08-01T23:00:13.763365: step 2133, loss 0.561736.
Train: 2018-08-01T23:00:13.921972: step 2134, loss 0.614588.
Train: 2018-08-01T23:00:14.089493: step 2135, loss 0.420654.
Train: 2018-08-01T23:00:14.256049: step 2136, loss 0.726062.
Train: 2018-08-01T23:00:14.420607: step 2137, loss 0.600995.
Train: 2018-08-01T23:00:14.586165: step 2138, loss 0.635106.
Train: 2018-08-01T23:00:14.755713: step 2139, loss 0.672428.
Train: 2018-08-01T23:00:14.935232: step 2140, loss 0.633751.
Test: 2018-08-01T23:00:15.483786: step 2140, loss 0.547783.
Train: 2018-08-01T23:00:15.646357: step 2141, loss 0.633153.
Train: 2018-08-01T23:00:15.841810: step 2142, loss 0.508937.
Train: 2018-08-01T23:00:16.008390: step 2143, loss 0.561766.
Train: 2018-08-01T23:00:16.172955: step 2144, loss 0.51063.
Train: 2018-08-01T23:00:16.334491: step 2145, loss 0.600608.
Train: 2018-08-01T23:00:16.505067: step 2146, loss 0.422211.
Train: 2018-08-01T23:00:16.672589: step 2147, loss 0.599246.
Train: 2018-08-01T23:00:16.835154: step 2148, loss 0.631612.
Train: 2018-08-01T23:00:16.995724: step 2149, loss 0.579117.
Train: 2018-08-01T23:00:17.158290: step 2150, loss 0.5114.
Test: 2018-08-01T23:00:17.705825: step 2150, loss 0.548018.
Train: 2018-08-01T23:00:17.870385: step 2151, loss 0.510876.
Train: 2018-08-01T23:00:18.032982: step 2152, loss 0.493546.
Train: 2018-08-01T23:00:18.200504: step 2153, loss 0.528186.
Train: 2018-08-01T23:00:18.374060: step 2154, loss 0.527363.
Train: 2018-08-01T23:00:18.542589: step 2155, loss 0.685285.
Train: 2018-08-01T23:00:18.703159: step 2156, loss 0.509781.
Train: 2018-08-01T23:00:18.864727: step 2157, loss 0.597555.
Train: 2018-08-01T23:00:19.030321: step 2158, loss 0.580533.
Train: 2018-08-01T23:00:19.201839: step 2159, loss 0.545615.
Train: 2018-08-01T23:00:19.365414: step 2160, loss 0.615205.
Test: 2018-08-01T23:00:19.909934: step 2160, loss 0.548055.
Train: 2018-08-01T23:00:20.081475: step 2161, loss 0.528155.
Train: 2018-08-01T23:00:20.255038: step 2162, loss 0.683701.
Train: 2018-08-01T23:00:20.429570: step 2163, loss 0.682879.
Train: 2018-08-01T23:00:20.610098: step 2164, loss 0.477045.
Train: 2018-08-01T23:00:20.778612: step 2165, loss 0.579752.
Train: 2018-08-01T23:00:20.951151: step 2166, loss 0.69845.
Train: 2018-08-01T23:00:21.120697: step 2167, loss 0.477398.
Train: 2018-08-01T23:00:21.286254: step 2168, loss 0.477494.
Train: 2018-08-01T23:00:21.446824: step 2169, loss 0.545244.
Train: 2018-08-01T23:00:21.610414: step 2170, loss 0.545577.
Test: 2018-08-01T23:00:22.149955: step 2170, loss 0.548378.
Train: 2018-08-01T23:00:22.316500: step 2171, loss 0.52875.
Train: 2018-08-01T23:00:22.481060: step 2172, loss 0.630926.
Train: 2018-08-01T23:00:22.644623: step 2173, loss 0.595985.
Train: 2018-08-01T23:00:22.805220: step 2174, loss 0.478329.
Train: 2018-08-01T23:00:22.971749: step 2175, loss 0.596557.
Train: 2018-08-01T23:00:23.138303: step 2176, loss 0.494327.
Train: 2018-08-01T23:00:23.303860: step 2177, loss 0.47735.
Train: 2018-08-01T23:00:23.461440: step 2178, loss 0.562866.
Train: 2018-08-01T23:00:23.627029: step 2179, loss 0.460222.
Train: 2018-08-01T23:00:23.796544: step 2180, loss 0.477031.
Test: 2018-08-01T23:00:24.332113: step 2180, loss 0.548158.
Train: 2018-08-01T23:00:24.501659: step 2181, loss 0.528651.
Train: 2018-08-01T23:00:24.674225: step 2182, loss 0.597522.
Train: 2018-08-01T23:00:24.840753: step 2183, loss 0.614187.
Train: 2018-08-01T23:00:25.007307: step 2184, loss 0.476172.
Train: 2018-08-01T23:00:25.167879: step 2185, loss 0.527591.
Train: 2018-08-01T23:00:25.332463: step 2186, loss 0.632142.
Train: 2018-08-01T23:00:25.494041: step 2187, loss 0.579763.
Train: 2018-08-01T23:00:25.658565: step 2188, loss 0.545787.
Train: 2018-08-01T23:00:25.827115: step 2189, loss 0.650469.
Train: 2018-08-01T23:00:25.989682: step 2190, loss 0.633335.
Test: 2018-08-01T23:00:26.519265: step 2190, loss 0.547899.
Train: 2018-08-01T23:00:26.682854: step 2191, loss 0.579831.
Train: 2018-08-01T23:00:26.861365: step 2192, loss 0.527146.
Train: 2018-08-01T23:00:27.023916: step 2193, loss 0.54502.
Train: 2018-08-01T23:00:27.189507: step 2194, loss 0.59778.
Train: 2018-08-01T23:00:27.346089: step 2195, loss 0.526507.
Train: 2018-08-01T23:00:27.517596: step 2196, loss 0.563029.
Train: 2018-08-01T23:00:27.682156: step 2197, loss 0.545153.
Train: 2018-08-01T23:00:27.842752: step 2198, loss 0.650512.
Train: 2018-08-01T23:00:28.006291: step 2199, loss 0.719266.
Train: 2018-08-01T23:00:28.171888: step 2200, loss 0.63211.
Test: 2018-08-01T23:00:28.699437: step 2200, loss 0.548072.
Train: 2018-08-01T23:00:29.480023: step 2201, loss 0.614713.
Train: 2018-08-01T23:00:29.645570: step 2202, loss 0.528233.
Train: 2018-08-01T23:00:29.808166: step 2203, loss 0.682559.
Train: 2018-08-01T23:00:29.968731: step 2204, loss 0.682191.
Train: 2018-08-01T23:00:30.131272: step 2205, loss 0.51184.
Train: 2018-08-01T23:00:30.291869: step 2206, loss 0.562566.
Train: 2018-08-01T23:00:30.453435: step 2207, loss 0.512485.
Train: 2018-08-01T23:00:30.619992: step 2208, loss 0.446186.
Train: 2018-08-01T23:00:30.787518: step 2209, loss 0.629507.
Train: 2018-08-01T23:00:30.955069: step 2210, loss 0.512903.
Test: 2018-08-01T23:00:31.492633: step 2210, loss 0.548783.
Train: 2018-08-01T23:00:31.654200: step 2211, loss 0.662094.
Train: 2018-08-01T23:00:31.818786: step 2212, loss 0.546013.
Train: 2018-08-01T23:00:31.986313: step 2213, loss 0.628724.
Train: 2018-08-01T23:00:32.146910: step 2214, loss 0.562921.
Train: 2018-08-01T23:00:32.313469: step 2215, loss 0.529687.
Train: 2018-08-01T23:00:32.477002: step 2216, loss 0.661374.
Train: 2018-08-01T23:00:32.641567: step 2217, loss 0.464518.
Train: 2018-08-01T23:00:32.808146: step 2218, loss 0.579438.
Train: 2018-08-01T23:00:32.974671: step 2219, loss 0.51386.
Train: 2018-08-01T23:00:33.153227: step 2220, loss 0.562957.
Test: 2018-08-01T23:00:33.690757: step 2220, loss 0.54907.
Train: 2018-08-01T23:00:33.883242: step 2221, loss 0.562888.
Train: 2018-08-01T23:00:34.048799: step 2222, loss 0.595477.
Train: 2018-08-01T23:00:34.238293: step 2223, loss 0.677501.
Train: 2018-08-01T23:00:34.428783: step 2224, loss 0.513669.
Train: 2018-08-01T23:00:34.709034: step 2225, loss 0.497352.
Train: 2018-08-01T23:00:34.897530: step 2226, loss 0.628274.
Train: 2018-08-01T23:00:35.052141: step 2227, loss 0.611955.
Train: 2018-08-01T23:00:35.216707: step 2228, loss 0.562735.
Train: 2018-08-01T23:00:35.375253: step 2229, loss 0.546616.
Train: 2018-08-01T23:00:35.539839: step 2230, loss 0.513792.
Test: 2018-08-01T23:00:36.077376: step 2230, loss 0.54909.
Train: 2018-08-01T23:00:36.240963: step 2231, loss 0.562756.
Train: 2018-08-01T23:00:36.407525: step 2232, loss 0.628169.
Train: 2018-08-01T23:00:36.569062: step 2233, loss 0.480901.
Train: 2018-08-01T23:00:36.741631: step 2234, loss 0.595605.
Train: 2018-08-01T23:00:36.904197: step 2235, loss 0.562635.
Train: 2018-08-01T23:00:37.062742: step 2236, loss 0.529935.
Train: 2018-08-01T23:00:37.227301: step 2237, loss 0.513237.
Train: 2018-08-01T23:00:37.394855: step 2238, loss 0.579149.
Train: 2018-08-01T23:00:37.559415: step 2239, loss 0.52971.
Train: 2018-08-01T23:00:37.726997: step 2240, loss 0.595834.
Test: 2018-08-01T23:00:38.244583: step 2240, loss 0.548744.
Train: 2018-08-01T23:00:38.409169: step 2241, loss 0.57945.
Train: 2018-08-01T23:00:38.575722: step 2242, loss 0.529363.
Train: 2018-08-01T23:00:38.739287: step 2243, loss 0.629253.
Train: 2018-08-01T23:00:38.902822: step 2244, loss 0.562549.
Train: 2018-08-01T23:00:39.067383: step 2245, loss 0.562608.
Train: 2018-08-01T23:00:39.234936: step 2246, loss 0.512503.
Train: 2018-08-01T23:00:39.397500: step 2247, loss 0.612782.
Train: 2018-08-01T23:00:39.562092: step 2248, loss 0.679818.
Train: 2018-08-01T23:00:39.723630: step 2249, loss 0.56256.
Train: 2018-08-01T23:00:39.885225: step 2250, loss 0.612711.
Test: 2018-08-01T23:00:40.421776: step 2250, loss 0.548639.
Train: 2018-08-01T23:00:40.594327: step 2251, loss 0.529276.
Train: 2018-08-01T23:00:40.762851: step 2252, loss 0.62944.
Train: 2018-08-01T23:00:40.936387: step 2253, loss 0.629212.
Train: 2018-08-01T23:00:41.098971: step 2254, loss 0.579438.
Train: 2018-08-01T23:00:41.260520: step 2255, loss 0.529589.
Train: 2018-08-01T23:00:41.423118: step 2256, loss 0.529573.
Train: 2018-08-01T23:00:41.585652: step 2257, loss 0.496437.
Train: 2018-08-01T23:00:41.750243: step 2258, loss 0.711707.
Train: 2018-08-01T23:00:41.919758: step 2259, loss 0.52949.
Train: 2018-08-01T23:00:42.093294: step 2260, loss 0.513169.
Test: 2018-08-01T23:00:42.646221: step 2260, loss 0.548875.
Train: 2018-08-01T23:00:42.812776: step 2261, loss 0.579216.
Train: 2018-08-01T23:00:42.973377: step 2262, loss 0.496708.
Train: 2018-08-01T23:00:43.140925: step 2263, loss 0.529656.
Train: 2018-08-01T23:00:43.321416: step 2264, loss 0.612406.
Train: 2018-08-01T23:00:43.483981: step 2265, loss 0.595861.
Train: 2018-08-01T23:00:43.650536: step 2266, loss 0.628816.
Train: 2018-08-01T23:00:43.826067: step 2267, loss 0.662062.
Train: 2018-08-01T23:00:44.025534: step 2268, loss 0.529643.
Train: 2018-08-01T23:00:44.192089: step 2269, loss 0.6287.
Train: 2018-08-01T23:00:44.358643: step 2270, loss 0.546155.
Test: 2018-08-01T23:00:44.900196: step 2270, loss 0.548937.
Train: 2018-08-01T23:00:45.079716: step 2271, loss 0.562752.
Train: 2018-08-01T23:00:45.241284: step 2272, loss 0.644971.
Train: 2018-08-01T23:00:45.402852: step 2273, loss 0.480604.
Train: 2018-08-01T23:00:45.565448: step 2274, loss 0.579218.
Train: 2018-08-01T23:00:45.735961: step 2275, loss 0.677509.
Train: 2018-08-01T23:00:45.910525: step 2276, loss 0.579235.
Train: 2018-08-01T23:00:46.080042: step 2277, loss 0.628036.
Train: 2018-08-01T23:00:46.249590: step 2278, loss 0.400095.
Train: 2018-08-01T23:00:46.415172: step 2279, loss 0.530204.
Train: 2018-08-01T23:00:46.578709: step 2280, loss 0.497643.
Test: 2018-08-01T23:00:47.106299: step 2280, loss 0.549113.
Train: 2018-08-01T23:00:47.278836: step 2281, loss 0.513803.
Train: 2018-08-01T23:00:47.440405: step 2282, loss 0.628175.
Train: 2018-08-01T23:00:47.600004: step 2283, loss 0.464262.
Train: 2018-08-01T23:00:47.762546: step 2284, loss 0.546399.
Train: 2018-08-01T23:00:47.925109: step 2285, loss 0.612213.
Train: 2018-08-01T23:00:48.090666: step 2286, loss 0.595854.
Train: 2018-08-01T23:00:48.250270: step 2287, loss 0.562615.
Train: 2018-08-01T23:00:48.408844: step 2288, loss 0.579517.
Train: 2018-08-01T23:00:48.570412: step 2289, loss 0.512671.
Train: 2018-08-01T23:00:48.735966: step 2290, loss 0.579328.
Test: 2018-08-01T23:00:49.273505: step 2290, loss 0.548611.
Train: 2018-08-01T23:00:49.444048: step 2291, loss 0.696329.
Train: 2018-08-01T23:00:49.610604: step 2292, loss 0.445855.
Train: 2018-08-01T23:00:49.786148: step 2293, loss 0.545861.
Train: 2018-08-01T23:00:49.946706: step 2294, loss 0.629634.
Train: 2018-08-01T23:00:50.105282: step 2295, loss 0.512241.
Train: 2018-08-01T23:00:50.269867: step 2296, loss 0.61297.
Train: 2018-08-01T23:00:50.435399: step 2297, loss 0.562516.
Train: 2018-08-01T23:00:50.607937: step 2298, loss 0.478563.
Train: 2018-08-01T23:00:50.772497: step 2299, loss 0.66355.
Train: 2018-08-01T23:00:50.948028: step 2300, loss 0.545635.
Test: 2018-08-01T23:00:51.467639: step 2300, loss 0.548455.
Train: 2018-08-01T23:00:52.258146: step 2301, loss 0.629875.
Train: 2018-08-01T23:00:52.423704: step 2302, loss 0.663458.
Train: 2018-08-01T23:00:52.590258: step 2303, loss 0.646632.
Train: 2018-08-01T23:00:52.752825: step 2304, loss 0.612684.
Train: 2018-08-01T23:00:52.911425: step 2305, loss 0.529179.
Train: 2018-08-01T23:00:53.088951: step 2306, loss 0.512712.
Train: 2018-08-01T23:00:53.257507: step 2307, loss 0.562723.
Train: 2018-08-01T23:00:53.430045: step 2308, loss 0.595793.
Train: 2018-08-01T23:00:53.595572: step 2309, loss 0.512817.
Train: 2018-08-01T23:00:53.760131: step 2310, loss 0.595655.
Test: 2018-08-01T23:00:54.295699: step 2310, loss 0.548775.
Train: 2018-08-01T23:00:54.467242: step 2311, loss 0.46313.
Train: 2018-08-01T23:00:54.632802: step 2312, loss 0.579199.
Train: 2018-08-01T23:00:54.800350: step 2313, loss 0.512872.
Train: 2018-08-01T23:00:54.962947: step 2314, loss 0.579156.
Train: 2018-08-01T23:00:55.135455: step 2315, loss 0.695928.
Train: 2018-08-01T23:00:55.295027: step 2316, loss 0.545981.
Train: 2018-08-01T23:00:55.459595: step 2317, loss 0.546017.
Train: 2018-08-01T23:00:55.627139: step 2318, loss 0.612345.
Train: 2018-08-01T23:00:55.800676: step 2319, loss 0.496108.
Train: 2018-08-01T23:00:55.978228: step 2320, loss 0.546164.
Test: 2018-08-01T23:00:56.526736: step 2320, loss 0.548665.
Train: 2018-08-01T23:00:56.698277: step 2321, loss 0.47938.
Train: 2018-08-01T23:00:56.860842: step 2322, loss 0.512567.
Train: 2018-08-01T23:00:57.032383: step 2323, loss 0.57936.
Train: 2018-08-01T23:00:57.204922: step 2324, loss 0.529107.
Train: 2018-08-01T23:00:57.368485: step 2325, loss 0.528798.
Train: 2018-08-01T23:00:57.542021: step 2326, loss 0.562462.
Train: 2018-08-01T23:00:57.713563: step 2327, loss 0.54569.
Train: 2018-08-01T23:00:57.883140: step 2328, loss 0.511685.
Train: 2018-08-01T23:00:58.041685: step 2329, loss 0.545358.
Train: 2018-08-01T23:00:58.209237: step 2330, loss 0.528437.
Test: 2018-08-01T23:00:58.740816: step 2330, loss 0.548142.
Train: 2018-08-01T23:00:58.902384: step 2331, loss 0.442415.
Train: 2018-08-01T23:00:59.063952: step 2332, loss 0.59702.
Train: 2018-08-01T23:00:59.224523: step 2333, loss 0.666169.
Train: 2018-08-01T23:00:59.391110: step 2334, loss 0.562866.
Train: 2018-08-01T23:00:59.552647: step 2335, loss 0.597247.
Train: 2018-08-01T23:00:59.711222: step 2336, loss 0.649154.
Train: 2018-08-01T23:00:59.871794: step 2337, loss 0.649664.
Train: 2018-08-01T23:01:00.041373: step 2338, loss 0.527973.
Train: 2018-08-01T23:01:00.211918: step 2339, loss 0.527795.
Train: 2018-08-01T23:01:00.386418: step 2340, loss 0.545006.
Test: 2018-08-01T23:01:00.926006: step 2340, loss 0.548014.
Train: 2018-08-01T23:01:01.092555: step 2341, loss 0.510655.
Train: 2018-08-01T23:01:01.272056: step 2342, loss 0.56271.
Train: 2018-08-01T23:01:01.433618: step 2343, loss 0.544966.
Train: 2018-08-01T23:01:01.598178: step 2344, loss 0.458595.
Train: 2018-08-01T23:01:01.757752: step 2345, loss 0.614782.
Train: 2018-08-01T23:01:01.917325: step 2346, loss 0.475559.
Train: 2018-08-01T23:01:02.094876: step 2347, loss 0.579923.
Train: 2018-08-01T23:01:02.257427: step 2348, loss 0.545274.
Train: 2018-08-01T23:01:02.423999: step 2349, loss 0.562642.
Train: 2018-08-01T23:01:02.585570: step 2350, loss 0.615086.
Test: 2018-08-01T23:01:03.126093: step 2350, loss 0.547879.
Train: 2018-08-01T23:01:03.306641: step 2351, loss 0.562635.
Train: 2018-08-01T23:01:03.477184: step 2352, loss 0.615092.
Train: 2018-08-01T23:01:03.643710: step 2353, loss 0.527814.
Train: 2018-08-01T23:01:03.806275: step 2354, loss 0.545172.
Train: 2018-08-01T23:01:03.974853: step 2355, loss 0.54501.
Train: 2018-08-01T23:01:04.144372: step 2356, loss 0.615136.
Train: 2018-08-01T23:01:04.305959: step 2357, loss 0.492849.
Train: 2018-08-01T23:01:04.477481: step 2358, loss 0.492779.
Train: 2018-08-01T23:01:04.641069: step 2359, loss 0.545086.
Train: 2018-08-01T23:01:04.803609: step 2360, loss 0.562855.
Test: 2018-08-01T23:01:05.340194: step 2360, loss 0.547852.
Train: 2018-08-01T23:01:05.516734: step 2361, loss 0.632883.
Train: 2018-08-01T23:01:05.679269: step 2362, loss 0.562344.
Train: 2018-08-01T23:01:05.846854: step 2363, loss 0.650404.
Train: 2018-08-01T23:01:06.009386: step 2364, loss 0.474606.
Train: 2018-08-01T23:01:06.174974: step 2365, loss 0.54495.
Train: 2018-08-01T23:01:06.343494: step 2366, loss 0.492367.
Train: 2018-08-01T23:01:06.508053: step 2367, loss 0.597769.
Train: 2018-08-01T23:01:06.667626: step 2368, loss 0.562639.
Train: 2018-08-01T23:01:06.847179: step 2369, loss 0.597776.
Train: 2018-08-01T23:01:07.007717: step 2370, loss 0.562522.
Test: 2018-08-01T23:01:07.537302: step 2370, loss 0.547865.
Train: 2018-08-01T23:01:07.703892: step 2371, loss 0.457279.
Train: 2018-08-01T23:01:07.867420: step 2372, loss 0.545063.
Train: 2018-08-01T23:01:08.029018: step 2373, loss 0.544956.
Train: 2018-08-01T23:01:08.193548: step 2374, loss 0.562535.
Train: 2018-08-01T23:01:08.365121: step 2375, loss 0.491947.
Train: 2018-08-01T23:01:08.533638: step 2376, loss 0.544911.
Train: 2018-08-01T23:01:08.696203: step 2377, loss 0.562664.
Train: 2018-08-01T23:01:08.869770: step 2378, loss 0.562614.
Train: 2018-08-01T23:01:09.034299: step 2379, loss 0.491598.
Train: 2018-08-01T23:01:09.198860: step 2380, loss 0.598369.
Test: 2018-08-01T23:01:09.750386: step 2380, loss 0.547754.
Train: 2018-08-01T23:01:09.915943: step 2381, loss 0.580416.
Train: 2018-08-01T23:01:10.081501: step 2382, loss 0.616115.
Train: 2018-08-01T23:01:10.253042: step 2383, loss 0.509386.
Train: 2018-08-01T23:01:10.416638: step 2384, loss 0.473394.
Train: 2018-08-01T23:01:10.590171: step 2385, loss 0.47364.
Train: 2018-08-01T23:01:10.765671: step 2386, loss 0.562739.
Train: 2018-08-01T23:01:10.925275: step 2387, loss 0.670112.
Train: 2018-08-01T23:01:11.096787: step 2388, loss 0.490999.
Train: 2018-08-01T23:01:11.258353: step 2389, loss 0.544661.
Train: 2018-08-01T23:01:11.421917: step 2390, loss 0.6164.
Test: 2018-08-01T23:01:11.967466: step 2390, loss 0.547704.
Train: 2018-08-01T23:01:12.132047: step 2391, loss 0.544775.
Train: 2018-08-01T23:01:12.297611: step 2392, loss 0.562555.
Train: 2018-08-01T23:01:12.465127: step 2393, loss 0.598651.
Train: 2018-08-01T23:01:12.634675: step 2394, loss 0.562984.
Train: 2018-08-01T23:01:12.797271: step 2395, loss 0.490973.
Train: 2018-08-01T23:01:12.958840: step 2396, loss 0.598428.
Train: 2018-08-01T23:01:13.121398: step 2397, loss 0.580417.
Train: 2018-08-01T23:01:13.292940: step 2398, loss 0.419418.
Train: 2018-08-01T23:01:13.459495: step 2399, loss 0.598695.
Train: 2018-08-01T23:01:13.629017: step 2400, loss 0.562818.
Test: 2018-08-01T23:01:14.175556: step 2400, loss 0.547703.
Train: 2018-08-01T23:01:14.936073: step 2401, loss 0.58062.
Train: 2018-08-01T23:01:15.113587: step 2402, loss 0.45484.
Train: 2018-08-01T23:01:15.278171: step 2403, loss 0.617061.
Train: 2018-08-01T23:01:15.443705: step 2404, loss 0.436777.
Train: 2018-08-01T23:01:15.605302: step 2405, loss 0.634759.
Train: 2018-08-01T23:01:15.773822: step 2406, loss 0.545124.
Train: 2018-08-01T23:01:15.932428: step 2407, loss 0.652783.
Train: 2018-08-01T23:01:16.099949: step 2408, loss 0.46653.
Train: 2018-08-01T23:01:16.266535: step 2409, loss 0.49118.
Train: 2018-08-01T23:01:16.433058: step 2410, loss 0.580699.
Test: 2018-08-01T23:01:16.983588: step 2410, loss 0.547686.
Train: 2018-08-01T23:01:17.150141: step 2411, loss 0.706965.
Train: 2018-08-01T23:01:17.316726: step 2412, loss 0.544598.
Train: 2018-08-01T23:01:17.490232: step 2413, loss 0.526938.
Train: 2018-08-01T23:01:17.654824: step 2414, loss 0.526937.
Train: 2018-08-01T23:01:17.824340: step 2415, loss 0.508513.
Train: 2018-08-01T23:01:17.986918: step 2416, loss 0.688343.
Train: 2018-08-01T23:01:18.156451: step 2417, loss 0.526806.
Train: 2018-08-01T23:01:18.324003: step 2418, loss 0.527009.
Train: 2018-08-01T23:01:18.496544: step 2419, loss 0.491095.
Train: 2018-08-01T23:01:18.662128: step 2420, loss 0.580381.
Test: 2018-08-01T23:01:19.202686: step 2420, loss 0.547718.
Train: 2018-08-01T23:01:19.376192: step 2421, loss 0.509088.
Train: 2018-08-01T23:01:19.545739: step 2422, loss 0.598754.
Train: 2018-08-01T23:01:19.720272: step 2423, loss 0.455269.
Train: 2018-08-01T23:01:19.911759: step 2424, loss 0.562531.
Train: 2018-08-01T23:01:20.149126: step 2425, loss 0.526781.
Train: 2018-08-01T23:01:20.318672: step 2426, loss 0.616781.
Train: 2018-08-01T23:01:20.499189: step 2427, loss 0.598855.
Train: 2018-08-01T23:01:20.661755: step 2428, loss 0.544922.
Train: 2018-08-01T23:01:20.835291: step 2429, loss 0.562322.
Train: 2018-08-01T23:01:20.995886: step 2430, loss 0.652518.
Test: 2018-08-01T23:01:21.532433: step 2430, loss 0.547715.
Train: 2018-08-01T23:01:21.694024: step 2431, loss 0.562517.
Train: 2018-08-01T23:01:21.863542: step 2432, loss 0.634351.
Train: 2018-08-01T23:01:22.034086: step 2433, loss 0.633732.
Train: 2018-08-01T23:01:22.205652: step 2434, loss 0.527361.
Train: 2018-08-01T23:01:22.367226: step 2435, loss 0.527249.
Train: 2018-08-01T23:01:22.531784: step 2436, loss 0.56268.
Train: 2018-08-01T23:01:22.694351: step 2437, loss 0.562807.
Train: 2018-08-01T23:01:22.864865: step 2438, loss 0.457118.
Train: 2018-08-01T23:01:23.039399: step 2439, loss 0.562517.
Train: 2018-08-01T23:01:23.203959: step 2440, loss 0.562384.
Test: 2018-08-01T23:01:23.745511: step 2440, loss 0.547856.
Train: 2018-08-01T23:01:23.912093: step 2441, loss 0.597318.
Train: 2018-08-01T23:01:24.078621: step 2442, loss 0.650083.
Train: 2018-08-01T23:01:24.245174: step 2443, loss 0.615025.
Train: 2018-08-01T23:01:24.407766: step 2444, loss 0.597374.
Train: 2018-08-01T23:01:24.582275: step 2445, loss 0.527664.
Train: 2018-08-01T23:01:24.749827: step 2446, loss 0.666748.
Train: 2018-08-01T23:01:24.917379: step 2447, loss 0.510899.
Train: 2018-08-01T23:01:25.096898: step 2448, loss 0.562341.
Train: 2018-08-01T23:01:25.295367: step 2449, loss 0.59715.
Train: 2018-08-01T23:01:25.473890: step 2450, loss 0.579898.
Test: 2018-08-01T23:01:26.013449: step 2450, loss 0.548175.
Train: 2018-08-01T23:01:26.180002: step 2451, loss 0.52843.
Train: 2018-08-01T23:01:26.351544: step 2452, loss 0.494331.
Train: 2018-08-01T23:01:26.515138: step 2453, loss 0.54533.
Train: 2018-08-01T23:01:26.706595: step 2454, loss 0.664756.
Train: 2018-08-01T23:01:26.877140: step 2455, loss 0.494493.
Train: 2018-08-01T23:01:27.044692: step 2456, loss 0.443642.
Train: 2018-08-01T23:01:27.219257: step 2457, loss 0.562399.
Train: 2018-08-01T23:01:27.395754: step 2458, loss 0.528521.
Train: 2018-08-01T23:01:27.589236: step 2459, loss 0.647794.
Train: 2018-08-01T23:01:27.760777: step 2460, loss 0.54541.
Test: 2018-08-01T23:01:28.290380: step 2460, loss 0.548186.
Train: 2018-08-01T23:01:28.466908: step 2461, loss 0.511225.
Train: 2018-08-01T23:01:28.635468: step 2462, loss 0.511279.
Train: 2018-08-01T23:01:28.805984: step 2463, loss 0.511119.
Train: 2018-08-01T23:01:28.967578: step 2464, loss 0.631334.
Train: 2018-08-01T23:01:29.128124: step 2465, loss 0.476443.
Train: 2018-08-01T23:01:29.287721: step 2466, loss 0.407384.
Train: 2018-08-01T23:01:29.453277: step 2467, loss 0.562491.
Train: 2018-08-01T23:01:29.614820: step 2468, loss 0.475585.
Train: 2018-08-01T23:01:29.776423: step 2469, loss 0.545124.
Train: 2018-08-01T23:01:29.942974: step 2470, loss 0.49246.
Test: 2018-08-01T23:01:30.488499: step 2470, loss 0.54781.
Train: 2018-08-01T23:01:30.667009: step 2471, loss 0.456935.
Train: 2018-08-01T23:01:30.837554: step 2472, loss 0.633574.
Train: 2018-08-01T23:01:31.004113: step 2473, loss 0.527018.
Train: 2018-08-01T23:01:31.173678: step 2474, loss 0.580519.
Train: 2018-08-01T23:01:31.336218: step 2475, loss 0.616601.
Train: 2018-08-01T23:01:31.498801: step 2476, loss 0.616483.
Train: 2018-08-01T23:01:31.668331: step 2477, loss 0.4189.
Train: 2018-08-01T23:01:31.835883: step 2478, loss 0.490649.
Train: 2018-08-01T23:01:32.002472: step 2479, loss 0.562822.
Train: 2018-08-01T23:01:32.174017: step 2480, loss 0.671709.
Test: 2018-08-01T23:01:32.720547: step 2480, loss 0.547659.
Train: 2018-08-01T23:01:32.890066: step 2481, loss 0.43583.
Train: 2018-08-01T23:01:33.050637: step 2482, loss 0.617368.
Train: 2018-08-01T23:01:33.222178: step 2483, loss 0.526477.
Train: 2018-08-01T23:01:33.390759: step 2484, loss 0.544713.
Train: 2018-08-01T23:01:33.571244: step 2485, loss 0.617741.
Train: 2018-08-01T23:01:33.736826: step 2486, loss 0.544586.
Train: 2018-08-01T23:01:33.901363: step 2487, loss 0.581118.
Train: 2018-08-01T23:01:34.059938: step 2488, loss 0.471504.
Train: 2018-08-01T23:01:34.233474: step 2489, loss 0.526318.
Train: 2018-08-01T23:01:34.401027: step 2490, loss 0.708853.
Test: 2018-08-01T23:01:34.939612: step 2490, loss 0.547652.
Train: 2018-08-01T23:01:35.100189: step 2491, loss 0.453332.
Train: 2018-08-01T23:01:35.262724: step 2492, loss 0.50816.
Train: 2018-08-01T23:01:35.422321: step 2493, loss 0.544693.
Train: 2018-08-01T23:01:35.594835: step 2494, loss 0.636227.
Train: 2018-08-01T23:01:35.751417: step 2495, loss 0.599255.
Train: 2018-08-01T23:01:35.932930: step 2496, loss 0.617661.
Train: 2018-08-01T23:01:36.097522: step 2497, loss 0.544683.
Train: 2018-08-01T23:01:36.264045: step 2498, loss 0.508211.
Train: 2018-08-01T23:01:36.429604: step 2499, loss 0.544671.
Train: 2018-08-01T23:01:36.597157: step 2500, loss 0.490372.
Test: 2018-08-01T23:01:37.121752: step 2500, loss 0.547654.
Train: 2018-08-01T23:01:37.911838: step 2501, loss 0.580912.
Train: 2018-08-01T23:01:38.092324: step 2502, loss 0.598933.
Train: 2018-08-01T23:01:38.255887: step 2503, loss 0.599157.
Train: 2018-08-01T23:01:38.417489: step 2504, loss 0.52669.
Train: 2018-08-01T23:01:38.578051: step 2505, loss 0.526412.
Train: 2018-08-01T23:01:38.737624: step 2506, loss 0.526727.
Train: 2018-08-01T23:01:38.901162: step 2507, loss 0.617051.
Train: 2018-08-01T23:01:39.064725: step 2508, loss 0.562775.
Train: 2018-08-01T23:01:39.235269: step 2509, loss 0.580602.
Train: 2018-08-01T23:01:39.403818: step 2510, loss 0.67018.
Test: 2018-08-01T23:01:39.940383: step 2510, loss 0.547687.
Train: 2018-08-01T23:01:40.099988: step 2511, loss 0.652522.
Train: 2018-08-01T23:01:40.270502: step 2512, loss 0.598499.
Train: 2018-08-01T23:01:40.433098: step 2513, loss 0.473851.
Train: 2018-08-01T23:01:40.599621: step 2514, loss 0.544588.
Train: 2018-08-01T23:01:40.759196: step 2515, loss 0.59802.
Train: 2018-08-01T23:01:40.922784: step 2516, loss 0.65103.
Train: 2018-08-01T23:01:41.092305: step 2517, loss 0.685757.
Train: 2018-08-01T23:01:41.257895: step 2518, loss 0.562361.
Train: 2018-08-01T23:01:41.426412: step 2519, loss 0.54488.
Train: 2018-08-01T23:01:41.592967: step 2520, loss 0.545147.
Test: 2018-08-01T23:01:42.125542: step 2520, loss 0.548025.
Train: 2018-08-01T23:01:42.304046: step 2521, loss 0.579704.
Train: 2018-08-01T23:01:42.468637: step 2522, loss 0.49369.
Train: 2018-08-01T23:01:42.646130: step 2523, loss 0.613926.
Train: 2018-08-01T23:01:42.815678: step 2524, loss 0.511174.
Train: 2018-08-01T23:01:42.993202: step 2525, loss 0.579665.
Train: 2018-08-01T23:01:43.165741: step 2526, loss 0.545186.
Train: 2018-08-01T23:01:43.375182: step 2527, loss 0.613447.
Train: 2018-08-01T23:01:43.606562: step 2528, loss 0.579781.
Train: 2018-08-01T23:01:43.781096: step 2529, loss 0.562634.
Train: 2018-08-01T23:01:43.954632: step 2530, loss 0.613162.
Test: 2018-08-01T23:01:44.492195: step 2530, loss 0.548348.
Train: 2018-08-01T23:01:44.662769: step 2531, loss 0.511651.
Train: 2018-08-01T23:01:44.826344: step 2532, loss 0.478185.
Train: 2018-08-01T23:01:44.999837: step 2533, loss 0.54548.
Train: 2018-08-01T23:01:45.166421: step 2534, loss 0.545505.
Train: 2018-08-01T23:01:45.330953: step 2535, loss 0.545562.
Train: 2018-08-01T23:01:45.549368: step 2536, loss 0.68099.
Train: 2018-08-01T23:01:45.727544: step 2537, loss 0.528759.
Train: 2018-08-01T23:01:45.908566: step 2538, loss 0.59628.
Train: 2018-08-01T23:01:46.092582: step 2539, loss 0.494824.
Train: 2018-08-01T23:01:46.259162: step 2540, loss 0.460982.
Test: 2018-08-01T23:01:46.803681: step 2540, loss 0.548295.
Train: 2018-08-01T23:01:46.978216: step 2541, loss 0.596385.
Train: 2018-08-01T23:01:47.142805: step 2542, loss 0.596703.
Train: 2018-08-01T23:01:47.316310: step 2543, loss 0.562431.
Train: 2018-08-01T23:01:47.485857: step 2544, loss 0.511617.
Train: 2018-08-01T23:01:47.653409: step 2545, loss 0.511646.
Train: 2018-08-01T23:01:47.819965: step 2546, loss 0.596745.
Train: 2018-08-01T23:01:47.985525: step 2547, loss 0.562569.
Train: 2018-08-01T23:01:48.159058: step 2548, loss 0.477018.
Train: 2018-08-01T23:01:48.325612: step 2549, loss 0.528256.
Train: 2018-08-01T23:01:48.508125: step 2550, loss 0.476488.
Test: 2018-08-01T23:01:49.061646: step 2550, loss 0.548023.
Train: 2018-08-01T23:01:49.233212: step 2551, loss 0.528003.
Train: 2018-08-01T23:01:49.395777: step 2552, loss 0.476105.
Train: 2018-08-01T23:01:49.563303: step 2553, loss 0.510277.
Train: 2018-08-01T23:01:49.732852: step 2554, loss 0.545128.
Train: 2018-08-01T23:01:49.895447: step 2555, loss 0.668124.
Train: 2018-08-01T23:01:50.057981: step 2556, loss 0.56254.
Train: 2018-08-01T23:01:50.229541: step 2557, loss 0.580309.
Train: 2018-08-01T23:01:50.400067: step 2558, loss 0.545194.
Train: 2018-08-01T23:01:50.570611: step 2559, loss 0.56242.
Train: 2018-08-01T23:01:50.734200: step 2560, loss 0.509607.
Test: 2018-08-01T23:01:51.284703: step 2560, loss 0.547736.
Train: 2018-08-01T23:01:51.448265: step 2561, loss 0.580449.
Train: 2018-08-01T23:01:51.613823: step 2562, loss 0.598182.
Train: 2018-08-01T23:01:51.775421: step 2563, loss 0.669517.
Train: 2018-08-01T23:01:51.944962: step 2564, loss 0.633642.
Train: 2018-08-01T23:01:52.121465: step 2565, loss 0.562546.
Train: 2018-08-01T23:01:52.290040: step 2566, loss 0.597944.
Train: 2018-08-01T23:01:52.457597: step 2567, loss 0.580095.
Train: 2018-08-01T23:01:52.622157: step 2568, loss 0.527262.
Train: 2018-08-01T23:01:52.803642: step 2569, loss 0.544967.
Train: 2018-08-01T23:01:52.976180: step 2570, loss 0.61502.
Test: 2018-08-01T23:01:53.518730: step 2570, loss 0.547865.
Train: 2018-08-01T23:01:53.682293: step 2571, loss 0.737177.
Train: 2018-08-01T23:01:53.842865: step 2572, loss 0.597332.
Train: 2018-08-01T23:01:54.006451: step 2573, loss 0.545161.
Train: 2018-08-01T23:01:54.176971: step 2574, loss 0.614219.
Train: 2018-08-01T23:01:54.340559: step 2575, loss 0.545284.
Train: 2018-08-01T23:01:54.503123: step 2576, loss 0.562505.
Train: 2018-08-01T23:01:54.676644: step 2577, loss 0.613646.
Train: 2018-08-01T23:01:54.842218: step 2578, loss 0.613516.
Train: 2018-08-01T23:01:55.012736: step 2579, loss 0.494832.
Train: 2018-08-01T23:01:55.176299: step 2580, loss 0.613042.
Test: 2018-08-01T23:01:55.705893: step 2580, loss 0.548426.
Train: 2018-08-01T23:01:55.872438: step 2581, loss 0.461542.
Train: 2018-08-01T23:01:56.058973: step 2582, loss 0.596159.
Train: 2018-08-01T23:01:56.238459: step 2583, loss 0.612767.
Train: 2018-08-01T23:01:56.404017: step 2584, loss 0.579156.
Train: 2018-08-01T23:01:56.564620: step 2585, loss 0.462321.
Train: 2018-08-01T23:01:56.737126: step 2586, loss 0.679554.
Train: 2018-08-01T23:01:56.907670: step 2587, loss 0.529139.
Train: 2018-08-01T23:01:57.079213: step 2588, loss 0.612595.
Train: 2018-08-01T23:01:57.247787: step 2589, loss 0.562584.
Train: 2018-08-01T23:01:57.421298: step 2590, loss 0.645703.
Test: 2018-08-01T23:01:57.962851: step 2590, loss 0.548736.
Train: 2018-08-01T23:01:58.127436: step 2591, loss 0.562562.
Train: 2018-08-01T23:01:58.290006: step 2592, loss 0.562632.
Train: 2018-08-01T23:01:58.458525: step 2593, loss 0.579129.
Train: 2018-08-01T23:01:58.624113: step 2594, loss 0.562493.
Train: 2018-08-01T23:01:58.787645: step 2595, loss 0.513318.
Train: 2018-08-01T23:01:58.950238: step 2596, loss 0.546187.
Train: 2018-08-01T23:01:59.111809: step 2597, loss 0.414649.
Train: 2018-08-01T23:01:59.272374: step 2598, loss 0.56266.
Train: 2018-08-01T23:01:59.433950: step 2599, loss 0.56256.
Train: 2018-08-01T23:01:59.594515: step 2600, loss 0.628922.
Test: 2018-08-01T23:02:00.145042: step 2600, loss 0.548694.
Train: 2018-08-01T23:02:01.032393: step 2601, loss 0.628966.
Train: 2018-08-01T23:02:01.203909: step 2602, loss 0.579113.
Train: 2018-08-01T23:02:01.368498: step 2603, loss 0.595718.
Train: 2018-08-01T23:02:01.530063: step 2604, loss 0.579287.
Train: 2018-08-01T23:02:01.693601: step 2605, loss 0.545884.
Train: 2018-08-01T23:02:01.872124: step 2606, loss 0.562594.
Train: 2018-08-01T23:02:02.050646: step 2607, loss 0.546181.
Train: 2018-08-01T23:02:02.213212: step 2608, loss 0.628783.
Train: 2018-08-01T23:02:02.395723: step 2609, loss 0.579258.
Train: 2018-08-01T23:02:02.629099: step 2610, loss 0.562748.
Test: 2018-08-01T23:02:03.173644: step 2610, loss 0.548734.
Train: 2018-08-01T23:02:03.361173: step 2611, loss 0.595791.
Train: 2018-08-01T23:02:03.526731: step 2612, loss 0.628823.
Train: 2018-08-01T23:02:03.692258: step 2613, loss 0.529561.
Train: 2018-08-01T23:02:03.852854: step 2614, loss 0.562706.
Train: 2018-08-01T23:02:04.017418: step 2615, loss 0.480291.
Train: 2018-08-01T23:02:04.189928: step 2616, loss 0.529464.
Train: 2018-08-01T23:02:04.357479: step 2617, loss 0.496538.
Train: 2018-08-01T23:02:04.523067: step 2618, loss 0.56266.
Train: 2018-08-01T23:02:04.692584: step 2619, loss 0.512685.
Train: 2018-08-01T23:02:04.855173: step 2620, loss 0.52924.
Test: 2018-08-01T23:02:05.387725: step 2620, loss 0.548537.
Train: 2018-08-01T23:02:05.559265: step 2621, loss 0.56262.
Train: 2018-08-01T23:02:05.720860: step 2622, loss 0.595906.
Train: 2018-08-01T23:02:05.881406: step 2623, loss 0.461697.
Train: 2018-08-01T23:02:06.046988: step 2624, loss 0.478274.
Train: 2018-08-01T23:02:06.208531: step 2625, loss 0.528485.
Train: 2018-08-01T23:02:06.369101: step 2626, loss 0.562338.
Train: 2018-08-01T23:02:06.524685: step 2627, loss 0.630959.
Train: 2018-08-01T23:02:06.690243: step 2628, loss 0.545328.
Train: 2018-08-01T23:02:06.849816: step 2629, loss 0.579416.
Train: 2018-08-01T23:02:07.022355: step 2630, loss 0.56229.
Test: 2018-08-01T23:02:07.556933: step 2630, loss 0.547985.
Train: 2018-08-01T23:02:07.717527: step 2631, loss 0.545194.
Train: 2018-08-01T23:02:07.882058: step 2632, loss 0.631816.
Train: 2018-08-01T23:02:08.052601: step 2633, loss 0.51064.
Train: 2018-08-01T23:02:08.223145: step 2634, loss 0.545186.
Train: 2018-08-01T23:02:08.384744: step 2635, loss 0.492881.
Train: 2018-08-01T23:02:08.547309: step 2636, loss 0.509921.
Train: 2018-08-01T23:02:08.712837: step 2637, loss 0.562201.
Train: 2018-08-01T23:02:08.878393: step 2638, loss 0.580374.
Train: 2018-08-01T23:02:09.046973: step 2639, loss 0.597381.
Train: 2018-08-01T23:02:09.216490: step 2640, loss 0.615589.
Test: 2018-08-01T23:02:09.741096: step 2640, loss 0.547795.
Train: 2018-08-01T23:02:09.906671: step 2641, loss 0.509864.
Train: 2018-08-01T23:02:10.066244: step 2642, loss 0.509488.
Train: 2018-08-01T23:02:10.232804: step 2643, loss 0.580264.
Train: 2018-08-01T23:02:10.401353: step 2644, loss 0.703694.
Train: 2018-08-01T23:02:10.564885: step 2645, loss 0.66869.
Train: 2018-08-01T23:02:10.724458: step 2646, loss 0.668312.
Train: 2018-08-01T23:02:10.886058: step 2647, loss 0.63234.
Train: 2018-08-01T23:02:11.047595: step 2648, loss 0.510045.
Train: 2018-08-01T23:02:11.221132: step 2649, loss 0.597251.
Train: 2018-08-01T23:02:11.386722: step 2650, loss 0.527691.
Test: 2018-08-01T23:02:11.925248: step 2650, loss 0.548037.
Train: 2018-08-01T23:02:12.090837: step 2651, loss 0.562682.
Train: 2018-08-01T23:02:12.258389: step 2652, loss 0.527994.
Train: 2018-08-01T23:02:12.423915: step 2653, loss 0.63109.
Train: 2018-08-01T23:02:12.586510: step 2654, loss 0.49424.
Train: 2018-08-01T23:02:12.752039: step 2655, loss 0.443069.
Train: 2018-08-01T23:02:12.917596: step 2656, loss 0.511309.
Train: 2018-08-01T23:02:13.085148: step 2657, loss 0.579561.
Train: 2018-08-01T23:02:13.248712: step 2658, loss 0.579525.
Train: 2018-08-01T23:02:13.409281: step 2659, loss 0.596809.
Train: 2018-08-01T23:02:13.577831: step 2660, loss 0.613714.
Test: 2018-08-01T23:02:14.107426: step 2660, loss 0.548138.
Train: 2018-08-01T23:02:14.272973: step 2661, loss 0.579507.
Train: 2018-08-01T23:02:14.433544: step 2662, loss 0.664801.
Train: 2018-08-01T23:02:14.596134: step 2663, loss 0.528406.
Train: 2018-08-01T23:02:14.764658: step 2664, loss 0.545551.
Train: 2018-08-01T23:02:14.929218: step 2665, loss 0.562515.
Train: 2018-08-01T23:02:15.099762: step 2666, loss 0.562508.
Train: 2018-08-01T23:02:15.270307: step 2667, loss 0.528539.
Train: 2018-08-01T23:02:15.433894: step 2668, loss 0.613277.
Train: 2018-08-01T23:02:15.593473: step 2669, loss 0.56249.
Train: 2018-08-01T23:02:15.762989: step 2670, loss 0.494721.
Test: 2018-08-01T23:02:16.288584: step 2670, loss 0.548298.
Train: 2018-08-01T23:02:16.462121: step 2671, loss 0.545527.
Train: 2018-08-01T23:02:16.633662: step 2672, loss 0.63022.
Train: 2018-08-01T23:02:16.802212: step 2673, loss 0.630181.
Train: 2018-08-01T23:02:16.964825: step 2674, loss 0.528705.
Train: 2018-08-01T23:02:17.125372: step 2675, loss 0.545543.
Train: 2018-08-01T23:02:17.285943: step 2676, loss 0.528688.
Train: 2018-08-01T23:02:17.449481: step 2677, loss 0.545481.
Train: 2018-08-01T23:02:17.614082: step 2678, loss 0.528642.
Train: 2018-08-01T23:02:17.775640: step 2679, loss 0.647113.
Train: 2018-08-01T23:02:17.935182: step 2680, loss 0.596323.
Test: 2018-08-01T23:02:18.459781: step 2680, loss 0.548314.
Train: 2018-08-01T23:02:18.633347: step 2681, loss 0.663941.
Train: 2018-08-01T23:02:18.796880: step 2682, loss 0.545675.
Train: 2018-08-01T23:02:18.959470: step 2683, loss 0.579251.
Train: 2018-08-01T23:02:19.123007: step 2684, loss 0.596191.
Train: 2018-08-01T23:02:19.284601: step 2685, loss 0.495389.
Train: 2018-08-01T23:02:19.448166: step 2686, loss 0.646428.
Train: 2018-08-01T23:02:19.612728: step 2687, loss 0.478799.
Train: 2018-08-01T23:02:19.777284: step 2688, loss 0.612697.
Train: 2018-08-01T23:02:19.935865: step 2689, loss 0.512337.
Train: 2018-08-01T23:02:20.110367: step 2690, loss 0.57932.
Test: 2018-08-01T23:02:20.641947: step 2690, loss 0.548521.
Train: 2018-08-01T23:02:20.806536: step 2691, loss 0.545836.
Train: 2018-08-01T23:02:20.971093: step 2692, loss 0.662956.
Train: 2018-08-01T23:02:21.131638: step 2693, loss 0.512372.
Train: 2018-08-01T23:02:21.312178: step 2694, loss 0.429032.
Train: 2018-08-01T23:02:21.490677: step 2695, loss 0.528976.
Train: 2018-08-01T23:02:21.651300: step 2696, loss 0.495376.
Train: 2018-08-01T23:02:21.811820: step 2697, loss 0.528838.
Train: 2018-08-01T23:02:21.973418: step 2698, loss 0.494894.
Train: 2018-08-01T23:02:22.134981: step 2699, loss 0.562529.
Train: 2018-08-01T23:02:22.296524: step 2700, loss 0.49424.
Test: 2018-08-01T23:02:22.824114: step 2700, loss 0.548127.
Train: 2018-08-01T23:02:23.626169: step 2701, loss 0.528314.
Train: 2018-08-01T23:02:23.791763: step 2702, loss 0.562439.
Train: 2018-08-01T23:02:23.956287: step 2703, loss 0.614093.
Train: 2018-08-01T23:02:24.131828: step 2704, loss 0.614452.
Train: 2018-08-01T23:02:24.297374: step 2705, loss 0.579791.
Train: 2018-08-01T23:02:24.459940: step 2706, loss 0.562577.
Train: 2018-08-01T23:02:24.622520: step 2707, loss 0.545196.
Train: 2018-08-01T23:02:24.788067: step 2708, loss 0.63211.
Train: 2018-08-01T23:02:24.947661: step 2709, loss 0.581068.
Train: 2018-08-01T23:02:25.111233: step 2710, loss 0.527648.
Test: 2018-08-01T23:02:25.648762: step 2710, loss 0.547899.
Train: 2018-08-01T23:02:25.810334: step 2711, loss 0.545045.
Train: 2018-08-01T23:02:25.980887: step 2712, loss 0.562327.
Train: 2018-08-01T23:02:26.148471: step 2713, loss 0.545034.
Train: 2018-08-01T23:02:26.311031: step 2714, loss 0.405484.
Train: 2018-08-01T23:02:26.470579: step 2715, loss 0.597348.
Train: 2018-08-01T23:02:26.640125: step 2716, loss 0.649806.
Train: 2018-08-01T23:02:26.805708: step 2717, loss 0.614879.
Train: 2018-08-01T23:02:26.964288: step 2718, loss 0.527426.
Train: 2018-08-01T23:02:27.125858: step 2719, loss 0.544968.
Train: 2018-08-01T23:02:27.293405: step 2720, loss 0.545148.
Test: 2018-08-01T23:02:27.827951: step 2720, loss 0.547826.
Train: 2018-08-01T23:02:27.988566: step 2721, loss 0.509976.
Train: 2018-08-01T23:02:28.151086: step 2722, loss 0.544846.
Train: 2018-08-01T23:02:28.313678: step 2723, loss 0.632619.
Train: 2018-08-01T23:02:28.473256: step 2724, loss 0.492051.
Train: 2018-08-01T23:02:28.634818: step 2725, loss 0.509718.
Train: 2018-08-01T23:02:28.798397: step 2726, loss 0.544886.
Train: 2018-08-01T23:02:28.959951: step 2727, loss 0.721537.
Train: 2018-08-01T23:02:29.119497: step 2728, loss 0.562199.
Train: 2018-08-01T23:02:29.281090: step 2729, loss 0.52687.
Train: 2018-08-01T23:02:29.455599: step 2730, loss 0.562675.
Test: 2018-08-01T23:02:29.985183: step 2730, loss 0.547787.
Train: 2018-08-01T23:02:30.144771: step 2731, loss 0.404251.
Train: 2018-08-01T23:02:30.307347: step 2732, loss 0.544988.
Train: 2018-08-01T23:02:30.474880: step 2733, loss 0.492154.
Train: 2018-08-01T23:02:30.642455: step 2734, loss 0.68695.
Train: 2018-08-01T23:02:30.802032: step 2735, loss 0.473679.
Train: 2018-08-01T23:02:30.967582: step 2736, loss 0.633547.
Train: 2018-08-01T23:02:31.128127: step 2737, loss 0.473946.
Train: 2018-08-01T23:02:31.299701: step 2738, loss 0.544631.
Train: 2018-08-01T23:02:31.459242: step 2739, loss 0.544854.
Train: 2018-08-01T23:02:31.629787: step 2740, loss 0.491486.
Test: 2018-08-01T23:02:32.168346: step 2740, loss 0.547678.
Train: 2018-08-01T23:02:32.355873: step 2741, loss 0.508896.
Train: 2018-08-01T23:02:32.554315: step 2742, loss 0.581037.
Train: 2018-08-01T23:02:32.744807: step 2743, loss 0.580799.
Train: 2018-08-01T23:02:32.937318: step 2744, loss 0.545257.
Train: 2018-08-01T23:02:33.127810: step 2745, loss 0.545115.
Train: 2018-08-01T23:02:33.298352: step 2746, loss 0.473101.
Train: 2018-08-01T23:02:33.467873: step 2747, loss 0.544655.
Train: 2018-08-01T23:02:33.629467: step 2748, loss 0.545092.
Train: 2018-08-01T23:02:33.802977: step 2749, loss 0.50874.
Train: 2018-08-01T23:02:33.962551: step 2750, loss 0.562963.
Test: 2018-08-01T23:02:34.499143: step 2750, loss 0.54762.
Train: 2018-08-01T23:02:34.663710: step 2751, loss 0.599485.
Train: 2018-08-01T23:02:34.828236: step 2752, loss 0.562504.
Train: 2018-08-01T23:02:34.989832: step 2753, loss 0.599162.
Train: 2018-08-01T23:02:35.149403: step 2754, loss 0.454209.
Train: 2018-08-01T23:02:35.322913: step 2755, loss 0.581193.
Train: 2018-08-01T23:02:35.485480: step 2756, loss 0.599359.
Train: 2018-08-01T23:02:35.645082: step 2757, loss 0.562848.
Train: 2018-08-01T23:02:35.814599: step 2758, loss 0.453948.
Train: 2018-08-01T23:02:35.981154: step 2759, loss 0.526588.
Train: 2018-08-01T23:02:36.146711: step 2760, loss 0.690545.
Test: 2018-08-01T23:02:36.682280: step 2760, loss 0.547617.
Train: 2018-08-01T23:02:36.843874: step 2761, loss 0.599289.
Train: 2018-08-01T23:02:37.015390: step 2762, loss 0.453846.
Train: 2018-08-01T23:02:37.174989: step 2763, loss 0.50846.
Train: 2018-08-01T23:02:37.341544: step 2764, loss 0.490104.
Train: 2018-08-01T23:02:37.514057: step 2765, loss 0.508231.
Train: 2018-08-01T23:02:37.680642: step 2766, loss 0.672238.
Train: 2018-08-01T23:02:37.843177: step 2767, loss 0.508315.
Train: 2018-08-01T23:02:38.007763: step 2768, loss 0.435342.
Train: 2018-08-01T23:02:38.169305: step 2769, loss 0.508085.
Train: 2018-08-01T23:02:38.328910: step 2770, loss 0.709351.
Test: 2018-08-01T23:02:38.868435: step 2770, loss 0.547614.
Train: 2018-08-01T23:02:39.033027: step 2771, loss 0.544673.
Train: 2018-08-01T23:02:39.201546: step 2772, loss 0.581119.
Train: 2018-08-01T23:02:39.362117: step 2773, loss 0.599433.
Train: 2018-08-01T23:02:39.524708: step 2774, loss 0.544644.
Train: 2018-08-01T23:02:39.689243: step 2775, loss 0.599314.
Train: 2018-08-01T23:02:39.851832: step 2776, loss 0.471769.
Train: 2018-08-01T23:02:40.015370: step 2777, loss 0.635548.
Train: 2018-08-01T23:02:40.177966: step 2778, loss 0.54471.
Train: 2018-08-01T23:02:40.338506: step 2779, loss 0.508402.
Train: 2018-08-01T23:02:40.503097: step 2780, loss 0.617204.
Test: 2018-08-01T23:02:41.049605: step 2780, loss 0.547623.
Train: 2018-08-01T23:02:41.220180: step 2781, loss 0.562804.
Train: 2018-08-01T23:02:41.390693: step 2782, loss 0.490455.
Train: 2018-08-01T23:02:41.556251: step 2783, loss 0.635036.
Train: 2018-08-01T23:02:41.721813: step 2784, loss 0.598819.
Train: 2018-08-01T23:02:41.884400: step 2785, loss 0.652669.
Train: 2018-08-01T23:02:42.053921: step 2786, loss 0.455122.
Train: 2018-08-01T23:02:42.218511: step 2787, loss 0.491047.
Train: 2018-08-01T23:02:42.381053: step 2788, loss 0.687847.
Train: 2018-08-01T23:02:42.549627: step 2789, loss 0.669696.
Train: 2018-08-01T23:02:42.727120: step 2790, loss 0.527044.
Test: 2018-08-01T23:02:43.269671: step 2790, loss 0.547721.
Train: 2018-08-01T23:02:43.444204: step 2791, loss 0.633457.
Train: 2018-08-01T23:02:43.605803: step 2792, loss 0.562551.
Train: 2018-08-01T23:02:43.767340: step 2793, loss 0.56252.
Train: 2018-08-01T23:02:43.932898: step 2794, loss 0.509888.
Train: 2018-08-01T23:02:44.095464: step 2795, loss 0.632467.
Train: 2018-08-01T23:02:44.257062: step 2796, loss 0.597341.
Train: 2018-08-01T23:02:44.424583: step 2797, loss 0.562443.
Train: 2018-08-01T23:02:44.593158: step 2798, loss 0.510487.
Train: 2018-08-01T23:02:44.760685: step 2799, loss 0.562438.
Train: 2018-08-01T23:02:44.926242: step 2800, loss 0.54518.
Test: 2018-08-01T23:02:45.471785: step 2800, loss 0.548016.
Train: 2018-08-01T23:02:46.255295: step 2801, loss 0.631348.
Train: 2018-08-01T23:02:46.423841: step 2802, loss 0.631196.
Train: 2018-08-01T23:02:46.596386: step 2803, loss 0.682346.
Train: 2018-08-01T23:02:46.761943: step 2804, loss 0.613595.
Train: 2018-08-01T23:02:46.927467: step 2805, loss 0.494591.
Train: 2018-08-01T23:02:47.092050: step 2806, loss 0.630107.
Train: 2018-08-01T23:02:47.261573: step 2807, loss 0.612995.
Train: 2018-08-01T23:02:47.426133: step 2808, loss 0.54574.
Train: 2018-08-01T23:02:47.591690: step 2809, loss 0.478999.
Train: 2018-08-01T23:02:47.764230: step 2810, loss 0.579201.
Test: 2018-08-01T23:02:48.309772: step 2810, loss 0.548606.
Train: 2018-08-01T23:02:48.475329: step 2811, loss 0.5459.
Train: 2018-08-01T23:02:48.636926: step 2812, loss 0.545928.
Train: 2018-08-01T23:02:48.807460: step 2813, loss 0.529343.
Train: 2018-08-01T23:02:48.975019: step 2814, loss 0.662183.
Train: 2018-08-01T23:02:49.139554: step 2815, loss 0.595725.
Train: 2018-08-01T23:02:49.306109: step 2816, loss 0.546032.
Train: 2018-08-01T23:02:49.466695: step 2817, loss 0.579122.
Train: 2018-08-01T23:02:49.637249: step 2818, loss 0.595617.
Train: 2018-08-01T23:02:49.799819: step 2819, loss 0.463717.
Train: 2018-08-01T23:02:49.964348: step 2820, loss 0.645055.
Test: 2018-08-01T23:02:50.506906: step 2820, loss 0.548823.
Train: 2018-08-01T23:02:50.676445: step 2821, loss 0.529669.
Train: 2018-08-01T23:02:50.842002: step 2822, loss 0.496731.
Train: 2018-08-01T23:02:51.004594: step 2823, loss 0.628574.
Train: 2018-08-01T23:02:51.169128: step 2824, loss 0.595592.
Train: 2018-08-01T23:02:51.348660: step 2825, loss 0.529637.
Train: 2018-08-01T23:02:51.512256: step 2826, loss 0.628581.
Train: 2018-08-01T23:02:51.683783: step 2827, loss 0.463714.
Train: 2018-08-01T23:02:51.845320: step 2828, loss 0.628613.
Train: 2018-08-01T23:02:52.007910: step 2829, loss 0.480073.
Train: 2018-08-01T23:02:52.181449: step 2830, loss 0.661777.
Test: 2018-08-01T23:02:52.715992: step 2830, loss 0.548745.
Train: 2018-08-01T23:02:52.873605: step 2831, loss 0.579119.
Train: 2018-08-01T23:02:53.040125: step 2832, loss 0.562589.
Train: 2018-08-01T23:02:53.209673: step 2833, loss 0.513004.
Train: 2018-08-01T23:02:53.377225: step 2834, loss 0.512951.
Train: 2018-08-01T23:02:53.547781: step 2835, loss 0.529421.
Train: 2018-08-01T23:02:53.711331: step 2836, loss 0.446284.
Train: 2018-08-01T23:02:53.880879: step 2837, loss 0.679255.
Train: 2018-08-01T23:02:54.048457: step 2838, loss 0.562513.
Train: 2018-08-01T23:02:54.213988: step 2839, loss 0.646178.
Train: 2018-08-01T23:02:54.387524: step 2840, loss 0.579245.
Test: 2018-08-01T23:02:54.928085: step 2840, loss 0.548485.
Train: 2018-08-01T23:02:55.091647: step 2841, loss 0.579242.
Train: 2018-08-01T23:02:55.256201: step 2842, loss 0.562504.
Train: 2018-08-01T23:02:55.419765: step 2843, loss 0.529024.
Train: 2018-08-01T23:02:55.592335: step 2844, loss 0.595998.
Train: 2018-08-01T23:02:55.754868: step 2845, loss 0.512242.
Train: 2018-08-01T23:02:55.919430: step 2846, loss 0.663101.
Train: 2018-08-01T23:02:56.084987: step 2847, loss 0.663038.
Train: 2018-08-01T23:02:56.257525: step 2848, loss 0.412001.
Train: 2018-08-01T23:02:56.427071: step 2849, loss 0.562502.
Train: 2018-08-01T23:02:56.589637: step 2850, loss 0.662962.
Test: 2018-08-01T23:02:57.124213: step 2850, loss 0.548498.
Train: 2018-08-01T23:02:57.286774: step 2851, loss 0.495596.
Train: 2018-08-01T23:02:57.455323: step 2852, loss 0.662898.
Train: 2018-08-01T23:02:57.615894: step 2853, loss 0.512372.
Train: 2018-08-01T23:02:57.791455: step 2854, loss 0.61264.
Train: 2018-08-01T23:02:57.963963: step 2855, loss 0.445656.
Train: 2018-08-01T23:02:58.124533: step 2856, loss 0.562511.
Train: 2018-08-01T23:02:58.283113: step 2857, loss 0.679626.
Train: 2018-08-01T23:02:58.445434: step 2858, loss 0.529075.
Train: 2018-08-01T23:02:58.608962: step 2859, loss 0.529078.
Train: 2018-08-01T23:02:58.775516: step 2860, loss 0.646147.
Test: 2018-08-01T23:02:59.312082: step 2860, loss 0.548512.
Train: 2018-08-01T23:02:59.476643: step 2861, loss 0.495648.
Train: 2018-08-01T23:02:59.642200: step 2862, loss 0.56251.
Train: 2018-08-01T23:02:59.814737: step 2863, loss 0.579239.
Train: 2018-08-01T23:02:59.979317: step 2864, loss 0.59598.
Train: 2018-08-01T23:03:00.147878: step 2865, loss 0.529034.
Train: 2018-08-01T23:03:00.317395: step 2866, loss 0.646207.
Train: 2018-08-01T23:03:00.479960: step 2867, loss 0.612691.
Train: 2018-08-01T23:03:00.641527: step 2868, loss 0.595925.
Train: 2018-08-01T23:03:00.806087: step 2869, loss 0.645896.
Train: 2018-08-01T23:03:00.967655: step 2870, loss 0.562542.
Test: 2018-08-01T23:03:01.488265: step 2870, loss 0.548666.
Train: 2018-08-01T23:03:01.651838: step 2871, loss 0.595735.
Train: 2018-08-01T23:03:01.816388: step 2872, loss 0.529483.
Train: 2018-08-01T23:03:01.978983: step 2873, loss 0.579109.
Train: 2018-08-01T23:03:02.145507: step 2874, loss 0.52962.
Train: 2018-08-01T23:03:02.309069: step 2875, loss 0.513174.
Train: 2018-08-01T23:03:02.474627: step 2876, loss 0.595574.
Train: 2018-08-01T23:03:02.638190: step 2877, loss 0.645006.
Train: 2018-08-01T23:03:02.798761: step 2878, loss 0.546167.
Train: 2018-08-01T23:03:02.961357: step 2879, loss 0.513301.
Train: 2018-08-01T23:03:03.122894: step 2880, loss 0.529735.
Test: 2018-08-01T23:03:03.663449: step 2880, loss 0.548836.
Train: 2018-08-01T23:03:03.824020: step 2881, loss 0.677844.
Train: 2018-08-01T23:03:03.989578: step 2882, loss 0.595519.
Train: 2018-08-01T23:03:04.147182: step 2883, loss 0.496941.
Train: 2018-08-01T23:03:04.311747: step 2884, loss 0.529786.
Train: 2018-08-01T23:03:04.471320: step 2885, loss 0.61195.
Train: 2018-08-01T23:03:04.636847: step 2886, loss 0.529751.
Train: 2018-08-01T23:03:04.801407: step 2887, loss 0.611981.
Train: 2018-08-01T23:03:04.977964: step 2888, loss 0.562624.
Train: 2018-08-01T23:03:05.144489: step 2889, loss 0.529714.
Train: 2018-08-01T23:03:05.314061: step 2890, loss 0.52968.
Test: 2018-08-01T23:03:05.862570: step 2890, loss 0.548794.
Train: 2018-08-01T23:03:06.026133: step 2891, loss 0.546112.
Train: 2018-08-01T23:03:06.192687: step 2892, loss 0.744292.
Train: 2018-08-01T23:03:06.354283: step 2893, loss 0.496611.
Train: 2018-08-01T23:03:06.523822: step 2894, loss 0.628591.
Train: 2018-08-01T23:03:06.693350: step 2895, loss 0.546124.
Train: 2018-08-01T23:03:06.858907: step 2896, loss 0.628516.
Train: 2018-08-01T23:03:07.023467: step 2897, loss 0.546166.
Train: 2018-08-01T23:03:07.183070: step 2898, loss 0.579072.
Train: 2018-08-01T23:03:07.353616: step 2899, loss 0.546206.
Train: 2018-08-01T23:03:07.526157: step 2900, loss 0.529788.
Test: 2018-08-01T23:03:08.070668: step 2900, loss 0.548874.
Train: 2018-08-01T23:03:08.826455: step 2901, loss 0.611931.
Train: 2018-08-01T23:03:08.997998: step 2902, loss 0.496926.
Train: 2018-08-01T23:03:09.166521: step 2903, loss 0.595515.
Train: 2018-08-01T23:03:09.334072: step 2904, loss 0.595527.
Train: 2018-08-01T23:03:09.493680: step 2905, loss 0.496812.
Train: 2018-08-01T23:03:09.664191: step 2906, loss 0.546138.
Train: 2018-08-01T23:03:09.830744: step 2907, loss 0.496604.
Train: 2018-08-01T23:03:09.992314: step 2908, loss 0.579118.
Train: 2018-08-01T23:03:10.160897: step 2909, loss 0.496252.
Train: 2018-08-01T23:03:10.325424: step 2910, loss 0.52928.
Test: 2018-08-01T23:03:10.867973: step 2910, loss 0.548543.
Train: 2018-08-01T23:03:11.043535: step 2911, loss 0.54583.
Train: 2018-08-01T23:03:11.208103: step 2912, loss 0.529008.
Train: 2018-08-01T23:03:11.378638: step 2913, loss 0.461643.
Train: 2018-08-01T23:03:11.543174: step 2914, loss 0.494902.
Train: 2018-08-01T23:03:11.706762: step 2915, loss 0.579424.
Train: 2018-08-01T23:03:11.886251: step 2916, loss 0.61362.
Train: 2018-08-01T23:03:12.051808: step 2917, loss 0.459701.
Train: 2018-08-01T23:03:12.216402: step 2918, loss 0.493625.
Train: 2018-08-01T23:03:12.386943: step 2919, loss 0.562436.
Train: 2018-08-01T23:03:12.550506: step 2920, loss 0.475596.
Test: 2018-08-01T23:03:13.094022: step 2920, loss 0.547842.
Train: 2018-08-01T23:03:13.256618: step 2921, loss 0.440224.
Train: 2018-08-01T23:03:13.424140: step 2922, loss 0.527344.
Train: 2018-08-01T23:03:13.599670: step 2923, loss 0.50948.
Train: 2018-08-01T23:03:13.762269: step 2924, loss 0.54479.
Train: 2018-08-01T23:03:13.930792: step 2925, loss 0.580534.
Train: 2018-08-01T23:03:14.098367: step 2926, loss 0.526727.
Train: 2018-08-01T23:03:14.258907: step 2927, loss 0.580802.
Train: 2018-08-01T23:03:14.419506: step 2928, loss 0.435894.
Train: 2018-08-01T23:03:14.578085: step 2929, loss 0.617499.
Train: 2018-08-01T23:03:14.747611: step 2930, loss 0.4898.
Test: 2018-08-01T23:03:15.297145: step 2930, loss 0.547603.
Train: 2018-08-01T23:03:15.457702: step 2931, loss 0.599656.
Train: 2018-08-01T23:03:15.627250: step 2932, loss 0.452639.
Train: 2018-08-01T23:03:15.787846: step 2933, loss 0.655386.
Train: 2018-08-01T23:03:15.952380: step 2934, loss 0.50762.
Train: 2018-08-01T23:03:16.129907: step 2935, loss 0.618755.
Train: 2018-08-01T23:03:16.293499: step 2936, loss 0.618826.
Train: 2018-08-01T23:03:16.464038: step 2937, loss 0.526062.
Train: 2018-08-01T23:03:16.626604: step 2938, loss 0.544614.
Train: 2018-08-01T23:03:16.786181: step 2939, loss 0.581726.
Train: 2018-08-01T23:03:16.948769: step 2940, loss 0.655905.
Test: 2018-08-01T23:03:17.487277: step 2940, loss 0.547623.
Train: 2018-08-01T23:03:17.658844: step 2941, loss 0.50758.
Train: 2018-08-01T23:03:17.837367: step 2942, loss 0.563107.
Train: 2018-08-01T23:03:18.012883: step 2943, loss 0.470724.
Train: 2018-08-01T23:03:18.176436: step 2944, loss 0.489206.
Train: 2018-08-01T23:03:18.340996: step 2945, loss 0.544612.
Train: 2018-08-01T23:03:18.500599: step 2946, loss 0.544611.
Train: 2018-08-01T23:03:18.673136: step 2947, loss 0.489123.
Train: 2018-08-01T23:03:18.837667: step 2948, loss 0.526095.
Train: 2018-08-01T23:03:19.005221: step 2949, loss 0.544612.
Train: 2018-08-01T23:03:19.181747: step 2950, loss 0.544613.
Test: 2018-08-01T23:03:19.719341: step 2950, loss 0.547635.
Train: 2018-08-01T23:03:19.887861: step 2951, loss 0.618926.
Train: 2018-08-01T23:03:20.055439: step 2952, loss 0.581767.
Train: 2018-08-01T23:03:20.220996: step 2953, loss 0.563179.
Train: 2018-08-01T23:03:20.381540: step 2954, loss 0.451853.
Train: 2018-08-01T23:03:20.547130: step 2955, loss 0.71166.
Train: 2018-08-01T23:03:20.708697: step 2956, loss 0.600199.
Train: 2018-08-01T23:03:20.877216: step 2957, loss 0.618562.
Train: 2018-08-01T23:03:21.035809: step 2958, loss 0.581477.
Train: 2018-08-01T23:03:21.198381: step 2959, loss 0.544615.
Train: 2018-08-01T23:03:21.371925: step 2960, loss 0.54462.
Test: 2018-08-01T23:03:21.891504: step 2960, loss 0.547598.
Train: 2018-08-01T23:03:22.064043: step 2961, loss 0.52635.
Train: 2018-08-01T23:03:22.233627: step 2962, loss 0.453432.
Train: 2018-08-01T23:03:22.404158: step 2963, loss 0.635784.
Train: 2018-08-01T23:03:22.569721: step 2964, loss 0.653838.
Train: 2018-08-01T23:03:22.729264: step 2965, loss 0.544652.
Train: 2018-08-01T23:03:22.899824: step 2966, loss 0.598968.
Train: 2018-08-01T23:03:23.059382: step 2967, loss 0.544681.
Train: 2018-08-01T23:03:23.232947: step 2968, loss 0.634701.
Train: 2018-08-01T23:03:23.397478: step 2969, loss 0.526782.
Train: 2018-08-01T23:03:23.561043: step 2970, loss 0.508965.
Test: 2018-08-01T23:03:24.107581: step 2970, loss 0.547661.
Train: 2018-08-01T23:03:24.286133: step 2971, loss 0.616174.
Train: 2018-08-01T23:03:24.452688: step 2972, loss 0.437933.
Train: 2018-08-01T23:03:24.623228: step 2973, loss 0.580376.
Train: 2018-08-01T23:03:24.792749: step 2974, loss 0.651454.
Train: 2018-08-01T23:03:24.956327: step 2975, loss 0.509346.
Train: 2018-08-01T23:03:25.120871: step 2976, loss 0.597964.
Train: 2018-08-01T23:03:25.300416: step 2977, loss 0.580207.
Train: 2018-08-01T23:03:25.462956: step 2978, loss 0.527233.
Train: 2018-08-01T23:03:25.624556: step 2979, loss 0.527274.
Train: 2018-08-01T23:03:25.788089: step 2980, loss 0.615302.
Test: 2018-08-01T23:03:26.331640: step 2980, loss 0.547777.
Train: 2018-08-01T23:03:26.494232: step 2981, loss 0.474633.
Train: 2018-08-01T23:03:26.659757: step 2982, loss 0.56249.
Train: 2018-08-01T23:03:26.824335: step 2983, loss 0.580048.
Train: 2018-08-01T23:03:26.993889: step 2984, loss 0.720425.
Train: 2018-08-01T23:03:27.160434: step 2985, loss 0.544972.
Train: 2018-08-01T23:03:27.332986: step 2986, loss 0.562459.
Train: 2018-08-01T23:03:27.497549: step 2987, loss 0.492785.
Train: 2018-08-01T23:03:27.663075: step 2988, loss 0.597243.
Train: 2018-08-01T23:03:27.829630: step 2989, loss 0.510328.
Train: 2018-08-01T23:03:27.992194: step 2990, loss 0.597161.
Test: 2018-08-01T23:03:28.531753: step 2990, loss 0.547917.
Train: 2018-08-01T23:03:28.694343: step 2991, loss 0.614459.
Train: 2018-08-01T23:03:28.862868: step 2992, loss 0.475889.
Train: 2018-08-01T23:03:29.039428: step 2993, loss 0.597037.
Train: 2018-08-01T23:03:29.220935: step 2994, loss 0.631577.
Train: 2018-08-01T23:03:29.380484: step 2995, loss 0.545173.
Train: 2018-08-01T23:03:29.546067: step 2996, loss 0.683037.
Train: 2018-08-01T23:03:29.706645: step 2997, loss 0.64831.
Train: 2018-08-01T23:03:29.871172: step 2998, loss 0.733499.
Train: 2018-08-01T23:03:30.031774: step 2999, loss 0.511429.
Train: 2018-08-01T23:03:30.207305: step 3000, loss 0.545526.
Test: 2018-08-01T23:03:30.738879: step 3000, loss 0.548349.
Train: 2018-08-01T23:03:31.525223: step 3001, loss 0.545609.
Train: 2018-08-01T23:03:31.686767: step 3002, loss 0.596065.
Train: 2018-08-01T23:03:31.869279: step 3003, loss 0.462069.
Train: 2018-08-01T23:03:32.040848: step 3004, loss 0.595925.
Train: 2018-08-01T23:03:32.203385: step 3005, loss 0.512462.
Train: 2018-08-01T23:03:32.373930: step 3006, loss 0.579186.
Train: 2018-08-01T23:03:32.548463: step 3007, loss 0.595836.
Train: 2018-08-01T23:03:32.709064: step 3008, loss 0.595805.
Train: 2018-08-01T23:03:32.873593: step 3009, loss 0.662225.
Train: 2018-08-01T23:03:33.037157: step 3010, loss 0.597904.
Test: 2018-08-01T23:03:33.565744: step 3010, loss 0.548747.
Train: 2018-08-01T23:03:33.730304: step 3011, loss 0.529541.
Train: 2018-08-01T23:03:33.893867: step 3012, loss 0.645031.
Train: 2018-08-01T23:03:34.056433: step 3013, loss 0.562624.
Train: 2018-08-01T23:03:34.225980: step 3014, loss 0.497066.
Train: 2018-08-01T23:03:34.388575: step 3015, loss 0.595414.
Train: 2018-08-01T23:03:34.555100: step 3016, loss 0.52996.
Train: 2018-08-01T23:03:34.719659: step 3017, loss 0.48094.
Train: 2018-08-01T23:03:34.886239: step 3018, loss 0.546302.
Train: 2018-08-01T23:03:35.046784: step 3019, loss 0.464331.
Train: 2018-08-01T23:03:35.213340: step 3020, loss 0.628368.
Test: 2018-08-01T23:03:35.753905: step 3020, loss 0.548821.
Train: 2018-08-01T23:03:35.917457: step 3021, loss 0.628468.
Train: 2018-08-01T23:03:36.080022: step 3022, loss 0.513174.
Train: 2018-08-01T23:03:36.254557: step 3023, loss 0.562589.
Train: 2018-08-01T23:03:36.419117: step 3024, loss 0.595629.
Train: 2018-08-01T23:03:36.586668: step 3025, loss 0.562569.
Train: 2018-08-01T23:03:36.755217: step 3026, loss 0.728149.
Train: 2018-08-01T23:03:36.921772: step 3027, loss 0.562574.
Train: 2018-08-01T23:03:37.083340: step 3028, loss 0.513054.
Train: 2018-08-01T23:03:37.250894: step 3029, loss 0.628608.
Train: 2018-08-01T23:03:37.418469: step 3030, loss 0.579083.
Test: 2018-08-01T23:03:37.949027: step 3030, loss 0.548818.
Train: 2018-08-01T23:03:38.109630: step 3031, loss 0.64494.
Train: 2018-08-01T23:03:38.272162: step 3032, loss 0.546197.
Train: 2018-08-01T23:03:38.439716: step 3033, loss 0.513433.
Train: 2018-08-01T23:03:38.605273: step 3034, loss 0.562647.
Train: 2018-08-01T23:03:38.773846: step 3035, loss 0.464308.
Train: 2018-08-01T23:03:38.944366: step 3036, loss 0.562635.
Train: 2018-08-01T23:03:39.106932: step 3037, loss 0.562621.
Train: 2018-08-01T23:03:39.268499: step 3038, loss 0.463826.
Train: 2018-08-01T23:03:39.433059: step 3039, loss 0.546067.
Train: 2018-08-01T23:03:39.596621: step 3040, loss 0.479726.
Test: 2018-08-01T23:03:40.131193: step 3040, loss 0.548595.
Train: 2018-08-01T23:03:40.293757: step 3041, loss 0.678984.
Train: 2018-08-01T23:03:40.456323: step 3042, loss 0.445812.
Train: 2018-08-01T23:03:40.630856: step 3043, loss 0.662899.
Train: 2018-08-01T23:03:40.793422: step 3044, loss 0.56248.
Train: 2018-08-01T23:03:40.955989: step 3045, loss 0.495282.
Train: 2018-08-01T23:03:41.116584: step 3046, loss 0.545616.
Train: 2018-08-01T23:03:41.279127: step 3047, loss 0.562449.
Train: 2018-08-01T23:03:41.444681: step 3048, loss 0.596306.
Train: 2018-08-01T23:03:41.607260: step 3049, loss 0.528509.
Train: 2018-08-01T23:03:41.769812: step 3050, loss 0.528429.
Test: 2018-08-01T23:03:42.300394: step 3050, loss 0.548155.
Train: 2018-08-01T23:03:42.468944: step 3051, loss 0.511297.
Train: 2018-08-01T23:03:42.630548: step 3052, loss 0.630802.
Train: 2018-08-01T23:03:42.793102: step 3053, loss 0.613794.
Train: 2018-08-01T23:03:42.959662: step 3054, loss 0.562421.
Train: 2018-08-01T23:03:43.127214: step 3055, loss 0.733919.
Train: 2018-08-01T23:03:43.305707: step 3056, loss 0.493952.
Train: 2018-08-01T23:03:43.477247: step 3057, loss 0.493999.
Train: 2018-08-01T23:03:43.647791: step 3058, loss 0.579534.
Train: 2018-08-01T23:03:43.815345: step 3059, loss 0.545307.
Train: 2018-08-01T23:03:43.978937: step 3060, loss 0.648027.
Test: 2018-08-01T23:03:44.524450: step 3060, loss 0.548098.
Train: 2018-08-01T23:03:44.691004: step 3061, loss 0.545316.
Train: 2018-08-01T23:03:44.857587: step 3062, loss 0.528231.
Train: 2018-08-01T23:03:45.025109: step 3063, loss 0.52823.
Train: 2018-08-01T23:03:45.198646: step 3064, loss 0.647945.
Train: 2018-08-01T23:03:45.365200: step 3065, loss 0.630787.
Train: 2018-08-01T23:03:45.541729: step 3066, loss 0.545362.
Train: 2018-08-01T23:03:45.704295: step 3067, loss 0.528345.
Train: 2018-08-01T23:03:45.866861: step 3068, loss 0.562426.
Train: 2018-08-01T23:03:46.029425: step 3069, loss 0.664553.
Train: 2018-08-01T23:03:46.199970: step 3070, loss 0.545444.
Test: 2018-08-01T23:03:46.742520: step 3070, loss 0.548231.
Train: 2018-08-01T23:03:46.904088: step 3071, loss 0.562435.
Train: 2018-08-01T23:03:47.073659: step 3072, loss 0.579378.
Train: 2018-08-01T23:03:47.240203: step 3073, loss 0.52861.
Train: 2018-08-01T23:03:47.402755: step 3074, loss 0.54554.
Train: 2018-08-01T23:03:47.562361: step 3075, loss 0.461052.
Train: 2018-08-01T23:03:47.733899: step 3076, loss 0.630125.
Train: 2018-08-01T23:03:47.900423: step 3077, loss 0.494743.
Train: 2018-08-01T23:03:48.069971: step 3078, loss 0.562437.
Train: 2018-08-01T23:03:48.242514: step 3079, loss 0.596364.
Train: 2018-08-01T23:03:48.410062: step 3080, loss 0.511508.
Test: 2018-08-01T23:03:48.948621: step 3080, loss 0.548196.
Train: 2018-08-01T23:03:49.120189: step 3081, loss 0.647416.
Train: 2018-08-01T23:03:49.287715: step 3082, loss 0.528437.
Train: 2018-08-01T23:03:49.451279: step 3083, loss 0.562428.
Train: 2018-08-01T23:03:49.609855: step 3084, loss 0.562427.
Train: 2018-08-01T23:03:49.773442: step 3085, loss 0.511366.
Train: 2018-08-01T23:03:49.945955: step 3086, loss 0.630587.
Train: 2018-08-01T23:03:50.125502: step 3087, loss 0.715801.
Train: 2018-08-01T23:03:50.294027: step 3088, loss 0.545424.
Train: 2018-08-01T23:03:50.455594: step 3089, loss 0.528481.
Train: 2018-08-01T23:03:50.622149: step 3090, loss 0.47764.
Test: 2018-08-01T23:03:51.154725: step 3090, loss 0.548226.
Train: 2018-08-01T23:03:51.322293: step 3091, loss 0.562433.
Train: 2018-08-01T23:03:51.489861: step 3092, loss 0.545459.
Train: 2018-08-01T23:03:51.650431: step 3093, loss 0.511476.
Train: 2018-08-01T23:03:51.815987: step 3094, loss 0.545418.
Train: 2018-08-01T23:03:51.984506: step 3095, loss 0.562424.
Train: 2018-08-01T23:03:52.149066: step 3096, loss 0.443006.
Train: 2018-08-01T23:03:52.312653: step 3097, loss 0.459748.
Train: 2018-08-01T23:03:52.475195: step 3098, loss 0.579604.
Train: 2018-08-01T23:03:52.650749: step 3099, loss 0.545175.
Train: 2018-08-01T23:03:52.819274: step 3100, loss 0.579734.
Test: 2018-08-01T23:03:53.348860: step 3100, loss 0.5479.
Train: 2018-08-01T23:03:54.145599: step 3101, loss 0.510367.
Train: 2018-08-01T23:03:54.316173: step 3102, loss 0.475385.
Train: 2018-08-01T23:03:54.477742: step 3103, loss 0.562459.
Train: 2018-08-01T23:03:54.639279: step 3104, loss 0.54493.
Train: 2018-08-01T23:03:54.802841: step 3105, loss 0.509675.
Train: 2018-08-01T23:03:54.971390: step 3106, loss 0.56252.
Train: 2018-08-01T23:03:55.131992: step 3107, loss 0.544814.
Train: 2018-08-01T23:03:55.299538: step 3108, loss 0.615921.
Train: 2018-08-01T23:03:55.469091: step 3109, loss 0.59822.
Train: 2018-08-01T23:03:55.626669: step 3110, loss 0.598269.
Test: 2018-08-01T23:03:56.160213: step 3110, loss 0.547656.
Train: 2018-08-01T23:03:56.327791: step 3111, loss 0.562598.
Train: 2018-08-01T23:03:56.564142: step 3112, loss 0.598293.
Train: 2018-08-01T23:03:56.736701: step 3113, loss 0.616113.
Train: 2018-08-01T23:03:56.900234: step 3114, loss 0.544767.
Train: 2018-08-01T23:03:57.067787: step 3115, loss 0.526978.
Train: 2018-08-01T23:03:57.228357: step 3116, loss 0.544781.
Train: 2018-08-01T23:03:57.388954: step 3117, loss 0.58035.
Train: 2018-08-01T23:03:57.552491: step 3118, loss 0.49148.
Train: 2018-08-01T23:03:57.716053: step 3119, loss 0.562563.
Train: 2018-08-01T23:03:57.875627: step 3120, loss 0.598118.
Test: 2018-08-01T23:03:58.426154: step 3120, loss 0.547682.
Train: 2018-08-01T23:03:58.589718: step 3121, loss 0.598095.
Train: 2018-08-01T23:03:58.761292: step 3122, loss 0.615797.
Train: 2018-08-01T23:03:58.931834: step 3123, loss 0.527104.
Train: 2018-08-01T23:03:59.095367: step 3124, loss 0.651.
Train: 2018-08-01T23:03:59.255936: step 3125, loss 0.474256.
Train: 2018-08-01T23:03:59.420522: step 3126, loss 0.509608.
Train: 2018-08-01T23:03:59.592063: step 3127, loss 0.491996.
Train: 2018-08-01T23:03:59.760618: step 3128, loss 0.544868.
Train: 2018-08-01T23:03:59.925192: step 3129, loss 0.54486.
Train: 2018-08-01T23:04:00.091702: step 3130, loss 0.491858.
Test: 2018-08-01T23:04:00.637244: step 3130, loss 0.547711.
Train: 2018-08-01T23:04:00.799810: step 3131, loss 0.58022.
Train: 2018-08-01T23:04:00.967362: step 3132, loss 0.509398.
Train: 2018-08-01T23:04:01.135942: step 3133, loss 0.615771.
Train: 2018-08-01T23:04:01.298501: step 3134, loss 0.633561.
Train: 2018-08-01T23:04:01.469021: step 3135, loss 0.50932.
Train: 2018-08-01T23:04:01.631585: step 3136, loss 0.509314.
Train: 2018-08-01T23:04:01.793187: step 3137, loss 0.61583.
Train: 2018-08-01T23:04:01.954754: step 3138, loss 0.651329.
Train: 2018-08-01T23:04:02.118315: step 3139, loss 0.527085.
Train: 2018-08-01T23:04:02.280851: step 3140, loss 0.597957.
Test: 2018-08-01T23:04:02.824397: step 3140, loss 0.547714.
Train: 2018-08-01T23:04:02.985996: step 3141, loss 0.580208.
Train: 2018-08-01T23:04:03.157532: step 3142, loss 0.580167.
Train: 2018-08-01T23:04:03.315086: step 3143, loss 0.597745.
Train: 2018-08-01T23:04:03.483650: step 3144, loss 0.615235.
Train: 2018-08-01T23:04:03.645203: step 3145, loss 0.457275.
Train: 2018-08-01T23:04:03.806771: step 3146, loss 0.457391.
Train: 2018-08-01T23:04:03.970334: step 3147, loss 0.544947.
Train: 2018-08-01T23:04:04.133921: step 3148, loss 0.615054.
Train: 2018-08-01T23:04:04.296493: step 3149, loss 0.544945.
Train: 2018-08-01T23:04:04.460025: step 3150, loss 0.597507.
Test: 2018-08-01T23:04:05.005597: step 3150, loss 0.547802.
Train: 2018-08-01T23:04:05.226376: step 3151, loss 0.474923.
Train: 2018-08-01T23:04:05.387975: step 3152, loss 0.61502.
Train: 2018-08-01T23:04:05.558522: step 3153, loss 0.562465.
Train: 2018-08-01T23:04:05.725043: step 3154, loss 0.457423.
Train: 2018-08-01T23:04:05.894590: step 3155, loss 0.544942.
Train: 2018-08-01T23:04:06.059181: step 3156, loss 0.597569.
Train: 2018-08-01T23:04:06.225705: step 3157, loss 0.492253.
Train: 2018-08-01T23:04:06.394281: step 3158, loss 0.632804.
Train: 2018-08-01T23:04:06.559812: step 3159, loss 0.562485.
Train: 2018-08-01T23:04:06.721381: step 3160, loss 0.562485.
Test: 2018-08-01T23:04:07.252959: step 3160, loss 0.547761.
Train: 2018-08-01T23:04:07.417550: step 3161, loss 0.632817.
Train: 2018-08-01T23:04:07.592053: step 3162, loss 0.650296.
Train: 2018-08-01T23:04:07.758607: step 3163, loss 0.527422.
Train: 2018-08-01T23:04:07.919203: step 3164, loss 0.562459.
Train: 2018-08-01T23:04:08.081769: step 3165, loss 0.632316.
Train: 2018-08-01T23:04:08.248297: step 3166, loss 0.475329.
Train: 2018-08-01T23:04:08.411891: step 3167, loss 0.49282.
Train: 2018-08-01T23:04:08.584400: step 3168, loss 0.59725.
Train: 2018-08-01T23:04:08.745993: step 3169, loss 0.562437.
Train: 2018-08-01T23:04:08.908533: step 3170, loss 0.492886.
Test: 2018-08-01T23:04:09.462084: step 3170, loss 0.54787.
Train: 2018-08-01T23:04:09.633626: step 3171, loss 0.492853.
Train: 2018-08-01T23:04:09.804139: step 3172, loss 0.684387.
Train: 2018-08-01T23:04:09.969696: step 3173, loss 0.614674.
Train: 2018-08-01T23:04:10.128327: step 3174, loss 0.562436.
Train: 2018-08-01T23:04:10.289841: step 3175, loss 0.458226.
Train: 2018-08-01T23:04:10.454400: step 3176, loss 0.579807.
Train: 2018-08-01T23:04:10.617963: step 3177, loss 0.440807.
Train: 2018-08-01T23:04:10.788507: step 3178, loss 0.649468.
Train: 2018-08-01T23:04:10.947115: step 3179, loss 0.632083.
Train: 2018-08-01T23:04:11.110646: step 3180, loss 0.475453.
Test: 2018-08-01T23:04:11.658195: step 3180, loss 0.547864.
Train: 2018-08-01T23:04:11.818784: step 3181, loss 0.614655.
Train: 2018-08-01T23:04:11.981344: step 3182, loss 0.666832.
Train: 2018-08-01T23:04:12.149868: step 3183, loss 0.597164.
Train: 2018-08-01T23:04:12.321422: step 3184, loss 0.527771.
Train: 2018-08-01T23:04:12.489959: step 3185, loss 0.614325.
Train: 2018-08-01T23:04:12.652525: step 3186, loss 0.631471.
Train: 2018-08-01T23:04:12.815090: step 3187, loss 0.682893.
Train: 2018-08-01T23:04:12.988626: step 3188, loss 0.596682.
Train: 2018-08-01T23:04:13.152220: step 3189, loss 0.494197.
Train: 2018-08-01T23:04:13.314755: step 3190, loss 0.477409.
Test: 2018-08-01T23:04:13.863287: step 3190, loss 0.54821.
Train: 2018-08-01T23:04:14.026408: step 3191, loss 0.562427.
Train: 2018-08-01T23:04:14.189138: step 3192, loss 0.511562.
Train: 2018-08-01T23:04:14.354695: step 3193, loss 0.477681.
Train: 2018-08-01T23:04:14.522246: step 3194, loss 0.528494.
Train: 2018-08-01T23:04:14.681826: step 3195, loss 0.698361.
Train: 2018-08-01T23:04:14.853338: step 3196, loss 0.579406.
Train: 2018-08-01T23:04:15.024878: step 3197, loss 0.511532.
Train: 2018-08-01T23:04:15.187475: step 3198, loss 0.562428.
Train: 2018-08-01T23:04:15.351007: step 3199, loss 0.528493.
Train: 2018-08-01T23:04:15.513572: step 3200, loss 0.630337.
Test: 2018-08-01T23:04:16.051135: step 3200, loss 0.548217.
Train: 2018-08-01T23:04:16.840761: step 3201, loss 0.562427.
Train: 2018-08-01T23:04:17.001332: step 3202, loss 0.511536.
Train: 2018-08-01T23:04:17.159907: step 3203, loss 0.528482.
Train: 2018-08-01T23:04:17.323445: step 3204, loss 0.528446.
Train: 2018-08-01T23:04:17.485055: step 3205, loss 0.477353.
Train: 2018-08-01T23:04:17.648601: step 3206, loss 0.46007.
Train: 2018-08-01T23:04:17.810168: step 3207, loss 0.511041.
Train: 2018-08-01T23:04:17.976709: step 3208, loss 0.459231.
Train: 2018-08-01T23:04:18.141289: step 3209, loss 0.666154.
Train: 2018-08-01T23:04:18.304847: step 3210, loss 0.458359.
Test: 2018-08-01T23:04:18.843381: step 3210, loss 0.547853.
Train: 2018-08-01T23:04:19.005973: step 3211, loss 0.54502.
Train: 2018-08-01T23:04:19.168541: step 3212, loss 0.544965.
Train: 2018-08-01T23:04:19.330106: step 3213, loss 0.615155.
Train: 2018-08-01T23:04:19.498661: step 3214, loss 0.597703.
Train: 2018-08-01T23:04:19.664218: step 3215, loss 0.58014.
Train: 2018-08-01T23:04:19.825780: step 3216, loss 0.456548.
Train: 2018-08-01T23:04:19.988321: step 3217, loss 0.597939.
Train: 2018-08-01T23:04:20.148892: step 3218, loss 0.544806.
Train: 2018-08-01T23:04:20.307492: step 3219, loss 0.580318.
Train: 2018-08-01T23:04:20.474048: step 3220, loss 0.580348.
Test: 2018-08-01T23:04:21.015575: step 3220, loss 0.547667.
Train: 2018-08-01T23:04:21.180135: step 3221, loss 0.455795.
Train: 2018-08-01T23:04:21.349682: step 3222, loss 0.526926.
Train: 2018-08-01T23:04:21.507290: step 3223, loss 0.562606.
Train: 2018-08-01T23:04:21.680801: step 3224, loss 0.508926.
Train: 2018-08-01T23:04:21.846390: step 3225, loss 0.706148.
Train: 2018-08-01T23:04:22.021890: step 3226, loss 0.526773.
Train: 2018-08-01T23:04:22.179469: step 3227, loss 0.580585.
Train: 2018-08-01T23:04:22.344066: step 3228, loss 0.634387.
Train: 2018-08-01T23:04:22.514572: step 3229, loss 0.49098.
Train: 2018-08-01T23:04:22.679132: step 3230, loss 0.562627.
Test: 2018-08-01T23:04:23.229661: step 3230, loss 0.547634.
Train: 2018-08-01T23:04:23.392226: step 3231, loss 0.526825.
Train: 2018-08-01T23:04:23.560801: step 3232, loss 0.473132.
Train: 2018-08-01T23:04:23.720348: step 3233, loss 0.562633.
Train: 2018-08-01T23:04:23.881939: step 3234, loss 0.580572.
Train: 2018-08-01T23:04:24.046477: step 3235, loss 0.616456.
Train: 2018-08-01T23:04:24.214029: step 3236, loss 0.598492.
Train: 2018-08-01T23:04:24.369613: step 3237, loss 0.473096.
Train: 2018-08-01T23:04:24.536193: step 3238, loss 0.562627.
Train: 2018-08-01T23:04:24.697739: step 3239, loss 0.580531.
Train: 2018-08-01T23:04:24.862296: step 3240, loss 0.616312.
Test: 2018-08-01T23:04:25.406841: step 3240, loss 0.547641.
Train: 2018-08-01T23:04:25.566414: step 3241, loss 0.455366.
Train: 2018-08-01T23:04:25.736984: step 3242, loss 0.473227.
Train: 2018-08-01T23:04:25.905532: step 3243, loss 0.491027.
Train: 2018-08-01T23:04:26.069071: step 3244, loss 0.562642.
Train: 2018-08-01T23:04:26.233631: step 3245, loss 0.56266.
Train: 2018-08-01T23:04:26.403176: step 3246, loss 0.634618.
Train: 2018-08-01T23:04:26.571758: step 3247, loss 0.580661.
Train: 2018-08-01T23:04:26.734292: step 3248, loss 0.50873.
Train: 2018-08-01T23:04:26.894889: step 3249, loss 0.544688.
Train: 2018-08-01T23:04:27.064423: step 3250, loss 0.490714.
Test: 2018-08-01T23:04:27.597983: step 3250, loss 0.547608.
Train: 2018-08-01T23:04:27.762574: step 3251, loss 0.652742.
Train: 2018-08-01T23:04:27.932120: step 3252, loss 0.562685.
Train: 2018-08-01T23:04:28.092660: step 3253, loss 0.544685.
Train: 2018-08-01T23:04:28.261236: step 3254, loss 0.50871.
Train: 2018-08-01T23:04:28.420814: step 3255, loss 0.63465.
Train: 2018-08-01T23:04:28.586365: step 3256, loss 0.508737.
Train: 2018-08-01T23:04:28.746943: step 3257, loss 0.562665.
Train: 2018-08-01T23:04:28.917481: step 3258, loss 0.544695.
Train: 2018-08-01T23:04:29.088998: step 3259, loss 0.508772.
Train: 2018-08-01T23:04:29.250566: step 3260, loss 0.634539.
Test: 2018-08-01T23:04:29.786141: step 3260, loss 0.547619.
Train: 2018-08-01T23:04:29.950742: step 3261, loss 0.526745.
Train: 2018-08-01T23:04:30.123232: step 3262, loss 0.544702.
Train: 2018-08-01T23:04:30.297768: step 3263, loss 0.562648.
Train: 2018-08-01T23:04:30.462326: step 3264, loss 0.688208.
Train: 2018-08-01T23:04:30.624922: step 3265, loss 0.544723.
Train: 2018-08-01T23:04:30.784465: step 3266, loss 0.509007.
Train: 2018-08-01T23:04:30.955008: step 3267, loss 0.705364.
Train: 2018-08-01T23:04:31.133531: step 3268, loss 0.580357.
Train: 2018-08-01T23:04:31.296097: step 3269, loss 0.580276.
Train: 2018-08-01T23:04:31.460656: step 3270, loss 0.562516.
Test: 2018-08-01T23:04:31.995230: step 3270, loss 0.547735.
Train: 2018-08-01T23:04:32.170758: step 3271, loss 0.562496.
Train: 2018-08-01T23:04:32.341302: step 3272, loss 0.615216.
Train: 2018-08-01T23:04:32.508854: step 3273, loss 0.632544.
Train: 2018-08-01T23:04:32.686411: step 3274, loss 0.42285.
Train: 2018-08-01T23:04:32.845954: step 3275, loss 0.527599.
Train: 2018-08-01T23:04:33.015501: step 3276, loss 0.666833.
Train: 2018-08-01T23:04:33.177070: step 3277, loss 0.545071.
Train: 2018-08-01T23:04:33.340632: step 3278, loss 0.545102.
Train: 2018-08-01T23:04:33.506189: step 3279, loss 0.475964.
Train: 2018-08-01T23:04:33.668787: step 3280, loss 0.648848.
Test: 2018-08-01T23:04:34.218286: step 3280, loss 0.547962.
Train: 2018-08-01T23:04:34.381848: step 3281, loss 0.596935.
Train: 2018-08-01T23:04:34.544440: step 3282, loss 0.596867.
Train: 2018-08-01T23:04:34.705985: step 3283, loss 0.545223.
Train: 2018-08-01T23:04:34.867575: step 3284, loss 0.631044.
Train: 2018-08-01T23:04:35.038094: step 3285, loss 0.545298.
Train: 2018-08-01T23:04:35.200660: step 3286, loss 0.579491.
Train: 2018-08-01T23:04:35.363224: step 3287, loss 0.630581.
Train: 2018-08-01T23:04:35.537757: step 3288, loss 0.630387.
Train: 2018-08-01T23:04:35.705315: step 3289, loss 0.562431.
Train: 2018-08-01T23:04:35.863912: step 3290, loss 0.596191.
Test: 2018-08-01T23:04:36.411423: step 3290, loss 0.548372.
Train: 2018-08-01T23:04:36.582975: step 3291, loss 0.612907.
Train: 2018-08-01T23:04:36.754511: step 3292, loss 0.612735.
Train: 2018-08-01T23:04:36.914105: step 3293, loss 0.579181.
Train: 2018-08-01T23:04:37.076682: step 3294, loss 0.529281.
Train: 2018-08-01T23:04:37.243225: step 3295, loss 0.562542.
Train: 2018-08-01T23:04:37.413744: step 3296, loss 0.661749.
Train: 2018-08-01T23:04:37.578304: step 3297, loss 0.579063.
Train: 2018-08-01T23:04:37.754832: step 3298, loss 0.513385.
Train: 2018-08-01T23:04:37.922384: step 3299, loss 0.529891.
Train: 2018-08-01T23:04:38.083968: step 3300, loss 0.480874.
Test: 2018-08-01T23:04:38.620517: step 3300, loss 0.548953.
Train: 2018-08-01T23:04:39.366772: step 3301, loss 0.579017.
Train: 2018-08-01T23:04:39.537317: step 3302, loss 0.529919.
Train: 2018-08-01T23:04:39.696914: step 3303, loss 0.579025.
Train: 2018-08-01T23:04:39.858457: step 3304, loss 0.611815.
Train: 2018-08-01T23:04:40.020024: step 3305, loss 0.562636.
Train: 2018-08-01T23:04:40.188575: step 3306, loss 0.57903.
Train: 2018-08-01T23:04:40.351171: step 3307, loss 0.595426.
Train: 2018-08-01T23:04:40.512742: step 3308, loss 0.67737.
Train: 2018-08-01T23:04:40.675297: step 3309, loss 0.579015.
Train: 2018-08-01T23:04:40.835870: step 3310, loss 0.660631.
Test: 2018-08-01T23:04:41.366433: step 3310, loss 0.549085.
Train: 2018-08-01T23:04:41.525999: step 3311, loss 0.580071.
Train: 2018-08-01T23:04:41.689563: step 3312, loss 0.692547.
Train: 2018-08-01T23:04:41.851130: step 3313, loss 0.627402.
Train: 2018-08-01T23:04:42.016687: step 3314, loss 0.643209.
Train: 2018-08-01T23:04:42.183242: step 3315, loss 0.562953.
Train: 2018-08-01T23:04:42.354814: step 3316, loss 0.515337.
Train: 2018-08-01T23:04:42.515406: step 3317, loss 0.547238.
Train: 2018-08-01T23:04:42.677920: step 3318, loss 0.452492.
Train: 2018-08-01T23:04:42.846469: step 3319, loss 0.610536.
Train: 2018-08-01T23:04:43.009065: step 3320, loss 0.673704.
Test: 2018-08-01T23:04:43.542623: step 3320, loss 0.549968.
Train: 2018-08-01T23:04:43.715172: step 3321, loss 0.563167.
Train: 2018-08-01T23:04:43.887686: step 3322, loss 0.468772.
Train: 2018-08-01T23:04:44.058263: step 3323, loss 0.484458.
Train: 2018-08-01T23:04:44.224786: step 3324, loss 0.610488.
Train: 2018-08-01T23:04:44.397354: step 3325, loss 0.563125.
Train: 2018-08-01T23:04:44.574849: step 3326, loss 0.531444.
Train: 2018-08-01T23:04:44.737447: step 3327, loss 0.563062.
Train: 2018-08-01T23:04:44.904997: step 3328, loss 0.547126.
Train: 2018-08-01T23:04:45.081528: step 3329, loss 0.610807.
Train: 2018-08-01T23:04:45.245090: step 3330, loss 0.578927.
Test: 2018-08-01T23:04:45.789608: step 3330, loss 0.549541.
Train: 2018-08-01T23:04:45.960147: step 3331, loss 0.594924.
Train: 2018-08-01T23:04:46.126734: step 3332, loss 0.594944.
Train: 2018-08-01T23:04:46.295252: step 3333, loss 0.627004.
Train: 2018-08-01T23:04:46.460807: step 3334, loss 0.562911.
Train: 2018-08-01T23:04:46.622376: step 3335, loss 0.530868.
Train: 2018-08-01T23:04:46.786936: step 3336, loss 0.643073.
Train: 2018-08-01T23:04:46.963463: step 3337, loss 0.466731.
Train: 2018-08-01T23:04:47.125062: step 3338, loss 0.498668.
Train: 2018-08-01T23:04:47.283639: step 3339, loss 0.434068.
Train: 2018-08-01T23:04:47.450169: step 3340, loss 0.611308.
Test: 2018-08-01T23:04:47.991715: step 3340, loss 0.549144.
Train: 2018-08-01T23:04:48.162259: step 3341, loss 0.530269.
Train: 2018-08-01T23:04:48.331834: step 3342, loss 0.595295.
Train: 2018-08-01T23:04:48.498385: step 3343, loss 0.595368.
Train: 2018-08-01T23:04:48.672925: step 3344, loss 0.661014.
Train: 2018-08-01T23:04:48.837454: step 3345, loss 0.496979.
Train: 2018-08-01T23:04:48.996030: step 3346, loss 0.496828.
Train: 2018-08-01T23:04:49.162610: step 3347, loss 0.61206.
Train: 2018-08-01T23:04:49.329173: step 3348, loss 0.595617.
Train: 2018-08-01T23:04:49.496713: step 3349, loss 0.529448.
Train: 2018-08-01T23:04:49.672222: step 3350, loss 0.545952.
Test: 2018-08-01T23:04:50.215770: step 3350, loss 0.548604.
Train: 2018-08-01T23:04:50.384318: step 3351, loss 0.479427.
Train: 2018-08-01T23:04:50.549877: step 3352, loss 0.562499.
Train: 2018-08-01T23:04:50.712471: step 3353, loss 0.579206.
Train: 2018-08-01T23:04:50.890000: step 3354, loss 0.612774.
Train: 2018-08-01T23:04:51.062536: step 3355, loss 0.562459.
Train: 2018-08-01T23:04:51.228063: step 3356, loss 0.545629.
Train: 2018-08-01T23:04:51.393620: step 3357, loss 0.528742.
Train: 2018-08-01T23:04:51.562200: step 3358, loss 0.54555.
Train: 2018-08-01T23:04:51.722740: step 3359, loss 0.511656.
Train: 2018-08-01T23:04:51.893319: step 3360, loss 0.579394.
Test: 2018-08-01T23:04:52.424898: step 3360, loss 0.548172.
Train: 2018-08-01T23:04:52.590448: step 3361, loss 0.562417.
Train: 2018-08-01T23:04:52.753017: step 3362, loss 0.681741.
Train: 2018-08-01T23:04:52.912602: step 3363, loss 0.613559.
Train: 2018-08-01T23:04:53.082141: step 3364, loss 0.545376.
Train: 2018-08-01T23:04:53.248687: step 3365, loss 0.562414.
Train: 2018-08-01T23:04:53.409261: step 3366, loss 0.562415.
Train: 2018-08-01T23:04:53.571828: step 3367, loss 0.528366.
Train: 2018-08-01T23:04:53.733394: step 3368, loss 0.562414.
Train: 2018-08-01T23:04:53.907899: step 3369, loss 0.613523.
Train: 2018-08-01T23:04:54.078444: step 3370, loss 0.511327.
Test: 2018-08-01T23:04:54.604038: step 3370, loss 0.548148.
Train: 2018-08-01T23:04:54.771591: step 3371, loss 0.57945.
Train: 2018-08-01T23:04:54.932176: step 3372, loss 0.49426.
Train: 2018-08-01T23:04:55.099713: step 3373, loss 0.511237.
Train: 2018-08-01T23:04:55.266294: step 3374, loss 0.476961.
Train: 2018-08-01T23:04:55.429830: step 3375, loss 0.476706.
Train: 2018-08-01T23:04:55.599378: step 3376, loss 0.510785.
Train: 2018-08-01T23:04:55.764960: step 3377, loss 0.510569.
Train: 2018-08-01T23:04:55.928497: step 3378, loss 0.614506.
Train: 2018-08-01T23:04:56.093058: step 3379, loss 0.666943.
Train: 2018-08-01T23:04:56.267591: step 3380, loss 0.579882.
Test: 2018-08-01T23:04:56.816124: step 3380, loss 0.547823.
Train: 2018-08-01T23:04:56.982710: step 3381, loss 0.579902.
Train: 2018-08-01T23:04:57.154246: step 3382, loss 0.475102.
Train: 2018-08-01T23:04:57.316806: step 3383, loss 0.562452.
Train: 2018-08-01T23:04:57.478388: step 3384, loss 0.422271.
Train: 2018-08-01T23:04:57.651916: step 3385, loss 0.404254.
Train: 2018-08-01T23:04:57.822469: step 3386, loss 0.544839.
Train: 2018-08-01T23:04:57.985998: step 3387, loss 0.615798.
Train: 2018-08-01T23:04:58.158537: step 3388, loss 0.616004.
Train: 2018-08-01T23:04:58.319142: step 3389, loss 0.687534.
Train: 2018-08-01T23:04:58.483666: step 3390, loss 0.50904.
Test: 2018-08-01T23:04:59.027214: step 3390, loss 0.54764.
Train: 2018-08-01T23:04:59.194766: step 3391, loss 0.50901.
Train: 2018-08-01T23:04:59.354377: step 3392, loss 0.580493.
Train: 2018-08-01T23:04:59.514944: step 3393, loss 0.508927.
Train: 2018-08-01T23:04:59.684457: step 3394, loss 0.54471.
Train: 2018-08-01T23:04:59.848019: step 3395, loss 0.544701.
Train: 2018-08-01T23:05:00.020583: step 3396, loss 0.6345.
Train: 2018-08-01T23:05:00.185120: step 3397, loss 0.544693.
Train: 2018-08-01T23:05:00.356693: step 3398, loss 0.598576.
Train: 2018-08-01T23:05:00.527204: step 3399, loss 0.526746.
Train: 2018-08-01T23:05:00.693761: step 3400, loss 0.544698.
Test: 2018-08-01T23:05:01.233342: step 3400, loss 0.547617.
Train: 2018-08-01T23:05:02.000500: step 3401, loss 0.652372.
Train: 2018-08-01T23:05:02.164037: step 3402, loss 0.616388.
Train: 2018-08-01T23:05:02.328599: step 3403, loss 0.580488.
Train: 2018-08-01T23:05:02.488189: step 3404, loss 0.633941.
Train: 2018-08-01T23:05:02.653729: step 3405, loss 0.598119.
Train: 2018-08-01T23:05:02.825270: step 3406, loss 0.615685.
Train: 2018-08-01T23:05:02.986837: step 3407, loss 0.527204.
Train: 2018-08-01T23:05:03.151443: step 3408, loss 0.474526.
Train: 2018-08-01T23:05:03.312996: step 3409, loss 0.597589.
Train: 2018-08-01T23:05:03.476527: step 3410, loss 0.597501.
Test: 2018-08-01T23:05:04.021072: step 3410, loss 0.547811.
Train: 2018-08-01T23:05:04.186631: step 3411, loss 0.544969.
Train: 2018-08-01T23:05:04.354183: step 3412, loss 0.59732.
Train: 2018-08-01T23:05:04.512784: step 3413, loss 0.510234.
Train: 2018-08-01T23:05:04.671358: step 3414, loss 0.631915.
Train: 2018-08-01T23:05:04.831906: step 3415, loss 0.597081.
Train: 2018-08-01T23:05:04.994470: step 3416, loss 0.562413.
Train: 2018-08-01T23:05:05.157037: step 3417, loss 0.476202.
Train: 2018-08-01T23:05:05.320598: step 3418, loss 0.562409.
Train: 2018-08-01T23:05:05.495132: step 3419, loss 0.510779.
Train: 2018-08-01T23:05:05.661713: step 3420, loss 0.510781.
Test: 2018-08-01T23:05:06.209223: step 3420, loss 0.547988.
Train: 2018-08-01T23:05:06.386749: step 3421, loss 0.614072.
Train: 2018-08-01T23:05:06.558290: step 3422, loss 0.52797.
Train: 2018-08-01T23:05:06.718875: step 3423, loss 0.579634.
Train: 2018-08-01T23:05:06.890436: step 3424, loss 0.527955.
Train: 2018-08-01T23:05:07.052001: step 3425, loss 0.476226.
Train: 2018-08-01T23:05:07.223524: step 3426, loss 0.562411.
Train: 2018-08-01T23:05:07.384084: step 3427, loss 0.597002.
Train: 2018-08-01T23:05:07.558617: step 3428, loss 0.614343.
Train: 2018-08-01T23:05:07.722178: step 3429, loss 0.579724.
Train: 2018-08-01T23:05:07.883777: step 3430, loss 0.493199.
Test: 2018-08-01T23:05:08.417321: step 3430, loss 0.547916.
Train: 2018-08-01T23:05:08.580900: step 3431, loss 0.579733.
Train: 2018-08-01T23:05:08.745474: step 3432, loss 0.545094.
Train: 2018-08-01T23:05:08.911027: step 3433, loss 0.579751.
Train: 2018-08-01T23:05:09.072600: step 3434, loss 0.683771.
Train: 2018-08-01T23:05:09.237153: step 3435, loss 0.527799.
Train: 2018-08-01T23:05:09.407704: step 3436, loss 0.562413.
Train: 2018-08-01T23:05:09.575224: step 3437, loss 0.493312.
Train: 2018-08-01T23:05:09.737816: step 3438, loss 0.441466.
Train: 2018-08-01T23:05:09.906341: step 3439, loss 0.545104.
Train: 2018-08-01T23:05:10.070900: step 3440, loss 0.631799.
Test: 2018-08-01T23:05:10.601482: step 3440, loss 0.547887.
Train: 2018-08-01T23:05:10.767039: step 3441, loss 0.631849.
Train: 2018-08-01T23:05:10.929613: step 3442, loss 0.597119.
Train: 2018-08-01T23:05:11.088182: step 3443, loss 0.458419.
Train: 2018-08-01T23:05:11.252765: step 3444, loss 0.597109.
Train: 2018-08-01T23:05:11.414315: step 3445, loss 0.579765.
Train: 2018-08-01T23:05:11.575908: step 3446, loss 0.475715.
Train: 2018-08-01T23:05:11.734453: step 3447, loss 0.510347.
Train: 2018-08-01T23:05:11.894057: step 3448, loss 0.579813.
Train: 2018-08-01T23:05:12.057590: step 3449, loss 0.61465.
Train: 2018-08-01T23:05:12.220156: step 3450, loss 0.457966.
Test: 2018-08-01T23:05:12.755723: step 3450, loss 0.547833.
Train: 2018-08-01T23:05:12.918287: step 3451, loss 0.667078.
Train: 2018-08-01T23:05:13.089863: step 3452, loss 0.667071.
Train: 2018-08-01T23:05:13.250431: step 3453, loss 0.562431.
Train: 2018-08-01T23:05:13.415957: step 3454, loss 0.649349.
Train: 2018-08-01T23:05:13.578525: step 3455, loss 0.614434.
Train: 2018-08-01T23:05:13.739093: step 3456, loss 0.545129.
Train: 2018-08-01T23:05:13.913628: step 3457, loss 0.510698.
Train: 2018-08-01T23:05:14.077190: step 3458, loss 0.682866.
Train: 2018-08-01T23:05:14.242747: step 3459, loss 0.579558.
Train: 2018-08-01T23:05:14.405312: step 3460, loss 0.54531.
Test: 2018-08-01T23:05:14.952850: step 3460, loss 0.548131.
Train: 2018-08-01T23:05:15.122401: step 3461, loss 0.477152.
Train: 2018-08-01T23:05:15.284990: step 3462, loss 0.613511.
Train: 2018-08-01T23:05:15.447527: step 3463, loss 0.562415.
Train: 2018-08-01T23:05:15.609096: step 3464, loss 0.579396.
Train: 2018-08-01T23:05:15.768702: step 3465, loss 0.52852.
Train: 2018-08-01T23:05:15.937218: step 3466, loss 0.477742.
Train: 2018-08-01T23:05:16.094822: step 3467, loss 0.630206.
Train: 2018-08-01T23:05:16.265340: step 3468, loss 0.613236.
Train: 2018-08-01T23:05:16.431895: step 3469, loss 0.562428.
Train: 2018-08-01T23:05:16.604434: step 3470, loss 0.596231.
Test: 2018-08-01T23:05:17.141997: step 3470, loss 0.548304.
Train: 2018-08-01T23:05:17.301578: step 3471, loss 0.613066.
Train: 2018-08-01T23:05:17.477101: step 3472, loss 0.49507.
Train: 2018-08-01T23:05:17.638670: step 3473, loss 0.51195.
Train: 2018-08-01T23:05:17.808245: step 3474, loss 0.596117.
Train: 2018-08-01T23:05:17.977762: step 3475, loss 0.545614.
Train: 2018-08-01T23:05:18.149304: step 3476, loss 0.545612.
Train: 2018-08-01T23:05:18.309874: step 3477, loss 0.646649.
Train: 2018-08-01T23:05:18.470474: step 3478, loss 0.697061.
Train: 2018-08-01T23:05:18.636028: step 3479, loss 0.59602.
Train: 2018-08-01T23:05:18.800563: step 3480, loss 0.529016.
Test: 2018-08-01T23:05:19.334138: step 3480, loss 0.548507.
Train: 2018-08-01T23:05:19.507695: step 3481, loss 0.579183.
Train: 2018-08-01T23:05:19.669272: step 3482, loss 0.479194.
Train: 2018-08-01T23:05:19.829812: step 3483, loss 0.579157.
Train: 2018-08-01T23:05:19.992376: step 3484, loss 0.51257.
Train: 2018-08-01T23:05:20.152957: step 3485, loss 0.61246.
Train: 2018-08-01T23:05:20.315545: step 3486, loss 0.695681.
Train: 2018-08-01T23:05:20.476085: step 3487, loss 0.612348.
Train: 2018-08-01T23:05:20.637652: step 3488, loss 0.728174.
Train: 2018-08-01T23:05:20.809194: step 3489, loss 0.644985.
Train: 2018-08-01T23:05:20.973754: step 3490, loss 0.497089.
Test: 2018-08-01T23:05:21.505340: step 3490, loss 0.549007.
Train: 2018-08-01T23:05:21.665904: step 3491, loss 0.432098.
Train: 2018-08-01T23:05:21.832459: step 3492, loss 0.562688.
Train: 2018-08-01T23:05:22.005995: step 3493, loss 0.644133.
Train: 2018-08-01T23:05:22.167597: step 3494, loss 0.546467.
Train: 2018-08-01T23:05:22.334129: step 3495, loss 0.514039.
Train: 2018-08-01T23:05:22.492693: step 3496, loss 0.54651.
Train: 2018-08-01T23:05:22.659248: step 3497, loss 0.562736.
Train: 2018-08-01T23:05:22.829792: step 3498, loss 0.514009.
Train: 2018-08-01T23:05:22.988369: step 3499, loss 0.595241.
Train: 2018-08-01T23:05:23.151941: step 3500, loss 0.497592.
Test: 2018-08-01T23:05:23.689495: step 3500, loss 0.549022.
Train: 2018-08-01T23:05:24.482326: step 3501, loss 0.530058.
Train: 2018-08-01T23:05:24.644891: step 3502, loss 0.611716.
Train: 2018-08-01T23:05:24.810450: step 3503, loss 0.5954.
Train: 2018-08-01T23:05:24.972018: step 3504, loss 0.480628.
Train: 2018-08-01T23:05:25.146550: step 3505, loss 0.546162.
Train: 2018-08-01T23:05:25.310139: step 3506, loss 0.546095.
Train: 2018-08-01T23:05:25.472680: step 3507, loss 0.529495.
Train: 2018-08-01T23:05:25.638236: step 3508, loss 0.612281.
Train: 2018-08-01T23:05:25.809778: step 3509, loss 0.545897.
Train: 2018-08-01T23:05:25.972374: step 3510, loss 0.545843.
Test: 2018-08-01T23:05:26.511901: step 3510, loss 0.548503.
Train: 2018-08-01T23:05:26.682445: step 3511, loss 0.545787.
Train: 2018-08-01T23:05:26.862987: step 3512, loss 0.562471.
Train: 2018-08-01T23:05:27.029544: step 3513, loss 0.411428.
Train: 2018-08-01T23:05:27.194078: step 3514, loss 0.478151.
Train: 2018-08-01T23:05:27.356642: step 3515, loss 0.494625.
Train: 2018-08-01T23:05:27.533171: step 3516, loss 0.613558.
Train: 2018-08-01T23:05:27.697730: step 3517, loss 0.648043.
Train: 2018-08-01T23:05:27.869297: step 3518, loss 0.631108.
Train: 2018-08-01T23:05:28.032835: step 3519, loss 0.528002.
Train: 2018-08-01T23:05:28.214351: step 3520, loss 0.562407.
Test: 2018-08-01T23:05:28.769865: step 3520, loss 0.547954.
Train: 2018-08-01T23:05:28.935423: step 3521, loss 0.562409.
Train: 2018-08-01T23:05:29.097015: step 3522, loss 0.510549.
Train: 2018-08-01T23:05:29.256564: step 3523, loss 0.562415.
Train: 2018-08-01T23:05:29.420127: step 3524, loss 0.597127.
Train: 2018-08-01T23:05:29.591678: step 3525, loss 0.545049.
Train: 2018-08-01T23:05:29.770190: step 3526, loss 0.597213.
Train: 2018-08-01T23:05:29.931789: step 3527, loss 0.57983.
Train: 2018-08-01T23:05:30.101339: step 3528, loss 0.440592.
Train: 2018-08-01T23:05:30.267886: step 3529, loss 0.614747.
Train: 2018-08-01T23:05:30.432420: step 3530, loss 0.57989.
Test: 2018-08-01T23:05:30.978993: step 3530, loss 0.547818.
Train: 2018-08-01T23:05:31.146512: step 3531, loss 0.527518.
Train: 2018-08-01T23:05:31.313090: step 3532, loss 0.527491.
Train: 2018-08-01T23:05:31.487630: step 3533, loss 0.614941.
Train: 2018-08-01T23:05:31.658144: step 3534, loss 0.737472.
Train: 2018-08-01T23:05:31.821707: step 3535, loss 0.527516.
Train: 2018-08-01T23:05:31.983276: step 3536, loss 0.579867.
Train: 2018-08-01T23:05:32.147835: step 3537, loss 0.614633.
Train: 2018-08-01T23:05:32.312396: step 3538, loss 0.51034.
Train: 2018-08-01T23:05:32.481941: step 3539, loss 0.475743.
Train: 2018-08-01T23:05:32.646530: step 3540, loss 0.423751.
Test: 2018-08-01T23:05:33.179080: step 3540, loss 0.547879.
Train: 2018-08-01T23:05:33.343638: step 3541, loss 0.597153.
Train: 2018-08-01T23:05:33.509197: step 3542, loss 0.684129.
Train: 2018-08-01T23:05:33.678766: step 3543, loss 0.492929.
Train: 2018-08-01T23:05:33.844331: step 3544, loss 0.545045.
Train: 2018-08-01T23:05:34.007888: step 3545, loss 0.614583.
Train: 2018-08-01T23:05:34.178407: step 3546, loss 0.527664.
Train: 2018-08-01T23:05:34.345974: step 3547, loss 0.63195.
Train: 2018-08-01T23:05:34.521520: step 3548, loss 0.475594.
Train: 2018-08-01T23:05:34.688077: step 3549, loss 0.527678.
Train: 2018-08-01T23:05:34.846620: step 3550, loss 0.684122.
Test: 2018-08-01T23:05:35.386179: step 3550, loss 0.547878.
Train: 2018-08-01T23:05:35.552766: step 3551, loss 0.579787.
Train: 2018-08-01T23:05:35.717294: step 3552, loss 0.64914.
Train: 2018-08-01T23:05:35.881884: step 3553, loss 0.38939.
Train: 2018-08-01T23:05:36.050428: step 3554, loss 0.700871.
Train: 2018-08-01T23:05:36.223938: step 3555, loss 0.545133.
Train: 2018-08-01T23:05:36.385534: step 3556, loss 0.596913.
Train: 2018-08-01T23:05:36.550071: step 3557, loss 0.579628.
Train: 2018-08-01T23:05:36.714658: step 3558, loss 0.545213.
Train: 2018-08-01T23:05:36.877225: step 3559, loss 0.510904.
Train: 2018-08-01T23:05:37.054718: step 3560, loss 0.562404.
Test: 2018-08-01T23:05:37.586322: step 3560, loss 0.548043.
Train: 2018-08-01T23:05:37.752880: step 3561, loss 0.579553.
Train: 2018-08-01T23:05:37.913452: step 3562, loss 0.493853.
Train: 2018-08-01T23:05:38.076985: step 3563, loss 0.493825.
Train: 2018-08-01T23:05:38.247530: step 3564, loss 0.579572.
Train: 2018-08-01T23:05:38.412088: step 3565, loss 0.476477.
Train: 2018-08-01T23:05:38.576650: step 3566, loss 0.59685.
Train: 2018-08-01T23:05:38.746197: step 3567, loss 0.510667.
Train: 2018-08-01T23:05:38.913747: step 3568, loss 0.596972.
Train: 2018-08-01T23:05:39.072357: step 3569, loss 0.579715.
Train: 2018-08-01T23:05:39.249875: step 3570, loss 0.51046.
Test: 2018-08-01T23:05:39.786416: step 3570, loss 0.547893.
Train: 2018-08-01T23:05:39.956961: step 3571, loss 0.562417.
Train: 2018-08-01T23:05:40.128531: step 3572, loss 0.56242.
Train: 2018-08-01T23:05:40.295056: step 3573, loss 0.562424.
Train: 2018-08-01T23:05:40.456655: step 3574, loss 0.632042.
Train: 2018-08-01T23:05:40.625173: step 3575, loss 0.562426.
Train: 2018-08-01T23:05:40.793723: step 3576, loss 0.66681.
Train: 2018-08-01T23:05:40.964268: step 3577, loss 0.579788.
Train: 2018-08-01T23:05:41.131819: step 3578, loss 0.527745.
Train: 2018-08-01T23:05:41.299396: step 3579, loss 0.579728.
Train: 2018-08-01T23:05:41.464927: step 3580, loss 0.579701.
Test: 2018-08-01T23:05:42.009503: step 3580, loss 0.54795.
Train: 2018-08-01T23:05:42.174032: step 3581, loss 0.562408.
Train: 2018-08-01T23:05:42.330639: step 3582, loss 0.631372.
Train: 2018-08-01T23:05:42.509167: step 3583, loss 0.476393.
Train: 2018-08-01T23:05:42.673696: step 3584, loss 0.528026.
Train: 2018-08-01T23:05:42.838258: step 3585, loss 0.562404.
Train: 2018-08-01T23:05:42.997861: step 3586, loss 0.579585.
Train: 2018-08-01T23:05:43.169372: step 3587, loss 0.493706.
Train: 2018-08-01T23:05:43.332935: step 3588, loss 0.631144.
Train: 2018-08-01T23:05:43.489516: step 3589, loss 0.613935.
Train: 2018-08-01T23:05:43.652081: step 3590, loss 0.545246.
Test: 2018-08-01T23:05:44.195628: step 3590, loss 0.548048.
Train: 2018-08-01T23:05:44.367196: step 3591, loss 0.596689.
Train: 2018-08-01T23:05:44.528738: step 3592, loss 0.562404.
Train: 2018-08-01T23:05:44.689339: step 3593, loss 0.613712.
Train: 2018-08-01T23:05:44.863843: step 3594, loss 0.494115.
Train: 2018-08-01T23:05:45.038374: step 3595, loss 0.494153.
Train: 2018-08-01T23:05:45.202934: step 3596, loss 0.511187.
Train: 2018-08-01T23:05:45.364504: step 3597, loss 0.51112.
Train: 2018-08-01T23:05:45.531058: step 3598, loss 0.459636.
Train: 2018-08-01T23:05:45.694621: step 3599, loss 0.528036.
Train: 2018-08-01T23:05:45.858184: step 3600, loss 0.596892.
Test: 2018-08-01T23:05:46.396757: step 3600, loss 0.547934.
Train: 2018-08-01T23:05:47.142788: step 3601, loss 0.493261.
Train: 2018-08-01T23:05:47.305355: step 3602, loss 0.666479.
Train: 2018-08-01T23:05:47.473876: step 3603, loss 0.614524.
Train: 2018-08-01T23:05:47.644420: step 3604, loss 0.475541.
Train: 2018-08-01T23:05:47.808014: step 3605, loss 0.562426.
Train: 2018-08-01T23:05:47.979554: step 3606, loss 0.492716.
Train: 2018-08-01T23:05:48.145081: step 3607, loss 0.614843.
Train: 2018-08-01T23:05:48.309640: step 3608, loss 0.527468.
Train: 2018-08-01T23:05:48.470211: step 3609, loss 0.509908.
Train: 2018-08-01T23:05:48.632801: step 3610, loss 0.667763.
Test: 2018-08-01T23:05:49.182308: step 3610, loss 0.547763.
Train: 2018-08-01T23:05:49.346899: step 3611, loss 0.457135.
Train: 2018-08-01T23:05:49.509466: step 3612, loss 0.562473.
Train: 2018-08-01T23:05:49.675025: step 3613, loss 0.527264.
Train: 2018-08-01T23:05:49.834564: step 3614, loss 0.650679.
Train: 2018-08-01T23:05:50.016112: step 3615, loss 0.456649.
Train: 2018-08-01T23:05:50.179660: step 3616, loss 0.491832.
Train: 2018-08-01T23:05:50.346227: step 3617, loss 0.456271.
Train: 2018-08-01T23:05:50.510757: step 3618, loss 0.491472.
Train: 2018-08-01T23:05:50.685318: step 3619, loss 0.580418.
Train: 2018-08-01T23:05:50.861848: step 3620, loss 0.56261.
Test: 2018-08-01T23:05:51.402373: step 3620, loss 0.547613.
Train: 2018-08-01T23:05:51.561978: step 3621, loss 0.562638.
Train: 2018-08-01T23:05:51.724543: step 3622, loss 0.562663.
Train: 2018-08-01T23:05:51.896078: step 3623, loss 0.490615.
Train: 2018-08-01T23:05:52.080560: step 3624, loss 0.508527.
Train: 2018-08-01T23:05:52.246118: step 3625, loss 0.617091.
Train: 2018-08-01T23:05:52.409681: step 3626, loss 0.562773.
Train: 2018-08-01T23:05:52.581223: step 3627, loss 0.544624.
Train: 2018-08-01T23:05:52.738800: step 3628, loss 0.508242.
Train: 2018-08-01T23:05:52.904388: step 3629, loss 0.544613.
Train: 2018-08-01T23:05:53.079888: step 3630, loss 0.599343.
Test: 2018-08-01T23:05:53.625430: step 3630, loss 0.547574.
Train: 2018-08-01T23:05:53.790988: step 3631, loss 0.544605.
Train: 2018-08-01T23:05:53.950561: step 3632, loss 0.599412.
Train: 2018-08-01T23:05:54.120146: step 3633, loss 0.562871.
Train: 2018-08-01T23:05:54.324593: step 3634, loss 0.544604.
Train: 2018-08-01T23:05:54.486168: step 3635, loss 0.59938.
Train: 2018-08-01T23:05:54.668672: step 3636, loss 0.526365.
Train: 2018-08-01T23:05:54.830233: step 3637, loss 0.708708.
Train: 2018-08-01T23:05:54.996798: step 3638, loss 0.562805.
Train: 2018-08-01T23:05:55.166344: step 3639, loss 0.599052.
Train: 2018-08-01T23:05:55.345857: step 3640, loss 0.598908.
Test: 2018-08-01T23:05:55.894691: step 3640, loss 0.547595.
Train: 2018-08-01T23:05:56.062271: step 3641, loss 0.58072.
Train: 2018-08-01T23:05:56.233793: step 3642, loss 0.544685.
Train: 2018-08-01T23:05:56.422281: step 3643, loss 0.526793.
Train: 2018-08-01T23:05:56.667651: step 3644, loss 0.562598.
Train: 2018-08-01T23:05:56.889033: step 3645, loss 0.687406.
Train: 2018-08-01T23:05:57.099470: step 3646, loss 0.509251.
Train: 2018-08-01T23:05:57.287970: step 3647, loss 0.544807.
Train: 2018-08-01T23:05:57.475497: step 3648, loss 0.509493.
Train: 2018-08-01T23:05:57.668963: step 3649, loss 0.54485.
Train: 2018-08-01T23:05:57.860442: step 3650, loss 0.615347.
Test: 2018-08-01T23:05:58.423961: step 3650, loss 0.547746.
Train: 2018-08-01T23:05:58.621402: step 3651, loss 0.615232.
Train: 2018-08-01T23:05:58.807904: step 3652, loss 0.59754.
Train: 2018-08-01T23:05:58.994455: step 3653, loss 0.492486.
Train: 2018-08-01T23:05:59.177916: step 3654, loss 0.527517.
Train: 2018-08-01T23:05:59.370414: step 3655, loss 0.579874.
Train: 2018-08-01T23:05:59.560892: step 3656, loss 0.614685.
Train: 2018-08-01T23:05:59.728473: step 3657, loss 0.579807.
Train: 2018-08-01T23:05:59.904997: step 3658, loss 0.579765.
Train: 2018-08-01T23:06:00.103444: step 3659, loss 0.527788.
Train: 2018-08-01T23:06:00.303905: step 3660, loss 0.579695.
Test: 2018-08-01T23:06:00.842464: step 3660, loss 0.547954.
Train: 2018-08-01T23:06:01.030998: step 3661, loss 0.545148.
Train: 2018-08-01T23:06:01.220455: step 3662, loss 0.59688.
Train: 2018-08-01T23:06:01.430892: step 3663, loss 0.579613.
Train: 2018-08-01T23:06:01.641331: step 3664, loss 0.613942.
Train: 2018-08-01T23:06:01.845782: step 3665, loss 0.510982.
Train: 2018-08-01T23:06:02.006357: step 3666, loss 0.613755.
Train: 2018-08-01T23:06:02.178893: step 3667, loss 0.630739.
Train: 2018-08-01T23:06:02.341458: step 3668, loss 0.647589.
Train: 2018-08-01T23:06:02.518010: step 3669, loss 0.613331.
Train: 2018-08-01T23:06:02.694513: step 3670, loss 0.410314.
Test: 2018-08-01T23:06:03.238091: step 3670, loss 0.548298.
Train: 2018-08-01T23:06:03.409627: step 3671, loss 0.511796.
Train: 2018-08-01T23:06:03.572196: step 3672, loss 0.562433.
Train: 2018-08-01T23:06:03.733766: step 3673, loss 0.579301.
Train: 2018-08-01T23:06:03.900291: step 3674, loss 0.545577.
Train: 2018-08-01T23:06:04.059894: step 3675, loss 0.646724.
Train: 2018-08-01T23:06:04.222459: step 3676, loss 0.528771.
Train: 2018-08-01T23:06:04.393998: step 3677, loss 0.545619.
Train: 2018-08-01T23:06:04.563518: step 3678, loss 0.579266.
Train: 2018-08-01T23:06:04.729075: step 3679, loss 0.495189.
Train: 2018-08-01T23:06:04.894633: step 3680, loss 0.64658.
Test: 2018-08-01T23:06:05.436185: step 3680, loss 0.548363.
Train: 2018-08-01T23:06:05.631781: step 3681, loss 0.461539.
Train: 2018-08-01T23:06:05.812270: step 3682, loss 0.663463.
Train: 2018-08-01T23:06:05.989828: step 3683, loss 0.629758.
Train: 2018-08-01T23:06:06.162333: step 3684, loss 0.562449.
Train: 2018-08-01T23:06:06.327890: step 3685, loss 0.528885.
Train: 2018-08-01T23:06:06.500428: step 3686, loss 0.596011.
Train: 2018-08-01T23:06:06.689921: step 3687, loss 0.512176.
Train: 2018-08-01T23:06:06.857474: step 3688, loss 0.461882.
Train: 2018-08-01T23:06:07.024029: step 3689, loss 0.596037.
Train: 2018-08-01T23:06:07.192578: step 3690, loss 0.579258.
Test: 2018-08-01T23:06:07.738123: step 3690, loss 0.548357.
Train: 2018-08-01T23:06:07.905673: step 3691, loss 0.511974.
Train: 2018-08-01T23:06:08.083197: step 3692, loss 0.629838.
Train: 2018-08-01T23:06:08.256734: step 3693, loss 0.495008.
Train: 2018-08-01T23:06:08.422290: step 3694, loss 0.511785.
Train: 2018-08-01T23:06:08.585854: step 3695, loss 0.545505.
Train: 2018-08-01T23:06:08.752409: step 3696, loss 0.596331.
Train: 2018-08-01T23:06:08.954867: step 3697, loss 0.54543.
Train: 2018-08-01T23:06:09.151342: step 3698, loss 0.56241.
Train: 2018-08-01T23:06:09.353835: step 3699, loss 0.647596.
Train: 2018-08-01T23:06:09.535323: step 3700, loss 0.579447.
Test: 2018-08-01T23:06:10.059937: step 3700, loss 0.54814.
Train: 2018-08-01T23:06:10.930039: step 3701, loss 0.579445.
Train: 2018-08-01T23:06:11.106567: step 3702, loss 0.613503.
Train: 2018-08-01T23:06:11.280102: step 3703, loss 0.596437.
Train: 2018-08-01T23:06:11.485552: step 3704, loss 0.545423.
Train: 2018-08-01T23:06:11.655099: step 3705, loss 0.579386.
Train: 2018-08-01T23:06:11.830632: step 3706, loss 0.613275.
Train: 2018-08-01T23:06:12.020124: step 3707, loss 0.579345.
Train: 2018-08-01T23:06:12.184686: step 3708, loss 0.562428.
Train: 2018-08-01T23:06:12.368195: step 3709, loss 0.511838.
Train: 2018-08-01T23:06:12.541730: step 3710, loss 0.66357.
Test: 2018-08-01T23:06:13.080289: step 3710, loss 0.548358.
Train: 2018-08-01T23:06:13.255821: step 3711, loss 0.51198.
Train: 2018-08-01T23:06:13.425366: step 3712, loss 0.612864.
Train: 2018-08-01T23:06:13.587934: step 3713, loss 0.579235.
Train: 2018-08-01T23:06:13.750607: step 3714, loss 0.562465.
Train: 2018-08-01T23:06:13.914201: step 3715, loss 0.462104.
Train: 2018-08-01T23:06:14.083727: step 3716, loss 0.59594.
Train: 2018-08-01T23:06:14.254261: step 3717, loss 0.646141.
Train: 2018-08-01T23:06:14.428795: step 3718, loss 0.57919.
Train: 2018-08-01T23:06:14.591391: step 3719, loss 0.512414.
Train: 2018-08-01T23:06:14.757914: step 3720, loss 0.562487.
Test: 2018-08-01T23:06:15.309442: step 3720, loss 0.548519.
Train: 2018-08-01T23:06:15.474031: step 3721, loss 0.512441.
Train: 2018-08-01T23:06:15.636592: step 3722, loss 0.479012.
Train: 2018-08-01T23:06:15.808107: step 3723, loss 0.529014.
Train: 2018-08-01T23:06:15.970699: step 3724, loss 0.545689.
Train: 2018-08-01T23:06:16.136230: step 3725, loss 0.6297.
Train: 2018-08-01T23:06:16.305776: step 3726, loss 0.511934.
Train: 2018-08-01T23:06:16.472363: step 3727, loss 0.545563.
Train: 2018-08-01T23:06:16.635920: step 3728, loss 0.528613.
Train: 2018-08-01T23:06:16.805443: step 3729, loss 0.613263.
Train: 2018-08-01T23:06:16.968006: step 3730, loss 0.562414.
Test: 2018-08-01T23:06:17.512552: step 3730, loss 0.548177.
Train: 2018-08-01T23:06:17.684128: step 3731, loss 0.562411.
Train: 2018-08-01T23:06:17.860620: step 3732, loss 0.511349.
Train: 2018-08-01T23:06:18.025206: step 3733, loss 0.630617.
Train: 2018-08-01T23:06:18.200712: step 3734, loss 0.511207.
Train: 2018-08-01T23:06:18.367266: step 3735, loss 0.528221.
Train: 2018-08-01T23:06:18.531828: step 3736, loss 0.545279.
Train: 2018-08-01T23:06:18.696420: step 3737, loss 0.596712.
Train: 2018-08-01T23:06:18.864936: step 3738, loss 0.510872.
Train: 2018-08-01T23:06:19.027536: step 3739, loss 0.493568.
Train: 2018-08-01T23:06:19.189069: step 3740, loss 0.579659.
Test: 2018-08-01T23:06:19.724639: step 3740, loss 0.547928.
Train: 2018-08-01T23:06:19.891221: step 3741, loss 0.527825.
Train: 2018-08-01T23:06:20.054780: step 3742, loss 0.493079.
Train: 2018-08-01T23:06:20.216349: step 3743, loss 0.527645.
Train: 2018-08-01T23:06:20.380902: step 3744, loss 0.579878.
Train: 2018-08-01T23:06:20.547439: step 3745, loss 0.40502.
Train: 2018-08-01T23:06:20.709032: step 3746, loss 0.59761.
Train: 2018-08-01T23:06:20.874565: step 3747, loss 0.544854.
Train: 2018-08-01T23:06:21.038128: step 3748, loss 0.527126.
Train: 2018-08-01T23:06:21.196733: step 3749, loss 0.544784.
Train: 2018-08-01T23:06:21.362261: step 3750, loss 0.615987.
Test: 2018-08-01T23:06:21.905816: step 3750, loss 0.547639.
Train: 2018-08-01T23:06:22.081338: step 3751, loss 0.616111.
Train: 2018-08-01T23:06:22.249887: step 3752, loss 0.526871.
Train: 2018-08-01T23:06:22.413483: step 3753, loss 0.52684.
Train: 2018-08-01T23:06:22.574021: step 3754, loss 0.490993.
Train: 2018-08-01T23:06:22.740576: step 3755, loss 0.544693.
Train: 2018-08-01T23:06:22.911119: step 3756, loss 0.5267.
Train: 2018-08-01T23:06:23.078698: step 3757, loss 0.454576.
Train: 2018-08-01T23:06:23.242263: step 3758, loss 0.598875.
Train: 2018-08-01T23:06:23.405822: step 3759, loss 0.435934.
Train: 2018-08-01T23:06:23.567367: step 3760, loss 0.544618.
Test: 2018-08-01T23:06:24.105926: step 3760, loss 0.547573.
Train: 2018-08-01T23:06:24.277478: step 3761, loss 0.617567.
Train: 2018-08-01T23:06:24.443024: step 3762, loss 0.5446.
Train: 2018-08-01T23:06:24.611575: step 3763, loss 0.45305.
Train: 2018-08-01T23:06:24.769152: step 3764, loss 0.618037.
Train: 2018-08-01T23:06:24.929736: step 3765, loss 0.581371.
Train: 2018-08-01T23:06:25.089298: step 3766, loss 0.599817.
Train: 2018-08-01T23:06:25.254854: step 3767, loss 0.618241.
Train: 2018-08-01T23:06:25.415455: step 3768, loss 0.507788.
Train: 2018-08-01T23:06:25.573029: step 3769, loss 0.434213.
Train: 2018-08-01T23:06:25.735601: step 3770, loss 0.526166.
Test: 2018-08-01T23:06:26.272135: step 3770, loss 0.547584.
Train: 2018-08-01T23:06:26.433702: step 3771, loss 0.618365.
Train: 2018-08-01T23:06:26.595277: step 3772, loss 0.544584.
Train: 2018-08-01T23:06:26.756843: step 3773, loss 0.618408.
Train: 2018-08-01T23:06:26.918406: step 3774, loss 0.599913.
Train: 2018-08-01T23:06:27.090945: step 3775, loss 0.526168.
Train: 2018-08-01T23:06:27.258498: step 3776, loss 0.452585.
Train: 2018-08-01T23:06:27.415107: step 3777, loss 0.599807.
Train: 2018-08-01T23:06:27.585623: step 3778, loss 0.746983.
Train: 2018-08-01T23:06:27.747191: step 3779, loss 0.526246.
Train: 2018-08-01T23:06:27.911752: step 3780, loss 0.507996.
Test: 2018-08-01T23:06:28.446322: step 3780, loss 0.547572.
Train: 2018-08-01T23:06:28.619884: step 3781, loss 0.526331.
Train: 2018-08-01T23:06:28.782426: step 3782, loss 0.50811.
Train: 2018-08-01T23:06:28.944016: step 3783, loss 0.489896.
Train: 2018-08-01T23:06:29.106573: step 3784, loss 0.562848.
Train: 2018-08-01T23:06:29.266131: step 3785, loss 0.635817.
Train: 2018-08-01T23:06:29.430722: step 3786, loss 0.489946.
Train: 2018-08-01T23:06:29.614200: step 3787, loss 0.599261.
Train: 2018-08-01T23:06:29.789731: step 3788, loss 0.653815.
Train: 2018-08-01T23:06:29.972243: step 3789, loss 0.562783.
Train: 2018-08-01T23:06:30.147774: step 3790, loss 0.490274.
Test: 2018-08-01T23:06:30.695315: step 3790, loss 0.547583.
Train: 2018-08-01T23:06:30.863861: step 3791, loss 0.598935.
Train: 2018-08-01T23:06:31.025429: step 3792, loss 0.689187.
Train: 2018-08-01T23:06:31.199992: step 3793, loss 0.598685.
Train: 2018-08-01T23:06:31.366517: step 3794, loss 0.616446.
Train: 2018-08-01T23:06:31.536062: step 3795, loss 0.580451.
Train: 2018-08-01T23:06:31.705610: step 3796, loss 0.615907.
Train: 2018-08-01T23:06:31.884158: step 3797, loss 0.615619.
Train: 2018-08-01T23:06:32.050713: step 3798, loss 0.474423.
Train: 2018-08-01T23:06:32.217262: step 3799, loss 0.509808.
Train: 2018-08-01T23:06:32.377838: step 3800, loss 0.54494.
Test: 2018-08-01T23:06:32.924360: step 3800, loss 0.547807.
Train: 2018-08-01T23:06:33.716692: step 3801, loss 0.649802.
Train: 2018-08-01T23:06:33.883247: step 3802, loss 0.579844.
Train: 2018-08-01T23:06:34.046811: step 3803, loss 0.475601.
Train: 2018-08-01T23:06:34.212368: step 3804, loss 0.631753.
Train: 2018-08-01T23:06:34.383909: step 3805, loss 0.579699.
Train: 2018-08-01T23:06:34.546507: step 3806, loss 0.579651.
Train: 2018-08-01T23:06:34.719023: step 3807, loss 0.596809.
Train: 2018-08-01T23:06:34.883573: step 3808, loss 0.59671.
Train: 2018-08-01T23:06:35.055146: step 3809, loss 0.528197.
Train: 2018-08-01T23:06:35.230672: step 3810, loss 0.613594.
Test: 2018-08-01T23:06:35.781204: step 3810, loss 0.548159.
Train: 2018-08-01T23:06:35.950736: step 3811, loss 0.562408.
Train: 2018-08-01T23:06:36.125285: step 3812, loss 0.562413.
Train: 2018-08-01T23:06:36.298820: step 3813, loss 0.528547.
Train: 2018-08-01T23:06:36.467339: step 3814, loss 0.477864.
Train: 2018-08-01T23:06:36.647869: step 3815, loss 0.54551.
Train: 2018-08-01T23:06:36.815409: step 3816, loss 0.630099.
Train: 2018-08-01T23:06:36.990940: step 3817, loss 0.511699.
Train: 2018-08-01T23:06:37.156498: step 3818, loss 0.579335.
Train: 2018-08-01T23:06:37.330064: step 3819, loss 0.596248.
Train: 2018-08-01T23:06:37.496587: step 3820, loss 0.562425.
Test: 2018-08-01T23:06:38.044150: step 3820, loss 0.548275.
Train: 2018-08-01T23:06:38.249576: step 3821, loss 0.61312.
Train: 2018-08-01T23:06:38.447047: step 3822, loss 0.64683.
Train: 2018-08-01T23:06:38.629560: step 3823, loss 0.478226.
Train: 2018-08-01T23:06:38.833015: step 3824, loss 0.511945.
Train: 2018-08-01T23:06:39.031489: step 3825, loss 0.646624.
Train: 2018-08-01T23:06:39.241922: step 3826, loss 0.461522.
Train: 2018-08-01T23:06:39.458344: step 3827, loss 0.56244.
Train: 2018-08-01T23:06:39.670776: step 3828, loss 0.62982.
Train: 2018-08-01T23:06:39.910137: step 3829, loss 0.511917.
Train: 2018-08-01T23:06:40.104422: step 3830, loss 0.663537.
Test: 2018-08-01T23:06:40.646981: step 3830, loss 0.548344.
Train: 2018-08-01T23:06:40.826529: step 3831, loss 0.579273.
Train: 2018-08-01T23:06:41.005036: step 3832, loss 0.528816.
Train: 2018-08-01T23:06:41.183545: step 3833, loss 0.545639.
Train: 2018-08-01T23:06:41.383013: step 3834, loss 0.495216.
Train: 2018-08-01T23:06:41.565525: step 3835, loss 0.528789.
Train: 2018-08-01T23:06:41.755020: step 3836, loss 0.495023.
Train: 2018-08-01T23:06:41.933541: step 3837, loss 0.54553.
Train: 2018-08-01T23:06:42.106080: step 3838, loss 0.596298.
Train: 2018-08-01T23:06:42.290587: step 3839, loss 0.562413.
Train: 2018-08-01T23:06:42.460134: step 3840, loss 0.562409.
Test: 2018-08-01T23:06:42.995702: step 3840, loss 0.548149.
Train: 2018-08-01T23:06:43.170235: step 3841, loss 0.681593.
Train: 2018-08-01T23:06:43.348759: step 3842, loss 0.528367.
Train: 2018-08-01T23:06:43.528315: step 3843, loss 0.57943.
Train: 2018-08-01T23:06:43.706801: step 3844, loss 0.528362.
Train: 2018-08-01T23:06:43.886322: step 3845, loss 0.545375.
Train: 2018-08-01T23:06:44.078807: step 3846, loss 0.528319.
Train: 2018-08-01T23:06:44.282263: step 3847, loss 0.647721.
Train: 2018-08-01T23:06:44.472754: step 3848, loss 0.545343.
Train: 2018-08-01T23:06:44.657262: step 3849, loss 0.511218.
Train: 2018-08-01T23:06:44.823816: step 3850, loss 0.562402.
Test: 2018-08-01T23:06:45.356393: step 3850, loss 0.54809.
Train: 2018-08-01T23:06:45.545886: step 3851, loss 0.682037.
Train: 2018-08-01T23:06:45.719421: step 3852, loss 0.425822.
Train: 2018-08-01T23:06:45.888967: step 3853, loss 0.562401.
Train: 2018-08-01T23:06:46.064498: step 3854, loss 0.630838.
Train: 2018-08-01T23:06:46.236061: step 3855, loss 0.562401.
Train: 2018-08-01T23:06:46.405587: step 3856, loss 0.562401.
Train: 2018-08-01T23:06:46.575133: step 3857, loss 0.562401.
Train: 2018-08-01T23:06:46.747692: step 3858, loss 0.596609.
Train: 2018-08-01T23:06:46.919214: step 3859, loss 0.545306.
Train: 2018-08-01T23:06:47.109721: step 3860, loss 0.630765.
Test: 2018-08-01T23:06:47.652254: step 3860, loss 0.548109.
Train: 2018-08-01T23:06:47.834766: step 3861, loss 0.562403.
Train: 2018-08-01T23:06:48.047198: step 3862, loss 0.51125.
Train: 2018-08-01T23:06:48.211759: step 3863, loss 0.51126.
Train: 2018-08-01T23:06:48.377350: step 3864, loss 0.63064.
Train: 2018-08-01T23:06:48.546895: step 3865, loss 0.63061.
Train: 2018-08-01T23:06:48.719400: step 3866, loss 0.630515.
Train: 2018-08-01T23:06:48.883961: step 3867, loss 0.443493.
Train: 2018-08-01T23:06:49.056530: step 3868, loss 0.59638.
Train: 2018-08-01T23:06:49.233053: step 3869, loss 0.579386.
Train: 2018-08-01T23:06:49.395622: step 3870, loss 0.596333.
Test: 2018-08-01T23:06:49.939149: step 3870, loss 0.548234.
Train: 2018-08-01T23:06:50.104724: step 3871, loss 0.528542.
Train: 2018-08-01T23:06:50.267264: step 3872, loss 0.56242.
Train: 2018-08-01T23:06:50.430826: step 3873, loss 0.528578.
Train: 2018-08-01T23:06:50.594389: step 3874, loss 0.494724.
Train: 2018-08-01T23:06:50.761941: step 3875, loss 0.545471.
Train: 2018-08-01T23:06:50.943477: step 3876, loss 0.73211.
Train: 2018-08-01T23:06:51.104026: step 3877, loss 0.545469.
Train: 2018-08-01T23:06:51.266592: step 3878, loss 0.596285.
Train: 2018-08-01T23:06:51.428165: step 3879, loss 0.596247.
Train: 2018-08-01T23:06:51.604721: step 3880, loss 0.596198.
Test: 2018-08-01T23:06:52.145242: step 3880, loss 0.548323.
Train: 2018-08-01T23:06:52.310830: step 3881, loss 0.579288.
Train: 2018-08-01T23:06:52.475389: step 3882, loss 0.562443.
Train: 2018-08-01T23:06:52.646903: step 3883, loss 0.562451.
Train: 2018-08-01T23:06:52.818444: step 3884, loss 0.595993.
Train: 2018-08-01T23:06:52.986023: step 3885, loss 0.545729.
Train: 2018-08-01T23:06:53.148561: step 3886, loss 0.562474.
Train: 2018-08-01T23:06:53.326118: step 3887, loss 0.512377.
Train: 2018-08-01T23:06:53.493643: step 3888, loss 0.529078.
Train: 2018-08-01T23:06:53.654211: step 3889, loss 0.495633.
Train: 2018-08-01T23:06:53.824753: step 3890, loss 0.579206.
Test: 2018-08-01T23:06:54.365309: step 3890, loss 0.548423.
Train: 2018-08-01T23:06:54.562781: step 3891, loss 0.512173.
Train: 2018-08-01T23:06:54.734322: step 3892, loss 0.512058.
Train: 2018-08-01T23:06:54.897885: step 3893, loss 0.52875.
Train: 2018-08-01T23:06:55.065438: step 3894, loss 0.477951.
Train: 2018-08-01T23:06:55.225010: step 3895, loss 0.630272.
Train: 2018-08-01T23:06:55.385580: step 3896, loss 0.562408.
Train: 2018-08-01T23:06:55.550175: step 3897, loss 0.579455.
Train: 2018-08-01T23:06:55.717692: step 3898, loss 0.647825.
Train: 2018-08-01T23:06:55.891229: step 3899, loss 0.596586.
Train: 2018-08-01T23:06:56.058782: step 3900, loss 0.459855.
Test: 2018-08-01T23:06:56.607341: step 3900, loss 0.548067.
Train: 2018-08-01T23:06:57.436550: step 3901, loss 0.630866.
Train: 2018-08-01T23:06:57.597148: step 3902, loss 0.476791.
Train: 2018-08-01T23:06:57.761681: step 3903, loss 0.613846.
Train: 2018-08-01T23:06:57.969129: step 3904, loss 0.648199.
Train: 2018-08-01T23:06:58.159626: step 3905, loss 0.528104.
Train: 2018-08-01T23:06:58.347116: step 3906, loss 0.528108.
Train: 2018-08-01T23:06:58.515665: step 3907, loss 0.596704.
Train: 2018-08-01T23:06:58.685213: step 3908, loss 0.579549.
Train: 2018-08-01T23:06:58.856754: step 3909, loss 0.545256.
Train: 2018-08-01T23:06:59.026340: step 3910, loss 0.562399.
Test: 2018-08-01T23:06:59.568851: step 3910, loss 0.548046.
Train: 2018-08-01T23:06:59.736402: step 3911, loss 0.57954.
Train: 2018-08-01T23:06:59.901991: step 3912, loss 0.510996.
Train: 2018-08-01T23:07:00.082504: step 3913, loss 0.635542.
Train: 2018-08-01T23:07:00.262994: step 3914, loss 0.579531.
Train: 2018-08-01T23:07:00.429550: step 3915, loss 0.425465.
Train: 2018-08-01T23:07:00.591150: step 3916, loss 0.579539.
Train: 2018-08-01T23:07:00.753683: step 3917, loss 0.59671.
Train: 2018-08-01T23:07:00.919267: step 3918, loss 0.545238.
Train: 2018-08-01T23:07:01.084818: step 3919, loss 0.562399.
Train: 2018-08-01T23:07:01.246391: step 3920, loss 0.545221.
Test: 2018-08-01T23:07:01.792906: step 3920, loss 0.548005.
Train: 2018-08-01T23:07:01.969435: step 3921, loss 0.545209.
Train: 2018-08-01T23:07:02.163945: step 3922, loss 0.579606.
Train: 2018-08-01T23:07:02.341439: step 3923, loss 0.5624.
Train: 2018-08-01T23:07:02.517967: step 3924, loss 0.510734.
Train: 2018-08-01T23:07:02.699516: step 3925, loss 0.510674.
Train: 2018-08-01T23:07:02.876034: step 3926, loss 0.527856.
Train: 2018-08-01T23:07:03.053562: step 3927, loss 0.648962.
Train: 2018-08-01T23:07:03.236047: step 3928, loss 0.545089.
Train: 2018-08-01T23:07:03.399610: step 3929, loss 0.649081.
Train: 2018-08-01T23:07:03.571151: step 3930, loss 0.56241.
Test: 2018-08-01T23:07:04.101732: step 3930, loss 0.547911.
Train: 2018-08-01T23:07:04.272277: step 3931, loss 0.527784.
Train: 2018-08-01T23:07:04.449802: step 3932, loss 0.666274.
Train: 2018-08-01T23:07:04.624336: step 3933, loss 0.493275.
Train: 2018-08-01T23:07:04.804853: step 3934, loss 0.527855.
Train: 2018-08-01T23:07:04.982410: step 3935, loss 0.545129.
Train: 2018-08-01T23:07:05.160902: step 3936, loss 0.614248.
Train: 2018-08-01T23:07:05.329477: step 3937, loss 0.545132.
Train: 2018-08-01T23:07:05.507973: step 3938, loss 0.493333.
Train: 2018-08-01T23:07:05.679517: step 3939, loss 0.562405.
Train: 2018-08-01T23:07:05.871004: step 3940, loss 0.510526.
Test: 2018-08-01T23:07:06.414551: step 3940, loss 0.547908.
Train: 2018-08-01T23:07:06.582102: step 3941, loss 0.597043.
Train: 2018-08-01T23:07:06.796528: step 3942, loss 0.56241.
Train: 2018-08-01T23:07:06.978045: step 3943, loss 0.545073.
Train: 2018-08-01T23:07:07.168542: step 3944, loss 0.52771.
Train: 2018-08-01T23:07:07.422855: step 3945, loss 0.614532.
Train: 2018-08-01T23:07:07.660221: step 3946, loss 0.545042.
Train: 2018-08-01T23:07:07.849714: step 3947, loss 0.510271.
Train: 2018-08-01T23:07:08.016268: step 3948, loss 0.527618.
Train: 2018-08-01T23:07:08.176840: step 3949, loss 0.545.
Train: 2018-08-01T23:07:08.336438: step 3950, loss 0.527526.
Test: 2018-08-01T23:07:08.870017: step 3950, loss 0.547798.
Train: 2018-08-01T23:07:09.031558: step 3951, loss 0.405074.
Train: 2018-08-01T23:07:09.213094: step 3952, loss 0.474695.
Train: 2018-08-01T23:07:09.388601: step 3953, loss 0.633025.
Train: 2018-08-01T23:07:09.604024: step 3954, loss 0.615574.
Train: 2018-08-01T23:07:09.780583: step 3955, loss 0.597966.
Train: 2018-08-01T23:07:09.950098: step 3956, loss 0.56253.
Train: 2018-08-01T23:07:10.124632: step 3957, loss 0.527023.
Train: 2018-08-01T23:07:10.292237: step 3958, loss 0.580324.
Train: 2018-08-01T23:07:10.454750: step 3959, loss 0.562552.
Train: 2018-08-01T23:07:10.635275: step 3960, loss 0.598153.
Test: 2018-08-01T23:07:11.180813: step 3960, loss 0.547653.
Train: 2018-08-01T23:07:11.366313: step 3961, loss 0.68712.
Train: 2018-08-01T23:07:11.529909: step 3962, loss 0.562537.
Train: 2018-08-01T23:07:11.688452: step 3963, loss 0.491627.
Train: 2018-08-01T23:07:11.847027: step 3964, loss 0.597931.
Train: 2018-08-01T23:07:12.012586: step 3965, loss 0.615555.
Train: 2018-08-01T23:07:12.176174: step 3966, loss 0.650719.
Train: 2018-08-01T23:07:12.333758: step 3967, loss 0.544882.
Train: 2018-08-01T23:07:12.497316: step 3968, loss 0.544916.
Train: 2018-08-01T23:07:12.668832: step 3969, loss 0.667431.
Train: 2018-08-01T23:07:12.845359: step 3970, loss 0.614727.
Test: 2018-08-01T23:07:13.397908: step 3970, loss 0.547877.
Train: 2018-08-01T23:07:13.557456: step 3971, loss 0.440893.
Train: 2018-08-01T23:07:13.723041: step 3972, loss 0.545084.
Train: 2018-08-01T23:07:13.895577: step 3973, loss 0.579704.
Train: 2018-08-01T23:07:14.066096: step 3974, loss 0.614208.
Train: 2018-08-01T23:07:14.235673: step 3975, loss 0.614084.
Train: 2018-08-01T23:07:14.406192: step 3976, loss 0.665464.
Train: 2018-08-01T23:07:14.568752: step 3977, loss 0.476872.
Train: 2018-08-01T23:07:14.749294: step 3978, loss 0.511212.
Train: 2018-08-01T23:07:14.923803: step 3979, loss 0.562404.
Train: 2018-08-01T23:07:15.091386: step 3980, loss 0.511354.
Test: 2018-08-01T23:07:15.631909: step 3980, loss 0.548161.
Train: 2018-08-01T23:07:15.804449: step 3981, loss 0.562407.
Train: 2018-08-01T23:07:15.968011: step 3982, loss 0.596423.
Train: 2018-08-01T23:07:16.138555: step 3983, loss 0.528418.
Train: 2018-08-01T23:07:16.303116: step 3984, loss 0.579403.
Train: 2018-08-01T23:07:16.469670: step 3985, loss 0.56241.
Train: 2018-08-01T23:07:16.645201: step 3986, loss 0.545426.
Train: 2018-08-01T23:07:16.813751: step 3987, loss 0.528439.
Train: 2018-08-01T23:07:16.981304: step 3988, loss 0.477429.
Train: 2018-08-01T23:07:17.138881: step 3989, loss 0.545376.
Train: 2018-08-01T23:07:17.296460: step 3990, loss 0.511214.
Test: 2018-08-01T23:07:17.834035: step 3990, loss 0.548074.
Train: 2018-08-01T23:07:17.995622: step 3991, loss 0.545292.
Train: 2018-08-01T23:07:18.163156: step 3992, loss 0.562399.
Train: 2018-08-01T23:07:18.330722: step 3993, loss 0.579592.
Train: 2018-08-01T23:07:18.505229: step 3994, loss 0.579625.
Train: 2018-08-01T23:07:18.675774: step 3995, loss 0.545153.
Train: 2018-08-01T23:07:18.841355: step 3996, loss 0.562404.
Train: 2018-08-01T23:07:19.013869: step 3997, loss 0.545109.
Train: 2018-08-01T23:07:19.190398: step 3998, loss 0.545088.
Train: 2018-08-01T23:07:19.349997: step 3999, loss 0.70119.
Train: 2018-08-01T23:07:19.516525: step 4000, loss 0.475741.
Test: 2018-08-01T23:07:20.062067: step 4000, loss 0.547889.
Train: 2018-08-01T23:07:20.921890: step 4001, loss 0.579756.
Train: 2018-08-01T23:07:21.097422: step 4002, loss 0.545065.
Train: 2018-08-01T23:07:21.269960: step 4003, loss 0.562414.
Train: 2018-08-01T23:07:21.442499: step 4004, loss 0.527693.
Train: 2018-08-01T23:07:21.616034: step 4005, loss 0.579791.
Train: 2018-08-01T23:07:21.775608: step 4006, loss 0.545037.
Train: 2018-08-01T23:07:21.955129: step 4007, loss 0.527637.
Train: 2018-08-01T23:07:22.136642: step 4008, loss 0.545015.
Train: 2018-08-01T23:07:22.308214: step 4009, loss 0.579854.
Train: 2018-08-01T23:07:22.495716: step 4010, loss 0.597307.
Test: 2018-08-01T23:07:23.048210: step 4010, loss 0.547825.
Train: 2018-08-01T23:07:23.218750: step 4011, loss 0.649628.
Train: 2018-08-01T23:07:23.381316: step 4012, loss 0.475341.
Train: 2018-08-01T23:07:23.550886: step 4013, loss 0.614678.
Train: 2018-08-01T23:07:23.720409: step 4014, loss 0.579826.
Train: 2018-08-01T23:07:23.889983: step 4015, loss 0.545032.
Train: 2018-08-01T23:07:24.077454: step 4016, loss 0.631915.
Train: 2018-08-01T23:07:24.241018: step 4017, loss 0.597102.
Train: 2018-08-01T23:07:24.405577: step 4018, loss 0.545099.
Train: 2018-08-01T23:07:24.570137: step 4019, loss 0.545125.
Train: 2018-08-01T23:07:24.743673: step 4020, loss 0.614176.
Test: 2018-08-01T23:07:25.281237: step 4020, loss 0.547977.
Train: 2018-08-01T23:07:25.454771: step 4021, loss 0.476279.
Train: 2018-08-01T23:07:25.621355: step 4022, loss 0.476318.
Train: 2018-08-01T23:07:25.788904: step 4023, loss 0.527937.
Train: 2018-08-01T23:07:25.972388: step 4024, loss 0.614165.
Train: 2018-08-01T23:07:26.140963: step 4025, loss 0.476099.
Train: 2018-08-01T23:07:26.310485: step 4026, loss 0.683426.
Train: 2018-08-01T23:07:26.488010: step 4027, loss 0.596968.
Train: 2018-08-01T23:07:26.656560: step 4028, loss 0.666.
Train: 2018-08-01T23:07:26.834085: step 4029, loss 0.579625.
Train: 2018-08-01T23:07:27.026570: step 4030, loss 0.510851.
Test: 2018-08-01T23:07:27.582090: step 4030, loss 0.54803.
Train: 2018-08-01T23:07:27.770582: step 4031, loss 0.562398.
Train: 2018-08-01T23:07:27.970049: step 4032, loss 0.528126.
Train: 2018-08-01T23:07:28.145579: step 4033, loss 0.511022.
Train: 2018-08-01T23:07:28.328118: step 4034, loss 0.579528.
Train: 2018-08-01T23:07:28.499664: step 4035, loss 0.613785.
Train: 2018-08-01T23:07:28.665221: step 4036, loss 0.545285.
Train: 2018-08-01T23:07:28.841738: step 4037, loss 0.596612.
Train: 2018-08-01T23:07:29.018247: step 4038, loss 0.596581.
Train: 2018-08-01T23:07:29.190816: step 4039, loss 0.664803.
Train: 2018-08-01T23:07:29.368310: step 4040, loss 0.562406.
Test: 2018-08-01T23:07:29.901884: step 4040, loss 0.548195.
Train: 2018-08-01T23:07:30.070460: step 4041, loss 0.511481.
Train: 2018-08-01T23:07:30.237986: step 4042, loss 0.494604.
Train: 2018-08-01T23:07:30.399554: step 4043, loss 0.443767.
Train: 2018-08-01T23:07:30.564113: step 4044, loss 0.460535.
Train: 2018-08-01T23:07:30.726679: step 4045, loss 0.477237.
Train: 2018-08-01T23:07:30.892263: step 4046, loss 0.613715.
Train: 2018-08-01T23:07:31.057826: step 4047, loss 0.528085.
Train: 2018-08-01T23:07:31.222354: step 4048, loss 0.527976.
Train: 2018-08-01T23:07:31.425374: step 4049, loss 0.510593.
Train: 2018-08-01T23:07:31.599934: step 4050, loss 0.510403.
Test: 2018-08-01T23:07:32.145449: step 4050, loss 0.547845.
Train: 2018-08-01T23:07:32.311007: step 4051, loss 0.527607.
Train: 2018-08-01T23:07:32.482548: step 4052, loss 0.632362.
Train: 2018-08-01T23:07:32.651097: step 4053, loss 0.527396.
Train: 2018-08-01T23:07:32.823668: step 4054, loss 0.544889.
Train: 2018-08-01T23:07:32.996177: step 4055, loss 0.474357.
Train: 2018-08-01T23:07:33.161732: step 4056, loss 0.544818.
Train: 2018-08-01T23:07:33.322304: step 4057, loss 0.580281.
Train: 2018-08-01T23:07:33.485865: step 4058, loss 0.544758.
Train: 2018-08-01T23:07:33.665386: step 4059, loss 0.526892.
Train: 2018-08-01T23:07:33.837951: step 4060, loss 0.49107.
Test: 2018-08-01T23:07:34.371535: step 4060, loss 0.547607.
Train: 2018-08-01T23:07:34.536091: step 4061, loss 0.580583.
Train: 2018-08-01T23:07:34.710606: step 4062, loss 0.544672.
Train: 2018-08-01T23:07:34.885124: step 4063, loss 0.544657.
Train: 2018-08-01T23:07:35.052678: step 4064, loss 0.562719.
Train: 2018-08-01T23:07:35.216240: step 4065, loss 0.562742.
Train: 2018-08-01T23:07:35.380813: step 4066, loss 0.435827.
Train: 2018-08-01T23:07:35.545360: step 4067, loss 0.544616.
Train: 2018-08-01T23:07:35.710918: step 4068, loss 0.63577.
Train: 2018-08-01T23:07:35.876476: step 4069, loss 0.526349.
Train: 2018-08-01T23:07:36.042063: step 4070, loss 0.581155.
Test: 2018-08-01T23:07:36.579608: step 4070, loss 0.547571.
Train: 2018-08-01T23:07:36.753163: step 4071, loss 0.508013.
Train: 2018-08-01T23:07:36.917723: step 4072, loss 0.544594.
Train: 2018-08-01T23:07:37.087238: step 4073, loss 0.617926.
Train: 2018-08-01T23:07:37.251798: step 4074, loss 0.471254.
Train: 2018-08-01T23:07:37.414398: step 4075, loss 0.489529.
Train: 2018-08-01T23:07:37.583932: step 4076, loss 0.452663.
Train: 2018-08-01T23:07:37.748504: step 4077, loss 0.56302.
Train: 2018-08-01T23:07:37.912050: step 4078, loss 0.581538.
Train: 2018-08-01T23:07:38.075597: step 4079, loss 0.526078.
Train: 2018-08-01T23:07:38.242175: step 4080, loss 0.414832.
Test: 2018-08-01T23:07:38.790694: step 4080, loss 0.547612.
Train: 2018-08-01T23:07:38.956275: step 4081, loss 0.656155.
Train: 2018-08-01T23:07:39.124792: step 4082, loss 0.54459.
Train: 2018-08-01T23:07:39.295335: step 4083, loss 0.675086.
Train: 2018-08-01T23:07:39.464914: step 4084, loss 0.563221.
Train: 2018-08-01T23:07:39.639417: step 4085, loss 0.544589.
Train: 2018-08-01T23:07:39.810958: step 4086, loss 0.581786.
Train: 2018-08-01T23:07:39.974521: step 4087, loss 0.52601.
Train: 2018-08-01T23:07:40.142104: step 4088, loss 0.693067.
Train: 2018-08-01T23:07:40.303640: step 4089, loss 0.61862.
Train: 2018-08-01T23:07:40.472198: step 4090, loss 0.655256.
Test: 2018-08-01T23:07:41.022750: step 4090, loss 0.547575.
Train: 2018-08-01T23:07:41.201272: step 4091, loss 0.507867.
Train: 2018-08-01T23:07:41.367795: step 4092, loss 0.562891.
Train: 2018-08-01T23:07:41.539363: step 4093, loss 0.708683.
Train: 2018-08-01T23:07:41.708910: step 4094, loss 0.562763.
Train: 2018-08-01T23:07:41.876435: step 4095, loss 0.652915.
Train: 2018-08-01T23:07:42.041994: step 4096, loss 0.562631.
Train: 2018-08-01T23:07:42.207577: step 4097, loss 0.598251.
Train: 2018-08-01T23:07:42.368147: step 4098, loss 0.527054.
Train: 2018-08-01T23:07:42.530728: step 4099, loss 0.633109.
Train: 2018-08-01T23:07:42.703226: step 4100, loss 0.650257.
Test: 2018-08-01T23:07:43.235802: step 4100, loss 0.54782.
Train: 2018-08-01T23:07:44.048436: step 4101, loss 0.789283.
Train: 2018-08-01T23:07:44.225930: step 4102, loss 0.614287.
Train: 2018-08-01T23:07:44.388526: step 4103, loss 0.613816.
Train: 2018-08-01T23:07:44.552057: step 4104, loss 0.545424.
Train: 2018-08-01T23:07:44.712627: step 4105, loss 0.612996.
Train: 2018-08-01T23:07:44.880210: step 4106, loss 0.646083.
Train: 2018-08-01T23:07:45.053747: step 4107, loss 0.612276.
Train: 2018-08-01T23:07:45.220281: step 4108, loss 0.529701.
Train: 2018-08-01T23:07:45.388821: step 4109, loss 0.529991.
Train: 2018-08-01T23:07:45.553379: step 4110, loss 0.448998.
Test: 2018-08-01T23:07:46.084959: step 4110, loss 0.549187.
Train: 2018-08-01T23:07:46.265510: step 4111, loss 0.595155.
Train: 2018-08-01T23:07:46.434026: step 4112, loss 0.643562.
Train: 2018-08-01T23:07:46.596592: step 4113, loss 0.562838.
Train: 2018-08-01T23:07:46.764173: step 4114, loss 0.498708.
Train: 2018-08-01T23:07:46.928704: step 4115, loss 0.61096.
Train: 2018-08-01T23:07:47.090305: step 4116, loss 0.530953.
Train: 2018-08-01T23:07:47.258821: step 4117, loss 0.69073.
Train: 2018-08-01T23:07:47.423381: step 4118, loss 0.499272.
Train: 2018-08-01T23:07:47.600906: step 4119, loss 0.547094.
Train: 2018-08-01T23:07:47.764470: step 4120, loss 0.578914.
Test: 2018-08-01T23:07:48.318988: step 4120, loss 0.549707.
Train: 2018-08-01T23:07:48.485541: step 4121, loss 0.499436.
Train: 2018-08-01T23:07:48.652117: step 4122, loss 0.531179.
Train: 2018-08-01T23:07:48.820680: step 4123, loss 0.547036.
Train: 2018-08-01T23:07:48.985206: step 4124, loss 0.515026.
Train: 2018-08-01T23:07:49.145777: step 4125, loss 0.450757.
Train: 2018-08-01T23:07:49.312362: step 4126, loss 0.514538.
Train: 2018-08-01T23:07:49.474896: step 4127, loss 0.530393.
Train: 2018-08-01T23:07:49.637463: step 4128, loss 0.578975.
Train: 2018-08-01T23:07:49.798059: step 4129, loss 0.59536.
Train: 2018-08-01T23:07:49.962625: step 4130, loss 0.595453.
Test: 2018-08-01T23:07:50.507166: step 4130, loss 0.548781.
Train: 2018-08-01T23:07:50.670720: step 4131, loss 0.562577.
Train: 2018-08-01T23:07:50.833292: step 4132, loss 0.562552.
Train: 2018-08-01T23:07:50.998857: step 4133, loss 0.545956.
Train: 2018-08-01T23:07:51.159420: step 4134, loss 0.595755.
Train: 2018-08-01T23:07:51.329938: step 4135, loss 0.595814.
Train: 2018-08-01T23:07:51.503473: step 4136, loss 0.662604.
Train: 2018-08-01T23:07:51.663048: step 4137, loss 0.545799.
Train: 2018-08-01T23:07:51.825612: step 4138, loss 0.579172.
Train: 2018-08-01T23:07:51.986184: step 4139, loss 0.629247.
Train: 2018-08-01T23:07:52.153762: step 4140, loss 0.662546.
Test: 2018-08-01T23:07:52.696285: step 4140, loss 0.54857.
Train: 2018-08-01T23:07:52.863871: step 4141, loss 0.57914.
Train: 2018-08-01T23:07:53.033417: step 4142, loss 0.529317.
Train: 2018-08-01T23:07:53.212932: step 4143, loss 0.579105.
Train: 2018-08-01T23:07:53.382477: step 4144, loss 0.545984.
Train: 2018-08-01T23:07:53.547220: step 4145, loss 0.645247.
Train: 2018-08-01T23:07:53.711786: step 4146, loss 0.595576.
Train: 2018-08-01T23:07:53.881342: step 4147, loss 0.463753.
Train: 2018-08-01T23:07:54.053871: step 4148, loss 0.661388.
Train: 2018-08-01T23:07:54.233391: step 4149, loss 0.611917.
Train: 2018-08-01T23:07:54.405924: step 4150, loss 0.46419.
Test: 2018-08-01T23:07:54.939472: step 4150, loss 0.548884.
Train: 2018-08-01T23:07:55.103060: step 4151, loss 0.48061.
Train: 2018-08-01T23:07:55.264629: step 4152, loss 0.546182.
Train: 2018-08-01T23:07:55.428166: step 4153, loss 0.496785.
Train: 2018-08-01T23:07:55.593723: step 4154, loss 0.579063.
Train: 2018-08-01T23:07:55.755291: step 4155, loss 0.496411.
Train: 2018-08-01T23:07:55.913899: step 4156, loss 0.512758.
Train: 2018-08-01T23:07:56.074464: step 4157, loss 0.612454.
Train: 2018-08-01T23:07:56.232017: step 4158, loss 0.612575.
Train: 2018-08-01T23:07:56.394582: step 4159, loss 0.54574.
Train: 2018-08-01T23:07:56.561137: step 4160, loss 0.59598.
Test: 2018-08-01T23:07:57.135699: step 4160, loss 0.548398.
Train: 2018-08-01T23:07:57.300286: step 4161, loss 0.579235.
Train: 2018-08-01T23:07:57.459863: step 4162, loss 0.528848.
Train: 2018-08-01T23:07:57.624394: step 4163, loss 0.629739.
Train: 2018-08-01T23:07:57.787956: step 4164, loss 0.528778.
Train: 2018-08-01T23:07:57.969470: step 4165, loss 0.495052.
Train: 2018-08-01T23:07:58.132063: step 4166, loss 0.629941.
Train: 2018-08-01T23:07:58.310558: step 4167, loss 0.562425.
Train: 2018-08-01T23:07:58.475157: step 4168, loss 0.613132.
Train: 2018-08-01T23:07:58.639710: step 4169, loss 0.562423.
Train: 2018-08-01T23:07:58.805239: step 4170, loss 0.630027.
Test: 2018-08-01T23:07:59.351777: step 4170, loss 0.548288.
Train: 2018-08-01T23:07:59.525311: step 4171, loss 0.528659.
Train: 2018-08-01T23:07:59.700874: step 4172, loss 0.444283.
Train: 2018-08-01T23:07:59.873412: step 4173, loss 0.494803.
Train: 2018-08-01T23:08:00.041931: step 4174, loss 0.630205.
Train: 2018-08-01T23:08:00.204497: step 4175, loss 0.59635.
Train: 2018-08-01T23:08:00.367088: step 4176, loss 0.57939.
Train: 2018-08-01T23:08:00.531647: step 4177, loss 0.511449.
Train: 2018-08-01T23:08:00.698208: step 4178, loss 0.511388.
Train: 2018-08-01T23:08:00.864757: step 4179, loss 0.596478.
Train: 2018-08-01T23:08:01.029292: step 4180, loss 0.511229.
Test: 2018-08-01T23:08:01.570844: step 4180, loss 0.548089.
Train: 2018-08-01T23:08:01.738395: step 4181, loss 0.630755.
Train: 2018-08-01T23:08:01.900986: step 4182, loss 0.545299.
Train: 2018-08-01T23:08:02.068512: step 4183, loss 0.545284.
Train: 2018-08-01T23:08:02.235093: step 4184, loss 0.528134.
Train: 2018-08-01T23:08:02.402620: step 4185, loss 0.596711.
Train: 2018-08-01T23:08:02.581142: step 4186, loss 0.596737.
Train: 2018-08-01T23:08:02.746726: step 4187, loss 0.613917.
Train: 2018-08-01T23:08:02.909265: step 4188, loss 0.510911.
Train: 2018-08-01T23:08:03.080807: step 4189, loss 0.579564.
Train: 2018-08-01T23:08:03.246398: step 4190, loss 0.665387.
Test: 2018-08-01T23:08:03.785923: step 4190, loss 0.548047.
Train: 2018-08-01T23:08:03.956499: step 4191, loss 0.613809.
Train: 2018-08-01T23:08:04.121057: step 4192, loss 0.528202.
Train: 2018-08-01T23:08:04.291570: step 4193, loss 0.545327.
Train: 2018-08-01T23:08:04.460119: step 4194, loss 0.545346.
Train: 2018-08-01T23:08:04.629698: step 4195, loss 0.528316.
Train: 2018-08-01T23:08:04.793229: step 4196, loss 0.698742.
Train: 2018-08-01T23:08:04.954797: step 4197, loss 0.477375.
Train: 2018-08-01T23:08:05.118391: step 4198, loss 0.511418.
Train: 2018-08-01T23:08:05.297880: step 4199, loss 0.477399.
Train: 2018-08-01T23:08:05.470426: step 4200, loss 0.613491.
Test: 2018-08-01T23:08:06.007982: step 4200, loss 0.548133.
Train: 2018-08-01T23:08:06.861990: step 4201, loss 0.647607.
Train: 2018-08-01T23:08:07.022560: step 4202, loss 0.460226.
Train: 2018-08-01T23:08:07.185126: step 4203, loss 0.460123.
Train: 2018-08-01T23:08:07.345727: step 4204, loss 0.579488.
Train: 2018-08-01T23:08:07.511280: step 4205, loss 0.476783.
Train: 2018-08-01T23:08:07.674848: step 4206, loss 0.562398.
Train: 2018-08-01T23:08:07.845362: step 4207, loss 0.579623.
Train: 2018-08-01T23:08:08.013934: step 4208, loss 0.54514.
Train: 2018-08-01T23:08:08.178473: step 4209, loss 0.631606.
Train: 2018-08-01T23:08:08.344028: step 4210, loss 0.579724.
Test: 2018-08-01T23:08:08.890567: step 4210, loss 0.5479.
Train: 2018-08-01T23:08:09.068092: step 4211, loss 0.527756.
Train: 2018-08-01T23:08:09.232680: step 4212, loss 0.562411.
Train: 2018-08-01T23:08:09.400205: step 4213, loss 0.614487.
Train: 2018-08-01T23:08:09.559806: step 4214, loss 0.599441.
Train: 2018-08-01T23:08:09.723341: step 4215, loss 0.510373.
Train: 2018-08-01T23:08:09.892897: step 4216, loss 0.545063.
Train: 2018-08-01T23:08:10.059474: step 4217, loss 0.527702.
Train: 2018-08-01T23:08:10.219047: step 4218, loss 0.527676.
Train: 2018-08-01T23:08:10.380584: step 4219, loss 0.510247.
Train: 2018-08-01T23:08:10.548166: step 4220, loss 0.545003.
Test: 2018-08-01T23:08:11.091715: step 4220, loss 0.547816.
Train: 2018-08-01T23:08:11.258237: step 4221, loss 0.562431.
Train: 2018-08-01T23:08:11.425815: step 4222, loss 0.544956.
Train: 2018-08-01T23:08:11.594339: step 4223, loss 0.544935.
Train: 2018-08-01T23:08:11.758899: step 4224, loss 0.4923.
Train: 2018-08-01T23:08:11.921464: step 4225, loss 0.615208.
Train: 2018-08-01T23:08:12.086025: step 4226, loss 0.45685.
Train: 2018-08-01T23:08:12.264547: step 4227, loss 0.54484.
Train: 2018-08-01T23:08:12.431102: step 4228, loss 0.474034.
Train: 2018-08-01T23:08:12.599653: step 4229, loss 0.562534.
Train: 2018-08-01T23:08:12.769200: step 4230, loss 0.562559.
Test: 2018-08-01T23:08:13.313742: step 4230, loss 0.547632.
Train: 2018-08-01T23:08:13.478302: step 4231, loss 0.633995.
Train: 2018-08-01T23:08:13.649844: step 4232, loss 0.616214.
Train: 2018-08-01T23:08:13.813437: step 4233, loss 0.508967.
Train: 2018-08-01T23:08:13.976969: step 4234, loss 0.473157.
Train: 2018-08-01T23:08:14.147514: step 4235, loss 0.598461.
Train: 2018-08-01T23:08:14.314069: step 4236, loss 0.598505.
Train: 2018-08-01T23:08:14.476660: step 4237, loss 0.526749.
Train: 2018-08-01T23:08:14.642195: step 4238, loss 0.580588.
Train: 2018-08-01T23:08:14.804790: step 4239, loss 0.562638.
Train: 2018-08-01T23:08:14.970346: step 4240, loss 0.580589.
Test: 2018-08-01T23:08:15.512864: step 4240, loss 0.547608.
Train: 2018-08-01T23:08:15.680416: step 4241, loss 0.562633.
Train: 2018-08-01T23:08:15.846004: step 4242, loss 0.472966.
Train: 2018-08-01T23:08:16.015520: step 4243, loss 0.437034.
Train: 2018-08-01T23:08:16.190053: step 4244, loss 0.670541.
Train: 2018-08-01T23:08:16.351621: step 4245, loss 0.526688.
Train: 2018-08-01T23:08:16.524161: step 4246, loss 0.652645.
Train: 2018-08-01T23:08:16.687741: step 4247, loss 0.616595.
Train: 2018-08-01T23:08:16.855291: step 4248, loss 0.562637.
Train: 2018-08-01T23:08:17.014874: step 4249, loss 0.580538.
Train: 2018-08-01T23:08:17.187388: step 4250, loss 0.598369.
Test: 2018-08-01T23:08:17.724957: step 4250, loss 0.547636.
Train: 2018-08-01T23:08:17.886552: step 4251, loss 0.687473.
Train: 2018-08-01T23:08:18.051107: step 4252, loss 0.491458.
Train: 2018-08-01T23:08:18.216662: step 4253, loss 0.597963.
Train: 2018-08-01T23:08:18.383191: step 4254, loss 0.438827.
Train: 2018-08-01T23:08:18.545755: step 4255, loss 0.580137.
Train: 2018-08-01T23:08:18.709319: step 4256, loss 0.597728.
Train: 2018-08-01T23:08:18.884849: step 4257, loss 0.580061.
Train: 2018-08-01T23:08:19.050406: step 4258, loss 0.544902.
Train: 2018-08-01T23:08:19.223942: step 4259, loss 0.615036.
Train: 2018-08-01T23:08:19.392492: step 4260, loss 0.667371.
Test: 2018-08-01T23:08:19.933047: step 4260, loss 0.547834.
Train: 2018-08-01T23:08:20.100600: step 4261, loss 0.562425.
Train: 2018-08-01T23:08:20.270146: step 4262, loss 0.545048.
Train: 2018-08-01T23:08:20.434706: step 4263, loss 0.493139.
Train: 2018-08-01T23:08:20.596306: step 4264, loss 0.579694.
Train: 2018-08-01T23:08:20.755874: step 4265, loss 0.631446.
Train: 2018-08-01T23:08:20.917420: step 4266, loss 0.631266.
Train: 2018-08-01T23:08:21.080012: step 4267, loss 0.52808.
Train: 2018-08-01T23:08:21.249559: step 4268, loss 0.596629.
Train: 2018-08-01T23:08:21.413123: step 4269, loss 0.63067.
Train: 2018-08-01T23:08:21.577682: step 4270, loss 0.511385.
Test: 2018-08-01T23:08:22.131171: step 4270, loss 0.548204.
Train: 2018-08-01T23:08:22.306728: step 4271, loss 0.511512.
Train: 2018-08-01T23:08:22.468299: step 4272, loss 0.494639.
Train: 2018-08-01T23:08:22.628867: step 4273, loss 0.579357.
Train: 2018-08-01T23:08:22.801379: step 4274, loss 0.647101.
Train: 2018-08-01T23:08:22.974915: step 4275, loss 0.528599.
Train: 2018-08-01T23:08:23.137512: step 4276, loss 0.461041.
Train: 2018-08-01T23:08:23.302041: step 4277, loss 0.477858.
Train: 2018-08-01T23:08:23.466628: step 4278, loss 0.511566.
Train: 2018-08-01T23:08:23.628200: step 4279, loss 0.562407.
Train: 2018-08-01T23:08:23.790765: step 4280, loss 0.613523.
Test: 2018-08-01T23:08:24.335287: step 4280, loss 0.548109.
Train: 2018-08-01T23:08:24.503828: step 4281, loss 0.47707.
Train: 2018-08-01T23:08:24.671409: step 4282, loss 0.562398.
Train: 2018-08-01T23:08:24.834974: step 4283, loss 0.510936.
Train: 2018-08-01T23:08:24.997509: step 4284, loss 0.614012.
Train: 2018-08-01T23:08:25.166086: step 4285, loss 0.579636.
Train: 2018-08-01T23:08:25.334607: step 4286, loss 0.614184.
Train: 2018-08-01T23:08:25.499198: step 4287, loss 0.562402.
Train: 2018-08-01T23:08:25.668740: step 4288, loss 0.527851.
Train: 2018-08-01T23:08:25.829316: step 4289, loss 0.545113.
Train: 2018-08-01T23:08:25.989862: step 4290, loss 0.631637.
Test: 2018-08-01T23:08:26.537424: step 4290, loss 0.547915.
Train: 2018-08-01T23:08:26.700988: step 4291, loss 0.493186.
Train: 2018-08-01T23:08:26.878480: step 4292, loss 0.562407.
Train: 2018-08-01T23:08:27.049053: step 4293, loss 0.597074.
Train: 2018-08-01T23:08:27.221589: step 4294, loss 0.579744.
Train: 2018-08-01T23:08:27.391111: step 4295, loss 0.57974.
Train: 2018-08-01T23:08:27.555701: step 4296, loss 0.527762.
Train: 2018-08-01T23:08:27.732225: step 4297, loss 0.562408.
Train: 2018-08-01T23:08:27.898753: step 4298, loss 0.579732.
Train: 2018-08-01T23:08:28.061319: step 4299, loss 0.597046.
Train: 2018-08-01T23:08:28.225879: step 4300, loss 0.493182.
Test: 2018-08-01T23:08:28.756461: step 4300, loss 0.54791.
Train: 2018-08-01T23:08:29.528335: step 4301, loss 0.614339.
Train: 2018-08-01T23:08:29.701877: step 4302, loss 0.579706.
Train: 2018-08-01T23:08:29.867459: step 4303, loss 0.51054.
Train: 2018-08-01T23:08:30.044954: step 4304, loss 0.579692.
Train: 2018-08-01T23:08:30.220516: step 4305, loss 0.545119.
Train: 2018-08-01T23:08:30.382053: step 4306, loss 0.579688.
Train: 2018-08-01T23:08:30.547610: step 4307, loss 0.458722.
Train: 2018-08-01T23:08:30.712178: step 4308, loss 0.5278.
Train: 2018-08-01T23:08:30.876730: step 4309, loss 0.51041.
Train: 2018-08-01T23:08:31.049269: step 4310, loss 0.510269.
Test: 2018-08-01T23:08:31.582843: step 4310, loss 0.547805.
Train: 2018-08-01T23:08:31.747430: step 4311, loss 0.562382.
Train: 2018-08-01T23:08:31.913989: step 4312, loss 0.649916.
Train: 2018-08-01T23:08:32.077520: step 4313, loss 0.545405.
Train: 2018-08-01T23:08:32.243077: step 4314, loss 0.59748.
Train: 2018-08-01T23:08:32.409638: step 4315, loss 0.562706.
Train: 2018-08-01T23:08:32.585189: step 4316, loss 0.509959.
Train: 2018-08-01T23:08:32.752748: step 4317, loss 0.649976.
Train: 2018-08-01T23:08:32.933232: step 4318, loss 0.719895.
Train: 2018-08-01T23:08:33.096796: step 4319, loss 0.632201.
Train: 2018-08-01T23:08:33.266373: step 4320, loss 0.649303.
Test: 2018-08-01T23:08:33.815874: step 4320, loss 0.547923.
Train: 2018-08-01T23:08:33.980433: step 4321, loss 0.579701.
Train: 2018-08-01T23:08:34.155964: step 4322, loss 0.510753.
Train: 2018-08-01T23:08:34.315546: step 4323, loss 0.648189.
Train: 2018-08-01T23:08:34.477105: step 4324, loss 0.545319.
Train: 2018-08-01T23:08:34.647650: step 4325, loss 0.562408.
Train: 2018-08-01T23:08:34.813235: step 4326, loss 0.664207.
Train: 2018-08-01T23:08:34.979762: step 4327, loss 0.579318.
Train: 2018-08-01T23:08:35.148340: step 4328, loss 0.49518.
Train: 2018-08-01T23:08:35.314867: step 4329, loss 0.528919.
Train: 2018-08-01T23:08:35.483444: step 4330, loss 0.56247.
Test: 2018-08-01T23:08:36.030958: step 4330, loss 0.548484.
Train: 2018-08-01T23:08:36.195512: step 4331, loss 0.612618.
Train: 2018-08-01T23:08:36.365074: step 4332, loss 0.645878.
Train: 2018-08-01T23:08:36.537623: step 4333, loss 0.595762.
Train: 2018-08-01T23:08:36.701160: step 4334, loss 0.595676.
Train: 2018-08-01T23:08:36.871705: step 4335, loss 0.496513.
Train: 2018-08-01T23:08:37.033273: step 4336, loss 0.49665.
Train: 2018-08-01T23:08:37.198858: step 4337, loss 0.595535.
Train: 2018-08-01T23:08:37.363424: step 4338, loss 0.463816.
Train: 2018-08-01T23:08:37.530944: step 4339, loss 0.529621.
Train: 2018-08-01T23:08:37.706499: step 4340, loss 0.529553.
Test: 2018-08-01T23:08:38.245034: step 4340, loss 0.548697.
Train: 2018-08-01T23:08:38.416575: step 4341, loss 0.579094.
Train: 2018-08-01T23:08:38.585124: step 4342, loss 0.512816.
Train: 2018-08-01T23:08:38.747720: step 4343, loss 0.662213.
Train: 2018-08-01T23:08:38.912250: step 4344, loss 0.512626.
Train: 2018-08-01T23:08:39.076831: step 4345, loss 0.512534.
Train: 2018-08-01T23:08:39.247385: step 4346, loss 0.562487.
Train: 2018-08-01T23:08:39.420908: step 4347, loss 0.612674.
Train: 2018-08-01T23:08:39.582489: step 4348, loss 0.579222.
Train: 2018-08-01T23:08:39.751039: step 4349, loss 0.612776.
Train: 2018-08-01T23:08:39.923577: step 4350, loss 0.478595.
Test: 2018-08-01T23:08:40.463136: step 4350, loss 0.548387.
Train: 2018-08-01T23:08:40.628719: step 4351, loss 0.596054.
Train: 2018-08-01T23:08:40.797241: step 4352, loss 0.579265.
Train: 2018-08-01T23:08:40.963764: step 4353, loss 0.478317.
Train: 2018-08-01T23:08:41.128354: step 4354, loss 0.562439.
Train: 2018-08-01T23:08:41.312832: step 4355, loss 0.646886.
Train: 2018-08-01T23:08:41.487396: step 4356, loss 0.63002.
Train: 2018-08-01T23:08:41.646970: step 4357, loss 0.511776.
Train: 2018-08-01T23:08:41.813524: step 4358, loss 0.528652.
Train: 2018-08-01T23:08:41.980074: step 4359, loss 0.528622.
Train: 2018-08-01T23:08:42.148624: step 4360, loss 0.460869.
Test: 2018-08-01T23:08:42.703116: step 4360, loss 0.548204.
Train: 2018-08-01T23:08:42.929512: step 4361, loss 0.596366.
Train: 2018-08-01T23:08:43.112024: step 4362, loss 0.579422.
Train: 2018-08-01T23:08:43.338418: step 4363, loss 0.545375.
Train: 2018-08-01T23:08:43.585756: step 4364, loss 0.596538.
Train: 2018-08-01T23:08:43.773255: step 4365, loss 0.596573.
Train: 2018-08-01T23:08:43.944797: step 4366, loss 0.494045.
Train: 2018-08-01T23:08:44.112349: step 4367, loss 0.528175.
Train: 2018-08-01T23:08:44.275910: step 4368, loss 0.579551.
Train: 2018-08-01T23:08:44.436481: step 4369, loss 0.44221.
Train: 2018-08-01T23:08:44.600045: step 4370, loss 0.510736.
Test: 2018-08-01T23:08:45.137137: step 4370, loss 0.547937.
Train: 2018-08-01T23:08:45.302311: step 4371, loss 0.510557.
Train: 2018-08-01T23:08:45.468407: step 4372, loss 0.493009.
Train: 2018-08-01T23:08:45.636924: step 4373, loss 0.614726.
Train: 2018-08-01T23:08:45.808497: step 4374, loss 0.614907.
Train: 2018-08-01T23:08:45.985991: step 4375, loss 0.474841.
Train: 2018-08-01T23:08:46.146561: step 4376, loss 0.527318.
Train: 2018-08-01T23:08:46.311153: step 4377, loss 0.580123.
Train: 2018-08-01T23:08:46.475682: step 4378, loss 0.615536.
Train: 2018-08-01T23:08:46.639244: step 4379, loss 0.474013.
Train: 2018-08-01T23:08:46.804833: step 4380, loss 0.580279.
Test: 2018-08-01T23:08:47.341367: step 4380, loss 0.547665.
Train: 2018-08-01T23:08:47.505928: step 4381, loss 0.491449.
Train: 2018-08-01T23:08:47.667496: step 4382, loss 0.526932.
Train: 2018-08-01T23:08:47.830087: step 4383, loss 0.687662.
Train: 2018-08-01T23:08:47.996616: step 4384, loss 0.5626.
Train: 2018-08-01T23:08:48.169185: step 4385, loss 0.562603.
Train: 2018-08-01T23:08:48.346682: step 4386, loss 0.598367.
Train: 2018-08-01T23:08:48.505257: step 4387, loss 0.544727.
Train: 2018-08-01T23:08:48.671859: step 4388, loss 0.509.
Train: 2018-08-01T23:08:48.856318: step 4389, loss 0.544727.
Train: 2018-08-01T23:08:49.021876: step 4390, loss 0.508971.
Test: 2018-08-01T23:08:49.569427: step 4390, loss 0.547626.
Train: 2018-08-01T23:08:49.735997: step 4391, loss 0.634185.
Train: 2018-08-01T23:08:49.899529: step 4392, loss 0.544719.
Train: 2018-08-01T23:08:50.070073: step 4393, loss 0.562606.
Train: 2018-08-01T23:08:50.242621: step 4394, loss 0.598364.
Train: 2018-08-01T23:08:50.408169: step 4395, loss 0.580459.
Train: 2018-08-01T23:08:50.574725: step 4396, loss 0.669643.
Train: 2018-08-01T23:08:50.738287: step 4397, loss 0.509174.
Train: 2018-08-01T23:08:50.899855: step 4398, loss 0.598068.
Train: 2018-08-01T23:08:51.064447: step 4399, loss 0.52708.
Train: 2018-08-01T23:08:51.236955: step 4400, loss 0.562513.
Test: 2018-08-01T23:08:51.785487: step 4400, loss 0.547709.
Train: 2018-08-01T23:08:52.545518: step 4401, loss 0.63316.
Train: 2018-08-01T23:08:52.718088: step 4402, loss 0.597724.
Train: 2018-08-01T23:08:52.893588: step 4403, loss 0.509762.
Train: 2018-08-01T23:08:53.065147: step 4404, loss 0.562457.
Train: 2018-08-01T23:08:53.224703: step 4405, loss 0.579951.
Train: 2018-08-01T23:08:53.381283: step 4406, loss 0.544971.
Train: 2018-08-01T23:08:53.549860: step 4407, loss 0.597314.
Train: 2018-08-01T23:08:53.713395: step 4408, loss 0.562426.
Train: 2018-08-01T23:08:53.878953: step 4409, loss 0.458181.
Train: 2018-08-01T23:08:54.048501: step 4410, loss 0.492937.
Test: 2018-08-01T23:08:54.593058: step 4410, loss 0.547864.
Train: 2018-08-01T23:08:54.760622: step 4411, loss 0.545036.
Train: 2018-08-01T23:08:54.925157: step 4412, loss 0.492814.
Train: 2018-08-01T23:08:55.087753: step 4413, loss 0.597303.
Train: 2018-08-01T23:08:55.254277: step 4414, loss 0.527526.
Train: 2018-08-01T23:08:55.417871: step 4415, loss 0.440077.
Train: 2018-08-01T23:08:55.590398: step 4416, loss 0.509854.
Train: 2018-08-01T23:08:55.751947: step 4417, loss 0.58007.
Train: 2018-08-01T23:08:55.915542: step 4418, loss 0.597778.
Train: 2018-08-01T23:08:56.081066: step 4419, loss 0.527153.
Train: 2018-08-01T23:08:56.256631: step 4420, loss 0.473956.
Test: 2018-08-01T23:08:56.792166: step 4420, loss 0.547666.
Train: 2018-08-01T23:08:56.970725: step 4421, loss 0.509244.
Train: 2018-08-01T23:08:57.132263: step 4422, loss 0.562573.
Train: 2018-08-01T23:08:57.291829: step 4423, loss 0.651991.
Train: 2018-08-01T23:08:57.453426: step 4424, loss 0.580511.
Train: 2018-08-01T23:08:57.616961: step 4425, loss 0.58053.
Train: 2018-08-01T23:08:57.780523: step 4426, loss 0.562621.
Train: 2018-08-01T23:08:57.951099: step 4427, loss 0.562621.
Train: 2018-08-01T23:08:58.112637: step 4428, loss 0.598449.
Train: 2018-08-01T23:08:58.285205: step 4429, loss 0.491008.
Train: 2018-08-01T23:08:58.447773: step 4430, loss 0.562615.
Test: 2018-08-01T23:08:59.003256: step 4430, loss 0.547621.
Train: 2018-08-01T23:08:59.176791: step 4431, loss 0.437278.
Train: 2018-08-01T23:08:59.346339: step 4432, loss 0.562632.
Train: 2018-08-01T23:08:59.515885: step 4433, loss 0.598564.
Train: 2018-08-01T23:08:59.691446: step 4434, loss 0.670465.
Train: 2018-08-01T23:08:59.862957: step 4435, loss 0.526744.
Train: 2018-08-01T23:09:00.027518: step 4436, loss 0.472957.
Train: 2018-08-01T23:09:00.191080: step 4437, loss 0.508806.
Train: 2018-08-01T23:09:00.355664: step 4438, loss 0.526724.
Train: 2018-08-01T23:09:00.517236: step 4439, loss 0.598633.
Train: 2018-08-01T23:09:00.680770: step 4440, loss 0.544674.
Test: 2018-08-01T23:09:01.215353: step 4440, loss 0.547598.
Train: 2018-08-01T23:09:01.381896: step 4441, loss 0.526667.
Train: 2018-08-01T23:09:01.542492: step 4442, loss 0.562683.
Train: 2018-08-01T23:09:01.710050: step 4443, loss 0.544661.
Train: 2018-08-01T23:09:01.877602: step 4444, loss 0.562697.
Train: 2018-08-01T23:09:02.047119: step 4445, loss 0.490516.
Train: 2018-08-01T23:09:02.216665: step 4446, loss 0.526581.
Train: 2018-08-01T23:09:02.377261: step 4447, loss 0.562734.
Train: 2018-08-01T23:09:02.539800: step 4448, loss 0.67142.
Train: 2018-08-01T23:09:02.702397: step 4449, loss 0.598936.
Train: 2018-08-01T23:09:02.862970: step 4450, loss 0.707318.
Test: 2018-08-01T23:09:03.397508: step 4450, loss 0.547596.
Train: 2018-08-01T23:09:03.565085: step 4451, loss 0.490626.
Train: 2018-08-01T23:09:03.733645: step 4452, loss 0.544681.
Train: 2018-08-01T23:09:03.894212: step 4453, loss 0.580573.
Train: 2018-08-01T23:09:04.060766: step 4454, loss 0.598415.
Train: 2018-08-01T23:09:04.224326: step 4455, loss 0.598298.
Train: 2018-08-01T23:09:04.391879: step 4456, loss 0.544757.
Train: 2018-08-01T23:09:04.554416: step 4457, loss 0.473749.
Train: 2018-08-01T23:09:04.723962: step 4458, loss 0.527053.
Train: 2018-08-01T23:09:04.890541: step 4459, loss 0.668885.
Train: 2018-08-01T23:09:05.056074: step 4460, loss 0.54482.
Test: 2018-08-01T23:09:05.595632: step 4460, loss 0.547711.
Train: 2018-08-01T23:09:05.760191: step 4461, loss 0.597806.
Train: 2018-08-01T23:09:05.924753: step 4462, loss 0.474403.
Train: 2018-08-01T23:09:06.087349: step 4463, loss 0.580076.
Train: 2018-08-01T23:09:06.251903: step 4464, loss 0.544888.
Train: 2018-08-01T23:09:06.415442: step 4465, loss 0.474631.
Train: 2018-08-01T23:09:06.589010: step 4466, loss 0.68549.
Train: 2018-08-01T23:09:06.761543: step 4467, loss 0.580008.
Train: 2018-08-01T23:09:06.927117: step 4468, loss 0.56245.
Train: 2018-08-01T23:09:07.095653: step 4469, loss 0.54495.
Train: 2018-08-01T23:09:07.267192: step 4470, loss 0.632322.
Test: 2018-08-01T23:09:07.811742: step 4470, loss 0.547831.
Train: 2018-08-01T23:09:07.978287: step 4471, loss 0.614727.
Train: 2018-08-01T23:09:08.143851: step 4472, loss 0.649337.
Train: 2018-08-01T23:09:08.310407: step 4473, loss 0.545093.
Train: 2018-08-01T23:09:08.469949: step 4474, loss 0.61418.
Train: 2018-08-01T23:09:08.631549: step 4475, loss 0.5624.
Train: 2018-08-01T23:09:08.805053: step 4476, loss 0.579535.
Train: 2018-08-01T23:09:08.969645: step 4477, loss 0.579479.
Train: 2018-08-01T23:09:09.135170: step 4478, loss 0.562406.
Train: 2018-08-01T23:09:09.296737: step 4479, loss 0.596356.
Train: 2018-08-01T23:09:09.466310: step 4480, loss 0.511667.
Test: 2018-08-01T23:09:10.016813: step 4480, loss 0.548289.
Train: 2018-08-01T23:09:10.180394: step 4481, loss 0.545543.
Train: 2018-08-01T23:09:10.346930: step 4482, loss 0.613012.
Train: 2018-08-01T23:09:10.509496: step 4483, loss 0.528792.
Train: 2018-08-01T23:09:10.671093: step 4484, loss 0.612858.
Train: 2018-08-01T23:09:10.834657: step 4485, loss 0.579229.
Train: 2018-08-01T23:09:10.999218: step 4486, loss 0.545725.
Train: 2018-08-01T23:09:11.166764: step 4487, loss 0.529037.
Train: 2018-08-01T23:09:11.329304: step 4488, loss 0.66273.
Train: 2018-08-01T23:09:11.489876: step 4489, loss 0.52914.
Train: 2018-08-01T23:09:11.654466: step 4490, loss 0.579151.
Test: 2018-08-01T23:09:12.195014: step 4490, loss 0.548576.
Train: 2018-08-01T23:09:12.365567: step 4491, loss 0.579139.
Train: 2018-08-01T23:09:12.529122: step 4492, loss 0.579126.
Train: 2018-08-01T23:09:12.696674: step 4493, loss 0.562523.
Train: 2018-08-01T23:09:12.858248: step 4494, loss 0.562531.
Train: 2018-08-01T23:09:13.020782: step 4495, loss 0.711566.
Train: 2018-08-01T23:09:13.184346: step 4496, loss 0.579069.
Train: 2018-08-01T23:09:13.342952: step 4497, loss 0.644882.
Train: 2018-08-01T23:09:13.505488: step 4498, loss 0.562624.
Train: 2018-08-01T23:09:13.671045: step 4499, loss 0.611678.
Train: 2018-08-01T23:09:13.834635: step 4500, loss 0.627817.
Test: 2018-08-01T23:09:14.383141: step 4500, loss 0.549172.
Train: 2018-08-01T23:09:15.219774: step 4501, loss 0.497904.
Train: 2018-08-01T23:09:15.391315: step 4502, loss 0.498098.
Train: 2018-08-01T23:09:15.556900: step 4503, loss 0.482014.
Train: 2018-08-01T23:09:15.719469: step 4504, loss 0.611282.
Train: 2018-08-01T23:09:15.883032: step 4505, loss 0.611285.
Train: 2018-08-01T23:09:16.055539: step 4506, loss 0.562784.
Train: 2018-08-01T23:09:16.225112: step 4507, loss 0.5951.
Train: 2018-08-01T23:09:16.388682: step 4508, loss 0.530502.
Train: 2018-08-01T23:09:16.559194: step 4509, loss 0.578942.
Train: 2018-08-01T23:09:16.727743: step 4510, loss 0.61125.
Test: 2018-08-01T23:09:17.258332: step 4510, loss 0.549276.
Train: 2018-08-01T23:09:17.426898: step 4511, loss 0.65967.
Train: 2018-08-01T23:09:17.594425: step 4512, loss 0.450014.
Train: 2018-08-01T23:09:17.771951: step 4513, loss 0.562814.
Train: 2018-08-01T23:09:17.932527: step 4514, loss 0.562805.
Train: 2018-08-01T23:09:18.094126: step 4515, loss 0.528346.
Train: 2018-08-01T23:09:18.264640: step 4516, loss 0.514255.
Train: 2018-08-01T23:09:18.433189: step 4517, loss 0.578957.
Train: 2018-08-01T23:09:18.595754: step 4518, loss 0.62771.
Train: 2018-08-01T23:09:18.773280: step 4519, loss 0.497651.
Train: 2018-08-01T23:09:18.930858: step 4520, loss 0.546383.
Test: 2018-08-01T23:09:19.459445: step 4520, loss 0.548974.
Train: 2018-08-01T23:09:19.624005: step 4521, loss 0.70972.
Train: 2018-08-01T23:09:19.794549: step 4522, loss 0.611673.
Train: 2018-08-01T23:09:19.960106: step 4523, loss 0.513688.
Train: 2018-08-01T23:09:20.121676: step 4524, loss 0.481007.
Train: 2018-08-01T23:09:20.308209: step 4525, loss 0.52992.
Train: 2018-08-01T23:09:20.471801: step 4526, loss 0.628223.
Train: 2018-08-01T23:09:20.635348: step 4527, loss 0.579029.
Train: 2018-08-01T23:09:20.803884: step 4528, loss 0.546163.
Train: 2018-08-01T23:09:20.970439: step 4529, loss 0.546129.
Train: 2018-08-01T23:09:21.143004: step 4530, loss 0.529605.
Test: 2018-08-01T23:09:21.696497: step 4530, loss 0.548722.
Train: 2018-08-01T23:09:21.869062: step 4531, loss 0.529517.
Train: 2018-08-01T23:09:22.031655: step 4532, loss 0.52941.
Train: 2018-08-01T23:09:22.205139: step 4533, loss 0.545901.
Train: 2018-08-01T23:09:22.371694: step 4534, loss 0.612483.
Train: 2018-08-01T23:09:22.537251: step 4535, loss 0.47899.
Train: 2018-08-01T23:09:22.708791: step 4536, loss 0.579214.
Train: 2018-08-01T23:09:22.869364: step 4537, loss 0.562447.
Train: 2018-08-01T23:09:23.045890: step 4538, loss 0.596122.
Train: 2018-08-01T23:09:23.213476: step 4539, loss 0.461185.
Train: 2018-08-01T23:09:23.378029: step 4540, loss 0.663998.
Test: 2018-08-01T23:09:23.922546: step 4540, loss 0.548217.
Train: 2018-08-01T23:09:24.091097: step 4541, loss 0.528506.
Train: 2018-08-01T23:09:24.259673: step 4542, loss 0.545424.
Train: 2018-08-01T23:09:24.424206: step 4543, loss 0.49433.
Train: 2018-08-01T23:09:24.588794: step 4544, loss 0.545334.
Train: 2018-08-01T23:09:24.757315: step 4545, loss 0.579515.
Train: 2018-08-01T23:09:24.928858: step 4546, loss 0.579554.
Train: 2018-08-01T23:09:25.093417: step 4547, loss 0.562398.
Train: 2018-08-01T23:09:25.259972: step 4548, loss 0.562399.
Train: 2018-08-01T23:09:25.427524: step 4549, loss 0.631358.
Train: 2018-08-01T23:09:25.601059: step 4550, loss 0.493428.
Test: 2018-08-01T23:09:26.140617: step 4550, loss 0.547945.
Train: 2018-08-01T23:09:26.311162: step 4551, loss 0.562402.
Train: 2018-08-01T23:09:26.474729: step 4552, loss 0.596971.
Train: 2018-08-01T23:09:26.635326: step 4553, loss 0.579696.
Train: 2018-08-01T23:09:26.796905: step 4554, loss 0.700744.
Train: 2018-08-01T23:09:26.972395: step 4555, loss 0.45886.
Train: 2018-08-01T23:09:27.144968: step 4556, loss 0.562401.
Train: 2018-08-01T23:09:27.313482: step 4557, loss 0.527906.
Train: 2018-08-01T23:09:27.472059: step 4558, loss 0.665912.
Train: 2018-08-01T23:09:27.637642: step 4559, loss 0.527943.
Train: 2018-08-01T23:09:27.800182: step 4560, loss 0.510751.
Test: 2018-08-01T23:09:28.341734: step 4560, loss 0.547981.
Train: 2018-08-01T23:09:28.502305: step 4561, loss 0.510744.
Train: 2018-08-01T23:09:28.671852: step 4562, loss 0.476231.
Train: 2018-08-01T23:09:28.834442: step 4563, loss 0.510592.
Train: 2018-08-01T23:09:29.005958: step 4564, loss 0.545091.
Train: 2018-08-01T23:09:29.177499: step 4565, loss 0.579775.
Train: 2018-08-01T23:09:29.346049: step 4566, loss 0.701598.
Train: 2018-08-01T23:09:29.507617: step 4567, loss 0.614595.
Train: 2018-08-01T23:09:29.671180: step 4568, loss 0.510296.
Train: 2018-08-01T23:09:29.827786: step 4569, loss 0.510309.
Train: 2018-08-01T23:09:29.990327: step 4570, loss 0.527661.
Test: 2018-08-01T23:09:30.524906: step 4570, loss 0.547854.
Train: 2018-08-01T23:09:30.696464: step 4571, loss 0.597208.
Train: 2018-08-01T23:09:30.863023: step 4572, loss 0.510223.
Train: 2018-08-01T23:09:31.030546: step 4573, loss 0.614672.
Train: 2018-08-01T23:09:31.194134: step 4574, loss 0.475337.
Train: 2018-08-01T23:09:31.356674: step 4575, loss 0.510109.
Train: 2018-08-01T23:09:31.523229: step 4576, loss 0.614855.
Train: 2018-08-01T23:09:31.684839: step 4577, loss 0.632391.
Train: 2018-08-01T23:09:31.850355: step 4578, loss 0.475028.
Train: 2018-08-01T23:09:32.009927: step 4579, loss 0.562442.
Train: 2018-08-01T23:09:32.185484: step 4580, loss 0.562446.
Test: 2018-08-01T23:09:32.733021: step 4580, loss 0.547775.
Train: 2018-08-01T23:09:32.914509: step 4581, loss 0.422266.
Train: 2018-08-01T23:09:33.086064: step 4582, loss 0.562462.
Train: 2018-08-01T23:09:33.252084: step 4583, loss 0.597688.
Train: 2018-08-01T23:09:33.425620: step 4584, loss 0.491959.
Train: 2018-08-01T23:09:33.587188: step 4585, loss 0.527159.
Train: 2018-08-01T23:09:33.753743: step 4586, loss 0.651074.
Train: 2018-08-01T23:09:33.922294: step 4587, loss 0.527071.
Train: 2018-08-01T23:09:34.090842: step 4588, loss 0.722223.
Train: 2018-08-01T23:09:34.256433: step 4589, loss 0.651108.
Train: 2018-08-01T23:09:34.417001: step 4590, loss 0.597839.
Test: 2018-08-01T23:09:34.964507: step 4590, loss 0.547726.
Train: 2018-08-01T23:09:35.126102: step 4591, loss 0.597711.
Train: 2018-08-01T23:09:35.298639: step 4592, loss 0.457114.
Train: 2018-08-01T23:09:35.473146: step 4593, loss 0.52739.
Train: 2018-08-01T23:09:35.640725: step 4594, loss 0.562446.
Train: 2018-08-01T23:09:35.808275: step 4595, loss 0.492454.
Train: 2018-08-01T23:09:35.972811: step 4596, loss 0.614939.
Train: 2018-08-01T23:09:36.139375: step 4597, loss 0.579925.
Train: 2018-08-01T23:09:36.320880: step 4598, loss 0.457621.
Train: 2018-08-01T23:09:36.488432: step 4599, loss 0.544958.
Train: 2018-08-01T23:09:36.660971: step 4600, loss 0.614918.
Test: 2018-08-01T23:09:37.195543: step 4600, loss 0.547794.
Train: 2018-08-01T23:09:37.982735: step 4601, loss 0.579929.
Train: 2018-08-01T23:09:38.148323: step 4602, loss 0.579918.
Train: 2018-08-01T23:09:38.311855: step 4603, loss 0.527499.
Train: 2018-08-01T23:09:38.475417: step 4604, loss 0.527506.
Train: 2018-08-01T23:09:38.646983: step 4605, loss 0.544966.
Train: 2018-08-01T23:09:38.820520: step 4606, loss 0.562436.
Train: 2018-08-01T23:09:38.994030: step 4607, loss 0.509994.
Train: 2018-08-01T23:09:39.167593: step 4608, loss 0.422453.
Train: 2018-08-01T23:09:39.339133: step 4609, loss 0.509808.
Train: 2018-08-01T23:09:39.500702: step 4610, loss 0.52726.
Test: 2018-08-01T23:09:40.029264: step 4610, loss 0.547703.
Train: 2018-08-01T23:09:40.191855: step 4611, loss 0.456495.
Train: 2018-08-01T23:09:40.351402: step 4612, loss 0.56253.
Train: 2018-08-01T23:09:40.510975: step 4613, loss 0.526932.
Train: 2018-08-01T23:09:40.673572: step 4614, loss 0.544715.
Train: 2018-08-01T23:09:40.847110: step 4615, loss 0.562636.
Train: 2018-08-01T23:09:41.021641: step 4616, loss 0.544667.
Train: 2018-08-01T23:09:41.183178: step 4617, loss 0.562704.
Train: 2018-08-01T23:09:41.352729: step 4618, loss 0.562734.
Train: 2018-08-01T23:09:41.521300: step 4619, loss 0.544626.
Train: 2018-08-01T23:09:41.688858: step 4620, loss 0.726286.
Test: 2018-08-01T23:09:42.215444: step 4620, loss 0.547574.
Train: 2018-08-01T23:09:42.378982: step 4621, loss 0.508317.
Train: 2018-08-01T23:09:42.541548: step 4622, loss 0.744266.
Train: 2018-08-01T23:09:42.715109: step 4623, loss 0.598931.
Train: 2018-08-01T23:09:42.884657: step 4624, loss 0.580734.
Train: 2018-08-01T23:09:43.052182: step 4625, loss 0.526695.
Train: 2018-08-01T23:09:43.213770: step 4626, loss 0.580559.
Train: 2018-08-01T23:09:43.381302: step 4627, loss 0.562598.
Train: 2018-08-01T23:09:43.555867: step 4628, loss 0.509068.
Train: 2018-08-01T23:09:43.718402: step 4629, loss 0.562556.
Train: 2018-08-01T23:09:43.881991: step 4630, loss 0.63363.
Test: 2018-08-01T23:09:44.437479: step 4630, loss 0.547678.
Train: 2018-08-01T23:09:44.603036: step 4631, loss 0.597969.
Train: 2018-08-01T23:09:44.769225: step 4632, loss 0.562498.
Train: 2018-08-01T23:09:44.937174: step 4633, loss 0.47438.
Train: 2018-08-01T23:09:45.103236: step 4634, loss 0.580066.
Train: 2018-08-01T23:09:45.270819: step 4635, loss 0.544894.
Train: 2018-08-01T23:09:45.435380: step 4636, loss 0.562455.
Train: 2018-08-01T23:09:45.598936: step 4637, loss 0.579972.
Train: 2018-08-01T23:09:45.767461: step 4638, loss 0.492446.
Train: 2018-08-01T23:09:45.931056: step 4639, loss 0.527453.
Train: 2018-08-01T23:09:46.101567: step 4640, loss 0.527448.
Test: 2018-08-01T23:09:46.639156: step 4640, loss 0.547784.
Train: 2018-08-01T23:09:46.803719: step 4641, loss 0.457402.
Train: 2018-08-01T23:09:46.967279: step 4642, loss 0.492284.
Train: 2018-08-01T23:09:47.127824: step 4643, loss 0.456922.
Train: 2018-08-01T23:09:47.288394: step 4644, loss 0.597815.
Train: 2018-08-01T23:09:47.449969: step 4645, loss 0.52709.
Train: 2018-08-01T23:09:47.628486: step 4646, loss 0.633602.
Train: 2018-08-01T23:09:47.791082: step 4647, loss 0.491375.
Train: 2018-08-01T23:09:47.958628: step 4648, loss 0.616078.
Train: 2018-08-01T23:09:48.121169: step 4649, loss 0.598297.
Train: 2018-08-01T23:09:48.285761: step 4650, loss 0.562588.
Test: 2018-08-01T23:09:48.834312: step 4650, loss 0.547627.
Train: 2018-08-01T23:09:48.993836: step 4651, loss 0.580459.
Train: 2018-08-01T23:09:49.156427: step 4652, loss 0.580454.
Train: 2018-08-01T23:09:49.316996: step 4653, loss 0.419737.
Train: 2018-08-01T23:09:49.493531: step 4654, loss 0.544716.
Train: 2018-08-01T23:09:49.657062: step 4655, loss 0.473083.
Train: 2018-08-01T23:09:49.819629: step 4656, loss 0.544687.
Train: 2018-08-01T23:09:49.983225: step 4657, loss 0.472722.
Train: 2018-08-01T23:09:50.146756: step 4658, loss 0.562696.
Train: 2018-08-01T23:09:50.309319: step 4659, loss 0.526548.
Train: 2018-08-01T23:09:50.471915: step 4660, loss 0.453933.
Test: 2018-08-01T23:09:51.016428: step 4660, loss 0.54757.
Train: 2018-08-01T23:09:51.180017: step 4661, loss 0.508198.
Train: 2018-08-01T23:09:51.342557: step 4662, loss 0.599425.
Train: 2018-08-01T23:09:51.503153: step 4663, loss 0.526263.
Train: 2018-08-01T23:09:51.677687: step 4664, loss 0.636471.
Train: 2018-08-01T23:09:51.841259: step 4665, loss 0.470981.
Train: 2018-08-01T23:09:52.005785: step 4666, loss 0.61834.
Train: 2018-08-01T23:09:52.166356: step 4667, loss 0.563038.
Train: 2018-08-01T23:09:52.323934: step 4668, loss 0.618454.
Train: 2018-08-01T23:09:52.483508: step 4669, loss 0.599962.
Train: 2018-08-01T23:09:52.646073: step 4670, loss 0.636779.
Test: 2018-08-01T23:09:53.191615: step 4670, loss 0.547575.
Train: 2018-08-01T23:09:53.360164: step 4671, loss 0.526184.
Train: 2018-08-01T23:09:53.526751: step 4672, loss 0.581318.
Train: 2018-08-01T23:09:53.688311: step 4673, loss 0.562918.
Train: 2018-08-01T23:09:53.851883: step 4674, loss 0.690922.
Train: 2018-08-01T23:09:54.027380: step 4675, loss 0.562827.
Train: 2018-08-01T23:09:54.191968: step 4676, loss 0.653551.
Train: 2018-08-01T23:09:54.357499: step 4677, loss 0.598852.
Train: 2018-08-01T23:09:54.518094: step 4678, loss 0.490734.
Train: 2018-08-01T23:09:54.679667: step 4679, loss 0.634276.
Train: 2018-08-01T23:09:54.855167: step 4680, loss 0.633916.
Test: 2018-08-01T23:09:55.395283: step 4680, loss 0.54767.
Train: 2018-08-01T23:09:55.559886: step 4681, loss 0.562528.
Train: 2018-08-01T23:09:55.720447: step 4682, loss 0.544834.
Train: 2018-08-01T23:09:55.885011: step 4683, loss 0.492123.
Train: 2018-08-01T23:09:56.049541: step 4684, loss 0.702745.
Train: 2018-08-01T23:09:56.214118: step 4685, loss 0.579885.
Train: 2018-08-01T23:09:56.376690: step 4686, loss 0.492912.
Train: 2018-08-01T23:09:56.546206: step 4687, loss 0.631694.
Train: 2018-08-01T23:09:56.713783: step 4688, loss 0.683182.
Train: 2018-08-01T23:09:56.917344: step 4689, loss 0.442261.
Train: 2018-08-01T23:09:57.082900: step 4690, loss 0.630841.
Test: 2018-08-01T23:09:57.611486: step 4690, loss 0.548126.
Train: 2018-08-01T23:09:57.774078: step 4691, loss 0.528308.
Train: 2018-08-01T23:09:57.941634: step 4692, loss 0.647398.
Train: 2018-08-01T23:09:58.106165: step 4693, loss 0.494684.
Train: 2018-08-01T23:09:58.272750: step 4694, loss 0.494852.
Train: 2018-08-01T23:09:58.434288: step 4695, loss 0.376795.
Train: 2018-08-01T23:09:58.600873: step 4696, loss 0.57933.
Train: 2018-08-01T23:09:58.766399: step 4697, loss 0.596291.
Train: 2018-08-01T23:09:58.930958: step 4698, loss 0.562412.
Train: 2018-08-01T23:09:59.094553: step 4699, loss 0.56241.
Train: 2018-08-01T23:09:59.258085: step 4700, loss 0.613364.
Test: 2018-08-01T23:09:59.800635: step 4700, loss 0.548184.
Train: 2018-08-01T23:10:00.532372: step 4701, loss 0.562408.
Train: 2018-08-01T23:10:00.697950: step 4702, loss 0.630356.
Train: 2018-08-01T23:10:00.862503: step 4703, loss 0.613322.
Train: 2018-08-01T23:10:01.029061: step 4704, loss 0.5963.
Train: 2018-08-01T23:10:01.197603: step 4705, loss 0.47787.
Train: 2018-08-01T23:10:01.363165: step 4706, loss 0.562421.
Train: 2018-08-01T23:10:01.530730: step 4707, loss 0.562422.
Train: 2018-08-01T23:10:01.703249: step 4708, loss 0.545528.
Train: 2018-08-01T23:10:01.869810: step 4709, loss 0.461039.
Train: 2018-08-01T23:10:02.036334: step 4710, loss 0.579344.
Test: 2018-08-01T23:10:02.576896: step 4710, loss 0.548219.
Train: 2018-08-01T23:10:02.736497: step 4711, loss 0.596313.
Train: 2018-08-01T23:10:02.900026: step 4712, loss 0.613299.
Train: 2018-08-01T23:10:03.080568: step 4713, loss 0.392799.
Train: 2018-08-01T23:10:03.249118: step 4714, loss 0.460374.
Train: 2018-08-01T23:10:03.414649: step 4715, loss 0.647761.
Train: 2018-08-01T23:10:03.586223: step 4716, loss 0.459734.
Train: 2018-08-01T23:10:03.746761: step 4717, loss 0.545225.
Train: 2018-08-01T23:10:03.909327: step 4718, loss 0.700249.
Train: 2018-08-01T23:10:04.078874: step 4719, loss 0.648644.
Train: 2018-08-01T23:10:04.234458: step 4720, loss 0.614126.
Test: 2018-08-01T23:10:04.777037: step 4720, loss 0.547976.
Train: 2018-08-01T23:10:04.948582: step 4721, loss 0.631288.
Train: 2018-08-01T23:10:05.120108: step 4722, loss 0.476463.
Train: 2018-08-01T23:10:05.285679: step 4723, loss 0.665457.
Train: 2018-08-01T23:10:05.454232: step 4724, loss 0.579537.
Train: 2018-08-01T23:10:05.625770: step 4725, loss 0.596606.
Train: 2018-08-01T23:10:05.795310: step 4726, loss 0.545337.
Train: 2018-08-01T23:10:05.966852: step 4727, loss 0.511313.
Train: 2018-08-01T23:10:06.136404: step 4728, loss 0.562404.
Train: 2018-08-01T23:10:06.306917: step 4729, loss 0.579406.
Train: 2018-08-01T23:10:06.468486: step 4730, loss 0.613361.
Test: 2018-08-01T23:10:07.017028: step 4730, loss 0.548212.
Train: 2018-08-01T23:10:07.265726: step 4731, loss 0.63024.
Train: 2018-08-01T23:10:07.431254: step 4732, loss 0.511673.
Train: 2018-08-01T23:10:07.597808: step 4733, loss 0.494856.
Train: 2018-08-01T23:10:07.759375: step 4734, loss 0.596203.
Train: 2018-08-01T23:10:07.925957: step 4735, loss 0.646823.
Train: 2018-08-01T23:10:08.093483: step 4736, loss 0.545584.
Train: 2018-08-01T23:10:08.258078: step 4737, loss 0.579265.
Train: 2018-08-01T23:10:08.415622: step 4738, loss 0.612857.
Train: 2018-08-01T23:10:08.581205: step 4739, loss 0.545682.
Train: 2018-08-01T23:10:08.748762: step 4740, loss 0.696441.
Test: 2018-08-01T23:10:09.284335: step 4740, loss 0.548502.
Train: 2018-08-01T23:10:09.451877: step 4741, loss 0.662631.
Train: 2018-08-01T23:10:09.614418: step 4742, loss 0.562509.
Train: 2018-08-01T23:10:09.780997: step 4743, loss 0.562538.
Train: 2018-08-01T23:10:09.950550: step 4744, loss 0.513085.
Train: 2018-08-01T23:10:10.112086: step 4745, loss 0.546127.
Train: 2018-08-01T23:10:10.285648: step 4746, loss 0.595465.
Train: 2018-08-01T23:10:10.451181: step 4747, loss 0.562616.
Train: 2018-08-01T23:10:10.612780: step 4748, loss 0.57901.
Train: 2018-08-01T23:10:10.781323: step 4749, loss 0.595358.
Train: 2018-08-01T23:10:10.945889: step 4750, loss 0.562661.
Test: 2018-08-01T23:10:11.486420: step 4750, loss 0.549019.
Train: 2018-08-01T23:10:11.649998: step 4751, loss 0.627909.
Train: 2018-08-01T23:10:11.811544: step 4752, loss 0.562698.
Train: 2018-08-01T23:10:11.977126: step 4753, loss 0.530224.
Train: 2018-08-01T23:10:12.173576: step 4754, loss 0.562727.
Train: 2018-08-01T23:10:12.339134: step 4755, loss 0.611409.
Train: 2018-08-01T23:10:12.497711: step 4756, loss 0.465499.
Train: 2018-08-01T23:10:12.661278: step 4757, loss 0.643845.
Train: 2018-08-01T23:10:12.827858: step 4758, loss 0.546524.
Train: 2018-08-01T23:10:12.993385: step 4759, loss 0.62761.
Train: 2018-08-01T23:10:13.160937: step 4760, loss 0.514132.
Test: 2018-08-01T23:10:13.704499: step 4760, loss 0.549169.
Train: 2018-08-01T23:10:13.875062: step 4761, loss 0.578954.
Train: 2018-08-01T23:10:14.044600: step 4762, loss 0.497885.
Train: 2018-08-01T23:10:14.217145: step 4763, loss 0.546485.
Train: 2018-08-01T23:10:14.385663: step 4764, loss 0.530166.
Train: 2018-08-01T23:10:14.551221: step 4765, loss 0.530062.
Train: 2018-08-01T23:10:14.715781: step 4766, loss 0.546293.
Train: 2018-08-01T23:10:14.890313: step 4767, loss 0.579019.
Train: 2018-08-01T23:10:15.062857: step 4768, loss 0.562592.
Train: 2018-08-01T23:10:15.237386: step 4769, loss 0.595543.
Train: 2018-08-01T23:10:15.415908: step 4770, loss 0.579071.
Test: 2018-08-01T23:10:15.961449: step 4770, loss 0.548691.
Train: 2018-08-01T23:10:16.131994: step 4771, loss 0.612167.
Train: 2018-08-01T23:10:16.296554: step 4772, loss 0.562537.
Train: 2018-08-01T23:10:16.468096: step 4773, loss 0.496273.
Train: 2018-08-01T23:10:16.635648: step 4774, loss 0.462944.
Train: 2018-08-01T23:10:16.807220: step 4775, loss 0.579147.
Train: 2018-08-01T23:10:16.971775: step 4776, loss 0.629278.
Train: 2018-08-01T23:10:17.137320: step 4777, loss 0.629377.
Train: 2018-08-01T23:10:17.301898: step 4778, loss 0.528994.
Train: 2018-08-01T23:10:17.472411: step 4779, loss 0.696482.
Train: 2018-08-01T23:10:17.635974: step 4780, loss 0.495528.
Test: 2018-08-01T23:10:18.176528: step 4780, loss 0.548449.
Train: 2018-08-01T23:10:18.337126: step 4781, loss 0.47878.
Train: 2018-08-01T23:10:18.501687: step 4782, loss 0.612745.
Train: 2018-08-01T23:10:18.665253: step 4783, loss 0.545678.
Train: 2018-08-01T23:10:18.834769: step 4784, loss 0.528867.
Train: 2018-08-01T23:10:19.001353: step 4785, loss 0.629702.
Train: 2018-08-01T23:10:19.160897: step 4786, loss 0.596081.
Train: 2018-08-01T23:10:19.321468: step 4787, loss 0.495168.
Train: 2018-08-01T23:10:19.489049: step 4788, loss 0.562436.
Train: 2018-08-01T23:10:19.652584: step 4789, loss 0.596134.
Train: 2018-08-01T23:10:19.814151: step 4790, loss 0.596145.
Test: 2018-08-01T23:10:20.343736: step 4790, loss 0.548317.
Train: 2018-08-01T23:10:20.510290: step 4791, loss 0.663562.
Train: 2018-08-01T23:10:20.677842: step 4792, loss 0.579265.
Train: 2018-08-01T23:10:20.836452: step 4793, loss 0.512048.
Train: 2018-08-01T23:10:21.003996: step 4794, loss 0.579237.
Train: 2018-08-01T23:10:21.170556: step 4795, loss 0.663106.
Train: 2018-08-01T23:10:21.336082: step 4796, loss 0.562464.
Train: 2018-08-01T23:10:21.500644: step 4797, loss 0.529061.
Train: 2018-08-01T23:10:21.663208: step 4798, loss 0.629234.
Train: 2018-08-01T23:10:21.823779: step 4799, loss 0.495871.
Train: 2018-08-01T23:10:21.987341: step 4800, loss 0.662371.
Test: 2018-08-01T23:10:22.531887: step 4800, loss 0.5486.
Train: 2018-08-01T23:10:23.326716: step 4801, loss 0.579123.
Train: 2018-08-01T23:10:23.491269: step 4802, loss 0.562525.
Train: 2018-08-01T23:10:23.653841: step 4803, loss 0.612191.
Train: 2018-08-01T23:10:23.820362: step 4804, loss 0.678157.
Train: 2018-08-01T23:10:23.982960: step 4805, loss 0.51323.
Train: 2018-08-01T23:10:24.147518: step 4806, loss 0.546197.
Train: 2018-08-01T23:10:24.316061: step 4807, loss 0.529859.
Train: 2018-08-01T23:10:24.484586: step 4808, loss 0.529896.
Train: 2018-08-01T23:10:24.650143: step 4809, loss 0.464423.
Train: 2018-08-01T23:10:24.813736: step 4810, loss 0.579017.
Test: 2018-08-01T23:10:25.355258: step 4810, loss 0.548854.
Train: 2018-08-01T23:10:25.529792: step 4811, loss 0.480498.
Train: 2018-08-01T23:10:25.690387: step 4812, loss 0.48024.
Train: 2018-08-01T23:10:25.856478: step 4813, loss 0.595615.
Train: 2018-08-01T23:10:26.021038: step 4814, loss 0.579109.
Train: 2018-08-01T23:10:26.199592: step 4815, loss 0.579135.
Train: 2018-08-01T23:10:26.369140: step 4816, loss 0.491357.
Train: 2018-08-01T23:10:26.533698: step 4817, loss 0.579194.
Train: 2018-08-01T23:10:26.709229: step 4818, loss 0.478589.
Train: 2018-08-01T23:10:26.878770: step 4819, loss 0.579273.
Train: 2018-08-01T23:10:27.040339: step 4820, loss 0.596211.
Test: 2018-08-01T23:10:27.568899: step 4820, loss 0.548233.
Train: 2018-08-01T23:10:27.742460: step 4821, loss 0.49467.
Train: 2018-08-01T23:10:27.906029: step 4822, loss 0.64737.
Train: 2018-08-01T23:10:28.071557: step 4823, loss 0.698577.
Train: 2018-08-01T23:10:28.234122: step 4824, loss 0.579417.
Train: 2018-08-01T23:10:28.392699: step 4825, loss 0.698409.
Train: 2018-08-01T23:10:28.553268: step 4826, loss 0.579366.
Train: 2018-08-01T23:10:28.734809: step 4827, loss 0.630056.
Train: 2018-08-01T23:10:28.900340: step 4828, loss 0.545581.
Train: 2018-08-01T23:10:29.060911: step 4829, loss 0.478419.
Train: 2018-08-01T23:10:29.223477: step 4830, loss 0.528875.
Test: 2018-08-01T23:10:29.773008: step 4830, loss 0.548399.
Train: 2018-08-01T23:10:29.943562: step 4831, loss 0.579231.
Train: 2018-08-01T23:10:30.110107: step 4832, loss 0.595997.
Train: 2018-08-01T23:10:30.272703: step 4833, loss 0.662995.
Train: 2018-08-01T23:10:30.435237: step 4834, loss 0.51232.
Train: 2018-08-01T23:10:30.601817: step 4835, loss 0.746137.
Train: 2018-08-01T23:10:30.778346: step 4836, loss 0.446088.
Train: 2018-08-01T23:10:30.942881: step 4837, loss 0.545909.
Train: 2018-08-01T23:10:31.120407: step 4838, loss 0.51276.
Train: 2018-08-01T23:10:31.281008: step 4839, loss 0.612283.
Train: 2018-08-01T23:10:31.456533: step 4840, loss 0.496224.
Test: 2018-08-01T23:10:32.002049: step 4840, loss 0.548635.
Train: 2018-08-01T23:10:32.179599: step 4841, loss 0.662032.
Train: 2018-08-01T23:10:32.343169: step 4842, loss 0.628805.
Train: 2018-08-01T23:10:32.510721: step 4843, loss 0.678313.
Train: 2018-08-01T23:10:32.673256: step 4844, loss 0.54609.
Train: 2018-08-01T23:10:32.837815: step 4845, loss 0.579034.
Train: 2018-08-01T23:10:33.019330: step 4846, loss 0.497031.
Train: 2018-08-01T23:10:33.183915: step 4847, loss 0.644531.
Train: 2018-08-01T23:10:33.347044: step 4848, loss 0.546305.
Train: 2018-08-01T23:10:33.514235: step 4849, loss 0.513701.
Train: 2018-08-01T23:10:33.678824: step 4850, loss 0.432139.
Test: 2018-08-01T23:10:34.223370: step 4850, loss 0.548957.
Train: 2018-08-01T23:10:34.385904: step 4851, loss 0.644397.
Train: 2018-08-01T23:10:34.548499: step 4852, loss 0.628083.
Train: 2018-08-01T23:10:34.712075: step 4853, loss 0.611711.
Train: 2018-08-01T23:10:34.878612: step 4854, loss 0.562655.
Train: 2018-08-01T23:10:35.038160: step 4855, loss 0.530004.
Train: 2018-08-01T23:10:35.199753: step 4856, loss 0.54633.
Train: 2018-08-01T23:10:35.370273: step 4857, loss 0.497295.
Train: 2018-08-01T23:10:35.542837: step 4858, loss 0.595374.
Train: 2018-08-01T23:10:35.707371: step 4859, loss 0.67734.
Train: 2018-08-01T23:10:35.868939: step 4860, loss 0.611763.
Test: 2018-08-01T23:10:36.407499: step 4860, loss 0.548947.
Train: 2018-08-01T23:10:36.577072: step 4861, loss 0.644429.
Train: 2018-08-01T23:10:36.743601: step 4862, loss 0.660585.
Train: 2018-08-01T23:10:36.907194: step 4863, loss 0.530182.
Train: 2018-08-01T23:10:37.074744: step 4864, loss 0.546512.
Train: 2018-08-01T23:10:37.244287: step 4865, loss 0.46559.
Train: 2018-08-01T23:10:37.405830: step 4866, loss 0.59515.
Train: 2018-08-01T23:10:37.570424: step 4867, loss 0.514161.
Train: 2018-08-01T23:10:37.734951: step 4868, loss 0.562742.
Train: 2018-08-01T23:10:37.907491: step 4869, loss 0.741254.
Train: 2018-08-01T23:10:38.072076: step 4870, loss 0.578951.
Test: 2018-08-01T23:10:38.606621: step 4870, loss 0.549231.
Train: 2018-08-01T23:10:38.777164: step 4871, loss 0.498088.
Train: 2018-08-01T23:10:38.944747: step 4872, loss 0.449607.
Train: 2018-08-01T23:10:39.106295: step 4873, loss 0.627542.
Train: 2018-08-01T23:10:39.284808: step 4874, loss 0.595164.
Train: 2018-08-01T23:10:39.443418: step 4875, loss 0.578955.
Train: 2018-08-01T23:10:39.610948: step 4876, loss 0.530295.
Train: 2018-08-01T23:10:39.770543: step 4877, loss 0.578961.
Train: 2018-08-01T23:10:39.941078: step 4878, loss 0.562715.
Train: 2018-08-01T23:10:40.101655: step 4879, loss 0.578969.
Train: 2018-08-01T23:10:40.265221: step 4880, loss 0.54642.
Test: 2018-08-01T23:10:40.806771: step 4880, loss 0.549039.
Train: 2018-08-01T23:10:40.979304: step 4881, loss 0.48121.
Train: 2018-08-01T23:10:41.146818: step 4882, loss 0.562657.
Train: 2018-08-01T23:10:41.313125: step 4883, loss 0.513503.
Train: 2018-08-01T23:10:41.476678: step 4884, loss 0.496888.
Train: 2018-08-01T23:10:41.641265: step 4885, loss 0.447099.
Train: 2018-08-01T23:10:41.813804: step 4886, loss 0.512756.
Train: 2018-08-01T23:10:41.986316: step 4887, loss 0.479045.
Train: 2018-08-01T23:10:42.154865: step 4888, loss 0.579245.
Train: 2018-08-01T23:10:42.322453: step 4889, loss 0.49483.
Train: 2018-08-01T23:10:42.487003: step 4890, loss 0.562405.
Test: 2018-08-01T23:10:43.028531: step 4890, loss 0.548077.
Train: 2018-08-01T23:10:43.193091: step 4891, loss 0.613702.
Train: 2018-08-01T23:10:43.357650: step 4892, loss 0.528046.
Train: 2018-08-01T23:10:43.522210: step 4893, loss 0.510648.
Train: 2018-08-01T23:10:43.683810: step 4894, loss 0.423757.
Train: 2018-08-01T23:10:43.844380: step 4895, loss 0.614742.
Train: 2018-08-01T23:10:44.016919: step 4896, loss 0.562447.
Train: 2018-08-01T23:10:44.187457: step 4897, loss 0.580063.
Train: 2018-08-01T23:10:44.359004: step 4898, loss 0.580144.
Train: 2018-08-01T23:10:44.523534: step 4899, loss 0.456305.
Train: 2018-08-01T23:10:44.687127: step 4900, loss 0.598076.
Test: 2018-08-01T23:10:45.232637: step 4900, loss 0.547643.
Train: 2018-08-01T23:10:46.013620: step 4901, loss 0.616017.
Train: 2018-08-01T23:10:46.183136: step 4902, loss 0.598273.
Train: 2018-08-01T23:10:46.346724: step 4903, loss 0.508997.
Train: 2018-08-01T23:10:46.514250: step 4904, loss 0.544712.
Train: 2018-08-01T23:10:46.692772: step 4905, loss 0.544702.
Train: 2018-08-01T23:10:46.860352: step 4906, loss 0.526756.
Train: 2018-08-01T23:10:47.022890: step 4907, loss 0.400975.
Train: 2018-08-01T23:10:47.194432: step 4908, loss 0.580708.
Train: 2018-08-01T23:10:47.365999: step 4909, loss 0.508497.
Train: 2018-08-01T23:10:47.538513: step 4910, loss 0.544627.
Test: 2018-08-01T23:10:48.082060: step 4910, loss 0.547571.
Train: 2018-08-01T23:10:48.249640: step 4911, loss 0.471912.
Train: 2018-08-01T23:10:48.412177: step 4912, loss 0.635796.
Train: 2018-08-01T23:10:48.573744: step 4913, loss 0.654241.
Train: 2018-08-01T23:10:48.734346: step 4914, loss 0.599436.
Train: 2018-08-01T23:10:48.899904: step 4915, loss 0.526322.
Train: 2018-08-01T23:10:49.062439: step 4916, loss 0.508051.
Train: 2018-08-01T23:10:49.228994: step 4917, loss 0.581157.
Train: 2018-08-01T23:10:49.398570: step 4918, loss 0.489754.
Train: 2018-08-01T23:10:49.560139: step 4919, loss 0.43483.
Train: 2018-08-01T23:10:49.721707: step 4920, loss 0.599587.
Test: 2018-08-01T23:10:50.268251: step 4920, loss 0.547571.
Train: 2018-08-01T23:10:50.436764: step 4921, loss 0.562941.
Train: 2018-08-01T23:10:50.609328: step 4922, loss 0.526213.
Train: 2018-08-01T23:10:50.775859: step 4923, loss 0.434236.
Train: 2018-08-01T23:10:50.955404: step 4924, loss 0.50771.
Train: 2018-08-01T23:10:51.129936: step 4925, loss 0.600031.
Train: 2018-08-01T23:10:51.295980: step 4926, loss 0.54458.
Train: 2018-08-01T23:10:51.458029: step 4927, loss 0.526038.
Train: 2018-08-01T23:10:51.624582: step 4928, loss 0.526011.
Train: 2018-08-01T23:10:51.791137: step 4929, loss 0.488781.
Train: 2018-08-01T23:10:51.953703: step 4930, loss 0.488665.
Test: 2018-08-01T23:10:52.484285: step 4930, loss 0.547634.
Train: 2018-08-01T23:10:52.648878: step 4931, loss 0.563284.
Train: 2018-08-01T23:10:52.812438: step 4932, loss 0.600787.
Train: 2018-08-01T23:10:52.977965: step 4933, loss 0.507105.
Train: 2018-08-01T23:10:53.138536: step 4934, loss 0.525834.
Train: 2018-08-01T23:10:53.315089: step 4935, loss 0.563421.
Train: 2018-08-01T23:10:53.486636: step 4936, loss 0.544621.
Train: 2018-08-01T23:10:53.649202: step 4937, loss 0.469265.
Train: 2018-08-01T23:10:53.816747: step 4938, loss 0.469149.
Train: 2018-08-01T23:10:53.977294: step 4939, loss 0.450068.
Train: 2018-08-01T23:10:54.142877: step 4940, loss 0.620574.
Test: 2018-08-01T23:10:54.668472: step 4940, loss 0.547772.
Train: 2018-08-01T23:10:54.830014: step 4941, loss 0.563693.
Train: 2018-08-01T23:10:54.996594: step 4942, loss 0.658918.
Train: 2018-08-01T23:10:55.162166: step 4943, loss 0.525656.
Train: 2018-08-01T23:10:55.327683: step 4944, loss 0.525657.
Train: 2018-08-01T23:10:55.500247: step 4945, loss 0.620816.
Train: 2018-08-01T23:10:55.661821: step 4946, loss 0.582708.
Train: 2018-08-01T23:10:55.828376: step 4947, loss 0.677575.
Train: 2018-08-01T23:10:55.989946: step 4948, loss 0.506792.
Train: 2018-08-01T23:10:56.149518: step 4949, loss 0.714609.
Train: 2018-08-01T23:10:56.320055: step 4950, loss 0.619839.
Test: 2018-08-01T23:10:56.847621: step 4950, loss 0.547642.
Train: 2018-08-01T23:10:57.032153: step 4951, loss 0.469739.
Train: 2018-08-01T23:10:57.194722: step 4952, loss 0.563239.
Train: 2018-08-01T23:10:57.361278: step 4953, loss 0.581757.
Train: 2018-08-01T23:10:57.526830: step 4954, loss 0.507538.
Train: 2018-08-01T23:10:57.686409: step 4955, loss 0.4707.
Train: 2018-08-01T23:10:57.856923: step 4956, loss 0.581464.
Train: 2018-08-01T23:10:58.020485: step 4957, loss 0.655029.
Train: 2018-08-01T23:10:58.180083: step 4958, loss 0.471181.
Train: 2018-08-01T23:10:58.344649: step 4959, loss 0.599545.
Train: 2018-08-01T23:10:58.508180: step 4960, loss 0.508044.
Test: 2018-08-01T23:10:59.048738: step 4960, loss 0.547568.
Train: 2018-08-01T23:10:59.214324: step 4961, loss 0.5446.
Train: 2018-08-01T23:10:59.380848: step 4962, loss 0.544605.
Train: 2018-08-01T23:10:59.544411: step 4963, loss 0.653814.
Train: 2018-08-01T23:10:59.706010: step 4964, loss 0.526464.
Train: 2018-08-01T23:10:59.883536: step 4965, loss 0.490266.
Train: 2018-08-01T23:11:00.054074: step 4966, loss 0.635153.
Train: 2018-08-01T23:11:00.215617: step 4967, loss 0.598847.
Train: 2018-08-01T23:11:00.379210: step 4968, loss 0.616752.
Train: 2018-08-01T23:11:00.547761: step 4969, loss 0.562646.
Train: 2018-08-01T23:11:00.712320: step 4970, loss 0.437215.
Test: 2018-08-01T23:11:01.254839: step 4970, loss 0.547619.
Train: 2018-08-01T23:11:01.428400: step 4971, loss 0.598393.
Train: 2018-08-01T23:11:01.592935: step 4972, loss 0.634052.
Train: 2018-08-01T23:11:01.751542: step 4973, loss 0.580384.
Train: 2018-08-01T23:11:01.915105: step 4974, loss 0.509224.
Train: 2018-08-01T23:11:02.077638: step 4975, loss 0.580266.
Train: 2018-08-01T23:11:02.242199: step 4976, loss 0.597921.
Train: 2018-08-01T23:11:02.404797: step 4977, loss 0.615482.
Train: 2018-08-01T23:11:02.569325: step 4978, loss 0.562474.
Train: 2018-08-01T23:11:02.731890: step 4979, loss 0.492216.
Train: 2018-08-01T23:11:02.903457: step 4980, loss 0.56245.
Test: 2018-08-01T23:11:03.455954: step 4980, loss 0.547783.
Train: 2018-08-01T23:11:03.630489: step 4981, loss 0.579949.
Train: 2018-08-01T23:11:03.805052: step 4982, loss 0.475053.
Train: 2018-08-01T23:11:03.967587: step 4983, loss 0.562434.
Train: 2018-08-01T23:11:04.138130: step 4984, loss 0.527503.
Train: 2018-08-01T23:11:04.314690: step 4985, loss 0.5799.
Train: 2018-08-01T23:11:04.478221: step 4986, loss 0.614824.
Train: 2018-08-01T23:11:04.641809: step 4987, loss 0.579875.
Train: 2018-08-01T23:11:04.808364: step 4988, loss 0.527573.
Train: 2018-08-01T23:11:04.981899: step 4989, loss 0.562422.
Train: 2018-08-01T23:11:05.149458: step 4990, loss 0.597227.
Test: 2018-08-01T23:11:05.695966: step 4990, loss 0.547859.
Train: 2018-08-01T23:11:05.861549: step 4991, loss 0.510263.
Train: 2018-08-01T23:11:06.020099: step 4992, loss 0.492897.
Train: 2018-08-01T23:11:06.183689: step 4993, loss 0.66677.
Train: 2018-08-01T23:11:06.347256: step 4994, loss 0.597167.
Train: 2018-08-01T23:11:06.509822: step 4995, loss 0.631819.
Train: 2018-08-01T23:11:06.672387: step 4996, loss 0.510469.
Train: 2018-08-01T23:11:06.832927: step 4997, loss 0.45867.
Train: 2018-08-01T23:11:06.994496: step 4998, loss 0.54511.
Train: 2018-08-01T23:11:07.157061: step 4999, loss 0.631615.
Train: 2018-08-01T23:11:07.317631: step 5000, loss 0.648864.
Test: 2018-08-01T23:11:07.860182: step 5000, loss 0.547947.
Train: 2018-08-01T23:11:08.611774: step 5001, loss 0.5624.
Train: 2018-08-01T23:11:08.774340: step 5002, loss 0.614092.
Train: 2018-08-01T23:11:08.938874: step 5003, loss 0.631163.
Train: 2018-08-01T23:11:09.103435: step 5004, loss 0.52812.
Train: 2018-08-01T23:11:09.281957: step 5005, loss 0.5282.
Train: 2018-08-01T23:11:09.447514: step 5006, loss 0.681906.
Train: 2018-08-01T23:11:09.618059: step 5007, loss 0.579422.
Train: 2018-08-01T23:11:09.791595: step 5008, loss 0.443651.
Train: 2018-08-01T23:11:09.970118: step 5009, loss 0.596312.
Train: 2018-08-01T23:11:10.148641: step 5010, loss 0.528562.
Test: 2018-08-01T23:11:10.687201: step 5010, loss 0.548252.
Train: 2018-08-01T23:11:10.855751: step 5011, loss 0.511668.
Train: 2018-08-01T23:11:11.019320: step 5012, loss 0.562417.
Train: 2018-08-01T23:11:11.185868: step 5013, loss 0.562416.
Train: 2018-08-01T23:11:11.359403: step 5014, loss 0.613208.
Train: 2018-08-01T23:11:11.520997: step 5015, loss 0.460878.
Train: 2018-08-01T23:11:11.683568: step 5016, loss 0.54547.
Train: 2018-08-01T23:11:11.843141: step 5017, loss 0.56241.
Train: 2018-08-01T23:11:12.005701: step 5018, loss 0.494453.
Train: 2018-08-01T23:11:12.169239: step 5019, loss 0.613481.
Train: 2018-08-01T23:11:12.341804: step 5020, loss 0.511259.
Test: 2018-08-01T23:11:12.874358: step 5020, loss 0.548096.
Train: 2018-08-01T23:11:13.044898: step 5021, loss 0.613638.
Train: 2018-08-01T23:11:13.221425: step 5022, loss 0.545301.
Train: 2018-08-01T23:11:13.384988: step 5023, loss 0.545281.
Train: 2018-08-01T23:11:13.549575: step 5024, loss 0.613809.
Train: 2018-08-01T23:11:13.710121: step 5025, loss 0.562396.
Train: 2018-08-01T23:11:13.872685: step 5026, loss 0.510945.
Train: 2018-08-01T23:11:14.035250: step 5027, loss 0.545227.
Train: 2018-08-01T23:11:14.201832: step 5028, loss 0.459255.
Train: 2018-08-01T23:11:14.361410: step 5029, loss 0.614105.
Train: 2018-08-01T23:11:14.524966: step 5030, loss 0.631452.
Test: 2018-08-01T23:11:15.058516: step 5030, loss 0.547939.
Train: 2018-08-01T23:11:15.220082: step 5031, loss 0.59694.
Train: 2018-08-01T23:11:15.387636: step 5032, loss 0.6142.
Train: 2018-08-01T23:11:15.551224: step 5033, loss 0.51065.
Train: 2018-08-01T23:11:15.712792: step 5034, loss 0.510655.
Train: 2018-08-01T23:11:15.879320: step 5035, loss 0.579659.
Train: 2018-08-01T23:11:16.045876: step 5036, loss 0.5624.
Train: 2018-08-01T23:11:16.211433: step 5037, loss 0.631476.
Train: 2018-08-01T23:11:16.374996: step 5038, loss 0.596909.
Train: 2018-08-01T23:11:16.545541: step 5039, loss 0.579631.
Train: 2018-08-01T23:11:16.716084: step 5040, loss 0.493563.
Test: 2018-08-01T23:11:17.240682: step 5040, loss 0.547991.
Train: 2018-08-01T23:11:17.405242: step 5041, loss 0.614009.
Train: 2018-08-01T23:11:17.568806: step 5042, loss 0.665516.
Train: 2018-08-01T23:11:17.732399: step 5043, loss 0.545252.
Train: 2018-08-01T23:11:17.892965: step 5044, loss 0.459734.
Train: 2018-08-01T23:11:18.057531: step 5045, loss 0.630826.
Train: 2018-08-01T23:11:18.220090: step 5046, loss 0.494054.
Train: 2018-08-01T23:11:18.382629: step 5047, loss 0.494057.
Train: 2018-08-01T23:11:18.545225: step 5048, loss 0.493987.
Train: 2018-08-01T23:11:18.706794: step 5049, loss 0.613806.
Train: 2018-08-01T23:11:18.870350: step 5050, loss 0.57955.
Test: 2018-08-01T23:11:19.406890: step 5050, loss 0.548025.
Train: 2018-08-01T23:11:19.580427: step 5051, loss 0.579561.
Train: 2018-08-01T23:11:19.749974: step 5052, loss 0.665413.
Train: 2018-08-01T23:11:19.913548: step 5053, loss 0.596692.
Train: 2018-08-01T23:11:20.077125: step 5054, loss 0.665111.
Train: 2018-08-01T23:11:20.235708: step 5055, loss 0.477066.
Train: 2018-08-01T23:11:20.398241: step 5056, loss 0.562402.
Train: 2018-08-01T23:11:20.558811: step 5057, loss 0.528363.
Train: 2018-08-01T23:11:20.720380: step 5058, loss 0.562406.
Train: 2018-08-01T23:11:20.883968: step 5059, loss 0.494398.
Train: 2018-08-01T23:11:21.052491: step 5060, loss 0.613444.
Test: 2018-08-01T23:11:21.579084: step 5060, loss 0.548162.
Train: 2018-08-01T23:11:21.740652: step 5061, loss 0.613434.
Train: 2018-08-01T23:11:21.904215: step 5062, loss 0.52842.
Train: 2018-08-01T23:11:22.075757: step 5063, loss 0.562408.
Train: 2018-08-01T23:11:22.239319: step 5064, loss 0.579394.
Train: 2018-08-01T23:11:22.406871: step 5065, loss 0.494498.
Train: 2018-08-01T23:11:22.570465: step 5066, loss 0.562408.
Train: 2018-08-01T23:11:22.739980: step 5067, loss 0.545406.
Train: 2018-08-01T23:11:22.904541: step 5068, loss 0.6645.
Train: 2018-08-01T23:11:23.074088: step 5069, loss 0.562406.
Train: 2018-08-01T23:11:23.246626: step 5070, loss 0.596392.
Test: 2018-08-01T23:11:23.777217: step 5070, loss 0.548195.
Train: 2018-08-01T23:11:23.939804: step 5071, loss 0.613333.
Train: 2018-08-01T23:11:24.101367: step 5072, loss 0.545468.
Train: 2018-08-01T23:11:24.267897: step 5073, loss 0.545493.
Train: 2018-08-01T23:11:24.428477: step 5074, loss 0.663888.
Train: 2018-08-01T23:11:24.599011: step 5075, loss 0.461187.
Train: 2018-08-01T23:11:24.769555: step 5076, loss 0.528695.
Train: 2018-08-01T23:11:24.930126: step 5077, loss 0.545557.
Train: 2018-08-01T23:11:25.091720: step 5078, loss 0.545546.
Train: 2018-08-01T23:11:25.264232: step 5079, loss 0.562423.
Train: 2018-08-01T23:11:25.424830: step 5080, loss 0.56242.
Test: 2018-08-01T23:11:25.961369: step 5080, loss 0.548251.
Train: 2018-08-01T23:11:26.125929: step 5081, loss 0.680849.
Train: 2018-08-01T23:11:26.293513: step 5082, loss 0.680721.
Train: 2018-08-01T23:11:26.461064: step 5083, loss 0.646696.
Train: 2018-08-01T23:11:26.622601: step 5084, loss 0.629605.
Train: 2018-08-01T23:11:26.788167: step 5085, loss 0.64605.
Train: 2018-08-01T23:11:26.955710: step 5086, loss 0.562504.
Train: 2018-08-01T23:11:27.130244: step 5087, loss 0.661853.
Train: 2018-08-01T23:11:27.297797: step 5088, loss 0.579044.
Train: 2018-08-01T23:11:27.459389: step 5089, loss 0.579007.
Train: 2018-08-01T23:11:27.619966: step 5090, loss 0.546389.
Test: 2018-08-01T23:11:28.159492: step 5090, loss 0.549137.
Train: 2018-08-01T23:11:28.327071: step 5091, loss 0.514035.
Train: 2018-08-01T23:11:28.488614: step 5092, loss 0.449428.
Train: 2018-08-01T23:11:28.653173: step 5093, loss 0.595136.
Train: 2018-08-01T23:11:28.824715: step 5094, loss 0.546586.
Train: 2018-08-01T23:11:28.989306: step 5095, loss 0.546585.
Train: 2018-08-01T23:11:29.154850: step 5096, loss 0.578948.
Train: 2018-08-01T23:11:29.319393: step 5097, loss 0.514167.
Train: 2018-08-01T23:11:29.479967: step 5098, loss 0.530301.
Train: 2018-08-01T23:11:29.645546: step 5099, loss 0.530213.
Train: 2018-08-01T23:11:29.811078: step 5100, loss 0.562686.
Test: 2018-08-01T23:11:30.358616: step 5100, loss 0.548983.
Train: 2018-08-01T23:11:31.126603: step 5101, loss 0.627987.
Train: 2018-08-01T23:11:31.292127: step 5102, loss 0.513587.
Train: 2018-08-01T23:11:31.453695: step 5103, loss 0.562625.
Train: 2018-08-01T23:11:31.614296: step 5104, loss 0.513337.
Train: 2018-08-01T23:11:31.774866: step 5105, loss 0.562579.
Train: 2018-08-01T23:11:31.936429: step 5106, loss 0.595583.
Train: 2018-08-01T23:11:32.100964: step 5107, loss 0.579086.
Train: 2018-08-01T23:11:32.269547: step 5108, loss 0.579101.
Train: 2018-08-01T23:11:32.437100: step 5109, loss 0.54592.
Train: 2018-08-01T23:11:32.601626: step 5110, loss 0.479388.
Test: 2018-08-01T23:11:33.141188: step 5110, loss 0.548526.
Train: 2018-08-01T23:11:33.304747: step 5111, loss 0.562487.
Train: 2018-08-01T23:11:33.468310: step 5112, loss 0.579187.
Train: 2018-08-01T23:11:33.628910: step 5113, loss 0.495444.
Train: 2018-08-01T23:11:33.792443: step 5114, loss 0.47841.
Train: 2018-08-01T23:11:33.963018: step 5115, loss 0.613058.
Train: 2018-08-01T23:11:34.126550: step 5116, loss 0.545487.
Train: 2018-08-01T23:11:34.287119: step 5117, loss 0.616745.
Train: 2018-08-01T23:11:34.463673: step 5118, loss 0.49435.
Train: 2018-08-01T23:11:34.623222: step 5119, loss 0.57946.
Train: 2018-08-01T23:11:34.788779: step 5120, loss 0.442697.
Test: 2018-08-01T23:11:35.327340: step 5120, loss 0.548021.
Train: 2018-08-01T23:11:35.506859: step 5121, loss 0.493731.
Train: 2018-08-01T23:11:35.671448: step 5122, loss 0.66586.
Train: 2018-08-01T23:11:35.832987: step 5123, loss 0.579688.
Train: 2018-08-01T23:11:35.992588: step 5124, loss 0.614365.
Train: 2018-08-01T23:11:36.157153: step 5125, loss 0.614416.
Train: 2018-08-01T23:11:36.320718: step 5126, loss 0.579745.
Train: 2018-08-01T23:11:36.488235: step 5127, loss 0.597072.
Train: 2018-08-01T23:11:36.653834: step 5128, loss 0.458493.
Train: 2018-08-01T23:11:36.813398: step 5129, loss 0.614406.
Train: 2018-08-01T23:11:36.984908: step 5130, loss 0.597071.
Test: 2018-08-01T23:11:37.511501: step 5130, loss 0.547902.
Train: 2018-08-01T23:11:37.675095: step 5131, loss 0.579727.
Train: 2018-08-01T23:11:37.839623: step 5132, loss 0.666242.
Train: 2018-08-01T23:11:38.014182: step 5133, loss 0.545134.
Train: 2018-08-01T23:11:38.188721: step 5134, loss 0.631334.
Train: 2018-08-01T23:11:38.352282: step 5135, loss 0.528022.
Train: 2018-08-01T23:11:38.516846: step 5136, loss 0.528089.
Train: 2018-08-01T23:11:38.676411: step 5137, loss 0.545264.
Train: 2018-08-01T23:11:38.843969: step 5138, loss 0.442573.
Train: 2018-08-01T23:11:39.018497: step 5139, loss 0.493853.
Train: 2018-08-01T23:11:39.182063: step 5140, loss 0.596735.
Test: 2018-08-01T23:11:39.726578: step 5140, loss 0.548.
Train: 2018-08-01T23:11:39.892136: step 5141, loss 0.49363.
Train: 2018-08-01T23:11:40.054702: step 5142, loss 0.562398.
Train: 2018-08-01T23:11:40.228238: step 5143, loss 0.545138.
Train: 2018-08-01T23:11:40.391835: step 5144, loss 0.562403.
Train: 2018-08-01T23:11:40.546388: step 5145, loss 0.614384.
Train: 2018-08-01T23:11:40.708953: step 5146, loss 0.458373.
Train: 2018-08-01T23:11:40.869554: step 5147, loss 0.666677.
Train: 2018-08-01T23:11:41.032089: step 5148, loss 0.458113.
Train: 2018-08-01T23:11:41.195652: step 5149, loss 0.545006.
Train: 2018-08-01T23:11:41.358248: step 5150, loss 0.597324.
Test: 2018-08-01T23:11:41.905754: step 5150, loss 0.547806.
Train: 2018-08-01T23:11:42.066351: step 5151, loss 0.492564.
Train: 2018-08-01T23:11:42.233906: step 5152, loss 0.667447.
Train: 2018-08-01T23:11:42.396442: step 5153, loss 0.632457.
Train: 2018-08-01T23:11:42.561033: step 5154, loss 0.509975.
Train: 2018-08-01T23:11:42.725593: step 5155, loss 0.562437.
Train: 2018-08-01T23:11:42.888158: step 5156, loss 0.562436.
Train: 2018-08-01T23:11:43.056702: step 5157, loss 0.597389.
Train: 2018-08-01T23:11:43.218270: step 5158, loss 0.562432.
Train: 2018-08-01T23:11:43.388821: step 5159, loss 0.527529.
Train: 2018-08-01T23:11:43.559358: step 5160, loss 0.492644.
Test: 2018-08-01T23:11:44.099893: step 5160, loss 0.547809.
Train: 2018-08-01T23:11:44.272455: step 5161, loss 0.579893.
Train: 2018-08-01T23:11:44.437020: step 5162, loss 0.562432.
Train: 2018-08-01T23:11:44.600581: step 5163, loss 0.475085.
Train: 2018-08-01T23:11:44.774086: step 5164, loss 0.56244.
Train: 2018-08-01T23:11:44.937665: step 5165, loss 0.474863.
Train: 2018-08-01T23:11:45.099249: step 5166, loss 0.527343.
Train: 2018-08-01T23:11:45.269789: step 5167, loss 0.615273.
Train: 2018-08-01T23:11:45.433323: step 5168, loss 0.56248.
Train: 2018-08-01T23:11:45.595921: step 5169, loss 0.686009.
Train: 2018-08-01T23:11:45.766459: step 5170, loss 0.562482.
Test: 2018-08-01T23:11:46.309980: step 5170, loss 0.547725.
Train: 2018-08-01T23:11:46.477577: step 5171, loss 0.580095.
Train: 2018-08-01T23:11:46.650070: step 5172, loss 0.562471.
Train: 2018-08-01T23:11:46.816625: step 5173, loss 0.456985.
Train: 2018-08-01T23:11:46.992156: step 5174, loss 0.580056.
Train: 2018-08-01T23:11:47.154752: step 5175, loss 0.615238.
Train: 2018-08-01T23:11:47.319307: step 5176, loss 0.509733.
Train: 2018-08-01T23:11:47.478855: step 5177, loss 0.52731.
Train: 2018-08-01T23:11:47.643415: step 5178, loss 0.439375.
Train: 2018-08-01T23:11:47.810999: step 5179, loss 0.59772.
Train: 2018-08-01T23:11:47.973534: step 5180, loss 0.633058.
Test: 2018-08-01T23:11:48.518123: step 5180, loss 0.547712.
Train: 2018-08-01T23:11:48.689644: step 5181, loss 0.491912.
Train: 2018-08-01T23:11:48.856200: step 5182, loss 0.527172.
Train: 2018-08-01T23:11:49.025751: step 5183, loss 0.403359.
Train: 2018-08-01T23:11:49.194301: step 5184, loss 0.580267.
Train: 2018-08-01T23:11:49.356866: step 5185, loss 0.562547.
Train: 2018-08-01T23:11:49.521394: step 5186, loss 0.491265.
Train: 2018-08-01T23:11:49.687975: step 5187, loss 0.580469.
Train: 2018-08-01T23:11:49.850516: step 5188, loss 0.50887.
Train: 2018-08-01T23:11:50.015107: step 5189, loss 0.652445.
Train: 2018-08-01T23:11:50.172655: step 5190, loss 0.580629.
Test: 2018-08-01T23:11:50.698249: step 5190, loss 0.547597.
Train: 2018-08-01T23:11:50.861824: step 5191, loss 0.580641.
Train: 2018-08-01T23:11:51.029389: step 5192, loss 0.634591.
Train: 2018-08-01T23:11:51.198935: step 5193, loss 0.508756.
Train: 2018-08-01T23:11:51.365488: step 5194, loss 0.616501.
Train: 2018-08-01T23:11:51.531024: step 5195, loss 0.670209.
Train: 2018-08-01T23:11:51.701567: step 5196, loss 0.598357.
Train: 2018-08-01T23:11:51.868122: step 5197, loss 0.669509.
Train: 2018-08-01T23:11:52.043681: step 5198, loss 0.527039.
Train: 2018-08-01T23:11:52.211233: step 5199, loss 0.509462.
Train: 2018-08-01T23:11:52.384767: step 5200, loss 0.439054.
Test: 2018-08-01T23:11:52.932276: step 5200, loss 0.547723.
Train: 2018-08-01T23:11:53.713589: step 5201, loss 0.439139.
Train: 2018-08-01T23:11:53.877183: step 5202, loss 0.491933.
Train: 2018-08-01T23:11:54.037722: step 5203, loss 0.527157.
Train: 2018-08-01T23:11:54.207269: step 5204, loss 0.580217.
Train: 2018-08-01T23:11:54.370862: step 5205, loss 0.509326.
Train: 2018-08-01T23:11:54.546362: step 5206, loss 0.544771.
Train: 2018-08-01T23:11:54.711921: step 5207, loss 0.526953.
Train: 2018-08-01T23:11:54.871518: step 5208, loss 0.651762.
Train: 2018-08-01T23:11:55.040043: step 5209, loss 0.526884.
Train: 2018-08-01T23:11:55.214601: step 5210, loss 0.669754.
Test: 2018-08-01T23:11:55.756128: step 5210, loss 0.547633.
Train: 2018-08-01T23:11:55.926672: step 5211, loss 0.616114.
Train: 2018-08-01T23:11:56.096244: step 5212, loss 0.52693.
Train: 2018-08-01T23:11:56.266763: step 5213, loss 0.420185.
Train: 2018-08-01T23:11:56.432321: step 5214, loss 0.562559.
Train: 2018-08-01T23:11:56.590925: step 5215, loss 0.562565.
Train: 2018-08-01T23:11:56.766429: step 5216, loss 0.705211.
Train: 2018-08-01T23:11:56.985261: step 5217, loss 0.651553.
Train: 2018-08-01T23:11:57.155805: step 5218, loss 0.580278.
Train: 2018-08-01T23:11:57.322385: step 5219, loss 0.668681.
Train: 2018-08-01T23:11:57.479938: step 5220, loss 0.544858.
Test: 2018-08-01T23:11:58.022487: step 5220, loss 0.547756.
Train: 2018-08-01T23:11:58.189043: step 5221, loss 0.474682.
Train: 2018-08-01T23:11:58.352605: step 5222, loss 0.509894.
Train: 2018-08-01T23:11:58.523174: step 5223, loss 0.52745.
Train: 2018-08-01T23:11:58.696686: step 5224, loss 0.632366.
Train: 2018-08-01T23:11:58.857297: step 5225, loss 0.562429.
Train: 2018-08-01T23:11:59.019855: step 5226, loss 0.544999.
Train: 2018-08-01T23:11:59.183415: step 5227, loss 0.597225.
Train: 2018-08-01T23:11:59.348973: step 5228, loss 0.579788.
Train: 2018-08-01T23:11:59.525471: step 5229, loss 0.562409.
Train: 2018-08-01T23:11:59.694056: step 5230, loss 0.510463.
Test: 2018-08-01T23:12:00.237567: step 5230, loss 0.547916.
Train: 2018-08-01T23:12:00.400131: step 5231, loss 0.579704.
Train: 2018-08-01T23:12:00.564692: step 5232, loss 0.545118.
Train: 2018-08-01T23:12:00.725263: step 5233, loss 0.476037.
Train: 2018-08-01T23:12:00.888852: step 5234, loss 0.596971.
Train: 2018-08-01T23:12:01.053417: step 5235, loss 0.527831.
Train: 2018-08-01T23:12:01.222951: step 5236, loss 0.493223.
Train: 2018-08-01T23:12:01.385498: step 5237, loss 0.545085.
Train: 2018-08-01T23:12:01.548069: step 5238, loss 0.510366.
Train: 2018-08-01T23:12:01.718639: step 5239, loss 0.545031.
Train: 2018-08-01T23:12:01.888184: step 5240, loss 0.510157.
Test: 2018-08-01T23:12:02.424720: step 5240, loss 0.547806.
Train: 2018-08-01T23:12:02.586313: step 5241, loss 0.475096.
Train: 2018-08-01T23:12:02.750848: step 5242, loss 0.52739.
Train: 2018-08-01T23:12:02.923413: step 5243, loss 0.650433.
Train: 2018-08-01T23:12:03.084973: step 5244, loss 0.721117.
Train: 2018-08-01T23:12:03.252532: step 5245, loss 0.650552.
Train: 2018-08-01T23:12:03.420059: step 5246, loss 0.597628.
Train: 2018-08-01T23:12:03.600602: step 5247, loss 0.562452.
Train: 2018-08-01T23:12:03.767132: step 5248, loss 0.492428.
Train: 2018-08-01T23:12:03.933715: step 5249, loss 0.597409.
Train: 2018-08-01T23:12:04.100266: step 5250, loss 0.667192.
Test: 2018-08-01T23:12:04.648781: step 5250, loss 0.547842.
Train: 2018-08-01T23:12:04.815357: step 5251, loss 0.492781.
Train: 2018-08-01T23:12:04.983909: step 5252, loss 0.527653.
Train: 2018-08-01T23:12:05.152429: step 5253, loss 0.545049.
Train: 2018-08-01T23:12:05.323969: step 5254, loss 0.56241.
Train: 2018-08-01T23:12:05.488574: step 5255, loss 0.562408.
Train: 2018-08-01T23:12:05.656082: step 5256, loss 0.527751.
Train: 2018-08-01T23:12:05.817650: step 5257, loss 0.562407.
Train: 2018-08-01T23:12:05.978220: step 5258, loss 0.666349.
Train: 2018-08-01T23:12:06.144775: step 5259, loss 0.562403.
Train: 2018-08-01T23:12:06.313325: step 5260, loss 0.5624.
Test: 2018-08-01T23:12:06.849919: step 5260, loss 0.547959.
Train: 2018-08-01T23:12:07.014451: step 5261, loss 0.562399.
Train: 2018-08-01T23:12:07.183031: step 5262, loss 0.562397.
Train: 2018-08-01T23:12:07.347561: step 5263, loss 0.545195.
Train: 2018-08-01T23:12:07.516109: step 5264, loss 0.648333.
Train: 2018-08-01T23:12:07.672690: step 5265, loss 0.545243.
Train: 2018-08-01T23:12:07.835255: step 5266, loss 0.562396.
Train: 2018-08-01T23:12:07.996855: step 5267, loss 0.511091.
Train: 2018-08-01T23:12:08.163379: step 5268, loss 0.562397.
Train: 2018-08-01T23:12:08.326971: step 5269, loss 0.494052.
Train: 2018-08-01T23:12:08.490505: step 5270, loss 0.442711.
Test: 2018-08-01T23:12:09.030062: step 5270, loss 0.548043.
Train: 2018-08-01T23:12:09.241917: step 5271, loss 0.528117.
Train: 2018-08-01T23:12:09.407442: step 5272, loss 0.528025.
Train: 2018-08-01T23:12:09.570007: step 5273, loss 0.545162.
Train: 2018-08-01T23:12:09.738588: step 5274, loss 0.648829.
Train: 2018-08-01T23:12:09.902119: step 5275, loss 0.683557.
Train: 2018-08-01T23:12:10.065682: step 5276, loss 0.527811.
Train: 2018-08-01T23:12:10.231270: step 5277, loss 0.562403.
Train: 2018-08-01T23:12:10.403777: step 5278, loss 0.562403.
Train: 2018-08-01T23:12:10.568365: step 5279, loss 0.631557.
Train: 2018-08-01T23:12:10.733895: step 5280, loss 0.596937.
Test: 2018-08-01T23:12:11.270488: step 5280, loss 0.547961.
Train: 2018-08-01T23:12:11.450005: step 5281, loss 0.493436.
Train: 2018-08-01T23:12:11.617532: step 5282, loss 0.579631.
Train: 2018-08-01T23:12:11.792067: step 5283, loss 0.493511.
Train: 2018-08-01T23:12:11.952637: step 5284, loss 0.545169.
Train: 2018-08-01T23:12:12.112237: step 5285, loss 0.665835.
Train: 2018-08-01T23:12:12.288749: step 5286, loss 0.545175.
Train: 2018-08-01T23:12:12.462275: step 5287, loss 0.459131.
Train: 2018-08-01T23:12:12.632844: step 5288, loss 0.562397.
Train: 2018-08-01T23:12:12.803389: step 5289, loss 0.562398.
Train: 2018-08-01T23:12:12.964932: step 5290, loss 0.562399.
Test: 2018-08-01T23:12:13.493519: step 5290, loss 0.547943.
Train: 2018-08-01T23:12:13.668082: step 5291, loss 0.579665.
Train: 2018-08-01T23:12:13.829619: step 5292, loss 0.51059.
Train: 2018-08-01T23:12:13.992185: step 5293, loss 0.596977.
Train: 2018-08-01T23:12:14.170739: step 5294, loss 0.545109.
Train: 2018-08-01T23:12:14.340286: step 5295, loss 0.527796.
Train: 2018-08-01T23:12:14.505812: step 5296, loss 0.597048.
Train: 2018-08-01T23:12:14.667381: step 5297, loss 0.597061.
Train: 2018-08-01T23:12:14.828976: step 5298, loss 0.52776.
Train: 2018-08-01T23:12:14.987524: step 5299, loss 0.45844.
Train: 2018-08-01T23:12:15.147098: step 5300, loss 0.458263.
Test: 2018-08-01T23:12:15.692670: step 5300, loss 0.547841.
Train: 2018-08-01T23:12:16.544164: step 5301, loss 0.579831.
Train: 2018-08-01T23:12:16.713716: step 5302, loss 0.61479.
Train: 2018-08-01T23:12:16.878276: step 5303, loss 0.632345.
Train: 2018-08-01T23:12:17.045828: step 5304, loss 0.509995.
Train: 2018-08-01T23:12:17.221327: step 5305, loss 0.52745.
Train: 2018-08-01T23:12:17.385919: step 5306, loss 0.579961.
Train: 2018-08-01T23:12:17.551469: step 5307, loss 0.527391.
Train: 2018-08-01T23:12:17.724015: step 5308, loss 0.597551.
Train: 2018-08-01T23:12:17.894559: step 5309, loss 0.474677.
Train: 2018-08-01T23:12:18.064076: step 5310, loss 0.492132.
Test: 2018-08-01T23:12:18.611643: step 5310, loss 0.547721.
Train: 2018-08-01T23:12:18.775201: step 5311, loss 0.544855.
Train: 2018-08-01T23:12:18.939735: step 5312, loss 0.668478.
Train: 2018-08-01T23:12:19.100330: step 5313, loss 0.509483.
Train: 2018-08-01T23:12:19.262902: step 5314, loss 0.615573.
Train: 2018-08-01T23:12:19.430422: step 5315, loss 0.615578.
Train: 2018-08-01T23:12:19.596004: step 5316, loss 0.562499.
Train: 2018-08-01T23:12:19.760571: step 5317, loss 0.633145.
Train: 2018-08-01T23:12:19.927095: step 5318, loss 0.421438.
Train: 2018-08-01T23:12:20.095644: step 5319, loss 0.615389.
Train: 2018-08-01T23:12:20.261201: step 5320, loss 0.668227.
Test: 2018-08-01T23:12:20.795775: step 5320, loss 0.547739.
Train: 2018-08-01T23:12:20.958337: step 5321, loss 0.562466.
Train: 2018-08-01T23:12:21.134865: step 5322, loss 0.492249.
Train: 2018-08-01T23:12:21.305435: step 5323, loss 0.544915.
Train: 2018-08-01T23:12:21.467992: step 5324, loss 0.597499.
Train: 2018-08-01T23:12:21.635566: step 5325, loss 0.579948.
Train: 2018-08-01T23:12:21.808098: step 5326, loss 0.544954.
Train: 2018-08-01T23:12:21.970641: step 5327, loss 0.475109.
Train: 2018-08-01T23:12:22.140178: step 5328, loss 0.562433.
Train: 2018-08-01T23:12:22.311721: step 5329, loss 0.562434.
Train: 2018-08-01T23:12:22.478275: step 5330, loss 0.57991.
Test: 2018-08-01T23:12:23.009853: step 5330, loss 0.547803.
Train: 2018-08-01T23:12:23.172446: step 5331, loss 0.544962.
Train: 2018-08-01T23:12:23.335982: step 5332, loss 0.614849.
Train: 2018-08-01T23:12:23.495555: step 5333, loss 0.510058.
Train: 2018-08-01T23:12:23.665134: step 5334, loss 0.510059.
Train: 2018-08-01T23:12:23.830691: step 5335, loss 0.562433.
Train: 2018-08-01T23:12:23.998212: step 5336, loss 0.527479.
Train: 2018-08-01T23:12:24.164767: step 5337, loss 0.544945.
Train: 2018-08-01T23:12:24.327370: step 5338, loss 0.492393.
Train: 2018-08-01T23:12:24.496879: step 5339, loss 0.597545.
Train: 2018-08-01T23:12:24.675426: step 5340, loss 0.527329.
Test: 2018-08-01T23:12:25.218949: step 5340, loss 0.547738.
Train: 2018-08-01T23:12:25.387531: step 5341, loss 0.650417.
Train: 2018-08-01T23:12:25.551061: step 5342, loss 0.597644.
Train: 2018-08-01T23:12:25.712628: step 5343, loss 0.544886.
Train: 2018-08-01T23:12:25.877188: step 5344, loss 0.68545.
Train: 2018-08-01T23:12:26.045764: step 5345, loss 0.57998.
Train: 2018-08-01T23:12:26.206340: step 5346, loss 0.579928.
Train: 2018-08-01T23:12:26.371914: step 5347, loss 0.457744.
Train: 2018-08-01T23:12:26.552383: step 5348, loss 0.61473.
Train: 2018-08-01T23:12:26.713951: step 5349, loss 0.56242.
Train: 2018-08-01T23:12:26.894469: step 5350, loss 0.54503.
Test: 2018-08-01T23:12:27.451986: step 5350, loss 0.547869.
Train: 2018-08-01T23:12:27.652442: step 5351, loss 0.614516.
Train: 2018-08-01T23:12:27.834955: step 5352, loss 0.54507.
Train: 2018-08-01T23:12:28.001512: step 5353, loss 0.683606.
Train: 2018-08-01T23:12:28.175079: step 5354, loss 0.5624.
Train: 2018-08-01T23:12:28.348582: step 5355, loss 0.562397.
Train: 2018-08-01T23:12:28.641042: step 5356, loss 0.510882.
Train: 2018-08-01T23:12:28.801648: step 5357, loss 0.476667.
Train: 2018-08-01T23:12:28.978131: step 5358, loss 0.528106.
Train: 2018-08-01T23:12:29.160641: step 5359, loss 0.562396.
Train: 2018-08-01T23:12:29.322210: step 5360, loss 0.562396.
Test: 2018-08-01T23:12:29.856853: step 5360, loss 0.54802.
Train: 2018-08-01T23:12:30.022410: step 5361, loss 0.648233.
Train: 2018-08-01T23:12:30.190989: step 5362, loss 0.596698.
Train: 2018-08-01T23:12:30.358540: step 5363, loss 0.630907.
Train: 2018-08-01T23:12:30.520106: step 5364, loss 0.613664.
Train: 2018-08-01T23:12:30.694616: step 5365, loss 0.562401.
Train: 2018-08-01T23:12:30.896076: step 5366, loss 0.613399.
Train: 2018-08-01T23:12:31.078605: step 5367, loss 0.579359.
Train: 2018-08-01T23:12:31.253151: step 5368, loss 0.596215.
Train: 2018-08-01T23:12:31.426657: step 5369, loss 0.511903.
Train: 2018-08-01T23:12:31.587237: step 5370, loss 0.545632.
Test: 2018-08-01T23:12:32.131771: step 5370, loss 0.548393.
Train: 2018-08-01T23:12:32.301320: step 5371, loss 0.478519.
Train: 2018-08-01T23:12:32.477847: step 5372, loss 0.596023.
Train: 2018-08-01T23:12:32.649387: step 5373, loss 0.596013.
Train: 2018-08-01T23:12:32.815971: step 5374, loss 0.612759.
Train: 2018-08-01T23:12:32.980503: step 5375, loss 0.579206.
Train: 2018-08-01T23:12:33.145088: step 5376, loss 0.562469.
Train: 2018-08-01T23:12:33.320594: step 5377, loss 0.612574.
Train: 2018-08-01T23:12:33.481165: step 5378, loss 0.545819.
Train: 2018-08-01T23:12:33.649714: step 5379, loss 0.629086.
Train: 2018-08-01T23:12:33.834221: step 5380, loss 0.612347.
Test: 2018-08-01T23:12:34.366822: step 5380, loss 0.548654.
Train: 2018-08-01T23:12:34.534378: step 5381, loss 0.562529.
Train: 2018-08-01T23:12:34.694950: step 5382, loss 0.595611.
Train: 2018-08-01T23:12:34.869479: step 5383, loss 0.529581.
Train: 2018-08-01T23:12:35.075901: step 5384, loss 0.579047.
Train: 2018-08-01T23:12:35.260409: step 5385, loss 0.579036.
Train: 2018-08-01T23:12:35.437935: step 5386, loss 0.595445.
Train: 2018-08-01T23:12:35.610480: step 5387, loss 0.579014.
Train: 2018-08-01T23:12:35.774035: step 5388, loss 0.529909.
Train: 2018-08-01T23:12:35.934636: step 5389, loss 0.693464.
Train: 2018-08-01T23:12:36.098202: step 5390, loss 0.562675.
Test: 2018-08-01T23:12:36.641063: step 5390, loss 0.549075.
Train: 2018-08-01T23:12:36.810751: step 5391, loss 0.611513.
Train: 2018-08-01T23:12:36.979379: step 5392, loss 0.595186.
Train: 2018-08-01T23:12:37.142910: step 5393, loss 0.578947.
Train: 2018-08-01T23:12:37.305476: step 5394, loss 0.546655.
Train: 2018-08-01T23:12:37.483999: step 5395, loss 0.498373.
Train: 2018-08-01T23:12:37.645567: step 5396, loss 0.546717.
Train: 2018-08-01T23:12:37.816112: step 5397, loss 0.562821.
Train: 2018-08-01T23:12:37.980672: step 5398, loss 0.595051.
Train: 2018-08-01T23:12:38.148249: step 5399, loss 0.562814.
Train: 2018-08-01T23:12:38.334726: step 5400, loss 0.56281.
Test: 2018-08-01T23:12:38.874283: step 5400, loss 0.549296.
Train: 2018-08-01T23:12:39.654046: step 5401, loss 0.578935.
Train: 2018-08-01T23:12:39.822594: step 5402, loss 0.595074.
Train: 2018-08-01T23:12:39.989149: step 5403, loss 0.514387.
Train: 2018-08-01T23:12:40.155730: step 5404, loss 0.498167.
Train: 2018-08-01T23:12:40.319266: step 5405, loss 0.497991.
Train: 2018-08-01T23:12:40.481832: step 5406, loss 0.562717.
Train: 2018-08-01T23:12:40.645395: step 5407, loss 0.530084.
Train: 2018-08-01T23:12:40.807986: step 5408, loss 0.579001.
Train: 2018-08-01T23:12:40.971554: step 5409, loss 0.661066.
Train: 2018-08-01T23:12:41.135086: step 5410, loss 0.677617.
Test: 2018-08-01T23:12:41.669687: step 5410, loss 0.548853.
Train: 2018-08-01T23:12:41.842195: step 5411, loss 0.546183.
Train: 2018-08-01T23:12:42.004791: step 5412, loss 0.562605.
Train: 2018-08-01T23:12:42.170317: step 5413, loss 0.496915.
Train: 2018-08-01T23:12:42.345880: step 5414, loss 0.579036.
Train: 2018-08-01T23:12:42.509413: step 5415, loss 0.611968.
Train: 2018-08-01T23:12:42.671014: step 5416, loss 0.595513.
Train: 2018-08-01T23:12:42.837567: step 5417, loss 0.513187.
Train: 2018-08-01T23:12:43.000100: step 5418, loss 0.421957.
Train: 2018-08-01T23:12:43.163662: step 5419, loss 0.479888.
Train: 2018-08-01T23:12:43.344180: step 5420, loss 0.562514.
Test: 2018-08-01T23:12:43.890752: step 5420, loss 0.548527.
Train: 2018-08-01T23:12:44.062260: step 5421, loss 0.729198.
Train: 2018-08-01T23:12:44.226820: step 5422, loss 0.595857.
Train: 2018-08-01T23:12:44.393406: step 5423, loss 0.462299.
Train: 2018-08-01T23:12:44.564917: step 5424, loss 0.495537.
Train: 2018-08-01T23:12:44.731503: step 5425, loss 0.579233.
Train: 2018-08-01T23:12:44.902017: step 5426, loss 0.495131.
Train: 2018-08-01T23:12:45.065580: step 5427, loss 0.478001.
Train: 2018-08-01T23:12:45.233131: step 5428, loss 0.57937.
Train: 2018-08-01T23:12:45.407663: step 5429, loss 0.562402.
Train: 2018-08-01T23:12:45.569258: step 5430, loss 0.664889.
Test: 2018-08-01T23:12:46.104717: step 5430, loss 0.548071.
Train: 2018-08-01T23:12:46.271242: step 5431, loss 0.493966.
Train: 2018-08-01T23:12:46.447770: step 5432, loss 0.648139.
Train: 2018-08-01T23:12:46.610334: step 5433, loss 0.579559.
Train: 2018-08-01T23:12:46.776890: step 5434, loss 0.59674.
Train: 2018-08-01T23:12:46.936494: step 5435, loss 0.442198.
Train: 2018-08-01T23:12:47.101024: step 5436, loss 0.527994.
Train: 2018-08-01T23:12:47.262591: step 5437, loss 0.458975.
Train: 2018-08-01T23:12:47.425156: step 5438, loss 0.510515.
Train: 2018-08-01T23:12:47.592735: step 5439, loss 0.54505.
Train: 2018-08-01T23:12:47.757269: step 5440, loss 0.579848.
Test: 2018-08-01T23:12:48.295860: step 5440, loss 0.5478.
Train: 2018-08-01T23:12:48.460389: step 5441, loss 0.457578.
Train: 2018-08-01T23:12:48.625978: step 5442, loss 0.439619.
Train: 2018-08-01T23:12:48.796515: step 5443, loss 0.527201.
Train: 2018-08-01T23:12:48.974043: step 5444, loss 0.580257.
Train: 2018-08-01T23:12:49.134587: step 5445, loss 0.526937.
Train: 2018-08-01T23:12:49.304134: step 5446, loss 0.437392.
Train: 2018-08-01T23:12:49.471711: step 5447, loss 0.580639.
Train: 2018-08-01T23:12:49.632257: step 5448, loss 0.490456.
Train: 2018-08-01T23:12:49.795819: step 5449, loss 0.58092.
Train: 2018-08-01T23:12:49.965366: step 5450, loss 0.581045.
Test: 2018-08-01T23:12:50.511907: step 5450, loss 0.547567.
Train: 2018-08-01T23:12:50.681483: step 5451, loss 0.63597.
Train: 2018-08-01T23:12:50.843051: step 5452, loss 0.562894.
Train: 2018-08-01T23:12:51.005586: step 5453, loss 0.617886.
Train: 2018-08-01T23:12:51.174160: step 5454, loss 0.507936.
Train: 2018-08-01T23:12:51.334705: step 5455, loss 0.507912.
Train: 2018-08-01T23:12:51.501261: step 5456, loss 0.526228.
Train: 2018-08-01T23:12:51.662843: step 5457, loss 0.562963.
Train: 2018-08-01T23:12:51.831406: step 5458, loss 0.562978.
Train: 2018-08-01T23:12:51.991961: step 5459, loss 0.50777.
Train: 2018-08-01T23:12:52.153549: step 5460, loss 0.581429.
Test: 2018-08-01T23:12:52.698065: step 5460, loss 0.547577.
Train: 2018-08-01T23:12:52.860654: step 5461, loss 0.581445.
Train: 2018-08-01T23:12:53.025213: step 5462, loss 0.636733.
Train: 2018-08-01T23:12:53.188778: step 5463, loss 0.562988.
Train: 2018-08-01T23:12:53.353340: step 5464, loss 0.544583.
Train: 2018-08-01T23:12:53.518867: step 5465, loss 0.544585.
Train: 2018-08-01T23:12:53.684455: step 5466, loss 0.581265.
Train: 2018-08-01T23:12:53.845993: step 5467, loss 0.489652.
Train: 2018-08-01T23:12:54.007560: step 5468, loss 0.544591.
Train: 2018-08-01T23:12:54.169129: step 5469, loss 0.526298.
Train: 2018-08-01T23:12:54.332691: step 5470, loss 0.581178.
Test: 2018-08-01T23:12:54.874276: step 5470, loss 0.547567.
Train: 2018-08-01T23:12:55.056755: step 5471, loss 0.581159.
Train: 2018-08-01T23:12:55.221341: step 5472, loss 0.581126.
Train: 2018-08-01T23:12:55.391891: step 5473, loss 0.581082.
Train: 2018-08-01T23:12:55.561407: step 5474, loss 0.435344.
Train: 2018-08-01T23:12:55.723006: step 5475, loss 0.562818.
Train: 2018-08-01T23:12:55.884543: step 5476, loss 0.50819.
Train: 2018-08-01T23:12:56.048105: step 5477, loss 0.508173.
Train: 2018-08-01T23:12:56.218649: step 5478, loss 0.654.
Train: 2018-08-01T23:12:56.387199: step 5479, loss 0.672157.
Train: 2018-08-01T23:12:56.555748: step 5480, loss 0.508251.
Test: 2018-08-01T23:12:57.102288: step 5480, loss 0.547572.
Train: 2018-08-01T23:12:57.275861: step 5481, loss 0.526465.
Train: 2018-08-01T23:12:57.444405: step 5482, loss 0.544624.
Train: 2018-08-01T23:12:57.613946: step 5483, loss 0.580871.
Train: 2018-08-01T23:12:57.790449: step 5484, loss 0.472235.
Train: 2018-08-01T23:12:57.953046: step 5485, loss 0.490335.
Train: 2018-08-01T23:12:58.117573: step 5486, loss 0.490288.
Train: 2018-08-01T23:12:58.288117: step 5487, loss 0.508339.
Train: 2018-08-01T23:12:58.452703: step 5488, loss 0.617323.
Train: 2018-08-01T23:12:58.619263: step 5489, loss 0.580995.
Train: 2018-08-01T23:12:58.784790: step 5490, loss 0.635595.
Test: 2018-08-01T23:12:59.317367: step 5490, loss 0.54757.
Train: 2018-08-01T23:12:59.485940: step 5491, loss 0.635515.
Train: 2018-08-01T23:12:59.658455: step 5492, loss 0.526478.
Train: 2018-08-01T23:12:59.828037: step 5493, loss 0.490279.
Train: 2018-08-01T23:12:59.993591: step 5494, loss 0.490311.
Train: 2018-08-01T23:13:00.156148: step 5495, loss 0.508405.
Train: 2018-08-01T23:13:00.318715: step 5496, loss 0.490241.
Train: 2018-08-01T23:13:00.482282: step 5497, loss 0.635404.
Train: 2018-08-01T23:13:00.644818: step 5498, loss 0.617263.
Train: 2018-08-01T23:13:00.809409: step 5499, loss 0.56277.
Train: 2018-08-01T23:13:00.972972: step 5500, loss 0.453957.
Test: 2018-08-01T23:13:01.500529: step 5500, loss 0.547573.
Train: 2018-08-01T23:13:02.287261: step 5501, loss 0.526479.
Train: 2018-08-01T23:13:02.451791: step 5502, loss 0.453827.
Train: 2018-08-01T23:13:02.616350: step 5503, loss 0.581.
Train: 2018-08-01T23:13:02.782906: step 5504, loss 0.708587.
Train: 2018-08-01T23:13:02.945501: step 5505, loss 0.635625.
Train: 2018-08-01T23:13:03.109034: step 5506, loss 0.617284.
Train: 2018-08-01T23:13:03.273593: step 5507, loss 0.653332.
Train: 2018-08-01T23:13:03.437189: step 5508, loss 0.54465.
Train: 2018-08-01T23:13:03.601718: step 5509, loss 0.490717.
Train: 2018-08-01T23:13:03.773258: step 5510, loss 0.526743.
Test: 2018-08-01T23:13:04.308855: step 5510, loss 0.547613.
Train: 2018-08-01T23:13:04.475412: step 5511, loss 0.419301.
Train: 2018-08-01T23:13:04.639972: step 5512, loss 0.580538.
Train: 2018-08-01T23:13:04.806496: step 5513, loss 0.508862.
Train: 2018-08-01T23:13:04.977071: step 5514, loss 0.598477.
Train: 2018-08-01T23:13:05.138644: step 5515, loss 0.472998.
Train: 2018-08-01T23:13:05.311180: step 5516, loss 0.56263.
Train: 2018-08-01T23:13:05.473744: step 5517, loss 0.52673.
Train: 2018-08-01T23:13:05.638272: step 5518, loss 0.544677.
Train: 2018-08-01T23:13:05.799840: step 5519, loss 0.54467.
Train: 2018-08-01T23:13:05.974375: step 5520, loss 0.634694.
Test: 2018-08-01T23:13:06.514929: step 5520, loss 0.547593.
Train: 2018-08-01T23:13:06.685473: step 5521, loss 0.598669.
Train: 2018-08-01T23:13:06.846044: step 5522, loss 0.634594.
Train: 2018-08-01T23:13:07.006645: step 5523, loss 0.508788.
Train: 2018-08-01T23:13:07.172196: step 5524, loss 0.634333.
Train: 2018-08-01T23:13:07.348699: step 5525, loss 0.526823.
Train: 2018-08-01T23:13:07.516252: step 5526, loss 0.491147.
Train: 2018-08-01T23:13:07.678817: step 5527, loss 0.616123.
Train: 2018-08-01T23:13:07.848397: step 5528, loss 0.616034.
Train: 2018-08-01T23:13:08.013923: step 5529, loss 0.633683.
Train: 2018-08-01T23:13:08.178482: step 5530, loss 0.562521.
Test: 2018-08-01T23:13:08.732017: step 5530, loss 0.547696.
Train: 2018-08-01T23:13:08.901578: step 5531, loss 0.544821.
Train: 2018-08-01T23:13:09.065136: step 5532, loss 0.597753.
Train: 2018-08-01T23:13:09.227708: step 5533, loss 0.439368.
Train: 2018-08-01T23:13:09.389271: step 5534, loss 0.509749.
Train: 2018-08-01T23:13:09.550844: step 5535, loss 0.615168.
Train: 2018-08-01T23:13:09.717369: step 5536, loss 0.720434.
Train: 2018-08-01T23:13:09.881928: step 5537, loss 0.544943.
Train: 2018-08-01T23:13:10.055483: step 5538, loss 0.63223.
Train: 2018-08-01T23:13:10.230996: step 5539, loss 0.458081.
Train: 2018-08-01T23:13:10.404531: step 5540, loss 0.597132.
Test: 2018-08-01T23:13:10.955091: step 5540, loss 0.547899.
Train: 2018-08-01T23:13:11.124637: step 5541, loss 0.631705.
Train: 2018-08-01T23:13:11.292185: step 5542, loss 0.579675.
Train: 2018-08-01T23:13:11.454724: step 5543, loss 0.596845.
Train: 2018-08-01T23:13:11.632250: step 5544, loss 0.47655.
Train: 2018-08-01T23:13:11.796809: step 5545, loss 0.493826.
Train: 2018-08-01T23:13:11.976331: step 5546, loss 0.579532.
Train: 2018-08-01T23:13:12.144878: step 5547, loss 0.596648.
Train: 2018-08-01T23:13:12.316419: step 5548, loss 0.52818.
Train: 2018-08-01T23:13:12.480980: step 5549, loss 0.511092.
Train: 2018-08-01T23:13:12.661528: step 5550, loss 0.630831.
Test: 2018-08-01T23:13:13.194074: step 5550, loss 0.54808.
Train: 2018-08-01T23:13:13.359631: step 5551, loss 0.511104.
Train: 2018-08-01T23:13:13.519204: step 5552, loss 0.665004.
Train: 2018-08-01T23:13:13.691744: step 5553, loss 0.562398.
Train: 2018-08-01T23:13:13.851317: step 5554, loss 0.647685.
Train: 2018-08-01T23:13:14.017905: step 5555, loss 0.545386.
Train: 2018-08-01T23:13:14.192413: step 5556, loss 0.528437.
Train: 2018-08-01T23:13:14.354969: step 5557, loss 0.528477.
Train: 2018-08-01T23:13:14.531498: step 5558, loss 0.613287.
Train: 2018-08-01T23:13:14.698053: step 5559, loss 0.528535.
Train: 2018-08-01T23:13:14.871620: step 5560, loss 0.562415.
Test: 2018-08-01T23:13:15.416134: step 5560, loss 0.548243.
Train: 2018-08-01T23:13:15.584714: step 5561, loss 0.545491.
Train: 2018-08-01T23:13:15.754229: step 5562, loss 0.528567.
Train: 2018-08-01T23:13:15.910810: step 5563, loss 0.613217.
Train: 2018-08-01T23:13:16.091360: step 5564, loss 0.528555.
Train: 2018-08-01T23:13:16.259909: step 5565, loss 0.528542.
Train: 2018-08-01T23:13:16.435408: step 5566, loss 0.52851.
Train: 2018-08-01T23:13:16.594993: step 5567, loss 0.630305.
Train: 2018-08-01T23:13:16.757575: step 5568, loss 0.579386.
Train: 2018-08-01T23:13:16.924133: step 5569, loss 0.579385.
Train: 2018-08-01T23:13:17.083707: step 5570, loss 0.596354.
Test: 2018-08-01T23:13:17.634205: step 5570, loss 0.548208.
Train: 2018-08-01T23:13:17.799760: step 5571, loss 0.579371.
Train: 2018-08-01T23:13:17.972328: step 5572, loss 0.579359.
Train: 2018-08-01T23:13:18.136860: step 5573, loss 0.680916.
Train: 2018-08-01T23:13:18.311395: step 5574, loss 0.579307.
Train: 2018-08-01T23:13:18.473984: step 5575, loss 0.646624.
Train: 2018-08-01T23:13:18.641512: step 5576, loss 0.512123.
Train: 2018-08-01T23:13:18.803080: step 5577, loss 0.595935.
Train: 2018-08-01T23:13:18.964647: step 5578, loss 0.57917.
Train: 2018-08-01T23:13:19.135192: step 5579, loss 0.545847.
Train: 2018-08-01T23:13:19.299778: step 5580, loss 0.579125.
Test: 2018-08-01T23:13:19.839311: step 5580, loss 0.548634.
Train: 2018-08-01T23:13:20.001901: step 5581, loss 0.529352.
Train: 2018-08-01T23:13:20.169427: step 5582, loss 0.529395.
Train: 2018-08-01T23:13:20.339004: step 5583, loss 0.512844.
Train: 2018-08-01T23:13:20.498547: step 5584, loss 0.678545.
Train: 2018-08-01T23:13:20.666098: step 5585, loss 0.529426.
Train: 2018-08-01T23:13:20.828696: step 5586, loss 0.562538.
Train: 2018-08-01T23:13:20.991230: step 5587, loss 0.52945.
Train: 2018-08-01T23:13:21.152817: step 5588, loss 0.52943.
Train: 2018-08-01T23:13:21.318380: step 5589, loss 0.496242.
Train: 2018-08-01T23:13:21.482916: step 5590, loss 0.562512.
Test: 2018-08-01T23:13:22.023470: step 5590, loss 0.548562.
Train: 2018-08-01T23:13:22.189029: step 5591, loss 0.728923.
Train: 2018-08-01T23:13:22.351622: step 5592, loss 0.462718.
Train: 2018-08-01T23:13:22.514189: step 5593, loss 0.429306.
Train: 2018-08-01T23:13:22.677752: step 5594, loss 0.44556.
Train: 2018-08-01T23:13:22.840287: step 5595, loss 0.579232.
Train: 2018-08-01T23:13:23.008838: step 5596, loss 0.562431.
Train: 2018-08-01T23:13:23.177387: step 5597, loss 0.511669.
Train: 2018-08-01T23:13:23.346933: step 5598, loss 0.61337.
Train: 2018-08-01T23:13:23.508534: step 5599, loss 0.57944.
Train: 2018-08-01T23:13:23.673091: step 5600, loss 0.528234.
Test: 2018-08-01T23:13:24.211621: step 5600, loss 0.548052.
Train: 2018-08-01T23:13:24.966981: step 5601, loss 0.665172.
Train: 2018-08-01T23:13:25.131572: step 5602, loss 0.528105.
Train: 2018-08-01T23:13:25.305082: step 5603, loss 0.459384.
Train: 2018-08-01T23:13:25.469668: step 5604, loss 0.4591.
Train: 2018-08-01T23:13:25.634197: step 5605, loss 0.666115.
Train: 2018-08-01T23:13:25.812738: step 5606, loss 0.597047.
Train: 2018-08-01T23:13:25.978302: step 5607, loss 0.631781.
Train: 2018-08-01T23:13:26.145864: step 5608, loss 0.579754.
Train: 2018-08-01T23:13:26.313412: step 5609, loss 0.597091.
Train: 2018-08-01T23:13:26.481930: step 5610, loss 0.63172.
Test: 2018-08-01T23:13:27.022486: step 5610, loss 0.547918.
Train: 2018-08-01T23:13:27.189040: step 5611, loss 0.614298.
Train: 2018-08-01T23:13:27.355601: step 5612, loss 0.596915.
Train: 2018-08-01T23:13:27.543093: step 5613, loss 0.614032.
Train: 2018-08-01T23:13:27.708651: step 5614, loss 0.579553.
Train: 2018-08-01T23:13:27.911110: step 5615, loss 0.665015.
Train: 2018-08-01T23:13:28.121549: step 5616, loss 0.511317.
Train: 2018-08-01T23:13:28.294086: step 5617, loss 0.511484.
Train: 2018-08-01T23:13:28.499538: step 5618, loss 0.630175.
Train: 2018-08-01T23:13:28.663124: step 5619, loss 0.596207.
Train: 2018-08-01T23:13:28.824681: step 5620, loss 0.528751.
Test: 2018-08-01T23:13:29.369212: step 5620, loss 0.548371.
Train: 2018-08-01T23:13:29.544742: step 5621, loss 0.596054.
Train: 2018-08-01T23:13:29.711330: step 5622, loss 0.679811.
Train: 2018-08-01T23:13:29.884834: step 5623, loss 0.529078.
Train: 2018-08-01T23:13:30.049425: step 5624, loss 0.612446.
Train: 2018-08-01T23:13:30.214952: step 5625, loss 0.579112.
Train: 2018-08-01T23:13:30.378546: step 5626, loss 0.579084.
Train: 2018-08-01T23:13:30.551053: step 5627, loss 0.562566.
Train: 2018-08-01T23:13:30.724613: step 5628, loss 0.595489.
Train: 2018-08-01T23:13:30.897128: step 5629, loss 0.529802.
Train: 2018-08-01T23:13:31.075650: step 5630, loss 0.447991.
Test: 2018-08-01T23:13:31.609224: step 5630, loss 0.548905.
Train: 2018-08-01T23:13:31.785753: step 5631, loss 0.644552.
Train: 2018-08-01T23:13:31.958315: step 5632, loss 0.529888.
Train: 2018-08-01T23:13:32.121873: step 5633, loss 0.562633.
Train: 2018-08-01T23:13:32.290404: step 5634, loss 0.644515.
Train: 2018-08-01T23:13:32.459950: step 5635, loss 0.546281.
Train: 2018-08-01T23:13:32.656426: step 5636, loss 0.51359.
Train: 2018-08-01T23:13:32.832954: step 5637, loss 0.529918.
Train: 2018-08-01T23:13:33.002500: step 5638, loss 0.529866.
Train: 2018-08-01T23:13:33.169054: step 5639, loss 0.595433.
Train: 2018-08-01T23:13:33.346580: step 5640, loss 0.595461.
Test: 2018-08-01T23:13:33.891124: step 5640, loss 0.548824.
Train: 2018-08-01T23:13:34.056706: step 5641, loss 0.54615.
Train: 2018-08-01T23:13:34.226229: step 5642, loss 0.611964.
Train: 2018-08-01T23:13:34.389790: step 5643, loss 0.628441.
Train: 2018-08-01T23:13:34.570308: step 5644, loss 0.513224.
Train: 2018-08-01T23:13:34.735865: step 5645, loss 0.463824.
Train: 2018-08-01T23:13:34.901423: step 5646, loss 0.562565.
Train: 2018-08-01T23:13:35.071001: step 5647, loss 0.595604.
Train: 2018-08-01T23:13:35.237562: step 5648, loss 0.595639.
Train: 2018-08-01T23:13:35.411060: step 5649, loss 0.512836.
Train: 2018-08-01T23:13:35.581629: step 5650, loss 0.595704.
Test: 2018-08-01T23:13:36.111190: step 5650, loss 0.5486.
Train: 2018-08-01T23:13:36.284725: step 5651, loss 0.545899.
Train: 2018-08-01T23:13:36.461280: step 5652, loss 0.479331.
Train: 2018-08-01T23:13:36.633793: step 5653, loss 0.495769.
Train: 2018-08-01T23:13:36.807328: step 5654, loss 0.528986.
Train: 2018-08-01T23:13:36.981861: step 5655, loss 0.629654.
Train: 2018-08-01T23:13:37.145424: step 5656, loss 0.646651.
Train: 2018-08-01T23:13:37.309013: step 5657, loss 0.579289.
Train: 2018-08-01T23:13:37.473547: step 5658, loss 0.461208.
Train: 2018-08-01T23:13:37.646088: step 5659, loss 0.596234.
Train: 2018-08-01T23:13:37.883451: step 5660, loss 0.545482.
Test: 2018-08-01T23:13:38.421015: step 5660, loss 0.548207.
Train: 2018-08-01T23:13:38.591596: step 5661, loss 0.596332.
Train: 2018-08-01T23:13:38.757147: step 5662, loss 0.494495.
Train: 2018-08-01T23:13:38.930653: step 5663, loss 0.562404.
Train: 2018-08-01T23:13:39.101195: step 5664, loss 0.477184.
Train: 2018-08-01T23:13:39.269746: step 5665, loss 0.562397.
Train: 2018-08-01T23:13:39.437330: step 5666, loss 0.562396.
Train: 2018-08-01T23:13:39.600861: step 5667, loss 0.528034.
Train: 2018-08-01T23:13:39.769442: step 5668, loss 0.631305.
Train: 2018-08-01T23:13:39.935965: step 5669, loss 0.562399.
Train: 2018-08-01T23:13:40.094567: step 5670, loss 0.579671.
Test: 2018-08-01T23:13:40.624125: step 5670, loss 0.547928.
Train: 2018-08-01T23:13:40.793672: step 5671, loss 0.527833.
Train: 2018-08-01T23:13:40.963243: step 5672, loss 0.441265.
Train: 2018-08-01T23:13:41.129773: step 5673, loss 0.545056.
Train: 2018-08-01T23:13:41.290374: step 5674, loss 0.510209.
Train: 2018-08-01T23:13:41.455901: step 5675, loss 0.544971.
Train: 2018-08-01T23:13:41.628441: step 5676, loss 0.544931.
Train: 2018-08-01T23:13:41.790008: step 5677, loss 0.615155.
Train: 2018-08-01T23:13:41.953590: step 5678, loss 0.56247.
Train: 2018-08-01T23:13:42.118131: step 5679, loss 0.597727.
Train: 2018-08-01T23:13:42.282733: step 5680, loss 0.562484.
Test: 2018-08-01T23:13:42.820255: step 5680, loss 0.547709.
Train: 2018-08-01T23:13:42.988830: step 5681, loss 0.580137.
Train: 2018-08-01T23:13:43.158350: step 5682, loss 0.56249.
Train: 2018-08-01T23:13:43.322935: step 5683, loss 0.491873.
Train: 2018-08-01T23:13:43.486484: step 5684, loss 0.580169.
Train: 2018-08-01T23:13:43.651034: step 5685, loss 0.562501.
Train: 2018-08-01T23:13:43.819608: step 5686, loss 0.562504.
Train: 2018-08-01T23:13:43.994147: step 5687, loss 0.562505.
Train: 2018-08-01T23:13:44.164685: step 5688, loss 0.615591.
Train: 2018-08-01T23:13:44.332213: step 5689, loss 0.527138.
Train: 2018-08-01T23:13:44.496798: step 5690, loss 0.633205.
Test: 2018-08-01T23:13:45.044309: step 5690, loss 0.547708.
Train: 2018-08-01T23:13:45.210863: step 5691, loss 0.633099.
Train: 2018-08-01T23:13:45.376447: step 5692, loss 0.632919.
Train: 2018-08-01T23:13:45.548960: step 5693, loss 0.562456.
Train: 2018-08-01T23:13:45.715527: step 5694, loss 0.474932.
Train: 2018-08-01T23:13:45.890079: step 5695, loss 0.579911.
Train: 2018-08-01T23:13:46.060623: step 5696, loss 0.510087.
Train: 2018-08-01T23:13:46.222185: step 5697, loss 0.579858.
Train: 2018-08-01T23:13:46.384751: step 5698, loss 0.562421.
Train: 2018-08-01T23:13:46.556269: step 5699, loss 0.597216.
Train: 2018-08-01T23:13:46.727808: step 5700, loss 0.666658.
Test: 2018-08-01T23:13:47.266370: step 5700, loss 0.547899.
Train: 2018-08-01T23:13:48.129408: step 5701, loss 0.614378.
Train: 2018-08-01T23:13:48.297957: step 5702, loss 0.52787.
Train: 2018-08-01T23:13:48.468502: step 5703, loss 0.579617.
Train: 2018-08-01T23:13:48.629098: step 5704, loss 0.562396.
Train: 2018-08-01T23:13:48.801611: step 5705, loss 0.545261.
Train: 2018-08-01T23:13:48.967168: step 5706, loss 0.528191.
Train: 2018-08-01T23:13:49.131754: step 5707, loss 0.579481.
Train: 2018-08-01T23:13:49.295291: step 5708, loss 0.596522.
Train: 2018-08-01T23:13:49.455895: step 5709, loss 0.596468.
Train: 2018-08-01T23:13:49.617431: step 5710, loss 0.511408.
Test: 2018-08-01T23:13:50.155990: step 5710, loss 0.548186.
Train: 2018-08-01T23:13:50.326554: step 5711, loss 0.494478.
Train: 2018-08-01T23:13:50.494088: step 5712, loss 0.562407.
Train: 2018-08-01T23:13:50.673606: step 5713, loss 0.681336.
Train: 2018-08-01T23:13:50.841184: step 5714, loss 0.579373.
Train: 2018-08-01T23:13:51.009708: step 5715, loss 0.562415.
Train: 2018-08-01T23:13:51.180265: step 5716, loss 0.663874.
Train: 2018-08-01T23:13:51.346822: step 5717, loss 0.646732.
Train: 2018-08-01T23:13:51.511395: step 5718, loss 0.663218.
Train: 2018-08-01T23:13:51.675955: step 5719, loss 0.544647.
Train: 2018-08-01T23:13:51.839515: step 5720, loss 0.545856.
Test: 2018-08-01T23:13:52.367080: step 5720, loss 0.548634.
Train: 2018-08-01T23:13:52.531639: step 5721, loss 0.595692.
Train: 2018-08-01T23:13:52.698195: step 5722, loss 0.595604.
Train: 2018-08-01T23:13:52.859763: step 5723, loss 0.562579.
Train: 2018-08-01T23:13:53.024356: step 5724, loss 0.644698.
Train: 2018-08-01T23:13:53.201858: step 5725, loss 0.595352.
Train: 2018-08-01T23:13:53.371395: step 5726, loss 0.530113.
Train: 2018-08-01T23:13:53.532996: step 5727, loss 0.481514.
Train: 2018-08-01T23:13:53.693581: step 5728, loss 0.578957.
Train: 2018-08-01T23:13:53.866073: step 5729, loss 0.546534.
Train: 2018-08-01T23:13:54.039608: step 5730, loss 0.530342.
Test: 2018-08-01T23:13:54.587145: step 5730, loss 0.54917.
Train: 2018-08-01T23:13:54.753724: step 5731, loss 0.562743.
Train: 2018-08-01T23:13:54.921252: step 5732, loss 0.578956.
Train: 2018-08-01T23:13:55.086808: step 5733, loss 0.643863.
Train: 2018-08-01T23:13:55.253364: step 5734, loss 0.5141.
Train: 2018-08-01T23:13:55.417924: step 5735, loss 0.627612.
Train: 2018-08-01T23:13:55.579518: step 5736, loss 0.530326.
Train: 2018-08-01T23:13:55.744083: step 5737, loss 0.562742.
Train: 2018-08-01T23:13:55.924569: step 5738, loss 0.546518.
Train: 2018-08-01T23:13:56.093150: step 5739, loss 0.546495.
Train: 2018-08-01T23:13:56.271642: step 5740, loss 0.595218.
Test: 2018-08-01T23:13:56.815219: step 5740, loss 0.549085.
Train: 2018-08-01T23:13:56.977780: step 5741, loss 0.530175.
Train: 2018-08-01T23:13:57.175257: step 5742, loss 0.530113.
Train: 2018-08-01T23:13:57.337792: step 5743, loss 0.464743.
Train: 2018-08-01T23:13:57.501355: step 5744, loss 0.579009.
Train: 2018-08-01T23:13:57.664917: step 5745, loss 0.546167.
Train: 2018-08-01T23:13:57.835461: step 5746, loss 0.56257.
Train: 2018-08-01T23:13:58.003013: step 5747, loss 0.546008.
Train: 2018-08-01T23:13:58.172560: step 5748, loss 0.612287.
Train: 2018-08-01T23:13:58.337121: step 5749, loss 0.496006.
Train: 2018-08-01T23:13:58.499717: step 5750, loss 0.479101.
Test: 2018-08-01T23:13:59.036252: step 5750, loss 0.548436.
Train: 2018-08-01T23:13:59.215773: step 5751, loss 0.579208.
Train: 2018-08-01T23:13:59.385317: step 5752, loss 0.478399.
Train: 2018-08-01T23:13:59.564840: step 5753, loss 0.511762.
Train: 2018-08-01T23:13:59.769316: step 5754, loss 0.545436.
Train: 2018-08-01T23:14:00.000675: step 5755, loss 0.528291.
Train: 2018-08-01T23:14:00.180194: step 5756, loss 0.630945.
Train: 2018-08-01T23:14:00.352741: step 5757, loss 0.596784.
Train: 2018-08-01T23:14:00.520285: step 5758, loss 0.579634.
Train: 2018-08-01T23:14:00.696812: step 5759, loss 0.5624.
Train: 2018-08-01T23:14:00.857399: step 5760, loss 0.527807.
Test: 2018-08-01T23:14:01.387993: step 5760, loss 0.547893.
Train: 2018-08-01T23:14:01.550556: step 5761, loss 0.475741.
Train: 2018-08-01T23:14:01.716115: step 5762, loss 0.492869.
Train: 2018-08-01T23:14:01.877680: step 5763, loss 0.667142.
Train: 2018-08-01T23:14:02.045232: step 5764, loss 0.492505.
Train: 2018-08-01T23:14:02.209767: step 5765, loss 0.579975.
Train: 2018-08-01T23:14:02.382306: step 5766, loss 0.509773.
Train: 2018-08-01T23:14:02.546898: step 5767, loss 0.544868.
Train: 2018-08-01T23:14:02.708445: step 5768, loss 0.615426.
Train: 2018-08-01T23:14:02.871997: step 5769, loss 0.562496.
Train: 2018-08-01T23:14:03.035593: step 5770, loss 0.597877.
Test: 2018-08-01T23:14:03.573123: step 5770, loss 0.54769.
Train: 2018-08-01T23:14:03.743668: step 5771, loss 0.491735.
Train: 2018-08-01T23:14:03.910250: step 5772, loss 0.473945.
Train: 2018-08-01T23:14:04.075780: step 5773, loss 0.598039.
Train: 2018-08-01T23:14:04.246324: step 5774, loss 0.615879.
Train: 2018-08-01T23:14:04.417886: step 5775, loss 0.526976.
Train: 2018-08-01T23:14:04.600376: step 5776, loss 0.473563.
Train: 2018-08-01T23:14:04.763967: step 5777, loss 0.562568.
Train: 2018-08-01T23:14:04.932489: step 5778, loss 0.509013.
Train: 2018-08-01T23:14:05.098059: step 5779, loss 0.473144.
Train: 2018-08-01T23:14:05.265599: step 5780, loss 0.598517.
Test: 2018-08-01T23:14:05.812139: step 5780, loss 0.547598.
Train: 2018-08-01T23:14:05.976698: step 5781, loss 0.598608.
Train: 2018-08-01T23:14:06.139291: step 5782, loss 0.454682.
Train: 2018-08-01T23:14:06.308843: step 5783, loss 0.58073.
Train: 2018-08-01T23:14:06.470378: step 5784, loss 0.562712.
Train: 2018-08-01T23:14:06.637930: step 5785, loss 0.508451.
Train: 2018-08-01T23:14:06.799529: step 5786, loss 0.599001.
Train: 2018-08-01T23:14:06.962094: step 5787, loss 0.526482.
Train: 2018-08-01T23:14:07.129646: step 5788, loss 0.599101.
Train: 2018-08-01T23:14:07.297168: step 5789, loss 0.471949.
Train: 2018-08-01T23:14:07.460730: step 5790, loss 0.671951.
Test: 2018-08-01T23:14:08.003279: step 5790, loss 0.54757.
Train: 2018-08-01T23:14:08.169835: step 5791, loss 0.544613.
Train: 2018-08-01T23:14:08.341376: step 5792, loss 0.471911.
Train: 2018-08-01T23:14:08.502971: step 5793, loss 0.508234.
Train: 2018-08-01T23:14:08.675509: step 5794, loss 0.599239.
Train: 2018-08-01T23:14:08.843035: step 5795, loss 0.544606.
Train: 2018-08-01T23:14:09.011584: step 5796, loss 0.508155.
Train: 2018-08-01T23:14:09.175173: step 5797, loss 0.544601.
Train: 2018-08-01T23:14:09.341703: step 5798, loss 0.654137.
Train: 2018-08-01T23:14:09.506302: step 5799, loss 0.508113.
Train: 2018-08-01T23:14:09.671823: step 5800, loss 0.526358.
Test: 2018-08-01T23:14:10.217361: step 5800, loss 0.547567.
Train: 2018-08-01T23:14:11.014240: step 5801, loss 0.526353.
Train: 2018-08-01T23:14:11.176806: step 5802, loss 0.435063.
Train: 2018-08-01T23:14:11.341389: step 5803, loss 0.508009.
Train: 2018-08-01T23:14:11.519888: step 5804, loss 0.581256.
Train: 2018-08-01T23:14:11.694422: step 5805, loss 0.471129.
Train: 2018-08-01T23:14:11.864966: step 5806, loss 0.526172.
Train: 2018-08-01T23:14:12.028554: step 5807, loss 0.67376.
Train: 2018-08-01T23:14:12.192123: step 5808, loss 0.618433.
Train: 2018-08-01T23:14:12.363632: step 5809, loss 0.526126.
Train: 2018-08-01T23:14:12.530213: step 5810, loss 0.618378.
Test: 2018-08-01T23:14:13.072748: step 5810, loss 0.547577.
Train: 2018-08-01T23:14:13.237297: step 5811, loss 0.599863.
Train: 2018-08-01T23:14:13.404875: step 5812, loss 0.654951.
Train: 2018-08-01T23:14:13.567445: step 5813, loss 0.562925.
Train: 2018-08-01T23:14:13.731975: step 5814, loss 0.599444.
Train: 2018-08-01T23:14:13.918476: step 5815, loss 0.562826.
Train: 2018-08-01T23:14:14.085031: step 5816, loss 0.617262.
Train: 2018-08-01T23:14:14.247631: step 5817, loss 0.598908.
Train: 2018-08-01T23:14:14.411185: step 5818, loss 0.544661.
Train: 2018-08-01T23:14:14.574722: step 5819, loss 0.598528.
Train: 2018-08-01T23:14:14.738285: step 5820, loss 0.508967.
Test: 2018-08-01T23:14:15.280852: step 5820, loss 0.547641.
Train: 2018-08-01T23:14:15.450381: step 5821, loss 0.491278.
Train: 2018-08-01T23:14:15.614967: step 5822, loss 0.633704.
Train: 2018-08-01T23:14:15.780533: step 5823, loss 0.544786.
Train: 2018-08-01T23:14:15.952040: step 5824, loss 0.633297.
Train: 2018-08-01T23:14:16.126575: step 5825, loss 0.597764.
Train: 2018-08-01T23:14:16.299142: step 5826, loss 0.615193.
Train: 2018-08-01T23:14:16.462676: step 5827, loss 0.579947.
Train: 2018-08-01T23:14:16.633244: step 5828, loss 0.562425.
Train: 2018-08-01T23:14:16.799804: step 5829, loss 0.45819.
Train: 2018-08-01T23:14:16.969346: step 5830, loss 0.458373.
Test: 2018-08-01T23:14:17.516866: step 5830, loss 0.54789.
Train: 2018-08-01T23:14:17.690393: step 5831, loss 0.54507.
Train: 2018-08-01T23:14:17.852958: step 5832, loss 0.545068.
Train: 2018-08-01T23:14:18.034472: step 5833, loss 0.527717.
Train: 2018-08-01T23:14:18.200032: step 5834, loss 0.579772.
Train: 2018-08-01T23:14:18.356645: step 5835, loss 0.597148.
Train: 2018-08-01T23:14:18.518180: step 5836, loss 0.597141.
Train: 2018-08-01T23:14:18.687753: step 5837, loss 0.510355.
Train: 2018-08-01T23:14:18.861263: step 5838, loss 0.527704.
Train: 2018-08-01T23:14:19.031835: step 5839, loss 0.562412.
Train: 2018-08-01T23:14:19.198361: step 5840, loss 0.545042.
Test: 2018-08-01T23:14:19.745899: step 5840, loss 0.547859.
Train: 2018-08-01T23:14:19.918437: step 5841, loss 0.527651.
Train: 2018-08-01T23:14:20.092970: step 5842, loss 0.649421.
Train: 2018-08-01T23:14:20.265509: step 5843, loss 0.47545.
Train: 2018-08-01T23:14:20.435056: step 5844, loss 0.56242.
Train: 2018-08-01T23:14:20.604602: step 5845, loss 0.545001.
Train: 2018-08-01T23:14:20.783631: step 5846, loss 0.544988.
Train: 2018-08-01T23:14:20.948190: step 5847, loss 0.597339.
Train: 2018-08-01T23:14:21.121727: step 5848, loss 0.684653.
Train: 2018-08-01T23:14:21.288282: step 5849, loss 0.510128.
Train: 2018-08-01T23:14:21.468798: step 5850, loss 0.597262.
Test: 2018-08-01T23:14:22.011348: step 5850, loss 0.547849.
Train: 2018-08-01T23:14:22.175908: step 5851, loss 0.52762.
Train: 2018-08-01T23:14:22.342464: step 5852, loss 0.631968.
Train: 2018-08-01T23:14:22.504062: step 5853, loss 0.649207.
Train: 2018-08-01T23:14:22.668592: step 5854, loss 0.648953.
Train: 2018-08-01T23:14:22.834148: step 5855, loss 0.631367.
Train: 2018-08-01T23:14:22.996715: step 5856, loss 0.510907.
Train: 2018-08-01T23:14:23.162297: step 5857, loss 0.562397.
Train: 2018-08-01T23:14:23.328827: step 5858, loss 0.613559.
Train: 2018-08-01T23:14:23.511338: step 5859, loss 0.579399.
Train: 2018-08-01T23:14:23.683912: step 5860, loss 0.562414.
Test: 2018-08-01T23:14:24.232442: step 5860, loss 0.548286.
Train: 2018-08-01T23:14:24.396989: step 5861, loss 0.562424.
Train: 2018-08-01T23:14:24.579514: step 5862, loss 0.61295.
Train: 2018-08-01T23:14:24.741053: step 5863, loss 0.562449.
Train: 2018-08-01T23:14:24.918575: step 5864, loss 0.528986.
Train: 2018-08-01T23:14:25.093110: step 5865, loss 0.612594.
Train: 2018-08-01T23:14:25.259665: step 5866, loss 0.529155.
Train: 2018-08-01T23:14:25.423227: step 5867, loss 0.59578.
Train: 2018-08-01T23:14:25.585793: step 5868, loss 0.612343.
Train: 2018-08-01T23:14:25.748358: step 5869, loss 0.612243.
Train: 2018-08-01T23:14:25.918903: step 5870, loss 0.512976.
Test: 2018-08-01T23:14:26.455493: step 5870, loss 0.548747.
Train: 2018-08-01T23:14:26.619055: step 5871, loss 0.678054.
Train: 2018-08-01T23:14:26.786616: step 5872, loss 0.546146.
Train: 2018-08-01T23:14:26.950145: step 5873, loss 0.529805.
Train: 2018-08-01T23:14:27.113734: step 5874, loss 0.644529.
Train: 2018-08-01T23:14:27.280295: step 5875, loss 0.546318.
Train: 2018-08-01T23:14:27.461777: step 5876, loss 0.562676.
Train: 2018-08-01T23:14:27.638305: step 5877, loss 0.595257.
Train: 2018-08-01T23:14:27.799874: step 5878, loss 0.51395.
Train: 2018-08-01T23:14:27.961442: step 5879, loss 0.578963.
Train: 2018-08-01T23:14:28.129992: step 5880, loss 0.562724.
Test: 2018-08-01T23:14:28.665559: step 5880, loss 0.549135.
Train: 2018-08-01T23:14:28.829122: step 5881, loss 0.562727.
Train: 2018-08-01T23:14:28.991713: step 5882, loss 0.59519.
Train: 2018-08-01T23:14:29.170210: step 5883, loss 0.432941.
Train: 2018-08-01T23:14:29.344744: step 5884, loss 0.513936.
Train: 2018-08-01T23:14:29.513294: step 5885, loss 0.595287.
Train: 2018-08-01T23:14:29.681875: step 5886, loss 0.562653.
Train: 2018-08-01T23:14:29.844408: step 5887, loss 0.497122.
Train: 2018-08-01T23:14:30.011017: step 5888, loss 0.5626.
Train: 2018-08-01T23:14:30.172532: step 5889, loss 0.463687.
Train: 2018-08-01T23:14:30.335096: step 5890, loss 0.496311.
Test: 2018-08-01T23:14:30.879640: step 5890, loss 0.54856.
Train: 2018-08-01T23:14:31.045199: step 5891, loss 0.529209.
Train: 2018-08-01T23:14:31.217738: step 5892, loss 0.562465.
Train: 2018-08-01T23:14:31.389279: step 5893, loss 0.596073.
Train: 2018-08-01T23:14:31.551844: step 5894, loss 0.545541.
Train: 2018-08-01T23:14:31.712463: step 5895, loss 0.596309.
Train: 2018-08-01T23:14:31.887946: step 5896, loss 0.579404.
Train: 2018-08-01T23:14:32.052506: step 5897, loss 0.596481.
Train: 2018-08-01T23:14:32.227071: step 5898, loss 0.716018.
Train: 2018-08-01T23:14:32.405561: step 5899, loss 0.545346.
Train: 2018-08-01T23:14:32.573113: step 5900, loss 0.545355.
Test: 2018-08-01T23:14:33.113668: step 5900, loss 0.54813.
Train: 2018-08-01T23:14:33.939537: step 5901, loss 0.613527.
Train: 2018-08-01T23:14:34.110048: step 5902, loss 0.545376.
Train: 2018-08-01T23:14:34.281588: step 5903, loss 0.511352.
Train: 2018-08-01T23:14:34.447146: step 5904, loss 0.528356.
Train: 2018-08-01T23:14:34.616724: step 5905, loss 0.64759.
Train: 2018-08-01T23:14:34.787243: step 5906, loss 0.511315.
Train: 2018-08-01T23:14:34.950834: step 5907, loss 0.562401.
Train: 2018-08-01T23:14:35.114363: step 5908, loss 0.460157.
Train: 2018-08-01T23:14:35.283910: step 5909, loss 0.630693.
Train: 2018-08-01T23:14:35.448494: step 5910, loss 0.596568.
Test: 2018-08-01T23:14:35.990021: step 5910, loss 0.548089.
Train: 2018-08-01T23:14:36.152620: step 5911, loss 0.613659.
Train: 2018-08-01T23:14:36.316181: step 5912, loss 0.562398.
Train: 2018-08-01T23:14:36.484724: step 5913, loss 0.562399.
Train: 2018-08-01T23:14:36.647265: step 5914, loss 0.511232.
Train: 2018-08-01T23:14:36.810827: step 5915, loss 0.494152.
Train: 2018-08-01T23:14:36.971398: step 5916, loss 0.494055.
Train: 2018-08-01T23:14:37.137966: step 5917, loss 0.716527.
Train: 2018-08-01T23:14:37.299521: step 5918, loss 0.511034.
Train: 2018-08-01T23:14:37.468070: step 5919, loss 0.613786.
Train: 2018-08-01T23:14:37.640619: step 5920, loss 0.511021.
Test: 2018-08-01T23:14:38.177201: step 5920, loss 0.548048.
Train: 2018-08-01T23:14:38.341734: step 5921, loss 0.528128.
Train: 2018-08-01T23:14:38.511282: step 5922, loss 0.562395.
Train: 2018-08-01T23:14:38.681844: step 5923, loss 0.425058.
Train: 2018-08-01T23:14:38.854365: step 5924, loss 0.596832.
Train: 2018-08-01T23:14:39.014961: step 5925, loss 0.545145.
Train: 2018-08-01T23:14:39.184515: step 5926, loss 0.545112.
Train: 2018-08-01T23:14:39.355027: step 5927, loss 0.666372.
Train: 2018-08-01T23:14:39.524598: step 5928, loss 0.475742.
Train: 2018-08-01T23:14:39.693154: step 5929, loss 0.597132.
Train: 2018-08-01T23:14:39.857710: step 5930, loss 0.614538.
Test: 2018-08-01T23:14:40.390260: step 5930, loss 0.547865.
Train: 2018-08-01T23:14:40.556814: step 5931, loss 0.579787.
Train: 2018-08-01T23:14:40.716415: step 5932, loss 0.527678.
Train: 2018-08-01T23:14:40.886956: step 5933, loss 0.631891.
Train: 2018-08-01T23:14:41.061464: step 5934, loss 0.545057.
Train: 2018-08-01T23:14:41.224030: step 5935, loss 0.614434.
Train: 2018-08-01T23:14:41.393602: step 5936, loss 0.493136.
Train: 2018-08-01T23:14:41.563123: step 5937, loss 0.545092.
Train: 2018-08-01T23:14:41.733667: step 5938, loss 0.545092.
Train: 2018-08-01T23:14:41.901220: step 5939, loss 0.510453.
Train: 2018-08-01T23:14:42.062787: step 5940, loss 0.458401.
Test: 2018-08-01T23:14:42.606334: step 5940, loss 0.547863.
Train: 2018-08-01T23:14:42.781897: step 5941, loss 0.423404.
Train: 2018-08-01T23:14:42.952409: step 5942, loss 0.632226.
Train: 2018-08-01T23:14:43.121986: step 5943, loss 0.579936.
Train: 2018-08-01T23:14:43.292506: step 5944, loss 0.632579.
Train: 2018-08-01T23:14:43.453081: step 5945, loss 0.509816.
Train: 2018-08-01T23:14:43.625611: step 5946, loss 0.492182.
Train: 2018-08-01T23:14:43.790170: step 5947, loss 0.421607.
Train: 2018-08-01T23:14:43.955727: step 5948, loss 0.580177.
Train: 2018-08-01T23:14:44.117295: step 5949, loss 0.527055.
Train: 2018-08-01T23:14:44.284848: step 5950, loss 0.562549.
Test: 2018-08-01T23:14:44.821643: step 5950, loss 0.547634.
Train: 2018-08-01T23:14:44.998760: step 5951, loss 0.633937.
Train: 2018-08-01T23:14:45.166863: step 5952, loss 0.598316.
Train: 2018-08-01T23:14:45.330427: step 5953, loss 0.562592.
Train: 2018-08-01T23:14:45.493989: step 5954, loss 0.59836.
Train: 2018-08-01T23:14:45.661542: step 5955, loss 0.687726.
Train: 2018-08-01T23:14:45.828095: step 5956, loss 0.598246.
Train: 2018-08-01T23:14:45.988666: step 5957, loss 0.63371.
Train: 2018-08-01T23:14:46.147242: step 5958, loss 0.527065.
Train: 2018-08-01T23:14:46.315825: step 5959, loss 0.5625.
Train: 2018-08-01T23:14:46.477360: step 5960, loss 0.650647.
Test: 2018-08-01T23:14:47.021931: step 5960, loss 0.54775.
Train: 2018-08-01T23:14:47.195472: step 5961, loss 0.527326.
Train: 2018-08-01T23:14:47.366009: step 5962, loss 0.474866.
Train: 2018-08-01T23:14:47.542513: step 5963, loss 0.527457.
Train: 2018-08-01T23:14:47.717046: step 5964, loss 0.457577.
Train: 2018-08-01T23:14:47.881139: step 5965, loss 0.597417.
Train: 2018-08-01T23:14:48.048188: step 5966, loss 0.632402.
Train: 2018-08-01T23:14:48.210781: step 5967, loss 0.59738.
Train: 2018-08-01T23:14:48.380299: step 5968, loss 0.632215.
Train: 2018-08-01T23:14:48.555866: step 5969, loss 0.579823.
Train: 2018-08-01T23:14:48.727372: step 5970, loss 0.579772.
Test: 2018-08-01T23:14:49.257970: step 5970, loss 0.547906.
Train: 2018-08-01T23:14:49.419553: step 5971, loss 0.493144.
Train: 2018-08-01T23:14:49.595083: step 5972, loss 0.596988.
Train: 2018-08-01T23:14:49.768614: step 5973, loss 0.614188.
Train: 2018-08-01T23:14:49.935165: step 5974, loss 0.527954.
Train: 2018-08-01T23:14:50.105688: step 5975, loss 0.493622.
Train: 2018-08-01T23:14:50.269249: step 5976, loss 0.648328.
Train: 2018-08-01T23:14:50.430850: step 5977, loss 0.562396.
Train: 2018-08-01T23:14:50.596375: step 5978, loss 0.528134.
Train: 2018-08-01T23:14:50.770909: step 5979, loss 0.528164.
Train: 2018-08-01T23:14:50.934472: step 5980, loss 0.647958.
Test: 2018-08-01T23:14:51.466051: step 5980, loss 0.54809.
Train: 2018-08-01T23:14:51.635617: step 5981, loss 0.442794.
Train: 2018-08-01T23:14:51.803150: step 5982, loss 0.545302.
Train: 2018-08-01T23:14:51.967710: step 5983, loss 0.511074.
Train: 2018-08-01T23:14:52.130276: step 5984, loss 0.510999.
Train: 2018-08-01T23:14:52.297853: step 5985, loss 0.63107.
Train: 2018-08-01T23:14:52.463385: step 5986, loss 0.579579.
Train: 2018-08-01T23:14:52.635959: step 5987, loss 0.631161.
Train: 2018-08-01T23:14:52.820430: step 5988, loss 0.528035.
Train: 2018-08-01T23:14:52.989977: step 5989, loss 0.631115.
Train: 2018-08-01T23:14:53.149564: step 5990, loss 0.596717.
Test: 2018-08-01T23:14:53.682159: step 5990, loss 0.548047.
Train: 2018-08-01T23:14:53.847684: step 5991, loss 0.545261.
Train: 2018-08-01T23:14:54.010278: step 5992, loss 0.647976.
Train: 2018-08-01T23:14:54.172848: step 5993, loss 0.442863.
Train: 2018-08-01T23:14:54.339370: step 5994, loss 0.545325.
Train: 2018-08-01T23:14:54.502932: step 5995, loss 0.511174.
Train: 2018-08-01T23:14:54.672480: step 5996, loss 0.630757.
Train: 2018-08-01T23:14:54.836042: step 5997, loss 0.613655.
Train: 2018-08-01T23:14:55.002597: step 5998, loss 0.54533.
Train: 2018-08-01T23:14:55.163166: step 5999, loss 0.562399.
Train: 2018-08-01T23:14:55.339696: step 6000, loss 0.715845.
Test: 2018-08-01T23:14:55.889226: step 6000, loss 0.548169.
Train: 2018-08-01T23:14:56.706777: step 6001, loss 0.545404.
Train: 2018-08-01T23:14:56.887274: step 6002, loss 0.579372.
Train: 2018-08-01T23:14:57.048840: step 6003, loss 0.528569.
Train: 2018-08-01T23:14:57.229370: step 6004, loss 0.646916.
Train: 2018-08-01T23:14:57.394899: step 6005, loss 0.646704.
Train: 2018-08-01T23:14:57.557456: step 6006, loss 0.646414.
Train: 2018-08-01T23:14:57.722041: step 6007, loss 0.512319.
Train: 2018-08-01T23:14:57.886615: step 6008, loss 0.695796.
Train: 2018-08-01T23:14:58.053131: step 6009, loss 0.595685.
Train: 2018-08-01T23:14:58.217693: step 6010, loss 0.546063.
Test: 2018-08-01T23:14:58.765227: step 6010, loss 0.548838.
Train: 2018-08-01T23:14:58.927793: step 6011, loss 0.595464.
Train: 2018-08-01T23:14:59.099368: step 6012, loss 0.628103.
Train: 2018-08-01T23:14:59.261924: step 6013, loss 0.464937.
Train: 2018-08-01T23:14:59.426459: step 6014, loss 0.546457.
Train: 2018-08-01T23:14:59.589025: step 6015, loss 0.514045.
Train: 2018-08-01T23:14:59.753619: step 6016, loss 0.6114.
Train: 2018-08-01T23:14:59.919143: step 6017, loss 0.530335.
Train: 2018-08-01T23:15:00.087722: step 6018, loss 0.546545.
Train: 2018-08-01T23:15:00.252283: step 6019, loss 0.562744.
Train: 2018-08-01T23:15:00.421824: step 6020, loss 0.528138.
Test: 2018-08-01T23:15:00.953378: step 6020, loss 0.549124.
Train: 2018-08-01T23:15:01.121957: step 6021, loss 0.546483.
Train: 2018-08-01T23:15:01.289479: step 6022, loss 0.481375.
Train: 2018-08-01T23:15:01.452070: step 6023, loss 0.578985.
Train: 2018-08-01T23:15:01.623612: step 6024, loss 0.562643.
Train: 2018-08-01T23:15:01.801139: step 6025, loss 0.546217.
Train: 2018-08-01T23:15:01.964674: step 6026, loss 0.611926.
Train: 2018-08-01T23:15:02.129236: step 6027, loss 0.480207.
Train: 2018-08-01T23:15:02.293795: step 6028, loss 0.546025.
Train: 2018-08-01T23:15:02.458355: step 6029, loss 0.545947.
Train: 2018-08-01T23:15:02.621948: step 6030, loss 0.54587.
Test: 2018-08-01T23:15:03.165492: step 6030, loss 0.548507.
Train: 2018-08-01T23:15:03.334038: step 6031, loss 0.612542.
Train: 2018-08-01T23:15:03.495616: step 6032, loss 0.595917.
Train: 2018-08-01T23:15:03.658177: step 6033, loss 0.57921.
Train: 2018-08-01T23:15:03.833690: step 6034, loss 0.461822.
Train: 2018-08-01T23:15:03.994248: step 6035, loss 0.629715.
Train: 2018-08-01T23:15:04.163796: step 6036, loss 0.478212.
Train: 2018-08-01T23:15:04.330350: step 6037, loss 0.629986.
Train: 2018-08-01T23:15:04.495906: step 6038, loss 0.613165.
Train: 2018-08-01T23:15:04.657475: step 6039, loss 0.528566.
Train: 2018-08-01T23:15:04.825056: step 6040, loss 0.579357.
Test: 2018-08-01T23:15:05.372564: step 6040, loss 0.548212.
Train: 2018-08-01T23:15:05.539149: step 6041, loss 0.630237.
Train: 2018-08-01T23:15:05.702714: step 6042, loss 0.681066.
Train: 2018-08-01T23:15:05.864250: step 6043, loss 0.511675.
Train: 2018-08-01T23:15:06.027812: step 6044, loss 0.545526.
Train: 2018-08-01T23:15:06.194400: step 6045, loss 0.629965.
Train: 2018-08-01T23:15:06.358926: step 6046, loss 0.663579.
Train: 2018-08-01T23:15:06.514511: step 6047, loss 0.427979.
Train: 2018-08-01T23:15:06.677107: step 6048, loss 0.495245.
Train: 2018-08-01T23:15:06.840639: step 6049, loss 0.596066.
Train: 2018-08-01T23:15:07.004202: step 6050, loss 0.596071.
Test: 2018-08-01T23:15:07.556724: step 6050, loss 0.548365.
Train: 2018-08-01T23:15:07.719290: step 6051, loss 0.629683.
Train: 2018-08-01T23:15:07.892826: step 6052, loss 0.562448.
Train: 2018-08-01T23:15:08.056414: step 6053, loss 0.545682.
Train: 2018-08-01T23:15:08.220950: step 6054, loss 0.512174.
Train: 2018-08-01T23:15:08.383514: step 6055, loss 0.595987.
Train: 2018-08-01T23:15:08.559079: step 6056, loss 0.428345.
Train: 2018-08-01T23:15:08.721611: step 6057, loss 0.495249.
Train: 2018-08-01T23:15:08.893177: step 6058, loss 0.562432.
Train: 2018-08-01T23:15:09.058743: step 6059, loss 0.596215.
Train: 2018-08-01T23:15:09.226281: step 6060, loss 0.579346.
Test: 2018-08-01T23:15:09.767840: step 6060, loss 0.548211.
Train: 2018-08-01T23:15:09.933402: step 6061, loss 0.613284.
Train: 2018-08-01T23:15:10.106908: step 6062, loss 0.528471.
Train: 2018-08-01T23:15:10.279447: step 6063, loss 0.579395.
Train: 2018-08-01T23:15:10.441046: step 6064, loss 0.494392.
Train: 2018-08-01T23:15:10.606605: step 6065, loss 0.59647.
Train: 2018-08-01T23:15:10.779111: step 6066, loss 0.528292.
Train: 2018-08-01T23:15:10.956635: step 6067, loss 0.630721.
Train: 2018-08-01T23:15:11.129175: step 6068, loss 0.596572.
Train: 2018-08-01T23:15:11.294733: step 6069, loss 0.49406.
Train: 2018-08-01T23:15:11.467273: step 6070, loss 0.699196.
Test: 2018-08-01T23:15:12.001843: step 6070, loss 0.548097.
Train: 2018-08-01T23:15:12.165437: step 6071, loss 0.528242.
Train: 2018-08-01T23:15:12.327995: step 6072, loss 0.579466.
Train: 2018-08-01T23:15:12.497550: step 6073, loss 0.528291.
Train: 2018-08-01T23:15:12.666108: step 6074, loss 0.647657.
Train: 2018-08-01T23:15:12.833650: step 6075, loss 0.494295.
Train: 2018-08-01T23:15:13.000206: step 6076, loss 0.613471.
Train: 2018-08-01T23:15:13.166754: step 6077, loss 0.579411.
Train: 2018-08-01T23:15:13.336275: step 6078, loss 0.494456.
Train: 2018-08-01T23:15:13.519784: step 6079, loss 0.579396.
Train: 2018-08-01T23:15:13.696312: step 6080, loss 0.528432.
Test: 2018-08-01T23:15:14.241853: step 6080, loss 0.548175.
Train: 2018-08-01T23:15:14.411402: step 6081, loss 0.613389.
Train: 2018-08-01T23:15:14.572995: step 6082, loss 0.698313.
Train: 2018-08-01T23:15:14.752488: step 6083, loss 0.57936.
Train: 2018-08-01T23:15:14.921040: step 6084, loss 0.596233.
Train: 2018-08-01T23:15:15.083248: step 6085, loss 0.613014.
Train: 2018-08-01T23:15:15.253790: step 6086, loss 0.59606.
Train: 2018-08-01T23:15:15.418344: step 6087, loss 0.662978.
Train: 2018-08-01T23:15:15.598838: step 6088, loss 0.579162.
Train: 2018-08-01T23:15:15.759407: step 6089, loss 0.496084.
Train: 2018-08-01T23:15:15.932945: step 6090, loss 0.512842.
Test: 2018-08-01T23:15:16.472506: step 6090, loss 0.548693.
Train: 2018-08-01T23:15:16.637061: step 6091, loss 0.546003.
Train: 2018-08-01T23:15:16.807627: step 6092, loss 0.645174.
Train: 2018-08-01T23:15:16.975158: step 6093, loss 0.496602.
Train: 2018-08-01T23:15:17.138720: step 6094, loss 0.579053.
Train: 2018-08-01T23:15:17.302314: step 6095, loss 0.562578.
Train: 2018-08-01T23:15:17.469861: step 6096, loss 0.496731.
Train: 2018-08-01T23:15:17.632432: step 6097, loss 0.546098.
Train: 2018-08-01T23:15:17.805937: step 6098, loss 0.480086.
Train: 2018-08-01T23:15:17.969525: step 6099, loss 0.546006.
Train: 2018-08-01T23:15:18.147025: step 6100, loss 0.579106.
Test: 2018-08-01T23:15:18.689574: step 6100, loss 0.548589.
Train: 2018-08-01T23:15:19.453661: step 6101, loss 0.562507.
Train: 2018-08-01T23:15:19.620190: step 6102, loss 0.512524.
Train: 2018-08-01T23:15:19.783787: step 6103, loss 0.512364.
Train: 2018-08-01T23:15:19.947350: step 6104, loss 0.646264.
Train: 2018-08-01T23:15:20.109882: step 6105, loss 0.545654.
Train: 2018-08-01T23:15:20.274442: step 6106, loss 0.478307.
Train: 2018-08-01T23:15:20.436010: step 6107, loss 0.511785.
Train: 2018-08-01T23:15:20.599590: step 6108, loss 0.664072.
Train: 2018-08-01T23:15:20.774106: step 6109, loss 0.579382.
Train: 2018-08-01T23:15:20.937668: step 6110, loss 0.596399.
Test: 2018-08-01T23:15:21.475232: step 6110, loss 0.54816.
Train: 2018-08-01T23:15:21.639792: step 6111, loss 0.511375.
Train: 2018-08-01T23:15:21.807344: step 6112, loss 0.579436.
Train: 2018-08-01T23:15:21.970907: step 6113, loss 0.528292.
Train: 2018-08-01T23:15:22.138491: step 6114, loss 0.511156.
Train: 2018-08-01T23:15:22.306035: step 6115, loss 0.647986.
Train: 2018-08-01T23:15:22.468607: step 6116, loss 0.545267.
Train: 2018-08-01T23:15:22.642144: step 6117, loss 0.545251.
Train: 2018-08-01T23:15:22.807669: step 6118, loss 0.528072.
Train: 2018-08-01T23:15:22.970235: step 6119, loss 0.665518.
Train: 2018-08-01T23:15:23.133829: step 6120, loss 0.596761.
Test: 2018-08-01T23:15:23.674352: step 6120, loss 0.548018.
Train: 2018-08-01T23:15:23.842939: step 6121, loss 0.510888.
Train: 2018-08-01T23:15:24.018458: step 6122, loss 0.596737.
Train: 2018-08-01T23:15:24.187014: step 6123, loss 0.510906.
Train: 2018-08-01T23:15:24.352539: step 6124, loss 0.579565.
Train: 2018-08-01T23:15:24.517100: step 6125, loss 0.579567.
Train: 2018-08-01T23:15:24.680690: step 6126, loss 0.562396.
Train: 2018-08-01T23:15:24.851232: step 6127, loss 0.562396.
Train: 2018-08-01T23:15:25.013799: step 6128, loss 0.59672.
Train: 2018-08-01T23:15:25.180327: step 6129, loss 0.596697.
Train: 2018-08-01T23:15:25.340929: step 6130, loss 0.511003.
Test: 2018-08-01T23:15:25.870482: step 6130, loss 0.548054.
Train: 2018-08-01T23:15:26.037094: step 6131, loss 0.562396.
Train: 2018-08-01T23:15:26.196635: step 6132, loss 0.459653.
Train: 2018-08-01T23:15:26.360204: step 6133, loss 0.545247.
Train: 2018-08-01T23:15:26.527726: step 6134, loss 0.476522.
Train: 2018-08-01T23:15:26.689324: step 6135, loss 0.510734.
Train: 2018-08-01T23:15:26.865845: step 6136, loss 0.596953.
Train: 2018-08-01T23:15:27.034372: step 6137, loss 0.527773.
Train: 2018-08-01T23:15:27.213890: step 6138, loss 0.649216.
Train: 2018-08-01T23:15:27.381474: step 6139, loss 0.562414.
Train: 2018-08-01T23:15:27.549026: step 6140, loss 0.597201.
Test: 2018-08-01T23:15:28.084565: step 6140, loss 0.547851.
Train: 2018-08-01T23:15:28.253143: step 6141, loss 0.545022.
Train: 2018-08-01T23:15:28.422684: step 6142, loss 0.492812.
Train: 2018-08-01T23:15:28.583260: step 6143, loss 0.510149.
Train: 2018-08-01T23:15:28.750782: step 6144, loss 0.56243.
Train: 2018-08-01T23:15:28.921358: step 6145, loss 0.475002.
Train: 2018-08-01T23:15:29.083891: step 6146, loss 0.56245.
Train: 2018-08-01T23:15:29.247454: step 6147, loss 0.439427.
Train: 2018-08-01T23:15:29.406056: step 6148, loss 0.562486.
Train: 2018-08-01T23:15:29.567599: step 6149, loss 0.597917.
Train: 2018-08-01T23:15:29.732161: step 6150, loss 0.59802.
Test: 2018-08-01T23:15:30.279702: step 6150, loss 0.547658.
Train: 2018-08-01T23:15:30.449255: step 6151, loss 0.598086.
Train: 2018-08-01T23:15:30.615833: step 6152, loss 0.633691.
Train: 2018-08-01T23:15:30.779391: step 6153, loss 0.473655.
Train: 2018-08-01T23:15:30.951926: step 6154, loss 0.562549.
Train: 2018-08-01T23:15:31.120448: step 6155, loss 0.509154.
Train: 2018-08-01T23:15:31.292987: step 6156, loss 0.473464.
Train: 2018-08-01T23:15:31.473505: step 6157, loss 0.509009.
Train: 2018-08-01T23:15:31.633079: step 6158, loss 0.598416.
Train: 2018-08-01T23:15:31.796962: step 6159, loss 0.490892.
Train: 2018-08-01T23:15:31.970490: step 6160, loss 0.400874.
Test: 2018-08-01T23:15:32.514013: step 6160, loss 0.547584.
Train: 2018-08-01T23:15:32.680573: step 6161, loss 0.454395.
Train: 2018-08-01T23:15:32.845160: step 6162, loss 0.635337.
Train: 2018-08-01T23:15:33.022676: step 6163, loss 0.63563.
Train: 2018-08-01T23:15:33.185218: step 6164, loss 0.508126.
Train: 2018-08-01T23:15:33.346812: step 6165, loss 0.672541.
Train: 2018-08-01T23:15:33.513366: step 6166, loss 0.63601.
Train: 2018-08-01T23:15:33.676904: step 6167, loss 0.453268.
Train: 2018-08-01T23:15:33.838502: step 6168, loss 0.61769.
Train: 2018-08-01T23:15:34.004060: step 6169, loss 0.544597.
Train: 2018-08-01T23:15:34.182551: step 6170, loss 0.489835.
Test: 2018-08-01T23:15:34.715131: step 6170, loss 0.547567.
Train: 2018-08-01T23:15:34.883704: step 6171, loss 0.599381.
Train: 2018-08-01T23:15:35.049266: step 6172, loss 0.526344.
Train: 2018-08-01T23:15:35.210830: step 6173, loss 0.544599.
Train: 2018-08-01T23:15:35.376391: step 6174, loss 0.562852.
Train: 2018-08-01T23:15:35.544944: step 6175, loss 0.562849.
Train: 2018-08-01T23:15:35.704516: step 6176, loss 0.562843.
Train: 2018-08-01T23:15:35.870066: step 6177, loss 0.508138.
Train: 2018-08-01T23:15:36.049591: step 6178, loss 0.544603.
Train: 2018-08-01T23:15:36.216117: step 6179, loss 0.544602.
Train: 2018-08-01T23:15:36.383699: step 6180, loss 0.508128.
Test: 2018-08-01T23:15:36.915246: step 6180, loss 0.547567.
Train: 2018-08-01T23:15:37.099782: step 6181, loss 0.562849.
Train: 2018-08-01T23:15:37.265146: step 6182, loss 0.526343.
Train: 2018-08-01T23:15:37.428115: step 6183, loss 0.526329.
Train: 2018-08-01T23:15:37.599657: step 6184, loss 0.508028.
Train: 2018-08-01T23:15:37.769202: step 6185, loss 0.562898.
Train: 2018-08-01T23:15:37.930771: step 6186, loss 0.489615.
Train: 2018-08-01T23:15:38.093336: step 6187, loss 0.599649.
Train: 2018-08-01T23:15:38.260920: step 6188, loss 0.471111.
Train: 2018-08-01T23:15:38.419464: step 6189, loss 0.526182.
Train: 2018-08-01T23:15:38.584025: step 6190, loss 0.599876.
Test: 2018-08-01T23:15:39.113622: step 6190, loss 0.54758.
Train: 2018-08-01T23:15:39.279166: step 6191, loss 0.489235.
Train: 2018-08-01T23:15:39.439737: step 6192, loss 0.618484.
Train: 2018-08-01T23:15:39.601339: step 6193, loss 0.489128.
Train: 2018-08-01T23:15:39.763901: step 6194, loss 0.544579.
Train: 2018-08-01T23:15:39.934414: step 6195, loss 0.544579.
Train: 2018-08-01T23:15:40.097978: step 6196, loss 0.618731.
Train: 2018-08-01T23:15:40.263559: step 6197, loss 0.600182.
Train: 2018-08-01T23:15:40.427098: step 6198, loss 0.563096.
Train: 2018-08-01T23:15:40.591658: step 6199, loss 0.618565.
Train: 2018-08-01T23:15:40.761205: step 6200, loss 0.563039.
Test: 2018-08-01T23:15:41.291787: step 6200, loss 0.547576.
Train: 2018-08-01T23:15:42.140039: step 6201, loss 0.599849.
Train: 2018-08-01T23:15:42.315569: step 6202, loss 0.581335.
Train: 2018-08-01T23:15:42.477136: step 6203, loss 0.654538.
Train: 2018-08-01T23:15:42.639673: step 6204, loss 0.599356.
Train: 2018-08-01T23:15:42.811213: step 6205, loss 0.490093.
Train: 2018-08-01T23:15:42.977769: step 6206, loss 0.562747.
Train: 2018-08-01T23:15:43.146318: step 6207, loss 0.52658.
Train: 2018-08-01T23:15:43.313870: step 6208, loss 0.544658.
Train: 2018-08-01T23:15:43.481437: step 6209, loss 0.490712.
Train: 2018-08-01T23:15:43.643989: step 6210, loss 0.526709.
Test: 2018-08-01T23:15:44.187535: step 6210, loss 0.547602.
Train: 2018-08-01T23:15:44.353123: step 6211, loss 0.580601.
Train: 2018-08-01T23:15:44.533609: step 6212, loss 0.526744.
Train: 2018-08-01T23:15:44.699180: step 6213, loss 0.508819.
Train: 2018-08-01T23:15:44.859737: step 6214, loss 0.50881.
Train: 2018-08-01T23:15:45.025296: step 6215, loss 0.508775.
Train: 2018-08-01T23:15:45.186896: step 6216, loss 0.47276.
Train: 2018-08-01T23:15:45.353449: step 6217, loss 0.61674.
Train: 2018-08-01T23:15:45.522965: step 6218, loss 0.580734.
Train: 2018-08-01T23:15:45.688554: step 6219, loss 0.634908.
Train: 2018-08-01T23:15:45.849129: step 6220, loss 0.670936.
Test: 2018-08-01T23:15:46.385659: step 6220, loss 0.547594.
Train: 2018-08-01T23:15:46.560193: step 6221, loss 0.508667.
Train: 2018-08-01T23:15:46.724752: step 6222, loss 0.56265.
Train: 2018-08-01T23:15:46.903274: step 6223, loss 0.59853.
Train: 2018-08-01T23:15:47.070853: step 6224, loss 0.526788.
Train: 2018-08-01T23:15:47.231407: step 6225, loss 0.49105.
Train: 2018-08-01T23:15:47.407926: step 6226, loss 0.562595.
Train: 2018-08-01T23:15:47.567528: step 6227, loss 0.56259.
Train: 2018-08-01T23:15:47.740038: step 6228, loss 0.562585.
Train: 2018-08-01T23:15:47.916598: step 6229, loss 0.616125.
Train: 2018-08-01T23:15:48.079156: step 6230, loss 0.633853.
Test: 2018-08-01T23:15:48.619694: step 6230, loss 0.547656.
Train: 2018-08-01T23:15:48.783275: step 6231, loss 0.669208.
Train: 2018-08-01T23:15:48.948838: step 6232, loss 0.580219.
Train: 2018-08-01T23:15:49.115362: step 6233, loss 0.456653.
Train: 2018-08-01T23:15:49.275932: step 6234, loss 0.668083.
Train: 2018-08-01T23:15:49.446476: step 6235, loss 0.667693.
Train: 2018-08-01T23:15:49.608045: step 6236, loss 0.510062.
Train: 2018-08-01T23:15:49.769646: step 6237, loss 0.545024.
Train: 2018-08-01T23:15:49.945147: step 6238, loss 0.631768.
Train: 2018-08-01T23:15:50.105739: step 6239, loss 0.545127.
Train: 2018-08-01T23:15:50.270274: step 6240, loss 0.459093.
Test: 2018-08-01T23:15:50.803876: step 6240, loss 0.547998.
Train: 2018-08-01T23:15:50.970404: step 6241, loss 0.545202.
Train: 2018-08-01T23:15:51.132994: step 6242, loss 0.699818.
Train: 2018-08-01T23:15:51.300522: step 6243, loss 0.613776.
Train: 2018-08-01T23:15:51.465079: step 6244, loss 0.528264.
Train: 2018-08-01T23:15:51.626647: step 6245, loss 0.732631.
Train: 2018-08-01T23:15:51.804173: step 6246, loss 0.49466.
Train: 2018-08-01T23:15:51.967736: step 6247, loss 0.4949.
Train: 2018-08-01T23:15:52.133310: step 6248, loss 0.579281.
Train: 2018-08-01T23:15:52.297854: step 6249, loss 0.629706.
Train: 2018-08-01T23:15:52.461417: step 6250, loss 0.478602.
Test: 2018-08-01T23:15:52.990002: step 6250, loss 0.54843.
Train: 2018-08-01T23:15:53.158553: step 6251, loss 0.579211.
Train: 2018-08-01T23:15:53.329097: step 6252, loss 0.595932.
Train: 2018-08-01T23:15:53.498643: step 6253, loss 0.545764.
Train: 2018-08-01T23:15:53.659214: step 6254, loss 0.479014.
Train: 2018-08-01T23:15:53.832751: step 6255, loss 0.495665.
Train: 2018-08-01T23:15:54.000303: step 6256, loss 0.595928.
Train: 2018-08-01T23:15:54.165863: step 6257, loss 0.595958.
Train: 2018-08-01T23:15:54.331443: step 6258, loss 0.61273.
Train: 2018-08-01T23:15:54.491998: step 6259, loss 0.545705.
Train: 2018-08-01T23:15:54.663529: step 6260, loss 0.579213.
Test: 2018-08-01T23:15:55.206079: step 6260, loss 0.548428.
Train: 2018-08-01T23:15:55.392581: step 6261, loss 0.562458.
Train: 2018-08-01T23:15:55.630944: step 6262, loss 0.579212.
Train: 2018-08-01T23:15:55.813455: step 6263, loss 0.562459.
Train: 2018-08-01T23:15:55.995968: step 6264, loss 0.64621.
Train: 2018-08-01T23:15:56.165515: step 6265, loss 0.646106.
Train: 2018-08-01T23:15:56.324120: step 6266, loss 0.479048.
Train: 2018-08-01T23:15:56.486659: step 6267, loss 0.562486.
Train: 2018-08-01T23:15:56.657199: step 6268, loss 0.595817.
Train: 2018-08-01T23:15:56.820788: step 6269, loss 0.645734.
Train: 2018-08-01T23:15:56.983353: step 6270, loss 0.512674.
Test: 2018-08-01T23:15:57.547850: step 6270, loss 0.54862.
Train: 2018-08-01T23:15:57.718397: step 6271, loss 0.6289.
Train: 2018-08-01T23:15:57.886943: step 6272, loss 0.579096.
Train: 2018-08-01T23:15:58.048512: step 6273, loss 0.512942.
Train: 2018-08-01T23:15:58.225044: step 6274, loss 0.446889.
Train: 2018-08-01T23:15:58.389595: step 6275, loss 0.463259.
Train: 2018-08-01T23:15:58.553157: step 6276, loss 0.579114.
Train: 2018-08-01T23:15:58.717723: step 6277, loss 0.512578.
Train: 2018-08-01T23:15:58.887239: step 6278, loss 0.562478.
Train: 2018-08-01T23:15:59.047817: step 6279, loss 0.59595.
Train: 2018-08-01T23:15:59.213392: step 6280, loss 0.512104.
Test: 2018-08-01T23:15:59.743959: step 6280, loss 0.548343.
Train: 2018-08-01T23:15:59.910537: step 6281, loss 0.562436.
Train: 2018-08-01T23:16:00.077059: step 6282, loss 0.54555.
Train: 2018-08-01T23:16:00.247602: step 6283, loss 0.528573.
Train: 2018-08-01T23:16:00.415154: step 6284, loss 0.596355.
Train: 2018-08-01T23:16:00.589688: step 6285, loss 0.51137.
Train: 2018-08-01T23:16:00.757270: step 6286, loss 0.596518.
Train: 2018-08-01T23:16:00.914848: step 6287, loss 0.545303.
Train: 2018-08-01T23:16:01.081399: step 6288, loss 0.476746.
Train: 2018-08-01T23:16:01.250921: step 6289, loss 0.682691.
Train: 2018-08-01T23:16:01.432434: step 6290, loss 0.682815.
Test: 2018-08-01T23:16:01.977811: step 6290, loss 0.548004.
Train: 2018-08-01T23:16:02.156312: step 6291, loss 0.562396.
Train: 2018-08-01T23:16:02.331839: step 6292, loss 0.613913.
Train: 2018-08-01T23:16:02.498394: step 6293, loss 0.579541.
Train: 2018-08-01T23:16:02.667967: step 6294, loss 0.682217.
Train: 2018-08-01T23:16:02.837489: step 6295, loss 0.579459.
Train: 2018-08-01T23:16:03.019003: step 6296, loss 0.579409.
Train: 2018-08-01T23:16:03.183601: step 6297, loss 0.477659.
Train: 2018-08-01T23:16:03.347159: step 6298, loss 0.663968.
Train: 2018-08-01T23:16:03.508695: step 6299, loss 0.629931.
Train: 2018-08-01T23:16:03.676277: step 6300, loss 0.612885.
Test: 2018-08-01T23:16:04.212812: step 6300, loss 0.548436.
Train: 2018-08-01T23:16:05.003430: step 6301, loss 0.478723.
Train: 2018-08-01T23:16:05.167957: step 6302, loss 0.529051.
Train: 2018-08-01T23:16:05.330554: step 6303, loss 0.579169.
Train: 2018-08-01T23:16:05.495084: step 6304, loss 0.579154.
Train: 2018-08-01T23:16:05.655654: step 6305, loss 0.495931.
Train: 2018-08-01T23:16:05.820214: step 6306, loss 0.662341.
Train: 2018-08-01T23:16:05.988764: step 6307, loss 0.496054.
Train: 2018-08-01T23:16:06.149365: step 6308, loss 0.562511.
Train: 2018-08-01T23:16:06.328916: step 6309, loss 0.529295.
Train: 2018-08-01T23:16:06.493443: step 6310, loss 0.54589.
Test: 2018-08-01T23:16:07.029025: step 6310, loss 0.548573.
Train: 2018-08-01T23:16:07.201521: step 6311, loss 0.562501.
Train: 2018-08-01T23:16:07.363090: step 6312, loss 0.645737.
Train: 2018-08-01T23:16:07.527651: step 6313, loss 0.595781.
Train: 2018-08-01T23:16:07.696200: step 6314, loss 0.562504.
Train: 2018-08-01T23:16:07.864748: step 6315, loss 0.579125.
Train: 2018-08-01T23:16:08.033299: step 6316, loss 0.595724.
Train: 2018-08-01T23:16:08.201874: step 6317, loss 0.529346.
Train: 2018-08-01T23:16:08.364445: step 6318, loss 0.54594.
Train: 2018-08-01T23:16:08.529970: step 6319, loss 0.562522.
Train: 2018-08-01T23:16:08.694561: step 6320, loss 0.579109.
Test: 2018-08-01T23:16:09.227107: step 6320, loss 0.548628.
Train: 2018-08-01T23:16:09.391693: step 6321, loss 0.615606.
Train: 2018-08-01T23:16:09.558253: step 6322, loss 0.579103.
Train: 2018-08-01T23:16:09.736775: step 6323, loss 0.612225.
Train: 2018-08-01T23:16:09.906320: step 6324, loss 0.496377.
Train: 2018-08-01T23:16:10.073874: step 6325, loss 0.546002.
Train: 2018-08-01T23:16:10.237409: step 6326, loss 0.61218.
Train: 2018-08-01T23:16:10.403963: step 6327, loss 0.628706.
Train: 2018-08-01T23:16:10.582484: step 6328, loss 0.446916.
Train: 2018-08-01T23:16:10.747044: step 6329, loss 0.546011.
Train: 2018-08-01T23:16:10.920580: step 6330, loss 0.595645.
Test: 2018-08-01T23:16:11.469140: step 6330, loss 0.548659.
Train: 2018-08-01T23:16:11.690685: step 6331, loss 0.512834.
Train: 2018-08-01T23:16:11.864220: step 6332, loss 0.612293.
Train: 2018-08-01T23:16:12.035788: step 6333, loss 0.595719.
Train: 2018-08-01T23:16:12.201319: step 6334, loss 0.612331.
Train: 2018-08-01T23:16:12.371888: step 6335, loss 0.562517.
Train: 2018-08-01T23:16:12.535459: step 6336, loss 0.42979.
Train: 2018-08-01T23:16:12.703975: step 6337, loss 0.545881.
Train: 2018-08-01T23:16:12.880507: step 6338, loss 0.51251.
Train: 2018-08-01T23:16:13.062044: step 6339, loss 0.579181.
Train: 2018-08-01T23:16:13.226603: step 6340, loss 0.545712.
Test: 2018-08-01T23:16:13.757185: step 6340, loss 0.548388.
Train: 2018-08-01T23:16:13.924745: step 6341, loss 0.478497.
Train: 2018-08-01T23:16:14.088274: step 6342, loss 0.478172.
Train: 2018-08-01T23:16:14.250840: step 6343, loss 0.697864.
Train: 2018-08-01T23:16:14.412409: step 6344, loss 0.528476.
Train: 2018-08-01T23:16:14.583950: step 6345, loss 0.528386.
Train: 2018-08-01T23:16:14.748509: step 6346, loss 0.562399.
Train: 2018-08-01T23:16:14.919087: step 6347, loss 0.613694.
Train: 2018-08-01T23:16:15.088611: step 6348, loss 0.596645.
Train: 2018-08-01T23:16:15.257150: step 6349, loss 0.493843.
Train: 2018-08-01T23:16:15.418718: step 6350, loss 0.545227.
Test: 2018-08-01T23:16:15.960302: step 6350, loss 0.547992.
Train: 2018-08-01T23:16:16.126825: step 6351, loss 0.493592.
Train: 2018-08-01T23:16:16.290404: step 6352, loss 0.562399.
Train: 2018-08-01T23:16:16.453951: step 6353, loss 0.493244.
Train: 2018-08-01T23:16:16.625522: step 6354, loss 0.562409.
Train: 2018-08-01T23:16:16.794042: step 6355, loss 0.649387.
Train: 2018-08-01T23:16:16.957629: step 6356, loss 0.457932.
Train: 2018-08-01T23:16:17.120169: step 6357, loss 0.475132.
Train: 2018-08-01T23:16:17.284729: step 6358, loss 0.544925.
Train: 2018-08-01T23:16:17.451285: step 6359, loss 0.580045.
Train: 2018-08-01T23:16:17.612884: step 6360, loss 0.56248.
Test: 2018-08-01T23:16:18.165385: step 6360, loss 0.5477.
Train: 2018-08-01T23:16:18.333959: step 6361, loss 0.668508.
Train: 2018-08-01T23:16:18.498516: step 6362, loss 0.615529.
Train: 2018-08-01T23:16:18.659087: step 6363, loss 0.580164.
Train: 2018-08-01T23:16:18.828602: step 6364, loss 0.544836.
Train: 2018-08-01T23:16:18.993166: step 6365, loss 0.474262.
Train: 2018-08-01T23:16:19.158745: step 6366, loss 0.703748.
Train: 2018-08-01T23:16:19.325306: step 6367, loss 0.597742.
Train: 2018-08-01T23:16:19.487879: step 6368, loss 0.544874.
Train: 2018-08-01T23:16:19.654395: step 6369, loss 0.527326.
Train: 2018-08-01T23:16:19.817957: step 6370, loss 0.702849.
Test: 2018-08-01T23:16:20.351532: step 6370, loss 0.547789.
Train: 2018-08-01T23:16:20.524071: step 6371, loss 0.56244.
Train: 2018-08-01T23:16:20.689658: step 6372, loss 0.562428.
Train: 2018-08-01T23:16:20.859174: step 6373, loss 0.475413.
Train: 2018-08-01T23:16:21.023734: step 6374, loss 0.492887.
Train: 2018-08-01T23:16:21.186299: step 6375, loss 0.458123.
Train: 2018-08-01T23:16:21.352861: step 6376, loss 0.684277.
Train: 2018-08-01T23:16:21.520407: step 6377, loss 0.562418.
Train: 2018-08-01T23:16:21.687959: step 6378, loss 0.510244.
Train: 2018-08-01T23:16:21.865516: step 6379, loss 0.527626.
Train: 2018-08-01T23:16:22.032039: step 6380, loss 0.579828.
Test: 2018-08-01T23:16:22.572611: step 6380, loss 0.547839.
Train: 2018-08-01T23:16:22.736187: step 6381, loss 0.510178.
Train: 2018-08-01T23:16:22.899745: step 6382, loss 0.649585.
Train: 2018-08-01T23:16:23.077266: step 6383, loss 0.492724.
Train: 2018-08-01T23:16:23.239835: step 6384, loss 0.632167.
Train: 2018-08-01T23:16:23.402408: step 6385, loss 0.70183.
Train: 2018-08-01T23:16:23.565938: step 6386, loss 0.562415.
Train: 2018-08-01T23:16:23.726510: step 6387, loss 0.527731.
Train: 2018-08-01T23:16:23.889106: step 6388, loss 0.545095.
Train: 2018-08-01T23:16:24.059648: step 6389, loss 0.44139.
Train: 2018-08-01T23:16:24.226175: step 6390, loss 0.614298.
Test: 2018-08-01T23:16:24.759747: step 6390, loss 0.547921.
Train: 2018-08-01T23:16:24.924307: step 6391, loss 0.579697.
Train: 2018-08-01T23:16:25.086874: step 6392, loss 0.614259.
Train: 2018-08-01T23:16:25.253427: step 6393, loss 0.596928.
Train: 2018-08-01T23:16:25.426964: step 6394, loss 0.665808.
Train: 2018-08-01T23:16:25.591524: step 6395, loss 0.51085.
Train: 2018-08-01T23:16:25.753093: step 6396, loss 0.579543.
Train: 2018-08-01T23:16:25.917652: step 6397, loss 0.511061.
Train: 2018-08-01T23:16:26.091213: step 6398, loss 0.545304.
Train: 2018-08-01T23:16:26.259766: step 6399, loss 0.630724.
Train: 2018-08-01T23:16:26.436296: step 6400, loss 0.596506.
Test: 2018-08-01T23:16:26.965850: step 6400, loss 0.548151.
Train: 2018-08-01T23:16:27.762154: step 6401, loss 0.630479.
Train: 2018-08-01T23:16:27.922734: step 6402, loss 0.494525.
Train: 2018-08-01T23:16:28.084293: step 6403, loss 0.562413.
Train: 2018-08-01T23:16:28.249881: step 6404, loss 0.427021.
Train: 2018-08-01T23:16:28.413444: step 6405, loss 0.477704.
Train: 2018-08-01T23:16:28.578999: step 6406, loss 0.494485.
Train: 2018-08-01T23:16:28.742564: step 6407, loss 0.545367.
Train: 2018-08-01T23:16:28.906108: step 6408, loss 0.54531.
Train: 2018-08-01T23:16:29.066666: step 6409, loss 0.648093.
Train: 2018-08-01T23:16:29.231227: step 6410, loss 0.57956.
Test: 2018-08-01T23:16:29.769787: step 6410, loss 0.548008.
Train: 2018-08-01T23:16:29.937340: step 6411, loss 0.528031.
Train: 2018-08-01T23:16:30.098938: step 6412, loss 0.510775.
Train: 2018-08-01T23:16:30.262470: step 6413, loss 0.51067.
Train: 2018-08-01T23:16:30.426057: step 6414, loss 0.57969.
Train: 2018-08-01T23:16:30.589626: step 6415, loss 0.666351.
Train: 2018-08-01T23:16:30.761137: step 6416, loss 0.579736.
Train: 2018-08-01T23:16:30.925727: step 6417, loss 0.458438.
Train: 2018-08-01T23:16:31.087265: step 6418, loss 0.56241.
Train: 2018-08-01T23:16:31.253820: step 6419, loss 0.597166.
Train: 2018-08-01T23:16:31.417383: step 6420, loss 0.579802.
Test: 2018-08-01T23:16:31.951954: step 6420, loss 0.547854.
Train: 2018-08-01T23:16:32.114519: step 6421, loss 0.597198.
Train: 2018-08-01T23:16:32.283069: step 6422, loss 0.475493.
Train: 2018-08-01T23:16:32.448641: step 6423, loss 0.614621.
Train: 2018-08-01T23:16:32.618173: step 6424, loss 0.649424.
Train: 2018-08-01T23:16:32.791734: step 6425, loss 0.59717.
Train: 2018-08-01T23:16:32.954277: step 6426, loss 0.510367.
Train: 2018-08-01T23:16:33.120829: step 6427, loss 0.562407.
Train: 2018-08-01T23:16:33.284418: step 6428, loss 0.614363.
Train: 2018-08-01T23:16:33.449949: step 6429, loss 0.527818.
Train: 2018-08-01T23:16:33.612514: step 6430, loss 0.562401.
Test: 2018-08-01T23:16:34.140105: step 6430, loss 0.547944.
Train: 2018-08-01T23:16:34.305681: step 6431, loss 0.493348.
Train: 2018-08-01T23:16:34.482216: step 6432, loss 0.493329.
Train: 2018-08-01T23:16:34.650761: step 6433, loss 0.510533.
Train: 2018-08-01T23:16:34.812307: step 6434, loss 0.579729.
Train: 2018-08-01T23:16:34.971880: step 6435, loss 0.510368.
Train: 2018-08-01T23:16:35.135475: step 6436, loss 0.597179.
Train: 2018-08-01T23:16:35.300034: step 6437, loss 0.492804.
Train: 2018-08-01T23:16:35.471545: step 6438, loss 0.510105.
Train: 2018-08-01T23:16:35.637103: step 6439, loss 0.579923.
Train: 2018-08-01T23:16:35.800666: step 6440, loss 0.527404.
Test: 2018-08-01T23:16:36.338228: step 6440, loss 0.547753.
Train: 2018-08-01T23:16:36.517749: step 6441, loss 0.562458.
Train: 2018-08-01T23:16:36.696296: step 6442, loss 0.668038.
Train: 2018-08-01T23:16:36.861829: step 6443, loss 0.615261.
Train: 2018-08-01T23:16:37.026420: step 6444, loss 0.492129.
Train: 2018-08-01T23:16:37.204911: step 6445, loss 0.650412.
Train: 2018-08-01T23:16:37.370468: step 6446, loss 0.492183.
Train: 2018-08-01T23:16:37.532068: step 6447, loss 0.544892.
Train: 2018-08-01T23:16:37.694629: step 6448, loss 0.562461.
Train: 2018-08-01T23:16:37.860159: step 6449, loss 0.580032.
Train: 2018-08-01T23:16:38.035690: step 6450, loss 0.580025.
Test: 2018-08-01T23:16:38.579236: step 6450, loss 0.547756.
Train: 2018-08-01T23:16:38.744821: step 6451, loss 0.474684.
Train: 2018-08-01T23:16:38.907360: step 6452, loss 0.702986.
Train: 2018-08-01T23:16:39.073916: step 6453, loss 0.615067.
Train: 2018-08-01T23:16:39.242466: step 6454, loss 0.527442.
Train: 2018-08-01T23:16:39.410047: step 6455, loss 0.510018.
Train: 2018-08-01T23:16:39.574607: step 6456, loss 0.544971.
Train: 2018-08-01T23:16:39.738140: step 6457, loss 0.562429.
Train: 2018-08-01T23:16:39.899707: step 6458, loss 0.492651.
Train: 2018-08-01T23:16:40.079239: step 6459, loss 0.579883.
Train: 2018-08-01T23:16:40.257750: step 6460, loss 0.56243.
Test: 2018-08-01T23:16:40.806283: step 6460, loss 0.54781.
Train: 2018-08-01T23:16:40.970458: step 6461, loss 0.684646.
Train: 2018-08-01T23:16:41.137495: step 6462, loss 0.475281.
Train: 2018-08-01T23:16:41.309304: step 6463, loss 0.440465.
Train: 2018-08-01T23:16:41.475860: step 6464, loss 0.510085.
Train: 2018-08-01T23:16:41.655405: step 6465, loss 0.597401.
Train: 2018-08-01T23:16:41.838891: step 6466, loss 0.544938.
Train: 2018-08-01T23:16:42.005444: step 6467, loss 0.562448.
Train: 2018-08-01T23:16:42.172997: step 6468, loss 0.579999.
Train: 2018-08-01T23:16:42.353514: step 6469, loss 0.5449.
Train: 2018-08-01T23:16:42.528047: step 6470, loss 0.615172.
Test: 2018-08-01T23:16:43.072591: step 6470, loss 0.547749.
Train: 2018-08-01T23:16:43.249150: step 6471, loss 0.580027.
Train: 2018-08-01T23:16:43.411685: step 6472, loss 0.650251.
Train: 2018-08-01T23:16:43.573254: step 6473, loss 0.527394.
Train: 2018-08-01T23:16:43.747820: step 6474, loss 0.614962.
Train: 2018-08-01T23:16:43.912347: step 6475, loss 0.510014.
Train: 2018-08-01T23:16:44.074938: step 6476, loss 0.684619.
Train: 2018-08-01T23:16:44.247476: step 6477, loss 0.492791.
Train: 2018-08-01T23:16:44.409050: step 6478, loss 0.545034.
Train: 2018-08-01T23:16:44.574576: step 6479, loss 0.52769.
Train: 2018-08-01T23:16:44.737141: step 6480, loss 0.527708.
Test: 2018-08-01T23:16:45.281686: step 6480, loss 0.547881.
Train: 2018-08-01T23:16:45.442256: step 6481, loss 0.56241.
Train: 2018-08-01T23:16:45.610806: step 6482, loss 0.510361.
Train: 2018-08-01T23:16:45.780363: step 6483, loss 0.579774.
Train: 2018-08-01T23:16:45.944913: step 6484, loss 0.597147.
Train: 2018-08-01T23:16:46.114461: step 6485, loss 0.579774.
Train: 2018-08-01T23:16:46.279020: step 6486, loss 0.562411.
Train: 2018-08-01T23:16:46.439590: step 6487, loss 0.510379.
Train: 2018-08-01T23:16:46.601190: step 6488, loss 0.545062.
Train: 2018-08-01T23:16:46.763724: step 6489, loss 0.562411.
Train: 2018-08-01T23:16:46.936289: step 6490, loss 0.475599.
Test: 2018-08-01T23:16:47.476846: step 6490, loss 0.547854.
Train: 2018-08-01T23:16:47.639412: step 6491, loss 0.492856.
Train: 2018-08-01T23:16:47.801964: step 6492, loss 0.649587.
Train: 2018-08-01T23:16:47.967507: step 6493, loss 0.544981.
Train: 2018-08-01T23:16:48.134061: step 6494, loss 0.649748.
Train: 2018-08-01T23:16:48.311587: step 6495, loss 0.56243.
Train: 2018-08-01T23:16:48.477144: step 6496, loss 0.527537.
Train: 2018-08-01T23:16:48.643729: step 6497, loss 0.579873.
Train: 2018-08-01T23:16:48.807292: step 6498, loss 0.544986.
Train: 2018-08-01T23:16:48.978803: step 6499, loss 0.562426.
Train: 2018-08-01T23:16:49.146387: step 6500, loss 0.667044.
Test: 2018-08-01T23:16:49.684914: step 6500, loss 0.547844.
Train: 2018-08-01T23:16:50.490364: step 6501, loss 0.56242.
Train: 2018-08-01T23:16:50.660901: step 6502, loss 0.527658.
Train: 2018-08-01T23:16:50.825470: step 6503, loss 0.579773.
Train: 2018-08-01T23:16:50.987033: step 6504, loss 0.57975.
Train: 2018-08-01T23:16:51.154581: step 6505, loss 0.614358.
Train: 2018-08-01T23:16:51.324104: step 6506, loss 0.54512.
Train: 2018-08-01T23:16:51.487692: step 6507, loss 0.493382.
Train: 2018-08-01T23:16:51.661228: step 6508, loss 0.579646.
Train: 2018-08-01T23:16:51.826773: step 6509, loss 0.476219.
Train: 2018-08-01T23:16:51.990322: step 6510, loss 0.545151.
Test: 2018-08-01T23:16:52.522898: step 6510, loss 0.547944.
Train: 2018-08-01T23:16:52.686462: step 6511, loss 0.493348.
Train: 2018-08-01T23:16:52.855042: step 6512, loss 0.475931.
Train: 2018-08-01T23:16:53.022595: step 6513, loss 0.510375.
Train: 2018-08-01T23:16:53.188120: step 6514, loss 0.597225.
Train: 2018-08-01T23:16:53.352712: step 6515, loss 0.597319.
Train: 2018-08-01T23:16:53.515246: step 6516, loss 0.579908.
Train: 2018-08-01T23:16:53.683824: step 6517, loss 0.579933.
Train: 2018-08-01T23:16:53.862348: step 6518, loss 0.422396.
Train: 2018-08-01T23:16:54.030892: step 6519, loss 0.597559.
Train: 2018-08-01T23:16:54.199418: step 6520, loss 0.45697.
Test: 2018-08-01T23:16:54.730507: step 6520, loss 0.547715.
Train: 2018-08-01T23:16:54.894552: step 6521, loss 0.562483.
Train: 2018-08-01T23:16:55.057087: step 6522, loss 0.580184.
Train: 2018-08-01T23:16:55.225635: step 6523, loss 0.562516.
Train: 2018-08-01T23:16:55.393201: step 6524, loss 0.509285.
Train: 2018-08-01T23:16:55.553790: step 6525, loss 0.633693.
Train: 2018-08-01T23:16:55.724336: step 6526, loss 0.544753.
Train: 2018-08-01T23:16:55.889891: step 6527, loss 0.509113.
Train: 2018-08-01T23:16:56.052456: step 6528, loss 0.473368.
Train: 2018-08-01T23:16:56.227982: step 6529, loss 0.491061.
Train: 2018-08-01T23:16:56.398525: step 6530, loss 0.580567.
Test: 2018-08-01T23:16:56.928085: step 6530, loss 0.547598.
Train: 2018-08-01T23:16:57.091673: step 6531, loss 0.562654.
Train: 2018-08-01T23:16:57.270170: step 6532, loss 0.526647.
Train: 2018-08-01T23:16:57.436756: step 6533, loss 0.652959.
Train: 2018-08-01T23:16:57.600318: step 6534, loss 0.562706.
Train: 2018-08-01T23:16:57.760858: step 6535, loss 0.634962.
Train: 2018-08-01T23:16:57.926417: step 6536, loss 0.544651.
Train: 2018-08-01T23:16:58.087984: step 6537, loss 0.490562.
Train: 2018-08-01T23:16:58.251547: step 6538, loss 0.544655.
Train: 2018-08-01T23:16:58.415140: step 6539, loss 0.580727.
Train: 2018-08-01T23:16:58.584655: step 6540, loss 0.58072.
Test: 2018-08-01T23:16:59.126235: step 6540, loss 0.54759.
Train: 2018-08-01T23:16:59.292789: step 6541, loss 0.544659.
Train: 2018-08-01T23:16:59.456352: step 6542, loss 0.508638.
Train: 2018-08-01T23:16:59.614933: step 6543, loss 0.58069.
Train: 2018-08-01T23:16:59.778496: step 6544, loss 0.562672.
Train: 2018-08-01T23:16:59.943025: step 6545, loss 0.598672.
Train: 2018-08-01T23:17:00.107616: step 6546, loss 0.634589.
Train: 2018-08-01T23:17:00.275137: step 6547, loss 0.652359.
Train: 2018-08-01T23:17:00.452687: step 6548, loss 0.634142.
Train: 2018-08-01T23:17:00.622209: step 6549, loss 0.562559.
Train: 2018-08-01T23:17:00.795777: step 6550, loss 0.544784.
Test: 2018-08-01T23:17:01.340293: step 6550, loss 0.547694.
Train: 2018-08-01T23:17:01.505873: step 6551, loss 0.562501.
Train: 2018-08-01T23:17:01.667416: step 6552, loss 0.456716.
Train: 2018-08-01T23:17:01.830978: step 6553, loss 0.597678.
Train: 2018-08-01T23:17:01.994569: step 6554, loss 0.544891.
Train: 2018-08-01T23:17:02.159135: step 6555, loss 0.615086.
Train: 2018-08-01T23:17:02.330666: step 6556, loss 0.527431.
Train: 2018-08-01T23:17:02.496230: step 6557, loss 0.632351.
Train: 2018-08-01T23:17:02.656795: step 6558, loss 0.667037.
Train: 2018-08-01T23:17:02.820369: step 6559, loss 0.545045.
Train: 2018-08-01T23:17:02.980910: step 6560, loss 0.545094.
Test: 2018-08-01T23:17:03.523487: step 6560, loss 0.547943.
Train: 2018-08-01T23:17:03.686043: step 6561, loss 0.614193.
Train: 2018-08-01T23:17:03.848623: step 6562, loss 0.562397.
Train: 2018-08-01T23:17:04.012691: step 6563, loss 0.459455.
Train: 2018-08-01T23:17:04.178249: step 6564, loss 0.61381.
Train: 2018-08-01T23:17:04.343844: step 6565, loss 0.61372.
Train: 2018-08-01T23:17:04.517367: step 6566, loss 0.6136.
Train: 2018-08-01T23:17:04.681903: step 6567, loss 0.596438.
Train: 2018-08-01T23:17:04.841476: step 6568, loss 0.56241.
Train: 2018-08-01T23:17:05.012045: step 6569, loss 0.579334.
Train: 2018-08-01T23:17:05.175583: step 6570, loss 0.494949.
Test: 2018-08-01T23:17:05.710154: step 6570, loss 0.548325.
Train: 2018-08-01T23:17:05.874745: step 6571, loss 0.528739.
Train: 2018-08-01T23:17:06.039299: step 6572, loss 0.511925.
Train: 2018-08-01T23:17:06.201865: step 6573, loss 0.478223.
Train: 2018-08-01T23:17:06.364405: step 6574, loss 0.511814.
Train: 2018-08-01T23:17:06.523013: step 6575, loss 0.57933.
Train: 2018-08-01T23:17:06.689566: step 6576, loss 0.66408.
Train: 2018-08-01T23:17:06.855102: step 6577, loss 0.613255.
Train: 2018-08-01T23:17:07.017659: step 6578, loss 0.545477.
Train: 2018-08-01T23:17:07.189231: step 6579, loss 0.562415.
Train: 2018-08-01T23:17:07.357780: step 6580, loss 0.596277.
Test: 2018-08-01T23:17:07.903304: step 6580, loss 0.548248.
Train: 2018-08-01T23:17:08.065856: step 6581, loss 0.579337.
Train: 2018-08-01T23:17:08.235404: step 6582, loss 0.56242.
Train: 2018-08-01T23:17:08.401989: step 6583, loss 0.562422.
Train: 2018-08-01T23:17:08.570537: step 6584, loss 0.562424.
Train: 2018-08-01T23:17:08.734071: step 6585, loss 0.528667.
Train: 2018-08-01T23:17:08.898673: step 6586, loss 0.562424.
Train: 2018-08-01T23:17:09.060198: step 6587, loss 0.444215.
Train: 2018-08-01T23:17:09.227781: step 6588, loss 0.647034.
Train: 2018-08-01T23:17:09.388321: step 6589, loss 0.562415.
Train: 2018-08-01T23:17:09.550911: step 6590, loss 0.596303.
Test: 2018-08-01T23:17:10.087453: step 6590, loss 0.548222.
Train: 2018-08-01T23:17:10.253011: step 6591, loss 0.613252.
Train: 2018-08-01T23:17:10.415575: step 6592, loss 0.697891.
Train: 2018-08-01T23:17:10.578141: step 6593, loss 0.697529.
Train: 2018-08-01T23:17:10.746715: step 6594, loss 0.646498.
Train: 2018-08-01T23:17:10.912279: step 6595, loss 0.612631.
Train: 2018-08-01T23:17:11.080822: step 6596, loss 0.512623.
Train: 2018-08-01T23:17:11.255356: step 6597, loss 0.579092.
Train: 2018-08-01T23:17:11.421916: step 6598, loss 0.562566.
Train: 2018-08-01T23:17:11.585462: step 6599, loss 0.579033.
Train: 2018-08-01T23:17:11.751005: step 6600, loss 0.595397.
Test: 2018-08-01T23:17:12.290563: step 6600, loss 0.548983.
Train: 2018-08-01T23:17:13.055684: step 6601, loss 0.578991.
Train: 2018-08-01T23:17:13.224232: step 6602, loss 0.546409.
Train: 2018-08-01T23:17:13.395781: step 6603, loss 0.48149.
Train: 2018-08-01T23:17:13.559308: step 6604, loss 0.578961.
Train: 2018-08-01T23:17:13.725863: step 6605, loss 0.578959.
Train: 2018-08-01T23:17:13.887461: step 6606, loss 0.611401.
Train: 2018-08-01T23:17:14.050028: step 6607, loss 0.449319.
Train: 2018-08-01T23:17:14.214588: step 6608, loss 0.530287.
Train: 2018-08-01T23:17:14.377148: step 6609, loss 0.627725.
Train: 2018-08-01T23:17:14.540712: step 6610, loss 0.611499.
Test: 2018-08-01T23:17:15.084233: step 6610, loss 0.549085.
Train: 2018-08-01T23:17:15.251783: step 6611, loss 0.595234.
Train: 2018-08-01T23:17:15.414350: step 6612, loss 0.644003.
Train: 2018-08-01T23:17:15.572925: step 6613, loss 0.497791.
Train: 2018-08-01T23:17:15.736488: step 6614, loss 0.514028.
Train: 2018-08-01T23:17:15.898080: step 6615, loss 0.546467.
Train: 2018-08-01T23:17:16.068646: step 6616, loss 0.660328.
Train: 2018-08-01T23:17:16.232163: step 6617, loss 0.513901.
Train: 2018-08-01T23:17:16.392758: step 6618, loss 0.578974.
Train: 2018-08-01T23:17:16.558316: step 6619, loss 0.513815.
Train: 2018-08-01T23:17:16.721886: step 6620, loss 0.595303.
Test: 2018-08-01T23:17:17.266398: step 6620, loss 0.548981.
Train: 2018-08-01T23:17:17.430080: step 6621, loss 0.529991.
Train: 2018-08-01T23:17:17.590732: step 6622, loss 0.597546.
Train: 2018-08-01T23:17:17.757287: step 6623, loss 0.464354.
Train: 2018-08-01T23:17:17.920819: step 6624, loss 0.529752.
Train: 2018-08-01T23:17:18.083383: step 6625, loss 0.513138.
Train: 2018-08-01T23:17:18.248967: step 6626, loss 0.595627.
Train: 2018-08-01T23:17:18.410538: step 6627, loss 0.496145.
Train: 2018-08-01T23:17:18.576067: step 6628, loss 0.462537.
Train: 2018-08-01T23:17:18.736639: step 6629, loss 0.562461.
Train: 2018-08-01T23:17:18.902229: step 6630, loss 0.495126.
Test: 2018-08-01T23:17:19.446740: step 6630, loss 0.548249.
Train: 2018-08-01T23:17:19.613326: step 6631, loss 0.647016.
Train: 2018-08-01T23:17:19.779850: step 6632, loss 0.545427.
Train: 2018-08-01T23:17:19.946404: step 6633, loss 0.596482.
Train: 2018-08-01T23:17:20.110964: step 6634, loss 0.545312.
Train: 2018-08-01T23:17:20.272565: step 6635, loss 0.596659.
Train: 2018-08-01T23:17:20.439141: step 6636, loss 0.596723.
Train: 2018-08-01T23:17:20.601653: step 6637, loss 0.47648.
Train: 2018-08-01T23:17:20.768206: step 6638, loss 0.510727.
Train: 2018-08-01T23:17:20.931769: step 6639, loss 0.527854.
Train: 2018-08-01T23:17:21.108301: step 6640, loss 0.527753.
Test: 2018-08-01T23:17:21.644864: step 6640, loss 0.547859.
Train: 2018-08-01T23:17:21.814436: step 6641, loss 0.545032.
Train: 2018-08-01T23:17:21.978996: step 6642, loss 0.597302.
Train: 2018-08-01T23:17:22.141536: step 6643, loss 0.614867.
Train: 2018-08-01T23:17:22.310086: step 6644, loss 0.544942.
Train: 2018-08-01T23:17:22.473648: step 6645, loss 0.685085.
Train: 2018-08-01T23:17:22.634247: step 6646, loss 0.684987.
Train: 2018-08-01T23:17:22.797781: step 6647, loss 0.492586.
Train: 2018-08-01T23:17:22.973337: step 6648, loss 0.597301.
Train: 2018-08-01T23:17:23.133911: step 6649, loss 0.56242.
Train: 2018-08-01T23:17:23.298443: step 6650, loss 0.59717.
Test: 2018-08-01T23:17:23.838000: step 6650, loss 0.547887.
Train: 2018-08-01T23:17:24.003558: step 6651, loss 0.631776.
Train: 2018-08-01T23:17:24.168130: step 6652, loss 0.545112.
Train: 2018-08-01T23:17:24.339686: step 6653, loss 0.579647.
Train: 2018-08-01T23:17:24.505217: step 6654, loss 0.579602.
Train: 2018-08-01T23:17:24.665816: step 6655, loss 0.613884.
Train: 2018-08-01T23:17:24.845334: step 6656, loss 0.596618.
Train: 2018-08-01T23:17:25.024859: step 6657, loss 0.545345.
Train: 2018-08-01T23:17:25.194401: step 6658, loss 0.613451.
Train: 2018-08-01T23:17:25.363947: step 6659, loss 0.545457.
Train: 2018-08-01T23:17:25.529479: step 6660, loss 0.444045.
Test: 2018-08-01T23:17:26.065047: step 6660, loss 0.548265.
Train: 2018-08-01T23:17:26.228649: step 6661, loss 0.630037.
Train: 2018-08-01T23:17:26.391177: step 6662, loss 0.562425.
Train: 2018-08-01T23:17:26.550780: step 6663, loss 0.596152.
Train: 2018-08-01T23:17:26.722315: step 6664, loss 0.562436.
Train: 2018-08-01T23:17:26.898819: step 6665, loss 0.612881.
Train: 2018-08-01T23:17:27.071384: step 6666, loss 0.663134.
Train: 2018-08-01T23:17:27.235919: step 6667, loss 0.545744.
Train: 2018-08-01T23:17:27.400477: step 6668, loss 0.562484.
Train: 2018-08-01T23:17:27.570023: step 6669, loss 0.545855.
Train: 2018-08-01T23:17:27.737619: step 6670, loss 0.56251.
Test: 2018-08-01T23:17:28.283130: step 6670, loss 0.548625.
Train: 2018-08-01T23:17:28.453693: step 6671, loss 0.56252.
Train: 2018-08-01T23:17:28.621239: step 6672, loss 0.463077.
Train: 2018-08-01T23:17:28.792780: step 6673, loss 0.529347.
Train: 2018-08-01T23:17:28.956318: step 6674, loss 0.612344.
Train: 2018-08-01T23:17:29.120879: step 6675, loss 0.579129.
Train: 2018-08-01T23:17:29.285467: step 6676, loss 0.61238.
Train: 2018-08-01T23:17:29.456980: step 6677, loss 0.51266.
Train: 2018-08-01T23:17:29.628555: step 6678, loss 0.595758.
Train: 2018-08-01T23:17:29.791111: step 6679, loss 0.562506.
Train: 2018-08-01T23:17:29.955672: step 6680, loss 0.678911.
Test: 2018-08-01T23:17:30.493242: step 6680, loss 0.548615.
Train: 2018-08-01T23:17:30.655775: step 6681, loss 0.545915.
Train: 2018-08-01T23:17:30.821363: step 6682, loss 0.529358.
Train: 2018-08-01T23:17:30.981902: step 6683, loss 0.463058.
Train: 2018-08-01T23:17:31.161454: step 6684, loss 0.562516.
Train: 2018-08-01T23:17:31.334960: step 6685, loss 0.562506.
Train: 2018-08-01T23:17:31.501540: step 6686, loss 0.662398.
Train: 2018-08-01T23:17:31.680089: step 6687, loss 0.495915.
Train: 2018-08-01T23:17:31.847589: step 6688, loss 0.495844.
Train: 2018-08-01T23:17:32.017136: step 6689, loss 0.562479.
Train: 2018-08-01T23:17:32.177730: step 6690, loss 0.646113.
Test: 2018-08-01T23:17:32.696337: step 6690, loss 0.54845.
Train: 2018-08-01T23:17:32.856890: step 6691, loss 0.562465.
Train: 2018-08-01T23:17:33.019457: step 6692, loss 0.595954.
Train: 2018-08-01T23:17:33.186011: step 6693, loss 0.629446.
Train: 2018-08-01T23:17:33.352596: step 6694, loss 0.595926.
Train: 2018-08-01T23:17:33.527132: step 6695, loss 0.629303.
Train: 2018-08-01T23:17:33.695680: step 6696, loss 0.595828.
Train: 2018-08-01T23:17:33.861237: step 6697, loss 0.545875.
Train: 2018-08-01T23:17:34.028759: step 6698, loss 0.49612.
Train: 2018-08-01T23:17:34.205318: step 6699, loss 0.512743.
Train: 2018-08-01T23:17:34.371841: step 6700, loss 0.545914.
Test: 2018-08-01T23:17:34.902426: step 6700, loss 0.548594.
Train: 2018-08-01T23:17:35.685204: step 6701, loss 0.529276.
Train: 2018-08-01T23:17:35.847737: step 6702, loss 0.529216.
Train: 2018-08-01T23:17:36.006339: step 6703, loss 0.562486.
Train: 2018-08-01T23:17:36.185833: step 6704, loss 0.529063.
Train: 2018-08-01T23:17:36.352389: step 6705, loss 0.478737.
Train: 2018-08-01T23:17:36.519966: step 6706, loss 0.579248.
Train: 2018-08-01T23:17:36.683534: step 6707, loss 0.646704.
Train: 2018-08-01T23:17:36.846099: step 6708, loss 0.545548.
Train: 2018-08-01T23:17:37.022628: step 6709, loss 0.596229.
Train: 2018-08-01T23:17:37.184165: step 6710, loss 0.494737.
Test: 2018-08-01T23:17:37.715744: step 6710, loss 0.548215.
Train: 2018-08-01T23:17:37.881301: step 6711, loss 0.545458.
Train: 2018-08-01T23:17:38.048881: step 6712, loss 0.630361.
Train: 2018-08-01T23:17:38.213413: step 6713, loss 0.562405.
Train: 2018-08-01T23:17:38.382985: step 6714, loss 0.511356.
Train: 2018-08-01T23:17:38.545557: step 6715, loss 0.511276.
Train: 2018-08-01T23:17:38.711115: step 6716, loss 0.528239.
Train: 2018-08-01T23:17:38.872676: step 6717, loss 0.511028.
Train: 2018-08-01T23:17:39.033253: step 6718, loss 0.54522.
Train: 2018-08-01T23:17:39.196816: step 6719, loss 0.579625.
Train: 2018-08-01T23:17:39.367330: step 6720, loss 0.596939.
Test: 2018-08-01T23:17:39.900903: step 6720, loss 0.547918.
Train: 2018-08-01T23:17:40.068480: step 6721, loss 0.596999.
Train: 2018-08-01T23:17:40.234043: step 6722, loss 0.631659.
Train: 2018-08-01T23:17:40.406551: step 6723, loss 0.510478.
Train: 2018-08-01T23:17:40.579089: step 6724, loss 0.579723.
Train: 2018-08-01T23:17:40.744647: step 6725, loss 0.458482.
Train: 2018-08-01T23:17:40.908236: step 6726, loss 0.683862.
Train: 2018-08-01T23:17:41.070806: step 6727, loss 0.510377.
Train: 2018-08-01T23:17:41.234369: step 6728, loss 0.649169.
Train: 2018-08-01T23:17:41.402914: step 6729, loss 0.545073.
Train: 2018-08-01T23:17:41.563491: step 6730, loss 0.510435.
Test: 2018-08-01T23:17:42.095038: step 6730, loss 0.547897.
Train: 2018-08-01T23:17:42.261618: step 6731, loss 0.579734.
Train: 2018-08-01T23:17:42.429179: step 6732, loss 0.579731.
Train: 2018-08-01T23:17:42.597718: step 6733, loss 0.614357.
Train: 2018-08-01T23:17:42.773224: step 6734, loss 0.700775.
Train: 2018-08-01T23:17:42.936787: step 6735, loss 0.631361.
Train: 2018-08-01T23:17:43.099378: step 6736, loss 0.562396.
Train: 2018-08-01T23:17:43.269896: step 6737, loss 0.47685.
Train: 2018-08-01T23:17:43.438446: step 6738, loss 0.494086.
Train: 2018-08-01T23:17:43.605031: step 6739, loss 0.528295.
Train: 2018-08-01T23:17:43.770588: step 6740, loss 0.579465.
Test: 2018-08-01T23:17:44.308121: step 6740, loss 0.548112.
Train: 2018-08-01T23:17:44.475673: step 6741, loss 0.528276.
Train: 2018-08-01T23:17:44.638238: step 6742, loss 0.545332.
Train: 2018-08-01T23:17:44.804804: step 6743, loss 0.579475.
Train: 2018-08-01T23:17:44.981321: step 6744, loss 0.476993.
Train: 2018-08-01T23:17:45.147876: step 6745, loss 0.545289.
Train: 2018-08-01T23:17:45.315427: step 6746, loss 0.66522.
Train: 2018-08-01T23:17:45.481982: step 6747, loss 0.493852.
Train: 2018-08-01T23:17:45.647553: step 6748, loss 0.52809.
Train: 2018-08-01T23:17:45.821076: step 6749, loss 0.510863.
Train: 2018-08-01T23:17:45.983668: step 6750, loss 0.390258.
Test: 2018-08-01T23:17:46.524196: step 6750, loss 0.547925.
Train: 2018-08-01T23:17:46.693774: step 6751, loss 0.527821.
Train: 2018-08-01T23:17:46.856335: step 6752, loss 0.527675.
Train: 2018-08-01T23:17:47.029877: step 6753, loss 0.632226.
Train: 2018-08-01T23:17:47.194405: step 6754, loss 0.579944.
Train: 2018-08-01T23:17:47.357969: step 6755, loss 0.474746.
Train: 2018-08-01T23:17:47.526525: step 6756, loss 0.597673.
Train: 2018-08-01T23:17:47.689114: step 6757, loss 0.544846.
Train: 2018-08-01T23:17:47.859658: step 6758, loss 0.580186.
Train: 2018-08-01T23:17:48.033194: step 6759, loss 0.580228.
Train: 2018-08-01T23:17:48.204705: step 6760, loss 0.562525.
Test: 2018-08-01T23:17:48.742267: step 6760, loss 0.547669.
Train: 2018-08-01T23:17:48.913808: step 6761, loss 0.509285.
Train: 2018-08-01T23:17:49.081361: step 6762, loss 0.58032.
Train: 2018-08-01T23:17:49.247921: step 6763, loss 0.473588.
Train: 2018-08-01T23:17:49.410481: step 6764, loss 0.473426.
Train: 2018-08-01T23:17:49.578033: step 6765, loss 0.383781.
Train: 2018-08-01T23:17:49.738635: step 6766, loss 0.63454.
Train: 2018-08-01T23:17:49.906156: step 6767, loss 0.49056.
Train: 2018-08-01T23:17:50.074715: step 6768, loss 0.598942.
Train: 2018-08-01T23:17:50.233306: step 6769, loss 0.653536.
Train: 2018-08-01T23:17:50.396845: step 6770, loss 0.526447.
Test: 2018-08-01T23:17:50.934407: step 6770, loss 0.547572.
Train: 2018-08-01T23:17:51.106946: step 6771, loss 0.508225.
Train: 2018-08-01T23:17:51.267542: step 6772, loss 0.653954.
Train: 2018-08-01T23:17:51.428112: step 6773, loss 0.581058.
Train: 2018-08-01T23:17:51.590683: step 6774, loss 0.508171.
Train: 2018-08-01T23:17:51.753233: step 6775, loss 0.617494.
Train: 2018-08-01T23:17:51.924760: step 6776, loss 0.581025.
Train: 2018-08-01T23:17:52.089321: step 6777, loss 0.599175.
Train: 2018-08-01T23:17:52.254911: step 6778, loss 0.526467.
Train: 2018-08-01T23:17:52.421443: step 6779, loss 0.453965.
Train: 2018-08-01T23:17:52.586990: step 6780, loss 0.580899.
Test: 2018-08-01T23:17:53.127545: step 6780, loss 0.547576.
Train: 2018-08-01T23:17:53.293103: step 6781, loss 0.508365.
Train: 2018-08-01T23:17:53.465668: step 6782, loss 0.453939.
Train: 2018-08-01T23:17:53.631229: step 6783, loss 0.471952.
Train: 2018-08-01T23:17:53.795789: step 6784, loss 0.508184.
Train: 2018-08-01T23:17:53.960318: step 6785, loss 0.65418.
Train: 2018-08-01T23:17:54.123881: step 6786, loss 0.562879.
Train: 2018-08-01T23:17:54.285449: step 6787, loss 0.581187.
Train: 2018-08-01T23:17:54.445023: step 6788, loss 0.544594.
Train: 2018-08-01T23:17:54.612574: step 6789, loss 0.562897.
Train: 2018-08-01T23:17:54.777134: step 6790, loss 0.489681.
Test: 2018-08-01T23:17:55.307716: step 6790, loss 0.54757.
Train: 2018-08-01T23:17:55.482250: step 6791, loss 0.636182.
Train: 2018-08-01T23:17:55.646836: step 6792, loss 0.544592.
Train: 2018-08-01T23:17:55.816357: step 6793, loss 0.544593.
Train: 2018-08-01T23:17:55.982912: step 6794, loss 0.636064.
Train: 2018-08-01T23:17:56.149476: step 6795, loss 0.617662.
Train: 2018-08-01T23:17:56.323038: step 6796, loss 0.526383.
Train: 2018-08-01T23:17:56.488590: step 6797, loss 0.544613.
Train: 2018-08-01T23:17:56.651125: step 6798, loss 0.453811.
Train: 2018-08-01T23:17:56.813691: step 6799, loss 0.54462.
Train: 2018-08-01T23:17:56.994207: step 6800, loss 0.653573.
Test: 2018-08-01T23:17:57.560694: step 6800, loss 0.547575.
Train: 2018-08-01T23:17:58.379357: step 6801, loss 0.544627.
Train: 2018-08-01T23:17:58.547906: step 6802, loss 0.508415.
Train: 2018-08-01T23:17:58.712497: step 6803, loss 0.544635.
Train: 2018-08-01T23:17:58.877026: step 6804, loss 0.58082.
Train: 2018-08-01T23:17:59.036630: step 6805, loss 0.490414.
Train: 2018-08-01T23:17:59.205149: step 6806, loss 0.508487.
Train: 2018-08-01T23:17:59.383671: step 6807, loss 0.454191.
Train: 2018-08-01T23:17:59.547233: step 6808, loss 0.472131.
Train: 2018-08-01T23:17:59.722764: step 6809, loss 0.635491.
Train: 2018-08-01T23:17:59.890317: step 6810, loss 0.599208.
Test: 2018-08-01T23:18:00.431869: step 6810, loss 0.547569.
Train: 2018-08-01T23:18:00.605436: step 6811, loss 0.562816.
Train: 2018-08-01T23:18:00.766975: step 6812, loss 0.653882.
Train: 2018-08-01T23:18:00.928541: step 6813, loss 0.562802.
Train: 2018-08-01T23:18:01.098110: step 6814, loss 0.508285.
Train: 2018-08-01T23:18:01.262727: step 6815, loss 0.599085.
Train: 2018-08-01T23:18:01.427290: step 6816, loss 0.508361.
Train: 2018-08-01T23:18:01.593830: step 6817, loss 0.580873.
Train: 2018-08-01T23:18:01.758400: step 6818, loss 0.526528.
Train: 2018-08-01T23:18:01.924957: step 6819, loss 0.598924.
Train: 2018-08-01T23:18:02.099485: step 6820, loss 0.616943.
Test: 2018-08-01T23:18:02.638020: step 6820, loss 0.547587.
Train: 2018-08-01T23:18:02.803612: step 6821, loss 0.634851.
Train: 2018-08-01T23:18:03.001050: step 6822, loss 0.580644.
Train: 2018-08-01T23:18:03.193537: step 6823, loss 0.652279.
Train: 2018-08-01T23:18:03.405967: step 6824, loss 0.580438.
Train: 2018-08-01T23:18:03.610421: step 6825, loss 0.68701.
Train: 2018-08-01T23:18:03.795925: step 6826, loss 0.59786.
Train: 2018-08-01T23:18:03.960529: step 6827, loss 0.544886.
Train: 2018-08-01T23:18:04.129034: step 6828, loss 0.632394.
Train: 2018-08-01T23:18:04.293624: step 6829, loss 0.510248.
Train: 2018-08-01T23:18:04.461147: step 6830, loss 0.475839.
Test: 2018-08-01T23:18:04.997732: step 6830, loss 0.547942.
Train: 2018-08-01T23:18:05.162297: step 6831, loss 0.527867.
Train: 2018-08-01T23:18:05.329824: step 6832, loss 0.527931.
Train: 2018-08-01T23:18:05.491417: step 6833, loss 0.562397.
Train: 2018-08-01T23:18:05.653957: step 6834, loss 0.596783.
Train: 2018-08-01T23:18:05.813532: step 6835, loss 0.528062.
Train: 2018-08-01T23:18:05.980086: step 6836, loss 0.648156.
Train: 2018-08-01T23:18:06.151627: step 6837, loss 0.511049.
Train: 2018-08-01T23:18:06.320208: step 6838, loss 0.682078.
Train: 2018-08-01T23:18:06.482742: step 6839, loss 0.511253.
Train: 2018-08-01T23:18:06.650295: step 6840, loss 0.596442.
Test: 2018-08-01T23:18:07.182870: step 6840, loss 0.548185.
Train: 2018-08-01T23:18:07.354430: step 6841, loss 0.562407.
Train: 2018-08-01T23:18:07.527948: step 6842, loss 0.528506.
Train: 2018-08-01T23:18:07.708465: step 6843, loss 0.47774.
Train: 2018-08-01T23:18:07.882001: step 6844, loss 0.579356.
Train: 2018-08-01T23:18:08.045564: step 6845, loss 0.545468.
Train: 2018-08-01T23:18:08.217131: step 6846, loss 0.630222.
Train: 2018-08-01T23:18:08.381665: step 6847, loss 0.647124.
Train: 2018-08-01T23:18:08.545264: step 6848, loss 0.528598.
Train: 2018-08-01T23:18:08.714775: step 6849, loss 0.511746.
Train: 2018-08-01T23:18:08.884346: step 6850, loss 0.579313.
Test: 2018-08-01T23:18:09.432856: step 6850, loss 0.548285.
Train: 2018-08-01T23:18:09.602405: step 6851, loss 0.579309.
Train: 2018-08-01T23:18:09.767991: step 6852, loss 0.596179.
Train: 2018-08-01T23:18:09.947480: step 6853, loss 0.511848.
Train: 2018-08-01T23:18:10.121016: step 6854, loss 0.528707.
Train: 2018-08-01T23:18:10.286573: step 6855, loss 0.613042.
Train: 2018-08-01T23:18:10.465096: step 6856, loss 0.461216.
Train: 2018-08-01T23:18:10.634643: step 6857, loss 0.613105.
Train: 2018-08-01T23:18:10.800115: step 6858, loss 0.613134.
Train: 2018-08-01T23:18:10.961707: step 6859, loss 0.596223.
Train: 2018-08-01T23:18:11.133224: step 6860, loss 0.596202.
Test: 2018-08-01T23:18:11.674807: step 6860, loss 0.548299.
Train: 2018-08-01T23:18:11.871283: step 6861, loss 0.613041.
Train: 2018-08-01T23:18:12.043818: step 6862, loss 0.562434.
Train: 2018-08-01T23:18:12.211368: step 6863, loss 0.56244.
Train: 2018-08-01T23:18:12.384879: step 6864, loss 0.612831.
Train: 2018-08-01T23:18:12.587337: step 6865, loss 0.545692.
Train: 2018-08-01T23:18:12.782815: step 6866, loss 0.562463.
Train: 2018-08-01T23:18:13.029156: step 6867, loss 0.629355.
Train: 2018-08-01T23:18:13.352293: step 6868, loss 0.545793.
Train: 2018-08-01T23:18:13.707343: step 6869, loss 0.595817.
Train: 2018-08-01T23:18:13.887886: step 6870, loss 0.629039.
Test: 2018-08-01T23:18:14.433404: step 6870, loss 0.548625.
Train: 2018-08-01T23:18:14.603946: step 6871, loss 0.562519.
Train: 2018-08-01T23:18:14.773493: step 6872, loss 0.51287.
Train: 2018-08-01T23:18:14.940078: step 6873, loss 0.645235.
Train: 2018-08-01T23:18:15.112618: step 6874, loss 0.595566.
Train: 2018-08-01T23:18:15.328011: step 6875, loss 0.546119.
Train: 2018-08-01T23:18:15.517539: step 6876, loss 0.529728.
Train: 2018-08-01T23:18:15.689786: step 6877, loss 0.579026.
Train: 2018-08-01T23:18:15.851909: step 6878, loss 0.447765.
Train: 2018-08-01T23:18:16.018441: step 6879, loss 0.628316.
Train: 2018-08-01T23:18:16.184995: step 6880, loss 0.480435.
Test: 2018-08-01T23:18:16.723554: step 6880, loss 0.548798.
Train: 2018-08-01T23:18:16.884124: step 6881, loss 0.611968.
Train: 2018-08-01T23:18:17.060652: step 6882, loss 0.463715.
Train: 2018-08-01T23:18:17.237181: step 6883, loss 0.612112.
Train: 2018-08-01T23:18:17.408726: step 6884, loss 0.595634.
Train: 2018-08-01T23:18:17.582258: step 6885, loss 0.496272.
Train: 2018-08-01T23:18:17.747815: step 6886, loss 0.562515.
Train: 2018-08-01T23:18:17.906393: step 6887, loss 0.629043.
Train: 2018-08-01T23:18:18.078956: step 6888, loss 0.612446.
Train: 2018-08-01T23:18:18.243516: step 6889, loss 0.579146.
Train: 2018-08-01T23:18:18.410076: step 6890, loss 0.595795.
Test: 2018-08-01T23:18:18.957582: step 6890, loss 0.548563.
Train: 2018-08-01T23:18:19.128161: step 6891, loss 0.462649.
Train: 2018-08-01T23:18:19.292686: step 6892, loss 0.512501.
Train: 2018-08-01T23:18:19.470211: step 6893, loss 0.61257.
Train: 2018-08-01T23:18:19.637788: step 6894, loss 0.646054.
Train: 2018-08-01T23:18:19.803321: step 6895, loss 0.67946.
Train: 2018-08-01T23:18:19.973865: step 6896, loss 0.495769.
Train: 2018-08-01T23:18:20.138427: step 6897, loss 0.54582.
Train: 2018-08-01T23:18:20.313957: step 6898, loss 0.645812.
Train: 2018-08-01T23:18:20.476522: step 6899, loss 0.562499.
Train: 2018-08-01T23:18:20.646069: step 6900, loss 0.496024.
Test: 2018-08-01T23:18:21.187620: step 6900, loss 0.548586.
Train: 2018-08-01T23:18:22.015605: step 6901, loss 0.628998.
Train: 2018-08-01T23:18:22.191132: step 6902, loss 0.695381.
Train: 2018-08-01T23:18:22.363671: step 6903, loss 0.562533.
Train: 2018-08-01T23:18:22.530194: step 6904, loss 0.546031.
Train: 2018-08-01T23:18:22.699766: step 6905, loss 0.595549.
Train: 2018-08-01T23:18:22.870317: step 6906, loss 0.529669.
Train: 2018-08-01T23:18:23.034847: step 6907, loss 0.562595.
Train: 2018-08-01T23:18:23.212372: step 6908, loss 0.562603.
Train: 2018-08-01T23:18:23.381949: step 6909, loss 0.529779.
Train: 2018-08-01T23:18:23.548473: step 6910, loss 0.480519.
Test: 2018-08-01T23:18:24.079053: step 6910, loss 0.54882.
Train: 2018-08-01T23:18:24.258575: step 6911, loss 0.49681.
Train: 2018-08-01T23:18:24.428146: step 6912, loss 0.645023.
Train: 2018-08-01T23:18:24.595673: step 6913, loss 0.546044.
Train: 2018-08-01T23:18:24.764256: step 6914, loss 0.479857.
Train: 2018-08-01T23:18:24.928795: step 6915, loss 0.628863.
Train: 2018-08-01T23:18:25.094340: step 6916, loss 0.645571.
Train: 2018-08-01T23:18:25.272890: step 6917, loss 0.496047.
Train: 2018-08-01T23:18:25.439444: step 6918, loss 0.529222.
Train: 2018-08-01T23:18:25.632900: step 6919, loss 0.562487.
Train: 2018-08-01T23:18:25.815413: step 6920, loss 0.545775.
Test: 2018-08-01T23:18:26.351979: step 6920, loss 0.548451.
Train: 2018-08-01T23:18:26.519531: step 6921, loss 0.562464.
Train: 2018-08-01T23:18:26.692070: step 6922, loss 0.478625.
Train: 2018-08-01T23:18:26.860619: step 6923, loss 0.56244.
Train: 2018-08-01T23:18:27.036157: step 6924, loss 0.646763.
Train: 2018-08-01T23:18:27.215697: step 6925, loss 0.629979.
Train: 2018-08-01T23:18:27.391201: step 6926, loss 0.579314.
Train: 2018-08-01T23:18:27.561745: step 6927, loss 0.579313.
Train: 2018-08-01T23:18:27.744256: step 6928, loss 0.511769.
Train: 2018-08-01T23:18:27.968657: step 6929, loss 0.494841.
Train: 2018-08-01T23:18:28.175106: step 6930, loss 0.562416.
Test: 2018-08-01T23:18:28.713665: step 6930, loss 0.548218.
Train: 2018-08-01T23:18:28.881242: step 6931, loss 0.528512.
Train: 2018-08-01T23:18:29.161467: step 6932, loss 0.613357.
Train: 2018-08-01T23:18:29.366920: step 6933, loss 0.528404.
Train: 2018-08-01T23:18:29.592316: step 6934, loss 0.545376.
Train: 2018-08-01T23:18:29.833671: step 6935, loss 0.596506.
Train: 2018-08-01T23:18:30.012193: step 6936, loss 0.579468.
Train: 2018-08-01T23:18:30.190716: step 6937, loss 0.511159.
Train: 2018-08-01T23:18:30.363281: step 6938, loss 0.5795.
Train: 2018-08-01T23:18:30.527847: step 6939, loss 0.493918.
Train: 2018-08-01T23:18:30.692376: step 6940, loss 0.545243.
Test: 2018-08-01T23:18:31.229938: step 6940, loss 0.548004.
Train: 2018-08-01T23:18:31.407495: step 6941, loss 0.47646.
Train: 2018-08-01T23:18:31.587980: step 6942, loss 0.579639.
Train: 2018-08-01T23:18:31.754536: step 6943, loss 0.562402.
Train: 2018-08-01T23:18:31.926078: step 6944, loss 0.441142.
Train: 2018-08-01T23:18:32.101609: step 6945, loss 0.475468.
Train: 2018-08-01T23:18:32.311048: step 6946, loss 0.579905.
Train: 2018-08-01T23:18:32.494558: step 6947, loss 0.615069.
Train: 2018-08-01T23:18:32.681060: step 6948, loss 0.580051.
Train: 2018-08-01T23:18:32.859581: step 6949, loss 0.562478.
Train: 2018-08-01T23:18:33.025164: step 6950, loss 0.562489.
Test: 2018-08-01T23:18:33.564697: step 6950, loss 0.547697.
Train: 2018-08-01T23:18:33.755199: step 6951, loss 0.580175.
Train: 2018-08-01T23:18:33.926734: step 6952, loss 0.52712.
Train: 2018-08-01T23:18:34.088328: step 6953, loss 0.491655.
Train: 2018-08-01T23:18:34.263859: step 6954, loss 0.70454.
Train: 2018-08-01T23:18:34.435402: step 6955, loss 0.598018.
Train: 2018-08-01T23:18:34.600957: step 6956, loss 0.580249.
Train: 2018-08-01T23:18:34.770474: step 6957, loss 0.63334.
Train: 2018-08-01T23:18:34.934035: step 6958, loss 0.615498.
Train: 2018-08-01T23:18:35.100621: step 6959, loss 0.527243.
Train: 2018-08-01T23:18:35.278142: step 6960, loss 0.474572.
Test: 2018-08-01T23:18:35.815678: step 6960, loss 0.547751.
Train: 2018-08-01T23:18:35.980241: step 6961, loss 0.509764.
Train: 2018-08-01T23:18:36.151817: step 6962, loss 0.544894.
Train: 2018-08-01T23:18:36.317369: step 6963, loss 0.632737.
Train: 2018-08-01T23:18:36.488880: step 6964, loss 0.527351.
Train: 2018-08-01T23:18:36.654469: step 6965, loss 0.579999.
Train: 2018-08-01T23:18:36.817027: step 6966, loss 0.544917.
Train: 2018-08-01T23:18:36.984593: step 6967, loss 0.544922.
Train: 2018-08-01T23:18:37.153135: step 6968, loss 0.61501.
Train: 2018-08-01T23:18:37.318661: step 6969, loss 0.457424.
Train: 2018-08-01T23:18:37.478266: step 6970, loss 0.527419.
Test: 2018-08-01T23:18:38.019787: step 6970, loss 0.54777.
Train: 2018-08-01T23:18:38.190357: step 6971, loss 0.492332.
Train: 2018-08-01T23:18:38.356917: step 6972, loss 0.615141.
Train: 2018-08-01T23:18:38.518455: step 6973, loss 0.580037.
Train: 2018-08-01T23:18:38.683040: step 6974, loss 0.439398.
Train: 2018-08-01T23:18:38.846576: step 6975, loss 0.615327.
Train: 2018-08-01T23:18:39.010164: step 6976, loss 0.50958.
Train: 2018-08-01T23:18:39.174700: step 6977, loss 0.633139.
Train: 2018-08-01T23:18:39.360203: step 6978, loss 0.527162.
Train: 2018-08-01T23:18:39.521808: step 6979, loss 0.491788.
Train: 2018-08-01T23:18:39.687361: step 6980, loss 0.63333.
Test: 2018-08-01T23:18:40.223896: step 6980, loss 0.547683.
Train: 2018-08-01T23:18:40.389483: step 6981, loss 0.491677.
Train: 2018-08-01T23:18:40.553041: step 6982, loss 0.491605.
Train: 2018-08-01T23:18:40.729569: step 6983, loss 0.562536.
Train: 2018-08-01T23:18:40.896099: step 6984, loss 0.598133.
Train: 2018-08-01T23:18:41.054705: step 6985, loss 0.455718.
Train: 2018-08-01T23:18:41.223223: step 6986, loss 0.455508.
Train: 2018-08-01T23:18:41.384822: step 6987, loss 0.562608.
Train: 2018-08-01T23:18:41.547357: step 6988, loss 0.652413.
Train: 2018-08-01T23:18:41.708925: step 6989, loss 0.544676.
Train: 2018-08-01T23:18:41.871509: step 6990, loss 0.508678.
Test: 2018-08-01T23:18:42.408057: step 6990, loss 0.547589.
Train: 2018-08-01T23:18:42.570622: step 6991, loss 0.670823.
Train: 2018-08-01T23:18:42.735206: step 6992, loss 0.634755.
Train: 2018-08-01T23:18:42.902733: step 6993, loss 0.562662.
Train: 2018-08-01T23:18:43.074300: step 6994, loss 0.61655.
Train: 2018-08-01T23:18:43.249809: step 6995, loss 0.688121.
Train: 2018-08-01T23:18:43.432363: step 6996, loss 0.5983.
Train: 2018-08-01T23:18:43.618820: step 6997, loss 0.598113.
Train: 2018-08-01T23:18:43.846214: step 6998, loss 0.597921.
Train: 2018-08-01T23:18:44.159374: step 6999, loss 0.544855.
Train: 2018-08-01T23:18:44.393748: step 7000, loss 0.544903.
Test: 2018-08-01T23:18:44.935301: step 7000, loss 0.547791.
Train: 2018-08-01T23:18:45.693484: step 7001, loss 0.492469.
Train: 2018-08-01T23:18:45.859010: step 7002, loss 0.510064.
Train: 2018-08-01T23:18:46.019580: step 7003, loss 0.527557.
Train: 2018-08-01T23:18:46.182171: step 7004, loss 0.492728.
Train: 2018-08-01T23:18:46.349728: step 7005, loss 0.649581.
Train: 2018-08-01T23:18:46.514258: step 7006, loss 0.684322.
Train: 2018-08-01T23:18:46.678858: step 7007, loss 0.631879.
Train: 2018-08-01T23:18:46.845400: step 7008, loss 0.493185.
Train: 2018-08-01T23:18:47.008935: step 7009, loss 0.476068.
Train: 2018-08-01T23:18:47.175523: step 7010, loss 0.52789.
Test: 2018-08-01T23:18:47.715063: step 7010, loss 0.547952.
Train: 2018-08-01T23:18:47.881601: step 7011, loss 0.389873.
Train: 2018-08-01T23:18:48.043201: step 7012, loss 0.666178.
Train: 2018-08-01T23:18:48.212742: step 7013, loss 0.441255.
Train: 2018-08-01T23:18:48.383261: step 7014, loss 0.545061.
Train: 2018-08-01T23:18:48.545857: step 7015, loss 0.597194.
Train: 2018-08-01T23:18:48.714376: step 7016, loss 0.527589.
Train: 2018-08-01T23:18:48.878938: step 7017, loss 0.54498.
Train: 2018-08-01T23:18:49.039535: step 7018, loss 0.597398.
Train: 2018-08-01T23:18:49.211075: step 7019, loss 0.614943.
Train: 2018-08-01T23:18:49.382589: step 7020, loss 0.562441.
Test: 2018-08-01T23:18:49.926136: step 7020, loss 0.547784.
Train: 2018-08-01T23:18:50.093713: step 7021, loss 0.47492.
Train: 2018-08-01T23:18:50.256280: step 7022, loss 0.615028.
Train: 2018-08-01T23:18:50.421835: step 7023, loss 0.579982.
Train: 2018-08-01T23:18:50.591404: step 7024, loss 0.544918.
Train: 2018-08-01T23:18:50.754931: step 7025, loss 0.632583.
Train: 2018-08-01T23:18:50.919481: step 7026, loss 0.544929.
Train: 2018-08-01T23:18:51.082076: step 7027, loss 0.492427.
Train: 2018-08-01T23:18:51.248600: step 7028, loss 0.544934.
Train: 2018-08-01T23:18:51.414158: step 7029, loss 0.579964.
Train: 2018-08-01T23:18:51.575752: step 7030, loss 0.579966.
Test: 2018-08-01T23:18:52.117279: step 7030, loss 0.547777.
Train: 2018-08-01T23:18:52.282877: step 7031, loss 0.544929.
Train: 2018-08-01T23:18:52.452383: step 7032, loss 0.579961.
Train: 2018-08-01T23:18:52.614974: step 7033, loss 0.6675.
Train: 2018-08-01T23:18:52.782526: step 7034, loss 0.544959.
Train: 2018-08-01T23:18:52.941077: step 7035, loss 0.440289.
Train: 2018-08-01T23:18:53.105667: step 7036, loss 0.492611.
Train: 2018-08-01T23:18:53.270222: step 7037, loss 0.562435.
Train: 2018-08-01T23:18:53.431783: step 7038, loss 0.56244.
Train: 2018-08-01T23:18:53.595352: step 7039, loss 0.597471.
Train: 2018-08-01T23:18:53.757945: step 7040, loss 0.562445.
Test: 2018-08-01T23:18:54.295455: step 7040, loss 0.547775.
Train: 2018-08-01T23:18:54.458022: step 7041, loss 0.509885.
Train: 2018-08-01T23:18:54.632555: step 7042, loss 0.52738.
Train: 2018-08-01T23:18:54.807119: step 7043, loss 0.632681.
Train: 2018-08-01T23:18:54.969672: step 7044, loss 0.544901.
Train: 2018-08-01T23:18:55.132249: step 7045, loss 0.562457.
Train: 2018-08-01T23:18:55.307780: step 7046, loss 0.544898.
Train: 2018-08-01T23:18:55.468320: step 7047, loss 0.650275.
Train: 2018-08-01T23:18:55.642853: step 7048, loss 0.597538.
Train: 2018-08-01T23:18:55.803425: step 7049, loss 0.527417.
Train: 2018-08-01T23:18:55.973996: step 7050, loss 0.667413.
Test: 2018-08-01T23:18:56.512529: step 7050, loss 0.547816.
Train: 2018-08-01T23:18:56.684071: step 7051, loss 0.614781.
Train: 2018-08-01T23:18:56.850651: step 7052, loss 0.527626.
Train: 2018-08-01T23:18:57.023164: step 7053, loss 0.683893.
Train: 2018-08-01T23:18:57.193714: step 7054, loss 0.441406.
Train: 2018-08-01T23:18:57.365286: step 7055, loss 0.476129.
Train: 2018-08-01T23:18:57.526842: step 7056, loss 0.545151.
Train: 2018-08-01T23:18:57.699411: step 7057, loss 0.579646.
Train: 2018-08-01T23:18:57.860958: step 7058, loss 0.648606.
Train: 2018-08-01T23:18:58.023490: step 7059, loss 0.545183.
Train: 2018-08-01T23:18:58.190076: step 7060, loss 0.510819.
Test: 2018-08-01T23:18:58.716637: step 7060, loss 0.548004.
Train: 2018-08-01T23:18:58.880235: step 7061, loss 0.562396.
Train: 2018-08-01T23:18:59.053768: step 7062, loss 0.476485.
Train: 2018-08-01T23:18:59.221288: step 7063, loss 0.493595.
Train: 2018-08-01T23:18:59.387867: step 7064, loss 0.631336.
Train: 2018-08-01T23:18:59.549411: step 7065, loss 0.458916.
Train: 2018-08-01T23:18:59.713999: step 7066, loss 0.579687.
Train: 2018-08-01T23:18:59.881523: step 7067, loss 0.562405.
Train: 2018-08-01T23:19:00.057055: step 7068, loss 0.597089.
Train: 2018-08-01T23:19:00.227597: step 7069, loss 0.492995.
Train: 2018-08-01T23:19:00.398142: step 7070, loss 0.527649.
Test: 2018-08-01T23:19:00.946675: step 7070, loss 0.547836.
Train: 2018-08-01T23:19:01.115226: step 7071, loss 0.492748.
Train: 2018-08-01T23:19:01.280813: step 7072, loss 0.614833.
Train: 2018-08-01T23:19:01.440386: step 7073, loss 0.474959.
Train: 2018-08-01T23:19:01.618879: step 7074, loss 0.562453.
Train: 2018-08-01T23:19:01.788425: step 7075, loss 0.597637.
Train: 2018-08-01T23:19:01.950025: step 7076, loss 0.509638.
Train: 2018-08-01T23:19:02.113557: step 7077, loss 0.456596.
Train: 2018-08-01T23:19:02.279114: step 7078, loss 0.491684.
Train: 2018-08-01T23:19:02.442677: step 7079, loss 0.491441.
Train: 2018-08-01T23:19:02.612224: step 7080, loss 0.598286.
Test: 2018-08-01T23:19:03.156768: step 7080, loss 0.547614.
Train: 2018-08-01T23:19:03.331302: step 7081, loss 0.687992.
Train: 2018-08-01T23:19:03.500848: step 7082, loss 0.544694.
Train: 2018-08-01T23:19:03.659449: step 7083, loss 0.616477.
Train: 2018-08-01T23:19:03.827006: step 7084, loss 0.634426.
Train: 2018-08-01T23:19:03.995524: step 7085, loss 0.526767.
Train: 2018-08-01T23:19:04.171082: step 7086, loss 0.598445.
Train: 2018-08-01T23:19:04.343595: step 7087, loss 0.705741.
Train: 2018-08-01T23:19:04.508180: step 7088, loss 0.50907.
Train: 2018-08-01T23:19:04.671749: step 7089, loss 0.598132.
Train: 2018-08-01T23:19:04.852235: step 7090, loss 0.704468.
Test: 2018-08-01T23:19:05.392789: step 7090, loss 0.547704.
Train: 2018-08-01T23:19:05.561371: step 7091, loss 0.509511.
Train: 2018-08-01T23:19:05.738893: step 7092, loss 0.668062.
Train: 2018-08-01T23:19:05.910407: step 7093, loss 0.54493.
Train: 2018-08-01T23:19:06.076960: step 7094, loss 0.49266.
Train: 2018-08-01T23:19:06.240524: step 7095, loss 0.579811.
Train: 2018-08-01T23:19:06.406081: step 7096, loss 0.649132.
Train: 2018-08-01T23:19:06.572662: step 7097, loss 0.579679.
Train: 2018-08-01T23:19:06.741186: step 7098, loss 0.562397.
Train: 2018-08-01T23:19:06.907740: step 7099, loss 0.682468.
Train: 2018-08-01T23:19:07.068320: step 7100, loss 0.494124.
Test: 2018-08-01T23:19:07.596929: step 7100, loss 0.548159.
Train: 2018-08-01T23:19:08.354655: step 7101, loss 0.562404.
Train: 2018-08-01T23:19:08.522208: step 7102, loss 0.562411.
Train: 2018-08-01T23:19:08.686740: step 7103, loss 0.511677.
Train: 2018-08-01T23:19:08.850334: step 7104, loss 0.477989.
Train: 2018-08-01T23:19:09.023840: step 7105, loss 0.663742.
Train: 2018-08-01T23:19:09.190395: step 7106, loss 0.51185.
Train: 2018-08-01T23:19:09.351963: step 7107, loss 0.47818.
Train: 2018-08-01T23:19:09.519514: step 7108, loss 0.545563.
Train: 2018-08-01T23:19:09.688077: step 7109, loss 0.562424.
Train: 2018-08-01T23:19:09.862631: step 7110, loss 0.697641.
Test: 2018-08-01T23:19:10.405173: step 7110, loss 0.548286.
Train: 2018-08-01T23:19:10.564746: step 7111, loss 0.596192.
Train: 2018-08-01T23:19:10.725322: step 7112, loss 0.596148.
Train: 2018-08-01T23:19:10.890849: step 7113, loss 0.579266.
Train: 2018-08-01T23:19:11.063423: step 7114, loss 0.562445.
Train: 2018-08-01T23:19:11.225959: step 7115, loss 0.579224.
Train: 2018-08-01T23:19:11.396503: step 7116, loss 0.59595.
Train: 2018-08-01T23:19:11.560092: step 7117, loss 0.612609.
Train: 2018-08-01T23:19:11.727649: step 7118, loss 0.529142.
Train: 2018-08-01T23:19:11.894174: step 7119, loss 0.612434.
Train: 2018-08-01T23:19:12.066711: step 7120, loss 0.645566.
Test: 2018-08-01T23:19:12.602281: step 7120, loss 0.548668.
Train: 2018-08-01T23:19:12.771827: step 7121, loss 0.545975.
Train: 2018-08-01T23:19:12.937416: step 7122, loss 0.47996.
Train: 2018-08-01T23:19:13.101972: step 7123, loss 0.529545.
Train: 2018-08-01T23:19:13.267501: step 7124, loss 0.579066.
Train: 2018-08-01T23:19:13.431096: step 7125, loss 0.546053.
Train: 2018-08-01T23:19:13.594652: step 7126, loss 0.661628.
Train: 2018-08-01T23:19:13.769185: step 7127, loss 0.66152.
Train: 2018-08-01T23:19:13.946712: step 7128, loss 0.579039.
Train: 2018-08-01T23:19:14.123214: step 7129, loss 0.628249.
Train: 2018-08-01T23:19:14.288789: step 7130, loss 0.480854.
Test: 2018-08-01T23:19:14.827332: step 7130, loss 0.548976.
Train: 2018-08-01T23:19:14.991923: step 7131, loss 0.628002.
Train: 2018-08-01T23:19:15.162448: step 7132, loss 0.513771.
Train: 2018-08-01T23:19:15.336004: step 7133, loss 0.692993.
Train: 2018-08-01T23:19:15.504547: step 7134, loss 0.49775.
Train: 2018-08-01T23:19:15.671104: step 7135, loss 0.578957.
Train: 2018-08-01T23:19:15.841647: step 7136, loss 0.465529.
Train: 2018-08-01T23:19:16.019147: step 7137, loss 0.51409.
Train: 2018-08-01T23:19:16.188719: step 7138, loss 0.530229.
Train: 2018-08-01T23:19:16.365247: step 7139, loss 0.578974.
Train: 2018-08-01T23:19:16.538758: step 7140, loss 0.578985.
Test: 2018-08-01T23:19:17.082304: step 7140, loss 0.548967.
Train: 2018-08-01T23:19:17.254842: step 7141, loss 0.628024.
Train: 2018-08-01T23:19:17.420401: step 7142, loss 0.611704.
Train: 2018-08-01T23:19:17.580971: step 7143, loss 0.497243.
Train: 2018-08-01T23:19:17.747556: step 7144, loss 0.431674.
Train: 2018-08-01T23:19:17.910124: step 7145, loss 0.480473.
Train: 2018-08-01T23:19:18.077676: step 7146, loss 0.579063.
Train: 2018-08-01T23:19:18.240241: step 7147, loss 0.545964.
Train: 2018-08-01T23:19:18.402773: step 7148, loss 0.495974.
Train: 2018-08-01T23:19:18.562347: step 7149, loss 0.612604.
Train: 2018-08-01T23:19:18.722943: step 7150, loss 0.495376.
Test: 2018-08-01T23:19:19.260480: step 7150, loss 0.548331.
Train: 2018-08-01T23:19:19.426064: step 7151, loss 0.579275.
Train: 2018-08-01T23:19:19.595585: step 7152, loss 0.579324.
Train: 2018-08-01T23:19:19.758175: step 7153, loss 0.545456.
Train: 2018-08-01T23:19:19.922736: step 7154, loss 0.562404.
Train: 2018-08-01T23:19:20.082283: step 7155, loss 0.630617.
Train: 2018-08-01T23:19:20.255819: step 7156, loss 0.442843.
Train: 2018-08-01T23:19:20.423373: step 7157, loss 0.562396.
Train: 2018-08-01T23:19:20.587963: step 7158, loss 0.579578.
Train: 2018-08-01T23:19:20.752492: step 7159, loss 0.493512.
Train: 2018-08-01T23:19:20.918050: step 7160, loss 0.527852.
Test: 2018-08-01T23:19:21.448632: step 7160, loss 0.547895.
Train: 2018-08-01T23:19:21.608229: step 7161, loss 0.63173.
Train: 2018-08-01T23:19:21.771794: step 7162, loss 0.614501.
Train: 2018-08-01T23:19:21.944306: step 7163, loss 0.492903.
Train: 2018-08-01T23:19:22.120866: step 7164, loss 0.579828.
Train: 2018-08-01T23:19:22.286423: step 7165, loss 0.649576.
Train: 2018-08-01T23:19:22.455970: step 7166, loss 0.57985.
Train: 2018-08-01T23:19:22.646429: step 7167, loss 0.68435.
Train: 2018-08-01T23:19:22.821703: step 7168, loss 0.579793.
Train: 2018-08-01T23:19:22.986557: step 7169, loss 0.579745.
Train: 2018-08-01T23:19:23.147713: step 7170, loss 0.596993.
Test: 2018-08-01T23:19:23.695267: step 7170, loss 0.547956.
Train: 2018-08-01T23:19:23.868801: step 7171, loss 0.527903.
Train: 2018-08-01T23:19:24.042307: step 7172, loss 0.614038.
Train: 2018-08-01T23:19:24.206902: step 7173, loss 0.510887.
Train: 2018-08-01T23:19:24.383420: step 7174, loss 0.562396.
Train: 2018-08-01T23:19:24.544994: step 7175, loss 0.647992.
Train: 2018-08-01T23:19:24.707528: step 7176, loss 0.442869.
Train: 2018-08-01T23:19:24.874114: step 7177, loss 0.545331.
Train: 2018-08-01T23:19:25.035684: step 7178, loss 0.545333.
Train: 2018-08-01T23:19:25.197245: step 7179, loss 0.613605.
Train: 2018-08-01T23:19:25.364772: step 7180, loss 0.425933.
Test: 2018-08-01T23:19:25.907320: step 7180, loss 0.548091.
Train: 2018-08-01T23:19:26.076899: step 7181, loss 0.494058.
Train: 2018-08-01T23:19:26.244445: step 7182, loss 0.511015.
Train: 2018-08-01T23:19:26.407983: step 7183, loss 0.493681.
Train: 2018-08-01T23:19:26.572568: step 7184, loss 0.527913.
Train: 2018-08-01T23:19:26.734142: step 7185, loss 0.64895.
Train: 2018-08-01T23:19:26.897674: step 7186, loss 0.47568.
Train: 2018-08-01T23:19:27.060239: step 7187, loss 0.510216.
Train: 2018-08-01T23:19:27.222805: step 7188, loss 0.667207.
Train: 2018-08-01T23:19:27.386367: step 7189, loss 0.61491.
Train: 2018-08-01T23:19:27.557934: step 7190, loss 0.579942.
Test: 2018-08-01T23:19:28.097466: step 7190, loss 0.547784.
Train: 2018-08-01T23:19:28.272028: step 7191, loss 0.597452.
Train: 2018-08-01T23:19:28.439552: step 7192, loss 0.544942.
Train: 2018-08-01T23:19:28.601120: step 7193, loss 0.56244.
Train: 2018-08-01T23:19:28.763718: step 7194, loss 0.57993.
Train: 2018-08-01T23:19:28.930241: step 7195, loss 0.719783.
Train: 2018-08-01T23:19:29.099818: step 7196, loss 0.527557.
Train: 2018-08-01T23:19:29.271340: step 7197, loss 0.54502.
Train: 2018-08-01T23:19:29.438880: step 7198, loss 0.545044.
Train: 2018-08-01T23:19:29.602470: step 7199, loss 0.527717.
Train: 2018-08-01T23:19:29.768026: step 7200, loss 0.510405.
Test: 2018-08-01T23:19:30.310604: step 7200, loss 0.547891.
Train: 2018-08-01T23:19:31.123014: step 7201, loss 0.545072.
Train: 2018-08-01T23:19:31.302534: step 7202, loss 0.66646.
Train: 2018-08-01T23:19:31.471084: step 7203, loss 0.545086.
Train: 2018-08-01T23:19:31.631173: step 7204, loss 0.597011.
Train: 2018-08-01T23:19:31.795144: step 7205, loss 0.61424.
Train: 2018-08-01T23:19:31.960685: step 7206, loss 0.476178.
Train: 2018-08-01T23:19:32.133191: step 7207, loss 0.6141.
Train: 2018-08-01T23:19:32.296786: step 7208, loss 0.631243.
Train: 2018-08-01T23:19:32.462337: step 7209, loss 0.528049.
Train: 2018-08-01T23:19:32.632886: step 7210, loss 0.5281.
Test: 2018-08-01T23:19:33.169421: step 7210, loss 0.548048.
Train: 2018-08-01T23:19:33.339965: step 7211, loss 0.44246.
Train: 2018-08-01T23:19:33.505524: step 7212, loss 0.579548.
Train: 2018-08-01T23:19:33.668114: step 7213, loss 0.562396.
Train: 2018-08-01T23:19:33.837660: step 7214, loss 0.66545.
Train: 2018-08-01T23:19:34.001222: step 7215, loss 0.545237.
Train: 2018-08-01T23:19:34.161794: step 7216, loss 0.61384.
Train: 2018-08-01T23:19:34.327325: step 7217, loss 0.596646.
Train: 2018-08-01T23:19:34.499865: step 7218, loss 0.579493.
Train: 2018-08-01T23:19:34.666420: step 7219, loss 0.596528.
Train: 2018-08-01T23:19:34.831977: step 7220, loss 0.545374.
Test: 2018-08-01T23:19:35.355577: step 7220, loss 0.548169.
Train: 2018-08-01T23:19:35.517145: step 7221, loss 0.579405.
Train: 2018-08-01T23:19:35.684725: step 7222, loss 0.511495.
Train: 2018-08-01T23:19:35.851283: step 7223, loss 0.511533.
Train: 2018-08-01T23:19:36.015813: step 7224, loss 0.580503.
Train: 2018-08-01T23:19:36.182399: step 7225, loss 0.613294.
Train: 2018-08-01T23:19:36.344932: step 7226, loss 0.44378.
Train: 2018-08-01T23:19:36.507498: step 7227, loss 0.494541.
Train: 2018-08-01T23:19:36.682031: step 7228, loss 0.545401.
Train: 2018-08-01T23:19:36.852575: step 7229, loss 0.630564.
Train: 2018-08-01T23:19:37.019131: step 7230, loss 0.562399.
Test: 2018-08-01T23:19:37.545253: step 7230, loss 0.548102.
Train: 2018-08-01T23:19:37.714798: step 7231, loss 0.630687.
Train: 2018-08-01T23:19:37.888334: step 7232, loss 0.494126.
Train: 2018-08-01T23:19:38.049927: step 7233, loss 0.545315.
Train: 2018-08-01T23:19:38.214492: step 7234, loss 0.511095.
Train: 2018-08-01T23:19:38.387027: step 7235, loss 0.579526.
Train: 2018-08-01T23:19:38.548568: step 7236, loss 0.493786.
Train: 2018-08-01T23:19:38.709139: step 7237, loss 0.596777.
Train: 2018-08-01T23:19:38.878685: step 7238, loss 0.545181.
Train: 2018-08-01T23:19:39.045241: step 7239, loss 0.545156.
Train: 2018-08-01T23:19:39.212792: step 7240, loss 0.54513.
Test: 2018-08-01T23:19:39.750356: step 7240, loss 0.547916.
Train: 2018-08-01T23:19:39.921897: step 7241, loss 0.458599.
Train: 2018-08-01T23:19:40.097427: step 7242, loss 0.545057.
Train: 2018-08-01T23:19:40.265010: step 7243, loss 0.649446.
Train: 2018-08-01T23:19:40.433530: step 7244, loss 0.666995.
Train: 2018-08-01T23:19:40.600084: step 7245, loss 0.475314.
Train: 2018-08-01T23:19:40.765642: step 7246, loss 0.64961.
Train: 2018-08-01T23:19:40.927210: step 7247, loss 0.544996.
Train: 2018-08-01T23:19:41.094786: step 7248, loss 0.63211.
Train: 2018-08-01T23:19:41.269295: step 7249, loss 0.614614.
Train: 2018-08-01T23:19:41.435849: step 7250, loss 0.527685.
Test: 2018-08-01T23:19:41.973423: step 7250, loss 0.547888.
Train: 2018-08-01T23:19:42.136976: step 7251, loss 0.510389.
Train: 2018-08-01T23:19:42.303561: step 7252, loss 0.701057.
Train: 2018-08-01T23:19:42.479060: step 7253, loss 0.683414.
Train: 2018-08-01T23:19:42.645640: step 7254, loss 0.648478.
Train: 2018-08-01T23:19:42.809200: step 7255, loss 0.476746.
Train: 2018-08-01T23:19:42.976741: step 7256, loss 0.545324.
Train: 2018-08-01T23:19:43.135307: step 7257, loss 0.511314.
Train: 2018-08-01T23:19:43.313860: step 7258, loss 0.630415.
Train: 2018-08-01T23:19:43.481393: step 7259, loss 0.579371.
Train: 2018-08-01T23:19:43.647962: step 7260, loss 0.460897.
Test: 2018-08-01T23:19:44.186497: step 7260, loss 0.548258.
Train: 2018-08-01T23:19:44.354049: step 7261, loss 0.663883.
Train: 2018-08-01T23:19:44.514620: step 7262, loss 0.54555.
Train: 2018-08-01T23:19:44.676216: step 7263, loss 0.410774.
Train: 2018-08-01T23:19:44.842744: step 7264, loss 0.579297.
Train: 2018-08-01T23:19:45.004336: step 7265, loss 0.494897.
Train: 2018-08-01T23:19:45.169868: step 7266, loss 0.663891.
Train: 2018-08-01T23:19:45.341410: step 7267, loss 0.460937.
Train: 2018-08-01T23:19:45.510990: step 7268, loss 0.46076.
Train: 2018-08-01T23:19:45.675516: step 7269, loss 0.562406.
Train: 2018-08-01T23:19:45.840078: step 7270, loss 0.579444.
Test: 2018-08-01T23:19:46.374648: step 7270, loss 0.548092.
Train: 2018-08-01T23:19:46.543197: step 7271, loss 0.511148.
Train: 2018-08-01T23:19:46.718727: step 7272, loss 0.510999.
Train: 2018-08-01T23:19:46.897250: step 7273, loss 0.493636.
Train: 2018-08-01T23:19:47.061810: step 7274, loss 0.54514.
Train: 2018-08-01T23:19:47.232354: step 7275, loss 0.614385.
Train: 2018-08-01T23:19:47.396939: step 7276, loss 0.492925.
Train: 2018-08-01T23:19:47.558525: step 7277, loss 0.544994.
Train: 2018-08-01T23:19:47.720051: step 7278, loss 0.562437.
Train: 2018-08-01T23:19:47.882616: step 7279, loss 0.615059.
Train: 2018-08-01T23:19:48.059144: step 7280, loss 0.56246.
Test: 2018-08-01T23:19:48.586734: step 7280, loss 0.547737.
Train: 2018-08-01T23:19:48.754286: step 7281, loss 0.597651.
Train: 2018-08-01T23:19:48.921838: step 7282, loss 0.50966.
Train: 2018-08-01T23:19:49.085434: step 7283, loss 0.615362.
Train: 2018-08-01T23:19:49.248989: step 7284, loss 0.527216.
Train: 2018-08-01T23:19:49.409565: step 7285, loss 0.544841.
Train: 2018-08-01T23:19:49.576114: step 7286, loss 0.703773.
Train: 2018-08-01T23:19:49.752617: step 7287, loss 0.597754.
Train: 2018-08-01T23:19:49.914186: step 7288, loss 0.562472.
Train: 2018-08-01T23:19:50.079743: step 7289, loss 0.544889.
Train: 2018-08-01T23:19:50.247298: step 7290, loss 0.632644.
Test: 2018-08-01T23:19:50.770895: step 7290, loss 0.547783.
Train: 2018-08-01T23:19:50.934458: step 7291, loss 0.562442.
Train: 2018-08-01T23:19:51.104008: step 7292, loss 0.510029.
Train: 2018-08-01T23:19:51.268595: step 7293, loss 0.579873.
Train: 2018-08-01T23:19:51.429136: step 7294, loss 0.632102.
Train: 2018-08-01T23:19:51.606660: step 7295, loss 0.527658.
Train: 2018-08-01T23:19:51.768255: step 7296, loss 0.614457.
Train: 2018-08-01T23:19:51.933818: step 7297, loss 0.579713.
Train: 2018-08-01T23:19:52.100340: step 7298, loss 0.545133.
Train: 2018-08-01T23:19:52.268890: step 7299, loss 0.596866.
Train: 2018-08-01T23:19:52.449409: step 7300, loss 0.545201.
Test: 2018-08-01T23:19:52.992965: step 7300, loss 0.548022.
Train: 2018-08-01T23:19:53.884584: step 7301, loss 0.545231.
Train: 2018-08-01T23:19:54.059152: step 7302, loss 0.528114.
Train: 2018-08-01T23:19:54.224706: step 7303, loss 0.528138.
Train: 2018-08-01T23:19:54.392267: step 7304, loss 0.442505.
Train: 2018-08-01T23:19:54.559780: step 7305, loss 0.562396.
Train: 2018-08-01T23:19:54.741295: step 7306, loss 0.648314.
Train: 2018-08-01T23:19:54.903860: step 7307, loss 0.596766.
Train: 2018-08-01T23:19:55.079391: step 7308, loss 0.596751.
Train: 2018-08-01T23:19:55.249963: step 7309, loss 0.493748.
Train: 2018-08-01T23:19:55.418509: step 7310, loss 0.613892.
Test: 2018-08-01T23:19:55.944080: step 7310, loss 0.54803.
Train: 2018-08-01T23:19:56.110633: step 7311, loss 0.596707.
Train: 2018-08-01T23:19:56.273230: step 7312, loss 0.476708.
Train: 2018-08-01T23:19:56.432771: step 7313, loss 0.562396.
Train: 2018-08-01T23:19:56.597333: step 7314, loss 0.648141.
Train: 2018-08-01T23:19:56.764884: step 7315, loss 0.59666.
Train: 2018-08-01T23:19:56.936451: step 7316, loss 0.613719.
Train: 2018-08-01T23:19:57.113958: step 7317, loss 0.545326.
Train: 2018-08-01T23:19:57.306034: step 7318, loss 0.647628.
Train: 2018-08-01T23:19:57.480723: step 7319, loss 0.681401.
Train: 2018-08-01T23:19:57.645259: step 7320, loss 0.443924.
Test: 2018-08-01T23:19:58.189804: step 7320, loss 0.548275.
Train: 2018-08-01T23:19:58.357387: step 7321, loss 0.545527.
Train: 2018-08-01T23:19:58.520919: step 7322, loss 0.562427.
Train: 2018-08-01T23:19:58.687485: step 7323, loss 0.545582.
Train: 2018-08-01T23:19:58.852033: step 7324, loss 0.562435.
Train: 2018-08-01T23:19:59.016623: step 7325, loss 0.562438.
Train: 2018-08-01T23:19:59.183147: step 7326, loss 0.528806.
Train: 2018-08-01T23:19:59.345714: step 7327, loss 0.54562.
Train: 2018-08-01T23:19:59.514290: step 7328, loss 0.461476.
Train: 2018-08-01T23:19:59.684838: step 7329, loss 0.613014.
Train: 2018-08-01T23:19:59.852359: step 7330, loss 0.478022.
Test: 2018-08-01T23:20:00.387926: step 7330, loss 0.548248.
Train: 2018-08-01T23:20:00.550519: step 7331, loss 0.545496.
Train: 2018-08-01T23:20:00.713059: step 7332, loss 0.63026.
Train: 2018-08-01T23:20:00.888588: step 7333, loss 0.562407.
Train: 2018-08-01T23:20:01.060161: step 7334, loss 0.579406.
Train: 2018-08-01T23:20:01.222727: step 7335, loss 0.545389.
Train: 2018-08-01T23:20:01.392268: step 7336, loss 0.613495.
Train: 2018-08-01T23:20:01.554838: step 7337, loss 0.545368.
Train: 2018-08-01T23:20:01.719368: step 7338, loss 0.596481.
Train: 2018-08-01T23:20:01.878941: step 7339, loss 0.545363.
Train: 2018-08-01T23:20:02.050483: step 7340, loss 0.596481.
Test: 2018-08-01T23:20:02.592035: step 7340, loss 0.548137.
Train: 2018-08-01T23:20:02.761588: step 7341, loss 0.5113.
Train: 2018-08-01T23:20:02.928138: step 7342, loss 0.545358.
Train: 2018-08-01T23:20:03.095689: step 7343, loss 0.579454.
Train: 2018-08-01T23:20:03.258271: step 7344, loss 0.596522.
Train: 2018-08-01T23:20:03.421847: step 7345, loss 0.528281.
Train: 2018-08-01T23:20:03.588399: step 7346, loss 0.511199.
Train: 2018-08-01T23:20:03.754957: step 7347, loss 0.562397.
Train: 2018-08-01T23:20:03.918488: step 7348, loss 0.545292.
Train: 2018-08-01T23:20:04.085044: step 7349, loss 0.562396.
Train: 2018-08-01T23:20:04.250629: step 7350, loss 0.528104.
Test: 2018-08-01T23:20:04.788165: step 7350, loss 0.548016.
Train: 2018-08-01T23:20:04.953723: step 7351, loss 0.425021.
Train: 2018-08-01T23:20:05.119280: step 7352, loss 0.493473.
Train: 2018-08-01T23:20:05.289823: step 7353, loss 0.579707.
Train: 2018-08-01T23:20:05.456397: step 7354, loss 0.562412.
Train: 2018-08-01T23:20:05.620937: step 7355, loss 0.562421.
Train: 2018-08-01T23:20:05.784527: step 7356, loss 0.562431.
Train: 2018-08-01T23:20:05.946100: step 7357, loss 0.649945.
Train: 2018-08-01T23:20:06.115634: step 7358, loss 0.562445.
Train: 2018-08-01T23:20:06.286160: step 7359, loss 0.527399.
Train: 2018-08-01T23:20:06.455707: step 7360, loss 0.579994.
Test: 2018-08-01T23:20:06.987300: step 7360, loss 0.547759.
Train: 2018-08-01T23:20:07.160822: step 7361, loss 0.439599.
Train: 2018-08-01T23:20:07.331366: step 7362, loss 0.544878.
Train: 2018-08-01T23:20:07.494954: step 7363, loss 0.615364.
Train: 2018-08-01T23:20:07.656521: step 7364, loss 0.509549.
Train: 2018-08-01T23:20:07.819062: step 7365, loss 0.633201.
Train: 2018-08-01T23:20:07.988609: step 7366, loss 0.580184.
Train: 2018-08-01T23:20:08.153183: step 7367, loss 0.544821.
Train: 2018-08-01T23:20:08.319751: step 7368, loss 0.562503.
Train: 2018-08-01T23:20:08.482292: step 7369, loss 0.633236.
Train: 2018-08-01T23:20:08.649841: step 7370, loss 0.615486.
Test: 2018-08-01T23:20:09.175439: step 7370, loss 0.547719.
Train: 2018-08-01T23:20:09.346978: step 7371, loss 0.544852.
Train: 2018-08-01T23:20:09.514542: step 7372, loss 0.738514.
Train: 2018-08-01T23:20:09.679117: step 7373, loss 0.562451.
Train: 2018-08-01T23:20:09.844676: step 7374, loss 0.440148.
Train: 2018-08-01T23:20:10.010204: step 7375, loss 0.562427.
Train: 2018-08-01T23:20:10.181745: step 7376, loss 0.510175.
Train: 2018-08-01T23:20:10.344325: step 7377, loss 0.579825.
Train: 2018-08-01T23:20:10.503917: step 7378, loss 0.597196.
Train: 2018-08-01T23:20:10.680413: step 7379, loss 0.614512.
Train: 2018-08-01T23:20:10.850957: step 7380, loss 0.579739.
Test: 2018-08-01T23:20:11.400488: step 7380, loss 0.547921.
Train: 2018-08-01T23:20:11.572029: step 7381, loss 0.596992.
Train: 2018-08-01T23:20:11.741575: step 7382, loss 0.545147.
Train: 2018-08-01T23:20:11.907134: step 7383, loss 0.476307.
Train: 2018-08-01T23:20:12.071693: step 7384, loss 0.476349.
Train: 2018-08-01T23:20:12.238249: step 7385, loss 0.52795.
Train: 2018-08-01T23:20:12.400814: step 7386, loss 0.631382.
Train: 2018-08-01T23:20:12.566372: step 7387, loss 0.579646.
Train: 2018-08-01T23:20:12.735942: step 7388, loss 0.61413.
Train: 2018-08-01T23:20:12.894523: step 7389, loss 0.562398.
Train: 2018-08-01T23:20:13.057059: step 7390, loss 0.631242.
Test: 2018-08-01T23:20:13.599608: step 7390, loss 0.54801.
Train: 2018-08-01T23:20:13.789103: step 7391, loss 0.579575.
Train: 2018-08-01T23:20:13.957685: step 7392, loss 0.596688.
Train: 2018-08-01T23:20:14.120218: step 7393, loss 0.699253.
Train: 2018-08-01T23:20:14.286798: step 7394, loss 0.562401.
Train: 2018-08-01T23:20:14.454324: step 7395, loss 0.562409.
Train: 2018-08-01T23:20:14.618920: step 7396, loss 0.528584.
Train: 2018-08-01T23:20:14.784461: step 7397, loss 0.494921.
Train: 2018-08-01T23:20:14.945045: step 7398, loss 0.680438.
Train: 2018-08-01T23:20:15.106613: step 7399, loss 0.528817.
Train: 2018-08-01T23:20:15.273136: step 7400, loss 0.646352.
Test: 2018-08-01T23:20:15.806708: step 7400, loss 0.548458.
Train: 2018-08-01T23:20:16.579266: step 7401, loss 0.679562.
Train: 2018-08-01T23:20:16.746817: step 7402, loss 0.662278.
Train: 2018-08-01T23:20:16.907413: step 7403, loss 0.660773.
Train: 2018-08-01T23:20:17.078955: step 7404, loss 0.627593.
Train: 2018-08-01T23:20:17.249472: step 7405, loss 0.611672.
Train: 2018-08-01T23:20:17.411068: step 7406, loss 0.485227.
Train: 2018-08-01T23:20:17.580621: step 7407, loss 0.549467.
Train: 2018-08-01T23:20:17.746146: step 7408, loss 0.549482.
Train: 2018-08-01T23:20:17.913722: step 7409, loss 0.54826.
Train: 2018-08-01T23:20:18.076288: step 7410, loss 0.53244.
Test: 2018-08-01T23:20:18.609837: step 7410, loss 0.549643.
Train: 2018-08-01T23:20:18.781378: step 7411, loss 0.53108.
Train: 2018-08-01T23:20:18.951963: step 7412, loss 0.562865.
Train: 2018-08-01T23:20:19.116710: step 7413, loss 0.546785.
Train: 2018-08-01T23:20:19.280341: step 7414, loss 0.611093.
Train: 2018-08-01T23:20:19.445906: step 7415, loss 0.54675.
Train: 2018-08-01T23:20:19.609470: step 7416, loss 0.546726.
Train: 2018-08-01T23:20:19.774992: step 7417, loss 0.562812.
Train: 2018-08-01T23:20:19.944563: step 7418, loss 0.530507.
Train: 2018-08-01T23:20:20.115115: step 7419, loss 0.546595.
Train: 2018-08-01T23:20:20.283661: step 7420, loss 0.497891.
Test: 2018-08-01T23:20:20.816210: step 7420, loss 0.549083.
Train: 2018-08-01T23:20:20.983786: step 7421, loss 0.546438.
Train: 2018-08-01T23:20:21.148322: step 7422, loss 0.497373.
Train: 2018-08-01T23:20:21.314876: step 7423, loss 0.464252.
Train: 2018-08-01T23:20:21.483454: step 7424, loss 0.628529.
Train: 2018-08-01T23:20:21.648983: step 7425, loss 0.562539.
Train: 2018-08-01T23:20:21.812546: step 7426, loss 0.595755.
Train: 2018-08-01T23:20:21.984087: step 7427, loss 0.679199.
Train: 2018-08-01T23:20:22.148678: step 7428, loss 0.562486.
Train: 2018-08-01T23:20:22.321196: step 7429, loss 0.545776.
Train: 2018-08-01T23:20:22.484749: step 7430, loss 0.529024.
Test: 2018-08-01T23:20:23.022321: step 7430, loss 0.548436.
Train: 2018-08-01T23:20:23.186872: step 7431, loss 0.528958.
Train: 2018-08-01T23:20:23.358414: step 7432, loss 0.512085.
Train: 2018-08-01T23:20:23.533964: step 7433, loss 0.562442.
Train: 2018-08-01T23:20:23.696510: step 7434, loss 0.461147.
Train: 2018-08-01T23:20:23.862067: step 7435, loss 0.647159.
Train: 2018-08-01T23:20:24.030617: step 7436, loss 0.545428.
Train: 2018-08-01T23:20:24.199179: step 7437, loss 0.56241.
Train: 2018-08-01T23:20:24.368713: step 7438, loss 0.528281.
Train: 2018-08-01T23:20:24.532276: step 7439, loss 0.630824.
Train: 2018-08-01T23:20:24.694843: step 7440, loss 0.562404.
Test: 2018-08-01T23:20:25.216447: step 7440, loss 0.548049.
Train: 2018-08-01T23:20:25.382032: step 7441, loss 0.528118.
Train: 2018-08-01T23:20:25.545579: step 7442, loss 0.510899.
Train: 2018-08-01T23:20:25.709130: step 7443, loss 0.5452.
Train: 2018-08-01T23:20:25.881667: step 7444, loss 0.562407.
Train: 2018-08-01T23:20:26.045230: step 7445, loss 0.545135.
Train: 2018-08-01T23:20:26.210814: step 7446, loss 0.545105.
Train: 2018-08-01T23:20:26.373387: step 7447, loss 0.527732.
Train: 2018-08-01T23:20:26.544926: step 7448, loss 0.562424.
Train: 2018-08-01T23:20:26.711476: step 7449, loss 0.527597.
Train: 2018-08-01T23:20:26.878029: step 7450, loss 0.614806.
Test: 2018-08-01T23:20:27.408587: step 7450, loss 0.547809.
Train: 2018-08-01T23:20:27.571181: step 7451, loss 0.667304.
Train: 2018-08-01T23:20:27.741695: step 7452, loss 0.614844.
Train: 2018-08-01T23:20:27.904292: step 7453, loss 0.579881.
Train: 2018-08-01T23:20:28.079791: step 7454, loss 0.579849.
Train: 2018-08-01T23:20:28.251333: step 7455, loss 0.545036.
Train: 2018-08-01T23:20:28.417913: step 7456, loss 0.579789.
Train: 2018-08-01T23:20:28.577462: step 7457, loss 0.510392.
Train: 2018-08-01T23:20:28.742046: step 7458, loss 0.579748.
Train: 2018-08-01T23:20:28.907609: step 7459, loss 0.579732.
Train: 2018-08-01T23:20:29.076129: step 7460, loss 0.493211.
Test: 2018-08-01T23:20:29.611697: step 7460, loss 0.547924.
Train: 2018-08-01T23:20:29.780246: step 7461, loss 0.493205.
Train: 2018-08-01T23:20:29.941816: step 7462, loss 0.44117.
Train: 2018-08-01T23:20:30.111387: step 7463, loss 0.527685.
Train: 2018-08-01T23:20:30.282902: step 7464, loss 0.632111.
Train: 2018-08-01T23:20:30.450480: step 7465, loss 0.579884.
Train: 2018-08-01T23:20:30.611053: step 7466, loss 0.562441.
Train: 2018-08-01T23:20:30.769614: step 7467, loss 0.649857.
Train: 2018-08-01T23:20:30.935183: step 7468, loss 0.527496.
Train: 2018-08-01T23:20:31.098722: step 7469, loss 0.614862.
Train: 2018-08-01T23:20:31.281233: step 7470, loss 0.684648.
Test: 2018-08-01T23:20:31.823784: step 7470, loss 0.547848.
Train: 2018-08-01T23:20:31.991336: step 7471, loss 0.63208.
Train: 2018-08-01T23:20:32.160883: step 7472, loss 0.475656.
Train: 2018-08-01T23:20:32.329432: step 7473, loss 0.527774.
Train: 2018-08-01T23:20:32.493020: step 7474, loss 0.562412.
Train: 2018-08-01T23:20:32.665564: step 7475, loss 0.56241.
Train: 2018-08-01T23:20:32.847074: step 7476, loss 0.562409.
Train: 2018-08-01T23:20:33.023576: step 7477, loss 0.441678.
Train: 2018-08-01T23:20:33.194120: step 7478, loss 0.545145.
Train: 2018-08-01T23:20:33.364681: step 7479, loss 0.631541.
Train: 2018-08-01T23:20:33.530221: step 7480, loss 0.545129.
Test: 2018-08-01T23:20:34.063795: step 7480, loss 0.547937.
Train: 2018-08-01T23:20:34.243338: step 7481, loss 0.493273.
Train: 2018-08-01T23:20:34.407901: step 7482, loss 0.597021.
Train: 2018-08-01T23:20:34.571439: step 7483, loss 0.475849.
Train: 2018-08-01T23:20:34.756942: step 7484, loss 0.545075.
Train: 2018-08-01T23:20:34.923497: step 7485, loss 0.597169.
Train: 2018-08-01T23:20:35.098030: step 7486, loss 0.579816.
Train: 2018-08-01T23:20:35.283535: step 7487, loss 0.545025.
Train: 2018-08-01T23:20:35.530874: step 7488, loss 0.59726.
Train: 2018-08-01T23:20:35.737322: step 7489, loss 0.527596.
Train: 2018-08-01T23:20:35.925818: step 7490, loss 0.579859.
Test: 2018-08-01T23:20:36.461386: step 7490, loss 0.547837.
Train: 2018-08-01T23:20:36.627940: step 7491, loss 0.49271.
Train: 2018-08-01T23:20:36.798484: step 7492, loss 0.510084.
Train: 2018-08-01T23:20:36.965052: step 7493, loss 0.597408.
Train: 2018-08-01T23:20:37.127633: step 7494, loss 0.474953.
Train: 2018-08-01T23:20:37.290171: step 7495, loss 0.492314.
Train: 2018-08-01T23:20:37.455728: step 7496, loss 0.474543.
Train: 2018-08-01T23:20:37.618326: step 7497, loss 0.509541.
Train: 2018-08-01T23:20:37.787866: step 7498, loss 0.633421.
Train: 2018-08-01T23:20:37.968358: step 7499, loss 0.562546.
Train: 2018-08-01T23:20:38.132936: step 7500, loss 0.526953.
Test: 2018-08-01T23:20:38.659510: step 7500, loss 0.547641.
Train: 2018-08-01T23:20:39.441647: step 7501, loss 0.616124.
Train: 2018-08-01T23:20:39.607204: step 7502, loss 0.580463.
Train: 2018-08-01T23:20:39.771788: step 7503, loss 0.526845.
Train: 2018-08-01T23:20:39.939346: step 7504, loss 0.562612.
Train: 2018-08-01T23:20:40.100892: step 7505, loss 0.687981.
Train: 2018-08-01T23:20:40.275448: step 7506, loss 0.54472.
Train: 2018-08-01T23:20:40.439010: step 7507, loss 0.63407.
Train: 2018-08-01T23:20:40.602543: step 7508, loss 0.491252.
Train: 2018-08-01T23:20:40.778073: step 7509, loss 0.491313.
Train: 2018-08-01T23:20:40.949647: step 7510, loss 0.562568.
Test: 2018-08-01T23:20:41.473232: step 7510, loss 0.547652.
Train: 2018-08-01T23:20:41.637806: step 7511, loss 0.544756.
Train: 2018-08-01T23:20:41.814328: step 7512, loss 0.58038.
Train: 2018-08-01T23:20:41.975871: step 7513, loss 0.705008.
Train: 2018-08-01T23:20:42.141454: step 7514, loss 0.562543.
Train: 2018-08-01T23:20:42.302996: step 7515, loss 0.562525.
Train: 2018-08-01T23:20:42.468554: step 7516, loss 0.544828.
Train: 2018-08-01T23:20:42.638133: step 7517, loss 0.491906.
Train: 2018-08-01T23:20:42.812660: step 7518, loss 0.439051.
Train: 2018-08-01T23:20:42.983178: step 7519, loss 0.597802.
Train: 2018-08-01T23:20:43.159706: step 7520, loss 0.580157.
Test: 2018-08-01T23:20:43.706245: step 7520, loss 0.547714.
Train: 2018-08-01T23:20:43.879782: step 7521, loss 0.580155.
Train: 2018-08-01T23:20:44.046354: step 7522, loss 0.580145.
Train: 2018-08-01T23:20:44.217878: step 7523, loss 0.562491.
Train: 2018-08-01T23:20:44.400418: step 7524, loss 0.474372.
Train: 2018-08-01T23:20:44.578913: step 7525, loss 0.637719.
Train: 2018-08-01T23:20:44.766411: step 7526, loss 0.527247.
Train: 2018-08-01T23:20:44.939948: step 7527, loss 0.650559.
Train: 2018-08-01T23:20:45.116475: step 7528, loss 0.562474.
Train: 2018-08-01T23:20:45.282034: step 7529, loss 0.544905.
Train: 2018-08-01T23:20:45.455569: step 7530, loss 0.650161.
Test: 2018-08-01T23:20:45.980166: step 7530, loss 0.547795.
Train: 2018-08-01T23:20:46.147719: step 7531, loss 0.492456.
Train: 2018-08-01T23:20:46.315271: step 7532, loss 0.597396.
Train: 2018-08-01T23:20:46.478858: step 7533, loss 0.475199.
Train: 2018-08-01T23:20:46.664339: step 7534, loss 0.544992.
Train: 2018-08-01T23:20:46.826903: step 7535, loss 0.544993.
Train: 2018-08-01T23:20:46.989469: step 7536, loss 0.527547.
Train: 2018-08-01T23:20:47.149043: step 7537, loss 0.667165.
Train: 2018-08-01T23:20:47.318614: step 7538, loss 0.562433.
Train: 2018-08-01T23:20:47.480158: step 7539, loss 0.66694.
Train: 2018-08-01T23:20:47.649704: step 7540, loss 0.562422.
Test: 2018-08-01T23:20:48.197240: step 7540, loss 0.5479.
Train: 2018-08-01T23:20:48.365821: step 7541, loss 0.614419.
Train: 2018-08-01T23:20:48.530376: step 7542, loss 0.614262.
Train: 2018-08-01T23:20:48.697927: step 7543, loss 0.54518.
Train: 2018-08-01T23:20:48.862462: step 7544, loss 0.562403.
Train: 2018-08-01T23:20:49.028018: step 7545, loss 0.596667.
Train: 2018-08-01T23:20:49.199587: step 7546, loss 0.579488.
Train: 2018-08-01T23:20:49.367113: step 7547, loss 0.630548.
Train: 2018-08-01T23:20:49.533699: step 7548, loss 0.528468.
Train: 2018-08-01T23:20:49.699226: step 7549, loss 0.680923.
Train: 2018-08-01T23:20:49.872761: step 7550, loss 0.495009.
Test: 2018-08-01T23:20:50.418303: step 7550, loss 0.548371.
Train: 2018-08-01T23:20:50.584882: step 7551, loss 0.713758.
Train: 2018-08-01T23:20:50.760388: step 7552, loss 0.562473.
Train: 2018-08-01T23:20:50.934947: step 7553, loss 0.712456.
Train: 2018-08-01T23:20:51.102494: step 7554, loss 0.579101.
Train: 2018-08-01T23:20:51.275037: step 7555, loss 0.611987.
Train: 2018-08-01T23:20:51.442596: step 7556, loss 0.464419.
Train: 2018-08-01T23:20:51.609153: step 7557, loss 0.562678.
Train: 2018-08-01T23:20:51.778703: step 7558, loss 0.562711.
Train: 2018-08-01T23:20:51.942255: step 7559, loss 0.611418.
Train: 2018-08-01T23:20:52.108784: step 7560, loss 0.578954.
Test: 2018-08-01T23:20:52.641360: step 7560, loss 0.549293.
Train: 2018-08-01T23:20:52.811932: step 7561, loss 0.611219.
Train: 2018-08-01T23:20:52.983477: step 7562, loss 0.595024.
Train: 2018-08-01T23:20:53.147007: step 7563, loss 0.611009.
Train: 2018-08-01T23:20:53.312565: step 7564, loss 0.562935.
Train: 2018-08-01T23:20:53.482112: step 7565, loss 0.547033.
Train: 2018-08-01T23:20:53.645676: step 7566, loss 0.515268.
Train: 2018-08-01T23:20:53.805248: step 7567, loss 0.435794.
Train: 2018-08-01T23:20:53.974796: step 7568, loss 0.483315.
Train: 2018-08-01T23:20:54.137402: step 7569, loss 0.514967.
Train: 2018-08-01T23:20:54.303915: step 7570, loss 0.562874.
Test: 2018-08-01T23:20:54.838488: step 7570, loss 0.549322.
Train: 2018-08-01T23:20:55.011051: step 7571, loss 0.61118.
Train: 2018-08-01T23:20:55.176583: step 7572, loss 0.546615.
Train: 2018-08-01T23:20:55.345132: step 7573, loss 0.643838.
Train: 2018-08-01T23:20:55.510715: step 7574, loss 0.481496.
Train: 2018-08-01T23:20:55.677245: step 7575, loss 0.578986.
Train: 2018-08-01T23:20:55.860784: step 7576, loss 0.415614.
Train: 2018-08-01T23:20:56.031298: step 7577, loss 0.529772.
Train: 2018-08-01T23:20:56.194886: step 7578, loss 0.546061.
Train: 2018-08-01T23:20:56.356454: step 7579, loss 0.628883.
Train: 2018-08-01T23:20:56.520022: step 7580, loss 0.512558.
Test: 2018-08-01T23:20:57.054562: step 7580, loss 0.548479.
Train: 2018-08-01T23:20:57.222114: step 7581, loss 0.545761.
Train: 2018-08-01T23:20:57.395650: step 7582, loss 0.528888.
Train: 2018-08-01T23:20:57.562205: step 7583, loss 0.511872.
Train: 2018-08-01T23:20:57.733746: step 7584, loss 0.562422.
Train: 2018-08-01T23:20:57.899328: step 7585, loss 0.613426.
Train: 2018-08-01T23:20:58.072841: step 7586, loss 0.494182.
Train: 2018-08-01T23:20:58.234440: step 7587, loss 0.493921.
Train: 2018-08-01T23:20:58.403981: step 7588, loss 0.510813.
Train: 2018-08-01T23:20:58.571532: step 7589, loss 0.493295.
Train: 2018-08-01T23:20:58.748064: step 7590, loss 0.597158.
Test: 2018-08-01T23:20:59.286623: step 7590, loss 0.547828.
Train: 2018-08-01T23:20:59.468110: step 7591, loss 0.510109.
Train: 2018-08-01T23:20:59.630676: step 7592, loss 0.667566.
Train: 2018-08-01T23:20:59.791246: step 7593, loss 0.597586.
Train: 2018-08-01T23:20:59.956804: step 7594, loss 0.562473.
Train: 2018-08-01T23:21:00.131368: step 7595, loss 0.544871.
Train: 2018-08-01T23:21:00.302879: step 7596, loss 0.580124.
Train: 2018-08-01T23:21:00.468456: step 7597, loss 0.597794.
Train: 2018-08-01T23:21:00.638980: step 7598, loss 0.509539.
Train: 2018-08-01T23:21:00.807529: step 7599, loss 0.491835.
Train: 2018-08-01T23:21:00.978098: step 7600, loss 0.456338.
Test: 2018-08-01T23:21:01.510682: step 7600, loss 0.547674.
Train: 2018-08-01T23:21:02.295178: step 7601, loss 0.491547.
Train: 2018-08-01T23:21:02.465728: step 7602, loss 0.526947.
Train: 2018-08-01T23:21:02.633274: step 7603, loss 0.580469.
Train: 2018-08-01T23:21:02.797866: step 7604, loss 0.598467.
Train: 2018-08-01T23:21:02.963417: step 7605, loss 0.580598.
Train: 2018-08-01T23:21:03.134932: step 7606, loss 0.490753.
Train: 2018-08-01T23:21:03.301519: step 7607, loss 0.526657.
Train: 2018-08-01T23:21:03.469070: step 7608, loss 0.454408.
Train: 2018-08-01T23:21:03.632633: step 7609, loss 0.544639.
Train: 2018-08-01T23:21:03.799182: step 7610, loss 0.526464.
Test: 2018-08-01T23:21:04.339711: step 7610, loss 0.547575.
Train: 2018-08-01T23:21:04.508286: step 7611, loss 0.544613.
Train: 2018-08-01T23:21:04.684790: step 7612, loss 0.562868.
Train: 2018-08-01T23:21:04.850347: step 7613, loss 0.672728.
Train: 2018-08-01T23:21:05.025878: step 7614, loss 0.507977.
Train: 2018-08-01T23:21:05.191462: step 7615, loss 0.489621.
Train: 2018-08-01T23:21:05.376965: step 7616, loss 0.526241.
Train: 2018-08-01T23:21:05.561445: step 7617, loss 0.526209.
Train: 2018-08-01T23:21:05.728998: step 7618, loss 0.452525.
Train: 2018-08-01T23:21:05.896577: step 7619, loss 0.526123.
Train: 2018-08-01T23:21:06.063111: step 7620, loss 0.489045.
Test: 2018-08-01T23:21:06.606652: step 7620, loss 0.547609.
Train: 2018-08-01T23:21:06.772209: step 7621, loss 0.63745.
Train: 2018-08-01T23:21:06.939760: step 7622, loss 0.563193.
Train: 2018-08-01T23:21:07.098365: step 7623, loss 0.525968.
Train: 2018-08-01T23:21:07.269879: step 7624, loss 0.675136.
Train: 2018-08-01T23:21:07.439425: step 7625, loss 0.525956.
Train: 2018-08-01T23:21:07.605980: step 7626, loss 0.488694.
Train: 2018-08-01T23:21:07.772536: step 7627, loss 0.656442.
Train: 2018-08-01T23:21:07.935132: step 7628, loss 0.637698.
Train: 2018-08-01T23:21:08.100690: step 7629, loss 0.507428.
Train: 2018-08-01T23:21:08.268219: step 7630, loss 0.544587.
Test: 2018-08-01T23:21:08.810760: step 7630, loss 0.547599.
Train: 2018-08-01T23:21:08.977314: step 7631, loss 0.544586.
Train: 2018-08-01T23:21:09.145864: step 7632, loss 0.507579.
Train: 2018-08-01T23:21:09.311441: step 7633, loss 0.526094.
Train: 2018-08-01T23:21:09.477976: step 7634, loss 0.5261.
Train: 2018-08-01T23:21:09.650539: step 7635, loss 0.544586.
Train: 2018-08-01T23:21:09.823071: step 7636, loss 0.563069.
Train: 2018-08-01T23:21:09.984621: step 7637, loss 0.526108.
Train: 2018-08-01T23:21:10.149213: step 7638, loss 0.544585.
Train: 2018-08-01T23:21:10.319726: step 7639, loss 0.526108.
Train: 2018-08-01T23:21:10.489272: step 7640, loss 0.544585.
Test: 2018-08-01T23:21:11.031822: step 7640, loss 0.547591.
Train: 2018-08-01T23:21:11.197380: step 7641, loss 0.544585.
Train: 2018-08-01T23:21:11.368946: step 7642, loss 0.600052.
Train: 2018-08-01T23:21:11.542465: step 7643, loss 0.507629.
Train: 2018-08-01T23:21:11.712003: step 7644, loss 0.470673.
Train: 2018-08-01T23:21:11.882548: step 7645, loss 0.581577.
Train: 2018-08-01T23:21:12.045156: step 7646, loss 0.581591.
Train: 2018-08-01T23:21:12.208676: step 7647, loss 0.526085.
Train: 2018-08-01T23:21:12.374234: step 7648, loss 0.48908.
Train: 2018-08-01T23:21:12.546772: step 7649, loss 0.544586.
Train: 2018-08-01T23:21:12.721305: step 7650, loss 0.674301.
Test: 2018-08-01T23:21:13.246900: step 7650, loss 0.547596.
Train: 2018-08-01T23:21:13.410489: step 7651, loss 0.526074.
Train: 2018-08-01T23:21:13.576020: step 7652, loss 0.563083.
Train: 2018-08-01T23:21:13.742575: step 7653, loss 0.618512.
Train: 2018-08-01T23:21:13.915114: step 7654, loss 0.618382.
Train: 2018-08-01T23:21:14.077705: step 7655, loss 0.507784.
Train: 2018-08-01T23:21:14.242267: step 7656, loss 0.599695.
Train: 2018-08-01T23:21:14.408795: step 7657, loss 0.52627.
Train: 2018-08-01T23:21:14.581359: step 7658, loss 0.562891.
Train: 2018-08-01T23:21:14.752874: step 7659, loss 0.654147.
Train: 2018-08-01T23:21:14.917459: step 7660, loss 0.581015.
Test: 2018-08-01T23:21:15.450037: step 7660, loss 0.547579.
Train: 2018-08-01T23:21:15.619558: step 7661, loss 0.58091.
Train: 2018-08-01T23:21:15.786119: step 7662, loss 0.616962.
Train: 2018-08-01T23:21:15.948703: step 7663, loss 0.634704.
Train: 2018-08-01T23:21:16.115232: step 7664, loss 0.562624.
Train: 2018-08-01T23:21:16.290763: step 7665, loss 0.526898.
Train: 2018-08-01T23:21:16.458341: step 7666, loss 0.544772.
Train: 2018-08-01T23:21:16.623874: step 7667, loss 0.527083.
Train: 2018-08-01T23:21:16.790428: step 7668, loss 0.509476.
Train: 2018-08-01T23:21:16.957008: step 7669, loss 0.474251.
Train: 2018-08-01T23:21:17.121549: step 7670, loss 0.597782.
Test: 2018-08-01T23:21:17.649132: step 7670, loss 0.547724.
Train: 2018-08-01T23:21:17.823674: step 7671, loss 0.703533.
Train: 2018-08-01T23:21:17.991217: step 7672, loss 0.439408.
Train: 2018-08-01T23:21:18.153783: step 7673, loss 0.5449.
Train: 2018-08-01T23:21:18.317371: step 7674, loss 0.597571.
Train: 2018-08-01T23:21:18.484897: step 7675, loss 0.527387.
Train: 2018-08-01T23:21:18.649488: step 7676, loss 0.562453.
Train: 2018-08-01T23:21:18.814050: step 7677, loss 0.632505.
Train: 2018-08-01T23:21:18.979606: step 7678, loss 0.614898.
Train: 2018-08-01T23:21:19.145159: step 7679, loss 0.579878.
Train: 2018-08-01T23:21:19.308696: step 7680, loss 0.440607.
Test: 2018-08-01T23:21:19.854237: step 7680, loss 0.547857.
Train: 2018-08-01T23:21:20.020822: step 7681, loss 0.492842.
Train: 2018-08-01T23:21:20.198318: step 7682, loss 0.632051.
Train: 2018-08-01T23:21:20.370888: step 7683, loss 0.64941.
Train: 2018-08-01T23:21:20.551374: step 7684, loss 0.579784.
Train: 2018-08-01T23:21:20.714970: step 7685, loss 0.562413.
Train: 2018-08-01T23:21:20.889470: step 7686, loss 0.493204.
Train: 2018-08-01T23:21:21.056050: step 7687, loss 0.493244.
Train: 2018-08-01T23:21:21.221613: step 7688, loss 0.562409.
Train: 2018-08-01T23:21:21.389165: step 7689, loss 0.648941.
Train: 2018-08-01T23:21:21.558681: step 7690, loss 0.510539.
Test: 2018-08-01T23:21:22.095247: step 7690, loss 0.547931.
Train: 2018-08-01T23:21:22.265791: step 7691, loss 0.596983.
Train: 2018-08-01T23:21:22.432371: step 7692, loss 0.510578.
Train: 2018-08-01T23:21:22.608874: step 7693, loss 0.545128.
Train: 2018-08-01T23:21:22.777422: step 7694, loss 0.475978.
Train: 2018-08-01T23:21:22.941019: step 7695, loss 0.666296.
Train: 2018-08-01T23:21:23.106567: step 7696, loss 0.666282.
Train: 2018-08-01T23:21:23.280080: step 7697, loss 0.493278.
Train: 2018-08-01T23:21:23.444640: step 7698, loss 0.596953.
Train: 2018-08-01T23:21:23.610228: step 7699, loss 0.493382.
Train: 2018-08-01T23:21:23.776784: step 7700, loss 0.648694.
Test: 2018-08-01T23:21:24.302347: step 7700, loss 0.54797.
Train: 2018-08-01T23:21:25.096310: step 7701, loss 0.57964.
Train: 2018-08-01T23:21:25.267820: step 7702, loss 0.596828.
Train: 2018-08-01T23:21:25.434391: step 7703, loss 0.596766.
Train: 2018-08-01T23:21:25.596966: step 7704, loss 0.476674.
Train: 2018-08-01T23:21:25.767485: step 7705, loss 0.510996.
Train: 2018-08-01T23:21:25.943045: step 7706, loss 0.476705.
Train: 2018-08-01T23:21:26.109570: step 7707, loss 0.716891.
Train: 2018-08-01T23:21:26.281143: step 7708, loss 0.613847.
Train: 2018-08-01T23:21:26.448682: step 7709, loss 0.545281.
Train: 2018-08-01T23:21:26.617225: step 7710, loss 0.596601.
Test: 2018-08-01T23:21:27.141820: step 7710, loss 0.548109.
Train: 2018-08-01T23:21:27.315368: step 7711, loss 0.528261.
Train: 2018-08-01T23:21:27.480924: step 7712, loss 0.545349.
Train: 2018-08-01T23:21:27.643473: step 7713, loss 0.511267.
Train: 2018-08-01T23:21:27.810067: step 7714, loss 0.477146.
Train: 2018-08-01T23:21:27.972621: step 7715, loss 0.68196.
Train: 2018-08-01T23:21:28.139146: step 7716, loss 0.511184.
Train: 2018-08-01T23:21:28.313677: step 7717, loss 0.545322.
Train: 2018-08-01T23:21:28.484246: step 7718, loss 0.494028.
Train: 2018-08-01T23:21:28.647818: step 7719, loss 0.596647.
Train: 2018-08-01T23:21:28.822337: step 7720, loss 0.528121.
Test: 2018-08-01T23:21:29.360879: step 7720, loss 0.548028.
Train: 2018-08-01T23:21:29.526437: step 7721, loss 0.579566.
Train: 2018-08-01T23:21:29.688030: step 7722, loss 0.562401.
Train: 2018-08-01T23:21:29.849574: step 7723, loss 0.562402.
Train: 2018-08-01T23:21:30.009164: step 7724, loss 0.562402.
Train: 2018-08-01T23:21:30.177721: step 7725, loss 0.751872.
Train: 2018-08-01T23:21:30.348271: step 7726, loss 0.47647.
Train: 2018-08-01T23:21:30.519812: step 7727, loss 0.613923.
Train: 2018-08-01T23:21:30.684372: step 7728, loss 0.459503.
Train: 2018-08-01T23:21:30.851894: step 7729, loss 0.528092.
Train: 2018-08-01T23:21:31.015457: step 7730, loss 0.545233.
Test: 2018-08-01T23:21:31.547058: step 7730, loss 0.548011.
Train: 2018-08-01T23:21:31.715612: step 7731, loss 0.59677.
Train: 2018-08-01T23:21:31.884160: step 7732, loss 0.596781.
Train: 2018-08-01T23:21:32.045702: step 7733, loss 0.648332.
Train: 2018-08-01T23:21:32.212287: step 7734, loss 0.682522.
Train: 2018-08-01T23:21:32.377813: step 7735, loss 0.579507.
Train: 2018-08-01T23:21:32.542375: step 7736, loss 0.494197.
Train: 2018-08-01T23:21:32.708930: step 7737, loss 0.443252.
Train: 2018-08-01T23:21:32.878476: step 7738, loss 0.511326.
Train: 2018-08-01T23:21:33.050017: step 7739, loss 0.54536.
Train: 2018-08-01T23:21:33.218592: step 7740, loss 0.613603.
Test: 2018-08-01T23:21:33.758125: step 7740, loss 0.548107.
Train: 2018-08-01T23:21:33.929714: step 7741, loss 0.579476.
Train: 2018-08-01T23:21:34.094226: step 7742, loss 0.562403.
Train: 2018-08-01T23:21:34.264770: step 7743, loss 0.459952.
Train: 2018-08-01T23:21:34.434317: step 7744, loss 0.528195.
Train: 2018-08-01T23:21:34.600896: step 7745, loss 0.545263.
Train: 2018-08-01T23:21:34.766440: step 7746, loss 0.545227.
Train: 2018-08-01T23:21:34.935975: step 7747, loss 0.510771.
Train: 2018-08-01T23:21:35.106537: step 7748, loss 0.562405.
Train: 2018-08-01T23:21:35.271080: step 7749, loss 0.510515.
Train: 2018-08-01T23:21:35.441624: step 7750, loss 0.666501.
Test: 2018-08-01T23:21:35.975197: step 7750, loss 0.547876.
Train: 2018-08-01T23:21:36.140781: step 7751, loss 0.545052.
Train: 2018-08-01T23:21:36.307320: step 7752, loss 0.579805.
Train: 2018-08-01T23:21:36.470897: step 7753, loss 0.666805.
Train: 2018-08-01T23:21:36.635433: step 7754, loss 0.684083.
Train: 2018-08-01T23:21:36.823928: step 7755, loss 0.579746.
Train: 2018-08-01T23:21:36.991480: step 7756, loss 0.648839.
Train: 2018-08-01T23:21:37.187955: step 7757, loss 0.596844.
Train: 2018-08-01T23:21:37.365481: step 7758, loss 0.562401.
Train: 2018-08-01T23:21:37.612819: step 7759, loss 0.630771.
Train: 2018-08-01T23:21:37.786355: step 7760, loss 0.613466.
Test: 2018-08-01T23:21:38.317935: step 7760, loss 0.548232.
Train: 2018-08-01T23:21:38.480525: step 7761, loss 0.647124.
Train: 2018-08-01T23:21:38.657054: step 7762, loss 0.562437.
Train: 2018-08-01T23:21:38.823614: step 7763, loss 0.49539.
Train: 2018-08-01T23:21:38.988143: step 7764, loss 0.562477.
Train: 2018-08-01T23:21:39.150734: step 7765, loss 0.61249.
Train: 2018-08-01T23:21:39.316291: step 7766, loss 0.645571.
Train: 2018-08-01T23:21:39.476837: step 7767, loss 0.479838.
Train: 2018-08-01T23:21:39.640400: step 7768, loss 0.513054.
Train: 2018-08-01T23:21:39.805957: step 7769, loss 0.612031.
Train: 2018-08-01T23:21:39.968553: step 7770, loss 0.62842.
Test: 2018-08-01T23:21:40.493119: step 7770, loss 0.548865.
Train: 2018-08-01T23:21:40.662705: step 7771, loss 0.496944.
Train: 2018-08-01T23:21:40.827226: step 7772, loss 0.595424.
Train: 2018-08-01T23:21:40.989828: step 7773, loss 0.628156.
Train: 2018-08-01T23:21:41.162358: step 7774, loss 0.497267.
Train: 2018-08-01T23:21:41.325894: step 7775, loss 0.529987.
Train: 2018-08-01T23:21:41.501452: step 7776, loss 0.578999.
Train: 2018-08-01T23:21:41.667980: step 7777, loss 0.529976.
Train: 2018-08-01T23:21:41.834565: step 7778, loss 0.529942.
Train: 2018-08-01T23:21:41.996101: step 7779, loss 0.480749.
Train: 2018-08-01T23:21:42.166676: step 7780, loss 0.46407.
Test: 2018-08-01T23:21:42.701236: step 7780, loss 0.548759.
Train: 2018-08-01T23:21:42.872758: step 7781, loss 0.546077.
Train: 2018-08-01T23:21:43.032331: step 7782, loss 0.595664.
Train: 2018-08-01T23:21:43.203904: step 7783, loss 0.462789.
Train: 2018-08-01T23:21:43.362448: step 7784, loss 0.662679.
Train: 2018-08-01T23:21:43.525046: step 7785, loss 0.54572.
Train: 2018-08-01T23:21:43.698575: step 7786, loss 0.528865.
Train: 2018-08-01T23:21:43.861142: step 7787, loss 0.629822.
Train: 2018-08-01T23:21:44.030688: step 7788, loss 0.478046.
Train: 2018-08-01T23:21:44.195223: step 7789, loss 0.663988.
Train: 2018-08-01T23:21:44.359814: step 7790, loss 0.596313.
Test: 2018-08-01T23:21:44.882386: step 7790, loss 0.548216.
Train: 2018-08-01T23:21:45.045949: step 7791, loss 0.494585.
Train: 2018-08-01T23:21:45.229458: step 7792, loss 0.528442.
Train: 2018-08-01T23:21:45.391058: step 7793, loss 0.681539.
Train: 2018-08-01T23:21:45.554589: step 7794, loss 0.460295.
Train: 2018-08-01T23:21:45.719150: step 7795, loss 0.630588.
Train: 2018-08-01T23:21:45.884732: step 7796, loss 0.59651.
Train: 2018-08-01T23:21:46.051262: step 7797, loss 0.784061.
Train: 2018-08-01T23:21:46.214825: step 7798, loss 0.579402.
Train: 2018-08-01T23:21:46.377420: step 7799, loss 0.596288.
Train: 2018-08-01T23:21:46.542948: step 7800, loss 0.562431.
Test: 2018-08-01T23:21:47.084500: step 7800, loss 0.548356.
Train: 2018-08-01T23:21:47.928575: step 7801, loss 0.528798.
Train: 2018-08-01T23:21:48.093105: step 7802, loss 0.67995.
Train: 2018-08-01T23:21:48.263674: step 7803, loss 0.495585.
Train: 2018-08-01T23:21:48.426220: step 7804, loss 0.445695.
Train: 2018-08-01T23:21:48.591796: step 7805, loss 0.562487.
Train: 2018-08-01T23:21:48.756331: step 7806, loss 0.495753.
Train: 2018-08-01T23:21:48.920891: step 7807, loss 0.595885.
Train: 2018-08-01T23:21:49.083457: step 7808, loss 0.52905.
Train: 2018-08-01T23:21:49.252007: step 7809, loss 0.529004.
Train: 2018-08-01T23:21:49.421578: step 7810, loss 0.629504.
Test: 2018-08-01T23:21:49.949160: step 7810, loss 0.548415.
Train: 2018-08-01T23:21:50.117691: step 7811, loss 0.4283.
Train: 2018-08-01T23:21:50.292225: step 7812, loss 0.495189.
Train: 2018-08-01T23:21:50.454791: step 7813, loss 0.511812.
Train: 2018-08-01T23:21:50.636306: step 7814, loss 0.765699.
Train: 2018-08-01T23:21:50.804855: step 7815, loss 0.681055.
Train: 2018-08-01T23:21:50.970412: step 7816, loss 0.56242.
Train: 2018-08-01T23:21:51.141955: step 7817, loss 0.579331.
Train: 2018-08-01T23:21:51.323469: step 7818, loss 0.646855.
Train: 2018-08-01T23:21:51.515954: step 7819, loss 0.528746.
Train: 2018-08-01T23:21:51.701459: step 7820, loss 0.612902.
Test: 2018-08-01T23:21:52.237028: step 7820, loss 0.548399.
Train: 2018-08-01T23:21:52.445507: step 7821, loss 0.596021.
Train: 2018-08-01T23:21:52.618008: step 7822, loss 0.478744.
Train: 2018-08-01T23:21:52.783565: step 7823, loss 0.495541.
Train: 2018-08-01T23:21:52.954109: step 7824, loss 0.562467.
Train: 2018-08-01T23:21:53.116675: step 7825, loss 0.49547.
Train: 2018-08-01T23:21:53.284258: step 7826, loss 0.562456.
Train: 2018-08-01T23:21:53.463748: step 7827, loss 0.512048.
Train: 2018-08-01T23:21:53.632330: step 7828, loss 0.697139.
Train: 2018-08-01T23:21:53.807861: step 7829, loss 0.511929.
Train: 2018-08-01T23:21:53.971391: step 7830, loss 0.579287.
Test: 2018-08-01T23:21:54.511946: step 7830, loss 0.548316.
Train: 2018-08-01T23:21:54.689495: step 7831, loss 0.528715.
Train: 2018-08-01T23:21:54.855028: step 7832, loss 0.613062.
Train: 2018-08-01T23:21:55.018621: step 7833, loss 0.663716.
Train: 2018-08-01T23:21:55.184147: step 7834, loss 0.579293.
Train: 2018-08-01T23:21:55.362695: step 7835, loss 0.579276.
Train: 2018-08-01T23:21:55.535236: step 7836, loss 0.495196.
Train: 2018-08-01T23:21:55.707749: step 7837, loss 0.562446.
Train: 2018-08-01T23:21:55.878293: step 7838, loss 0.713718.
Train: 2018-08-01T23:21:56.053848: step 7839, loss 0.595995.
Train: 2018-08-01T23:21:56.217419: step 7840, loss 0.562472.
Test: 2018-08-01T23:21:56.747967: step 7840, loss 0.54851.
Train: 2018-08-01T23:21:56.915520: step 7841, loss 0.645926.
Train: 2018-08-01T23:21:57.091051: step 7842, loss 0.495967.
Train: 2018-08-01T23:21:57.254614: step 7843, loss 0.462885.
Train: 2018-08-01T23:21:57.472071: step 7844, loss 0.562517.
Train: 2018-08-01T23:21:57.653619: step 7845, loss 0.645569.
Train: 2018-08-01T23:21:57.823134: step 7846, loss 0.628898.
Train: 2018-08-01T23:21:57.991682: step 7847, loss 0.595662.
Train: 2018-08-01T23:21:58.159273: step 7848, loss 0.529494.
Train: 2018-08-01T23:21:58.335762: step 7849, loss 0.579071.
Train: 2018-08-01T23:21:58.508327: step 7850, loss 0.546084.
Test: 2018-08-01T23:21:59.036890: step 7850, loss 0.548781.
Train: 2018-08-01T23:21:59.217406: step 7851, loss 0.562578.
Train: 2018-08-01T23:21:59.382963: step 7852, loss 0.579052.
Train: 2018-08-01T23:21:59.546562: step 7853, loss 0.480289.
Train: 2018-08-01T23:21:59.715107: step 7854, loss 0.496677.
Train: 2018-08-01T23:21:59.880664: step 7855, loss 0.529542.
Train: 2018-08-01T23:22:00.059156: step 7856, loss 0.54599.
Train: 2018-08-01T23:22:00.222719: step 7857, loss 0.512736.
Train: 2018-08-01T23:22:00.387279: step 7858, loss 0.462602.
Train: 2018-08-01T23:22:00.551870: step 7859, loss 0.562471.
Train: 2018-08-01T23:22:00.717420: step 7860, loss 0.562449.
Test: 2018-08-01T23:22:01.249993: step 7860, loss 0.548313.
Train: 2018-08-01T23:22:01.413561: step 7861, loss 0.646744.
Train: 2018-08-01T23:22:01.590064: step 7862, loss 0.528626.
Train: 2018-08-01T23:22:01.773573: step 7863, loss 0.545474.
Train: 2018-08-01T23:22:01.987003: step 7864, loss 0.545423.
Train: 2018-08-01T23:22:02.149594: step 7865, loss 0.579437.
Train: 2018-08-01T23:22:02.318117: step 7866, loss 0.630676.
Train: 2018-08-01T23:22:02.487664: step 7867, loss 0.64782.
Train: 2018-08-01T23:22:02.665189: step 7868, loss 0.52825.
Train: 2018-08-01T23:22:02.831744: step 7869, loss 0.647792.
Train: 2018-08-01T23:22:02.998323: step 7870, loss 0.49417.
Test: 2018-08-01T23:22:03.536859: step 7870, loss 0.548119.
Train: 2018-08-01T23:22:03.716411: step 7871, loss 0.528286.
Train: 2018-08-01T23:22:03.882935: step 7872, loss 0.494133.
Train: 2018-08-01T23:22:04.051508: step 7873, loss 0.630775.
Train: 2018-08-01T23:22:04.218062: step 7874, loss 0.562401.
Train: 2018-08-01T23:22:04.380603: step 7875, loss 0.579506.
Train: 2018-08-01T23:22:04.548155: step 7876, loss 0.528188.
Train: 2018-08-01T23:22:04.713714: step 7877, loss 0.630864.
Train: 2018-08-01T23:22:04.884258: step 7878, loss 0.63083.
Train: 2018-08-01T23:22:05.047820: step 7879, loss 0.476989.
Train: 2018-08-01T23:22:05.218390: step 7880, loss 0.562401.
Test: 2018-08-01T23:22:05.756925: step 7880, loss 0.548097.
Train: 2018-08-01T23:22:05.933479: step 7881, loss 0.579483.
Train: 2018-08-01T23:22:06.104994: step 7882, loss 0.596557.
Train: 2018-08-01T23:22:06.277532: step 7883, loss 0.579468.
Train: 2018-08-01T23:22:06.450112: step 7884, loss 0.562404.
Train: 2018-08-01T23:22:06.630620: step 7885, loss 0.511294.
Train: 2018-08-01T23:22:06.795150: step 7886, loss 0.51129.
Train: 2018-08-01T23:22:06.956749: step 7887, loss 0.460083.
Train: 2018-08-01T23:22:07.122275: step 7888, loss 0.562401.
Train: 2018-08-01T23:22:07.287832: step 7889, loss 0.613789.
Train: 2018-08-01T23:22:07.451395: step 7890, loss 0.596699.
Test: 2018-08-01T23:22:07.977986: step 7890, loss 0.548031.
Train: 2018-08-01T23:22:08.152520: step 7891, loss 0.562399.
Train: 2018-08-01T23:22:08.321069: step 7892, loss 0.613899.
Train: 2018-08-01T23:22:08.485655: step 7893, loss 0.459436.
Train: 2018-08-01T23:22:08.659197: step 7894, loss 0.5624.
Train: 2018-08-01T23:22:08.824749: step 7895, loss 0.614001.
Train: 2018-08-01T23:22:09.010228: step 7896, loss 0.545196.
Train: 2018-08-01T23:22:09.189779: step 7897, loss 0.631246.
Train: 2018-08-01T23:22:09.372261: step 7898, loss 0.5624.
Train: 2018-08-01T23:22:09.537818: step 7899, loss 0.59678.
Train: 2018-08-01T23:22:09.707389: step 7900, loss 0.545227.
Test: 2018-08-01T23:22:10.248916: step 7900, loss 0.548029.
Train: 2018-08-01T23:22:11.224531: step 7901, loss 0.493757.
Train: 2018-08-01T23:22:11.393081: step 7902, loss 0.596734.
Train: 2018-08-01T23:22:11.557640: step 7903, loss 0.562399.
Train: 2018-08-01T23:22:11.719233: step 7904, loss 0.613886.
Train: 2018-08-01T23:22:11.878815: step 7905, loss 0.493811.
Train: 2018-08-01T23:22:12.041375: step 7906, loss 0.442344.
Train: 2018-08-01T23:22:12.206935: step 7907, loss 0.528029.
Train: 2018-08-01T23:22:12.372493: step 7908, loss 0.527952.
Train: 2018-08-01T23:22:12.542009: step 7909, loss 0.545135.
Train: 2018-08-01T23:22:12.706603: step 7910, loss 0.493159.
Test: 2018-08-01T23:22:13.243135: step 7910, loss 0.547872.
Train: 2018-08-01T23:22:13.407726: step 7911, loss 0.614523.
Train: 2018-08-01T23:22:13.580233: step 7912, loss 0.545017.
Train: 2018-08-01T23:22:13.754766: step 7913, loss 0.614758.
Train: 2018-08-01T23:22:13.921321: step 7914, loss 0.544973.
Train: 2018-08-01T23:22:14.097850: step 7915, loss 0.632367.
Train: 2018-08-01T23:22:14.274377: step 7916, loss 0.579921.
Train: 2018-08-01T23:22:14.442948: step 7917, loss 0.527487.
Train: 2018-08-01T23:22:14.607487: step 7918, loss 0.54496.
Train: 2018-08-01T23:22:14.766088: step 7919, loss 0.527472.
Train: 2018-08-01T23:22:14.927656: step 7920, loss 0.509953.
Test: 2018-08-01T23:22:15.464197: step 7920, loss 0.547779.
Train: 2018-08-01T23:22:15.687601: step 7921, loss 0.492369.
Train: 2018-08-01T23:22:15.855152: step 7922, loss 0.667808.
Train: 2018-08-01T23:22:16.015754: step 7923, loss 0.562462.
Train: 2018-08-01T23:22:16.191254: step 7924, loss 0.50976.
Train: 2018-08-01T23:22:16.353844: step 7925, loss 0.544885.
Train: 2018-08-01T23:22:16.523366: step 7926, loss 0.544874.
Train: 2018-08-01T23:22:16.687954: step 7927, loss 0.668192.
Train: 2018-08-01T23:22:16.861464: step 7928, loss 0.509656.
Train: 2018-08-01T23:22:17.026022: step 7929, loss 0.615301.
Train: 2018-08-01T23:22:17.201552: step 7930, loss 0.668039.
Test: 2018-08-01T23:22:17.738131: step 7930, loss 0.54776.
Train: 2018-08-01T23:22:17.909687: step 7931, loss 0.527351.
Train: 2018-08-01T23:22:18.081200: step 7932, loss 0.492349.
Train: 2018-08-01T23:22:18.244764: step 7933, loss 0.579965.
Train: 2018-08-01T23:22:18.404339: step 7934, loss 0.544943.
Train: 2018-08-01T23:22:18.572887: step 7935, loss 0.54495.
Train: 2018-08-01T23:22:18.734455: step 7936, loss 0.509979.
Train: 2018-08-01T23:22:18.904027: step 7937, loss 0.632421.
Train: 2018-08-01T23:22:19.064597: step 7938, loss 0.544957.
Train: 2018-08-01T23:22:19.232155: step 7939, loss 0.579913.
Train: 2018-08-01T23:22:19.397735: step 7940, loss 0.544972.
Test: 2018-08-01T23:22:19.938237: step 7940, loss 0.547817.
Train: 2018-08-01T23:22:20.100803: step 7941, loss 0.527523.
Train: 2018-08-01T23:22:20.273342: step 7942, loss 0.579889.
Train: 2018-08-01T23:22:20.439896: step 7943, loss 0.649687.
Train: 2018-08-01T23:22:20.606450: step 7944, loss 0.597274.
Train: 2018-08-01T23:22:20.777992: step 7945, loss 0.597199.
Train: 2018-08-01T23:22:20.942585: step 7946, loss 0.61446.
Train: 2018-08-01T23:22:21.119080: step 7947, loss 0.614302.
Train: 2018-08-01T23:22:21.298612: step 7948, loss 0.562402.
Train: 2018-08-01T23:22:21.471170: step 7949, loss 0.493655.
Train: 2018-08-01T23:22:21.641683: step 7950, loss 0.528088.
Test: 2018-08-01T23:22:22.176254: step 7950, loss 0.548049.
Train: 2018-08-01T23:22:22.345827: step 7951, loss 0.579536.
Train: 2018-08-01T23:22:22.514352: step 7952, loss 0.545284.
Train: 2018-08-01T23:22:22.682924: step 7953, loss 0.613703.
Train: 2018-08-01T23:22:22.844469: step 7954, loss 0.545326.
Train: 2018-08-01T23:22:23.007034: step 7955, loss 0.545346.
Train: 2018-08-01T23:22:23.170621: step 7956, loss 0.494228.
Train: 2018-08-01T23:22:23.342138: step 7957, loss 0.528301.
Train: 2018-08-01T23:22:23.510686: step 7958, loss 0.511201.
Train: 2018-08-01T23:22:23.674297: step 7959, loss 0.545306.
Train: 2018-08-01T23:22:23.832825: step 7960, loss 0.682269.
Test: 2018-08-01T23:22:24.357427: step 7960, loss 0.548063.
Train: 2018-08-01T23:22:24.521983: step 7961, loss 0.493919.
Train: 2018-08-01T23:22:24.686569: step 7962, loss 0.716605.
Train: 2018-08-01T23:22:24.861077: step 7963, loss 0.596613.
Train: 2018-08-01T23:22:25.024640: step 7964, loss 0.596549.
Train: 2018-08-01T23:22:25.190197: step 7965, loss 0.562404.
Train: 2018-08-01T23:22:25.361739: step 7966, loss 0.681417.
Train: 2018-08-01T23:22:25.533308: step 7967, loss 0.494656.
Train: 2018-08-01T23:22:25.697840: step 7968, loss 0.613134.
Train: 2018-08-01T23:22:25.882346: step 7969, loss 0.511858.
Train: 2018-08-01T23:22:26.051898: step 7970, loss 0.579271.
Test: 2018-08-01T23:22:26.601455: step 7970, loss 0.548375.
Train: 2018-08-01T23:22:26.766983: step 7971, loss 0.629665.
Train: 2018-08-01T23:22:26.934560: step 7972, loss 0.461876.
Train: 2018-08-01T23:22:27.100125: step 7973, loss 0.528952.
Train: 2018-08-01T23:22:27.264652: step 7974, loss 0.629488.
Train: 2018-08-01T23:22:27.442176: step 7975, loss 0.629435.
Train: 2018-08-01T23:22:27.610755: step 7976, loss 0.612615.
Train: 2018-08-01T23:22:27.795233: step 7977, loss 0.545813.
Train: 2018-08-01T23:22:27.955803: step 7978, loss 0.545852.
Train: 2018-08-01T23:22:28.120364: step 7979, loss 0.595763.
Train: 2018-08-01T23:22:28.281963: step 7980, loss 0.628931.
Test: 2018-08-01T23:22:28.816504: step 7980, loss 0.548662.
Train: 2018-08-01T23:22:28.996044: step 7981, loss 0.479704.
Train: 2018-08-01T23:22:29.160583: step 7982, loss 0.463201.
Train: 2018-08-01T23:22:29.328135: step 7983, loss 0.512801.
Train: 2018-08-01T23:22:29.504663: step 7984, loss 0.529296.
Train: 2018-08-01T23:22:29.668226: step 7985, loss 0.579148.
Train: 2018-08-01T23:22:29.835807: step 7986, loss 0.512436.
Train: 2018-08-01T23:22:30.009343: step 7987, loss 0.545741.
Train: 2018-08-01T23:22:30.183848: step 7988, loss 0.629555.
Train: 2018-08-01T23:22:30.363368: step 7989, loss 0.696853.
Train: 2018-08-01T23:22:30.528926: step 7990, loss 0.59603.
Test: 2018-08-01T23:22:31.049533: step 7990, loss 0.548411.
Train: 2018-08-01T23:22:31.215117: step 7991, loss 0.545684.
Train: 2018-08-01T23:22:31.387645: step 7992, loss 0.562459.
Train: 2018-08-01T23:22:31.554209: step 7993, loss 0.579215.
Train: 2018-08-01T23:22:31.713777: step 7994, loss 0.54572.
Train: 2018-08-01T23:22:31.876323: step 7995, loss 0.512242.
Train: 2018-08-01T23:22:32.040883: step 7996, loss 0.579214.
Train: 2018-08-01T23:22:32.204494: step 7997, loss 0.445136.
Train: 2018-08-01T23:22:32.372024: step 7998, loss 0.612846.
Train: 2018-08-01T23:22:32.539582: step 7999, loss 0.495151.
Train: 2018-08-01T23:22:32.725055: step 8000, loss 0.596157.
Test: 2018-08-01T23:22:33.258628: step 8000, loss 0.548282.
Train: 2018-08-01T23:22:34.078238: step 8001, loss 0.562426.
Train: 2018-08-01T23:22:34.246758: step 8002, loss 0.630086.
Train: 2018-08-01T23:22:34.424308: step 8003, loss 0.596265.
Train: 2018-08-01T23:22:34.593830: step 8004, loss 0.630102.
Train: 2018-08-01T23:22:34.759413: step 8005, loss 0.613129.
Train: 2018-08-01T23:22:34.925941: step 8006, loss 0.56243.
Train: 2018-08-01T23:22:35.094522: step 8007, loss 0.478201.
Train: 2018-08-01T23:22:35.261046: step 8008, loss 0.478199.
Train: 2018-08-01T23:22:35.450545: step 8009, loss 0.596171.
Train: 2018-08-01T23:22:35.631057: step 8010, loss 0.579311.
Test: 2018-08-01T23:22:36.169645: step 8010, loss 0.54828.
Train: 2018-08-01T23:22:36.366092: step 8011, loss 0.613102.
Train: 2018-08-01T23:22:36.546609: step 8012, loss 0.545539.
Train: 2018-08-01T23:22:36.711189: step 8013, loss 0.596203.
Train: 2018-08-01T23:22:36.883708: step 8014, loss 0.629953.
Train: 2018-08-01T23:22:37.056246: step 8015, loss 0.596149.
Train: 2018-08-01T23:22:37.218813: step 8016, loss 0.646588.
Train: 2018-08-01T23:22:37.385366: step 8017, loss 0.512103.
Train: 2018-08-01T23:22:37.550955: step 8018, loss 0.495439.
Train: 2018-08-01T23:22:37.721499: step 8019, loss 0.528963.
Train: 2018-08-01T23:22:37.884053: step 8020, loss 0.545706.
Test: 2018-08-01T23:22:38.412621: step 8020, loss 0.548419.
Train: 2018-08-01T23:22:38.580198: step 8021, loss 0.612753.
Train: 2018-08-01T23:22:38.750718: step 8022, loss 0.545696.
Train: 2018-08-01T23:22:38.926248: step 8023, loss 0.512159.
Train: 2018-08-01T23:22:39.088839: step 8024, loss 0.612804.
Train: 2018-08-01T23:22:39.256365: step 8025, loss 0.562451.
Train: 2018-08-01T23:22:39.425912: step 8026, loss 0.495279.
Train: 2018-08-01T23:22:39.597486: step 8027, loss 0.511995.
Train: 2018-08-01T23:22:39.775985: step 8028, loss 0.562434.
Train: 2018-08-01T23:22:39.942557: step 8029, loss 0.494889.
Train: 2018-08-01T23:22:40.108114: step 8030, loss 0.494686.
Test: 2018-08-01T23:22:40.649643: step 8030, loss 0.548177.
Train: 2018-08-01T23:22:40.830160: step 8031, loss 0.647383.
Train: 2018-08-01T23:22:40.995736: step 8032, loss 0.511317.
Train: 2018-08-01T23:22:41.161305: step 8033, loss 0.579475.
Train: 2018-08-01T23:22:41.330846: step 8034, loss 0.52818.
Train: 2018-08-01T23:22:41.506350: step 8035, loss 0.528096.
Train: 2018-08-01T23:22:41.677892: step 8036, loss 0.476414.
Train: 2018-08-01T23:22:41.858410: step 8037, loss 0.579663.
Train: 2018-08-01T23:22:42.022003: step 8038, loss 0.597034.
Train: 2018-08-01T23:22:42.190522: step 8039, loss 0.51036.
Train: 2018-08-01T23:22:42.364058: step 8040, loss 0.458035.
Test: 2018-08-01T23:22:42.897637: step 8040, loss 0.54781.
Train: 2018-08-01T23:22:43.065184: step 8041, loss 0.597365.
Train: 2018-08-01T23:22:43.232737: step 8042, loss 0.614997.
Train: 2018-08-01T23:22:43.395334: step 8043, loss 0.544909.
Train: 2018-08-01T23:22:43.556899: step 8044, loss 0.544887.
Train: 2018-08-01T23:22:43.718469: step 8045, loss 0.509645.
Train: 2018-08-01T23:22:43.888017: step 8046, loss 0.544841.
Train: 2018-08-01T23:22:44.091440: step 8047, loss 0.580196.
Train: 2018-08-01T23:22:44.261984: step 8048, loss 0.633391.
Train: 2018-08-01T23:22:44.449483: step 8049, loss 0.562521.
Train: 2018-08-01T23:22:44.635984: step 8050, loss 0.491612.
Test: 2018-08-01T23:22:45.169934: step 8050, loss 0.54767.
Train: 2018-08-01T23:22:45.358435: step 8051, loss 0.633518.
Train: 2018-08-01T23:22:45.544966: step 8052, loss 0.544786.
Train: 2018-08-01T23:22:45.718468: step 8053, loss 0.598018.
Train: 2018-08-01T23:22:45.906991: step 8054, loss 0.562525.
Train: 2018-08-01T23:22:46.161286: step 8055, loss 0.66885.
Train: 2018-08-01T23:22:46.355765: step 8056, loss 0.562504.
Train: 2018-08-01T23:22:46.524339: step 8057, loss 0.615426.
Train: 2018-08-01T23:22:46.700842: step 8058, loss 0.597667.
Train: 2018-08-01T23:22:46.868394: step 8059, loss 0.492282.
Train: 2018-08-01T23:22:47.060881: step 8060, loss 0.544935.
Test: 2018-08-01T23:22:47.593455: step 8060, loss 0.547797.
Train: 2018-08-01T23:22:47.769985: step 8061, loss 0.579925.
Train: 2018-08-01T23:22:47.942522: step 8062, loss 0.457689.
Train: 2018-08-01T23:22:48.108081: step 8063, loss 0.597347.
Train: 2018-08-01T23:22:48.272641: step 8064, loss 0.614772.
Train: 2018-08-01T23:22:48.437201: step 8065, loss 0.597273.
Train: 2018-08-01T23:22:48.605750: step 8066, loss 0.631992.
Train: 2018-08-01T23:22:48.772336: step 8067, loss 0.649146.
Train: 2018-08-01T23:22:48.936864: step 8068, loss 0.545122.
Train: 2018-08-01T23:22:49.105415: step 8069, loss 0.527944.
Train: 2018-08-01T23:22:49.270997: step 8070, loss 0.579586.
Test: 2018-08-01T23:22:49.792577: step 8070, loss 0.548039.
Train: 2018-08-01T23:22:49.959131: step 8071, loss 0.493809.
Train: 2018-08-01T23:22:50.124715: step 8072, loss 0.562398.
Train: 2018-08-01T23:22:50.291269: step 8073, loss 0.425504.
Train: 2018-08-01T23:22:50.453810: step 8074, loss 0.579531.
Train: 2018-08-01T23:22:50.619400: step 8075, loss 0.596692.
Train: 2018-08-01T23:22:50.795927: step 8076, loss 0.562398.
Train: 2018-08-01T23:22:50.967437: step 8077, loss 0.528089.
Train: 2018-08-01T23:22:51.133992: step 8078, loss 0.631064.
Train: 2018-08-01T23:22:51.297567: step 8079, loss 0.631036.
Train: 2018-08-01T23:22:51.464134: step 8080, loss 0.596669.
Test: 2018-08-01T23:22:51.991699: step 8080, loss 0.548076.
Train: 2018-08-01T23:22:52.160248: step 8081, loss 0.630819.
Train: 2018-08-01T23:22:52.330791: step 8082, loss 0.613582.
Train: 2018-08-01T23:22:52.495352: step 8083, loss 0.511388.
Train: 2018-08-01T23:22:52.668889: step 8084, loss 0.528468.
Train: 2018-08-01T23:22:52.832479: step 8085, loss 0.460711.
Train: 2018-08-01T23:22:52.999031: step 8086, loss 0.494578.
Train: 2018-08-01T23:22:53.165582: step 8087, loss 0.56241.
Train: 2018-08-01T23:22:53.329153: step 8088, loss 0.52839.
Train: 2018-08-01T23:22:53.498670: step 8089, loss 0.613522.
Train: 2018-08-01T23:22:53.672207: step 8090, loss 0.562402.
Test: 2018-08-01T23:22:54.212761: step 8090, loss 0.548109.
Train: 2018-08-01T23:22:54.384302: step 8091, loss 0.511197.
Train: 2018-08-01T23:22:54.559833: step 8092, loss 0.613682.
Train: 2018-08-01T23:22:54.720404: step 8093, loss 0.562399.
Train: 2018-08-01T23:22:54.885996: step 8094, loss 0.459716.
Train: 2018-08-01T23:22:55.048557: step 8095, loss 0.5281.
Train: 2018-08-01T23:22:55.223086: step 8096, loss 0.493639.
Train: 2018-08-01T23:22:55.389651: step 8097, loss 0.665867.
Train: 2018-08-01T23:22:55.549189: step 8098, loss 0.545137.
Train: 2018-08-01T23:22:55.711780: step 8099, loss 0.527825.
Train: 2018-08-01T23:22:55.874321: step 8100, loss 0.527768.
Test: 2018-08-01T23:22:56.411424: step 8100, loss 0.54788.
Train: 2018-08-01T23:22:57.211738: step 8101, loss 0.579769.
Train: 2018-08-01T23:22:57.378293: step 8102, loss 0.458126.
Train: 2018-08-01T23:22:57.559807: step 8103, loss 0.667018.
Train: 2018-08-01T23:22:57.722372: step 8104, loss 0.649676.
Train: 2018-08-01T23:22:57.882943: step 8105, loss 0.57987.
Train: 2018-08-01T23:22:58.042542: step 8106, loss 0.475282.
Train: 2018-08-01T23:22:58.213086: step 8107, loss 0.579868.
Train: 2018-08-01T23:22:58.385599: step 8108, loss 0.544987.
Train: 2018-08-01T23:22:58.555146: step 8109, loss 0.510082.
Train: 2018-08-01T23:22:58.719732: step 8110, loss 0.579903.
Test: 2018-08-01T23:22:59.255276: step 8110, loss 0.547801.
Train: 2018-08-01T23:22:59.423849: step 8111, loss 0.579917.
Train: 2018-08-01T23:22:59.590380: step 8112, loss 0.509987.
Train: 2018-08-01T23:22:59.759926: step 8113, loss 0.492443.
Train: 2018-08-01T23:22:59.933461: step 8114, loss 0.509857.
Train: 2018-08-01T23:23:00.104031: step 8115, loss 0.52732.
Train: 2018-08-01T23:23:00.278560: step 8116, loss 0.544863.
Train: 2018-08-01T23:23:00.443121: step 8117, loss 0.580152.
Train: 2018-08-01T23:23:00.609654: step 8118, loss 0.562506.
Train: 2018-08-01T23:23:00.787206: step 8119, loss 0.527084.
Train: 2018-08-01T23:23:00.949744: step 8120, loss 0.527035.
Test: 2018-08-01T23:23:01.480353: step 8120, loss 0.547656.
Train: 2018-08-01T23:23:01.651867: step 8121, loss 0.491415.
Train: 2018-08-01T23:23:01.838370: step 8122, loss 0.52691.
Train: 2018-08-01T23:23:02.002930: step 8123, loss 0.491077.
Train: 2018-08-01T23:23:02.165496: step 8124, loss 0.580571.
Train: 2018-08-01T23:23:02.333078: step 8125, loss 0.544674.
Train: 2018-08-01T23:23:02.496641: step 8126, loss 0.580719.
Train: 2018-08-01T23:23:02.659210: step 8127, loss 0.639775.
Train: 2018-08-01T23:23:02.832711: step 8128, loss 0.562714.
Train: 2018-08-01T23:23:02.994279: step 8129, loss 0.508502.
Train: 2018-08-01T23:23:03.165821: step 8130, loss 0.562725.
Test: 2018-08-01T23:23:03.693411: step 8130, loss 0.547581.
Train: 2018-08-01T23:23:03.855003: step 8131, loss 0.49036.
Train: 2018-08-01T23:23:04.031507: step 8132, loss 0.635208.
Train: 2018-08-01T23:23:04.211028: step 8133, loss 0.58086.
Train: 2018-08-01T23:23:04.380575: step 8134, loss 0.653267.
Train: 2018-08-01T23:23:04.551118: step 8135, loss 0.454285.
Train: 2018-08-01T23:23:04.713714: step 8136, loss 0.562712.
Train: 2018-08-01T23:23:04.876279: step 8137, loss 0.59882.
Train: 2018-08-01T23:23:05.038814: step 8138, loss 0.490545.
Train: 2018-08-01T23:23:05.208361: step 8139, loss 0.526623.
Train: 2018-08-01T23:23:05.374946: step 8140, loss 0.490544.
Test: 2018-08-01T23:23:05.917499: step 8140, loss 0.547586.
Train: 2018-08-01T23:23:06.084019: step 8141, loss 0.598815.
Train: 2018-08-01T23:23:06.246616: step 8142, loss 0.616885.
Train: 2018-08-01T23:23:06.415135: step 8143, loss 0.526606.
Train: 2018-08-01T23:23:06.574741: step 8144, loss 0.598777.
Train: 2018-08-01T23:23:06.748274: step 8145, loss 0.454542.
Train: 2018-08-01T23:23:06.915796: step 8146, loss 0.56269.
Train: 2018-08-01T23:23:07.089332: step 8147, loss 0.616805.
Train: 2018-08-01T23:23:07.256909: step 8148, loss 0.526635.
Train: 2018-08-01T23:23:07.431418: step 8149, loss 0.652783.
Train: 2018-08-01T23:23:07.593984: step 8150, loss 0.580653.
Test: 2018-08-01T23:23:08.126559: step 8150, loss 0.547605.
Train: 2018-08-01T23:23:08.290141: step 8151, loss 0.580598.
Train: 2018-08-01T23:23:08.455679: step 8152, loss 0.723885.
Train: 2018-08-01T23:23:08.620239: step 8153, loss 0.580419.
Train: 2018-08-01T23:23:08.786795: step 8154, loss 0.527008.
Train: 2018-08-01T23:23:08.953350: step 8155, loss 0.5094.
Train: 2018-08-01T23:23:09.113921: step 8156, loss 0.491854.
Train: 2018-08-01T23:23:09.277483: step 8157, loss 0.456664.
Train: 2018-08-01T23:23:09.452043: step 8158, loss 0.615411.
Train: 2018-08-01T23:23:09.618572: step 8159, loss 0.544854.
Train: 2018-08-01T23:23:09.785155: step 8160, loss 0.632972.
Test: 2018-08-01T23:23:10.320696: step 8160, loss 0.547737.
Train: 2018-08-01T23:23:10.489269: step 8161, loss 0.668052.
Train: 2018-08-01T23:23:10.656796: step 8162, loss 0.50982.
Train: 2018-08-01T23:23:10.818363: step 8163, loss 0.544935.
Train: 2018-08-01T23:23:10.985915: step 8164, loss 0.50999.
Train: 2018-08-01T23:23:11.148512: step 8165, loss 0.562436.
Train: 2018-08-01T23:23:11.313072: step 8166, loss 0.614806.
Train: 2018-08-01T23:23:11.475606: step 8167, loss 0.562427.
Train: 2018-08-01T23:23:11.650140: step 8168, loss 0.632054.
Train: 2018-08-01T23:23:11.816695: step 8169, loss 0.649252.
Train: 2018-08-01T23:23:11.982253: step 8170, loss 0.666255.
Test: 2018-08-01T23:23:12.511836: step 8170, loss 0.547974.
Train: 2018-08-01T23:23:12.675424: step 8171, loss 0.5624.
Train: 2018-08-01T23:23:12.836968: step 8172, loss 0.579552.
Train: 2018-08-01T23:23:13.000530: step 8173, loss 0.545316.
Train: 2018-08-01T23:23:13.173069: step 8174, loss 0.57943.
Train: 2018-08-01T23:23:13.341653: step 8175, loss 0.613318.
Train: 2018-08-01T23:23:13.518147: step 8176, loss 0.663853.
Train: 2018-08-01T23:23:13.691716: step 8177, loss 0.528798.
Train: 2018-08-01T23:23:13.857267: step 8178, loss 0.445162.
Train: 2018-08-01T23:23:14.021800: step 8179, loss 0.545737.
Train: 2018-08-01T23:23:14.186360: step 8180, loss 0.579187.
Test: 2018-08-01T23:23:14.712953: step 8180, loss 0.548499.
Train: 2018-08-01T23:23:14.878535: step 8181, loss 0.545784.
Train: 2018-08-01T23:23:15.050051: step 8182, loss 0.512428.
Train: 2018-08-01T23:23:15.215642: step 8183, loss 0.428956.
Train: 2018-08-01T23:23:15.384159: step 8184, loss 0.646132.
Train: 2018-08-01T23:23:15.547746: step 8185, loss 0.562462.
Train: 2018-08-01T23:23:15.713278: step 8186, loss 0.461853.
Train: 2018-08-01T23:23:15.875844: step 8187, loss 0.579255.
Train: 2018-08-01T23:23:16.038410: step 8188, loss 0.562435.
Train: 2018-08-01T23:23:16.218926: step 8189, loss 0.562427.
Train: 2018-08-01T23:23:16.389502: step 8190, loss 0.646973.
Test: 2018-08-01T23:23:16.923200: step 8190, loss 0.548254.
Train: 2018-08-01T23:23:17.094711: step 8191, loss 0.545503.
Train: 2018-08-01T23:23:17.263259: step 8192, loss 0.579346.
Train: 2018-08-01T23:23:17.439787: step 8193, loss 0.596285.
Train: 2018-08-01T23:23:17.609334: step 8194, loss 0.697868.
Train: 2018-08-01T23:23:17.777883: step 8195, loss 0.494848.
Train: 2018-08-01T23:23:17.951451: step 8196, loss 0.596186.
Train: 2018-08-01T23:23:18.116977: step 8197, loss 0.629865.
Train: 2018-08-01T23:23:18.287522: step 8198, loss 0.511974.
Train: 2018-08-01T23:23:18.450087: step 8199, loss 0.57925.
Train: 2018-08-01T23:23:18.611655: step 8200, loss 0.663162.
Test: 2018-08-01T23:23:19.143235: step 8200, loss 0.548444.
Train: 2018-08-01T23:23:19.963005: step 8201, loss 0.579206.
Train: 2018-08-01T23:23:20.129529: step 8202, loss 0.545777.
Train: 2018-08-01T23:23:20.321043: step 8203, loss 0.595828.
Train: 2018-08-01T23:23:20.483582: step 8204, loss 0.545869.
Train: 2018-08-01T23:23:20.682052: step 8205, loss 0.628947.
Train: 2018-08-01T23:23:20.881519: step 8206, loss 0.595669.
Train: 2018-08-01T23:23:21.081985: step 8207, loss 0.612132.
Train: 2018-08-01T23:23:21.245547: step 8208, loss 0.496665.
Train: 2018-08-01T23:23:21.415123: step 8209, loss 0.546135.
Train: 2018-08-01T23:23:21.579653: step 8210, loss 0.447522.
Test: 2018-08-01T23:23:22.114227: step 8210, loss 0.548803.
Train: 2018-08-01T23:23:22.277787: step 8211, loss 0.496747.
Train: 2018-08-01T23:23:22.440352: step 8212, loss 0.595562.
Train: 2018-08-01T23:23:22.606935: step 8213, loss 0.529498.
Train: 2018-08-01T23:23:22.772490: step 8214, loss 0.54597.
Train: 2018-08-01T23:23:22.938022: step 8215, loss 0.612324.
Train: 2018-08-01T23:23:23.108566: step 8216, loss 0.545879.
Train: 2018-08-01T23:23:23.270172: step 8217, loss 0.479222.
Train: 2018-08-01T23:23:23.429707: step 8218, loss 0.612588.
Train: 2018-08-01T23:23:23.588282: step 8219, loss 0.595938.
Train: 2018-08-01T23:23:23.756832: step 8220, loss 0.62949.
Test: 2018-08-01T23:23:24.297388: step 8220, loss 0.548423.
Train: 2018-08-01T23:23:24.468962: step 8221, loss 0.495414.
Train: 2018-08-01T23:23:24.634487: step 8222, loss 0.596017.
Train: 2018-08-01T23:23:24.798049: step 8223, loss 0.495268.
Train: 2018-08-01T23:23:24.979617: step 8224, loss 0.511965.
Train: 2018-08-01T23:23:25.150142: step 8225, loss 0.61303.
Train: 2018-08-01T23:23:25.315699: step 8226, loss 0.528641.
Train: 2018-08-01T23:23:25.480225: step 8227, loss 0.562418.
Train: 2018-08-01T23:23:25.644785: step 8228, loss 0.477643.
Train: 2018-08-01T23:23:25.820347: step 8229, loss 0.528398.
Train: 2018-08-01T23:23:25.987868: step 8230, loss 0.545343.
Test: 2018-08-01T23:23:26.515458: step 8230, loss 0.548068.
Train: 2018-08-01T23:23:26.680049: step 8231, loss 0.528173.
Train: 2018-08-01T23:23:26.844609: step 8232, loss 0.613907.
Train: 2018-08-01T23:23:27.008141: step 8233, loss 0.631231.
Train: 2018-08-01T23:23:27.176690: step 8234, loss 0.52795.
Train: 2018-08-01T23:23:27.337262: step 8235, loss 0.596897.
Train: 2018-08-01T23:23:27.497869: step 8236, loss 0.562401.
Train: 2018-08-01T23:23:27.659400: step 8237, loss 0.51059.
Train: 2018-08-01T23:23:27.820968: step 8238, loss 0.562404.
Train: 2018-08-01T23:23:27.994537: step 8239, loss 0.614344.
Train: 2018-08-01T23:23:28.159065: step 8240, loss 0.614356.
Test: 2018-08-01T23:23:28.692639: step 8240, loss 0.547914.
Train: 2018-08-01T23:23:28.863213: step 8241, loss 0.562406.
Train: 2018-08-01T23:23:29.030759: step 8242, loss 0.614292.
Train: 2018-08-01T23:23:29.194298: step 8243, loss 0.562402.
Train: 2018-08-01T23:23:29.365868: step 8244, loss 0.683161.
Train: 2018-08-01T23:23:29.534413: step 8245, loss 0.527996.
Train: 2018-08-01T23:23:29.701968: step 8246, loss 0.493741.
Train: 2018-08-01T23:23:29.872520: step 8247, loss 0.5281.
Train: 2018-08-01T23:23:30.035080: step 8248, loss 0.459532.
Train: 2018-08-01T23:23:30.210580: step 8249, loss 0.631064.
Train: 2018-08-01T23:23:30.379131: step 8250, loss 0.579567.
Test: 2018-08-01T23:23:30.917690: step 8250, loss 0.548022.
Train: 2018-08-01T23:23:31.083247: step 8251, loss 0.528063.
Train: 2018-08-01T23:23:31.246814: step 8252, loss 0.579571.
Train: 2018-08-01T23:23:31.425334: step 8253, loss 0.562397.
Train: 2018-08-01T23:23:31.591888: step 8254, loss 0.716989.
Train: 2018-08-01T23:23:31.761454: step 8255, loss 0.476709.
Train: 2018-08-01T23:23:31.929017: step 8256, loss 0.596648.
Train: 2018-08-01T23:23:32.109505: step 8257, loss 0.596608.
Train: 2018-08-01T23:23:32.274065: step 8258, loss 0.528244.
Train: 2018-08-01T23:23:32.439652: step 8259, loss 0.579463.
Train: 2018-08-01T23:23:32.601189: step 8260, loss 0.562402.
Test: 2018-08-01T23:23:33.122809: step 8260, loss 0.548144.
Train: 2018-08-01T23:23:33.295365: step 8261, loss 0.494287.
Train: 2018-08-01T23:23:33.463909: step 8262, loss 0.494269.
Train: 2018-08-01T23:23:33.636452: step 8263, loss 0.664736.
Train: 2018-08-01T23:23:33.802010: step 8264, loss 0.528303.
Train: 2018-08-01T23:23:33.975544: step 8265, loss 0.562402.
Train: 2018-08-01T23:23:34.148055: step 8266, loss 0.460065.
Train: 2018-08-01T23:23:34.317632: step 8267, loss 0.476967.
Train: 2018-08-01T23:23:34.482192: step 8268, loss 0.562398.
Train: 2018-08-01T23:23:34.643760: step 8269, loss 0.562398.
Train: 2018-08-01T23:23:34.815297: step 8270, loss 0.493513.
Test: 2018-08-01T23:23:35.354829: step 8270, loss 0.547936.
Train: 2018-08-01T23:23:35.528370: step 8271, loss 0.510576.
Train: 2018-08-01T23:23:35.723843: step 8272, loss 0.493061.
Train: 2018-08-01T23:23:35.888402: step 8273, loss 0.614652.
Train: 2018-08-01T23:23:36.064931: step 8274, loss 0.632278.
Train: 2018-08-01T23:23:36.234507: step 8275, loss 0.544951.
Train: 2018-08-01T23:23:36.408049: step 8276, loss 0.492382.
Train: 2018-08-01T23:23:36.569613: step 8277, loss 0.650247.
Train: 2018-08-01T23:23:36.737164: step 8278, loss 0.509749.
Train: 2018-08-01T23:23:36.909672: step 8279, loss 0.668046.
Train: 2018-08-01T23:23:37.073235: step 8280, loss 0.52729.
Test: 2018-08-01T23:23:37.605811: step 8280, loss 0.547739.
Train: 2018-08-01T23:23:37.770374: step 8281, loss 0.668016.
Train: 2018-08-01T23:23:37.935970: step 8282, loss 0.527332.
Train: 2018-08-01T23:23:38.104502: step 8283, loss 0.562456.
Train: 2018-08-01T23:23:38.283999: step 8284, loss 0.579983.
Train: 2018-08-01T23:23:38.443610: step 8285, loss 0.650002.
Train: 2018-08-01T23:23:38.609135: step 8286, loss 0.562435.
Train: 2018-08-01T23:23:38.771700: step 8287, loss 0.510134.
Train: 2018-08-01T23:23:38.938255: step 8288, loss 0.57983.
Train: 2018-08-01T23:23:39.110794: step 8289, loss 0.545034.
Train: 2018-08-01T23:23:39.275379: step 8290, loss 0.54505.
Test: 2018-08-01T23:23:39.797971: step 8290, loss 0.547883.
Train: 2018-08-01T23:23:39.961543: step 8291, loss 0.562411.
Train: 2018-08-01T23:23:40.127109: step 8292, loss 0.527736.
Train: 2018-08-01T23:23:40.295628: step 8293, loss 0.649077.
Train: 2018-08-01T23:23:40.460217: step 8294, loss 0.579714.
Train: 2018-08-01T23:23:40.621779: step 8295, loss 0.458727.
Train: 2018-08-01T23:23:40.794293: step 8296, loss 0.527842.
Train: 2018-08-01T23:23:40.954899: step 8297, loss 0.510535.
Train: 2018-08-01T23:23:41.117429: step 8298, loss 0.562406.
Train: 2018-08-01T23:23:41.278000: step 8299, loss 0.475759.
Train: 2018-08-01T23:23:41.452534: step 8300, loss 0.562414.
Test: 2018-08-01T23:23:41.977131: step 8300, loss 0.547848.
Train: 2018-08-01T23:23:42.765359: step 8301, loss 0.56242.
Train: 2018-08-01T23:23:42.930920: step 8302, loss 0.492696.
Train: 2018-08-01T23:23:43.093512: step 8303, loss 0.579912.
Train: 2018-08-01T23:23:43.258075: step 8304, loss 0.579955.
Train: 2018-08-01T23:23:43.425628: step 8305, loss 0.562452.
Train: 2018-08-01T23:23:43.589187: step 8306, loss 0.632682.
Train: 2018-08-01T23:23:43.755742: step 8307, loss 0.580014.
Train: 2018-08-01T23:23:43.929279: step 8308, loss 0.492258.
Train: 2018-08-01T23:23:44.103780: step 8309, loss 0.667823.
Train: 2018-08-01T23:23:44.274325: step 8310, loss 0.579996.
Test: 2018-08-01T23:23:44.805904: step 8310, loss 0.547777.
Train: 2018-08-01T23:23:44.969498: step 8311, loss 0.685087.
Train: 2018-08-01T23:23:45.136052: step 8312, loss 0.614836.
Train: 2018-08-01T23:23:45.297618: step 8313, loss 0.4928.
Train: 2018-08-01T23:23:45.462175: step 8314, loss 0.475581.
Train: 2018-08-01T23:23:45.631720: step 8315, loss 0.492998.
Train: 2018-08-01T23:23:45.796257: step 8316, loss 0.545054.
Train: 2018-08-01T23:23:45.959820: step 8317, loss 0.753458.
Train: 2018-08-01T23:23:46.127371: step 8318, loss 0.458454.
Train: 2018-08-01T23:23:46.289940: step 8319, loss 0.562407.
Train: 2018-08-01T23:23:46.459485: step 8320, loss 0.545102.
Test: 2018-08-01T23:23:46.988071: step 8320, loss 0.54792.
Train: 2018-08-01T23:23:47.160640: step 8321, loss 0.458622.
Train: 2018-08-01T23:23:47.327198: step 8322, loss 0.597043.
Train: 2018-08-01T23:23:47.487735: step 8323, loss 0.527753.
Train: 2018-08-01T23:23:47.667254: step 8324, loss 0.527722.
Train: 2018-08-01T23:23:47.832838: step 8325, loss 0.579782.
Train: 2018-08-01T23:23:47.996406: step 8326, loss 0.4755.
Train: 2018-08-01T23:23:48.156946: step 8327, loss 0.492747.
Train: 2018-08-01T23:23:48.320508: step 8328, loss 0.544966.
Train: 2018-08-01T23:23:48.490100: step 8329, loss 0.579962.
Train: 2018-08-01T23:23:48.655644: step 8330, loss 0.597563.
Test: 2018-08-01T23:23:49.179212: step 8330, loss 0.547746.
Train: 2018-08-01T23:23:49.353747: step 8331, loss 0.580041.
Train: 2018-08-01T23:23:49.521324: step 8332, loss 0.474512.
Train: 2018-08-01T23:23:49.693865: step 8333, loss 0.685861.
Train: 2018-08-01T23:23:49.855431: step 8334, loss 0.597728.
Train: 2018-08-01T23:23:50.019965: step 8335, loss 0.580087.
Train: 2018-08-01T23:23:50.183539: step 8336, loss 0.544882.
Train: 2018-08-01T23:23:50.346125: step 8337, loss 0.509723.
Train: 2018-08-01T23:23:50.509656: step 8338, loss 0.650381.
Train: 2018-08-01T23:23:50.674216: step 8339, loss 0.562459.
Train: 2018-08-01T23:23:50.837792: step 8340, loss 0.509839.
Test: 2018-08-01T23:23:51.356427: step 8340, loss 0.547771.
Train: 2018-08-01T23:23:51.520981: step 8341, loss 0.597511.
Train: 2018-08-01T23:23:51.684516: step 8342, loss 0.52742.
Train: 2018-08-01T23:23:51.846083: step 8343, loss 0.509929.
Train: 2018-08-01T23:23:52.019619: step 8344, loss 0.474894.
Train: 2018-08-01T23:23:52.186174: step 8345, loss 0.439701.
Train: 2018-08-01T23:23:52.348739: step 8346, loss 0.615237.
Train: 2018-08-01T23:23:52.510346: step 8347, loss 0.632969.
Train: 2018-08-01T23:23:52.678857: step 8348, loss 0.562485.
Train: 2018-08-01T23:23:52.841454: step 8349, loss 0.615407.
Train: 2018-08-01T23:23:53.004986: step 8350, loss 0.527222.
Test: 2018-08-01T23:23:53.531579: step 8350, loss 0.54772.
Train: 2018-08-01T23:23:53.710108: step 8351, loss 0.562485.
Train: 2018-08-01T23:23:53.873689: step 8352, loss 0.474335.
Train: 2018-08-01T23:23:54.033236: step 8353, loss 0.438948.
Train: 2018-08-01T23:23:54.205776: step 8354, loss 0.597901.
Train: 2018-08-01T23:23:54.369338: step 8355, loss 0.509341.
Train: 2018-08-01T23:23:54.538911: step 8356, loss 0.420406.
Train: 2018-08-01T23:23:54.705440: step 8357, loss 0.580411.
Train: 2018-08-01T23:23:54.864045: step 8358, loss 0.419472.
Train: 2018-08-01T23:23:55.031597: step 8359, loss 0.544679.
Train: 2018-08-01T23:23:55.200118: step 8360, loss 0.562703.
Test: 2018-08-01T23:23:55.720726: step 8360, loss 0.547578.
Train: 2018-08-01T23:23:55.887312: step 8361, loss 0.580866.
Train: 2018-08-01T23:23:56.052838: step 8362, loss 0.599124.
Train: 2018-08-01T23:23:56.222385: step 8363, loss 0.490005.
Train: 2018-08-01T23:23:56.382989: step 8364, loss 0.617592.
Train: 2018-08-01T23:23:56.544524: step 8365, loss 0.599411.
Train: 2018-08-01T23:23:56.708117: step 8366, loss 0.434923.
Train: 2018-08-01T23:23:56.874641: step 8367, loss 0.691105.
Train: 2018-08-01T23:23:57.040215: step 8368, loss 0.507972.
Train: 2018-08-01T23:23:57.211766: step 8369, loss 0.67281.
Train: 2018-08-01T23:23:57.373309: step 8370, loss 0.526304.
Test: 2018-08-01T23:23:57.961333: step 8370, loss 0.54757.
Train: 2018-08-01T23:23:58.128912: step 8371, loss 0.599422.
Train: 2018-08-01T23:23:58.297493: step 8372, loss 0.617589.
Train: 2018-08-01T23:23:58.462047: step 8373, loss 0.562815.
Train: 2018-08-01T23:23:58.635584: step 8374, loss 0.562783.
Train: 2018-08-01T23:23:58.799122: step 8375, loss 0.617117.
Train: 2018-08-01T23:23:58.961688: step 8376, loss 0.634988.
Train: 2018-08-01T23:23:59.129263: step 8377, loss 0.526669.
Train: 2018-08-01T23:23:59.291804: step 8378, loss 0.562634.
Train: 2018-08-01T23:23:59.454401: step 8379, loss 0.616275.
Train: 2018-08-01T23:23:59.628931: step 8380, loss 0.526915.
Test: 2018-08-01T23:24:00.145523: step 8380, loss 0.547659.
Train: 2018-08-01T23:24:00.321078: step 8381, loss 0.491438.
Train: 2018-08-01T23:24:00.484647: step 8382, loss 0.615771.
Train: 2018-08-01T23:24:00.649206: step 8383, loss 0.456284.
Train: 2018-08-01T23:24:00.829724: step 8384, loss 0.580198.
Train: 2018-08-01T23:24:01.014225: step 8385, loss 0.633193.
Train: 2018-08-01T23:24:01.176766: step 8386, loss 0.509573.
Train: 2018-08-01T23:24:01.358280: step 8387, loss 0.544861.
Train: 2018-08-01T23:24:01.546777: step 8388, loss 0.685684.
Train: 2018-08-01T23:24:01.705677: step 8389, loss 0.580013.
Train: 2018-08-01T23:24:01.871224: step 8390, loss 0.649981.
Test: 2018-08-01T23:24:02.397816: step 8390, loss 0.547823.
Train: 2018-08-01T23:24:02.562376: step 8391, loss 0.632192.
Train: 2018-08-01T23:24:02.724943: step 8392, loss 0.527688.
Train: 2018-08-01T23:24:02.888505: step 8393, loss 0.597006.
Train: 2018-08-01T23:24:03.057081: step 8394, loss 0.45899.
Train: 2018-08-01T23:24:03.224606: step 8395, loss 0.510791.
Train: 2018-08-01T23:24:03.397146: step 8396, loss 0.6827.
Train: 2018-08-01T23:24:03.567690: step 8397, loss 0.596678.
Train: 2018-08-01T23:24:03.740228: step 8398, loss 0.579489.
Train: 2018-08-01T23:24:03.903791: step 8399, loss 0.443112.
Train: 2018-08-01T23:24:04.077354: step 8400, loss 0.545375.
Test: 2018-08-01T23:24:04.599930: step 8400, loss 0.54815.
Train: 2018-08-01T23:24:05.394385: step 8401, loss 0.52836.
Train: 2018-08-01T23:24:05.563961: step 8402, loss 0.511327.
Train: 2018-08-01T23:24:05.740460: step 8403, loss 0.579444.
Train: 2018-08-01T23:24:05.908011: step 8404, loss 0.545348.
Train: 2018-08-01T23:24:06.073569: step 8405, loss 0.613604.
Train: 2018-08-01T23:24:06.243146: step 8406, loss 0.528264.
Train: 2018-08-01T23:24:06.407675: step 8407, loss 0.545322.
Train: 2018-08-01T23:24:06.570272: step 8408, loss 0.476945.
Train: 2018-08-01T23:24:06.745771: step 8409, loss 0.545272.
Train: 2018-08-01T23:24:06.919308: step 8410, loss 0.596717.
Test: 2018-08-01T23:24:07.452178: step 8410, loss 0.548008.
Train: 2018-08-01T23:24:07.614743: step 8411, loss 0.52803.
Train: 2018-08-01T23:24:07.785321: step 8412, loss 0.579611.
Train: 2018-08-01T23:24:07.949875: step 8413, loss 0.717511.
Train: 2018-08-01T23:24:08.117417: step 8414, loss 0.52797.
Train: 2018-08-01T23:24:08.281993: step 8415, loss 0.614011.
Train: 2018-08-01T23:24:08.449512: step 8416, loss 0.51085.
Train: 2018-08-01T23:24:08.612075: step 8417, loss 0.528046.
Train: 2018-08-01T23:24:08.787654: step 8418, loss 0.476507.
Train: 2018-08-01T23:24:08.951169: step 8419, loss 0.527992.
Train: 2018-08-01T23:24:09.113761: step 8420, loss 0.596865.
Test: 2018-08-01T23:24:09.654300: step 8420, loss 0.547954.
Train: 2018-08-01T23:24:09.823838: step 8421, loss 0.545148.
Train: 2018-08-01T23:24:09.993415: step 8422, loss 0.596947.
Train: 2018-08-01T23:24:10.163953: step 8423, loss 0.545121.
Train: 2018-08-01T23:24:10.329516: step 8424, loss 0.562404.
Train: 2018-08-01T23:24:10.489058: step 8425, loss 0.648932.
Train: 2018-08-01T23:24:10.656611: step 8426, loss 0.579696.
Train: 2018-08-01T23:24:10.822169: step 8427, loss 0.666057.
Train: 2018-08-01T23:24:10.983736: step 8428, loss 0.580782.
Train: 2018-08-01T23:24:11.155278: step 8429, loss 0.562397.
Train: 2018-08-01T23:24:11.324824: step 8430, loss 0.596702.
Test: 2018-08-01T23:24:11.862387: step 8430, loss 0.548071.
Train: 2018-08-01T23:24:12.025950: step 8431, loss 0.647942.
Train: 2018-08-01T23:24:12.192536: step 8432, loss 0.511258.
Train: 2018-08-01T23:24:12.360058: step 8433, loss 0.511387.
Train: 2018-08-01T23:24:12.522653: step 8434, loss 0.460512.
Train: 2018-08-01T23:24:12.687213: step 8435, loss 0.562407.
Train: 2018-08-01T23:24:12.859751: step 8436, loss 0.477433.
Train: 2018-08-01T23:24:13.025305: step 8437, loss 0.579426.
Train: 2018-08-01T23:24:13.187875: step 8438, loss 0.460135.
Train: 2018-08-01T23:24:13.355397: step 8439, loss 0.476945.
Train: 2018-08-01T23:24:13.517987: step 8440, loss 0.579551.
Test: 2018-08-01T23:24:14.055524: step 8440, loss 0.547988.
Train: 2018-08-01T23:24:14.226068: step 8441, loss 0.562398.
Train: 2018-08-01T23:24:14.390655: step 8442, loss 0.665933.
Train: 2018-08-01T23:24:14.556187: step 8443, loss 0.42423.
Train: 2018-08-01T23:24:14.723739: step 8444, loss 0.597048.
Train: 2018-08-01T23:24:14.886305: step 8445, loss 0.649191.
Train: 2018-08-01T23:24:15.061835: step 8446, loss 0.579778.
Train: 2018-08-01T23:24:15.236368: step 8447, loss 0.614515.
Train: 2018-08-01T23:24:15.404917: step 8448, loss 0.597123.
Train: 2018-08-01T23:24:15.572469: step 8449, loss 0.527737.
Train: 2018-08-01T23:24:15.738026: step 8450, loss 0.493103.
Test: 2018-08-01T23:24:16.275633: step 8450, loss 0.547893.
Train: 2018-08-01T23:24:16.472703: step 8451, loss 0.475736.
Train: 2018-08-01T23:24:16.641247: step 8452, loss 0.527686.
Train: 2018-08-01T23:24:16.812794: step 8453, loss 0.475429.
Train: 2018-08-01T23:24:16.986330: step 8454, loss 0.63223.
Train: 2018-08-01T23:24:17.149894: step 8455, loss 0.562436.
Train: 2018-08-01T23:24:17.320406: step 8456, loss 0.579945.
Train: 2018-08-01T23:24:17.484002: step 8457, loss 0.579966.
Train: 2018-08-01T23:24:17.648529: step 8458, loss 0.492336.
Train: 2018-08-01T23:24:17.828064: step 8459, loss 0.509798.
Train: 2018-08-01T23:24:17.990615: step 8460, loss 0.615228.
Test: 2018-08-01T23:24:18.521230: step 8460, loss 0.547732.
Train: 2018-08-01T23:24:18.689745: step 8461, loss 0.492061.
Train: 2018-08-01T23:24:18.868271: step 8462, loss 0.54485.
Train: 2018-08-01T23:24:19.031866: step 8463, loss 0.580159.
Train: 2018-08-01T23:24:19.199417: step 8464, loss 0.544818.
Train: 2018-08-01T23:24:19.362946: step 8465, loss 0.509392.
Train: 2018-08-01T23:24:19.535485: step 8466, loss 0.544788.
Train: 2018-08-01T23:24:19.697100: step 8467, loss 0.562539.
Train: 2018-08-01T23:24:19.860642: step 8468, loss 0.580345.
Train: 2018-08-01T23:24:20.025181: step 8469, loss 0.615988.
Train: 2018-08-01T23:24:20.188740: step 8470, loss 0.615985.
Test: 2018-08-01T23:24:20.725303: step 8470, loss 0.547652.
Train: 2018-08-01T23:24:20.888867: step 8471, loss 0.580343.
Train: 2018-08-01T23:24:21.059411: step 8472, loss 0.633624.
Train: 2018-08-01T23:24:21.225977: step 8473, loss 0.544791.
Train: 2018-08-01T23:24:21.390525: step 8474, loss 0.633305.
Train: 2018-08-01T23:24:21.559075: step 8475, loss 0.58014.
Train: 2018-08-01T23:24:21.731614: step 8476, loss 0.580072.
Train: 2018-08-01T23:24:21.893182: step 8477, loss 0.580004.
Train: 2018-08-01T23:24:22.054776: step 8478, loss 0.562441.
Train: 2018-08-01T23:24:22.231311: step 8479, loss 0.597327.
Train: 2018-08-01T23:24:22.402845: step 8480, loss 0.666793.
Test: 2018-08-01T23:24:22.936392: step 8480, loss 0.547903.
Train: 2018-08-01T23:24:23.100987: step 8481, loss 0.545086.
Train: 2018-08-01T23:24:23.267508: step 8482, loss 0.476117.
Train: 2018-08-01T23:24:23.429076: step 8483, loss 0.545177.
Train: 2018-08-01T23:24:23.595630: step 8484, loss 0.613978.
Train: 2018-08-01T23:24:23.769166: step 8485, loss 0.476621.
Train: 2018-08-01T23:24:23.943725: step 8486, loss 0.596682.
Train: 2018-08-01T23:24:24.116239: step 8487, loss 0.545274.
Train: 2018-08-01T23:24:24.288809: step 8488, loss 0.562397.
Train: 2018-08-01T23:24:24.453338: step 8489, loss 0.528203.
Train: 2018-08-01T23:24:24.618896: step 8490, loss 0.562398.
Test: 2018-08-01T23:24:25.140517: step 8490, loss 0.548083.
Train: 2018-08-01T23:24:25.304089: step 8491, loss 0.664967.
Train: 2018-08-01T23:24:25.477610: step 8492, loss 0.5624.
Train: 2018-08-01T23:24:25.643158: step 8493, loss 0.511274.
Train: 2018-08-01T23:24:25.813726: step 8494, loss 0.477231.
Train: 2018-08-01T23:24:25.985243: step 8495, loss 0.630599.
Train: 2018-08-01T23:24:26.156783: step 8496, loss 0.579447.
Train: 2018-08-01T23:24:26.319399: step 8497, loss 0.528326.
Train: 2018-08-01T23:24:26.485904: step 8498, loss 0.477198.
Train: 2018-08-01T23:24:26.653457: step 8499, loss 0.5624.
Train: 2018-08-01T23:24:26.823004: step 8500, loss 0.596575.
Test: 2018-08-01T23:24:27.347596: step 8500, loss 0.548078.
Train: 2018-08-01T23:24:28.113342: step 8501, loss 0.596599.
Train: 2018-08-01T23:24:28.290866: step 8502, loss 0.545295.
Train: 2018-08-01T23:24:28.452469: step 8503, loss 0.613724.
Train: 2018-08-01T23:24:28.620984: step 8504, loss 0.579499.
Train: 2018-08-01T23:24:28.785575: step 8505, loss 0.613669.
Train: 2018-08-01T23:24:28.950104: step 8506, loss 0.528265.
Train: 2018-08-01T23:24:29.114689: step 8507, loss 0.528289.
Train: 2018-08-01T23:24:29.286238: step 8508, loss 0.528291.
Train: 2018-08-01T23:24:29.446777: step 8509, loss 0.545337.
Train: 2018-08-01T23:24:29.610375: step 8510, loss 0.681926.
Test: 2018-08-01T23:24:30.130984: step 8510, loss 0.548119.
Train: 2018-08-01T23:24:30.294510: step 8511, loss 0.630622.
Train: 2018-08-01T23:24:30.459101: step 8512, loss 0.562404.
Train: 2018-08-01T23:24:30.632608: step 8513, loss 0.528431.
Train: 2018-08-01T23:24:30.801188: step 8514, loss 0.596349.
Train: 2018-08-01T23:24:30.967710: step 8515, loss 0.630191.
Train: 2018-08-01T23:24:31.132270: step 8516, loss 0.494803.
Train: 2018-08-01T23:24:31.300821: step 8517, loss 0.494875.
Train: 2018-08-01T23:24:31.462413: step 8518, loss 0.528642.
Train: 2018-08-01T23:24:31.629941: step 8519, loss 0.613132.
Train: 2018-08-01T23:24:31.801482: step 8520, loss 0.545518.
Test: 2018-08-01T23:24:32.334064: step 8520, loss 0.548262.
Train: 2018-08-01T23:24:32.498619: step 8521, loss 0.63005.
Train: 2018-08-01T23:24:32.670188: step 8522, loss 0.562423.
Train: 2018-08-01T23:24:32.837711: step 8523, loss 0.545541.
Train: 2018-08-01T23:24:33.001305: step 8524, loss 0.478029.
Train: 2018-08-01T23:24:33.167830: step 8525, loss 0.545523.
Train: 2018-08-01T23:24:33.336379: step 8526, loss 0.630105.
Train: 2018-08-01T23:24:33.499942: step 8527, loss 0.545492.
Train: 2018-08-01T23:24:33.665499: step 8528, loss 0.579349.
Train: 2018-08-01T23:24:33.845019: step 8529, loss 0.630166.
Train: 2018-08-01T23:24:34.006606: step 8530, loss 0.647037.
Test: 2018-08-01T23:24:34.527194: step 8530, loss 0.54828.
Train: 2018-08-01T23:24:34.688764: step 8531, loss 0.596204.
Train: 2018-08-01T23:24:34.866288: step 8532, loss 0.511877.
Train: 2018-08-01T23:24:35.026893: step 8533, loss 0.562437.
Train: 2018-08-01T23:24:35.189425: step 8534, loss 0.579256.
Train: 2018-08-01T23:24:35.357974: step 8535, loss 0.545651.
Train: 2018-08-01T23:24:35.523532: step 8536, loss 0.596019.
Train: 2018-08-01T23:24:35.688092: step 8537, loss 0.512154.
Train: 2018-08-01T23:24:35.857649: step 8538, loss 0.629519.
Train: 2018-08-01T23:24:36.022198: step 8539, loss 0.512215.
Train: 2018-08-01T23:24:36.187756: step 8540, loss 0.478723.
Test: 2018-08-01T23:24:36.715346: step 8540, loss 0.54841.
Train: 2018-08-01T23:24:36.905837: step 8541, loss 0.562454.
Train: 2018-08-01T23:24:37.072950: step 8542, loss 0.545655.
Train: 2018-08-01T23:24:37.236645: step 8543, loss 0.579259.
Train: 2018-08-01T23:24:37.399737: step 8544, loss 0.495083.
Train: 2018-08-01T23:24:37.568285: step 8545, loss 0.579302.
Train: 2018-08-01T23:24:37.732847: step 8546, loss 0.494803.
Train: 2018-08-01T23:24:37.900399: step 8547, loss 0.596313.
Train: 2018-08-01T23:24:38.064958: step 8548, loss 0.579391.
Train: 2018-08-01T23:24:38.228521: step 8549, loss 0.477363.
Train: 2018-08-01T23:24:38.395102: step 8550, loss 0.5624.
Test: 2018-08-01T23:24:38.930643: step 8550, loss 0.548082.
Train: 2018-08-01T23:24:39.096201: step 8551, loss 0.562398.
Train: 2018-08-01T23:24:39.261789: step 8552, loss 0.476727.
Train: 2018-08-01T23:24:39.425322: step 8553, loss 0.528016.
Train: 2018-08-01T23:24:39.588884: step 8554, loss 0.5969.
Train: 2018-08-01T23:24:39.763451: step 8555, loss 0.545109.
Train: 2018-08-01T23:24:39.930998: step 8556, loss 0.493054.
Train: 2018-08-01T23:24:40.094533: step 8557, loss 0.545022.
Train: 2018-08-01T23:24:40.267071: step 8558, loss 0.510077.
Train: 2018-08-01T23:24:40.425648: step 8559, loss 0.597471.
Train: 2018-08-01T23:24:40.606218: step 8560, loss 0.457107.
Test: 2018-08-01T23:24:41.141759: step 8560, loss 0.547721.
Train: 2018-08-01T23:24:41.309286: step 8561, loss 0.544855.
Train: 2018-08-01T23:24:41.471850: step 8562, loss 0.633257.
Train: 2018-08-01T23:24:41.633444: step 8563, loss 0.615695.
Train: 2018-08-01T23:24:41.804960: step 8564, loss 0.615755.
Train: 2018-08-01T23:24:41.974526: step 8565, loss 0.509299.
Train: 2018-08-01T23:24:42.141095: step 8566, loss 0.491514.
Train: 2018-08-01T23:24:42.304624: step 8567, loss 0.509202.
Train: 2018-08-01T23:24:42.476166: step 8568, loss 0.633829.
Train: 2018-08-01T23:24:42.639760: step 8569, loss 0.633881.
Train: 2018-08-01T23:24:42.809299: step 8570, loss 0.544746.
Test: 2018-08-01T23:24:43.340906: step 8570, loss 0.547645.
Train: 2018-08-01T23:24:43.509440: step 8571, loss 0.509126.
Train: 2018-08-01T23:24:43.676956: step 8572, loss 0.491296.
Train: 2018-08-01T23:24:43.839552: step 8573, loss 0.669596.
Train: 2018-08-01T23:24:44.016051: step 8574, loss 0.580394.
Train: 2018-08-01T23:24:44.191580: step 8575, loss 0.580369.
Train: 2018-08-01T23:24:44.351592: step 8576, loss 0.6337.
Train: 2018-08-01T23:24:44.512132: step 8577, loss 0.580278.
Train: 2018-08-01T23:24:44.673730: step 8578, loss 0.456276.
Train: 2018-08-01T23:24:44.848232: step 8579, loss 0.562506.
Train: 2018-08-01T23:24:45.016783: step 8580, loss 0.544821.
Test: 2018-08-01T23:24:45.538413: step 8580, loss 0.547701.
Train: 2018-08-01T23:24:45.712921: step 8581, loss 0.527157.
Train: 2018-08-01T23:24:45.877482: step 8582, loss 0.650842.
Train: 2018-08-01T23:24:46.046057: step 8583, loss 0.562487.
Train: 2018-08-01T23:24:46.214607: step 8584, loss 0.474383.
Train: 2018-08-01T23:24:46.377145: step 8585, loss 0.54486.
Train: 2018-08-01T23:24:46.549685: step 8586, loss 0.439139.
Train: 2018-08-01T23:24:46.713271: step 8587, loss 0.597793.
Train: 2018-08-01T23:24:46.879802: step 8588, loss 0.49182.
Train: 2018-08-01T23:24:47.047354: step 8589, loss 0.456301.
Train: 2018-08-01T23:24:47.212913: step 8590, loss 0.509267.
Test: 2018-08-01T23:24:47.740501: step 8590, loss 0.547643.
Train: 2018-08-01T23:24:47.924035: step 8591, loss 0.562562.
Train: 2018-08-01T23:24:48.104553: step 8592, loss 0.508987.
Train: 2018-08-01T23:24:48.268098: step 8593, loss 0.490921.
Train: 2018-08-01T23:24:48.436649: step 8594, loss 0.61664.
Train: 2018-08-01T23:24:48.599236: step 8595, loss 0.544654.
Train: 2018-08-01T23:24:48.763766: step 8596, loss 0.635032.
Train: 2018-08-01T23:24:48.924337: step 8597, loss 0.47226.
Train: 2018-08-01T23:24:49.092885: step 8598, loss 0.508371.
Train: 2018-08-01T23:24:49.262464: step 8599, loss 0.617291.
Train: 2018-08-01T23:24:49.429984: step 8600, loss 0.544613.
Test: 2018-08-01T23:24:49.963558: step 8600, loss 0.547569.
Train: 2018-08-01T23:24:50.722819: step 8601, loss 0.526402.
Train: 2018-08-01T23:24:50.890406: step 8602, loss 0.653978.
Train: 2018-08-01T23:24:51.048947: step 8603, loss 0.635719.
Train: 2018-08-01T23:24:51.230462: step 8604, loss 0.562807.
Train: 2018-08-01T23:24:51.396025: step 8605, loss 0.562785.
Train: 2018-08-01T23:24:51.570555: step 8606, loss 0.472068.
Train: 2018-08-01T23:24:51.737138: step 8607, loss 0.508361.
Train: 2018-08-01T23:24:51.913635: step 8608, loss 0.526488.
Train: 2018-08-01T23:24:52.079221: step 8609, loss 0.580915.
Train: 2018-08-01T23:24:52.245747: step 8610, loss 0.453889.
Test: 2018-08-01T23:24:52.773338: step 8610, loss 0.547571.
Train: 2018-08-01T23:24:52.947871: step 8611, loss 0.544617.
Train: 2018-08-01T23:24:53.145343: step 8612, loss 0.562805.
Train: 2018-08-01T23:24:53.349797: step 8613, loss 0.617451.
Train: 2018-08-01T23:24:53.512361: step 8614, loss 0.690281.
Train: 2018-08-01T23:24:53.673930: step 8615, loss 0.599133.
Train: 2018-08-01T23:24:53.845497: step 8616, loss 0.617136.
Train: 2018-08-01T23:24:54.017043: step 8617, loss 0.544644.
Train: 2018-08-01T23:24:54.185562: step 8618, loss 0.54466.
Train: 2018-08-01T23:24:54.352147: step 8619, loss 0.652526.
Train: 2018-08-01T23:24:54.520665: step 8620, loss 0.670073.
Test: 2018-08-01T23:24:55.049252: step 8620, loss 0.547641.
Train: 2018-08-01T23:24:55.211827: step 8621, loss 0.473454.
Train: 2018-08-01T23:24:55.381366: step 8622, loss 0.669118.
Train: 2018-08-01T23:24:55.546948: step 8623, loss 0.597865.
Train: 2018-08-01T23:24:55.707494: step 8624, loss 0.63286.
Train: 2018-08-01T23:24:55.871056: step 8625, loss 0.649952.
Train: 2018-08-01T23:24:56.033647: step 8626, loss 0.51024.
Train: 2018-08-01T23:24:56.197215: step 8627, loss 0.562405.
Train: 2018-08-01T23:24:56.359785: step 8628, loss 0.614085.
Train: 2018-08-01T23:24:56.520320: step 8629, loss 0.648122.
Train: 2018-08-01T23:24:56.686901: step 8630, loss 0.613543.
Test: 2018-08-01T23:24:57.217457: step 8630, loss 0.548223.
Train: 2018-08-01T23:24:57.394982: step 8631, loss 0.545467.
Train: 2018-08-01T23:24:57.573505: step 8632, loss 0.545569.
Train: 2018-08-01T23:24:57.744049: step 8633, loss 0.512082.
Train: 2018-08-01T23:24:57.911630: step 8634, loss 0.512246.
Train: 2018-08-01T23:24:58.080151: step 8635, loss 0.612601.
Train: 2018-08-01T23:24:58.255681: step 8636, loss 0.562487.
Train: 2018-08-01T23:24:58.447200: step 8637, loss 0.662323.
Train: 2018-08-01T23:24:58.609765: step 8638, loss 0.496195.
Train: 2018-08-01T23:24:58.771327: step 8639, loss 0.463222.
Train: 2018-08-01T23:24:58.938886: step 8640, loss 0.579091.
Test: 2018-08-01T23:24:59.463453: step 8640, loss 0.548674.
Train: 2018-08-01T23:24:59.626049: step 8641, loss 0.579091.
Train: 2018-08-01T23:24:59.792603: step 8642, loss 0.57909.
Train: 2018-08-01T23:24:59.951173: step 8643, loss 0.529445.
Train: 2018-08-01T23:25:00.113714: step 8644, loss 0.545982.
Train: 2018-08-01T23:25:00.291239: step 8645, loss 0.479696.
Train: 2018-08-01T23:25:00.453804: step 8646, loss 0.529309.
Train: 2018-08-01T23:25:00.619397: step 8647, loss 0.629082.
Train: 2018-08-01T23:25:00.781927: step 8648, loss 0.52915.
Train: 2018-08-01T23:25:00.952472: step 8649, loss 0.645978.
Train: 2018-08-01T23:25:01.127005: step 8650, loss 0.612597.
Test: 2018-08-01T23:25:01.657618: step 8650, loss 0.54849.
Train: 2018-08-01T23:25:01.823144: step 8651, loss 0.51237.
Train: 2018-08-01T23:25:01.990697: step 8652, loss 0.545761.
Train: 2018-08-01T23:25:02.157277: step 8653, loss 0.445376.
Train: 2018-08-01T23:25:02.321812: step 8654, loss 0.495358.
Train: 2018-08-01T23:25:02.489409: step 8655, loss 0.528766.
Train: 2018-08-01T23:25:02.655938: step 8656, loss 0.511722.
Train: 2018-08-01T23:25:02.819481: step 8657, loss 0.528463.
Train: 2018-08-01T23:25:02.980077: step 8658, loss 0.528305.
Train: 2018-08-01T23:25:03.143645: step 8659, loss 0.562397.
Train: 2018-08-01T23:25:03.322137: step 8660, loss 0.562397.
Test: 2018-08-01T23:25:03.863693: step 8660, loss 0.547949.
Train: 2018-08-01T23:25:04.031242: step 8661, loss 0.579657.
Train: 2018-08-01T23:25:04.192835: step 8662, loss 0.527788.
Train: 2018-08-01T23:25:04.354409: step 8663, loss 0.631867.
Train: 2018-08-01T23:25:04.529908: step 8664, loss 0.475448.
Train: 2018-08-01T23:25:04.693504: step 8665, loss 0.562428.
Train: 2018-08-01T23:25:04.864016: step 8666, loss 0.492492.
Train: 2018-08-01T23:25:05.029573: step 8667, loss 0.562453.
Train: 2018-08-01T23:25:05.206132: step 8668, loss 0.703198.
Train: 2018-08-01T23:25:05.387637: step 8669, loss 0.580068.
Train: 2018-08-01T23:25:05.556165: step 8670, loss 0.632861.
Test: 2018-08-01T23:25:06.090743: step 8670, loss 0.547744.
Train: 2018-08-01T23:25:06.252335: step 8671, loss 0.562464.
Train: 2018-08-01T23:25:06.416895: step 8672, loss 0.562458.
Train: 2018-08-01T23:25:06.582423: step 8673, loss 0.615074.
Train: 2018-08-01T23:25:06.760944: step 8674, loss 0.544935.
Train: 2018-08-01T23:25:06.930491: step 8675, loss 0.632373.
Train: 2018-08-01T23:25:07.092059: step 8676, loss 0.614754.
Train: 2018-08-01T23:25:07.270582: step 8677, loss 0.68415.
Train: 2018-08-01T23:25:07.432150: step 8678, loss 0.597028.
Train: 2018-08-01T23:25:07.598731: step 8679, loss 0.59686.
Train: 2018-08-01T23:25:07.765284: step 8680, loss 0.545247.
Test: 2018-08-01T23:25:08.295854: step 8680, loss 0.548095.
Train: 2018-08-01T23:25:08.457440: step 8681, loss 0.511157.
Train: 2018-08-01T23:25:08.620001: step 8682, loss 0.528338.
Train: 2018-08-01T23:25:08.780547: step 8683, loss 0.545408.
Train: 2018-08-01T23:25:08.945105: step 8684, loss 0.528465.
Train: 2018-08-01T23:25:09.107670: step 8685, loss 0.545453.
Train: 2018-08-01T23:25:09.283201: step 8686, loss 0.545461.
Train: 2018-08-01T23:25:09.450780: step 8687, loss 0.562412.
Train: 2018-08-01T23:25:09.625287: step 8688, loss 0.494612.
Train: 2018-08-01T23:25:09.785858: step 8689, loss 0.56241.
Train: 2018-08-01T23:25:09.962400: step 8690, loss 0.545421.
Test: 2018-08-01T23:25:10.484989: step 8690, loss 0.548164.
Train: 2018-08-01T23:25:10.648578: step 8691, loss 0.613425.
Train: 2018-08-01T23:25:10.814110: step 8692, loss 0.596429.
Train: 2018-08-01T23:25:10.973709: step 8693, loss 0.647448.
Train: 2018-08-01T23:25:11.139266: step 8694, loss 0.52844.
Train: 2018-08-01T23:25:11.307823: step 8695, loss 0.596351.
Train: 2018-08-01T23:25:11.466396: step 8696, loss 0.647165.
Train: 2018-08-01T23:25:11.628957: step 8697, loss 0.57933.
Train: 2018-08-01T23:25:11.794514: step 8698, loss 0.59617.
Train: 2018-08-01T23:25:11.961051: step 8699, loss 0.646575.
Train: 2018-08-01T23:25:12.124606: step 8700, loss 0.595988.
Test: 2018-08-01T23:25:12.652210: step 8700, loss 0.548487.
Train: 2018-08-01T23:25:13.474362: step 8701, loss 0.54577.
Train: 2018-08-01T23:25:13.635952: step 8702, loss 0.51253.
Train: 2018-08-01T23:25:13.796493: step 8703, loss 0.64563.
Train: 2018-08-01T23:25:13.959086: step 8704, loss 0.512793.
Train: 2018-08-01T23:25:14.120653: step 8705, loss 0.57909.
Train: 2018-08-01T23:25:14.287220: step 8706, loss 0.529508.
Train: 2018-08-01T23:25:14.459732: step 8707, loss 0.463493.
Train: 2018-08-01T23:25:14.623308: step 8708, loss 0.579077.
Train: 2018-08-01T23:25:14.783853: step 8709, loss 0.628711.
Train: 2018-08-01T23:25:14.958387: step 8710, loss 0.661775.
Test: 2018-08-01T23:25:15.479993: step 8710, loss 0.548732.
Train: 2018-08-01T23:25:15.644585: step 8711, loss 0.562558.
Train: 2018-08-01T23:25:15.814099: step 8712, loss 0.579057.
Train: 2018-08-01T23:25:15.980679: step 8713, loss 0.546117.
Train: 2018-08-01T23:25:16.148205: step 8714, loss 0.611942.
Train: 2018-08-01T23:25:16.309774: step 8715, loss 0.529748.
Train: 2018-08-01T23:25:16.478324: step 8716, loss 0.579026.
Train: 2018-08-01T23:25:16.641916: step 8717, loss 0.496984.
Train: 2018-08-01T23:25:16.806467: step 8718, loss 0.562607.
Train: 2018-08-01T23:25:16.967018: step 8719, loss 0.66119.
Train: 2018-08-01T23:25:17.130591: step 8720, loss 0.546189.
Test: 2018-08-01T23:25:17.655185: step 8720, loss 0.548866.
Train: 2018-08-01T23:25:17.815781: step 8721, loss 0.546197.
Train: 2018-08-01T23:25:17.991280: step 8722, loss 0.529775.
Train: 2018-08-01T23:25:18.153870: step 8723, loss 0.562601.
Train: 2018-08-01T23:25:18.319401: step 8724, loss 0.513253.
Train: 2018-08-01T23:25:18.488948: step 8725, loss 0.364859.
Train: 2018-08-01T23:25:18.655536: step 8726, loss 0.645332.
Train: 2018-08-01T23:25:18.818069: step 8727, loss 0.628966.
Train: 2018-08-01T23:25:18.983627: step 8728, loss 0.529204.
Train: 2018-08-01T23:25:19.145222: step 8729, loss 0.61588.
Train: 2018-08-01T23:25:19.311749: step 8730, loss 0.478921.
Test: 2018-08-01T23:25:19.846351: step 8730, loss 0.548428.
Train: 2018-08-01T23:25:20.007889: step 8731, loss 0.59597.
Train: 2018-08-01T23:25:20.172448: step 8732, loss 0.495294.
Train: 2018-08-01T23:25:20.332021: step 8733, loss 0.612949.
Train: 2018-08-01T23:25:20.496614: step 8734, loss 0.494951.
Train: 2018-08-01T23:25:20.661153: step 8735, loss 0.511668.
Train: 2018-08-01T23:25:20.826730: step 8736, loss 0.528462.
Train: 2018-08-01T23:25:20.993285: step 8737, loss 0.528333.
Train: 2018-08-01T23:25:21.157856: step 8738, loss 0.528199.
Train: 2018-08-01T23:25:21.318384: step 8739, loss 0.613896.
Train: 2018-08-01T23:25:21.482944: step 8740, loss 0.545185.
Test: 2018-08-01T23:25:22.010544: step 8740, loss 0.547948.
Train: 2018-08-01T23:25:22.179124: step 8741, loss 0.631441.
Train: 2018-08-01T23:25:22.344641: step 8742, loss 0.510553.
Train: 2018-08-01T23:25:22.506234: step 8743, loss 0.527772.
Train: 2018-08-01T23:25:22.665784: step 8744, loss 0.562412.
Train: 2018-08-01T23:25:22.828382: step 8745, loss 0.458069.
Train: 2018-08-01T23:25:22.990938: step 8746, loss 0.649674.
Train: 2018-08-01T23:25:23.155474: step 8747, loss 0.544959.
Train: 2018-08-01T23:25:23.317068: step 8748, loss 0.61496.
Train: 2018-08-01T23:25:23.479608: step 8749, loss 0.54493.
Train: 2018-08-01T23:25:23.646193: step 8750, loss 0.4748.
Test: 2018-08-01T23:25:24.175746: step 8750, loss 0.547753.
Train: 2018-08-01T23:25:24.343298: step 8751, loss 0.404391.
Train: 2018-08-01T23:25:24.501902: step 8752, loss 0.63301.
Train: 2018-08-01T23:25:24.660476: step 8753, loss 0.474128.
Train: 2018-08-01T23:25:24.829998: step 8754, loss 0.456126.
Train: 2018-08-01T23:25:24.989602: step 8755, loss 0.437886.
Train: 2018-08-01T23:25:25.158146: step 8756, loss 0.544703.
Train: 2018-08-01T23:25:25.324674: step 8757, loss 0.490666.
Train: 2018-08-01T23:25:25.489234: step 8758, loss 0.490344.
Train: 2018-08-01T23:25:25.649807: step 8759, loss 0.562809.
Train: 2018-08-01T23:25:25.820349: step 8760, loss 0.764044.
Test: 2018-08-01T23:25:26.350932: step 8760, loss 0.547569.
Train: 2018-08-01T23:25:26.518484: step 8761, loss 0.617846.
Train: 2018-08-01T23:25:26.683044: step 8762, loss 0.434669.
Train: 2018-08-01T23:25:26.851594: step 8763, loss 0.599645.
Train: 2018-08-01T23:25:27.023136: step 8764, loss 0.562955.
Train: 2018-08-01T23:25:27.184736: step 8765, loss 0.599726.
Train: 2018-08-01T23:25:27.349263: step 8766, loss 0.599717.
Train: 2018-08-01T23:25:27.525791: step 8767, loss 0.581309.
Train: 2018-08-01T23:25:27.690354: step 8768, loss 0.654623.
Train: 2018-08-01T23:25:27.853931: step 8769, loss 0.617764.
Train: 2018-08-01T23:25:28.016509: step 8770, loss 0.508136.
Test: 2018-08-01T23:25:28.535094: step 8770, loss 0.54757.
Train: 2018-08-01T23:25:28.698682: step 8771, loss 0.526422.
Train: 2018-08-01T23:25:28.860248: step 8772, loss 0.54462.
Train: 2018-08-01T23:25:29.031765: step 8773, loss 0.5265.
Train: 2018-08-01T23:25:29.194331: step 8774, loss 0.598951.
Train: 2018-08-01T23:25:29.358921: step 8775, loss 0.580792.
Train: 2018-08-01T23:25:29.521456: step 8776, loss 0.418373.
Train: 2018-08-01T23:25:29.684022: step 8777, loss 0.562694.
Train: 2018-08-01T23:25:29.851574: step 8778, loss 0.526615.
Train: 2018-08-01T23:25:30.018159: step 8779, loss 0.688995.
Train: 2018-08-01T23:25:30.190667: step 8780, loss 0.598699.
Test: 2018-08-01T23:25:30.722259: step 8780, loss 0.5476.
Train: 2018-08-01T23:25:30.891818: step 8781, loss 0.598596.
Train: 2018-08-01T23:25:31.060341: step 8782, loss 0.508844.
Train: 2018-08-01T23:25:31.224902: step 8783, loss 0.705759.
Train: 2018-08-01T23:25:31.393476: step 8784, loss 0.473428.
Train: 2018-08-01T23:25:31.556018: step 8785, loss 0.615913.
Train: 2018-08-01T23:25:31.718608: step 8786, loss 0.52705.
Train: 2018-08-01T23:25:31.882145: step 8787, loss 0.527112.
Train: 2018-08-01T23:25:32.052709: step 8788, loss 0.509488.
Train: 2018-08-01T23:25:32.219278: step 8789, loss 0.739049.
Train: 2018-08-01T23:25:32.387847: step 8790, loss 0.509678.
Test: 2018-08-01T23:25:32.912392: step 8790, loss 0.547756.
Train: 2018-08-01T23:25:33.076952: step 8791, loss 0.509787.
Train: 2018-08-01T23:25:33.243505: step 8792, loss 0.544918.
Train: 2018-08-01T23:25:33.414050: step 8793, loss 0.72006.
Train: 2018-08-01T23:25:33.575644: step 8794, loss 0.614794.
Train: 2018-08-01T23:25:33.739211: step 8795, loss 0.52764.
Train: 2018-08-01T23:25:33.903767: step 8796, loss 0.5104.
Train: 2018-08-01T23:25:34.072321: step 8797, loss 0.631613.
Train: 2018-08-01T23:25:34.243850: step 8798, loss 0.545146.
Train: 2018-08-01T23:25:34.414375: step 8799, loss 0.52797.
Train: 2018-08-01T23:25:34.578969: step 8800, loss 0.579583.
Test: 2018-08-01T23:25:35.108520: step 8800, loss 0.548029.
Train: 2018-08-01T23:25:35.913199: step 8801, loss 0.579553.
Train: 2018-08-01T23:25:36.081740: step 8802, loss 0.64803.
Train: 2018-08-01T23:25:36.246299: step 8803, loss 0.545321.
Train: 2018-08-01T23:25:36.413852: step 8804, loss 0.545364.
Train: 2018-08-01T23:25:36.580406: step 8805, loss 0.630429.
Train: 2018-08-01T23:25:36.762892: step 8806, loss 0.562411.
Train: 2018-08-01T23:25:36.929448: step 8807, loss 0.680864.
Train: 2018-08-01T23:25:37.094009: step 8808, loss 0.62985.
Train: 2018-08-01T23:25:37.266547: step 8809, loss 0.562451.
Train: 2018-08-01T23:25:37.431107: step 8810, loss 0.629318.
Test: 2018-08-01T23:25:37.960699: step 8810, loss 0.548572.
Train: 2018-08-01T23:25:38.130264: step 8811, loss 0.495963.
Train: 2018-08-01T23:25:38.292806: step 8812, loss 0.512767.
Train: 2018-08-01T23:25:38.457364: step 8813, loss 0.595647.
Train: 2018-08-01T23:25:38.622942: step 8814, loss 0.529507.
Train: 2018-08-01T23:25:38.788477: step 8815, loss 0.661582.
Train: 2018-08-01T23:25:38.953067: step 8816, loss 0.39796.
Train: 2018-08-01T23:25:39.116601: step 8817, loss 0.579049.
Train: 2018-08-01T23:25:39.282184: step 8818, loss 0.529618.
Train: 2018-08-01T23:25:39.456691: step 8819, loss 0.595555.
Train: 2018-08-01T23:25:39.625242: step 8820, loss 0.529553.
Test: 2018-08-01T23:25:40.157818: step 8820, loss 0.548716.
Train: 2018-08-01T23:25:40.328363: step 8821, loss 0.512983.
Train: 2018-08-01T23:25:40.490928: step 8822, loss 0.579091.
Train: 2018-08-01T23:25:40.689396: step 8823, loss 0.479614.
Train: 2018-08-01T23:25:40.848995: step 8824, loss 0.479352.
Train: 2018-08-01T23:25:41.013540: step 8825, loss 0.512386.
Train: 2018-08-01T23:25:41.176124: step 8826, loss 0.512135.
Train: 2018-08-01T23:25:41.339664: step 8827, loss 0.680415.
Train: 2018-08-01T23:25:41.501227: step 8828, loss 0.562421.
Train: 2018-08-01T23:25:41.672767: step 8829, loss 0.579354.
Train: 2018-08-01T23:25:41.843311: step 8830, loss 0.511491.
Test: 2018-08-01T23:25:42.382880: step 8830, loss 0.548155.
Train: 2018-08-01T23:25:42.548428: step 8831, loss 0.579419.
Train: 2018-08-01T23:25:42.712015: step 8832, loss 0.511246.
Train: 2018-08-01T23:25:42.871594: step 8833, loss 0.613687.
Train: 2018-08-01T23:25:43.034130: step 8834, loss 0.425402.
Train: 2018-08-01T23:25:43.196724: step 8835, loss 0.493658.
Train: 2018-08-01T23:25:43.363275: step 8836, loss 0.614169.
Train: 2018-08-01T23:25:43.529832: step 8837, loss 0.631634.
Train: 2018-08-01T23:25:43.696384: step 8838, loss 0.510403.
Train: 2018-08-01T23:25:43.858924: step 8839, loss 0.510296.
Train: 2018-08-01T23:25:44.023483: step 8840, loss 0.562422.
Test: 2018-08-01T23:25:44.559051: step 8840, loss 0.547811.
Train: 2018-08-01T23:25:44.725607: step 8841, loss 0.475132.
Train: 2018-08-01T23:25:44.891164: step 8842, loss 0.439822.
Train: 2018-08-01T23:25:45.049772: step 8843, loss 0.544872.
Train: 2018-08-01T23:25:45.222278: step 8844, loss 0.580175.
Train: 2018-08-01T23:25:45.394817: step 8845, loss 0.527049.
Train: 2018-08-01T23:25:45.558406: step 8846, loss 0.473549.
Train: 2018-08-01T23:25:45.724936: step 8847, loss 0.598351.
Train: 2018-08-01T23:25:45.884540: step 8848, loss 0.616435.
Train: 2018-08-01T23:25:46.054055: step 8849, loss 0.598594.
Train: 2018-08-01T23:25:46.220610: step 8850, loss 0.616642.
Test: 2018-08-01T23:25:46.754184: step 8850, loss 0.547595.
Train: 2018-08-01T23:25:46.919766: step 8851, loss 0.562665.
Train: 2018-08-01T23:25:47.102253: step 8852, loss 0.562665.
Train: 2018-08-01T23:25:47.275791: step 8853, loss 0.490691.
Train: 2018-08-01T23:25:47.443341: step 8854, loss 0.688707.
Train: 2018-08-01T23:25:47.606904: step 8855, loss 0.616604.
Train: 2018-08-01T23:25:47.773460: step 8856, loss 0.508796.
Train: 2018-08-01T23:25:47.947992: step 8857, loss 0.490925.
Train: 2018-08-01T23:25:48.111556: step 8858, loss 0.508861.
Train: 2018-08-01T23:25:48.280105: step 8859, loss 0.562622.
Train: 2018-08-01T23:25:48.446660: step 8860, loss 0.437126.
Test: 2018-08-01T23:25:48.971271: step 8860, loss 0.547603.
Train: 2018-08-01T23:25:49.138838: step 8861, loss 0.634479.
Train: 2018-08-01T23:25:49.304367: step 8862, loss 0.454846.
Train: 2018-08-01T23:25:49.475939: step 8863, loss 0.454682.
Train: 2018-08-01T23:25:49.639500: step 8864, loss 0.508554.
Train: 2018-08-01T23:25:49.806056: step 8865, loss 0.508425.
Train: 2018-08-01T23:25:49.977592: step 8866, loss 0.599113.
Train: 2018-08-01T23:25:50.144150: step 8867, loss 0.599231.
Train: 2018-08-01T23:25:50.305716: step 8868, loss 0.599303.
Train: 2018-08-01T23:25:50.467289: step 8869, loss 0.635822.
Train: 2018-08-01T23:25:50.640825: step 8870, loss 0.526371.
Test: 2018-08-01T23:25:51.161402: step 8870, loss 0.547568.
Train: 2018-08-01T23:25:51.328980: step 8871, loss 0.653971.
Train: 2018-08-01T23:25:51.492552: step 8872, loss 0.526413.
Train: 2018-08-01T23:25:51.654111: step 8873, loss 0.471911.
Train: 2018-08-01T23:25:51.825657: step 8874, loss 0.56279.
Train: 2018-08-01T23:25:51.992182: step 8875, loss 0.617299.
Train: 2018-08-01T23:25:52.152783: step 8876, loss 0.562772.
Train: 2018-08-01T23:25:52.327286: step 8877, loss 0.599013.
Train: 2018-08-01T23:25:52.493858: step 8878, loss 0.508441.
Train: 2018-08-01T23:25:52.654424: step 8879, loss 0.490405.
Train: 2018-08-01T23:25:52.818001: step 8880, loss 0.562718.
Test: 2018-08-01T23:25:53.323622: step 8880, loss 0.547582.
Train: 2018-08-01T23:25:53.487185: step 8881, loss 0.671147.
Train: 2018-08-01T23:25:53.651745: step 8882, loss 0.688959.
Train: 2018-08-01T23:25:53.817332: step 8883, loss 0.652521.
Train: 2018-08-01T23:25:53.989874: step 8884, loss 0.598386.
Train: 2018-08-01T23:25:54.166400: step 8885, loss 0.544751.
Train: 2018-08-01T23:25:54.328935: step 8886, loss 0.580254.
Train: 2018-08-01T23:25:54.493495: step 8887, loss 0.580151.
Train: 2018-08-01T23:25:54.657058: step 8888, loss 0.597639.
Train: 2018-08-01T23:25:54.819656: step 8889, loss 0.579955.
Train: 2018-08-01T23:25:54.985181: step 8890, loss 0.562426.
Test: 2018-08-01T23:25:55.508781: step 8890, loss 0.547867.
Train: 2018-08-01T23:25:55.676333: step 8891, loss 0.684009.
Train: 2018-08-01T23:25:55.854856: step 8892, loss 0.562402.
Train: 2018-08-01T23:25:56.017420: step 8893, loss 0.562396.
Train: 2018-08-01T23:25:56.198936: step 8894, loss 0.596639.
Train: 2018-08-01T23:25:56.370478: step 8895, loss 0.613536.
Train: 2018-08-01T23:25:56.546010: step 8896, loss 0.579374.
Train: 2018-08-01T23:25:56.746472: step 8897, loss 0.562424.
Train: 2018-08-01T23:25:56.921005: step 8898, loss 0.528803.
Train: 2018-08-01T23:25:57.097534: step 8899, loss 0.612756.
Train: 2018-08-01T23:25:57.271069: step 8900, loss 0.629308.
Test: 2018-08-01T23:25:57.801655: step 8900, loss 0.548565.
Train: 2018-08-01T23:25:58.613287: step 8901, loss 0.562499.
Train: 2018-08-01T23:25:58.808752: step 8902, loss 0.562524.
Train: 2018-08-01T23:25:58.976304: step 8903, loss 0.628661.
Train: 2018-08-01T23:25:59.156820: step 8904, loss 0.677841.
Train: 2018-08-01T23:25:59.322378: step 8905, loss 0.57901.
Train: 2018-08-01T23:25:59.487935: step 8906, loss 0.595285.
Train: 2018-08-01T23:25:59.662470: step 8907, loss 0.62764.
Train: 2018-08-01T23:25:59.827028: step 8908, loss 0.546649.
Train: 2018-08-01T23:26:00.001564: step 8909, loss 0.54677.
Train: 2018-08-01T23:26:00.182111: step 8910, loss 0.643023.
Test: 2018-08-01T23:26:00.715654: step 8910, loss 0.549587.
Train: 2018-08-01T23:26:00.892213: step 8911, loss 0.499108.
Train: 2018-08-01T23:26:01.064720: step 8912, loss 0.515213.
Train: 2018-08-01T23:26:01.232273: step 8913, loss 0.531184.
Train: 2018-08-01T23:26:01.395836: step 8914, loss 0.531186.
Train: 2018-08-01T23:26:01.563418: step 8915, loss 0.578911.
Train: 2018-08-01T23:26:01.733931: step 8916, loss 0.578912.
Train: 2018-08-01T23:26:01.897495: step 8917, loss 0.642679.
Train: 2018-08-01T23:26:02.060090: step 8918, loss 0.515181.
Train: 2018-08-01T23:26:02.231601: step 8919, loss 0.515144.
Train: 2018-08-01T23:26:02.393170: step 8920, loss 0.546978.
Test: 2018-08-01T23:26:02.931730: step 8920, loss 0.549519.
Train: 2018-08-01T23:26:03.104293: step 8921, loss 0.594916.
Train: 2018-08-01T23:26:03.272856: step 8922, loss 0.594942.
Train: 2018-08-01T23:26:03.444358: step 8923, loss 0.578921.
Train: 2018-08-01T23:26:03.609916: step 8924, loss 0.482597.
Train: 2018-08-01T23:26:03.769502: step 8925, loss 0.578928.
Train: 2018-08-01T23:26:03.931058: step 8926, loss 0.530553.
Train: 2018-08-01T23:26:04.092627: step 8927, loss 0.562774.
Train: 2018-08-01T23:26:04.270151: step 8928, loss 0.497892.
Train: 2018-08-01T23:26:04.442691: step 8929, loss 0.513885.
Train: 2018-08-01T23:26:04.604276: step 8930, loss 0.578995.
Test: 2018-08-01T23:26:05.138844: step 8930, loss 0.548882.
Train: 2018-08-01T23:26:05.313322: step 8931, loss 0.447804.
Train: 2018-08-01T23:26:05.482040: step 8932, loss 0.595548.
Train: 2018-08-01T23:26:05.650779: step 8933, loss 0.529407.
Train: 2018-08-01T23:26:05.826331: step 8934, loss 0.579139.
Train: 2018-08-01T23:26:05.994834: step 8935, loss 0.57918.
Train: 2018-08-01T23:26:06.174355: step 8936, loss 0.495407.
Train: 2018-08-01T23:26:06.351880: step 8937, loss 0.646598.
Train: 2018-08-01T23:26:06.523445: step 8938, loss 0.528678.
Train: 2018-08-01T23:26:06.697980: step 8939, loss 0.562417.
Train: 2018-08-01T23:26:06.862544: step 8940, loss 0.579376.
Test: 2018-08-01T23:26:07.387112: step 8940, loss 0.548169.
Train: 2018-08-01T23:26:07.549678: step 8941, loss 0.562405.
Train: 2018-08-01T23:26:07.710248: step 8942, loss 0.5113.
Train: 2018-08-01T23:26:07.876829: step 8943, loss 0.545322.
Train: 2018-08-01T23:26:08.069288: step 8944, loss 0.493915.
Train: 2018-08-01T23:26:08.245817: step 8945, loss 0.579573.
Train: 2018-08-01T23:26:08.441294: step 8946, loss 0.596846.
Train: 2018-08-01T23:26:08.648740: step 8947, loss 0.493372.
Train: 2018-08-01T23:26:08.827262: step 8948, loss 0.510492.
Train: 2018-08-01T23:26:09.016756: step 8949, loss 0.649208.
Train: 2018-08-01T23:26:09.176329: step 8950, loss 0.631961.
Test: 2018-08-01T23:26:09.712895: step 8950, loss 0.547854.
Train: 2018-08-01T23:26:09.883439: step 8951, loss 0.684159.
Train: 2018-08-01T23:26:10.051988: step 8952, loss 0.562413.
Train: 2018-08-01T23:26:10.222533: step 8953, loss 0.510388.
Train: 2018-08-01T23:26:10.391082: step 8954, loss 0.562408.
Train: 2018-08-01T23:26:10.558633: step 8955, loss 0.579728.
Train: 2018-08-01T23:26:10.733168: step 8956, loss 0.527789.
Train: 2018-08-01T23:26:10.914713: step 8957, loss 0.614318.
Train: 2018-08-01T23:26:11.078244: step 8958, loss 0.493253.
Train: 2018-08-01T23:26:11.254774: step 8959, loss 0.527824.
Train: 2018-08-01T23:26:11.417365: step 8960, loss 0.493207.
Test: 2018-08-01T23:26:11.955906: step 8960, loss 0.547899.
Train: 2018-08-01T23:26:12.122454: step 8961, loss 0.597058.
Train: 2018-08-01T23:26:12.290005: step 8962, loss 0.510389.
Train: 2018-08-01T23:26:12.451596: step 8963, loss 0.614509.
Train: 2018-08-01T23:26:12.617132: step 8964, loss 0.54504.
Train: 2018-08-01T23:26:12.777732: step 8965, loss 0.562416.
Train: 2018-08-01T23:26:12.943290: step 8966, loss 0.423249.
Train: 2018-08-01T23:26:13.107819: step 8967, loss 0.579866.
Train: 2018-08-01T23:26:13.281356: step 8968, loss 0.440121.
Train: 2018-08-01T23:26:13.445916: step 8969, loss 0.75532.
Train: 2018-08-01T23:26:13.609506: step 8970, loss 0.562451.
Test: 2018-08-01T23:26:14.143051: step 8970, loss 0.547764.
Train: 2018-08-01T23:26:14.307642: step 8971, loss 0.579993.
Train: 2018-08-01T23:26:14.468183: step 8972, loss 0.579988.
Train: 2018-08-01T23:26:14.627756: step 8973, loss 0.632561.
Train: 2018-08-01T23:26:14.791319: step 8974, loss 0.492436.
Train: 2018-08-01T23:26:14.951889: step 8975, loss 0.474971.
Train: 2018-08-01T23:26:15.112495: step 8976, loss 0.474904.
Train: 2018-08-01T23:26:15.281010: step 8977, loss 0.579994.
Train: 2018-08-01T23:26:15.445570: step 8978, loss 0.597591.
Train: 2018-08-01T23:26:15.617114: step 8979, loss 0.492154.
Train: 2018-08-01T23:26:15.787655: step 8980, loss 0.580077.
Test: 2018-08-01T23:26:16.319235: step 8980, loss 0.547722.
Train: 2018-08-01T23:26:16.513942: step 8981, loss 0.544856.
Train: 2018-08-01T23:26:16.680470: step 8982, loss 0.597769.
Train: 2018-08-01T23:26:16.853009: step 8983, loss 0.527191.
Train: 2018-08-01T23:26:17.016598: step 8984, loss 0.61548.
Train: 2018-08-01T23:26:17.177144: step 8985, loss 0.61547.
Train: 2018-08-01T23:26:17.341703: step 8986, loss 0.650694.
Train: 2018-08-01T23:26:17.504269: step 8987, loss 0.527267.
Train: 2018-08-01T23:26:17.670824: step 8988, loss 0.474593.
Train: 2018-08-01T23:26:17.832391: step 8989, loss 0.544892.
Train: 2018-08-01T23:26:18.000979: step 8990, loss 0.527324.
Test: 2018-08-01T23:26:18.532544: step 8990, loss 0.547746.
Train: 2018-08-01T23:26:18.703089: step 8991, loss 0.615185.
Train: 2018-08-01T23:26:18.864496: step 8992, loss 0.56246.
Train: 2018-08-01T23:26:19.034472: step 8993, loss 0.492233.
Train: 2018-08-01T23:26:19.201002: step 8994, loss 0.597587.
Train: 2018-08-01T23:26:19.368578: step 8995, loss 0.597579.
Train: 2018-08-01T23:26:19.532116: step 8996, loss 0.685287.
Train: 2018-08-01T23:26:19.698676: step 8997, loss 0.562442.
Train: 2018-08-01T23:26:19.865254: step 8998, loss 0.544969.
Train: 2018-08-01T23:26:20.023833: step 8999, loss 0.579854.
Train: 2018-08-01T23:26:20.189358: step 9000, loss 0.579811.
Test: 2018-08-01T23:26:20.717982: step 9000, loss 0.547877.
Train: 2018-08-01T23:26:21.584310: step 9001, loss 0.562411.
Train: 2018-08-01T23:26:21.760839: step 9002, loss 0.631698.
Train: 2018-08-01T23:26:21.934374: step 9003, loss 0.441483.
Train: 2018-08-01T23:26:22.108906: step 9004, loss 0.476097.
Train: 2018-08-01T23:26:22.269504: step 9005, loss 0.579671.
Train: 2018-08-01T23:26:22.438028: step 9006, loss 0.596949.
Train: 2018-08-01T23:26:22.602612: step 9007, loss 0.527864.
Train: 2018-08-01T23:26:22.764156: step 9008, loss 0.614216.
Train: 2018-08-01T23:26:22.932705: step 9009, loss 0.631444.
Train: 2018-08-01T23:26:23.096293: step 9010, loss 0.545165.
Test: 2018-08-01T23:26:23.623856: step 9010, loss 0.547984.
Train: 2018-08-01T23:26:23.798391: step 9011, loss 0.57961.
Train: 2018-08-01T23:26:23.962951: step 9012, loss 0.579586.
Train: 2018-08-01T23:26:24.134517: step 9013, loss 0.545232.
Train: 2018-08-01T23:26:24.302045: step 9014, loss 0.648122.
Train: 2018-08-01T23:26:24.467602: step 9015, loss 0.54529.
Train: 2018-08-01T23:26:24.636151: step 9016, loss 0.51117.
Train: 2018-08-01T23:26:24.802731: step 9017, loss 0.613587.
Train: 2018-08-01T23:26:24.969261: step 9018, loss 0.477216.
Train: 2018-08-01T23:26:25.133852: step 9019, loss 0.562402.
Train: 2018-08-01T23:26:25.310377: step 9020, loss 0.613513.
Test: 2018-08-01T23:26:25.851901: step 9020, loss 0.548147.
Train: 2018-08-01T23:26:26.015498: step 9021, loss 0.596451.
Train: 2018-08-01T23:26:26.182019: step 9022, loss 0.528396.
Train: 2018-08-01T23:26:26.343587: step 9023, loss 0.681377.
Train: 2018-08-01T23:26:26.513159: step 9024, loss 0.579369.
Train: 2018-08-01T23:26:26.692654: step 9025, loss 0.545498.
Train: 2018-08-01T23:26:26.867187: step 9026, loss 0.680659.
Train: 2018-08-01T23:26:27.026955: step 9027, loss 0.495097.
Train: 2018-08-01T23:26:27.190484: step 9028, loss 0.579247.
Train: 2018-08-01T23:26:27.361029: step 9029, loss 0.595995.
Train: 2018-08-01T23:26:27.523594: step 9030, loss 0.580315.
Test: 2018-08-01T23:26:28.056170: step 9030, loss 0.548493.
Train: 2018-08-01T23:26:28.220737: step 9031, loss 0.49568.
Train: 2018-08-01T23:26:28.389280: step 9032, loss 0.562481.
Train: 2018-08-01T23:26:28.555834: step 9033, loss 0.645865.
Train: 2018-08-01T23:26:28.726393: step 9034, loss 0.612436.
Train: 2018-08-01T23:26:28.892932: step 9035, loss 0.628946.
Train: 2018-08-01T23:26:29.062479: step 9036, loss 0.529418.
Train: 2018-08-01T23:26:29.229059: step 9037, loss 0.512979.
Train: 2018-08-01T23:26:29.392598: step 9038, loss 0.480015.
Train: 2018-08-01T23:26:29.561173: step 9039, loss 0.595592.
Train: 2018-08-01T23:26:29.722715: step 9040, loss 0.54603.
Test: 2018-08-01T23:26:30.238337: step 9040, loss 0.548705.
Train: 2018-08-01T23:26:30.402922: step 9041, loss 0.46336.
Train: 2018-08-01T23:26:30.566487: step 9042, loss 0.512826.
Train: 2018-08-01T23:26:30.736031: step 9043, loss 0.628977.
Train: 2018-08-01T23:26:30.898602: step 9044, loss 0.445985.
Train: 2018-08-01T23:26:31.064162: step 9045, loss 0.462262.
Train: 2018-08-01T23:26:31.228713: step 9046, loss 0.528886.
Train: 2018-08-01T23:26:31.407211: step 9047, loss 0.545565.
Train: 2018-08-01T23:26:31.572769: step 9048, loss 0.579358.
Train: 2018-08-01T23:26:31.733371: step 9049, loss 0.528382.
Train: 2018-08-01T23:26:31.897931: step 9050, loss 0.562399.
Test: 2018-08-01T23:26:32.428482: step 9050, loss 0.548039.
Train: 2018-08-01T23:26:32.599026: step 9051, loss 0.545251.
Train: 2018-08-01T23:26:32.763610: step 9052, loss 0.648435.
Train: 2018-08-01T23:26:32.927149: step 9053, loss 0.596881.
Train: 2018-08-01T23:26:33.095698: step 9054, loss 0.510617.
Train: 2018-08-01T23:26:33.262252: step 9055, loss 0.59699.
Train: 2018-08-01T23:26:33.422841: step 9056, loss 0.59703.
Train: 2018-08-01T23:26:33.601346: step 9057, loss 0.545087.
Train: 2018-08-01T23:26:33.764939: step 9058, loss 0.614401.
Train: 2018-08-01T23:26:33.928472: step 9059, loss 0.683696.
Train: 2018-08-01T23:26:34.108989: step 9060, loss 0.510528.
Test: 2018-08-01T23:26:34.639596: step 9060, loss 0.547937.
Train: 2018-08-01T23:26:34.805154: step 9061, loss 0.562401.
Train: 2018-08-01T23:26:34.975672: step 9062, loss 0.545145.
Train: 2018-08-01T23:26:35.143229: step 9063, loss 0.527911.
Train: 2018-08-01T23:26:35.308725: step 9064, loss 0.579641.
Train: 2018-08-01T23:26:35.469327: step 9065, loss 0.700284.
Train: 2018-08-01T23:26:35.642832: step 9066, loss 0.579591.
Train: 2018-08-01T23:26:35.807416: step 9067, loss 0.545244.
Train: 2018-08-01T23:26:35.979929: step 9068, loss 0.596635.
Train: 2018-08-01T23:26:36.146485: step 9069, loss 0.61364.
Train: 2018-08-01T23:26:36.315034: step 9070, loss 0.562402.
Test: 2018-08-01T23:26:36.840630: step 9070, loss 0.548179.
Train: 2018-08-01T23:26:37.004218: step 9071, loss 0.579397.
Train: 2018-08-01T23:26:37.173739: step 9072, loss 0.579361.
Train: 2018-08-01T23:26:37.347274: step 9073, loss 0.494796.
Train: 2018-08-01T23:26:37.509866: step 9074, loss 0.494875.
Train: 2018-08-01T23:26:37.676395: step 9075, loss 0.461091.
Train: 2018-08-01T23:26:37.843948: step 9076, loss 0.613171.
Train: 2018-08-01T23:26:38.014491: step 9077, loss 0.545484.
Train: 2018-08-01T23:26:38.182043: step 9078, loss 0.630207.
Train: 2018-08-01T23:26:38.346628: step 9079, loss 0.596307.
Train: 2018-08-01T23:26:38.507207: step 9080, loss 0.5116.
Test: 2018-08-01T23:26:39.025788: step 9080, loss 0.548225.
Train: 2018-08-01T23:26:39.194366: step 9081, loss 0.664075.
Train: 2018-08-01T23:26:39.359894: step 9082, loss 0.494726.
Train: 2018-08-01T23:26:39.530471: step 9083, loss 0.528572.
Train: 2018-08-01T23:26:39.694999: step 9084, loss 0.562415.
Train: 2018-08-01T23:26:39.861579: step 9085, loss 0.545472.
Train: 2018-08-01T23:26:40.035091: step 9086, loss 0.494592.
Train: 2018-08-01T23:26:40.203639: step 9087, loss 0.579393.
Train: 2018-08-01T23:26:40.380177: step 9088, loss 0.511376.
Train: 2018-08-01T23:26:40.551709: step 9089, loss 0.59649.
Train: 2018-08-01T23:26:40.721281: step 9090, loss 0.63067.
Test: 2018-08-01T23:26:41.261809: step 9090, loss 0.548104.
Train: 2018-08-01T23:26:41.435347: step 9091, loss 0.647754.
Train: 2018-08-01T23:26:41.618855: step 9092, loss 0.596505.
Train: 2018-08-01T23:26:41.797404: step 9093, loss 0.579429.
Train: 2018-08-01T23:26:41.969939: step 9094, loss 0.596405.
Train: 2018-08-01T23:26:42.133523: step 9095, loss 0.68118.
Train: 2018-08-01T23:26:42.308044: step 9096, loss 0.596235.
Train: 2018-08-01T23:26:42.474602: step 9097, loss 0.596125.
Train: 2018-08-01T23:26:42.639155: step 9098, loss 0.495321.
Train: 2018-08-01T23:26:42.801695: step 9099, loss 0.528973.
Train: 2018-08-01T23:26:42.972264: step 9100, loss 0.529029.
Test: 2018-08-01T23:26:43.505293: step 9100, loss 0.548482.
Train: 2018-08-01T23:26:44.265012: step 9101, loss 0.562473.
Train: 2018-08-01T23:26:44.427566: step 9102, loss 0.579177.
Train: 2018-08-01T23:26:44.598100: step 9103, loss 0.54579.
Train: 2018-08-01T23:26:44.761674: step 9104, loss 0.662601.
Train: 2018-08-01T23:26:44.931210: step 9105, loss 0.412566.
Train: 2018-08-01T23:26:45.094799: step 9106, loss 0.629185.
Train: 2018-08-01T23:26:45.258335: step 9107, loss 0.512466.
Train: 2018-08-01T23:26:45.428911: step 9108, loss 0.579168.
Train: 2018-08-01T23:26:45.599424: step 9109, loss 0.512388.
Train: 2018-08-01T23:26:45.773957: step 9110, loss 0.62935.
Test: 2018-08-01T23:26:46.314514: step 9110, loss 0.548463.
Train: 2018-08-01T23:26:46.488080: step 9111, loss 0.478843.
Train: 2018-08-01T23:26:46.658619: step 9112, loss 0.545706.
Train: 2018-08-01T23:26:46.836118: step 9113, loss 0.629586.
Train: 2018-08-01T23:26:47.000704: step 9114, loss 0.629628.
Train: 2018-08-01T23:26:47.160252: step 9115, loss 0.612816.
Train: 2018-08-01T23:26:47.324829: step 9116, loss 0.696627.
Train: 2018-08-01T23:26:47.491392: step 9117, loss 0.629357.
Train: 2018-08-01T23:26:47.658918: step 9118, loss 0.54583.
Train: 2018-08-01T23:26:47.820514: step 9119, loss 0.56251.
Train: 2018-08-01T23:26:47.993050: step 9120, loss 0.612238.
Test: 2018-08-01T23:26:48.519618: step 9120, loss 0.548718.
Train: 2018-08-01T23:26:48.689164: step 9121, loss 0.479949.
Train: 2018-08-01T23:26:48.855745: step 9122, loss 0.595564.
Train: 2018-08-01T23:26:49.016290: step 9123, loss 0.480194.
Train: 2018-08-01T23:26:49.179854: step 9124, loss 0.562574.
Train: 2018-08-01T23:26:49.352392: step 9125, loss 0.529605.
Train: 2018-08-01T23:26:49.524931: step 9126, loss 0.711048.
Train: 2018-08-01T23:26:49.680545: step 9127, loss 0.562576.
Train: 2018-08-01T23:26:49.846103: step 9128, loss 0.529676.
Train: 2018-08-01T23:26:50.016647: step 9129, loss 0.611938.
Train: 2018-08-01T23:26:50.185165: step 9130, loss 0.529733.
Test: 2018-08-01T23:26:50.699790: step 9130, loss 0.548843.
Train: 2018-08-01T23:26:50.870334: step 9131, loss 0.579031.
Train: 2018-08-01T23:26:51.044897: step 9132, loss 0.562603.
Train: 2018-08-01T23:26:51.223398: step 9133, loss 0.562605.
Train: 2018-08-01T23:26:51.393934: step 9134, loss 0.595452.
Train: 2018-08-01T23:26:51.569464: step 9135, loss 0.513352.
Train: 2018-08-01T23:26:51.743026: step 9136, loss 0.628318.
Train: 2018-08-01T23:26:51.909556: step 9137, loss 0.546181.
Train: 2018-08-01T23:26:52.073138: step 9138, loss 0.579029.
Train: 2018-08-01T23:26:52.239673: step 9139, loss 0.546177.
Train: 2018-08-01T23:26:52.407256: step 9140, loss 0.611897.
Test: 2018-08-01T23:26:52.946783: step 9140, loss 0.548845.
Train: 2018-08-01T23:26:53.121341: step 9141, loss 0.546174.
Train: 2018-08-01T23:26:53.296846: step 9142, loss 0.5626.
Train: 2018-08-01T23:26:53.466393: step 9143, loss 0.546162.
Train: 2018-08-01T23:26:53.642505: step 9144, loss 0.496802.
Train: 2018-08-01T23:26:53.804049: step 9145, loss 0.529619.
Train: 2018-08-01T23:26:53.966590: step 9146, loss 0.413907.
Train: 2018-08-01T23:26:54.129186: step 9147, loss 0.595706.
Train: 2018-08-01T23:26:54.295735: step 9148, loss 0.495871.
Train: 2018-08-01T23:26:54.454286: step 9149, loss 0.529004.
Train: 2018-08-01T23:26:54.620840: step 9150, loss 0.562443.
Test: 2018-08-01T23:26:55.158404: step 9150, loss 0.54829.
Train: 2018-08-01T23:26:55.326980: step 9151, loss 0.545545.
Train: 2018-08-01T23:26:55.495528: step 9152, loss 0.545462.
Train: 2018-08-01T23:26:55.661061: step 9153, loss 0.579422.
Train: 2018-08-01T23:26:55.829640: step 9154, loss 0.562399.
Train: 2018-08-01T23:26:56.008132: step 9155, loss 0.528141.
Train: 2018-08-01T23:26:56.169730: step 9156, loss 0.596763.
Train: 2018-08-01T23:26:56.347225: step 9157, loss 0.68297.
Train: 2018-08-01T23:26:56.510814: step 9158, loss 0.493477.
Train: 2018-08-01T23:26:56.677347: step 9159, loss 0.510641.
Train: 2018-08-01T23:26:56.855866: step 9160, loss 0.579689.
Test: 2018-08-01T23:26:57.385450: step 9160, loss 0.547909.
Train: 2018-08-01T23:26:57.552031: step 9161, loss 0.527782.
Train: 2018-08-01T23:26:57.725541: step 9162, loss 0.510382.
Train: 2018-08-01T23:26:57.884145: step 9163, loss 0.579799.
Train: 2018-08-01T23:26:58.047713: step 9164, loss 0.562421.
Train: 2018-08-01T23:26:58.208284: step 9165, loss 0.49266.
Train: 2018-08-01T23:26:58.381818: step 9166, loss 0.544954.
Train: 2018-08-01T23:26:58.546378: step 9167, loss 0.544924.
Train: 2018-08-01T23:26:58.718919: step 9168, loss 0.527332.
Train: 2018-08-01T23:26:58.894416: step 9169, loss 0.456832.
Train: 2018-08-01T23:26:59.073936: step 9170, loss 0.509484.
Test: 2018-08-01T23:26:59.599531: step 9170, loss 0.547671.
Train: 2018-08-01T23:26:59.768081: step 9171, loss 0.668968.
Train: 2018-08-01T23:26:59.937654: step 9172, loss 0.526994.
Train: 2018-08-01T23:27:00.102217: step 9173, loss 0.526937.
Train: 2018-08-01T23:27:00.266748: step 9174, loss 0.509026.
Train: 2018-08-01T23:27:00.436320: step 9175, loss 0.562606.
Train: 2018-08-01T23:27:00.595902: step 9176, loss 0.490875.
Train: 2018-08-01T23:27:00.772397: step 9177, loss 0.634618.
Train: 2018-08-01T23:27:00.942940: step 9178, loss 0.562676.
Train: 2018-08-01T23:27:01.102538: step 9179, loss 0.544655.
Train: 2018-08-01T23:27:01.271300: step 9180, loss 0.490486.
Test: 2018-08-01T23:27:01.797921: step 9180, loss 0.547579.
Train: 2018-08-01T23:27:01.965444: step 9181, loss 0.598898.
Train: 2018-08-01T23:27:02.132023: step 9182, loss 0.508427.
Train: 2018-08-01T23:27:02.298580: step 9183, loss 0.580884.
Train: 2018-08-01T23:27:02.458154: step 9184, loss 0.599052.
Train: 2018-08-01T23:27:02.625678: step 9185, loss 0.526458.
Train: 2018-08-01T23:27:02.793231: step 9186, loss 0.689662.
Train: 2018-08-01T23:27:02.957791: step 9187, loss 0.579574.
Train: 2018-08-01T23:27:03.121384: step 9188, loss 0.562617.
Train: 2018-08-01T23:27:03.281955: step 9189, loss 0.578291.
Train: 2018-08-01T23:27:03.451496: step 9190, loss 0.454278.
Test: 2018-08-01T23:27:03.994021: step 9190, loss 0.548333.
Train: 2018-08-01T23:27:04.162571: step 9191, loss 0.596296.
Train: 2018-08-01T23:27:04.332117: step 9192, loss 0.552254.
Train: 2018-08-01T23:27:04.498698: step 9193, loss 0.67526.
Train: 2018-08-01T23:27:04.663258: step 9194, loss 0.523557.
Train: 2018-08-01T23:27:04.831782: step 9195, loss 0.611987.
Train: 2018-08-01T23:27:04.995370: step 9196, loss 0.604719.
Train: 2018-08-01T23:27:05.169878: step 9197, loss 0.651077.
Train: 2018-08-01T23:27:05.338454: step 9198, loss 0.67237.
Train: 2018-08-01T23:27:05.499997: step 9199, loss 0.573268.
Train: 2018-08-01T23:27:05.665552: step 9200, loss 0.578804.
Test: 2018-08-01T23:27:06.205111: step 9200, loss 0.551456.
Train: 2018-08-01T23:27:06.952695: step 9201, loss 0.560493.
Train: 2018-08-01T23:27:07.116228: step 9202, loss 0.503726.
Train: 2018-08-01T23:27:07.278803: step 9203, loss 0.558675.
Train: 2018-08-01T23:27:07.442380: step 9204, loss 0.560492.
Train: 2018-08-01T23:27:07.603941: step 9205, loss 0.516583.
Train: 2018-08-01T23:27:07.767512: step 9206, loss 0.575591.
Train: 2018-08-01T23:27:07.933044: step 9207, loss 0.51458.
Train: 2018-08-01T23:27:08.099599: step 9208, loss 0.482589.
Train: 2018-08-01T23:27:08.261197: step 9209, loss 0.615343.
Train: 2018-08-01T23:27:08.425727: step 9210, loss 0.540897.
Test: 2018-08-01T23:27:08.953316: step 9210, loss 0.54803.
Train: 2018-08-01T23:27:09.123860: step 9211, loss 0.509485.
Train: 2018-08-01T23:27:09.300388: step 9212, loss 0.583019.
Train: 2018-08-01T23:27:09.465945: step 9213, loss 0.584579.
Train: 2018-08-01T23:27:09.642505: step 9214, loss 0.660283.
Train: 2018-08-01T23:27:09.808057: step 9215, loss 0.542221.
Train: 2018-08-01T23:27:09.981567: step 9216, loss 0.488904.
Train: 2018-08-01T23:27:10.142164: step 9217, loss 0.457964.
Train: 2018-08-01T23:27:10.302735: step 9218, loss 0.639219.
Train: 2018-08-01T23:27:10.464308: step 9219, loss 0.604404.
Train: 2018-08-01T23:27:10.643797: step 9220, loss 0.417515.
Test: 2018-08-01T23:27:11.186347: step 9220, loss 0.548224.
Train: 2018-08-01T23:27:11.362875: step 9221, loss 0.599957.
Train: 2018-08-01T23:27:11.533419: step 9222, loss 0.564014.
Train: 2018-08-01T23:27:11.697985: step 9223, loss 0.526533.
Train: 2018-08-01T23:27:11.865533: step 9224, loss 0.582353.
Train: 2018-08-01T23:27:12.049040: step 9225, loss 0.636362.
Train: 2018-08-01T23:27:12.221579: step 9226, loss 0.600186.
Train: 2018-08-01T23:27:12.394118: step 9227, loss 0.636843.
Train: 2018-08-01T23:27:12.562668: step 9228, loss 0.563109.
Train: 2018-08-01T23:27:12.724235: step 9229, loss 0.634802.
Train: 2018-08-01T23:27:12.885833: step 9230, loss 0.580469.
Test: 2018-08-01T23:27:13.415389: step 9230, loss 0.549327.
Train: 2018-08-01T23:27:13.577953: step 9231, loss 0.548965.
Train: 2018-08-01T23:27:13.756476: step 9232, loss 0.570016.
Train: 2018-08-01T23:27:13.925051: step 9233, loss 0.53122.
Train: 2018-08-01T23:27:14.090613: step 9234, loss 0.564763.
Train: 2018-08-01T23:27:14.257163: step 9235, loss 0.597105.
Train: 2018-08-01T23:27:14.425687: step 9236, loss 0.670947.
Train: 2018-08-01T23:27:14.594271: step 9237, loss 0.568263.
Train: 2018-08-01T23:27:14.758796: step 9238, loss 0.629678.
Train: 2018-08-01T23:27:14.925380: step 9239, loss 0.433832.
Train: 2018-08-01T23:27:15.092932: step 9240, loss 0.659899.
Test: 2018-08-01T23:27:15.624489: step 9240, loss 0.550523.
Train: 2018-08-01T23:27:15.795026: step 9241, loss 0.572088.
Train: 2018-08-01T23:27:15.970557: step 9242, loss 0.610455.
Train: 2018-08-01T23:27:16.146088: step 9243, loss 0.459686.
Train: 2018-08-01T23:27:16.325634: step 9244, loss 0.660845.
Train: 2018-08-01T23:27:16.499154: step 9245, loss 0.599672.
Train: 2018-08-01T23:27:16.670722: step 9246, loss 0.558302.
Train: 2018-08-01T23:27:16.839245: step 9247, loss 0.540296.
Train: 2018-08-01T23:27:17.015774: step 9248, loss 0.472987.
Train: 2018-08-01T23:27:17.185321: step 9249, loss 0.53402.
Train: 2018-08-01T23:27:17.354893: step 9250, loss 0.597718.
Test: 2018-08-01T23:27:17.897416: step 9250, loss 0.549797.
Train: 2018-08-01T23:27:18.070986: step 9251, loss 0.598983.
Train: 2018-08-01T23:27:18.245487: step 9252, loss 0.564478.
Train: 2018-08-01T23:27:18.421042: step 9253, loss 0.512998.
Train: 2018-08-01T23:27:18.595551: step 9254, loss 0.495302.
Train: 2018-08-01T23:27:18.785043: step 9255, loss 0.564978.
Train: 2018-08-01T23:27:18.960600: step 9256, loss 0.61529.
Train: 2018-08-01T23:27:19.136106: step 9257, loss 0.59882.
Train: 2018-08-01T23:27:19.313661: step 9258, loss 0.531324.
Train: 2018-08-01T23:27:19.491156: step 9259, loss 0.461598.
Train: 2018-08-01T23:27:19.674666: step 9260, loss 0.582296.
Test: 2018-08-01T23:27:20.191284: step 9260, loss 0.549853.
Train: 2018-08-01T23:27:20.367843: step 9261, loss 0.739056.
Train: 2018-08-01T23:27:20.547333: step 9262, loss 0.634072.
Train: 2018-08-01T23:27:20.722864: step 9263, loss 0.529874.
Train: 2018-08-01T23:27:20.907402: step 9264, loss 0.58175.
Train: 2018-08-01T23:27:21.081904: step 9265, loss 0.529776.
Train: 2018-08-01T23:27:21.261450: step 9266, loss 0.564183.
Train: 2018-08-01T23:27:21.446928: step 9267, loss 0.529921.
Train: 2018-08-01T23:27:21.626448: step 9268, loss 0.633395.
Train: 2018-08-01T23:27:21.799012: step 9269, loss 0.667814.
Train: 2018-08-01T23:27:21.980502: step 9270, loss 0.615824.
Test: 2018-08-01T23:27:22.515073: step 9270, loss 0.549824.
Train: 2018-08-01T23:27:22.697584: step 9271, loss 0.564275.
Train: 2018-08-01T23:27:22.882120: step 9272, loss 0.478668.
Train: 2018-08-01T23:27:23.070607: step 9273, loss 0.529972.
Train: 2018-08-01T23:27:23.246152: step 9274, loss 0.61545.
Train: 2018-08-01T23:27:23.424670: step 9275, loss 0.666287.
Train: 2018-08-01T23:27:23.604162: step 9276, loss 0.547032.
Train: 2018-08-01T23:27:23.782684: step 9277, loss 0.682656.
Train: 2018-08-01T23:27:23.956246: step 9278, loss 0.54708.
Train: 2018-08-01T23:27:24.133745: step 9279, loss 0.479754.
Train: 2018-08-01T23:27:24.316258: step 9280, loss 0.563911.
Test: 2018-08-01T23:27:24.847837: step 9280, loss 0.549815.
Train: 2018-08-01T23:27:25.024365: step 9281, loss 0.530293.
Train: 2018-08-01T23:27:25.200892: step 9282, loss 0.630982.
Train: 2018-08-01T23:27:25.381411: step 9283, loss 0.547044.
Train: 2018-08-01T23:27:25.561944: step 9284, loss 0.563769.
Train: 2018-08-01T23:27:25.739471: step 9285, loss 0.56375.
Train: 2018-08-01T23:27:25.917977: step 9286, loss 0.597195.
Train: 2018-08-01T23:27:26.096523: step 9287, loss 0.496852.
Train: 2018-08-01T23:27:26.277021: step 9288, loss 0.597073.
Train: 2018-08-01T23:27:26.461523: step 9289, loss 0.480071.
Train: 2018-08-01T23:27:26.644066: step 9290, loss 0.513388.
Test: 2018-08-01T23:27:27.183592: step 9290, loss 0.549497.
Train: 2018-08-01T23:27:27.365139: step 9291, loss 0.546771.
Train: 2018-08-01T23:27:27.545651: step 9292, loss 0.546695.
Train: 2018-08-01T23:27:27.728162: step 9293, loss 0.613964.
Train: 2018-08-01T23:27:27.903693: step 9294, loss 0.512843.
Train: 2018-08-01T23:27:28.083224: step 9295, loss 0.597171.
Train: 2018-08-01T23:27:28.257752: step 9296, loss 0.546439.
Train: 2018-08-01T23:27:28.447215: step 9297, loss 0.563325.
Train: 2018-08-01T23:27:28.631722: step 9298, loss 0.614194.
Train: 2018-08-01T23:27:28.809281: step 9299, loss 0.648144.
Train: 2018-08-01T23:27:28.984803: step 9300, loss 0.597175.
Test: 2018-08-01T23:27:29.514363: step 9300, loss 0.54905.
Train: 2018-08-01T23:27:30.287286: step 9301, loss 0.512413.
Train: 2018-08-01T23:27:30.462814: step 9302, loss 0.4616.
Train: 2018-08-01T23:27:30.640345: step 9303, loss 0.580152.
Train: 2018-08-01T23:27:30.815845: step 9304, loss 0.495265.
Train: 2018-08-01T23:27:31.002347: step 9305, loss 0.529125.
Train: 2018-08-01T23:27:31.184858: step 9306, loss 0.580177.
Train: 2018-08-01T23:27:31.365376: step 9307, loss 0.546027.
Train: 2018-08-01T23:27:31.537915: step 9308, loss 0.52886.
Train: 2018-08-01T23:27:31.714473: step 9309, loss 0.494455.
Train: 2018-08-01T23:27:31.894989: step 9310, loss 0.545854.
Test: 2018-08-01T23:27:32.437510: step 9310, loss 0.548598.
Train: 2018-08-01T23:27:32.621050: step 9311, loss 0.666611.
Train: 2018-08-01T23:27:32.798564: step 9312, loss 0.424803.
Train: 2018-08-01T23:27:32.989060: step 9313, loss 0.545701.
Train: 2018-08-01T23:27:33.167589: step 9314, loss 0.563028.
Train: 2018-08-01T23:27:33.347079: step 9315, loss 0.545603.
Train: 2018-08-01T23:27:33.534577: step 9316, loss 0.510602.
Train: 2018-08-01T23:27:33.712163: step 9317, loss 0.545499.
Train: 2018-08-01T23:27:33.893204: step 9318, loss 0.527901.
Train: 2018-08-01T23:27:34.070706: step 9319, loss 0.563048.
Train: 2018-08-01T23:27:34.250248: step 9320, loss 0.492353.
Test: 2018-08-01T23:27:34.791768: step 9320, loss 0.548217.
Train: 2018-08-01T23:27:34.967299: step 9321, loss 0.509872.
Train: 2018-08-01T23:27:35.143827: step 9322, loss 0.669864.
Train: 2018-08-01T23:27:35.328360: step 9323, loss 0.563091.
Train: 2018-08-01T23:27:35.504862: step 9324, loss 0.456015.
Train: 2018-08-01T23:27:35.687375: step 9325, loss 0.509438.
Train: 2018-08-01T23:27:35.872910: step 9326, loss 0.599019.
Train: 2018-08-01T23:27:36.050438: step 9327, loss 0.617079.
Train: 2018-08-01T23:27:36.228926: step 9328, loss 0.545155.
Train: 2018-08-01T23:27:36.406452: step 9329, loss 0.599164.
Train: 2018-08-01T23:27:36.584008: step 9330, loss 0.455086.
Test: 2018-08-01T23:27:37.127526: step 9330, loss 0.54805.
Train: 2018-08-01T23:27:37.308043: step 9331, loss 0.466965.
Train: 2018-08-01T23:27:37.483573: step 9332, loss 0.581253.
Train: 2018-08-01T23:27:37.658106: step 9333, loss 0.5813.
Train: 2018-08-01T23:27:37.842638: step 9334, loss 0.690118.
Train: 2018-08-01T23:27:38.025154: step 9335, loss 0.617526.
Train: 2018-08-01T23:27:38.230576: step 9336, loss 0.653577.
Train: 2018-08-01T23:27:38.417956: step 9337, loss 0.599175.
Train: 2018-08-01T23:27:38.598011: step 9338, loss 0.50915.
Train: 2018-08-01T23:27:38.790498: step 9339, loss 0.634764.
Train: 2018-08-01T23:27:38.974999: step 9340, loss 0.580865.
Test: 2018-08-01T23:27:39.511537: step 9340, loss 0.548044.
Train: 2018-08-01T23:27:39.692088: step 9341, loss 0.509525.
Train: 2018-08-01T23:27:39.877587: step 9342, loss 0.634012.
Train: 2018-08-01T23:27:40.057110: step 9343, loss 0.47435.
Train: 2018-08-01T23:27:40.238595: step 9344, loss 0.49217.
Train: 2018-08-01T23:27:40.428116: step 9345, loss 0.509878.
Train: 2018-08-01T23:27:40.611625: step 9346, loss 0.545204.
Train: 2018-08-01T23:27:40.796105: step 9347, loss 0.492193.
Train: 2018-08-01T23:27:40.975625: step 9348, loss 0.562868.
Train: 2018-08-01T23:27:41.154146: step 9349, loss 0.509766.
Train: 2018-08-01T23:27:41.335662: step 9350, loss 0.669236.
Test: 2018-08-01T23:27:41.871230: step 9350, loss 0.548034.
Train: 2018-08-01T23:27:42.051747: step 9351, loss 0.61603.
Train: 2018-08-01T23:27:42.230296: step 9352, loss 0.56286.
Train: 2018-08-01T23:27:42.418797: step 9353, loss 0.56285.
Train: 2018-08-01T23:27:42.606264: step 9354, loss 0.509866.
Train: 2018-08-01T23:27:42.789799: step 9355, loss 0.61578.
Train: 2018-08-01T23:27:42.968322: step 9356, loss 0.580454.
Train: 2018-08-01T23:27:43.144856: step 9357, loss 0.615624.
Train: 2018-08-01T23:27:43.351274: step 9358, loss 0.527657.
Train: 2018-08-01T23:27:43.537775: step 9359, loss 0.597859.
Train: 2018-08-01T23:27:43.723279: step 9360, loss 0.475225.
Test: 2018-08-01T23:27:44.250868: step 9360, loss 0.548106.
Train: 2018-08-01T23:27:44.430390: step 9361, loss 0.632754.
Train: 2018-08-01T23:27:44.609942: step 9362, loss 0.545277.
Train: 2018-08-01T23:27:44.789465: step 9363, loss 0.615095.
Train: 2018-08-01T23:27:44.966955: step 9364, loss 0.51048.
Train: 2018-08-01T23:27:45.146474: step 9365, loss 0.649717.
Train: 2018-08-01T23:27:45.327989: step 9366, loss 0.510631.
Train: 2018-08-01T23:27:45.506513: step 9367, loss 0.580044.
Train: 2018-08-01T23:27:45.685035: step 9368, loss 0.476137.
Train: 2018-08-01T23:27:45.862585: step 9369, loss 0.545384.
Train: 2018-08-01T23:27:46.048089: step 9370, loss 0.476119.
Test: 2018-08-01T23:27:46.586626: step 9370, loss 0.548174.
Train: 2018-08-01T23:27:46.765148: step 9371, loss 0.528013.
Train: 2018-08-01T23:27:46.959628: step 9372, loss 0.475847.
Train: 2018-08-01T23:27:47.149121: step 9373, loss 0.632374.
Train: 2018-08-01T23:27:47.328641: step 9374, loss 0.545261.
Train: 2018-08-01T23:27:47.504202: step 9375, loss 0.44042.
Train: 2018-08-01T23:27:47.684715: step 9376, loss 0.650331.
Train: 2018-08-01T23:27:47.861243: step 9377, loss 0.422341.
Train: 2018-08-01T23:27:48.039740: step 9378, loss 0.492323.
Train: 2018-08-01T23:27:48.217266: step 9379, loss 0.545089.
Train: 2018-08-01T23:27:48.393793: step 9380, loss 0.562784.
Test: 2018-08-01T23:27:48.926370: step 9380, loss 0.547911.
Train: 2018-08-01T23:27:49.131820: step 9381, loss 0.562806.
Train: 2018-08-01T23:27:49.314331: step 9382, loss 0.491485.
Train: 2018-08-01T23:27:49.496844: step 9383, loss 0.598645.
Train: 2018-08-01T23:27:49.678358: step 9384, loss 0.634618.
Train: 2018-08-01T23:27:49.864859: step 9385, loss 0.526986.
Train: 2018-08-01T23:27:50.042418: step 9386, loss 0.562895.
Train: 2018-08-01T23:27:50.224896: step 9387, loss 0.490954.
Train: 2018-08-01T23:27:50.406413: step 9388, loss 0.653005.
Train: 2018-08-01T23:27:50.585932: step 9389, loss 0.490846.
Train: 2018-08-01T23:27:50.769466: step 9390, loss 0.61703.
Test: 2018-08-01T23:27:51.298027: step 9390, loss 0.547824.
Train: 2018-08-01T23:27:51.476550: step 9391, loss 0.526859.
Train: 2018-08-01T23:27:51.661057: step 9392, loss 0.635073.
Train: 2018-08-01T23:27:51.843594: step 9393, loss 0.526871.
Train: 2018-08-01T23:27:52.023091: step 9394, loss 0.526881.
Train: 2018-08-01T23:27:52.209593: step 9395, loss 0.634936.
Train: 2018-08-01T23:27:52.399119: step 9396, loss 0.598857.
Train: 2018-08-01T23:27:52.582594: step 9397, loss 0.616724.
Train: 2018-08-01T23:27:52.765106: step 9398, loss 0.509107.
Train: 2018-08-01T23:27:52.944657: step 9399, loss 0.598573.
Train: 2018-08-01T23:27:53.137137: step 9400, loss 0.52711.
Test: 2018-08-01T23:27:53.673678: step 9400, loss 0.54786.
Train: 2018-08-01T23:27:54.500914: step 9401, loss 0.651847.
Train: 2018-08-01T23:27:54.680459: step 9402, loss 0.56275.
Train: 2018-08-01T23:27:54.861948: step 9403, loss 0.615883.
Train: 2018-08-01T23:27:55.039504: step 9404, loss 0.65101.
Train: 2018-08-01T23:27:55.229964: step 9405, loss 0.474745.
Train: 2018-08-01T23:27:55.414502: step 9406, loss 0.56266.
Train: 2018-08-01T23:27:55.595999: step 9407, loss 0.492663.
Train: 2018-08-01T23:27:55.783511: step 9408, loss 0.580111.
Train: 2018-08-01T23:27:55.963029: step 9409, loss 0.545186.
Train: 2018-08-01T23:27:56.141526: step 9410, loss 0.632332.
Test: 2018-08-01T23:27:56.677094: step 9410, loss 0.548053.
Train: 2018-08-01T23:27:56.859638: step 9411, loss 0.64957.
Train: 2018-08-01T23:27:57.040150: step 9412, loss 0.510603.
Train: 2018-08-01T23:27:57.221640: step 9413, loss 0.545303.
Train: 2018-08-01T23:27:57.417118: step 9414, loss 0.683466.
Train: 2018-08-01T23:27:57.593676: step 9415, loss 0.614216.
Train: 2018-08-01T23:27:57.777155: step 9416, loss 0.579733.
Train: 2018-08-01T23:27:57.950715: step 9417, loss 0.494268.
Train: 2018-08-01T23:27:58.141212: step 9418, loss 0.630745.
Train: 2018-08-01T23:27:58.321729: step 9419, loss 0.681503.
Train: 2018-08-01T23:27:58.517203: step 9420, loss 0.596424.
Test: 2018-08-01T23:27:59.056726: step 9420, loss 0.548526.
Train: 2018-08-01T23:27:59.253233: step 9421, loss 0.478462.
Train: 2018-08-01T23:27:59.482809: step 9422, loss 0.495497.
Train: 2018-08-01T23:27:59.661331: step 9423, loss 0.529117.
Train: 2018-08-01T23:27:59.841827: step 9424, loss 0.629638.
Train: 2018-08-01T23:28:00.018343: step 9425, loss 0.596095.
Train: 2018-08-01T23:28:00.202876: step 9426, loss 0.579347.
Train: 2018-08-01T23:28:00.384364: step 9427, loss 0.529338.
Train: 2018-08-01T23:28:00.563911: step 9428, loss 0.562672.
Train: 2018-08-01T23:28:00.743433: step 9429, loss 0.579307.
Train: 2018-08-01T23:28:00.923965: step 9430, loss 0.512826.
Test: 2018-08-01T23:28:01.461486: step 9430, loss 0.54876.
Train: 2018-08-01T23:28:01.644025: step 9431, loss 0.546057.
Train: 2018-08-01T23:28:01.826511: step 9432, loss 0.595925.
Train: 2018-08-01T23:28:02.002041: step 9433, loss 0.562671.
Train: 2018-08-01T23:28:02.183555: step 9434, loss 0.496151.
Train: 2018-08-01T23:28:02.367096: step 9435, loss 0.612611.
Train: 2018-08-01T23:28:02.562575: step 9436, loss 0.462705.
Train: 2018-08-01T23:28:02.741066: step 9437, loss 0.579334.
Train: 2018-08-01T23:28:02.925603: step 9438, loss 0.57935.
Train: 2018-08-01T23:28:03.108085: step 9439, loss 0.646346.
Train: 2018-08-01T23:28:03.288647: step 9440, loss 0.646346.
Test: 2018-08-01T23:28:03.821178: step 9440, loss 0.54862.
Train: 2018-08-01T23:28:04.014685: step 9441, loss 0.562622.
Train: 2018-08-01T23:28:04.192185: step 9442, loss 0.629456.
Train: 2018-08-01T23:28:04.369712: step 9443, loss 0.62934.
Train: 2018-08-01T23:28:04.550254: step 9444, loss 0.529396.
Train: 2018-08-01T23:28:04.729782: step 9445, loss 0.479663.
Train: 2018-08-01T23:28:04.908299: step 9446, loss 0.678858.
Train: 2018-08-01T23:28:05.094774: step 9447, loss 0.562681.
Train: 2018-08-01T23:28:05.278310: step 9448, loss 0.513068.
Train: 2018-08-01T23:28:05.461791: step 9449, loss 0.628828.
Train: 2018-08-01T23:28:05.644338: step 9450, loss 0.661764.
Test: 2018-08-01T23:28:06.164957: step 9450, loss 0.54894.
Train: 2018-08-01T23:28:06.345460: step 9451, loss 0.595658.
Train: 2018-08-01T23:28:06.525948: step 9452, loss 0.52992.
Train: 2018-08-01T23:28:06.710479: step 9453, loss 0.579153.
Train: 2018-08-01T23:28:06.893990: step 9454, loss 0.497357.
Train: 2018-08-01T23:28:07.079468: step 9455, loss 0.497388.
Train: 2018-08-01T23:28:07.257990: step 9456, loss 0.51368.
Train: 2018-08-01T23:28:07.450476: step 9457, loss 0.562763.
Train: 2018-08-01T23:28:07.627004: step 9458, loss 0.497044.
Train: 2018-08-01T23:28:07.810694: step 9459, loss 0.529771.
Train: 2018-08-01T23:28:07.993597: step 9460, loss 0.546162.
Test: 2018-08-01T23:28:08.518200: step 9460, loss 0.548779.
Train: 2018-08-01T23:28:08.705681: step 9461, loss 0.479767.
Train: 2018-08-01T23:28:08.892174: step 9462, loss 0.512676.
Train: 2018-08-01T23:28:09.071725: step 9463, loss 0.612797.
Train: 2018-08-01T23:28:09.253242: step 9464, loss 0.579377.
Train: 2018-08-01T23:28:09.442753: step 9465, loss 0.562563.
Train: 2018-08-01T23:28:09.621223: step 9466, loss 0.630125.
Train: 2018-08-01T23:28:09.801742: step 9467, loss 0.511793.
Train: 2018-08-01T23:28:09.995258: step 9468, loss 0.579504.
Train: 2018-08-01T23:28:10.170754: step 9469, loss 0.528563.
Train: 2018-08-01T23:28:10.352271: step 9470, loss 0.613613.
Test: 2018-08-01T23:28:10.873921: step 9470, loss 0.548258.
Train: 2018-08-01T23:28:11.059437: step 9471, loss 0.596613.
Train: 2018-08-01T23:28:11.241916: step 9472, loss 0.443153.
Train: 2018-08-01T23:28:11.419450: step 9473, loss 0.528339.
Train: 2018-08-01T23:28:11.596973: step 9474, loss 0.528249.
Train: 2018-08-01T23:28:11.771475: step 9475, loss 0.700028.
Train: 2018-08-01T23:28:11.955015: step 9476, loss 0.596916.
Train: 2018-08-01T23:28:12.134505: step 9477, loss 0.579718.
Train: 2018-08-01T23:28:12.315022: step 9478, loss 0.528138.
Train: 2018-08-01T23:28:12.487592: step 9479, loss 0.562519.
Train: 2018-08-01T23:28:12.667114: step 9480, loss 0.562518.
Test: 2018-08-01T23:28:13.194672: step 9480, loss 0.548115.
Train: 2018-08-01T23:28:13.373219: step 9481, loss 0.545318.
Train: 2018-08-01T23:28:13.549747: step 9482, loss 0.579725.
Train: 2018-08-01T23:28:13.727249: step 9483, loss 0.579724.
Train: 2018-08-01T23:28:13.903780: step 9484, loss 0.614132.
Train: 2018-08-01T23:28:14.082323: step 9485, loss 0.528136.
Train: 2018-08-01T23:28:14.258827: step 9486, loss 0.631245.
Train: 2018-08-01T23:28:14.435354: step 9487, loss 0.596833.
Train: 2018-08-01T23:28:14.625876: step 9488, loss 0.596772.
Train: 2018-08-01T23:28:14.804368: step 9489, loss 0.545418.
Train: 2018-08-01T23:28:14.987902: step 9490, loss 0.681981.
Test: 2018-08-01T23:28:15.524443: step 9490, loss 0.548274.
Train: 2018-08-01T23:28:15.708954: step 9491, loss 0.494473.
Train: 2018-08-01T23:28:15.900468: step 9492, loss 0.647417.
Train: 2018-08-01T23:28:16.077994: step 9493, loss 0.477883.
Train: 2018-08-01T23:28:16.262470: step 9494, loss 0.562533.
Train: 2018-08-01T23:28:16.443018: step 9495, loss 0.461207.
Train: 2018-08-01T23:28:16.618518: step 9496, loss 0.511835.
Train: 2018-08-01T23:28:16.810006: step 9497, loss 0.680986.
Train: 2018-08-01T23:28:16.987531: step 9498, loss 0.545614.
Train: 2018-08-01T23:28:17.159106: step 9499, loss 0.545617.
Train: 2018-08-01T23:28:17.335601: step 9500, loss 0.613267.
Test: 2018-08-01T23:28:17.867181: step 9500, loss 0.548374.
Train: 2018-08-01T23:28:18.691150: step 9501, loss 0.528725.
Train: 2018-08-01T23:28:18.868675: step 9502, loss 0.478009.
Train: 2018-08-01T23:28:19.045230: step 9503, loss 0.562523.
Train: 2018-08-01T23:28:19.226748: step 9504, loss 0.545566.
Train: 2018-08-01T23:28:19.407236: step 9505, loss 0.528564.
Train: 2018-08-01T23:28:19.575811: step 9506, loss 0.579519.
Train: 2018-08-01T23:28:19.750318: step 9507, loss 0.511412.
Train: 2018-08-01T23:28:19.925849: step 9508, loss 0.5625.
Train: 2018-08-01T23:28:20.107364: step 9509, loss 0.528297.
Train: 2018-08-01T23:28:20.283916: step 9510, loss 0.596777.
Test: 2018-08-01T23:28:20.824447: step 9510, loss 0.548123.
Train: 2018-08-01T23:28:21.021920: step 9511, loss 0.459502.
Train: 2018-08-01T23:28:21.203434: step 9512, loss 0.562496.
Train: 2018-08-01T23:28:21.377004: step 9513, loss 0.579764.
Train: 2018-08-01T23:28:21.554495: step 9514, loss 0.545208.
Train: 2018-08-01T23:28:21.734015: step 9515, loss 0.493176.
Train: 2018-08-01T23:28:21.917525: step 9516, loss 0.64943.
Train: 2018-08-01T23:28:22.096264: step 9517, loss 0.545112.
Train: 2018-08-01T23:28:22.267312: step 9518, loss 0.545094.
Train: 2018-08-01T23:28:22.441871: step 9519, loss 0.684688.
Train: 2018-08-01T23:28:22.612389: step 9520, loss 0.562522.
Test: 2018-08-01T23:28:23.147976: step 9520, loss 0.547925.
Train: 2018-08-01T23:28:23.337450: step 9521, loss 0.562522.
Train: 2018-08-01T23:28:23.510988: step 9522, loss 0.59736.
Train: 2018-08-01T23:28:23.689509: step 9523, loss 0.52771.
Train: 2018-08-01T23:28:23.859082: step 9524, loss 0.440764.
Train: 2018-08-01T23:28:24.027637: step 9525, loss 0.492855.
Train: 2018-08-01T23:28:24.206154: step 9526, loss 0.475263.
Train: 2018-08-01T23:28:24.382682: step 9527, loss 0.545029.
Train: 2018-08-01T23:28:24.552217: step 9528, loss 0.562551.
Train: 2018-08-01T23:28:24.721782: step 9529, loss 0.492148.
Train: 2018-08-01T23:28:24.900273: step 9530, loss 0.615573.
Test: 2018-08-01T23:28:25.423905: step 9530, loss 0.547779.
Train: 2018-08-01T23:28:25.592423: step 9531, loss 0.580298.
Train: 2018-08-01T23:28:25.762992: step 9532, loss 0.598061.
Train: 2018-08-01T23:28:25.938508: step 9533, loss 0.52714.
Train: 2018-08-01T23:28:26.117020: step 9534, loss 0.615895.
Train: 2018-08-01T23:28:26.286567: step 9535, loss 0.598141.
Train: 2018-08-01T23:28:26.462098: step 9536, loss 0.491621.
Train: 2018-08-01T23:28:26.648599: step 9537, loss 0.527109.
Train: 2018-08-01T23:28:26.819144: step 9538, loss 0.615938.
Train: 2018-08-01T23:28:26.989688: step 9539, loss 0.598158.
Train: 2018-08-01T23:28:27.166216: step 9540, loss 0.562618.
Test: 2018-08-01T23:28:27.699822: step 9540, loss 0.547758.
Train: 2018-08-01T23:28:27.870365: step 9541, loss 0.509398.
Train: 2018-08-01T23:28:28.039906: step 9542, loss 0.598083.
Train: 2018-08-01T23:28:28.213441: step 9543, loss 0.633498.
Train: 2018-08-01T23:28:28.385955: step 9544, loss 0.544899.
Train: 2018-08-01T23:28:28.570470: step 9545, loss 0.385912.
Train: 2018-08-01T23:28:28.739011: step 9546, loss 0.474153.
Train: 2018-08-01T23:28:28.924516: step 9547, loss 0.562602.
Train: 2018-08-01T23:28:29.099049: step 9548, loss 0.562618.
Train: 2018-08-01T23:28:29.265604: step 9549, loss 0.598203.
Train: 2018-08-01T23:28:29.432184: step 9550, loss 0.580436.
Test: 2018-08-01T23:28:29.965731: step 9550, loss 0.547729.
Train: 2018-08-01T23:28:30.133283: step 9551, loss 0.598249.
Train: 2018-08-01T23:28:30.310835: step 9552, loss 0.580435.
Train: 2018-08-01T23:28:30.480375: step 9553, loss 0.580417.
Train: 2018-08-01T23:28:30.647938: step 9554, loss 0.527081.
Train: 2018-08-01T23:28:30.814468: step 9555, loss 0.491573.
Train: 2018-08-01T23:28:30.986005: step 9556, loss 0.562619.
Train: 2018-08-01T23:28:31.155582: step 9557, loss 0.704807.
Train: 2018-08-01T23:28:31.328121: step 9558, loss 0.544865.
Train: 2018-08-01T23:28:31.505615: step 9559, loss 0.527168.
Train: 2018-08-01T23:28:31.675187: step 9560, loss 0.615669.
Test: 2018-08-01T23:28:32.211728: step 9560, loss 0.547781.
Train: 2018-08-01T23:28:32.380276: step 9561, loss 0.509582.
Train: 2018-08-01T23:28:32.556806: step 9562, loss 0.544919.
Train: 2018-08-01T23:28:32.728347: step 9563, loss 0.49202.
Train: 2018-08-01T23:28:32.894901: step 9564, loss 0.562563.
Train: 2018-08-01T23:28:33.066443: step 9565, loss 0.562564.
Train: 2018-08-01T23:28:33.232025: step 9566, loss 0.6155.
Train: 2018-08-01T23:28:33.414512: step 9567, loss 0.580189.
Train: 2018-08-01T23:28:33.577078: step 9568, loss 0.474488.
Train: 2018-08-01T23:28:33.755600: step 9569, loss 0.580169.
Train: 2018-08-01T23:28:33.921157: step 9570, loss 0.562551.
Test: 2018-08-01T23:28:34.437784: step 9570, loss 0.547803.
Train: 2018-08-01T23:28:34.605329: step 9571, loss 0.474496.
Train: 2018-08-01T23:28:34.772907: step 9572, loss 0.544927.
Train: 2018-08-01T23:28:34.936443: step 9573, loss 0.509622.
Train: 2018-08-01T23:28:35.101036: step 9574, loss 0.562572.
Train: 2018-08-01T23:28:35.270551: step 9575, loss 0.527183.
Train: 2018-08-01T23:28:35.445111: step 9576, loss 0.704414.
Train: 2018-08-01T23:28:35.612663: step 9577, loss 0.527157.
Train: 2018-08-01T23:28:35.779190: step 9578, loss 0.491738.
Train: 2018-08-01T23:28:35.950761: step 9579, loss 0.527142.
Train: 2018-08-01T23:28:36.115304: step 9580, loss 0.527113.
Test: 2018-08-01T23:28:36.638892: step 9580, loss 0.547731.
Train: 2018-08-01T23:28:36.809437: step 9581, loss 0.615907.
Train: 2018-08-01T23:28:36.984967: step 9582, loss 0.580383.
Train: 2018-08-01T23:28:37.156509: step 9583, loss 0.56261.
Train: 2018-08-01T23:28:37.322091: step 9584, loss 0.598142.
Train: 2018-08-01T23:28:37.492610: step 9585, loss 0.580354.
Train: 2018-08-01T23:28:37.670136: step 9586, loss 0.65126.
Train: 2018-08-01T23:28:37.838710: step 9587, loss 0.474115.
Train: 2018-08-01T23:28:38.012222: step 9588, loss 0.438843.
Train: 2018-08-01T23:28:38.180802: step 9589, loss 0.63333.
Train: 2018-08-01T23:28:38.348323: step 9590, loss 0.509522.
Test: 2018-08-01T23:28:38.875912: step 9590, loss 0.54776.
Train: 2018-08-01T23:28:39.055432: step 9591, loss 0.562571.
Train: 2018-08-01T23:28:39.223982: step 9592, loss 0.615645.
Train: 2018-08-01T23:28:39.390562: step 9593, loss 0.562567.
Train: 2018-08-01T23:28:39.561107: step 9594, loss 0.65089.
Train: 2018-08-01T23:28:39.733620: step 9595, loss 0.580178.
Train: 2018-08-01T23:28:39.905162: step 9596, loss 0.632901.
Train: 2018-08-01T23:28:40.084681: step 9597, loss 0.457291.
Train: 2018-08-01T23:28:40.260213: step 9598, loss 0.59754.
Train: 2018-08-01T23:28:40.423774: step 9599, loss 0.492565.
Train: 2018-08-01T23:28:40.592350: step 9600, loss 0.545026.
Test: 2018-08-01T23:28:41.124901: step 9600, loss 0.547871.
Train: 2018-08-01T23:28:41.932416: step 9601, loss 0.49263.
Train: 2018-08-01T23:28:42.106910: step 9602, loss 0.579977.
Train: 2018-08-01T23:28:42.271318: step 9603, loss 0.5625.
Train: 2018-08-01T23:28:42.438895: step 9604, loss 0.649925.
Train: 2018-08-01T23:28:42.615436: step 9605, loss 0.597423.
Train: 2018-08-01T23:28:42.784945: step 9606, loss 0.562489.
Train: 2018-08-01T23:28:42.949506: step 9607, loss 0.527663.
Train: 2018-08-01T23:28:43.123065: step 9608, loss 0.544999.
Train: 2018-08-01T23:28:43.300592: step 9609, loss 0.439705.
Train: 2018-08-01T23:28:43.471142: step 9610, loss 0.562539.
Test: 2018-08-01T23:28:43.998720: step 9610, loss 0.547667.
Train: 2018-08-01T23:28:44.176258: step 9611, loss 0.562291.
Train: 2018-08-01T23:28:44.344775: step 9612, loss 0.576953.
Train: 2018-08-01T23:28:44.511329: step 9613, loss 0.583004.
Train: 2018-08-01T23:28:44.684866: step 9614, loss 0.545788.
Train: 2018-08-01T23:28:44.851460: step 9615, loss 0.546234.
Train: 2018-08-01T23:28:45.017976: step 9616, loss 0.545076.
Train: 2018-08-01T23:28:45.181570: step 9617, loss 0.540617.
Train: 2018-08-01T23:28:45.351111: step 9618, loss 0.582319.
Train: 2018-08-01T23:28:45.517640: step 9619, loss 0.525897.
Train: 2018-08-01T23:28:45.685218: step 9620, loss 0.48678.
Test: 2018-08-01T23:28:46.220761: step 9620, loss 0.547831.
Train: 2018-08-01T23:28:46.398311: step 9621, loss 0.581906.
Train: 2018-08-01T23:28:46.570847: step 9622, loss 0.627338.
Train: 2018-08-01T23:28:46.741401: step 9623, loss 0.599738.
Train: 2018-08-01T23:28:46.910947: step 9624, loss 0.567796.
Train: 2018-08-01T23:28:47.087475: step 9625, loss 0.561436.
Train: 2018-08-01T23:28:47.256991: step 9626, loss 0.582266.
Train: 2018-08-01T23:28:47.422549: step 9627, loss 0.719764.
Train: 2018-08-01T23:28:47.587139: step 9628, loss 0.545556.
Train: 2018-08-01T23:28:47.758680: step 9629, loss 0.478646.
Train: 2018-08-01T23:28:47.930191: step 9630, loss 0.547052.
Test: 2018-08-01T23:28:48.464793: step 9630, loss 0.54866.
Train: 2018-08-01T23:28:48.643285: step 9631, loss 0.546819.
Train: 2018-08-01T23:28:48.810867: step 9632, loss 0.669861.
Train: 2018-08-01T23:28:48.980414: step 9633, loss 0.527644.
Train: 2018-08-01T23:28:49.144943: step 9634, loss 0.545338.
Train: 2018-08-01T23:28:49.305538: step 9635, loss 0.528127.
Train: 2018-08-01T23:28:49.480079: step 9636, loss 0.480629.
Train: 2018-08-01T23:28:49.648611: step 9637, loss 0.624317.
Train: 2018-08-01T23:28:49.817171: step 9638, loss 0.514728.
Train: 2018-08-01T23:28:49.988687: step 9639, loss 0.4831.
Train: 2018-08-01T23:28:50.155274: step 9640, loss 0.467339.
Test: 2018-08-01T23:28:50.672863: step 9640, loss 0.548717.
Train: 2018-08-01T23:28:50.849388: step 9641, loss 0.677722.
Train: 2018-08-01T23:28:51.016938: step 9642, loss 0.635307.
Train: 2018-08-01T23:28:51.185490: step 9643, loss 0.512349.
Train: 2018-08-01T23:28:51.354069: step 9644, loss 0.512161.
Train: 2018-08-01T23:28:51.517641: step 9645, loss 0.631957.
Train: 2018-08-01T23:28:51.687176: step 9646, loss 0.511579.
Train: 2018-08-01T23:28:51.855697: step 9647, loss 0.528392.
Train: 2018-08-01T23:28:52.022253: step 9648, loss 0.476584.
Train: 2018-08-01T23:28:52.200774: step 9649, loss 0.562868.
Train: 2018-08-01T23:28:52.371319: step 9650, loss 0.649636.
Test: 2018-08-01T23:28:52.904911: step 9650, loss 0.548349.
Train: 2018-08-01T23:28:53.073442: step 9651, loss 0.528156.
Train: 2018-08-01T23:28:53.237030: step 9652, loss 0.510748.
Train: 2018-08-01T23:28:53.403559: step 9653, loss 0.54545.
Train: 2018-08-01T23:28:53.574104: step 9654, loss 0.580185.
Train: 2018-08-01T23:28:53.740683: step 9655, loss 0.473634.
Train: 2018-08-01T23:28:53.913232: step 9656, loss 0.576625.
Train: 2018-08-01T23:28:54.088752: step 9657, loss 0.622075.
Train: 2018-08-01T23:28:54.256280: step 9658, loss 0.525525.
Train: 2018-08-01T23:28:54.426824: step 9659, loss 0.588108.
Train: 2018-08-01T23:28:54.590386: step 9660, loss 0.603165.
Test: 2018-08-01T23:28:55.127950: step 9660, loss 0.548043.
Train: 2018-08-01T23:28:55.293506: step 9661, loss 0.507677.
Train: 2018-08-01T23:28:55.462056: step 9662, loss 0.615965.
Train: 2018-08-01T23:28:55.624622: step 9663, loss 0.545569.
Train: 2018-08-01T23:28:55.792175: step 9664, loss 0.581197.
Train: 2018-08-01T23:28:55.965710: step 9665, loss 0.58009.
Train: 2018-08-01T23:28:56.128276: step 9666, loss 0.6153.
Train: 2018-08-01T23:28:56.294856: step 9667, loss 0.545337.
Train: 2018-08-01T23:28:56.464376: step 9668, loss 0.545129.
Train: 2018-08-01T23:28:56.638932: step 9669, loss 0.649148.
Train: 2018-08-01T23:28:56.805489: step 9670, loss 0.63223.
Test: 2018-08-01T23:28:57.338041: step 9670, loss 0.548405.
Train: 2018-08-01T23:28:57.517587: step 9671, loss 0.581707.
Train: 2018-08-01T23:28:57.681123: step 9672, loss 0.476345.
Train: 2018-08-01T23:28:57.857651: step 9673, loss 0.683612.
Train: 2018-08-01T23:28:58.024238: step 9674, loss 0.528927.
Train: 2018-08-01T23:28:58.199737: step 9675, loss 0.512329.
Train: 2018-08-01T23:28:58.378285: step 9676, loss 0.545592.
Train: 2018-08-01T23:28:58.553791: step 9677, loss 0.530175.
Train: 2018-08-01T23:28:58.717390: step 9678, loss 0.562133.
Train: 2018-08-01T23:28:58.881949: step 9679, loss 0.563316.
Train: 2018-08-01T23:28:59.055490: step 9680, loss 0.562842.
Test: 2018-08-01T23:28:59.578092: step 9680, loss 0.54857.
Train: 2018-08-01T23:28:59.748602: step 9681, loss 0.528902.
Train: 2018-08-01T23:28:59.916153: step 9682, loss 0.49465.
Train: 2018-08-01T23:29:00.087695: step 9683, loss 0.682364.
Train: 2018-08-01T23:29:00.254250: step 9684, loss 0.56274.
Train: 2018-08-01T23:29:00.419808: step 9685, loss 0.562887.
Train: 2018-08-01T23:29:00.589386: step 9686, loss 0.596995.
Train: 2018-08-01T23:29:00.757935: step 9687, loss 0.545675.
Train: 2018-08-01T23:29:00.924459: step 9688, loss 0.528773.
Train: 2018-08-01T23:29:01.090016: step 9689, loss 0.596942.
Train: 2018-08-01T23:29:01.258583: step 9690, loss 0.664928.
Test: 2018-08-01T23:29:01.795131: step 9690, loss 0.54857.
Train: 2018-08-01T23:29:01.974651: step 9691, loss 0.511768.
Train: 2018-08-01T23:29:02.143201: step 9692, loss 0.630726.
Train: 2018-08-01T23:29:02.316757: step 9693, loss 0.478075.
Train: 2018-08-01T23:29:02.484289: step 9694, loss 0.562819.
Train: 2018-08-01T23:29:02.650842: step 9695, loss 0.495053.
Train: 2018-08-01T23:29:02.829392: step 9696, loss 0.562774.
Train: 2018-08-01T23:29:02.998912: step 9697, loss 0.545801.
Train: 2018-08-01T23:29:03.167462: step 9698, loss 0.596719.
Train: 2018-08-01T23:29:03.335038: step 9699, loss 0.477786.
Train: 2018-08-01T23:29:03.507553: step 9700, loss 0.52871.
Test: 2018-08-01T23:29:04.040129: step 9700, loss 0.548448.
Train: 2018-08-01T23:29:04.877139: step 9701, loss 0.477434.
Train: 2018-08-01T23:29:05.043692: step 9702, loss 0.545607.
Train: 2018-08-01T23:29:05.214262: step 9703, loss 0.57984.
Train: 2018-08-01T23:29:05.378823: step 9704, loss 0.597311.
Train: 2018-08-01T23:29:05.547378: step 9705, loss 0.579955.
Train: 2018-08-01T23:29:05.714899: step 9706, loss 0.562662.
Train: 2018-08-01T23:29:05.880463: step 9707, loss 0.579845.
Train: 2018-08-01T23:29:06.046013: step 9708, loss 0.563123.
Train: 2018-08-01T23:29:06.222542: step 9709, loss 0.545245.
Train: 2018-08-01T23:29:06.393112: step 9710, loss 0.614641.
Test: 2018-08-01T23:29:06.919679: step 9710, loss 0.548197.
Train: 2018-08-01T23:29:07.094237: step 9711, loss 0.562853.
Train: 2018-08-01T23:29:07.274760: step 9712, loss 0.579902.
Train: 2018-08-01T23:29:07.454275: step 9713, loss 0.38911.
Train: 2018-08-01T23:29:07.616814: step 9714, loss 0.651265.
Train: 2018-08-01T23:29:07.788387: step 9715, loss 0.510528.
Train: 2018-08-01T23:29:07.959928: step 9716, loss 0.580052.
Train: 2018-08-01T23:29:08.126483: step 9717, loss 0.632342.
Train: 2018-08-01T23:29:08.300019: step 9718, loss 0.562552.
Train: 2018-08-01T23:29:08.465572: step 9719, loss 0.580406.
Train: 2018-08-01T23:29:08.634094: step 9720, loss 0.580098.
Test: 2018-08-01T23:29:09.170660: step 9720, loss 0.548251.
Train: 2018-08-01T23:29:09.342252: step 9721, loss 0.599054.
Train: 2018-08-01T23:29:09.520750: step 9722, loss 0.717502.
Train: 2018-08-01T23:29:09.697279: step 9723, loss 0.511238.
Train: 2018-08-01T23:29:09.861838: step 9724, loss 0.631211.
Train: 2018-08-01T23:29:10.034383: step 9725, loss 0.597649.
Train: 2018-08-01T23:29:10.203899: step 9726, loss 0.513366.
Train: 2018-08-01T23:29:10.382421: step 9727, loss 0.562889.
Train: 2018-08-01T23:29:10.551968: step 9728, loss 0.477934.
Train: 2018-08-01T23:29:10.719521: step 9729, loss 0.51161.
Train: 2018-08-01T23:29:10.892090: step 9730, loss 0.545497.
Test: 2018-08-01T23:29:11.425632: step 9730, loss 0.548388.
Train: 2018-08-01T23:29:11.607173: step 9731, loss 0.54503.
Train: 2018-08-01T23:29:11.784672: step 9732, loss 0.561296.
Train: 2018-08-01T23:29:11.946684: step 9733, loss 0.527946.
Train: 2018-08-01T23:29:12.113215: step 9734, loss 0.497891.
Train: 2018-08-01T23:29:12.281764: step 9735, loss 0.601935.
Train: 2018-08-01T23:29:12.461308: step 9736, loss 0.523727.
Train: 2018-08-01T23:29:12.636814: step 9737, loss 0.538982.
Train: 2018-08-01T23:29:12.803370: step 9738, loss 0.471938.
Train: 2018-08-01T23:29:12.969941: step 9739, loss 0.48039.
Train: 2018-08-01T23:29:13.137477: step 9740, loss 0.507209.
Test: 2018-08-01T23:29:13.679044: step 9740, loss 0.553797.
Train: 2018-08-01T23:29:13.844613: step 9741, loss 0.603729.
Train: 2018-08-01T23:29:14.015129: step 9742, loss 0.556052.
Train: 2018-08-01T23:29:14.178693: step 9743, loss 0.58332.
Train: 2018-08-01T23:29:14.346245: step 9744, loss 0.583622.
Train: 2018-08-01T23:29:14.523801: step 9745, loss 0.546711.
Train: 2018-08-01T23:29:14.691353: step 9746, loss 0.529262.
Train: 2018-08-01T23:29:14.861868: step 9747, loss 0.559165.
Train: 2018-08-01T23:29:15.033445: step 9748, loss 0.577424.
Train: 2018-08-01T23:29:15.200959: step 9749, loss 0.670073.
Train: 2018-08-01T23:29:15.373532: step 9750, loss 0.493441.
Test: 2018-08-01T23:29:15.906100: step 9750, loss 0.548716.
Train: 2018-08-01T23:29:16.076649: step 9751, loss 0.649471.
Train: 2018-08-01T23:29:16.245179: step 9752, loss 0.614483.
Train: 2018-08-01T23:29:16.415739: step 9753, loss 0.818584.
Train: 2018-08-01T23:29:16.584263: step 9754, loss 0.529127.
Train: 2018-08-01T23:29:16.752812: step 9755, loss 0.496293.
Train: 2018-08-01T23:29:16.920389: step 9756, loss 0.479299.
Train: 2018-08-01T23:29:17.083927: step 9757, loss 0.581193.
Train: 2018-08-01T23:29:17.248486: step 9758, loss 0.479697.
Train: 2018-08-01T23:29:17.412074: step 9759, loss 0.580183.
Train: 2018-08-01T23:29:17.588603: step 9760, loss 0.580233.
Test: 2018-08-01T23:29:18.120156: step 9760, loss 0.549241.
Train: 2018-08-01T23:29:18.290700: step 9761, loss 0.579894.
Train: 2018-08-01T23:29:18.456271: step 9762, loss 0.529872.
Train: 2018-08-01T23:29:18.633808: step 9763, loss 0.596585.
Train: 2018-08-01T23:29:18.796349: step 9764, loss 0.647069.
Train: 2018-08-01T23:29:18.961936: step 9765, loss 0.54715.
Train: 2018-08-01T23:29:19.130465: step 9766, loss 0.596512.
Train: 2018-08-01T23:29:19.304990: step 9767, loss 0.62969.
Train: 2018-08-01T23:29:19.477528: step 9768, loss 0.546705.
Train: 2018-08-01T23:29:19.647083: step 9769, loss 0.546743.
Train: 2018-08-01T23:29:19.812656: step 9770, loss 0.563299.
Test: 2018-08-01T23:29:20.347202: step 9770, loss 0.549467.
Train: 2018-08-01T23:29:20.514755: step 9771, loss 0.513753.
Train: 2018-08-01T23:29:20.682332: step 9772, loss 0.513748.
Train: 2018-08-01T23:29:20.843906: step 9773, loss 0.645933.
Train: 2018-08-01T23:29:21.015449: step 9774, loss 0.546733.
Train: 2018-08-01T23:29:21.183004: step 9775, loss 0.596368.
Train: 2018-08-01T23:29:21.350539: step 9776, loss 0.612839.
Train: 2018-08-01T23:29:21.517109: step 9777, loss 0.579812.
Train: 2018-08-01T23:29:21.687628: step 9778, loss 0.546848.
Train: 2018-08-01T23:29:21.854175: step 9779, loss 0.546757.
Train: 2018-08-01T23:29:22.018765: step 9780, loss 0.579763.
Test: 2018-08-01T23:29:22.561284: step 9780, loss 0.549422.
Train: 2018-08-01T23:29:22.730854: step 9781, loss 0.629201.
Train: 2018-08-01T23:29:22.901384: step 9782, loss 0.579702.
Train: 2018-08-01T23:29:23.069932: step 9783, loss 0.579685.
Train: 2018-08-01T23:29:23.234484: step 9784, loss 0.612539.
Train: 2018-08-01T23:29:23.399075: step 9785, loss 0.563233.
Train: 2018-08-01T23:29:23.573612: step 9786, loss 0.59601.
Train: 2018-08-01T23:29:23.750129: step 9787, loss 0.546875.
Train: 2018-08-01T23:29:23.915694: step 9788, loss 0.54689.
Train: 2018-08-01T23:29:24.094186: step 9789, loss 0.514212.
Train: 2018-08-01T23:29:24.260741: step 9790, loss 0.595907.
Test: 2018-08-01T23:29:24.794315: step 9790, loss 0.549504.
Train: 2018-08-01T23:29:24.959871: step 9791, loss 0.628603.
Train: 2018-08-01T23:29:25.135433: step 9792, loss 0.530516.
Train: 2018-08-01T23:29:25.307966: step 9793, loss 0.612201.
Train: 2018-08-01T23:29:25.473499: step 9794, loss 0.481535.
Train: 2018-08-01T23:29:25.637076: step 9795, loss 0.612196.
Train: 2018-08-01T23:29:25.802619: step 9796, loss 0.497752.
Train: 2018-08-01T23:29:25.966182: step 9797, loss 0.464883.
Train: 2018-08-01T23:29:26.135760: step 9798, loss 0.464534.
Train: 2018-08-01T23:29:26.297297: step 9799, loss 0.464028.
Train: 2018-08-01T23:29:26.460859: step 9800, loss 0.562983.
Test: 2018-08-01T23:29:26.989446: step 9800, loss 0.548964.
Train: 2018-08-01T23:29:27.751592: step 9801, loss 0.562939.
Train: 2018-08-01T23:29:27.918181: step 9802, loss 0.495819.
Train: 2018-08-01T23:29:28.092712: step 9803, loss 0.562872.
Train: 2018-08-01T23:29:28.258263: step 9804, loss 0.579798.
Train: 2018-08-01T23:29:28.425822: step 9805, loss 0.613901.
Train: 2018-08-01T23:29:28.595368: step 9806, loss 0.56282.
Train: 2018-08-01T23:29:28.761924: step 9807, loss 0.494318.
Train: 2018-08-01T23:29:28.931440: step 9808, loss 0.459699.
Train: 2018-08-01T23:29:29.096031: step 9809, loss 0.597336.
Train: 2018-08-01T23:29:29.261588: step 9810, loss 0.528143.
Test: 2018-08-01T23:29:29.794163: step 9810, loss 0.548241.
Train: 2018-08-01T23:29:29.957721: step 9811, loss 0.597602.
Train: 2018-08-01T23:29:30.127275: step 9812, loss 0.527922.
Train: 2018-08-01T23:29:30.291827: step 9813, loss 0.562818.
Train: 2018-08-01T23:29:30.456388: step 9814, loss 0.510192.
Train: 2018-08-01T23:29:30.622948: step 9815, loss 0.65084.
Train: 2018-08-01T23:29:30.788478: step 9816, loss 0.580471.
Train: 2018-08-01T23:29:30.966034: step 9817, loss 0.492289.
Train: 2018-08-01T23:29:31.137573: step 9818, loss 0.615869.
Train: 2018-08-01T23:29:31.302133: step 9819, loss 0.562858.
Train: 2018-08-01T23:29:31.469654: step 9820, loss 0.580549.
Test: 2018-08-01T23:29:32.001247: step 9820, loss 0.548037.
Train: 2018-08-01T23:29:32.171817: step 9821, loss 0.580548.
Train: 2018-08-01T23:29:32.341348: step 9822, loss 0.598227.
Train: 2018-08-01T23:29:32.506881: step 9823, loss 0.509812.
Train: 2018-08-01T23:29:32.685405: step 9824, loss 0.562832.
Train: 2018-08-01T23:29:32.848993: step 9825, loss 0.545157.
Train: 2018-08-01T23:29:33.018538: step 9826, loss 0.598162.
Train: 2018-08-01T23:29:33.185092: step 9827, loss 0.456862.
Train: 2018-08-01T23:29:33.350625: step 9828, loss 0.633513.
Train: 2018-08-01T23:29:33.514220: step 9829, loss 0.56281.
Train: 2018-08-01T23:29:33.681741: step 9830, loss 0.598126.
Test: 2018-08-01T23:29:34.219304: step 9830, loss 0.548018.
Train: 2018-08-01T23:29:34.392840: step 9831, loss 0.562793.
Train: 2018-08-01T23:29:34.564410: step 9832, loss 0.633291.
Train: 2018-08-01T23:29:34.732931: step 9833, loss 0.527584.
Train: 2018-08-01T23:29:34.911453: step 9834, loss 0.597891.
Train: 2018-08-01T23:29:35.083541: step 9835, loss 0.580278.
Train: 2018-08-01T23:29:35.253762: step 9836, loss 0.545229.
Train: 2018-08-01T23:29:35.429801: step 9837, loss 0.492828.
Train: 2018-08-01T23:29:35.601336: step 9838, loss 0.580179.
Train: 2018-08-01T23:29:35.767890: step 9839, loss 0.545256.
Train: 2018-08-01T23:29:35.934450: step 9840, loss 0.510372.
Test: 2018-08-01T23:29:36.468019: step 9840, loss 0.54809.
Train: 2018-08-01T23:29:36.632583: step 9841, loss 0.562701.
Train: 2018-08-01T23:29:36.802100: step 9842, loss 0.510341.
Train: 2018-08-01T23:29:36.968686: step 9843, loss 0.510294.
Train: 2018-08-01T23:29:37.138225: step 9844, loss 0.5802.
Train: 2018-08-01T23:29:37.303783: step 9845, loss 0.457623.
Train: 2018-08-01T23:29:37.479320: step 9846, loss 0.597829.
Train: 2018-08-01T23:29:37.643849: step 9847, loss 0.615475.
Train: 2018-08-01T23:29:37.820378: step 9848, loss 0.597911.
Train: 2018-08-01T23:29:37.987957: step 9849, loss 0.633091.
Train: 2018-08-01T23:29:38.163491: step 9850, loss 0.580282.
Test: 2018-08-01T23:29:38.691060: step 9850, loss 0.548007.
Train: 2018-08-01T23:29:38.859599: step 9851, loss 0.545154.
Train: 2018-08-01T23:29:39.025156: step 9852, loss 0.562692.
Train: 2018-08-01T23:29:39.190745: step 9853, loss 0.545176.
Train: 2018-08-01T23:29:39.360260: step 9854, loss 0.492697.
Train: 2018-08-01T23:29:39.525826: step 9855, loss 0.562678.
Train: 2018-08-01T23:29:39.692402: step 9856, loss 0.562676.
Train: 2018-08-01T23:29:39.854938: step 9857, loss 0.615196.
Train: 2018-08-01T23:29:40.018501: step 9858, loss 0.510223.
Train: 2018-08-01T23:29:40.188072: step 9859, loss 0.510191.
Train: 2018-08-01T23:29:40.356597: step 9860, loss 0.59768.
Test: 2018-08-01T23:29:40.884199: step 9860, loss 0.54801.
Train: 2018-08-01T23:29:41.051738: step 9861, loss 0.615189.
Train: 2018-08-01T23:29:41.223281: step 9862, loss 0.685116.
Train: 2018-08-01T23:29:41.391830: step 9863, loss 0.614997.
Train: 2018-08-01T23:29:41.559408: step 9864, loss 0.562642.
Train: 2018-08-01T23:29:41.722945: step 9865, loss 0.527949.
Train: 2018-08-01T23:29:41.890497: step 9866, loss 0.666455.
Train: 2018-08-01T23:29:42.056054: step 9867, loss 0.579864.
Train: 2018-08-01T23:29:42.229591: step 9868, loss 0.528249.
Train: 2018-08-01T23:29:42.399164: step 9869, loss 0.511199.
Train: 2018-08-01T23:29:42.565692: step 9870, loss 0.596838.
Test: 2018-08-01T23:29:43.109239: step 9870, loss 0.548312.
Train: 2018-08-01T23:29:43.275825: step 9871, loss 0.579692.
Train: 2018-08-01T23:29:43.440379: step 9872, loss 0.511479.
Train: 2018-08-01T23:29:43.606910: step 9873, loss 0.460444.
Train: 2018-08-01T23:29:43.776479: step 9874, loss 0.511487.
Train: 2018-08-01T23:29:43.950016: step 9875, loss 0.528473.
Train: 2018-08-01T23:29:44.118566: step 9876, loss 0.562603.
Train: 2018-08-01T23:29:44.293075: step 9877, loss 0.545471.
Train: 2018-08-01T23:29:44.463649: step 9878, loss 0.596917.
Train: 2018-08-01T23:29:44.632174: step 9879, loss 0.493882.
Train: 2018-08-01T23:29:44.797725: step 9880, loss 0.476527.
Test: 2018-08-01T23:29:45.333325: step 9880, loss 0.548137.
Train: 2018-08-01T23:29:45.501874: step 9881, loss 0.528062.
Train: 2018-08-01T23:29:45.670417: step 9882, loss 0.562601.
Train: 2018-08-01T23:29:45.832957: step 9883, loss 0.59736.
Train: 2018-08-01T23:29:45.999544: step 9884, loss 0.61485.
Train: 2018-08-01T23:29:46.173081: step 9885, loss 0.614906.
Train: 2018-08-01T23:29:46.345612: step 9886, loss 0.492884.
Train: 2018-08-01T23:29:46.512168: step 9887, loss 0.64987.
Train: 2018-08-01T23:29:46.677725: step 9888, loss 0.527725.
Train: 2018-08-01T23:29:46.841262: step 9889, loss 0.580057.
Train: 2018-08-01T23:29:47.009835: step 9890, loss 0.614934.
Test: 2018-08-01T23:29:47.539406: step 9890, loss 0.548013.
Train: 2018-08-01T23:29:47.703986: step 9891, loss 0.614874.
Train: 2018-08-01T23:29:47.871539: step 9892, loss 0.597381.
Train: 2018-08-01T23:29:48.038091: step 9893, loss 0.614653.
Train: 2018-08-01T23:29:48.214591: step 9894, loss 0.56258.
Train: 2018-08-01T23:29:48.387154: step 9895, loss 0.648886.
Train: 2018-08-01T23:29:48.560666: step 9896, loss 0.52817.
Train: 2018-08-01T23:29:48.731210: step 9897, loss 0.596871.
Train: 2018-08-01T23:29:48.902777: step 9898, loss 0.596767.
Train: 2018-08-01T23:29:49.069306: step 9899, loss 0.579613.
Train: 2018-08-01T23:29:49.235861: step 9900, loss 0.630534.
Test: 2018-08-01T23:29:49.758463: step 9900, loss 0.548411.
Train: 2018-08-01T23:29:50.536526: step 9901, loss 0.596429.
Train: 2018-08-01T23:29:50.706098: step 9902, loss 0.54574.
Train: 2018-08-01T23:29:50.881603: step 9903, loss 0.596207.
Train: 2018-08-01T23:29:51.050178: step 9904, loss 0.596107.
Train: 2018-08-01T23:29:51.216727: step 9905, loss 0.579325.
Train: 2018-08-01T23:29:51.399219: step 9906, loss 0.496166.
Train: 2018-08-01T23:29:51.566803: step 9907, loss 0.595863.
Train: 2018-08-01T23:29:51.733325: step 9908, loss 0.6786.
Train: 2018-08-01T23:29:51.900879: step 9909, loss 0.480234.
Train: 2018-08-01T23:29:52.066464: step 9910, loss 0.5298.
Test: 2018-08-01T23:29:52.596019: step 9910, loss 0.548965.
Train: 2018-08-01T23:29:52.764569: step 9911, loss 0.595644.
Train: 2018-08-01T23:29:52.936111: step 9912, loss 0.562751.
Train: 2018-08-01T23:29:53.097679: step 9913, loss 0.546344.
Train: 2018-08-01T23:29:53.266255: step 9914, loss 0.513538.
Train: 2018-08-01T23:29:53.431817: step 9915, loss 0.562755.
Train: 2018-08-01T23:29:53.599363: step 9916, loss 0.579177.
Train: 2018-08-01T23:29:53.763936: step 9917, loss 0.464114.
Train: 2018-08-01T23:29:53.932478: step 9918, loss 0.529769.
Train: 2018-08-01T23:29:54.098017: step 9919, loss 0.562696.
Train: 2018-08-01T23:29:54.272538: step 9920, loss 0.529546.
Test: 2018-08-01T23:29:54.804123: step 9920, loss 0.548739.
Train: 2018-08-01T23:29:54.973665: step 9921, loss 0.629107.
Train: 2018-08-01T23:29:55.141242: step 9922, loss 0.512711.
Train: 2018-08-01T23:29:55.319739: step 9923, loss 0.529254.
Train: 2018-08-01T23:29:55.490284: step 9924, loss 0.512408.
Train: 2018-08-01T23:29:55.655872: step 9925, loss 0.629746.
Train: 2018-08-01T23:29:55.827382: step 9926, loss 0.545748.
Train: 2018-08-01T23:29:55.995962: step 9927, loss 0.646883.
Train: 2018-08-01T23:29:56.161515: step 9928, loss 0.545685.
Train: 2018-08-01T23:29:56.327047: step 9929, loss 0.528771.
Train: 2018-08-01T23:29:56.501580: step 9930, loss 0.579468.
Test: 2018-08-01T23:29:57.040140: step 9930, loss 0.548364.
Train: 2018-08-01T23:29:57.214699: step 9931, loss 0.54561.
Train: 2018-08-01T23:29:57.391202: step 9932, loss 0.613417.
Train: 2018-08-01T23:29:57.561745: step 9933, loss 0.580636.
Train: 2018-08-01T23:29:57.730325: step 9934, loss 0.579506.
Train: 2018-08-01T23:29:57.906823: step 9935, loss 0.596468.
Train: 2018-08-01T23:29:58.072411: step 9936, loss 0.579493.
Train: 2018-08-01T23:29:58.240954: step 9937, loss 0.596423.
Train: 2018-08-01T23:29:58.405490: step 9938, loss 0.562542.
Train: 2018-08-01T23:29:58.566061: step 9939, loss 0.511832.
Train: 2018-08-01T23:29:58.735608: step 9940, loss 0.528738.
Test: 2018-08-01T23:29:59.261203: step 9940, loss 0.548379.
Train: 2018-08-01T23:29:59.434738: step 9941, loss 0.47798.
Train: 2018-08-01T23:29:59.612290: step 9942, loss 0.545591.
Train: 2018-08-01T23:29:59.777852: step 9943, loss 0.494615.
Train: 2018-08-01T23:29:59.941409: step 9944, loss 0.579551.
Train: 2018-08-01T23:30:00.107963: step 9945, loss 0.562518.
Train: 2018-08-01T23:30:00.274524: step 9946, loss 0.579621.
Train: 2018-08-01T23:30:00.452019: step 9947, loss 0.528244.
Train: 2018-08-01T23:30:00.617594: step 9948, loss 0.631193.
Train: 2018-08-01T23:30:00.793612: step 9949, loss 0.528146.
Train: 2018-08-01T23:30:00.955684: step 9950, loss 0.648534.
Test: 2018-08-01T23:30:01.480282: step 9950, loss 0.548108.
Train: 2018-08-01T23:30:01.656809: step 9951, loss 0.717312.
Train: 2018-08-01T23:30:01.826361: step 9952, loss 0.49389.
Train: 2018-08-01T23:30:01.992912: step 9953, loss 0.476846.
Train: 2018-08-01T23:30:02.157472: step 9954, loss 0.596779.
Train: 2018-08-01T23:30:02.320036: step 9955, loss 0.648153.
Train: 2018-08-01T23:30:02.482633: step 9956, loss 0.613809.
Train: 2018-08-01T23:30:02.645168: step 9957, loss 0.477204.
Train: 2018-08-01T23:30:02.819732: step 9958, loss 0.647755.
Train: 2018-08-01T23:30:02.982297: step 9959, loss 0.562512.
Train: 2018-08-01T23:30:03.146826: step 9960, loss 0.647445.
Test: 2018-08-01T23:30:03.682395: step 9960, loss 0.548339.
Train: 2018-08-01T23:30:03.856928: step 9961, loss 0.596396.
Train: 2018-08-01T23:30:04.017498: step 9962, loss 0.579416.
Train: 2018-08-01T23:30:04.183081: step 9963, loss 0.51204.
Train: 2018-08-01T23:30:04.348619: step 9964, loss 0.545746.
Train: 2018-08-01T23:30:04.520183: step 9965, loss 0.596118.
Train: 2018-08-01T23:30:04.701695: step 9966, loss 0.579316.
Train: 2018-08-01T23:30:04.873212: step 9967, loss 0.512394.
Train: 2018-08-01T23:30:05.038769: step 9968, loss 0.596005.
Train: 2018-08-01T23:30:05.206321: step 9969, loss 0.612677.
Train: 2018-08-01T23:30:05.376866: step 9970, loss 0.562588.
Test: 2018-08-01T23:30:05.914428: step 9970, loss 0.548654.
Train: 2018-08-01T23:30:06.076037: step 9971, loss 0.595895.
Train: 2018-08-01T23:30:06.248536: step 9972, loss 0.545985.
Train: 2018-08-01T23:30:06.413095: step 9973, loss 0.512804.
Train: 2018-08-01T23:30:06.576711: step 9974, loss 0.645627.
Train: 2018-08-01T23:30:06.740300: step 9975, loss 0.479719.
Train: 2018-08-01T23:30:06.910792: step 9976, loss 0.546033.
Train: 2018-08-01T23:30:07.073357: step 9977, loss 0.579212.
Train: 2018-08-01T23:30:07.242936: step 9978, loss 0.595821.
Train: 2018-08-01T23:30:07.415468: step 9979, loss 0.529399.
Train: 2018-08-01T23:30:07.580031: step 9980, loss 0.629069.
Test: 2018-08-01T23:30:08.116577: step 9980, loss 0.548698.
Train: 2018-08-01T23:30:08.289134: step 9981, loss 0.545997.
Train: 2018-08-01T23:30:08.454693: step 9982, loss 0.529386.
Train: 2018-08-01T23:30:08.618253: step 9983, loss 0.46287.
Train: 2018-08-01T23:30:08.788773: step 9984, loss 0.562584.
Train: 2018-08-01T23:30:08.954362: step 9985, loss 0.445673.
Train: 2018-08-01T23:30:09.118925: step 9986, loss 0.461944.
Train: 2018-08-01T23:30:09.283481: step 9987, loss 0.562522.
Train: 2018-08-01T23:30:09.456985: step 9988, loss 0.545567.
Train: 2018-08-01T23:30:09.618554: step 9989, loss 0.596529.
Train: 2018-08-01T23:30:09.785834: step 9990, loss 0.562488.
Test: 2018-08-01T23:30:10.320375: step 9990, loss 0.548136.
Train: 2018-08-01T23:30:10.485933: step 9991, loss 0.596758.
Train: 2018-08-01T23:30:10.652499: step 9992, loss 0.476595.
Train: 2018-08-01T23:30:10.815084: step 9993, loss 0.562487.
Train: 2018-08-01T23:30:10.992591: step 9994, loss 0.545198.
Train: 2018-08-01T23:30:11.156141: step 9995, loss 0.510461.
Train: 2018-08-01T23:30:11.322726: step 9996, loss 0.492882.
Train: 2018-08-01T23:30:11.486283: step 9997, loss 0.632435.
Train: 2018-08-01T23:30:11.657800: step 9998, loss 0.457392.
Train: 2018-08-01T23:30:11.821362: step 9999, loss 0.527372.
Train: 2018-08-01T23:30:11.983928: step 10000, loss 0.615553.
Test: 2018-08-01T23:30:12.496557: step 10000, loss 0.54777.
Train: 2018-08-01T23:30:13.315019: step 10001, loss 0.704229.
Train: 2018-08-01T23:30:13.479610: step 10002, loss 0.598009.
Train: 2018-08-01T23:30:13.642177: step 10003, loss 0.544894.
Train: 2018-08-01T23:30:13.812688: step 10004, loss 0.544896.
Train: 2018-08-01T23:30:13.978245: step 10005, loss 0.597969.
Train: 2018-08-01T23:30:14.146796: step 10006, loss 0.491871.
Train: 2018-08-01T23:30:14.312352: step 10007, loss 0.47417.
Train: 2018-08-01T23:30:14.484891: step 10008, loss 0.544885.
Train: 2018-08-01T23:30:14.647456: step 10009, loss 0.54487.
Train: 2018-08-01T23:30:14.810022: step 10010, loss 0.651412.
Test: 2018-08-01T23:30:15.335647: step 10010, loss 0.547744.
Train: 2018-08-01T23:30:15.499183: step 10011, loss 0.633644.
Train: 2018-08-01T23:30:15.663766: step 10012, loss 0.527133.
Train: 2018-08-01T23:30:15.828326: step 10013, loss 0.527152.
Train: 2018-08-01T23:30:16.000845: step 10014, loss 0.491722.
Train: 2018-08-01T23:30:16.170407: step 10015, loss 0.544869.
Train: 2018-08-01T23:30:16.339949: step 10016, loss 0.491628.
Train: 2018-08-01T23:30:16.511500: step 10017, loss 0.651484.
Train: 2018-08-01T23:30:16.676033: step 10018, loss 0.491523.
Train: 2018-08-01T23:30:16.852561: step 10019, loss 0.722743.
Train: 2018-08-01T23:30:17.018149: step 10020, loss 0.509324.
Test: 2018-08-01T23:30:17.545713: step 10020, loss 0.547743.
Train: 2018-08-01T23:30:17.711297: step 10021, loss 0.562606.
Train: 2018-08-01T23:30:17.872860: step 10022, loss 0.580333.
Train: 2018-08-01T23:30:18.044376: step 10023, loss 0.474016.
Train: 2018-08-01T23:30:18.210955: step 10024, loss 0.438564.
Train: 2018-08-01T23:30:18.377519: step 10025, loss 0.456099.
Train: 2018-08-01T23:30:18.544066: step 10026, loss 0.544825.
Train: 2018-08-01T23:30:18.704647: step 10027, loss 0.562657.
Train: 2018-08-01T23:30:18.874172: step 10028, loss 0.580582.
Train: 2018-08-01T23:30:19.047694: step 10029, loss 0.724109.
Train: 2018-08-01T23:30:19.214248: step 10030, loss 0.49101.
Test: 2018-08-01T23:30:19.750815: step 10030, loss 0.547683.
Train: 2018-08-01T23:30:19.946421: step 10031, loss 0.544769.
Train: 2018-08-01T23:30:20.115967: step 10032, loss 0.52684.
Train: 2018-08-01T23:30:20.289470: step 10033, loss 0.616511.
Train: 2018-08-01T23:30:20.454054: step 10034, loss 0.544765.
Train: 2018-08-01T23:30:20.619619: step 10035, loss 0.580617.
Train: 2018-08-01T23:30:20.793154: step 10036, loss 0.526856.
Train: 2018-08-01T23:30:20.953693: step 10037, loss 0.455218.
Train: 2018-08-01T23:30:21.120248: step 10038, loss 0.580626.
Train: 2018-08-01T23:30:21.283812: step 10039, loss 0.508869.
Train: 2018-08-01T23:30:21.465325: step 10040, loss 0.454918.
Test: 2018-08-01T23:30:22.003886: step 10040, loss 0.54766.
Train: 2018-08-01T23:30:22.165455: step 10041, loss 0.688812.
Train: 2018-08-01T23:30:22.338019: step 10042, loss 0.616789.
Train: 2018-08-01T23:30:22.514546: step 10043, loss 0.490725.
Train: 2018-08-01T23:30:22.680111: step 10044, loss 0.49071.
Train: 2018-08-01T23:30:22.845667: step 10045, loss 0.544724.
Train: 2018-08-01T23:30:23.009225: step 10046, loss 0.580808.
Train: 2018-08-01T23:30:23.176751: step 10047, loss 0.544714.
Train: 2018-08-01T23:30:23.357267: step 10048, loss 0.490519.
Train: 2018-08-01T23:30:23.518867: step 10049, loss 0.653231.
Train: 2018-08-01T23:30:23.688383: step 10050, loss 0.562788.
Test: 2018-08-01T23:30:24.211984: step 10050, loss 0.547646.
Train: 2018-08-01T23:30:24.382561: step 10051, loss 0.635088.
Train: 2018-08-01T23:30:24.542131: step 10052, loss 0.689102.
Train: 2018-08-01T23:30:24.708686: step 10053, loss 0.634678.
Train: 2018-08-01T23:30:24.882217: step 10054, loss 0.508937.
Train: 2018-08-01T23:30:25.044792: step 10055, loss 0.562647.
Train: 2018-08-01T23:30:25.207322: step 10056, loss 0.633836.
Train: 2018-08-01T23:30:25.376902: step 10057, loss 0.580323.
Train: 2018-08-01T23:30:25.538468: step 10058, loss 0.63323.
Train: 2018-08-01T23:30:25.709015: step 10059, loss 0.615289.
Train: 2018-08-01T23:30:25.881545: step 10060, loss 0.527505.
Test: 2018-08-01T23:30:26.416091: step 10060, loss 0.547892.
Train: 2018-08-01T23:30:26.585664: step 10061, loss 0.545058.
Train: 2018-08-01T23:30:26.746235: step 10062, loss 0.440909.
Train: 2018-08-01T23:30:26.919747: step 10063, loss 0.493098.
Train: 2018-08-01T23:30:27.089316: step 10064, loss 0.649166.
Train: 2018-08-01T23:30:27.257841: step 10065, loss 0.510528.
Train: 2018-08-01T23:30:27.428386: step 10066, loss 0.493257.
Train: 2018-08-01T23:30:27.590982: step 10067, loss 0.579774.
Train: 2018-08-01T23:30:27.752543: step 10068, loss 0.597088.
Train: 2018-08-01T23:30:27.917079: step 10069, loss 0.54516.
Train: 2018-08-01T23:30:28.081667: step 10070, loss 0.597064.
Test: 2018-08-01T23:30:28.598257: step 10070, loss 0.547984.
Train: 2018-08-01T23:30:28.770797: step 10071, loss 0.718057.
Train: 2018-08-01T23:30:28.931398: step 10072, loss 0.614165.
Train: 2018-08-01T23:30:29.091971: step 10073, loss 0.493753.
Train: 2018-08-01T23:30:29.253506: step 10074, loss 0.528177.
Train: 2018-08-01T23:30:29.416095: step 10075, loss 0.54534.
Train: 2018-08-01T23:30:29.587614: step 10076, loss 0.562455.
Train: 2018-08-01T23:30:29.752207: step 10077, loss 0.52829.
Train: 2018-08-01T23:30:29.924744: step 10078, loss 0.545377.
Train: 2018-08-01T23:30:30.097250: step 10079, loss 0.545375.
Train: 2018-08-01T23:30:30.268823: step 10080, loss 0.511198.
Test: 2018-08-01T23:30:30.810353: step 10080, loss 0.54813.
Train: 2018-08-01T23:30:30.977896: step 10081, loss 0.579557.
Train: 2018-08-01T23:30:31.139464: step 10082, loss 0.545335.
Train: 2018-08-01T23:30:31.313997: step 10083, loss 0.545318.
Train: 2018-08-01T23:30:31.478559: step 10084, loss 0.631061.
Train: 2018-08-01T23:30:31.640128: step 10085, loss 0.510996.
Train: 2018-08-01T23:30:31.804711: step 10086, loss 0.562451.
Train: 2018-08-01T23:30:31.978257: step 10087, loss 0.613979.
Train: 2018-08-01T23:30:32.144808: step 10088, loss 0.459414.
Train: 2018-08-01T23:30:32.314324: step 10089, loss 0.56245.
Train: 2018-08-01T23:30:32.476889: step 10090, loss 0.596888.
Test: 2018-08-01T23:30:33.016447: step 10090, loss 0.548024.
Train: 2018-08-01T23:30:33.192008: step 10091, loss 0.683053.
Train: 2018-08-01T23:30:33.367508: step 10092, loss 0.596862.
Train: 2018-08-01T23:30:33.529076: step 10093, loss 0.682682.
Train: 2018-08-01T23:30:33.695657: step 10094, loss 0.579567.
Train: 2018-08-01T23:30:33.858196: step 10095, loss 0.494211.
Train: 2018-08-01T23:30:34.028775: step 10096, loss 0.511373.
Train: 2018-08-01T23:30:34.196327: step 10097, loss 0.477397.
Train: 2018-08-01T23:30:34.358859: step 10098, loss 0.596496.
Train: 2018-08-01T23:30:34.533392: step 10099, loss 0.528414.
Train: 2018-08-01T23:30:34.708956: step 10100, loss 0.511364.
Test: 2018-08-01T23:30:35.240502: step 10100, loss 0.548172.
Train: 2018-08-01T23:30:36.073329: step 10101, loss 0.460139.
Train: 2018-08-01T23:30:36.234896: step 10102, loss 0.528249.
Train: 2018-08-01T23:30:36.402449: step 10103, loss 0.613902.
Train: 2018-08-01T23:30:36.572026: step 10104, loss 0.493706.
Train: 2018-08-01T23:30:36.739548: step 10105, loss 0.717547.
Train: 2018-08-01T23:30:36.912106: step 10106, loss 0.562448.
Train: 2018-08-01T23:30:37.075649: step 10107, loss 0.562448.
Train: 2018-08-01T23:30:37.242241: step 10108, loss 0.734831.
Train: 2018-08-01T23:30:37.420727: step 10109, loss 0.57964.
Train: 2018-08-01T23:30:37.583292: step 10110, loss 0.528146.
Test: 2018-08-01T23:30:38.111880: step 10110, loss 0.54811.
Train: 2018-08-01T23:30:38.283446: step 10111, loss 0.63092.
Train: 2018-08-01T23:30:38.446983: step 10112, loss 0.545374.
Train: 2018-08-01T23:30:38.609574: step 10113, loss 0.579486.
Train: 2018-08-01T23:30:38.780111: step 10114, loss 0.528455.
Train: 2018-08-01T23:30:38.971593: step 10115, loss 0.545481.
Train: 2018-08-01T23:30:39.139135: step 10116, loss 0.47766.
Train: 2018-08-01T23:30:39.336623: step 10117, loss 0.562457.
Train: 2018-08-01T23:30:39.557017: step 10118, loss 0.511525.
Train: 2018-08-01T23:30:39.799368: step 10119, loss 0.545454.
Train: 2018-08-01T23:30:40.005817: step 10120, loss 0.630544.
Test: 2018-08-01T23:30:40.542382: step 10120, loss 0.54819.
Train: 2018-08-01T23:30:40.712926: step 10121, loss 0.596505.
Train: 2018-08-01T23:30:40.878517: step 10122, loss 0.596497.
Train: 2018-08-01T23:30:41.042087: step 10123, loss 0.494405.
Train: 2018-08-01T23:30:41.206631: step 10124, loss 0.630523.
Train: 2018-08-01T23:30:41.373161: step 10125, loss 0.579458.
Train: 2018-08-01T23:30:41.544703: step 10126, loss 0.562452.
Train: 2018-08-01T23:30:41.722228: step 10127, loss 0.511508.
Train: 2018-08-01T23:30:41.902745: step 10128, loss 0.698332.
Train: 2018-08-01T23:30:42.066366: step 10129, loss 0.545504.
Train: 2018-08-01T23:30:42.234857: step 10130, loss 0.579389.
Test: 2018-08-01T23:30:42.774447: step 10130, loss 0.548311.
Train: 2018-08-01T23:30:42.942978: step 10131, loss 0.697691.
Train: 2018-08-01T23:30:43.108547: step 10132, loss 0.511942.
Train: 2018-08-01T23:30:43.289040: step 10133, loss 0.61291.
Train: 2018-08-01T23:30:43.459584: step 10134, loss 0.5625.
Train: 2018-08-01T23:30:43.648080: step 10135, loss 0.545792.
Train: 2018-08-01T23:30:43.815632: step 10136, loss 0.629288.
Train: 2018-08-01T23:30:43.986176: step 10137, loss 0.579187.
Train: 2018-08-01T23:30:44.157717: step 10138, loss 0.562556.
Train: 2018-08-01T23:30:44.329259: step 10139, loss 0.529432.
Train: 2018-08-01T23:30:44.498806: step 10140, loss 0.579131.
Test: 2018-08-01T23:30:45.044347: step 10140, loss 0.548752.
Train: 2018-08-01T23:30:45.206943: step 10141, loss 0.562592.
Train: 2018-08-01T23:30:45.384438: step 10142, loss 0.595621.
Train: 2018-08-01T23:30:45.557974: step 10143, loss 0.562611.
Train: 2018-08-01T23:30:45.721563: step 10144, loss 0.661443.
Train: 2018-08-01T23:30:45.892081: step 10145, loss 0.611932.
Train: 2018-08-01T23:30:46.076587: step 10146, loss 0.529906.
Train: 2018-08-01T23:30:46.233194: step 10147, loss 0.546338.
Train: 2018-08-01T23:30:46.399725: step 10148, loss 0.611694.
Train: 2018-08-01T23:30:46.565281: step 10149, loss 0.627927.
Train: 2018-08-01T23:30:46.737820: step 10150, loss 0.513969.
Test: 2018-08-01T23:30:47.277378: step 10150, loss 0.549164.
Train: 2018-08-01T23:30:47.441951: step 10151, loss 0.611482.
Train: 2018-08-01T23:30:47.607524: step 10152, loss 0.578995.
Train: 2018-08-01T23:30:47.771084: step 10153, loss 0.627539.
Train: 2018-08-01T23:30:47.941633: step 10154, loss 0.530545.
Train: 2018-08-01T23:30:48.119128: step 10155, loss 0.578974.
Train: 2018-08-01T23:30:48.293661: step 10156, loss 0.611172.
Train: 2018-08-01T23:30:48.468195: step 10157, loss 0.643253.
Train: 2018-08-01T23:30:48.633776: step 10158, loss 0.514851.
Train: 2018-08-01T23:30:48.797342: step 10159, loss 0.530943.
Train: 2018-08-01T23:30:48.974848: step 10160, loss 0.642944.
Test: 2018-08-01T23:30:49.498441: step 10160, loss 0.549607.
Train: 2018-08-01T23:30:49.663000: step 10161, loss 0.54701.
Train: 2018-08-01T23:30:49.835564: step 10162, loss 0.483204.
Train: 2018-08-01T23:30:50.005100: step 10163, loss 0.594925.
Train: 2018-08-01T23:30:50.178637: step 10164, loss 0.674837.
Train: 2018-08-01T23:30:50.345208: step 10165, loss 0.547031.
Train: 2018-08-01T23:30:50.510765: step 10166, loss 0.594902.
Train: 2018-08-01T23:30:50.679309: step 10167, loss 0.483325.
Train: 2018-08-01T23:30:50.848831: step 10168, loss 0.578951.
Train: 2018-08-01T23:30:51.016382: step 10169, loss 0.46719.
Train: 2018-08-01T23:30:51.180973: step 10170, loss 0.530927.
Test: 2018-08-01T23:30:51.721509: step 10170, loss 0.549451.
Train: 2018-08-01T23:30:51.894038: step 10171, loss 0.482593.
Train: 2018-08-01T23:30:52.068570: step 10172, loss 0.562839.
Train: 2018-08-01T23:30:52.235127: step 10173, loss 0.611398.
Train: 2018-08-01T23:30:52.406690: step 10174, loss 0.611518.
Train: 2018-08-01T23:30:52.579205: step 10175, loss 0.562722.
Train: 2018-08-01T23:30:52.747755: step 10176, loss 0.546368.
Train: 2018-08-01T23:30:52.933258: step 10177, loss 0.562672.
Train: 2018-08-01T23:30:53.112796: step 10178, loss 0.497014.
Train: 2018-08-01T23:30:53.324214: step 10179, loss 0.529689.
Train: 2018-08-01T23:30:53.518693: step 10180, loss 0.54606.
Test: 2018-08-01T23:30:54.061242: step 10180, loss 0.548666.
Train: 2018-08-01T23:30:54.229810: step 10181, loss 0.595734.
Train: 2018-08-01T23:30:54.395349: step 10182, loss 0.479348.
Train: 2018-08-01T23:30:54.557940: step 10183, loss 0.545804.
Train: 2018-08-01T23:30:54.722505: step 10184, loss 0.579262.
Train: 2018-08-01T23:30:54.886038: step 10185, loss 0.478313.
Train: 2018-08-01T23:30:55.048628: step 10186, loss 0.579363.
Train: 2018-08-01T23:30:55.216165: step 10187, loss 0.494555.
Train: 2018-08-01T23:30:55.382741: step 10188, loss 0.528337.
Train: 2018-08-01T23:30:55.547271: step 10189, loss 0.493919.
Train: 2018-08-01T23:30:55.705847: step 10190, loss 0.510781.
Test: 2018-08-01T23:30:56.250391: step 10190, loss 0.547944.
Train: 2018-08-01T23:30:56.424924: step 10191, loss 0.475886.
Train: 2018-08-01T23:30:56.596466: step 10192, loss 0.579874.
Train: 2018-08-01T23:30:56.768007: step 10193, loss 0.68503.
Train: 2018-08-01T23:30:56.931603: step 10194, loss 0.509824.
Train: 2018-08-01T23:30:57.092141: step 10195, loss 0.527287.
Train: 2018-08-01T23:30:57.259693: step 10196, loss 0.668543.
Train: 2018-08-01T23:30:57.423256: step 10197, loss 0.562539.
Train: 2018-08-01T23:30:57.615740: step 10198, loss 0.509416.
Train: 2018-08-01T23:30:57.776372: step 10199, loss 0.527081.
Train: 2018-08-01T23:30:57.957319: step 10200, loss 0.544802.
Test: 2018-08-01T23:30:58.489311: step 10200, loss 0.547681.
Train: 2018-08-01T23:30:59.284725: step 10201, loss 0.598196.
Train: 2018-08-01T23:30:59.451273: step 10202, loss 0.509135.
Train: 2018-08-01T23:30:59.616805: step 10203, loss 0.544763.
Train: 2018-08-01T23:30:59.787350: step 10204, loss 0.562625.
Train: 2018-08-01T23:30:59.961915: step 10205, loss 0.598428.
Train: 2018-08-01T23:31:00.133425: step 10206, loss 0.508931.
Train: 2018-08-01T23:31:00.300013: step 10207, loss 0.419282.
Train: 2018-08-01T23:31:00.477529: step 10208, loss 0.580652.
Train: 2018-08-01T23:31:00.641099: step 10209, loss 0.544696.
Train: 2018-08-01T23:31:00.806625: step 10210, loss 0.598815.
Test: 2018-08-01T23:31:01.338205: step 10210, loss 0.547615.
Train: 2018-08-01T23:31:01.503760: step 10211, loss 0.544678.
Train: 2018-08-01T23:31:01.679292: step 10212, loss 0.653165.
Train: 2018-08-01T23:31:01.842854: step 10213, loss 0.598895.
Train: 2018-08-01T23:31:02.010439: step 10214, loss 0.580787.
Train: 2018-08-01T23:31:02.175995: step 10215, loss 0.526661.
Train: 2018-08-01T23:31:02.347506: step 10216, loss 0.472651.
Train: 2018-08-01T23:31:02.523067: step 10217, loss 0.598738.
Train: 2018-08-01T23:31:02.687628: step 10218, loss 0.652739.
Train: 2018-08-01T23:31:02.860135: step 10219, loss 0.490784.
Train: 2018-08-01T23:31:03.023699: step 10220, loss 0.526753.
Test: 2018-08-01T23:31:03.548295: step 10220, loss 0.547635.
Train: 2018-08-01T23:31:03.711896: step 10221, loss 0.61653.
Train: 2018-08-01T23:31:03.877448: step 10222, loss 0.634381.
Train: 2018-08-01T23:31:04.039016: step 10223, loss 0.50896.
Train: 2018-08-01T23:31:04.205570: step 10224, loss 0.580483.
Train: 2018-08-01T23:31:04.378077: step 10225, loss 0.509097.
Train: 2018-08-01T23:31:04.541671: step 10226, loss 0.562594.
Train: 2018-08-01T23:31:04.707198: step 10227, loss 0.544783.
Train: 2018-08-01T23:31:04.877742: step 10228, loss 0.491419.
Train: 2018-08-01T23:31:05.037315: step 10229, loss 0.544787.
Train: 2018-08-01T23:31:05.198884: step 10230, loss 0.580386.
Test: 2018-08-01T23:31:05.721485: step 10230, loss 0.547678.
Train: 2018-08-01T23:31:05.885050: step 10231, loss 0.615985.
Train: 2018-08-01T23:31:06.047614: step 10232, loss 0.633711.
Train: 2018-08-01T23:31:06.211178: step 10233, loss 0.544811.
Train: 2018-08-01T23:31:06.376744: step 10234, loss 0.505845.
Train: 2018-08-01T23:31:06.542292: step 10235, loss 0.420897.
Train: 2018-08-01T23:31:06.706852: step 10236, loss 0.544822.
Train: 2018-08-01T23:31:06.872410: step 10237, loss 0.491544.
Train: 2018-08-01T23:31:07.034011: step 10238, loss 0.544857.
Train: 2018-08-01T23:31:07.200533: step 10239, loss 0.633888.
Train: 2018-08-01T23:31:07.365123: step 10240, loss 0.544746.
Test: 2018-08-01T23:31:07.891685: step 10240, loss 0.54766.
Train: 2018-08-01T23:31:08.054276: step 10241, loss 0.491149.
Train: 2018-08-01T23:31:08.221802: step 10242, loss 0.580691.
Train: 2018-08-01T23:31:08.387384: step 10243, loss 0.544858.
Train: 2018-08-01T23:31:08.551944: step 10244, loss 0.508897.
Train: 2018-08-01T23:31:08.716479: step 10245, loss 0.670251.
Train: 2018-08-01T23:31:08.887023: step 10246, loss 0.688023.
Train: 2018-08-01T23:31:09.053579: step 10247, loss 0.526955.
Train: 2018-08-01T23:31:09.218138: step 10248, loss 0.580857.
Train: 2018-08-01T23:31:09.383696: step 10249, loss 0.509112.
Train: 2018-08-01T23:31:09.548258: step 10250, loss 0.509156.
Test: 2018-08-01T23:31:10.072855: step 10250, loss 0.547681.
Train: 2018-08-01T23:31:10.238412: step 10251, loss 0.615971.
Train: 2018-08-01T23:31:10.402005: step 10252, loss 0.420389.
Train: 2018-08-01T23:31:10.567557: step 10253, loss 0.598157.
Train: 2018-08-01T23:31:10.731094: step 10254, loss 0.562589.
Train: 2018-08-01T23:31:10.888699: step 10255, loss 0.562578.
Train: 2018-08-01T23:31:11.056225: step 10256, loss 0.651569.
Train: 2018-08-01T23:31:11.216797: step 10257, loss 0.598115.
Train: 2018-08-01T23:31:11.381357: step 10258, loss 0.598028.
Train: 2018-08-01T23:31:11.557885: step 10259, loss 0.615623.
Train: 2018-08-01T23:31:11.726433: step 10260, loss 0.597777.
Test: 2018-08-01T23:31:12.254049: step 10260, loss 0.547782.
Train: 2018-08-01T23:31:12.418609: step 10261, loss 0.474615.
Train: 2018-08-01T23:31:12.587132: step 10262, loss 0.562483.
Train: 2018-08-01T23:31:12.754685: step 10263, loss 0.614953.
Train: 2018-08-01T23:31:12.919246: step 10264, loss 0.562427.
Train: 2018-08-01T23:31:13.081810: step 10265, loss 0.509701.
Train: 2018-08-01T23:31:13.244403: step 10266, loss 0.527961.
Train: 2018-08-01T23:31:13.414951: step 10267, loss 0.476004.
Train: 2018-08-01T23:31:13.588486: step 10268, loss 0.665874.
Train: 2018-08-01T23:31:13.753041: step 10269, loss 0.562372.
Train: 2018-08-01T23:31:13.917576: step 10270, loss 0.595124.
Test: 2018-08-01T23:31:14.443177: step 10270, loss 0.548336.
Train: 2018-08-01T23:31:14.609751: step 10271, loss 0.525679.
Train: 2018-08-01T23:31:14.774287: step 10272, loss 0.61565.
Train: 2018-08-01T23:31:14.935854: step 10273, loss 0.613603.
Train: 2018-08-01T23:31:15.100413: step 10274, loss 0.612576.
Train: 2018-08-01T23:31:15.262995: step 10275, loss 0.60695.
Train: 2018-08-01T23:31:15.429559: step 10276, loss 0.532429.
Train: 2018-08-01T23:31:15.600109: step 10277, loss 0.487313.
Train: 2018-08-01T23:31:15.768628: step 10278, loss 0.601813.
Train: 2018-08-01T23:31:15.937208: step 10279, loss 0.552784.
Train: 2018-08-01T23:31:16.107722: step 10280, loss 0.660916.
Test: 2018-08-01T23:31:16.643289: step 10280, loss 0.548425.
Train: 2018-08-01T23:31:16.819818: step 10281, loss 0.598482.
Train: 2018-08-01T23:31:16.992356: step 10282, loss 0.546334.
Train: 2018-08-01T23:31:17.163898: step 10283, loss 0.580328.
Train: 2018-08-01T23:31:17.341424: step 10284, loss 0.494182.
Train: 2018-08-01T23:31:17.504986: step 10285, loss 0.648369.
Train: 2018-08-01T23:31:17.668575: step 10286, loss 0.613969.
Train: 2018-08-01T23:31:17.839125: step 10287, loss 0.528186.
Train: 2018-08-01T23:31:18.008640: step 10288, loss 0.510979.
Train: 2018-08-01T23:31:18.172229: step 10289, loss 0.562469.
Train: 2018-08-01T23:31:18.359701: step 10290, loss 0.596746.
Test: 2018-08-01T23:31:18.886294: step 10290, loss 0.548139.
Train: 2018-08-01T23:31:19.054843: step 10291, loss 0.545355.
Train: 2018-08-01T23:31:19.219402: step 10292, loss 0.528251.
Train: 2018-08-01T23:31:19.417872: step 10293, loss 0.648046.
Train: 2018-08-01T23:31:19.593402: step 10294, loss 0.545375.
Train: 2018-08-01T23:31:19.756966: step 10295, loss 0.442878.
Train: 2018-08-01T23:31:19.933494: step 10296, loss 0.613791.
Train: 2018-08-01T23:31:20.100049: step 10297, loss 0.596696.
Train: 2018-08-01T23:31:20.266619: step 10298, loss 0.579579.
Train: 2018-08-01T23:31:20.435153: step 10299, loss 0.579572.
Train: 2018-08-01T23:31:20.600721: step 10300, loss 0.59665.
Test: 2018-08-01T23:31:21.121328: step 10300, loss 0.548176.
Train: 2018-08-01T23:31:21.893915: step 10301, loss 0.511258.
Train: 2018-08-01T23:31:22.062448: step 10302, loss 0.528334.
Train: 2018-08-01T23:31:22.243972: step 10303, loss 0.647841.
Train: 2018-08-01T23:31:22.416521: step 10304, loss 0.596585.
Train: 2018-08-01T23:31:22.581047: step 10305, loss 0.494333.
Train: 2018-08-01T23:31:22.752621: step 10306, loss 0.647628.
Train: 2018-08-01T23:31:22.915178: step 10307, loss 0.443422.
Train: 2018-08-01T23:31:23.093707: step 10308, loss 0.647555.
Train: 2018-08-01T23:31:23.257263: step 10309, loss 0.56247.
Train: 2018-08-01T23:31:23.427784: step 10310, loss 0.545477.
Test: 2018-08-01T23:31:23.953378: step 10310, loss 0.548245.
Train: 2018-08-01T23:31:24.117938: step 10311, loss 0.545482.
Train: 2018-08-01T23:31:24.280503: step 10312, loss 0.698376.
Train: 2018-08-01T23:31:24.447083: step 10313, loss 0.562473.
Train: 2018-08-01T23:31:24.611619: step 10314, loss 0.613245.
Train: 2018-08-01T23:31:24.776210: step 10315, loss 0.494958.
Train: 2018-08-01T23:31:24.945756: step 10316, loss 0.511896.
Train: 2018-08-01T23:31:25.118296: step 10317, loss 0.613077.
Train: 2018-08-01T23:31:25.285849: step 10318, loss 0.56249.
Train: 2018-08-01T23:31:25.450376: step 10319, loss 0.511977.
Train: 2018-08-01T23:31:25.620920: step 10320, loss 0.56249.
Test: 2018-08-01T23:31:26.148523: step 10320, loss 0.548379.
Train: 2018-08-01T23:31:26.311110: step 10321, loss 0.562488.
Train: 2018-08-01T23:31:26.478627: step 10322, loss 0.410795.
Train: 2018-08-01T23:31:26.638226: step 10323, loss 0.51177.
Train: 2018-08-01T23:31:26.814730: step 10324, loss 0.545506.
Train: 2018-08-01T23:31:26.985273: step 10325, loss 0.494395.
Train: 2018-08-01T23:31:27.158810: step 10326, loss 0.613703.
Train: 2018-08-01T23:31:27.326392: step 10327, loss 0.459644.
Train: 2018-08-01T23:31:27.499923: step 10328, loss 0.596857.
Train: 2018-08-01T23:31:27.669444: step 10329, loss 0.510668.
Train: 2018-08-01T23:31:27.835998: step 10330, loss 0.579778.
Test: 2018-08-01T23:31:28.373562: step 10330, loss 0.547913.
Train: 2018-08-01T23:31:28.540145: step 10331, loss 0.475588.
Train: 2018-08-01T23:31:28.706702: step 10332, loss 0.527588.
Train: 2018-08-01T23:31:28.878239: step 10333, loss 0.650053.
Train: 2018-08-01T23:31:29.041775: step 10334, loss 0.492297.
Train: 2018-08-01T23:31:29.208331: step 10335, loss 0.65053.
Train: 2018-08-01T23:31:29.375883: step 10336, loss 0.597774.
Train: 2018-08-01T23:31:29.544432: step 10337, loss 0.527259.
Train: 2018-08-01T23:31:29.711984: step 10338, loss 0.633131.
Train: 2018-08-01T23:31:29.879542: step 10339, loss 0.456665.
Train: 2018-08-01T23:31:30.050106: step 10340, loss 0.509543.
Test: 2018-08-01T23:31:30.584651: step 10340, loss 0.547731.
Train: 2018-08-01T23:31:30.749211: step 10341, loss 0.474075.
Train: 2018-08-01T23:31:30.921749: step 10342, loss 0.527083.
Train: 2018-08-01T23:31:31.086322: step 10343, loss 0.544799.
Train: 2018-08-01T23:31:31.252889: step 10344, loss 0.544774.
Train: 2018-08-01T23:31:31.415455: step 10345, loss 0.616294.
Train: 2018-08-01T23:31:31.578993: step 10346, loss 0.598471.
Train: 2018-08-01T23:31:31.739584: step 10347, loss 0.562657.
Train: 2018-08-01T23:31:31.910108: step 10348, loss 0.473028.
Train: 2018-08-01T23:31:32.071676: step 10349, loss 0.670391.
Train: 2018-08-01T23:31:32.239228: step 10350, loss 0.508834.
Test: 2018-08-01T23:31:32.766845: step 10350, loss 0.547642.
Train: 2018-08-01T23:31:32.935367: step 10351, loss 0.544723.
Train: 2018-08-01T23:31:33.103916: step 10352, loss 0.634507.
Train: 2018-08-01T23:31:33.275489: step 10353, loss 0.634437.
Train: 2018-08-01T23:31:33.443009: step 10354, loss 0.47311.
Train: 2018-08-01T23:31:33.609565: step 10355, loss 0.508952.
Train: 2018-08-01T23:31:33.775147: step 10356, loss 0.473153.
Train: 2018-08-01T23:31:33.945697: step 10357, loss 0.580572.
Train: 2018-08-01T23:31:34.120201: step 10358, loss 0.580591.
Train: 2018-08-01T23:31:34.290769: step 10359, loss 0.59853.
Train: 2018-08-01T23:31:34.454305: step 10360, loss 0.526805.
Test: 2018-08-01T23:31:34.982892: step 10360, loss 0.547646.
Train: 2018-08-01T23:31:35.151442: step 10361, loss 0.490958.
Train: 2018-08-01T23:31:35.322985: step 10362, loss 0.472971.
Train: 2018-08-01T23:31:35.488541: step 10363, loss 0.634569.
Train: 2018-08-01T23:31:35.650137: step 10364, loss 0.508748.
Train: 2018-08-01T23:31:35.811678: step 10365, loss 0.508704.
Train: 2018-08-01T23:31:35.970294: step 10366, loss 0.598769.
Train: 2018-08-01T23:31:36.144787: step 10367, loss 0.472533.
Train: 2018-08-01T23:31:36.314359: step 10368, loss 0.508538.
Train: 2018-08-01T23:31:36.477898: step 10369, loss 0.417911.
Train: 2018-08-01T23:31:36.657417: step 10370, loss 0.580998.
Test: 2018-08-01T23:31:37.181017: step 10370, loss 0.547601.
Train: 2018-08-01T23:31:37.353555: step 10371, loss 0.544637.
Train: 2018-08-01T23:31:37.540058: step 10372, loss 0.471524.
Train: 2018-08-01T23:31:37.728555: step 10373, loss 0.489605.
Train: 2018-08-01T23:31:37.935999: step 10374, loss 0.636656.
Train: 2018-08-01T23:31:38.109536: step 10375, loss 0.673756.
Train: 2018-08-01T23:31:38.284068: step 10376, loss 0.710699.
Train: 2018-08-01T23:31:38.444665: step 10377, loss 0.415685.
Train: 2018-08-01T23:31:38.615209: step 10378, loss 0.673537.
Train: 2018-08-01T23:31:38.776752: step 10379, loss 0.544614.
Train: 2018-08-01T23:31:38.939349: step 10380, loss 0.471185.
Test: 2018-08-01T23:31:39.473887: step 10380, loss 0.547601.
Train: 2018-08-01T23:31:39.640493: step 10381, loss 0.526266.
Train: 2018-08-01T23:31:39.805003: step 10382, loss 0.526268.
Train: 2018-08-01T23:31:39.966572: step 10383, loss 0.599676.
Train: 2018-08-01T23:31:40.129137: step 10384, loss 0.562961.
Train: 2018-08-01T23:31:40.295691: step 10385, loss 0.617942.
Train: 2018-08-01T23:31:40.474214: step 10386, loss 0.544622.
Train: 2018-08-01T23:31:40.636804: step 10387, loss 0.709113.
Train: 2018-08-01T23:31:40.802337: step 10388, loss 0.581064.
Train: 2018-08-01T23:31:40.964933: step 10389, loss 0.635398.
Train: 2018-08-01T23:31:41.133485: step 10390, loss 0.689234.
Test: 2018-08-01T23:31:41.666027: step 10390, loss 0.547631.
Train: 2018-08-01T23:31:41.827608: step 10391, loss 0.472852.
Train: 2018-08-01T23:31:42.000135: step 10392, loss 0.634186.
Train: 2018-08-01T23:31:42.168712: step 10393, loss 0.544783.
Train: 2018-08-01T23:31:42.334255: step 10394, loss 0.633442.
Train: 2018-08-01T23:31:42.513794: step 10395, loss 0.75649.
Train: 2018-08-01T23:31:42.687325: step 10396, loss 0.457449.
Train: 2018-08-01T23:31:42.859861: step 10397, loss 0.597273.
Train: 2018-08-01T23:31:43.020433: step 10398, loss 0.562435.
Train: 2018-08-01T23:31:43.179011: step 10399, loss 0.631368.
Train: 2018-08-01T23:31:43.346563: step 10400, loss 0.493857.
Test: 2018-08-01T23:31:43.883112: step 10400, loss 0.548128.
Train: 2018-08-01T23:31:44.700955: step 10401, loss 0.494125.
Train: 2018-08-01T23:31:44.869503: step 10402, loss 0.511328.
Train: 2018-08-01T23:31:45.032087: step 10403, loss 0.579443.
Train: 2018-08-01T23:31:45.203610: step 10404, loss 0.528465.
Train: 2018-08-01T23:31:45.375152: step 10405, loss 0.49455.
Train: 2018-08-01T23:31:45.538740: step 10406, loss 0.528479.
Train: 2018-08-01T23:31:45.702306: step 10407, loss 0.613415.
Train: 2018-08-01T23:31:45.867835: step 10408, loss 0.630413.
Train: 2018-08-01T23:31:46.032396: step 10409, loss 0.562435.
Train: 2018-08-01T23:31:46.197953: step 10410, loss 0.46065.
Test: 2018-08-01T23:31:46.720555: step 10410, loss 0.548217.
Train: 2018-08-01T23:31:46.884143: step 10411, loss 0.511497.
Train: 2018-08-01T23:31:47.045717: step 10412, loss 0.596445.
Train: 2018-08-01T23:31:47.216231: step 10413, loss 0.63052.
Train: 2018-08-01T23:31:47.384779: step 10414, loss 0.613489.
Train: 2018-08-01T23:31:47.552332: step 10415, loss 0.59644.
Train: 2018-08-01T23:31:47.719915: step 10416, loss 0.579416.
Train: 2018-08-01T23:31:47.887436: step 10417, loss 0.562438.
Train: 2018-08-01T23:31:48.051031: step 10418, loss 0.562441.
Train: 2018-08-01T23:31:48.222540: step 10419, loss 0.562444.
Train: 2018-08-01T23:31:48.385132: step 10420, loss 0.511736.
Test: 2018-08-01T23:31:48.908705: step 10420, loss 0.548292.
Train: 2018-08-01T23:31:49.080247: step 10421, loss 0.528639.
Train: 2018-08-01T23:31:49.244833: step 10422, loss 0.54553.
Train: 2018-08-01T23:31:49.405379: step 10423, loss 0.630156.
Train: 2018-08-01T23:31:49.568941: step 10424, loss 0.630141.
Train: 2018-08-01T23:31:49.736493: step 10425, loss 0.511735.
Train: 2018-08-01T23:31:49.905071: step 10426, loss 0.613145.
Train: 2018-08-01T23:31:50.067632: step 10427, loss 0.494919.
Train: 2018-08-01T23:31:50.242141: step 10428, loss 0.646884.
Train: 2018-08-01T23:31:50.405704: step 10429, loss 0.461239.
Train: 2018-08-01T23:31:50.566275: step 10430, loss 0.629974.
Test: 2018-08-01T23:31:51.087880: step 10430, loss 0.548321.
Train: 2018-08-01T23:31:51.254466: step 10431, loss 0.545577.
Train: 2018-08-01T23:31:51.419992: step 10432, loss 0.545578.
Train: 2018-08-01T23:31:51.582584: step 10433, loss 0.511812.
Train: 2018-08-01T23:31:51.752130: step 10434, loss 0.562447.
Train: 2018-08-01T23:31:51.909714: step 10435, loss 0.52861.
Train: 2018-08-01T23:31:52.077261: step 10436, loss 0.613266.
Train: 2018-08-01T23:31:52.242792: step 10437, loss 0.596343.
Train: 2018-08-01T23:31:52.409347: step 10438, loss 0.562436.
Train: 2018-08-01T23:31:52.571914: step 10439, loss 0.596349.
Train: 2018-08-01T23:31:52.742458: step 10440, loss 0.409885.
Test: 2018-08-01T23:31:53.271044: step 10440, loss 0.548208.
Train: 2018-08-01T23:31:53.439644: step 10441, loss 0.596402.
Train: 2018-08-01T23:31:53.607146: step 10442, loss 0.494395.
Train: 2018-08-01T23:31:53.774713: step 10443, loss 0.681759.
Train: 2018-08-01T23:31:53.942283: step 10444, loss 0.52832.
Train: 2018-08-01T23:31:54.107833: step 10445, loss 0.698947.
Train: 2018-08-01T23:31:54.272367: step 10446, loss 0.613551.
Train: 2018-08-01T23:31:54.434963: step 10447, loss 0.630461.
Train: 2018-08-01T23:31:54.597498: step 10448, loss 0.477633.
Train: 2018-08-01T23:31:54.759102: step 10449, loss 0.426917.
Train: 2018-08-01T23:31:54.921658: step 10450, loss 0.51156.
Test: 2018-08-01T23:31:55.447228: step 10450, loss 0.548204.
Train: 2018-08-01T23:31:55.610825: step 10451, loss 0.460498.
Train: 2018-08-01T23:31:55.776346: step 10452, loss 0.562424.
Train: 2018-08-01T23:31:55.950880: step 10453, loss 0.476958.
Train: 2018-08-01T23:31:56.123445: step 10454, loss 0.562419.
Train: 2018-08-01T23:31:56.284015: step 10455, loss 0.648519.
Train: 2018-08-01T23:31:56.447581: step 10456, loss 0.52792.
Train: 2018-08-01T23:31:56.611116: step 10457, loss 0.614291.
Train: 2018-08-01T23:31:56.775702: step 10458, loss 0.718202.
Train: 2018-08-01T23:31:56.939269: step 10459, loss 0.493283.
Train: 2018-08-01T23:31:57.099813: step 10460, loss 0.562425.
Test: 2018-08-01T23:31:57.634382: step 10460, loss 0.547955.
Train: 2018-08-01T23:31:57.801933: step 10461, loss 0.545145.
Train: 2018-08-01T23:31:57.982449: step 10462, loss 0.562424.
Train: 2018-08-01T23:31:58.147009: step 10463, loss 0.476021.
Train: 2018-08-01T23:31:58.314562: step 10464, loss 0.597033.
Train: 2018-08-01T23:31:58.477152: step 10465, loss 0.510487.
Train: 2018-08-01T23:31:58.657645: step 10466, loss 0.545095.
Train: 2018-08-01T23:31:58.824231: step 10467, loss 0.579794.
Train: 2018-08-01T23:31:58.993772: step 10468, loss 0.579813.
Train: 2018-08-01T23:31:59.157309: step 10469, loss 0.666754.
Train: 2018-08-01T23:31:59.318877: step 10470, loss 0.545068.
Test: 2018-08-01T23:31:59.857438: step 10470, loss 0.547901.
Train: 2018-08-01T23:32:00.038952: step 10471, loss 0.545079.
Train: 2018-08-01T23:32:00.226450: step 10472, loss 0.545086.
Train: 2018-08-01T23:32:00.400324: step 10473, loss 0.493061.
Train: 2018-08-01T23:32:00.564900: step 10474, loss 0.458294.
Train: 2018-08-01T23:32:00.731462: step 10475, loss 0.545044.
Train: 2018-08-01T23:32:00.894005: step 10476, loss 0.562447.
Train: 2018-08-01T23:32:01.068562: step 10477, loss 0.579921.
Train: 2018-08-01T23:32:01.234122: step 10478, loss 0.614935.
Train: 2018-08-01T23:32:01.404639: step 10479, loss 0.422467.
Train: 2018-08-01T23:32:01.570196: step 10480, loss 0.615096.
Test: 2018-08-01T23:32:02.107759: step 10480, loss 0.547773.
Train: 2018-08-01T23:32:02.273317: step 10481, loss 0.527354.
Train: 2018-08-01T23:32:02.438906: step 10482, loss 0.703218.
Train: 2018-08-01T23:32:02.626372: step 10483, loss 0.580065.
Train: 2018-08-01T23:32:02.792953: step 10484, loss 0.544918.
Train: 2018-08-01T23:32:02.958485: step 10485, loss 0.509824.
Train: 2018-08-01T23:32:03.129030: step 10486, loss 0.492269.
Train: 2018-08-01T23:32:03.298576: step 10487, loss 0.580052.
Train: 2018-08-01T23:32:03.472112: step 10488, loss 0.632803.
Train: 2018-08-01T23:32:03.671579: step 10489, loss 0.492205.
Train: 2018-08-01T23:32:03.836140: step 10490, loss 0.685519.
Test: 2018-08-01T23:32:04.376694: step 10490, loss 0.54778.
Train: 2018-08-01T23:32:04.540257: step 10491, loss 0.597577.
Train: 2018-08-01T23:32:04.703854: step 10492, loss 0.5975.
Train: 2018-08-01T23:32:04.864422: step 10493, loss 0.475072.
Train: 2018-08-01T23:32:05.028951: step 10494, loss 0.527529.
Train: 2018-08-01T23:32:05.191536: step 10495, loss 0.649729.
Train: 2018-08-01T23:32:05.358101: step 10496, loss 0.52759.
Train: 2018-08-01T23:32:05.517645: step 10497, loss 0.510212.
Train: 2018-08-01T23:32:05.680234: step 10498, loss 0.597253.
Train: 2018-08-01T23:32:05.842806: step 10499, loss 0.562438.
Train: 2018-08-01T23:32:06.023292: step 10500, loss 0.562435.
Test: 2018-08-01T23:32:06.561883: step 10500, loss 0.547889.
Train: 2018-08-01T23:32:07.340864: step 10501, loss 0.562433.
Train: 2018-08-01T23:32:07.505445: step 10502, loss 0.510358.
Train: 2018-08-01T23:32:07.669990: step 10503, loss 0.631872.
Train: 2018-08-01T23:32:07.832547: step 10504, loss 0.493054.
Train: 2018-08-01T23:32:07.999077: step 10505, loss 0.579776.
Train: 2018-08-01T23:32:08.167626: step 10506, loss 0.493056.
Train: 2018-08-01T23:32:08.337173: step 10507, loss 0.562431.
Train: 2018-08-01T23:32:08.508713: step 10508, loss 0.545063.
Train: 2018-08-01T23:32:08.683248: step 10509, loss 0.545051.
Train: 2018-08-01T23:32:08.847807: step 10510, loss 0.579841.
Test: 2018-08-01T23:32:09.388390: step 10510, loss 0.54786.
Train: 2018-08-01T23:32:09.555942: step 10511, loss 0.649501.
Train: 2018-08-01T23:32:09.722469: step 10512, loss 0.614632.
Train: 2018-08-01T23:32:09.883039: step 10513, loss 0.614547.
Train: 2018-08-01T23:32:10.048598: step 10514, loss 0.579761.
Train: 2018-08-01T23:32:10.211163: step 10515, loss 0.631598.
Train: 2018-08-01T23:32:10.375772: step 10516, loss 0.527938.
Train: 2018-08-01T23:32:10.548262: step 10517, loss 0.510815.
Train: 2018-08-01T23:32:10.712855: step 10518, loss 0.545237.
Train: 2018-08-01T23:32:10.876416: step 10519, loss 0.613901.
Train: 2018-08-01T23:32:11.044968: step 10520, loss 0.579549.
Test: 2018-08-01T23:32:11.572523: step 10520, loss 0.548093.
Train: 2018-08-01T23:32:11.738081: step 10521, loss 0.562416.
Train: 2018-08-01T23:32:11.898679: step 10522, loss 0.664889.
Train: 2018-08-01T23:32:12.068199: step 10523, loss 0.69865.
Train: 2018-08-01T23:32:12.228797: step 10524, loss 0.528534.
Train: 2018-08-01T23:32:12.403303: step 10525, loss 0.494898.
Train: 2018-08-01T23:32:12.569876: step 10526, loss 0.612998.
Train: 2018-08-01T23:32:12.730455: step 10527, loss 0.495252.
Train: 2018-08-01T23:32:12.900973: step 10528, loss 0.57925.
Train: 2018-08-01T23:32:13.070545: step 10529, loss 0.59599.
Train: 2018-08-01T23:32:13.237074: step 10530, loss 0.562485.
Test: 2018-08-01T23:32:13.774637: step 10530, loss 0.548506.
Train: 2018-08-01T23:32:13.942190: step 10531, loss 0.66272.
Train: 2018-08-01T23:32:14.104755: step 10532, loss 0.579168.
Train: 2018-08-01T23:32:14.275329: step 10533, loss 0.462854.
Train: 2018-08-01T23:32:14.438886: step 10534, loss 0.579135.
Train: 2018-08-01T23:32:14.600454: step 10535, loss 0.544846.
Train: 2018-08-01T23:32:14.778953: step 10536, loss 0.496213.
Train: 2018-08-01T23:32:14.947558: step 10537, loss 0.496146.
Train: 2018-08-01T23:32:15.114083: step 10538, loss 0.562521.
Train: 2018-08-01T23:32:15.284600: step 10539, loss 0.64582.
Train: 2018-08-01T23:32:15.451155: step 10540, loss 0.545835.
Test: 2018-08-01T23:32:15.982735: step 10540, loss 0.54853.
Train: 2018-08-01T23:32:16.158264: step 10541, loss 0.51245.
Train: 2018-08-01T23:32:16.334793: step 10542, loss 0.462227.
Train: 2018-08-01T23:32:16.502371: step 10543, loss 0.42837.
Train: 2018-08-01T23:32:16.667910: step 10544, loss 0.562451.
Train: 2018-08-01T23:32:16.831491: step 10545, loss 0.494741.
Train: 2018-08-01T23:32:16.998019: step 10546, loss 0.732529.
Train: 2018-08-01T23:32:17.163608: step 10547, loss 0.59651.
Train: 2018-08-01T23:32:17.325176: step 10548, loss 0.596553.
Train: 2018-08-01T23:32:17.499679: step 10549, loss 0.664898.
Train: 2018-08-01T23:32:17.667257: step 10550, loss 0.562417.
Test: 2018-08-01T23:32:18.192825: step 10550, loss 0.548139.
Train: 2018-08-01T23:32:18.360377: step 10551, loss 0.511264.
Train: 2018-08-01T23:32:18.528958: step 10552, loss 0.460101.
Train: 2018-08-01T23:32:18.690496: step 10553, loss 0.613658.
Train: 2018-08-01T23:32:18.856086: step 10554, loss 0.613694.
Train: 2018-08-01T23:32:19.030587: step 10555, loss 0.494051.
Train: 2018-08-01T23:32:19.198165: step 10556, loss 0.5282.
Train: 2018-08-01T23:32:19.367685: step 10557, loss 0.613806.
Train: 2018-08-01T23:32:19.531272: step 10558, loss 0.596691.
Train: 2018-08-01T23:32:19.707775: step 10559, loss 0.613825.
Train: 2018-08-01T23:32:19.874362: step 10560, loss 0.562414.
Test: 2018-08-01T23:32:20.417879: step 10560, loss 0.548087.
Train: 2018-08-01T23:32:20.618456: step 10561, loss 0.562414.
Train: 2018-08-01T23:32:20.789005: step 10562, loss 0.562414.
Train: 2018-08-01T23:32:20.953533: step 10563, loss 0.664941.
Train: 2018-08-01T23:32:21.120119: step 10564, loss 0.562417.
Train: 2018-08-01T23:32:21.297647: step 10565, loss 0.511358.
Train: 2018-08-01T23:32:21.462206: step 10566, loss 0.61344.
Train: 2018-08-01T23:32:21.642723: step 10567, loss 0.545445.
Train: 2018-08-01T23:32:21.809297: step 10568, loss 0.562428.
Train: 2018-08-01T23:32:21.980837: step 10569, loss 0.630213.
Train: 2018-08-01T23:32:22.144382: step 10570, loss 0.528606.
Test: 2018-08-01T23:32:22.680916: step 10570, loss 0.54829.
Train: 2018-08-01T23:32:22.848493: step 10571, loss 0.562439.
Train: 2018-08-01T23:32:23.017043: step 10572, loss 0.545562.
Train: 2018-08-01T23:32:23.182575: step 10573, loss 0.494958.
Train: 2018-08-01T23:32:23.349154: step 10574, loss 0.562442.
Train: 2018-08-01T23:32:23.511727: step 10575, loss 0.528653.
Train: 2018-08-01T23:32:23.685262: step 10576, loss 0.528611.
Train: 2018-08-01T23:32:23.851812: step 10577, loss 0.57937.
Train: 2018-08-01T23:32:24.017370: step 10578, loss 0.545467.
Train: 2018-08-01T23:32:24.181934: step 10579, loss 0.579408.
Train: 2018-08-01T23:32:24.351449: step 10580, loss 0.613426.
Test: 2018-08-01T23:32:24.875057: step 10580, loss 0.548182.
Train: 2018-08-01T23:32:25.037617: step 10581, loss 0.630439.
Train: 2018-08-01T23:32:25.213177: step 10582, loss 0.596403.
Train: 2018-08-01T23:32:25.377707: step 10583, loss 0.6303.
Train: 2018-08-01T23:32:25.543264: step 10584, loss 0.596296.
Train: 2018-08-01T23:32:25.717798: step 10585, loss 0.562439.
Train: 2018-08-01T23:32:25.884352: step 10586, loss 0.562447.
Train: 2018-08-01T23:32:26.057913: step 10587, loss 0.528806.
Train: 2018-08-01T23:32:26.225440: step 10588, loss 0.663296.
Train: 2018-08-01T23:32:26.395013: step 10589, loss 0.579236.
Train: 2018-08-01T23:32:26.558551: step 10590, loss 0.562484.
Test: 2018-08-01T23:32:27.095116: step 10590, loss 0.548522.
Train: 2018-08-01T23:32:27.262694: step 10591, loss 0.595874.
Train: 2018-08-01T23:32:27.437226: step 10592, loss 0.579161.
Train: 2018-08-01T23:32:27.599786: step 10593, loss 0.579138.
Train: 2018-08-01T23:32:27.765357: step 10594, loss 0.628843.
Train: 2018-08-01T23:32:27.929885: step 10595, loss 0.562565.
Train: 2018-08-01T23:32:28.094476: step 10596, loss 0.529617.
Train: 2018-08-01T23:32:28.260002: step 10597, loss 0.513225.
Train: 2018-08-01T23:32:28.444535: step 10598, loss 0.562605.
Train: 2018-08-01T23:32:28.620039: step 10599, loss 0.611945.
Train: 2018-08-01T23:32:28.789611: step 10600, loss 0.546185.
Test: 2018-08-01T23:32:29.327828: step 10600, loss 0.548866.
Train: 2018-08-01T23:32:30.062176: step 10601, loss 0.496923.
Train: 2018-08-01T23:32:30.222749: step 10602, loss 0.513295.
Train: 2018-08-01T23:32:30.387306: step 10603, loss 0.611998.
Train: 2018-08-01T23:32:30.555855: step 10604, loss 0.496657.
Train: 2018-08-01T23:32:30.725378: step 10605, loss 0.628635.
Train: 2018-08-01T23:32:30.899912: step 10606, loss 0.479909.
Train: 2018-08-01T23:32:31.067463: step 10607, loss 0.479702.
Train: 2018-08-01T23:32:31.239013: step 10608, loss 0.496013.
Train: 2018-08-01T23:32:31.406566: step 10609, loss 0.612588.
Train: 2018-08-01T23:32:31.577110: step 10610, loss 0.579226.
Test: 2018-08-01T23:32:32.120656: step 10610, loss 0.548396.
Train: 2018-08-01T23:32:32.286221: step 10611, loss 0.579258.
Train: 2018-08-01T23:32:32.452799: step 10612, loss 0.629785.
Train: 2018-08-01T23:32:32.631296: step 10613, loss 0.596148.
Train: 2018-08-01T23:32:32.793884: step 10614, loss 0.545589.
Train: 2018-08-01T23:32:32.956451: step 10615, loss 0.579312.
Train: 2018-08-01T23:32:33.122005: step 10616, loss 0.511813.
Train: 2018-08-01T23:32:33.286540: step 10617, loss 0.562437.
Train: 2018-08-01T23:32:33.453093: step 10618, loss 0.494766.
Train: 2018-08-01T23:32:33.627628: step 10619, loss 0.596333.
Train: 2018-08-01T23:32:33.790194: step 10620, loss 0.613356.
Test: 2018-08-01T23:32:34.330747: step 10620, loss 0.548197.
Train: 2018-08-01T23:32:34.497329: step 10621, loss 0.528448.
Train: 2018-08-01T23:32:34.664855: step 10622, loss 0.56242.
Train: 2018-08-01T23:32:34.828445: step 10623, loss 0.528375.
Train: 2018-08-01T23:32:34.993975: step 10624, loss 0.494232.
Train: 2018-08-01T23:32:35.155571: step 10625, loss 0.511154.
Train: 2018-08-01T23:32:35.320104: step 10626, loss 0.579547.
Train: 2018-08-01T23:32:35.480720: step 10627, loss 0.528058.
Train: 2018-08-01T23:32:35.645268: step 10628, loss 0.527969.
Train: 2018-08-01T23:32:35.810817: step 10629, loss 0.476059.
Train: 2018-08-01T23:32:35.990312: step 10630, loss 0.493071.
Test: 2018-08-01T23:32:36.518898: step 10630, loss 0.547853.
Train: 2018-08-01T23:32:36.684456: step 10631, loss 0.527606.
Train: 2018-08-01T23:32:36.857992: step 10632, loss 0.649917.
Train: 2018-08-01T23:32:37.023549: step 10633, loss 0.597544.
Train: 2018-08-01T23:32:37.187145: step 10634, loss 0.615187.
Train: 2018-08-01T23:32:37.359650: step 10635, loss 0.474562.
Train: 2018-08-01T23:32:37.532189: step 10636, loss 0.527258.
Train: 2018-08-01T23:32:37.702735: step 10637, loss 0.63312.
Train: 2018-08-01T23:32:37.883251: step 10638, loss 0.56251.
Train: 2018-08-01T23:32:38.043822: step 10639, loss 0.650905.
Train: 2018-08-01T23:32:38.207080: step 10640, loss 0.686153.
Test: 2018-08-01T23:32:38.736175: step 10640, loss 0.547739.
Train: 2018-08-01T23:32:38.902760: step 10641, loss 0.580109.
Train: 2018-08-01T23:32:39.073274: step 10642, loss 0.474626.
Train: 2018-08-01T23:32:39.241824: step 10643, loss 0.597565.
Train: 2018-08-01T23:32:39.409375: step 10644, loss 0.544942.
Train: 2018-08-01T23:32:39.580916: step 10645, loss 0.544959.
Train: 2018-08-01T23:32:39.748494: step 10646, loss 0.562449.
Train: 2018-08-01T23:32:39.922033: step 10647, loss 0.510068.
Train: 2018-08-01T23:32:40.094543: step 10648, loss 0.70209.
Train: 2018-08-01T23:32:40.254142: step 10649, loss 0.666932.
Train: 2018-08-01T23:32:40.427652: step 10650, loss 0.47566.
Test: 2018-08-01T23:32:40.950256: step 10650, loss 0.547918.
Train: 2018-08-01T23:32:41.123793: step 10651, loss 0.579737.
Train: 2018-08-01T23:32:41.288376: step 10652, loss 0.6661.
Train: 2018-08-01T23:32:41.452912: step 10653, loss 0.614073.
Train: 2018-08-01T23:32:41.622466: step 10654, loss 0.579562.
Train: 2018-08-01T23:32:41.789047: step 10655, loss 0.476976.
Train: 2018-08-01T23:32:41.955599: step 10656, loss 0.494211.
Train: 2018-08-01T23:32:42.125116: step 10657, loss 0.579451.
Train: 2018-08-01T23:32:42.288705: step 10658, loss 0.596455.
Train: 2018-08-01T23:32:42.456260: step 10659, loss 0.511434.
Train: 2018-08-01T23:32:42.632758: step 10660, loss 0.579408.
Test: 2018-08-01T23:32:43.172316: step 10660, loss 0.548206.
Train: 2018-08-01T23:32:43.346850: step 10661, loss 0.579399.
Train: 2018-08-01T23:32:43.521383: step 10662, loss 0.528497.
Train: 2018-08-01T23:32:43.699905: step 10663, loss 0.579385.
Train: 2018-08-01T23:32:43.866485: step 10664, loss 0.477652.
Train: 2018-08-01T23:32:44.031019: step 10665, loss 0.443617.
Train: 2018-08-01T23:32:44.208576: step 10666, loss 0.562417.
Train: 2018-08-01T23:32:44.376128: step 10667, loss 0.630666.
Train: 2018-08-01T23:32:44.536693: step 10668, loss 0.511157.
Train: 2018-08-01T23:32:44.700230: step 10669, loss 0.56241.
Train: 2018-08-01T23:32:44.863823: step 10670, loss 0.528114.
Test: 2018-08-01T23:32:45.405359: step 10670, loss 0.548021.
Train: 2018-08-01T23:32:45.573896: step 10671, loss 0.51086.
Train: 2018-08-01T23:32:45.743468: step 10672, loss 0.614098.
Train: 2018-08-01T23:32:45.915017: step 10673, loss 0.562413.
Train: 2018-08-01T23:32:46.082561: step 10674, loss 0.545135.
Train: 2018-08-01T23:32:46.249117: step 10675, loss 0.545111.
Train: 2018-08-01T23:32:46.419634: step 10676, loss 0.45842.
Train: 2018-08-01T23:32:46.587188: step 10677, loss 0.579813.
Train: 2018-08-01T23:32:46.756735: step 10678, loss 0.579861.
Train: 2018-08-01T23:32:46.930270: step 10679, loss 0.579899.
Train: 2018-08-01T23:32:47.097850: step 10680, loss 0.579927.
Test: 2018-08-01T23:32:47.623418: step 10680, loss 0.547804.
Train: 2018-08-01T23:32:47.790002: step 10681, loss 0.579945.
Train: 2018-08-01T23:32:47.954557: step 10682, loss 0.614955.
Train: 2018-08-01T23:32:48.118120: step 10683, loss 0.492482.
Train: 2018-08-01T23:32:48.295619: step 10684, loss 0.474944.
Train: 2018-08-01T23:32:48.475139: step 10685, loss 0.597527.
Train: 2018-08-01T23:32:48.639701: step 10686, loss 0.544919.
Train: 2018-08-01T23:32:48.811272: step 10687, loss 0.632741.
Train: 2018-08-01T23:32:48.989763: step 10688, loss 0.685421.
Train: 2018-08-01T23:32:49.155353: step 10689, loss 0.632577.
Train: 2018-08-01T23:32:49.333845: step 10690, loss 0.649837.
Test: 2018-08-01T23:32:49.871411: step 10690, loss 0.547855.
Train: 2018-08-01T23:32:50.039956: step 10691, loss 0.457982.
Train: 2018-08-01T23:32:50.209503: step 10692, loss 0.492938.
Train: 2018-08-01T23:32:50.377102: step 10693, loss 0.597137.
Train: 2018-08-01T23:32:50.545606: step 10694, loss 0.545088.
Train: 2018-08-01T23:32:50.710211: step 10695, loss 0.597049.
Train: 2018-08-01T23:32:50.883732: step 10696, loss 0.545125.
Train: 2018-08-01T23:32:51.059233: step 10697, loss 0.562414.
Train: 2018-08-01T23:32:51.226783: step 10698, loss 0.527903.
Train: 2018-08-01T23:32:51.405307: step 10699, loss 0.476173.
Train: 2018-08-01T23:32:51.569866: step 10700, loss 0.527886.
Test: 2018-08-01T23:32:52.106433: step 10700, loss 0.54794.
Train: 2018-08-01T23:32:52.953625: step 10701, loss 0.562415.
Train: 2018-08-01T23:32:53.119183: step 10702, loss 0.545111.
Train: 2018-08-01T23:32:53.296707: step 10703, loss 0.56242.
Train: 2018-08-01T23:32:53.464261: step 10704, loss 0.614466.
Train: 2018-08-01T23:32:53.626857: step 10705, loss 0.527721.
Train: 2018-08-01T23:32:53.791385: step 10706, loss 0.562425.
Train: 2018-08-01T23:32:53.958938: step 10707, loss 0.579798.
Train: 2018-08-01T23:32:54.124522: step 10708, loss 0.545052.
Train: 2018-08-01T23:32:54.290053: step 10709, loss 0.649336.
Train: 2018-08-01T23:32:54.460597: step 10710, loss 0.649248.
Test: 2018-08-01T23:32:55.001152: step 10710, loss 0.547911.
Train: 2018-08-01T23:32:55.182668: step 10711, loss 0.579745.
Train: 2018-08-01T23:32:55.398091: step 10712, loss 0.596987.
Train: 2018-08-01T23:32:55.601547: step 10713, loss 0.458965.
Train: 2018-08-01T23:32:55.765109: step 10714, loss 0.579637.
Train: 2018-08-01T23:32:55.933689: step 10715, loss 0.476362.
Train: 2018-08-01T23:32:56.104203: step 10716, loss 0.579625.
Train: 2018-08-01T23:32:56.263776: step 10717, loss 0.596844.
Train: 2018-08-01T23:32:56.433324: step 10718, loss 0.476364.
Train: 2018-08-01T23:32:56.601903: step 10719, loss 0.614082.
Train: 2018-08-01T23:32:56.765462: step 10720, loss 0.493514.
Test: 2018-08-01T23:32:57.296043: step 10720, loss 0.547973.
Train: 2018-08-01T23:32:57.460595: step 10721, loss 0.545169.
Train: 2018-08-01T23:32:57.637106: step 10722, loss 0.648719.
Train: 2018-08-01T23:32:57.801696: step 10723, loss 0.562412.
Train: 2018-08-01T23:32:57.974203: step 10724, loss 0.510656.
Train: 2018-08-01T23:32:58.144773: step 10725, loss 0.579673.
Train: 2018-08-01T23:32:58.313323: step 10726, loss 0.562412.
Train: 2018-08-01T23:32:58.475866: step 10727, loss 0.562413.
Train: 2018-08-01T23:32:58.650424: step 10728, loss 0.49334.
Train: 2018-08-01T23:32:58.812987: step 10729, loss 0.527839.
Train: 2018-08-01T23:32:58.979541: step 10730, loss 0.458537.
Test: 2018-08-01T23:32:59.507107: step 10730, loss 0.547885.
Train: 2018-08-01T23:32:59.673661: step 10731, loss 0.475609.
Train: 2018-08-01T23:32:59.837254: step 10732, loss 0.649583.
Train: 2018-08-01T23:33:00.002781: step 10733, loss 0.527513.
Train: 2018-08-01T23:33:00.167341: step 10734, loss 0.63248.
Train: 2018-08-01T23:33:00.328909: step 10735, loss 0.61503.
Train: 2018-08-01T23:33:00.494499: step 10736, loss 0.49236.
Train: 2018-08-01T23:33:00.670028: step 10737, loss 0.562464.
Train: 2018-08-01T23:33:00.846557: step 10738, loss 0.527355.
Train: 2018-08-01T23:33:01.011085: step 10739, loss 0.580053.
Train: 2018-08-01T23:33:01.173683: step 10740, loss 0.474529.
Test: 2018-08-01T23:33:01.699256: step 10740, loss 0.547734.
Train: 2018-08-01T23:33:01.867795: step 10741, loss 0.456756.
Train: 2018-08-01T23:33:02.031389: step 10742, loss 0.544834.
Train: 2018-08-01T23:33:02.193923: step 10743, loss 0.580261.
Train: 2018-08-01T23:33:02.360480: step 10744, loss 0.509245.
Train: 2018-08-01T23:33:02.530025: step 10745, loss 0.491309.
Train: 2018-08-01T23:33:02.705580: step 10746, loss 0.616227.
Train: 2018-08-01T23:33:02.871144: step 10747, loss 0.544713.
Train: 2018-08-01T23:33:03.036670: step 10748, loss 0.580591.
Train: 2018-08-01T23:33:03.201253: step 10749, loss 0.5986.
Train: 2018-08-01T23:33:03.364794: step 10750, loss 0.526705.
Test: 2018-08-01T23:33:03.904350: step 10750, loss 0.547606.
Train: 2018-08-01T23:33:04.070931: step 10751, loss 0.54468.
Train: 2018-08-01T23:33:04.236490: step 10752, loss 0.65274.
Train: 2018-08-01T23:33:04.402047: step 10753, loss 0.580677.
Train: 2018-08-01T23:33:04.566606: step 10754, loss 0.454776.
Train: 2018-08-01T23:33:04.731141: step 10755, loss 0.598651.
Train: 2018-08-01T23:33:04.896710: step 10756, loss 0.562669.
Train: 2018-08-01T23:33:05.069263: step 10757, loss 0.562665.
Train: 2018-08-01T23:33:05.234795: step 10758, loss 0.544689.
Train: 2018-08-01T23:33:05.399355: step 10759, loss 0.544692.
Train: 2018-08-01T23:33:05.563915: step 10760, loss 0.634489.
Test: 2018-08-01T23:33:06.089522: step 10760, loss 0.547619.
Train: 2018-08-01T23:33:06.255067: step 10761, loss 0.616443.
Train: 2018-08-01T23:33:06.422650: step 10762, loss 0.508925.
Train: 2018-08-01T23:33:06.589205: step 10763, loss 0.562603.
Train: 2018-08-01T23:33:06.761739: step 10764, loss 0.56259.
Train: 2018-08-01T23:33:06.927271: step 10765, loss 0.473445.
Train: 2018-08-01T23:33:07.093857: step 10766, loss 0.633881.
Train: 2018-08-01T23:33:07.260406: step 10767, loss 0.651591.
Train: 2018-08-01T23:33:07.428930: step 10768, loss 0.509263.
Train: 2018-08-01T23:33:07.593523: step 10769, loss 0.668933.
Train: 2018-08-01T23:33:07.762038: step 10770, loss 0.562511.
Test: 2018-08-01T23:33:08.304589: step 10770, loss 0.54773.
Train: 2018-08-01T23:33:08.468176: step 10771, loss 0.580121.
Train: 2018-08-01T23:33:08.635728: step 10772, loss 0.52732.
Train: 2018-08-01T23:33:08.808268: step 10773, loss 0.632616.
Train: 2018-08-01T23:33:08.971805: step 10774, loss 0.579932.
Train: 2018-08-01T23:33:09.133373: step 10775, loss 0.527576.
Train: 2018-08-01T23:33:09.297979: step 10776, loss 0.52765.
Train: 2018-08-01T23:33:09.462493: step 10777, loss 0.527702.
Train: 2018-08-01T23:33:09.632071: step 10778, loss 0.579763.
Train: 2018-08-01T23:33:09.799628: step 10779, loss 0.579739.
Train: 2018-08-01T23:33:09.970162: step 10780, loss 0.562414.
Test: 2018-08-01T23:33:10.510691: step 10780, loss 0.547946.
Train: 2018-08-01T23:33:10.675251: step 10781, loss 0.510585.
Train: 2018-08-01T23:33:10.845795: step 10782, loss 0.631487.
Train: 2018-08-01T23:33:11.011384: step 10783, loss 0.424453.
Train: 2018-08-01T23:33:11.184914: step 10784, loss 0.579669.
Train: 2018-08-01T23:33:11.346456: step 10785, loss 0.631474.
Train: 2018-08-01T23:33:11.511050: step 10786, loss 0.510648.
Train: 2018-08-01T23:33:11.684579: step 10787, loss 0.596924.
Train: 2018-08-01T23:33:11.849118: step 10788, loss 0.56241.
Train: 2018-08-01T23:33:12.014671: step 10789, loss 0.596896.
Train: 2018-08-01T23:33:12.185239: step 10790, loss 0.493496.
Test: 2018-08-01T23:33:12.710809: step 10790, loss 0.547979.
Train: 2018-08-01T23:33:12.882376: step 10791, loss 0.545176.
Train: 2018-08-01T23:33:13.057912: step 10792, loss 0.596889.
Train: 2018-08-01T23:33:13.222473: step 10793, loss 0.631361.
Train: 2018-08-01T23:33:13.396008: step 10794, loss 0.527972.
Train: 2018-08-01T23:33:13.560538: step 10795, loss 0.648449.
Train: 2018-08-01T23:33:13.726122: step 10796, loss 0.493698.
Train: 2018-08-01T23:33:13.885668: step 10797, loss 0.579573.
Train: 2018-08-01T23:33:14.055215: step 10798, loss 0.682479.
Train: 2018-08-01T23:33:14.223790: step 10799, loss 0.528188.
Train: 2018-08-01T23:33:14.386361: step 10800, loss 0.562409.
Test: 2018-08-01T23:33:14.911924: step 10800, loss 0.548131.
Train: 2018-08-01T23:33:15.793433: step 10801, loss 0.613567.
Train: 2018-08-01T23:33:15.960995: step 10802, loss 0.579429.
Train: 2018-08-01T23:33:16.135519: step 10803, loss 0.630327.
Train: 2018-08-01T23:33:16.302082: step 10804, loss 0.579352.
Train: 2018-08-01T23:33:16.467658: step 10805, loss 0.562437.
Train: 2018-08-01T23:33:16.629230: step 10806, loss 0.528783.
Train: 2018-08-01T23:33:16.799774: step 10807, loss 0.579257.
Train: 2018-08-01T23:33:16.972282: step 10808, loss 0.67987.
Train: 2018-08-01T23:33:17.134848: step 10809, loss 0.462189.
Train: 2018-08-01T23:33:17.298856: step 10810, loss 0.595875.
Test: 2018-08-01T23:33:17.836404: step 10810, loss 0.548545.
Train: 2018-08-01T23:33:18.014917: step 10811, loss 0.595829.
Train: 2018-08-01T23:33:18.189450: step 10812, loss 0.462717.
Train: 2018-08-01T23:33:18.355007: step 10813, loss 0.512616.
Train: 2018-08-01T23:33:18.520566: step 10814, loss 0.595801.
Train: 2018-08-01T23:33:18.705073: step 10815, loss 0.54585.
Train: 2018-08-01T23:33:18.875616: step 10816, loss 0.529167.
Train: 2018-08-01T23:33:19.037210: step 10817, loss 0.512429.
Train: 2018-08-01T23:33:19.210721: step 10818, loss 0.529035.
Train: 2018-08-01T23:33:19.376279: step 10819, loss 0.512174.
Train: 2018-08-01T23:33:19.542832: step 10820, loss 0.528818.
Test: 2018-08-01T23:33:20.077434: step 10820, loss 0.548307.
Train: 2018-08-01T23:33:20.245953: step 10821, loss 0.646804.
Train: 2018-08-01T23:33:20.410544: step 10822, loss 0.613141.
Train: 2018-08-01T23:33:20.578114: step 10823, loss 0.579346.
Train: 2018-08-01T23:33:20.740674: step 10824, loss 0.562426.
Train: 2018-08-01T23:33:20.904194: step 10825, loss 0.562424.
Train: 2018-08-01T23:33:21.084710: step 10826, loss 0.681041.
Train: 2018-08-01T23:33:21.250269: step 10827, loss 0.494736.
Train: 2018-08-01T23:33:21.413831: step 10828, loss 0.647032.
Train: 2018-08-01T23:33:21.580385: step 10829, loss 0.528636.
Train: 2018-08-01T23:33:21.748934: step 10830, loss 0.629982.
Test: 2018-08-01T23:33:22.279548: step 10830, loss 0.548321.
Train: 2018-08-01T23:33:22.445074: step 10831, loss 0.61302.
Train: 2018-08-01T23:33:22.621603: step 10832, loss 0.545625.
Train: 2018-08-01T23:33:22.779210: step 10833, loss 0.478476.
Train: 2018-08-01T23:33:22.942008: step 10834, loss 0.444891.
Train: 2018-08-01T23:33:23.118420: step 10835, loss 0.579275.
Train: 2018-08-01T23:33:23.293950: step 10836, loss 0.526489.
Train: 2018-08-01T23:33:23.459533: step 10837, loss 0.596206.
Train: 2018-08-01T23:33:23.621106: step 10838, loss 0.579338.
Train: 2018-08-01T23:33:23.782669: step 10839, loss 0.6132.
Train: 2018-08-01T23:33:23.946245: step 10840, loss 0.511646.
Test: 2018-08-01T23:33:24.456841: step 10840, loss 0.548236.
Train: 2018-08-01T23:33:24.624395: step 10841, loss 0.511595.
Train: 2018-08-01T23:33:24.786991: step 10842, loss 0.57939.
Train: 2018-08-01T23:33:24.949550: step 10843, loss 0.545422.
Train: 2018-08-01T23:33:25.116080: step 10844, loss 0.477321.
Train: 2018-08-01T23:33:25.278668: step 10845, loss 0.494153.
Train: 2018-08-01T23:33:25.440245: step 10846, loss 0.613775.
Train: 2018-08-01T23:33:25.602783: step 10847, loss 0.579568.
Train: 2018-08-01T23:33:25.771352: step 10848, loss 0.528017.
Train: 2018-08-01T23:33:25.940875: step 10849, loss 0.596871.
Train: 2018-08-01T23:33:26.105435: step 10850, loss 0.527897.
Test: 2018-08-01T23:33:26.636042: step 10850, loss 0.547936.
Train: 2018-08-01T23:33:26.802571: step 10851, loss 0.510553.
Train: 2018-08-01T23:33:26.965137: step 10852, loss 0.579743.
Train: 2018-08-01T23:33:27.137674: step 10853, loss 0.649212.
Train: 2018-08-01T23:33:27.312214: step 10854, loss 0.649237.
Train: 2018-08-01T23:33:27.472804: step 10855, loss 0.597107.
Train: 2018-08-01T23:33:27.640331: step 10856, loss 0.59705.
Train: 2018-08-01T23:33:27.807883: step 10857, loss 0.493275.
Train: 2018-08-01T23:33:27.973441: step 10858, loss 0.493324.
Train: 2018-08-01T23:33:28.148972: step 10859, loss 0.579689.
Train: 2018-08-01T23:33:28.323505: step 10860, loss 0.493297.
Test: 2018-08-01T23:33:28.867052: step 10860, loss 0.547929.
Train: 2018-08-01T23:33:29.044609: step 10861, loss 0.493225.
Train: 2018-08-01T23:33:29.213126: step 10862, loss 0.527755.
Train: 2018-08-01T23:33:29.383699: step 10863, loss 0.527682.
Train: 2018-08-01T23:33:29.546269: step 10864, loss 0.649499.
Train: 2018-08-01T23:33:29.717808: step 10865, loss 0.579863.
Train: 2018-08-01T23:33:29.884333: step 10866, loss 0.457808.
Train: 2018-08-01T23:33:30.054896: step 10867, loss 0.527499.
Train: 2018-08-01T23:33:30.236422: step 10868, loss 0.544943.
Train: 2018-08-01T23:33:30.407933: step 10869, loss 0.632657.
Train: 2018-08-01T23:33:30.573516: step 10870, loss 0.597594.
Test: 2018-08-01T23:33:31.107065: step 10870, loss 0.54776.
Train: 2018-08-01T23:33:31.283592: step 10871, loss 0.615166.
Train: 2018-08-01T23:33:31.444811: step 10872, loss 0.474699.
Train: 2018-08-01T23:33:31.607345: step 10873, loss 0.492215.
Train: 2018-08-01T23:33:31.771905: step 10874, loss 0.632832.
Train: 2018-08-01T23:33:31.943429: step 10875, loss 0.527292.
Train: 2018-08-01T23:33:32.112963: step 10876, loss 0.597688.
Train: 2018-08-01T23:33:32.276524: step 10877, loss 0.562481.
Train: 2018-08-01T23:33:32.439095: step 10878, loss 0.63288.
Train: 2018-08-01T23:33:32.607641: step 10879, loss 0.509738.
Train: 2018-08-01T23:33:32.768238: step 10880, loss 0.474616.
Test: 2018-08-01T23:33:33.293809: step 10880, loss 0.547749.
Train: 2018-08-01T23:33:33.459365: step 10881, loss 0.54489.
Train: 2018-08-01T23:33:33.628935: step 10882, loss 0.527278.
Train: 2018-08-01T23:33:33.795490: step 10883, loss 0.580111.
Train: 2018-08-01T23:33:33.965011: step 10884, loss 0.597767.
Train: 2018-08-01T23:33:34.130569: step 10885, loss 0.615409.
Train: 2018-08-01T23:33:34.298121: step 10886, loss 0.597738.
Train: 2018-08-01T23:33:34.462683: step 10887, loss 0.509676.
Train: 2018-08-01T23:33:34.640207: step 10888, loss 0.527292.
Train: 2018-08-01T23:33:34.805764: step 10889, loss 0.562477.
Train: 2018-08-01T23:33:34.975339: step 10890, loss 0.562477.
Test: 2018-08-01T23:33:35.496916: step 10890, loss 0.547748.
Train: 2018-08-01T23:33:35.663471: step 10891, loss 0.632826.
Train: 2018-08-01T23:33:35.830025: step 10892, loss 0.544903.
Train: 2018-08-01T23:33:35.996589: step 10893, loss 0.597559.
Train: 2018-08-01T23:33:36.222974: step 10894, loss 0.685111.
Train: 2018-08-01T23:33:36.400501: step 10895, loss 0.667237.
Train: 2018-08-01T23:33:36.562068: step 10896, loss 0.579812.
Train: 2018-08-01T23:33:36.726630: step 10897, loss 0.700903.
Train: 2018-08-01T23:33:36.893230: step 10898, loss 0.63123.
Train: 2018-08-01T23:33:37.062729: step 10899, loss 0.613686.
Train: 2018-08-01T23:33:37.230313: step 10900, loss 0.613352.
Test: 2018-08-01T23:33:37.764853: step 10900, loss 0.548318.
Train: 2018-08-01T23:33:38.529157: step 10901, loss 0.663611.
Train: 2018-08-01T23:33:38.704688: step 10902, loss 0.579206.
Train: 2018-08-01T23:33:38.869248: step 10903, loss 0.645574.
Train: 2018-08-01T23:33:39.035834: step 10904, loss 0.529618.
Train: 2018-08-01T23:33:39.207369: step 10905, loss 0.579017.
Train: 2018-08-01T23:33:39.373933: step 10906, loss 0.465019.
Train: 2018-08-01T23:33:39.557409: step 10907, loss 0.643863.
Train: 2018-08-01T23:33:39.721994: step 10908, loss 0.611257.
Train: 2018-08-01T23:33:39.916451: step 10909, loss 0.546773.
Train: 2018-08-01T23:33:40.110928: step 10910, loss 0.482785.
Test: 2018-08-01T23:33:40.648491: step 10910, loss 0.549529.
Train: 2018-08-01T23:33:40.815048: step 10911, loss 0.546927.
Train: 2018-08-01T23:33:40.980604: step 10912, loss 0.499.
Train: 2018-08-01T23:33:41.142172: step 10913, loss 0.594918.
Train: 2018-08-01T23:33:41.304737: step 10914, loss 0.498935.
Train: 2018-08-01T23:33:41.470296: step 10915, loss 0.626997.
Train: 2018-08-01T23:33:41.641862: step 10916, loss 0.578928.
Train: 2018-08-01T23:33:41.814376: step 10917, loss 0.659128.
Train: 2018-08-01T23:33:41.982950: step 10918, loss 0.498811.
Train: 2018-08-01T23:33:42.145491: step 10919, loss 0.530836.
Train: 2018-08-01T23:33:42.309072: step 10920, loss 0.53078.
Test: 2018-08-01T23:33:42.832661: step 10920, loss 0.549389.
Train: 2018-08-01T23:33:42.998241: step 10921, loss 0.562854.
Train: 2018-08-01T23:33:43.172744: step 10922, loss 0.385599.
Train: 2018-08-01T23:33:43.342291: step 10923, loss 0.546572.
Train: 2018-08-01T23:33:43.504857: step 10924, loss 0.546433.
Train: 2018-08-01T23:33:43.674436: step 10925, loss 0.497239.
Train: 2018-08-01T23:33:43.840958: step 10926, loss 0.496813.
Train: 2018-08-01T23:33:44.007512: step 10927, loss 0.612198.
Train: 2018-08-01T23:33:44.171075: step 10928, loss 0.529244.
Train: 2018-08-01T23:33:44.337629: step 10929, loss 0.612632.
Train: 2018-08-01T23:33:44.510202: step 10930, loss 0.596023.
Test: 2018-08-01T23:33:45.046734: step 10930, loss 0.548349.
Train: 2018-08-01T23:33:45.213314: step 10931, loss 0.562445.
Train: 2018-08-01T23:33:45.375854: step 10932, loss 0.596192.
Train: 2018-08-01T23:33:45.542437: step 10933, loss 0.646993.
Train: 2018-08-01T23:33:45.705001: step 10934, loss 0.528581.
Train: 2018-08-01T23:33:45.871558: step 10935, loss 0.545482.
Train: 2018-08-01T23:33:46.037117: step 10936, loss 0.681145.
Train: 2018-08-01T23:33:46.201647: step 10937, loss 0.460725.
Train: 2018-08-01T23:33:46.363248: step 10938, loss 0.596351.
Train: 2018-08-01T23:33:46.528772: step 10939, loss 0.613337.
Train: 2018-08-01T23:33:46.705300: step 10940, loss 0.630286.
Test: 2018-08-01T23:33:47.247849: step 10940, loss 0.548233.
Train: 2018-08-01T23:33:47.413433: step 10941, loss 0.647141.
Train: 2018-08-01T23:33:47.576970: step 10942, loss 0.494823.
Train: 2018-08-01T23:33:47.740533: step 10943, loss 0.545551.
Train: 2018-08-01T23:33:47.904122: step 10944, loss 0.663657.
Train: 2018-08-01T23:33:48.069656: step 10945, loss 0.478276.
Train: 2018-08-01T23:33:48.237206: step 10946, loss 0.49515.
Train: 2018-08-01T23:33:48.405781: step 10947, loss 0.646617.
Train: 2018-08-01T23:33:48.570315: step 10948, loss 0.511979.
Train: 2018-08-01T23:33:48.736880: step 10949, loss 0.612926.
Train: 2018-08-01T23:33:48.905420: step 10950, loss 0.511994.
Test: 2018-08-01T23:33:49.441985: step 10950, loss 0.548358.
Train: 2018-08-01T23:33:49.608539: step 10951, loss 0.562446.
Train: 2018-08-01T23:33:49.772127: step 10952, loss 0.562444.
Train: 2018-08-01T23:33:49.936663: step 10953, loss 0.562442.
Train: 2018-08-01T23:33:50.105213: step 10954, loss 0.478203.
Train: 2018-08-01T23:33:50.268805: step 10955, loss 0.528677.
Train: 2018-08-01T23:33:50.429345: step 10956, loss 0.647005.
Train: 2018-08-01T23:33:50.594932: step 10957, loss 0.545496.
Train: 2018-08-01T23:33:50.756472: step 10958, loss 0.511589.
Train: 2018-08-01T23:33:50.919037: step 10959, loss 0.681227.
Train: 2018-08-01T23:33:51.088614: step 10960, loss 0.443641.
Test: 2018-08-01T23:33:51.626179: step 10960, loss 0.548182.
Train: 2018-08-01T23:33:51.793730: step 10961, loss 0.545418.
Train: 2018-08-01T23:33:51.959256: step 10962, loss 0.528359.
Train: 2018-08-01T23:33:52.123844: step 10963, loss 0.511222.
Train: 2018-08-01T23:33:52.300390: step 10964, loss 0.613729.
Train: 2018-08-01T23:33:52.474903: step 10965, loss 0.47672.
Train: 2018-08-01T23:33:52.641456: step 10966, loss 0.596774.
Train: 2018-08-01T23:33:52.803011: step 10967, loss 0.493529.
Train: 2018-08-01T23:33:52.967594: step 10968, loss 0.545141.
Train: 2018-08-01T23:33:53.136149: step 10969, loss 0.562413.
Train: 2018-08-01T23:33:53.303693: step 10970, loss 0.614492.
Test: 2018-08-01T23:33:53.843220: step 10970, loss 0.547868.
Train: 2018-08-01T23:33:54.005785: step 10971, loss 0.63195.
Train: 2018-08-01T23:33:54.182346: step 10972, loss 0.631963.
Train: 2018-08-01T23:33:54.346898: step 10973, loss 0.562421.
Train: 2018-08-01T23:33:54.511454: step 10974, loss 0.579772.
Train: 2018-08-01T23:33:54.677988: step 10975, loss 0.493076.
Train: 2018-08-01T23:33:54.842548: step 10976, loss 0.562416.
Train: 2018-08-01T23:33:55.006120: step 10977, loss 0.493075.
Train: 2018-08-01T23:33:55.171668: step 10978, loss 0.527714.
Train: 2018-08-01T23:33:55.335257: step 10979, loss 0.44079.
Train: 2018-08-01T23:33:55.501785: step 10980, loss 0.579859.
Test: 2018-08-01T23:33:56.029375: step 10980, loss 0.547812.
Train: 2018-08-01T23:33:56.198953: step 10981, loss 0.667255.
Train: 2018-08-01T23:33:56.361487: step 10982, loss 0.49253.
Train: 2018-08-01T23:33:56.524054: step 10983, loss 0.579953.
Train: 2018-08-01T23:33:56.686649: step 10984, loss 0.474853.
Train: 2018-08-01T23:33:56.850194: step 10985, loss 0.544908.
Train: 2018-08-01T23:33:57.009787: step 10986, loss 0.544884.
Train: 2018-08-01T23:33:57.177321: step 10987, loss 0.491978.
Train: 2018-08-01T23:33:57.343861: step 10988, loss 0.491804.
Train: 2018-08-01T23:33:57.513439: step 10989, loss 0.473856.
Train: 2018-08-01T23:33:57.692928: step 10990, loss 0.562566.
Test: 2018-08-01T23:33:58.221542: step 10990, loss 0.547632.
Train: 2018-08-01T23:33:58.394053: step 10991, loss 0.455361.
Train: 2018-08-01T23:33:58.572601: step 10992, loss 0.616512.
Train: 2018-08-01T23:33:58.739386: step 10993, loss 0.54467.
Train: 2018-08-01T23:33:58.898155: step 10994, loss 0.598851.
Train: 2018-08-01T23:33:59.059712: step 10995, loss 0.598949.
Train: 2018-08-01T23:33:59.225245: step 10996, loss 0.599005.
Train: 2018-08-01T23:33:59.388808: step 10997, loss 0.65341.
Train: 2018-08-01T23:33:59.559376: step 10998, loss 0.617077.
Train: 2018-08-01T23:33:59.725914: step 10999, loss 0.580799.
Train: 2018-08-01T23:33:59.894483: step 11000, loss 0.598769.
Test: 2018-08-01T23:34:00.420050: step 11000, loss 0.547603.
Train: 2018-08-01T23:34:01.206480: step 11001, loss 0.562667.
Train: 2018-08-01T23:34:01.396946: step 11002, loss 0.526752.
Train: 2018-08-01T23:34:01.557517: step 11003, loss 0.473072.
Train: 2018-08-01T23:34:01.762968: step 11004, loss 0.634207.
Train: 2018-08-01T23:34:01.926530: step 11005, loss 0.491124.
Train: 2018-08-01T23:34:02.096077: step 11006, loss 0.598303.
Train: 2018-08-01T23:34:02.265649: step 11007, loss 0.65175.
Train: 2018-08-01T23:34:02.432179: step 11008, loss 0.687086.
Train: 2018-08-01T23:34:02.593773: step 11009, loss 0.456223.
Train: 2018-08-01T23:34:02.759332: step 11010, loss 0.50948.
Test: 2018-08-01T23:34:03.297866: step 11010, loss 0.547716.
Train: 2018-08-01T23:34:03.464425: step 11011, loss 0.633097.
Train: 2018-08-01T23:34:03.631993: step 11012, loss 0.527265.
Train: 2018-08-01T23:34:03.798552: step 11013, loss 0.457008.
Train: 2018-08-01T23:34:03.961117: step 11014, loss 0.738217.
Train: 2018-08-01T23:34:04.129641: step 11015, loss 0.615036.
Train: 2018-08-01T23:34:04.300185: step 11016, loss 0.56244.
Train: 2018-08-01T23:34:04.467762: step 11017, loss 0.510181.
Train: 2018-08-01T23:34:04.636287: step 11018, loss 0.614565.
Train: 2018-08-01T23:34:04.818799: step 11019, loss 0.666425.
Train: 2018-08-01T23:34:04.998319: step 11020, loss 0.579674.
Test: 2018-08-01T23:34:05.543861: step 11020, loss 0.548001.
Train: 2018-08-01T23:34:05.728368: step 11021, loss 0.562404.
Train: 2018-08-01T23:34:05.887940: step 11022, loss 0.425284.
Train: 2018-08-01T23:34:06.055518: step 11023, loss 0.579525.
Train: 2018-08-01T23:34:06.228056: step 11024, loss 0.613706.
Train: 2018-08-01T23:34:06.393588: step 11025, loss 0.545337.
Train: 2018-08-01T23:34:06.563160: step 11026, loss 0.596497.
Train: 2018-08-01T23:34:06.737668: step 11027, loss 0.545396.
Train: 2018-08-01T23:34:06.903227: step 11028, loss 0.545421.
Train: 2018-08-01T23:34:07.073771: step 11029, loss 0.562415.
Train: 2018-08-01T23:34:07.238330: step 11030, loss 0.528487.
Test: 2018-08-01T23:34:07.769909: step 11030, loss 0.548213.
Train: 2018-08-01T23:34:07.934503: step 11031, loss 0.562417.
Train: 2018-08-01T23:34:08.099030: step 11032, loss 0.562418.
Train: 2018-08-01T23:34:08.262624: step 11033, loss 0.511533.
Train: 2018-08-01T23:34:08.430146: step 11034, loss 0.562416.
Train: 2018-08-01T23:34:08.592711: step 11035, loss 0.528438.
Train: 2018-08-01T23:34:08.767244: step 11036, loss 0.511383.
Train: 2018-08-01T23:34:08.937788: step 11037, loss 0.494239.
Train: 2018-08-01T23:34:09.102348: step 11038, loss 0.511134.
Train: 2018-08-01T23:34:09.261949: step 11039, loss 0.476669.
Train: 2018-08-01T23:34:09.431468: step 11040, loss 0.596844.
Test: 2018-08-01T23:34:09.957064: step 11040, loss 0.547941.
Train: 2018-08-01T23:34:10.123643: step 11041, loss 0.476023.
Train: 2018-08-01T23:34:10.289175: step 11042, loss 0.597117.
Train: 2018-08-01T23:34:10.451740: step 11043, loss 0.527614.
Train: 2018-08-01T23:34:10.616328: step 11044, loss 0.597369.
Train: 2018-08-01T23:34:10.783853: step 11045, loss 0.685004.
Train: 2018-08-01T23:34:10.948441: step 11046, loss 0.650021.
Train: 2018-08-01T23:34:11.112006: step 11047, loss 0.63243.
Train: 2018-08-01T23:34:11.284567: step 11048, loss 0.562438.
Train: 2018-08-01T23:34:11.453097: step 11049, loss 0.666999.
Train: 2018-08-01T23:34:11.624606: step 11050, loss 0.545049.
Test: 2018-08-01T23:34:12.170147: step 11050, loss 0.547907.
Train: 2018-08-01T23:34:12.349667: step 11051, loss 0.614384.
Train: 2018-08-01T23:34:12.512233: step 11052, loss 0.596941.
Train: 2018-08-01T23:34:12.678787: step 11053, loss 0.493578.
Train: 2018-08-01T23:34:12.849331: step 11054, loss 0.579572.
Train: 2018-08-01T23:34:13.022867: step 11055, loss 0.476742.
Train: 2018-08-01T23:34:13.198400: step 11056, loss 0.596645.
Train: 2018-08-01T23:34:13.363956: step 11057, loss 0.42559.
Train: 2018-08-01T23:34:13.535524: step 11058, loss 0.596644.
Train: 2018-08-01T23:34:13.707038: step 11059, loss 0.648044.
Train: 2018-08-01T23:34:13.879577: step 11060, loss 0.49395.
Test: 2018-08-01T23:34:14.416143: step 11060, loss 0.548069.
Train: 2018-08-01T23:34:14.586687: step 11061, loss 0.51105.
Train: 2018-08-01T23:34:14.755236: step 11062, loss 0.545267.
Train: 2018-08-01T23:34:14.929794: step 11063, loss 0.528091.
Train: 2018-08-01T23:34:15.096325: step 11064, loss 0.631133.
Train: 2018-08-01T23:34:15.262879: step 11065, loss 0.528024.
Train: 2018-08-01T23:34:15.429434: step 11066, loss 0.510792.
Train: 2018-08-01T23:34:15.600976: step 11067, loss 0.545174.
Train: 2018-08-01T23:34:15.774542: step 11068, loss 0.596923.
Train: 2018-08-01T23:34:15.941065: step 11069, loss 0.476037.
Train: 2018-08-01T23:34:16.107621: step 11070, loss 0.562411.
Test: 2018-08-01T23:34:16.628236: step 11070, loss 0.547893.
Train: 2018-08-01T23:34:16.803759: step 11071, loss 0.579758.
Train: 2018-08-01T23:34:16.966351: step 11072, loss 0.458222.
Train: 2018-08-01T23:34:17.128915: step 11073, loss 0.579841.
Train: 2018-08-01T23:34:17.291485: step 11074, loss 0.475181.
Train: 2018-08-01T23:34:17.459008: step 11075, loss 0.474921.
Train: 2018-08-01T23:34:17.621573: step 11076, loss 0.597622.
Train: 2018-08-01T23:34:17.790130: step 11077, loss 0.52723.
Train: 2018-08-01T23:34:17.953714: step 11078, loss 0.491772.
Train: 2018-08-01T23:34:18.121261: step 11079, loss 0.491538.
Train: 2018-08-01T23:34:18.295771: step 11080, loss 0.491275.
Test: 2018-08-01T23:34:18.821321: step 11080, loss 0.547622.
Train: 2018-08-01T23:34:18.987908: step 11081, loss 0.508895.
Train: 2018-08-01T23:34:19.155464: step 11082, loss 0.508691.
Train: 2018-08-01T23:34:19.329961: step 11083, loss 0.598887.
Train: 2018-08-01T23:34:19.499533: step 11084, loss 0.453895.
Train: 2018-08-01T23:34:19.672077: step 11085, loss 0.61753.
Train: 2018-08-01T23:34:19.841592: step 11086, loss 0.636041.
Train: 2018-08-01T23:34:20.020115: step 11087, loss 0.544596.
Train: 2018-08-01T23:34:20.184709: step 11088, loss 0.654674.
Train: 2018-08-01T23:34:20.354253: step 11089, loss 0.452864.
Train: 2018-08-01T23:34:20.518783: step 11090, loss 0.562959.
Test: 2018-08-01T23:34:21.058341: step 11090, loss 0.547579.
Train: 2018-08-01T23:34:21.277879: step 11091, loss 0.654889.
Train: 2018-08-01T23:34:21.442440: step 11092, loss 0.581333.
Train: 2018-08-01T23:34:21.607996: step 11093, loss 0.562944.
Train: 2018-08-01T23:34:21.785522: step 11094, loss 0.599584.
Train: 2018-08-01T23:34:21.957063: step 11095, loss 0.544599.
Train: 2018-08-01T23:34:22.128606: step 11096, loss 0.690755.
Train: 2018-08-01T23:34:22.302142: step 11097, loss 0.544614.
Train: 2018-08-01T23:34:22.465704: step 11098, loss 0.526471.
Train: 2018-08-01T23:34:22.638243: step 11099, loss 0.544637.
Train: 2018-08-01T23:34:22.805820: step 11100, loss 0.61695.
Test: 2018-08-01T23:34:23.329395: step 11100, loss 0.547595.
Train: 2018-08-01T23:34:24.144489: step 11101, loss 0.56269.
Train: 2018-08-01T23:34:24.311062: step 11102, loss 0.580637.
Train: 2018-08-01T23:34:24.487541: step 11103, loss 0.598484.
Train: 2018-08-01T23:34:24.655093: step 11104, loss 0.491113.
Train: 2018-08-01T23:34:24.822670: step 11105, loss 0.651754.
Train: 2018-08-01T23:34:24.996182: step 11106, loss 0.633664.
Train: 2018-08-01T23:34:25.162768: step 11107, loss 0.47398.
Train: 2018-08-01T23:34:25.325327: step 11108, loss 0.544836.
Train: 2018-08-01T23:34:25.488895: step 11109, loss 0.562487.
Train: 2018-08-01T23:34:25.655448: step 11110, loss 0.615256.
Test: 2018-08-01T23:34:26.179026: step 11110, loss 0.547766.
Train: 2018-08-01T23:34:26.358539: step 11111, loss 0.615104.
Train: 2018-08-01T23:34:26.533073: step 11112, loss 0.614922.
Train: 2018-08-01T23:34:26.699627: step 11113, loss 0.510148.
Train: 2018-08-01T23:34:26.869175: step 11114, loss 0.579803.
Train: 2018-08-01T23:34:27.047697: step 11115, loss 0.649089.
Train: 2018-08-01T23:34:27.215249: step 11116, loss 0.562407.
Train: 2018-08-01T23:34:27.378838: step 11117, loss 0.596827.
Train: 2018-08-01T23:34:27.545394: step 11118, loss 0.459503.
Train: 2018-08-01T23:34:27.716908: step 11119, loss 0.596644.
Train: 2018-08-01T23:34:27.883487: step 11120, loss 0.528232.
Test: 2018-08-01T23:34:28.431995: step 11120, loss 0.548116.
Train: 2018-08-01T23:34:28.597554: step 11121, loss 0.562405.
Train: 2018-08-01T23:34:28.764134: step 11122, loss 0.596495.
Train: 2018-08-01T23:34:28.923713: step 11123, loss 0.477319.
Train: 2018-08-01T23:34:29.106193: step 11124, loss 0.68153.
Train: 2018-08-01T23:34:29.264473: step 11125, loss 0.545427.
Train: 2018-08-01T23:34:29.426661: step 11126, loss 0.494567.
Train: 2018-08-01T23:34:29.593254: step 11127, loss 0.494577.
Train: 2018-08-01T23:34:29.761312: step 11128, loss 0.613343.
Train: 2018-08-01T23:34:29.924902: step 11129, loss 0.630327.
Train: 2018-08-01T23:34:30.096467: step 11130, loss 0.511528.
Test: 2018-08-01T23:34:30.633980: step 11130, loss 0.548212.
Train: 2018-08-01T23:34:30.797576: step 11131, loss 0.613305.
Train: 2018-08-01T23:34:30.962103: step 11132, loss 0.562418.
Train: 2018-08-01T23:34:31.132673: step 11133, loss 0.56242.
Train: 2018-08-01T23:34:31.301222: step 11134, loss 0.579352.
Train: 2018-08-01T23:34:31.470771: step 11135, loss 0.494747.
Train: 2018-08-01T23:34:31.633312: step 11136, loss 0.562422.
Train: 2018-08-01T23:34:31.799864: step 11137, loss 0.634678.
Train: 2018-08-01T23:34:31.975425: step 11138, loss 0.511649.
Train: 2018-08-01T23:34:32.144941: step 11139, loss 0.596279.
Train: 2018-08-01T23:34:32.320472: step 11140, loss 0.579347.
Test: 2018-08-01T23:34:32.855043: step 11140, loss 0.548258.
Train: 2018-08-01T23:34:33.028609: step 11141, loss 0.562424.
Train: 2018-08-01T23:34:33.203138: step 11142, loss 0.444047.
Train: 2018-08-01T23:34:33.384626: step 11143, loss 0.528545.
Train: 2018-08-01T23:34:33.554204: step 11144, loss 0.664245.
Train: 2018-08-01T23:34:33.727710: step 11145, loss 0.528463.
Train: 2018-08-01T23:34:33.891273: step 11146, loss 0.579402.
Train: 2018-08-01T23:34:34.059854: step 11147, loss 0.579409.
Train: 2018-08-01T23:34:34.224408: step 11148, loss 0.477407.
Train: 2018-08-01T23:34:34.386948: step 11149, loss 0.613488.
Train: 2018-08-01T23:34:34.550538: step 11150, loss 0.477222.
Test: 2018-08-01T23:34:35.085081: step 11150, loss 0.548111.
Train: 2018-08-01T23:34:35.255636: step 11151, loss 0.562405.
Train: 2018-08-01T23:34:35.421208: step 11152, loss 0.49401.
Train: 2018-08-01T23:34:35.581780: step 11153, loss 0.47669.
Train: 2018-08-01T23:34:35.750303: step 11154, loss 0.596812.
Train: 2018-08-01T23:34:35.922841: step 11155, loss 0.579657.
Train: 2018-08-01T23:34:36.094409: step 11156, loss 0.700718.
Train: 2018-08-01T23:34:36.259940: step 11157, loss 0.562408.
Train: 2018-08-01T23:34:36.428490: step 11158, loss 0.579692.
Train: 2018-08-01T23:34:36.595046: step 11159, loss 0.510578.
Train: 2018-08-01T23:34:36.767583: step 11160, loss 0.545125.
Test: 2018-08-01T23:34:37.304149: step 11160, loss 0.547928.
Train: 2018-08-01T23:34:37.478721: step 11161, loss 0.596992.
Train: 2018-08-01T23:34:37.648237: step 11162, loss 0.579696.
Train: 2018-08-01T23:34:37.815813: step 11163, loss 0.54513.
Train: 2018-08-01T23:34:37.980349: step 11164, loss 0.596995.
Train: 2018-08-01T23:34:38.148891: step 11165, loss 0.510587.
Train: 2018-08-01T23:34:38.320433: step 11166, loss 0.527825.
Train: 2018-08-01T23:34:38.489980: step 11167, loss 0.510523.
Train: 2018-08-01T23:34:38.658528: step 11168, loss 0.527814.
Train: 2018-08-01T23:34:38.825084: step 11169, loss 0.718634.
Train: 2018-08-01T23:34:38.992642: step 11170, loss 0.545087.
Test: 2018-08-01T23:34:39.531226: step 11170, loss 0.547908.
Train: 2018-08-01T23:34:39.706726: step 11171, loss 0.493128.
Train: 2018-08-01T23:34:39.876304: step 11172, loss 0.597075.
Train: 2018-08-01T23:34:40.043851: step 11173, loss 0.597072.
Train: 2018-08-01T23:34:40.210380: step 11174, loss 0.527776.
Train: 2018-08-01T23:34:40.373942: step 11175, loss 0.614365.
Train: 2018-08-01T23:34:40.555457: step 11176, loss 0.631621.
Train: 2018-08-01T23:34:40.727008: step 11177, loss 0.545138.
Train: 2018-08-01T23:34:40.890588: step 11178, loss 0.493423.
Train: 2018-08-01T23:34:41.069085: step 11179, loss 0.562406.
Train: 2018-08-01T23:34:41.236637: step 11180, loss 0.527932.
Test: 2018-08-01T23:34:41.770241: step 11180, loss 0.547969.
Train: 2018-08-01T23:34:41.939782: step 11181, loss 0.545165.
Train: 2018-08-01T23:34:42.109304: step 11182, loss 0.596906.
Train: 2018-08-01T23:34:42.280846: step 11183, loss 0.545158.
Train: 2018-08-01T23:34:42.448409: step 11184, loss 0.527905.
Train: 2018-08-01T23:34:42.612988: step 11185, loss 0.527884.
Train: 2018-08-01T23:34:42.784514: step 11186, loss 0.579689.
Train: 2018-08-01T23:34:42.947098: step 11187, loss 0.579702.
Train: 2018-08-01T23:34:43.115614: step 11188, loss 0.527815.
Train: 2018-08-01T23:34:43.284162: step 11189, loss 0.475856.
Train: 2018-08-01T23:34:43.454706: step 11190, loss 0.562416.
Test: 2018-08-01T23:34:43.993268: step 11190, loss 0.547871.
Train: 2018-08-01T23:34:44.163838: step 11191, loss 0.562421.
Train: 2018-08-01T23:34:44.340341: step 11192, loss 0.475419.
Train: 2018-08-01T23:34:44.514873: step 11193, loss 0.527542.
Train: 2018-08-01T23:34:44.691401: step 11194, loss 0.492466.
Train: 2018-08-01T23:34:44.855507: step 11195, loss 0.527352.
Train: 2018-08-01T23:34:45.026050: step 11196, loss 0.492013.
Train: 2018-08-01T23:34:45.200584: step 11197, loss 0.420991.
Train: 2018-08-01T23:34:45.372125: step 11198, loss 0.509188.
Train: 2018-08-01T23:34:45.538704: step 11199, loss 0.526829.
Train: 2018-08-01T23:34:45.704263: step 11200, loss 0.634607.
Test: 2018-08-01T23:34:46.237839: step 11200, loss 0.547591.
Train: 2018-08-01T23:34:47.007093: step 11201, loss 0.562708.
Train: 2018-08-01T23:34:47.172645: step 11202, loss 0.635182.
Train: 2018-08-01T23:34:47.339174: step 11203, loss 0.580908.
Train: 2018-08-01T23:34:47.503736: step 11204, loss 0.453837.
Train: 2018-08-01T23:34:47.668320: step 11205, loss 0.744798.
Train: 2018-08-01T23:34:47.848845: step 11206, loss 0.526432.
Train: 2018-08-01T23:34:48.016412: step 11207, loss 0.599165.
Train: 2018-08-01T23:34:48.185918: step 11208, loss 0.49013.
Train: 2018-08-01T23:34:48.359476: step 11209, loss 0.599116.
Train: 2018-08-01T23:34:48.528993: step 11210, loss 0.617231.
Test: 2018-08-01T23:34:49.066588: step 11210, loss 0.547581.
Train: 2018-08-01T23:34:49.247073: step 11211, loss 0.490267.
Train: 2018-08-01T23:34:49.412656: step 11212, loss 0.617083.
Train: 2018-08-01T23:34:49.584175: step 11213, loss 0.45422.
Train: 2018-08-01T23:34:49.761698: step 11214, loss 0.508476.
Train: 2018-08-01T23:34:49.930267: step 11215, loss 0.598926.
Train: 2018-08-01T23:34:50.094808: step 11216, loss 0.50846.
Train: 2018-08-01T23:34:50.265377: step 11217, loss 0.580839.
Train: 2018-08-01T23:34:50.438914: step 11218, loss 0.544641.
Train: 2018-08-01T23:34:50.615420: step 11219, loss 0.526542.
Train: 2018-08-01T23:34:50.788977: step 11220, loss 0.526534.
Test: 2018-08-01T23:34:51.331502: step 11220, loss 0.547582.
Train: 2018-08-01T23:34:51.510050: step 11221, loss 0.52652.
Train: 2018-08-01T23:34:51.677607: step 11222, loss 0.635291.
Train: 2018-08-01T23:34:51.847123: step 11223, loss 0.599008.
Train: 2018-08-01T23:34:52.013678: step 11224, loss 0.671384.
Train: 2018-08-01T23:34:52.182252: step 11225, loss 0.598832.
Train: 2018-08-01T23:34:52.357758: step 11226, loss 0.544671.
Train: 2018-08-01T23:34:52.524315: step 11227, loss 0.436933.
Train: 2018-08-01T23:34:52.694858: step 11228, loss 0.616475.
Train: 2018-08-01T23:34:52.873405: step 11229, loss 0.508871.
Train: 2018-08-01T23:34:53.044952: step 11230, loss 0.508907.
Test: 2018-08-01T23:34:53.571514: step 11230, loss 0.547623.
Train: 2018-08-01T23:34:53.743083: step 11231, loss 0.508913.
Train: 2018-08-01T23:34:53.907649: step 11232, loss 0.473074.
Train: 2018-08-01T23:34:54.088159: step 11233, loss 0.616442.
Train: 2018-08-01T23:34:54.268649: step 11234, loss 0.490858.
Train: 2018-08-01T23:34:54.443183: step 11235, loss 0.562653.
Train: 2018-08-01T23:34:54.612759: step 11236, loss 0.508705.
Train: 2018-08-01T23:34:54.787294: step 11237, loss 0.436589.
Train: 2018-08-01T23:34:54.951855: step 11238, loss 0.707241.
Train: 2018-08-01T23:34:55.119376: step 11239, loss 0.68924.
Train: 2018-08-01T23:34:55.293910: step 11240, loss 0.580751.
Test: 2018-08-01T23:34:55.826485: step 11240, loss 0.547597.
Train: 2018-08-01T23:34:55.998052: step 11241, loss 0.580699.
Train: 2018-08-01T23:34:56.163617: step 11242, loss 0.598621.
Train: 2018-08-01T23:34:56.329168: step 11243, loss 0.688191.
Train: 2018-08-01T23:34:56.493728: step 11244, loss 0.509001.
Train: 2018-08-01T23:34:56.659260: step 11245, loss 0.598182.
Train: 2018-08-01T23:34:56.829803: step 11246, loss 0.615782.
Train: 2018-08-01T23:34:56.987383: step 11247, loss 0.527148.
Train: 2018-08-01T23:34:57.155963: step 11248, loss 0.597732.
Train: 2018-08-01T23:34:57.322511: step 11249, loss 0.457086.
Train: 2018-08-01T23:34:57.487071: step 11250, loss 0.544922.
Test: 2018-08-01T23:34:58.028599: step 11250, loss 0.547786.
Train: 2018-08-01T23:34:58.205158: step 11251, loss 0.474895.
Train: 2018-08-01T23:34:58.371708: step 11252, loss 0.597472.
Train: 2018-08-01T23:34:58.548240: step 11253, loss 0.544946.
Train: 2018-08-01T23:34:58.720748: step 11254, loss 0.649923.
Train: 2018-08-01T23:34:58.891292: step 11255, loss 0.597373.
Train: 2018-08-01T23:34:59.055878: step 11256, loss 0.579862.
Train: 2018-08-01T23:34:59.225400: step 11257, loss 0.579817.
Train: 2018-08-01T23:34:59.396942: step 11258, loss 0.614483.
Train: 2018-08-01T23:34:59.564543: step 11259, loss 0.56241.
Train: 2018-08-01T23:34:59.735038: step 11260, loss 0.631453.
Test: 2018-08-01T23:35:00.259659: step 11260, loss 0.547996.
Train: 2018-08-01T23:35:00.428220: step 11261, loss 0.493589.
Train: 2018-08-01T23:35:00.595736: step 11262, loss 0.528064.
Train: 2018-08-01T23:35:00.759300: step 11263, loss 0.545255.
Train: 2018-08-01T23:35:00.928846: step 11264, loss 0.528141.
Train: 2018-08-01T23:35:01.096431: step 11265, loss 0.613777.
Train: 2018-08-01T23:35:01.263950: step 11266, loss 0.613724.
Train: 2018-08-01T23:35:01.441475: step 11267, loss 0.511171.
Train: 2018-08-01T23:35:01.613017: step 11268, loss 0.562405.
Train: 2018-08-01T23:35:01.782564: step 11269, loss 0.49419.
Train: 2018-08-01T23:35:01.950149: step 11270, loss 0.511219.
Test: 2018-08-01T23:35:02.478742: step 11270, loss 0.548099.
Train: 2018-08-01T23:35:02.645258: step 11271, loss 0.511155.
Train: 2018-08-01T23:35:02.812810: step 11272, loss 0.647979.
Train: 2018-08-01T23:35:02.987377: step 11273, loss 0.596647.
Train: 2018-08-01T23:35:03.155892: step 11274, loss 0.630882.
Train: 2018-08-01T23:35:03.325465: step 11275, loss 0.562402.
Train: 2018-08-01T23:35:03.493988: step 11276, loss 0.494073.
Train: 2018-08-01T23:35:03.658580: step 11277, loss 0.596573.
Train: 2018-08-01T23:35:03.827098: step 11278, loss 0.528248.
Train: 2018-08-01T23:35:04.018585: step 11279, loss 0.562403.
Train: 2018-08-01T23:35:04.191155: step 11280, loss 0.596572.
Test: 2018-08-01T23:35:04.735671: step 11280, loss 0.548103.
Train: 2018-08-01T23:35:04.907212: step 11281, loss 0.49409.
Train: 2018-08-01T23:35:05.078783: step 11282, loss 0.545312.
Train: 2018-08-01T23:35:05.251291: step 11283, loss 0.579509.
Train: 2018-08-01T23:35:05.413856: step 11284, loss 0.511049.
Train: 2018-08-01T23:35:05.596401: step 11285, loss 0.630963.
Train: 2018-08-01T23:35:05.759957: step 11286, loss 0.596688.
Train: 2018-08-01T23:35:05.932484: step 11287, loss 0.562401.
Train: 2018-08-01T23:35:06.103039: step 11288, loss 0.562401.
Train: 2018-08-01T23:35:06.271595: step 11289, loss 0.459646.
Train: 2018-08-01T23:35:06.445099: step 11290, loss 0.47666.
Test: 2018-08-01T23:35:06.977677: step 11290, loss 0.548006.
Train: 2018-08-01T23:35:07.156229: step 11291, loss 0.493638.
Train: 2018-08-01T23:35:07.324756: step 11292, loss 0.596898.
Train: 2018-08-01T23:35:07.495292: step 11293, loss 0.458681.
Train: 2018-08-01T23:35:07.662845: step 11294, loss 0.545065.
Train: 2018-08-01T23:35:07.831425: step 11295, loss 0.562426.
Train: 2018-08-01T23:35:07.998963: step 11296, loss 0.614835.
Train: 2018-08-01T23:35:08.166499: step 11297, loss 0.527447.
Train: 2018-08-01T23:35:08.341032: step 11298, loss 0.544919.
Train: 2018-08-01T23:35:08.505591: step 11299, loss 0.474589.
Train: 2018-08-01T23:35:08.682145: step 11300, loss 0.491964.
Test: 2018-08-01T23:35:09.226665: step 11300, loss 0.547694.
Train: 2018-08-01T23:35:10.024235: step 11301, loss 0.597905.
Train: 2018-08-01T23:35:10.199794: step 11302, loss 0.491554.
Train: 2018-08-01T23:35:10.367320: step 11303, loss 0.491345.
Train: 2018-08-01T23:35:10.533898: step 11304, loss 0.616216.
Train: 2018-08-01T23:35:10.711422: step 11305, loss 0.544703.
Train: 2018-08-01T23:35:10.882958: step 11306, loss 0.598575.
Train: 2018-08-01T23:35:11.050525: step 11307, loss 0.508694.
Train: 2018-08-01T23:35:11.226022: step 11308, loss 0.688871.
Train: 2018-08-01T23:35:11.398591: step 11309, loss 0.436522.
Train: 2018-08-01T23:35:11.579109: step 11310, loss 0.598804.
Test: 2018-08-01T23:35:12.115644: step 11310, loss 0.547588.
Train: 2018-08-01T23:35:12.278235: step 11311, loss 0.616897.
Train: 2018-08-01T23:35:12.445763: step 11312, loss 0.562708.
Train: 2018-08-01T23:35:12.614310: step 11313, loss 0.544656.
Train: 2018-08-01T23:35:12.774882: step 11314, loss 0.562698.
Train: 2018-08-01T23:35:12.940640: step 11315, loss 0.526628.
Train: 2018-08-01T23:35:13.114683: step 11316, loss 0.616783.
Train: 2018-08-01T23:35:13.284256: step 11317, loss 0.616718.
Train: 2018-08-01T23:35:13.452778: step 11318, loss 0.490738.
Train: 2018-08-01T23:35:13.613375: step 11319, loss 0.544684.
Train: 2018-08-01T23:35:13.779933: step 11320, loss 0.68833.
Test: 2018-08-01T23:35:14.319462: step 11320, loss 0.547619.
Train: 2018-08-01T23:35:14.492998: step 11321, loss 0.741732.
Train: 2018-08-01T23:35:14.660549: step 11322, loss 0.526921.
Train: 2018-08-01T23:35:14.832124: step 11323, loss 0.598046.
Train: 2018-08-01T23:35:14.996677: step 11324, loss 0.544826.
Train: 2018-08-01T23:35:15.191156: step 11325, loss 0.756242.
Train: 2018-08-01T23:35:15.353724: step 11326, loss 0.579954.
Train: 2018-08-01T23:35:15.519254: step 11327, loss 0.49281.
Train: 2018-08-01T23:35:15.686805: step 11328, loss 0.545084.
Train: 2018-08-01T23:35:15.854358: step 11329, loss 0.579668.
Train: 2018-08-01T23:35:16.021938: step 11330, loss 0.4764.
Test: 2018-08-01T23:35:16.534540: step 11330, loss 0.548026.
Train: 2018-08-01T23:35:16.703088: step 11331, loss 0.562401.
Train: 2018-08-01T23:35:16.868672: step 11332, loss 0.665209.
Train: 2018-08-01T23:35:17.043205: step 11333, loss 0.613644.
Train: 2018-08-01T23:35:17.210733: step 11334, loss 0.494336.
Train: 2018-08-01T23:35:17.377288: step 11335, loss 0.596374.
Train: 2018-08-01T23:35:17.550854: step 11336, loss 0.596296.
Train: 2018-08-01T23:35:17.726355: step 11337, loss 0.613104.
Train: 2018-08-01T23:35:17.891942: step 11338, loss 0.545601.
Train: 2018-08-01T23:35:18.060511: step 11339, loss 0.579246.
Train: 2018-08-01T23:35:18.226051: step 11340, loss 0.579217.
Test: 2018-08-01T23:35:18.773578: step 11340, loss 0.548482.
Train: 2018-08-01T23:35:18.941135: step 11341, loss 0.545764.
Train: 2018-08-01T23:35:19.108657: step 11342, loss 0.545806.
Train: 2018-08-01T23:35:19.278205: step 11343, loss 0.595817.
Train: 2018-08-01T23:35:19.455762: step 11344, loss 0.645675.
Train: 2018-08-01T23:35:19.624313: step 11345, loss 0.512757.
Train: 2018-08-01T23:35:19.798814: step 11346, loss 0.529407.
Train: 2018-08-01T23:35:19.972375: step 11347, loss 0.562541.
Train: 2018-08-01T23:35:20.146082: step 11348, loss 0.512909.
Train: 2018-08-01T23:35:20.311664: step 11349, loss 0.529434.
Train: 2018-08-01T23:35:20.483198: step 11350, loss 0.595677.
Test: 2018-08-01T23:35:21.018749: step 11350, loss 0.548643.
Train: 2018-08-01T23:35:21.182311: step 11351, loss 0.57911.
Train: 2018-08-01T23:35:21.350891: step 11352, loss 0.52935.
Train: 2018-08-01T23:35:21.519435: step 11353, loss 0.695352.
Train: 2018-08-01T23:35:21.684992: step 11354, loss 0.512771.
Train: 2018-08-01T23:35:21.849527: step 11355, loss 0.52936.
Train: 2018-08-01T23:35:22.012092: step 11356, loss 0.512744.
Train: 2018-08-01T23:35:22.184666: step 11357, loss 0.612363.
Train: 2018-08-01T23:35:22.348225: step 11358, loss 0.545882.
Train: 2018-08-01T23:35:22.514803: step 11359, loss 0.545861.
Train: 2018-08-01T23:35:22.677314: step 11360, loss 0.545834.
Test: 2018-08-01T23:35:23.213880: step 11360, loss 0.548514.
Train: 2018-08-01T23:35:23.381467: step 11361, loss 0.495743.
Train: 2018-08-01T23:35:23.559955: step 11362, loss 0.562472.
Train: 2018-08-01T23:35:23.724538: step 11363, loss 0.663051.
Train: 2018-08-01T23:35:23.892067: step 11364, loss 0.495359.
Train: 2018-08-01T23:35:24.060617: step 11365, loss 0.562449.
Train: 2018-08-01T23:35:24.237176: step 11366, loss 0.562442.
Train: 2018-08-01T23:35:24.406718: step 11367, loss 0.495027.
Train: 2018-08-01T23:35:24.584217: step 11368, loss 0.596214.
Train: 2018-08-01T23:35:24.757753: step 11369, loss 0.647031.
Train: 2018-08-01T23:35:24.929320: step 11370, loss 0.647053.
Test: 2018-08-01T23:35:25.476222: step 11370, loss 0.548265.
Train: 2018-08-01T23:35:25.652785: step 11371, loss 0.562424.
Train: 2018-08-01T23:35:25.824318: step 11372, loss 0.511745.
Train: 2018-08-01T23:35:25.996862: step 11373, loss 0.545532.
Train: 2018-08-01T23:35:26.160421: step 11374, loss 0.545524.
Train: 2018-08-01T23:35:26.323981: step 11375, loss 0.596247.
Train: 2018-08-01T23:35:26.496526: step 11376, loss 0.528596.
Train: 2018-08-01T23:35:26.665076: step 11377, loss 0.579346.
Train: 2018-08-01T23:35:26.833593: step 11378, loss 0.664009.
Train: 2018-08-01T23:35:27.000148: step 11379, loss 0.545512.
Train: 2018-08-01T23:35:27.169727: step 11380, loss 0.579325.
Test: 2018-08-01T23:35:27.705297: step 11380, loss 0.548291.
Train: 2018-08-01T23:35:27.869826: step 11381, loss 0.596196.
Train: 2018-08-01T23:35:28.031391: step 11382, loss 0.478125.
Train: 2018-08-01T23:35:28.193983: step 11383, loss 0.596165.
Train: 2018-08-01T23:35:28.358518: step 11384, loss 0.562434.
Train: 2018-08-01T23:35:28.523078: step 11385, loss 0.596151.
Train: 2018-08-01T23:35:28.693652: step 11386, loss 0.545589.
Train: 2018-08-01T23:35:28.867158: step 11387, loss 0.579282.
Train: 2018-08-01T23:35:29.032714: step 11388, loss 0.56244.
Train: 2018-08-01T23:35:29.203258: step 11389, loss 0.646599.
Train: 2018-08-01T23:35:29.366853: step 11390, loss 0.646477.
Test: 2018-08-01T23:35:29.900395: step 11390, loss 0.548425.
Train: 2018-08-01T23:35:30.067964: step 11391, loss 0.713313.
Train: 2018-08-01T23:35:30.232509: step 11392, loss 0.562487.
Train: 2018-08-01T23:35:30.397069: step 11393, loss 0.579128.
Train: 2018-08-01T23:35:30.560657: step 11394, loss 0.529446.
Train: 2018-08-01T23:35:30.723196: step 11395, loss 0.463558.
Train: 2018-08-01T23:35:30.890779: step 11396, loss 0.628525.
Train: 2018-08-01T23:35:31.061292: step 11397, loss 0.628429.
Train: 2018-08-01T23:35:31.223857: step 11398, loss 0.496932.
Train: 2018-08-01T23:35:31.384453: step 11399, loss 0.546216.
Train: 2018-08-01T23:35:31.554000: step 11400, loss 0.513434.
Test: 2018-08-01T23:35:32.085555: step 11400, loss 0.54888.
Train: 2018-08-01T23:35:32.843278: step 11401, loss 0.447776.
Train: 2018-08-01T23:35:33.009834: step 11402, loss 0.644836.
Train: 2018-08-01T23:35:33.173396: step 11403, loss 0.727249.
Train: 2018-08-01T23:35:33.336984: step 11404, loss 0.546158.
Train: 2018-08-01T23:35:33.499524: step 11405, loss 0.562608.
Train: 2018-08-01T23:35:33.674058: step 11406, loss 0.562615.
Train: 2018-08-01T23:35:33.844601: step 11407, loss 0.562619.
Train: 2018-08-01T23:35:34.008164: step 11408, loss 0.579022.
Train: 2018-08-01T23:35:34.172758: step 11409, loss 0.644593.
Train: 2018-08-01T23:35:34.342298: step 11410, loss 0.611746.
Test: 2018-08-01T23:35:34.884823: step 11410, loss 0.548983.
Train: 2018-08-01T23:35:35.057360: step 11411, loss 0.513657.
Train: 2018-08-01T23:35:35.232892: step 11412, loss 0.611634.
Train: 2018-08-01T23:35:35.400443: step 11413, loss 0.481194.
Train: 2018-08-01T23:35:35.568021: step 11414, loss 0.49747.
Train: 2018-08-01T23:35:35.730561: step 11415, loss 0.595322.
Train: 2018-08-01T23:35:35.898139: step 11416, loss 0.497278.
Train: 2018-08-01T23:35:36.071668: step 11417, loss 0.628153.
Train: 2018-08-01T23:35:36.238229: step 11418, loss 0.611813.
Train: 2018-08-01T23:35:36.406753: step 11419, loss 0.611824.
Train: 2018-08-01T23:35:36.576299: step 11420, loss 0.595413.
Test: 2018-08-01T23:35:37.108909: step 11420, loss 0.548915.
Train: 2018-08-01T23:35:37.276427: step 11421, loss 0.579015.
Train: 2018-08-01T23:35:37.451958: step 11422, loss 0.562641.
Train: 2018-08-01T23:35:37.614552: step 11423, loss 0.579006.
Train: 2018-08-01T23:35:37.781079: step 11424, loss 0.595352.
Train: 2018-08-01T23:35:37.953618: step 11425, loss 0.546326.
Train: 2018-08-01T23:35:38.117181: step 11426, loss 0.595324.
Train: 2018-08-01T23:35:38.280743: step 11427, loss 0.432131.
Train: 2018-08-01T23:35:38.449325: step 11428, loss 0.546309.
Train: 2018-08-01T23:35:38.611883: step 11429, loss 0.562635.
Train: 2018-08-01T23:35:38.779411: step 11430, loss 0.464152.
Test: 2018-08-01T23:35:39.301016: step 11430, loss 0.548793.
Train: 2018-08-01T23:35:39.470562: step 11431, loss 0.628458.
Train: 2018-08-01T23:35:39.640140: step 11432, loss 0.579069.
Train: 2018-08-01T23:35:39.811650: step 11433, loss 0.562549.
Train: 2018-08-01T23:35:39.984210: step 11434, loss 0.512835.
Train: 2018-08-01T23:35:40.161714: step 11435, loss 0.529298.
Train: 2018-08-01T23:35:40.342232: step 11436, loss 0.545838.
Train: 2018-08-01T23:35:40.513787: step 11437, loss 0.62931.
Train: 2018-08-01T23:35:40.683512: step 11438, loss 0.508909.
Train: 2018-08-01T23:35:40.861038: step 11439, loss 0.512118.
Train: 2018-08-01T23:35:41.032579: step 11440, loss 0.54561.
Test: 2018-08-01T23:35:41.573134: step 11440, loss 0.548291.
Train: 2018-08-01T23:35:41.737720: step 11441, loss 0.545545.
Train: 2018-08-01T23:35:41.903272: step 11442, loss 0.528545.
Train: 2018-08-01T23:35:42.072825: step 11443, loss 0.579404.
Train: 2018-08-01T23:35:42.239380: step 11444, loss 0.562405.
Train: 2018-08-01T23:35:42.404910: step 11445, loss 0.647825.
Train: 2018-08-01T23:35:42.579474: step 11446, loss 0.493995.
Train: 2018-08-01T23:35:42.746996: step 11447, loss 0.510996.
Train: 2018-08-01T23:35:42.912580: step 11448, loss 0.5624.
Train: 2018-08-01T23:35:43.087108: step 11449, loss 0.562401.
Train: 2018-08-01T23:35:43.253672: step 11450, loss 0.5279.
Test: 2018-08-01T23:35:43.788213: step 11450, loss 0.547927.
Train: 2018-08-01T23:35:43.952779: step 11451, loss 0.59699.
Train: 2018-08-01T23:35:44.123343: step 11452, loss 0.597047.
Train: 2018-08-01T23:35:44.294877: step 11453, loss 0.597077.
Train: 2018-08-01T23:35:44.466400: step 11454, loss 0.562412.
Train: 2018-08-01T23:35:44.633952: step 11455, loss 0.475718.
Train: 2018-08-01T23:35:44.798538: step 11456, loss 0.545053.
Train: 2018-08-01T23:35:44.964102: step 11457, loss 0.631973.
Train: 2018-08-01T23:35:45.133616: step 11458, loss 0.579813.
Train: 2018-08-01T23:35:45.299173: step 11459, loss 0.527639.
Train: 2018-08-01T23:35:45.468739: step 11460, loss 0.562422.
Test: 2018-08-01T23:35:45.993319: step 11460, loss 0.54785.
Train: 2018-08-01T23:35:46.157879: step 11461, loss 0.527617.
Train: 2018-08-01T23:35:46.324462: step 11462, loss 0.649504.
Train: 2018-08-01T23:35:46.487029: step 11463, loss 0.492807.
Train: 2018-08-01T23:35:46.650586: step 11464, loss 0.457962.
Train: 2018-08-01T23:35:46.824097: step 11465, loss 0.544989.
Train: 2018-08-01T23:35:46.990652: step 11466, loss 0.52749.
Train: 2018-08-01T23:35:47.155243: step 11467, loss 0.614981.
Train: 2018-08-01T23:35:47.323789: step 11468, loss 0.457275.
Train: 2018-08-01T23:35:47.489326: step 11469, loss 0.562466.
Train: 2018-08-01T23:35:47.652882: step 11470, loss 0.456827.
Test: 2018-08-01T23:35:48.185458: step 11470, loss 0.547705.
Train: 2018-08-01T23:35:48.354008: step 11471, loss 0.491833.
Train: 2018-08-01T23:35:48.532556: step 11472, loss 0.633468.
Train: 2018-08-01T23:35:48.711052: step 11473, loss 0.54477.
Train: 2018-08-01T23:35:48.883617: step 11474, loss 0.562566.
Train: 2018-08-01T23:35:49.050172: step 11475, loss 0.633986.
Train: 2018-08-01T23:35:49.216700: step 11476, loss 0.544728.
Train: 2018-08-01T23:35:49.378269: step 11477, loss 0.562595.
Train: 2018-08-01T23:35:49.546843: step 11478, loss 0.651996.
Train: 2018-08-01T23:35:49.719390: step 11479, loss 0.598312.
Train: 2018-08-01T23:35:49.889901: step 11480, loss 0.544741.
Test: 2018-08-01T23:35:50.427002: step 11480, loss 0.547648.
Train: 2018-08-01T23:35:50.594522: step 11481, loss 0.633809.
Train: 2018-08-01T23:35:50.766597: step 11482, loss 0.615857.
Train: 2018-08-01T23:35:50.933128: step 11483, loss 0.562521.
Train: 2018-08-01T23:35:51.141819: step 11484, loss 0.456478.
Train: 2018-08-01T23:35:51.310352: step 11485, loss 0.544842.
Train: 2018-08-01T23:35:51.482891: step 11486, loss 0.685952.
Train: 2018-08-01T23:35:51.648447: step 11487, loss 0.580064.
Train: 2018-08-01T23:35:51.825973: step 11488, loss 0.527367.
Train: 2018-08-01T23:35:51.991556: step 11489, loss 0.597469.
Train: 2018-08-01T23:35:52.153123: step 11490, loss 0.597376.
Test: 2018-08-01T23:35:52.680688: step 11490, loss 0.547838.
Train: 2018-08-01T23:35:52.845249: step 11491, loss 0.632115.
Train: 2018-08-01T23:35:53.016820: step 11492, loss 0.66658.
Train: 2018-08-01T23:35:53.186336: step 11493, loss 0.527849.
Train: 2018-08-01T23:35:53.352891: step 11494, loss 0.579613.
Train: 2018-08-01T23:35:53.514460: step 11495, loss 0.510955.
Train: 2018-08-01T23:35:53.678023: step 11496, loss 0.425567.
Train: 2018-08-01T23:35:53.840619: step 11497, loss 0.562401.
Train: 2018-08-01T23:35:54.004181: step 11498, loss 0.528206.
Train: 2018-08-01T23:35:54.168736: step 11499, loss 0.562401.
Train: 2018-08-01T23:35:54.336263: step 11500, loss 0.528182.
Test: 2018-08-01T23:35:54.857868: step 11500, loss 0.548061.
Train: 2018-08-01T23:35:55.641838: step 11501, loss 0.596647.
Train: 2018-08-01T23:35:55.814377: step 11502, loss 0.493889.
Train: 2018-08-01T23:35:55.981953: step 11503, loss 0.648147.
Train: 2018-08-01T23:35:56.149482: step 11504, loss 0.476667.
Train: 2018-08-01T23:35:56.320025: step 11505, loss 0.665398.
Train: 2018-08-01T23:35:56.495566: step 11506, loss 0.613872.
Train: 2018-08-01T23:35:56.667097: step 11507, loss 0.510992.
Train: 2018-08-01T23:35:56.833652: step 11508, loss 0.5624.
Train: 2018-08-01T23:35:56.999234: step 11509, loss 0.511025.
Train: 2018-08-01T23:35:57.161813: step 11510, loss 0.630935.
Test: 2018-08-01T23:35:57.695347: step 11510, loss 0.548061.
Train: 2018-08-01T23:35:57.871900: step 11511, loss 0.528152.
Train: 2018-08-01T23:35:58.042450: step 11512, loss 0.5624.
Train: 2018-08-01T23:35:58.218979: step 11513, loss 0.5624.
Train: 2018-08-01T23:35:58.382510: step 11514, loss 0.613774.
Train: 2018-08-01T23:35:58.547100: step 11515, loss 0.425506.
Train: 2018-08-01T23:35:58.709635: step 11516, loss 0.579536.
Train: 2018-08-01T23:35:58.878185: step 11517, loss 0.5624.
Train: 2018-08-01T23:35:59.048729: step 11518, loss 0.631076.
Train: 2018-08-01T23:35:59.221268: step 11519, loss 0.579565.
Train: 2018-08-01T23:35:59.400789: step 11520, loss 0.5624.
Test: 2018-08-01T23:35:59.941377: step 11520, loss 0.548038.
Train: 2018-08-01T23:36:00.109952: step 11521, loss 0.545249.
Train: 2018-08-01T23:36:00.275509: step 11522, loss 0.630992.
Train: 2018-08-01T23:36:00.439046: step 11523, loss 0.613786.
Train: 2018-08-01T23:36:00.609617: step 11524, loss 0.511108.
Train: 2018-08-01T23:36:00.775182: step 11525, loss 0.630735.
Train: 2018-08-01T23:36:00.943727: step 11526, loss 0.613563.
Train: 2018-08-01T23:36:01.119230: step 11527, loss 0.630458.
Train: 2018-08-01T23:36:01.286792: step 11528, loss 0.562415.
Train: 2018-08-01T23:36:01.511299: step 11529, loss 0.494779.
Train: 2018-08-01T23:36:01.675860: step 11530, loss 0.545541.
Test: 2018-08-01T23:36:02.203480: step 11530, loss 0.548304.
Train: 2018-08-01T23:36:02.372022: step 11531, loss 0.528691.
Train: 2018-08-01T23:36:02.536558: step 11532, loss 0.511836.
Train: 2018-08-01T23:36:02.704109: step 11533, loss 0.646807.
Train: 2018-08-01T23:36:02.870695: step 11534, loss 0.478117.
Train: 2018-08-01T23:36:03.052180: step 11535, loss 0.613056.
Train: 2018-08-01T23:36:03.226712: step 11536, loss 0.56243.
Train: 2018-08-01T23:36:03.398257: step 11537, loss 0.511809.
Train: 2018-08-01T23:36:03.567831: step 11538, loss 0.545539.
Train: 2018-08-01T23:36:03.734395: step 11539, loss 0.630048.
Train: 2018-08-01T23:36:03.902938: step 11540, loss 0.511707.
Test: 2018-08-01T23:36:04.435484: step 11540, loss 0.548253.
Train: 2018-08-01T23:36:04.605055: step 11541, loss 0.647016.
Train: 2018-08-01T23:36:04.778566: step 11542, loss 0.528603.
Train: 2018-08-01T23:36:04.941131: step 11543, loss 0.579334.
Train: 2018-08-01T23:36:05.106689: step 11544, loss 0.562423.
Train: 2018-08-01T23:36:05.269279: step 11545, loss 0.545516.
Train: 2018-08-01T23:36:05.438829: step 11546, loss 0.579334.
Train: 2018-08-01T23:36:05.609347: step 11547, loss 0.528598.
Train: 2018-08-01T23:36:05.772939: step 11548, loss 0.562421.
Train: 2018-08-01T23:36:05.939462: step 11549, loss 0.613215.
Train: 2018-08-01T23:36:06.103051: step 11550, loss 0.460851.
Test: 2018-08-01T23:36:06.638601: step 11550, loss 0.54822.
Train: 2018-08-01T23:36:06.805174: step 11551, loss 0.647177.
Train: 2018-08-01T23:36:06.975692: step 11552, loss 0.528511.
Train: 2018-08-01T23:36:07.144273: step 11553, loss 0.477603.
Train: 2018-08-01T23:36:07.314786: step 11554, loss 0.56241.
Train: 2018-08-01T23:36:07.482339: step 11555, loss 0.545383.
Train: 2018-08-01T23:36:07.651922: step 11556, loss 0.460077.
Train: 2018-08-01T23:36:07.813454: step 11557, loss 0.54529.
Train: 2018-08-01T23:36:07.975052: step 11558, loss 0.493742.
Train: 2018-08-01T23:36:08.137588: step 11559, loss 0.579633.
Train: 2018-08-01T23:36:08.305138: step 11560, loss 0.596979.
Test: 2018-08-01T23:36:08.841715: step 11560, loss 0.547901.
Train: 2018-08-01T23:36:09.003273: step 11561, loss 0.527756.
Train: 2018-08-01T23:36:09.169855: step 11562, loss 0.440811.
Train: 2018-08-01T23:36:09.334388: step 11563, loss 0.562431.
Train: 2018-08-01T23:36:09.498973: step 11564, loss 0.474915.
Train: 2018-08-01T23:36:09.681490: step 11565, loss 0.544885.
Train: 2018-08-01T23:36:09.860013: step 11566, loss 0.491867.
Train: 2018-08-01T23:36:10.034549: step 11567, loss 0.580268.
Train: 2018-08-01T23:36:10.197083: step 11568, loss 0.633786.
Train: 2018-08-01T23:36:10.366656: step 11569, loss 0.723204.
Train: 2018-08-01T23:36:10.536202: step 11570, loss 0.437693.
Test: 2018-08-01T23:36:11.066759: step 11570, loss 0.547631.
Train: 2018-08-01T23:36:11.229323: step 11571, loss 0.56259.
Train: 2018-08-01T23:36:11.395876: step 11572, loss 0.544718.
Train: 2018-08-01T23:36:11.558442: step 11573, loss 0.473104.
Train: 2018-08-01T23:36:11.726019: step 11574, loss 0.562632.
Train: 2018-08-01T23:36:11.898561: step 11575, loss 0.598587.
Train: 2018-08-01T23:36:12.070075: step 11576, loss 0.634596.
Train: 2018-08-01T23:36:12.236678: step 11577, loss 0.490747.
Train: 2018-08-01T23:36:12.421136: step 11578, loss 0.508702.
Train: 2018-08-01T23:36:12.600657: step 11579, loss 0.68871.
Train: 2018-08-01T23:36:12.789152: step 11580, loss 0.544674.
Test: 2018-08-01T23:36:13.314758: step 11580, loss 0.547603.
Train: 2018-08-01T23:36:13.477312: step 11581, loss 0.508731.
Train: 2018-08-01T23:36:13.640874: step 11582, loss 0.54468.
Train: 2018-08-01T23:36:13.814411: step 11583, loss 0.598598.
Train: 2018-08-01T23:36:13.978000: step 11584, loss 0.508763.
Train: 2018-08-01T23:36:14.138570: step 11585, loss 0.508763.
Train: 2018-08-01T23:36:14.308092: step 11586, loss 0.63454.
Train: 2018-08-01T23:36:14.472679: step 11587, loss 0.508763.
Train: 2018-08-01T23:36:14.638237: step 11588, loss 0.544684.
Train: 2018-08-01T23:36:14.803797: step 11589, loss 0.508754.
Train: 2018-08-01T23:36:14.968353: step 11590, loss 0.454791.
Test: 2018-08-01T23:36:15.506888: step 11590, loss 0.547595.
Train: 2018-08-01T23:36:15.677431: step 11591, loss 0.634731.
Train: 2018-08-01T23:36:15.841254: step 11592, loss 0.490588.
Train: 2018-08-01T23:36:16.006370: step 11593, loss 0.652951.
Train: 2018-08-01T23:36:16.172925: step 11594, loss 0.526607.
Train: 2018-08-01T23:36:16.336487: step 11595, loss 0.544653.
Train: 2018-08-01T23:36:16.501078: step 11596, loss 0.562706.
Train: 2018-08-01T23:36:16.671591: step 11597, loss 0.562707.
Train: 2018-08-01T23:36:16.840167: step 11598, loss 0.436328.
Train: 2018-08-01T23:36:17.014674: step 11599, loss 0.616965.
Train: 2018-08-01T23:36:17.187215: step 11600, loss 0.562729.
Test: 2018-08-01T23:36:17.727795: step 11600, loss 0.547582.
Train: 2018-08-01T23:36:18.554206: step 11601, loss 0.580822.
Train: 2018-08-01T23:36:18.719734: step 11602, loss 0.616986.
Train: 2018-08-01T23:36:18.901248: step 11603, loss 0.61691.
Train: 2018-08-01T23:36:19.091739: step 11604, loss 0.544659.
Train: 2018-08-01T23:36:19.265275: step 11605, loss 0.472669.
Train: 2018-08-01T23:36:19.426843: step 11606, loss 0.634635.
Train: 2018-08-01T23:36:19.589439: step 11607, loss 0.544683.
Train: 2018-08-01T23:36:19.757958: step 11608, loss 0.61646.
Train: 2018-08-01T23:36:19.921521: step 11609, loss 0.580518.
Train: 2018-08-01T23:36:20.097051: step 11610, loss 0.56259.
Test: 2018-08-01T23:36:20.624641: step 11610, loss 0.547643.
Train: 2018-08-01T23:36:20.799176: step 11611, loss 0.544744.
Train: 2018-08-01T23:36:20.968753: step 11612, loss 0.580347.
Train: 2018-08-01T23:36:21.134279: step 11613, loss 0.61581.
Train: 2018-08-01T23:36:21.297872: step 11614, loss 0.456257.
Train: 2018-08-01T23:36:21.466422: step 11615, loss 0.544817.
Train: 2018-08-01T23:36:21.632949: step 11616, loss 0.562503.
Train: 2018-08-01T23:36:21.800532: step 11617, loss 0.562497.
Train: 2018-08-01T23:36:21.976028: step 11618, loss 0.580141.
Train: 2018-08-01T23:36:22.147571: step 11619, loss 0.439071.
Train: 2018-08-01T23:36:22.318113: step 11620, loss 0.597776.
Test: 2018-08-01T23:36:22.864654: step 11620, loss 0.547715.
Train: 2018-08-01T23:36:23.127697: step 11621, loss 0.615421.
Train: 2018-08-01T23:36:23.306189: step 11622, loss 0.633001.
Train: 2018-08-01T23:36:23.471772: step 11623, loss 0.562473.
Train: 2018-08-01T23:36:23.637334: step 11624, loss 0.509766.
Train: 2018-08-01T23:36:23.812835: step 11625, loss 0.702859.
Train: 2018-08-01T23:36:23.975431: step 11626, loss 0.527446.
Train: 2018-08-01T23:36:24.140957: step 11627, loss 0.457671.
Train: 2018-08-01T23:36:24.304550: step 11628, loss 0.579884.
Train: 2018-08-01T23:36:24.471099: step 11629, loss 0.544991.
Train: 2018-08-01T23:36:24.636632: step 11630, loss 0.475278.
Test: 2018-08-01T23:36:25.166217: step 11630, loss 0.547823.
Train: 2018-08-01T23:36:25.334766: step 11631, loss 0.544987.
Train: 2018-08-01T23:36:25.502319: step 11632, loss 0.684652.
Train: 2018-08-01T23:36:25.666878: step 11633, loss 0.56243.
Train: 2018-08-01T23:36:25.831438: step 11634, loss 0.6147.
Train: 2018-08-01T23:36:25.997048: step 11635, loss 0.718973.
Train: 2018-08-01T23:36:26.169535: step 11636, loss 0.649045.
Train: 2018-08-01T23:36:26.333104: step 11637, loss 0.476185.
Train: 2018-08-01T23:36:26.508654: step 11638, loss 0.459259.
Train: 2018-08-01T23:36:26.674216: step 11639, loss 0.562399.
Train: 2018-08-01T23:36:26.844760: step 11640, loss 0.510951.
Test: 2018-08-01T23:36:27.384286: step 11640, loss 0.548042.
Train: 2018-08-01T23:36:27.552867: step 11641, loss 0.528108.
Train: 2018-08-01T23:36:27.717428: step 11642, loss 0.5967.
Train: 2018-08-01T23:36:27.888963: step 11643, loss 0.510959.
Train: 2018-08-01T23:36:28.053498: step 11644, loss 0.545243.
Train: 2018-08-01T23:36:28.217061: step 11645, loss 0.613908.
Train: 2018-08-01T23:36:28.382618: step 11646, loss 0.476559.
Train: 2018-08-01T23:36:28.546184: step 11647, loss 0.528022.
Train: 2018-08-01T23:36:28.710742: step 11648, loss 0.510751.
Train: 2018-08-01T23:36:28.871342: step 11649, loss 0.579657.
Train: 2018-08-01T23:36:29.043850: step 11650, loss 0.527837.
Test: 2018-08-01T23:36:29.578422: step 11650, loss 0.547907.
Train: 2018-08-01T23:36:29.746970: step 11651, loss 0.54509.
Train: 2018-08-01T23:36:29.914523: step 11652, loss 0.527705.
Train: 2018-08-01T23:36:30.083072: step 11653, loss 0.579815.
Train: 2018-08-01T23:36:30.247664: step 11654, loss 0.492726.
Train: 2018-08-01T23:36:30.410247: step 11655, loss 0.544967.
Train: 2018-08-01T23:36:30.575787: step 11656, loss 0.702556.
Train: 2018-08-01T23:36:30.753280: step 11657, loss 0.614996.
Train: 2018-08-01T23:36:30.921830: step 11658, loss 0.632459.
Train: 2018-08-01T23:36:31.086421: step 11659, loss 0.510018.
Train: 2018-08-01T23:36:31.252972: step 11660, loss 0.492601.
Test: 2018-08-01T23:36:31.787541: step 11660, loss 0.547812.
Train: 2018-08-01T23:36:31.953074: step 11661, loss 0.51005.
Train: 2018-08-01T23:36:32.118663: step 11662, loss 0.597392.
Train: 2018-08-01T23:36:32.287181: step 11663, loss 0.562439.
Train: 2018-08-01T23:36:32.457725: step 11664, loss 0.527474.
Train: 2018-08-01T23:36:32.623283: step 11665, loss 0.422499.
Train: 2018-08-01T23:36:32.786876: step 11666, loss 0.632599.
Train: 2018-08-01T23:36:32.955423: step 11667, loss 0.615129.
Train: 2018-08-01T23:36:33.130924: step 11668, loss 0.562461.
Train: 2018-08-01T23:36:33.294488: step 11669, loss 0.650267.
Train: 2018-08-01T23:36:33.463036: step 11670, loss 0.562455.
Test: 2018-08-01T23:36:33.994627: step 11670, loss 0.54778.
Train: 2018-08-01T23:36:34.160181: step 11671, loss 0.579966.
Train: 2018-08-01T23:36:34.326727: step 11672, loss 0.597427.
Train: 2018-08-01T23:36:34.492311: step 11673, loss 0.614814.
Train: 2018-08-01T23:36:34.660836: step 11674, loss 0.614674.
Train: 2018-08-01T23:36:34.822432: step 11675, loss 0.562415.
Train: 2018-08-01T23:36:34.988983: step 11676, loss 0.597036.
Train: 2018-08-01T23:36:35.161522: step 11677, loss 0.579663.
Train: 2018-08-01T23:36:35.335033: step 11678, loss 0.545192.
Train: 2018-08-01T23:36:35.498627: step 11679, loss 0.613891.
Train: 2018-08-01T23:36:35.666294: step 11680, loss 0.511065.
Test: 2018-08-01T23:36:36.188919: step 11680, loss 0.548101.
Train: 2018-08-01T23:36:36.352460: step 11681, loss 0.596557.
Train: 2018-08-01T23:36:36.518018: step 11682, loss 0.545366.
Train: 2018-08-01T23:36:36.674625: step 11683, loss 0.545399.
Train: 2018-08-01T23:36:36.847138: step 11684, loss 0.477479.
Train: 2018-08-01T23:36:37.018712: step 11685, loss 0.681333.
Train: 2018-08-01T23:36:37.190221: step 11686, loss 0.562414.
Train: 2018-08-01T23:36:37.358770: step 11687, loss 0.579354.
Train: 2018-08-01T23:36:37.530361: step 11688, loss 0.697707.
Train: 2018-08-01T23:36:37.693906: step 11689, loss 0.562434.
Train: 2018-08-01T23:36:37.859462: step 11690, loss 0.562448.
Test: 2018-08-01T23:36:38.394023: step 11690, loss 0.548429.
Train: 2018-08-01T23:36:38.565571: step 11691, loss 0.579217.
Train: 2018-08-01T23:36:38.733097: step 11692, loss 0.595901.
Train: 2018-08-01T23:36:38.895662: step 11693, loss 0.512493.
Train: 2018-08-01T23:36:39.061219: step 11694, loss 0.6457.
Train: 2018-08-01T23:36:39.229802: step 11695, loss 0.579116.
Train: 2018-08-01T23:36:39.399316: step 11696, loss 0.661853.
Train: 2018-08-01T23:36:39.567867: step 11697, loss 0.562571.
Train: 2018-08-01T23:36:39.738409: step 11698, loss 0.562601.
Train: 2018-08-01T23:36:39.900999: step 11699, loss 0.497076.
Train: 2018-08-01T23:36:40.070522: step 11700, loss 0.562641.
Test: 2018-08-01T23:36:40.614068: step 11700, loss 0.54896.
Train: 2018-08-01T23:36:41.433678: step 11701, loss 0.611701.
Train: 2018-08-01T23:36:41.605243: step 11702, loss 0.481045.
Train: 2018-08-01T23:36:41.771773: step 11703, loss 0.497358.
Train: 2018-08-01T23:36:41.937358: step 11704, loss 0.546301.
Train: 2018-08-01T23:36:42.111894: step 11705, loss 0.513501.
Train: 2018-08-01T23:36:42.279446: step 11706, loss 0.644703.
Train: 2018-08-01T23:36:42.449990: step 11707, loss 0.595472.
Train: 2018-08-01T23:36:42.621501: step 11708, loss 0.562595.
Train: 2018-08-01T23:36:42.784093: step 11709, loss 0.579045.
Train: 2018-08-01T23:36:42.965606: step 11710, loss 0.529658.
Test: 2018-08-01T23:36:43.509130: step 11710, loss 0.548774.
Train: 2018-08-01T23:36:43.679673: step 11711, loss 0.496649.
Train: 2018-08-01T23:36:43.846227: step 11712, loss 0.628629.
Train: 2018-08-01T23:36:44.020807: step 11713, loss 0.562548.
Train: 2018-08-01T23:36:44.193324: step 11714, loss 0.529433.
Train: 2018-08-01T23:36:44.358857: step 11715, loss 0.562527.
Train: 2018-08-01T23:36:44.523464: step 11716, loss 0.612335.
Train: 2018-08-01T23:36:44.690968: step 11717, loss 0.662223.
Train: 2018-08-01T23:36:44.855560: step 11718, loss 0.529307.
Train: 2018-08-01T23:36:45.022083: step 11719, loss 0.728545.
Train: 2018-08-01T23:36:45.189636: step 11720, loss 0.512862.
Test: 2018-08-01T23:36:45.715230: step 11720, loss 0.548703.
Train: 2018-08-01T23:36:45.878794: step 11721, loss 0.645219.
Train: 2018-08-01T23:36:46.046374: step 11722, loss 0.595554.
Train: 2018-08-01T23:36:46.213897: step 11723, loss 0.595488.
Train: 2018-08-01T23:36:46.386469: step 11724, loss 0.546219.
Train: 2018-08-01T23:36:46.559972: step 11725, loss 0.546275.
Train: 2018-08-01T23:36:46.728522: step 11726, loss 0.480947.
Train: 2018-08-01T23:36:46.903086: step 11727, loss 0.578999.
Train: 2018-08-01T23:36:47.072620: step 11728, loss 0.611693.
Train: 2018-08-01T23:36:47.253120: step 11729, loss 0.497309.
Train: 2018-08-01T23:36:47.418200: step 11730, loss 0.513602.
Test: 2018-08-01T23:36:47.963601: step 11730, loss 0.54892.
Train: 2018-08-01T23:36:48.132154: step 11731, loss 0.52988.
Train: 2018-08-01T23:36:48.307682: step 11732, loss 0.562613.
Train: 2018-08-01T23:36:48.477263: step 11733, loss 0.579041.
Train: 2018-08-01T23:36:48.648794: step 11734, loss 0.546098.
Train: 2018-08-01T23:36:48.824319: step 11735, loss 0.513024.
Train: 2018-08-01T23:36:48.992880: step 11736, loss 0.529422.
Train: 2018-08-01T23:36:49.164391: step 11737, loss 0.545904.
Train: 2018-08-01T23:36:49.333939: step 11738, loss 0.612483.
Train: 2018-08-01T23:36:49.499525: step 11739, loss 0.580293.
Train: 2018-08-01T23:36:49.675026: step 11740, loss 0.478821.
Test: 2018-08-01T23:36:50.219571: step 11740, loss 0.548402.
Train: 2018-08-01T23:36:50.407095: step 11741, loss 0.528893.
Train: 2018-08-01T23:36:50.605539: step 11742, loss 0.545603.
Train: 2018-08-01T23:36:50.803011: step 11743, loss 0.6131.
Train: 2018-08-01T23:36:50.972582: step 11744, loss 0.562419.
Train: 2018-08-01T23:36:51.135123: step 11745, loss 0.460629.
Train: 2018-08-01T23:36:51.301678: step 11746, loss 0.596451.
Train: 2018-08-01T23:36:51.469230: step 11747, loss 0.408798.
Train: 2018-08-01T23:36:51.640771: step 11748, loss 0.630988.
Train: 2018-08-01T23:36:51.811316: step 11749, loss 0.614005.
Train: 2018-08-01T23:36:51.974877: step 11750, loss 0.596876.
Test: 2018-08-01T23:36:52.517429: step 11750, loss 0.54795.
Train: 2018-08-01T23:36:52.682034: step 11751, loss 0.648703.
Train: 2018-08-01T23:36:52.846548: step 11752, loss 0.614176.
Train: 2018-08-01T23:36:53.026093: step 11753, loss 0.614127.
Train: 2018-08-01T23:36:53.192654: step 11754, loss 0.5624.
Train: 2018-08-01T23:36:53.359204: step 11755, loss 0.596777.
Train: 2018-08-01T23:36:53.525733: step 11756, loss 0.613869.
Train: 2018-08-01T23:36:53.698304: step 11757, loss 0.511055.
Train: 2018-08-01T23:36:53.864842: step 11758, loss 0.579491.
Train: 2018-08-01T23:36:54.038396: step 11759, loss 0.596531.
Train: 2018-08-01T23:36:54.205939: step 11760, loss 0.613501.
Test: 2018-08-01T23:36:54.738491: step 11760, loss 0.548182.
Train: 2018-08-01T23:36:54.910062: step 11761, loss 0.528429.
Train: 2018-08-01T23:36:55.081598: step 11762, loss 0.545451.
Train: 2018-08-01T23:36:55.249125: step 11763, loss 0.647126.
Train: 2018-08-01T23:36:55.415706: step 11764, loss 0.511718.
Train: 2018-08-01T23:36:55.583231: step 11765, loss 0.545548.
Train: 2018-08-01T23:36:55.753808: step 11766, loss 0.646763.
Train: 2018-08-01T23:36:55.929307: step 11767, loss 0.545607.
Train: 2018-08-01T23:36:56.094864: step 11768, loss 0.427986.
Train: 2018-08-01T23:36:56.258458: step 11769, loss 0.495154.
Train: 2018-08-01T23:36:56.425985: step 11770, loss 0.54558.
Test: 2018-08-01T23:36:56.955563: step 11770, loss 0.548284.
Train: 2018-08-01T23:36:57.119126: step 11771, loss 0.545538.
Train: 2018-08-01T23:36:57.289695: step 11772, loss 0.579344.
Train: 2018-08-01T23:36:57.456226: step 11773, loss 0.647184.
Train: 2018-08-01T23:36:57.624805: step 11774, loss 0.613289.
Train: 2018-08-01T23:36:57.791329: step 11775, loss 0.545465.
Train: 2018-08-01T23:36:57.951901: step 11776, loss 0.426839.
Train: 2018-08-01T23:36:58.122470: step 11777, loss 0.647316.
Train: 2018-08-01T23:36:58.289996: step 11778, loss 0.545419.
Train: 2018-08-01T23:36:58.454587: step 11779, loss 0.511396.
Train: 2018-08-01T23:36:58.622139: step 11780, loss 0.66466.
Test: 2018-08-01T23:36:59.168648: step 11780, loss 0.548148.
Train: 2018-08-01T23:36:59.336225: step 11781, loss 0.54538.
Train: 2018-08-01T23:36:59.496771: step 11782, loss 0.511322.
Train: 2018-08-01T23:36:59.658344: step 11783, loss 0.528318.
Train: 2018-08-01T23:36:59.825891: step 11784, loss 0.613602.
Train: 2018-08-01T23:36:59.998430: step 11785, loss 0.511179.
Train: 2018-08-01T23:37:00.167975: step 11786, loss 0.528211.
Train: 2018-08-01T23:37:00.329586: step 11787, loss 0.528154.
Train: 2018-08-01T23:37:00.499090: step 11788, loss 0.579558.
Train: 2018-08-01T23:37:00.666672: step 11789, loss 0.545217.
Train: 2018-08-01T23:37:00.834225: step 11790, loss 0.476343.
Test: 2018-08-01T23:37:01.377773: step 11790, loss 0.547953.
Train: 2018-08-01T23:37:01.554301: step 11791, loss 0.596924.
Train: 2018-08-01T23:37:01.719856: step 11792, loss 0.493238.
Train: 2018-08-01T23:37:01.882393: step 11793, loss 0.545075.
Train: 2018-08-01T23:37:02.061913: step 11794, loss 0.579809.
Train: 2018-08-01T23:37:02.243454: step 11795, loss 0.545006.
Train: 2018-08-01T23:37:02.412974: step 11796, loss 0.457678.
Train: 2018-08-01T23:37:02.578572: step 11797, loss 0.632527.
Train: 2018-08-01T23:37:02.742094: step 11798, loss 0.580015.
Train: 2018-08-01T23:37:02.907683: step 11799, loss 0.474586.
Train: 2018-08-01T23:37:03.072212: step 11800, loss 0.509626.
Test: 2018-08-01T23:37:03.609775: step 11800, loss 0.547707.
Train: 2018-08-01T23:37:04.417657: step 11801, loss 0.650855.
Train: 2018-08-01T23:37:04.636758: step 11802, loss 0.509437.
Train: 2018-08-01T23:37:04.806304: step 11803, loss 0.615693.
Train: 2018-08-01T23:37:04.982858: step 11804, loss 0.473856.
Train: 2018-08-01T23:37:05.154374: step 11805, loss 0.544778.
Train: 2018-08-01T23:37:05.320928: step 11806, loss 0.491371.
Train: 2018-08-01T23:37:05.485488: step 11807, loss 0.616103.
Train: 2018-08-01T23:37:05.647056: step 11808, loss 0.54473.
Train: 2018-08-01T23:37:05.826576: step 11809, loss 0.59838.
Train: 2018-08-01T23:37:06.021090: step 11810, loss 0.562611.
Test: 2018-08-01T23:37:06.563605: step 11810, loss 0.547624.
Train: 2018-08-01T23:37:06.728167: step 11811, loss 0.52681.
Train: 2018-08-01T23:37:06.890733: step 11812, loss 0.490966.
Train: 2018-08-01T23:37:07.068257: step 11813, loss 0.598516.
Train: 2018-08-01T23:37:07.235840: step 11814, loss 0.598547.
Train: 2018-08-01T23:37:07.402365: step 11815, loss 0.526742.
Train: 2018-08-01T23:37:07.565961: step 11816, loss 0.472868.
Train: 2018-08-01T23:37:07.734507: step 11817, loss 0.598621.
Train: 2018-08-01T23:37:07.899036: step 11818, loss 0.598647.
Train: 2018-08-01T23:37:08.065592: step 11819, loss 0.52669.
Train: 2018-08-01T23:37:08.243116: step 11820, loss 0.580659.
Test: 2018-08-01T23:37:08.771717: step 11820, loss 0.547603.
Train: 2018-08-01T23:37:08.951223: step 11821, loss 0.45474.
Train: 2018-08-01T23:37:09.113789: step 11822, loss 0.508654.
Train: 2018-08-01T23:37:09.283341: step 11823, loss 0.598771.
Train: 2018-08-01T23:37:09.459865: step 11824, loss 0.580758.
Train: 2018-08-01T23:37:09.627445: step 11825, loss 0.544653.
Train: 2018-08-01T23:37:09.826911: step 11826, loss 0.707216.
Train: 2018-08-01T23:37:10.023362: step 11827, loss 0.436486.
Train: 2018-08-01T23:37:10.204898: step 11828, loss 0.598746.
Train: 2018-08-01T23:37:10.391375: step 11829, loss 0.544667.
Train: 2018-08-01T23:37:10.578872: step 11830, loss 0.63469.
Test: 2018-08-01T23:37:11.119428: step 11830, loss 0.547605.
Train: 2018-08-01T23:37:11.328868: step 11831, loss 0.526708.
Train: 2018-08-01T23:37:11.487470: step 11832, loss 0.490832.
Train: 2018-08-01T23:37:11.663973: step 11833, loss 0.56264.
Train: 2018-08-01T23:37:11.843533: step 11834, loss 0.652351.
Train: 2018-08-01T23:37:12.032986: step 11835, loss 0.580531.
Train: 2018-08-01T23:37:12.208517: step 11836, loss 0.437454.
Train: 2018-08-01T23:37:12.375100: step 11837, loss 0.616225.
Train: 2018-08-01T23:37:12.544618: step 11838, loss 0.687594.
Train: 2018-08-01T23:37:12.722144: step 11839, loss 0.633796.
Train: 2018-08-01T23:37:12.910639: step 11840, loss 0.509301.
Test: 2018-08-01T23:37:13.455184: step 11840, loss 0.547693.
Train: 2018-08-01T23:37:13.627722: step 11841, loss 0.615602.
Train: 2018-08-01T23:37:13.794277: step 11842, loss 0.509571.
Train: 2018-08-01T23:37:13.962827: step 11843, loss 0.544876.
Train: 2018-08-01T23:37:14.152351: step 11844, loss 0.527332.
Train: 2018-08-01T23:37:14.320870: step 11845, loss 0.527369.
Train: 2018-08-01T23:37:14.491425: step 11846, loss 0.579986.
Train: 2018-08-01T23:37:14.668971: step 11847, loss 0.579965.
Train: 2018-08-01T23:37:14.837490: step 11848, loss 0.492467.
Train: 2018-08-01T23:37:15.006038: step 11849, loss 0.579935.
Train: 2018-08-01T23:37:15.171595: step 11850, loss 0.475018.
Test: 2018-08-01T23:37:15.700183: step 11850, loss 0.547791.
Train: 2018-08-01T23:37:15.869729: step 11851, loss 0.562445.
Train: 2018-08-01T23:37:16.043265: step 11852, loss 0.509915.
Train: 2018-08-01T23:37:16.212814: step 11853, loss 0.509852.
Train: 2018-08-01T23:37:16.377372: step 11854, loss 0.544897.
Train: 2018-08-01T23:37:16.552902: step 11855, loss 0.615273.
Train: 2018-08-01T23:37:16.722475: step 11856, loss 0.509638.
Train: 2018-08-01T23:37:16.900002: step 11857, loss 0.615405.
Train: 2018-08-01T23:37:17.067527: step 11858, loss 0.527199.
Train: 2018-08-01T23:37:17.237105: step 11859, loss 0.703775.
Train: 2018-08-01T23:37:17.410609: step 11860, loss 0.544852.
Test: 2018-08-01T23:37:17.952161: step 11860, loss 0.547729.
Train: 2018-08-01T23:37:18.120749: step 11861, loss 0.615325.
Train: 2018-08-01T23:37:18.289261: step 11862, loss 0.580051.
Train: 2018-08-01T23:37:18.465815: step 11863, loss 0.43963.
Train: 2018-08-01T23:37:18.634364: step 11864, loss 0.474732.
Train: 2018-08-01T23:37:18.803886: step 11865, loss 0.597592.
Train: 2018-08-01T23:37:18.975427: step 11866, loss 0.632751.
Train: 2018-08-01T23:37:19.149989: step 11867, loss 0.386877.
Train: 2018-08-01T23:37:19.317513: step 11868, loss 0.580061.
Train: 2018-08-01T23:37:19.491048: step 11869, loss 0.527254.
Train: 2018-08-01T23:37:19.662589: step 11870, loss 0.633049.
Test: 2018-08-01T23:37:20.186246: step 11870, loss 0.547714.
Train: 2018-08-01T23:37:20.355792: step 11871, loss 0.491909.
Train: 2018-08-01T23:37:20.526336: step 11872, loss 0.562499.
Train: 2018-08-01T23:37:20.691926: step 11873, loss 0.686292.
Train: 2018-08-01T23:37:20.873410: step 11874, loss 0.668496.
Train: 2018-08-01T23:37:21.042955: step 11875, loss 0.527238.
Train: 2018-08-01T23:37:21.209517: step 11876, loss 0.650419.
Train: 2018-08-01T23:37:21.376091: step 11877, loss 0.544917.
Train: 2018-08-01T23:37:21.542620: step 11878, loss 0.579935.
Train: 2018-08-01T23:37:21.709208: step 11879, loss 0.422857.
Train: 2018-08-01T23:37:21.878721: step 11880, loss 0.614751.
Test: 2018-08-01T23:37:22.425261: step 11880, loss 0.547838.
Train: 2018-08-01T23:37:22.604816: step 11881, loss 0.614686.
Train: 2018-08-01T23:37:22.800286: step 11882, loss 0.614583.
Train: 2018-08-01T23:37:22.970816: step 11883, loss 0.579757.
Train: 2018-08-01T23:37:23.150349: step 11884, loss 0.579708.
Train: 2018-08-01T23:37:23.314908: step 11885, loss 0.631428.
Train: 2018-08-01T23:37:23.482434: step 11886, loss 0.493607.
Train: 2018-08-01T23:37:23.644033: step 11887, loss 0.562399.
Train: 2018-08-01T23:37:23.844466: step 11888, loss 0.545267.
Train: 2018-08-01T23:37:24.020031: step 11889, loss 0.545291.
Train: 2018-08-01T23:37:24.188547: step 11890, loss 0.476941.
Test: 2018-08-01T23:37:24.716199: step 11890, loss 0.548081.
Train: 2018-08-01T23:37:24.885395: step 11891, loss 0.476903.
Train: 2018-08-01T23:37:25.052321: step 11892, loss 0.596656.
Train: 2018-08-01T23:37:25.221074: step 11893, loss 0.630983.
Train: 2018-08-01T23:37:25.389623: step 11894, loss 0.476679.
Train: 2018-08-01T23:37:25.556177: step 11895, loss 0.579563.
Train: 2018-08-01T23:37:25.732706: step 11896, loss 0.562399.
Train: 2018-08-01T23:37:25.908267: step 11897, loss 0.699924.
Train: 2018-08-01T23:37:26.077814: step 11898, loss 0.493734.
Train: 2018-08-01T23:37:26.241345: step 11899, loss 0.613885.
Train: 2018-08-01T23:37:26.410893: step 11900, loss 0.408096.
Test: 2018-08-01T23:37:26.953443: step 11900, loss 0.548022.
Train: 2018-08-01T23:37:27.772911: step 11901, loss 0.596737.
Train: 2018-08-01T23:37:27.946415: step 11902, loss 0.510852.
Train: 2018-08-01T23:37:28.114966: step 11903, loss 0.614022.
Train: 2018-08-01T23:37:28.289530: step 11904, loss 0.476318.
Train: 2018-08-01T23:37:28.464064: step 11905, loss 0.579648.
Train: 2018-08-01T23:37:28.631585: step 11906, loss 0.579672.
Train: 2018-08-01T23:37:28.798139: step 11907, loss 0.527837.
Train: 2018-08-01T23:37:28.972676: step 11908, loss 0.545101.
Train: 2018-08-01T23:37:29.153222: step 11909, loss 0.52775.
Train: 2018-08-01T23:37:29.323760: step 11910, loss 0.562414.
Test: 2018-08-01T23:37:29.864289: step 11910, loss 0.54786.
Train: 2018-08-01T23:37:30.034833: step 11911, loss 0.597191.
Train: 2018-08-01T23:37:30.209366: step 11912, loss 0.562421.
Train: 2018-08-01T23:37:30.376918: step 11913, loss 0.666894.
Train: 2018-08-01T23:37:30.544504: step 11914, loss 0.475446.
Train: 2018-08-01T23:37:30.720033: step 11915, loss 0.54502.
Train: 2018-08-01T23:37:30.896529: step 11916, loss 0.597243.
Train: 2018-08-01T23:37:31.074055: step 11917, loss 0.632057.
Train: 2018-08-01T23:37:31.249587: step 11918, loss 0.614584.
Train: 2018-08-01T23:37:31.430102: step 11919, loss 0.57977.
Train: 2018-08-01T23:37:31.600678: step 11920, loss 0.510442.
Test: 2018-08-01T23:37:32.151005: step 11920, loss 0.547916.
Train: 2018-08-01T23:37:32.323071: step 11921, loss 0.527798.
Train: 2018-08-01T23:37:32.491610: step 11922, loss 0.527813.
Train: 2018-08-01T23:37:32.656192: step 11923, loss 0.614301.
Train: 2018-08-01T23:37:32.839673: step 11924, loss 0.631548.
Train: 2018-08-01T23:37:33.007224: step 11925, loss 0.614171.
Train: 2018-08-01T23:37:33.175775: step 11926, loss 0.545184.
Train: 2018-08-01T23:37:33.345321: step 11927, loss 0.545215.
Train: 2018-08-01T23:37:33.519880: step 11928, loss 0.579558.
Train: 2018-08-01T23:37:33.685412: step 11929, loss 0.630931.
Train: 2018-08-01T23:37:33.853988: step 11930, loss 0.596583.
Test: 2018-08-01T23:37:34.391523: step 11930, loss 0.548129.
Train: 2018-08-01T23:37:34.559108: step 11931, loss 0.528312.
Train: 2018-08-01T23:37:34.723667: step 11932, loss 0.528379.
Train: 2018-08-01T23:37:34.889195: step 11933, loss 0.61339.
Train: 2018-08-01T23:37:35.060771: step 11934, loss 0.562413.
Train: 2018-08-01T23:37:35.239272: step 11935, loss 0.613227.
Train: 2018-08-01T23:37:35.410831: step 11936, loss 0.630024.
Train: 2018-08-01T23:37:35.576386: step 11937, loss 0.629835.
Train: 2018-08-01T23:37:35.755877: step 11938, loss 0.545662.
Train: 2018-08-01T23:37:35.927419: step 11939, loss 0.612681.
Train: 2018-08-01T23:37:36.105968: step 11940, loss 0.662576.
Test: 2018-08-01T23:37:36.645527: step 11940, loss 0.548611.
Train: 2018-08-01T23:37:36.809088: step 11941, loss 0.579121.
Train: 2018-08-01T23:37:36.977631: step 11942, loss 0.529479.
Train: 2018-08-01T23:37:37.143169: step 11943, loss 0.595537.
Train: 2018-08-01T23:37:37.315708: step 11944, loss 0.595459.
Train: 2018-08-01T23:37:37.483260: step 11945, loss 0.529893.
Train: 2018-08-01T23:37:37.662799: step 11946, loss 0.562661.
Train: 2018-08-01T23:37:37.847286: step 11947, loss 0.530077.
Train: 2018-08-01T23:37:38.015845: step 11948, loss 0.562692.
Train: 2018-08-01T23:37:38.185319: step 11949, loss 0.595249.
Train: 2018-08-01T23:37:38.371846: step 11950, loss 0.481423.
Test: 2018-08-01T23:37:38.909382: step 11950, loss 0.549082.
Train: 2018-08-01T23:37:39.077958: step 11951, loss 0.578973.
Train: 2018-08-01T23:37:39.246482: step 11952, loss 0.546422.
Train: 2018-08-01T23:37:39.415063: step 11953, loss 0.513814.
Train: 2018-08-01T23:37:39.587596: step 11954, loss 0.644276.
Train: 2018-08-01T23:37:39.758135: step 11955, loss 0.546335.
Train: 2018-08-01T23:37:39.937666: step 11956, loss 0.529969.
Train: 2018-08-01T23:37:40.105186: step 11957, loss 0.660844.
Train: 2018-08-01T23:37:40.281745: step 11958, loss 0.595372.
Train: 2018-08-01T23:37:40.452258: step 11959, loss 0.562646.
Train: 2018-08-01T23:37:40.616844: step 11960, loss 0.579002.
Test: 2018-08-01T23:37:41.154382: step 11960, loss 0.548963.
Train: 2018-08-01T23:37:41.332925: step 11961, loss 0.595346.
Train: 2018-08-01T23:37:41.506471: step 11962, loss 0.660675.
Train: 2018-08-01T23:37:41.678014: step 11963, loss 0.611587.
Train: 2018-08-01T23:37:41.842593: step 11964, loss 0.530189.
Train: 2018-08-01T23:37:42.019098: step 11965, loss 0.595199.
Train: 2018-08-01T23:37:42.189614: step 11966, loss 0.627577.
Train: 2018-08-01T23:37:42.366143: step 11967, loss 0.546611.
Train: 2018-08-01T23:37:42.545691: step 11968, loss 0.465965.
Train: 2018-08-01T23:37:42.715244: step 11969, loss 0.433641.
Train: 2018-08-01T23:37:42.889774: step 11970, loss 0.546574.
Test: 2018-08-01T23:37:43.430298: step 11970, loss 0.549132.
Train: 2018-08-01T23:37:43.598876: step 11971, loss 0.546492.
Train: 2018-08-01T23:37:43.770420: step 11972, loss 0.627836.
Train: 2018-08-01T23:37:43.948911: step 11973, loss 0.578988.
Train: 2018-08-01T23:37:44.126468: step 11974, loss 0.464602.
Train: 2018-08-01T23:37:44.296013: step 11975, loss 0.447857.
Train: 2018-08-01T23:37:44.473535: step 11976, loss 0.644959.
Train: 2018-08-01T23:37:44.646073: step 11977, loss 0.479902.
Train: 2018-08-01T23:37:44.821604: step 11978, loss 0.562517.
Train: 2018-08-01T23:37:44.988166: step 11979, loss 0.495814.
Train: 2018-08-01T23:37:45.154722: step 11980, loss 0.445221.
Test: 2018-08-01T23:37:45.702224: step 11980, loss 0.548321.
Train: 2018-08-01T23:37:45.878752: step 11981, loss 0.562434.
Train: 2018-08-01T23:37:46.043342: step 11982, loss 0.579363.
Train: 2018-08-01T23:37:46.207902: step 11983, loss 0.528345.
Train: 2018-08-01T23:37:46.386395: step 11984, loss 0.682186.
Train: 2018-08-01T23:37:46.555942: step 11985, loss 0.459462.
Train: 2018-08-01T23:37:46.733498: step 11986, loss 0.682952.
Train: 2018-08-01T23:37:46.899031: step 11987, loss 0.579651.
Train: 2018-08-01T23:37:47.070567: step 11988, loss 0.424244.
Train: 2018-08-01T23:37:47.237150: step 11989, loss 0.614378.
Train: 2018-08-01T23:37:47.405701: step 11990, loss 0.492984.
Test: 2018-08-01T23:37:47.945228: step 11990, loss 0.547847.
Train: 2018-08-01T23:37:48.108822: step 11991, loss 0.649447.
Train: 2018-08-01T23:37:48.277371: step 11992, loss 0.614701.
Train: 2018-08-01T23:37:48.445890: step 11993, loss 0.544997.
Train: 2018-08-01T23:37:48.609480: step 11994, loss 0.632171.
Train: 2018-08-01T23:37:48.781991: step 11995, loss 0.527579.
Train: 2018-08-01T23:37:48.953558: step 11996, loss 0.562425.
Train: 2018-08-01T23:37:49.120121: step 11997, loss 0.57984.
Train: 2018-08-01T23:37:49.289665: step 11998, loss 0.510199.
Train: 2018-08-01T23:37:49.461206: step 11999, loss 0.562423.
Train: 2018-08-01T23:37:49.633715: step 12000, loss 0.666916.
Test: 2018-08-01T23:37:50.161304: step 12000, loss 0.547856.
Train: 2018-08-01T23:37:51.046987: step 12001, loss 0.562419.
Train: 2018-08-01T23:37:51.221516: step 12002, loss 0.579786.
Train: 2018-08-01T23:37:51.387051: step 12003, loss 0.493023.
Train: 2018-08-01T23:37:51.554595: step 12004, loss 0.649119.
Train: 2018-08-01T23:37:51.721182: step 12005, loss 0.45851.
Train: 2018-08-01T23:37:51.885709: step 12006, loss 0.510457.
Train: 2018-08-01T23:37:52.049273: step 12007, loss 0.510423.
Train: 2018-08-01T23:37:52.217823: step 12008, loss 0.614465.
Train: 2018-08-01T23:37:52.389364: step 12009, loss 0.527696.
Train: 2018-08-01T23:37:52.560905: step 12010, loss 0.59716.
Test: 2018-08-01T23:37:53.100464: step 12010, loss 0.547865.
Train: 2018-08-01T23:37:53.268041: step 12011, loss 0.666674.
Train: 2018-08-01T23:37:53.440554: step 12012, loss 0.597123.
Train: 2018-08-01T23:37:53.605114: step 12013, loss 0.579737.
Train: 2018-08-01T23:37:53.777653: step 12014, loss 0.510509.
Train: 2018-08-01T23:37:53.944207: step 12015, loss 0.527836.
Train: 2018-08-01T23:37:54.109796: step 12016, loss 0.545125.
Train: 2018-08-01T23:37:54.275322: step 12017, loss 0.648786.
Train: 2018-08-01T23:37:54.445865: step 12018, loss 0.562401.
Train: 2018-08-01T23:37:54.617407: step 12019, loss 0.5624.
Train: 2018-08-01T23:37:54.786965: step 12020, loss 0.596833.
Test: 2018-08-01T23:37:55.321532: step 12020, loss 0.548001.
Train: 2018-08-01T23:37:55.489108: step 12021, loss 0.528013.
Train: 2018-08-01T23:37:55.654635: step 12022, loss 0.682647.
Train: 2018-08-01T23:37:55.823211: step 12023, loss 0.528122.
Train: 2018-08-01T23:37:55.988750: step 12024, loss 0.511071.
Train: 2018-08-01T23:37:56.154319: step 12025, loss 0.545303.
Train: 2018-08-01T23:37:56.321850: step 12026, loss 0.545312.
Train: 2018-08-01T23:37:56.486411: step 12027, loss 0.613652.
Train: 2018-08-01T23:37:56.647994: step 12028, loss 0.57947.
Train: 2018-08-01T23:37:56.815564: step 12029, loss 0.511243.
Train: 2018-08-01T23:37:56.984112: step 12030, loss 0.596502.
Test: 2018-08-01T23:37:57.511669: step 12030, loss 0.548134.
Train: 2018-08-01T23:37:57.679249: step 12031, loss 0.511283.
Train: 2018-08-01T23:37:57.863730: step 12032, loss 0.528316.
Train: 2018-08-01T23:37:58.023332: step 12033, loss 0.579455.
Train: 2018-08-01T23:37:58.198834: step 12034, loss 0.562401.
Train: 2018-08-01T23:37:58.365389: step 12035, loss 0.545334.
Train: 2018-08-01T23:37:58.531943: step 12036, loss 0.5624.
Train: 2018-08-01T23:37:58.706510: step 12037, loss 0.562399.
Train: 2018-08-01T23:37:58.874062: step 12038, loss 0.579493.
Train: 2018-08-01T23:37:59.044572: step 12039, loss 0.511106.
Train: 2018-08-01T23:37:59.210161: step 12040, loss 0.544144.
Test: 2018-08-01T23:37:59.749688: step 12040, loss 0.548052.
Train: 2018-08-01T23:37:59.918240: step 12041, loss 0.562397.
Train: 2018-08-01T23:38:00.088780: step 12042, loss 0.648138.
Train: 2018-08-01T23:38:00.252370: step 12043, loss 0.596683.
Train: 2018-08-01T23:38:00.413938: step 12044, loss 0.459614.
Train: 2018-08-01T23:38:00.575480: step 12045, loss 0.442394.
Train: 2018-08-01T23:38:00.748045: step 12046, loss 0.562397.
Train: 2018-08-01T23:38:00.914574: step 12047, loss 0.614055.
Train: 2018-08-01T23:38:01.079134: step 12048, loss 0.493446.
Train: 2018-08-01T23:38:01.248707: step 12049, loss 0.596947.
Train: 2018-08-01T23:38:01.415266: step 12050, loss 0.510522.
Test: 2018-08-01T23:38:01.974740: step 12050, loss 0.5479.
Train: 2018-08-01T23:38:02.149273: step 12051, loss 0.597059.
Train: 2018-08-01T23:38:02.316856: step 12052, loss 0.45834.
Train: 2018-08-01T23:38:02.485401: step 12053, loss 0.631962.
Train: 2018-08-01T23:38:02.656916: step 12054, loss 0.527608.
Train: 2018-08-01T23:38:02.828488: step 12055, loss 0.527562.
Train: 2018-08-01T23:38:03.000995: step 12056, loss 0.527508.
Train: 2018-08-01T23:38:03.177524: step 12057, loss 0.492456.
Train: 2018-08-01T23:38:03.344105: step 12058, loss 0.650165.
Train: 2018-08-01T23:38:03.508665: step 12059, loss 0.580022.
Train: 2018-08-01T23:38:03.671235: step 12060, loss 0.439439.
Test: 2018-08-01T23:38:04.197797: step 12060, loss 0.547728.
Train: 2018-08-01T23:38:04.370362: step 12061, loss 0.509637.
Train: 2018-08-01T23:38:04.535923: step 12062, loss 0.615468.
Train: 2018-08-01T23:38:04.702479: step 12063, loss 0.562504.
Train: 2018-08-01T23:38:04.871024: step 12064, loss 0.633345.
Train: 2018-08-01T23:38:05.032592: step 12065, loss 0.615646.
Train: 2018-08-01T23:38:05.198154: step 12066, loss 0.580209.
Train: 2018-08-01T23:38:05.366672: step 12067, loss 0.59787.
Train: 2018-08-01T23:38:05.531232: step 12068, loss 0.421218.
Train: 2018-08-01T23:38:05.693829: step 12069, loss 0.580166.
Train: 2018-08-01T23:38:05.867334: step 12070, loss 0.59784.
Test: 2018-08-01T23:38:06.400907: step 12070, loss 0.547704.
Train: 2018-08-01T23:38:06.578463: step 12071, loss 0.544832.
Train: 2018-08-01T23:38:06.751969: step 12072, loss 0.615473.
Train: 2018-08-01T23:38:06.915532: step 12073, loss 0.50956.
Train: 2018-08-01T23:38:07.085078: step 12074, loss 0.456661.
Train: 2018-08-01T23:38:07.262603: step 12075, loss 0.562492.
Train: 2018-08-01T23:38:07.427190: step 12076, loss 0.491817.
Train: 2018-08-01T23:38:07.591724: step 12077, loss 0.562509.
Train: 2018-08-01T23:38:07.771243: step 12078, loss 0.491633.
Train: 2018-08-01T23:38:07.940793: step 12079, loss 0.562535.
Train: 2018-08-01T23:38:08.108387: step 12080, loss 0.526975.
Test: 2018-08-01T23:38:08.644934: step 12080, loss 0.547643.
Train: 2018-08-01T23:38:08.815453: step 12081, loss 0.544745.
Train: 2018-08-01T23:38:08.983038: step 12082, loss 0.54473.
Train: 2018-08-01T23:38:09.146567: step 12083, loss 0.544716.
Train: 2018-08-01T23:38:09.316115: step 12084, loss 0.634259.
Train: 2018-08-01T23:38:09.495634: step 12085, loss 0.52678.
Train: 2018-08-01T23:38:09.657234: step 12086, loss 0.59849.
Train: 2018-08-01T23:38:09.819799: step 12087, loss 0.490894.
Train: 2018-08-01T23:38:09.985325: step 12088, loss 0.598532.
Train: 2018-08-01T23:38:10.151912: step 12089, loss 0.526736.
Train: 2018-08-01T23:38:10.318463: step 12090, loss 0.670397.
Test: 2018-08-01T23:38:10.842061: step 12090, loss 0.547608.
Train: 2018-08-01T23:38:11.003634: step 12091, loss 0.526751.
Train: 2018-08-01T23:38:11.170190: step 12092, loss 0.70605.
Train: 2018-08-01T23:38:11.333735: step 12093, loss 0.526832.
Train: 2018-08-01T23:38:11.509283: step 12094, loss 0.56258.
Train: 2018-08-01T23:38:11.675807: step 12095, loss 0.4913.
Train: 2018-08-01T23:38:11.844355: step 12096, loss 0.615954.
Train: 2018-08-01T23:38:12.003960: step 12097, loss 0.509227.
Train: 2018-08-01T23:38:12.172514: step 12098, loss 0.633559.
Train: 2018-08-01T23:38:12.335075: step 12099, loss 0.56252.
Train: 2018-08-01T23:38:12.496646: step 12100, loss 0.580202.
Test: 2018-08-01T23:38:13.033177: step 12100, loss 0.547705.
Train: 2018-08-01T23:38:13.851269: step 12101, loss 0.703785.
Train: 2018-08-01T23:38:14.014831: step 12102, loss 0.527275.
Train: 2018-08-01T23:38:14.180388: step 12103, loss 0.632648.
Train: 2018-08-01T23:38:14.350959: step 12104, loss 0.562438.
Train: 2018-08-01T23:38:14.516514: step 12105, loss 0.614711.
Train: 2018-08-01T23:38:14.699003: step 12106, loss 0.597142.
Train: 2018-08-01T23:38:14.863562: step 12107, loss 0.631595.
Train: 2018-08-01T23:38:15.028122: step 12108, loss 0.527956.
Train: 2018-08-01T23:38:15.194677: step 12109, loss 0.579557.
Train: 2018-08-01T23:38:15.358240: step 12110, loss 0.596601.
Test: 2018-08-01T23:38:15.886853: step 12110, loss 0.548133.
Train: 2018-08-01T23:38:16.058399: step 12111, loss 0.528321.
Train: 2018-08-01T23:38:16.226918: step 12112, loss 0.528419.
Train: 2018-08-01T23:38:16.393473: step 12113, loss 0.494568.
Train: 2018-08-01T23:38:16.558032: step 12114, loss 0.613257.
Train: 2018-08-01T23:38:16.728596: step 12115, loss 0.511645.
Train: 2018-08-01T23:38:16.891142: step 12116, loss 0.511671.
Train: 2018-08-01T23:38:17.055703: step 12117, loss 0.630104.
Train: 2018-08-01T23:38:17.216277: step 12118, loss 0.545509.
Train: 2018-08-01T23:38:17.378837: step 12119, loss 0.579328.
Train: 2018-08-01T23:38:17.539435: step 12120, loss 0.461029.
Test: 2018-08-01T23:38:18.066026: step 12120, loss 0.548253.
Train: 2018-08-01T23:38:18.229592: step 12121, loss 0.579336.
Train: 2018-08-01T23:38:18.400163: step 12122, loss 0.477765.
Train: 2018-08-01T23:38:18.567659: step 12123, loss 0.511523.
Train: 2018-08-01T23:38:18.737238: step 12124, loss 0.545401.
Train: 2018-08-01T23:38:18.899771: step 12125, loss 0.562402.
Train: 2018-08-01T23:38:19.069350: step 12126, loss 0.59657.
Train: 2018-08-01T23:38:19.234877: step 12127, loss 0.545285.
Train: 2018-08-01T23:38:19.409427: step 12128, loss 0.493835.
Train: 2018-08-01T23:38:19.577959: step 12129, loss 0.613944.
Train: 2018-08-01T23:38:19.743548: step 12130, loss 0.562398.
Test: 2018-08-01T23:38:20.266121: step 12130, loss 0.547971.
Train: 2018-08-01T23:38:20.438690: step 12131, loss 0.493479.
Train: 2018-08-01T23:38:20.642030: step 12132, loss 0.527869.
Train: 2018-08-01T23:38:20.825542: step 12133, loss 0.579712.
Train: 2018-08-01T23:38:20.981572: step 12134, loss 0.510395.
Train: 2018-08-01T23:38:21.158632: step 12135, loss 0.51028.
Train: 2018-08-01T23:38:21.318205: step 12136, loss 0.544999.
Train: 2018-08-01T23:38:21.485790: step 12137, loss 0.544963.
Train: 2018-08-01T23:38:21.655304: step 12138, loss 0.579963.
Train: 2018-08-01T23:38:21.821877: step 12139, loss 0.562457.
Train: 2018-08-01T23:38:21.994373: step 12140, loss 0.632793.
Test: 2018-08-01T23:38:22.537017: step 12140, loss 0.547738.
Train: 2018-08-01T23:38:22.712549: step 12141, loss 0.668018.
Train: 2018-08-01T23:38:22.900046: step 12142, loss 0.474591.
Train: 2018-08-01T23:38:23.072587: step 12143, loss 0.544887.
Train: 2018-08-01T23:38:23.255122: step 12144, loss 0.632805.
Train: 2018-08-01T23:38:23.419659: step 12145, loss 0.54489.
Train: 2018-08-01T23:38:23.600174: step 12146, loss 0.527331.
Train: 2018-08-01T23:38:23.774709: step 12147, loss 0.650282.
Train: 2018-08-01T23:38:23.966196: step 12148, loss 0.544911.
Train: 2018-08-01T23:38:24.128815: step 12149, loss 0.562449.
Train: 2018-08-01T23:38:24.297311: step 12150, loss 0.614973.
Test: 2018-08-01T23:38:24.845844: step 12150, loss 0.547798.
Train: 2018-08-01T23:38:25.081240: step 12151, loss 0.527474.
Train: 2018-08-01T23:38:25.252756: step 12152, loss 0.614825.
Train: 2018-08-01T23:38:25.424330: step 12153, loss 0.562427.
Train: 2018-08-01T23:38:25.588885: step 12154, loss 0.510199.
Train: 2018-08-01T23:38:25.749428: step 12155, loss 0.510236.
Train: 2018-08-01T23:38:25.918010: step 12156, loss 0.597206.
Train: 2018-08-01T23:38:26.083562: step 12157, loss 0.510263.
Train: 2018-08-01T23:38:26.245104: step 12158, loss 0.527642.
Train: 2018-08-01T23:38:26.411684: step 12159, loss 0.440632.
Train: 2018-08-01T23:38:26.575248: step 12160, loss 0.510122.
Test: 2018-08-01T23:38:27.102811: step 12160, loss 0.5478.
Train: 2018-08-01T23:38:27.269366: step 12161, loss 0.562437.
Train: 2018-08-01T23:38:27.434923: step 12162, loss 0.615001.
Train: 2018-08-01T23:38:27.609456: step 12163, loss 0.615072.
Train: 2018-08-01T23:38:27.767066: step 12164, loss 0.597547.
Train: 2018-08-01T23:38:27.927640: step 12165, loss 0.667708.
Train: 2018-08-01T23:38:28.093192: step 12166, loss 0.579959.
Train: 2018-08-01T23:38:28.253735: step 12167, loss 0.475024.
Train: 2018-08-01T23:38:28.424303: step 12168, loss 0.510013.
Train: 2018-08-01T23:38:28.593856: step 12169, loss 0.562436.
Train: 2018-08-01T23:38:28.757388: step 12170, loss 0.684792.
Test: 2018-08-01T23:38:29.274007: step 12170, loss 0.547816.
Train: 2018-08-01T23:38:29.440586: step 12171, loss 0.544978.
Train: 2018-08-01T23:38:29.602129: step 12172, loss 0.614717.
Train: 2018-08-01T23:38:29.764721: step 12173, loss 0.649409.
Train: 2018-08-01T23:38:29.926295: step 12174, loss 0.562411.
Train: 2018-08-01T23:38:30.100828: step 12175, loss 0.5278.
Train: 2018-08-01T23:38:30.264360: step 12176, loss 0.510595.
Train: 2018-08-01T23:38:30.437895: step 12177, loss 0.458897.
Train: 2018-08-01T23:38:30.602456: step 12178, loss 0.510627.
Train: 2018-08-01T23:38:30.783004: step 12179, loss 0.596955.
Train: 2018-08-01T23:38:30.947558: step 12180, loss 0.648826.
Test: 2018-08-01T23:38:31.477117: step 12180, loss 0.547939.
Train: 2018-08-01T23:38:31.639714: step 12181, loss 0.527859.
Train: 2018-08-01T23:38:31.812222: step 12182, loss 0.5106.
Train: 2018-08-01T23:38:31.977779: step 12183, loss 0.562402.
Train: 2018-08-01T23:38:32.143362: step 12184, loss 0.458711.
Train: 2018-08-01T23:38:32.315900: step 12185, loss 0.579718.
Train: 2018-08-01T23:38:32.482430: step 12186, loss 0.562409.
Train: 2018-08-01T23:38:32.654990: step 12187, loss 0.68389.
Train: 2018-08-01T23:38:32.817534: step 12188, loss 0.597096.
Train: 2018-08-01T23:38:33.010021: step 12189, loss 0.545083.
Train: 2018-08-01T23:38:33.195560: step 12190, loss 0.614339.
Test: 2018-08-01T23:38:33.730095: step 12190, loss 0.547928.
Train: 2018-08-01T23:38:33.897646: step 12191, loss 0.545117.
Train: 2018-08-01T23:38:34.062231: step 12192, loss 0.596937.
Train: 2018-08-01T23:38:34.243723: step 12193, loss 0.631371.
Train: 2018-08-01T23:38:34.434244: step 12194, loss 0.476378.
Train: 2018-08-01T23:38:34.607763: step 12195, loss 0.52802.
Train: 2018-08-01T23:38:34.774303: step 12196, loss 0.493664.
Train: 2018-08-01T23:38:34.936869: step 12197, loss 0.631175.
Train: 2018-08-01T23:38:35.102436: step 12198, loss 0.648338.
Train: 2018-08-01T23:38:35.266022: step 12199, loss 0.528073.
Train: 2018-08-01T23:38:35.431571: step 12200, loss 0.579544.
Test: 2018-08-01T23:38:35.958139: step 12200, loss 0.548054.
Train: 2018-08-01T23:38:36.718967: step 12201, loss 0.545268.
Train: 2018-08-01T23:38:36.920430: step 12202, loss 0.562397.
Train: 2018-08-01T23:38:37.078009: step 12203, loss 0.613718.
Train: 2018-08-01T23:38:37.240574: step 12204, loss 0.596569.
Train: 2018-08-01T23:38:37.406157: step 12205, loss 0.630632.
Train: 2018-08-01T23:38:37.571689: step 12206, loss 0.460298.
Train: 2018-08-01T23:38:37.738274: step 12207, loss 0.596418.
Train: 2018-08-01T23:38:37.904827: step 12208, loss 0.579396.
Train: 2018-08-01T23:38:38.064400: step 12209, loss 0.596347.
Train: 2018-08-01T23:38:38.243892: step 12210, loss 0.562414.
Test: 2018-08-01T23:38:38.781454: step 12210, loss 0.548249.
Train: 2018-08-01T23:38:38.944020: step 12211, loss 0.59626.
Train: 2018-08-01T23:38:39.103612: step 12212, loss 0.511743.
Train: 2018-08-01T23:38:39.294085: step 12213, loss 0.562426.
Train: 2018-08-01T23:38:39.470646: step 12214, loss 0.528683.
Train: 2018-08-01T23:38:39.637198: step 12215, loss 0.613046.
Train: 2018-08-01T23:38:39.802725: step 12216, loss 0.545568.
Train: 2018-08-01T23:38:39.969310: step 12217, loss 0.562431.
Train: 2018-08-01T23:38:40.147836: step 12218, loss 0.6467.
Train: 2018-08-01T23:38:40.313359: step 12219, loss 0.579269.
Train: 2018-08-01T23:38:40.481908: step 12220, loss 0.612867.
Test: 2018-08-01T23:38:41.004538: step 12220, loss 0.548405.
Train: 2018-08-01T23:38:41.170100: step 12221, loss 0.579229.
Train: 2018-08-01T23:38:41.346598: step 12222, loss 0.612695.
Train: 2018-08-01T23:38:41.521131: step 12223, loss 0.545771.
Train: 2018-08-01T23:38:41.688682: step 12224, loss 0.612511.
Train: 2018-08-01T23:38:41.846261: step 12225, loss 0.595776.
Train: 2018-08-01T23:38:42.011818: step 12226, loss 0.512727.
Train: 2018-08-01T23:38:42.169398: step 12227, loss 0.529379.
Train: 2018-08-01T23:38:42.343931: step 12228, loss 0.562533.
Train: 2018-08-01T23:38:42.505500: step 12229, loss 0.479751.
Train: 2018-08-01T23:38:42.677040: step 12230, loss 0.579101.
Test: 2018-08-01T23:38:43.200678: step 12230, loss 0.548636.
Train: 2018-08-01T23:38:43.364234: step 12231, loss 0.479601.
Train: 2018-08-01T23:38:43.529762: step 12232, loss 0.645595.
Train: 2018-08-01T23:38:43.691329: step 12233, loss 0.529249.
Train: 2018-08-01T23:38:43.860876: step 12234, loss 0.645735.
Train: 2018-08-01T23:38:44.021447: step 12235, loss 0.529204.
Train: 2018-08-01T23:38:44.182018: step 12236, loss 0.629115.
Train: 2018-08-01T23:38:44.346579: step 12237, loss 0.645739.
Train: 2018-08-01T23:38:44.510171: step 12238, loss 0.645625.
Train: 2018-08-01T23:38:44.680684: step 12239, loss 0.612273.
Train: 2018-08-01T23:38:44.846242: step 12240, loss 0.579082.
Test: 2018-08-01T23:38:45.368844: step 12240, loss 0.548759.
Train: 2018-08-01T23:38:45.534402: step 12241, loss 0.529586.
Train: 2018-08-01T23:38:45.698988: step 12242, loss 0.579045.
Train: 2018-08-01T23:38:45.857565: step 12243, loss 0.595465.
Train: 2018-08-01T23:38:46.029114: step 12244, loss 0.529817.
Train: 2018-08-01T23:38:46.197628: step 12245, loss 0.579011.
Train: 2018-08-01T23:38:46.362189: step 12246, loss 0.546275.
Train: 2018-08-01T23:38:46.520765: step 12247, loss 0.497226.
Train: 2018-08-01T23:38:46.690313: step 12248, loss 0.677196.
Train: 2018-08-01T23:38:46.856867: step 12249, loss 0.578998.
Train: 2018-08-01T23:38:47.044370: step 12250, loss 0.562661.
Test: 2018-08-01T23:38:47.583924: step 12250, loss 0.549003.
Train: 2018-08-01T23:38:47.746527: step 12251, loss 0.562668.
Train: 2018-08-01T23:38:47.908056: step 12252, loss 0.546363.
Train: 2018-08-01T23:38:48.069625: step 12253, loss 0.497434.
Train: 2018-08-01T23:38:48.251166: step 12254, loss 0.562663.
Train: 2018-08-01T23:38:48.421714: step 12255, loss 0.562651.
Train: 2018-08-01T23:38:48.589236: step 12256, loss 0.513537.
Train: 2018-08-01T23:38:48.749807: step 12257, loss 0.546219.
Train: 2018-08-01T23:38:48.919353: step 12258, loss 0.595469.
Train: 2018-08-01T23:38:49.094915: step 12259, loss 0.727198.
Train: 2018-08-01T23:38:49.260468: step 12260, loss 0.529699.
Test: 2018-08-01T23:38:49.776063: step 12260, loss 0.548824.
Train: 2018-08-01T23:38:49.942619: step 12261, loss 0.463933.
Train: 2018-08-01T23:38:50.108175: step 12262, loss 0.529646.
Train: 2018-08-01T23:38:50.277721: step 12263, loss 0.513067.
Train: 2018-08-01T23:38:50.440287: step 12264, loss 0.512917.
Train: 2018-08-01T23:38:50.604848: step 12265, loss 0.678682.
Train: 2018-08-01T23:38:50.770405: step 12266, loss 0.496048.
Train: 2018-08-01T23:38:50.939983: step 12267, loss 0.529191.
Train: 2018-08-01T23:38:51.098529: step 12268, loss 0.52909.
Train: 2018-08-01T23:38:51.263088: step 12269, loss 0.579205.
Train: 2018-08-01T23:38:51.425669: step 12270, loss 0.545668.
Test: 2018-08-01T23:38:51.956280: step 12270, loss 0.548351.
Train: 2018-08-01T23:38:52.125811: step 12271, loss 0.579264.
Train: 2018-08-01T23:38:52.296350: step 12272, loss 0.57929.
Train: 2018-08-01T23:38:52.459913: step 12273, loss 0.562425.
Train: 2018-08-01T23:38:52.627471: step 12274, loss 0.494769.
Train: 2018-08-01T23:38:52.790006: step 12275, loss 0.54546.
Train: 2018-08-01T23:38:52.949579: step 12276, loss 0.613389.
Train: 2018-08-01T23:38:53.114171: step 12277, loss 0.613462.
Train: 2018-08-01T23:38:53.280694: step 12278, loss 0.562403.
Train: 2018-08-01T23:38:53.444289: step 12279, loss 0.528321.
Train: 2018-08-01T23:38:53.607821: step 12280, loss 0.596517.
Test: 2018-08-01T23:38:54.142392: step 12280, loss 0.548109.
Train: 2018-08-01T23:38:54.306954: step 12281, loss 0.5112.
Train: 2018-08-01T23:38:54.467522: step 12282, loss 0.562399.
Train: 2018-08-01T23:38:54.628092: step 12283, loss 0.596607.
Train: 2018-08-01T23:38:54.790683: step 12284, loss 0.545285.
Train: 2018-08-01T23:38:54.953256: step 12285, loss 0.493899.
Train: 2018-08-01T23:38:55.112797: step 12286, loss 0.528094.
Train: 2018-08-01T23:38:55.277369: step 12287, loss 0.579581.
Train: 2018-08-01T23:38:55.451916: step 12288, loss 0.579607.
Train: 2018-08-01T23:38:55.616450: step 12289, loss 0.476263.
Train: 2018-08-01T23:38:55.780012: step 12290, loss 0.614192.
Test: 2018-08-01T23:38:56.314584: step 12290, loss 0.54793.
Train: 2018-08-01T23:38:56.487152: step 12291, loss 0.441415.
Train: 2018-08-01T23:38:56.654675: step 12292, loss 0.423765.
Train: 2018-08-01T23:38:56.819235: step 12293, loss 0.545016.
Train: 2018-08-01T23:38:56.983795: step 12294, loss 0.63233.
Train: 2018-08-01T23:38:57.145363: step 12295, loss 0.562447.
Train: 2018-08-01T23:38:57.312915: step 12296, loss 0.492223.
Train: 2018-08-01T23:38:57.483460: step 12297, loss 0.509646.
Train: 2018-08-01T23:38:57.647023: step 12298, loss 0.456499.
Train: 2018-08-01T23:38:57.807593: step 12299, loss 0.509307.
Train: 2018-08-01T23:38:57.982127: step 12300, loss 0.509113.
Test: 2018-08-01T23:38:58.512729: step 12300, loss 0.547619.
Train: 2018-08-01T23:38:59.383478: step 12301, loss 0.491022.
Train: 2018-08-01T23:38:59.556017: step 12302, loss 0.598617.
Train: 2018-08-01T23:38:59.732553: step 12303, loss 0.508558.
Train: 2018-08-01T23:38:59.911067: step 12304, loss 0.562747.
Train: 2018-08-01T23:39:00.088592: step 12305, loss 0.671841.
Train: 2018-08-01T23:39:00.260134: step 12306, loss 0.52641.
Train: 2018-08-01T23:39:00.425691: step 12307, loss 0.708662.
Train: 2018-08-01T23:39:00.598230: step 12308, loss 0.544607.
Train: 2018-08-01T23:39:00.770769: step 12309, loss 0.544608.
Train: 2018-08-01T23:39:00.933363: step 12310, loss 0.617419.
Test: 2018-08-01T23:39:01.475884: step 12310, loss 0.547571.
Train: 2018-08-01T23:39:01.646456: step 12311, loss 0.544615.
Train: 2018-08-01T23:39:01.814013: step 12312, loss 0.562781.
Train: 2018-08-01T23:39:01.976576: step 12313, loss 0.544624.
Train: 2018-08-01T23:39:02.139150: step 12314, loss 0.580878.
Train: 2018-08-01T23:39:02.297719: step 12315, loss 0.725663.
Train: 2018-08-01T23:39:02.459279: step 12316, loss 0.562696.
Train: 2018-08-01T23:39:02.626808: step 12317, loss 0.616622.
Train: 2018-08-01T23:39:02.832257: step 12318, loss 0.526776.
Train: 2018-08-01T23:39:02.996844: step 12319, loss 0.544721.
Train: 2018-08-01T23:39:03.163372: step 12320, loss 0.544743.
Test: 2018-08-01T23:39:03.698940: step 12320, loss 0.547655.
Train: 2018-08-01T23:39:03.866518: step 12321, loss 0.6159.
Train: 2018-08-01T23:39:04.035087: step 12322, loss 0.491585.
Train: 2018-08-01T23:39:04.198632: step 12323, loss 0.544807.
Train: 2018-08-01T23:39:04.370147: step 12324, loss 0.509462.
Train: 2018-08-01T23:39:04.536702: step 12325, loss 0.509492.
Train: 2018-08-01T23:39:04.706278: step 12326, loss 0.491823.
Train: 2018-08-01T23:39:04.873801: step 12327, loss 0.421038.
Train: 2018-08-01T23:39:05.041352: step 12328, loss 0.509339.
Train: 2018-08-01T23:39:05.202921: step 12329, loss 0.526989.
Train: 2018-08-01T23:39:05.368478: step 12330, loss 0.598226.
Test: 2018-08-01T23:39:05.906046: step 12330, loss 0.547628.
Train: 2018-08-01T23:39:06.076610: step 12331, loss 0.544723.
Train: 2018-08-01T23:39:06.242195: step 12332, loss 0.580505.
Train: 2018-08-01T23:39:06.405240: step 12333, loss 0.688084.
Train: 2018-08-01T23:39:06.573789: step 12334, loss 0.544701.
Train: 2018-08-01T23:39:06.745330: step 12335, loss 0.598429.
Train: 2018-08-01T23:39:06.910914: step 12336, loss 0.544711.
Train: 2018-08-01T23:39:07.079438: step 12337, loss 0.651996.
Train: 2018-08-01T23:39:07.246990: step 12338, loss 0.562578.
Train: 2018-08-01T23:39:07.407585: step 12339, loss 0.633815.
Train: 2018-08-01T23:39:07.577138: step 12340, loss 0.704666.
Test: 2018-08-01T23:39:08.109683: step 12340, loss 0.547693.
Train: 2018-08-01T23:39:08.275242: step 12341, loss 0.637978.
Train: 2018-08-01T23:39:08.451769: step 12342, loss 0.597674.
Train: 2018-08-01T23:39:08.617326: step 12343, loss 0.597469.
Train: 2018-08-01T23:39:08.793854: step 12344, loss 0.527578.
Train: 2018-08-01T23:39:08.963401: step 12345, loss 0.545061.
Train: 2018-08-01T23:39:09.130959: step 12346, loss 0.527828.
Train: 2018-08-01T23:39:09.308479: step 12347, loss 0.596878.
Train: 2018-08-01T23:39:09.473040: step 12348, loss 0.562397.
Train: 2018-08-01T23:39:09.638622: step 12349, loss 0.682381.
Train: 2018-08-01T23:39:09.806179: step 12350, loss 0.5624.
Test: 2018-08-01T23:39:10.345707: step 12350, loss 0.548164.
Train: 2018-08-01T23:39:10.522234: step 12351, loss 0.579412.
Train: 2018-08-01T23:39:10.685823: step 12352, loss 0.57936.
Train: 2018-08-01T23:39:10.852377: step 12353, loss 0.562424.
Train: 2018-08-01T23:39:11.017910: step 12354, loss 0.511923.
Train: 2018-08-01T23:39:11.177504: step 12355, loss 0.629664.
Train: 2018-08-01T23:39:11.346032: step 12356, loss 0.545697.
Train: 2018-08-01T23:39:11.512607: step 12357, loss 0.529018.
Train: 2018-08-01T23:39:11.687146: step 12358, loss 0.52907.
Train: 2018-08-01T23:39:11.856667: step 12359, loss 0.545787.
Train: 2018-08-01T23:39:12.024220: step 12360, loss 0.612548.
Test: 2018-08-01T23:39:12.549813: step 12360, loss 0.548523.
Train: 2018-08-01T23:39:12.723349: step 12361, loss 0.562486.
Train: 2018-08-01T23:39:12.890928: step 12362, loss 0.579154.
Train: 2018-08-01T23:39:13.070422: step 12363, loss 0.512541.
Train: 2018-08-01T23:39:13.235001: step 12364, loss 0.579148.
Train: 2018-08-01T23:39:13.397548: step 12365, loss 0.72903.
Train: 2018-08-01T23:39:13.573079: step 12366, loss 0.545896.
Train: 2018-08-01T23:39:13.737666: step 12367, loss 0.446429.
Train: 2018-08-01T23:39:13.900229: step 12368, loss 0.545933.
Train: 2018-08-01T23:39:14.063800: step 12369, loss 0.47953.
Train: 2018-08-01T23:39:14.226333: step 12370, loss 0.74541.
Test: 2018-08-01T23:39:14.762898: step 12370, loss 0.548601.
Train: 2018-08-01T23:39:14.935436: step 12371, loss 0.545899.
Train: 2018-08-01T23:39:15.102988: step 12372, loss 0.529307.
Train: 2018-08-01T23:39:15.268546: step 12373, loss 0.66215.
Train: 2018-08-01T23:39:15.438118: step 12374, loss 0.562523.
Train: 2018-08-01T23:39:15.603675: step 12375, loss 0.545961.
Train: 2018-08-01T23:39:15.772200: step 12376, loss 0.661894.
Train: 2018-08-01T23:39:15.939751: step 12377, loss 0.52949.
Train: 2018-08-01T23:39:16.102343: step 12378, loss 0.628606.
Train: 2018-08-01T23:39:16.270867: step 12379, loss 0.61202.
Train: 2018-08-01T23:39:16.438449: step 12380, loss 0.529702.
Test: 2018-08-01T23:39:16.968030: step 12380, loss 0.548853.
Train: 2018-08-01T23:39:17.136583: step 12381, loss 0.628296.
Train: 2018-08-01T23:39:17.312083: step 12382, loss 0.579014.
Train: 2018-08-01T23:39:17.473677: step 12383, loss 0.644424.
Train: 2018-08-01T23:39:17.637215: step 12384, loss 0.595293.
Train: 2018-08-01T23:39:17.800778: step 12385, loss 0.627752.
Train: 2018-08-01T23:39:17.973316: step 12386, loss 0.562748.
Train: 2018-08-01T23:39:18.138873: step 12387, loss 0.562785.
Train: 2018-08-01T23:39:18.306424: step 12388, loss 0.514465.
Train: 2018-08-01T23:39:18.470985: step 12389, loss 0.514543.
Train: 2018-08-01T23:39:18.636542: step 12390, loss 0.611118.
Test: 2018-08-01T23:39:19.161141: step 12390, loss 0.549378.
Train: 2018-08-01T23:39:19.340661: step 12391, loss 0.498515.
Train: 2018-08-01T23:39:19.502255: step 12392, loss 0.562837.
Train: 2018-08-01T23:39:19.664828: step 12393, loss 0.562828.
Train: 2018-08-01T23:39:19.831349: step 12394, loss 0.530581.
Train: 2018-08-01T23:39:19.998929: step 12395, loss 0.530514.
Train: 2018-08-01T23:39:20.164484: step 12396, loss 0.465725.
Train: 2018-08-01T23:39:20.327024: step 12397, loss 0.44911.
Train: 2018-08-01T23:39:20.491584: step 12398, loss 0.611613.
Train: 2018-08-01T23:39:20.663139: step 12399, loss 0.595388.
Train: 2018-08-01T23:39:20.826713: step 12400, loss 0.611892.
Test: 2018-08-01T23:39:21.346299: step 12400, loss 0.54879.
Train: 2018-08-01T23:39:22.137491: step 12401, loss 0.595516.
Train: 2018-08-01T23:39:22.309021: step 12402, loss 0.645044.
Train: 2018-08-01T23:39:22.471593: step 12403, loss 0.463547.
Train: 2018-08-01T23:39:22.638173: step 12404, loss 0.678291.
Train: 2018-08-01T23:39:22.798743: step 12405, loss 0.661772.
Train: 2018-08-01T23:39:22.965299: step 12406, loss 0.463445.
Train: 2018-08-01T23:39:23.130825: step 12407, loss 0.595604.
Train: 2018-08-01T23:39:23.298377: step 12408, loss 0.612135.
Train: 2018-08-01T23:39:23.460967: step 12409, loss 0.479955.
Train: 2018-08-01T23:39:23.627497: step 12410, loss 0.529479.
Test: 2018-08-01T23:39:24.159076: step 12410, loss 0.548672.
Train: 2018-08-01T23:39:24.332613: step 12411, loss 0.69499.
Train: 2018-08-01T23:39:24.511165: step 12412, loss 0.463258.
Train: 2018-08-01T23:39:24.673700: step 12413, loss 0.579097.
Train: 2018-08-01T23:39:24.842249: step 12414, loss 0.545949.
Train: 2018-08-01T23:39:25.007806: step 12415, loss 0.529328.
Train: 2018-08-01T23:39:25.176381: step 12416, loss 0.545886.
Train: 2018-08-01T23:39:25.333935: step 12417, loss 0.629096.
Train: 2018-08-01T23:39:25.501487: step 12418, loss 0.612476.
Train: 2018-08-01T23:39:25.672032: step 12419, loss 0.612476.
Train: 2018-08-01T23:39:25.833599: step 12420, loss 0.462591.
Test: 2018-08-01T23:39:26.358208: step 12420, loss 0.548533.
Train: 2018-08-01T23:39:26.515775: step 12421, loss 0.562489.
Train: 2018-08-01T23:39:26.697290: step 12422, loss 0.495752.
Train: 2018-08-01T23:39:26.858891: step 12423, loss 0.462183.
Train: 2018-08-01T23:39:27.020435: step 12424, loss 0.629531.
Train: 2018-08-01T23:39:27.188976: step 12425, loss 0.562445.
Train: 2018-08-01T23:39:27.356529: step 12426, loss 0.562437.
Train: 2018-08-01T23:39:27.518104: step 12427, loss 0.596158.
Train: 2018-08-01T23:39:27.683102: step 12428, loss 0.596192.
Train: 2018-08-01T23:39:27.853660: step 12429, loss 0.54553.
Train: 2018-08-01T23:39:28.017222: step 12430, loss 0.545513.
Test: 2018-08-01T23:39:28.538836: step 12430, loss 0.548245.
Train: 2018-08-01T23:39:28.718380: step 12431, loss 0.562417.
Train: 2018-08-01T23:39:28.883938: step 12432, loss 0.528531.
Train: 2018-08-01T23:39:29.051457: step 12433, loss 0.528481.
Train: 2018-08-01T23:39:29.222002: step 12434, loss 0.630387.
Train: 2018-08-01T23:39:29.386561: step 12435, loss 0.528393.
Train: 2018-08-01T23:39:29.551121: step 12436, loss 0.562404.
Train: 2018-08-01T23:39:29.724658: step 12437, loss 0.528318.
Train: 2018-08-01T23:39:29.890215: step 12438, loss 0.460005.
Train: 2018-08-01T23:39:30.051636: step 12439, loss 0.630845.
Train: 2018-08-01T23:39:30.218191: step 12440, loss 0.562397.
Test: 2018-08-01T23:39:30.760741: step 12440, loss 0.548027.
Train: 2018-08-01T23:39:30.933304: step 12441, loss 0.442277.
Train: 2018-08-01T23:39:31.095876: step 12442, loss 0.614024.
Train: 2018-08-01T23:39:31.264393: step 12443, loss 0.510679.
Train: 2018-08-01T23:39:31.449929: step 12444, loss 0.562403.
Train: 2018-08-01T23:39:31.628452: step 12445, loss 0.475825.
Train: 2018-08-01T23:39:31.792980: step 12446, loss 0.579782.
Train: 2018-08-01T23:39:31.953580: step 12447, loss 0.47537.
Train: 2018-08-01T23:39:32.119109: step 12448, loss 0.579901.
Train: 2018-08-01T23:39:32.281705: step 12449, loss 0.544932.
Train: 2018-08-01T23:39:32.462222: step 12450, loss 0.474666.
Test: 2018-08-01T23:39:32.992774: step 12450, loss 0.547725.
Train: 2018-08-01T23:39:33.161323: step 12451, loss 0.562478.
Train: 2018-08-01T23:39:33.333872: step 12452, loss 0.562497.
Train: 2018-08-01T23:39:33.497424: step 12453, loss 0.473943.
Train: 2018-08-01T23:39:33.676946: step 12454, loss 0.526995.
Train: 2018-08-01T23:39:33.842534: step 12455, loss 0.598237.
Train: 2018-08-01T23:39:34.005092: step 12456, loss 0.544718.
Train: 2018-08-01T23:39:34.166635: step 12457, loss 0.59845.
Train: 2018-08-01T23:39:34.327230: step 12458, loss 0.634404.
Train: 2018-08-01T23:39:34.492762: step 12459, loss 0.634428.
Train: 2018-08-01T23:39:34.671286: step 12460, loss 0.419153.
Test: 2018-08-01T23:39:35.208849: step 12460, loss 0.547605.
Train: 2018-08-01T23:39:35.377398: step 12461, loss 0.652395.
Train: 2018-08-01T23:39:35.543999: step 12462, loss 0.490855.
Train: 2018-08-01T23:39:35.709525: step 12463, loss 0.616494.
Train: 2018-08-01T23:39:35.882050: step 12464, loss 0.562633.
Train: 2018-08-01T23:39:36.055585: step 12465, loss 0.562627.
Train: 2018-08-01T23:39:36.223168: step 12466, loss 0.473001.
Train: 2018-08-01T23:39:36.386727: step 12467, loss 0.544694.
Train: 2018-08-01T23:39:36.559240: step 12468, loss 0.580572.
Train: 2018-08-01T23:39:36.722802: step 12469, loss 0.47292.
Train: 2018-08-01T23:39:36.895348: step 12470, loss 0.508759.
Test: 2018-08-01T23:39:37.425953: step 12470, loss 0.547597.
Train: 2018-08-01T23:39:37.596491: step 12471, loss 0.616625.
Train: 2018-08-01T23:39:37.771014: step 12472, loss 0.544668.
Train: 2018-08-01T23:39:37.938551: step 12473, loss 0.400603.
Train: 2018-08-01T23:39:38.109096: step 12474, loss 0.4905.
Train: 2018-08-01T23:39:38.280636: step 12475, loss 0.544635.
Train: 2018-08-01T23:39:38.447218: step 12476, loss 0.599071.
Train: 2018-08-01T23:39:38.618733: step 12477, loss 0.61734.
Train: 2018-08-01T23:39:38.781330: step 12478, loss 0.581001.
Train: 2018-08-01T23:39:38.958825: step 12479, loss 0.54461.
Train: 2018-08-01T23:39:39.126409: step 12480, loss 0.61743.
Test: 2018-08-01T23:39:39.667929: step 12480, loss 0.54757.
Train: 2018-08-01T23:39:39.834486: step 12481, loss 0.599195.
Train: 2018-08-01T23:39:40.001038: step 12482, loss 0.526442.
Train: 2018-08-01T23:39:40.169587: step 12483, loss 0.708063.
Train: 2018-08-01T23:39:40.338138: step 12484, loss 0.671423.
Train: 2018-08-01T23:39:40.502729: step 12485, loss 0.58074.
Train: 2018-08-01T23:39:40.667257: step 12486, loss 0.544677.
Train: 2018-08-01T23:39:40.841790: step 12487, loss 0.634269.
Train: 2018-08-01T23:39:41.008350: step 12488, loss 0.580416.
Train: 2018-08-01T23:39:41.184874: step 12489, loss 0.509234.
Train: 2018-08-01T23:39:41.349434: step 12490, loss 0.456232.
Test: 2018-08-01T23:39:41.889989: step 12490, loss 0.547693.
Train: 2018-08-01T23:39:42.058537: step 12491, loss 0.52713.
Train: 2018-08-01T23:39:42.221103: step 12492, loss 0.633176.
Train: 2018-08-01T23:39:42.382702: step 12493, loss 0.562485.
Train: 2018-08-01T23:39:42.544239: step 12494, loss 0.50966.
Train: 2018-08-01T23:39:42.713811: step 12495, loss 0.615228.
Train: 2018-08-01T23:39:42.873360: step 12496, loss 0.580016.
Train: 2018-08-01T23:39:43.034959: step 12497, loss 0.422244.
Train: 2018-08-01T23:39:43.204474: step 12498, loss 0.632562.
Train: 2018-08-01T23:39:43.367041: step 12499, loss 0.457367.
Train: 2018-08-01T23:39:43.530633: step 12500, loss 0.685106.
Test: 2018-08-01T23:39:44.060196: step 12500, loss 0.547787.
Train: 2018-08-01T23:39:44.933131: step 12501, loss 0.562442.
Train: 2018-08-01T23:39:45.101636: step 12502, loss 0.527473.
Train: 2018-08-01T23:39:45.263235: step 12503, loss 0.562435.
Train: 2018-08-01T23:39:45.436778: step 12504, loss 0.579894.
Train: 2018-08-01T23:39:45.613269: step 12505, loss 0.66711.
Train: 2018-08-01T23:39:45.789822: step 12506, loss 0.545012.
Train: 2018-08-01T23:39:45.956384: step 12507, loss 0.545037.
Train: 2018-08-01T23:39:46.118916: step 12508, loss 0.440935.
Train: 2018-08-01T23:39:46.282507: step 12509, loss 0.475617.
Train: 2018-08-01T23:39:46.448036: step 12510, loss 0.562417.
Test: 2018-08-01T23:39:46.987600: step 12510, loss 0.547846.
Train: 2018-08-01T23:39:47.162153: step 12511, loss 0.492803.
Train: 2018-08-01T23:39:47.334667: step 12512, loss 0.475234.
Train: 2018-08-01T23:39:47.500224: step 12513, loss 0.632393.
Train: 2018-08-01T23:39:47.661792: step 12514, loss 0.614996.
Train: 2018-08-01T23:39:47.828378: step 12515, loss 0.544921.
Train: 2018-08-01T23:39:47.999889: step 12516, loss 0.544912.
Train: 2018-08-01T23:39:48.184395: step 12517, loss 0.527344.
Train: 2018-08-01T23:39:48.346960: step 12518, loss 0.562465.
Train: 2018-08-01T23:39:48.516535: step 12519, loss 0.439306.
Train: 2018-08-01T23:39:48.690068: step 12520, loss 0.544848.
Test: 2018-08-01T23:39:49.226624: step 12520, loss 0.547698.
Train: 2018-08-01T23:39:49.387205: step 12521, loss 0.527146.
Train: 2018-08-01T23:39:49.549745: step 12522, loss 0.473918.
Train: 2018-08-01T23:39:49.718320: step 12523, loss 0.402554.
Train: 2018-08-01T23:39:49.916764: step 12524, loss 0.616171.
Train: 2018-08-01T23:39:50.075340: step 12525, loss 0.616393.
Train: 2018-08-01T23:39:50.235911: step 12526, loss 0.598578.
Train: 2018-08-01T23:39:50.398507: step 12527, loss 0.562663.
Train: 2018-08-01T23:39:50.567028: step 12528, loss 0.670767.
Train: 2018-08-01T23:39:50.727632: step 12529, loss 0.562673.
Train: 2018-08-01T23:39:50.921080: step 12530, loss 0.526668.
Test: 2018-08-01T23:39:51.451661: step 12530, loss 0.547595.
Train: 2018-08-01T23:39:51.617226: step 12531, loss 0.454683.
Train: 2018-08-01T23:39:51.793773: step 12532, loss 0.580695.
Train: 2018-08-01T23:39:51.969277: step 12533, loss 0.562686.
Train: 2018-08-01T23:39:52.141816: step 12534, loss 0.490553.
Train: 2018-08-01T23:39:52.305397: step 12535, loss 0.562703.
Train: 2018-08-01T23:39:52.469962: step 12536, loss 0.580783.
Train: 2018-08-01T23:39:52.638489: step 12537, loss 0.635021.
Train: 2018-08-01T23:39:52.808060: step 12538, loss 0.508521.
Train: 2018-08-01T23:39:52.978610: step 12539, loss 0.580769.
Train: 2018-08-01T23:39:53.142142: step 12540, loss 0.562702.
Test: 2018-08-01T23:39:53.682707: step 12540, loss 0.547587.
Train: 2018-08-01T23:39:53.850280: step 12541, loss 0.598778.
Train: 2018-08-01T23:39:54.024815: step 12542, loss 0.562682.
Train: 2018-08-01T23:39:54.189368: step 12543, loss 0.580669.
Train: 2018-08-01T23:39:54.357923: step 12544, loss 0.544677.
Train: 2018-08-01T23:39:54.518487: step 12545, loss 0.508777.
Train: 2018-08-01T23:39:54.686039: step 12546, loss 0.5088.
Train: 2018-08-01T23:39:54.847614: step 12547, loss 0.598525.
Train: 2018-08-01T23:39:55.016163: step 12548, loss 0.580565.
Train: 2018-08-01T23:39:55.188696: step 12549, loss 0.634305.
Train: 2018-08-01T23:39:55.375210: step 12550, loss 0.580491.
Test: 2018-08-01T23:39:55.902066: step 12550, loss 0.547632.
Train: 2018-08-01T23:39:56.071587: step 12551, loss 0.544728.
Train: 2018-08-01T23:39:56.236175: step 12552, loss 0.437791.
Train: 2018-08-01T23:39:56.408719: step 12553, loss 0.598216.
Train: 2018-08-01T23:39:56.576238: step 12554, loss 0.651633.
Train: 2018-08-01T23:39:56.752798: step 12555, loss 0.562547.
Train: 2018-08-01T23:39:56.919346: step 12556, loss 0.491523.
Train: 2018-08-01T23:39:57.086906: step 12557, loss 0.527048.
Train: 2018-08-01T23:39:57.264399: step 12558, loss 0.580258.
Train: 2018-08-01T23:39:57.436937: step 12559, loss 0.56252.
Train: 2018-08-01T23:39:57.609501: step 12560, loss 0.438531.
Test: 2018-08-01T23:39:58.141055: step 12560, loss 0.547677.
Train: 2018-08-01T23:39:58.330574: step 12561, loss 0.509338.
Train: 2018-08-01T23:39:58.504085: step 12562, loss 0.580285.
Train: 2018-08-01T23:39:58.668675: step 12563, loss 0.651387.
Train: 2018-08-01T23:39:58.846171: step 12564, loss 0.544775.
Train: 2018-08-01T23:39:59.007738: step 12565, loss 0.420471.
Train: 2018-08-01T23:39:59.175290: step 12566, loss 0.633685.
Train: 2018-08-01T23:39:59.348857: step 12567, loss 0.598131.
Train: 2018-08-01T23:39:59.518373: step 12568, loss 0.544762.
Train: 2018-08-01T23:39:59.685926: step 12569, loss 0.491413.
Train: 2018-08-01T23:39:59.855472: step 12570, loss 0.526961.
Test: 2018-08-01T23:40:00.393057: step 12570, loss 0.547645.
Train: 2018-08-01T23:40:00.571557: step 12571, loss 0.615999.
Train: 2018-08-01T23:40:00.751078: step 12572, loss 0.562562.
Train: 2018-08-01T23:40:00.916177: step 12573, loss 0.544748.
Train: 2018-08-01T23:40:01.086278: step 12574, loss 0.616006.
Train: 2018-08-01T23:40:01.252783: step 12575, loss 0.509151.
Train: 2018-08-01T23:40:01.428326: step 12576, loss 0.651556.
Train: 2018-08-01T23:40:01.644767: step 12577, loss 0.651428.
Train: 2018-08-01T23:40:01.806329: step 12578, loss 0.615726.
Train: 2018-08-01T23:40:01.980837: step 12579, loss 0.615547.
Train: 2018-08-01T23:40:02.145399: step 12580, loss 0.509617.
Test: 2018-08-01T23:40:02.672987: step 12580, loss 0.547746.
Train: 2018-08-01T23:40:02.838545: step 12581, loss 0.562464.
Train: 2018-08-01T23:40:03.003135: step 12582, loss 0.527381.
Train: 2018-08-01T23:40:03.160708: step 12583, loss 0.61496.
Train: 2018-08-01T23:40:03.327237: step 12584, loss 0.562433.
Train: 2018-08-01T23:40:03.492796: step 12585, loss 0.527568.
Train: 2018-08-01T23:40:03.659380: step 12586, loss 0.545018.
Train: 2018-08-01T23:40:03.825905: step 12587, loss 0.527653.
Train: 2018-08-01T23:40:03.990490: step 12588, loss 0.718752.
Train: 2018-08-01T23:40:04.156021: step 12589, loss 0.752975.
Train: 2018-08-01T23:40:04.324572: step 12590, loss 0.510685.
Test: 2018-08-01T23:40:04.850167: step 12590, loss 0.548017.
Train: 2018-08-01T23:40:05.010737: step 12591, loss 0.562397.
Train: 2018-08-01T23:40:05.177292: step 12592, loss 0.562398.
Train: 2018-08-01T23:40:05.341878: step 12593, loss 0.51122.
Train: 2018-08-01T23:40:05.511399: step 12594, loss 0.596453.
Train: 2018-08-01T23:40:05.692926: step 12595, loss 0.613363.
Train: 2018-08-01T23:40:05.859469: step 12596, loss 0.52854.
Train: 2018-08-01T23:40:06.024054: step 12597, loss 0.562422.
Train: 2018-08-01T23:40:06.200584: step 12598, loss 0.562428.
Train: 2018-08-01T23:40:06.376088: step 12599, loss 0.528737.
Train: 2018-08-01T23:40:06.545635: step 12600, loss 0.4951.
Test: 2018-08-01T23:40:07.085193: step 12600, loss 0.548336.
Train: 2018-08-01T23:40:07.870476: step 12601, loss 0.764496.
Train: 2018-08-01T23:40:08.041020: step 12602, loss 0.478471.
Train: 2018-08-01T23:40:08.206579: step 12603, loss 0.629563.
Train: 2018-08-01T23:40:08.375159: step 12604, loss 0.595955.
Train: 2018-08-01T23:40:08.540710: step 12605, loss 0.562473.
Train: 2018-08-01T23:40:08.714220: step 12606, loss 0.662582.
Train: 2018-08-01T23:40:08.882802: step 12607, loss 0.5126.
Train: 2018-08-01T23:40:09.049326: step 12608, loss 0.496101.
Train: 2018-08-01T23:40:09.216877: step 12609, loss 0.79483.
Train: 2018-08-01T23:40:09.382459: step 12610, loss 0.612142.
Test: 2018-08-01T23:40:09.924984: step 12610, loss 0.548794.
Train: 2018-08-01T23:40:10.097523: step 12611, loss 0.480256.
Train: 2018-08-01T23:40:10.268067: step 12612, loss 0.694024.
Train: 2018-08-01T23:40:10.436625: step 12613, loss 0.513546.
Train: 2018-08-01T23:40:10.608157: step 12614, loss 0.562667.
Train: 2018-08-01T23:40:10.776708: step 12615, loss 0.595264.
Train: 2018-08-01T23:40:10.947283: step 12616, loss 0.562717.
Train: 2018-08-01T23:40:11.115801: step 12617, loss 0.514084.
Train: 2018-08-01T23:40:11.287374: step 12618, loss 0.514133.
Train: 2018-08-01T23:40:11.460878: step 12619, loss 0.481705.
Train: 2018-08-01T23:40:11.622446: step 12620, loss 0.514027.
Test: 2018-08-01T23:40:12.163001: step 12620, loss 0.549076.
Train: 2018-08-01T23:40:12.351519: step 12621, loss 0.54643.
Train: 2018-08-01T23:40:12.520047: step 12622, loss 0.546363.
Train: 2018-08-01T23:40:12.693601: step 12623, loss 0.513583.
Train: 2018-08-01T23:40:12.872138: step 12624, loss 0.628244.
Train: 2018-08-01T23:40:13.037664: step 12625, loss 0.579036.
Train: 2018-08-01T23:40:13.203269: step 12626, loss 0.546109.
Train: 2018-08-01T23:40:13.369802: step 12627, loss 0.628573.
Train: 2018-08-01T23:40:13.538325: step 12628, loss 0.579073.
Train: 2018-08-01T23:40:13.725825: step 12629, loss 0.595608.
Train: 2018-08-01T23:40:13.902376: step 12630, loss 0.628683.
Test: 2018-08-01T23:40:14.444901: step 12630, loss 0.548715.
Train: 2018-08-01T23:40:14.611456: step 12631, loss 0.645172.
Train: 2018-08-01T23:40:14.779009: step 12632, loss 0.513073.
Train: 2018-08-01T23:40:14.957562: step 12633, loss 0.529596.
Train: 2018-08-01T23:40:15.143061: step 12634, loss 0.546082.
Train: 2018-08-01T23:40:15.308617: step 12635, loss 0.480092.
Train: 2018-08-01T23:40:15.471159: step 12636, loss 0.54603.
Train: 2018-08-01T23:40:15.637713: step 12637, loss 0.446661.
Train: 2018-08-01T23:40:15.804269: step 12638, loss 0.496067.
Train: 2018-08-01T23:40:15.969825: step 12639, loss 0.5625.
Train: 2018-08-01T23:40:16.132421: step 12640, loss 0.562463.
Test: 2018-08-01T23:40:16.669954: step 12640, loss 0.548376.
Train: 2018-08-01T23:40:16.833527: step 12641, loss 0.629654.
Train: 2018-08-01T23:40:17.015057: step 12642, loss 0.544473.
Train: 2018-08-01T23:40:17.186573: step 12643, loss 0.545547.
Train: 2018-08-01T23:40:17.353151: step 12644, loss 0.545499.
Train: 2018-08-01T23:40:17.520679: step 12645, loss 0.528491.
Train: 2018-08-01T23:40:17.695212: step 12646, loss 0.613423.
Train: 2018-08-01T23:40:17.861766: step 12647, loss 0.596473.
Train: 2018-08-01T23:40:18.026340: step 12648, loss 0.477131.
Train: 2018-08-01T23:40:18.218812: step 12649, loss 0.596582.
Train: 2018-08-01T23:40:18.401326: step 12650, loss 0.562398.
Test: 2018-08-01T23:40:18.914989: step 12650, loss 0.548045.
Train: 2018-08-01T23:40:19.100456: step 12651, loss 0.579538.
Train: 2018-08-01T23:40:19.268008: step 12652, loss 0.579555.
Train: 2018-08-01T23:40:19.429576: step 12653, loss 0.596735.
Train: 2018-08-01T23:40:19.601118: step 12654, loss 0.631081.
Train: 2018-08-01T23:40:19.771661: step 12655, loss 0.510927.
Train: 2018-08-01T23:40:19.940211: step 12656, loss 0.476616.
Train: 2018-08-01T23:40:20.111752: step 12657, loss 0.493695.
Train: 2018-08-01T23:40:20.285288: step 12658, loss 0.527981.
Train: 2018-08-01T23:40:20.448885: step 12659, loss 0.510662.
Train: 2018-08-01T23:40:20.621419: step 12660, loss 0.631573.
Test: 2018-08-01T23:40:21.156958: step 12660, loss 0.547907.
Train: 2018-08-01T23:40:21.331491: step 12661, loss 0.631674.
Train: 2018-08-01T23:40:21.505028: step 12662, loss 0.510441.
Train: 2018-08-01T23:40:21.687540: step 12663, loss 0.423699.
Train: 2018-08-01T23:40:21.863071: step 12664, loss 0.527648.
Train: 2018-08-01T23:40:22.031620: step 12665, loss 0.597295.
Train: 2018-08-01T23:40:22.199173: step 12666, loss 0.649779.
Train: 2018-08-01T23:40:22.364755: step 12667, loss 0.632356.
Train: 2018-08-01T23:40:22.535274: step 12668, loss 0.579909.
Train: 2018-08-01T23:40:22.695844: step 12669, loss 0.475127.
Train: 2018-08-01T23:40:22.865391: step 12670, loss 0.527496.
Test: 2018-08-01T23:40:23.390987: step 12670, loss 0.547798.
Train: 2018-08-01T23:40:23.559535: step 12671, loss 0.562439.
Train: 2018-08-01T23:40:23.721129: step 12672, loss 0.527448.
Train: 2018-08-01T23:40:23.890650: step 12673, loss 0.492387.
Train: 2018-08-01T23:40:24.051221: step 12674, loss 0.54491.
Train: 2018-08-01T23:40:24.209798: step 12675, loss 0.615195.
Train: 2018-08-01T23:40:24.375355: step 12676, loss 0.56247.
Train: 2018-08-01T23:40:24.537920: step 12677, loss 0.527268.
Train: 2018-08-01T23:40:24.716443: step 12678, loss 0.421521.
Train: 2018-08-01T23:40:24.879008: step 12679, loss 0.474169.
Train: 2018-08-01T23:40:25.090468: step 12680, loss 0.63342.
Test: 2018-08-01T23:40:25.632993: step 12680, loss 0.547664.
Train: 2018-08-01T23:40:25.821490: step 12681, loss 0.527016.
Train: 2018-08-01T23:40:25.996022: step 12682, loss 0.580354.
Train: 2018-08-01T23:40:26.153600: step 12683, loss 0.580397.
Train: 2018-08-01T23:40:26.320168: step 12684, loss 0.651816.
Train: 2018-08-01T23:40:26.481724: step 12685, loss 0.544734.
Train: 2018-08-01T23:40:26.642295: step 12686, loss 0.580418.
Train: 2018-08-01T23:40:26.805857: step 12687, loss 0.509071.
Train: 2018-08-01T23:40:26.989392: step 12688, loss 0.455553.
Train: 2018-08-01T23:40:27.155922: step 12689, loss 0.705477.
Train: 2018-08-01T23:40:27.317490: step 12690, loss 0.651828.
Test: 2018-08-01T23:40:27.858045: step 12690, loss 0.547644.
Train: 2018-08-01T23:40:28.024599: step 12691, loss 0.544747.
Train: 2018-08-01T23:40:28.195143: step 12692, loss 0.63371.
Train: 2018-08-01T23:40:28.380647: step 12693, loss 0.562531.
Train: 2018-08-01T23:40:28.558173: step 12694, loss 0.50939.
Train: 2018-08-01T23:40:28.725725: step 12695, loss 0.509456.
Train: 2018-08-01T23:40:28.899261: step 12696, loss 0.650846.
Train: 2018-08-01T23:40:29.105709: step 12697, loss 0.633032.
Train: 2018-08-01T23:40:29.274259: step 12698, loss 0.597649.
Train: 2018-08-01T23:40:29.443805: step 12699, loss 0.492298.
Train: 2018-08-01T23:40:29.620333: step 12700, loss 0.667488.
Test: 2018-08-01T23:40:30.161885: step 12700, loss 0.547815.
Train: 2018-08-01T23:40:30.977934: step 12701, loss 0.614794.
Train: 2018-08-01T23:40:31.143491: step 12702, loss 0.579813.
Train: 2018-08-01T23:40:31.311087: step 12703, loss 0.649083.
Train: 2018-08-01T23:40:31.486606: step 12704, loss 0.54514.
Train: 2018-08-01T23:40:31.669085: step 12705, loss 0.476404.
Train: 2018-08-01T23:40:31.836637: step 12706, loss 0.493748.
Train: 2018-08-01T23:40:32.000200: step 12707, loss 0.579542.
Train: 2018-08-01T23:40:32.168784: step 12708, loss 0.579523.
Train: 2018-08-01T23:40:32.341314: step 12709, loss 0.562398.
Train: 2018-08-01T23:40:32.525820: step 12710, loss 0.511143.
Test: 2018-08-01T23:40:33.067348: step 12710, loss 0.548097.
Train: 2018-08-01T23:40:33.245897: step 12711, loss 0.57948.
Train: 2018-08-01T23:40:33.415417: step 12712, loss 0.545328.
Train: 2018-08-01T23:40:33.582969: step 12713, loss 0.545331.
Train: 2018-08-01T23:40:33.771466: step 12714, loss 0.45998.
Train: 2018-08-01T23:40:33.937044: step 12715, loss 0.562398.
Train: 2018-08-01T23:40:34.100586: step 12716, loss 0.613752.
Train: 2018-08-01T23:40:34.263150: step 12717, loss 0.613778.
Train: 2018-08-01T23:40:34.434725: step 12718, loss 0.562397.
Train: 2018-08-01T23:40:34.631168: step 12719, loss 0.579517.
Train: 2018-08-01T23:40:34.798750: step 12720, loss 0.476834.
Test: 2018-08-01T23:40:35.345257: step 12720, loss 0.548056.
Train: 2018-08-01T23:40:35.522795: step 12721, loss 0.476765.
Train: 2018-08-01T23:40:35.709284: step 12722, loss 0.613873.
Train: 2018-08-01T23:40:35.882828: step 12723, loss 0.493694.
Train: 2018-08-01T23:40:36.055361: step 12724, loss 0.545191.
Train: 2018-08-01T23:40:36.233882: step 12725, loss 0.545161.
Train: 2018-08-01T23:40:36.410437: step 12726, loss 0.424233.
Train: 2018-08-01T23:40:36.578960: step 12727, loss 0.770387.
Train: 2018-08-01T23:40:36.753493: step 12728, loss 0.597085.
Train: 2018-08-01T23:40:36.922076: step 12729, loss 0.510404.
Train: 2018-08-01T23:40:37.093585: step 12730, loss 0.56241.
Test: 2018-08-01T23:40:37.617185: step 12730, loss 0.547882.
Train: 2018-08-01T23:40:37.790721: step 12731, loss 0.597114.
Train: 2018-08-01T23:40:37.969243: step 12732, loss 0.527712.
Train: 2018-08-01T23:40:38.133803: step 12733, loss 0.614478.
Train: 2018-08-01T23:40:38.314322: step 12734, loss 0.579759.
Train: 2018-08-01T23:40:38.485863: step 12735, loss 0.579746.
Train: 2018-08-01T23:40:38.691313: step 12736, loss 0.57973.
Train: 2018-08-01T23:40:38.860859: step 12737, loss 0.527795.
Train: 2018-08-01T23:40:39.026416: step 12738, loss 0.579701.
Train: 2018-08-01T23:40:39.207957: step 12739, loss 0.562403.
Train: 2018-08-01T23:40:39.388476: step 12740, loss 0.631497.
Test: 2018-08-01T23:40:39.926013: step 12740, loss 0.547957.
Train: 2018-08-01T23:40:40.103563: step 12741, loss 0.579648.
Train: 2018-08-01T23:40:40.279069: step 12742, loss 0.545178.
Train: 2018-08-01T23:40:40.444627: step 12743, loss 0.596795.
Train: 2018-08-01T23:40:40.626142: step 12744, loss 0.579569.
Train: 2018-08-01T23:40:40.791699: step 12745, loss 0.545254.
Train: 2018-08-01T23:40:40.962243: step 12746, loss 0.613763.
Train: 2018-08-01T23:40:41.132786: step 12747, loss 0.494036.
Train: 2018-08-01T23:40:41.305325: step 12748, loss 0.511163.
Train: 2018-08-01T23:40:41.489873: step 12749, loss 0.528239.
Train: 2018-08-01T23:40:41.661406: step 12750, loss 0.579488.
Test: 2018-08-01T23:40:42.198937: step 12750, loss 0.548084.
Train: 2018-08-01T23:40:42.373494: step 12751, loss 0.511117.
Train: 2018-08-01T23:40:42.541022: step 12752, loss 0.613727.
Train: 2018-08-01T23:40:42.709590: step 12753, loss 0.596623.
Train: 2018-08-01T23:40:42.885102: step 12754, loss 0.511075.
Train: 2018-08-01T23:40:43.055648: step 12755, loss 0.545283.
Train: 2018-08-01T23:40:43.234200: step 12756, loss 0.493895.
Train: 2018-08-01T23:40:43.408735: step 12757, loss 0.528095.
Train: 2018-08-01T23:40:43.585230: step 12758, loss 0.562397.
Train: 2018-08-01T23:40:43.764751: step 12759, loss 0.579608.
Train: 2018-08-01T23:40:43.940281: step 12760, loss 0.493476.
Test: 2018-08-01T23:40:44.470864: step 12760, loss 0.547944.
Train: 2018-08-01T23:40:44.640436: step 12761, loss 0.562401.
Train: 2018-08-01T23:40:44.813960: step 12762, loss 0.475924.
Train: 2018-08-01T23:40:44.985516: step 12763, loss 0.579753.
Train: 2018-08-01T23:40:45.164010: step 12764, loss 0.614559.
Train: 2018-08-01T23:40:45.342534: step 12765, loss 0.510213.
Train: 2018-08-01T23:40:45.527039: step 12766, loss 0.579859.
Train: 2018-08-01T23:40:45.694591: step 12767, loss 0.544976.
Train: 2018-08-01T23:40:45.876108: step 12768, loss 0.544958.
Train: 2018-08-01T23:40:46.042662: step 12769, loss 0.527437.
Train: 2018-08-01T23:40:46.221184: step 12770, loss 0.527388.
Test: 2018-08-01T23:40:46.760741: step 12770, loss 0.547752.
Train: 2018-08-01T23:40:46.929292: step 12771, loss 0.580024.
Train: 2018-08-01T23:40:47.103824: step 12772, loss 0.597644.
Train: 2018-08-01T23:40:47.273390: step 12773, loss 0.580073.
Train: 2018-08-01T23:40:47.453889: step 12774, loss 0.527261.
Train: 2018-08-01T23:40:47.627424: step 12775, loss 0.54486.
Train: 2018-08-01T23:40:47.802958: step 12776, loss 0.580115.
Train: 2018-08-01T23:40:47.979483: step 12777, loss 0.562485.
Train: 2018-08-01T23:40:48.155040: step 12778, loss 0.580129.
Train: 2018-08-01T23:40:48.330545: step 12779, loss 0.527205.
Train: 2018-08-01T23:40:48.505079: step 12780, loss 0.686011.
Test: 2018-08-01T23:40:49.034665: step 12780, loss 0.547724.
Train: 2018-08-01T23:40:49.205219: step 12781, loss 0.562479.
Train: 2018-08-01T23:40:49.373757: step 12782, loss 0.562471.
Train: 2018-08-01T23:40:49.544301: step 12783, loss 0.650338.
Train: 2018-08-01T23:40:49.718859: step 12784, loss 0.597518.
Train: 2018-08-01T23:40:49.885414: step 12785, loss 0.457518.
Train: 2018-08-01T23:40:50.063911: step 12786, loss 0.66724.
Train: 2018-08-01T23:40:50.237479: step 12787, loss 0.562425.
Train: 2018-08-01T23:40:50.410017: step 12788, loss 0.510255.
Train: 2018-08-01T23:40:50.581527: step 12789, loss 0.545049.
Train: 2018-08-01T23:40:50.758081: step 12790, loss 0.545065.
Test: 2018-08-01T23:40:51.293624: step 12790, loss 0.547894.
Train: 2018-08-01T23:40:51.463170: step 12791, loss 0.527742.
Train: 2018-08-01T23:40:51.632717: step 12792, loss 0.614397.
Train: 2018-08-01T23:40:51.800269: step 12793, loss 0.510466.
Train: 2018-08-01T23:40:51.967823: step 12794, loss 0.545095.
Train: 2018-08-01T23:40:52.133405: step 12795, loss 0.61434.
Train: 2018-08-01T23:40:52.300931: step 12796, loss 0.441306.
Train: 2018-08-01T23:40:52.467488: step 12797, loss 0.54509.
Train: 2018-08-01T23:40:52.642021: step 12798, loss 0.631752.
Train: 2018-08-01T23:40:52.807576: step 12799, loss 0.545073.
Train: 2018-08-01T23:40:52.977124: step 12800, loss 0.631769.
Test: 2018-08-01T23:40:53.511695: step 12800, loss 0.547899.
Train: 2018-08-01T23:40:54.302463: step 12801, loss 0.614388.
Train: 2018-08-01T23:40:54.472041: step 12802, loss 0.59701.
Train: 2018-08-01T23:40:54.641556: step 12803, loss 0.596947.
Train: 2018-08-01T23:40:54.811104: step 12804, loss 0.596873.
Train: 2018-08-01T23:40:54.981680: step 12805, loss 0.528005.
Train: 2018-08-01T23:40:55.148233: step 12806, loss 0.545229.
Train: 2018-08-01T23:40:55.316778: step 12807, loss 0.493811.
Train: 2018-08-01T23:40:55.491286: step 12808, loss 0.493825.
Train: 2018-08-01T23:40:55.667848: step 12809, loss 0.545241.
Train: 2018-08-01T23:40:55.846336: step 12810, loss 0.528054.
Test: 2018-08-01T23:40:56.370934: step 12810, loss 0.548.
Train: 2018-08-01T23:40:56.538520: step 12811, loss 0.596785.
Train: 2018-08-01T23:40:56.702049: step 12812, loss 0.545192.
Train: 2018-08-01T23:40:56.865626: step 12813, loss 0.717377.
Train: 2018-08-01T23:40:57.029204: step 12814, loss 0.528004.
Train: 2018-08-01T23:40:57.198752: step 12815, loss 0.493662.
Train: 2018-08-01T23:40:57.370295: step 12816, loss 0.682712.
Train: 2018-08-01T23:40:57.538811: step 12817, loss 0.52807.
Train: 2018-08-01T23:40:57.705367: step 12818, loss 0.596696.
Train: 2018-08-01T23:40:57.874945: step 12819, loss 0.562397.
Train: 2018-08-01T23:40:58.047477: step 12820, loss 0.476843.
Test: 2018-08-01T23:40:58.582029: step 12820, loss 0.548066.
Train: 2018-08-01T23:40:58.746609: step 12821, loss 0.545283.
Train: 2018-08-01T23:40:58.920149: step 12822, loss 0.545275.
Train: 2018-08-01T23:40:59.085677: step 12823, loss 0.545264.
Train: 2018-08-01T23:40:59.254226: step 12824, loss 0.528104.
Train: 2018-08-01T23:40:59.425806: step 12825, loss 0.59673.
Train: 2018-08-01T23:40:59.591344: step 12826, loss 0.631105.
Train: 2018-08-01T23:40:59.757911: step 12827, loss 0.476545.
Train: 2018-08-01T23:40:59.929421: step 12828, loss 0.545213.
Train: 2018-08-01T23:41:00.102957: step 12829, loss 0.562397.
Train: 2018-08-01T23:41:00.272503: step 12830, loss 0.631259.
Test: 2018-08-01T23:41:00.812070: step 12830, loss 0.547985.
Train: 2018-08-01T23:41:00.988620: step 12831, loss 0.527974.
Train: 2018-08-01T23:41:01.163156: step 12832, loss 0.579615.
Train: 2018-08-01T23:41:01.341655: step 12833, loss 0.596834.
Train: 2018-08-01T23:41:01.508233: step 12834, loss 0.614028.
Train: 2018-08-01T23:41:01.673784: step 12835, loss 0.579588.
Train: 2018-08-01T23:41:01.844327: step 12836, loss 0.631074.
Train: 2018-08-01T23:41:02.010856: step 12837, loss 0.545263.
Train: 2018-08-01T23:41:02.183420: step 12838, loss 0.613717.
Train: 2018-08-01T23:41:02.355935: step 12839, loss 0.54533.
Train: 2018-08-01T23:41:02.526479: step 12840, loss 0.579443.
Test: 2018-08-01T23:41:03.057060: step 12840, loss 0.54816.
Train: 2018-08-01T23:41:03.240569: step 12841, loss 0.528383.
Train: 2018-08-01T23:41:03.405154: step 12842, loss 0.5794.
Train: 2018-08-01T23:41:03.575257: step 12843, loss 0.477546.
Train: 2018-08-01T23:41:03.748328: step 12844, loss 0.630311.
Train: 2018-08-01T23:41:03.920875: step 12845, loss 0.562411.
Train: 2018-08-01T23:41:04.093380: step 12846, loss 0.511559.
Train: 2018-08-01T23:41:04.264945: step 12847, loss 0.562413.
Train: 2018-08-01T23:41:04.441481: step 12848, loss 0.528498.
Train: 2018-08-01T23:41:04.618008: step 12849, loss 0.596347.
Train: 2018-08-01T23:41:04.787524: step 12850, loss 0.613323.
Test: 2018-08-01T23:41:05.326085: step 12850, loss 0.548208.
Train: 2018-08-01T23:41:05.496662: step 12851, loss 0.596336.
Train: 2018-08-01T23:41:05.665178: step 12852, loss 0.545467.
Train: 2018-08-01T23:41:05.841737: step 12853, loss 0.528541.
Train: 2018-08-01T23:41:06.009259: step 12854, loss 0.613228.
Train: 2018-08-01T23:41:06.188778: step 12855, loss 0.511637.
Train: 2018-08-01T23:41:06.357329: step 12856, loss 0.714782.
Train: 2018-08-01T23:41:06.522916: step 12857, loss 0.494837.
Train: 2018-08-01T23:41:06.693428: step 12858, loss 0.461123.
Train: 2018-08-01T23:41:06.861979: step 12859, loss 0.511731.
Train: 2018-08-01T23:41:07.037509: step 12860, loss 0.528574.
Test: 2018-08-01T23:41:07.567093: step 12860, loss 0.548216.
Train: 2018-08-01T23:41:07.738661: step 12861, loss 0.596319.
Train: 2018-08-01T23:41:07.905189: step 12862, loss 0.56241.
Train: 2018-08-01T23:41:08.070748: step 12863, loss 0.511427.
Train: 2018-08-01T23:41:08.246309: step 12864, loss 0.613473.
Train: 2018-08-01T23:41:08.424801: step 12865, loss 0.545364.
Train: 2018-08-01T23:41:08.595372: step 12866, loss 0.528288.
Train: 2018-08-01T23:41:08.766917: step 12867, loss 0.528237.
Train: 2018-08-01T23:41:08.941453: step 12868, loss 0.442618.
Train: 2018-08-01T23:41:09.115969: step 12869, loss 0.510896.
Train: 2018-08-01T23:41:09.301457: step 12870, loss 0.459025.
Test: 2018-08-01T23:41:09.851035: step 12870, loss 0.547912.
Train: 2018-08-01T23:41:10.022560: step 12871, loss 0.597023.
Train: 2018-08-01T23:41:10.201083: step 12872, loss 0.631901.
Train: 2018-08-01T23:41:10.377614: step 12873, loss 0.701716.
Train: 2018-08-01T23:41:10.545157: step 12874, loss 0.614668.
Train: 2018-08-01T23:41:10.711687: step 12875, loss 0.614636.
Train: 2018-08-01T23:41:10.880236: step 12876, loss 0.597184.
Train: 2018-08-01T23:41:11.048785: step 12877, loss 0.475635.
Train: 2018-08-01T23:41:11.221350: step 12878, loss 0.59711.
Train: 2018-08-01T23:41:11.392901: step 12879, loss 0.631751.
Train: 2018-08-01T23:41:11.564407: step 12880, loss 0.700857.
Test: 2018-08-01T23:41:12.087011: step 12880, loss 0.547957.
Train: 2018-08-01T23:41:12.258577: step 12881, loss 0.545152.
Train: 2018-08-01T23:41:12.431093: step 12882, loss 0.476403.
Train: 2018-08-01T23:41:12.597645: step 12883, loss 0.68262.
Train: 2018-08-01T23:41:12.767193: step 12884, loss 0.528147.
Train: 2018-08-01T23:41:12.934776: step 12885, loss 0.459869.
Train: 2018-08-01T23:41:13.108311: step 12886, loss 0.494081.
Train: 2018-08-01T23:41:13.289796: step 12887, loss 0.511136.
Train: 2018-08-01T23:41:13.462334: step 12888, loss 0.562398.
Train: 2018-08-01T23:41:13.627917: step 12889, loss 0.579522.
Train: 2018-08-01T23:41:13.799433: step 12890, loss 0.528123.
Test: 2018-08-01T23:41:14.345006: step 12890, loss 0.548031.
Train: 2018-08-01T23:41:14.514521: step 12891, loss 0.596708.
Train: 2018-08-01T23:41:14.684068: step 12892, loss 0.510902.
Train: 2018-08-01T23:41:14.851620: step 12893, loss 0.562397.
Train: 2018-08-01T23:41:15.022165: step 12894, loss 0.596803.
Train: 2018-08-01T23:41:15.191740: step 12895, loss 0.510765.
Train: 2018-08-01T23:41:15.365278: step 12896, loss 0.596859.
Train: 2018-08-01T23:41:15.533827: step 12897, loss 0.614116.
Train: 2018-08-01T23:41:15.706336: step 12898, loss 0.527929.
Train: 2018-08-01T23:41:15.876898: step 12899, loss 0.665832.
Train: 2018-08-01T23:41:16.056400: step 12900, loss 0.527962.
Test: 2018-08-01T23:41:16.589015: step 12900, loss 0.547989.
Train: 2018-08-01T23:41:17.355903: step 12901, loss 0.527983.
Train: 2018-08-01T23:41:17.529468: step 12902, loss 0.614014.
Train: 2018-08-01T23:41:17.697987: step 12903, loss 0.613972.
Train: 2018-08-01T23:41:17.867535: step 12904, loss 0.648233.
Train: 2018-08-01T23:41:18.036112: step 12905, loss 0.562397.
Train: 2018-08-01T23:41:18.206654: step 12906, loss 0.545311.
Train: 2018-08-01T23:41:18.372185: step 12907, loss 0.613573.
Train: 2018-08-01T23:41:18.546750: step 12908, loss 0.46029.
Train: 2018-08-01T23:41:18.712276: step 12909, loss 0.613432.
Train: 2018-08-01T23:41:18.883818: step 12910, loss 0.63036.
Test: 2018-08-01T23:41:19.420409: step 12910, loss 0.548216.
Train: 2018-08-01T23:41:19.586938: step 12911, loss 0.562413.
Train: 2018-08-01T23:41:19.760476: step 12912, loss 0.613189.
Train: 2018-08-01T23:41:19.929024: step 12913, loss 0.511769.
Train: 2018-08-01T23:41:20.092586: step 12914, loss 0.528702.
Train: 2018-08-01T23:41:20.266122: step 12915, loss 0.596138.
Train: 2018-08-01T23:41:20.439658: step 12916, loss 0.612946.
Train: 2018-08-01T23:41:20.612228: step 12917, loss 0.562443.
Train: 2018-08-01T23:41:20.779750: step 12918, loss 0.461721.
Train: 2018-08-01T23:41:20.950293: step 12919, loss 0.62962.
Train: 2018-08-01T23:41:21.129813: step 12920, loss 0.545668.
Test: 2018-08-01T23:41:21.652433: step 12920, loss 0.548402.
Train: 2018-08-01T23:41:21.820997: step 12921, loss 0.495337.
Train: 2018-08-01T23:41:21.993504: step 12922, loss 0.528864.
Train: 2018-08-01T23:41:22.167071: step 12923, loss 0.512002.
Train: 2018-08-01T23:41:22.335610: step 12924, loss 0.528743.
Train: 2018-08-01T23:41:22.520097: step 12925, loss 0.596192.
Train: 2018-08-01T23:41:22.688646: step 12926, loss 0.511689.
Train: 2018-08-01T23:41:22.858192: step 12927, loss 0.562414.
Train: 2018-08-01T23:41:23.030765: step 12928, loss 0.647316.
Train: 2018-08-01T23:41:23.200309: step 12929, loss 0.528422.
Train: 2018-08-01T23:41:23.369826: step 12930, loss 0.545394.
Test: 2018-08-01T23:41:23.905393: step 12930, loss 0.54814.
Train: 2018-08-01T23:41:24.075945: step 12931, loss 0.477241.
Train: 2018-08-01T23:41:24.242500: step 12932, loss 0.630686.
Train: 2018-08-01T23:41:24.414042: step 12933, loss 0.511124.
Train: 2018-08-01T23:41:24.588593: step 12934, loss 0.562397.
Train: 2018-08-01T23:41:24.760139: step 12935, loss 0.562397.
Train: 2018-08-01T23:41:24.930652: step 12936, loss 0.562397.
Train: 2018-08-01T23:41:25.098204: step 12937, loss 0.682762.
Train: 2018-08-01T23:41:25.264793: step 12938, loss 0.54521.
Train: 2018-08-01T23:41:25.441318: step 12939, loss 0.613945.
Train: 2018-08-01T23:41:25.617835: step 12940, loss 0.528062.
Test: 2018-08-01T23:41:26.141417: step 12940, loss 0.548026.
Train: 2018-08-01T23:41:26.313955: step 12941, loss 0.562397.
Train: 2018-08-01T23:41:26.483502: step 12942, loss 0.476616.
Train: 2018-08-01T23:41:26.659058: step 12943, loss 0.580713.
Train: 2018-08-01T23:41:26.832599: step 12944, loss 0.613939.
Train: 2018-08-01T23:41:27.013087: step 12945, loss 0.545219.
Train: 2018-08-01T23:41:27.181635: step 12946, loss 0.510863.
Train: 2018-08-01T23:41:27.346226: step 12947, loss 0.545206.
Train: 2018-08-01T23:41:27.515773: step 12948, loss 0.51078.
Train: 2018-08-01T23:41:27.688281: step 12949, loss 0.614093.
Train: 2018-08-01T23:41:27.852842: step 12950, loss 0.734821.
Test: 2018-08-01T23:41:28.398409: step 12950, loss 0.547985.
Train: 2018-08-01T23:41:28.569928: step 12951, loss 0.527975.
Train: 2018-08-01T23:41:28.738474: step 12952, loss 0.528013.
Train: 2018-08-01T23:41:28.907023: step 12953, loss 0.510851.
Train: 2018-08-01T23:41:29.078568: step 12954, loss 0.545212.
Train: 2018-08-01T23:41:29.251129: step 12955, loss 0.510822.
Train: 2018-08-01T23:41:29.418655: step 12956, loss 0.510769.
Train: 2018-08-01T23:41:29.587205: step 12957, loss 0.493448.
Train: 2018-08-01T23:41:29.758748: step 12958, loss 0.579681.
Train: 2018-08-01T23:41:29.934278: step 12959, loss 0.47585.
Train: 2018-08-01T23:41:30.098836: step 12960, loss 0.579772.
Test: 2018-08-01T23:41:30.637398: step 12960, loss 0.547849.
Train: 2018-08-01T23:41:30.815920: step 12961, loss 0.510221.
Train: 2018-08-01T23:41:30.988459: step 12962, loss 0.527537.
Train: 2018-08-01T23:41:31.156011: step 12963, loss 0.509955.
Train: 2018-08-01T23:41:31.322566: step 12964, loss 0.562456.
Train: 2018-08-01T23:41:31.498097: step 12965, loss 0.492076.
Train: 2018-08-01T23:41:31.680609: step 12966, loss 0.615464.
Train: 2018-08-01T23:41:31.849157: step 12967, loss 0.474026.
Train: 2018-08-01T23:41:32.020700: step 12968, loss 0.509281.
Train: 2018-08-01T23:41:32.204210: step 12969, loss 0.509134.
Train: 2018-08-01T23:41:32.382757: step 12970, loss 0.508979.
Test: 2018-08-01T23:41:32.920295: step 12970, loss 0.547608.
Train: 2018-08-01T23:41:33.093830: step 12971, loss 0.562628.
Train: 2018-08-01T23:41:33.264375: step 12972, loss 0.562663.
Train: 2018-08-01T23:41:33.434919: step 12973, loss 0.562696.
Train: 2018-08-01T23:41:33.612445: step 12974, loss 0.616978.
Train: 2018-08-01T23:41:33.780994: step 12975, loss 0.490312.
Train: 2018-08-01T23:41:33.957522: step 12976, loss 0.617186.
Train: 2018-08-01T23:41:34.125097: step 12977, loss 0.471999.
Train: 2018-08-01T23:41:34.296640: step 12978, loss 0.617357.
Train: 2018-08-01T23:41:34.472179: step 12979, loss 0.671998.
Train: 2018-08-01T23:41:34.646679: step 12980, loss 0.653706.
Test: 2018-08-01T23:41:35.180279: step 12980, loss 0.547573.
Train: 2018-08-01T23:41:35.357779: step 12981, loss 0.49019.
Train: 2018-08-01T23:41:35.524333: step 12982, loss 0.508382.
Train: 2018-08-01T23:41:35.694888: step 12983, loss 0.598972.
Train: 2018-08-01T23:41:35.865422: step 12984, loss 0.56273.
Train: 2018-08-01T23:41:36.033004: step 12985, loss 0.616936.
Train: 2018-08-01T23:41:36.199532: step 12986, loss 0.562693.
Train: 2018-08-01T23:41:36.376082: step 12987, loss 0.598686.
Train: 2018-08-01T23:41:36.545608: step 12988, loss 0.41891.
Train: 2018-08-01T23:41:36.716146: step 12989, loss 0.562643.
Train: 2018-08-01T23:41:36.882702: step 12990, loss 0.616495.
Test: 2018-08-01T23:41:37.420296: step 12990, loss 0.54761.
Train: 2018-08-01T23:41:37.595822: step 12991, loss 0.43711.
Train: 2018-08-01T23:41:37.775323: step 12992, loss 0.562628.
Train: 2018-08-01T23:41:37.949848: step 12993, loss 0.58057.
Train: 2018-08-01T23:41:38.128371: step 12994, loss 0.634372.
Train: 2018-08-01T23:41:38.298940: step 12995, loss 0.58053.
Train: 2018-08-01T23:41:38.474447: step 12996, loss 0.616266.
Train: 2018-08-01T23:41:38.642025: step 12997, loss 0.509027.
Train: 2018-08-01T23:41:38.814537: step 12998, loss 0.598222.
Train: 2018-08-01T23:41:38.986078: step 12999, loss 0.651531.
Train: 2018-08-01T23:41:39.149642: step 13000, loss 0.527038.
Test: 2018-08-01T23:41:39.685211: step 13000, loss 0.547685.
Train: 2018-08-01T23:41:40.473519: step 13001, loss 0.562512.
Train: 2018-08-01T23:41:40.644062: step 13002, loss 0.491819.
Train: 2018-08-01T23:41:40.808622: step 13003, loss 0.633092.
Train: 2018-08-01T23:41:40.976202: step 13004, loss 0.544861.
Train: 2018-08-01T23:41:41.143726: step 13005, loss 0.650406.
Train: 2018-08-01T23:41:41.320255: step 13006, loss 0.562453.
Train: 2018-08-01T23:41:41.494788: step 13007, loss 0.614933.
Train: 2018-08-01T23:41:41.666329: step 13008, loss 0.475202.
Train: 2018-08-01T23:41:41.842857: step 13009, loss 0.545005.
Train: 2018-08-01T23:41:42.014429: step 13010, loss 0.492835.
Test: 2018-08-01T23:41:42.536016: step 13010, loss 0.547854.
Train: 2018-08-01T23:41:42.705551: step 13011, loss 0.5972.
Train: 2018-08-01T23:41:42.875126: step 13012, loss 0.562415.
Train: 2018-08-01T23:41:43.046639: step 13013, loss 0.63188.
Train: 2018-08-01T23:41:43.215221: step 13014, loss 0.579748.
Train: 2018-08-01T23:41:43.389722: step 13015, loss 0.545095.
Train: 2018-08-01T23:41:43.562261: step 13016, loss 0.666125.
Train: 2018-08-01T23:41:43.725823: step 13017, loss 0.458948.
Train: 2018-08-01T23:41:43.895371: step 13018, loss 0.700199.
Train: 2018-08-01T23:41:44.064916: step 13019, loss 0.579574.
Train: 2018-08-01T23:41:44.230474: step 13020, loss 0.493872.
Test: 2018-08-01T23:41:44.776016: step 13020, loss 0.548074.
Train: 2018-08-01T23:41:44.953542: step 13021, loss 0.493976.
Train: 2018-08-01T23:41:45.128103: step 13022, loss 0.613692.
Train: 2018-08-01T23:41:45.297647: step 13023, loss 0.664879.
Train: 2018-08-01T23:41:45.468166: step 13024, loss 0.545362.
Train: 2018-08-01T23:41:45.636715: step 13025, loss 0.545397.
Train: 2018-08-01T23:41:45.803270: step 13026, loss 0.545424.
Train: 2018-08-01T23:41:45.971819: step 13027, loss 0.579377.
Train: 2018-08-01T23:41:46.154335: step 13028, loss 0.545465.
Train: 2018-08-01T23:41:46.317895: step 13029, loss 0.630157.
Train: 2018-08-01T23:41:46.487441: step 13030, loss 0.663873.
Test: 2018-08-01T23:41:47.029991: step 13030, loss 0.54831.
Train: 2018-08-01T23:41:47.200534: step 13031, loss 0.579291.
Train: 2018-08-01T23:41:47.370081: step 13032, loss 0.61289.
Train: 2018-08-01T23:41:47.535638: step 13033, loss 0.495399.
Train: 2018-08-01T23:41:47.711200: step 13034, loss 0.478791.
Train: 2018-08-01T23:41:47.883710: step 13035, loss 0.562467.
Train: 2018-08-01T23:41:48.053255: step 13036, loss 0.529015.
Train: 2018-08-01T23:41:48.222828: step 13037, loss 0.562466.
Train: 2018-08-01T23:41:48.390373: step 13038, loss 0.595943.
Train: 2018-08-01T23:41:48.560898: step 13039, loss 0.528985.
Train: 2018-08-01T23:41:48.730445: step 13040, loss 0.696446.
Test: 2018-08-01T23:41:49.253047: step 13040, loss 0.548462.
Train: 2018-08-01T23:41:49.425587: step 13041, loss 0.428664.
Train: 2018-08-01T23:41:49.593167: step 13042, loss 0.478772.
Train: 2018-08-01T23:41:49.765677: step 13043, loss 0.579225.
Train: 2018-08-01T23:41:49.933256: step 13044, loss 0.562446.
Train: 2018-08-01T23:41:50.100781: step 13045, loss 0.596086.
Train: 2018-08-01T23:41:50.275315: step 13046, loss 0.629791.
Train: 2018-08-01T23:41:50.440899: step 13047, loss 0.562435.
Train: 2018-08-01T23:41:50.612413: step 13048, loss 0.545597.
Train: 2018-08-01T23:41:50.783981: step 13049, loss 0.596121.
Train: 2018-08-01T23:41:50.951527: step 13050, loss 0.562435.
Test: 2018-08-01T23:41:51.491065: step 13050, loss 0.548333.
Train: 2018-08-01T23:41:51.664600: step 13051, loss 0.612955.
Train: 2018-08-01T23:41:51.831186: step 13052, loss 0.562438.
Train: 2018-08-01T23:41:51.997711: step 13053, loss 0.596077.
Train: 2018-08-01T23:41:52.164296: step 13054, loss 0.562445.
Train: 2018-08-01T23:41:52.330850: step 13055, loss 0.612815.
Train: 2018-08-01T23:41:52.501389: step 13056, loss 0.478623.
Train: 2018-08-01T23:41:52.671939: step 13057, loss 0.52892.
Train: 2018-08-01T23:41:52.836469: step 13058, loss 0.545675.
Train: 2018-08-01T23:41:53.011998: step 13059, loss 0.545656.
Train: 2018-08-01T23:41:53.182543: step 13060, loss 0.495206.
Test: 2018-08-01T23:41:53.711148: step 13060, loss 0.548331.
Train: 2018-08-01T23:41:53.881675: step 13061, loss 0.545592.
Train: 2018-08-01T23:41:54.048229: step 13062, loss 0.596181.
Train: 2018-08-01T23:41:54.220768: step 13063, loss 0.461012.
Train: 2018-08-01T23:41:54.396298: step 13064, loss 0.579361.
Train: 2018-08-01T23:41:54.566843: step 13065, loss 0.54542.
Train: 2018-08-01T23:41:54.734395: step 13066, loss 0.579429.
Train: 2018-08-01T23:41:54.901948: step 13067, loss 0.664748.
Train: 2018-08-01T23:41:55.069532: step 13068, loss 0.5624.
Train: 2018-08-01T23:41:55.243036: step 13069, loss 0.477061.
Train: 2018-08-01T23:41:55.417569: step 13070, loss 0.494029.
Test: 2018-08-01T23:41:55.942180: step 13070, loss 0.548052.
Train: 2018-08-01T23:41:56.109742: step 13071, loss 0.562396.
Train: 2018-08-01T23:41:56.272294: step 13072, loss 0.493736.
Train: 2018-08-01T23:41:56.444848: step 13073, loss 0.545185.
Train: 2018-08-01T23:41:56.610379: step 13074, loss 0.527883.
Train: 2018-08-01T23:41:56.782921: step 13075, loss 0.648945.
Train: 2018-08-01T23:41:56.965446: step 13076, loss 0.562408.
Train: 2018-08-01T23:41:57.129604: step 13077, loss 0.562411.
Train: 2018-08-01T23:41:57.299853: step 13078, loss 0.545043.
Train: 2018-08-01T23:41:57.472420: step 13079, loss 0.631981.
Train: 2018-08-01T23:41:57.640931: step 13080, loss 0.631985.
Test: 2018-08-01T23:41:58.181486: step 13080, loss 0.547864.
Train: 2018-08-01T23:41:58.352034: step 13081, loss 0.579791.
Train: 2018-08-01T23:41:58.528590: step 13082, loss 0.458269.
Train: 2018-08-01T23:41:58.710073: step 13083, loss 0.614503.
Train: 2018-08-01T23:41:58.879647: step 13084, loss 0.631838.
Train: 2018-08-01T23:41:59.047173: step 13085, loss 0.545074.
Train: 2018-08-01T23:41:59.214725: step 13086, loss 0.614356.
Train: 2018-08-01T23:41:59.385304: step 13087, loss 0.527826.
Train: 2018-08-01T23:41:59.552826: step 13088, loss 0.596943.
Train: 2018-08-01T23:41:59.715412: step 13089, loss 0.648633.
Train: 2018-08-01T23:41:59.882938: step 13090, loss 0.562397.
Test: 2018-08-01T23:42:00.413521: step 13090, loss 0.548021.
Train: 2018-08-01T23:42:00.587057: step 13091, loss 0.562396.
Train: 2018-08-01T23:42:00.754633: step 13092, loss 0.562396.
Train: 2018-08-01T23:42:00.925186: step 13093, loss 0.562397.
Train: 2018-08-01T23:42:01.090709: step 13094, loss 0.562399.
Train: 2018-08-01T23:42:01.268235: step 13095, loss 0.494205.
Train: 2018-08-01T23:42:01.443765: step 13096, loss 0.511271.
Train: 2018-08-01T23:42:01.701077: step 13097, loss 0.681755.
Train: 2018-08-01T23:42:01.868631: step 13098, loss 0.681608.
Train: 2018-08-01T23:42:02.051142: step 13099, loss 0.562408.
Train: 2018-08-01T23:42:02.222683: step 13100, loss 0.562414.
Test: 2018-08-01T23:42:02.757278: step 13100, loss 0.548266.
Train: 2018-08-01T23:42:03.598071: step 13101, loss 0.613133.
Train: 2018-08-01T23:42:03.776594: step 13102, loss 0.646728.
Train: 2018-08-01T23:42:03.943149: step 13103, loss 0.512044.
Train: 2018-08-01T23:42:04.112697: step 13104, loss 0.528936.
Train: 2018-08-01T23:42:04.283271: step 13105, loss 0.579199.
Train: 2018-08-01T23:42:04.454781: step 13106, loss 0.579181.
Train: 2018-08-01T23:42:04.624353: step 13107, loss 0.612523.
Train: 2018-08-01T23:42:04.791910: step 13108, loss 0.579142.
Train: 2018-08-01T23:42:04.957448: step 13109, loss 0.496065.
Train: 2018-08-01T23:42:05.125016: step 13110, loss 0.579115.
Test: 2018-08-01T23:42:05.643622: step 13110, loss 0.548634.
Train: 2018-08-01T23:42:05.819134: step 13111, loss 0.579108.
Train: 2018-08-01T23:42:05.987728: step 13112, loss 0.529387.
Train: 2018-08-01T23:42:06.157230: step 13113, loss 0.612234.
Train: 2018-08-01T23:42:06.324782: step 13114, loss 0.545981.
Train: 2018-08-01T23:42:06.491338: step 13115, loss 0.562539.
Train: 2018-08-01T23:42:06.661912: step 13116, loss 0.397084.
Train: 2018-08-01T23:42:06.827439: step 13117, loss 0.463014.
Train: 2018-08-01T23:42:06.991999: step 13118, loss 0.445976.
Train: 2018-08-01T23:42:07.158585: step 13119, loss 0.595927.
Train: 2018-08-01T23:42:07.325133: step 13120, loss 0.579245.
Test: 2018-08-01T23:42:07.866661: step 13120, loss 0.548312.
Train: 2018-08-01T23:42:08.052165: step 13121, loss 0.596149.
Train: 2018-08-01T23:42:08.212741: step 13122, loss 0.444078.
Train: 2018-08-01T23:42:08.386291: step 13123, loss 0.596362.
Train: 2018-08-01T23:42:08.552845: step 13124, loss 0.579435.
Train: 2018-08-01T23:42:08.723389: step 13125, loss 0.528239.
Train: 2018-08-01T23:42:08.887975: step 13126, loss 0.493875.
Train: 2018-08-01T23:42:09.061518: step 13127, loss 0.562396.
Train: 2018-08-01T23:42:09.235021: step 13128, loss 0.562399.
Train: 2018-08-01T23:42:09.403601: step 13129, loss 0.683481.
Train: 2018-08-01T23:42:09.572120: step 13130, loss 0.493154.
Test: 2018-08-01T23:42:10.111679: step 13130, loss 0.547887.
Train: 2018-08-01T23:42:10.280258: step 13131, loss 0.631781.
Train: 2018-08-01T23:42:10.450780: step 13132, loss 0.527704.
Train: 2018-08-01T23:42:10.624307: step 13133, loss 0.440823.
Train: 2018-08-01T23:42:10.789865: step 13134, loss 0.562421.
Train: 2018-08-01T23:42:10.953454: step 13135, loss 0.562429.
Train: 2018-08-01T23:42:11.127961: step 13136, loss 0.562437.
Train: 2018-08-01T23:42:11.303491: step 13137, loss 0.544933.
Train: 2018-08-01T23:42:11.477028: step 13138, loss 0.615072.
Train: 2018-08-01T23:42:11.647573: step 13139, loss 0.562456.
Train: 2018-08-01T23:42:11.814127: step 13140, loss 0.562459.
Test: 2018-08-01T23:42:12.342715: step 13140, loss 0.547748.
Train: 2018-08-01T23:42:12.511264: step 13141, loss 0.527321.
Train: 2018-08-01T23:42:12.681839: step 13142, loss 0.58005.
Train: 2018-08-01T23:42:12.854347: step 13143, loss 0.597651.
Train: 2018-08-01T23:42:13.018907: step 13144, loss 0.632821.
Train: 2018-08-01T23:42:13.193440: step 13145, loss 0.527324.
Train: 2018-08-01T23:42:13.365978: step 13146, loss 0.509786.
Train: 2018-08-01T23:42:13.542507: step 13147, loss 0.61513.
Train: 2018-08-01T23:42:13.710059: step 13148, loss 0.667723.
Train: 2018-08-01T23:42:13.883623: step 13149, loss 0.509919.
Train: 2018-08-01T23:42:14.048180: step 13150, loss 0.475007.
Test: 2018-08-01T23:42:14.577740: step 13150, loss 0.547795.
Train: 2018-08-01T23:42:14.744294: step 13151, loss 0.562437.
Train: 2018-08-01T23:42:14.909884: step 13152, loss 0.457536.
Train: 2018-08-01T23:42:15.092363: step 13153, loss 0.474917.
Train: 2018-08-01T23:42:15.257949: step 13154, loss 0.632627.
Train: 2018-08-01T23:42:15.429486: step 13155, loss 0.562459.
Train: 2018-08-01T23:42:15.596050: step 13156, loss 0.492162.
Train: 2018-08-01T23:42:15.758611: step 13157, loss 0.597678.
Train: 2018-08-01T23:42:15.932119: step 13158, loss 0.544859.
Train: 2018-08-01T23:42:16.098673: step 13159, loss 0.597753.
Train: 2018-08-01T23:42:16.266227: step 13160, loss 0.562485.
Test: 2018-08-01T23:42:16.799799: step 13160, loss 0.547712.
Train: 2018-08-01T23:42:16.977351: step 13161, loss 0.509555.
Train: 2018-08-01T23:42:17.155848: step 13162, loss 0.562492.
Train: 2018-08-01T23:42:17.323400: step 13163, loss 0.509491.
Train: 2018-08-01T23:42:17.488990: step 13164, loss 0.509439.
Train: 2018-08-01T23:42:17.658534: step 13165, loss 0.456214.
Train: 2018-08-01T23:42:17.828050: step 13166, loss 0.562537.
Train: 2018-08-01T23:42:17.996600: step 13167, loss 0.65159.
Train: 2018-08-01T23:42:18.164151: step 13168, loss 0.491277.
Train: 2018-08-01T23:42:18.330707: step 13169, loss 0.54473.
Train: 2018-08-01T23:42:18.511223: step 13170, loss 0.419582.
Test: 2018-08-01T23:42:19.022857: step 13170, loss 0.54761.
Train: 2018-08-01T23:42:19.210356: step 13171, loss 0.580555.
Train: 2018-08-01T23:42:19.380899: step 13172, loss 0.526704.
Train: 2018-08-01T23:42:19.556444: step 13173, loss 0.616728.
Train: 2018-08-01T23:42:19.728969: step 13174, loss 0.544653.
Train: 2018-08-01T23:42:19.908488: step 13175, loss 0.472386.
Train: 2018-08-01T23:42:20.078035: step 13176, loss 0.617047.
Train: 2018-08-01T23:42:20.241599: step 13177, loss 0.635243.
Train: 2018-08-01T23:42:20.411145: step 13178, loss 0.508385.
Train: 2018-08-01T23:42:20.580692: step 13179, loss 0.490238.
Train: 2018-08-01T23:42:20.748273: step 13180, loss 0.417572.
Test: 2018-08-01T23:42:21.285807: step 13180, loss 0.547569.
Train: 2018-08-01T23:42:21.455353: step 13181, loss 0.581005.
Train: 2018-08-01T23:42:21.628890: step 13182, loss 0.581068.
Train: 2018-08-01T23:42:21.797472: step 13183, loss 0.59937.
Train: 2018-08-01T23:42:21.968981: step 13184, loss 0.599401.
Train: 2018-08-01T23:42:22.142517: step 13185, loss 0.58113.
Train: 2018-08-01T23:42:22.313061: step 13186, loss 0.690658.
Train: 2018-08-01T23:42:22.482645: step 13187, loss 0.544606.
Train: 2018-08-01T23:42:22.651157: step 13188, loss 0.58098.
Train: 2018-08-01T23:42:22.819706: step 13189, loss 0.562767.
Train: 2018-08-01T23:42:22.995236: step 13190, loss 0.598954.
Test: 2018-08-01T23:42:23.511878: step 13190, loss 0.547583.
Train: 2018-08-01T23:42:23.679408: step 13191, loss 0.580771.
Train: 2018-08-01T23:42:23.857930: step 13192, loss 0.598708.
Train: 2018-08-01T23:42:24.026508: step 13193, loss 0.508756.
Train: 2018-08-01T23:42:24.194059: step 13194, loss 0.670166.
Train: 2018-08-01T23:42:24.363610: step 13195, loss 0.598314.
Train: 2018-08-01T23:42:24.533126: step 13196, loss 0.669345.
Train: 2018-08-01T23:42:24.712666: step 13197, loss 0.633372.
Train: 2018-08-01T23:42:24.878222: step 13198, loss 0.456754.
Train: 2018-08-01T23:42:25.043785: step 13199, loss 0.580017.
Train: 2018-08-01T23:42:25.221286: step 13200, loss 0.562441.
Test: 2018-08-01T23:42:25.769820: step 13200, loss 0.547821.
Train: 2018-08-01T23:42:26.551154: step 13201, loss 0.632198.
Train: 2018-08-01T23:42:26.723702: step 13202, loss 0.666668.
Train: 2018-08-01T23:42:26.900215: step 13203, loss 0.562403.
Train: 2018-08-01T23:42:27.075753: step 13204, loss 0.545183.
Train: 2018-08-01T23:42:27.245274: step 13205, loss 0.596693.
Train: 2018-08-01T23:42:27.418840: step 13206, loss 0.562398.
Train: 2018-08-01T23:42:27.586363: step 13207, loss 0.681553.
Train: 2018-08-01T23:42:27.754943: step 13208, loss 0.680993.
Train: 2018-08-01T23:42:27.925479: step 13209, loss 0.427722.
Train: 2018-08-01T23:42:28.093039: step 13210, loss 0.629577.
Test: 2018-08-01T23:42:28.630571: step 13210, loss 0.548474.
Train: 2018-08-01T23:42:28.801139: step 13211, loss 0.512326.
Train: 2018-08-01T23:42:28.969683: step 13212, loss 0.662499.
Train: 2018-08-01T23:42:29.144198: step 13213, loss 0.562514.
Train: 2018-08-01T23:42:29.316771: step 13214, loss 0.579087.
Train: 2018-08-01T23:42:29.482293: step 13215, loss 0.513079.
Train: 2018-08-01T23:42:29.656853: step 13216, loss 0.529659.
Train: 2018-08-01T23:42:29.835369: step 13217, loss 0.480383.
Train: 2018-08-01T23:42:30.002934: step 13218, loss 0.661269.
Train: 2018-08-01T23:42:30.169485: step 13219, loss 0.464031.
Train: 2018-08-01T23:42:30.347980: step 13220, loss 0.579034.
Test: 2018-08-01T23:42:30.880565: step 13220, loss 0.54882.
Train: 2018-08-01T23:42:31.053095: step 13221, loss 0.562592.
Train: 2018-08-01T23:42:31.223669: step 13222, loss 0.546132.
Train: 2018-08-01T23:42:31.393185: step 13223, loss 0.628454.
Train: 2018-08-01T23:42:31.561761: step 13224, loss 0.480242.
Train: 2018-08-01T23:42:31.735302: step 13225, loss 0.529591.
Train: 2018-08-01T23:42:31.902823: step 13226, loss 0.579072.
Train: 2018-08-01T23:42:32.068380: step 13227, loss 0.562542.
Train: 2018-08-01T23:42:32.234935: step 13228, loss 0.545966.
Train: 2018-08-01T23:42:32.405511: step 13229, loss 0.462965.
Train: 2018-08-01T23:42:32.583019: step 13230, loss 0.495928.
Test: 2018-08-01T23:42:33.128546: step 13230, loss 0.548486.
Train: 2018-08-01T23:42:33.302106: step 13231, loss 0.51236.
Train: 2018-08-01T23:42:33.474622: step 13232, loss 0.596002.
Train: 2018-08-01T23:42:33.639207: step 13233, loss 0.545606.
Train: 2018-08-01T23:42:33.806757: step 13234, loss 0.511766.
Train: 2018-08-01T23:42:33.974285: step 13235, loss 0.579361.
Train: 2018-08-01T23:42:34.146824: step 13236, loss 0.528402.
Train: 2018-08-01T23:42:34.316402: step 13237, loss 0.613574.
Train: 2018-08-01T23:42:34.486916: step 13238, loss 0.528204.
Train: 2018-08-01T23:42:34.654466: step 13239, loss 0.596676.
Train: 2018-08-01T23:42:34.834021: step 13240, loss 0.63108.
Test: 2018-08-01T23:42:35.373545: step 13240, loss 0.548008.
Train: 2018-08-01T23:42:35.543091: step 13241, loss 0.665493.
Train: 2018-08-01T23:42:35.715630: step 13242, loss 0.613904.
Train: 2018-08-01T23:42:35.890193: step 13243, loss 0.579541.
Train: 2018-08-01T23:42:36.054725: step 13244, loss 0.617179.
Train: 2018-08-01T23:42:36.221278: step 13245, loss 0.579482.
Train: 2018-08-01T23:42:36.384875: step 13246, loss 0.528305.
Train: 2018-08-01T23:42:36.556413: step 13247, loss 0.545379.
Train: 2018-08-01T23:42:36.727925: step 13248, loss 0.545398.
Train: 2018-08-01T23:42:36.901471: step 13249, loss 0.613392.
Train: 2018-08-01T23:42:37.063029: step 13250, loss 0.409643.
Test: 2018-08-01T23:42:37.589620: step 13250, loss 0.548178.
Train: 2018-08-01T23:42:37.756176: step 13251, loss 0.477451.
Train: 2018-08-01T23:42:37.927717: step 13252, loss 0.528348.
Train: 2018-08-01T23:42:38.096267: step 13253, loss 0.579467.
Train: 2018-08-01T23:42:38.264815: step 13254, loss 0.6137.
Train: 2018-08-01T23:42:38.439349: step 13255, loss 0.682228.
Train: 2018-08-01T23:42:38.624854: step 13256, loss 0.579504.
Train: 2018-08-01T23:42:38.794400: step 13257, loss 0.528212.
Train: 2018-08-01T23:42:38.962950: step 13258, loss 0.528222.
Train: 2018-08-01T23:42:39.135489: step 13259, loss 0.664948.
Train: 2018-08-01T23:42:39.309051: step 13260, loss 0.528256.
Test: 2018-08-01T23:42:39.835618: step 13260, loss 0.548113.
Train: 2018-08-01T23:42:40.004166: step 13261, loss 0.5624.
Train: 2018-08-01T23:42:40.174743: step 13262, loss 0.579453.
Train: 2018-08-01T23:42:40.342290: step 13263, loss 0.562401.
Train: 2018-08-01T23:42:40.517822: step 13264, loss 0.647558.
Train: 2018-08-01T23:42:40.688337: step 13265, loss 0.613413.
Train: 2018-08-01T23:42:40.853925: step 13266, loss 0.477583.
Train: 2018-08-01T23:42:41.021447: step 13267, loss 0.579364.
Train: 2018-08-01T23:42:41.194011: step 13268, loss 0.613227.
Train: 2018-08-01T23:42:41.365527: step 13269, loss 0.579331.
Train: 2018-08-01T23:42:41.533079: step 13270, loss 0.562424.
Test: 2018-08-01T23:42:42.059701: step 13270, loss 0.548307.
Train: 2018-08-01T23:42:42.227254: step 13271, loss 0.697346.
Train: 2018-08-01T23:42:42.399763: step 13272, loss 0.596069.
Train: 2018-08-01T23:42:42.563325: step 13273, loss 0.528935.
Train: 2018-08-01T23:42:42.734867: step 13274, loss 0.595914.
Train: 2018-08-01T23:42:42.907436: step 13275, loss 0.462396.
Train: 2018-08-01T23:42:43.075979: step 13276, loss 0.662506.
Train: 2018-08-01T23:42:43.245532: step 13277, loss 0.495953.
Train: 2018-08-01T23:42:43.421058: step 13278, loss 0.595755.
Train: 2018-08-01T23:42:43.590579: step 13279, loss 0.628943.
Train: 2018-08-01T23:42:43.755164: step 13280, loss 0.512792.
Test: 2018-08-01T23:42:44.280749: step 13280, loss 0.54866.
Train: 2018-08-01T23:42:44.452308: step 13281, loss 0.628791.
Train: 2018-08-01T23:42:44.630830: step 13282, loss 0.579083.
Train: 2018-08-01T23:42:44.808324: step 13283, loss 0.645125.
Train: 2018-08-01T23:42:44.980863: step 13284, loss 0.611995.
Train: 2018-08-01T23:42:45.151407: step 13285, loss 0.513361.
Train: 2018-08-01T23:42:45.323945: step 13286, loss 0.546219.
Train: 2018-08-01T23:42:45.494521: step 13287, loss 0.59539.
Train: 2018-08-01T23:42:45.658083: step 13288, loss 0.562643.
Train: 2018-08-01T23:42:45.828626: step 13289, loss 0.628017.
Train: 2018-08-01T23:42:45.990164: step 13290, loss 0.578985.
Test: 2018-08-01T23:42:46.528727: step 13290, loss 0.549057.
Train: 2018-08-01T23:42:46.699269: step 13291, loss 0.513841.
Train: 2018-08-01T23:42:46.875796: step 13292, loss 0.562699.
Train: 2018-08-01T23:42:47.040358: step 13293, loss 0.562704.
Train: 2018-08-01T23:42:47.204918: step 13294, loss 0.530178.
Train: 2018-08-01T23:42:47.371471: step 13295, loss 0.611518.
Train: 2018-08-01T23:42:47.545007: step 13296, loss 0.497622.
Train: 2018-08-01T23:42:47.715568: step 13297, loss 0.464968.
Train: 2018-08-01T23:42:47.884102: step 13298, loss 0.595322.
Train: 2018-08-01T23:42:48.050683: step 13299, loss 0.513559.
Train: 2018-08-01T23:42:48.214243: step 13300, loss 0.562617.
Test: 2018-08-01T23:42:48.756768: step 13300, loss 0.548822.
Train: 2018-08-01T23:42:49.591589: step 13301, loss 0.546147.
Train: 2018-08-01T23:42:49.759105: step 13302, loss 0.595551.
Train: 2018-08-01T23:42:49.928652: step 13303, loss 0.529506.
Train: 2018-08-01T23:42:50.096229: step 13304, loss 0.562535.
Train: 2018-08-01T23:42:50.265750: step 13305, loss 0.462908.
Train: 2018-08-01T23:42:50.437293: step 13306, loss 0.495848.
Train: 2018-08-01T23:42:50.605842: step 13307, loss 0.51227.
Train: 2018-08-01T23:42:50.768407: step 13308, loss 0.596066.
Train: 2018-08-01T23:42:50.936957: step 13309, loss 0.545557.
Train: 2018-08-01T23:42:51.109496: step 13310, loss 0.630157.
Test: 2018-08-01T23:42:51.637085: step 13310, loss 0.548197.
Train: 2018-08-01T23:42:51.806633: step 13311, loss 0.528462.
Train: 2018-08-01T23:42:51.976179: step 13312, loss 0.47731.
Train: 2018-08-01T23:42:52.139742: step 13313, loss 0.579481.
Train: 2018-08-01T23:42:52.315272: step 13314, loss 0.528138.
Train: 2018-08-01T23:42:52.494823: step 13315, loss 0.528031.
Train: 2018-08-01T23:42:52.664339: step 13316, loss 0.579642.
Train: 2018-08-01T23:42:52.827901: step 13317, loss 0.666128.
Train: 2018-08-01T23:42:52.994456: step 13318, loss 0.493189.
Train: 2018-08-01T23:42:53.160039: step 13319, loss 0.475733.
Train: 2018-08-01T23:42:53.326569: step 13320, loss 0.614573.
Test: 2018-08-01T23:42:53.867138: step 13320, loss 0.547843.
Train: 2018-08-01T23:42:54.038666: step 13321, loss 0.57984.
Train: 2018-08-01T23:42:54.210213: step 13322, loss 0.649606.
Train: 2018-08-01T23:42:54.385743: step 13323, loss 0.632167.
Train: 2018-08-01T23:42:54.551295: step 13324, loss 0.527591.
Train: 2018-08-01T23:42:54.713860: step 13325, loss 0.666873.
Train: 2018-08-01T23:42:54.883406: step 13326, loss 0.527665.
Train: 2018-08-01T23:42:55.061930: step 13327, loss 0.510352.
Train: 2018-08-01T23:42:55.229482: step 13328, loss 0.597103.
Train: 2018-08-01T23:42:55.398032: step 13329, loss 0.510422.
Train: 2018-08-01T23:42:55.562591: step 13330, loss 0.56241.
Test: 2018-08-01T23:42:56.099157: step 13330, loss 0.547903.
Train: 2018-08-01T23:42:56.264714: step 13331, loss 0.562409.
Train: 2018-08-01T23:42:56.431268: step 13332, loss 0.614368.
Train: 2018-08-01T23:42:56.597824: step 13333, loss 0.527799.
Train: 2018-08-01T23:42:56.762384: step 13334, loss 0.614299.
Train: 2018-08-01T23:42:56.935947: step 13335, loss 0.683359.
Train: 2018-08-01T23:42:57.109490: step 13336, loss 0.545167.
Train: 2018-08-01T23:42:57.277034: step 13337, loss 0.562399.
Train: 2018-08-01T23:42:57.442566: step 13338, loss 0.648222.
Train: 2018-08-01T23:42:57.609152: step 13339, loss 0.476825.
Train: 2018-08-01T23:42:57.774677: step 13340, loss 0.54531.
Test: 2018-08-01T23:42:58.313238: step 13340, loss 0.548105.
Train: 2018-08-01T23:42:58.497744: step 13341, loss 0.545329.
Train: 2018-08-01T23:42:58.659339: step 13342, loss 0.596522.
Train: 2018-08-01T23:42:58.836840: step 13343, loss 0.545362.
Train: 2018-08-01T23:42:59.004421: step 13344, loss 0.460228.
Train: 2018-08-01T23:42:59.173937: step 13345, loss 0.613531.
Train: 2018-08-01T23:42:59.344512: step 13346, loss 0.54536.
Train: 2018-08-01T23:42:59.518018: step 13347, loss 0.613546.
Train: 2018-08-01T23:42:59.692552: step 13348, loss 0.613524.
Train: 2018-08-01T23:42:59.863094: step 13349, loss 0.562405.
Train: 2018-08-01T23:43:00.029650: step 13350, loss 0.528395.
Test: 2018-08-01T23:43:00.570205: step 13350, loss 0.548172.
Train: 2018-08-01T23:43:00.735793: step 13351, loss 0.528407.
Train: 2018-08-01T23:43:00.898328: step 13352, loss 0.477392.
Train: 2018-08-01T23:43:01.062913: step 13353, loss 0.596457.
Train: 2018-08-01T23:43:01.233457: step 13354, loss 0.613522.
Train: 2018-08-01T23:43:01.400984: step 13355, loss 0.494243.
Train: 2018-08-01T23:43:01.573524: step 13356, loss 0.528288.
Train: 2018-08-01T23:43:01.745064: step 13357, loss 0.59656.
Train: 2018-08-01T23:43:01.908652: step 13358, loss 0.596586.
Train: 2018-08-01T23:43:02.071192: step 13359, loss 0.562398.
Train: 2018-08-01T23:43:02.236750: step 13360, loss 0.545297.
Test: 2018-08-01T23:43:02.771356: step 13360, loss 0.548071.
Train: 2018-08-01T23:43:02.938898: step 13361, loss 0.493962.
Train: 2018-08-01T23:43:03.116397: step 13362, loss 0.630926.
Train: 2018-08-01T23:43:03.293924: step 13363, loss 0.54526.
Train: 2018-08-01T23:43:03.459480: step 13364, loss 0.613833.
Train: 2018-08-01T23:43:03.625069: step 13365, loss 0.596679.
Train: 2018-08-01T23:43:03.798574: step 13366, loss 0.613784.
Train: 2018-08-01T23:43:03.963135: step 13367, loss 0.59661.
Train: 2018-08-01T23:43:04.128691: step 13368, loss 0.596555.
Train: 2018-08-01T23:43:04.294275: step 13369, loss 0.630579.
Train: 2018-08-01T23:43:04.461819: step 13370, loss 0.545408.
Test: 2018-08-01T23:43:05.002356: step 13370, loss 0.548208.
Train: 2018-08-01T23:43:05.180905: step 13371, loss 0.596335.
Train: 2018-08-01T23:43:05.349429: step 13372, loss 0.579339.
Train: 2018-08-01T23:43:05.521001: step 13373, loss 0.663715.
Train: 2018-08-01T23:43:05.698526: step 13374, loss 0.612907.
Train: 2018-08-01T23:43:05.873058: step 13375, loss 0.595974.
Train: 2018-08-01T23:43:06.049557: step 13376, loss 0.495707.
Train: 2018-08-01T23:43:06.219127: step 13377, loss 0.662409.
Train: 2018-08-01T23:43:06.387654: step 13378, loss 0.545912.
Train: 2018-08-01T23:43:06.557200: step 13379, loss 0.529518.
Train: 2018-08-01T23:43:06.741740: step 13380, loss 0.579073.
Test: 2018-08-01T23:43:07.267333: step 13380, loss 0.548783.
Train: 2018-08-01T23:43:07.444890: step 13381, loss 0.595547.
Train: 2018-08-01T23:43:07.612412: step 13382, loss 0.513253.
Train: 2018-08-01T23:43:07.781969: step 13383, loss 0.529736.
Train: 2018-08-01T23:43:07.951536: step 13384, loss 0.628313.
Train: 2018-08-01T23:43:08.121080: step 13385, loss 0.496974.
Train: 2018-08-01T23:43:08.291598: step 13386, loss 0.546199.
Train: 2018-08-01T23:43:08.469146: step 13387, loss 0.513344.
Train: 2018-08-01T23:43:08.634678: step 13388, loss 0.513266.
Train: 2018-08-01T23:43:08.799268: step 13389, loss 0.546099.
Train: 2018-08-01T23:43:08.960833: step 13390, loss 0.496501.
Test: 2018-08-01T23:43:09.500395: step 13390, loss 0.548661.
Train: 2018-08-01T23:43:09.665946: step 13391, loss 0.479705.
Train: 2018-08-01T23:43:09.833475: step 13392, loss 0.579137.
Train: 2018-08-01T23:43:10.004018: step 13393, loss 0.445631.
Train: 2018-08-01T23:43:10.171600: step 13394, loss 0.596005.
Train: 2018-08-01T23:43:10.340145: step 13395, loss 0.545593.
Train: 2018-08-01T23:43:10.505678: step 13396, loss 0.528604.
Train: 2018-08-01T23:43:10.670237: step 13397, loss 0.613342.
Train: 2018-08-01T23:43:10.836792: step 13398, loss 0.494297.
Train: 2018-08-01T23:43:11.004344: step 13399, loss 0.5624.
Train: 2018-08-01T23:43:11.174889: step 13400, loss 0.596686.
Test: 2018-08-01T23:43:11.707495: step 13400, loss 0.548008.
Train: 2018-08-01T23:43:12.569075: step 13401, loss 0.545213.
Train: 2018-08-01T23:43:12.744598: step 13402, loss 0.545173.
Train: 2018-08-01T23:43:12.907139: step 13403, loss 0.562403.
Train: 2018-08-01T23:43:13.073694: step 13404, loss 0.597018.
Train: 2018-08-01T23:43:13.243240: step 13405, loss 0.63173.
Train: 2018-08-01T23:43:13.419770: step 13406, loss 0.666424.
Train: 2018-08-01T23:43:13.589347: step 13407, loss 0.631671.
Train: 2018-08-01T23:43:13.757875: step 13408, loss 0.545123.
Train: 2018-08-01T23:43:13.924451: step 13409, loss 0.545148.
Train: 2018-08-01T23:43:14.089978: step 13410, loss 0.562401.
Test: 2018-08-01T23:43:14.625547: step 13410, loss 0.547985.
Train: 2018-08-01T23:43:14.792100: step 13411, loss 0.527972.
Train: 2018-08-01T23:43:14.962645: step 13412, loss 0.562399.
Train: 2018-08-01T23:43:15.126207: step 13413, loss 0.528007.
Train: 2018-08-01T23:43:15.296751: step 13414, loss 0.648381.
Train: 2018-08-01T23:43:15.472282: step 13415, loss 0.631104.
Train: 2018-08-01T23:43:15.645850: step 13416, loss 0.562398.
Train: 2018-08-01T23:43:15.813370: step 13417, loss 0.630844.
Train: 2018-08-01T23:43:15.980923: step 13418, loss 0.69894.
Train: 2018-08-01T23:43:16.150469: step 13419, loss 0.51142.
Train: 2018-08-01T23:43:16.312068: step 13420, loss 0.57936.
Test: 2018-08-01T23:43:16.845610: step 13420, loss 0.548277.
Train: 2018-08-01T23:43:17.010170: step 13421, loss 0.511741.
Train: 2018-08-01T23:43:17.172737: step 13422, loss 0.697333.
Train: 2018-08-01T23:43:17.336298: step 13423, loss 0.562446.
Train: 2018-08-01T23:43:17.504880: step 13424, loss 0.461963.
Train: 2018-08-01T23:43:17.683395: step 13425, loss 0.646107.
Train: 2018-08-01T23:43:17.846933: step 13426, loss 0.495726.
Train: 2018-08-01T23:43:18.021468: step 13427, loss 0.495802.
Train: 2018-08-01T23:43:18.199991: step 13428, loss 0.679204.
Train: 2018-08-01T23:43:18.376563: step 13429, loss 0.612444.
Train: 2018-08-01T23:43:18.546103: step 13430, loss 0.612359.
Test: 2018-08-01T23:43:19.076646: step 13430, loss 0.548647.
Train: 2018-08-01T23:43:19.244198: step 13431, loss 0.595681.
Train: 2018-08-01T23:43:19.410753: step 13432, loss 0.595616.
Train: 2018-08-01T23:43:19.578305: step 13433, loss 0.59555.
Train: 2018-08-01T23:43:19.749847: step 13434, loss 0.431038.
Train: 2018-08-01T23:43:19.924381: step 13435, loss 0.480408.
Train: 2018-08-01T23:43:20.090936: step 13436, loss 0.546134.
Train: 2018-08-01T23:43:20.256492: step 13437, loss 0.447251.
Train: 2018-08-01T23:43:20.426040: step 13438, loss 0.628649.
Train: 2018-08-01T23:43:20.595586: step 13439, loss 0.545984.
Train: 2018-08-01T23:43:20.764136: step 13440, loss 0.496174.
Test: 2018-08-01T23:43:21.299704: step 13440, loss 0.548573.
Train: 2018-08-01T23:43:21.478227: step 13441, loss 0.612406.
Train: 2018-08-01T23:43:21.651764: step 13442, loss 0.512489.
Train: 2018-08-01T23:43:21.813340: step 13443, loss 0.529056.
Train: 2018-08-01T23:43:21.979885: step 13444, loss 0.545702.
Train: 2018-08-01T23:43:22.147439: step 13445, loss 0.512029.
Train: 2018-08-01T23:43:22.313027: step 13446, loss 0.596154.
Train: 2018-08-01T23:43:22.479580: step 13447, loss 0.494798.
Train: 2018-08-01T23:43:22.647132: step 13448, loss 0.562412.
Train: 2018-08-01T23:43:22.821637: step 13449, loss 0.562405.
Train: 2018-08-01T23:43:22.989187: step 13450, loss 0.442982.
Test: 2018-08-01T23:43:23.520797: step 13450, loss 0.548053.
Train: 2018-08-01T23:43:23.693313: step 13451, loss 0.562397.
Train: 2018-08-01T23:43:23.859861: step 13452, loss 0.648364.
Train: 2018-08-01T23:43:24.036390: step 13453, loss 0.527939.
Train: 2018-08-01T23:43:24.205934: step 13454, loss 0.596944.
Train: 2018-08-01T23:43:24.373488: step 13455, loss 0.735399.
Train: 2018-08-01T23:43:24.547023: step 13456, loss 0.458696.
Train: 2018-08-01T23:43:24.714576: step 13457, loss 0.493225.
Train: 2018-08-01T23:43:24.883125: step 13458, loss 0.47581.
Train: 2018-08-01T23:43:25.049679: step 13459, loss 0.753386.
Train: 2018-08-01T23:43:25.212271: step 13460, loss 0.562412.
Test: 2018-08-01T23:43:25.742850: step 13460, loss 0.547884.
Train: 2018-08-01T23:43:25.912373: step 13461, loss 0.527716.
Train: 2018-08-01T23:43:26.083931: step 13462, loss 0.597108.
Train: 2018-08-01T23:43:26.248533: step 13463, loss 0.475705.
Train: 2018-08-01T23:43:26.414032: step 13464, loss 0.579765.
Train: 2018-08-01T23:43:26.580588: step 13465, loss 0.406169.
Train: 2018-08-01T23:43:26.748140: step 13466, loss 0.649427.
Train: 2018-08-01T23:43:26.917685: step 13467, loss 0.527587.
Train: 2018-08-01T23:43:27.090224: step 13468, loss 0.579867.
Train: 2018-08-01T23:43:27.252821: step 13469, loss 0.579885.
Train: 2018-08-01T23:43:27.421339: step 13470, loss 0.579896.
Test: 2018-08-01T23:43:27.948928: step 13470, loss 0.547807.
Train: 2018-08-01T23:43:28.121468: step 13471, loss 0.737095.
Train: 2018-08-01T23:43:28.299991: step 13472, loss 0.492712.
Train: 2018-08-01T23:43:28.471531: step 13473, loss 0.492785.
Train: 2018-08-01T23:43:28.639085: step 13474, loss 0.56242.
Train: 2018-08-01T23:43:28.804642: step 13475, loss 0.666844.
Train: 2018-08-01T23:43:28.976208: step 13476, loss 0.545038.
Train: 2018-08-01T23:43:29.138770: step 13477, loss 0.545055.
Train: 2018-08-01T23:43:29.315277: step 13478, loss 0.545069.
Train: 2018-08-01T23:43:29.487815: step 13479, loss 0.475758.
Train: 2018-08-01T23:43:29.657362: step 13480, loss 0.545069.
Test: 2018-08-01T23:43:30.191933: step 13480, loss 0.547881.
Train: 2018-08-01T23:43:30.365469: step 13481, loss 0.562411.
Train: 2018-08-01T23:43:30.533021: step 13482, loss 0.54505.
Train: 2018-08-01T23:43:30.702567: step 13483, loss 0.492914.
Train: 2018-08-01T23:43:30.868127: step 13484, loss 0.649428.
Train: 2018-08-01T23:43:31.034705: step 13485, loss 0.579825.
Train: 2018-08-01T23:43:31.203229: step 13486, loss 0.545016.
Train: 2018-08-01T23:43:31.367790: step 13487, loss 0.579825.
Train: 2018-08-01T23:43:31.537337: step 13488, loss 0.579821.
Train: 2018-08-01T23:43:31.711901: step 13489, loss 0.510235.
Train: 2018-08-01T23:43:31.883412: step 13490, loss 0.597216.
Test: 2018-08-01T23:43:32.423967: step 13490, loss 0.547852.
Train: 2018-08-01T23:43:32.592516: step 13491, loss 0.492842.
Train: 2018-08-01T23:43:32.768046: step 13492, loss 0.56242.
Train: 2018-08-01T23:43:32.941607: step 13493, loss 0.579835.
Train: 2018-08-01T23:43:33.112155: step 13494, loss 0.527588.
Train: 2018-08-01T23:43:33.286661: step 13495, loss 0.614707.
Train: 2018-08-01T23:43:33.461193: step 13496, loss 0.562424.
Train: 2018-08-01T23:43:33.632734: step 13497, loss 0.597263.
Train: 2018-08-01T23:43:33.799290: step 13498, loss 0.49279.
Train: 2018-08-01T23:43:33.975817: step 13499, loss 0.54501.
Train: 2018-08-01T23:43:34.146361: step 13500, loss 0.527586.
Test: 2018-08-01T23:43:34.680948: step 13500, loss 0.547828.
Train: 2018-08-01T23:43:35.508692: step 13501, loss 0.597288.
Train: 2018-08-01T23:43:35.674280: step 13502, loss 0.527556.
Train: 2018-08-01T23:43:35.839807: step 13503, loss 0.527538.
Train: 2018-08-01T23:43:36.011349: step 13504, loss 0.510049.
Train: 2018-08-01T23:43:36.187877: step 13505, loss 0.457519.
Train: 2018-08-01T23:43:36.355429: step 13506, loss 0.527386.
Train: 2018-08-01T23:43:36.520016: step 13507, loss 0.474563.
Train: 2018-08-01T23:43:36.689536: step 13508, loss 0.580128.
Train: 2018-08-01T23:43:36.866094: step 13509, loss 0.615584.
Train: 2018-08-01T23:43:37.035640: step 13510, loss 0.580243.
Test: 2018-08-01T23:43:37.573173: step 13510, loss 0.547668.
Train: 2018-08-01T23:43:37.749720: step 13511, loss 0.544782.
Train: 2018-08-01T23:43:37.915284: step 13512, loss 0.54477.
Train: 2018-08-01T23:43:38.081846: step 13513, loss 0.615927.
Train: 2018-08-01T23:43:38.255350: step 13514, loss 0.491359.
Train: 2018-08-01T23:43:38.424895: step 13515, loss 0.633838.
Train: 2018-08-01T23:43:38.596463: step 13516, loss 0.544745.
Train: 2018-08-01T23:43:38.765013: step 13517, loss 0.509103.
Train: 2018-08-01T23:43:38.937557: step 13518, loss 0.491243.
Train: 2018-08-01T23:43:39.115067: step 13519, loss 0.562583.
Train: 2018-08-01T23:43:39.287590: step 13520, loss 0.544717.
Test: 2018-08-01T23:43:39.812188: step 13520, loss 0.547619.
Train: 2018-08-01T23:43:39.978742: step 13521, loss 0.616292.
Train: 2018-08-01T23:43:40.143329: step 13522, loss 0.491009.
Train: 2018-08-01T23:43:40.320827: step 13523, loss 0.526784.
Train: 2018-08-01T23:43:40.497387: step 13524, loss 0.652313.
Train: 2018-08-01T23:43:40.665906: step 13525, loss 0.598493.
Train: 2018-08-01T23:43:40.840470: step 13526, loss 0.616378.
Train: 2018-08-01T23:43:41.005996: step 13527, loss 0.59839.
Train: 2018-08-01T23:43:41.175569: step 13528, loss 0.562584.
Train: 2018-08-01T23:43:41.346118: step 13529, loss 0.616046.
Train: 2018-08-01T23:43:41.511670: step 13530, loss 0.455849.
Test: 2018-08-01T23:43:42.049207: step 13530, loss 0.547661.
Train: 2018-08-01T23:43:42.217756: step 13531, loss 0.491474.
Train: 2018-08-01T23:43:42.389299: step 13532, loss 0.544751.
Train: 2018-08-01T23:43:42.554856: step 13533, loss 0.580515.
Train: 2018-08-01T23:43:42.721409: step 13534, loss 0.509229.
Train: 2018-08-01T23:43:42.890957: step 13535, loss 0.438151.
Train: 2018-08-01T23:43:43.059530: step 13536, loss 0.562545.
Train: 2018-08-01T23:43:43.226105: step 13537, loss 0.509106.
Train: 2018-08-01T23:43:43.398639: step 13538, loss 0.598294.
Train: 2018-08-01T23:43:43.564158: step 13539, loss 0.52685.
Train: 2018-08-01T23:43:43.733736: step 13540, loss 0.580527.
Test: 2018-08-01T23:43:44.265317: step 13540, loss 0.547614.
Train: 2018-08-01T23:43:44.445826: step 13541, loss 0.562617.
Train: 2018-08-01T23:43:44.615348: step 13542, loss 0.544695.
Train: 2018-08-01T23:43:44.783923: step 13543, loss 0.544691.
Train: 2018-08-01T23:43:44.952472: step 13544, loss 0.544686.
Train: 2018-08-01T23:43:45.118022: step 13545, loss 0.562644.
Train: 2018-08-01T23:43:45.295555: step 13546, loss 0.616567.
Train: 2018-08-01T23:43:45.466072: step 13547, loss 0.472816.
Train: 2018-08-01T23:43:45.636618: step 13548, loss 0.562653.
Train: 2018-08-01T23:43:45.810154: step 13549, loss 0.544672.
Train: 2018-08-01T23:43:45.977723: step 13550, loss 0.562664.
Test: 2018-08-01T23:43:46.507323: step 13550, loss 0.547595.
Train: 2018-08-01T23:43:46.676837: step 13551, loss 0.670667.
Train: 2018-08-01T23:43:46.843391: step 13552, loss 0.616588.
Train: 2018-08-01T23:43:47.012970: step 13553, loss 0.598521.
Train: 2018-08-01T23:43:47.176500: step 13554, loss 0.652122.
Train: 2018-08-01T23:43:47.344053: step 13555, loss 0.616106.
Train: 2018-08-01T23:43:47.512604: step 13556, loss 0.544767.
Train: 2018-08-01T23:43:47.679157: step 13557, loss 0.633385.
Train: 2018-08-01T23:43:47.857710: step 13558, loss 0.509549.
Train: 2018-08-01T23:43:48.027227: step 13559, loss 0.56247.
Train: 2018-08-01T23:43:48.209748: step 13560, loss 0.562454.
Test: 2018-08-01T23:43:48.752289: step 13560, loss 0.547788.
Train: 2018-08-01T23:43:48.921836: step 13561, loss 0.544943.
Train: 2018-08-01T23:43:49.085429: step 13562, loss 0.475126.
Train: 2018-08-01T23:43:49.256939: step 13563, loss 0.387967.
Train: 2018-08-01T23:43:49.425521: step 13564, loss 0.510019.
Train: 2018-08-01T23:43:49.593041: step 13565, loss 0.527431.
Train: 2018-08-01T23:43:49.763616: step 13566, loss 0.615086.
Train: 2018-08-01T23:43:49.932154: step 13567, loss 0.527331.
Train: 2018-08-01T23:43:50.097725: step 13568, loss 0.615239.
Train: 2018-08-01T23:43:50.260258: step 13569, loss 0.562472.
Train: 2018-08-01T23:43:50.431823: step 13570, loss 0.632903.
Test: 2018-08-01T23:43:50.968389: step 13570, loss 0.547736.
Train: 2018-08-01T23:43:51.139924: step 13571, loss 0.544874.
Train: 2018-08-01T23:43:51.308456: step 13572, loss 0.615235.
Train: 2018-08-01T23:43:51.480029: step 13573, loss 0.509754.
Train: 2018-08-01T23:43:51.651539: step 13574, loss 0.615144.
Train: 2018-08-01T23:43:51.821100: step 13575, loss 0.615011.
Train: 2018-08-01T23:43:51.995644: step 13576, loss 0.544819.
Train: 2018-08-01T23:43:52.161204: step 13577, loss 0.579464.
Train: 2018-08-01T23:43:52.330753: step 13578, loss 0.494397.
Train: 2018-08-01T23:43:52.499271: step 13579, loss 0.716173.
Train: 2018-08-01T23:43:52.673836: step 13580, loss 0.611754.
Test: 2018-08-01T23:43:53.199401: step 13580, loss 0.548382.
Train: 2018-08-01T23:43:53.368985: step 13581, loss 0.543836.
Train: 2018-08-01T23:43:53.538524: step 13582, loss 0.4795.
Train: 2018-08-01T23:43:53.707044: step 13583, loss 0.527811.
Train: 2018-08-01T23:43:53.875593: step 13584, loss 0.478262.
Train: 2018-08-01T23:43:54.056137: step 13585, loss 0.561041.
Train: 2018-08-01T23:43:54.225656: step 13586, loss 0.545034.
Train: 2018-08-01T23:43:54.394237: step 13587, loss 0.578502.
Train: 2018-08-01T23:43:54.562786: step 13588, loss 0.564106.
Train: 2018-08-01T23:43:54.747294: step 13589, loss 0.562505.
Train: 2018-08-01T23:43:54.918829: step 13590, loss 0.615512.
Test: 2018-08-01T23:43:55.454403: step 13590, loss 0.54834.
Train: 2018-08-01T23:43:55.624916: step 13591, loss 0.529744.
Train: 2018-08-01T23:43:55.794462: step 13592, loss 0.667957.
Train: 2018-08-01T23:43:55.964010: step 13593, loss 0.563263.
Train: 2018-08-01T23:43:56.131561: step 13594, loss 0.68324.
Train: 2018-08-01T23:43:56.308121: step 13595, loss 0.509129.
Train: 2018-08-01T23:43:56.494617: step 13596, loss 0.630821.
Train: 2018-08-01T23:43:56.667131: step 13597, loss 0.578725.
Train: 2018-08-01T23:43:56.839702: step 13598, loss 0.479334.
Train: 2018-08-01T23:43:57.019189: step 13599, loss 0.529525.
Train: 2018-08-01T23:43:57.189760: step 13600, loss 0.528931.
Test: 2018-08-01T23:43:57.720315: step 13600, loss 0.547937.
Train: 2018-08-01T23:43:58.500883: step 13601, loss 0.68424.
Train: 2018-08-01T23:43:58.676414: step 13602, loss 0.440971.
Train: 2018-08-01T23:43:58.852941: step 13603, loss 0.544725.
Train: 2018-08-01T23:43:59.030467: step 13604, loss 0.489967.
Train: 2018-08-01T23:43:59.210018: step 13605, loss 0.581624.
Train: 2018-08-01T23:43:59.376542: step 13606, loss 0.548486.
Train: 2018-08-01T23:43:59.538110: step 13607, loss 0.54939.
Train: 2018-08-01T23:43:59.705703: step 13608, loss 0.560404.
Train: 2018-08-01T23:43:59.879223: step 13609, loss 0.542261.
Train: 2018-08-01T23:44:00.063704: step 13610, loss 0.623459.
Test: 2018-08-01T23:44:00.573384: step 13610, loss 0.547969.
Train: 2018-08-01T23:44:00.741905: step 13611, loss 0.622816.
Train: 2018-08-01T23:44:00.911439: step 13612, loss 0.54817.
Train: 2018-08-01T23:44:01.080985: step 13613, loss 0.492794.
Train: 2018-08-01T23:44:01.249564: step 13614, loss 0.510583.
Train: 2018-08-01T23:44:01.417088: step 13615, loss 0.651797.
Train: 2018-08-01T23:44:01.585636: step 13616, loss 0.598021.
Train: 2018-08-01T23:44:01.805050: step 13617, loss 0.528523.
Train: 2018-08-01T23:44:01.974597: step 13618, loss 0.580837.
Train: 2018-08-01T23:44:02.148161: step 13619, loss 0.700275.
Train: 2018-08-01T23:44:02.315685: step 13620, loss 0.461971.
Test: 2018-08-01T23:44:02.851253: step 13620, loss 0.549298.
Train: 2018-08-01T23:44:03.025808: step 13621, loss 0.543763.
Train: 2018-08-01T23:44:03.197351: step 13622, loss 0.530658.
Train: 2018-08-01T23:44:03.363883: step 13623, loss 0.627169.
Train: 2018-08-01T23:44:03.543403: step 13624, loss 0.556518.
Train: 2018-08-01T23:44:03.717967: step 13625, loss 0.509185.
Train: 2018-08-01T23:44:03.885489: step 13626, loss 0.514077.
Train: 2018-08-01T23:44:04.062017: step 13627, loss 0.599187.
Train: 2018-08-01T23:44:04.237578: step 13628, loss 0.587276.
Train: 2018-08-01T23:44:04.412083: step 13629, loss 0.511865.
Train: 2018-08-01T23:44:04.581652: step 13630, loss 0.421056.
Test: 2018-08-01T23:44:05.104231: step 13630, loss 0.549537.
Train: 2018-08-01T23:44:05.271784: step 13631, loss 0.620569.
Train: 2018-08-01T23:44:05.456314: step 13632, loss 0.513316.
Train: 2018-08-01T23:44:05.625836: step 13633, loss 0.510939.
Train: 2018-08-01T23:44:05.797411: step 13634, loss 0.430245.
Train: 2018-08-01T23:44:05.964930: step 13635, loss 0.514763.
Train: 2018-08-01T23:44:06.128525: step 13636, loss 0.617991.
Train: 2018-08-01T23:44:06.312999: step 13637, loss 0.664905.
Train: 2018-08-01T23:44:06.484540: step 13638, loss 0.490327.
Train: 2018-08-01T23:44:06.652092: step 13639, loss 0.639447.
Train: 2018-08-01T23:44:06.817650: step 13640, loss 0.57414.
Test: 2018-08-01T23:44:07.351224: step 13640, loss 0.549795.
Train: 2018-08-01T23:44:07.524790: step 13641, loss 0.511259.
Train: 2018-08-01T23:44:07.692336: step 13642, loss 0.588118.
Train: 2018-08-01T23:44:07.862892: step 13643, loss 0.579659.
Train: 2018-08-01T23:44:08.040382: step 13644, loss 0.606853.
Train: 2018-08-01T23:44:08.223909: step 13645, loss 0.584902.
Train: 2018-08-01T23:44:08.398453: step 13646, loss 0.581043.
Train: 2018-08-01T23:44:08.569991: step 13647, loss 0.524519.
Train: 2018-08-01T23:44:08.737548: step 13648, loss 0.630906.
Train: 2018-08-01T23:44:08.904104: step 13649, loss 0.648573.
Train: 2018-08-01T23:44:09.078630: step 13650, loss 0.531021.
Test: 2018-08-01T23:44:09.608190: step 13650, loss 0.550804.
Train: 2018-08-01T23:44:09.777737: step 13651, loss 0.496195.
Train: 2018-08-01T23:44:09.945291: step 13652, loss 0.61772.
Train: 2018-08-01T23:44:10.114836: step 13653, loss 0.582428.
Train: 2018-08-01T23:44:10.282388: step 13654, loss 0.531816.
Train: 2018-08-01T23:44:10.447945: step 13655, loss 0.565293.
Train: 2018-08-01T23:44:10.627465: step 13656, loss 0.48014.
Train: 2018-08-01T23:44:10.798037: step 13657, loss 0.496711.
Train: 2018-08-01T23:44:10.972554: step 13658, loss 0.564604.
Train: 2018-08-01T23:44:11.143086: step 13659, loss 0.49727.
Train: 2018-08-01T23:44:11.317646: step 13660, loss 0.632071.
Test: 2018-08-01T23:44:11.857177: step 13660, loss 0.550549.
Train: 2018-08-01T23:44:12.030714: step 13661, loss 0.494638.
Train: 2018-08-01T23:44:12.213252: step 13662, loss 0.54849.
Train: 2018-08-01T23:44:12.387777: step 13663, loss 0.608802.
Train: 2018-08-01T23:44:12.558334: step 13664, loss 0.569144.
Train: 2018-08-01T23:44:12.728886: step 13665, loss 0.562522.
Train: 2018-08-01T23:44:12.896415: step 13666, loss 0.472352.
Train: 2018-08-01T23:44:13.076917: step 13667, loss 0.566761.
Train: 2018-08-01T23:44:13.243472: step 13668, loss 0.580208.
Train: 2018-08-01T23:44:13.411024: step 13669, loss 0.651467.
Train: 2018-08-01T23:44:13.579573: step 13670, loss 0.668956.
Test: 2018-08-01T23:44:14.113149: step 13670, loss 0.550402.
Train: 2018-08-01T23:44:14.279719: step 13671, loss 0.653149.
Train: 2018-08-01T23:44:14.453237: step 13672, loss 0.513094.
Train: 2018-08-01T23:44:14.623784: step 13673, loss 0.582675.
Train: 2018-08-01T23:44:14.795324: step 13674, loss 0.530273.
Train: 2018-08-01T23:44:14.965868: step 13675, loss 0.687036.
Train: 2018-08-01T23:44:15.134417: step 13676, loss 0.565124.
Train: 2018-08-01T23:44:15.304961: step 13677, loss 0.478359.
Train: 2018-08-01T23:44:15.473541: step 13678, loss 0.478416.
Train: 2018-08-01T23:44:15.641094: step 13679, loss 0.70387.
Train: 2018-08-01T23:44:15.808645: step 13680, loss 0.583048.
Test: 2018-08-01T23:44:16.346182: step 13680, loss 0.550581.
Train: 2018-08-01T23:44:16.523703: step 13681, loss 0.634248.
Train: 2018-08-01T23:44:16.693250: step 13682, loss 0.478255.
Train: 2018-08-01T23:44:16.860803: step 13683, loss 0.63398.
Train: 2018-08-01T23:44:17.032343: step 13684, loss 0.633834.
Train: 2018-08-01T23:44:17.200894: step 13685, loss 0.547812.
Train: 2018-08-01T23:44:17.380413: step 13686, loss 0.496441.
Train: 2018-08-01T23:44:17.546968: step 13687, loss 0.513379.
Train: 2018-08-01T23:44:17.716515: step 13688, loss 0.479334.
Train: 2018-08-01T23:44:17.883068: step 13689, loss 0.513579.
Train: 2018-08-01T23:44:18.059633: step 13690, loss 0.530499.
Test: 2018-08-01T23:44:18.602147: step 13690, loss 0.550377.
Train: 2018-08-01T23:44:18.773688: step 13691, loss 0.599151.
Train: 2018-08-01T23:44:18.945261: step 13692, loss 0.581942.
Train: 2018-08-01T23:44:19.111816: step 13693, loss 0.581901.
Train: 2018-08-01T23:44:19.281331: step 13694, loss 0.564648.
Train: 2018-08-01T23:44:19.453903: step 13695, loss 0.581852.
Train: 2018-08-01T23:44:19.625412: step 13696, loss 0.581818.
Train: 2018-08-01T23:44:19.791966: step 13697, loss 0.598841.
Train: 2018-08-01T23:44:19.963508: step 13698, loss 0.547535.
Train: 2018-08-01T23:44:20.133055: step 13699, loss 0.547354.
Train: 2018-08-01T23:44:20.372415: step 13700, loss 0.58173.
Test: 2018-08-01T23:44:20.912972: step 13700, loss 0.549973.
Train: 2018-08-01T23:44:21.792590: step 13701, loss 0.56463.
Train: 2018-08-01T23:44:21.969119: step 13702, loss 0.530006.
Train: 2018-08-01T23:44:22.140685: step 13703, loss 0.633294.
Train: 2018-08-01T23:44:22.316189: step 13704, loss 0.513245.
Train: 2018-08-01T23:44:22.501705: step 13705, loss 0.426533.
Train: 2018-08-01T23:44:22.671242: step 13706, loss 0.460772.
Train: 2018-08-01T23:44:22.840801: step 13707, loss 0.546937.
Train: 2018-08-01T23:44:23.008339: step 13708, loss 0.616377.
Train: 2018-08-01T23:44:23.174925: step 13709, loss 0.546771.
Train: 2018-08-01T23:44:23.345438: step 13710, loss 0.581583.
Test: 2018-08-01T23:44:23.883017: step 13710, loss 0.549532.
Train: 2018-08-01T23:44:24.049588: step 13711, loss 0.598972.
Train: 2018-08-01T23:44:24.225088: step 13712, loss 0.476817.
Train: 2018-08-01T23:44:24.395662: step 13713, loss 0.599079.
Train: 2018-08-01T23:44:24.582158: step 13714, loss 0.581583.
Train: 2018-08-01T23:44:24.748688: step 13715, loss 0.564092.
Train: 2018-08-01T23:44:24.915242: step 13716, loss 0.546496.
Train: 2018-08-01T23:44:25.097779: step 13717, loss 0.458747.
Train: 2018-08-01T23:44:25.266335: step 13718, loss 0.45854.
Train: 2018-08-01T23:44:25.434860: step 13719, loss 0.404979.
Train: 2018-08-01T23:44:25.609417: step 13720, loss 0.617162.
Test: 2018-08-01T23:44:26.145954: step 13720, loss 0.549149.
Train: 2018-08-01T23:44:26.456873: step 13721, loss 0.545836.
Train: 2018-08-01T23:44:26.626416: step 13722, loss 0.528419.
Train: 2018-08-01T23:44:26.796963: step 13723, loss 0.528298.
Train: 2018-08-01T23:44:26.976454: step 13724, loss 0.49313.
Train: 2018-08-01T23:44:27.146001: step 13725, loss 0.492216.
Train: 2018-08-01T23:44:27.314552: step 13726, loss 0.618389.
Train: 2018-08-01T23:44:27.486094: step 13727, loss 0.691072.
Train: 2018-08-01T23:44:27.658631: step 13728, loss 0.545723.
Train: 2018-08-01T23:44:27.825186: step 13729, loss 0.509719.
Train: 2018-08-01T23:44:27.999751: step 13730, loss 0.600533.
Test: 2018-08-01T23:44:28.521325: step 13730, loss 0.548948.
Train: 2018-08-01T23:44:28.688895: step 13731, loss 0.618722.
Train: 2018-08-01T23:44:28.854434: step 13732, loss 0.491702.
Train: 2018-08-01T23:44:29.020990: step 13733, loss 0.618714.
Train: 2018-08-01T23:44:29.194550: step 13734, loss 0.455098.
Train: 2018-08-01T23:44:29.360083: step 13735, loss 0.654472.
Train: 2018-08-01T23:44:29.527634: step 13736, loss 0.600426.
Train: 2018-08-01T23:44:29.695200: step 13737, loss 0.52775.
Train: 2018-08-01T23:44:29.863736: step 13738, loss 0.581874.
Train: 2018-08-01T23:44:30.042260: step 13739, loss 0.635716.
Train: 2018-08-01T23:44:30.210820: step 13740, loss 0.527813.
Test: 2018-08-01T23:44:30.755353: step 13740, loss 0.548826.
Train: 2018-08-01T23:44:30.923902: step 13741, loss 0.600044.
Train: 2018-08-01T23:44:31.097448: step 13742, loss 0.636892.
Train: 2018-08-01T23:44:31.266985: step 13743, loss 0.474154.
Train: 2018-08-01T23:44:31.433541: step 13744, loss 0.58195.
Train: 2018-08-01T23:44:31.607094: step 13745, loss 0.564781.
Train: 2018-08-01T23:44:31.777620: step 13746, loss 0.634736.
Train: 2018-08-01T23:44:31.950159: step 13747, loss 0.547248.
Train: 2018-08-01T23:44:32.126688: step 13748, loss 0.528297.
Train: 2018-08-01T23:44:32.293278: step 13749, loss 0.527909.
Train: 2018-08-01T23:44:32.461791: step 13750, loss 0.456522.
Test: 2018-08-01T23:44:32.982430: step 13750, loss 0.548799.
Train: 2018-08-01T23:44:33.147985: step 13751, loss 0.547252.
Train: 2018-08-01T23:44:33.316506: step 13752, loss 0.67266.
Train: 2018-08-01T23:44:33.488048: step 13753, loss 0.563988.
Train: 2018-08-01T23:44:33.656628: step 13754, loss 0.456641.
Train: 2018-08-01T23:44:33.831130: step 13755, loss 0.528586.
Train: 2018-08-01T23:44:33.996689: step 13756, loss 0.54513.
Train: 2018-08-01T23:44:34.161277: step 13757, loss 0.529932.
Train: 2018-08-01T23:44:34.332790: step 13758, loss 0.617528.
Train: 2018-08-01T23:44:34.504332: step 13759, loss 0.758606.
Train: 2018-08-01T23:44:34.674875: step 13760, loss 0.599426.
Test: 2018-08-01T23:44:35.200470: step 13760, loss 0.548955.
Train: 2018-08-01T23:44:35.374035: step 13761, loss 0.563824.
Train: 2018-08-01T23:44:35.548539: step 13762, loss 0.63411.
Train: 2018-08-01T23:44:35.714098: step 13763, loss 0.651305.
Train: 2018-08-01T23:44:35.886636: step 13764, loss 0.459311.
Train: 2018-08-01T23:44:36.055185: step 13765, loss 0.494256.
Train: 2018-08-01T23:44:36.222772: step 13766, loss 0.494142.
Train: 2018-08-01T23:44:36.394278: step 13767, loss 0.528739.
Train: 2018-08-01T23:44:36.556875: step 13768, loss 0.581167.
Train: 2018-08-01T23:44:36.722402: step 13769, loss 0.5984.
Train: 2018-08-01T23:44:36.894940: step 13770, loss 0.598278.
Test: 2018-08-01T23:44:37.438488: step 13770, loss 0.549116.
Train: 2018-08-01T23:44:37.606057: step 13771, loss 0.511726.
Train: 2018-08-01T23:44:37.778603: step 13772, loss 0.598498.
Train: 2018-08-01T23:44:37.943138: step 13773, loss 0.545964.
Train: 2018-08-01T23:44:38.111718: step 13774, loss 0.54637.
Train: 2018-08-01T23:44:38.285249: step 13775, loss 0.598153.
Train: 2018-08-01T23:44:38.452800: step 13776, loss 0.597971.
Train: 2018-08-01T23:44:38.625346: step 13777, loss 0.494441.
Train: 2018-08-01T23:44:38.795858: step 13778, loss 0.615011.
Train: 2018-08-01T23:44:38.962437: step 13779, loss 0.684163.
Train: 2018-08-01T23:44:39.129966: step 13780, loss 0.632255.
Test: 2018-08-01T23:44:39.667529: step 13780, loss 0.549126.
Train: 2018-08-01T23:44:39.838104: step 13781, loss 0.546654.
Train: 2018-08-01T23:44:40.014601: step 13782, loss 0.632782.
Train: 2018-08-01T23:44:40.184147: step 13783, loss 0.49449.
Train: 2018-08-01T23:44:40.355690: step 13784, loss 0.632476.
Train: 2018-08-01T23:44:40.524238: step 13785, loss 0.544691.
Train: 2018-08-01T23:44:40.689795: step 13786, loss 0.427664.
Train: 2018-08-01T23:44:40.856374: step 13787, loss 0.531986.
Train: 2018-08-01T23:44:41.032879: step 13788, loss 0.463547.
Train: 2018-08-01T23:44:41.201429: step 13789, loss 0.599973.
Train: 2018-08-01T23:44:41.373001: step 13790, loss 0.547314.
Test: 2018-08-01T23:44:41.903552: step 13790, loss 0.549338.
Train: 2018-08-01T23:44:42.072126: step 13791, loss 0.530451.
Train: 2018-08-01T23:44:42.236662: step 13792, loss 0.546534.
Train: 2018-08-01T23:44:42.409200: step 13793, loss 0.598403.
Train: 2018-08-01T23:44:42.578747: step 13794, loss 0.636957.
Train: 2018-08-01T23:44:42.746299: step 13795, loss 0.615173.
Train: 2018-08-01T23:44:42.917841: step 13796, loss 0.563404.
Train: 2018-08-01T23:44:43.088383: step 13797, loss 0.515612.
Train: 2018-08-01T23:44:43.256954: step 13798, loss 0.531457.
Train: 2018-08-01T23:44:43.429471: step 13799, loss 0.579384.
Train: 2018-08-01T23:44:43.606000: step 13800, loss 0.599256.
Test: 2018-08-01T23:44:44.150544: step 13800, loss 0.549187.
Train: 2018-08-01T23:44:44.970572: step 13801, loss 0.581136.
Train: 2018-08-01T23:44:45.143110: step 13802, loss 0.495057.
Train: 2018-08-01T23:44:45.308700: step 13803, loss 0.529523.
Train: 2018-08-01T23:44:45.479214: step 13804, loss 0.546252.
Train: 2018-08-01T23:44:45.646764: step 13805, loss 0.581103.
Train: 2018-08-01T23:44:45.813317: step 13806, loss 0.460028.
Train: 2018-08-01T23:44:45.985857: step 13807, loss 0.511386.
Train: 2018-08-01T23:44:46.153408: step 13808, loss 0.599055.
Train: 2018-08-01T23:44:46.323954: step 13809, loss 0.494032.
Train: 2018-08-01T23:44:46.495525: step 13810, loss 0.546286.
Test: 2018-08-01T23:44:47.034080: step 13810, loss 0.549091.
Train: 2018-08-01T23:44:47.216568: step 13811, loss 0.581351.
Train: 2018-08-01T23:44:47.388132: step 13812, loss 0.634.
Train: 2018-08-01T23:44:47.556665: step 13813, loss 0.370492.
Train: 2018-08-01T23:44:47.729197: step 13814, loss 0.546166.
Train: 2018-08-01T23:44:47.899750: step 13815, loss 0.634496.
Train: 2018-08-01T23:44:48.067293: step 13816, loss 0.652321.
Train: 2018-08-01T23:44:48.245849: step 13817, loss 0.546047.
Train: 2018-08-01T23:44:48.415393: step 13818, loss 0.634642.
Train: 2018-08-01T23:44:48.589926: step 13819, loss 0.546013.
Train: 2018-08-01T23:44:48.759470: step 13820, loss 0.652231.
Test: 2018-08-01T23:44:49.300996: step 13820, loss 0.548874.
Train: 2018-08-01T23:44:49.480515: step 13821, loss 0.652042.
Train: 2018-08-01T23:44:49.648067: step 13822, loss 0.546012.
Train: 2018-08-01T23:44:49.811655: step 13823, loss 0.528434.
Train: 2018-08-01T23:44:49.975222: step 13824, loss 0.651366.
Train: 2018-08-01T23:44:50.147731: step 13825, loss 0.563549.
Train: 2018-08-01T23:44:50.324290: step 13826, loss 0.633385.
Train: 2018-08-01T23:44:50.492809: step 13827, loss 0.528671.
Train: 2018-08-01T23:44:50.655407: step 13828, loss 0.615566.
Train: 2018-08-01T23:44:50.823924: step 13829, loss 0.528813.
Train: 2018-08-01T23:44:51.008430: step 13830, loss 0.546146.
Test: 2018-08-01T23:44:51.543999: step 13830, loss 0.548964.
Train: 2018-08-01T23:44:51.719529: step 13831, loss 0.597893.
Train: 2018-08-01T23:44:51.887083: step 13832, loss 0.597802.
Train: 2018-08-01T23:44:52.056637: step 13833, loss 0.614871.
Train: 2018-08-01T23:44:52.228170: step 13834, loss 0.614709.
Train: 2018-08-01T23:44:52.405695: step 13835, loss 0.665716.
Train: 2018-08-01T23:44:52.587210: step 13836, loss 0.563337.
Train: 2018-08-01T23:44:52.755795: step 13837, loss 0.512565.
Train: 2018-08-01T23:44:52.928298: step 13838, loss 0.715217.
Train: 2018-08-01T23:44:53.098867: step 13839, loss 0.546535.
Train: 2018-08-01T23:44:53.266425: step 13840, loss 0.513166.
Test: 2018-08-01T23:44:53.794982: step 13840, loss 0.549366.
Train: 2018-08-01T23:44:53.971535: step 13841, loss 0.630106.
Train: 2018-08-01T23:44:54.154022: step 13842, loss 0.530078.
Train: 2018-08-01T23:44:54.330575: step 13843, loss 0.613146.
Train: 2018-08-01T23:44:54.503089: step 13844, loss 0.563364.
Train: 2018-08-01T23:44:54.671637: step 13845, loss 0.513836.
Train: 2018-08-01T23:44:54.851158: step 13846, loss 0.510602.
Train: 2018-08-01T23:44:55.019739: step 13847, loss 0.629307.
Train: 2018-08-01T23:44:55.196236: step 13848, loss 0.546897.
Train: 2018-08-01T23:44:55.374789: step 13849, loss 0.596273.
Train: 2018-08-01T23:44:55.542310: step 13850, loss 0.596252.
Test: 2018-08-01T23:44:56.076880: step 13850, loss 0.549602.
Train: 2018-08-01T23:44:56.245430: step 13851, loss 0.57978.
Train: 2018-08-01T23:44:56.419965: step 13852, loss 0.546952.
Train: 2018-08-01T23:44:56.588513: step 13853, loss 0.497776.
Train: 2018-08-01T23:44:56.754100: step 13854, loss 0.514131.
Train: 2018-08-01T23:44:56.921658: step 13855, loss 0.563319.
Train: 2018-08-01T23:44:57.097179: step 13856, loss 0.61264.
Train: 2018-08-01T23:44:57.265728: step 13857, loss 0.563284.
Train: 2018-08-01T23:44:57.436248: step 13858, loss 0.480926.
Train: 2018-08-01T23:44:57.605794: step 13859, loss 0.530272.
Train: 2018-08-01T23:44:57.771351: step 13860, loss 0.546679.
Test: 2018-08-01T23:44:58.293956: step 13860, loss 0.549307.
Train: 2018-08-01T23:44:58.473475: step 13861, loss 0.662678.
Train: 2018-08-01T23:44:58.644049: step 13862, loss 0.546583.
Train: 2018-08-01T23:44:58.811577: step 13863, loss 0.629622.
Train: 2018-08-01T23:44:58.977161: step 13864, loss 0.546538.
Train: 2018-08-01T23:44:59.144713: step 13865, loss 0.529893.
Train: 2018-08-01T23:44:59.324210: step 13866, loss 0.579774.
Train: 2018-08-01T23:44:59.492750: step 13867, loss 0.529809.
Train: 2018-08-01T23:44:59.664316: step 13868, loss 0.629813.
Train: 2018-08-01T23:44:59.832872: step 13869, loss 0.646505.
Train: 2018-08-01T23:44:59.999395: step 13870, loss 0.663099.
Test: 2018-08-01T23:45:00.540949: step 13870, loss 0.549178.
Train: 2018-08-01T23:45:00.724483: step 13871, loss 0.513218.
Train: 2018-08-01T23:45:00.895002: step 13872, loss 0.496653.
Train: 2018-08-01T23:45:01.063581: step 13873, loss 0.563101.
Train: 2018-08-01T23:45:01.235125: step 13874, loss 0.513241.
Train: 2018-08-01T23:45:01.402670: step 13875, loss 0.596349.
Train: 2018-08-01T23:45:01.578194: step 13876, loss 0.563074.
Train: 2018-08-01T23:45:01.745753: step 13877, loss 0.479811.
Train: 2018-08-01T23:45:01.912281: step 13878, loss 0.663126.
Train: 2018-08-01T23:45:02.084847: step 13879, loss 0.54637.
Train: 2018-08-01T23:45:02.254367: step 13880, loss 0.596421.
Test: 2018-08-01T23:45:02.793925: step 13880, loss 0.549048.
Train: 2018-08-01T23:45:02.974444: step 13881, loss 0.613117.
Train: 2018-08-01T23:45:03.144015: step 13882, loss 0.613085.
Train: 2018-08-01T23:45:03.309580: step 13883, loss 0.596364.
Train: 2018-08-01T23:45:03.483088: step 13884, loss 0.579678.
Train: 2018-08-01T23:45:03.663600: step 13885, loss 0.496554.
Train: 2018-08-01T23:45:03.837137: step 13886, loss 0.5298.
Train: 2018-08-01T23:45:04.009675: step 13887, loss 0.646154.
Train: 2018-08-01T23:45:04.178225: step 13888, loss 0.612859.
Train: 2018-08-01T23:45:04.351760: step 13889, loss 0.513262.
Train: 2018-08-01T23:45:04.525323: step 13890, loss 0.496692.
Test: 2018-08-01T23:45:05.064857: step 13890, loss 0.549124.
Train: 2018-08-01T23:45:05.236396: step 13891, loss 0.546425.
Train: 2018-08-01T23:45:05.405941: step 13892, loss 0.579653.
Train: 2018-08-01T23:45:05.575523: step 13893, loss 0.612887.
Train: 2018-08-01T23:45:05.746083: step 13894, loss 0.446589.
Train: 2018-08-01T23:45:05.919568: step 13895, loss 0.513002.
Train: 2018-08-01T23:45:06.090113: step 13896, loss 0.562962.
Train: 2018-08-01T23:45:06.258664: step 13897, loss 0.445762.
Train: 2018-08-01T23:45:06.428240: step 13898, loss 0.613336.
Train: 2018-08-01T23:45:06.594792: step 13899, loss 0.495504.
Train: 2018-08-01T23:45:06.769329: step 13900, loss 0.664349.
Test: 2018-08-01T23:45:07.302872: step 13900, loss 0.548698.
Train: 2018-08-01T23:45:08.143254: step 13901, loss 0.545943.
Train: 2018-08-01T23:45:08.314801: step 13902, loss 0.579845.
Train: 2018-08-01T23:45:08.484316: step 13903, loss 0.545872.
Train: 2018-08-01T23:45:08.660869: step 13904, loss 0.545838.
Train: 2018-08-01T23:45:08.831421: step 13905, loss 0.596961.
Train: 2018-08-01T23:45:09.007949: step 13906, loss 0.56285.
Train: 2018-08-01T23:45:09.175468: step 13907, loss 0.477417.
Train: 2018-08-01T23:45:09.345047: step 13908, loss 0.631325.
Train: 2018-08-01T23:45:09.515588: step 13909, loss 0.648522.
Train: 2018-08-01T23:45:09.688129: step 13910, loss 0.579969.
Test: 2018-08-01T23:45:10.218692: step 13910, loss 0.548495.
Train: 2018-08-01T23:45:10.389223: step 13911, loss 0.528588.
Train: 2018-08-01T23:45:10.564753: step 13912, loss 0.494342.
Train: 2018-08-01T23:45:10.729345: step 13913, loss 0.579964.
Train: 2018-08-01T23:45:10.906840: step 13914, loss 0.631416.
Train: 2018-08-01T23:45:11.071400: step 13915, loss 0.494257.
Train: 2018-08-01T23:45:11.240947: step 13916, loss 0.631424.
Train: 2018-08-01T23:45:11.404539: step 13917, loss 0.631394.
Train: 2018-08-01T23:45:11.572061: step 13918, loss 0.49432.
Train: 2018-08-01T23:45:11.741608: step 13919, loss 0.562809.
Train: 2018-08-01T23:45:11.909160: step 13920, loss 0.511458.
Test: 2018-08-01T23:45:12.441735: step 13920, loss 0.548463.
Train: 2018-08-01T23:45:12.608292: step 13921, loss 0.579892.
Train: 2018-08-01T23:45:12.773849: step 13922, loss 0.631317.
Train: 2018-08-01T23:45:12.945415: step 13923, loss 0.545681.
Train: 2018-08-01T23:45:13.116957: step 13924, loss 0.545685.
Train: 2018-08-01T23:45:13.281491: step 13925, loss 0.545684.
Train: 2018-08-01T23:45:13.453032: step 13926, loss 0.528571.
Train: 2018-08-01T23:45:13.623577: step 13927, loss 0.511423.
Train: 2018-08-01T23:45:13.791130: step 13928, loss 0.6485.
Train: 2018-08-01T23:45:13.971677: step 13929, loss 0.648494.
Train: 2018-08-01T23:45:14.143188: step 13930, loss 0.579902.
Test: 2018-08-01T23:45:14.681747: step 13930, loss 0.54846.
Train: 2018-08-01T23:45:14.848303: step 13931, loss 0.562779.
Train: 2018-08-01T23:45:15.017850: step 13932, loss 0.562765.
Train: 2018-08-01T23:45:15.184435: step 13933, loss 0.613962.
Train: 2018-08-01T23:45:15.353982: step 13934, loss 0.630915.
Train: 2018-08-01T23:45:15.519532: step 13935, loss 0.562778.
Train: 2018-08-01T23:45:15.690052: step 13936, loss 0.545823.
Train: 2018-08-01T23:45:15.856639: step 13937, loss 0.545854.
Train: 2018-08-01T23:45:16.035130: step 13938, loss 0.528966.
Train: 2018-08-01T23:45:16.202708: step 13939, loss 0.596587.
Train: 2018-08-01T23:45:16.370235: step 13940, loss 0.596558.
Test: 2018-08-01T23:45:16.891843: step 13940, loss 0.548667.
Train: 2018-08-01T23:45:17.064379: step 13941, loss 0.613339.
Train: 2018-08-01T23:45:17.230935: step 13942, loss 0.562793.
Train: 2018-08-01T23:45:17.404469: step 13943, loss 0.545992.
Train: 2018-08-01T23:45:17.571025: step 13944, loss 0.579588.
Train: 2018-08-01T23:45:17.742565: step 13945, loss 0.59634.
Train: 2018-08-01T23:45:17.914108: step 13946, loss 0.629784.
Train: 2018-08-01T23:45:18.082688: step 13947, loss 0.546114.
Train: 2018-08-01T23:45:18.252224: step 13948, loss 0.646207.
Train: 2018-08-01T23:45:18.422747: step 13949, loss 0.679279.
Train: 2018-08-01T23:45:18.592295: step 13950, loss 0.546313.
Test: 2018-08-01T23:45:19.136839: step 13950, loss 0.549062.
Train: 2018-08-01T23:45:19.319374: step 13951, loss 0.52986.
Train: 2018-08-01T23:45:19.494907: step 13952, loss 0.56292.
Train: 2018-08-01T23:45:19.664428: step 13953, loss 0.513591.
Train: 2018-08-01T23:45:19.841954: step 13954, loss 0.612235.
Train: 2018-08-01T23:45:20.015519: step 13955, loss 0.480903.
Train: 2018-08-01T23:45:20.183068: step 13956, loss 0.497295.
Train: 2018-08-01T23:45:20.348600: step 13957, loss 0.61222.
Train: 2018-08-01T23:45:20.515154: step 13958, loss 0.497162.
Train: 2018-08-01T23:45:20.679747: step 13959, loss 0.612299.
Train: 2018-08-01T23:45:20.856242: step 13960, loss 0.595852.
Test: 2018-08-01T23:45:21.389823: step 13960, loss 0.549086.
Train: 2018-08-01T23:45:21.560361: step 13961, loss 0.595847.
Train: 2018-08-01T23:45:21.728945: step 13962, loss 0.562889.
Train: 2018-08-01T23:45:21.896462: step 13963, loss 0.579372.
Train: 2018-08-01T23:45:22.065042: step 13964, loss 0.5464.
Train: 2018-08-01T23:45:22.234557: step 13965, loss 0.529898.
Train: 2018-08-01T23:45:22.405103: step 13966, loss 0.447328.
Train: 2018-08-01T23:45:22.573650: step 13967, loss 0.496648.
Train: 2018-08-01T23:45:22.749181: step 13968, loss 0.546212.
Train: 2018-08-01T23:45:22.919782: step 13969, loss 0.612792.
Train: 2018-08-01T23:45:23.085302: step 13970, loss 0.512662.
Test: 2018-08-01T23:45:23.626868: step 13970, loss 0.548728.
Train: 2018-08-01T23:45:23.799413: step 13971, loss 0.562763.
Train: 2018-08-01T23:45:23.967924: step 13972, loss 0.579548.
Train: 2018-08-01T23:45:24.139465: step 13973, loss 0.529064.
Train: 2018-08-01T23:45:24.310034: step 13974, loss 0.545835.
Train: 2018-08-01T23:45:24.485565: step 13975, loss 0.494996.
Train: 2018-08-01T23:45:24.656084: step 13976, loss 0.494752.
Train: 2018-08-01T23:45:24.823668: step 13977, loss 0.511524.
Train: 2018-08-01T23:45:24.994182: step 13978, loss 0.614076.
Train: 2018-08-01T23:45:25.173700: step 13979, loss 0.528318.
Train: 2018-08-01T23:45:25.344244: step 13980, loss 0.545461.
Test: 2018-08-01T23:45:25.882807: step 13980, loss 0.548207.
Train: 2018-08-01T23:45:26.050384: step 13981, loss 0.562689.
Train: 2018-08-01T23:45:26.227883: step 13982, loss 0.580036.
Train: 2018-08-01T23:45:26.397455: step 13983, loss 0.61485.
Train: 2018-08-01T23:45:26.566010: step 13984, loss 0.458267.
Train: 2018-08-01T23:45:26.733531: step 13985, loss 0.632492.
Train: 2018-08-01T23:45:26.902080: step 13986, loss 0.597659.
Train: 2018-08-01T23:45:27.074620: step 13987, loss 0.650135.
Train: 2018-08-01T23:45:27.242202: step 13988, loss 0.510293.
Train: 2018-08-01T23:45:27.424683: step 13989, loss 0.527761.
Train: 2018-08-01T23:45:27.610219: step 13990, loss 0.545227.
Test: 2018-08-01T23:45:28.155758: step 13990, loss 0.548063.
Train: 2018-08-01T23:45:28.334252: step 13991, loss 0.580203.
Train: 2018-08-01T23:45:28.508786: step 13992, loss 0.615201.
Train: 2018-08-01T23:45:28.680326: step 13993, loss 0.510244.
Train: 2018-08-01T23:45:28.856885: step 13994, loss 0.545218.
Train: 2018-08-01T23:45:29.026401: step 13995, loss 0.475228.
Train: 2018-08-01T23:45:29.202956: step 13996, loss 0.492633.
Train: 2018-08-01T23:45:29.369484: step 13997, loss 0.457386.
Train: 2018-08-01T23:45:29.532055: step 13998, loss 0.562737.
Train: 2018-08-01T23:45:29.704589: step 13999, loss 0.580415.
Train: 2018-08-01T23:45:29.871147: step 14000, loss 0.59817.
Test: 2018-08-01T23:45:30.401725: step 14000, loss 0.547935.
Train: 2018-08-01T23:45:31.222281: step 14001, loss 0.545056.
Train: 2018-08-01T23:45:31.389804: step 14002, loss 0.562787.
Train: 2018-08-01T23:45:31.557356: step 14003, loss 0.651641.
Train: 2018-08-01T23:45:31.727905: step 14004, loss 0.68715.
Train: 2018-08-01T23:45:31.894485: step 14005, loss 0.545048.
Train: 2018-08-01T23:45:32.068987: step 14006, loss 0.527356.
Train: 2018-08-01T23:45:32.239566: step 14007, loss 0.633496.
Train: 2018-08-01T23:45:32.419077: step 14008, loss 0.633343.
Train: 2018-08-01T23:45:32.586635: step 14009, loss 0.56272.
Train: 2018-08-01T23:45:32.757147: step 14010, loss 0.545147.
Test: 2018-08-01T23:45:33.274795: step 14010, loss 0.548021.
Train: 2018-08-01T23:45:33.462263: step 14011, loss 0.510134.
Train: 2018-08-01T23:45:33.635827: step 14012, loss 0.615173.
Train: 2018-08-01T23:45:33.809335: step 14013, loss 0.510286.
Train: 2018-08-01T23:45:33.981873: step 14014, loss 0.632443.
Train: 2018-08-01T23:45:34.154461: step 14015, loss 0.632296.
Train: 2018-08-01T23:45:34.322962: step 14016, loss 0.562651.
Train: 2018-08-01T23:45:34.495501: step 14017, loss 0.406797.
Train: 2018-08-01T23:45:34.667080: step 14018, loss 0.649208.
Train: 2018-08-01T23:45:34.833597: step 14019, loss 0.562643.
Train: 2018-08-01T23:45:34.999180: step 14020, loss 0.648973.
Test: 2018-08-01T23:45:35.536717: step 14020, loss 0.548206.
Train: 2018-08-01T23:45:35.709287: step 14021, loss 0.493722.
Train: 2018-08-01T23:45:35.875831: step 14022, loss 0.528215.
Train: 2018-08-01T23:45:36.047354: step 14023, loss 0.442251.
Train: 2018-08-01T23:45:36.214905: step 14024, loss 0.510981.
Train: 2018-08-01T23:45:36.379465: step 14025, loss 0.579872.
Train: 2018-08-01T23:45:36.549011: step 14026, loss 0.562629.
Train: 2018-08-01T23:45:36.722578: step 14027, loss 0.545345.
Train: 2018-08-01T23:45:36.889129: step 14028, loss 0.562631.
Train: 2018-08-01T23:45:37.060644: step 14029, loss 0.631927.
Train: 2018-08-01T23:45:37.229192: step 14030, loss 0.51066.
Test: 2018-08-01T23:45:37.768779: step 14030, loss 0.548115.
Train: 2018-08-01T23:45:37.939295: step 14031, loss 0.666642.
Train: 2018-08-01T23:45:38.108841: step 14032, loss 0.649232.
Train: 2018-08-01T23:45:38.275422: step 14033, loss 0.562623.
Train: 2018-08-01T23:45:38.451959: step 14034, loss 0.579876.
Train: 2018-08-01T23:45:38.618480: step 14035, loss 0.597066.
Train: 2018-08-01T23:45:38.786032: step 14036, loss 0.4423.
Train: 2018-08-01T23:45:38.951590: step 14037, loss 0.528248.
Train: 2018-08-01T23:45:39.119166: step 14038, loss 0.511055.
Train: 2018-08-01T23:45:39.283702: step 14039, loss 0.51101.
Train: 2018-08-01T23:45:39.453272: step 14040, loss 0.631516.
Test: 2018-08-01T23:45:39.978871: step 14040, loss 0.548181.
Train: 2018-08-01T23:45:40.142406: step 14041, loss 0.579843.
Train: 2018-08-01T23:45:40.309957: step 14042, loss 0.545376.
Train: 2018-08-01T23:45:40.480502: step 14043, loss 0.528131.
Train: 2018-08-01T23:45:40.651046: step 14044, loss 0.597113.
Train: 2018-08-01T23:45:40.820593: step 14045, loss 0.614373.
Train: 2018-08-01T23:45:40.988175: step 14046, loss 0.648832.
Train: 2018-08-01T23:45:41.161680: step 14047, loss 0.562605.
Train: 2018-08-01T23:45:41.338242: step 14048, loss 0.511034.
Train: 2018-08-01T23:45:41.506776: step 14049, loss 0.596958.
Train: 2018-08-01T23:45:41.676305: step 14050, loss 0.476806.
Test: 2018-08-01T23:45:42.216888: step 14050, loss 0.548228.
Train: 2018-08-01T23:45:42.382417: step 14051, loss 0.442465.
Train: 2018-08-01T23:45:42.552987: step 14052, loss 0.596985.
Train: 2018-08-01T23:45:42.720514: step 14053, loss 0.579812.
Train: 2018-08-01T23:45:42.887068: step 14054, loss 0.597051.
Train: 2018-08-01T23:45:43.055619: step 14055, loss 0.579828.
Train: 2018-08-01T23:45:43.232146: step 14056, loss 0.528137.
Train: 2018-08-01T23:45:43.397703: step 14057, loss 0.476412.
Train: 2018-08-01T23:45:43.563291: step 14058, loss 0.579861.
Train: 2018-08-01T23:45:43.734802: step 14059, loss 0.562598.
Train: 2018-08-01T23:45:43.900360: step 14060, loss 0.597199.
Test: 2018-08-01T23:45:44.434974: step 14060, loss 0.548107.
Train: 2018-08-01T23:45:44.611492: step 14061, loss 0.614519.
Train: 2018-08-01T23:45:44.783997: step 14062, loss 0.493396.
Train: 2018-08-01T23:45:44.955565: step 14063, loss 0.510667.
Train: 2018-08-01T23:45:45.125085: step 14064, loss 0.57993.
Train: 2018-08-01T23:45:45.292638: step 14065, loss 0.545256.
Train: 2018-08-01T23:45:45.472157: step 14066, loss 0.527881.
Train: 2018-08-01T23:45:45.646724: step 14067, loss 0.493074.
Train: 2018-08-01T23:45:45.819229: step 14068, loss 0.771624.
Train: 2018-08-01T23:45:45.984787: step 14069, loss 0.632208.
Train: 2018-08-01T23:45:46.162338: step 14070, loss 0.545232.
Test: 2018-08-01T23:45:46.698879: step 14070, loss 0.548074.
Train: 2018-08-01T23:45:46.867428: step 14071, loss 0.527912.
Train: 2018-08-01T23:45:47.034980: step 14072, loss 0.562592.
Train: 2018-08-01T23:45:47.201535: step 14073, loss 0.701078.
Train: 2018-08-01T23:45:47.373075: step 14074, loss 0.562584.
Train: 2018-08-01T23:45:47.550630: step 14075, loss 0.597028.
Train: 2018-08-01T23:45:47.722174: step 14076, loss 0.545401.
Train: 2018-08-01T23:45:47.896676: step 14077, loss 0.528296.
Train: 2018-08-01T23:45:48.069243: step 14078, loss 0.545462.
Train: 2018-08-01T23:45:48.234774: step 14079, loss 0.528384.
Train: 2018-08-01T23:45:48.406313: step 14080, loss 0.477138.
Test: 2018-08-01T23:45:48.941883: step 14080, loss 0.548256.
Train: 2018-08-01T23:45:49.106442: step 14081, loss 0.562576.
Train: 2018-08-01T23:45:49.275022: step 14082, loss 0.562574.
Train: 2018-08-01T23:45:49.443540: step 14083, loss 0.61394.
Train: 2018-08-01T23:45:49.610122: step 14084, loss 0.631051.
Train: 2018-08-01T23:45:49.779676: step 14085, loss 0.562572.
Train: 2018-08-01T23:45:49.953178: step 14086, loss 0.596743.
Train: 2018-08-01T23:45:50.117738: step 14087, loss 0.47726.
Train: 2018-08-01T23:45:50.285290: step 14088, loss 0.596698.
Train: 2018-08-01T23:45:50.452842: step 14089, loss 0.562572.
Train: 2018-08-01T23:45:50.620426: step 14090, loss 0.545523.
Test: 2018-08-01T23:45:51.156960: step 14090, loss 0.548295.
Train: 2018-08-01T23:45:51.322532: step 14091, loss 0.664857.
Train: 2018-08-01T23:45:51.509050: step 14092, loss 0.545556.
Train: 2018-08-01T23:45:51.684551: step 14093, loss 0.579577.
Train: 2018-08-01T23:45:51.848139: step 14094, loss 0.528614.
Train: 2018-08-01T23:45:52.015664: step 14095, loss 0.596511.
Train: 2018-08-01T23:45:52.190199: step 14096, loss 0.545614.
Train: 2018-08-01T23:45:52.357775: step 14097, loss 0.562602.
Train: 2018-08-01T23:45:52.525302: step 14098, loss 0.613218.
Train: 2018-08-01T23:45:52.693851: step 14099, loss 0.562831.
Train: 2018-08-01T23:45:52.871378: step 14100, loss 0.611957.
Test: 2018-08-01T23:45:53.418918: step 14100, loss 0.548836.
Train: 2018-08-01T23:45:54.269200: step 14101, loss 0.478519.
Train: 2018-08-01T23:45:54.440769: step 14102, loss 0.674395.
Train: 2018-08-01T23:45:54.610319: step 14103, loss 0.526167.
Train: 2018-08-01T23:45:54.779861: step 14104, loss 0.612517.
Train: 2018-08-01T23:45:54.948384: step 14105, loss 0.612439.
Train: 2018-08-01T23:45:55.122949: step 14106, loss 0.487715.
Train: 2018-08-01T23:45:55.288506: step 14107, loss 0.514261.
Train: 2018-08-01T23:45:55.457038: step 14108, loss 0.566986.
Train: 2018-08-01T23:45:55.621584: step 14109, loss 0.551446.
Train: 2018-08-01T23:45:55.800107: step 14110, loss 0.595158.
Test: 2018-08-01T23:45:56.335676: step 14110, loss 0.548757.
Train: 2018-08-01T23:45:56.503254: step 14111, loss 0.530744.
Train: 2018-08-01T23:45:56.674794: step 14112, loss 0.614502.
Train: 2018-08-01T23:45:56.844355: step 14113, loss 0.528747.
Train: 2018-08-01T23:45:57.018874: step 14114, loss 0.460461.
Train: 2018-08-01T23:45:57.185405: step 14115, loss 0.443001.
Train: 2018-08-01T23:45:57.354950: step 14116, loss 0.562696.
Train: 2018-08-01T23:45:57.521531: step 14117, loss 0.476524.
Train: 2018-08-01T23:45:57.686066: step 14118, loss 0.528093.
Train: 2018-08-01T23:45:57.852646: step 14119, loss 0.545357.
Train: 2018-08-01T23:45:58.020173: step 14120, loss 0.615192.
Test: 2018-08-01T23:45:58.549757: step 14120, loss 0.548117.
Train: 2018-08-01T23:45:58.720302: step 14121, loss 0.545267.
Train: 2018-08-01T23:45:58.883864: step 14122, loss 0.580387.
Train: 2018-08-01T23:45:59.063384: step 14123, loss 0.580443.
Train: 2018-08-01T23:45:59.234951: step 14124, loss 0.456936.
Train: 2018-08-01T23:45:59.411453: step 14125, loss 0.527454.
Train: 2018-08-01T23:45:59.580017: step 14126, loss 0.58063.
Train: 2018-08-01T23:45:59.746583: step 14127, loss 0.598483.
Train: 2018-08-01T23:45:59.919096: step 14128, loss 0.473799.
Train: 2018-08-01T23:46:00.089640: step 14129, loss 0.473606.
Train: 2018-08-01T23:46:00.254226: step 14130, loss 0.419594.
Test: 2018-08-01T23:46:00.793758: step 14130, loss 0.547926.
Train: 2018-08-01T23:46:00.960346: step 14131, loss 0.616999.
Train: 2018-08-01T23:46:01.125901: step 14132, loss 0.617206.
Train: 2018-08-01T23:46:01.294420: step 14133, loss 0.58115.
Train: 2018-08-01T23:46:01.462970: step 14134, loss 0.54495.
Train: 2018-08-01T23:46:01.631519: step 14135, loss 0.508648.
Train: 2018-08-01T23:46:01.860166: step 14136, loss 0.581284.
Train: 2018-08-01T23:46:02.024726: step 14137, loss 0.453926.
Train: 2018-08-01T23:46:02.199259: step 14138, loss 0.69082.
Train: 2018-08-01T23:46:02.366812: step 14139, loss 0.617879.
Train: 2018-08-01T23:46:02.539358: step 14140, loss 0.5996.
Test: 2018-08-01T23:46:03.064962: step 14140, loss 0.547865.
Train: 2018-08-01T23:46:03.237518: step 14141, loss 0.508482.
Train: 2018-08-01T23:46:03.406033: step 14142, loss 0.617705.
Train: 2018-08-01T23:46:03.573611: step 14143, loss 0.544905.
Train: 2018-08-01T23:46:03.741137: step 14144, loss 0.508598.
Train: 2018-08-01T23:46:03.921655: step 14145, loss 0.526762.
Train: 2018-08-01T23:46:04.086214: step 14146, loss 0.490483.
Train: 2018-08-01T23:46:04.253792: step 14147, loss 0.60177.
Train: 2018-08-01T23:46:04.425309: step 14148, loss 0.563045.
Train: 2018-08-01T23:46:04.594878: step 14149, loss 0.581179.
Train: 2018-08-01T23:46:04.765429: step 14150, loss 0.508639.
Test: 2018-08-01T23:46:05.303965: step 14150, loss 0.547843.
Train: 2018-08-01T23:46:05.478492: step 14151, loss 0.544895.
Train: 2018-08-01T23:46:05.650056: step 14152, loss 0.6174.
Train: 2018-08-01T23:46:05.815622: step 14153, loss 0.490562.
Train: 2018-08-01T23:46:05.984140: step 14154, loss 0.671664.
Train: 2018-08-01T23:46:06.151694: step 14155, loss 0.617228.
Train: 2018-08-01T23:46:06.323266: step 14156, loss 0.617079.
Train: 2018-08-01T23:46:06.491789: step 14157, loss 0.598899.
Train: 2018-08-01T23:46:06.657342: step 14158, loss 0.509073.
Train: 2018-08-01T23:46:06.839853: step 14159, loss 0.616538.
Train: 2018-08-01T23:46:07.008403: step 14160, loss 0.54498.
Test: 2018-08-01T23:46:07.545966: step 14160, loss 0.547895.
Train: 2018-08-01T23:46:07.716510: step 14161, loss 0.633997.
Train: 2018-08-01T23:46:07.885060: step 14162, loss 0.598254.
Train: 2018-08-01T23:46:08.047644: step 14163, loss 0.651141.
Train: 2018-08-01T23:46:08.214181: step 14164, loss 0.457097.
Train: 2018-08-01T23:46:08.384723: step 14165, loss 0.668029.
Train: 2018-08-01T23:46:08.559282: step 14166, loss 0.562674.
Train: 2018-08-01T23:46:08.726844: step 14167, loss 0.527805.
Train: 2018-08-01T23:46:08.903395: step 14168, loss 0.580026.
Train: 2018-08-01T23:46:09.072910: step 14169, loss 0.510648.
Train: 2018-08-01T23:46:09.251407: step 14170, loss 0.562633.
Test: 2018-08-01T23:46:09.788975: step 14170, loss 0.548168.
Train: 2018-08-01T23:46:09.959542: step 14171, loss 0.562629.
Train: 2018-08-01T23:46:10.127097: step 14172, loss 0.579867.
Train: 2018-08-01T23:46:10.297641: step 14173, loss 0.562622.
Train: 2018-08-01T23:46:10.472144: step 14174, loss 0.528242.
Train: 2018-08-01T23:46:10.640696: step 14175, loss 0.562617.
Train: 2018-08-01T23:46:10.811236: step 14176, loss 0.596936.
Train: 2018-08-01T23:46:10.980808: step 14177, loss 0.494057.
Train: 2018-08-01T23:46:11.159306: step 14178, loss 0.562613.
Train: 2018-08-01T23:46:11.333839: step 14179, loss 0.545476.
Train: 2018-08-01T23:46:11.510368: step 14180, loss 0.545472.
Test: 2018-08-01T23:46:12.048938: step 14180, loss 0.548253.
Train: 2018-08-01T23:46:12.214504: step 14181, loss 0.511176.
Train: 2018-08-01T23:46:12.390017: step 14182, loss 0.493959.
Train: 2018-08-01T23:46:12.561557: step 14183, loss 0.579801.
Train: 2018-08-01T23:46:12.732101: step 14184, loss 0.459292.
Train: 2018-08-01T23:46:12.903643: step 14185, loss 0.528077.
Train: 2018-08-01T23:46:13.071226: step 14186, loss 0.579927.
Train: 2018-08-01T23:46:13.240743: step 14187, loss 0.61468.
Train: 2018-08-01T23:46:13.415276: step 14188, loss 0.649508.
Train: 2018-08-01T23:46:13.583825: step 14189, loss 0.632129.
Train: 2018-08-01T23:46:13.755367: step 14190, loss 0.684142.
Test: 2018-08-01T23:46:14.287944: step 14190, loss 0.548105.
Train: 2018-08-01T23:46:14.460494: step 14191, loss 0.562606.
Train: 2018-08-01T23:46:14.636038: step 14192, loss 0.597155.
Train: 2018-08-01T23:46:14.802571: step 14193, loss 0.545363.
Train: 2018-08-01T23:46:14.973112: step 14194, loss 0.562593.
Train: 2018-08-01T23:46:15.138679: step 14195, loss 0.545428.
Train: 2018-08-01T23:46:15.308234: step 14196, loss 0.511176.
Train: 2018-08-01T23:46:15.490727: step 14197, loss 0.56259.
Train: 2018-08-01T23:46:15.661272: step 14198, loss 0.562589.
Train: 2018-08-01T23:46:15.826859: step 14199, loss 0.596807.
Train: 2018-08-01T23:46:16.000376: step 14200, loss 0.5284.
Test: 2018-08-01T23:46:16.541918: step 14200, loss 0.548278.
Train: 2018-08-01T23:46:17.458840: step 14201, loss 0.579675.
Train: 2018-08-01T23:46:17.626422: step 14202, loss 0.596744.
Train: 2018-08-01T23:46:17.796962: step 14203, loss 0.630838.
Train: 2018-08-01T23:46:17.979449: step 14204, loss 0.51149.
Train: 2018-08-01T23:46:18.154012: step 14205, loss 0.494517.
Train: 2018-08-01T23:46:18.322548: step 14206, loss 0.562587.
Train: 2018-08-01T23:46:18.496068: step 14207, loss 0.630685.
Train: 2018-08-01T23:46:18.669604: step 14208, loss 0.59661.
Train: 2018-08-01T23:46:18.839170: step 14209, loss 0.613566.
Train: 2018-08-01T23:46:19.019668: step 14210, loss 0.613483.
Test: 2018-08-01T23:46:19.554239: step 14210, loss 0.548421.
Train: 2018-08-01T23:46:19.719821: step 14211, loss 0.596449.
Train: 2018-08-01T23:46:19.885353: step 14212, loss 0.630145.
Train: 2018-08-01T23:46:20.049914: step 14213, loss 0.54578.
Train: 2018-08-01T23:46:20.219461: step 14214, loss 0.596206.
Train: 2018-08-01T23:46:20.394999: step 14215, loss 0.54589.
Train: 2018-08-01T23:46:20.575508: step 14216, loss 0.479091.
Train: 2018-08-01T23:46:20.743087: step 14217, loss 0.495845.
Train: 2018-08-01T23:46:20.912631: step 14218, loss 0.629486.
Train: 2018-08-01T23:46:21.080185: step 14219, loss 0.612757.
Train: 2018-08-01T23:46:21.251701: step 14220, loss 0.596025.
Test: 2018-08-01T23:46:21.787270: step 14220, loss 0.548706.
Train: 2018-08-01T23:46:21.955845: step 14221, loss 0.645982.
Train: 2018-08-01T23:46:22.126391: step 14222, loss 0.546048.
Train: 2018-08-01T23:46:22.296943: step 14223, loss 0.529492.
Train: 2018-08-01T23:46:22.467482: step 14224, loss 0.496368.
Train: 2018-08-01T23:46:22.637023: step 14225, loss 0.612443.
Train: 2018-08-01T23:46:22.812557: step 14226, loss 0.678733.
Train: 2018-08-01T23:46:22.982100: step 14227, loss 0.595795.
Train: 2018-08-01T23:46:23.146636: step 14228, loss 0.612248.
Train: 2018-08-01T23:46:23.323193: step 14229, loss 0.57921.
Train: 2018-08-01T23:46:23.494730: step 14230, loss 0.529914.
Test: 2018-08-01T23:46:24.039249: step 14230, loss 0.549047.
Train: 2018-08-01T23:46:24.217797: step 14231, loss 0.51358.
Train: 2018-08-01T23:46:24.387344: step 14232, loss 0.595567.
Train: 2018-08-01T23:46:24.555869: step 14233, loss 0.480901.
Train: 2018-08-01T23:46:24.722423: step 14234, loss 0.579173.
Train: 2018-08-01T23:46:24.898982: step 14235, loss 0.497191.
Train: 2018-08-01T23:46:25.068524: step 14236, loss 0.562763.
Train: 2018-08-01T23:46:25.245050: step 14237, loss 0.579196.
Train: 2018-08-01T23:46:25.412609: step 14238, loss 0.513327.
Train: 2018-08-01T23:46:25.579164: step 14239, loss 0.57922.
Train: 2018-08-01T23:46:25.743711: step 14240, loss 0.546172.
Test: 2018-08-01T23:46:26.280257: step 14240, loss 0.548818.
Train: 2018-08-01T23:46:26.498831: step 14241, loss 0.546125.
Train: 2018-08-01T23:46:26.663366: step 14242, loss 0.61246.
Train: 2018-08-01T23:46:26.832937: step 14243, loss 0.579279.
Train: 2018-08-01T23:46:27.009469: step 14244, loss 0.529389.
Train: 2018-08-01T23:46:27.180000: step 14245, loss 0.679242.
Train: 2018-08-01T23:46:27.348528: step 14246, loss 0.595949.
Train: 2018-08-01T23:46:27.528079: step 14247, loss 0.57929.
Train: 2018-08-01T23:46:27.705605: step 14248, loss 0.595909.
Train: 2018-08-01T23:46:27.875121: step 14249, loss 0.52944.
Train: 2018-08-01T23:46:28.044667: step 14250, loss 0.695494.
Test: 2018-08-01T23:46:28.581233: step 14250, loss 0.548804.
Train: 2018-08-01T23:46:28.749783: step 14251, loss 0.512975.
Train: 2018-08-01T23:46:28.913371: step 14252, loss 0.579235.
Train: 2018-08-01T23:46:29.085885: step 14253, loss 0.480043.
Train: 2018-08-01T23:46:29.253435: step 14254, loss 0.52962.
Train: 2018-08-01T23:46:29.431959: step 14255, loss 0.496482.
Train: 2018-08-01T23:46:29.612477: step 14256, loss 0.595832.
Train: 2018-08-01T23:46:29.781026: step 14257, loss 0.562658.
Train: 2018-08-01T23:46:29.957554: step 14258, loss 0.529397.
Train: 2018-08-01T23:46:30.138072: step 14259, loss 0.545981.
Train: 2018-08-01T23:46:30.303655: step 14260, loss 0.495876.
Test: 2018-08-01T23:46:30.834211: step 14260, loss 0.548594.
Train: 2018-08-01T23:46:31.022706: step 14261, loss 0.495673.
Train: 2018-08-01T23:46:31.194248: step 14262, loss 0.596173.
Train: 2018-08-01T23:46:31.362797: step 14263, loss 0.596253.
Train: 2018-08-01T23:46:31.534361: step 14264, loss 0.545692.
Train: 2018-08-01T23:46:31.698899: step 14265, loss 0.528737.
Train: 2018-08-01T23:46:31.867474: step 14266, loss 0.596454.
Train: 2018-08-01T23:46:32.032034: step 14267, loss 0.596509.
Train: 2018-08-01T23:46:32.203581: step 14268, loss 0.545541.
Train: 2018-08-01T23:46:32.373122: step 14269, loss 0.630632.
Train: 2018-08-01T23:46:32.543641: step 14270, loss 0.579565.
Test: 2018-08-01T23:46:33.074248: step 14270, loss 0.548279.
Train: 2018-08-01T23:46:33.246761: step 14271, loss 0.528484.
Train: 2018-08-01T23:46:33.414345: step 14272, loss 0.545501.
Train: 2018-08-01T23:46:33.579871: step 14273, loss 0.494345.
Train: 2018-08-01T23:46:33.749418: step 14274, loss 0.545457.
Train: 2018-08-01T23:46:33.915990: step 14275, loss 0.56253.
Train: 2018-08-01T23:46:34.085550: step 14276, loss 0.579659.
Train: 2018-08-01T23:46:34.271024: step 14277, loss 0.511077.
Train: 2018-08-01T23:46:34.448569: step 14278, loss 0.596887.
Train: 2018-08-01T23:46:34.623082: step 14279, loss 0.493731.
Train: 2018-08-01T23:46:34.801605: step 14280, loss 0.528064.
Test: 2018-08-01T23:46:35.333210: step 14280, loss 0.548069.
Train: 2018-08-01T23:46:35.500737: step 14281, loss 0.5798.
Train: 2018-08-01T23:46:35.670295: step 14282, loss 0.579832.
Train: 2018-08-01T23:46:35.836836: step 14283, loss 0.545213.
Train: 2018-08-01T23:46:36.007382: step 14284, loss 0.545192.
Train: 2018-08-01T23:46:36.172939: step 14285, loss 0.57991.
Train: 2018-08-01T23:46:36.341488: step 14286, loss 0.527769.
Train: 2018-08-01T23:46:36.513055: step 14287, loss 0.562547.
Train: 2018-08-01T23:46:36.679583: step 14288, loss 0.54512.
Train: 2018-08-01T23:46:36.850128: step 14289, loss 0.667263.
Train: 2018-08-01T23:46:37.024661: step 14290, loss 0.405547.
Test: 2018-08-01T23:46:37.567243: step 14290, loss 0.547925.
Train: 2018-08-01T23:46:37.741744: step 14291, loss 0.580034.
Train: 2018-08-01T23:46:37.910326: step 14292, loss 0.597558.
Train: 2018-08-01T23:46:38.074854: step 14293, loss 0.597582.
Train: 2018-08-01T23:46:38.240412: step 14294, loss 0.632598.
Train: 2018-08-01T23:46:38.411953: step 14295, loss 0.615034.
Train: 2018-08-01T23:46:38.581528: step 14296, loss 0.632402.
Train: 2018-08-01T23:46:38.751047: step 14297, loss 0.649638.
Train: 2018-08-01T23:46:38.914609: step 14298, loss 0.527814.
Train: 2018-08-01T23:46:39.094155: step 14299, loss 0.579837.
Train: 2018-08-01T23:46:39.263701: step 14300, loss 0.614317.
Test: 2018-08-01T23:46:39.803234: step 14300, loss 0.548103.
Train: 2018-08-01T23:46:40.664805: step 14301, loss 0.47646.
Train: 2018-08-01T23:46:40.830393: step 14302, loss 0.493785.
Train: 2018-08-01T23:46:41.005893: step 14303, loss 0.631205.
Train: 2018-08-01T23:46:41.176470: step 14304, loss 0.613957.
Train: 2018-08-01T23:46:41.345029: step 14305, loss 0.528287.
Train: 2018-08-01T23:46:41.512538: step 14306, loss 0.528334.
Train: 2018-08-01T23:46:41.688095: step 14307, loss 0.562514.
Train: 2018-08-01T23:46:41.861636: step 14308, loss 0.596648.
Train: 2018-08-01T23:46:42.038133: step 14309, loss 0.647762.
Train: 2018-08-01T23:46:42.208686: step 14310, loss 0.562518.
Test: 2018-08-01T23:46:42.744245: step 14310, loss 0.548302.
Train: 2018-08-01T23:46:42.914789: step 14311, loss 0.613466.
Train: 2018-08-01T23:46:43.083351: step 14312, loss 0.562527.
Train: 2018-08-01T23:46:43.251888: step 14313, loss 0.444198.
Train: 2018-08-01T23:46:43.421436: step 14314, loss 0.427315.
Train: 2018-08-01T23:46:43.595002: step 14315, loss 0.613327.
Train: 2018-08-01T23:46:43.759557: step 14316, loss 0.596424.
Train: 2018-08-01T23:46:43.931073: step 14317, loss 0.511652.
Train: 2018-08-01T23:46:44.096664: step 14318, loss 0.545543.
Train: 2018-08-01T23:46:44.266202: step 14319, loss 0.511521.
Train: 2018-08-01T23:46:44.435723: step 14320, loss 0.613605.
Test: 2018-08-01T23:46:44.978309: step 14320, loss 0.548234.
Train: 2018-08-01T23:46:45.156827: step 14321, loss 0.443171.
Train: 2018-08-01T23:46:45.325376: step 14322, loss 0.682158.
Train: 2018-08-01T23:46:45.496888: step 14323, loss 0.61382.
Train: 2018-08-01T23:46:45.661474: step 14324, loss 0.613815.
Train: 2018-08-01T23:46:45.838973: step 14325, loss 0.596685.
Train: 2018-08-01T23:46:46.008520: step 14326, loss 0.494226.
Train: 2018-08-01T23:46:46.182055: step 14327, loss 0.596643.
Train: 2018-08-01T23:46:46.349640: step 14328, loss 0.528387.
Train: 2018-08-01T23:46:46.521180: step 14329, loss 0.511326.
Train: 2018-08-01T23:46:46.692690: step 14330, loss 0.477142.
Test: 2018-08-01T23:46:47.226278: step 14330, loss 0.548179.
Train: 2018-08-01T23:46:47.398802: step 14331, loss 0.613816.
Train: 2018-08-01T23:46:47.568349: step 14332, loss 0.54538.
Train: 2018-08-01T23:46:47.734906: step 14333, loss 0.596782.
Train: 2018-08-01T23:46:47.905448: step 14334, loss 0.5968.
Train: 2018-08-01T23:46:48.080978: step 14335, loss 0.57965.
Train: 2018-08-01T23:46:48.246555: step 14336, loss 0.51106.
Train: 2018-08-01T23:46:48.413090: step 14337, loss 0.579654.
Train: 2018-08-01T23:46:48.592636: step 14338, loss 0.493861.
Train: 2018-08-01T23:46:48.765181: step 14339, loss 0.493782.
Train: 2018-08-01T23:46:48.933700: step 14340, loss 0.493646.
Test: 2018-08-01T23:46:49.465280: step 14340, loss 0.548048.
Train: 2018-08-01T23:46:49.639812: step 14341, loss 0.579761.
Train: 2018-08-01T23:46:49.808377: step 14342, loss 0.5971.
Train: 2018-08-01T23:46:49.981928: step 14343, loss 0.614476.
Train: 2018-08-01T23:46:50.161418: step 14344, loss 0.458509.
Train: 2018-08-01T23:46:50.328989: step 14345, loss 0.510414.
Train: 2018-08-01T23:46:50.507492: step 14346, loss 0.510297.
Train: 2018-08-01T23:46:50.681029: step 14347, loss 0.475249.
Train: 2018-08-01T23:46:50.853586: step 14348, loss 0.66766.
Train: 2018-08-01T23:46:51.020122: step 14349, loss 0.509902.
Train: 2018-08-01T23:46:51.197648: step 14350, loss 0.544976.
Test: 2018-08-01T23:46:51.728229: step 14350, loss 0.547817.
Train: 2018-08-01T23:46:51.894817: step 14351, loss 0.544951.
Train: 2018-08-01T23:46:52.064363: step 14352, loss 0.59792.
Train: 2018-08-01T23:46:52.231913: step 14353, loss 0.527225.
Train: 2018-08-01T23:46:52.401430: step 14354, loss 0.491751.
Train: 2018-08-01T23:46:52.571974: step 14355, loss 0.598137.
Train: 2018-08-01T23:46:52.745509: step 14356, loss 0.47374.
Train: 2018-08-01T23:46:52.916055: step 14357, loss 0.509196.
Train: 2018-08-01T23:46:53.084635: step 14358, loss 0.634159.
Train: 2018-08-01T23:46:53.257173: step 14359, loss 0.652162.
Train: 2018-08-01T23:46:53.427686: step 14360, loss 0.652156.
Test: 2018-08-01T23:46:53.974226: step 14360, loss 0.54772.
Train: 2018-08-01T23:46:54.156737: step 14361, loss 0.544815.
Train: 2018-08-01T23:46:54.324320: step 14362, loss 0.544823.
Train: 2018-08-01T23:46:54.500851: step 14363, loss 0.455662.
Train: 2018-08-01T23:46:54.676348: step 14364, loss 0.437773.
Train: 2018-08-01T23:46:54.852876: step 14365, loss 0.562686.
Train: 2018-08-01T23:46:55.020428: step 14366, loss 0.652223.
Train: 2018-08-01T23:46:55.188012: step 14367, loss 0.491073.
Train: 2018-08-01T23:46:55.359552: step 14368, loss 0.544788.
Train: 2018-08-01T23:46:55.536080: step 14369, loss 0.562721.
Train: 2018-08-01T23:46:55.705644: step 14370, loss 0.526823.
Test: 2018-08-01T23:46:56.242163: step 14370, loss 0.547691.
Train: 2018-08-01T23:46:56.414701: step 14371, loss 0.616628.
Train: 2018-08-01T23:46:56.584247: step 14372, loss 0.544794.
Train: 2018-08-01T23:46:56.761774: step 14373, loss 0.56266.
Train: 2018-08-01T23:46:56.934313: step 14374, loss 0.56291.
Train: 2018-08-01T23:46:57.105878: step 14375, loss 0.527135.
Train: 2018-08-01T23:46:57.285374: step 14376, loss 0.634569.
Train: 2018-08-01T23:46:57.456941: step 14377, loss 0.580703.
Train: 2018-08-01T23:46:57.627490: step 14378, loss 0.526847.
Train: 2018-08-01T23:46:57.799998: step 14379, loss 0.634396.
Train: 2018-08-01T23:46:57.975546: step 14380, loss 0.544801.
Test: 2018-08-01T23:46:58.511098: step 14380, loss 0.547718.
Train: 2018-08-01T23:46:58.689658: step 14381, loss 0.491216.
Train: 2018-08-01T23:46:58.861162: step 14382, loss 0.473391.
Train: 2018-08-01T23:46:59.029711: step 14383, loss 0.509079.
Train: 2018-08-01T23:46:59.203275: step 14384, loss 0.455375.
Train: 2018-08-01T23:46:59.377811: step 14385, loss 0.670266.
Train: 2018-08-01T23:46:59.552344: step 14386, loss 0.508916.
Train: 2018-08-01T23:46:59.726849: step 14387, loss 0.58068.
Train: 2018-08-01T23:46:59.894426: step 14388, loss 0.526812.
Train: 2018-08-01T23:47:00.061985: step 14389, loss 0.562743.
Train: 2018-08-01T23:47:00.231529: step 14390, loss 0.454837.
Test: 2018-08-01T23:47:00.762078: step 14390, loss 0.547682.
Train: 2018-08-01T23:47:00.938634: step 14391, loss 0.670866.
Train: 2018-08-01T23:47:01.108154: step 14392, loss 0.634837.
Train: 2018-08-01T23:47:01.274740: step 14393, loss 0.634752.
Train: 2018-08-01T23:47:01.446269: step 14394, loss 0.490879.
Train: 2018-08-01T23:47:01.616800: step 14395, loss 0.526831.
Train: 2018-08-01T23:47:01.786341: step 14396, loss 0.50891.
Train: 2018-08-01T23:47:01.957882: step 14397, loss 0.580651.
Train: 2018-08-01T23:47:02.125435: step 14398, loss 0.562711.
Train: 2018-08-01T23:47:02.297974: step 14399, loss 0.49102.
Train: 2018-08-01T23:47:02.468549: step 14400, loss 0.544781.
Test: 2018-08-01T23:47:03.010069: step 14400, loss 0.547694.
Train: 2018-08-01T23:47:03.904903: step 14401, loss 0.473033.
Train: 2018-08-01T23:47:04.077441: step 14402, loss 0.580689.
Train: 2018-08-01T23:47:04.251006: step 14403, loss 0.562737.
Train: 2018-08-01T23:47:04.418560: step 14404, loss 0.526766.
Train: 2018-08-01T23:47:04.588103: step 14405, loss 0.436718.
Train: 2018-08-01T23:47:04.759618: step 14406, loss 0.526689.
Train: 2018-08-01T23:47:04.924209: step 14407, loss 0.635159.
Train: 2018-08-01T23:47:05.099708: step 14408, loss 0.544715.
Train: 2018-08-01T23:47:05.268283: step 14409, loss 0.635329.
Train: 2018-08-01T23:47:05.435854: step 14410, loss 0.617193.
Test: 2018-08-01T23:47:05.967421: step 14410, loss 0.547659.
Train: 2018-08-01T23:47:06.138930: step 14411, loss 0.50851.
Train: 2018-08-01T23:47:06.308479: step 14412, loss 0.49043.
Train: 2018-08-01T23:47:06.485042: step 14413, loss 0.544714.
Train: 2018-08-01T23:47:06.665523: step 14414, loss 0.599042.
Train: 2018-08-01T23:47:06.834099: step 14415, loss 0.580924.
Train: 2018-08-01T23:47:07.003650: step 14416, loss 0.617095.
Train: 2018-08-01T23:47:07.170200: step 14417, loss 0.544722.
Train: 2018-08-01T23:47:07.335731: step 14418, loss 0.67107.
Train: 2018-08-01T23:47:07.501288: step 14419, loss 0.544744.
Train: 2018-08-01T23:47:07.666847: step 14420, loss 0.580683.
Test: 2018-08-01T23:47:08.200421: step 14420, loss 0.54769.
Train: 2018-08-01T23:47:08.368969: step 14421, loss 0.580615.
Train: 2018-08-01T23:47:08.545498: step 14422, loss 0.616293.
Train: 2018-08-01T23:47:08.720030: step 14423, loss 0.633918.
Train: 2018-08-01T23:47:08.890575: step 14424, loss 0.527105.
Train: 2018-08-01T23:47:09.061119: step 14425, loss 0.527191.
Train: 2018-08-01T23:47:09.236650: step 14426, loss 0.580221.
Train: 2018-08-01T23:47:09.402207: step 14427, loss 0.61538.
Train: 2018-08-01T23:47:09.567765: step 14428, loss 0.492308.
Train: 2018-08-01T23:47:09.738309: step 14429, loss 0.580046.
Train: 2018-08-01T23:47:09.911870: step 14430, loss 0.527537.
Test: 2018-08-01T23:47:10.454420: step 14430, loss 0.547883.
Train: 2018-08-01T23:47:10.622972: step 14431, loss 0.527579.
Train: 2018-08-01T23:47:10.796481: step 14432, loss 0.64975.
Train: 2018-08-01T23:47:10.979989: step 14433, loss 0.545081.
Train: 2018-08-01T23:47:11.150533: step 14434, loss 0.51033.
Train: 2018-08-01T23:47:11.334044: step 14435, loss 0.492994.
Train: 2018-08-01T23:47:11.505601: step 14436, loss 0.562488.
Train: 2018-08-01T23:47:11.678153: step 14437, loss 0.614627.
Train: 2018-08-01T23:47:11.847696: step 14438, loss 0.545117.
Train: 2018-08-01T23:47:12.018214: step 14439, loss 0.562485.
Train: 2018-08-01T23:47:12.187761: step 14440, loss 0.57984.
Test: 2018-08-01T23:47:12.727319: step 14440, loss 0.547957.
Train: 2018-08-01T23:47:12.897881: step 14441, loss 0.579828.
Train: 2018-08-01T23:47:13.070401: step 14442, loss 0.527816.
Train: 2018-08-01T23:47:13.241942: step 14443, loss 0.441191.
Train: 2018-08-01T23:47:13.413509: step 14444, loss 0.545131.
Train: 2018-08-01T23:47:13.587047: step 14445, loss 0.54511.
Train: 2018-08-01T23:47:13.763548: step 14446, loss 0.597292.
Train: 2018-08-01T23:47:13.940091: step 14447, loss 0.527662.
Train: 2018-08-01T23:47:14.107628: step 14448, loss 0.71128.
Train: 2018-08-01T23:47:14.276179: step 14449, loss 0.492821.
Train: 2018-08-01T23:47:14.441766: step 14450, loss 0.545074.
Test: 2018-08-01T23:47:14.979356: step 14450, loss 0.547905.
Train: 2018-08-01T23:47:15.147848: step 14451, loss 0.47539.
Train: 2018-08-01T23:47:15.327368: step 14452, loss 0.527611.
Train: 2018-08-01T23:47:15.496915: step 14453, loss 0.632383.
Train: 2018-08-01T23:47:15.663470: step 14454, loss 0.545027.
Train: 2018-08-01T23:47:15.838003: step 14455, loss 0.667429.
Train: 2018-08-01T23:47:16.013567: step 14456, loss 0.562502.
Train: 2018-08-01T23:47:16.193079: step 14457, loss 0.649768.
Train: 2018-08-01T23:47:16.369606: step 14458, loss 0.527654.
Train: 2018-08-01T23:47:16.541123: step 14459, loss 0.562485.
Train: 2018-08-01T23:47:16.707704: step 14460, loss 0.475632.
Test: 2018-08-01T23:47:17.253226: step 14460, loss 0.547936.
Train: 2018-08-01T23:47:17.432740: step 14461, loss 0.527743.
Train: 2018-08-01T23:47:17.600292: step 14462, loss 0.562481.
Train: 2018-08-01T23:47:17.769864: step 14463, loss 0.4582.
Train: 2018-08-01T23:47:17.940382: step 14464, loss 0.545078.
Train: 2018-08-01T23:47:18.110926: step 14465, loss 0.457866.
Train: 2018-08-01T23:47:18.284476: step 14466, loss 0.475067.
Train: 2018-08-01T23:47:18.459021: step 14467, loss 0.527418.
Train: 2018-08-01T23:47:18.632533: step 14468, loss 0.597774.
Train: 2018-08-01T23:47:18.813049: step 14469, loss 0.56256.
Train: 2018-08-01T23:47:18.981624: step 14470, loss 0.491749.
Test: 2018-08-01T23:47:19.507193: step 14470, loss 0.547728.
Train: 2018-08-01T23:47:19.671754: step 14471, loss 0.491561.
Train: 2018-08-01T23:47:19.850291: step 14472, loss 0.58045.
Train: 2018-08-01T23:47:20.020822: step 14473, loss 0.687751.
Train: 2018-08-01T23:47:20.188372: step 14474, loss 0.562662.
Train: 2018-08-01T23:47:20.356948: step 14475, loss 0.616351.
Train: 2018-08-01T23:47:20.531456: step 14476, loss 0.508997.
Train: 2018-08-01T23:47:20.705025: step 14477, loss 0.634243.
Train: 2018-08-01T23:47:20.884538: step 14478, loss 0.562659.
Train: 2018-08-01T23:47:21.055056: step 14479, loss 0.49119.
Train: 2018-08-01T23:47:21.229615: step 14480, loss 0.455456.
Test: 2018-08-01T23:47:21.772140: step 14480, loss 0.547683.
Train: 2018-08-01T23:47:21.954652: step 14481, loss 0.508997.
Train: 2018-08-01T23:47:22.130182: step 14482, loss 0.508922.
Train: 2018-08-01T23:47:22.298732: step 14483, loss 0.616572.
Train: 2018-08-01T23:47:22.476258: step 14484, loss 0.562713.
Train: 2018-08-01T23:47:22.650816: step 14485, loss 0.616693.
Train: 2018-08-01T23:47:22.826321: step 14486, loss 0.616686.
Train: 2018-08-01T23:47:22.994870: step 14487, loss 0.544739.
Train: 2018-08-01T23:47:23.169404: step 14488, loss 0.490867.
Train: 2018-08-01T23:47:23.332967: step 14489, loss 0.49086.
Train: 2018-08-01T23:47:23.503511: step 14490, loss 0.508783.
Test: 2018-08-01T23:47:24.028108: step 14490, loss 0.547654.
Train: 2018-08-01T23:47:24.199649: step 14491, loss 0.634728.
Train: 2018-08-01T23:47:24.365221: step 14492, loss 0.544726.
Train: 2018-08-01T23:47:24.531762: step 14493, loss 0.634757.
Train: 2018-08-01T23:47:24.708314: step 14494, loss 0.562721.
Train: 2018-08-01T23:47:24.878862: step 14495, loss 0.526762.
Train: 2018-08-01T23:47:25.046405: step 14496, loss 0.598633.
Train: 2018-08-01T23:47:25.228899: step 14497, loss 0.598583.
Train: 2018-08-01T23:47:25.401463: step 14498, loss 0.562676.
Train: 2018-08-01T23:47:25.576968: step 14499, loss 0.58055.
Train: 2018-08-01T23:47:25.744544: step 14500, loss 0.580502.
Test: 2018-08-01T23:47:26.282083: step 14500, loss 0.547699.
Train: 2018-08-01T23:47:27.161300: step 14501, loss 0.5448.
Train: 2018-08-01T23:47:27.330871: step 14502, loss 0.491428.
Train: 2018-08-01T23:47:27.498392: step 14503, loss 0.740445.
Train: 2018-08-01T23:47:27.678908: step 14504, loss 0.633503.
Train: 2018-08-01T23:47:27.847466: step 14505, loss 0.509558.
Train: 2018-08-01T23:47:28.021992: step 14506, loss 0.650615.
Train: 2018-08-01T23:47:28.190567: step 14507, loss 0.509861.
Train: 2018-08-01T23:47:28.363111: step 14508, loss 0.527494.
Train: 2018-08-01T23:47:28.539634: step 14509, loss 0.56249.
Train: 2018-08-01T23:47:28.714166: step 14510, loss 0.545049.
Test: 2018-08-01T23:47:29.240739: step 14510, loss 0.547901.
Train: 2018-08-01T23:47:29.407288: step 14511, loss 0.475448.
Train: 2018-08-01T23:47:29.579842: step 14512, loss 0.632079.
Train: 2018-08-01T23:47:29.752366: step 14513, loss 0.64937.
Train: 2018-08-01T23:47:29.922935: step 14514, loss 0.475767.
Train: 2018-08-01T23:47:30.085475: step 14515, loss 0.562462.
Train: 2018-08-01T23:47:30.255023: step 14516, loss 0.510537.
Train: 2018-08-01T23:47:30.425566: step 14517, loss 0.614378.
Train: 2018-08-01T23:47:30.599102: step 14518, loss 0.527875.
Train: 2018-08-01T23:47:30.769678: step 14519, loss 0.527885.
Train: 2018-08-01T23:47:30.936202: step 14520, loss 0.579747.
Test: 2018-08-01T23:47:31.472768: step 14520, loss 0.547981.
Train: 2018-08-01T23:47:31.644342: step 14521, loss 0.493308.
Train: 2018-08-01T23:47:31.816848: step 14522, loss 0.666268.
Train: 2018-08-01T23:47:31.985396: step 14523, loss 0.527878.
Train: 2018-08-01T23:47:32.156939: step 14524, loss 0.54517.
Train: 2018-08-01T23:47:32.328479: step 14525, loss 0.545169.
Train: 2018-08-01T23:47:32.498027: step 14526, loss 0.510581.
Train: 2018-08-01T23:47:32.667573: step 14527, loss 0.562458.
Train: 2018-08-01T23:47:32.835126: step 14528, loss 0.649067.
Train: 2018-08-01T23:47:33.003675: step 14529, loss 0.527833.
Train: 2018-08-01T23:47:33.175217: step 14530, loss 0.597084.
Test: 2018-08-01T23:47:33.709787: step 14530, loss 0.547967.
Train: 2018-08-01T23:47:33.882338: step 14531, loss 0.49324.
Train: 2018-08-01T23:47:34.047883: step 14532, loss 0.666332.
Train: 2018-08-01T23:47:34.216432: step 14533, loss 0.700813.
Train: 2018-08-01T23:47:34.386010: step 14534, loss 0.458978.
Train: 2018-08-01T23:47:34.557539: step 14535, loss 0.545224.
Train: 2018-08-01T23:47:34.723109: step 14536, loss 0.66572.
Train: 2018-08-01T23:47:34.897611: step 14537, loss 0.510925.
Train: 2018-08-01T23:47:35.071158: step 14538, loss 0.562448.
Train: 2018-08-01T23:47:35.244702: step 14539, loss 0.511046.
Train: 2018-08-01T23:47:35.414262: step 14540, loss 0.511061.
Test: 2018-08-01T23:47:35.948801: step 14540, loss 0.548097.
Train: 2018-08-01T23:47:36.117352: step 14541, loss 0.459627.
Train: 2018-08-01T23:47:36.284929: step 14542, loss 0.665457.
Train: 2018-08-01T23:47:36.456475: step 14543, loss 0.493758.
Train: 2018-08-01T23:47:36.628013: step 14544, loss 0.493679.
Train: 2018-08-01T23:47:36.804514: step 14545, loss 0.614126.
Train: 2018-08-01T23:47:36.975060: step 14546, loss 0.631425.
Train: 2018-08-01T23:47:37.148595: step 14547, loss 0.562449.
Train: 2018-08-01T23:47:37.319164: step 14548, loss 0.493476.
Train: 2018-08-01T23:47:37.490680: step 14549, loss 0.596967.
Train: 2018-08-01T23:47:37.665214: step 14550, loss 0.493393.
Test: 2018-08-01T23:47:38.201778: step 14550, loss 0.547978.
Train: 2018-08-01T23:47:38.370329: step 14551, loss 0.614308.
Train: 2018-08-01T23:47:38.543865: step 14552, loss 0.527869.
Train: 2018-08-01T23:47:38.716435: step 14553, loss 0.631673.
Train: 2018-08-01T23:47:38.894957: step 14554, loss 0.510553.
Train: 2018-08-01T23:47:39.079464: step 14555, loss 0.614376.
Train: 2018-08-01T23:47:39.252969: step 14556, loss 0.493246.
Train: 2018-08-01T23:47:39.422515: step 14557, loss 0.579766.
Train: 2018-08-01T23:47:39.592093: step 14558, loss 0.493186.
Train: 2018-08-01T23:47:39.757619: step 14559, loss 0.527782.
Train: 2018-08-01T23:47:39.931156: step 14560, loss 0.614549.
Test: 2018-08-01T23:47:40.452762: step 14560, loss 0.547914.
Train: 2018-08-01T23:47:40.622307: step 14561, loss 0.666701.
Train: 2018-08-01T23:47:40.790858: step 14562, loss 0.458315.
Train: 2018-08-01T23:47:40.956416: step 14563, loss 0.562461.
Train: 2018-08-01T23:47:41.128954: step 14564, loss 0.579837.
Train: 2018-08-01T23:47:41.297504: step 14565, loss 0.545085.
Train: 2018-08-01T23:47:41.469070: step 14566, loss 0.492932.
Train: 2018-08-01T23:47:41.640609: step 14567, loss 0.597273.
Train: 2018-08-01T23:47:41.818111: step 14568, loss 0.562468.
Train: 2018-08-01T23:47:41.991649: step 14569, loss 0.475365.
Train: 2018-08-01T23:47:42.163188: step 14570, loss 0.59737.
Test: 2018-08-01T23:47:42.703775: step 14570, loss 0.547856.
Train: 2018-08-01T23:47:42.879301: step 14571, loss 0.527554.
Train: 2018-08-01T23:47:43.053809: step 14572, loss 0.545.
Train: 2018-08-01T23:47:43.220363: step 14573, loss 0.68502.
Train: 2018-08-01T23:47:43.388913: step 14574, loss 0.544991.
Train: 2018-08-01T23:47:43.572455: step 14575, loss 0.614949.
Train: 2018-08-01T23:47:43.743994: step 14576, loss 0.510071.
Train: 2018-08-01T23:47:43.926476: step 14577, loss 0.457695.
Train: 2018-08-01T23:47:44.092061: step 14578, loss 0.562482.
Train: 2018-08-01T23:47:44.259585: step 14579, loss 0.579984.
Train: 2018-08-01T23:47:44.429132: step 14580, loss 0.492462.
Test: 2018-08-01T23:47:44.958715: step 14580, loss 0.547815.
Train: 2018-08-01T23:47:45.132270: step 14581, loss 0.597554.
Train: 2018-08-01T23:47:45.300801: step 14582, loss 0.492332.
Train: 2018-08-01T23:47:45.472344: step 14583, loss 0.544938.
Train: 2018-08-01T23:47:45.641889: step 14584, loss 0.668069.
Train: 2018-08-01T23:47:45.809473: step 14585, loss 0.544922.
Train: 2018-08-01T23:47:45.985997: step 14586, loss 0.562512.
Train: 2018-08-01T23:47:46.158508: step 14587, loss 0.650458.
Train: 2018-08-01T23:47:46.331047: step 14588, loss 0.492238.
Train: 2018-08-01T23:47:46.503617: step 14589, loss 0.544941.
Train: 2018-08-01T23:47:46.674162: step 14590, loss 0.580061.
Test: 2018-08-01T23:47:47.211693: step 14590, loss 0.547802.
Train: 2018-08-01T23:47:47.383248: step 14591, loss 0.509843.
Train: 2018-08-01T23:47:47.549815: step 14592, loss 0.580058.
Train: 2018-08-01T23:47:47.719335: step 14593, loss 0.580056.
Train: 2018-08-01T23:47:47.890878: step 14594, loss 0.544949.
Train: 2018-08-01T23:47:48.055446: step 14595, loss 0.474766.
Train: 2018-08-01T23:47:48.227977: step 14596, loss 0.562502.
Train: 2018-08-01T23:47:48.394558: step 14597, loss 0.562506.
Train: 2018-08-01T23:47:48.564078: step 14598, loss 0.456985.
Train: 2018-08-01T23:47:48.730663: step 14599, loss 0.50966.
Train: 2018-08-01T23:47:48.904195: step 14600, loss 0.633179.
Test: 2018-08-01T23:47:49.441731: step 14600, loss 0.547738.
Train: 2018-08-01T23:47:50.248127: step 14601, loss 0.562543.
Train: 2018-08-01T23:47:50.418650: step 14602, loss 0.580243.
Train: 2018-08-01T23:47:50.592190: step 14603, loss 0.54485.
Train: 2018-08-01T23:47:50.759731: step 14604, loss 0.562556.
Train: 2018-08-01T23:47:50.932305: step 14605, loss 0.580277.
Train: 2018-08-01T23:47:51.101817: step 14606, loss 0.509402.
Train: 2018-08-01T23:47:51.279373: step 14607, loss 0.527104.
Train: 2018-08-01T23:47:51.458890: step 14608, loss 0.598061.
Train: 2018-08-01T23:47:51.634411: step 14609, loss 0.580322.
Train: 2018-08-01T23:47:51.802943: step 14610, loss 0.509326.
Test: 2018-08-01T23:47:52.332542: step 14610, loss 0.547705.
Train: 2018-08-01T23:47:52.502074: step 14611, loss 0.580332.
Train: 2018-08-01T23:47:52.667656: step 14612, loss 0.562575.
Train: 2018-08-01T23:47:52.850144: step 14613, loss 0.580334.
Train: 2018-08-01T23:47:53.016698: step 14614, loss 0.562572.
Train: 2018-08-01T23:47:53.192281: step 14615, loss 0.562569.
Train: 2018-08-01T23:47:53.364768: step 14616, loss 0.438409.
Train: 2018-08-01T23:47:53.533317: step 14617, loss 0.633596.
Train: 2018-08-01T23:47:53.717855: step 14618, loss 0.580328.
Train: 2018-08-01T23:47:53.897369: step 14619, loss 0.651309.
Train: 2018-08-01T23:47:54.064895: step 14620, loss 0.580277.
Test: 2018-08-01T23:47:54.600504: step 14620, loss 0.547732.
Train: 2018-08-01T23:47:54.769014: step 14621, loss 0.615608.
Train: 2018-08-01T23:47:54.949562: step 14622, loss 0.562527.
Train: 2018-08-01T23:47:55.117084: step 14623, loss 0.509694.
Train: 2018-08-01T23:47:55.290620: step 14624, loss 0.562505.
Train: 2018-08-01T23:47:55.458199: step 14625, loss 0.544939.
Train: 2018-08-01T23:47:55.627717: step 14626, loss 0.562491.
Train: 2018-08-01T23:47:55.793278: step 14627, loss 0.615051.
Train: 2018-08-01T23:47:55.961826: step 14628, loss 0.527494.
Train: 2018-08-01T23:47:56.139351: step 14629, loss 0.562473.
Train: 2018-08-01T23:47:56.306930: step 14630, loss 0.649741.
Test: 2018-08-01T23:47:56.837486: step 14630, loss 0.547876.
Train: 2018-08-01T23:47:57.013016: step 14631, loss 0.597293.
Train: 2018-08-01T23:47:57.193534: step 14632, loss 0.597199.
Train: 2018-08-01T23:47:57.360088: step 14633, loss 0.527795.
Train: 2018-08-01T23:47:57.529665: step 14634, loss 0.597021.
Train: 2018-08-01T23:47:57.697212: step 14635, loss 0.562438.
Train: 2018-08-01T23:47:57.876737: step 14636, loss 0.631285.
Train: 2018-08-01T23:47:58.044289: step 14637, loss 0.59676.
Train: 2018-08-01T23:47:58.221784: step 14638, loss 0.545325.
Train: 2018-08-01T23:47:58.394323: step 14639, loss 0.528305.
Train: 2018-08-01T23:47:58.566888: step 14640, loss 0.647618.
Test: 2018-08-01T23:47:59.107416: step 14640, loss 0.548219.
Train: 2018-08-01T23:47:59.282947: step 14641, loss 0.613408.
Train: 2018-08-01T23:47:59.457511: step 14642, loss 0.477787.
Train: 2018-08-01T23:47:59.625033: step 14643, loss 0.511742.
Train: 2018-08-01T23:47:59.793614: step 14644, loss 0.545567.
Train: 2018-08-01T23:47:59.963160: step 14645, loss 0.427363.
Train: 2018-08-01T23:48:00.136665: step 14646, loss 0.613202.
Train: 2018-08-01T23:48:00.304218: step 14647, loss 0.528594.
Train: 2018-08-01T23:48:00.473790: step 14648, loss 0.545498.
Train: 2018-08-01T23:48:00.645330: step 14649, loss 0.528495.
Train: 2018-08-01T23:48:00.812858: step 14650, loss 0.409389.
Test: 2018-08-01T23:48:01.344435: step 14650, loss 0.548141.
Train: 2018-08-01T23:48:01.514980: step 14651, loss 0.579506.
Train: 2018-08-01T23:48:01.688516: step 14652, loss 0.579558.
Train: 2018-08-01T23:48:01.908927: step 14653, loss 0.528092.
Train: 2018-08-01T23:48:02.073489: step 14654, loss 0.682966.
Train: 2018-08-01T23:48:02.259989: step 14655, loss 0.648604.
Train: 2018-08-01T23:48:02.430532: step 14656, loss 0.562434.
Train: 2018-08-01T23:48:02.605108: step 14657, loss 0.545214.
Train: 2018-08-01T23:48:02.770624: step 14658, loss 0.510781.
Train: 2018-08-01T23:48:02.943163: step 14659, loss 0.562434.
Train: 2018-08-01T23:48:03.110739: step 14660, loss 0.579671.
Test: 2018-08-01T23:48:03.646282: step 14660, loss 0.547998.
Train: 2018-08-01T23:48:03.822836: step 14661, loss 0.493474.
Train: 2018-08-01T23:48:04.005322: step 14662, loss 0.631474.
Train: 2018-08-01T23:48:04.179882: step 14663, loss 0.579696.
Train: 2018-08-01T23:48:04.345414: step 14664, loss 0.579692.
Train: 2018-08-01T23:48:04.509004: step 14665, loss 0.596932.
Train: 2018-08-01T23:48:04.680518: step 14666, loss 0.614133.
Train: 2018-08-01T23:48:04.845111: step 14667, loss 0.493605.
Train: 2018-08-01T23:48:05.012661: step 14668, loss 0.562432.
Train: 2018-08-01T23:48:05.178188: step 14669, loss 0.528047.
Train: 2018-08-01T23:48:05.345770: step 14670, loss 0.596819.
Test: 2018-08-01T23:48:05.876352: step 14670, loss 0.548039.
Train: 2018-08-01T23:48:06.046865: step 14671, loss 0.510869.
Train: 2018-08-01T23:48:06.214442: step 14672, loss 0.614011.
Train: 2018-08-01T23:48:06.384992: step 14673, loss 0.648363.
Train: 2018-08-01T23:48:06.560523: step 14674, loss 0.545271.
Train: 2018-08-01T23:48:06.735060: step 14675, loss 0.545291.
Train: 2018-08-01T23:48:06.898589: step 14676, loss 0.56243.
Train: 2018-08-01T23:48:07.065175: step 14677, loss 0.476864.
Train: 2018-08-01T23:48:07.239677: step 14678, loss 0.476816.
Train: 2018-08-01T23:48:07.408227: step 14679, loss 0.579582.
Train: 2018-08-01T23:48:07.574781: step 14680, loss 0.613953.
Test: 2018-08-01T23:48:08.113341: step 14680, loss 0.548042.
Train: 2018-08-01T23:48:08.287874: step 14681, loss 0.545248.
Train: 2018-08-01T23:48:08.457452: step 14682, loss 0.56243.
Train: 2018-08-01T23:48:08.625971: step 14683, loss 0.614038.
Train: 2018-08-01T23:48:08.804519: step 14684, loss 0.665623.
Train: 2018-08-01T23:48:08.972070: step 14685, loss 0.493747.
Train: 2018-08-01T23:48:09.139629: step 14686, loss 0.596753.
Train: 2018-08-01T23:48:09.306172: step 14687, loss 0.545284.
Train: 2018-08-01T23:48:09.482681: step 14688, loss 0.579564.
Train: 2018-08-01T23:48:09.656217: step 14689, loss 0.562429.
Train: 2018-08-01T23:48:09.824765: step 14690, loss 0.596648.
Test: 2018-08-01T23:48:10.360370: step 14690, loss 0.548119.
Train: 2018-08-01T23:48:10.531875: step 14691, loss 0.630791.
Train: 2018-08-01T23:48:10.698431: step 14692, loss 0.579489.
Train: 2018-08-01T23:48:10.877975: step 14693, loss 0.596482.
Train: 2018-08-01T23:48:11.057502: step 14694, loss 0.647364.
Train: 2018-08-01T23:48:11.227055: step 14695, loss 0.579378.
Train: 2018-08-01T23:48:11.395567: step 14696, loss 0.579336.
Train: 2018-08-01T23:48:11.555140: step 14697, loss 0.545642.
Train: 2018-08-01T23:48:11.719731: step 14698, loss 0.562481.
Train: 2018-08-01T23:48:11.887252: step 14699, loss 0.411722.
Train: 2018-08-01T23:48:12.058820: step 14700, loss 0.646284.
Test: 2018-08-01T23:48:12.595360: step 14700, loss 0.54847.
Train: 2018-08-01T23:48:13.400159: step 14701, loss 0.579239.
Train: 2018-08-01T23:48:13.568684: step 14702, loss 0.562498.
Train: 2018-08-01T23:48:13.731281: step 14703, loss 0.545782.
Train: 2018-08-01T23:48:13.898800: step 14704, loss 0.529075.
Train: 2018-08-01T23:48:14.068351: step 14705, loss 0.529063.
Train: 2018-08-01T23:48:14.244876: step 14706, loss 0.512296.
Train: 2018-08-01T23:48:14.416435: step 14707, loss 0.612768.
Train: 2018-08-01T23:48:14.587990: step 14708, loss 0.528939.
Train: 2018-08-01T23:48:14.762492: step 14709, loss 0.612858.
Train: 2018-08-01T23:48:14.932038: step 14710, loss 0.495271.
Test: 2018-08-01T23:48:15.478578: step 14710, loss 0.54838.
Train: 2018-08-01T23:48:15.655131: step 14711, loss 0.528818.
Train: 2018-08-01T23:48:15.827671: step 14712, loss 0.562461.
Train: 2018-08-01T23:48:16.000183: step 14713, loss 0.596227.
Train: 2018-08-01T23:48:16.164770: step 14714, loss 0.646976.
Train: 2018-08-01T23:48:16.336284: step 14715, loss 0.562452.
Train: 2018-08-01T23:48:16.516829: step 14716, loss 0.49485.
Train: 2018-08-01T23:48:16.687346: step 14717, loss 0.410203.
Train: 2018-08-01T23:48:16.856919: step 14718, loss 0.630312.
Train: 2018-08-01T23:48:17.028435: step 14719, loss 0.613433.
Train: 2018-08-01T23:48:17.204963: step 14720, loss 0.579449.
Test: 2018-08-01T23:48:17.737538: step 14720, loss 0.548176.
Train: 2018-08-01T23:48:17.913069: step 14721, loss 0.579458.
Train: 2018-08-01T23:48:18.083644: step 14722, loss 0.494306.
Train: 2018-08-01T23:48:18.252189: step 14723, loss 0.698859.
Train: 2018-08-01T23:48:18.419753: step 14724, loss 0.664677.
Train: 2018-08-01T23:48:18.589261: step 14725, loss 0.647462.
Train: 2018-08-01T23:48:18.756813: step 14726, loss 0.596348.
Train: 2018-08-01T23:48:18.924397: step 14727, loss 0.646946.
Train: 2018-08-01T23:48:19.102888: step 14728, loss 0.680279.
Train: 2018-08-01T23:48:19.274456: step 14729, loss 0.595974.
Train: 2018-08-01T23:48:19.441010: step 14730, loss 0.562523.
Test: 2018-08-01T23:48:19.978580: step 14730, loss 0.548674.
Train: 2018-08-01T23:48:20.148095: step 14731, loss 0.595709.
Train: 2018-08-01T23:48:20.313679: step 14732, loss 0.628596.
Train: 2018-08-01T23:48:20.485218: step 14733, loss 0.480543.
Train: 2018-08-01T23:48:20.654741: step 14734, loss 0.448095.
Train: 2018-08-01T23:48:20.823290: step 14735, loss 0.513619.
Train: 2018-08-01T23:48:20.989845: step 14736, loss 0.644444.
Train: 2018-08-01T23:48:21.164377: step 14737, loss 0.59536.
Train: 2018-08-01T23:48:21.330942: step 14738, loss 0.513749.
Train: 2018-08-01T23:48:21.512448: step 14739, loss 0.546388.
Train: 2018-08-01T23:48:21.685984: step 14740, loss 0.513749.
Test: 2018-08-01T23:48:22.225542: step 14740, loss 0.549008.
Train: 2018-08-01T23:48:22.393126: step 14741, loss 0.579021.
Train: 2018-08-01T23:48:22.565632: step 14742, loss 0.579027.
Train: 2018-08-01T23:48:22.745151: step 14743, loss 0.480862.
Train: 2018-08-01T23:48:22.916724: step 14744, loss 0.579044.
Train: 2018-08-01T23:48:23.086240: step 14745, loss 0.628328.
Train: 2018-08-01T23:48:23.257807: step 14746, loss 0.513319.
Train: 2018-08-01T23:48:23.441291: step 14747, loss 0.628455.
Train: 2018-08-01T23:48:23.606862: step 14748, loss 0.529669.
Train: 2018-08-01T23:48:23.778390: step 14749, loss 0.562597.
Train: 2018-08-01T23:48:23.946940: step 14750, loss 0.612105.
Test: 2018-08-01T23:48:24.478518: step 14750, loss 0.548758.
Train: 2018-08-01T23:48:24.659036: step 14751, loss 0.529561.
Train: 2018-08-01T23:48:24.838555: step 14752, loss 0.430355.
Train: 2018-08-01T23:48:25.009100: step 14753, loss 0.645437.
Train: 2018-08-01T23:48:25.184630: step 14754, loss 0.54594.
Train: 2018-08-01T23:48:25.356173: step 14755, loss 0.446114.
Train: 2018-08-01T23:48:25.526748: step 14756, loss 0.462387.
Train: 2018-08-01T23:48:25.697261: step 14757, loss 0.579246.
Train: 2018-08-01T23:48:25.873788: step 14758, loss 0.646611.
Train: 2018-08-01T23:48:26.043336: step 14759, loss 0.562455.
Train: 2018-08-01T23:48:26.223852: step 14760, loss 0.562447.
Test: 2018-08-01T23:48:26.757460: step 14760, loss 0.548256.
Train: 2018-08-01T23:48:27.048009: step 14761, loss 0.477741.
Train: 2018-08-01T23:48:27.218584: step 14762, loss 0.562433.
Train: 2018-08-01T23:48:27.390126: step 14763, loss 0.596506.
Train: 2018-08-01T23:48:27.565664: step 14764, loss 0.511206.
Train: 2018-08-01T23:48:27.738164: step 14765, loss 0.493956.
Train: 2018-08-01T23:48:27.915714: step 14766, loss 0.510905.
Train: 2018-08-01T23:48:28.087262: step 14767, loss 0.57966.
Train: 2018-08-01T23:48:28.256777: step 14768, loss 0.562429.
Train: 2018-08-01T23:48:28.435331: step 14769, loss 0.5971.
Train: 2018-08-01T23:48:28.611830: step 14770, loss 0.44088.
Test: 2018-08-01T23:48:29.138421: step 14770, loss 0.54786.
Train: 2018-08-01T23:48:29.307970: step 14771, loss 0.597296.
Train: 2018-08-01T23:48:29.476548: step 14772, loss 0.562459.
Train: 2018-08-01T23:48:29.642079: step 14773, loss 0.597475.
Train: 2018-08-01T23:48:29.810623: step 14774, loss 0.43978.
Train: 2018-08-01T23:48:29.992138: step 14775, loss 0.650376.
Train: 2018-08-01T23:48:30.165675: step 14776, loss 0.492097.
Train: 2018-08-01T23:48:30.334225: step 14777, loss 0.439062.
Train: 2018-08-01T23:48:30.510777: step 14778, loss 0.544838.
Train: 2018-08-01T23:48:30.685317: step 14779, loss 0.544806.
Train: 2018-08-01T23:48:30.855830: step 14780, loss 0.544779.
Test: 2018-08-01T23:48:31.398380: step 14780, loss 0.547657.
Train: 2018-08-01T23:48:31.566960: step 14781, loss 0.651859.
Train: 2018-08-01T23:48:31.733483: step 14782, loss 0.634106.
Train: 2018-08-01T23:48:31.904062: step 14783, loss 0.508998.
Train: 2018-08-01T23:48:32.074571: step 14784, loss 0.508972.
Train: 2018-08-01T23:48:32.251099: step 14785, loss 0.526827.
Train: 2018-08-01T23:48:32.421644: step 14786, loss 0.508866.
Train: 2018-08-01T23:48:32.591190: step 14787, loss 0.490833.
Train: 2018-08-01T23:48:32.766746: step 14788, loss 0.562692.
Train: 2018-08-01T23:48:32.948265: step 14789, loss 0.47254.
Train: 2018-08-01T23:48:33.119796: step 14790, loss 0.617001.
Test: 2018-08-01T23:48:33.647368: step 14790, loss 0.547601.
Train: 2018-08-01T23:48:33.821933: step 14791, loss 0.544656.
Train: 2018-08-01T23:48:33.992446: step 14792, loss 0.544648.
Train: 2018-08-01T23:48:34.159001: step 14793, loss 0.544642.
Train: 2018-08-01T23:48:34.328547: step 14794, loss 0.617398.
Train: 2018-08-01T23:48:34.501115: step 14795, loss 0.526438.
Train: 2018-08-01T23:48:34.671630: step 14796, loss 0.581047.
Train: 2018-08-01T23:48:34.846163: step 14797, loss 0.562842.
Train: 2018-08-01T23:48:35.016706: step 14798, loss 0.599257.
Train: 2018-08-01T23:48:35.187278: step 14799, loss 0.508244.
Train: 2018-08-01T23:48:35.354803: step 14800, loss 0.471862.
Test: 2018-08-01T23:48:35.900343: step 14800, loss 0.547593.
Train: 2018-08-01T23:48:36.726568: step 14801, loss 0.617467.
Train: 2018-08-01T23:48:36.895118: step 14802, loss 0.435392.
Train: 2018-08-01T23:48:37.061672: step 14803, loss 0.581089.
Train: 2018-08-01T23:48:37.229224: step 14804, loss 0.39867.
Train: 2018-08-01T23:48:37.394807: step 14805, loss 0.617779.
Train: 2018-08-01T23:48:37.564329: step 14806, loss 0.544614.
Train: 2018-08-01T23:48:37.729886: step 14807, loss 0.599629.
Train: 2018-08-01T23:48:37.902425: step 14808, loss 0.489564.
Train: 2018-08-01T23:48:38.073967: step 14809, loss 0.526238.
Train: 2018-08-01T23:48:38.251526: step 14810, loss 0.618179.
Test: 2018-08-01T23:48:38.784068: step 14810, loss 0.547598.
Train: 2018-08-01T23:48:38.953615: step 14811, loss 0.581403.
Train: 2018-08-01T23:48:39.127152: step 14812, loss 0.489422.
Train: 2018-08-01T23:48:39.293737: step 14813, loss 0.56301.
Train: 2018-08-01T23:48:39.468240: step 14814, loss 0.526196.
Train: 2018-08-01T23:48:39.640809: step 14815, loss 0.544605.
Train: 2018-08-01T23:48:39.812355: step 14816, loss 0.470902.
Train: 2018-08-01T23:48:39.982864: step 14817, loss 0.544603.
Train: 2018-08-01T23:48:40.154437: step 14818, loss 0.581544.
Train: 2018-08-01T23:48:40.323952: step 14819, loss 0.784865.
Train: 2018-08-01T23:48:40.496522: step 14820, loss 0.655237.
Test: 2018-08-01T23:48:41.027078: step 14820, loss 0.547595.
Train: 2018-08-01T23:48:41.194624: step 14821, loss 0.507855.
Train: 2018-08-01T23:48:41.364172: step 14822, loss 0.636252.
Train: 2018-08-01T23:48:41.532751: step 14823, loss 0.581151.
Train: 2018-08-01T23:48:41.700272: step 14824, loss 0.49003.
Train: 2018-08-01T23:48:41.871838: step 14825, loss 0.599111.
Train: 2018-08-01T23:48:42.050336: step 14826, loss 0.490342.
Train: 2018-08-01T23:48:42.221878: step 14827, loss 0.580809.
Train: 2018-08-01T23:48:42.393420: step 14828, loss 0.616818.
Train: 2018-08-01T23:48:42.571943: step 14829, loss 0.562682.
Train: 2018-08-01T23:48:42.757447: step 14830, loss 0.544712.
Test: 2018-08-01T23:48:43.293015: step 14830, loss 0.547639.
Train: 2018-08-01T23:48:43.477521: step 14831, loss 0.63424.
Train: 2018-08-01T23:48:43.652061: step 14832, loss 0.455511.
Train: 2018-08-01T23:48:43.826614: step 14833, loss 0.509121.
Train: 2018-08-01T23:48:44.003117: step 14834, loss 0.580389.
Train: 2018-08-01T23:48:44.173661: step 14835, loss 0.615942.
Train: 2018-08-01T23:48:44.344230: step 14836, loss 0.491519.
Train: 2018-08-01T23:48:44.509763: step 14837, loss 0.61579.
Train: 2018-08-01T23:48:44.676316: step 14838, loss 0.58026.
Train: 2018-08-01T23:48:44.847859: step 14839, loss 0.59791.
Train: 2018-08-01T23:48:45.015437: step 14840, loss 0.562513.
Test: 2018-08-01T23:48:45.545996: step 14840, loss 0.547745.
Train: 2018-08-01T23:48:45.723525: step 14841, loss 0.597743.
Train: 2018-08-01T23:48:45.892068: step 14842, loss 0.562487.
Train: 2018-08-01T23:48:46.057664: step 14843, loss 0.63265.
Train: 2018-08-01T23:48:46.232181: step 14844, loss 0.509983.
Train: 2018-08-01T23:48:46.403730: step 14845, loss 0.614825.
Train: 2018-08-01T23:48:46.572249: step 14846, loss 0.492791.
Train: 2018-08-01T23:48:46.746813: step 14847, loss 0.579827.
Train: 2018-08-01T23:48:46.915331: step 14848, loss 0.545072.
Train: 2018-08-01T23:48:47.083903: step 14849, loss 0.510409.
Train: 2018-08-01T23:48:47.257416: step 14850, loss 0.527762.
Test: 2018-08-01T23:48:47.795978: step 14850, loss 0.547913.
Train: 2018-08-01T23:48:47.965524: step 14851, loss 0.510429.
Train: 2018-08-01T23:48:48.132098: step 14852, loss 0.579779.
Train: 2018-08-01T23:48:48.303621: step 14853, loss 0.649203.
Train: 2018-08-01T23:48:48.479152: step 14854, loss 0.56243.
Train: 2018-08-01T23:48:48.647724: step 14855, loss 0.579754.
Train: 2018-08-01T23:48:48.823230: step 14856, loss 0.614351.
Train: 2018-08-01T23:48:49.001754: step 14857, loss 0.476024.
Train: 2018-08-01T23:48:49.176287: step 14858, loss 0.631521.
Train: 2018-08-01T23:48:49.351818: step 14859, loss 0.596927.
Train: 2018-08-01T23:48:49.522363: step 14860, loss 0.579645.
Test: 2018-08-01T23:48:50.049951: step 14860, loss 0.548019.
Train: 2018-08-01T23:48:50.220521: step 14861, loss 0.614006.
Train: 2018-08-01T23:48:50.405002: step 14862, loss 0.562418.
Train: 2018-08-01T23:48:50.580534: step 14863, loss 0.613785.
Train: 2018-08-01T23:48:50.753072: step 14864, loss 0.56242.
Train: 2018-08-01T23:48:50.920623: step 14865, loss 0.511303.
Train: 2018-08-01T23:48:51.089207: step 14866, loss 0.511373.
Train: 2018-08-01T23:48:51.261749: step 14867, loss 0.613455.
Train: 2018-08-01T23:48:51.429295: step 14868, loss 0.562428.
Train: 2018-08-01T23:48:51.596816: step 14869, loss 0.59638.
Train: 2018-08-01T23:48:51.765366: step 14870, loss 0.562433.
Test: 2018-08-01T23:48:52.307916: step 14870, loss 0.548256.
Train: 2018-08-01T23:48:52.488465: step 14871, loss 0.494702.
Train: 2018-08-01T23:48:52.659988: step 14872, loss 0.579369.
Train: 2018-08-01T23:48:52.829522: step 14873, loss 0.528578.
Train: 2018-08-01T23:48:53.005052: step 14874, loss 0.680981.
Train: 2018-08-01T23:48:53.176594: step 14875, loss 0.511703.
Train: 2018-08-01T23:48:53.347138: step 14876, loss 0.680773.
Train: 2018-08-01T23:48:53.517681: step 14877, loss 0.613058.
Train: 2018-08-01T23:48:53.684262: step 14878, loss 0.629768.
Train: 2018-08-01T23:48:53.855778: step 14879, loss 0.495377.
Train: 2018-08-01T23:48:54.023354: step 14880, loss 0.562482.
Test: 2018-08-01T23:48:54.551945: step 14880, loss 0.548492.
Train: 2018-08-01T23:48:54.725452: step 14881, loss 0.462188.
Train: 2018-08-01T23:48:54.899009: step 14882, loss 0.529051.
Train: 2018-08-01T23:48:55.075516: step 14883, loss 0.562486.
Train: 2018-08-01T23:48:55.247059: step 14884, loss 0.579225.
Train: 2018-08-01T23:48:55.412615: step 14885, loss 0.629482.
Train: 2018-08-01T23:48:55.579202: step 14886, loss 0.512255.
Train: 2018-08-01T23:48:55.753703: step 14887, loss 0.595978.
Train: 2018-08-01T23:48:55.921287: step 14888, loss 0.56248.
Train: 2018-08-01T23:48:56.091833: step 14889, loss 0.512237.
Train: 2018-08-01T23:48:56.275343: step 14890, loss 0.495429.
Test: 2018-08-01T23:48:56.818858: step 14890, loss 0.548406.
Train: 2018-08-01T23:48:56.994387: step 14891, loss 0.579259.
Train: 2018-08-01T23:48:57.168952: step 14892, loss 0.495198.
Train: 2018-08-01T23:48:57.334510: step 14893, loss 0.562451.
Train: 2018-08-01T23:48:57.502057: step 14894, loss 0.528661.
Train: 2018-08-01T23:48:57.670612: step 14895, loss 0.562435.
Train: 2018-08-01T23:48:57.840159: step 14896, loss 0.511517.
Train: 2018-08-01T23:48:58.012665: step 14897, loss 0.528387.
Train: 2018-08-01T23:48:58.182213: step 14898, loss 0.562418.
Train: 2018-08-01T23:48:58.358740: step 14899, loss 0.528186.
Train: 2018-08-01T23:48:58.524323: step 14900, loss 0.648241.
Test: 2018-08-01T23:48:59.054897: step 14900, loss 0.548021.
Train: 2018-08-01T23:48:59.987364: step 14901, loss 0.528036.
Train: 2018-08-01T23:49:00.166884: step 14902, loss 0.579636.
Train: 2018-08-01T23:49:00.334436: step 14903, loss 0.476205.
Train: 2018-08-01T23:49:00.503018: step 14904, loss 0.562421.
Train: 2018-08-01T23:49:00.673557: step 14905, loss 0.614384.
Train: 2018-08-01T23:49:00.845071: step 14906, loss 0.510409.
Train: 2018-08-01T23:49:01.014649: step 14907, loss 0.614539.
Train: 2018-08-01T23:49:01.188155: step 14908, loss 0.545051.
Train: 2018-08-01T23:49:01.356704: step 14909, loss 0.458046.
Train: 2018-08-01T23:49:01.523260: step 14910, loss 0.562445.
Test: 2018-08-01T23:49:02.052844: step 14910, loss 0.547823.
Train: 2018-08-01T23:49:02.231399: step 14911, loss 0.562452.
Train: 2018-08-01T23:49:02.400922: step 14912, loss 0.544961.
Train: 2018-08-01T23:49:02.573451: step 14913, loss 0.667635.
Train: 2018-08-01T23:49:02.746988: step 14914, loss 0.544939.
Train: 2018-08-01T23:49:02.911548: step 14915, loss 0.492342.
Train: 2018-08-01T23:49:03.078102: step 14916, loss 0.597573.
Train: 2018-08-01T23:49:03.252663: step 14917, loss 0.615146.
Train: 2018-08-01T23:49:03.421211: step 14918, loss 0.492275.
Train: 2018-08-01T23:49:03.585744: step 14919, loss 0.580034.
Train: 2018-08-01T23:49:03.751302: step 14920, loss 0.580037.
Test: 2018-08-01T23:49:04.266925: step 14920, loss 0.547773.
Train: 2018-08-01T23:49:04.441473: step 14921, loss 0.580033.
Train: 2018-08-01T23:49:04.613997: step 14922, loss 0.562473.
Train: 2018-08-01T23:49:04.782546: step 14923, loss 0.56247.
Train: 2018-08-01T23:49:04.957079: step 14924, loss 0.439748.
Train: 2018-08-01T23:49:05.126626: step 14925, loss 0.580024.
Train: 2018-08-01T23:49:05.292184: step 14926, loss 0.527351.
Train: 2018-08-01T23:49:05.459735: step 14927, loss 0.544902.
Train: 2018-08-01T23:49:05.627318: step 14928, loss 0.615289.
Train: 2018-08-01T23:49:05.796834: step 14929, loss 0.56249.
Train: 2018-08-01T23:49:05.980362: step 14930, loss 0.580095.
Test: 2018-08-01T23:49:06.518905: step 14930, loss 0.547751.
Train: 2018-08-01T23:49:06.692441: step 14931, loss 0.632892.
Train: 2018-08-01T23:49:06.863982: step 14932, loss 0.650377.
Train: 2018-08-01T23:49:07.027544: step 14933, loss 0.544931.
Train: 2018-08-01T23:49:07.203075: step 14934, loss 0.667481.
Train: 2018-08-01T23:49:07.383593: step 14935, loss 0.527552.
Train: 2018-08-01T23:49:07.552148: step 14936, loss 0.52763.
Train: 2018-08-01T23:49:07.730697: step 14937, loss 0.475572.
Train: 2018-08-01T23:49:07.902206: step 14938, loss 0.614519.
Train: 2018-08-01T23:49:08.076739: step 14939, loss 0.458373.
Train: 2018-08-01T23:49:08.241300: step 14940, loss 0.597122.
Test: 2018-08-01T23:49:08.773876: step 14940, loss 0.547903.
Train: 2018-08-01T23:49:08.943453: step 14941, loss 0.562427.
Train: 2018-08-01T23:49:09.116960: step 14942, loss 0.47573.
Train: 2018-08-01T23:49:09.319839: step 14943, loss 0.562428.
Train: 2018-08-01T23:49:09.494338: step 14944, loss 0.545061.
Train: 2018-08-01T23:49:09.662921: step 14945, loss 0.736287.
Train: 2018-08-01T23:49:09.837462: step 14946, loss 0.545071.
Train: 2018-08-01T23:49:10.004972: step 14947, loss 0.493078.
Train: 2018-08-01T23:49:10.175517: step 14948, loss 0.562425.
Train: 2018-08-01T23:49:10.339106: step 14949, loss 0.493104.
Train: 2018-08-01T23:49:10.508627: step 14950, loss 0.52774.
Test: 2018-08-01T23:49:11.051177: step 14950, loss 0.54789.
Train: 2018-08-01T23:49:11.221720: step 14951, loss 0.597153.
Train: 2018-08-01T23:49:11.391266: step 14952, loss 0.475578.
Train: 2018-08-01T23:49:11.560815: step 14953, loss 0.632026.
Train: 2018-08-01T23:49:11.734349: step 14954, loss 0.562436.
Train: 2018-08-01T23:49:11.905892: step 14955, loss 0.510204.
Train: 2018-08-01T23:49:12.074441: step 14956, loss 0.667008.
Train: 2018-08-01T23:49:12.241994: step 14957, loss 0.632109.
Train: 2018-08-01T23:49:12.409544: step 14958, loss 0.649389.
Train: 2018-08-01T23:49:12.582083: step 14959, loss 0.597119.
Train: 2018-08-01T23:49:12.758612: step 14960, loss 0.441342.
Test: 2018-08-01T23:49:13.285233: step 14960, loss 0.547948.
Train: 2018-08-01T23:49:13.455749: step 14961, loss 0.527857.
Train: 2018-08-01T23:49:13.628286: step 14962, loss 0.562418.
Train: 2018-08-01T23:49:13.802821: step 14963, loss 0.648755.
Train: 2018-08-01T23:49:13.976358: step 14964, loss 0.579656.
Train: 2018-08-01T23:49:14.147897: step 14965, loss 0.527988.
Train: 2018-08-01T23:49:14.320463: step 14966, loss 0.545218.
Train: 2018-08-01T23:49:14.490981: step 14967, loss 0.59678.
Train: 2018-08-01T23:49:14.672526: step 14968, loss 0.510918.
Train: 2018-08-01T23:49:14.855007: step 14969, loss 0.631053.
Train: 2018-08-01T23:49:15.036554: step 14970, loss 0.528134.
Test: 2018-08-01T23:49:15.574086: step 14970, loss 0.548069.
Train: 2018-08-01T23:49:15.748649: step 14971, loss 0.596671.
Train: 2018-08-01T23:49:15.921157: step 14972, loss 0.682195.
Train: 2018-08-01T23:49:16.089706: step 14973, loss 0.545348.
Train: 2018-08-01T23:49:16.259253: step 14974, loss 0.579451.
Train: 2018-08-01T23:49:16.429797: step 14975, loss 0.664403.
Train: 2018-08-01T23:49:16.601349: step 14976, loss 0.681016.
Train: 2018-08-01T23:49:16.775872: step 14977, loss 0.46127.
Train: 2018-08-01T23:49:16.950406: step 14978, loss 0.495186.
Train: 2018-08-01T23:49:17.117959: step 14979, loss 0.512079.
Train: 2018-08-01T23:49:17.297479: step 14980, loss 0.612827.
Test: 2018-08-01T23:49:17.875180: step 14980, loss 0.548427.
Train: 2018-08-01T23:49:18.045723: step 14981, loss 0.56247.
Train: 2018-08-01T23:49:18.215272: step 14982, loss 0.663004.
Train: 2018-08-01T23:49:18.387834: step 14983, loss 0.612641.
Train: 2018-08-01T23:49:18.559352: step 14984, loss 0.612526.
Train: 2018-08-01T23:49:18.726904: step 14985, loss 0.529272.
Train: 2018-08-01T23:49:18.902466: step 14986, loss 0.512767.
Train: 2018-08-01T23:49:19.069988: step 14987, loss 0.61226.
Train: 2018-08-01T23:49:19.243522: step 14988, loss 0.512919.
Train: 2018-08-01T23:49:19.413070: step 14989, loss 0.496415.
Train: 2018-08-01T23:49:19.591593: step 14990, loss 0.562556.
Test: 2018-08-01T23:49:20.131148: step 14990, loss 0.548686.
Train: 2018-08-01T23:49:20.301707: step 14991, loss 0.562551.
Train: 2018-08-01T23:49:20.478226: step 14992, loss 0.711661.
Train: 2018-08-01T23:49:20.649766: step 14993, loss 0.612189.
Train: 2018-08-01T23:49:20.821309: step 14994, loss 0.562572.
Train: 2018-08-01T23:49:20.994871: step 14995, loss 0.64501.
Train: 2018-08-01T23:49:21.163414: step 14996, loss 0.529724.
Train: 2018-08-01T23:49:21.330977: step 14997, loss 0.529796.
Train: 2018-08-01T23:49:21.503507: step 14998, loss 0.546233.
Train: 2018-08-01T23:49:21.673059: step 14999, loss 0.529851.
Train: 2018-08-01T23:49:21.847561: step 15000, loss 0.562634.
Test: 2018-08-01T23:49:22.386121: step 15000, loss 0.548894.
Train: 2018-08-01T23:49:23.203119: step 15001, loss 0.513418.
Train: 2018-08-01T23:49:23.378684: step 15002, loss 0.546194.
Train: 2018-08-01T23:49:23.548222: step 15003, loss 0.579054.
Train: 2018-08-01T23:49:23.716777: step 15004, loss 0.612004.
Train: 2018-08-01T23:49:23.896267: step 15005, loss 0.595546.
Train: 2018-08-01T23:49:24.068819: step 15006, loss 0.595548.
Train: 2018-08-01T23:49:24.238352: step 15007, loss 0.513164.
Train: 2018-08-01T23:49:24.415878: step 15008, loss 0.595557.
Train: 2018-08-01T23:49:24.585425: step 15009, loss 0.579072.
Train: 2018-08-01T23:49:24.752010: step 15010, loss 0.628543.
Test: 2018-08-01T23:49:25.295539: step 15010, loss 0.548793.
Train: 2018-08-01T23:49:25.466099: step 15011, loss 0.644972.
Train: 2018-08-01T23:49:25.638608: step 15012, loss 0.595498.
Train: 2018-08-01T23:49:25.804185: step 15013, loss 0.611863.
Train: 2018-08-01T23:49:25.978730: step 15014, loss 0.497156.
Train: 2018-08-01T23:49:26.155256: step 15015, loss 0.611727.
Train: 2018-08-01T23:49:26.326825: step 15016, loss 0.562675.
Train: 2018-08-01T23:49:26.511276: step 15017, loss 0.497443.
Train: 2018-08-01T23:49:26.685840: step 15018, loss 0.611624.
Train: 2018-08-01T23:49:26.863335: step 15019, loss 0.693122.
Train: 2018-08-01T23:49:27.046848: step 15020, loss 0.59525.
Test: 2018-08-01T23:49:27.589395: step 15020, loss 0.549158.
Train: 2018-08-01T23:49:27.761964: step 15021, loss 0.578973.
Train: 2018-08-01T23:49:27.932477: step 15022, loss 0.562773.
Train: 2018-08-01T23:49:28.098065: step 15023, loss 0.562796.
Train: 2018-08-01T23:49:28.274562: step 15024, loss 0.562814.
Train: 2018-08-01T23:49:28.446104: step 15025, loss 0.659543.
Train: 2018-08-01T23:49:28.615650: step 15026, loss 0.627186.
Train: 2018-08-01T23:49:28.789187: step 15027, loss 0.578934.
Train: 2018-08-01T23:49:28.963721: step 15028, loss 0.562937.
Train: 2018-08-01T23:49:29.137271: step 15029, loss 0.515095.
Train: 2018-08-01T23:49:29.304808: step 15030, loss 0.499211.
Test: 2018-08-01T23:49:29.824450: step 15030, loss 0.549621.
Train: 2018-08-01T23:49:29.993965: step 15031, loss 0.610826.
Train: 2018-08-01T23:49:30.170527: step 15032, loss 0.41945.
Train: 2018-08-01T23:49:30.342077: step 15033, loss 0.594915.
Train: 2018-08-01T23:49:30.515573: step 15034, loss 0.626982.
Train: 2018-08-01T23:49:30.679145: step 15035, loss 0.482746.
Train: 2018-08-01T23:49:30.847683: step 15036, loss 0.691417.
Train: 2018-08-01T23:49:31.025234: step 15037, loss 0.546794.
Train: 2018-08-01T23:49:31.203763: step 15038, loss 0.530689.
Train: 2018-08-01T23:49:31.376301: step 15039, loss 0.611156.
Train: 2018-08-01T23:49:31.543822: step 15040, loss 0.56283.
Test: 2018-08-01T23:49:32.097342: step 15040, loss 0.549313.
Train: 2018-08-01T23:49:32.265934: step 15041, loss 0.466044.
Train: 2018-08-01T23:49:32.438432: step 15042, loss 0.595126.
Train: 2018-08-01T23:49:32.615010: step 15043, loss 0.578965.
Train: 2018-08-01T23:49:32.788515: step 15044, loss 0.578972.
Train: 2018-08-01T23:49:32.955049: step 15045, loss 0.465241.
Train: 2018-08-01T23:49:33.126616: step 15046, loss 0.530105.
Train: 2018-08-01T23:49:33.293165: step 15047, loss 0.562662.
Train: 2018-08-01T23:49:33.465719: step 15048, loss 0.480628.
Train: 2018-08-01T23:49:33.636228: step 15049, loss 0.579062.
Train: 2018-08-01T23:49:33.803782: step 15050, loss 0.509669.
Test: 2018-08-01T23:49:34.334394: step 15050, loss 0.548631.
Train: 2018-08-01T23:49:34.509894: step 15051, loss 0.496138.
Train: 2018-08-01T23:49:34.680438: step 15052, loss 0.562498.
Train: 2018-08-01T23:49:34.845053: step 15053, loss 0.629479.
Train: 2018-08-01T23:49:35.011553: step 15054, loss 0.461641.
Train: 2018-08-01T23:49:35.181133: step 15055, loss 0.545566.
Train: 2018-08-01T23:49:35.347654: step 15056, loss 0.494652.
Train: 2018-08-01T23:49:35.515232: step 15057, loss 0.494323.
Train: 2018-08-01T23:49:35.686748: step 15058, loss 0.528187.
Train: 2018-08-01T23:49:35.851327: step 15059, loss 0.56241.
Train: 2018-08-01T23:49:36.016893: step 15060, loss 0.648803.
Test: 2018-08-01T23:49:36.551449: step 15060, loss 0.54791.
Train: 2018-08-01T23:49:36.724997: step 15061, loss 0.614404.
Train: 2018-08-01T23:49:36.893553: step 15062, loss 0.510344.
Train: 2018-08-01T23:49:37.065094: step 15063, loss 0.52763.
Train: 2018-08-01T23:49:37.246609: step 15064, loss 0.527551.
Train: 2018-08-01T23:49:37.413133: step 15065, loss 0.492485.
Train: 2018-08-01T23:49:37.592676: step 15066, loss 0.597567.
Train: 2018-08-01T23:49:37.762201: step 15067, loss 0.580073.
Train: 2018-08-01T23:49:37.930749: step 15068, loss 0.562492.
Train: 2018-08-01T23:49:38.107277: step 15069, loss 0.509546.
Train: 2018-08-01T23:49:38.281837: step 15070, loss 0.650958.
Test: 2018-08-01T23:49:38.825358: step 15070, loss 0.5477.
Train: 2018-08-01T23:49:38.999922: step 15071, loss 0.527124.
Train: 2018-08-01T23:49:39.172454: step 15072, loss 0.562527.
Train: 2018-08-01T23:49:39.344004: step 15073, loss 0.491626.
Train: 2018-08-01T23:49:39.510526: step 15074, loss 0.562544.
Train: 2018-08-01T23:49:39.680073: step 15075, loss 0.598101.
Train: 2018-08-01T23:49:39.854605: step 15076, loss 0.580341.
Train: 2018-08-01T23:49:40.025175: step 15077, loss 0.598129.
Train: 2018-08-01T23:49:40.194696: step 15078, loss 0.527002.
Train: 2018-08-01T23:49:40.362278: step 15079, loss 0.58033.
Train: 2018-08-01T23:49:40.531796: step 15080, loss 0.598089.
Test: 2018-08-01T23:49:41.064372: step 15080, loss 0.547679.
Train: 2018-08-01T23:49:41.234941: step 15081, loss 0.509287.
Train: 2018-08-01T23:49:41.407484: step 15082, loss 0.544794.
Train: 2018-08-01T23:49:41.580026: step 15083, loss 0.580287.
Train: 2018-08-01T23:49:41.746579: step 15084, loss 0.456106.
Train: 2018-08-01T23:49:41.914131: step 15085, loss 0.527035.
Train: 2018-08-01T23:49:42.085670: step 15086, loss 0.598106.
Train: 2018-08-01T23:49:42.259177: step 15087, loss 0.633695.
Train: 2018-08-01T23:49:42.432733: step 15088, loss 0.509233.
Train: 2018-08-01T23:49:42.603258: step 15089, loss 0.580328.
Train: 2018-08-01T23:49:42.772822: step 15090, loss 0.544782.
Test: 2018-08-01T23:49:43.315354: step 15090, loss 0.547673.
Train: 2018-08-01T23:49:43.483904: step 15091, loss 0.56255.
Train: 2018-08-01T23:49:43.654474: step 15092, loss 0.598073.
Train: 2018-08-01T23:49:43.823994: step 15093, loss 0.58029.
Train: 2018-08-01T23:49:43.996567: step 15094, loss 0.633453.
Train: 2018-08-01T23:49:44.163115: step 15095, loss 0.562519.
Train: 2018-08-01T23:49:44.328646: step 15096, loss 0.633154.
Train: 2018-08-01T23:49:44.498226: step 15097, loss 0.580103.
Train: 2018-08-01T23:49:44.676714: step 15098, loss 0.580038.
Train: 2018-08-01T23:49:44.848281: step 15099, loss 0.562458.
Train: 2018-08-01T23:49:45.024812: step 15100, loss 0.597389.
Test: 2018-08-01T23:49:45.567359: step 15100, loss 0.547846.
Train: 2018-08-01T23:49:46.377780: step 15101, loss 0.527592.
Train: 2018-08-01T23:49:46.547353: step 15102, loss 0.597196.
Train: 2018-08-01T23:49:46.712910: step 15103, loss 0.597103.
Train: 2018-08-01T23:49:46.883428: step 15104, loss 0.562415.
Train: 2018-08-01T23:49:47.051011: step 15105, loss 0.510659.
Train: 2018-08-01T23:49:47.227509: step 15106, loss 0.631306.
Train: 2018-08-01T23:49:47.399079: step 15107, loss 0.545225.
Train: 2018-08-01T23:49:47.569620: step 15108, loss 0.528106.
Train: 2018-08-01T23:49:47.740138: step 15109, loss 0.493889.
Train: 2018-08-01T23:49:47.922650: step 15110, loss 0.459644.
Test: 2018-08-01T23:49:48.450241: step 15110, loss 0.548047.
Train: 2018-08-01T23:49:48.617819: step 15111, loss 0.493808.
Train: 2018-08-01T23:49:48.789375: step 15112, loss 0.579595.
Train: 2018-08-01T23:49:48.957883: step 15113, loss 0.510762.
Train: 2018-08-01T23:49:49.135434: step 15114, loss 0.579665.
Train: 2018-08-01T23:49:49.307948: step 15115, loss 0.73525.
Train: 2018-08-01T23:49:49.480487: step 15116, loss 0.545143.
Train: 2018-08-01T23:49:49.650034: step 15117, loss 0.579671.
Train: 2018-08-01T23:49:49.828556: step 15118, loss 0.476176.
Train: 2018-08-01T23:49:49.997136: step 15119, loss 0.579667.
Train: 2018-08-01T23:49:50.174657: step 15120, loss 0.57967.
Test: 2018-08-01T23:49:50.714187: step 15120, loss 0.54796.
Train: 2018-08-01T23:49:50.881759: step 15121, loss 0.596926.
Train: 2018-08-01T23:49:51.059266: step 15122, loss 0.648649.
Train: 2018-08-01T23:49:51.230807: step 15123, loss 0.476318.
Train: 2018-08-01T23:49:51.401351: step 15124, loss 0.562408.
Train: 2018-08-01T23:49:51.572894: step 15125, loss 0.493584.
Train: 2018-08-01T23:49:51.741442: step 15126, loss 0.596843.
Train: 2018-08-01T23:49:51.910016: step 15127, loss 0.803468.
Train: 2018-08-01T23:49:52.081559: step 15128, loss 0.57957.
Train: 2018-08-01T23:49:52.248087: step 15129, loss 0.647949.
Train: 2018-08-01T23:49:52.420627: step 15130, loss 0.562412.
Test: 2018-08-01T23:49:52.948215: step 15130, loss 0.548203.
Train: 2018-08-01T23:49:53.119786: step 15131, loss 0.613351.
Train: 2018-08-01T23:49:53.288308: step 15132, loss 0.528611.
Train: 2018-08-01T23:49:53.459848: step 15133, loss 0.596158.
Train: 2018-08-01T23:49:53.631408: step 15134, loss 0.444824.
Train: 2018-08-01T23:49:53.806946: step 15135, loss 0.663174.
Train: 2018-08-01T23:49:53.976468: step 15136, loss 0.612709.
Train: 2018-08-01T23:49:54.143022: step 15137, loss 0.478995.
Train: 2018-08-01T23:49:54.308579: step 15138, loss 0.529139.
Train: 2018-08-01T23:49:54.481118: step 15139, loss 0.595837.
Train: 2018-08-01T23:49:54.648696: step 15140, loss 0.562504.
Test: 2018-08-01T23:49:55.188228: step 15140, loss 0.548572.
Train: 2018-08-01T23:49:55.368745: step 15141, loss 0.529222.
Train: 2018-08-01T23:49:55.538317: step 15142, loss 0.495938.
Train: 2018-08-01T23:49:55.710831: step 15143, loss 0.529182.
Train: 2018-08-01T23:49:55.888357: step 15144, loss 0.612549.
Train: 2018-08-01T23:49:56.059898: step 15145, loss 0.562488.
Train: 2018-08-01T23:49:56.234432: step 15146, loss 0.562484.
Train: 2018-08-01T23:49:56.407967: step 15147, loss 0.512313.
Train: 2018-08-01T23:49:56.577539: step 15148, loss 0.528977.
Train: 2018-08-01T23:49:56.745066: step 15149, loss 0.545683.
Train: 2018-08-01T23:49:56.913616: step 15150, loss 0.713769.
Test: 2018-08-01T23:49:57.446192: step 15150, loss 0.548379.
Train: 2018-08-01T23:49:57.620745: step 15151, loss 0.461606.
Train: 2018-08-01T23:49:57.795259: step 15152, loss 0.461476.
Train: 2018-08-01T23:49:57.968795: step 15153, loss 0.528692.
Train: 2018-08-01T23:49:58.133356: step 15154, loss 0.528584.
Train: 2018-08-01T23:49:58.307890: step 15155, loss 0.52847.
Train: 2018-08-01T23:49:58.479430: step 15156, loss 0.596474.
Train: 2018-08-01T23:49:58.649973: step 15157, loss 0.596556.
Train: 2018-08-01T23:49:58.833483: step 15158, loss 0.61372.
Train: 2018-08-01T23:49:58.997071: step 15159, loss 0.511047.
Train: 2018-08-01T23:49:59.167591: step 15160, loss 0.562406.
Test: 2018-08-01T23:49:59.687215: step 15160, loss 0.548029.
Train: 2018-08-01T23:49:59.871707: step 15161, loss 0.613917.
Train: 2018-08-01T23:50:00.041255: step 15162, loss 0.579586.
Train: 2018-08-01T23:50:00.209804: step 15163, loss 0.562407.
Train: 2018-08-01T23:50:00.383340: step 15164, loss 0.493659.
Train: 2018-08-01T23:50:00.555879: step 15165, loss 0.665645.
Train: 2018-08-01T23:50:00.724452: step 15166, loss 0.510809.
Train: 2018-08-01T23:50:00.896000: step 15167, loss 0.510792.
Train: 2018-08-01T23:50:01.066514: step 15168, loss 0.527964.
Train: 2018-08-01T23:50:01.231074: step 15169, loss 0.562409.
Train: 2018-08-01T23:50:01.400621: step 15170, loss 0.579677.
Test: 2018-08-01T23:50:01.942174: step 15170, loss 0.547942.
Train: 2018-08-01T23:50:02.126680: step 15171, loss 0.493289.
Train: 2018-08-01T23:50:02.314237: step 15172, loss 0.631653.
Train: 2018-08-01T23:50:02.482797: step 15173, loss 0.614372.
Train: 2018-08-01T23:50:02.655317: step 15174, loss 0.545102.
Train: 2018-08-01T23:50:02.828860: step 15175, loss 0.44122.
Train: 2018-08-01T23:50:02.997407: step 15176, loss 0.57976.
Train: 2018-08-01T23:50:03.165957: step 15177, loss 0.562422.
Train: 2018-08-01T23:50:03.339462: step 15178, loss 0.597183.
Train: 2018-08-01T23:50:03.513996: step 15179, loss 0.597198.
Train: 2018-08-01T23:50:03.684547: step 15180, loss 0.545042.
Test: 2018-08-01T23:50:04.224097: step 15180, loss 0.547868.
Train: 2018-08-01T23:50:04.393644: step 15181, loss 0.510273.
Train: 2018-08-01T23:50:04.562227: step 15182, loss 0.492841.
Train: 2018-08-01T23:50:04.729745: step 15183, loss 0.64955.
Train: 2018-08-01T23:50:04.901309: step 15184, loss 0.57986.
Train: 2018-08-01T23:50:05.070862: step 15185, loss 0.632129.
Train: 2018-08-01T23:50:05.245367: step 15186, loss 0.510215.
Train: 2018-08-01T23:50:05.414913: step 15187, loss 0.614625.
Train: 2018-08-01T23:50:05.581501: step 15188, loss 0.631947.
Train: 2018-08-01T23:50:05.751026: step 15189, loss 0.597114.
Train: 2018-08-01T23:50:05.928541: step 15190, loss 0.458565.
Test: 2018-08-01T23:50:06.469110: step 15190, loss 0.547929.
Train: 2018-08-01T23:50:06.640637: step 15191, loss 0.475927.
Train: 2018-08-01T23:50:06.814173: step 15192, loss 0.579722.
Train: 2018-08-01T23:50:06.987709: step 15193, loss 0.648974.
Train: 2018-08-01T23:50:07.160274: step 15194, loss 0.545118.
Train: 2018-08-01T23:50:07.327832: step 15195, loss 0.545128.
Train: 2018-08-01T23:50:07.497372: step 15196, loss 0.614242.
Train: 2018-08-01T23:50:07.666894: step 15197, loss 0.493376.
Train: 2018-08-01T23:50:07.842456: step 15198, loss 0.614182.
Train: 2018-08-01T23:50:08.012992: step 15199, loss 0.596897.
Train: 2018-08-01T23:50:08.180520: step 15200, loss 0.579631.
Test: 2018-08-01T23:50:08.722073: step 15200, loss 0.548002.
Train: 2018-08-01T23:50:09.539049: step 15201, loss 0.596809.
Train: 2018-08-01T23:50:09.714554: step 15202, loss 0.545233.
Train: 2018-08-01T23:50:09.886122: step 15203, loss 0.528103.
Train: 2018-08-01T23:50:10.062625: step 15204, loss 0.562405.
Train: 2018-08-01T23:50:10.241148: step 15205, loss 0.511017.
Train: 2018-08-01T23:50:10.412715: step 15206, loss 0.562406.
Train: 2018-08-01T23:50:10.580241: step 15207, loss 0.493863.
Train: 2018-08-01T23:50:10.749837: step 15208, loss 0.562405.
Train: 2018-08-01T23:50:10.915369: step 15209, loss 0.665435.
Train: 2018-08-01T23:50:11.083894: step 15210, loss 0.562405.
Test: 2018-08-01T23:50:11.620492: step 15210, loss 0.548041.
Train: 2018-08-01T23:50:11.797019: step 15211, loss 0.493788.
Train: 2018-08-01T23:50:11.970555: step 15212, loss 0.716874.
Train: 2018-08-01T23:50:12.142098: step 15213, loss 0.613809.
Train: 2018-08-01T23:50:12.317597: step 15214, loss 0.5966.
Train: 2018-08-01T23:50:12.495147: step 15215, loss 0.630627.
Train: 2018-08-01T23:50:12.660704: step 15216, loss 0.596416.
Train: 2018-08-01T23:50:12.829228: step 15217, loss 0.562422.
Train: 2018-08-01T23:50:13.000769: step 15218, loss 0.477959.
Train: 2018-08-01T23:50:13.175322: step 15219, loss 0.579306.
Train: 2018-08-01T23:50:13.359843: step 15220, loss 0.629838.
Test: 2018-08-01T23:50:13.885406: step 15220, loss 0.548376.
Train: 2018-08-01T23:50:14.057997: step 15221, loss 0.680118.
Train: 2018-08-01T23:50:14.233485: step 15222, loss 0.495478.
Train: 2018-08-01T23:50:14.406044: step 15223, loss 0.51235.
Train: 2018-08-01T23:50:14.572568: step 15224, loss 0.545798.
Train: 2018-08-01T23:50:14.754083: step 15225, loss 0.545815.
Train: 2018-08-01T23:50:14.920638: step 15226, loss 0.495806.
Train: 2018-08-01T23:50:15.092210: step 15227, loss 0.529121.
Train: 2018-08-01T23:50:15.259731: step 15228, loss 0.562484.
Train: 2018-08-01T23:50:15.434298: step 15229, loss 0.529023.
Train: 2018-08-01T23:50:15.625753: step 15230, loss 0.562468.
Test: 2018-08-01T23:50:16.164313: step 15230, loss 0.548408.
Train: 2018-08-01T23:50:16.329901: step 15231, loss 0.512117.
Train: 2018-08-01T23:50:16.496451: step 15232, loss 0.56245.
Train: 2018-08-01T23:50:16.658018: step 15233, loss 0.596147.
Train: 2018-08-01T23:50:16.833551: step 15234, loss 0.528683.
Train: 2018-08-01T23:50:17.002099: step 15235, loss 0.579337.
Train: 2018-08-01T23:50:17.173614: step 15236, loss 0.579357.
Train: 2018-08-01T23:50:17.347177: step 15237, loss 0.528523.
Train: 2018-08-01T23:50:17.522714: step 15238, loss 0.562419.
Train: 2018-08-01T23:50:17.696243: step 15239, loss 0.545417.
Train: 2018-08-01T23:50:17.861776: step 15240, loss 0.511339.
Test: 2018-08-01T23:50:18.388368: step 15240, loss 0.548123.
Train: 2018-08-01T23:50:18.566915: step 15241, loss 0.545349.
Train: 2018-08-01T23:50:18.733472: step 15242, loss 0.528213.
Train: 2018-08-01T23:50:18.896011: step 15243, loss 0.596683.
Train: 2018-08-01T23:50:19.063593: step 15244, loss 0.648243.
Train: 2018-08-01T23:50:19.231114: step 15245, loss 0.562406.
Train: 2018-08-01T23:50:19.397701: step 15246, loss 0.42498.
Train: 2018-08-01T23:50:19.570234: step 15247, loss 0.614053.
Train: 2018-08-01T23:50:19.745764: step 15248, loss 0.545172.
Train: 2018-08-01T23:50:19.910325: step 15249, loss 0.476114.
Train: 2018-08-01T23:50:20.075857: step 15250, loss 0.614313.
Test: 2018-08-01T23:50:20.617429: step 15250, loss 0.54791.
Train: 2018-08-01T23:50:20.784988: step 15251, loss 0.527769.
Train: 2018-08-01T23:50:20.953542: step 15252, loss 0.597126.
Train: 2018-08-01T23:50:21.126081: step 15253, loss 0.510312.
Train: 2018-08-01T23:50:21.291633: step 15254, loss 0.510235.
Train: 2018-08-01T23:50:21.466166: step 15255, loss 0.632169.
Train: 2018-08-01T23:50:21.629729: step 15256, loss 0.510092.
Train: 2018-08-01T23:50:21.795291: step 15257, loss 0.562444.
Train: 2018-08-01T23:50:21.966801: step 15258, loss 0.684917.
Train: 2018-08-01T23:50:22.139340: step 15259, loss 0.579932.
Train: 2018-08-01T23:50:22.304897: step 15260, loss 0.579914.
Test: 2018-08-01T23:50:22.840466: step 15260, loss 0.547824.
Train: 2018-08-01T23:50:23.010039: step 15261, loss 0.44027.
Train: 2018-08-01T23:50:23.172579: step 15262, loss 0.562441.
Train: 2018-08-01T23:50:23.340161: step 15263, loss 0.614864.
Train: 2018-08-01T23:50:23.521644: step 15264, loss 0.527505.
Train: 2018-08-01T23:50:23.686230: step 15265, loss 0.63233.
Train: 2018-08-01T23:50:23.847799: step 15266, loss 0.61481.
Train: 2018-08-01T23:50:24.012334: step 15267, loss 0.632155.
Train: 2018-08-01T23:50:24.177892: step 15268, loss 0.475479.
Train: 2018-08-01T23:50:24.347447: step 15269, loss 0.527678.
Train: 2018-08-01T23:50:24.518980: step 15270, loss 0.527693.
Test: 2018-08-01T23:50:25.048563: step 15270, loss 0.547881.
Train: 2018-08-01T23:50:25.214152: step 15271, loss 0.631879.
Train: 2018-08-01T23:50:25.379703: step 15272, loss 0.63181.
Train: 2018-08-01T23:50:25.548254: step 15273, loss 0.562414.
Train: 2018-08-01T23:50:25.713810: step 15274, loss 0.527838.
Train: 2018-08-01T23:50:25.877378: step 15275, loss 0.493338.
Train: 2018-08-01T23:50:26.039913: step 15276, loss 0.596944.
Train: 2018-08-01T23:50:26.215474: step 15277, loss 0.596924.
Train: 2018-08-01T23:50:26.380003: step 15278, loss 0.45896.
Train: 2018-08-01T23:50:26.548554: step 15279, loss 0.527907.
Train: 2018-08-01T23:50:26.714116: step 15280, loss 0.614209.
Test: 2018-08-01T23:50:27.239705: step 15280, loss 0.547949.
Train: 2018-08-01T23:50:27.429231: step 15281, loss 0.545141.
Train: 2018-08-01T23:50:27.605728: step 15282, loss 0.527862.
Train: 2018-08-01T23:50:27.784251: step 15283, loss 0.510551.
Train: 2018-08-01T23:50:27.948842: step 15284, loss 0.597034.
Train: 2018-08-01T23:50:28.121375: step 15285, loss 0.631705.
Train: 2018-08-01T23:50:28.297908: step 15286, loss 0.545097.
Train: 2018-08-01T23:50:28.461465: step 15287, loss 0.475839.
Train: 2018-08-01T23:50:28.627995: step 15288, loss 0.597083.
Train: 2018-08-01T23:50:28.792580: step 15289, loss 0.562417.
Train: 2018-08-01T23:50:28.955150: step 15290, loss 0.579766.
Test: 2018-08-01T23:50:29.499664: step 15290, loss 0.54789.
Train: 2018-08-01T23:50:29.666246: step 15291, loss 0.597117.
Train: 2018-08-01T23:50:29.834793: step 15292, loss 0.47571.
Train: 2018-08-01T23:50:29.999360: step 15293, loss 0.545065.
Train: 2018-08-01T23:50:30.163914: step 15294, loss 0.649269.
Train: 2018-08-01T23:50:30.336459: step 15295, loss 0.527696.
Train: 2018-08-01T23:50:30.508998: step 15296, loss 0.492969.
Train: 2018-08-01T23:50:30.675521: step 15297, loss 0.579802.
Train: 2018-08-01T23:50:30.845067: step 15298, loss 0.614592.
Train: 2018-08-01T23:50:31.009651: step 15299, loss 0.579809.
Train: 2018-08-01T23:50:31.183188: step 15300, loss 0.510292.
Test: 2018-08-01T23:50:31.724717: step 15300, loss 0.547869.
Train: 2018-08-01T23:50:32.556147: step 15301, loss 0.597185.
Train: 2018-08-01T23:50:32.721710: step 15302, loss 0.614549.
Train: 2018-08-01T23:50:32.890228: step 15303, loss 0.510346.
Train: 2018-08-01T23:50:33.061770: step 15304, loss 0.510358.
Train: 2018-08-01T23:50:33.226330: step 15305, loss 0.545059.
Train: 2018-08-01T23:50:33.393908: step 15306, loss 0.510306.
Train: 2018-08-01T23:50:33.561435: step 15307, loss 0.631997.
Train: 2018-08-01T23:50:33.728015: step 15308, loss 0.597215.
Train: 2018-08-01T23:50:33.891584: step 15309, loss 0.527649.
Train: 2018-08-01T23:50:34.059105: step 15310, loss 0.631977.
Test: 2018-08-01T23:50:34.590684: step 15310, loss 0.547875.
Train: 2018-08-01T23:50:34.768209: step 15311, loss 0.510306.
Train: 2018-08-01T23:50:34.938778: step 15312, loss 0.597158.
Train: 2018-08-01T23:50:35.101319: step 15313, loss 0.597133.
Train: 2018-08-01T23:50:35.285855: step 15314, loss 0.579754.
Train: 2018-08-01T23:50:35.462352: step 15315, loss 0.61436.
Train: 2018-08-01T23:50:35.629905: step 15316, loss 0.648824.
Train: 2018-08-01T23:50:35.798455: step 15317, loss 0.527941.
Train: 2018-08-01T23:50:35.966007: step 15318, loss 0.717161.
Train: 2018-08-01T23:50:36.136551: step 15319, loss 0.545279.
Train: 2018-08-01T23:50:36.305131: step 15320, loss 0.596537.
Test: 2018-08-01T23:50:36.846653: step 15320, loss 0.548174.
Train: 2018-08-01T23:50:37.025220: step 15321, loss 0.647432.
Train: 2018-08-01T23:50:37.189735: step 15322, loss 0.579352.
Train: 2018-08-01T23:50:37.357288: step 15323, loss 0.495007.
Train: 2018-08-01T23:50:37.534843: step 15324, loss 0.562449.
Train: 2018-08-01T23:50:37.700370: step 15325, loss 0.562461.
Train: 2018-08-01T23:50:37.867923: step 15326, loss 0.545738.
Train: 2018-08-01T23:50:38.041459: step 15327, loss 0.529065.
Train: 2018-08-01T23:50:38.207044: step 15328, loss 0.662648.
Train: 2018-08-01T23:50:38.375566: step 15329, loss 0.529184.
Train: 2018-08-01T23:50:38.541148: step 15330, loss 0.579143.
Test: 2018-08-01T23:50:39.078734: step 15330, loss 0.548607.
Train: 2018-08-01T23:50:39.245271: step 15331, loss 0.529293.
Train: 2018-08-01T23:50:39.412793: step 15332, loss 0.579124.
Train: 2018-08-01T23:50:39.577352: step 15333, loss 0.579117.
Train: 2018-08-01T23:50:39.745903: step 15334, loss 0.52938.
Train: 2018-08-01T23:50:39.914451: step 15335, loss 0.562534.
Train: 2018-08-01T23:50:40.081032: step 15336, loss 0.545958.
Train: 2018-08-01T23:50:40.246563: step 15337, loss 0.512785.
Train: 2018-08-01T23:50:40.414149: step 15338, loss 0.512717.
Train: 2018-08-01T23:50:40.583709: step 15339, loss 0.545875.
Train: 2018-08-01T23:50:40.766174: step 15340, loss 0.479153.
Test: 2018-08-01T23:50:41.300756: step 15340, loss 0.548474.
Train: 2018-08-01T23:50:41.469295: step 15341, loss 0.529033.
Train: 2018-08-01T23:50:41.641859: step 15342, loss 0.495341.
Train: 2018-08-01T23:50:41.809410: step 15343, loss 0.495044.
Train: 2018-08-01T23:50:41.993892: step 15344, loss 0.545494.
Train: 2018-08-01T23:50:42.163439: step 15345, loss 0.5284.
Train: 2018-08-01T23:50:42.339967: step 15346, loss 0.476985.
Train: 2018-08-01T23:50:42.511508: step 15347, loss 0.631093.
Train: 2018-08-01T23:50:42.678064: step 15348, loss 0.476115.
Train: 2018-08-01T23:50:42.841651: step 15349, loss 0.63199.
Train: 2018-08-01T23:50:43.007184: step 15350, loss 0.545083.
Test: 2018-08-01T23:50:43.549733: step 15350, loss 0.547819.
Train: 2018-08-01T23:50:43.712305: step 15351, loss 0.673886.
Train: 2018-08-01T23:50:43.887830: step 15352, loss 0.527532.
Train: 2018-08-01T23:50:44.054415: step 15353, loss 0.527436.
Train: 2018-08-01T23:50:44.227921: step 15354, loss 0.650241.
Train: 2018-08-01T23:50:44.397498: step 15355, loss 0.474801.
Train: 2018-08-01T23:50:44.565019: step 15356, loss 0.544852.
Train: 2018-08-01T23:50:44.728583: step 15357, loss 0.439312.
Train: 2018-08-01T23:50:44.901151: step 15358, loss 0.544968.
Train: 2018-08-01T23:50:45.076656: step 15359, loss 0.59796.
Train: 2018-08-01T23:50:45.262157: step 15360, loss 0.650757.
Test: 2018-08-01T23:50:45.794745: step 15360, loss 0.547646.
Train: 2018-08-01T23:50:45.967273: step 15361, loss 0.52818.
Train: 2018-08-01T23:50:46.136841: step 15362, loss 0.580891.
Train: 2018-08-01T23:50:46.305368: step 15363, loss 0.580325.
Train: 2018-08-01T23:50:46.472935: step 15364, loss 0.474019.
Train: 2018-08-01T23:50:46.648483: step 15365, loss 0.704303.
Train: 2018-08-01T23:50:46.815004: step 15366, loss 0.58021.
Train: 2018-08-01T23:50:46.985549: step 15367, loss 0.721559.
Train: 2018-08-01T23:50:47.153101: step 15368, loss 0.632937.
Train: 2018-08-01T23:50:47.324673: step 15369, loss 0.544918.
Train: 2018-08-01T23:50:47.487238: step 15370, loss 0.457554.
Test: 2018-08-01T23:50:48.011806: step 15370, loss 0.547824.
Train: 2018-08-01T23:50:48.185342: step 15371, loss 0.562436.
Train: 2018-08-01T23:50:48.354887: step 15372, loss 0.545009.
Train: 2018-08-01T23:50:48.522445: step 15373, loss 0.52763.
Train: 2018-08-01T23:50:48.694016: step 15374, loss 0.614578.
Train: 2018-08-01T23:50:48.864526: step 15375, loss 0.597139.
Train: 2018-08-01T23:50:49.038075: step 15376, loss 0.527759.
Train: 2018-08-01T23:50:49.208637: step 15377, loss 0.614337.
Train: 2018-08-01T23:50:49.377184: step 15378, loss 0.596963.
Train: 2018-08-01T23:50:49.544733: step 15379, loss 0.545167.
Train: 2018-08-01T23:50:49.722264: step 15380, loss 0.527984.
Test: 2018-08-01T23:50:50.263785: step 15380, loss 0.548009.
Train: 2018-08-01T23:50:50.431337: step 15381, loss 0.631176.
Train: 2018-08-01T23:50:50.646088: step 15382, loss 0.510926.
Train: 2018-08-01T23:50:50.832586: step 15383, loss 0.476693.
Train: 2018-08-01T23:50:51.001134: step 15384, loss 0.562405.
Train: 2018-08-01T23:50:51.178683: step 15385, loss 0.631012.
Train: 2018-08-01T23:50:51.346243: step 15386, loss 0.613824.
Train: 2018-08-01T23:50:51.518751: step 15387, loss 0.647986.
Train: 2018-08-01T23:50:51.696301: step 15388, loss 0.562408.
Train: 2018-08-01T23:50:51.891755: step 15389, loss 0.596487.
Train: 2018-08-01T23:50:52.073298: step 15390, loss 0.579412.
Test: 2018-08-01T23:50:52.622799: step 15390, loss 0.548223.
Train: 2018-08-01T23:50:52.797333: step 15391, loss 0.579377.
Train: 2018-08-01T23:50:52.975881: step 15392, loss 0.647008.
Train: 2018-08-01T23:50:53.148413: step 15393, loss 0.478138.
Train: 2018-08-01T23:50:53.323957: step 15394, loss 0.545615.
Train: 2018-08-01T23:50:53.540346: step 15395, loss 0.52883.
Train: 2018-08-01T23:50:53.712911: step 15396, loss 0.528852.
Train: 2018-08-01T23:50:53.923323: step 15397, loss 0.495245.
Train: 2018-08-01T23:50:54.145729: step 15398, loss 0.528808.
Train: 2018-08-01T23:50:54.399052: step 15399, loss 0.663522.
Train: 2018-08-01T23:50:54.569637: step 15400, loss 0.495059.
Test: 2018-08-01T23:50:55.109153: step 15400, loss 0.548318.
Train: 2018-08-01T23:50:55.994321: step 15401, loss 0.545576.
Train: 2018-08-01T23:50:56.158856: step 15402, loss 0.629968.
Train: 2018-08-01T23:50:56.325411: step 15403, loss 0.57932.
Train: 2018-08-01T23:50:56.495955: step 15404, loss 0.511779.
Train: 2018-08-01T23:50:56.665502: step 15405, loss 0.528636.
Train: 2018-08-01T23:50:56.830062: step 15406, loss 0.54551.
Train: 2018-08-01T23:50:57.008584: step 15407, loss 0.528542.
Train: 2018-08-01T23:50:57.174142: step 15408, loss 0.443624.
Train: 2018-08-01T23:50:57.340723: step 15409, loss 0.494306.
Train: 2018-08-01T23:50:57.509272: step 15410, loss 0.699157.
Test: 2018-08-01T23:50:58.040824: step 15410, loss 0.548069.
Train: 2018-08-01T23:50:58.201424: step 15411, loss 0.545285.
Train: 2018-08-01T23:50:58.378663: step 15412, loss 0.579556.
Train: 2018-08-01T23:50:58.549207: step 15413, loss 0.545233.
Train: 2018-08-01T23:50:58.728727: step 15414, loss 0.562406.
Train: 2018-08-01T23:50:58.893288: step 15415, loss 0.493533.
Train: 2018-08-01T23:50:59.064830: step 15416, loss 0.510647.
Train: 2018-08-01T23:50:59.226397: step 15417, loss 0.493222.
Train: 2018-08-01T23:50:59.398823: step 15418, loss 0.51036.
Train: 2018-08-01T23:50:59.572357: step 15419, loss 0.457941.
Train: 2018-08-01T23:50:59.740906: step 15420, loss 0.579943.
Test: 2018-08-01T23:51:00.284453: step 15420, loss 0.547763.
Train: 2018-08-01T23:51:00.453004: step 15421, loss 0.632709.
Train: 2018-08-01T23:51:00.618560: step 15422, loss 0.544879.
Train: 2018-08-01T23:51:00.793095: step 15423, loss 0.650708.
Train: 2018-08-01T23:51:00.958652: step 15424, loss 0.580158.
Train: 2018-08-01T23:51:01.120244: step 15425, loss 0.491848.
Train: 2018-08-01T23:51:01.284806: step 15426, loss 0.597883.
Train: 2018-08-01T23:51:01.450347: step 15427, loss 0.580211.
Train: 2018-08-01T23:51:01.619884: step 15428, loss 0.47403.
Train: 2018-08-01T23:51:01.784474: step 15429, loss 0.527089.
Train: 2018-08-01T23:51:01.954988: step 15430, loss 0.580281.
Test: 2018-08-01T23:51:02.490556: step 15430, loss 0.547672.
Train: 2018-08-01T23:51:02.664092: step 15431, loss 0.580306.
Train: 2018-08-01T23:51:02.837653: step 15432, loss 0.527008.
Train: 2018-08-01T23:51:03.002213: step 15433, loss 0.633697.
Train: 2018-08-01T23:51:03.172733: step 15434, loss 0.544774.
Train: 2018-08-01T23:51:03.342280: step 15435, loss 0.544775.
Train: 2018-08-01T23:51:03.510829: step 15436, loss 0.615885.
Train: 2018-08-01T23:51:03.676405: step 15437, loss 0.580309.
Train: 2018-08-01T23:51:03.851917: step 15438, loss 0.633515.
Train: 2018-08-01T23:51:04.017499: step 15439, loss 0.580231.
Train: 2018-08-01T23:51:04.186057: step 15440, loss 0.509491.
Test: 2018-08-01T23:51:04.720611: step 15440, loss 0.547719.
Train: 2018-08-01T23:51:04.886186: step 15441, loss 0.474254.
Train: 2018-08-01T23:51:05.052708: step 15442, loss 0.597791.
Train: 2018-08-01T23:51:05.219261: step 15443, loss 0.474315.
Train: 2018-08-01T23:51:05.381826: step 15444, loss 0.509563.
Train: 2018-08-01T23:51:05.545396: step 15445, loss 0.580166.
Train: 2018-08-01T23:51:05.720920: step 15446, loss 0.668552.
Train: 2018-08-01T23:51:05.885480: step 15447, loss 0.562501.
Train: 2018-08-01T23:51:06.051038: step 15448, loss 0.509569.
Train: 2018-08-01T23:51:06.215624: step 15449, loss 0.562493.
Train: 2018-08-01T23:51:06.388170: step 15450, loss 0.527225.
Test: 2018-08-01T23:51:06.918718: step 15450, loss 0.547725.
Train: 2018-08-01T23:51:07.087268: step 15451, loss 0.491951.
Train: 2018-08-01T23:51:07.250859: step 15452, loss 0.527194.
Train: 2018-08-01T23:51:07.412425: step 15453, loss 0.597856.
Train: 2018-08-01T23:51:07.576983: step 15454, loss 0.633249.
Train: 2018-08-01T23:51:07.744542: step 15455, loss 0.562508.
Train: 2018-08-01T23:51:07.913098: step 15456, loss 0.562504.
Train: 2018-08-01T23:51:08.074655: step 15457, loss 0.52719.
Train: 2018-08-01T23:51:08.237224: step 15458, loss 0.562498.
Train: 2018-08-01T23:51:08.402751: step 15459, loss 0.597793.
Train: 2018-08-01T23:51:08.568333: step 15460, loss 0.562492.
Test: 2018-08-01T23:51:09.095924: step 15460, loss 0.547731.
Train: 2018-08-01T23:51:09.260496: step 15461, loss 0.562487.
Train: 2018-08-01T23:51:09.427076: step 15462, loss 0.615305.
Train: 2018-08-01T23:51:09.596598: step 15463, loss 0.474566.
Train: 2018-08-01T23:51:09.767141: step 15464, loss 0.632781.
Train: 2018-08-01T23:51:09.932730: step 15465, loss 0.615132.
Train: 2018-08-01T23:51:10.100251: step 15466, loss 0.579978.
Train: 2018-08-01T23:51:10.276833: step 15467, loss 0.562446.
Train: 2018-08-01T23:51:10.442338: step 15468, loss 0.579894.
Train: 2018-08-01T23:51:10.608923: step 15469, loss 0.562431.
Train: 2018-08-01T23:51:10.775471: step 15470, loss 0.492869.
Test: 2018-08-01T23:51:11.314022: step 15470, loss 0.547873.
Train: 2018-08-01T23:51:11.483557: step 15471, loss 0.545046.
Train: 2018-08-01T23:51:11.648140: step 15472, loss 0.614524.
Train: 2018-08-01T23:51:11.820653: step 15473, loss 0.562419.
Train: 2018-08-01T23:51:11.987208: step 15474, loss 0.458462.
Train: 2018-08-01T23:51:12.151768: step 15475, loss 0.597088.
Train: 2018-08-01T23:51:12.318353: step 15476, loss 0.493082.
Train: 2018-08-01T23:51:12.490862: step 15477, loss 0.545071.
Train: 2018-08-01T23:51:12.655426: step 15478, loss 0.562421.
Train: 2018-08-01T23:51:12.821975: step 15479, loss 0.527669.
Train: 2018-08-01T23:51:12.990543: step 15480, loss 0.545029.
Test: 2018-08-01T23:51:13.523103: step 15480, loss 0.547845.
Train: 2018-08-01T23:51:13.687686: step 15481, loss 0.492758.
Train: 2018-08-01T23:51:13.856236: step 15482, loss 0.579891.
Train: 2018-08-01T23:51:14.019798: step 15483, loss 0.579924.
Train: 2018-08-01T23:51:14.186354: step 15484, loss 0.597445.
Train: 2018-08-01T23:51:14.349920: step 15485, loss 0.52744.
Train: 2018-08-01T23:51:14.516471: step 15486, loss 0.615012.
Train: 2018-08-01T23:51:14.686991: step 15487, loss 0.579972.
Train: 2018-08-01T23:51:14.860526: step 15488, loss 0.439871.
Train: 2018-08-01T23:51:15.025113: step 15489, loss 0.562459.
Train: 2018-08-01T23:51:15.190675: step 15490, loss 0.597569.
Test: 2018-08-01T23:51:15.720227: step 15490, loss 0.547762.
Train: 2018-08-01T23:51:15.894762: step 15491, loss 0.544907.
Train: 2018-08-01T23:51:16.079321: step 15492, loss 0.650316.
Train: 2018-08-01T23:51:16.244855: step 15493, loss 0.45713.
Train: 2018-08-01T23:51:16.414402: step 15494, loss 0.597603.
Train: 2018-08-01T23:51:16.579959: step 15495, loss 0.580037.
Train: 2018-08-01T23:51:16.745547: step 15496, loss 0.597594.
Train: 2018-08-01T23:51:16.916061: step 15497, loss 0.527365.
Train: 2018-08-01T23:51:17.084612: step 15498, loss 0.527374.
Train: 2018-08-01T23:51:17.251209: step 15499, loss 0.52737.
Train: 2018-08-01T23:51:17.415726: step 15500, loss 0.474689.
Test: 2018-08-01T23:51:17.944343: step 15500, loss 0.54775.
Train: 2018-08-01T23:51:18.706918: step 15501, loss 0.562474.
Train: 2018-08-01T23:51:18.873448: step 15502, loss 0.738542.
Train: 2018-08-01T23:51:19.042027: step 15503, loss 0.650396.
Train: 2018-08-01T23:51:19.205558: step 15504, loss 0.509832.
Train: 2018-08-01T23:51:19.374134: step 15505, loss 0.562454.
Train: 2018-08-01T23:51:19.543656: step 15506, loss 0.632416.
Train: 2018-08-01T23:51:19.710241: step 15507, loss 0.57989.
Train: 2018-08-01T23:51:19.875767: step 15508, loss 0.492782.
Train: 2018-08-01T23:51:20.045340: step 15509, loss 0.492863.
Train: 2018-08-01T23:51:20.214891: step 15510, loss 0.562424.
Test: 2018-08-01T23:51:20.748435: step 15510, loss 0.547867.
Train: 2018-08-01T23:51:20.913991: step 15511, loss 0.527656.
Train: 2018-08-01T23:51:21.080577: step 15512, loss 0.545036.
Train: 2018-08-01T23:51:21.245107: step 15513, loss 0.475449.
Train: 2018-08-01T23:51:21.412659: step 15514, loss 0.527587.
Train: 2018-08-01T23:51:21.589213: step 15515, loss 0.510079.
Train: 2018-08-01T23:51:21.755742: step 15516, loss 0.474985.
Train: 2018-08-01T23:51:21.918306: step 15517, loss 0.474725.
Train: 2018-08-01T23:51:22.084862: step 15518, loss 0.544868.
Train: 2018-08-01T23:51:22.248425: step 15519, loss 0.650915.
Train: 2018-08-01T23:51:22.416973: step 15520, loss 0.668828.
Test: 2018-08-01T23:51:22.946564: step 15520, loss 0.547685.
Train: 2018-08-01T23:51:23.111150: step 15521, loss 0.52708.
Train: 2018-08-01T23:51:23.271717: step 15522, loss 0.668941.
Train: 2018-08-01T23:51:23.440240: step 15523, loss 0.509367.
Train: 2018-08-01T23:51:23.619758: step 15524, loss 0.544808.
Train: 2018-08-01T23:51:23.788307: step 15525, loss 0.544808.
Train: 2018-08-01T23:51:23.964862: step 15526, loss 0.580239.
Train: 2018-08-01T23:51:24.131416: step 15527, loss 0.615655.
Train: 2018-08-01T23:51:24.300938: step 15528, loss 0.562513.
Train: 2018-08-01T23:51:24.476499: step 15529, loss 0.597856.
Train: 2018-08-01T23:51:24.645049: step 15530, loss 0.562496.
Test: 2018-08-01T23:51:25.179598: step 15530, loss 0.547729.
Train: 2018-08-01T23:51:25.343180: step 15531, loss 0.580111.
Train: 2018-08-01T23:51:25.515693: step 15532, loss 0.492095.
Train: 2018-08-01T23:51:25.679278: step 15533, loss 0.580059.
Train: 2018-08-01T23:51:25.845834: step 15534, loss 0.61518.
Train: 2018-08-01T23:51:26.011390: step 15535, loss 0.667722.
Train: 2018-08-01T23:51:26.180913: step 15536, loss 0.492473.
Train: 2018-08-01T23:51:26.350460: step 15537, loss 0.614831.
Train: 2018-08-01T23:51:26.525018: step 15538, loss 0.562431.
Train: 2018-08-01T23:51:26.690559: step 15539, loss 0.562424.
Train: 2018-08-01T23:51:26.858127: step 15540, loss 0.614476.
Test: 2018-08-01T23:51:27.397674: step 15540, loss 0.547918.
Train: 2018-08-01T23:51:27.574188: step 15541, loss 0.527794.
Train: 2018-08-01T23:51:27.736752: step 15542, loss 0.648797.
Train: 2018-08-01T23:51:27.901317: step 15543, loss 0.562405.
Train: 2018-08-01T23:51:28.073877: step 15544, loss 0.648322.
Train: 2018-08-01T23:51:28.240414: step 15545, loss 0.511035.
Train: 2018-08-01T23:51:28.409978: step 15546, loss 0.528244.
Train: 2018-08-01T23:51:28.577531: step 15547, loss 0.596509.
Train: 2018-08-01T23:51:28.742092: step 15548, loss 0.647492.
Train: 2018-08-01T23:51:28.909642: step 15549, loss 0.613312.
Train: 2018-08-01T23:51:29.075176: step 15550, loss 0.528613.
Test: 2018-08-01T23:51:29.613736: step 15550, loss 0.548314.
Train: 2018-08-01T23:51:29.784279: step 15551, loss 0.562436.
Train: 2018-08-01T23:51:29.946877: step 15552, loss 0.596097.
Train: 2018-08-01T23:51:30.119415: step 15553, loss 0.545671.
Train: 2018-08-01T23:51:30.292920: step 15554, loss 0.562466.
Train: 2018-08-01T23:51:30.453515: step 15555, loss 0.595926.
Train: 2018-08-01T23:51:30.631015: step 15556, loss 0.612567.
Train: 2018-08-01T23:51:30.793607: step 15557, loss 0.495882.
Train: 2018-08-01T23:51:30.961133: step 15558, loss 0.595782.
Train: 2018-08-01T23:51:31.127719: step 15559, loss 0.51267.
Train: 2018-08-01T23:51:31.297266: step 15560, loss 0.562519.
Test: 2018-08-01T23:51:31.825822: step 15560, loss 0.548614.
Train: 2018-08-01T23:51:31.994397: step 15561, loss 0.545914.
Train: 2018-08-01T23:51:32.160925: step 15562, loss 0.662172.
Train: 2018-08-01T23:51:32.326514: step 15563, loss 0.628881.
Train: 2018-08-01T23:51:32.495059: step 15564, loss 0.595653.
Train: 2018-08-01T23:51:32.666599: step 15565, loss 0.463447.
Train: 2018-08-01T23:51:32.839140: step 15566, loss 0.579076.
Train: 2018-08-01T23:51:33.004702: step 15567, loss 0.562567.
Train: 2018-08-01T23:51:33.171254: step 15568, loss 0.628574.
Train: 2018-08-01T23:51:33.348781: step 15569, loss 0.612027.
Train: 2018-08-01T23:51:33.516319: step 15570, loss 0.595505.
Test: 2018-08-01T23:51:34.050874: step 15570, loss 0.548855.
Train: 2018-08-01T23:51:34.219423: step 15571, loss 0.513332.
Train: 2018-08-01T23:51:34.384014: step 15572, loss 0.57903.
Train: 2018-08-01T23:51:34.552575: step 15573, loss 0.529823.
Train: 2018-08-01T23:51:34.718121: step 15574, loss 0.579024.
Train: 2018-08-01T23:51:34.890662: step 15575, loss 0.546229.
Train: 2018-08-01T23:51:35.055215: step 15576, loss 0.513419.
Train: 2018-08-01T23:51:35.223739: step 15577, loss 0.529772.
Train: 2018-08-01T23:51:35.396277: step 15578, loss 0.595493.
Train: 2018-08-01T23:51:35.571834: step 15579, loss 0.54612.
Train: 2018-08-01T23:51:35.735370: step 15580, loss 0.595554.
Test: 2018-08-01T23:51:36.278919: step 15580, loss 0.548746.
Train: 2018-08-01T23:51:36.445503: step 15581, loss 0.579072.
Train: 2018-08-01T23:51:36.610066: step 15582, loss 0.546045.
Train: 2018-08-01T23:51:36.777585: step 15583, loss 0.54602.
Train: 2018-08-01T23:51:36.946166: step 15584, loss 0.545989.
Train: 2018-08-01T23:51:37.110725: step 15585, loss 0.612267.
Train: 2018-08-01T23:51:37.276252: step 15586, loss 0.562526.
Train: 2018-08-01T23:51:37.446821: step 15587, loss 0.711947.
Train: 2018-08-01T23:51:37.613351: step 15588, loss 0.612267.
Train: 2018-08-01T23:51:37.777936: step 15589, loss 0.612185.
Train: 2018-08-01T23:51:37.940475: step 15590, loss 0.480035.
Test: 2018-08-01T23:51:38.480064: step 15590, loss 0.548761.
Train: 2018-08-01T23:51:38.645591: step 15591, loss 0.463613.
Train: 2018-08-01T23:51:38.824114: step 15592, loss 0.579073.
Train: 2018-08-01T23:51:38.993692: step 15593, loss 0.546038.
Train: 2018-08-01T23:51:39.162255: step 15594, loss 0.579088.
Train: 2018-08-01T23:51:39.324806: step 15595, loss 0.479793.
Train: 2018-08-01T23:51:39.500331: step 15596, loss 0.595698.
Train: 2018-08-01T23:51:39.665865: step 15597, loss 0.579127.
Train: 2018-08-01T23:51:39.831447: step 15598, loss 0.495994.
Train: 2018-08-01T23:51:40.002020: step 15599, loss 0.495836.
Train: 2018-08-01T23:51:40.169517: step 15600, loss 0.529046.
Test: 2018-08-01T23:51:40.700130: step 15600, loss 0.548416.
Train: 2018-08-01T23:51:41.512242: step 15601, loss 0.679854.
Train: 2018-08-01T23:51:41.680817: step 15602, loss 0.562453.
Train: 2018-08-01T23:51:41.847371: step 15603, loss 0.562447.
Train: 2018-08-01T23:51:42.009942: step 15604, loss 0.545609.
Train: 2018-08-01T23:51:42.184444: step 15605, loss 0.613006.
Train: 2018-08-01T23:51:42.355020: step 15606, loss 0.596166.
Train: 2018-08-01T23:51:42.520577: step 15607, loss 0.528701.
Train: 2018-08-01T23:51:42.687133: step 15608, loss 0.511802.
Train: 2018-08-01T23:51:42.849666: step 15609, loss 0.562427.
Train: 2018-08-01T23:51:43.016254: step 15610, loss 0.477822.
Test: 2018-08-01T23:51:43.547799: step 15610, loss 0.548214.
Train: 2018-08-01T23:51:43.714356: step 15611, loss 0.511534.
Train: 2018-08-01T23:51:43.882930: step 15612, loss 0.596431.
Train: 2018-08-01T23:51:44.044503: step 15613, loss 0.528312.
Train: 2018-08-01T23:51:44.216046: step 15614, loss 0.562404.
Train: 2018-08-01T23:51:44.380573: step 15615, loss 0.528149.
Train: 2018-08-01T23:51:44.542143: step 15616, loss 0.459388.
Train: 2018-08-01T23:51:44.708728: step 15617, loss 0.562404.
Train: 2018-08-01T23:51:44.873258: step 15618, loss 0.510543.
Train: 2018-08-01T23:51:45.036819: step 15619, loss 0.562416.
Train: 2018-08-01T23:51:45.206367: step 15620, loss 0.614649.
Test: 2018-08-01T23:51:45.731963: step 15620, loss 0.547825.
Train: 2018-08-01T23:51:45.903528: step 15621, loss 0.597323.
Train: 2018-08-01T23:51:46.069061: step 15622, loss 0.667253.
Train: 2018-08-01T23:51:46.230629: step 15623, loss 0.492579.
Train: 2018-08-01T23:51:46.404179: step 15624, loss 0.649825.
Train: 2018-08-01T23:51:46.568758: step 15625, loss 0.614835.
Train: 2018-08-01T23:51:46.733285: step 15626, loss 0.579875.
Train: 2018-08-01T23:51:46.903828: step 15627, loss 0.492763.
Train: 2018-08-01T23:51:47.069411: step 15628, loss 0.579834.
Train: 2018-08-01T23:51:47.237936: step 15629, loss 0.57982.
Train: 2018-08-01T23:51:47.407510: step 15630, loss 0.579802.
Test: 2018-08-01T23:51:47.943052: step 15630, loss 0.547879.
Train: 2018-08-01T23:51:48.117584: step 15631, loss 0.475608.
Train: 2018-08-01T23:51:48.287131: step 15632, loss 0.510323.
Train: 2018-08-01T23:51:48.457675: step 15633, loss 0.475523.
Train: 2018-08-01T23:51:48.637226: step 15634, loss 0.510189.
Train: 2018-08-01T23:51:48.804772: step 15635, loss 0.562435.
Train: 2018-08-01T23:51:48.973297: step 15636, loss 0.527463.
Train: 2018-08-01T23:51:49.141878: step 15637, loss 0.597517.
Train: 2018-08-01T23:51:49.310427: step 15638, loss 0.58002.
Train: 2018-08-01T23:51:49.480978: step 15639, loss 0.544893.
Train: 2018-08-01T23:51:49.657500: step 15640, loss 0.509689.
Test: 2018-08-01T23:51:50.194034: step 15640, loss 0.547727.
Train: 2018-08-01T23:51:50.368598: step 15641, loss 0.580109.
Train: 2018-08-01T23:51:50.540125: step 15642, loss 0.580136.
Train: 2018-08-01T23:51:50.707661: step 15643, loss 0.59781.
Train: 2018-08-01T23:51:50.877238: step 15644, loss 0.597813.
Train: 2018-08-01T23:51:51.045757: step 15645, loss 0.633089.
Train: 2018-08-01T23:51:51.218306: step 15646, loss 0.632975.
Train: 2018-08-01T23:51:51.383885: step 15647, loss 0.404242.
Train: 2018-08-01T23:51:51.551404: step 15648, loss 0.54489.
Train: 2018-08-01T23:51:51.719985: step 15649, loss 0.632795.
Train: 2018-08-01T23:51:51.891520: step 15650, loss 0.527336.
Test: 2018-08-01T23:51:52.419085: step 15650, loss 0.547761.
Train: 2018-08-01T23:51:52.585640: step 15651, loss 0.615133.
Train: 2018-08-01T23:51:52.767186: step 15652, loss 0.562456.
Train: 2018-08-01T23:51:52.938697: step 15653, loss 0.509903.
Train: 2018-08-01T23:51:53.102259: step 15654, loss 0.597467.
Train: 2018-08-01T23:51:53.275819: step 15655, loss 0.474976.
Train: 2018-08-01T23:51:53.451350: step 15656, loss 0.614945.
Train: 2018-08-01T23:51:53.627853: step 15657, loss 0.474987.
Train: 2018-08-01T23:51:53.796403: step 15658, loss 0.509936.
Train: 2018-08-01T23:51:53.982905: step 15659, loss 0.492349.
Train: 2018-08-01T23:51:54.150457: step 15660, loss 0.615149.
Test: 2018-08-01T23:51:54.699016: step 15660, loss 0.547749.
Train: 2018-08-01T23:51:54.873525: step 15661, loss 0.50973.
Train: 2018-08-01T23:51:55.039082: step 15662, loss 0.509656.
Train: 2018-08-01T23:51:55.206658: step 15663, loss 0.615422.
Train: 2018-08-01T23:51:55.383187: step 15664, loss 0.615484.
Train: 2018-08-01T23:51:55.551711: step 15665, loss 0.527171.
Train: 2018-08-01T23:51:55.722304: step 15666, loss 0.562503.
Train: 2018-08-01T23:51:55.894819: step 15667, loss 0.562505.
Train: 2018-08-01T23:51:56.061348: step 15668, loss 0.597874.
Train: 2018-08-01T23:51:56.224942: step 15669, loss 0.52715.
Train: 2018-08-01T23:51:56.392463: step 15670, loss 0.491793.
Test: 2018-08-01T23:51:56.926062: step 15670, loss 0.547695.
Train: 2018-08-01T23:51:57.095615: step 15671, loss 0.562511.
Train: 2018-08-01T23:51:57.262139: step 15672, loss 0.527104.
Train: 2018-08-01T23:51:57.429691: step 15673, loss 0.509351.
Train: 2018-08-01T23:51:57.592256: step 15674, loss 0.562536.
Train: 2018-08-01T23:51:57.758843: step 15675, loss 0.473678.
Train: 2018-08-01T23:51:57.928365: step 15676, loss 0.580378.
Train: 2018-08-01T23:51:58.095941: step 15677, loss 0.651787.
Train: 2018-08-01T23:51:58.272464: step 15678, loss 0.437672.
Train: 2018-08-01T23:51:58.448991: step 15679, loss 0.616219.
Train: 2018-08-01T23:51:58.612554: step 15680, loss 0.544718.
Test: 2018-08-01T23:51:59.146102: step 15680, loss 0.547623.
Train: 2018-08-01T23:51:59.315648: step 15681, loss 0.508916.
Train: 2018-08-01T23:51:59.493205: step 15682, loss 0.598462.
Train: 2018-08-01T23:51:59.663749: step 15683, loss 0.472987.
Train: 2018-08-01T23:51:59.828279: step 15684, loss 0.580598.
Train: 2018-08-01T23:51:59.997853: step 15685, loss 0.688451.
Train: 2018-08-01T23:52:00.169399: step 15686, loss 0.652416.
Train: 2018-08-01T23:52:00.339929: step 15687, loss 0.544704.
Train: 2018-08-01T23:52:00.508459: step 15688, loss 0.598369.
Train: 2018-08-01T23:52:00.686989: step 15689, loss 0.633955.
Train: 2018-08-01T23:52:00.852542: step 15690, loss 0.633708.
Test: 2018-08-01T23:52:01.396118: step 15690, loss 0.547683.
Train: 2018-08-01T23:52:01.562643: step 15691, loss 0.668847.
Train: 2018-08-01T23:52:01.729197: step 15692, loss 0.509589.
Train: 2018-08-01T23:52:01.896748: step 15693, loss 0.632724.
Train: 2018-08-01T23:52:02.061309: step 15694, loss 0.509985.
Train: 2018-08-01T23:52:02.225894: step 15695, loss 0.59728.
Train: 2018-08-01T23:52:02.457275: step 15696, loss 0.475603.
Train: 2018-08-01T23:52:02.624802: step 15697, loss 0.510433.
Train: 2018-08-01T23:52:02.801331: step 15698, loss 0.527799.
Train: 2018-08-01T23:52:02.976861: step 15699, loss 0.579702.
Train: 2018-08-01T23:52:03.140457: step 15700, loss 0.631525.
Test: 2018-08-01T23:52:03.687965: step 15700, loss 0.54796.
Train: 2018-08-01T23:52:04.526542: step 15701, loss 0.545155.
Train: 2018-08-01T23:52:04.694132: step 15702, loss 0.459045.
Train: 2018-08-01T23:52:04.860648: step 15703, loss 0.545173.
Train: 2018-08-01T23:52:05.028200: step 15704, loss 0.596877.
Train: 2018-08-01T23:52:05.196775: step 15705, loss 0.596874.
Train: 2018-08-01T23:52:05.371282: step 15706, loss 0.493503.
Train: 2018-08-01T23:52:05.541853: step 15707, loss 0.614099.
Train: 2018-08-01T23:52:05.709379: step 15708, loss 0.510725.
Train: 2018-08-01T23:52:05.882916: step 15709, loss 0.54517.
Train: 2018-08-01T23:52:06.052506: step 15710, loss 0.545161.
Test: 2018-08-01T23:52:06.591034: step 15710, loss 0.547955.
Train: 2018-08-01T23:52:06.757577: step 15711, loss 0.683188.
Train: 2018-08-01T23:52:06.922161: step 15712, loss 0.614114.
Train: 2018-08-01T23:52:07.086728: step 15713, loss 0.614025.
Train: 2018-08-01T23:52:07.255271: step 15714, loss 0.407876.
Train: 2018-08-01T23:52:07.421827: step 15715, loss 0.545229.
Train: 2018-08-01T23:52:07.590351: step 15716, loss 0.665471.
Train: 2018-08-01T23:52:07.752917: step 15717, loss 0.493763.
Train: 2018-08-01T23:52:07.919471: step 15718, loss 0.579559.
Train: 2018-08-01T23:52:08.092011: step 15719, loss 0.493785.
Train: 2018-08-01T23:52:08.256570: step 15720, loss 0.579567.
Test: 2018-08-01T23:52:08.795130: step 15720, loss 0.54802.
Train: 2018-08-01T23:52:08.961711: step 15721, loss 0.716958.
Train: 2018-08-01T23:52:09.129237: step 15722, loss 0.596685.
Train: 2018-08-01T23:52:09.300808: step 15723, loss 0.682142.
Train: 2018-08-01T23:52:09.468361: step 15724, loss 0.545361.
Train: 2018-08-01T23:52:09.636910: step 15725, loss 0.664375.
Train: 2018-08-01T23:52:09.811416: step 15726, loss 0.59627.
Train: 2018-08-01T23:52:09.976971: step 15727, loss 0.478165.
Train: 2018-08-01T23:52:10.144551: step 15728, loss 0.596071.
Train: 2018-08-01T23:52:10.311104: step 15729, loss 0.713368.
Train: 2018-08-01T23:52:10.482643: step 15730, loss 0.512408.
Test: 2018-08-01T23:52:11.012234: step 15730, loss 0.548573.
Train: 2018-08-01T23:52:11.189762: step 15731, loss 0.47932.
Train: 2018-08-01T23:52:11.356309: step 15732, loss 0.562511.
Train: 2018-08-01T23:52:11.520875: step 15733, loss 0.612275.
Train: 2018-08-01T23:52:11.695401: step 15734, loss 0.51287.
Train: 2018-08-01T23:52:11.862930: step 15735, loss 0.496347.
Train: 2018-08-01T23:52:12.031479: step 15736, loss 0.546193.
Train: 2018-08-01T23:52:12.197061: step 15737, loss 0.561931.
Train: 2018-08-01T23:52:12.365612: step 15738, loss 0.646036.
Train: 2018-08-01T23:52:12.529147: step 15739, loss 0.595763.
Train: 2018-08-01T23:52:12.695704: step 15740, loss 0.646003.
Test: 2018-08-01T23:52:13.236288: step 15740, loss 0.54863.
Train: 2018-08-01T23:52:13.411814: step 15741, loss 0.479088.
Train: 2018-08-01T23:52:13.578374: step 15742, loss 0.579831.
Train: 2018-08-01T23:52:13.745921: step 15743, loss 0.56243.
Train: 2018-08-01T23:52:13.906465: step 15744, loss 0.463082.
Train: 2018-08-01T23:52:14.075041: step 15745, loss 0.479435.
Train: 2018-08-01T23:52:14.255558: step 15746, loss 0.695704.
Train: 2018-08-01T23:52:14.419127: step 15747, loss 0.512452.
Train: 2018-08-01T23:52:14.586673: step 15748, loss 0.630171.
Train: 2018-08-01T23:52:14.755222: step 15749, loss 0.495645.
Train: 2018-08-01T23:52:14.922749: step 15750, loss 0.680129.
Test: 2018-08-01T23:52:15.450339: step 15750, loss 0.548533.
Train: 2018-08-01T23:52:15.616925: step 15751, loss 0.729604.
Train: 2018-08-01T23:52:15.784477: step 15752, loss 0.628868.
Train: 2018-08-01T23:52:15.967986: step 15753, loss 0.496544.
Train: 2018-08-01T23:52:16.142489: step 15754, loss 0.628442.
Train: 2018-08-01T23:52:16.312064: step 15755, loss 0.546191.
Train: 2018-08-01T23:52:16.493569: step 15756, loss 0.447923.
Train: 2018-08-01T23:52:16.660133: step 15757, loss 0.529854.
Train: 2018-08-01T23:52:16.828654: step 15758, loss 0.529824.
Train: 2018-08-01T23:52:17.001193: step 15759, loss 0.595453.
Train: 2018-08-01T23:52:17.166781: step 15760, loss 0.529737.
Test: 2018-08-01T23:52:17.693379: step 15760, loss 0.548813.
Train: 2018-08-01T23:52:17.858900: step 15761, loss 0.595504.
Train: 2018-08-01T23:52:18.030441: step 15762, loss 0.562585.
Train: 2018-08-01T23:52:18.196024: step 15763, loss 0.595547.
Train: 2018-08-01T23:52:18.366574: step 15764, loss 0.546082.
Train: 2018-08-01T23:52:18.530107: step 15765, loss 0.529558.
Train: 2018-08-01T23:52:18.704665: step 15766, loss 0.64519.
Train: 2018-08-01T23:52:18.874187: step 15767, loss 0.595613.
Train: 2018-08-01T23:52:19.044761: step 15768, loss 0.529515.
Train: 2018-08-01T23:52:19.211290: step 15769, loss 0.430329.
Train: 2018-08-01T23:52:19.379834: step 15770, loss 0.562551.
Test: 2018-08-01T23:52:19.922386: step 15770, loss 0.548612.
Train: 2018-08-01T23:52:20.095933: step 15771, loss 0.545915.
Train: 2018-08-01T23:52:20.263497: step 15772, loss 0.629069.
Train: 2018-08-01T23:52:20.430026: step 15773, loss 0.562499.
Train: 2018-08-01T23:52:20.599619: step 15774, loss 0.49569.
Train: 2018-08-01T23:52:20.767159: step 15775, loss 0.579584.
Train: 2018-08-01T23:52:20.944677: step 15776, loss 0.512063.
Train: 2018-08-01T23:52:21.117190: step 15777, loss 0.562435.
Train: 2018-08-01T23:52:21.299703: step 15778, loss 0.56237.
Train: 2018-08-01T23:52:21.462298: step 15779, loss 0.579331.
Train: 2018-08-01T23:52:21.627851: step 15780, loss 0.596226.
Test: 2018-08-01T23:52:22.168413: step 15780, loss 0.548222.
Train: 2018-08-01T23:52:22.347925: step 15781, loss 0.511481.
Train: 2018-08-01T23:52:22.526423: step 15782, loss 0.42623.
Train: 2018-08-01T23:52:22.692005: step 15783, loss 0.630472.
Train: 2018-08-01T23:52:22.873526: step 15784, loss 0.579952.
Train: 2018-08-01T23:52:23.039053: step 15785, loss 0.528181.
Train: 2018-08-01T23:52:23.205641: step 15786, loss 0.547662.
Train: 2018-08-01T23:52:23.374183: step 15787, loss 0.596135.
Train: 2018-08-01T23:52:23.544731: step 15788, loss 0.47457.
Train: 2018-08-01T23:52:23.711291: step 15789, loss 0.526908.
Train: 2018-08-01T23:52:23.883794: step 15790, loss 0.669096.
Test: 2018-08-01T23:52:24.421391: step 15790, loss 0.547746.
Train: 2018-08-01T23:52:24.593896: step 15791, loss 0.508898.
Train: 2018-08-01T23:52:24.764441: step 15792, loss 0.579984.
Train: 2018-08-01T23:52:24.931992: step 15793, loss 0.559391.
Train: 2018-08-01T23:52:25.096553: step 15794, loss 0.529842.
Train: 2018-08-01T23:52:25.263133: step 15795, loss 0.631581.
Train: 2018-08-01T23:52:25.428668: step 15796, loss 0.612116.
Train: 2018-08-01T23:52:25.599210: step 15797, loss 0.547567.
Train: 2018-08-01T23:52:25.788703: step 15798, loss 0.47363.
Train: 2018-08-01T23:52:25.955256: step 15799, loss 0.56413.
Train: 2018-08-01T23:52:26.122809: step 15800, loss 0.565246.
Test: 2018-08-01T23:52:26.666356: step 15800, loss 0.547656.
Train: 2018-08-01T23:52:27.447741: step 15801, loss 0.510332.
Train: 2018-08-01T23:52:27.615324: step 15802, loss 0.634973.
Train: 2018-08-01T23:52:27.786834: step 15803, loss 0.617969.
Train: 2018-08-01T23:52:27.951399: step 15804, loss 0.563737.
Train: 2018-08-01T23:52:28.116970: step 15805, loss 0.580269.
Train: 2018-08-01T23:52:28.283549: step 15806, loss 0.424194.
Train: 2018-08-01T23:52:28.450090: step 15807, loss 0.613956.
Train: 2018-08-01T23:52:28.626615: step 15808, loss 0.597047.
Train: 2018-08-01T23:52:28.794166: step 15809, loss 0.579588.
Train: 2018-08-01T23:52:28.961694: step 15810, loss 0.476415.
Test: 2018-08-01T23:52:29.500276: step 15810, loss 0.548002.
Train: 2018-08-01T23:52:29.747719: step 15811, loss 0.596823.
Train: 2018-08-01T23:52:29.915271: step 15812, loss 0.510796.
Train: 2018-08-01T23:52:30.088802: step 15813, loss 0.510765.
Train: 2018-08-01T23:52:30.264338: step 15814, loss 0.631364.
Train: 2018-08-01T23:52:30.437844: step 15815, loss 0.596896.
Train: 2018-08-01T23:52:30.610381: step 15816, loss 0.562414.
Train: 2018-08-01T23:52:30.782920: step 15817, loss 0.717483.
Train: 2018-08-01T23:52:30.951470: step 15818, loss 0.596789.
Train: 2018-08-01T23:52:31.119046: step 15819, loss 0.562413.
Train: 2018-08-01T23:52:31.284608: step 15820, loss 0.630819.
Test: 2018-08-01T23:52:31.821146: step 15820, loss 0.548141.
Train: 2018-08-01T23:52:31.987725: step 15821, loss 0.698802.
Train: 2018-08-01T23:52:32.164228: step 15822, loss 0.52849.
Train: 2018-08-01T23:52:32.328819: step 15823, loss 0.528627.
Train: 2018-08-01T23:52:32.501329: step 15824, loss 0.511876.
Train: 2018-08-01T23:52:32.672894: step 15825, loss 0.646589.
Train: 2018-08-01T23:52:32.839437: step 15826, loss 0.528906.
Train: 2018-08-01T23:52:33.008996: step 15827, loss 0.629469.
Train: 2018-08-01T23:52:33.175550: step 15828, loss 0.462267.
Train: 2018-08-01T23:52:33.341114: step 15829, loss 0.579186.
Train: 2018-08-01T23:52:33.505668: step 15830, loss 0.529151.
Test: 2018-08-01T23:52:34.047194: step 15830, loss 0.548542.
Train: 2018-08-01T23:52:34.220731: step 15831, loss 0.612515.
Train: 2018-08-01T23:52:34.383322: step 15832, loss 0.529194.
Train: 2018-08-01T23:52:34.547887: step 15833, loss 0.512547.
Train: 2018-08-01T23:52:34.716432: step 15834, loss 0.545839.
Train: 2018-08-01T23:52:34.890940: step 15835, loss 0.612539.
Train: 2018-08-01T23:52:35.063509: step 15836, loss 0.512445.
Train: 2018-08-01T23:52:35.233056: step 15837, loss 0.56249.
Train: 2018-08-01T23:52:35.405565: step 15838, loss 0.52905.
Train: 2018-08-01T23:52:35.571122: step 15839, loss 0.595958.
Train: 2018-08-01T23:52:35.738718: step 15840, loss 0.562471.
Test: 2018-08-01T23:52:36.261306: step 15840, loss 0.548422.
Train: 2018-08-01T23:52:36.425835: step 15841, loss 0.562466.
Train: 2018-08-01T23:52:36.594411: step 15842, loss 0.562461.
Train: 2018-08-01T23:52:36.771936: step 15843, loss 0.478458.
Train: 2018-08-01T23:52:36.942481: step 15844, loss 0.511945.
Train: 2018-08-01T23:52:37.108037: step 15845, loss 0.528681.
Train: 2018-08-01T23:52:37.271575: step 15846, loss 0.579355.
Train: 2018-08-01T23:52:37.441121: step 15847, loss 0.494549.
Train: 2018-08-01T23:52:37.606679: step 15848, loss 0.528371.
Train: 2018-08-01T23:52:37.778221: step 15849, loss 0.630722.
Train: 2018-08-01T23:52:37.945772: step 15850, loss 0.63086.
Test: 2018-08-01T23:52:38.497299: step 15850, loss 0.548064.
Train: 2018-08-01T23:52:38.958067: step 15851, loss 0.511022.
Train: 2018-08-01T23:52:39.180472: step 15852, loss 0.579561.
Train: 2018-08-01T23:52:39.352014: step 15853, loss 0.631102.
Train: 2018-08-01T23:52:39.522557: step 15854, loss 0.562405.
Train: 2018-08-01T23:52:39.688116: step 15855, loss 0.562406.
Train: 2018-08-01T23:52:39.852701: step 15856, loss 0.493662.
Train: 2018-08-01T23:52:40.017266: step 15857, loss 0.54504.
Train: 2018-08-01T23:52:40.185809: step 15858, loss 0.580414.
Train: 2018-08-01T23:52:40.349378: step 15859, loss 0.614799.
Train: 2018-08-01T23:52:40.520889: step 15860, loss 0.666211.
Test: 2018-08-01T23:52:41.060451: step 15860, loss 0.548005.
Train: 2018-08-01T23:52:41.225032: step 15861, loss 0.476407.
Train: 2018-08-01T23:52:41.387602: step 15862, loss 0.493647.
Train: 2018-08-01T23:52:41.553129: step 15863, loss 0.64844.
Train: 2018-08-01T23:52:41.716718: step 15864, loss 0.648395.
Train: 2018-08-01T23:52:41.883248: step 15865, loss 0.493729.
Train: 2018-08-01T23:52:42.054788: step 15866, loss 0.734007.
Train: 2018-08-01T23:52:42.218376: step 15867, loss 0.613741.
Train: 2018-08-01T23:52:42.382911: step 15868, loss 0.562412.
Train: 2018-08-01T23:52:42.545503: step 15869, loss 0.545413.
Train: 2018-08-01T23:52:42.722004: step 15870, loss 0.54546.
Test: 2018-08-01T23:52:43.262560: step 15870, loss 0.548252.
Train: 2018-08-01T23:52:43.428147: step 15871, loss 0.545498.
Train: 2018-08-01T23:52:43.595695: step 15872, loss 0.545527.
Train: 2018-08-01T23:52:43.759231: step 15873, loss 0.562439.
Train: 2018-08-01T23:52:43.928779: step 15874, loss 0.57932.
Train: 2018-08-01T23:52:44.099323: step 15875, loss 0.478142.
Train: 2018-08-01T23:52:44.266875: step 15876, loss 0.579314.
Train: 2018-08-01T23:52:44.431461: step 15877, loss 0.528704.
Train: 2018-08-01T23:52:44.596021: step 15878, loss 0.545561.
Train: 2018-08-01T23:52:44.760580: step 15879, loss 0.646932.
Train: 2018-08-01T23:52:44.930127: step 15880, loss 0.596227.
Test: 2018-08-01T23:52:45.459695: step 15880, loss 0.548309.
Train: 2018-08-01T23:52:45.623275: step 15881, loss 0.545564.
Train: 2018-08-01T23:52:45.786842: step 15882, loss 0.528698.
Train: 2018-08-01T23:52:45.950406: step 15883, loss 0.444302.
Train: 2018-08-01T23:52:46.117927: step 15884, loss 0.528618.
Train: 2018-08-01T23:52:46.282487: step 15885, loss 0.443793.
Train: 2018-08-01T23:52:46.446075: step 15886, loss 0.579435.
Train: 2018-08-01T23:52:46.617621: step 15887, loss 0.613615.
Train: 2018-08-01T23:52:46.784176: step 15888, loss 0.579518.
Train: 2018-08-01T23:52:46.949734: step 15889, loss 0.545285.
Train: 2018-08-01T23:52:47.127229: step 15890, loss 0.528094.
Test: 2018-08-01T23:52:47.655815: step 15890, loss 0.548016.
Train: 2018-08-01T23:52:47.831345: step 15891, loss 0.493633.
Train: 2018-08-01T23:52:47.998899: step 15892, loss 0.458956.
Train: 2018-08-01T23:52:48.161489: step 15893, loss 0.545114.
Train: 2018-08-01T23:52:48.323058: step 15894, loss 0.579808.
Train: 2018-08-01T23:52:48.490616: step 15895, loss 0.457875.
Train: 2018-08-01T23:52:48.665141: step 15896, loss 0.56246.
Train: 2018-08-01T23:52:48.833667: step 15897, loss 0.474667.
Train: 2018-08-01T23:52:49.000253: step 15898, loss 0.59778.
Train: 2018-08-01T23:52:49.168771: step 15899, loss 0.615616.
Train: 2018-08-01T23:52:49.333356: step 15900, loss 0.527072.
Test: 2018-08-01T23:52:49.878872: step 15900, loss 0.547675.
Train: 2018-08-01T23:52:50.634586: step 15901, loss 0.4559.
Train: 2018-08-01T23:52:50.798151: step 15902, loss 0.580428.
Train: 2018-08-01T23:52:50.963676: step 15903, loss 0.652037.
Train: 2018-08-01T23:52:51.129233: step 15904, loss 0.49101.
Train: 2018-08-01T23:52:51.296784: step 15905, loss 0.598519.
Train: 2018-08-01T23:52:51.469324: step 15906, loss 0.562657.
Train: 2018-08-01T23:52:51.633885: step 15907, loss 0.580634.
Train: 2018-08-01T23:52:51.800473: step 15908, loss 0.490769.
Train: 2018-08-01T23:52:51.979990: step 15909, loss 0.526693.
Train: 2018-08-01T23:52:52.146544: step 15910, loss 0.616747.
Test: 2018-08-01T23:52:52.674152: step 15910, loss 0.547607.
Train: 2018-08-01T23:52:52.838663: step 15911, loss 0.688859.
Train: 2018-08-01T23:52:53.007212: step 15912, loss 0.652667.
Train: 2018-08-01T23:52:53.173768: step 15913, loss 0.508802.
Train: 2018-08-01T23:52:53.337336: step 15914, loss 0.580553.
Train: 2018-08-01T23:52:53.507874: step 15915, loss 0.526848.
Train: 2018-08-01T23:52:53.671436: step 15916, loss 0.580458.
Train: 2018-08-01T23:52:53.836995: step 15917, loss 0.544757.
Train: 2018-08-01T23:52:53.999560: step 15918, loss 0.580376.
Train: 2018-08-01T23:52:54.162164: step 15919, loss 0.509233.
Train: 2018-08-01T23:52:54.335692: step 15920, loss 0.633593.
Test: 2018-08-01T23:52:54.876216: step 15920, loss 0.547692.
Train: 2018-08-01T23:52:55.048761: step 15921, loss 0.544809.
Train: 2018-08-01T23:52:55.212348: step 15922, loss 0.580229.
Train: 2018-08-01T23:52:55.378872: step 15923, loss 0.474149.
Train: 2018-08-01T23:52:55.547422: step 15924, loss 0.633172.
Train: 2018-08-01T23:52:55.707993: step 15925, loss 0.562502.
Train: 2018-08-01T23:52:55.888510: step 15926, loss 0.597726.
Train: 2018-08-01T23:52:56.052074: step 15927, loss 0.509726.
Train: 2018-08-01T23:52:56.213640: step 15928, loss 0.52734.
Train: 2018-08-01T23:52:56.390169: step 15929, loss 0.527354.
Train: 2018-08-01T23:52:56.568691: step 15930, loss 0.597592.
Test: 2018-08-01T23:52:57.099274: step 15930, loss 0.547775.
Train: 2018-08-01T23:52:57.265827: step 15931, loss 0.58002.
Train: 2018-08-01T23:52:57.444382: step 15932, loss 0.544931.
Train: 2018-08-01T23:52:57.610906: step 15933, loss 0.562463.
Train: 2018-08-01T23:52:57.780453: step 15934, loss 0.509916.
Train: 2018-08-01T23:52:57.948030: step 15935, loss 0.492394.
Train: 2018-08-01T23:52:58.111568: step 15936, loss 0.527398.
Train: 2018-08-01T23:52:58.276127: step 15937, loss 0.61514.
Train: 2018-08-01T23:52:58.449689: step 15938, loss 0.650287.
Train: 2018-08-01T23:52:58.619210: step 15939, loss 0.527376.
Train: 2018-08-01T23:52:58.790752: step 15940, loss 0.597546.
Test: 2018-08-01T23:52:59.313355: step 15940, loss 0.547788.
Train: 2018-08-01T23:52:59.485924: step 15941, loss 0.544939.
Train: 2018-08-01T23:52:59.650454: step 15942, loss 0.457386.
Train: 2018-08-01T23:52:59.817039: step 15943, loss 0.597513.
Train: 2018-08-01T23:52:59.982566: step 15944, loss 0.562463.
Train: 2018-08-01T23:53:00.151115: step 15945, loss 0.562464.
Train: 2018-08-01T23:53:00.316704: step 15946, loss 0.527405.
Train: 2018-08-01T23:53:00.488245: step 15947, loss 0.562465.
Train: 2018-08-01T23:53:00.660753: step 15948, loss 0.685252.
Train: 2018-08-01T23:53:00.829302: step 15949, loss 0.562459.
Train: 2018-08-01T23:53:00.997852: step 15950, loss 0.562453.
Test: 2018-08-01T23:53:01.527435: step 15950, loss 0.547819.
Train: 2018-08-01T23:53:01.694989: step 15951, loss 0.527509.
Train: 2018-08-01T23:53:01.867557: step 15952, loss 0.544988.
Train: 2018-08-01T23:53:02.031089: step 15953, loss 0.543832.
Train: 2018-08-01T23:53:02.199638: step 15954, loss 0.49267.
Train: 2018-08-01T23:53:02.362229: step 15955, loss 0.579898.
Train: 2018-08-01T23:53:02.534743: step 15956, loss 0.440226.
Train: 2018-08-01T23:53:02.697339: step 15957, loss 0.492483.
Train: 2018-08-01T23:53:02.865889: step 15958, loss 0.544928.
Train: 2018-08-01T23:53:03.034441: step 15959, loss 0.492157.
Train: 2018-08-01T23:53:03.208972: step 15960, loss 0.49196.
Test: 2018-08-01T23:53:03.738525: step 15960, loss 0.547701.
Train: 2018-08-01T23:53:03.917048: step 15961, loss 0.527126.
Train: 2018-08-01T23:53:04.079613: step 15962, loss 0.562549.
Train: 2018-08-01T23:53:04.247166: step 15963, loss 0.580391.
Train: 2018-08-01T23:53:04.412723: step 15964, loss 0.580458.
Train: 2018-08-01T23:53:04.591271: step 15965, loss 0.544723.
Train: 2018-08-01T23:53:04.765801: step 15966, loss 0.562634.
Train: 2018-08-01T23:53:04.937320: step 15967, loss 0.616501.
Train: 2018-08-01T23:53:05.111861: step 15968, loss 0.616532.
Train: 2018-08-01T23:53:05.274418: step 15969, loss 0.598554.
Train: 2018-08-01T23:53:05.439976: step 15970, loss 0.508834.
Test: 2018-08-01T23:53:05.982527: step 15970, loss 0.547622.
Train: 2018-08-01T23:53:06.147086: step 15971, loss 0.526777.
Train: 2018-08-01T23:53:06.308654: step 15972, loss 0.472984.
Train: 2018-08-01T23:53:06.473224: step 15973, loss 0.419052.
Train: 2018-08-01T23:53:06.641780: step 15974, loss 0.49069.
Train: 2018-08-01T23:53:06.801362: step 15975, loss 0.526608.
Train: 2018-08-01T23:53:06.977865: step 15976, loss 0.653301.
Train: 2018-08-01T23:53:07.139461: step 15977, loss 0.47209.
Train: 2018-08-01T23:53:07.304991: step 15978, loss 0.526448.
Train: 2018-08-01T23:53:07.471577: step 15979, loss 0.562839.
Train: 2018-08-01T23:53:07.638101: step 15980, loss 0.526354.
Test: 2018-08-01T23:53:08.168681: step 15980, loss 0.54758.
Train: 2018-08-01T23:53:08.337231: step 15981, loss 0.489727.
Train: 2018-08-01T23:53:08.501821: step 15982, loss 0.544599.
Train: 2018-08-01T23:53:08.664358: step 15983, loss 0.618116.
Train: 2018-08-01T23:53:08.830942: step 15984, loss 0.562995.
Train: 2018-08-01T23:53:08.997466: step 15985, loss 0.599844.
Train: 2018-08-01T23:53:09.165020: step 15986, loss 0.489337.
Train: 2018-08-01T23:53:09.343566: step 15987, loss 0.59989.
Train: 2018-08-01T23:53:09.508102: step 15988, loss 0.526159.
Train: 2018-08-01T23:53:09.676682: step 15989, loss 0.599905.
Train: 2018-08-01T23:53:09.844229: step 15990, loss 0.544592.
Test: 2018-08-01T23:53:10.384757: step 15990, loss 0.547588.
Train: 2018-08-01T23:53:10.551343: step 15991, loss 0.563015.
Train: 2018-08-01T23:53:10.716870: step 15992, loss 0.747133.
Train: 2018-08-01T23:53:10.892432: step 15993, loss 0.507879.
Train: 2018-08-01T23:53:11.059952: step 15994, loss 0.544601.
Train: 2018-08-01T23:53:11.221521: step 15995, loss 0.526323.
Train: 2018-08-01T23:53:11.387109: step 15996, loss 0.599378.
Train: 2018-08-01T23:53:11.548646: step 15997, loss 0.508177.
Train: 2018-08-01T23:53:11.713206: step 15998, loss 0.617406.
Train: 2018-08-01T23:53:11.874775: step 15999, loss 0.708064.
Train: 2018-08-01T23:53:12.041329: step 16000, loss 0.508467.
Test: 2018-08-01T23:53:12.571912: step 16000, loss 0.547598.
Train: 2018-08-01T23:53:13.335850: step 16001, loss 0.616817.
Train: 2018-08-01T23:53:13.505425: step 16002, loss 0.562662.
Train: 2018-08-01T23:53:13.677935: step 16003, loss 0.580544.
Train: 2018-08-01T23:53:13.845518: step 16004, loss 0.580452.
Train: 2018-08-01T23:53:14.015059: step 16005, loss 0.580362.
Train: 2018-08-01T23:53:14.179624: step 16006, loss 0.509321.
Train: 2018-08-01T23:53:14.341187: step 16007, loss 0.562517.
Train: 2018-08-01T23:53:14.507717: step 16008, loss 0.509539.
Train: 2018-08-01T23:53:14.677288: step 16009, loss 0.58012.
Train: 2018-08-01T23:53:14.851825: step 16010, loss 0.562482.
Test: 2018-08-01T23:53:15.384373: step 16010, loss 0.547758.
Train: 2018-08-01T23:53:15.556952: step 16011, loss 0.615188.
Train: 2018-08-01T23:53:15.728454: step 16012, loss 0.562461.
Train: 2018-08-01T23:53:15.904014: step 16013, loss 0.562451.
Train: 2018-08-01T23:53:16.075550: step 16014, loss 0.614837.
Train: 2018-08-01T23:53:16.239088: step 16015, loss 0.63212.
Train: 2018-08-01T23:53:16.405642: step 16016, loss 0.597154.
Train: 2018-08-01T23:53:16.572228: step 16017, loss 0.614333.
Train: 2018-08-01T23:53:16.736783: step 16018, loss 0.700327.
Train: 2018-08-01T23:53:16.908330: step 16019, loss 0.699583.
Train: 2018-08-01T23:53:17.069867: step 16020, loss 0.494288.
Test: 2018-08-01T23:53:17.588480: step 16020, loss 0.548235.
Train: 2018-08-01T23:53:17.753074: step 16021, loss 0.596311.
Train: 2018-08-01T23:53:17.929596: step 16022, loss 0.562441.
Train: 2018-08-01T23:53:18.096124: step 16023, loss 0.629583.
Train: 2018-08-01T23:53:18.271679: step 16024, loss 0.562488.
Train: 2018-08-01T23:53:18.439239: step 16025, loss 0.629004.
Train: 2018-08-01T23:53:18.605786: step 16026, loss 0.496393.
Train: 2018-08-01T23:53:18.771343: step 16027, loss 0.529611.
Train: 2018-08-01T23:53:18.939893: step 16028, loss 0.579047.
Train: 2018-08-01T23:53:19.107420: step 16029, loss 0.611848.
Train: 2018-08-01T23:53:19.271008: step 16030, loss 0.660839.
Test: 2018-08-01T23:53:19.800567: step 16030, loss 0.549035.
Train: 2018-08-01T23:53:19.967153: step 16031, loss 0.595296.
Train: 2018-08-01T23:53:20.130685: step 16032, loss 0.497756.
Train: 2018-08-01T23:53:20.294278: step 16033, loss 0.627588.
Train: 2018-08-01T23:53:20.459830: step 16034, loss 0.530463.
Train: 2018-08-01T23:53:20.627357: step 16035, loss 0.659617.
Train: 2018-08-01T23:53:20.792948: step 16036, loss 0.514591.
Train: 2018-08-01T23:53:20.956477: step 16037, loss 0.514695.
Train: 2018-08-01T23:53:21.122058: step 16038, loss 0.54683.
Train: 2018-08-01T23:53:21.287617: step 16039, loss 0.514728.
Train: 2018-08-01T23:53:21.465118: step 16040, loss 0.611068.
Test: 2018-08-01T23:53:22.004676: step 16040, loss 0.549404.
Train: 2018-08-01T23:53:22.170232: step 16041, loss 0.514646.
Train: 2018-08-01T23:53:22.333821: step 16042, loss 0.562845.
Train: 2018-08-01T23:53:22.499352: step 16043, loss 0.466137.
Train: 2018-08-01T23:53:22.666905: step 16044, loss 0.546627.
Train: 2018-08-01T23:53:22.845458: step 16045, loss 0.627603.
Train: 2018-08-01T23:53:23.016019: step 16046, loss 0.676441.
Train: 2018-08-01T23:53:23.182551: step 16047, loss 0.562726.
Train: 2018-08-01T23:53:23.347087: step 16048, loss 0.578975.
Train: 2018-08-01T23:53:23.512643: step 16049, loss 0.676517.
Train: 2018-08-01T23:53:23.685214: step 16050, loss 0.57897.
Test: 2018-08-01T23:53:24.224754: step 16050, loss 0.549174.
Train: 2018-08-01T23:53:24.391295: step 16051, loss 0.595177.
Train: 2018-08-01T23:53:24.559850: step 16052, loss 0.498014.
Train: 2018-08-01T23:53:24.738398: step 16053, loss 0.530398.
Train: 2018-08-01T23:53:24.902927: step 16054, loss 0.530373.
Train: 2018-08-01T23:53:25.085440: step 16055, loss 0.546535.
Train: 2018-08-01T23:53:25.249043: step 16056, loss 0.562732.
Train: 2018-08-01T23:53:25.414590: step 16057, loss 0.562714.
Train: 2018-08-01T23:53:25.581145: step 16058, loss 0.578986.
Train: 2018-08-01T23:53:25.747670: step 16059, loss 0.546371.
Train: 2018-08-01T23:53:25.912229: step 16060, loss 0.59534.
Test: 2018-08-01T23:53:26.448802: step 16060, loss 0.548958.
Train: 2018-08-01T23:53:26.615377: step 16061, loss 0.546298.
Train: 2018-08-01T23:53:26.782934: step 16062, loss 0.497126.
Train: 2018-08-01T23:53:26.949487: step 16063, loss 0.562616.
Train: 2018-08-01T23:53:27.119034: step 16064, loss 0.562595.
Train: 2018-08-01T23:53:27.283565: step 16065, loss 0.546084.
Train: 2018-08-01T23:53:27.449152: step 16066, loss 0.61215.
Train: 2018-08-01T23:53:27.625677: step 16067, loss 0.612212.
Train: 2018-08-01T23:53:27.792228: step 16068, loss 0.612242.
Train: 2018-08-01T23:53:27.956810: step 16069, loss 0.562539.
Train: 2018-08-01T23:53:28.125362: step 16070, loss 0.529397.
Test: 2018-08-01T23:53:28.639938: step 16070, loss 0.548647.
Train: 2018-08-01T23:53:28.804528: step 16071, loss 0.562532.
Train: 2018-08-01T23:53:28.970106: step 16072, loss 0.512743.
Train: 2018-08-01T23:53:29.133646: step 16073, loss 0.479415.
Train: 2018-08-01T23:53:29.301169: step 16074, loss 0.529167.
Train: 2018-08-01T23:53:29.465729: step 16075, loss 0.529048.
Train: 2018-08-01T23:53:29.632309: step 16076, loss 0.663088.
Train: 2018-08-01T23:53:29.793853: step 16077, loss 0.478469.
Train: 2018-08-01T23:53:29.959410: step 16078, loss 0.495073.
Train: 2018-08-01T23:53:30.131979: step 16079, loss 0.56243.
Train: 2018-08-01T23:53:30.298504: step 16080, loss 0.477653.
Test: 2018-08-01T23:53:30.827135: step 16080, loss 0.548157.
Train: 2018-08-01T23:53:30.993676: step 16081, loss 0.579434.
Train: 2018-08-01T23:53:31.162196: step 16082, loss 0.579487.
Train: 2018-08-01T23:53:31.332769: step 16083, loss 0.528146.
Train: 2018-08-01T23:53:31.498295: step 16084, loss 0.493678.
Train: 2018-08-01T23:53:31.668840: step 16085, loss 0.683122.
Train: 2018-08-01T23:53:31.834398: step 16086, loss 0.579682.
Train: 2018-08-01T23:53:32.000982: step 16087, loss 0.631585.
Train: 2018-08-01T23:53:32.168505: step 16088, loss 0.579707.
Train: 2018-08-01T23:53:32.331128: step 16089, loss 0.545119.
Train: 2018-08-01T23:53:32.510615: step 16090, loss 0.493239.
Test: 2018-08-01T23:53:33.049150: step 16090, loss 0.547918.
Train: 2018-08-01T23:53:33.214739: step 16091, loss 0.579723.
Train: 2018-08-01T23:53:33.385278: step 16092, loss 0.545094.
Train: 2018-08-01T23:53:33.546852: step 16093, loss 0.527749.
Train: 2018-08-01T23:53:33.712378: step 16094, loss 0.562419.
Train: 2018-08-01T23:53:33.875941: step 16095, loss 0.545051.
Train: 2018-08-01T23:53:34.042495: step 16096, loss 0.545036.
Train: 2018-08-01T23:53:34.206088: step 16097, loss 0.632065.
Train: 2018-08-01T23:53:34.371616: step 16098, loss 0.562429.
Train: 2018-08-01T23:53:34.534181: step 16099, loss 0.632069.
Train: 2018-08-01T23:53:34.697744: step 16100, loss 0.631997.
Test: 2018-08-01T23:53:35.230319: step 16100, loss 0.547883.
Train: 2018-08-01T23:53:36.092797: step 16101, loss 0.475619.
Train: 2018-08-01T23:53:36.261347: step 16102, loss 0.493016.
Train: 2018-08-01T23:53:36.422915: step 16103, loss 0.510346.
Train: 2018-08-01T23:53:36.585509: step 16104, loss 0.52767.
Train: 2018-08-01T23:53:36.755028: step 16105, loss 0.632027.
Train: 2018-08-01T23:53:36.924574: step 16106, loss 0.492809.
Train: 2018-08-01T23:53:37.088161: step 16107, loss 0.562431.
Train: 2018-08-01T23:53:37.256686: step 16108, loss 0.510112.
Train: 2018-08-01T23:53:37.418284: step 16109, loss 0.544973.
Train: 2018-08-01T23:53:37.579852: step 16110, loss 0.579942.
Test: 2018-08-01T23:53:38.118382: step 16110, loss 0.547787.
Train: 2018-08-01T23:53:38.283965: step 16111, loss 0.492397.
Train: 2018-08-01T23:53:38.449530: step 16112, loss 0.562462.
Train: 2018-08-01T23:53:38.614079: step 16113, loss 0.650344.
Train: 2018-08-01T23:53:38.780642: step 16114, loss 0.650362.
Train: 2018-08-01T23:53:38.951187: step 16115, loss 0.527347.
Train: 2018-08-01T23:53:39.115716: step 16116, loss 0.544913.
Train: 2018-08-01T23:53:39.289253: step 16117, loss 0.562461.
Train: 2018-08-01T23:53:39.456804: step 16118, loss 0.527386.
Train: 2018-08-01T23:53:39.621390: step 16119, loss 0.492309.
Train: 2018-08-01T23:53:39.791934: step 16120, loss 0.54491.
Test: 2018-08-01T23:53:40.319499: step 16120, loss 0.547755.
Train: 2018-08-01T23:53:40.491071: step 16121, loss 0.650325.
Train: 2018-08-01T23:53:40.656628: step 16122, loss 0.667856.
Train: 2018-08-01T23:53:40.824180: step 16123, loss 0.615057.
Train: 2018-08-01T23:53:41.007659: step 16124, loss 0.492481.
Train: 2018-08-01T23:53:41.178203: step 16125, loss 0.544972.
Train: 2018-08-01T23:53:41.347776: step 16126, loss 0.614792.
Train: 2018-08-01T23:53:41.518293: step 16127, loss 0.632124.
Train: 2018-08-01T23:53:41.681856: step 16128, loss 0.545042.
Train: 2018-08-01T23:53:41.848411: step 16129, loss 0.475692.
Train: 2018-08-01T23:53:42.023973: step 16130, loss 0.545082.
Test: 2018-08-01T23:53:42.559509: step 16130, loss 0.547906.
Train: 2018-08-01T23:53:42.726065: step 16131, loss 0.49311.
Train: 2018-08-01T23:53:42.890626: step 16132, loss 0.475737.
Train: 2018-08-01T23:53:43.055184: step 16133, loss 0.579784.
Train: 2018-08-01T23:53:43.227724: step 16134, loss 0.475496.
Train: 2018-08-01T23:53:43.392284: step 16135, loss 0.545006.
Train: 2018-08-01T23:53:43.558838: step 16136, loss 0.510052.
Train: 2018-08-01T23:53:43.728385: step 16137, loss 0.527434.
Train: 2018-08-01T23:53:43.892945: step 16138, loss 0.562464.
Train: 2018-08-01T23:53:44.056508: step 16139, loss 0.562478.
Train: 2018-08-01T23:53:44.220096: step 16140, loss 0.615398.
Test: 2018-08-01T23:53:44.757665: step 16140, loss 0.547714.
Train: 2018-08-01T23:53:44.931198: step 16141, loss 0.597808.
Train: 2018-08-01T23:53:45.095731: step 16142, loss 0.650812.
Train: 2018-08-01T23:53:45.260291: step 16143, loss 0.580141.
Train: 2018-08-01T23:53:45.432829: step 16144, loss 0.527234.
Train: 2018-08-01T23:53:45.601379: step 16145, loss 0.597716.
Train: 2018-08-01T23:53:45.773916: step 16146, loss 0.474495.
Train: 2018-08-01T23:53:45.940503: step 16147, loss 0.492085.
Train: 2018-08-01T23:53:46.107028: step 16148, loss 0.615327.
Train: 2018-08-01T23:53:46.271620: step 16149, loss 0.49202.
Train: 2018-08-01T23:53:46.436180: step 16150, loss 0.580121.
Test: 2018-08-01T23:53:46.976702: step 16150, loss 0.54772.
Train: 2018-08-01T23:53:47.142259: step 16151, loss 0.633056.
Train: 2018-08-01T23:53:47.307841: step 16152, loss 0.562489.
Train: 2018-08-01T23:53:47.473374: step 16153, loss 0.615345.
Train: 2018-08-01T23:53:47.636936: step 16154, loss 0.49209.
Train: 2018-08-01T23:53:47.803491: step 16155, loss 0.597657.
Train: 2018-08-01T23:53:47.979044: step 16156, loss 0.509742.
Train: 2018-08-01T23:53:48.146603: step 16157, loss 0.544896.
Train: 2018-08-01T23:53:48.308144: step 16158, loss 0.597618.
Train: 2018-08-01T23:53:48.471736: step 16159, loss 0.562467.
Train: 2018-08-01T23:53:48.638259: step 16160, loss 0.562464.
Test: 2018-08-01T23:53:49.173834: step 16160, loss 0.547768.
Train: 2018-08-01T23:53:49.337395: step 16161, loss 0.650192.
Train: 2018-08-01T23:53:49.504974: step 16162, loss 0.632514.
Train: 2018-08-01T23:53:49.669530: step 16163, loss 0.597381.
Train: 2018-08-01T23:53:49.832068: step 16164, loss 0.457907.
Train: 2018-08-01T23:53:49.993667: step 16165, loss 0.458029.
Train: 2018-08-01T23:53:50.160217: step 16166, loss 0.510213.
Train: 2018-08-01T23:53:50.322788: step 16167, loss 0.527588.
Train: 2018-08-01T23:53:50.492305: step 16168, loss 0.527548.
Train: 2018-08-01T23:53:50.657887: step 16169, loss 0.64979.
Train: 2018-08-01T23:53:50.821455: step 16170, loss 0.510022.
Test: 2018-08-01T23:53:51.360993: step 16170, loss 0.547801.
Train: 2018-08-01T23:53:51.526539: step 16171, loss 0.544957.
Train: 2018-08-01T23:53:51.694125: step 16172, loss 0.597453.
Train: 2018-08-01T23:53:51.855660: step 16173, loss 0.614971.
Train: 2018-08-01T23:53:52.022215: step 16174, loss 0.544949.
Train: 2018-08-01T23:53:52.186805: step 16175, loss 0.562446.
Train: 2018-08-01T23:53:52.352331: step 16176, loss 0.527471.
Train: 2018-08-01T23:53:52.516924: step 16177, loss 0.544956.
Train: 2018-08-01T23:53:52.682474: step 16178, loss 0.579939.
Train: 2018-08-01T23:53:52.846040: step 16179, loss 0.562446.
Train: 2018-08-01T23:53:53.010571: step 16180, loss 0.562445.
Test: 2018-08-01T23:53:53.546139: step 16180, loss 0.5478.
Train: 2018-08-01T23:53:53.713692: step 16181, loss 0.527468.
Train: 2018-08-01T23:53:53.881245: step 16182, loss 0.579939.
Train: 2018-08-01T23:53:54.041834: step 16183, loss 0.562445.
Train: 2018-08-01T23:53:54.212390: step 16184, loss 0.614916.
Train: 2018-08-01T23:53:54.376950: step 16185, loss 0.49254.
Train: 2018-08-01T23:53:54.542510: step 16186, loss 0.492533.
Train: 2018-08-01T23:53:54.709057: step 16187, loss 0.544951.
Train: 2018-08-01T23:53:54.872595: step 16188, loss 0.492398.
Train: 2018-08-01T23:53:55.038152: step 16189, loss 0.527369.
Train: 2018-08-01T23:53:55.204739: step 16190, loss 0.597635.
Test: 2018-08-01T23:53:55.735287: step 16190, loss 0.547737.
Train: 2018-08-01T23:53:55.898881: step 16191, loss 0.580083.
Train: 2018-08-01T23:53:56.061416: step 16192, loss 0.527245.
Train: 2018-08-01T23:53:56.223010: step 16193, loss 0.562491.
Train: 2018-08-01T23:53:56.386547: step 16194, loss 0.491871.
Train: 2018-08-01T23:53:56.552129: step 16195, loss 0.544823.
Train: 2018-08-01T23:53:56.713699: step 16196, loss 0.597955.
Train: 2018-08-01T23:53:56.881255: step 16197, loss 0.722129.
Train: 2018-08-01T23:53:57.050804: step 16198, loss 0.668773.
Train: 2018-08-01T23:53:57.224338: step 16199, loss 0.615481.
Train: 2018-08-01T23:53:57.396847: step 16200, loss 0.492067.
Test: 2018-08-01T23:53:57.926430: step 16200, loss 0.547757.
Train: 2018-08-01T23:53:58.753909: step 16201, loss 0.562466.
Train: 2018-08-01T23:53:58.934401: step 16202, loss 0.50986.
Train: 2018-08-01T23:53:59.095006: step 16203, loss 0.614987.
Train: 2018-08-01T23:53:59.257566: step 16204, loss 0.649847.
Train: 2018-08-01T23:53:59.422123: step 16205, loss 0.510137.
Train: 2018-08-01T23:53:59.589681: step 16206, loss 0.684209.
Train: 2018-08-01T23:53:59.755207: step 16207, loss 0.597091.
Train: 2018-08-01T23:53:59.924787: step 16208, loss 0.596958.
Train: 2018-08-01T23:54:00.103305: step 16209, loss 0.545192.
Train: 2018-08-01T23:54:00.266871: step 16210, loss 0.596714.
Test: 2018-08-01T23:54:00.799442: step 16210, loss 0.548085.
Train: 2018-08-01T23:54:00.970982: step 16211, loss 0.5966.
Train: 2018-08-01T23:54:01.137512: step 16212, loss 0.596485.
Train: 2018-08-01T23:54:01.302072: step 16213, loss 0.511483.
Train: 2018-08-01T23:54:01.473614: step 16214, loss 0.494682.
Train: 2018-08-01T23:54:01.640169: step 16215, loss 0.528597.
Train: 2018-08-01T23:54:01.805754: step 16216, loss 0.528619.
Train: 2018-08-01T23:54:01.982254: step 16217, loss 0.646947.
Train: 2018-08-01T23:54:02.148841: step 16218, loss 0.56243.
Train: 2018-08-01T23:54:02.314397: step 16219, loss 0.511831.
Train: 2018-08-01T23:54:02.501866: step 16220, loss 0.613031.
Test: 2018-08-01T23:54:03.040455: step 16220, loss 0.548325.
Train: 2018-08-01T23:54:03.206007: step 16221, loss 0.528732.
Train: 2018-08-01T23:54:03.370573: step 16222, loss 0.461338.
Train: 2018-08-01T23:54:03.541118: step 16223, loss 0.59618.
Train: 2018-08-01T23:54:03.704682: step 16224, loss 0.545542.
Train: 2018-08-01T23:54:03.873229: step 16225, loss 0.460997.
Train: 2018-08-01T23:54:04.040768: step 16226, loss 0.613257.
Train: 2018-08-01T23:54:04.213322: step 16227, loss 0.5115.
Train: 2018-08-01T23:54:04.379845: step 16228, loss 0.545402.
Train: 2018-08-01T23:54:04.552384: step 16229, loss 0.511272.
Train: 2018-08-01T23:54:04.724923: step 16230, loss 0.613675.
Test: 2018-08-01T23:54:05.253537: step 16230, loss 0.548066.
Train: 2018-08-01T23:54:05.418069: step 16231, loss 0.528162.
Train: 2018-08-01T23:54:05.587638: step 16232, loss 0.459473.
Train: 2018-08-01T23:54:05.764175: step 16233, loss 0.648457.
Train: 2018-08-01T23:54:05.926740: step 16234, loss 0.476202.
Train: 2018-08-01T23:54:06.092267: step 16235, loss 0.579696.
Train: 2018-08-01T23:54:06.257849: step 16236, loss 0.562412.
Train: 2018-08-01T23:54:06.420389: step 16237, loss 0.614494.
Train: 2018-08-01T23:54:06.580991: step 16238, loss 0.614548.
Train: 2018-08-01T23:54:06.745520: step 16239, loss 0.684067.
Train: 2018-08-01T23:54:06.914103: step 16240, loss 0.649169.
Test: 2018-08-01T23:54:07.447644: step 16240, loss 0.547919.
Train: 2018-08-01T23:54:07.625216: step 16241, loss 0.56241.
Train: 2018-08-01T23:54:07.791725: step 16242, loss 0.476094.
Train: 2018-08-01T23:54:07.955287: step 16243, loss 0.631384.
Train: 2018-08-01T23:54:08.125862: step 16244, loss 0.631252.
Train: 2018-08-01T23:54:08.304384: step 16245, loss 0.545235.
Train: 2018-08-01T23:54:08.470939: step 16246, loss 0.545272.
Train: 2018-08-01T23:54:08.635493: step 16247, loss 0.579502.
Train: 2018-08-01T23:54:08.800028: step 16248, loss 0.528265.
Train: 2018-08-01T23:54:08.977553: step 16249, loss 0.579456.
Train: 2018-08-01T23:54:09.145106: step 16250, loss 0.511318.
Test: 2018-08-01T23:54:09.691675: step 16250, loss 0.548152.
Train: 2018-08-01T23:54:09.857238: step 16251, loss 0.511336.
Train: 2018-08-01T23:54:10.025751: step 16252, loss 0.613502.
Train: 2018-08-01T23:54:10.192306: step 16253, loss 0.545381.
Train: 2018-08-01T23:54:10.361853: step 16254, loss 0.635053.
Train: 2018-08-01T23:54:10.528408: step 16255, loss 0.613437.
Train: 2018-08-01T23:54:10.692968: step 16256, loss 0.596376.
Train: 2018-08-01T23:54:10.863512: step 16257, loss 0.562418.
Train: 2018-08-01T23:54:11.025080: step 16258, loss 0.528582.
Train: 2018-08-01T23:54:11.197644: step 16259, loss 0.596233.
Train: 2018-08-01T23:54:11.360214: step 16260, loss 0.478026.
Test: 2018-08-01T23:54:11.901737: step 16260, loss 0.548293.
Train: 2018-08-01T23:54:12.066330: step 16261, loss 0.613077.
Train: 2018-08-01T23:54:12.234871: step 16262, loss 0.545561.
Train: 2018-08-01T23:54:12.415394: step 16263, loss 0.478096.
Train: 2018-08-01T23:54:12.577960: step 16264, loss 0.613085.
Train: 2018-08-01T23:54:12.740519: step 16265, loss 0.494869.
Train: 2018-08-01T23:54:12.915052: step 16266, loss 0.477868.
Train: 2018-08-01T23:54:13.079616: step 16267, loss 0.562417.
Train: 2018-08-01T23:54:13.247173: step 16268, loss 0.426496.
Train: 2018-08-01T23:54:13.409736: step 16269, loss 0.579461.
Train: 2018-08-01T23:54:13.577282: step 16270, loss 0.545288.
Test: 2018-08-01T23:54:14.113823: step 16270, loss 0.548024.
Train: 2018-08-01T23:54:14.278414: step 16271, loss 0.545232.
Train: 2018-08-01T23:54:14.442944: step 16272, loss 0.562403.
Train: 2018-08-01T23:54:14.618473: step 16273, loss 0.527864.
Train: 2018-08-01T23:54:14.785059: step 16274, loss 0.614378.
Train: 2018-08-01T23:54:14.952612: step 16275, loss 0.458282.
Train: 2018-08-01T23:54:15.125120: step 16276, loss 0.492786.
Train: 2018-08-01T23:54:15.295664: step 16277, loss 0.544965.
Train: 2018-08-01T23:54:15.462219: step 16278, loss 0.579991.
Train: 2018-08-01T23:54:15.637748: step 16279, loss 0.667983.
Train: 2018-08-01T23:54:15.802309: step 16280, loss 0.509666.
Test: 2018-08-01T23:54:16.326908: step 16280, loss 0.547723.
Train: 2018-08-01T23:54:16.492464: step 16281, loss 0.509592.
Train: 2018-08-01T23:54:16.666001: step 16282, loss 0.42116.
Train: 2018-08-01T23:54:16.830561: step 16283, loss 0.580258.
Train: 2018-08-01T23:54:16.998139: step 16284, loss 0.56255.
Train: 2018-08-01T23:54:17.161675: step 16285, loss 0.491265.
Train: 2018-08-01T23:54:17.329256: step 16286, loss 0.580482.
Train: 2018-08-01T23:54:17.501766: step 16287, loss 0.544701.
Train: 2018-08-01T23:54:17.667324: step 16288, loss 0.598576.
Train: 2018-08-01T23:54:17.841888: step 16289, loss 0.544676.
Train: 2018-08-01T23:54:18.008413: step 16290, loss 0.472615.
Test: 2018-08-01T23:54:18.555961: step 16290, loss 0.547589.
Train: 2018-08-01T23:54:18.729510: step 16291, loss 0.544654.
Train: 2018-08-01T23:54:18.899031: step 16292, loss 0.580822.
Train: 2018-08-01T23:54:19.065586: step 16293, loss 0.562751.
Train: 2018-08-01T23:54:19.230177: step 16294, loss 0.617178.
Train: 2018-08-01T23:54:19.395728: step 16295, loss 0.417646.
Train: 2018-08-01T23:54:19.561261: step 16296, loss 0.526447.
Train: 2018-08-01T23:54:19.726819: step 16297, loss 0.544613.
Train: 2018-08-01T23:54:19.892401: step 16298, loss 0.526367.
Train: 2018-08-01T23:54:20.056960: step 16299, loss 0.508053.
Train: 2018-08-01T23:54:20.225526: step 16300, loss 0.581221.
Test: 2018-08-01T23:54:20.761056: step 16300, loss 0.547574.
Train: 2018-08-01T23:54:21.542377: step 16301, loss 0.599614.
Train: 2018-08-01T23:54:21.708983: step 16302, loss 0.654712.
Train: 2018-08-01T23:54:21.873524: step 16303, loss 0.562933.
Train: 2018-08-01T23:54:22.040048: step 16304, loss 0.691198.
Train: 2018-08-01T23:54:22.210626: step 16305, loss 0.562879.
Train: 2018-08-01T23:54:22.376180: step 16306, loss 0.599309.
Train: 2018-08-01T23:54:22.539743: step 16307, loss 0.599163.
Train: 2018-08-01T23:54:22.703306: step 16308, loss 0.562756.
Train: 2018-08-01T23:54:22.868862: step 16309, loss 0.526581.
Train: 2018-08-01T23:54:23.034389: step 16310, loss 0.508619.
Test: 2018-08-01T23:54:23.577937: step 16310, loss 0.5476.
Train: 2018-08-01T23:54:23.745520: step 16311, loss 0.580658.
Train: 2018-08-01T23:54:23.912073: step 16312, loss 0.490815.
Train: 2018-08-01T23:54:24.075637: step 16313, loss 0.688219.
Train: 2018-08-01T23:54:24.254154: step 16314, loss 0.598396.
Train: 2018-08-01T23:54:24.419717: step 16315, loss 0.544738.
Train: 2018-08-01T23:54:24.584246: step 16316, loss 0.437983.
Train: 2018-08-01T23:54:24.745814: step 16317, loss 0.615897.
Train: 2018-08-01T23:54:24.910375: step 16318, loss 0.580293.
Train: 2018-08-01T23:54:25.075943: step 16319, loss 0.527073.
Train: 2018-08-01T23:54:25.241507: step 16320, loss 0.527105.
Test: 2018-08-01T23:54:25.778055: step 16320, loss 0.547695.
Train: 2018-08-01T23:54:25.941618: step 16321, loss 0.580203.
Train: 2018-08-01T23:54:26.105212: step 16322, loss 0.491799.
Train: 2018-08-01T23:54:26.267745: step 16323, loss 0.58018.
Train: 2018-08-01T23:54:26.435311: step 16324, loss 0.49182.
Train: 2018-08-01T23:54:26.600880: step 16325, loss 0.474105.
Train: 2018-08-01T23:54:26.763445: step 16326, loss 0.668763.
Train: 2018-08-01T23:54:26.925020: step 16327, loss 0.597928.
Train: 2018-08-01T23:54:27.088584: step 16328, loss 0.527121.
Train: 2018-08-01T23:54:27.254135: step 16329, loss 0.544818.
Train: 2018-08-01T23:54:27.420663: step 16330, loss 0.562509.
Test: 2018-08-01T23:54:27.954238: step 16330, loss 0.547697.
Train: 2018-08-01T23:54:28.121815: step 16331, loss 0.54482.
Train: 2018-08-01T23:54:28.285377: step 16332, loss 0.562508.
Train: 2018-08-01T23:54:28.448946: step 16333, loss 0.509448.
Train: 2018-08-01T23:54:28.621484: step 16334, loss 0.544815.
Train: 2018-08-01T23:54:28.792029: step 16335, loss 0.527102.
Train: 2018-08-01T23:54:28.958553: step 16336, loss 0.562523.
Train: 2018-08-01T23:54:29.123137: step 16337, loss 0.527056.
Train: 2018-08-01T23:54:29.292690: step 16338, loss 0.704571.
Train: 2018-08-01T23:54:29.469230: step 16339, loss 0.456115.
Train: 2018-08-01T23:54:29.634776: step 16340, loss 0.562531.
Test: 2018-08-01T23:54:30.173335: step 16340, loss 0.547673.
Train: 2018-08-01T23:54:30.384898: step 16341, loss 0.722242.
Train: 2018-08-01T23:54:30.550431: step 16342, loss 0.615646.
Train: 2018-08-01T23:54:30.718013: step 16343, loss 0.544835.
Train: 2018-08-01T23:54:30.886566: step 16344, loss 0.562484.
Train: 2018-08-01T23:54:31.051091: step 16345, loss 0.597647.
Train: 2018-08-01T23:54:31.215684: step 16346, loss 0.474732.
Train: 2018-08-01T23:54:31.395172: step 16347, loss 0.650083.
Train: 2018-08-01T23:54:31.558734: step 16348, loss 0.649879.
Train: 2018-08-01T23:54:31.725315: step 16349, loss 0.440413.
Train: 2018-08-01T23:54:31.891844: step 16350, loss 0.527608.
Test: 2018-08-01T23:54:32.432399: step 16350, loss 0.547856.
Train: 2018-08-01T23:54:32.605966: step 16351, loss 0.66679.
Train: 2018-08-01T23:54:32.770526: step 16352, loss 0.59713.
Train: 2018-08-01T23:54:32.934058: step 16353, loss 0.441205.
Train: 2018-08-01T23:54:33.099646: step 16354, loss 0.562408.
Train: 2018-08-01T23:54:33.266204: step 16355, loss 0.562407.
Train: 2018-08-01T23:54:33.431728: step 16356, loss 0.579693.
Train: 2018-08-01T23:54:33.607283: step 16357, loss 0.596954.
Train: 2018-08-01T23:54:33.772847: step 16358, loss 0.596913.
Train: 2018-08-01T23:54:33.935381: step 16359, loss 0.631315.
Train: 2018-08-01T23:54:34.101935: step 16360, loss 0.579588.
Test: 2018-08-01T23:54:34.641493: step 16360, loss 0.548041.
Train: 2018-08-01T23:54:34.822042: step 16361, loss 0.596694.
Train: 2018-08-01T23:54:34.988599: step 16362, loss 0.579503.
Train: 2018-08-01T23:54:35.154123: step 16363, loss 0.562404.
Train: 2018-08-01T23:54:35.317686: step 16364, loss 0.613462.
Train: 2018-08-01T23:54:35.482273: step 16365, loss 0.528472.
Train: 2018-08-01T23:54:35.645834: step 16366, loss 0.477735.
Train: 2018-08-01T23:54:35.810369: step 16367, loss 0.460831.
Train: 2018-08-01T23:54:35.977955: step 16368, loss 0.579533.
Train: 2018-08-01T23:54:36.142482: step 16369, loss 0.5794.
Train: 2018-08-01T23:54:36.308069: step 16370, loss 0.579356.
Test: 2018-08-01T23:54:36.845602: step 16370, loss 0.548205.
Train: 2018-08-01T23:54:37.014183: step 16371, loss 0.732087.
Train: 2018-08-01T23:54:37.179733: step 16372, loss 0.663992.
Train: 2018-08-01T23:54:37.345265: step 16373, loss 0.579302.
Train: 2018-08-01T23:54:37.513814: step 16374, loss 0.528814.
Train: 2018-08-01T23:54:37.679398: step 16375, loss 0.562456.
Train: 2018-08-01T23:54:37.845927: step 16376, loss 0.528988.
Train: 2018-08-01T23:54:38.012482: step 16377, loss 0.662778.
Train: 2018-08-01T23:54:38.182029: step 16378, loss 0.579164.
Train: 2018-08-01T23:54:38.345622: step 16379, loss 0.512609.
Train: 2018-08-01T23:54:38.515165: step 16380, loss 0.628952.
Test: 2018-08-01T23:54:39.044729: step 16380, loss 0.548653.
Train: 2018-08-01T23:54:39.219289: step 16381, loss 0.512812.
Train: 2018-08-01T23:54:39.391827: step 16382, loss 0.579095.
Train: 2018-08-01T23:54:39.555357: step 16383, loss 0.579086.
Train: 2018-08-01T23:54:39.719917: step 16384, loss 0.546039.
Train: 2018-08-01T23:54:39.884478: step 16385, loss 0.579071.
Train: 2018-08-01T23:54:40.050035: step 16386, loss 0.562569.
Train: 2018-08-01T23:54:40.216589: step 16387, loss 0.463648.
Train: 2018-08-01T23:54:40.380183: step 16388, loss 0.612084.
Train: 2018-08-01T23:54:40.544712: step 16389, loss 0.56256.
Train: 2018-08-01T23:54:40.709272: step 16390, loss 0.595601.
Test: 2018-08-01T23:54:41.237860: step 16390, loss 0.548718.
Train: 2018-08-01T23:54:41.401422: step 16391, loss 0.612127.
Train: 2018-08-01T23:54:41.566013: step 16392, loss 0.513015.
Train: 2018-08-01T23:54:41.738522: step 16393, loss 0.61212.
Train: 2018-08-01T23:54:41.907071: step 16394, loss 0.446944.
Train: 2018-08-01T23:54:42.073626: step 16395, loss 0.645261.
Train: 2018-08-01T23:54:42.247193: step 16396, loss 0.430157.
Train: 2018-08-01T23:54:42.412719: step 16397, loss 0.562525.
Train: 2018-08-01T23:54:42.586254: step 16398, loss 0.496008.
Train: 2018-08-01T23:54:42.749843: step 16399, loss 0.545812.
Train: 2018-08-01T23:54:42.915403: step 16400, loss 0.595929.
Test: 2018-08-01T23:54:43.441969: step 16400, loss 0.548415.
Train: 2018-08-01T23:54:44.220116: step 16401, loss 0.512149.
Train: 2018-08-01T23:54:44.384707: step 16402, loss 0.528805.
Train: 2018-08-01T23:54:44.547242: step 16403, loss 0.579304.
Train: 2018-08-01T23:54:44.710835: step 16404, loss 0.427071.
Train: 2018-08-01T23:54:44.882379: step 16405, loss 0.426465.
Train: 2018-08-01T23:54:45.052901: step 16406, loss 0.579495.
Train: 2018-08-01T23:54:45.224456: step 16407, loss 0.510862.
Train: 2018-08-01T23:54:45.389022: step 16408, loss 0.510597.
Train: 2018-08-01T23:54:45.558559: step 16409, loss 0.51033.
Train: 2018-08-01T23:54:45.721130: step 16410, loss 0.597349.
Test: 2018-08-01T23:54:46.240722: step 16410, loss 0.547772.
Train: 2018-08-01T23:54:46.404303: step 16411, loss 0.579988.
Train: 2018-08-01T23:54:46.567871: step 16412, loss 0.580071.
Train: 2018-08-01T23:54:46.734414: step 16413, loss 0.633088.
Train: 2018-08-01T23:54:46.899983: step 16414, loss 0.562504.
Train: 2018-08-01T23:54:47.080500: step 16415, loss 0.580215.
Train: 2018-08-01T23:54:47.252012: step 16416, loss 0.527085.
Train: 2018-08-01T23:54:47.415602: step 16417, loss 0.544791.
Train: 2018-08-01T23:54:47.581162: step 16418, loss 0.562539.
Train: 2018-08-01T23:54:47.742730: step 16419, loss 0.686984.
Train: 2018-08-01T23:54:47.910278: step 16420, loss 0.615824.
Test: 2018-08-01T23:54:48.438839: step 16420, loss 0.547677.
Train: 2018-08-01T23:54:48.607387: step 16421, loss 0.615727.
Train: 2018-08-01T23:54:48.771972: step 16422, loss 0.527122.
Train: 2018-08-01T23:54:48.943527: step 16423, loss 0.544833.
Train: 2018-08-01T23:54:49.112063: step 16424, loss 0.56249.
Train: 2018-08-01T23:54:49.274604: step 16425, loss 0.580101.
Train: 2018-08-01T23:54:49.440193: step 16426, loss 0.544879.
Train: 2018-08-01T23:54:49.617713: step 16427, loss 0.597611.
Train: 2018-08-01T23:54:49.781249: step 16428, loss 0.68527.
Train: 2018-08-01T23:54:49.948827: step 16429, loss 0.544953.
Train: 2018-08-01T23:54:50.113361: step 16430, loss 0.440331.
Test: 2018-08-01T23:54:50.634967: step 16430, loss 0.547834.
Train: 2018-08-01T23:54:50.802520: step 16431, loss 0.492713.
Train: 2018-08-01T23:54:50.967080: step 16432, loss 0.667018.
Train: 2018-08-01T23:54:51.134656: step 16433, loss 0.510198.
Train: 2018-08-01T23:54:51.299223: step 16434, loss 0.597221.
Train: 2018-08-01T23:54:51.462786: step 16435, loss 0.492889.
Train: 2018-08-01T23:54:51.628312: step 16436, loss 0.510271.
Train: 2018-08-01T23:54:51.792872: step 16437, loss 0.562421.
Train: 2018-08-01T23:54:51.965436: step 16438, loss 0.579828.
Train: 2018-08-01T23:54:52.131997: step 16439, loss 0.562424.
Train: 2018-08-01T23:54:52.298551: step 16440, loss 0.492779.
Test: 2018-08-01T23:54:52.820126: step 16440, loss 0.547833.
Train: 2018-08-01T23:54:52.986681: step 16441, loss 0.51014.
Train: 2018-08-01T23:54:53.155229: step 16442, loss 0.475148.
Train: 2018-08-01T23:54:53.319790: step 16443, loss 0.579947.
Train: 2018-08-01T23:54:53.490359: step 16444, loss 0.720286.
Train: 2018-08-01T23:54:53.651902: step 16445, loss 0.527393.
Train: 2018-08-01T23:54:53.813498: step 16446, loss 0.527391.
Train: 2018-08-01T23:54:53.976036: step 16447, loss 0.509838.
Train: 2018-08-01T23:54:54.147577: step 16448, loss 0.544904.
Train: 2018-08-01T23:54:54.311140: step 16449, loss 0.615197.
Train: 2018-08-01T23:54:54.470714: step 16450, loss 0.474566.
Test: 2018-08-01T23:54:54.999301: step 16450, loss 0.547735.
Train: 2018-08-01T23:54:55.167850: step 16451, loss 0.562476.
Train: 2018-08-01T23:54:55.329452: step 16452, loss 0.562482.
Train: 2018-08-01T23:54:55.498965: step 16453, loss 0.562488.
Train: 2018-08-01T23:54:55.662528: step 16454, loss 0.544843.
Train: 2018-08-01T23:54:55.828085: step 16455, loss 0.52717.
Train: 2018-08-01T23:54:55.990675: step 16456, loss 0.580188.
Train: 2018-08-01T23:54:56.161220: step 16457, loss 0.633286.
Train: 2018-08-01T23:54:56.326780: step 16458, loss 0.597879.
Train: 2018-08-01T23:54:56.492340: step 16459, loss 0.474153.
Train: 2018-08-01T23:54:56.657866: step 16460, loss 0.562501.
Test: 2018-08-01T23:54:57.199432: step 16460, loss 0.547701.
Train: 2018-08-01T23:54:57.365974: step 16461, loss 0.721574.
Train: 2018-08-01T23:54:57.536548: step 16462, loss 0.491937.
Train: 2018-08-01T23:54:57.702109: step 16463, loss 0.650578.
Train: 2018-08-01T23:54:57.874639: step 16464, loss 0.474563.
Train: 2018-08-01T23:54:58.039199: step 16465, loss 0.492201.
Train: 2018-08-01T23:54:58.205728: step 16466, loss 0.615164.
Train: 2018-08-01T23:54:58.369291: step 16467, loss 0.597568.
Train: 2018-08-01T23:54:58.554821: step 16468, loss 0.615054.
Train: 2018-08-01T23:54:58.729331: step 16469, loss 0.544943.
Train: 2018-08-01T23:54:58.898876: step 16470, loss 0.579914.
Test: 2018-08-01T23:54:59.430454: step 16470, loss 0.547821.
Train: 2018-08-01T23:54:59.600032: step 16471, loss 0.579879.
Train: 2018-08-01T23:54:59.764587: step 16472, loss 0.632093.
Train: 2018-08-01T23:54:59.935136: step 16473, loss 0.666651.
Train: 2018-08-01T23:55:00.102658: step 16474, loss 0.458566.
Train: 2018-08-01T23:55:00.265256: step 16475, loss 0.545132.
Train: 2018-08-01T23:55:00.429808: step 16476, loss 0.614136.
Train: 2018-08-01T23:55:00.592349: step 16477, loss 0.579607.
Train: 2018-08-01T23:55:00.766914: step 16478, loss 0.54523.
Train: 2018-08-01T23:55:00.933437: step 16479, loss 0.510981.
Train: 2018-08-01T23:55:01.097031: step 16480, loss 0.682273.
Test: 2018-08-01T23:55:01.635610: step 16480, loss 0.548096.
Train: 2018-08-01T23:55:01.803113: step 16481, loss 0.562401.
Train: 2018-08-01T23:55:01.966675: step 16482, loss 0.630587.
Train: 2018-08-01T23:55:02.130264: step 16483, loss 0.528417.
Train: 2018-08-01T23:55:02.291805: step 16484, loss 0.664175.
Train: 2018-08-01T23:55:02.466338: step 16485, loss 0.596232.
Train: 2018-08-01T23:55:02.629903: step 16486, loss 0.495052.
Train: 2018-08-01T23:55:02.794495: step 16487, loss 0.579256.
Train: 2018-08-01T23:55:02.960049: step 16488, loss 0.562455.
Train: 2018-08-01T23:55:03.125629: step 16489, loss 0.545717.
Train: 2018-08-01T23:55:03.293129: step 16490, loss 0.696276.
Test: 2018-08-01T23:55:03.817726: step 16490, loss 0.548525.
Train: 2018-08-01T23:55:03.982286: step 16491, loss 0.529138.
Train: 2018-08-01T23:55:04.146847: step 16492, loss 0.662338.
Train: 2018-08-01T23:55:04.310440: step 16493, loss 0.545941.
Train: 2018-08-01T23:55:04.476001: step 16494, loss 0.496383.
Train: 2018-08-01T23:55:04.643519: step 16495, loss 0.595595.
Train: 2018-08-01T23:55:04.808105: step 16496, loss 0.645042.
Train: 2018-08-01T23:55:04.976629: step 16497, loss 0.628405.
Train: 2018-08-01T23:55:05.139225: step 16498, loss 0.562619.
Train: 2018-08-01T23:55:05.303754: step 16499, loss 0.595364.
Train: 2018-08-01T23:55:05.478312: step 16500, loss 0.530044.
Test: 2018-08-01T23:55:06.009910: step 16500, loss 0.549056.
Train: 2018-08-01T23:55:06.812223: step 16501, loss 0.562693.
Train: 2018-08-01T23:55:06.975774: step 16502, loss 0.513924.
Train: 2018-08-01T23:55:07.154277: step 16503, loss 0.513947.
Train: 2018-08-01T23:55:07.320858: step 16504, loss 0.644033.
Train: 2018-08-01T23:55:07.485392: step 16505, loss 0.546457.
Train: 2018-08-01T23:55:07.649978: step 16506, loss 0.546459.
Train: 2018-08-01T23:55:07.816508: step 16507, loss 0.497667.
Train: 2018-08-01T23:55:07.981100: step 16508, loss 0.481266.
Train: 2018-08-01T23:55:08.145652: step 16509, loss 0.530002.
Train: 2018-08-01T23:55:08.312216: step 16510, loss 0.62816.
Test: 2018-08-01T23:55:08.848747: step 16510, loss 0.548867.
Train: 2018-08-01T23:55:09.024312: step 16511, loss 0.529783.
Train: 2018-08-01T23:55:09.193852: step 16512, loss 0.513227.
Train: 2018-08-01T23:55:09.358410: step 16513, loss 0.546059.
Train: 2018-08-01T23:55:09.522946: step 16514, loss 0.628763.
Train: 2018-08-01T23:55:09.685541: step 16515, loss 0.529348.
Train: 2018-08-01T23:55:09.852066: step 16516, loss 0.529254.
Train: 2018-08-01T23:55:10.031585: step 16517, loss 0.579161.
Train: 2018-08-01T23:55:10.196170: step 16518, loss 0.64602.
Train: 2018-08-01T23:55:10.359709: step 16519, loss 0.562471.
Train: 2018-08-01T23:55:10.527261: step 16520, loss 0.62942.
Test: 2018-08-01T23:55:11.057842: step 16520, loss 0.548452.
Train: 2018-08-01T23:55:11.223400: step 16521, loss 0.579203.
Train: 2018-08-01T23:55:11.393978: step 16522, loss 0.579201.
Train: 2018-08-01T23:55:11.555538: step 16523, loss 0.56247.
Train: 2018-08-01T23:55:11.735049: step 16524, loss 0.646077.
Train: 2018-08-01T23:55:11.910594: step 16525, loss 0.56248.
Train: 2018-08-01T23:55:12.078147: step 16526, loss 0.662556.
Train: 2018-08-01T23:55:12.240711: step 16527, loss 0.545865.
Train: 2018-08-01T23:55:12.429202: step 16528, loss 0.429661.
Train: 2018-08-01T23:55:12.596729: step 16529, loss 0.545899.
Train: 2018-08-01T23:55:12.758321: step 16530, loss 0.612384.
Test: 2018-08-01T23:55:13.296915: step 16530, loss 0.548586.
Train: 2018-08-01T23:55:13.469454: step 16531, loss 0.545882.
Train: 2018-08-01T23:55:13.634014: step 16532, loss 0.595768.
Train: 2018-08-01T23:55:13.797577: step 16533, loss 0.562506.
Train: 2018-08-01T23:55:13.964130: step 16534, loss 0.446089.
Train: 2018-08-01T23:55:14.126697: step 16535, loss 0.545832.
Train: 2018-08-01T23:55:14.291257: step 16536, loss 0.562478.
Train: 2018-08-01T23:55:14.456813: step 16537, loss 0.595815.
Train: 2018-08-01T23:55:14.632375: step 16538, loss 0.61297.
Train: 2018-08-01T23:55:14.795907: step 16539, loss 0.529273.
Train: 2018-08-01T23:55:14.959501: step 16540, loss 0.54569.
Test: 2018-08-01T23:55:15.501023: step 16540, loss 0.548391.
Train: 2018-08-01T23:55:15.677551: step 16541, loss 0.646376.
Train: 2018-08-01T23:55:15.841144: step 16542, loss 0.612831.
Train: 2018-08-01T23:55:16.014674: step 16543, loss 0.495317.
Train: 2018-08-01T23:55:16.176217: step 16544, loss 0.629614.
Train: 2018-08-01T23:55:16.344766: step 16545, loss 0.629582.
Train: 2018-08-01T23:55:16.509327: step 16546, loss 0.579219.
Train: 2018-08-01T23:55:16.670906: step 16547, loss 0.595939.
Train: 2018-08-01T23:55:16.839444: step 16548, loss 0.545768.
Train: 2018-08-01T23:55:17.003028: step 16549, loss 0.479042.
Train: 2018-08-01T23:55:17.164576: step 16550, loss 0.579174.
Test: 2018-08-01T23:55:17.685197: step 16550, loss 0.548502.
Train: 2018-08-01T23:55:17.850742: step 16551, loss 0.64595.
Train: 2018-08-01T23:55:18.014305: step 16552, loss 0.545812.
Train: 2018-08-01T23:55:18.181856: step 16553, loss 0.445836.
Train: 2018-08-01T23:55:18.344421: step 16554, loss 0.495741.
Train: 2018-08-01T23:55:18.512011: step 16555, loss 0.544634.
Train: 2018-08-01T23:55:18.676564: step 16556, loss 0.595981.
Train: 2018-08-01T23:55:18.846081: step 16557, loss 0.629606.
Train: 2018-08-01T23:55:19.014630: step 16558, loss 0.495254.
Train: 2018-08-01T23:55:19.188167: step 16559, loss 0.528791.
Train: 2018-08-01T23:55:19.358710: step 16560, loss 0.562433.
Test: 2018-08-01T23:55:19.889323: step 16560, loss 0.548284.
Train: 2018-08-01T23:55:20.056868: step 16561, loss 0.562426.
Train: 2018-08-01T23:55:20.232375: step 16562, loss 0.528587.
Train: 2018-08-01T23:55:20.401952: step 16563, loss 0.528512.
Train: 2018-08-01T23:55:20.568502: step 16564, loss 0.596391.
Train: 2018-08-01T23:55:20.732038: step 16565, loss 0.545386.
Train: 2018-08-01T23:55:20.912557: step 16566, loss 0.528304.
Train: 2018-08-01T23:55:21.076122: step 16567, loss 0.613655.
Train: 2018-08-01T23:55:21.241707: step 16568, loss 0.511085.
Train: 2018-08-01T23:55:21.405265: step 16569, loss 0.545264.
Train: 2018-08-01T23:55:21.570796: step 16570, loss 0.665399.
Test: 2018-08-01T23:55:22.103374: step 16570, loss 0.54802.
Train: 2018-08-01T23:55:22.270956: step 16571, loss 0.63108.
Train: 2018-08-01T23:55:22.437479: step 16572, loss 0.562399.
Train: 2018-08-01T23:55:22.603037: step 16573, loss 0.59669.
Train: 2018-08-01T23:55:22.766599: step 16574, loss 0.545272.
Train: 2018-08-01T23:55:22.934152: step 16575, loss 0.528171.
Train: 2018-08-01T23:55:23.104695: step 16576, loss 0.511067.
Train: 2018-08-01T23:55:23.282247: step 16577, loss 0.57952.
Train: 2018-08-01T23:55:23.456758: step 16578, loss 0.493898.
Train: 2018-08-01T23:55:23.621343: step 16579, loss 0.528107.
Train: 2018-08-01T23:55:23.794851: step 16580, loss 0.562399.
Test: 2018-08-01T23:55:24.331416: step 16580, loss 0.547998.
Train: 2018-08-01T23:55:24.506947: step 16581, loss 0.613991.
Train: 2018-08-01T23:55:24.669512: step 16582, loss 0.51078.
Train: 2018-08-01T23:55:24.837065: step 16583, loss 0.68299.
Train: 2018-08-01T23:55:25.010601: step 16584, loss 0.527967.
Train: 2018-08-01T23:55:25.176189: step 16585, loss 0.476323.
Train: 2018-08-01T23:55:25.342714: step 16586, loss 0.562401.
Train: 2018-08-01T23:55:25.508271: step 16587, loss 0.545151.
Train: 2018-08-01T23:55:25.678845: step 16588, loss 0.614214.
Train: 2018-08-01T23:55:25.840383: step 16589, loss 0.527854.
Train: 2018-08-01T23:55:26.006938: step 16590, loss 0.631551.
Test: 2018-08-01T23:55:26.541530: step 16590, loss 0.547934.
Train: 2018-08-01T23:55:26.705072: step 16591, loss 0.510563.
Train: 2018-08-01T23:55:26.870635: step 16592, loss 0.545118.
Train: 2018-08-01T23:55:27.036185: step 16593, loss 0.579702.
Train: 2018-08-01T23:55:27.197785: step 16594, loss 0.579707.
Train: 2018-08-01T23:55:27.366303: step 16595, loss 0.527808.
Train: 2018-08-01T23:55:27.534853: step 16596, loss 0.68355.
Train: 2018-08-01T23:55:27.700441: step 16597, loss 0.441423.
Train: 2018-08-01T23:55:27.864971: step 16598, loss 0.510533.
Train: 2018-08-01T23:55:28.033548: step 16599, loss 0.527788.
Train: 2018-08-01T23:55:28.197114: step 16600, loss 0.545076.
Test: 2018-08-01T23:55:28.731653: step 16600, loss 0.547877.
Train: 2018-08-01T23:55:29.497467: step 16601, loss 0.492975.
Train: 2018-08-01T23:55:29.660056: step 16602, loss 0.510225.
Train: 2018-08-01T23:55:29.824619: step 16603, loss 0.457759.
Train: 2018-08-01T23:55:29.994140: step 16604, loss 0.544936.
Train: 2018-08-01T23:55:30.169669: step 16605, loss 0.59761.
Train: 2018-08-01T23:55:30.336225: step 16606, loss 0.456765.
Train: 2018-08-01T23:55:30.501782: step 16607, loss 0.544821.
Train: 2018-08-01T23:55:30.670332: step 16608, loss 0.580275.
Train: 2018-08-01T23:55:30.832897: step 16609, loss 0.651522.
Train: 2018-08-01T23:55:31.001446: step 16610, loss 0.526934.
Test: 2018-08-01T23:55:31.537013: step 16610, loss 0.547637.
Train: 2018-08-01T23:55:31.704597: step 16611, loss 0.509055.
Train: 2018-08-01T23:55:31.867157: step 16612, loss 0.473226.
Train: 2018-08-01T23:55:32.031723: step 16613, loss 0.580543.
Train: 2018-08-01T23:55:32.207223: step 16614, loss 0.59856.
Train: 2018-08-01T23:55:32.374801: step 16615, loss 0.526695.
Train: 2018-08-01T23:55:32.540364: step 16616, loss 0.562673.
Train: 2018-08-01T23:55:32.705920: step 16617, loss 0.598741.
Train: 2018-08-01T23:55:32.884413: step 16618, loss 0.526622.
Train: 2018-08-01T23:55:33.049994: step 16619, loss 0.5627.
Train: 2018-08-01T23:55:33.217550: step 16620, loss 0.634927.
Test: 2018-08-01T23:55:33.757080: step 16620, loss 0.547588.
Train: 2018-08-01T23:55:33.926627: step 16621, loss 0.544654.
Train: 2018-08-01T23:55:34.100163: step 16622, loss 0.59876.
Train: 2018-08-01T23:55:34.280710: step 16623, loss 0.652754.
Train: 2018-08-01T23:55:34.446237: step 16624, loss 0.562653.
Train: 2018-08-01T23:55:34.610797: step 16625, loss 0.508823.
Train: 2018-08-01T23:55:34.777352: step 16626, loss 0.526794.
Train: 2018-08-01T23:55:34.939918: step 16627, loss 0.473139.
Train: 2018-08-01T23:55:35.104511: step 16628, loss 0.652089.
Train: 2018-08-01T23:55:35.270060: step 16629, loss 0.526843.
Train: 2018-08-01T23:55:35.433622: step 16630, loss 0.669774.
Test: 2018-08-01T23:55:35.964210: step 16630, loss 0.547642.
Train: 2018-08-01T23:55:36.129737: step 16631, loss 0.544743.
Train: 2018-08-01T23:55:36.298287: step 16632, loss 0.776087.
Train: 2018-08-01T23:55:36.460852: step 16633, loss 0.509373.
Train: 2018-08-01T23:55:36.638409: step 16634, loss 0.491878.
Train: 2018-08-01T23:55:36.804958: step 16635, loss 0.580092.
Train: 2018-08-01T23:55:36.970515: step 16636, loss 0.580036.
Train: 2018-08-01T23:55:37.136071: step 16637, loss 0.57998.
Train: 2018-08-01T23:55:37.301635: step 16638, loss 0.475015.
Train: 2018-08-01T23:55:37.467187: step 16639, loss 0.47511.
Train: 2018-08-01T23:55:37.629756: step 16640, loss 0.702165.
Test: 2018-08-01T23:55:38.160310: step 16640, loss 0.547829.
Train: 2018-08-01T23:55:38.329855: step 16641, loss 0.457824.
Train: 2018-08-01T23:55:38.507381: step 16642, loss 0.579855.
Train: 2018-08-01T23:55:38.682912: step 16643, loss 0.475332.
Train: 2018-08-01T23:55:38.848470: step 16644, loss 0.49271.
Train: 2018-08-01T23:55:39.012057: step 16645, loss 0.457707.
Train: 2018-08-01T23:55:39.174628: step 16646, loss 0.422442.
Train: 2018-08-01T23:55:39.351125: step 16647, loss 0.527318.
Train: 2018-08-01T23:55:39.521682: step 16648, loss 0.650716.
Train: 2018-08-01T23:55:39.688252: step 16649, loss 0.597883.
Train: 2018-08-01T23:55:39.854828: step 16650, loss 0.47393.
Test: 2018-08-01T23:55:40.399324: step 16650, loss 0.547664.
Train: 2018-08-01T23:55:40.565878: step 16651, loss 0.598063.
Train: 2018-08-01T23:55:40.732432: step 16652, loss 0.598138.
Train: 2018-08-01T23:55:40.895996: step 16653, loss 0.580371.
Train: 2018-08-01T23:55:41.060584: step 16654, loss 0.616024.
Train: 2018-08-01T23:55:41.225147: step 16655, loss 0.580376.
Train: 2018-08-01T23:55:41.399650: step 16656, loss 0.615964.
Train: 2018-08-01T23:55:41.565207: step 16657, loss 0.544767.
Train: 2018-08-01T23:55:41.729792: step 16658, loss 0.598053.
Train: 2018-08-01T23:55:41.893354: step 16659, loss 0.597985.
Train: 2018-08-01T23:55:42.057918: step 16660, loss 0.509422.
Test: 2018-08-01T23:55:42.591489: step 16660, loss 0.5477.
Train: 2018-08-01T23:55:42.755050: step 16661, loss 0.615524.
Train: 2018-08-01T23:55:42.928561: step 16662, loss 0.615414.
Train: 2018-08-01T23:55:43.108082: step 16663, loss 0.650467.
Train: 2018-08-01T23:55:43.274668: step 16664, loss 0.562454.
Train: 2018-08-01T23:55:43.448223: step 16665, loss 0.614887.
Train: 2018-08-01T23:55:43.611762: step 16666, loss 0.649522.
Train: 2018-08-01T23:55:43.775325: step 16667, loss 0.631777.
Train: 2018-08-01T23:55:43.945843: step 16668, loss 0.527893.
Train: 2018-08-01T23:55:44.110434: step 16669, loss 0.61395.
Train: 2018-08-01T23:55:44.277955: step 16670, loss 0.528183.
Test: 2018-08-01T23:55:44.811530: step 16670, loss 0.548127.
Train: 2018-08-01T23:55:44.980078: step 16671, loss 0.74993.
Train: 2018-08-01T23:55:45.144637: step 16672, loss 0.630214.
Train: 2018-08-01T23:55:45.305239: step 16673, loss 0.62982.
Train: 2018-08-01T23:55:45.483731: step 16674, loss 0.579203.
Train: 2018-08-01T23:55:45.649288: step 16675, loss 0.595777.
Train: 2018-08-01T23:55:45.815871: step 16676, loss 0.628706.
Train: 2018-08-01T23:55:45.979439: step 16677, loss 0.546159.
Train: 2018-08-01T23:55:46.148979: step 16678, loss 0.464525.
Train: 2018-08-01T23:55:46.314541: step 16679, loss 0.513768.
Train: 2018-08-01T23:55:46.487049: step 16680, loss 0.530151.
Test: 2018-08-01T23:55:47.008655: step 16680, loss 0.549097.
Train: 2018-08-01T23:55:47.182218: step 16681, loss 0.48142.
Train: 2018-08-01T23:55:47.349743: step 16682, loss 0.64404.
Train: 2018-08-01T23:55:47.523279: step 16683, loss 0.562712.
Train: 2018-08-01T23:55:47.690860: step 16684, loss 0.546464.
Train: 2018-08-01T23:55:47.855391: step 16685, loss 0.578968.
Train: 2018-08-01T23:55:48.021971: step 16686, loss 0.578968.
Train: 2018-08-01T23:55:48.186509: step 16687, loss 0.59522.
Train: 2018-08-01T23:55:48.351086: step 16688, loss 0.643954.
Train: 2018-08-01T23:55:48.525600: step 16689, loss 0.595183.
Train: 2018-08-01T23:55:48.687211: step 16690, loss 0.578953.
Test: 2018-08-01T23:55:49.219744: step 16690, loss 0.549229.
Train: 2018-08-01T23:55:49.384304: step 16691, loss 0.578947.
Train: 2018-08-01T23:55:49.551911: step 16692, loss 0.498186.
Train: 2018-08-01T23:55:49.719439: step 16693, loss 0.546641.
Train: 2018-08-01T23:55:49.885989: step 16694, loss 0.54663.
Train: 2018-08-01T23:55:50.052518: step 16695, loss 0.578946.
Train: 2018-08-01T23:55:50.221067: step 16696, loss 0.578948.
Train: 2018-08-01T23:55:50.389634: step 16697, loss 0.481819.
Train: 2018-08-01T23:55:50.555200: step 16698, loss 0.578958.
Train: 2018-08-01T23:55:50.731741: step 16699, loss 0.627706.
Train: 2018-08-01T23:55:50.895274: step 16700, loss 0.70903.
Test: 2018-08-01T23:55:51.432828: step 16700, loss 0.549134.
Train: 2018-08-01T23:55:52.192315: step 16701, loss 0.481559.
Train: 2018-08-01T23:55:52.356875: step 16702, loss 0.530256.
Train: 2018-08-01T23:55:52.527388: step 16703, loss 0.627711.
Train: 2018-08-01T23:55:52.692946: step 16704, loss 0.56272.
Train: 2018-08-01T23:55:52.861495: step 16705, loss 0.578966.
Train: 2018-08-01T23:55:53.028049: step 16706, loss 0.530225.
Train: 2018-08-01T23:55:53.201622: step 16707, loss 0.562711.
Train: 2018-08-01T23:55:53.363187: step 16708, loss 0.546432.
Train: 2018-08-01T23:55:53.529710: step 16709, loss 0.660421.
Train: 2018-08-01T23:55:53.697287: step 16710, loss 0.578977.
Test: 2018-08-01T23:55:54.222856: step 16710, loss 0.549068.
Train: 2018-08-01T23:55:54.395421: step 16711, loss 0.627806.
Train: 2018-08-01T23:55:54.558958: step 16712, loss 0.66026.
Train: 2018-08-01T23:55:54.724516: step 16713, loss 0.562738.
Train: 2018-08-01T23:55:54.890073: step 16714, loss 0.627514.
Train: 2018-08-01T23:55:55.052639: step 16715, loss 0.578941.
Train: 2018-08-01T23:55:55.226205: step 16716, loss 0.498393.
Train: 2018-08-01T23:55:55.393757: step 16717, loss 0.57893.
Train: 2018-08-01T23:55:55.563273: step 16718, loss 0.450297.
Train: 2018-08-01T23:55:55.730825: step 16719, loss 0.530638.
Train: 2018-08-01T23:55:55.894387: step 16720, loss 0.530557.
Test: 2018-08-01T23:55:56.437935: step 16720, loss 0.549245.
Train: 2018-08-01T23:55:56.604490: step 16721, loss 0.611272.
Train: 2018-08-01T23:55:56.772042: step 16722, loss 0.57895.
Train: 2018-08-01T23:55:56.936633: step 16723, loss 0.611373.
Train: 2018-08-01T23:55:57.102159: step 16724, loss 0.54652.
Train: 2018-08-01T23:55:57.268714: step 16725, loss 0.514024.
Train: 2018-08-01T23:55:57.433304: step 16726, loss 0.562707.
Train: 2018-08-01T23:55:57.598833: step 16727, loss 0.497515.
Train: 2018-08-01T23:55:57.768409: step 16728, loss 0.578996.
Train: 2018-08-01T23:55:57.932944: step 16729, loss 0.529878.
Train: 2018-08-01T23:55:58.095503: step 16730, loss 0.480488.
Test: 2018-08-01T23:55:58.631072: step 16730, loss 0.548765.
Train: 2018-08-01T23:55:58.803642: step 16731, loss 0.595546.
Train: 2018-08-01T23:55:58.985162: step 16732, loss 0.512928.
Train: 2018-08-01T23:55:59.144698: step 16733, loss 0.695313.
Train: 2018-08-01T23:55:59.310257: step 16734, loss 0.479392.
Train: 2018-08-01T23:55:59.486815: step 16735, loss 0.529158.
Train: 2018-08-01T23:55:59.648352: step 16736, loss 0.595903.
Train: 2018-08-01T23:55:59.815907: step 16737, loss 0.512205.
Train: 2018-08-01T23:55:59.983461: step 16738, loss 0.562447.
Train: 2018-08-01T23:56:00.150011: step 16739, loss 0.495068.
Train: 2018-08-01T23:56:00.324571: step 16740, loss 0.562423.
Test: 2018-08-01T23:56:00.863106: step 16740, loss 0.548221.
Train: 2018-08-01T23:56:01.027691: step 16741, loss 0.545465.
Train: 2018-08-01T23:56:01.204218: step 16742, loss 0.596407.
Train: 2018-08-01T23:56:01.368754: step 16743, loss 0.545365.
Train: 2018-08-01T23:56:01.543313: step 16744, loss 0.477014.
Train: 2018-08-01T23:56:01.708873: step 16745, loss 0.511.
Train: 2018-08-01T23:56:01.883378: step 16746, loss 0.579593.
Train: 2018-08-01T23:56:02.050930: step 16747, loss 0.510661.
Train: 2018-08-01T23:56:02.217486: step 16748, loss 0.493185.
Train: 2018-08-01T23:56:02.379052: step 16749, loss 0.562416.
Train: 2018-08-01T23:56:02.566033: step 16750, loss 0.632165.
Test: 2018-08-01T23:56:03.091604: step 16750, loss 0.547805.
Train: 2018-08-01T23:56:03.258158: step 16751, loss 0.579908.
Train: 2018-08-01T23:56:03.424711: step 16752, loss 0.52744.
Train: 2018-08-01T23:56:03.595283: step 16753, loss 0.527383.
Train: 2018-08-01T23:56:03.762809: step 16754, loss 0.50975.
Train: 2018-08-01T23:56:03.930385: step 16755, loss 0.615323.
Train: 2018-08-01T23:56:04.095918: step 16756, loss 0.491923.
Train: 2018-08-01T23:56:04.259505: step 16757, loss 0.544823.
Train: 2018-08-01T23:56:04.425063: step 16758, loss 0.562517.
Train: 2018-08-01T23:56:04.591593: step 16759, loss 0.598026.
Train: 2018-08-01T23:56:04.758148: step 16760, loss 0.669134.
Test: 2018-08-01T23:56:05.298704: step 16760, loss 0.547666.
Train: 2018-08-01T23:56:05.467278: step 16761, loss 0.651315.
Train: 2018-08-01T23:56:05.642783: step 16762, loss 0.544796.
Train: 2018-08-01T23:56:05.812330: step 16763, loss 0.633307.
Train: 2018-08-01T23:56:05.980903: step 16764, loss 0.615469.
Train: 2018-08-01T23:56:06.152451: step 16765, loss 0.492045.
Train: 2018-08-01T23:56:06.315017: step 16766, loss 0.47458.
Train: 2018-08-01T23:56:06.484533: step 16767, loss 0.474621.
Train: 2018-08-01T23:56:06.648096: step 16768, loss 0.544886.
Train: 2018-08-01T23:56:06.812655: step 16769, loss 0.597655.
Train: 2018-08-01T23:56:06.982229: step 16770, loss 0.544876.
Test: 2018-08-01T23:56:07.524790: step 16770, loss 0.547735.
Train: 2018-08-01T23:56:07.687324: step 16771, loss 0.580072.
Train: 2018-08-01T23:56:07.851878: step 16772, loss 0.527276.
Train: 2018-08-01T23:56:08.018432: step 16773, loss 0.54487.
Train: 2018-08-01T23:56:08.184014: step 16774, loss 0.632922.
Train: 2018-08-01T23:56:08.347583: step 16775, loss 0.615274.
Train: 2018-08-01T23:56:08.515131: step 16776, loss 0.492156.
Train: 2018-08-01T23:56:08.679696: step 16777, loss 0.615177.
Train: 2018-08-01T23:56:08.842261: step 16778, loss 0.597562.
Train: 2018-08-01T23:56:09.009813: step 16779, loss 0.509871.
Train: 2018-08-01T23:56:09.179330: step 16780, loss 0.544933.
Test: 2018-08-01T23:56:09.717889: step 16780, loss 0.547785.
Train: 2018-08-01T23:56:09.880455: step 16781, loss 0.614959.
Train: 2018-08-01T23:56:10.051997: step 16782, loss 0.684828.
Train: 2018-08-01T23:56:10.219548: step 16783, loss 0.632171.
Train: 2018-08-01T23:56:10.391089: step 16784, loss 0.51029.
Train: 2018-08-01T23:56:10.557644: step 16785, loss 0.614406.
Train: 2018-08-01T23:56:10.718214: step 16786, loss 0.527841.
Train: 2018-08-01T23:56:10.883773: step 16787, loss 0.579643.
Train: 2018-08-01T23:56:11.050361: step 16788, loss 0.562398.
Train: 2018-08-01T23:56:11.228871: step 16789, loss 0.493726.
Train: 2018-08-01T23:56:11.394408: step 16790, loss 0.528093.
Test: 2018-08-01T23:56:11.921030: step 16790, loss 0.548039.
Train: 2018-08-01T23:56:12.085559: step 16791, loss 0.459518.
Train: 2018-08-01T23:56:12.247128: step 16792, loss 0.545232.
Train: 2018-08-01T23:56:12.414706: step 16793, loss 0.631146.
Train: 2018-08-01T23:56:12.578243: step 16794, loss 0.510828.
Train: 2018-08-01T23:56:12.737847: step 16795, loss 0.579603.
Train: 2018-08-01T23:56:12.900382: step 16796, loss 0.562399.
Train: 2018-08-01T23:56:13.080925: step 16797, loss 0.596842.
Train: 2018-08-01T23:56:13.253463: step 16798, loss 0.579619.
Train: 2018-08-01T23:56:13.421986: step 16799, loss 0.527969.
Train: 2018-08-01T23:56:13.586547: step 16800, loss 0.579617.
Test: 2018-08-01T23:56:14.117129: step 16800, loss 0.547982.
Train: 2018-08-01T23:56:14.898951: step 16801, loss 0.527966.
Train: 2018-08-01T23:56:15.079469: step 16802, loss 0.631292.
Train: 2018-08-01T23:56:15.247020: step 16803, loss 0.562398.
Train: 2018-08-01T23:56:15.410585: step 16804, loss 0.545195.
Train: 2018-08-01T23:56:15.573149: step 16805, loss 0.493605.
Train: 2018-08-01T23:56:15.738705: step 16806, loss 0.648449.
Train: 2018-08-01T23:56:15.904289: step 16807, loss 0.579598.
Train: 2018-08-01T23:56:16.067826: step 16808, loss 0.631146.
Train: 2018-08-01T23:56:16.234381: step 16809, loss 0.510921.
Train: 2018-08-01T23:56:16.399938: step 16810, loss 0.493814.
Test: 2018-08-01T23:56:16.925532: step 16810, loss 0.548036.
Train: 2018-08-01T23:56:17.097074: step 16811, loss 0.562397.
Train: 2018-08-01T23:56:17.259672: step 16812, loss 0.510934.
Train: 2018-08-01T23:56:17.425198: step 16813, loss 0.562397.
Train: 2018-08-01T23:56:17.601725: step 16814, loss 0.613952.
Train: 2018-08-01T23:56:17.767314: step 16815, loss 0.545212.
Train: 2018-08-01T23:56:17.936829: step 16816, loss 0.648346.
Train: 2018-08-01T23:56:18.102419: step 16817, loss 0.613917.
Train: 2018-08-01T23:56:18.270976: step 16818, loss 0.562397.
Train: 2018-08-01T23:56:18.439512: step 16819, loss 0.562398.
Train: 2018-08-01T23:56:18.604046: step 16820, loss 0.511099.
Test: 2018-08-01T23:56:19.122659: step 16820, loss 0.548086.
Train: 2018-08-01T23:56:19.299212: step 16821, loss 0.562399.
Train: 2018-08-01T23:56:19.462784: step 16822, loss 0.613657.
Train: 2018-08-01T23:56:19.626314: step 16823, loss 0.545332.
Train: 2018-08-01T23:56:19.789876: step 16824, loss 0.528288.
Train: 2018-08-01T23:56:19.955458: step 16825, loss 0.66473.
Train: 2018-08-01T23:56:20.118997: step 16826, loss 0.443202.
Train: 2018-08-01T23:56:20.282560: step 16827, loss 0.5113.
Train: 2018-08-01T23:56:20.442160: step 16828, loss 0.613558.
Train: 2018-08-01T23:56:20.614704: step 16829, loss 0.630626.
Train: 2018-08-01T23:56:20.777236: step 16830, loss 0.460139.
Test: 2018-08-01T23:56:21.306821: step 16830, loss 0.548117.
Train: 2018-08-01T23:56:21.476396: step 16831, loss 0.528286.
Train: 2018-08-01T23:56:21.640958: step 16832, loss 0.494086.
Train: 2018-08-01T23:56:21.806509: step 16833, loss 0.596625.
Train: 2018-08-01T23:56:21.968054: step 16834, loss 0.510985.
Train: 2018-08-01T23:56:22.136602: step 16835, loss 0.425028.
Train: 2018-08-01T23:56:22.307172: step 16836, loss 0.5624.
Train: 2018-08-01T23:56:22.470734: step 16837, loss 0.545114.
Train: 2018-08-01T23:56:22.638295: step 16838, loss 0.5971.
Train: 2018-08-01T23:56:22.809834: step 16839, loss 0.510262.
Train: 2018-08-01T23:56:22.972399: step 16840, loss 0.527561.
Test: 2018-08-01T23:56:23.500956: step 16840, loss 0.547798.
Train: 2018-08-01T23:56:23.668507: step 16841, loss 0.63237.
Train: 2018-08-01T23:56:23.835062: step 16842, loss 0.614978.
Train: 2018-08-01T23:56:24.001617: step 16843, loss 0.50988.
Train: 2018-08-01T23:56:24.170167: step 16844, loss 0.527366.
Train: 2018-08-01T23:56:24.342706: step 16845, loss 0.650318.
Train: 2018-08-01T23:56:24.510288: step 16846, loss 0.509744.
Train: 2018-08-01T23:56:24.678806: step 16847, loss 0.544882.
Train: 2018-08-01T23:56:24.854370: step 16848, loss 0.421671.
Train: 2018-08-01T23:56:25.016903: step 16849, loss 0.474266.
Train: 2018-08-01T23:56:25.183457: step 16850, loss 0.509402.
Test: 2018-08-01T23:56:25.730995: step 16850, loss 0.547662.
Train: 2018-08-01T23:56:25.899576: step 16851, loss 0.615838.
Train: 2018-08-01T23:56:26.073107: step 16852, loss 0.687222.
Train: 2018-08-01T23:56:26.238637: step 16853, loss 0.491296.
Train: 2018-08-01T23:56:26.406214: step 16854, loss 0.544736.
Train: 2018-08-01T23:56:26.570774: step 16855, loss 0.598305.
Train: 2018-08-01T23:56:26.732344: step 16856, loss 0.619769.
Train: 2018-08-01T23:56:26.898898: step 16857, loss 0.544725.
Train: 2018-08-01T23:56:27.079416: step 16858, loss 0.419734.
Train: 2018-08-01T23:56:27.245975: step 16859, loss 0.562598.
Train: 2018-08-01T23:56:27.421476: step 16860, loss 0.526802.
Test: 2018-08-01T23:56:27.958043: step 16860, loss 0.547611.
Train: 2018-08-01T23:56:28.127621: step 16861, loss 0.652268.
Train: 2018-08-01T23:56:28.294166: step 16862, loss 0.544696.
Train: 2018-08-01T23:56:28.458702: step 16863, loss 0.472975.
Train: 2018-08-01T23:56:28.621301: step 16864, loss 0.508789.
Train: 2018-08-01T23:56:28.796798: step 16865, loss 0.598606.
Train: 2018-08-01T23:56:28.959394: step 16866, loss 0.526682.
Train: 2018-08-01T23:56:29.126947: step 16867, loss 0.436617.
Train: 2018-08-01T23:56:29.293471: step 16868, loss 0.562701.
Train: 2018-08-01T23:56:29.455038: step 16869, loss 0.598895.
Train: 2018-08-01T23:56:29.616608: step 16870, loss 0.580845.
Test: 2018-08-01T23:56:30.153180: step 16870, loss 0.547577.
Train: 2018-08-01T23:56:30.317734: step 16871, loss 0.49028.
Train: 2018-08-01T23:56:30.486282: step 16872, loss 0.580906.
Train: 2018-08-01T23:56:30.649845: step 16873, loss 0.490156.
Train: 2018-08-01T23:56:30.820415: step 16874, loss 0.526434.
Train: 2018-08-01T23:56:30.985946: step 16875, loss 0.562818.
Train: 2018-08-01T23:56:31.148512: step 16876, loss 0.5993.
Train: 2018-08-01T23:56:31.314097: step 16877, loss 0.654045.
Train: 2018-08-01T23:56:31.478630: step 16878, loss 0.435259.
Train: 2018-08-01T23:56:31.645215: step 16879, loss 0.654016.
Train: 2018-08-01T23:56:31.813734: step 16880, loss 0.581049.
Test: 2018-08-01T23:56:32.342344: step 16880, loss 0.54757.
Train: 2018-08-01T23:56:32.528248: step 16881, loss 0.508207.
Train: 2018-08-01T23:56:32.694803: step 16882, loss 0.653769.
Train: 2018-08-01T23:56:32.869367: step 16883, loss 0.471974.
Train: 2018-08-01T23:56:33.038884: step 16884, loss 0.562772.
Train: 2018-08-01T23:56:33.206435: step 16885, loss 0.562763.
Train: 2018-08-01T23:56:33.372023: step 16886, loss 0.526507.
Train: 2018-08-01T23:56:33.542567: step 16887, loss 0.562746.
Train: 2018-08-01T23:56:33.710088: step 16888, loss 0.544634.
Train: 2018-08-01T23:56:33.889609: step 16889, loss 0.598925.
Train: 2018-08-01T23:56:34.052173: step 16890, loss 0.56272.
Test: 2018-08-01T23:56:34.595752: step 16890, loss 0.547585.
Train: 2018-08-01T23:56:34.765267: step 16891, loss 0.598823.
Train: 2018-08-01T23:56:34.929828: step 16892, loss 0.616777.
Train: 2018-08-01T23:56:35.095385: step 16893, loss 0.59864.
Train: 2018-08-01T23:56:35.270942: step 16894, loss 0.652344.
Train: 2018-08-01T23:56:35.436504: step 16895, loss 0.562595.
Train: 2018-08-01T23:56:35.608015: step 16896, loss 0.473482.
Train: 2018-08-01T23:56:35.771577: step 16897, loss 0.562545.
Train: 2018-08-01T23:56:35.934169: step 16898, loss 0.562529.
Train: 2018-08-01T23:56:36.098704: step 16899, loss 0.580227.
Train: 2018-08-01T23:56:36.261269: step 16900, loss 0.438767.
Test: 2018-08-01T23:56:36.798832: step 16900, loss 0.547701.
Train: 2018-08-01T23:56:37.572085: step 16901, loss 0.59784.
Train: 2018-08-01T23:56:37.739636: step 16902, loss 0.562493.
Train: 2018-08-01T23:56:37.901205: step 16903, loss 0.633056.
Train: 2018-08-01T23:56:38.067759: step 16904, loss 0.61531.
Train: 2018-08-01T23:56:38.239333: step 16905, loss 0.509753.
Train: 2018-08-01T23:56:38.412867: step 16906, loss 0.527367.
Train: 2018-08-01T23:56:38.579404: step 16907, loss 0.544923.
Train: 2018-08-01T23:56:38.745980: step 16908, loss 0.562446.
Train: 2018-08-01T23:56:38.908543: step 16909, loss 0.632453.
Train: 2018-08-01T23:56:39.074069: step 16910, loss 0.702229.
Test: 2018-08-01T23:56:39.609638: step 16910, loss 0.547839.
Train: 2018-08-01T23:56:39.778187: step 16911, loss 0.492762.
Train: 2018-08-01T23:56:39.949760: step 16912, loss 0.631927.
Train: 2018-08-01T23:56:40.111295: step 16913, loss 0.649043.
Train: 2018-08-01T23:56:40.275857: step 16914, loss 0.614182.
Train: 2018-08-01T23:56:40.439445: step 16915, loss 0.596773.
Train: 2018-08-01T23:56:40.611958: step 16916, loss 0.562398.
Train: 2018-08-01T23:56:40.778513: step 16917, loss 0.477159.
Train: 2018-08-01T23:56:40.949056: step 16918, loss 0.511376.
Train: 2018-08-01T23:56:41.116609: step 16919, loss 0.511445.
Train: 2018-08-01T23:56:41.281200: step 16920, loss 0.477507.
Test: 2018-08-01T23:56:41.822752: step 16920, loss 0.548176.
Train: 2018-08-01T23:56:41.997287: step 16921, loss 0.596397.
Train: 2018-08-01T23:56:42.163835: step 16922, loss 0.681408.
Train: 2018-08-01T23:56:42.327404: step 16923, loss 0.630318.
Train: 2018-08-01T23:56:42.503925: step 16924, loss 0.511593.
Train: 2018-08-01T23:56:42.666466: step 16925, loss 0.630101.
Train: 2018-08-01T23:56:42.830028: step 16926, loss 0.47799.
Train: 2018-08-01T23:56:42.993590: step 16927, loss 0.579305.
Train: 2018-08-01T23:56:43.160170: step 16928, loss 0.646763.
Train: 2018-08-01T23:56:43.325728: step 16929, loss 0.444576.
Train: 2018-08-01T23:56:43.492257: step 16930, loss 0.528755.
Test: 2018-08-01T23:56:44.030818: step 16930, loss 0.548321.
Train: 2018-08-01T23:56:44.193415: step 16931, loss 0.511878.
Train: 2018-08-01T23:56:44.359965: step 16932, loss 0.663679.
Train: 2018-08-01T23:56:44.532489: step 16933, loss 0.697413.
Train: 2018-08-01T23:56:44.711000: step 16934, loss 0.562436.
Train: 2018-08-01T23:56:44.894509: step 16935, loss 0.545633.
Train: 2018-08-01T23:56:45.063059: step 16936, loss 0.646396.
Train: 2018-08-01T23:56:45.227619: step 16937, loss 0.612715.
Train: 2018-08-01T23:56:45.391206: step 16938, loss 0.562475.
Train: 2018-08-01T23:56:45.561758: step 16939, loss 0.56249.
Train: 2018-08-01T23:56:45.731305: step 16940, loss 0.545869.
Test: 2018-08-01T23:56:46.254899: step 16940, loss 0.548605.
Train: 2018-08-01T23:56:46.418464: step 16941, loss 0.496079.
Train: 2018-08-01T23:56:46.582995: step 16942, loss 0.529306.
Train: 2018-08-01T23:56:46.748584: step 16943, loss 0.529292.
Train: 2018-08-01T23:56:46.917107: step 16944, loss 0.562506.
Train: 2018-08-01T23:56:47.085652: step 16945, loss 0.5625.
Train: 2018-08-01T23:56:47.253204: step 16946, loss 0.429245.
Train: 2018-08-01T23:56:47.419792: step 16947, loss 0.562476.
Train: 2018-08-01T23:56:47.594293: step 16948, loss 0.57921.
Train: 2018-08-01T23:56:47.760846: step 16949, loss 0.612807.
Train: 2018-08-01T23:56:47.927402: step 16950, loss 0.461594.
Test: 2018-08-01T23:56:48.464965: step 16950, loss 0.548319.
Train: 2018-08-01T23:56:48.637504: step 16951, loss 0.51187.
Train: 2018-08-01T23:56:48.810073: step 16952, loss 0.511696.
Train: 2018-08-01T23:56:48.976597: step 16953, loss 0.494531.
Train: 2018-08-01T23:56:49.144148: step 16954, loss 0.63057.
Train: 2018-08-01T23:56:49.305750: step 16955, loss 0.562398.
Train: 2018-08-01T23:56:49.481248: step 16956, loss 0.528125.
Train: 2018-08-01T23:56:49.647802: step 16957, loss 0.493661.
Train: 2018-08-01T23:56:49.807407: step 16958, loss 0.493425.
Train: 2018-08-01T23:56:49.979946: step 16959, loss 0.545093.
Train: 2018-08-01T23:56:50.149462: step 16960, loss 0.545037.
Test: 2018-08-01T23:56:50.692012: step 16960, loss 0.547822.
Train: 2018-08-01T23:56:50.857600: step 16961, loss 0.562428.
Train: 2018-08-01T23:56:51.024123: step 16962, loss 0.632433.
Train: 2018-08-01T23:56:51.186689: step 16963, loss 0.422204.
Train: 2018-08-01T23:56:51.353243: step 16964, loss 0.562469.
Train: 2018-08-01T23:56:51.522821: step 16965, loss 0.650713.
Train: 2018-08-01T23:56:51.694332: step 16966, loss 0.633183.
Train: 2018-08-01T23:56:51.855933: step 16967, loss 0.5625.
Train: 2018-08-01T23:56:52.026476: step 16968, loss 0.491779.
Train: 2018-08-01T23:56:52.192999: step 16969, loss 0.438618.
Train: 2018-08-01T23:56:52.359578: step 16970, loss 0.580269.
Test: 2018-08-01T23:56:52.886177: step 16970, loss 0.547659.
Train: 2018-08-01T23:56:53.051734: step 16971, loss 0.633638.
Train: 2018-08-01T23:56:53.217313: step 16972, loss 0.580333.
Train: 2018-08-01T23:56:53.378829: step 16973, loss 0.598128.
Train: 2018-08-01T23:56:53.547378: step 16974, loss 0.544763.
Train: 2018-08-01T23:56:53.716925: step 16975, loss 0.526986.
Train: 2018-08-01T23:56:53.881486: step 16976, loss 0.526981.
Train: 2018-08-01T23:56:54.045080: step 16977, loss 0.669299.
Train: 2018-08-01T23:56:54.215593: step 16978, loss 0.491449.
Train: 2018-08-01T23:56:54.379154: step 16979, loss 0.615854.
Train: 2018-08-01T23:56:54.545709: step 16980, loss 0.580289.
Test: 2018-08-01T23:56:55.085268: step 16980, loss 0.547674.
Train: 2018-08-01T23:56:55.253847: step 16981, loss 0.50932.
Train: 2018-08-01T23:56:55.417378: step 16982, loss 0.473886.
Train: 2018-08-01T23:56:55.590941: step 16983, loss 0.527049.
Train: 2018-08-01T23:56:55.757496: step 16984, loss 0.420495.
Train: 2018-08-01T23:56:55.923027: step 16985, loss 0.615955.
Train: 2018-08-01T23:56:56.087588: step 16986, loss 0.580395.
Train: 2018-08-01T23:56:56.250153: step 16987, loss 0.562577.
Train: 2018-08-01T23:56:56.412718: step 16988, loss 0.63402.
Train: 2018-08-01T23:56:56.572323: step 16989, loss 0.562582.
Train: 2018-08-01T23:56:56.745854: step 16990, loss 0.491194.
Test: 2018-08-01T23:56:57.281397: step 16990, loss 0.547632.
Train: 2018-08-01T23:56:57.453953: step 16991, loss 0.544729.
Train: 2018-08-01T23:56:57.618494: step 16992, loss 0.526862.
Train: 2018-08-01T23:56:57.781060: step 16993, loss 0.544718.
Train: 2018-08-01T23:56:57.948613: step 16994, loss 0.580492.
Train: 2018-08-01T23:56:58.116196: step 16995, loss 0.562605.
Train: 2018-08-01T23:56:58.278757: step 16996, loss 0.634208.
Train: 2018-08-01T23:56:58.443290: step 16997, loss 0.562599.
Train: 2018-08-01T23:56:58.621814: step 16998, loss 0.598328.
Train: 2018-08-01T23:56:58.784378: step 16999, loss 0.651798.
Train: 2018-08-01T23:56:58.949937: step 17000, loss 0.526955.
Test: 2018-08-01T23:56:59.482512: step 17000, loss 0.547662.
Train: 2018-08-01T23:57:00.242934: step 17001, loss 0.580304.
Train: 2018-08-01T23:57:00.410450: step 17002, loss 0.562522.
Train: 2018-08-01T23:57:00.580031: step 17003, loss 0.580201.
Train: 2018-08-01T23:57:00.754530: step 17004, loss 0.509521.
Train: 2018-08-01T23:57:00.925106: step 17005, loss 0.615388.
Train: 2018-08-01T23:57:01.093625: step 17006, loss 0.580073.
Train: 2018-08-01T23:57:01.263202: step 17007, loss 0.580025.
Train: 2018-08-01T23:57:01.428729: step 17008, loss 0.439762.
Train: 2018-08-01T23:57:01.595283: step 17009, loss 0.650047.
Train: 2018-08-01T23:57:01.764831: step 17010, loss 0.614917.
Test: 2018-08-01T23:57:02.299401: step 17010, loss 0.547815.
Train: 2018-08-01T23:57:02.473935: step 17011, loss 0.492612.
Train: 2018-08-01T23:57:02.635534: step 17012, loss 0.579862.
Train: 2018-08-01T23:57:02.798092: step 17013, loss 0.545008.
Train: 2018-08-01T23:57:02.959637: step 17014, loss 0.562419.
Train: 2018-08-01T23:57:03.120235: step 17015, loss 0.684089.
Train: 2018-08-01T23:57:03.284793: step 17016, loss 0.545071.
Train: 2018-08-01T23:57:03.448329: step 17017, loss 0.475887.
Train: 2018-08-01T23:57:03.611893: step 17018, loss 0.52782.
Train: 2018-08-01T23:57:03.775455: step 17019, loss 0.493249.
Train: 2018-08-01T23:57:03.940015: step 17020, loss 0.545103.
Test: 2018-08-01T23:57:04.472592: step 17020, loss 0.547906.
Train: 2018-08-01T23:57:04.644133: step 17021, loss 0.562407.
Train: 2018-08-01T23:57:04.821659: step 17022, loss 0.597069.
Train: 2018-08-01T23:57:04.989211: step 17023, loss 0.61441.
Train: 2018-08-01T23:57:05.154768: step 17024, loss 0.527759.
Train: 2018-08-01T23:57:05.321353: step 17025, loss 0.510439.
Train: 2018-08-01T23:57:05.485913: step 17026, loss 0.527742.
Train: 2018-08-01T23:57:05.650443: step 17027, loss 0.458307.
Train: 2018-08-01T23:57:05.815003: step 17028, loss 0.545028.
Train: 2018-08-01T23:57:05.981557: step 17029, loss 0.492711.
Train: 2018-08-01T23:57:06.144123: step 17030, loss 0.544958.
Test: 2018-08-01T23:57:06.672740: step 17030, loss 0.547772.
Train: 2018-08-01T23:57:06.839265: step 17031, loss 0.579976.
Train: 2018-08-01T23:57:06.998869: step 17032, loss 0.615156.
Train: 2018-08-01T23:57:07.168385: step 17033, loss 0.544881.
Train: 2018-08-01T23:57:07.334940: step 17034, loss 0.632904.
Train: 2018-08-01T23:57:07.498502: step 17035, loss 0.492038.
Train: 2018-08-01T23:57:07.668049: step 17036, loss 0.544855.
Train: 2018-08-01T23:57:07.831648: step 17037, loss 0.58013.
Train: 2018-08-01T23:57:07.995175: step 17038, loss 0.597797.
Train: 2018-08-01T23:57:08.157740: step 17039, loss 0.527185.
Train: 2018-08-01T23:57:08.328285: step 17040, loss 0.491858.
Test: 2018-08-01T23:57:08.852882: step 17040, loss 0.547697.
Train: 2018-08-01T23:57:09.019436: step 17041, loss 0.544822.
Train: 2018-08-01T23:57:09.183997: step 17042, loss 0.580209.
Train: 2018-08-01T23:57:09.352546: step 17043, loss 0.562515.
Train: 2018-08-01T23:57:09.523090: step 17044, loss 0.562519.
Train: 2018-08-01T23:57:09.683661: step 17045, loss 0.615709.
Train: 2018-08-01T23:57:09.851212: step 17046, loss 0.704291.
Train: 2018-08-01T23:57:10.012782: step 17047, loss 0.562501.
Train: 2018-08-01T23:57:10.176343: step 17048, loss 0.544845.
Train: 2018-08-01T23:57:10.338940: step 17049, loss 0.738569.
Train: 2018-08-01T23:57:10.511473: step 17050, loss 0.597529.
Test: 2018-08-01T23:57:11.036057: step 17050, loss 0.547808.
Train: 2018-08-01T23:57:11.211608: step 17051, loss 0.614829.
Train: 2018-08-01T23:57:11.379130: step 17052, loss 0.666736.
Train: 2018-08-01T23:57:11.548675: step 17053, loss 0.545113.
Train: 2018-08-01T23:57:11.716228: step 17054, loss 0.61402.
Train: 2018-08-01T23:57:11.878824: step 17055, loss 0.579518.
Train: 2018-08-01T23:57:12.053357: step 17056, loss 0.562402.
Train: 2018-08-01T23:57:12.219881: step 17057, loss 0.409726.
Train: 2018-08-01T23:57:12.388431: step 17058, loss 0.545479.
Train: 2018-08-01T23:57:12.561967: step 17059, loss 0.545505.
Train: 2018-08-01T23:57:12.725536: step 17060, loss 0.477921.
Test: 2018-08-01T23:57:13.270074: step 17060, loss 0.548262.
Train: 2018-08-01T23:57:13.433637: step 17061, loss 0.47788.
Train: 2018-08-01T23:57:13.598221: step 17062, loss 0.460797.
Train: 2018-08-01T23:57:13.761759: step 17063, loss 0.562408.
Train: 2018-08-01T23:57:13.925348: step 17064, loss 0.579435.
Train: 2018-08-01T23:57:14.097892: step 17065, loss 0.545329.
Train: 2018-08-01T23:57:14.265444: step 17066, loss 0.459747.
Train: 2018-08-01T23:57:14.429972: step 17067, loss 0.545229.
Train: 2018-08-01T23:57:14.596528: step 17068, loss 0.682967.
Train: 2018-08-01T23:57:14.760091: step 17069, loss 0.61414.
Train: 2018-08-01T23:57:14.922657: step 17070, loss 0.5624.
Test: 2018-08-01T23:57:15.459222: step 17070, loss 0.547946.
Train: 2018-08-01T23:57:15.627771: step 17071, loss 0.545139.
Train: 2018-08-01T23:57:15.789339: step 17072, loss 0.614216.
Train: 2018-08-01T23:57:15.962906: step 17073, loss 0.666009.
Train: 2018-08-01T23:57:16.135440: step 17074, loss 0.66584.
Train: 2018-08-01T23:57:16.300971: step 17075, loss 0.63116.
Train: 2018-08-01T23:57:16.463562: step 17076, loss 0.665177.
Train: 2018-08-01T23:57:16.636104: step 17077, loss 0.647657.
Train: 2018-08-01T23:57:16.806619: step 17078, loss 0.52849.
Train: 2018-08-01T23:57:16.974201: step 17079, loss 0.562424.
Train: 2018-08-01T23:57:17.135765: step 17080, loss 0.646554.
Test: 2018-08-01T23:57:17.668352: step 17080, loss 0.548439.
Train: 2018-08-01T23:57:17.839884: step 17081, loss 0.545716.
Train: 2018-08-01T23:57:18.006413: step 17082, loss 0.545803.
Train: 2018-08-01T23:57:18.173990: step 17083, loss 0.678905.
Train: 2018-08-01T23:57:18.335561: step 17084, loss 0.612201.
Train: 2018-08-01T23:57:18.502118: step 17085, loss 0.529618.
Train: 2018-08-01T23:57:18.672632: step 17086, loss 0.562607.
Train: 2018-08-01T23:57:18.844203: step 17087, loss 0.529899.
Train: 2018-08-01T23:57:19.009762: step 17088, loss 0.546325.
Train: 2018-08-01T23:57:19.175288: step 17089, loss 0.644221.
Train: 2018-08-01T23:57:19.354808: step 17090, loss 0.644048.
Test: 2018-08-01T23:57:19.876413: step 17090, loss 0.549161.
Train: 2018-08-01T23:57:20.037981: step 17091, loss 0.578956.
Train: 2018-08-01T23:57:20.205564: step 17092, loss 0.562777.
Train: 2018-08-01T23:57:20.372119: step 17093, loss 0.530558.
Train: 2018-08-01T23:57:20.552630: step 17094, loss 0.54673.
Train: 2018-08-01T23:57:20.718163: step 17095, loss 0.627182.
Train: 2018-08-01T23:57:20.882723: step 17096, loss 0.627097.
Train: 2018-08-01T23:57:21.054264: step 17097, loss 0.59494.
Train: 2018-08-01T23:57:21.221816: step 17098, loss 0.594899.
Train: 2018-08-01T23:57:21.385379: step 17099, loss 0.562968.
Train: 2018-08-01T23:57:21.548942: step 17100, loss 0.419771.
Test: 2018-08-01T23:57:22.075565: step 17100, loss 0.549649.
Train: 2018-08-01T23:57:22.875152: step 17101, loss 0.594839.
Train: 2018-08-01T23:57:23.046691: step 17102, loss 0.515176.
Train: 2018-08-01T23:57:23.211223: step 17103, loss 0.483174.
Train: 2018-08-01T23:57:23.381767: step 17104, loss 0.514914.
Train: 2018-08-01T23:57:23.549319: step 17105, loss 0.546811.
Train: 2018-08-01T23:57:23.714876: step 17106, loss 0.61116.
Train: 2018-08-01T23:57:23.881431: step 17107, loss 0.546629.
Train: 2018-08-01T23:57:24.046017: step 17108, loss 0.676166.
Train: 2018-08-01T23:57:24.204598: step 17109, loss 0.578957.
Train: 2018-08-01T23:57:24.382118: step 17110, loss 0.57896.
Test: 2018-08-01T23:57:24.909709: step 17110, loss 0.54912.
Train: 2018-08-01T23:57:25.073244: step 17111, loss 0.595206.
Train: 2018-08-01T23:57:25.239825: step 17112, loss 0.513977.
Train: 2018-08-01T23:57:25.404359: step 17113, loss 0.481374.
Train: 2018-08-01T23:57:25.570955: step 17114, loss 0.595289.
Train: 2018-08-01T23:57:25.732482: step 17115, loss 0.562657.
Train: 2018-08-01T23:57:25.895076: step 17116, loss 0.595372.
Train: 2018-08-01T23:57:26.058631: step 17117, loss 0.513459.
Train: 2018-08-01T23:57:26.223172: step 17118, loss 0.546182.
Train: 2018-08-01T23:57:26.387757: step 17119, loss 0.611964.
Train: 2018-08-01T23:57:26.558301: step 17120, loss 0.463674.
Test: 2018-08-01T23:57:27.073928: step 17120, loss 0.548706.
Train: 2018-08-01T23:57:27.233470: step 17121, loss 0.529487.
Train: 2018-08-01T23:57:27.400058: step 17122, loss 0.545941.
Train: 2018-08-01T23:57:27.566579: step 17123, loss 0.579137.
Train: 2018-08-01T23:57:27.731143: step 17124, loss 0.595847.
Train: 2018-08-01T23:57:27.896697: step 17125, loss 0.545756.
Train: 2018-08-01T23:57:28.059294: step 17126, loss 0.646221.
Train: 2018-08-01T23:57:28.222825: step 17127, loss 0.545689.
Train: 2018-08-01T23:57:28.391383: step 17128, loss 0.461748.
Train: 2018-08-01T23:57:28.554970: step 17129, loss 0.612914.
Train: 2018-08-01T23:57:28.722490: step 17130, loss 0.52873.
Test: 2018-08-01T23:57:29.256066: step 17130, loss 0.548287.
Train: 2018-08-01T23:57:29.421650: step 17131, loss 0.528658.
Train: 2018-08-01T23:57:29.586181: step 17132, loss 0.579339.
Train: 2018-08-01T23:57:29.761737: step 17133, loss 0.647176.
Train: 2018-08-01T23:57:29.929289: step 17134, loss 0.630256.
Train: 2018-08-01T23:57:30.092826: step 17135, loss 0.579366.
Train: 2018-08-01T23:57:30.259382: step 17136, loss 0.528529.
Train: 2018-08-01T23:57:30.422945: step 17137, loss 0.562414.
Train: 2018-08-01T23:57:30.588502: step 17138, loss 0.579356.
Train: 2018-08-01T23:57:30.753061: step 17139, loss 0.680983.
Train: 2018-08-01T23:57:30.916624: step 17140, loss 0.528605.
Test: 2018-08-01T23:57:31.445212: step 17140, loss 0.548282.
Train: 2018-08-01T23:57:31.609773: step 17141, loss 0.579313.
Train: 2018-08-01T23:57:31.777324: step 17142, loss 0.494953.
Train: 2018-08-01T23:57:31.944876: step 17143, loss 0.528693.
Train: 2018-08-01T23:57:32.107482: step 17144, loss 0.562427.
Train: 2018-08-01T23:57:32.270006: step 17145, loss 0.579309.
Train: 2018-08-01T23:57:32.434567: step 17146, loss 0.51176.
Train: 2018-08-01T23:57:32.604114: step 17147, loss 0.511706.
Train: 2018-08-01T23:57:32.766678: step 17148, loss 0.596281.
Train: 2018-08-01T23:57:32.933234: step 17149, loss 0.545462.
Train: 2018-08-01T23:57:33.099789: step 17150, loss 0.545438.
Test: 2018-08-01T23:57:33.627378: step 17150, loss 0.548176.
Train: 2018-08-01T23:57:33.789974: step 17151, loss 0.596396.
Train: 2018-08-01T23:57:33.958493: step 17152, loss 0.528389.
Train: 2018-08-01T23:57:34.132028: step 17153, loss 0.494289.
Train: 2018-08-01T23:57:34.297586: step 17154, loss 0.374699.
Train: 2018-08-01T23:57:34.469153: step 17155, loss 0.493837.
Train: 2018-08-01T23:57:34.633718: step 17156, loss 0.596847.
Train: 2018-08-01T23:57:34.797265: step 17157, loss 0.61774.
Train: 2018-08-01T23:57:34.968791: step 17158, loss 0.59709.
Train: 2018-08-01T23:57:35.131357: step 17159, loss 0.57979.
Train: 2018-08-01T23:57:35.305891: step 17160, loss 0.649426.
Test: 2018-08-01T23:57:35.832488: step 17160, loss 0.547846.
Train: 2018-08-01T23:57:35.999039: step 17161, loss 0.440592.
Train: 2018-08-01T23:57:36.165592: step 17162, loss 0.632161.
Train: 2018-08-01T23:57:36.329156: step 17163, loss 0.52754.
Train: 2018-08-01T23:57:36.495710: step 17164, loss 0.527512.
Train: 2018-08-01T23:57:36.663262: step 17165, loss 0.509992.
Train: 2018-08-01T23:57:36.826824: step 17166, loss 0.632494.
Train: 2018-08-01T23:57:36.991386: step 17167, loss 0.57997.
Train: 2018-08-01T23:57:37.157970: step 17168, loss 0.579975.
Train: 2018-08-01T23:57:37.329513: step 17169, loss 0.562448.
Train: 2018-08-01T23:57:37.494066: step 17170, loss 0.562448.
Test: 2018-08-01T23:57:38.036598: step 17170, loss 0.547776.
Train: 2018-08-01T23:57:38.210127: step 17171, loss 0.702601.
Train: 2018-08-01T23:57:38.380702: step 17172, loss 0.649851.
Train: 2018-08-01T23:57:38.546262: step 17173, loss 0.632141.
Train: 2018-08-01T23:57:38.709791: step 17174, loss 0.545049.
Train: 2018-08-01T23:57:38.874351: step 17175, loss 0.527789.
Train: 2018-08-01T23:57:39.039909: step 17176, loss 0.596932.
Train: 2018-08-01T23:57:39.205497: step 17177, loss 0.476303.
Train: 2018-08-01T23:57:39.379002: step 17178, loss 0.493609.
Train: 2018-08-01T23:57:39.548549: step 17179, loss 0.596784.
Train: 2018-08-01T23:57:39.712112: step 17180, loss 0.613942.
Test: 2018-08-01T23:57:40.244687: step 17180, loss 0.548028.
Train: 2018-08-01T23:57:40.410276: step 17181, loss 0.596715.
Train: 2018-08-01T23:57:40.579818: step 17182, loss 0.579527.
Train: 2018-08-01T23:57:40.741361: step 17183, loss 0.562398.
Train: 2018-08-01T23:57:40.904924: step 17184, loss 0.613624.
Train: 2018-08-01T23:57:41.068486: step 17185, loss 0.562402.
Train: 2018-08-01T23:57:41.233071: step 17186, loss 0.579413.
Train: 2018-08-01T23:57:41.397606: step 17187, loss 0.579385.
Train: 2018-08-01T23:57:41.557179: step 17188, loss 0.5963.
Train: 2018-08-01T23:57:41.734705: step 17189, loss 0.596233.
Train: 2018-08-01T23:57:41.903287: step 17190, loss 0.697358.
Test: 2018-08-01T23:57:42.440817: step 17190, loss 0.548379.
Train: 2018-08-01T23:57:42.602386: step 17191, loss 0.461655.
Train: 2018-08-01T23:57:42.768939: step 17192, loss 0.512167.
Train: 2018-08-01T23:57:42.940515: step 17193, loss 0.545717.
Train: 2018-08-01T23:57:43.108033: step 17194, loss 0.579199.
Train: 2018-08-01T23:57:43.275586: step 17195, loss 0.512309.
Train: 2018-08-01T23:57:43.457100: step 17196, loss 0.512303.
Train: 2018-08-01T23:57:43.620663: step 17197, loss 0.579202.
Train: 2018-08-01T23:57:43.788248: step 17198, loss 0.528965.
Train: 2018-08-01T23:57:43.950806: step 17199, loss 0.562455.
Train: 2018-08-01T23:57:44.113346: step 17200, loss 0.56245.
Test: 2018-08-01T23:57:44.645923: step 17200, loss 0.548374.
Train: 2018-08-01T23:57:45.401941: step 17201, loss 0.579248.
Train: 2018-08-01T23:57:45.565535: step 17202, loss 0.562441.
Train: 2018-08-01T23:57:45.733087: step 17203, loss 0.51195.
Train: 2018-08-01T23:57:45.904613: step 17204, loss 0.545578.
Train: 2018-08-01T23:57:46.069188: step 17205, loss 0.545544.
Train: 2018-08-01T23:57:46.238703: step 17206, loss 0.596241.
Train: 2018-08-01T23:57:46.400303: step 17207, loss 0.562416.
Train: 2018-08-01T23:57:46.563865: step 17208, loss 0.613257.
Train: 2018-08-01T23:57:46.734410: step 17209, loss 0.47765.
Train: 2018-08-01T23:57:46.897944: step 17210, loss 0.562409.
Test: 2018-08-01T23:57:47.424535: step 17210, loss 0.548169.
Train: 2018-08-01T23:57:47.589095: step 17211, loss 0.630409.
Train: 2018-08-01T23:57:47.763627: step 17212, loss 0.630429.
Train: 2018-08-01T23:57:47.935194: step 17213, loss 0.443443.
Train: 2018-08-01T23:57:48.097734: step 17214, loss 0.528377.
Train: 2018-08-01T23:57:48.262294: step 17215, loss 0.579441.
Train: 2018-08-01T23:57:48.428849: step 17216, loss 0.5624.
Train: 2018-08-01T23:57:48.599394: step 17217, loss 0.562399.
Train: 2018-08-01T23:57:48.761992: step 17218, loss 0.528214.
Train: 2018-08-01T23:57:48.927547: step 17219, loss 0.630858.
Train: 2018-08-01T23:57:49.095068: step 17220, loss 0.545277.
Test: 2018-08-01T23:57:49.634625: step 17220, loss 0.548054.
Train: 2018-08-01T23:57:49.800183: step 17221, loss 0.511013.
Train: 2018-08-01T23:57:49.963746: step 17222, loss 0.579544.
Train: 2018-08-01T23:57:50.139278: step 17223, loss 0.545235.
Train: 2018-08-01T23:57:50.305831: step 17224, loss 0.528042.
Train: 2018-08-01T23:57:50.472385: step 17225, loss 0.527997.
Train: 2018-08-01T23:57:50.644951: step 17226, loss 0.579627.
Train: 2018-08-01T23:57:50.811480: step 17227, loss 0.493399.
Train: 2018-08-01T23:57:50.983021: step 17228, loss 0.545117.
Train: 2018-08-01T23:57:51.145587: step 17229, loss 0.614369.
Train: 2018-08-01T23:57:51.312172: step 17230, loss 0.51039.
Test: 2018-08-01T23:57:51.844717: step 17230, loss 0.547869.
Train: 2018-08-01T23:57:52.009279: step 17231, loss 0.510307.
Train: 2018-08-01T23:57:52.174835: step 17232, loss 0.666857.
Train: 2018-08-01T23:57:52.339395: step 17233, loss 0.545007.
Train: 2018-08-01T23:57:52.509939: step 17234, loss 0.544998.
Train: 2018-08-01T23:57:52.675528: step 17235, loss 0.544988.
Train: 2018-08-01T23:57:52.841053: step 17236, loss 0.597338.
Train: 2018-08-01T23:57:53.005614: step 17237, loss 0.597348.
Train: 2018-08-01T23:57:53.173197: step 17238, loss 0.510071.
Train: 2018-08-01T23:57:53.340718: step 17239, loss 0.579891.
Train: 2018-08-01T23:57:53.507274: step 17240, loss 0.579893.
Test: 2018-08-01T23:57:54.037885: step 17240, loss 0.547813.
Train: 2018-08-01T23:57:54.203412: step 17241, loss 0.614801.
Train: 2018-08-01T23:57:54.365978: step 17242, loss 0.527547.
Train: 2018-08-01T23:57:54.532563: step 17243, loss 0.701881.
Train: 2018-08-01T23:57:54.699113: step 17244, loss 0.510243.
Train: 2018-08-01T23:57:54.864644: step 17245, loss 0.562413.
Train: 2018-08-01T23:57:55.036186: step 17246, loss 0.597096.
Train: 2018-08-01T23:57:55.199749: step 17247, loss 0.545092.
Train: 2018-08-01T23:57:55.365306: step 17248, loss 0.545113.
Train: 2018-08-01T23:57:55.531860: step 17249, loss 0.596949.
Train: 2018-08-01T23:57:55.697418: step 17250, loss 0.579649.
Test: 2018-08-01T23:57:56.228000: step 17250, loss 0.547975.
Train: 2018-08-01T23:57:56.395551: step 17251, loss 0.510727.
Train: 2018-08-01T23:57:56.566127: step 17252, loss 0.493548.
Train: 2018-08-01T23:57:56.738635: step 17253, loss 0.493524.
Train: 2018-08-01T23:57:56.902198: step 17254, loss 0.665838.
Train: 2018-08-01T23:57:57.065760: step 17255, loss 0.476226.
Train: 2018-08-01T23:57:57.227328: step 17256, loss 0.47615.
Train: 2018-08-01T23:57:57.392917: step 17257, loss 0.545119.
Train: 2018-08-01T23:57:57.561467: step 17258, loss 0.579724.
Train: 2018-08-01T23:57:57.726993: step 17259, loss 0.56241.
Train: 2018-08-01T23:57:57.891554: step 17260, loss 0.388744.
Test: 2018-08-01T23:57:58.427121: step 17260, loss 0.547831.
Train: 2018-08-01T23:57:58.596668: step 17261, loss 0.492714.
Train: 2018-08-01T23:57:58.775190: step 17262, loss 0.579938.
Train: 2018-08-01T23:57:58.940779: step 17263, loss 0.580013.
Train: 2018-08-01T23:57:59.119296: step 17264, loss 0.527268.
Train: 2018-08-01T23:57:59.289839: step 17265, loss 0.491886.
Train: 2018-08-01T23:57:59.458374: step 17266, loss 0.668772.
Train: 2018-08-01T23:57:59.634923: step 17267, loss 0.615734.
Train: 2018-08-01T23:57:59.801448: step 17268, loss 0.50929.
Train: 2018-08-01T23:57:59.964038: step 17269, loss 0.580304.
Train: 2018-08-01T23:58:00.143533: step 17270, loss 0.509211.
Test: 2018-08-01T23:58:00.680130: step 17270, loss 0.54765.
Train: 2018-08-01T23:58:00.842664: step 17271, loss 0.615947.
Train: 2018-08-01T23:58:01.012215: step 17272, loss 0.633765.
Train: 2018-08-01T23:58:01.175774: step 17273, loss 0.54476.
Train: 2018-08-01T23:58:01.340333: step 17274, loss 0.65143.
Train: 2018-08-01T23:58:01.505922: step 17275, loss 0.544783.
Train: 2018-08-01T23:58:01.675437: step 17276, loss 0.456203.
Train: 2018-08-01T23:58:01.842024: step 17277, loss 0.562517.
Train: 2018-08-01T23:58:02.013534: step 17278, loss 0.509371.
Train: 2018-08-01T23:58:02.181095: step 17279, loss 0.456181.
Train: 2018-08-01T23:58:02.345672: step 17280, loss 0.58029.
Test: 2018-08-01T23:58:02.873237: step 17280, loss 0.547658.
Train: 2018-08-01T23:58:03.034804: step 17281, loss 0.598099.
Train: 2018-08-01T23:58:03.247570: step 17282, loss 0.615893.
Train: 2018-08-01T23:58:03.411108: step 17283, loss 0.544767.
Train: 2018-08-01T23:58:03.573716: step 17284, loss 0.491451.
Train: 2018-08-01T23:58:03.739256: step 17285, loss 0.687033.
Train: 2018-08-01T23:58:03.907779: step 17286, loss 0.52701.
Train: 2018-08-01T23:58:04.071342: step 17287, loss 0.544781.
Train: 2018-08-01T23:58:04.235928: step 17288, loss 0.491557.
Train: 2018-08-01T23:58:04.400463: step 17289, loss 0.615781.
Train: 2018-08-01T23:58:04.580979: step 17290, loss 0.58027.
Test: 2018-08-01T23:58:05.126522: step 17290, loss 0.547677.
Train: 2018-08-01T23:58:05.297065: step 17291, loss 0.456151.
Train: 2018-08-01T23:58:05.462623: step 17292, loss 0.580267.
Train: 2018-08-01T23:58:05.630175: step 17293, loss 0.544786.
Train: 2018-08-01T23:58:05.801741: step 17294, loss 0.456038.
Train: 2018-08-01T23:58:05.972260: step 17295, loss 0.42033.
Train: 2018-08-01T23:58:06.142805: step 17296, loss 0.598237.
Train: 2018-08-01T23:58:06.312378: step 17297, loss 0.562594.
Train: 2018-08-01T23:58:06.475914: step 17298, loss 0.65214.
Train: 2018-08-01T23:58:06.642469: step 17299, loss 0.616356.
Train: 2018-08-01T23:58:06.810047: step 17300, loss 0.473085.
Test: 2018-08-01T23:58:07.346586: step 17300, loss 0.547616.
Train: 2018-08-01T23:58:08.151106: step 17301, loss 0.544702.
Train: 2018-08-01T23:58:08.315681: step 17302, loss 0.580554.
Train: 2018-08-01T23:58:08.482205: step 17303, loss 0.65229.
Train: 2018-08-01T23:58:08.646765: step 17304, loss 0.670097.
Train: 2018-08-01T23:58:08.814317: step 17305, loss 0.598332.
Train: 2018-08-01T23:58:08.979893: step 17306, loss 0.509107.
Train: 2018-08-01T23:58:09.145463: step 17307, loss 0.56255.
Train: 2018-08-01T23:58:09.309992: step 17308, loss 0.615792.
Train: 2018-08-01T23:58:09.475549: step 17309, loss 0.597933.
Train: 2018-08-01T23:58:09.638114: step 17310, loss 0.703772.
Test: 2018-08-01T23:58:10.172685: step 17310, loss 0.547745.
Train: 2018-08-01T23:58:10.340238: step 17311, loss 0.527305.
Train: 2018-08-01T23:58:10.504798: step 17312, loss 0.544931.
Train: 2018-08-01T23:58:10.672349: step 17313, loss 0.49258.
Train: 2018-08-01T23:58:10.836936: step 17314, loss 0.475277.
Train: 2018-08-01T23:58:11.001470: step 17315, loss 0.545006.
Train: 2018-08-01T23:58:11.169022: step 17316, loss 0.649487.
Train: 2018-08-01T23:58:11.340594: step 17317, loss 0.68412.
Train: 2018-08-01T23:58:11.506121: step 17318, loss 0.614408.
Train: 2018-08-01T23:58:11.676682: step 17319, loss 0.527859.
Train: 2018-08-01T23:58:11.843220: step 17320, loss 0.665753.
Test: 2018-08-01T23:58:12.374824: step 17320, loss 0.548029.
Train: 2018-08-01T23:58:12.546369: step 17321, loss 0.631034.
Train: 2018-08-01T23:58:12.715920: step 17322, loss 0.545317.
Train: 2018-08-01T23:58:12.883439: step 17323, loss 0.47731.
Train: 2018-08-01T23:58:13.062958: step 17324, loss 0.56241.
Train: 2018-08-01T23:58:13.226521: step 17325, loss 0.613262.
Train: 2018-08-01T23:58:13.394074: step 17326, loss 0.562422.
Train: 2018-08-01T23:58:13.564617: step 17327, loss 0.545557.
Train: 2018-08-01T23:58:13.730175: step 17328, loss 0.57928.
Train: 2018-08-01T23:58:13.897728: step 17329, loss 0.52881.
Train: 2018-08-01T23:58:14.062318: step 17330, loss 0.512045.
Test: 2018-08-01T23:58:14.599852: step 17330, loss 0.548379.
Train: 2018-08-01T23:58:14.766405: step 17331, loss 0.612845.
Train: 2018-08-01T23:58:14.935962: step 17332, loss 0.579237.
Train: 2018-08-01T23:58:15.098518: step 17333, loss 0.596001.
Train: 2018-08-01T23:58:15.265072: step 17334, loss 0.595968.
Train: 2018-08-01T23:58:15.430630: step 17335, loss 0.595928.
Train: 2018-08-01T23:58:15.599180: step 17336, loss 0.495672.
Train: 2018-08-01T23:58:15.761789: step 17337, loss 0.529094.
Train: 2018-08-01T23:58:15.929297: step 17338, loss 0.512392.
Train: 2018-08-01T23:58:16.094853: step 17339, loss 0.478917.
Train: 2018-08-01T23:58:16.261409: step 17340, loss 0.545715.
Test: 2018-08-01T23:58:16.790024: step 17340, loss 0.548396.
Train: 2018-08-01T23:58:16.969542: step 17341, loss 0.562451.
Train: 2018-08-01T23:58:17.140091: step 17342, loss 0.478343.
Train: 2018-08-01T23:58:17.303622: step 17343, loss 0.596173.
Train: 2018-08-01T23:58:17.468216: step 17344, loss 0.545508.
Train: 2018-08-01T23:58:17.630748: step 17345, loss 0.511552.
Train: 2018-08-01T23:58:17.794310: step 17346, loss 0.528399.
Train: 2018-08-01T23:58:17.956877: step 17347, loss 0.579458.
Train: 2018-08-01T23:58:18.122465: step 17348, loss 0.630802.
Train: 2018-08-01T23:58:18.287020: step 17349, loss 0.562397.
Train: 2018-08-01T23:58:18.451555: step 17350, loss 0.562397.
Test: 2018-08-01T23:58:18.982135: step 17350, loss 0.548023.
Train: 2018-08-01T23:58:19.152685: step 17351, loss 0.510899.
Train: 2018-08-01T23:58:19.320231: step 17352, loss 0.562397.
Train: 2018-08-01T23:58:19.481800: step 17353, loss 0.527954.
Train: 2018-08-01T23:58:19.647357: step 17354, loss 0.596909.
Train: 2018-08-01T23:58:19.820919: step 17355, loss 0.545128.
Train: 2018-08-01T23:58:19.984487: step 17356, loss 0.631589.
Train: 2018-08-01T23:58:20.150044: step 17357, loss 0.597003.
Train: 2018-08-01T23:58:20.319561: step 17358, loss 0.545111.
Train: 2018-08-01T23:58:20.484121: step 17359, loss 0.579696.
Train: 2018-08-01T23:58:20.645714: step 17360, loss 0.527831.
Test: 2018-08-01T23:58:21.187242: step 17360, loss 0.547926.
Train: 2018-08-01T23:58:21.366761: step 17361, loss 0.59698.
Train: 2018-08-01T23:58:21.531321: step 17362, loss 0.648812.
Train: 2018-08-01T23:58:21.696910: step 17363, loss 0.52789.
Train: 2018-08-01T23:58:21.871437: step 17364, loss 0.527921.
Train: 2018-08-01T23:58:22.039961: step 17365, loss 0.631329.
Train: 2018-08-01T23:58:22.201530: step 17366, loss 0.527977.
Train: 2018-08-01T23:58:22.367087: step 17367, loss 0.528001.
Train: 2018-08-01T23:58:22.536634: step 17368, loss 0.562397.
Train: 2018-08-01T23:58:22.701218: step 17369, loss 0.596782.
Train: 2018-08-01T23:58:22.868771: step 17370, loss 0.562397.
Test: 2018-08-01T23:58:23.410298: step 17370, loss 0.548017.
Train: 2018-08-01T23:58:23.583863: step 17371, loss 0.682598.
Train: 2018-08-01T23:58:23.753409: step 17372, loss 0.579531.
Train: 2018-08-01T23:58:23.922927: step 17373, loss 0.528205.
Train: 2018-08-01T23:58:24.090479: step 17374, loss 0.494115.
Train: 2018-08-01T23:58:24.253079: step 17375, loss 0.511206.
Train: 2018-08-01T23:58:24.416608: step 17376, loss 0.545328.
Train: 2018-08-01T23:58:24.579208: step 17377, loss 0.545317.
Train: 2018-08-01T23:58:24.743734: step 17378, loss 0.528207.
Train: 2018-08-01T23:58:24.908294: step 17379, loss 0.579513.
Train: 2018-08-01T23:58:25.073882: step 17380, loss 0.528135.
Test: 2018-08-01T23:58:25.611415: step 17380, loss 0.548033.
Train: 2018-08-01T23:58:25.782955: step 17381, loss 0.425177.
Train: 2018-08-01T23:58:25.946519: step 17382, loss 0.665623.
Train: 2018-08-01T23:58:26.125072: step 17383, loss 0.596851.
Train: 2018-08-01T23:58:26.286609: step 17384, loss 0.545161.
Train: 2018-08-01T23:58:26.465170: step 17385, loss 0.545148.
Train: 2018-08-01T23:58:26.627697: step 17386, loss 0.596938.
Train: 2018-08-01T23:58:26.794252: step 17387, loss 0.527852.
Train: 2018-08-01T23:58:26.961830: step 17388, loss 0.562403.
Train: 2018-08-01T23:58:27.128389: step 17389, loss 0.545104.
Train: 2018-08-01T23:58:27.303890: step 17390, loss 0.562406.
Test: 2018-08-01T23:58:27.828525: step 17390, loss 0.547898.
Train: 2018-08-01T23:58:27.992059: step 17391, loss 0.562407.
Train: 2018-08-01T23:58:28.163594: step 17392, loss 0.527734.
Train: 2018-08-01T23:58:28.341117: step 17393, loss 0.597121.
Train: 2018-08-01T23:58:28.505677: step 17394, loss 0.562412.
Train: 2018-08-01T23:58:28.679214: step 17395, loss 0.510313.
Train: 2018-08-01T23:58:28.842775: step 17396, loss 0.52765.
Train: 2018-08-01T23:58:29.008333: step 17397, loss 0.510204.
Train: 2018-08-01T23:58:29.186889: step 17398, loss 0.5973.
Train: 2018-08-01T23:58:29.355436: step 17399, loss 0.562431.
Train: 2018-08-01T23:58:29.522988: step 17400, loss 0.527489.
Test: 2018-08-01T23:58:30.065519: step 17400, loss 0.54779.
Train: 2018-08-01T23:58:30.876636: step 17401, loss 0.544945.
Train: 2018-08-01T23:58:31.043191: step 17402, loss 0.509893.
Train: 2018-08-01T23:58:31.210768: step 17403, loss 0.580005.
Train: 2018-08-01T23:58:31.386303: step 17404, loss 0.474599.
Train: 2018-08-01T23:58:31.550834: step 17405, loss 0.580089.
Train: 2018-08-01T23:58:31.723404: step 17406, loss 0.509557.
Train: 2018-08-01T23:58:31.894914: step 17407, loss 0.562502.
Train: 2018-08-01T23:58:32.059501: step 17408, loss 0.562515.
Train: 2018-08-01T23:58:32.228023: step 17409, loss 0.598012.
Train: 2018-08-01T23:58:32.399565: step 17410, loss 0.651322.
Test: 2018-08-01T23:58:32.942114: step 17410, loss 0.547668.
Train: 2018-08-01T23:58:33.154928: step 17411, loss 0.56253.
Train: 2018-08-01T23:58:33.319519: step 17412, loss 0.527048.
Train: 2018-08-01T23:58:33.494021: step 17413, loss 0.491579.
Train: 2018-08-01T23:58:33.660607: step 17414, loss 0.56253.
Train: 2018-08-01T23:58:33.825137: step 17415, loss 0.50926.
Train: 2018-08-01T23:58:33.987728: step 17416, loss 0.58032.
Train: 2018-08-01T23:58:34.159243: step 17417, loss 0.526973.
Train: 2018-08-01T23:58:34.325831: step 17418, loss 0.544752.
Train: 2018-08-01T23:58:34.494379: step 17419, loss 0.651666.
Train: 2018-08-01T23:58:34.664891: step 17420, loss 0.633811.
Test: 2018-08-01T23:58:35.195498: step 17420, loss 0.547653.
Train: 2018-08-01T23:58:35.362028: step 17421, loss 0.633697.
Train: 2018-08-01T23:58:35.531574: step 17422, loss 0.686749.
Train: 2018-08-01T23:58:35.692176: step 17423, loss 0.544822.
Train: 2018-08-01T23:58:35.856737: step 17424, loss 0.562478.
Train: 2018-08-01T23:58:36.030273: step 17425, loss 0.597591.
Train: 2018-08-01T23:58:36.195799: step 17426, loss 0.544936.
Train: 2018-08-01T23:58:36.364374: step 17427, loss 0.475143.
Train: 2018-08-01T23:58:36.539897: step 17428, loss 0.440396.
Train: 2018-08-01T23:58:36.718402: step 17429, loss 0.649618.
Train: 2018-08-01T23:58:36.882988: step 17430, loss 0.562423.
Test: 2018-08-01T23:58:37.423518: step 17430, loss 0.547844.
Train: 2018-08-01T23:58:37.589101: step 17431, loss 0.527608.
Train: 2018-08-01T23:58:37.761613: step 17432, loss 0.632017.
Train: 2018-08-01T23:58:37.925176: step 17433, loss 0.527662.
Train: 2018-08-01T23:58:38.091766: step 17434, loss 0.562413.
Train: 2018-08-01T23:58:38.257318: step 17435, loss 0.475658.
Train: 2018-08-01T23:58:38.423842: step 17436, loss 0.527694.
Train: 2018-08-01T23:58:38.593421: step 17437, loss 0.684034.
Train: 2018-08-01T23:58:38.755981: step 17438, loss 0.545053.
Train: 2018-08-01T23:58:38.920547: step 17439, loss 0.527714.
Train: 2018-08-01T23:58:39.095049: step 17440, loss 0.510372.
Test: 2018-08-01T23:58:39.635604: step 17440, loss 0.547878.
Train: 2018-08-01T23:58:39.805177: step 17441, loss 0.579767.
Train: 2018-08-01T23:58:39.979683: step 17442, loss 0.527692.
Train: 2018-08-01T23:58:40.150228: step 17443, loss 0.597157.
Train: 2018-08-01T23:58:40.311796: step 17444, loss 0.71877.
Train: 2018-08-01T23:58:40.483354: step 17445, loss 0.441053.
Train: 2018-08-01T23:58:40.655876: step 17446, loss 0.649067.
Train: 2018-08-01T23:58:40.822437: step 17447, loss 0.527791.
Train: 2018-08-01T23:58:40.985993: step 17448, loss 0.527819.
Train: 2018-08-01T23:58:41.157575: step 17449, loss 0.52783.
Train: 2018-08-01T23:58:41.329077: step 17450, loss 0.527825.
Test: 2018-08-01T23:58:41.862676: step 17450, loss 0.547918.
Train: 2018-08-01T23:58:42.027241: step 17451, loss 0.700797.
Train: 2018-08-01T23:58:42.191771: step 17452, loss 0.631502.
Train: 2018-08-01T23:58:42.356331: step 17453, loss 0.596872.
Train: 2018-08-01T23:58:42.524880: step 17454, loss 0.631172.
Train: 2018-08-01T23:58:42.689471: step 17455, loss 0.476702.
Train: 2018-08-01T23:58:42.861006: step 17456, loss 0.476848.
Train: 2018-08-01T23:58:43.024575: step 17457, loss 0.61371.
Train: 2018-08-01T23:58:43.193121: step 17458, loss 0.544172.
Train: 2018-08-01T23:58:43.364636: step 17459, loss 0.630704.
Train: 2018-08-01T23:58:43.535201: step 17460, loss 0.494198.
Test: 2018-08-01T23:58:44.061796: step 17460, loss 0.548129.
Train: 2018-08-01T23:58:44.228326: step 17461, loss 0.613533.
Train: 2018-08-01T23:58:44.397873: step 17462, loss 0.477272.
Train: 2018-08-01T23:58:44.562459: step 17463, loss 0.477248.
Train: 2018-08-01T23:58:44.726993: step 17464, loss 0.664736.
Train: 2018-08-01T23:58:44.890582: step 17465, loss 0.579455.
Train: 2018-08-01T23:58:45.058135: step 17466, loss 0.511253.
Train: 2018-08-01T23:58:45.221671: step 17467, loss 0.613572.
Train: 2018-08-01T23:58:45.387258: step 17468, loss 0.477138.
Train: 2018-08-01T23:58:45.561787: step 17469, loss 0.630674.
Train: 2018-08-01T23:58:45.735298: step 17470, loss 0.562399.
Test: 2018-08-01T23:58:46.264882: step 17470, loss 0.548108.
Train: 2018-08-01T23:58:46.431436: step 17471, loss 0.579466.
Train: 2018-08-01T23:58:46.606015: step 17472, loss 0.545337.
Train: 2018-08-01T23:58:46.770529: step 17473, loss 0.579462.
Train: 2018-08-01T23:58:46.936118: step 17474, loss 0.528282.
Train: 2018-08-01T23:58:47.101645: step 17475, loss 0.613592.
Train: 2018-08-01T23:58:47.281166: step 17476, loss 0.596514.
Train: 2018-08-01T23:58:47.445725: step 17477, loss 0.494231.
Train: 2018-08-01T23:58:47.619261: step 17478, loss 0.61354.
Train: 2018-08-01T23:58:47.783822: step 17479, loss 0.562402.
Train: 2018-08-01T23:58:47.946418: step 17480, loss 0.562402.
Test: 2018-08-01T23:58:48.474988: step 17480, loss 0.548147.
Train: 2018-08-01T23:58:48.639534: step 17481, loss 0.715622.
Train: 2018-08-01T23:58:48.813070: step 17482, loss 0.494478.
Train: 2018-08-01T23:58:48.976632: step 17483, loss 0.511525.
Train: 2018-08-01T23:58:49.140195: step 17484, loss 0.664147.
Train: 2018-08-01T23:58:49.299770: step 17485, loss 0.579343.
Train: 2018-08-01T23:58:49.464351: step 17486, loss 0.697601.
Train: 2018-08-01T23:58:49.626895: step 17487, loss 0.579274.
Train: 2018-08-01T23:58:49.799461: step 17488, loss 0.74707.
Train: 2018-08-01T23:58:49.962996: step 17489, loss 0.595856.
Train: 2018-08-01T23:58:50.129551: step 17490, loss 0.628926.
Test: 2018-08-01T23:58:50.666117: step 17490, loss 0.54876.
Train: 2018-08-01T23:58:50.831674: step 17491, loss 0.529668.
Train: 2018-08-01T23:58:50.998228: step 17492, loss 0.546173.
Train: 2018-08-01T23:58:51.162788: step 17493, loss 0.546272.
Train: 2018-08-01T23:58:51.325379: step 17494, loss 0.481087.
Train: 2018-08-01T23:58:51.486952: step 17495, loss 0.464913.
Train: 2018-08-01T23:58:51.653478: step 17496, loss 0.530074.
Train: 2018-08-01T23:58:51.820031: step 17497, loss 0.513706.
Train: 2018-08-01T23:58:51.978635: step 17498, loss 0.562648.
Train: 2018-08-01T23:58:52.150174: step 17499, loss 0.56263.
Train: 2018-08-01T23:58:52.316722: step 17500, loss 0.496969.
Test: 2018-08-01T23:58:52.852272: step 17500, loss 0.548808.
Train: 2018-08-01T23:58:53.615903: step 17501, loss 0.677781.
Train: 2018-08-01T23:58:53.790468: step 17502, loss 0.579051.
Train: 2018-08-01T23:58:53.957999: step 17503, loss 0.628502.
Train: 2018-08-01T23:58:54.122548: step 17504, loss 0.595532.
Train: 2018-08-01T23:58:54.286111: step 17505, loss 0.513177.
Train: 2018-08-01T23:58:54.449705: step 17506, loss 0.562579.
Train: 2018-08-01T23:58:54.615263: step 17507, loss 0.6285.
Train: 2018-08-01T23:58:54.776825: step 17508, loss 0.447263.
Train: 2018-08-01T23:58:54.945349: step 17509, loss 0.727541.
Train: 2018-08-01T23:58:55.115894: step 17510, loss 0.562576.
Test: 2018-08-01T23:58:55.655462: step 17510, loss 0.548794.
Train: 2018-08-01T23:58:55.829017: step 17511, loss 0.496714.
Train: 2018-08-01T23:58:55.993577: step 17512, loss 0.595527.
Train: 2018-08-01T23:58:56.155114: step 17513, loss 0.56258.
Train: 2018-08-01T23:58:56.318677: step 17514, loss 0.612006.
Train: 2018-08-01T23:58:56.482240: step 17515, loss 0.562583.
Train: 2018-08-01T23:58:56.653782: step 17516, loss 0.562586.
Train: 2018-08-01T23:58:56.826352: step 17517, loss 0.710725.
Train: 2018-08-01T23:58:57.000880: step 17518, loss 0.546184.
Train: 2018-08-01T23:58:57.168437: step 17519, loss 0.628211.
Train: 2018-08-01T23:58:57.334986: step 17520, loss 0.529926.
Test: 2018-08-01T23:58:57.875516: step 17520, loss 0.54898.
Train: 2018-08-01T23:58:58.043068: step 17521, loss 0.611669.
Train: 2018-08-01T23:58:58.207661: step 17522, loss 0.56268.
Train: 2018-08-01T23:58:58.375213: step 17523, loss 0.627822.
Train: 2018-08-01T23:58:58.544753: step 17524, loss 0.497738.
Train: 2018-08-01T23:58:58.714273: step 17525, loss 0.530265.
Train: 2018-08-01T23:58:58.882849: step 17526, loss 0.546499.
Train: 2018-08-01T23:58:59.055363: step 17527, loss 0.497774.
Train: 2018-08-01T23:58:59.224933: step 17528, loss 0.660287.
Train: 2018-08-01T23:58:59.388497: step 17529, loss 0.546445.
Train: 2018-08-01T23:58:59.555052: step 17530, loss 0.481347.
Test: 2018-08-01T23:59:00.086641: step 17530, loss 0.549033.
Train: 2018-08-01T23:59:00.253191: step 17531, loss 0.530081.
Train: 2018-08-01T23:59:00.430710: step 17532, loss 0.677028.
Train: 2018-08-01T23:59:00.592254: step 17533, loss 0.611693.
Train: 2018-08-01T23:59:00.756823: step 17534, loss 0.562655.
Train: 2018-08-01T23:59:00.930375: step 17535, loss 0.464585.
Train: 2018-08-01T23:59:01.095922: step 17536, loss 0.546266.
Train: 2018-08-01T23:59:01.262462: step 17537, loss 0.579022.
Train: 2018-08-01T23:59:01.424029: step 17538, loss 0.513316.
Train: 2018-08-01T23:59:01.588590: step 17539, loss 0.57905.
Train: 2018-08-01T23:59:01.752183: step 17540, loss 0.529567.
Test: 2018-08-01T23:59:02.281751: step 17540, loss 0.548698.
Train: 2018-08-01T23:59:02.454275: step 17541, loss 0.512931.
Train: 2018-08-01T23:59:02.624819: step 17542, loss 0.529348.
Train: 2018-08-01T23:59:02.795397: step 17543, loss 0.545859.
Train: 2018-08-01T23:59:02.958926: step 17544, loss 0.462301.
Train: 2018-08-01T23:59:03.125506: step 17545, loss 0.512143.
Train: 2018-08-01T23:59:03.289057: step 17546, loss 0.51188.
Train: 2018-08-01T23:59:03.446648: step 17547, loss 0.54548.
Train: 2018-08-01T23:59:03.612181: step 17548, loss 0.681535.
Train: 2018-08-01T23:59:03.779732: step 17549, loss 0.528274.
Train: 2018-08-01T23:59:03.950302: step 17550, loss 0.493946.
Test: 2018-08-01T23:59:04.477898: step 17550, loss 0.548018.
Train: 2018-08-01T23:59:04.646415: step 17551, loss 0.596747.
Train: 2018-08-01T23:59:04.809978: step 17552, loss 0.596841.
Train: 2018-08-01T23:59:04.976534: step 17553, loss 0.51064.
Train: 2018-08-01T23:59:05.146079: step 17554, loss 0.54511.
Train: 2018-08-01T23:59:05.316657: step 17555, loss 0.527736.
Train: 2018-08-01T23:59:05.490190: step 17556, loss 0.492888.
Train: 2018-08-01T23:59:05.668708: step 17557, loss 0.579867.
Train: 2018-08-01T23:59:05.832276: step 17558, loss 0.509988.
Train: 2018-08-01T23:59:05.993844: step 17559, loss 0.597524.
Train: 2018-08-01T23:59:06.163387: step 17560, loss 0.527319.
Test: 2018-08-01T23:59:06.701921: step 17560, loss 0.547729.
Train: 2018-08-01T23:59:06.874490: step 17561, loss 0.509638.
Train: 2018-08-01T23:59:07.042045: step 17562, loss 0.509513.
Train: 2018-08-01T23:59:07.212586: step 17563, loss 0.544803.
Train: 2018-08-01T23:59:07.378145: step 17564, loss 0.438192.
Train: 2018-08-01T23:59:07.554641: step 17565, loss 0.68742.
Train: 2018-08-01T23:59:07.720231: step 17566, loss 0.526853.
Train: 2018-08-01T23:59:07.884789: step 17567, loss 0.562612.
Train: 2018-08-01T23:59:08.050316: step 17568, loss 0.544694.
Train: 2018-08-01T23:59:08.219888: step 17569, loss 0.598577.
Train: 2018-08-01T23:59:08.393424: step 17570, loss 0.544676.
Test: 2018-08-01T23:59:08.932957: step 17570, loss 0.547597.
Train: 2018-08-01T23:59:09.102534: step 17571, loss 0.526674.
Train: 2018-08-01T23:59:09.274045: step 17572, loss 0.562679.
Train: 2018-08-01T23:59:09.436637: step 17573, loss 0.616781.
Train: 2018-08-01T23:59:09.602198: step 17574, loss 0.544658.
Train: 2018-08-01T23:59:09.767773: step 17575, loss 0.49057.
Train: 2018-08-01T23:59:09.936275: step 17576, loss 0.598783.
Train: 2018-08-01T23:59:10.098840: step 17577, loss 0.562698.
Train: 2018-08-01T23:59:10.266424: step 17578, loss 0.58074.
Train: 2018-08-01T23:59:10.433945: step 17579, loss 0.544656.
Train: 2018-08-01T23:59:10.596509: step 17580, loss 0.508599.
Test: 2018-08-01T23:59:11.128089: step 17580, loss 0.547589.
Train: 2018-08-01T23:59:11.297666: step 17581, loss 0.454489.
Train: 2018-08-01T23:59:11.461198: step 17582, loss 0.580766.
Train: 2018-08-01T23:59:11.631774: step 17583, loss 0.526569.
Train: 2018-08-01T23:59:11.800316: step 17584, loss 0.70749.
Train: 2018-08-01T23:59:11.965880: step 17585, loss 0.526566.
Train: 2018-08-01T23:59:12.130408: step 17586, loss 0.562713.
Train: 2018-08-01T23:59:12.295966: step 17587, loss 0.54465.
Train: 2018-08-01T23:59:12.458532: step 17588, loss 0.670966.
Train: 2018-08-01T23:59:12.619102: step 17589, loss 0.562674.
Train: 2018-08-01T23:59:12.782697: step 17590, loss 0.454806.
Test: 2018-08-01T23:59:13.318234: step 17590, loss 0.547603.
Train: 2018-08-01T23:59:13.481828: step 17591, loss 0.706365.
Train: 2018-08-01T23:59:13.646357: step 17592, loss 0.67015.
Train: 2018-08-01T23:59:13.817897: step 17593, loss 0.562584.
Train: 2018-08-01T23:59:13.982458: step 17594, loss 0.598144.
Train: 2018-08-01T23:59:14.155993: step 17595, loss 0.473871.
Train: 2018-08-01T23:59:14.330559: step 17596, loss 0.403293.
Train: 2018-08-01T23:59:14.496091: step 17597, loss 0.633261.
Train: 2018-08-01T23:59:14.663638: step 17598, loss 0.580168.
Train: 2018-08-01T23:59:14.823210: step 17599, loss 0.4919.
Train: 2018-08-01T23:59:14.989792: step 17600, loss 0.438997.
Test: 2018-08-01T23:59:15.515361: step 17600, loss 0.547705.
Train: 2018-08-01T23:59:16.315785: step 17601, loss 0.562495.
Train: 2018-08-01T23:59:16.481342: step 17602, loss 0.597863.
Train: 2018-08-01T23:59:16.646899: step 17603, loss 0.633251.
Train: 2018-08-01T23:59:16.823471: step 17604, loss 0.456452.
Train: 2018-08-01T23:59:16.989016: step 17605, loss 0.527132.
Train: 2018-08-01T23:59:17.170512: step 17606, loss 0.66873.
Train: 2018-08-01T23:59:17.341044: step 17607, loss 0.544814.
Train: 2018-08-01T23:59:17.513611: step 17608, loss 0.527133.
Train: 2018-08-01T23:59:17.679171: step 17609, loss 0.562504.
Train: 2018-08-01T23:59:17.842703: step 17610, loss 0.509451.
Test: 2018-08-01T23:59:18.371290: step 17610, loss 0.547691.
Train: 2018-08-01T23:59:18.539840: step 17611, loss 0.491735.
Train: 2018-08-01T23:59:18.704400: step 17612, loss 0.597947.
Train: 2018-08-01T23:59:18.866991: step 17613, loss 0.544795.
Train: 2018-08-01T23:59:19.034542: step 17614, loss 0.615739.
Train: 2018-08-01T23:59:19.200108: step 17615, loss 0.491587.
Train: 2018-08-01T23:59:19.382611: step 17616, loss 0.509292.
Train: 2018-08-01T23:59:19.554128: step 17617, loss 0.615838.
Train: 2018-08-01T23:59:19.721679: step 17618, loss 0.686937.
Train: 2018-08-01T23:59:19.885268: step 17619, loss 0.562529.
Train: 2018-08-01T23:59:20.049804: step 17620, loss 0.473919.
Test: 2018-08-01T23:59:20.594374: step 17620, loss 0.547682.
Train: 2018-08-01T23:59:20.759959: step 17621, loss 0.651094.
Train: 2018-08-01T23:59:20.933474: step 17622, loss 0.562506.
Train: 2018-08-01T23:59:21.097028: step 17623, loss 0.527165.
Train: 2018-08-01T23:59:21.259570: step 17624, loss 0.509541.
Train: 2018-08-01T23:59:21.422155: step 17625, loss 0.650719.
Train: 2018-08-01T23:59:21.593700: step 17626, loss 0.632964.
Train: 2018-08-01T23:59:21.756272: step 17627, loss 0.527303.
Train: 2018-08-01T23:59:21.918808: step 17628, loss 0.580009.
Train: 2018-08-01T23:59:22.086384: step 17629, loss 0.562448.
Train: 2018-08-01T23:59:22.252946: step 17630, loss 0.579931.
Test: 2018-08-01T23:59:22.785490: step 17630, loss 0.547811.
Train: 2018-08-01T23:59:22.951073: step 17631, loss 0.544972.
Train: 2018-08-01T23:59:23.114638: step 17632, loss 0.597296.
Train: 2018-08-01T23:59:23.283160: step 17633, loss 0.579823.
Train: 2018-08-01T23:59:23.444756: step 17634, loss 0.631894.
Train: 2018-08-01T23:59:23.609288: step 17635, loss 0.423822.
Train: 2018-08-01T23:59:23.775867: step 17636, loss 0.545093.
Train: 2018-08-01T23:59:23.944421: step 17637, loss 0.510484.
Train: 2018-08-01T23:59:24.109974: step 17638, loss 0.545093.
Train: 2018-08-01T23:59:24.274541: step 17639, loss 0.631698.
Train: 2018-08-01T23:59:24.439094: step 17640, loss 0.597035.
Test: 2018-08-01T23:59:24.979624: step 17640, loss 0.547919.
Train: 2018-08-01T23:59:25.142189: step 17641, loss 0.510509.
Train: 2018-08-01T23:59:25.303758: step 17642, loss 0.510517.
Train: 2018-08-01T23:59:25.466323: step 17643, loss 0.527795.
Train: 2018-08-01T23:59:25.644846: step 17644, loss 0.63169.
Train: 2018-08-01T23:59:25.808409: step 17645, loss 0.441178.
Train: 2018-08-01T23:59:25.972002: step 17646, loss 0.631786.
Train: 2018-08-01T23:59:26.138527: step 17647, loss 0.52771.
Train: 2018-08-01T23:59:26.305081: step 17648, loss 0.475596.
Train: 2018-08-01T23:59:26.470639: step 17649, loss 0.597207.
Train: 2018-08-01T23:59:26.639213: step 17650, loss 0.545008.
Test: 2018-08-01T23:59:27.166778: step 17650, loss 0.547827.
Train: 2018-08-01T23:59:27.329374: step 17651, loss 0.597295.
Train: 2018-08-01T23:59:27.500901: step 17652, loss 0.492651.
Train: 2018-08-01T23:59:27.669466: step 17653, loss 0.562434.
Train: 2018-08-01T23:59:27.841999: step 17654, loss 0.614905.
Train: 2018-08-01T23:59:28.003566: step 17655, loss 0.56244.
Train: 2018-08-01T23:59:28.173093: step 17656, loss 0.579939.
Train: 2018-08-01T23:59:28.335679: step 17657, loss 0.509951.
Train: 2018-08-01T23:59:28.504233: step 17658, loss 0.527431.
Train: 2018-08-01T23:59:28.668793: step 17659, loss 0.579969.
Train: 2018-08-01T23:59:28.842329: step 17660, loss 0.509858.
Test: 2018-08-01T23:59:29.370896: step 17660, loss 0.54776.
Train: 2018-08-01T23:59:29.535476: step 17661, loss 0.615104.
Train: 2018-08-01T23:59:29.702025: step 17662, loss 0.650223.
Train: 2018-08-01T23:59:29.868586: step 17663, loss 0.544916.
Train: 2018-08-01T23:59:30.034113: step 17664, loss 0.579969.
Train: 2018-08-01T23:59:30.199671: step 17665, loss 0.684977.
Train: 2018-08-01T23:59:30.365237: step 17666, loss 0.667193.
Train: 2018-08-01T23:59:30.544778: step 17667, loss 0.579813.
Train: 2018-08-01T23:59:30.714295: step 17668, loss 0.631735.
Train: 2018-08-01T23:59:30.879852: step 17669, loss 0.510626.
Train: 2018-08-01T23:59:31.052391: step 17670, loss 0.510786.
Test: 2018-08-01T23:59:31.580008: step 17670, loss 0.548021.
Train: 2018-08-01T23:59:31.745538: step 17671, loss 0.510897.
Train: 2018-08-01T23:59:31.908102: step 17672, loss 0.545251.
Train: 2018-08-01T23:59:32.071665: step 17673, loss 0.613786.
Train: 2018-08-01T23:59:32.242222: step 17674, loss 0.562397.
Train: 2018-08-01T23:59:32.406803: step 17675, loss 0.494074.
Train: 2018-08-01T23:59:32.570363: step 17676, loss 0.528246.
Train: 2018-08-01T23:59:32.750875: step 17677, loss 0.545318.
Train: 2018-08-01T23:59:32.916438: step 17678, loss 0.476952.
Train: 2018-08-01T23:59:33.080968: step 17679, loss 0.579514.
Train: 2018-08-01T23:59:33.246526: step 17680, loss 0.562397.
Test: 2018-08-01T23:59:33.781127: step 17680, loss 0.548029.
Train: 2018-08-01T23:59:33.953663: step 17681, loss 0.510926.
Train: 2018-08-01T23:59:34.122184: step 17682, loss 0.45928.
Train: 2018-08-01T23:59:34.286744: step 17683, loss 0.458981.
Train: 2018-08-01T23:59:34.469256: step 17684, loss 0.562405.
Train: 2018-08-01T23:59:34.637837: step 17685, loss 0.579781.
Train: 2018-08-01T23:59:34.801368: step 17686, loss 0.597259.
Train: 2018-08-01T23:59:34.965962: step 17687, loss 0.597341.
Train: 2018-08-01T23:59:35.129516: step 17688, loss 0.492519.
Train: 2018-08-01T23:59:35.292056: step 17689, loss 0.544929.
Train: 2018-08-01T23:59:35.462634: step 17690, loss 0.667772.
Test: 2018-08-01T23:59:36.006149: step 17690, loss 0.547754.
Train: 2018-08-01T23:59:36.170741: step 17691, loss 0.509779.
Train: 2018-08-01T23:59:36.355240: step 17692, loss 0.667927.
Train: 2018-08-01T23:59:36.519775: step 17693, loss 0.527326.
Train: 2018-08-01T23:59:36.683371: step 17694, loss 0.544895.
Train: 2018-08-01T23:59:36.849893: step 17695, loss 0.597593.
Train: 2018-08-01T23:59:37.011461: step 17696, loss 0.580016.
Train: 2018-08-01T23:59:37.180010: step 17697, loss 0.527364.
Train: 2018-08-01T23:59:37.354568: step 17698, loss 0.597533.
Train: 2018-08-01T23:59:37.530099: step 17699, loss 0.615028.
Train: 2018-08-01T23:59:37.694668: step 17700, loss 0.597444.
Test: 2018-08-01T23:59:38.232246: step 17700, loss 0.547805.
Train: 2018-08-01T23:59:39.020616: step 17701, loss 0.544964.
Train: 2018-08-01T23:59:39.183205: step 17702, loss 0.510097.
Train: 2018-08-01T23:59:39.350761: step 17703, loss 0.579857.
Train: 2018-08-01T23:59:39.517285: step 17704, loss 0.579838.
Train: 2018-08-01T23:59:39.678886: step 17705, loss 0.597212.
Train: 2018-08-01T23:59:39.840450: step 17706, loss 0.492929.
Train: 2018-08-01T23:59:40.007998: step 17707, loss 0.527686.
Train: 2018-08-01T23:59:40.172535: step 17708, loss 0.579777.
Train: 2018-08-01T23:59:40.338125: step 17709, loss 0.510333.
Train: 2018-08-01T23:59:40.503648: step 17710, loss 0.492944.
Test: 2018-08-01T23:59:41.041212: step 17710, loss 0.547855.
Train: 2018-08-01T23:59:41.206794: step 17711, loss 0.597196.
Train: 2018-08-01T23:59:41.376318: step 17712, loss 0.458014.
Train: 2018-08-01T23:59:41.543867: step 17713, loss 0.667033.
Train: 2018-08-01T23:59:41.709451: step 17714, loss 0.544987.
Train: 2018-08-01T23:59:41.880966: step 17715, loss 0.649667.
Train: 2018-08-01T23:59:42.046525: step 17716, loss 0.510125.
Train: 2018-08-01T23:59:42.211084: step 17717, loss 0.492696.
Train: 2018-08-01T23:59:42.382625: step 17718, loss 0.562428.
Train: 2018-08-01T23:59:42.551174: step 17719, loss 0.597347.
Train: 2018-08-01T23:59:42.718726: step 17720, loss 0.579891.
Test: 2018-08-01T23:59:43.253311: step 17720, loss 0.547813.
Train: 2018-08-01T23:59:43.420876: step 17721, loss 0.527518.
Train: 2018-08-01T23:59:43.586408: step 17722, loss 0.492591.
Train: 2018-08-01T23:59:43.752996: step 17723, loss 0.457562.
Train: 2018-08-01T23:59:43.922535: step 17724, loss 0.509889.
Train: 2018-08-01T23:59:44.089095: step 17725, loss 0.597593.
Train: 2018-08-01T23:59:44.251660: step 17726, loss 0.58007.
Train: 2018-08-01T23:59:44.415226: step 17727, loss 0.580103.
Train: 2018-08-01T23:59:44.580751: step 17728, loss 0.509566.
Train: 2018-08-01T23:59:44.752290: step 17729, loss 0.597826.
Train: 2018-08-01T23:59:44.914855: step 17730, loss 0.633215.
Test: 2018-08-01T23:59:45.448431: step 17730, loss 0.547699.
Train: 2018-08-01T23:59:45.611021: step 17731, loss 0.597844.
Train: 2018-08-01T23:59:45.781541: step 17732, loss 0.58015.
Train: 2018-08-01T23:59:45.940140: step 17733, loss 0.668318.
Train: 2018-08-01T23:59:46.107668: step 17734, loss 0.632854.
Train: 2018-08-01T23:59:46.270266: step 17735, loss 0.544912.
Train: 2018-08-01T23:59:46.435816: step 17736, loss 0.544947.
Train: 2018-08-01T23:59:46.608329: step 17737, loss 0.457714.
Train: 2018-08-01T23:59:46.774883: step 17738, loss 0.649627.
Train: 2018-08-01T23:59:46.942467: step 17739, loss 0.545013.
Train: 2018-08-01T23:59:47.115972: step 17740, loss 0.614561.
Test: 2018-08-01T23:59:47.645557: step 17740, loss 0.547885.
Train: 2018-08-01T23:59:47.813139: step 17741, loss 0.527718.
Train: 2018-08-01T23:59:47.975673: step 17742, loss 0.562407.
Train: 2018-08-01T23:59:48.148243: step 17743, loss 0.579702.
Train: 2018-08-01T23:59:48.314792: step 17744, loss 0.700582.
Train: 2018-08-01T23:59:48.493290: step 17745, loss 0.579615.
Train: 2018-08-01T23:59:48.662837: step 17746, loss 0.562396.
Train: 2018-08-01T23:59:48.826425: step 17747, loss 0.562397.
Train: 2018-08-01T23:59:48.989993: step 17748, loss 0.511186.
Train: 2018-08-01T23:59:49.156543: step 17749, loss 0.630578.
Train: 2018-08-01T23:59:49.319083: step 17750, loss 0.477384.
Test: 2018-08-01T23:59:49.864625: step 17750, loss 0.548181.
Train: 2018-08-01T23:59:50.032232: step 17751, loss 0.579396.
Train: 2018-08-01T23:59:50.208705: step 17752, loss 0.766067.
Train: 2018-08-01T23:59:50.374293: step 17753, loss 0.494787.
Train: 2018-08-01T23:59:50.544843: step 17754, loss 0.528692.
Train: 2018-08-01T23:59:50.714353: step 17755, loss 0.545593.
Train: 2018-08-01T23:59:50.879934: step 17756, loss 0.545618.
Train: 2018-08-01T23:59:51.045501: step 17757, loss 0.478395.
Train: 2018-08-01T23:59:51.215039: step 17758, loss 0.596079.
Train: 2018-08-01T23:59:51.378602: step 17759, loss 0.526553.
Train: 2018-08-01T23:59:51.549122: step 17760, loss 0.629769.
Test: 2018-08-01T23:59:52.080724: step 17760, loss 0.548346.
Train: 2018-08-01T23:59:52.246257: step 17761, loss 0.528781.
Train: 2018-08-01T23:59:52.418800: step 17762, loss 0.57927.
Train: 2018-08-01T23:59:52.582358: step 17763, loss 0.562436.
Train: 2018-08-01T23:59:52.750909: step 17764, loss 0.596108.
Train: 2018-08-01T23:59:52.913505: step 17765, loss 0.612908.
Train: 2018-08-01T23:59:53.080030: step 17766, loss 0.461723.
Train: 2018-08-01T23:59:53.243593: step 17767, loss 0.663307.
Train: 2018-08-01T23:59:53.416156: step 17768, loss 0.495279.
Train: 2018-08-01T23:59:53.581718: step 17769, loss 0.562406.
Train: 2018-08-01T23:59:53.745250: step 17770, loss 0.61286.
Test: 2018-08-01T23:59:54.281817: step 17770, loss 0.548374.
Train: 2018-08-01T23:59:54.455383: step 17771, loss 0.579239.
Train: 2018-08-01T23:59:54.622906: step 17772, loss 0.545714.
Train: 2018-08-01T23:59:54.787466: step 17773, loss 0.545643.
Train: 2018-08-01T23:59:54.965024: step 17774, loss 0.478548.
Train: 2018-08-01T23:59:55.133570: step 17775, loss 0.579349.
Train: 2018-08-01T23:59:55.300095: step 17776, loss 0.646713.
Train: 2018-08-01T23:59:55.459667: step 17777, loss 0.545576.
Train: 2018-08-01T23:59:55.624253: step 17778, loss 0.528714.
Train: 2018-08-01T23:59:55.786818: step 17779, loss 0.613037.
Train: 2018-08-01T23:59:55.957365: step 17780, loss 0.596167.
Test: 2018-08-01T23:59:56.489914: step 17780, loss 0.54831.
Train: 2018-08-01T23:59:56.655471: step 17781, loss 0.663599.
Train: 2018-08-01T23:59:56.819062: step 17782, loss 0.612931.
Train: 2018-08-01T23:59:56.998554: step 17783, loss 0.495275.
Train: 2018-08-01T23:59:57.172089: step 17784, loss 0.596005.
Train: 2018-08-01T23:59:57.337671: step 17785, loss 0.579213.
Train: 2018-08-01T23:59:57.500242: step 17786, loss 0.612659.
Train: 2018-08-01T23:59:57.670758: step 17787, loss 0.612576.
Train: 2018-08-01T23:59:57.842329: step 17788, loss 0.595813.
Train: 2018-08-01T23:59:58.013840: step 17789, loss 0.562509.
Train: 2018-08-01T23:59:58.180420: step 17790, loss 0.562525.
Test: 2018-08-01T23:59:58.727936: step 17790, loss 0.548678.
Train: 2018-08-01T23:59:58.896481: step 17791, loss 0.612197.
Train: 2018-08-01T23:59:59.057052: step 17792, loss 0.479981.
Train: 2018-08-01T23:59:59.233605: step 17793, loss 0.546059.
Train: 2018-08-01T23:59:59.403150: step 17794, loss 0.562565.
Train: 2018-08-01T23:59:59.570677: step 17795, loss 0.595561.
Train: 2018-08-01T23:59:59.741221: step 17796, loss 0.628529.
Train: 2018-08-01T23:59:59.910768: step 17797, loss 0.57905.
Train: 2018-08-02T00:00:00.085341: step 17798, loss 0.66128.
Train: 2018-08-02T00:00:00.258838: step 17799, loss 0.513394.
Train: 2018-08-02T00:00:00.430378: step 17800, loss 0.497088.
Test: 2018-08-02T00:00:00.972930: step 17800, loss 0.54891.
Train: 2018-08-02T00:00:01.752168: step 17801, loss 0.529862.
Train: 2018-08-02T00:00:01.916729: step 17802, loss 0.562624.
Train: 2018-08-02T00:00:02.081322: step 17803, loss 0.693848.
Train: 2018-08-02T00:00:02.251834: step 17804, loss 0.529863.
Train: 2018-08-02T00:00:02.425369: step 17805, loss 0.57901.
Train: 2018-08-02T00:00:02.590952: step 17806, loss 0.529905.
Train: 2018-08-02T00:00:02.752525: step 17807, loss 0.497157.
Train: 2018-08-02T00:00:02.918053: step 17808, loss 0.562625.
Train: 2018-08-02T00:00:03.081647: step 17809, loss 0.5462.
Train: 2018-08-02T00:00:03.286498: step 17810, loss 0.595475.
Test: 2018-08-02T00:00:03.827085: step 17810, loss 0.548808.
Train: 2018-08-02T00:00:03.992641: step 17811, loss 0.562588.
Train: 2018-08-02T00:00:04.157202: step 17812, loss 0.595526.
Train: 2018-08-02T00:00:04.320732: step 17813, loss 0.513122.
Train: 2018-08-02T00:00:04.492316: step 17814, loss 0.694619.
Train: 2018-08-02T00:00:04.655865: step 17815, loss 0.496575.
Train: 2018-08-02T00:00:04.824412: step 17816, loss 0.480023.
Train: 2018-08-02T00:00:04.986961: step 17817, loss 0.595623.
Train: 2018-08-02T00:00:05.152509: step 17818, loss 0.579096.
Train: 2018-08-02T00:00:05.317069: step 17819, loss 0.496215.
Train: 2018-08-02T00:00:05.483625: step 17820, loss 0.595736.
Test: 2018-08-02T00:00:06.012211: step 17820, loss 0.548573.
Train: 2018-08-02T00:00:06.177768: step 17821, loss 0.662317.
Train: 2018-08-02T00:00:06.349311: step 17822, loss 0.512599.
Train: 2018-08-02T00:00:06.514869: step 17823, loss 0.545851.
Train: 2018-08-02T00:00:06.680426: step 17824, loss 0.529162.
Train: 2018-08-02T00:00:06.843022: step 17825, loss 0.645931.
Train: 2018-08-02T00:00:07.011540: step 17826, loss 0.595868.
Train: 2018-08-02T00:00:07.193055: step 17827, loss 0.562482.
Train: 2018-08-02T00:00:07.355619: step 17828, loss 0.529105.
Train: 2018-08-02T00:00:07.522175: step 17829, loss 0.545782.
Train: 2018-08-02T00:00:07.693741: step 17830, loss 0.545765.
Test: 2018-08-02T00:00:08.226293: step 17830, loss 0.548462.
Train: 2018-08-02T00:00:08.393875: step 17831, loss 0.646106.
Train: 2018-08-02T00:00:08.566409: step 17832, loss 0.646089.
Train: 2018-08-02T00:00:08.730969: step 17833, loss 0.57918.
Train: 2018-08-02T00:00:08.897532: step 17834, loss 0.579165.
Train: 2018-08-02T00:00:09.072031: step 17835, loss 0.662434.
Train: 2018-08-02T00:00:09.234623: step 17836, loss 0.562512.
Train: 2018-08-02T00:00:09.398159: step 17837, loss 0.595681.
Train: 2018-08-02T00:00:09.568734: step 17838, loss 0.529473.
Train: 2018-08-02T00:00:09.733263: step 17839, loss 0.595581.
Train: 2018-08-02T00:00:09.900843: step 17840, loss 0.562574.
Test: 2018-08-02T00:00:10.446357: step 17840, loss 0.548806.
Train: 2018-08-02T00:00:10.620924: step 17841, loss 0.595502.
Train: 2018-08-02T00:00:10.791468: step 17842, loss 0.628322.
Train: 2018-08-02T00:00:10.955029: step 17843, loss 0.611798.
Train: 2018-08-02T00:00:11.120555: step 17844, loss 0.628038.
Train: 2018-08-02T00:00:11.287136: step 17845, loss 0.530103.
Train: 2018-08-02T00:00:11.461673: step 17846, loss 0.530204.
Train: 2018-08-02T00:00:11.630218: step 17847, loss 0.627657.
Train: 2018-08-02T00:00:11.793789: step 17848, loss 0.578952.
Train: 2018-08-02T00:00:11.971312: step 17849, loss 0.595111.
Train: 2018-08-02T00:00:12.137865: step 17850, loss 0.562805.
Test: 2018-08-02T00:00:12.671443: step 17850, loss 0.54934.
Train: 2018-08-02T00:00:12.837002: step 17851, loss 0.659463.
Train: 2018-08-02T00:00:13.004544: step 17852, loss 0.530743.
Train: 2018-08-02T00:00:13.178056: step 17853, loss 0.530826.
Train: 2018-08-02T00:00:13.344638: step 17854, loss 0.498833.
Train: 2018-08-02T00:00:13.516177: step 17855, loss 0.626993.
Train: 2018-08-02T00:00:13.681709: step 17856, loss 0.530867.
Train: 2018-08-02T00:00:13.843277: step 17857, loss 0.514826.
Train: 2018-08-02T00:00:14.024791: step 17858, loss 0.562878.
Train: 2018-08-02T00:00:14.193366: step 17859, loss 0.594994.
Train: 2018-08-02T00:00:14.371865: step 17860, loss 0.53068.
Test: 2018-08-02T00:00:14.906468: step 17860, loss 0.549336.
Train: 2018-08-02T00:00:15.072018: step 17861, loss 0.466174.
Train: 2018-08-02T00:00:15.234584: step 17862, loss 0.578943.
Train: 2018-08-02T00:00:15.400114: step 17863, loss 0.416916.
Train: 2018-08-02T00:00:15.579666: step 17864, loss 0.611543.
Train: 2018-08-02T00:00:15.757195: step 17865, loss 0.529962.
Train: 2018-08-02T00:00:15.917757: step 17866, loss 0.562612.
Train: 2018-08-02T00:00:16.086312: step 17867, loss 0.529629.
Train: 2018-08-02T00:00:16.254861: step 17868, loss 0.579085.
Train: 2018-08-02T00:00:16.432356: step 17869, loss 0.579116.
Train: 2018-08-02T00:00:16.605922: step 17870, loss 0.545849.
Test: 2018-08-02T00:00:17.149445: step 17870, loss 0.548496.
Train: 2018-08-02T00:00:17.318016: step 17871, loss 0.64597.
Train: 2018-08-02T00:00:17.483576: step 17872, loss 0.478845.
Train: 2018-08-02T00:00:17.650133: step 17873, loss 0.562455.
Train: 2018-08-02T00:00:17.819674: step 17874, loss 0.495194.
Train: 2018-08-02T00:00:17.984238: step 17875, loss 0.56243.
Train: 2018-08-02T00:00:18.150808: step 17876, loss 0.613173.
Train: 2018-08-02T00:00:18.315350: step 17877, loss 0.477653.
Train: 2018-08-02T00:00:18.479915: step 17878, loss 0.613417.
Train: 2018-08-02T00:00:18.643445: step 17879, loss 0.562403.
Train: 2018-08-02T00:00:18.810024: step 17880, loss 0.64775.
Test: 2018-08-02T00:00:19.348562: step 17880, loss 0.548099.
Train: 2018-08-02T00:00:19.521124: step 17881, loss 0.5624.
Train: 2018-08-02T00:00:19.691668: step 17882, loss 0.630745.
Train: 2018-08-02T00:00:19.870196: step 17883, loss 0.613632.
Train: 2018-08-02T00:00:20.037742: step 17884, loss 0.596516.
Train: 2018-08-02T00:00:20.203309: step 17885, loss 0.511308.
Train: 2018-08-02T00:00:20.368862: step 17886, loss 0.460277.
Train: 2018-08-02T00:00:20.531425: step 17887, loss 0.630548.
Train: 2018-08-02T00:00:20.708967: step 17888, loss 0.579437.
Train: 2018-08-02T00:00:20.877504: step 17889, loss 0.613489.
Train: 2018-08-02T00:00:21.038044: step 17890, loss 0.579417.
Test: 2018-08-02T00:00:21.571617: step 17890, loss 0.548178.
Train: 2018-08-02T00:00:21.742161: step 17891, loss 0.647373.
Train: 2018-08-02T00:00:21.906721: step 17892, loss 0.562413.
Train: 2018-08-02T00:00:22.071312: step 17893, loss 0.545494.
Train: 2018-08-02T00:00:22.237862: step 17894, loss 0.545523.
Train: 2018-08-02T00:00:22.404416: step 17895, loss 0.562426.
Train: 2018-08-02T00:00:22.569980: step 17896, loss 0.46122.
Train: 2018-08-02T00:00:22.744482: step 17897, loss 0.494905.
Train: 2018-08-02T00:00:22.920013: step 17898, loss 0.511696.
Train: 2018-08-02T00:00:23.086577: step 17899, loss 0.579361.
Train: 2018-08-02T00:00:23.261132: step 17900, loss 0.613343.
Test: 2018-08-02T00:00:23.787694: step 17900, loss 0.548177.
Train: 2018-08-02T00:00:24.577382: step 17901, loss 0.579401.
Train: 2018-08-02T00:00:24.743936: step 17902, loss 0.630426.
Train: 2018-08-02T00:00:24.909494: step 17903, loss 0.494409.
Train: 2018-08-02T00:00:25.074047: step 17904, loss 0.528383.
Train: 2018-08-02T00:00:25.237585: step 17905, loss 0.579433.
Train: 2018-08-02T00:00:25.414113: step 17906, loss 0.579445.
Train: 2018-08-02T00:00:25.579702: step 17907, loss 0.511246.
Train: 2018-08-02T00:00:25.746251: step 17908, loss 0.630691.
Train: 2018-08-02T00:00:25.910808: step 17909, loss 0.528252.
Train: 2018-08-02T00:00:26.079365: step 17910, loss 0.562399.
Test: 2018-08-02T00:00:26.612908: step 17910, loss 0.548085.
Train: 2018-08-02T00:00:26.778466: step 17911, loss 0.511118.
Train: 2018-08-02T00:00:26.943025: step 17912, loss 0.511055.
Train: 2018-08-02T00:00:27.106613: step 17913, loss 0.613834.
Train: 2018-08-02T00:00:27.274140: step 17914, loss 0.545236.
Train: 2018-08-02T00:00:27.436737: step 17915, loss 0.493679.
Train: 2018-08-02T00:00:27.603285: step 17916, loss 0.562399.
Train: 2018-08-02T00:00:27.768850: step 17917, loss 0.596885.
Train: 2018-08-02T00:00:27.934376: step 17918, loss 0.510623.
Train: 2018-08-02T00:00:28.103954: step 17919, loss 0.596981.
Train: 2018-08-02T00:00:28.282445: step 17920, loss 0.683538.
Test: 2018-08-02T00:00:28.804051: step 17920, loss 0.547927.
Train: 2018-08-02T00:00:28.984568: step 17921, loss 0.596987.
Train: 2018-08-02T00:00:29.152156: step 17922, loss 0.458787.
Train: 2018-08-02T00:00:29.314685: step 17923, loss 0.562404.
Train: 2018-08-02T00:00:29.480274: step 17924, loss 0.527847.
Train: 2018-08-02T00:00:29.641812: step 17925, loss 0.493244.
Train: 2018-08-02T00:00:29.814384: step 17926, loss 0.579725.
Train: 2018-08-02T00:00:29.980937: step 17927, loss 0.510404.
Train: 2018-08-02T00:00:30.147490: step 17928, loss 0.614508.
Train: 2018-08-02T00:00:30.312020: step 17929, loss 0.562417.
Train: 2018-08-02T00:00:30.477577: step 17930, loss 0.545031.
Test: 2018-08-02T00:00:31.023119: step 17930, loss 0.54785.
Train: 2018-08-02T00:00:31.192664: step 17931, loss 0.54502.
Train: 2018-08-02T00:00:31.359221: step 17932, loss 0.754013.
Train: 2018-08-02T00:00:31.526821: step 17933, loss 0.545033.
Train: 2018-08-02T00:00:31.698313: step 17934, loss 0.562414.
Train: 2018-08-02T00:00:31.871850: step 17935, loss 0.683769.
Train: 2018-08-02T00:00:32.045411: step 17936, loss 0.614265.
Train: 2018-08-02T00:00:32.212981: step 17937, loss 0.493482.
Train: 2018-08-02T00:00:32.375529: step 17938, loss 0.682754.
Train: 2018-08-02T00:00:32.539096: step 17939, loss 0.476733.
Train: 2018-08-02T00:00:32.702654: step 17940, loss 0.579498.
Test: 2018-08-02T00:00:33.245179: step 17940, loss 0.548112.
Train: 2018-08-02T00:00:33.469478: step 17941, loss 0.511208.
Train: 2018-08-02T00:00:33.638059: step 17942, loss 0.579448.
Train: 2018-08-02T00:00:33.803585: step 17943, loss 0.545379.
Train: 2018-08-02T00:00:33.970189: step 17944, loss 0.460341.
Train: 2018-08-02T00:00:34.131739: step 17945, loss 0.545382.
Train: 2018-08-02T00:00:34.300257: step 17946, loss 0.562403.
Train: 2018-08-02T00:00:34.463821: step 17947, loss 0.596506.
Train: 2018-08-02T00:00:34.639382: step 17948, loss 0.69886.
Train: 2018-08-02T00:00:34.806904: step 17949, loss 0.477254.
Train: 2018-08-02T00:00:34.973484: step 17950, loss 0.664558.
Test: 2018-08-02T00:00:35.498086: step 17950, loss 0.548173.
Train: 2018-08-02T00:00:35.676609: step 17951, loss 0.545409.
Train: 2018-08-02T00:00:35.842166: step 17952, loss 0.562409.
Train: 2018-08-02T00:00:36.014675: step 17953, loss 0.511532.
Train: 2018-08-02T00:00:36.181255: step 17954, loss 0.562412.
Train: 2018-08-02T00:00:36.353799: step 17955, loss 0.579367.
Train: 2018-08-02T00:00:36.517332: step 17956, loss 0.545465.
Train: 2018-08-02T00:00:36.684914: step 17957, loss 0.61326.
Train: 2018-08-02T00:00:36.859446: step 17958, loss 0.47773.
Train: 2018-08-02T00:00:37.029011: step 17959, loss 0.528519.
Train: 2018-08-02T00:00:37.195548: step 17960, loss 0.613308.
Test: 2018-08-02T00:00:37.736074: step 17960, loss 0.548199.
Train: 2018-08-02T00:00:37.901655: step 17961, loss 0.596351.
Train: 2018-08-02T00:00:38.072207: step 17962, loss 0.494542.
Train: 2018-08-02T00:00:38.241745: step 17963, loss 0.630333.
Train: 2018-08-02T00:00:38.412293: step 17964, loss 0.596365.
Train: 2018-08-02T00:00:38.579851: step 17965, loss 0.596345.
Train: 2018-08-02T00:00:38.756376: step 17966, loss 0.579364.
Train: 2018-08-02T00:00:38.925925: step 17967, loss 0.51162.
Train: 2018-08-02T00:00:39.091480: step 17968, loss 0.647058.
Train: 2018-08-02T00:00:39.255039: step 17969, loss 0.494801.
Train: 2018-08-02T00:00:39.426554: step 17970, loss 0.562422.
Test: 2018-08-02T00:00:39.963144: step 17970, loss 0.548272.
Train: 2018-08-02T00:00:40.135684: step 17971, loss 0.613119.
Train: 2018-08-02T00:00:40.300218: step 17972, loss 0.629966.
Train: 2018-08-02T00:00:40.475783: step 17973, loss 0.478144.
Train: 2018-08-02T00:00:40.642303: step 17974, loss 0.59614.
Train: 2018-08-02T00:00:40.805866: step 17975, loss 0.495061.
Train: 2018-08-02T00:00:40.972422: step 17976, loss 0.579284.
Train: 2018-08-02T00:00:41.139973: step 17977, loss 0.596142.
Train: 2018-08-02T00:00:41.316502: step 17978, loss 0.562433.
Train: 2018-08-02T00:00:41.483057: step 17979, loss 0.596131.
Train: 2018-08-02T00:00:41.644651: step 17980, loss 0.495076.
Test: 2018-08-02T00:00:42.173211: step 17980, loss 0.548324.
Train: 2018-08-02T00:00:42.340763: step 17981, loss 0.579282.
Train: 2018-08-02T00:00:42.513328: step 17982, loss 0.629846.
Train: 2018-08-02T00:00:42.678860: step 17983, loss 0.612963.
Train: 2018-08-02T00:00:42.844417: step 17984, loss 0.612905.
Train: 2018-08-02T00:00:43.011999: step 17985, loss 0.562448.
Train: 2018-08-02T00:00:43.180549: step 17986, loss 0.562456.
Train: 2018-08-02T00:00:43.353088: step 17987, loss 0.478744.
Train: 2018-08-02T00:00:43.520608: step 17988, loss 0.595953.
Train: 2018-08-02T00:00:43.684171: step 17989, loss 0.445282.
Train: 2018-08-02T00:00:43.852721: step 17990, loss 0.51216.
Test: 2018-08-02T00:00:44.380341: step 17990, loss 0.548376.
Train: 2018-08-02T00:00:44.552880: step 17991, loss 0.495241.
Train: 2018-08-02T00:00:44.716443: step 17992, loss 0.51188.
Train: 2018-08-02T00:00:44.888978: step 17993, loss 0.528603.
Train: 2018-08-02T00:00:45.052559: step 17994, loss 0.460593.
Train: 2018-08-02T00:00:45.217075: step 17995, loss 0.545352.
Train: 2018-08-02T00:00:45.384626: step 17996, loss 0.545272.
Train: 2018-08-02T00:00:45.550183: step 17997, loss 0.545199.
Train: 2018-08-02T00:00:45.715740: step 17998, loss 0.510599.
Train: 2018-08-02T00:00:45.881298: step 17999, loss 0.579751.
Train: 2018-08-02T00:00:46.053863: step 18000, loss 0.423189.
Test: 2018-08-02T00:00:46.593396: step 18000, loss 0.547792.
Train: 2018-08-02T00:00:47.390716: step 18001, loss 0.509965.
Train: 2018-08-02T00:00:47.569263: step 18002, loss 0.544884.
Train: 2018-08-02T00:00:47.738785: step 18003, loss 0.58016.
Train: 2018-08-02T00:00:47.903378: step 18004, loss 0.473849.
Train: 2018-08-02T00:00:48.069900: step 18005, loss 0.473481.
Train: 2018-08-02T00:00:48.235491: step 18006, loss 0.562611.
Train: 2018-08-02T00:00:48.403011: step 18007, loss 0.544672.
Train: 2018-08-02T00:00:48.567569: step 18008, loss 0.490464.
Train: 2018-08-02T00:00:48.739144: step 18009, loss 0.580905.
Train: 2018-08-02T00:00:48.906696: step 18010, loss 0.453583.
Test: 2018-08-02T00:00:49.444251: step 18010, loss 0.547568.
Train: 2018-08-02T00:00:49.610781: step 18011, loss 0.636017.
Train: 2018-08-02T00:00:49.775341: step 18012, loss 0.471245.
Train: 2018-08-02T00:00:49.953897: step 18013, loss 0.59978.
Train: 2018-08-02T00:00:50.127432: step 18014, loss 0.489249.
Train: 2018-08-02T00:00:50.289966: step 18015, loss 0.637063.
Train: 2018-08-02T00:00:50.455549: step 18016, loss 0.507533.
Train: 2018-08-02T00:00:50.627064: step 18017, loss 0.618806.
Train: 2018-08-02T00:00:50.790661: step 18018, loss 0.526013.
Train: 2018-08-02T00:00:50.961170: step 18019, loss 0.433079.
Train: 2018-08-02T00:00:51.125761: step 18020, loss 0.488721.
Test: 2018-08-02T00:00:51.663312: step 18020, loss 0.547628.
Train: 2018-08-02T00:00:51.836863: step 18021, loss 0.525925.
Train: 2018-08-02T00:00:52.001415: step 18022, loss 0.451037.
Train: 2018-08-02T00:00:52.170962: step 18023, loss 0.582154.
Train: 2018-08-02T00:00:52.337546: step 18024, loss 0.619888.
Train: 2018-08-02T00:00:52.511053: step 18025, loss 0.525786.
Train: 2018-08-02T00:00:52.679576: step 18026, loss 0.48804.
Train: 2018-08-02T00:00:52.844138: step 18027, loss 0.487952.
Train: 2018-08-02T00:00:53.016707: step 18028, loss 0.677212.
Train: 2018-08-02T00:00:53.188217: step 18029, loss 0.696202.
Train: 2018-08-02T00:00:53.360782: step 18030, loss 0.525732.
Test: 2018-08-02T00:00:53.888346: step 18030, loss 0.547713.
Train: 2018-08-02T00:00:54.050911: step 18031, loss 0.639106.
Train: 2018-08-02T00:00:54.226443: step 18032, loss 0.620035.
Train: 2018-08-02T00:00:54.390038: step 18033, loss 0.469427.
Train: 2018-08-02T00:00:54.560574: step 18034, loss 0.544608.
Train: 2018-08-02T00:00:54.723149: step 18035, loss 0.694458.
Train: 2018-08-02T00:00:54.890666: step 18036, loss 0.600605.
Train: 2018-08-02T00:00:55.055248: step 18037, loss 0.637601.
Train: 2018-08-02T00:00:55.224773: step 18038, loss 0.54458.
Train: 2018-08-02T00:00:55.392350: step 18039, loss 0.563029.
Train: 2018-08-02T00:00:55.558881: step 18040, loss 0.489448.
Test: 2018-08-02T00:00:56.094448: step 18040, loss 0.547569.
Train: 2018-08-02T00:00:56.269008: step 18041, loss 0.581245.
Train: 2018-08-02T00:00:56.438557: step 18042, loss 0.526321.
Train: 2018-08-02T00:00:56.607108: step 18043, loss 0.581068.
Train: 2018-08-02T00:00:56.773633: step 18044, loss 0.562799.
Train: 2018-08-02T00:00:56.940233: step 18045, loss 0.544624.
Train: 2018-08-02T00:00:57.105745: step 18046, loss 0.59894.
Train: 2018-08-02T00:00:57.267345: step 18047, loss 0.45437.
Train: 2018-08-02T00:00:57.437856: step 18048, loss 0.544655.
Train: 2018-08-02T00:00:57.599426: step 18049, loss 0.652773.
Train: 2018-08-02T00:00:57.765980: step 18050, loss 0.526693.
Test: 2018-08-02T00:00:58.300550: step 18050, loss 0.547605.
Train: 2018-08-02T00:00:58.474087: step 18051, loss 0.58059.
Train: 2018-08-02T00:00:58.638676: step 18052, loss 0.490938.
Train: 2018-08-02T00:00:58.821159: step 18053, loss 0.473086.
Train: 2018-08-02T00:00:58.983725: step 18054, loss 0.59843.
Train: 2018-08-02T00:00:59.151278: step 18055, loss 0.490999.
Train: 2018-08-02T00:00:59.316834: step 18056, loss 0.544702.
Train: 2018-08-02T00:00:59.490404: step 18057, loss 0.544699.
Train: 2018-08-02T00:00:59.655928: step 18058, loss 0.490907.
Train: 2018-08-02T00:00:59.828467: step 18059, loss 0.526734.
Train: 2018-08-02T00:00:59.994052: step 18060, loss 0.505125.
Test: 2018-08-02T00:01:00.529593: step 18060, loss 0.547592.
Train: 2018-08-02T00:01:00.707118: step 18061, loss 0.544663.
Train: 2018-08-02T00:01:00.875667: step 18062, loss 0.526609.
Train: 2018-08-02T00:01:01.044217: step 18063, loss 0.671183.
Train: 2018-08-02T00:01:01.213794: step 18064, loss 0.490398.
Train: 2018-08-02T00:01:01.378358: step 18065, loss 0.562733.
Train: 2018-08-02T00:01:01.549893: step 18066, loss 0.508413.
Train: 2018-08-02T00:01:01.719442: step 18067, loss 0.526497.
Train: 2018-08-02T00:01:01.890954: step 18068, loss 0.526468.
Train: 2018-08-02T00:01:02.054515: step 18069, loss 0.580971.
Train: 2018-08-02T00:01:02.220073: step 18070, loss 0.508222.
Test: 2018-08-02T00:01:02.747694: step 18070, loss 0.547569.
Train: 2018-08-02T00:01:02.918207: step 18071, loss 0.544606.
Train: 2018-08-02T00:01:03.089748: step 18072, loss 0.544602.
Train: 2018-08-02T00:01:03.251316: step 18073, loss 0.471556.
Train: 2018-08-02T00:01:03.417895: step 18074, loss 0.562889.
Train: 2018-08-02T00:01:03.583429: step 18075, loss 0.471292.
Train: 2018-08-02T00:01:03.759985: step 18076, loss 0.507852.
Train: 2018-08-02T00:01:03.927537: step 18077, loss 0.507755.
Train: 2018-08-02T00:01:04.094095: step 18078, loss 0.54458.
Train: 2018-08-02T00:01:04.264632: step 18079, loss 0.600106.
Train: 2018-08-02T00:01:04.445156: step 18080, loss 0.600192.
Test: 2018-08-02T00:01:04.987700: step 18080, loss 0.547598.
Train: 2018-08-02T00:01:05.153259: step 18081, loss 0.581682.
Train: 2018-08-02T00:01:05.317824: step 18082, loss 0.526027.
Train: 2018-08-02T00:01:05.486341: step 18083, loss 0.600266.
Train: 2018-08-02T00:01:05.653893: step 18084, loss 0.581692.
Train: 2018-08-02T00:01:05.816458: step 18085, loss 0.65583.
Train: 2018-08-02T00:01:05.985039: step 18086, loss 0.489066.
Train: 2018-08-02T00:01:06.151588: step 18087, loss 0.618514.
Train: 2018-08-02T00:01:06.314163: step 18088, loss 0.599925.
Train: 2018-08-02T00:01:06.482678: step 18089, loss 0.489369.
Train: 2018-08-02T00:01:06.647239: step 18090, loss 0.507829.
Test: 2018-08-02T00:01:07.184802: step 18090, loss 0.547571.
Train: 2018-08-02T00:01:07.353351: step 18091, loss 0.544585.
Train: 2018-08-02T00:01:07.526913: step 18092, loss 0.489543.
Train: 2018-08-02T00:01:07.689479: step 18093, loss 0.562934.
Train: 2018-08-02T00:01:07.857005: step 18094, loss 0.452848.
Train: 2018-08-02T00:01:08.020592: step 18095, loss 0.599693.
Train: 2018-08-02T00:01:08.182134: step 18096, loss 0.618065.
Train: 2018-08-02T00:01:08.348691: step 18097, loss 0.618115.
Train: 2018-08-02T00:01:08.518266: step 18098, loss 0.507845.
Train: 2018-08-02T00:01:08.690804: step 18099, loss 0.471027.
Train: 2018-08-02T00:01:08.855368: step 18100, loss 0.563126.
Test: 2018-08-02T00:01:09.390905: step 18100, loss 0.547581.
Train: 2018-08-02T00:01:10.188021: step 18101, loss 0.599087.
Train: 2018-08-02T00:01:10.354607: step 18102, loss 0.508363.
Train: 2018-08-02T00:01:10.525122: step 18103, loss 0.56383.
Train: 2018-08-02T00:01:10.690709: step 18104, loss 0.489807.
Train: 2018-08-02T00:01:10.855246: step 18105, loss 0.636631.
Train: 2018-08-02T00:01:11.020795: step 18106, loss 0.636594.
Train: 2018-08-02T00:01:11.182394: step 18107, loss 0.672653.
Train: 2018-08-02T00:01:11.345927: step 18108, loss 0.617336.
Train: 2018-08-02T00:01:11.519463: step 18109, loss 0.598963.
Train: 2018-08-02T00:01:11.686018: step 18110, loss 0.706994.
Test: 2018-08-02T00:01:12.226573: step 18110, loss 0.547609.
Train: 2018-08-02T00:01:12.397116: step 18111, loss 0.580567.
Train: 2018-08-02T00:01:12.558710: step 18112, loss 0.544735.
Train: 2018-08-02T00:01:12.724242: step 18113, loss 0.491513.
Train: 2018-08-02T00:01:12.886832: step 18114, loss 0.509429.
Train: 2018-08-02T00:01:13.065330: step 18115, loss 0.4919.
Train: 2018-08-02T00:01:13.230887: step 18116, loss 0.562483.
Train: 2018-08-02T00:01:13.398440: step 18117, loss 0.597668.
Train: 2018-08-02T00:01:13.568983: step 18118, loss 0.509771.
Train: 2018-08-02T00:01:13.735563: step 18119, loss 0.597554.
Train: 2018-08-02T00:01:13.901097: step 18120, loss 0.527411.
Test: 2018-08-02T00:01:14.434671: step 18120, loss 0.547791.
Train: 2018-08-02T00:01:14.599242: step 18121, loss 0.509934.
Train: 2018-08-02T00:01:14.775758: step 18122, loss 0.719968.
Train: 2018-08-02T00:01:14.942341: step 18123, loss 0.632286.
Train: 2018-08-02T00:01:15.117844: step 18124, loss 0.597245.
Train: 2018-08-02T00:01:15.285396: step 18125, loss 0.527718.
Train: 2018-08-02T00:01:15.448958: step 18126, loss 0.597028.
Train: 2018-08-02T00:01:15.618523: step 18127, loss 0.56241.
Train: 2018-08-02T00:01:15.780098: step 18128, loss 0.510761.
Train: 2018-08-02T00:01:15.941676: step 18129, loss 0.545218.
Train: 2018-08-02T00:01:16.108195: step 18130, loss 0.66541.
Test: 2018-08-02T00:01:16.635786: step 18130, loss 0.548067.
Train: 2018-08-02T00:01:16.810318: step 18131, loss 0.528158.
Train: 2018-08-02T00:01:16.976873: step 18132, loss 0.562408.
Train: 2018-08-02T00:01:17.141465: step 18133, loss 0.61361.
Train: 2018-08-02T00:01:17.317961: step 18134, loss 0.562413.
Train: 2018-08-02T00:01:17.483538: step 18135, loss 0.545419.
Train: 2018-08-02T00:01:17.650100: step 18136, loss 0.579394.
Train: 2018-08-02T00:01:17.811668: step 18137, loss 0.51158.
Train: 2018-08-02T00:01:17.978202: step 18138, loss 0.579364.
Train: 2018-08-02T00:01:18.146747: step 18139, loss 0.562428.
Train: 2018-08-02T00:01:18.306350: step 18140, loss 0.494772.
Test: 2018-08-02T00:01:18.834935: step 18140, loss 0.548258.
Train: 2018-08-02T00:01:19.002459: step 18141, loss 0.647039.
Train: 2018-08-02T00:01:19.168016: step 18142, loss 0.562431.
Train: 2018-08-02T00:01:19.345542: step 18143, loss 0.545535.
Train: 2018-08-02T00:01:19.511099: step 18144, loss 0.579328.
Train: 2018-08-02T00:01:19.676657: step 18145, loss 0.596208.
Train: 2018-08-02T00:01:19.845231: step 18146, loss 0.562439.
Train: 2018-08-02T00:01:20.007799: step 18147, loss 0.646742.
Train: 2018-08-02T00:01:20.174351: step 18148, loss 0.629768.
Train: 2018-08-02T00:01:20.339884: step 18149, loss 0.56246.
Train: 2018-08-02T00:01:20.503472: step 18150, loss 0.579222.
Test: 2018-08-02T00:01:21.042006: step 18150, loss 0.548487.
Train: 2018-08-02T00:01:21.209575: step 18151, loss 0.579198.
Train: 2018-08-02T00:01:21.376148: step 18152, loss 0.545816.
Train: 2018-08-02T00:01:21.547655: step 18153, loss 0.545852.
Train: 2018-08-02T00:01:21.711243: step 18154, loss 0.562513.
Train: 2018-08-02T00:01:21.880764: step 18155, loss 0.612382.
Train: 2018-08-02T00:01:22.047344: step 18156, loss 0.579126.
Train: 2018-08-02T00:01:22.217862: step 18157, loss 0.628835.
Train: 2018-08-02T00:01:22.386412: step 18158, loss 0.479864.
Train: 2018-08-02T00:01:22.556987: step 18159, loss 0.546031.
Train: 2018-08-02T00:01:22.725532: step 18160, loss 0.546035.
Test: 2018-08-02T00:01:23.263069: step 18160, loss 0.548718.
Train: 2018-08-02T00:01:23.428645: step 18161, loss 0.51297.
Train: 2018-08-02T00:01:23.604190: step 18162, loss 0.579098.
Train: 2018-08-02T00:01:23.766722: step 18163, loss 0.678472.
Train: 2018-08-02T00:01:23.927319: step 18164, loss 0.529458.
Train: 2018-08-02T00:01:24.090857: step 18165, loss 0.59564.
Train: 2018-08-02T00:01:24.256414: step 18166, loss 0.546022.
Train: 2018-08-02T00:01:24.423991: step 18167, loss 0.562558.
Train: 2018-08-02T00:01:24.596505: step 18168, loss 0.595624.
Train: 2018-08-02T00:01:24.777021: step 18169, loss 0.546033.
Train: 2018-08-02T00:01:24.940585: step 18170, loss 0.512975.
Test: 2018-08-02T00:01:25.478149: step 18170, loss 0.5487.
Train: 2018-08-02T00:01:25.651709: step 18171, loss 0.612183.
Train: 2018-08-02T00:01:25.823235: step 18172, loss 0.579098.
Train: 2018-08-02T00:01:25.994774: step 18173, loss 0.612192.
Train: 2018-08-02T00:01:26.164313: step 18174, loss 0.562555.
Train: 2018-08-02T00:01:26.332863: step 18175, loss 0.529498.
Train: 2018-08-02T00:01:26.502410: step 18176, loss 0.595626.
Train: 2018-08-02T00:01:26.667993: step 18177, loss 0.595622.
Train: 2018-08-02T00:01:26.842501: step 18178, loss 0.546039.
Train: 2018-08-02T00:01:27.011050: step 18179, loss 0.595607.
Train: 2018-08-02T00:01:27.176606: step 18180, loss 0.612109.
Test: 2018-08-02T00:01:27.701205: step 18180, loss 0.548761.
Train: 2018-08-02T00:01:27.870777: step 18181, loss 0.447095.
Train: 2018-08-02T00:01:28.036334: step 18182, loss 0.57908.
Train: 2018-08-02T00:01:28.197903: step 18183, loss 0.546037.
Train: 2018-08-02T00:01:28.361469: step 18184, loss 0.496388.
Train: 2018-08-02T00:01:28.524032: step 18185, loss 0.529388.
Train: 2018-08-02T00:01:28.693552: step 18186, loss 0.545905.
Train: 2018-08-02T00:01:28.857140: step 18187, loss 0.529189.
Train: 2018-08-02T00:01:29.022672: step 18188, loss 0.629304.
Train: 2018-08-02T00:01:29.201194: step 18189, loss 0.512278.
Train: 2018-08-02T00:01:29.363794: step 18190, loss 0.646321.
Test: 2018-08-02T00:01:29.901351: step 18190, loss 0.548402.
Train: 2018-08-02T00:01:30.065882: step 18191, loss 0.545672.
Train: 2018-08-02T00:01:30.235456: step 18192, loss 0.596066.
Train: 2018-08-02T00:01:30.398027: step 18193, loss 0.495185.
Train: 2018-08-02T00:01:30.566545: step 18194, loss 0.579287.
Train: 2018-08-02T00:01:30.732103: step 18195, loss 0.528711.
Train: 2018-08-02T00:01:30.894693: step 18196, loss 0.545541.
Train: 2018-08-02T00:01:31.055265: step 18197, loss 0.57935.
Train: 2018-08-02T00:01:31.221824: step 18198, loss 0.477691.
Train: 2018-08-02T00:01:31.393363: step 18199, loss 0.630373.
Train: 2018-08-02T00:01:31.560887: step 18200, loss 0.494368.
Test: 2018-08-02T00:01:32.100445: step 18200, loss 0.548134.
Train: 2018-08-02T00:01:32.848693: step 18201, loss 0.613556.
Train: 2018-08-02T00:01:33.014250: step 18202, loss 0.459989.
Train: 2018-08-02T00:01:33.175819: step 18203, loss 0.682206.
Train: 2018-08-02T00:01:33.338360: step 18204, loss 0.545281.
Train: 2018-08-02T00:01:33.500948: step 18205, loss 0.545266.
Train: 2018-08-02T00:01:33.666475: step 18206, loss 0.579562.
Train: 2018-08-02T00:01:33.830038: step 18207, loss 0.493735.
Train: 2018-08-02T00:01:33.996619: step 18208, loss 0.613989.
Train: 2018-08-02T00:01:34.172123: step 18209, loss 0.545201.
Train: 2018-08-02T00:01:34.335711: step 18210, loss 0.648506.
Test: 2018-08-02T00:01:34.867297: step 18210, loss 0.547994.
Train: 2018-08-02T00:01:35.033846: step 18211, loss 0.59683.
Train: 2018-08-02T00:01:35.194423: step 18212, loss 0.442033.
Train: 2018-08-02T00:01:35.357978: step 18213, loss 0.424721.
Train: 2018-08-02T00:01:35.524509: step 18214, loss 0.614179.
Train: 2018-08-02T00:01:35.698051: step 18215, loss 0.510557.
Train: 2018-08-02T00:01:35.868589: step 18216, loss 0.441159.
Train: 2018-08-02T00:01:36.035144: step 18217, loss 0.614578.
Train: 2018-08-02T00:01:36.201723: step 18218, loss 0.545006.
Train: 2018-08-02T00:01:36.363291: step 18219, loss 0.562443.
Train: 2018-08-02T00:01:36.527827: step 18220, loss 0.562452.
Test: 2018-08-02T00:01:37.060402: step 18220, loss 0.547775.
Train: 2018-08-02T00:01:37.223996: step 18221, loss 0.527386.
Train: 2018-08-02T00:01:37.394534: step 18222, loss 0.509753.
Train: 2018-08-02T00:01:37.566051: step 18223, loss 0.703412.
Train: 2018-08-02T00:01:37.732631: step 18224, loss 0.615348.
Train: 2018-08-02T00:01:37.900188: step 18225, loss 0.580095.
Train: 2018-08-02T00:01:38.062754: step 18226, loss 0.580076.
Train: 2018-08-02T00:01:38.226318: step 18227, loss 0.544895.
Train: 2018-08-02T00:01:38.389848: step 18228, loss 0.509774.
Train: 2018-08-02T00:01:38.564382: step 18229, loss 0.597595.
Train: 2018-08-02T00:01:38.731935: step 18230, loss 0.509806.
Test: 2018-08-02T00:01:39.258526: step 18230, loss 0.547766.
Train: 2018-08-02T00:01:39.428073: step 18231, loss 0.58002.
Train: 2018-08-02T00:01:39.589667: step 18232, loss 0.562464.
Train: 2018-08-02T00:01:39.759188: step 18233, loss 0.580008.
Train: 2018-08-02T00:01:39.921779: step 18234, loss 0.544924.
Train: 2018-08-02T00:01:40.088339: step 18235, loss 0.509868.
Train: 2018-08-02T00:01:40.250899: step 18236, loss 0.56246.
Train: 2018-08-02T00:01:40.425425: step 18237, loss 0.527381.
Train: 2018-08-02T00:01:40.591992: step 18238, loss 0.544913.
Train: 2018-08-02T00:01:40.758547: step 18239, loss 0.509777.
Train: 2018-08-02T00:01:40.927066: step 18240, loss 0.562475.
Test: 2018-08-02T00:01:41.470613: step 18240, loss 0.54774.
Train: 2018-08-02T00:01:41.635199: step 18241, loss 0.527272.
Train: 2018-08-02T00:01:41.803746: step 18242, loss 0.580118.
Train: 2018-08-02T00:01:41.974266: step 18243, loss 0.491919.
Train: 2018-08-02T00:01:42.134863: step 18244, loss 0.562507.
Train: 2018-08-02T00:01:42.308400: step 18245, loss 0.580214.
Train: 2018-08-02T00:01:42.470965: step 18246, loss 0.509381.
Train: 2018-08-02T00:01:42.636527: step 18247, loss 0.651232.
Train: 2018-08-02T00:01:42.808063: step 18248, loss 0.509315.
Train: 2018-08-02T00:01:42.972624: step 18249, loss 0.686794.
Train: 2018-08-02T00:01:43.139153: step 18250, loss 0.50934.
Test: 2018-08-02T00:01:43.680705: step 18250, loss 0.547686.
Train: 2018-08-02T00:01:43.849280: step 18251, loss 0.544805.
Train: 2018-08-02T00:01:44.014837: step 18252, loss 0.68654.
Train: 2018-08-02T00:01:44.177377: step 18253, loss 0.438739.
Train: 2018-08-02T00:01:44.343957: step 18254, loss 0.509477.
Train: 2018-08-02T00:01:44.507494: step 18255, loss 0.56251.
Train: 2018-08-02T00:01:44.681031: step 18256, loss 0.597888.
Train: 2018-08-02T00:01:44.848609: step 18257, loss 0.562509.
Train: 2018-08-02T00:01:45.014141: step 18258, loss 0.633207.
Train: 2018-08-02T00:01:45.176731: step 18259, loss 0.597799.
Train: 2018-08-02T00:01:45.343285: step 18260, loss 0.456772.
Test: 2018-08-02T00:01:45.874867: step 18260, loss 0.547734.
Train: 2018-08-02T00:01:46.038428: step 18261, loss 0.650553.
Train: 2018-08-02T00:01:46.202992: step 18262, loss 0.580062.
Train: 2018-08-02T00:01:46.369549: step 18263, loss 0.702926.
Train: 2018-08-02T00:01:46.541058: step 18264, loss 0.61494.
Train: 2018-08-02T00:01:46.712632: step 18265, loss 0.562432.
Train: 2018-08-02T00:01:46.875190: step 18266, loss 0.579789.
Train: 2018-08-02T00:01:47.035767: step 18267, loss 0.562413.
Train: 2018-08-02T00:01:47.200327: step 18268, loss 0.545155.
Train: 2018-08-02T00:01:47.363859: step 18269, loss 0.527992.
Train: 2018-08-02T00:01:47.535403: step 18270, loss 0.562404.
Test: 2018-08-02T00:01:48.074958: step 18270, loss 0.548048.
Train: 2018-08-02T00:01:48.238545: step 18271, loss 0.596691.
Train: 2018-08-02T00:01:48.405101: step 18272, loss 0.596622.
Train: 2018-08-02T00:01:48.569635: step 18273, loss 0.528268.
Train: 2018-08-02T00:01:48.736221: step 18274, loss 0.511283.
Train: 2018-08-02T00:01:48.906735: step 18275, loss 0.443198.
Train: 2018-08-02T00:01:49.074315: step 18276, loss 0.749945.
Train: 2018-08-02T00:01:49.240850: step 18277, loss 0.511348.
Train: 2018-08-02T00:01:49.404429: step 18278, loss 0.562412.
Train: 2018-08-02T00:01:49.564999: step 18279, loss 0.596408.
Train: 2018-08-02T00:01:49.732552: step 18280, loss 0.579396.
Test: 2018-08-02T00:01:50.261114: step 18280, loss 0.548215.
Train: 2018-08-02T00:01:50.424676: step 18281, loss 0.511535.
Train: 2018-08-02T00:01:50.593226: step 18282, loss 0.477635.
Train: 2018-08-02T00:01:50.757786: step 18283, loss 0.511495.
Train: 2018-08-02T00:01:50.920376: step 18284, loss 0.613419.
Train: 2018-08-02T00:01:51.081954: step 18285, loss 0.494348.
Train: 2018-08-02T00:01:51.244485: step 18286, loss 0.613543.
Train: 2018-08-02T00:01:51.408078: step 18287, loss 0.528289.
Train: 2018-08-02T00:01:51.571638: step 18288, loss 0.511164.
Train: 2018-08-02T00:01:51.743152: step 18289, loss 0.733524.
Train: 2018-08-02T00:01:51.903748: step 18290, loss 0.596604.
Test: 2018-08-02T00:01:52.438308: step 18290, loss 0.548102.
Train: 2018-08-02T00:01:52.604848: step 18291, loss 0.494082.
Train: 2018-08-02T00:01:52.767448: step 18292, loss 0.596566.
Train: 2018-08-02T00:01:52.936961: step 18293, loss 0.647766.
Train: 2018-08-02T00:01:53.105535: step 18294, loss 0.562408.
Train: 2018-08-02T00:01:53.274060: step 18295, loss 0.57943.
Train: 2018-08-02T00:01:53.439647: step 18296, loss 0.613396.
Train: 2018-08-02T00:01:53.603213: step 18297, loss 0.460662.
Train: 2018-08-02T00:01:53.768761: step 18298, loss 0.494606.
Train: 2018-08-02T00:01:53.930331: step 18299, loss 0.545453.
Train: 2018-08-02T00:01:54.096891: step 18300, loss 0.613353.
Test: 2018-08-02T00:01:54.628469: step 18300, loss 0.548195.
Train: 2018-08-02T00:01:55.394613: step 18301, loss 0.477511.
Train: 2018-08-02T00:01:55.561181: step 18302, loss 0.613422.
Train: 2018-08-02T00:01:55.734702: step 18303, loss 0.494364.
Train: 2018-08-02T00:01:55.899262: step 18304, loss 0.579445.
Train: 2018-08-02T00:01:56.071829: step 18305, loss 0.664732.
Train: 2018-08-02T00:01:56.234340: step 18306, loss 0.630594.
Train: 2018-08-02T00:01:56.398900: step 18307, loss 0.545386.
Train: 2018-08-02T00:01:56.571465: step 18308, loss 0.477373.
Train: 2018-08-02T00:01:56.735030: step 18309, loss 0.579424.
Train: 2018-08-02T00:01:56.901583: step 18310, loss 0.579426.
Test: 2018-08-02T00:01:57.435162: step 18310, loss 0.548163.
Train: 2018-08-02T00:01:57.600692: step 18311, loss 0.613453.
Train: 2018-08-02T00:01:57.767268: step 18312, loss 0.52841.
Train: 2018-08-02T00:01:57.929807: step 18313, loss 0.494423.
Train: 2018-08-02T00:01:58.099354: step 18314, loss 0.528388.
Train: 2018-08-02T00:01:58.263915: step 18315, loss 0.681633.
Train: 2018-08-02T00:01:58.442437: step 18316, loss 0.528364.
Train: 2018-08-02T00:01:58.617996: step 18317, loss 0.51134.
Train: 2018-08-02T00:01:58.782562: step 18318, loss 0.5113.
Train: 2018-08-02T00:01:58.966038: step 18319, loss 0.562406.
Train: 2018-08-02T00:01:59.128628: step 18320, loss 0.596569.
Test: 2018-08-02T00:01:59.647262: step 18320, loss 0.54809.
Train: 2018-08-02T00:01:59.824773: step 18321, loss 0.63078.
Train: 2018-08-02T00:02:00.005260: step 18322, loss 0.562404.
Train: 2018-08-02T00:02:00.168853: step 18323, loss 0.476984.
Train: 2018-08-02T00:02:00.337398: step 18324, loss 0.596605.
Train: 2018-08-02T00:02:00.497974: step 18325, loss 0.613725.
Train: 2018-08-02T00:02:00.665513: step 18326, loss 0.511099.
Train: 2018-08-02T00:02:00.831077: step 18327, loss 0.493969.
Train: 2018-08-02T00:02:00.995611: step 18328, loss 0.442483.
Train: 2018-08-02T00:02:01.160173: step 18329, loss 0.613944.
Train: 2018-08-02T00:02:01.325730: step 18330, loss 0.665677.
Test: 2018-08-02T00:02:01.867283: step 18330, loss 0.547987.
Train: 2018-08-02T00:02:02.035859: step 18331, loss 0.527971.
Train: 2018-08-02T00:02:02.205389: step 18332, loss 0.493492.
Train: 2018-08-02T00:02:02.370936: step 18333, loss 0.510642.
Train: 2018-08-02T00:02:02.539498: step 18334, loss 0.579699.
Train: 2018-08-02T00:02:02.706065: step 18335, loss 0.545094.
Train: 2018-08-02T00:02:02.867633: step 18336, loss 0.527723.
Train: 2018-08-02T00:02:03.043165: step 18337, loss 0.527662.
Train: 2018-08-02T00:02:03.209718: step 18338, loss 0.614679.
Train: 2018-08-02T00:02:03.439445: step 18339, loss 0.475244.
Train: 2018-08-02T00:02:03.602012: step 18340, loss 0.52749.
Test: 2018-08-02T00:02:04.130596: step 18340, loss 0.547783.
Train: 2018-08-02T00:02:04.297177: step 18341, loss 0.632519.
Train: 2018-08-02T00:02:04.466699: step 18342, loss 0.562457.
Train: 2018-08-02T00:02:04.628297: step 18343, loss 0.509802.
Train: 2018-08-02T00:02:04.794821: step 18344, loss 0.615208.
Train: 2018-08-02T00:02:04.962373: step 18345, loss 0.562473.
Train: 2018-08-02T00:02:05.124939: step 18346, loss 0.562476.
Train: 2018-08-02T00:02:05.291517: step 18347, loss 0.544876.
Train: 2018-08-02T00:02:05.456078: step 18348, loss 0.492041.
Train: 2018-08-02T00:02:05.630621: step 18349, loss 0.721171.
Train: 2018-08-02T00:02:05.794171: step 18350, loss 0.49203.
Test: 2018-08-02T00:02:06.331740: step 18350, loss 0.547734.
Train: 2018-08-02T00:02:06.499296: step 18351, loss 0.615313.
Train: 2018-08-02T00:02:06.666816: step 18352, loss 0.527284.
Train: 2018-08-02T00:02:06.832374: step 18353, loss 0.580062.
Train: 2018-08-02T00:02:07.005910: step 18354, loss 0.527315.
Train: 2018-08-02T00:02:07.170504: step 18355, loss 0.580041.
Train: 2018-08-02T00:02:07.340018: step 18356, loss 0.650288.
Train: 2018-08-02T00:02:07.502611: step 18357, loss 0.509851.
Train: 2018-08-02T00:02:07.674124: step 18358, loss 0.509893.
Train: 2018-08-02T00:02:07.837687: step 18359, loss 0.632518.
Train: 2018-08-02T00:02:08.005238: step 18360, loss 0.527452.
Test: 2018-08-02T00:02:08.543830: step 18360, loss 0.547801.
Train: 2018-08-02T00:02:08.707390: step 18361, loss 0.637049.
Train: 2018-08-02T00:02:08.876909: step 18362, loss 0.492605.
Train: 2018-08-02T00:02:09.039474: step 18363, loss 0.4403.
Train: 2018-08-02T00:02:09.205031: step 18364, loss 0.544972.
Train: 2018-08-02T00:02:09.372617: step 18365, loss 0.492499.
Train: 2018-08-02T00:02:09.543158: step 18366, loss 0.59749.
Train: 2018-08-02T00:02:09.705723: step 18367, loss 0.597537.
Train: 2018-08-02T00:02:09.870254: step 18368, loss 0.56246.
Train: 2018-08-02T00:02:10.035841: step 18369, loss 0.49224.
Train: 2018-08-02T00:02:10.200370: step 18370, loss 0.650354.
Test: 2018-08-02T00:02:10.734942: step 18370, loss 0.547751.
Train: 2018-08-02T00:02:10.901525: step 18371, loss 0.615192.
Train: 2018-08-02T00:02:11.067054: step 18372, loss 0.597581.
Train: 2018-08-02T00:02:11.243625: step 18373, loss 0.562456.
Train: 2018-08-02T00:02:11.410162: step 18374, loss 0.597476.
Train: 2018-08-02T00:02:11.577688: step 18375, loss 0.579926.
Train: 2018-08-02T00:02:11.743277: step 18376, loss 0.649695.
Train: 2018-08-02T00:02:11.906808: step 18377, loss 0.545022.
Train: 2018-08-02T00:02:12.070372: step 18378, loss 0.527695.
Train: 2018-08-02T00:02:12.241912: step 18379, loss 0.545082.
Train: 2018-08-02T00:02:12.410464: step 18380, loss 0.545102.
Test: 2018-08-02T00:02:12.953013: step 18380, loss 0.547929.
Train: 2018-08-02T00:02:13.124554: step 18381, loss 0.493245.
Train: 2018-08-02T00:02:13.292140: step 18382, loss 0.579699.
Train: 2018-08-02T00:02:13.459691: step 18383, loss 0.510547.
Train: 2018-08-02T00:02:13.625249: step 18384, loss 0.475935.
Train: 2018-08-02T00:02:13.798752: step 18385, loss 0.597055.
Train: 2018-08-02T00:02:13.965335: step 18386, loss 0.614426.
Train: 2018-08-02T00:02:14.131860: step 18387, loss 0.423701.
Train: 2018-08-02T00:02:14.296447: step 18388, loss 0.49293.
Train: 2018-08-02T00:02:14.462976: step 18389, loss 0.614681.
Train: 2018-08-02T00:02:14.632555: step 18390, loss 0.510096.
Test: 2018-08-02T00:02:15.171084: step 18390, loss 0.547802.
Train: 2018-08-02T00:02:15.343621: step 18391, loss 0.597406.
Train: 2018-08-02T00:02:15.511198: step 18392, loss 0.527436.
Train: 2018-08-02T00:02:15.679723: step 18393, loss 0.45725.
Train: 2018-08-02T00:02:15.854282: step 18394, loss 0.56247.
Train: 2018-08-02T00:02:16.020811: step 18395, loss 0.70349.
Train: 2018-08-02T00:02:16.186368: step 18396, loss 0.650636.
Train: 2018-08-02T00:02:16.349931: step 18397, loss 0.544868.
Train: 2018-08-02T00:02:16.519478: step 18398, loss 0.50968.
Train: 2018-08-02T00:02:16.683041: step 18399, loss 0.456892.
Train: 2018-08-02T00:02:16.847601: step 18400, loss 0.562482.
Test: 2018-08-02T00:02:17.380178: step 18400, loss 0.54772.
Train: 2018-08-02T00:02:18.230698: step 18401, loss 0.668312.
Train: 2018-08-02T00:02:18.402210: step 18402, loss 0.544858.
Train: 2018-08-02T00:02:18.578769: step 18403, loss 0.52724.
Train: 2018-08-02T00:02:18.747288: step 18404, loss 0.527237.
Train: 2018-08-02T00:02:18.907891: step 18405, loss 0.580118.
Train: 2018-08-02T00:02:19.075409: step 18406, loss 0.509589.
Train: 2018-08-02T00:02:19.240968: step 18407, loss 0.615426.
Train: 2018-08-02T00:02:19.410514: step 18408, loss 0.527207.
Train: 2018-08-02T00:02:19.583053: step 18409, loss 0.580138.
Train: 2018-08-02T00:02:19.748610: step 18410, loss 0.615424.
Test: 2018-08-02T00:02:20.289165: step 18410, loss 0.547724.
Train: 2018-08-02T00:02:20.456748: step 18411, loss 0.774032.
Train: 2018-08-02T00:02:20.630284: step 18412, loss 0.474639.
Train: 2018-08-02T00:02:20.805815: step 18413, loss 0.562453.
Train: 2018-08-02T00:02:20.978323: step 18414, loss 0.579934.
Train: 2018-08-02T00:02:21.142883: step 18415, loss 0.527527.
Train: 2018-08-02T00:02:21.308440: step 18416, loss 0.579856.
Train: 2018-08-02T00:02:21.469039: step 18417, loss 0.579822.
Train: 2018-08-02T00:02:21.641549: step 18418, loss 0.458209.
Train: 2018-08-02T00:02:21.817114: step 18419, loss 0.666603.
Train: 2018-08-02T00:02:21.983667: step 18420, loss 0.545077.
Test: 2018-08-02T00:02:22.527214: step 18420, loss 0.547912.
Train: 2018-08-02T00:02:22.696755: step 18421, loss 0.579724.
Train: 2018-08-02T00:02:22.860292: step 18422, loss 0.47596.
Train: 2018-08-02T00:02:23.034856: step 18423, loss 0.614269.
Train: 2018-08-02T00:02:23.214376: step 18424, loss 0.52786.
Train: 2018-08-02T00:02:23.381924: step 18425, loss 0.579673.
Train: 2018-08-02T00:02:23.551445: step 18426, loss 0.579663.
Train: 2018-08-02T00:02:23.718007: step 18427, loss 0.614141.
Train: 2018-08-02T00:02:23.887579: step 18428, loss 0.562402.
Train: 2018-08-02T00:02:24.052105: step 18429, loss 0.459199.
Train: 2018-08-02T00:02:24.217694: step 18430, loss 0.45917.
Test: 2018-08-02T00:02:24.748246: step 18430, loss 0.547971.
Train: 2018-08-02T00:02:24.914799: step 18431, loss 0.562402.
Train: 2018-08-02T00:02:25.083350: step 18432, loss 0.527887.
Train: 2018-08-02T00:02:25.247936: step 18433, loss 0.614275.
Train: 2018-08-02T00:02:25.413467: step 18434, loss 0.614321.
Train: 2018-08-02T00:02:25.583014: step 18435, loss 0.597019.
Train: 2018-08-02T00:02:25.748571: step 18436, loss 0.493218.
Train: 2018-08-02T00:02:25.913160: step 18437, loss 0.562409.
Train: 2018-08-02T00:02:26.079687: step 18438, loss 0.545096.
Train: 2018-08-02T00:02:26.241253: step 18439, loss 0.545087.
Train: 2018-08-02T00:02:26.406811: step 18440, loss 0.527739.
Test: 2018-08-02T00:02:26.928417: step 18440, loss 0.547882.
Train: 2018-08-02T00:02:27.099984: step 18441, loss 0.510348.
Train: 2018-08-02T00:02:27.262550: step 18442, loss 0.597194.
Train: 2018-08-02T00:02:27.429107: step 18443, loss 0.614626.
Train: 2018-08-02T00:02:27.595634: step 18444, loss 0.54502.
Train: 2018-08-02T00:02:27.766177: step 18445, loss 0.579833.
Train: 2018-08-02T00:02:27.931768: step 18446, loss 0.527606.
Train: 2018-08-02T00:02:28.091309: step 18447, loss 0.597259.
Train: 2018-08-02T00:02:28.264844: step 18448, loss 0.614668.
Train: 2018-08-02T00:02:28.431399: step 18449, loss 0.61462.
Train: 2018-08-02T00:02:28.593965: step 18450, loss 0.458183.
Test: 2018-08-02T00:02:29.120559: step 18450, loss 0.547871.
Train: 2018-08-02T00:02:29.298083: step 18451, loss 0.545046.
Train: 2018-08-02T00:02:29.470620: step 18452, loss 0.527667.
Train: 2018-08-02T00:02:29.638173: step 18453, loss 0.492876.
Train: 2018-08-02T00:02:29.802734: step 18454, loss 0.545014.
Train: 2018-08-02T00:02:29.966297: step 18455, loss 0.527557.
Train: 2018-08-02T00:02:30.132850: step 18456, loss 0.632305.
Train: 2018-08-02T00:02:30.303426: step 18457, loss 0.422622.
Train: 2018-08-02T00:02:30.471969: step 18458, loss 0.597485.
Train: 2018-08-02T00:02:30.636504: step 18459, loss 0.544913.
Train: 2018-08-02T00:02:30.801095: step 18460, loss 0.474604.
Test: 2018-08-02T00:02:31.348601: step 18460, loss 0.547729.
Train: 2018-08-02T00:02:31.518147: step 18461, loss 0.4744.
Train: 2018-08-02T00:02:31.685730: step 18462, loss 0.597851.
Train: 2018-08-02T00:02:31.851257: step 18463, loss 0.615671.
Train: 2018-08-02T00:02:32.020842: step 18464, loss 0.491565.
Train: 2018-08-02T00:02:32.194340: step 18465, loss 0.580325.
Train: 2018-08-02T00:02:32.360895: step 18466, loss 0.544756.
Train: 2018-08-02T00:02:32.529444: step 18467, loss 0.580404.
Train: 2018-08-02T00:02:32.708965: step 18468, loss 0.544734.
Train: 2018-08-02T00:02:32.884521: step 18469, loss 0.61619.
Train: 2018-08-02T00:02:33.055065: step 18470, loss 0.50899.
Test: 2018-08-02T00:02:33.591604: step 18470, loss 0.547627.
Train: 2018-08-02T00:02:33.780344: step 18471, loss 0.508962.
Train: 2018-08-02T00:02:33.947896: step 18472, loss 0.544711.
Train: 2018-08-02T00:02:34.111458: step 18473, loss 0.473026.
Train: 2018-08-02T00:02:34.275021: step 18474, loss 0.562643.
Train: 2018-08-02T00:02:34.452548: step 18475, loss 0.598631.
Train: 2018-08-02T00:02:34.615144: step 18476, loss 0.562671.
Train: 2018-08-02T00:02:34.781667: step 18477, loss 0.562679.
Train: 2018-08-02T00:02:34.949219: step 18478, loss 0.65279.
Train: 2018-08-02T00:02:35.113811: step 18479, loss 0.562674.
Train: 2018-08-02T00:02:35.275348: step 18480, loss 0.508697.
Test: 2018-08-02T00:02:35.800973: step 18480, loss 0.547601.
Train: 2018-08-02T00:02:35.967522: step 18481, loss 0.526693.
Train: 2018-08-02T00:02:36.131076: step 18482, loss 0.670565.
Train: 2018-08-02T00:02:36.305594: step 18483, loss 0.67039.
Train: 2018-08-02T00:02:36.478163: step 18484, loss 0.508893.
Train: 2018-08-02T00:02:36.654691: step 18485, loss 0.562594.
Train: 2018-08-02T00:02:36.829220: step 18486, loss 0.705274.
Train: 2018-08-02T00:02:36.993753: step 18487, loss 0.580316.
Train: 2018-08-02T00:02:37.165294: step 18488, loss 0.527101.
Train: 2018-08-02T00:02:37.327887: step 18489, loss 0.527184.
Train: 2018-08-02T00:02:37.506415: step 18490, loss 0.59771.
Test: 2018-08-02T00:02:38.039971: step 18490, loss 0.547752.
Train: 2018-08-02T00:02:38.209504: step 18491, loss 0.703018.
Train: 2018-08-02T00:02:38.376066: step 18492, loss 0.492465.
Train: 2018-08-02T00:02:38.553585: step 18493, loss 0.597315.
Train: 2018-08-02T00:02:38.720138: step 18494, loss 0.649352.
Train: 2018-08-02T00:02:38.889712: step 18495, loss 0.597039.
Train: 2018-08-02T00:02:39.055274: step 18496, loss 0.596887.
Train: 2018-08-02T00:02:39.222795: step 18497, loss 0.562399.
Train: 2018-08-02T00:02:39.389378: step 18498, loss 0.511095.
Train: 2018-08-02T00:02:39.553910: step 18499, loss 0.511241.
Train: 2018-08-02T00:02:39.719498: step 18500, loss 0.715621.
Test: 2018-08-02T00:02:40.262018: step 18500, loss 0.548211.
Train: 2018-08-02T00:02:41.020151: step 18501, loss 0.528493.
Train: 2018-08-02T00:02:41.191691: step 18502, loss 0.680809.
Train: 2018-08-02T00:02:41.369218: step 18503, loss 0.495071.
Train: 2018-08-02T00:02:41.532814: step 18504, loss 0.56245.
Train: 2018-08-02T00:02:41.706316: step 18505, loss 0.545708.
Train: 2018-08-02T00:02:41.871873: step 18506, loss 0.595917.
Train: 2018-08-02T00:02:42.037430: step 18507, loss 0.545797.
Train: 2018-08-02T00:02:42.212993: step 18508, loss 0.41254.
Train: 2018-08-02T00:02:42.386524: step 18509, loss 0.545812.
Train: 2018-08-02T00:02:42.559038: step 18510, loss 0.529092.
Test: 2018-08-02T00:02:43.094605: step 18510, loss 0.548471.
Train: 2018-08-02T00:02:43.267174: step 18511, loss 0.478868.
Train: 2018-08-02T00:02:43.430731: step 18512, loss 0.545692.
Train: 2018-08-02T00:02:43.596264: step 18513, loss 0.528821.
Train: 2018-08-02T00:02:43.762843: step 18514, loss 0.579295.
Train: 2018-08-02T00:02:43.929404: step 18515, loss 0.579328.
Train: 2018-08-02T00:02:44.103906: step 18516, loss 0.528538.
Train: 2018-08-02T00:02:44.270462: step 18517, loss 0.66429.
Train: 2018-08-02T00:02:44.436019: step 18518, loss 0.630379.
Train: 2018-08-02T00:02:44.598585: step 18519, loss 0.511446.
Train: 2018-08-02T00:02:44.761150: step 18520, loss 0.596402.
Test: 2018-08-02T00:02:45.292760: step 18520, loss 0.548176.
Train: 2018-08-02T00:02:45.458287: step 18521, loss 0.426436.
Train: 2018-08-02T00:02:45.631847: step 18522, loss 0.562405.
Train: 2018-08-02T00:02:45.798378: step 18523, loss 0.494164.
Train: 2018-08-02T00:02:45.969918: step 18524, loss 0.511091.
Train: 2018-08-02T00:02:46.133481: step 18525, loss 0.545244.
Train: 2018-08-02T00:02:46.305022: step 18526, loss 0.700039.
Train: 2018-08-02T00:02:46.467625: step 18527, loss 0.510747.
Train: 2018-08-02T00:02:46.633157: step 18528, loss 0.631366.
Train: 2018-08-02T00:02:46.797739: step 18529, loss 0.562402.
Train: 2018-08-02T00:02:46.967253: step 18530, loss 0.648651.
Test: 2018-08-02T00:02:47.498863: step 18530, loss 0.54797.
Train: 2018-08-02T00:02:47.665386: step 18531, loss 0.579634.
Train: 2018-08-02T00:02:47.828948: step 18532, loss 0.631254.
Train: 2018-08-02T00:02:48.009465: step 18533, loss 0.682657.
Train: 2018-08-02T00:02:48.173060: step 18534, loss 0.57952.
Train: 2018-08-02T00:02:48.346599: step 18535, loss 0.528273.
Train: 2018-08-02T00:02:48.512153: step 18536, loss 0.613471.
Train: 2018-08-02T00:02:48.676683: step 18537, loss 0.460582.
Train: 2018-08-02T00:02:48.843240: step 18538, loss 0.511561.
Train: 2018-08-02T00:02:49.005834: step 18539, loss 0.477687.
Train: 2018-08-02T00:02:49.173355: step 18540, loss 0.494567.
Test: 2018-08-02T00:02:49.700969: step 18540, loss 0.54818.
Train: 2018-08-02T00:02:49.869493: step 18541, loss 0.511432.
Train: 2018-08-02T00:02:50.050042: step 18542, loss 0.579437.
Train: 2018-08-02T00:02:50.216574: step 18543, loss 0.494141.
Train: 2018-08-02T00:02:50.385116: step 18544, loss 0.613733.
Train: 2018-08-02T00:02:50.556657: step 18545, loss 0.59668.
Train: 2018-08-02T00:02:50.724210: step 18546, loss 0.579557.
Train: 2018-08-02T00:02:50.885778: step 18547, loss 0.510885.
Train: 2018-08-02T00:02:51.051359: step 18548, loss 0.562399.
Train: 2018-08-02T00:02:51.216892: step 18549, loss 0.527969.
Train: 2018-08-02T00:02:51.383472: step 18550, loss 0.458949.
Test: 2018-08-02T00:02:51.923004: step 18550, loss 0.547927.
Train: 2018-08-02T00:02:52.090583: step 18551, loss 0.562405.
Train: 2018-08-02T00:02:52.254150: step 18552, loss 0.510413.
Train: 2018-08-02T00:02:52.422694: step 18553, loss 0.562418.
Train: 2018-08-02T00:02:52.586256: step 18554, loss 0.684414.
Train: 2018-08-02T00:02:52.750822: step 18555, loss 0.492678.
Train: 2018-08-02T00:02:52.915351: step 18556, loss 0.510047.
Train: 2018-08-02T00:02:53.085900: step 18557, loss 0.52745.
Train: 2018-08-02T00:02:53.252450: step 18558, loss 0.615053.
Train: 2018-08-02T00:02:53.421025: step 18559, loss 0.61512.
Train: 2018-08-02T00:02:53.592541: step 18560, loss 0.474667.
Test: 2018-08-02T00:02:54.136099: step 18560, loss 0.547745.
Train: 2018-08-02T00:02:54.309649: step 18561, loss 0.58005.
Train: 2018-08-02T00:02:54.474215: step 18562, loss 0.597668.
Train: 2018-08-02T00:02:54.637782: step 18563, loss 0.685691.
Train: 2018-08-02T00:02:54.814275: step 18564, loss 0.562466.
Train: 2018-08-02T00:02:54.980831: step 18565, loss 0.650224.
Train: 2018-08-02T00:02:55.144407: step 18566, loss 0.632487.
Train: 2018-08-02T00:02:55.313940: step 18567, loss 0.544978.
Train: 2018-08-02T00:02:55.483486: step 18568, loss 0.666865.
Train: 2018-08-02T00:02:55.646052: step 18569, loss 0.579751.
Train: 2018-08-02T00:02:55.830559: step 18570, loss 0.614228.
Test: 2018-08-02T00:02:56.358180: step 18570, loss 0.547993.
Train: 2018-08-02T00:02:56.528711: step 18571, loss 0.631214.
Train: 2018-08-02T00:02:56.693252: step 18572, loss 0.511028.
Train: 2018-08-02T00:02:56.853843: step 18573, loss 0.460021.
Train: 2018-08-02T00:02:57.021375: step 18574, loss 0.579439.
Train: 2018-08-02T00:02:57.184938: step 18575, loss 0.494393.
Train: 2018-08-02T00:02:57.353521: step 18576, loss 0.596395.
Train: 2018-08-02T00:02:57.522037: step 18577, loss 0.477532.
Train: 2018-08-02T00:02:57.684602: step 18578, loss 0.579392.
Train: 2018-08-02T00:02:57.848195: step 18579, loss 0.664305.
Train: 2018-08-02T00:02:58.014720: step 18580, loss 0.579374.
Test: 2018-08-02T00:02:58.544304: step 18580, loss 0.548236.
Train: 2018-08-02T00:02:58.708865: step 18581, loss 0.545482.
Train: 2018-08-02T00:02:58.885405: step 18582, loss 0.714698.
Train: 2018-08-02T00:02:59.055937: step 18583, loss 0.528692.
Train: 2018-08-02T00:02:59.228477: step 18584, loss 0.646606.
Train: 2018-08-02T00:02:59.404006: step 18585, loss 0.545672.
Train: 2018-08-02T00:02:59.574550: step 18586, loss 0.646167.
Train: 2018-08-02T00:02:59.742103: step 18587, loss 0.495748.
Train: 2018-08-02T00:02:59.904697: step 18588, loss 0.562498.
Train: 2018-08-02T00:03:00.071222: step 18589, loss 0.579133.
Train: 2018-08-02T00:03:00.241766: step 18590, loss 0.545925.
Test: 2018-08-02T00:03:00.779347: step 18590, loss 0.548648.
Train: 2018-08-02T00:03:00.948876: step 18591, loss 0.61226.
Train: 2018-08-02T00:03:01.121440: step 18592, loss 0.512894.
Train: 2018-08-02T00:03:01.287969: step 18593, loss 0.52947.
Train: 2018-08-02T00:03:01.452529: step 18594, loss 0.546008.
Train: 2018-08-02T00:03:01.624096: step 18595, loss 0.579089.
Train: 2018-08-02T00:03:01.792647: step 18596, loss 0.595639.
Train: 2018-08-02T00:03:01.956214: step 18597, loss 0.579089.
Train: 2018-08-02T00:03:02.123761: step 18598, loss 0.512923.
Train: 2018-08-02T00:03:02.290290: step 18599, loss 0.52944.
Train: 2018-08-02T00:03:02.453878: step 18600, loss 0.661946.
Test: 2018-08-02T00:03:02.996428: step 18600, loss 0.548665.
Train: 2018-08-02T00:03:03.808321: step 18601, loss 0.595662.
Train: 2018-08-02T00:03:03.977893: step 18602, loss 0.545988.
Train: 2018-08-02T00:03:04.144454: step 18603, loss 0.595638.
Train: 2018-08-02T00:03:04.310011: step 18604, loss 0.6287.
Train: 2018-08-02T00:03:04.475568: step 18605, loss 0.529528.
Train: 2018-08-02T00:03:04.639125: step 18606, loss 0.529554.
Train: 2018-08-02T00:03:04.809676: step 18607, loss 0.480038.
Train: 2018-08-02T00:03:04.978218: step 18608, loss 0.479914.
Train: 2018-08-02T00:03:05.141757: step 18609, loss 0.579102.
Train: 2018-08-02T00:03:05.306347: step 18610, loss 0.645544.
Test: 2018-08-02T00:03:05.842883: step 18610, loss 0.548593.
Train: 2018-08-02T00:03:06.003454: step 18611, loss 0.512651.
Train: 2018-08-02T00:03:06.173032: step 18612, loss 0.579145.
Train: 2018-08-02T00:03:06.342549: step 18613, loss 0.529157.
Train: 2018-08-02T00:03:06.513119: step 18614, loss 0.529089.
Train: 2018-08-02T00:03:06.677684: step 18615, loss 0.545737.
Train: 2018-08-02T00:03:06.850191: step 18616, loss 0.579225.
Train: 2018-08-02T00:03:07.016775: step 18617, loss 0.478453.
Train: 2018-08-02T00:03:07.183298: step 18618, loss 0.444501.
Train: 2018-08-02T00:03:07.361823: step 18619, loss 0.596261.
Train: 2018-08-02T00:03:07.527410: step 18620, loss 0.664284.
Test: 2018-08-02T00:03:08.060954: step 18620, loss 0.548165.
Train: 2018-08-02T00:03:08.226511: step 18621, loss 0.562407.
Train: 2018-08-02T00:03:08.394093: step 18622, loss 0.562404.
Train: 2018-08-02T00:03:08.559620: step 18623, loss 0.596519.
Train: 2018-08-02T00:03:08.731190: step 18624, loss 0.579473.
Train: 2018-08-02T00:03:08.898746: step 18625, loss 0.545319.
Train: 2018-08-02T00:03:09.065299: step 18626, loss 0.579493.
Train: 2018-08-02T00:03:09.240800: step 18627, loss 0.545298.
Train: 2018-08-02T00:03:09.421317: step 18628, loss 0.630846.
Train: 2018-08-02T00:03:09.586898: step 18629, loss 0.511082.
Train: 2018-08-02T00:03:09.751458: step 18630, loss 0.511063.
Test: 2018-08-02T00:03:10.283046: step 18630, loss 0.548054.
Train: 2018-08-02T00:03:10.450564: step 18631, loss 0.493878.
Train: 2018-08-02T00:03:10.619115: step 18632, loss 0.51091.
Train: 2018-08-02T00:03:10.796658: step 18633, loss 0.614011.
Train: 2018-08-02T00:03:10.963194: step 18634, loss 0.5624.
Train: 2018-08-02T00:03:11.129751: step 18635, loss 0.752165.
Train: 2018-08-02T00:03:11.305282: step 18636, loss 0.5624.
Train: 2018-08-02T00:03:11.478817: step 18637, loss 0.631217.
Train: 2018-08-02T00:03:11.641383: step 18638, loss 0.510894.
Train: 2018-08-02T00:03:11.810928: step 18639, loss 0.648136.
Train: 2018-08-02T00:03:11.979509: step 18640, loss 0.562399.
Test: 2018-08-02T00:03:12.516044: step 18640, loss 0.548103.
Train: 2018-08-02T00:03:12.681632: step 18641, loss 0.545327.
Train: 2018-08-02T00:03:12.849160: step 18642, loss 0.647641.
Train: 2018-08-02T00:03:13.011719: step 18643, loss 0.511397.
Train: 2018-08-02T00:03:13.180267: step 18644, loss 0.494505.
Train: 2018-08-02T00:03:13.345825: step 18645, loss 0.562412.
Train: 2018-08-02T00:03:13.531357: step 18646, loss 0.562413.
Train: 2018-08-02T00:03:13.696888: step 18647, loss 0.528499.
Train: 2018-08-02T00:03:13.875440: step 18648, loss 0.52849.
Train: 2018-08-02T00:03:14.043958: step 18649, loss 0.528464.
Train: 2018-08-02T00:03:14.219524: step 18650, loss 0.562409.
Test: 2018-08-02T00:03:14.749073: step 18650, loss 0.548161.
Train: 2018-08-02T00:03:14.917651: step 18651, loss 0.528383.
Train: 2018-08-02T00:03:15.082216: step 18652, loss 0.57944.
Train: 2018-08-02T00:03:15.248769: step 18653, loss 0.562402.
Train: 2018-08-02T00:03:15.422276: step 18654, loss 0.545329.
Train: 2018-08-02T00:03:15.585890: step 18655, loss 0.613677.
Train: 2018-08-02T00:03:15.760372: step 18656, loss 0.579498.
Train: 2018-08-02T00:03:15.929918: step 18657, loss 0.5624.
Train: 2018-08-02T00:03:16.094510: step 18658, loss 0.528197.
Train: 2018-08-02T00:03:16.260068: step 18659, loss 0.511066.
Train: 2018-08-02T00:03:16.427613: step 18660, loss 0.511003.
Test: 2018-08-02T00:03:16.957186: step 18660, loss 0.548026.
Train: 2018-08-02T00:03:17.121731: step 18661, loss 0.562398.
Train: 2018-08-02T00:03:17.289329: step 18662, loss 0.654082.
Train: 2018-08-02T00:03:17.465813: step 18663, loss 0.510816.
Train: 2018-08-02T00:03:17.639348: step 18664, loss 0.52798.
Train: 2018-08-02T00:03:17.804906: step 18665, loss 0.5624.
Train: 2018-08-02T00:03:17.977469: step 18666, loss 0.562401.
Train: 2018-08-02T00:03:18.155026: step 18667, loss 0.545136.
Train: 2018-08-02T00:03:18.325513: step 18668, loss 0.666119.
Train: 2018-08-02T00:03:18.494063: step 18669, loss 0.648795.
Train: 2018-08-02T00:03:18.662612: step 18670, loss 0.510649.
Test: 2018-08-02T00:03:19.202170: step 18670, loss 0.547965.
Train: 2018-08-02T00:03:19.380723: step 18671, loss 0.5624.
Train: 2018-08-02T00:03:19.561211: step 18672, loss 0.64853.
Train: 2018-08-02T00:03:19.726768: step 18673, loss 0.562398.
Train: 2018-08-02T00:03:19.892325: step 18674, loss 0.699734.
Train: 2018-08-02T00:03:20.054916: step 18675, loss 0.579508.
Train: 2018-08-02T00:03:20.226447: step 18676, loss 0.647671.
Train: 2018-08-02T00:03:20.394018: step 18677, loss 0.613361.
Train: 2018-08-02T00:03:20.564553: step 18678, loss 0.477873.
Train: 2018-08-02T00:03:20.730110: step 18679, loss 0.562431.
Train: 2018-08-02T00:03:20.893680: step 18680, loss 0.495149.
Test: 2018-08-02T00:03:21.434204: step 18680, loss 0.548375.
Train: 2018-08-02T00:03:21.597791: step 18681, loss 0.663267.
Train: 2018-08-02T00:03:21.764354: step 18682, loss 0.612746.
Train: 2018-08-02T00:03:21.932870: step 18683, loss 0.495609.
Train: 2018-08-02T00:03:22.102448: step 18684, loss 0.54579.
Train: 2018-08-02T00:03:22.266978: step 18685, loss 0.612508.
Train: 2018-08-02T00:03:22.434566: step 18686, loss 0.54585.
Train: 2018-08-02T00:03:22.605104: step 18687, loss 0.529245.
Train: 2018-08-02T00:03:22.768668: step 18688, loss 0.562507.
Train: 2018-08-02T00:03:22.932225: step 18689, loss 0.49603.
Train: 2018-08-02T00:03:23.099752: step 18690, loss 0.612405.
Test: 2018-08-02T00:03:23.631331: step 18690, loss 0.548571.
Train: 2018-08-02T00:03:23.796888: step 18691, loss 0.479321.
Train: 2018-08-02T00:03:23.961448: step 18692, loss 0.645793.
Train: 2018-08-02T00:03:24.128026: step 18693, loss 0.579154.
Train: 2018-08-02T00:03:24.290568: step 18694, loss 0.495839.
Train: 2018-08-02T00:03:24.449176: step 18695, loss 0.562485.
Train: 2018-08-02T00:03:24.614731: step 18696, loss 0.612574.
Train: 2018-08-02T00:03:24.776269: step 18697, loss 0.562477.
Train: 2018-08-02T00:03:24.940861: step 18698, loss 0.562474.
Train: 2018-08-02T00:03:25.103422: step 18699, loss 0.512323.
Train: 2018-08-02T00:03:25.265959: step 18700, loss 0.478786.
Test: 2018-08-02T00:03:25.795544: step 18700, loss 0.548406.
Train: 2018-08-02T00:03:26.594862: step 18701, loss 0.495353.
Train: 2018-08-02T00:03:26.759423: step 18702, loss 0.646577.
Train: 2018-08-02T00:03:26.923982: step 18703, loss 0.511865.
Train: 2018-08-02T00:03:27.096555: step 18704, loss 0.579318.
Train: 2018-08-02T00:03:27.264074: step 18705, loss 0.579343.
Train: 2018-08-02T00:03:27.436643: step 18706, loss 0.562414.
Train: 2018-08-02T00:03:27.608180: step 18707, loss 0.579383.
Train: 2018-08-02T00:03:27.771716: step 18708, loss 0.511436.
Train: 2018-08-02T00:03:27.937299: step 18709, loss 0.528367.
Train: 2018-08-02T00:03:28.099871: step 18710, loss 0.511244.
Test: 2018-08-02T00:03:28.635407: step 18710, loss 0.548083.
Train: 2018-08-02T00:03:28.802960: step 18711, loss 0.562399.
Train: 2018-08-02T00:03:28.968518: step 18712, loss 0.579532.
Train: 2018-08-02T00:03:29.134102: step 18713, loss 0.510901.
Train: 2018-08-02T00:03:29.302625: step 18714, loss 0.700041.
Train: 2018-08-02T00:03:29.467190: step 18715, loss 0.631234.
Train: 2018-08-02T00:03:29.630747: step 18716, loss 0.528006.
Train: 2018-08-02T00:03:29.805280: step 18717, loss 0.562398.
Train: 2018-08-02T00:03:29.978846: step 18718, loss 0.510833.
Train: 2018-08-02T00:03:30.141382: step 18719, loss 0.510806.
Train: 2018-08-02T00:03:30.306971: step 18720, loss 0.562398.
Test: 2018-08-02T00:03:30.829544: step 18720, loss 0.547967.
Train: 2018-08-02T00:03:31.005073: step 18721, loss 0.665807.
Train: 2018-08-02T00:03:31.170661: step 18722, loss 0.596851.
Train: 2018-08-02T00:03:31.337185: step 18723, loss 0.596818.
Train: 2018-08-02T00:03:31.500748: step 18724, loss 0.424897.
Train: 2018-08-02T00:03:31.659324: step 18725, loss 0.527999.
Train: 2018-08-02T00:03:31.828870: step 18726, loss 0.596834.
Train: 2018-08-02T00:03:31.991448: step 18727, loss 0.562399.
Train: 2018-08-02T00:03:32.157024: step 18728, loss 0.665802.
Train: 2018-08-02T00:03:32.330555: step 18729, loss 0.59683.
Train: 2018-08-02T00:03:32.500077: step 18730, loss 0.562397.
Test: 2018-08-02T00:03:33.018691: step 18730, loss 0.548019.
Train: 2018-08-02T00:03:33.184274: step 18731, loss 0.579567.
Train: 2018-08-02T00:03:33.353794: step 18732, loss 0.579543.
Train: 2018-08-02T00:03:33.514366: step 18733, loss 0.579518.
Train: 2018-08-02T00:03:33.676931: step 18734, loss 0.579492.
Train: 2018-08-02T00:03:33.852492: step 18735, loss 0.494138.
Train: 2018-08-02T00:03:34.018018: step 18736, loss 0.579458.
Train: 2018-08-02T00:03:34.182604: step 18737, loss 0.630582.
Train: 2018-08-02T00:03:34.349133: step 18738, loss 0.579423.
Train: 2018-08-02T00:03:34.509704: step 18739, loss 0.59639.
Train: 2018-08-02T00:03:34.678254: step 18740, loss 0.511534.
Test: 2018-08-02T00:03:35.203848: step 18740, loss 0.548227.
Train: 2018-08-02T00:03:35.377385: step 18741, loss 0.528528.
Train: 2018-08-02T00:03:35.539968: step 18742, loss 0.545478.
Train: 2018-08-02T00:03:35.712520: step 18743, loss 0.596289.
Train: 2018-08-02T00:03:35.877048: step 18744, loss 0.460843.
Train: 2018-08-02T00:03:36.040673: step 18745, loss 0.477679.
Train: 2018-08-02T00:03:36.220202: step 18746, loss 0.562408.
Train: 2018-08-02T00:03:36.388718: step 18747, loss 0.698547.
Train: 2018-08-02T00:03:36.558265: step 18748, loss 0.511355.
Train: 2018-08-02T00:03:36.724819: step 18749, loss 0.494293.
Train: 2018-08-02T00:03:36.888383: step 18750, loss 0.545346.
Test: 2018-08-02T00:03:37.425945: step 18750, loss 0.548094.
Train: 2018-08-02T00:03:37.590529: step 18751, loss 0.528234.
Train: 2018-08-02T00:03:37.756094: step 18752, loss 0.493931.
Train: 2018-08-02T00:03:37.924613: step 18753, loss 0.545234.
Train: 2018-08-02T00:03:38.091167: step 18754, loss 0.596817.
Train: 2018-08-02T00:03:38.257752: step 18755, loss 0.5624.
Train: 2018-08-02T00:03:38.421284: step 18756, loss 0.545129.
Train: 2018-08-02T00:03:38.580858: step 18757, loss 0.597011.
Train: 2018-08-02T00:03:38.745417: step 18758, loss 0.545086.
Train: 2018-08-02T00:03:38.917982: step 18759, loss 0.579752.
Train: 2018-08-02T00:03:39.086507: step 18760, loss 0.5277.
Test: 2018-08-02T00:03:39.625067: step 18760, loss 0.547865.
Train: 2018-08-02T00:03:39.792644: step 18761, loss 0.545039.
Train: 2018-08-02T00:03:39.954212: step 18762, loss 0.701598.
Train: 2018-08-02T00:03:40.119760: step 18763, loss 0.458116.
Train: 2018-08-02T00:03:40.286300: step 18764, loss 0.545024.
Train: 2018-08-02T00:03:40.452876: step 18765, loss 0.579829.
Train: 2018-08-02T00:03:40.620406: step 18766, loss 0.510178.
Train: 2018-08-02T00:03:40.782996: step 18767, loss 0.527561.
Train: 2018-08-02T00:03:40.946559: step 18768, loss 0.579887.
Train: 2018-08-02T00:03:41.110122: step 18769, loss 0.667264.
Train: 2018-08-02T00:03:41.272663: step 18770, loss 0.597353.
Test: 2018-08-02T00:03:41.815212: step 18770, loss 0.547822.
Train: 2018-08-02T00:03:41.982764: step 18771, loss 0.544986.
Train: 2018-08-02T00:03:42.147350: step 18772, loss 0.527569.
Train: 2018-08-02T00:03:42.313879: step 18773, loss 0.492734.
Train: 2018-08-02T00:03:42.481431: step 18774, loss 0.562426.
Train: 2018-08-02T00:03:42.644994: step 18775, loss 0.475218.
Train: 2018-08-02T00:03:42.812571: step 18776, loss 0.597373.
Train: 2018-08-02T00:03:42.976109: step 18777, loss 0.492499.
Train: 2018-08-02T00:03:43.137707: step 18778, loss 0.597473.
Train: 2018-08-02T00:03:43.308221: step 18779, loss 0.632574.
Train: 2018-08-02T00:03:43.475798: step 18780, loss 0.509863.
Test: 2018-08-02T00:03:44.013336: step 18780, loss 0.547766.
Train: 2018-08-02T00:03:44.187901: step 18781, loss 0.457225.
Train: 2018-08-02T00:03:44.354449: step 18782, loss 0.492185.
Train: 2018-08-02T00:03:44.522007: step 18783, loss 0.615314.
Train: 2018-08-02T00:03:44.686562: step 18784, loss 0.544847.
Train: 2018-08-02T00:03:44.853122: step 18785, loss 0.527169.
Train: 2018-08-02T00:03:45.019670: step 18786, loss 0.509429.
Train: 2018-08-02T00:03:45.184230: step 18787, loss 0.491605.
Train: 2018-08-02T00:03:45.352779: step 18788, loss 0.651428.
Train: 2018-08-02T00:03:45.515320: step 18789, loss 0.50916.
Train: 2018-08-02T00:03:45.680878: step 18790, loss 0.473435.
Test: 2018-08-02T00:03:46.214451: step 18790, loss 0.547626.
Train: 2018-08-02T00:03:46.378040: step 18791, loss 0.455367.
Train: 2018-08-02T00:03:46.544569: step 18792, loss 0.455031.
Train: 2018-08-02T00:03:46.707134: step 18793, loss 0.724763.
Train: 2018-08-02T00:03:46.873713: step 18794, loss 0.580734.
Train: 2018-08-02T00:03:47.039277: step 18795, loss 0.562708.
Train: 2018-08-02T00:03:47.200847: step 18796, loss 0.526565.
Train: 2018-08-02T00:03:47.364408: step 18797, loss 0.508442.
Train: 2018-08-02T00:03:47.531960: step 18798, loss 0.526505.
Train: 2018-08-02T00:03:47.696520: step 18799, loss 0.508315.
Train: 2018-08-02T00:03:47.859081: step 18800, loss 0.526424.
Test: 2018-08-02T00:03:48.393626: step 18800, loss 0.547569.
Train: 2018-08-02T00:03:49.127553: step 18801, loss 0.562831.
Train: 2018-08-02T00:03:49.292130: step 18802, loss 0.635877.
Train: 2018-08-02T00:03:49.453712: step 18803, loss 0.581124.
Train: 2018-08-02T00:03:49.622231: step 18804, loss 0.599385.
Train: 2018-08-02T00:03:49.791778: step 18805, loss 0.654101.
Train: 2018-08-02T00:03:49.960328: step 18806, loss 0.581038.
Train: 2018-08-02T00:03:50.124888: step 18807, loss 0.599147.
Train: 2018-08-02T00:03:50.292440: step 18808, loss 0.526495.
Train: 2018-08-02T00:03:50.457033: step 18809, loss 0.508445.
Train: 2018-08-02T00:03:50.622557: step 18810, loss 0.580788.
Test: 2018-08-02T00:03:51.156137: step 18810, loss 0.547586.
Train: 2018-08-02T00:03:51.335681: step 18811, loss 0.418339.
Train: 2018-08-02T00:03:51.506194: step 18812, loss 0.508552.
Train: 2018-08-02T00:03:51.668771: step 18813, loss 0.634961.
Train: 2018-08-02T00:03:51.833335: step 18814, loss 0.490479.
Train: 2018-08-02T00:03:51.995885: step 18815, loss 0.598837.
Train: 2018-08-02T00:03:52.156481: step 18816, loss 0.43629.
Train: 2018-08-02T00:03:52.317058: step 18817, loss 0.526559.
Train: 2018-08-02T00:03:52.480590: step 18818, loss 0.526519.
Train: 2018-08-02T00:03:52.658146: step 18819, loss 0.526509.
Train: 2018-08-02T00:03:52.825692: step 18820, loss 0.617324.
Test: 2018-08-02T00:03:53.355276: step 18820, loss 0.547571.
Train: 2018-08-02T00:03:53.530808: step 18821, loss 0.508252.
Train: 2018-08-02T00:03:53.695373: step 18822, loss 0.599196.
Train: 2018-08-02T00:03:53.856910: step 18823, loss 0.489995.
Train: 2018-08-02T00:03:54.021507: step 18824, loss 0.599231.
Train: 2018-08-02T00:03:54.184075: step 18825, loss 0.489769.
Train: 2018-08-02T00:03:54.350616: step 18826, loss 0.599618.
Train: 2018-08-02T00:03:54.515177: step 18827, loss 0.580952.
Train: 2018-08-02T00:03:54.679719: step 18828, loss 0.471422.
Train: 2018-08-02T00:03:54.848285: step 18829, loss 0.581288.
Train: 2018-08-02T00:03:55.025785: step 18830, loss 0.509663.
Test: 2018-08-02T00:03:55.558363: step 18830, loss 0.547578.
Train: 2018-08-02T00:03:55.721952: step 18831, loss 0.526707.
Train: 2018-08-02T00:03:55.888506: step 18832, loss 0.636031.
Train: 2018-08-02T00:03:56.054068: step 18833, loss 0.562947.
Train: 2018-08-02T00:03:56.220617: step 18834, loss 0.69095.
Train: 2018-08-02T00:03:56.391167: step 18835, loss 0.489913.
Train: 2018-08-02T00:03:56.558724: step 18836, loss 0.653852.
Train: 2018-08-02T00:03:56.726271: step 18837, loss 0.453806.
Train: 2018-08-02T00:03:56.899802: step 18838, loss 0.490193.
Train: 2018-08-02T00:03:57.063338: step 18839, loss 0.544625.
Train: 2018-08-02T00:03:57.237899: step 18840, loss 0.544627.
Test: 2018-08-02T00:03:57.769452: step 18840, loss 0.547577.
Train: 2018-08-02T00:03:57.937033: step 18841, loss 0.453945.
Train: 2018-08-02T00:03:58.102562: step 18842, loss 0.508309.
Train: 2018-08-02T00:03:58.288066: step 18843, loss 0.781011.
Train: 2018-08-02T00:03:58.451664: step 18844, loss 0.544623.
Train: 2018-08-02T00:03:58.620178: step 18845, loss 0.653433.
Train: 2018-08-02T00:03:58.788727: step 18846, loss 0.580822.
Train: 2018-08-02T00:03:58.960269: step 18847, loss 0.616831.
Train: 2018-08-02T00:03:59.124828: step 18848, loss 0.58065.
Train: 2018-08-02T00:03:59.299378: step 18849, loss 0.544699.
Train: 2018-08-02T00:03:59.459932: step 18850, loss 0.473201.
Test: 2018-08-02T00:03:59.986554: step 18850, loss 0.547636.
Train: 2018-08-02T00:04:00.155100: step 18851, loss 0.544733.
Train: 2018-08-02T00:04:00.319638: step 18852, loss 0.544744.
Train: 2018-08-02T00:04:00.482224: step 18853, loss 0.526941.
Train: 2018-08-02T00:04:00.646785: step 18854, loss 0.633764.
Train: 2018-08-02T00:04:00.807331: step 18855, loss 0.562546.
Train: 2018-08-02T00:04:00.990839: step 18856, loss 0.544787.
Train: 2018-08-02T00:04:01.153436: step 18857, loss 0.509349.
Train: 2018-08-02T00:04:01.320990: step 18858, loss 0.580237.
Train: 2018-08-02T00:04:01.499511: step 18859, loss 0.615621.
Train: 2018-08-02T00:04:01.668030: step 18860, loss 0.650882.
Test: 2018-08-02T00:04:02.209607: step 18860, loss 0.547724.
Train: 2018-08-02T00:04:02.380151: step 18861, loss 0.527227.
Train: 2018-08-02T00:04:02.549673: step 18862, loss 0.54488.
Train: 2018-08-02T00:04:02.721239: step 18863, loss 0.667871.
Train: 2018-08-02T00:04:02.880788: step 18864, loss 0.579968.
Train: 2018-08-02T00:04:03.050335: step 18865, loss 0.510041.
Train: 2018-08-02T00:04:03.220914: step 18866, loss 0.545.
Train: 2018-08-02T00:04:03.443315: step 18867, loss 0.666843.
Train: 2018-08-02T00:04:03.611834: step 18868, loss 0.545064.
Train: 2018-08-02T00:04:03.774399: step 18869, loss 0.458544.
Train: 2018-08-02T00:04:03.938984: step 18870, loss 0.579707.
Test: 2018-08-02T00:04:04.468543: step 18870, loss 0.547937.
Train: 2018-08-02T00:04:04.632136: step 18871, loss 0.596969.
Train: 2018-08-02T00:04:04.797696: step 18872, loss 0.545149.
Train: 2018-08-02T00:04:04.961257: step 18873, loss 0.476209.
Train: 2018-08-02T00:04:05.124822: step 18874, loss 0.579648.
Train: 2018-08-02T00:04:05.292342: step 18875, loss 0.579647.
Train: 2018-08-02T00:04:05.456932: step 18876, loss 0.59688.
Train: 2018-08-02T00:04:05.623480: step 18877, loss 0.614079.
Train: 2018-08-02T00:04:05.802004: step 18878, loss 0.66561.
Train: 2018-08-02T00:04:05.964586: step 18879, loss 0.49378.
Train: 2018-08-02T00:04:06.132127: step 18880, loss 0.562402.
Test: 2018-08-02T00:04:06.658714: step 18880, loss 0.548077.
Train: 2018-08-02T00:04:06.825243: step 18881, loss 0.528186.
Train: 2018-08-02T00:04:06.988806: step 18882, loss 0.596596.
Train: 2018-08-02T00:04:07.152396: step 18883, loss 0.442861.
Train: 2018-08-02T00:04:07.314934: step 18884, loss 0.630761.
Train: 2018-08-02T00:04:07.482486: step 18885, loss 0.54532.
Train: 2018-08-02T00:04:07.646088: step 18886, loss 0.545321.
Train: 2018-08-02T00:04:07.806663: step 18887, loss 0.562404.
Train: 2018-08-02T00:04:07.976166: step 18888, loss 0.511132.
Train: 2018-08-02T00:04:08.142747: step 18889, loss 0.613721.
Train: 2018-08-02T00:04:08.309307: step 18890, loss 0.647943.
Test: 2018-08-02T00:04:08.837864: step 18890, loss 0.548093.
Train: 2018-08-02T00:04:08.999456: step 18891, loss 0.511134.
Train: 2018-08-02T00:04:09.166017: step 18892, loss 0.647833.
Train: 2018-08-02T00:04:09.331543: step 18893, loss 0.647716.
Train: 2018-08-02T00:04:09.495105: step 18894, loss 0.613472.
Train: 2018-08-02T00:04:09.657672: step 18895, loss 0.52847.
Train: 2018-08-02T00:04:09.824251: step 18896, loss 0.511608.
Train: 2018-08-02T00:04:09.989814: step 18897, loss 0.562424.
Train: 2018-08-02T00:04:10.159342: step 18898, loss 0.596231.
Train: 2018-08-02T00:04:10.329905: step 18899, loss 0.52867.
Train: 2018-08-02T00:04:10.505432: step 18900, loss 0.427476.
Test: 2018-08-02T00:04:11.040973: step 18900, loss 0.548282.
Train: 2018-08-02T00:04:11.822317: step 18901, loss 0.579321.
Train: 2018-08-02T00:04:11.987843: step 18902, loss 0.613158.
Train: 2018-08-02T00:04:12.153402: step 18903, loss 0.663918.
Train: 2018-08-02T00:04:12.318960: step 18904, loss 0.630014.
Train: 2018-08-02T00:04:12.490531: step 18905, loss 0.528706.
Train: 2018-08-02T00:04:12.667063: step 18906, loss 0.47822.
Train: 2018-08-02T00:04:12.842585: step 18907, loss 0.596132.
Train: 2018-08-02T00:04:13.007120: step 18908, loss 0.528755.
Train: 2018-08-02T00:04:13.170708: step 18909, loss 0.629827.
Train: 2018-08-02T00:04:13.335242: step 18910, loss 0.545604.
Test: 2018-08-02T00:04:13.854878: step 18910, loss 0.548347.
Train: 2018-08-02T00:04:14.019444: step 18911, loss 0.629769.
Train: 2018-08-02T00:04:14.186966: step 18912, loss 0.512009.
Train: 2018-08-02T00:04:14.355515: step 18913, loss 0.512023.
Train: 2018-08-02T00:04:14.520074: step 18914, loss 0.52881.
Train: 2018-08-02T00:04:14.698598: step 18915, loss 0.646618.
Train: 2018-08-02T00:04:14.869141: step 18916, loss 0.612938.
Train: 2018-08-02T00:04:15.036724: step 18917, loss 0.6129.
Train: 2018-08-02T00:04:15.201279: step 18918, loss 0.579246.
Train: 2018-08-02T00:04:15.371818: step 18919, loss 0.66308.
Train: 2018-08-02T00:04:15.540347: step 18920, loss 0.478845.
Test: 2018-08-02T00:04:16.072970: step 18920, loss 0.548489.
Train: 2018-08-02T00:04:16.239478: step 18921, loss 0.679427.
Train: 2018-08-02T00:04:16.403066: step 18922, loss 0.529169.
Train: 2018-08-02T00:04:16.568634: step 18923, loss 0.612407.
Train: 2018-08-02T00:04:16.733174: step 18924, loss 0.645504.
Train: 2018-08-02T00:04:16.901709: step 18925, loss 0.579091.
Train: 2018-08-02T00:04:17.066301: step 18926, loss 0.513082.
Train: 2018-08-02T00:04:17.230829: step 18927, loss 0.562586.
Train: 2018-08-02T00:04:17.402400: step 18928, loss 0.595482.
Train: 2018-08-02T00:04:17.571928: step 18929, loss 0.644683.
Train: 2018-08-02T00:04:17.735479: step 18930, loss 0.497158.
Test: 2018-08-02T00:04:18.268065: step 18930, loss 0.548961.
Train: 2018-08-02T00:04:18.439627: step 18931, loss 0.628055.
Train: 2018-08-02T00:04:18.608174: step 18932, loss 0.513716.
Train: 2018-08-02T00:04:18.772707: step 18933, loss 0.595292.
Train: 2018-08-02T00:04:18.949265: step 18934, loss 0.644131.
Train: 2018-08-02T00:04:19.112823: step 18935, loss 0.497701.
Train: 2018-08-02T00:04:19.279352: step 18936, loss 0.627696.
Train: 2018-08-02T00:04:19.444909: step 18937, loss 0.514083.
Train: 2018-08-02T00:04:19.608507: step 18938, loss 0.497891.
Train: 2018-08-02T00:04:19.773058: step 18939, loss 0.627645.
Train: 2018-08-02T00:04:19.935623: step 18940, loss 0.530283.
Test: 2018-08-02T00:04:20.464218: step 18940, loss 0.549133.
Train: 2018-08-02T00:04:20.630770: step 18941, loss 0.578966.
Train: 2018-08-02T00:04:20.793336: step 18942, loss 0.497744.
Train: 2018-08-02T00:04:20.959892: step 18943, loss 0.660331.
Train: 2018-08-02T00:04:21.126447: step 18944, loss 0.432517.
Train: 2018-08-02T00:04:21.288988: step 18945, loss 0.627919.
Train: 2018-08-02T00:04:21.454563: step 18946, loss 0.595325.
Train: 2018-08-02T00:04:21.622120: step 18947, loss 0.595342.
Train: 2018-08-02T00:04:21.791636: step 18948, loss 0.529959.
Train: 2018-08-02T00:04:21.956196: step 18949, loss 0.562644.
Train: 2018-08-02T00:04:22.119758: step 18950, loss 0.513494.
Test: 2018-08-02T00:04:22.654330: step 18950, loss 0.548876.
Train: 2018-08-02T00:04:22.822904: step 18951, loss 0.579026.
Train: 2018-08-02T00:04:22.991459: step 18952, loss 0.595471.
Train: 2018-08-02T00:04:23.155988: step 18953, loss 0.694198.
Train: 2018-08-02T00:04:23.318554: step 18954, loss 0.611908.
Train: 2018-08-02T00:04:23.480122: step 18955, loss 0.431318.
Train: 2018-08-02T00:04:23.648671: step 18956, loss 0.48048.
Train: 2018-08-02T00:04:23.812234: step 18957, loss 0.595508.
Train: 2018-08-02T00:04:23.976795: step 18958, loss 0.595545.
Train: 2018-08-02T00:04:24.142351: step 18959, loss 0.612071.
Train: 2018-08-02T00:04:24.311899: step 18960, loss 0.562564.
Test: 2018-08-02T00:04:24.852454: step 18960, loss 0.548733.
Train: 2018-08-02T00:04:25.015047: step 18961, loss 0.496508.
Train: 2018-08-02T00:04:25.182571: step 18962, loss 0.413717.
Train: 2018-08-02T00:04:25.347132: step 18963, loss 0.527117.
Train: 2018-08-02T00:04:25.524688: step 18964, loss 0.512516.
Train: 2018-08-02T00:04:25.693206: step 18965, loss 0.495548.
Train: 2018-08-02T00:04:25.856769: step 18966, loss 0.579258.
Train: 2018-08-02T00:04:26.024321: step 18967, loss 0.579311.
Train: 2018-08-02T00:04:26.187884: step 18968, loss 0.630183.
Train: 2018-08-02T00:04:26.364443: step 18969, loss 0.596372.
Train: 2018-08-02T00:04:26.532962: step 18970, loss 0.545402.
Test: 2018-08-02T00:04:27.056562: step 18970, loss 0.54814.
Train: 2018-08-02T00:04:27.219128: step 18971, loss 0.579442.
Train: 2018-08-02T00:04:27.388674: step 18972, loss 0.630638.
Train: 2018-08-02T00:04:27.559218: step 18973, loss 0.528277.
Train: 2018-08-02T00:04:27.723779: step 18974, loss 0.477026.
Train: 2018-08-02T00:04:27.885377: step 18975, loss 0.493974.
Train: 2018-08-02T00:04:28.063870: step 18976, loss 0.510947.
Train: 2018-08-02T00:04:28.232419: step 18977, loss 0.510792.
Train: 2018-08-02T00:04:28.404988: step 18978, loss 0.648713.
Train: 2018-08-02T00:04:28.571512: step 18979, loss 0.545113.
Train: 2018-08-02T00:04:28.745073: step 18980, loss 0.545084.
Test: 2018-08-02T00:04:29.286601: step 18980, loss 0.547879.
Train: 2018-08-02T00:04:29.451189: step 18981, loss 0.545056.
Train: 2018-08-02T00:04:29.615721: step 18982, loss 0.527636.
Train: 2018-08-02T00:04:29.780281: step 18983, loss 0.492709.
Train: 2018-08-02T00:04:29.943844: step 18984, loss 0.579918.
Train: 2018-08-02T00:04:30.107405: step 18985, loss 0.56245.
Train: 2018-08-02T00:04:30.275990: step 18986, loss 0.544909.
Train: 2018-08-02T00:04:30.440525: step 18987, loss 0.615222.
Train: 2018-08-02T00:04:30.604123: step 18988, loss 0.544875.
Train: 2018-08-02T00:04:30.769635: step 18989, loss 0.597718.
Train: 2018-08-02T00:04:30.933198: step 18990, loss 0.491986.
Test: 2018-08-02T00:04:31.473767: step 18990, loss 0.547716.
Train: 2018-08-02T00:04:31.646293: step 18991, loss 0.615426.
Train: 2018-08-02T00:04:31.808878: step 18992, loss 0.509544.
Train: 2018-08-02T00:04:31.976409: step 18993, loss 0.456512.
Train: 2018-08-02T00:04:32.139972: step 18994, loss 0.527112.
Train: 2018-08-02T00:04:32.304532: step 18995, loss 0.527052.
Train: 2018-08-02T00:04:32.467126: step 18996, loss 0.633668.
Train: 2018-08-02T00:04:32.637677: step 18997, loss 0.526959.
Train: 2018-08-02T00:04:32.806216: step 18998, loss 0.526925.
Train: 2018-08-02T00:04:32.971780: step 18999, loss 0.616129.
Train: 2018-08-02T00:04:33.135311: step 19000, loss 0.705458.
Test: 2018-08-02T00:04:33.672883: step 19000, loss 0.547641.
Train: 2018-08-02T00:04:34.482375: step 19001, loss 0.633909.
Train: 2018-08-02T00:04:34.644938: step 19002, loss 0.526969.
Train: 2018-08-02T00:04:34.817478: step 19003, loss 0.509253.
Train: 2018-08-02T00:04:34.984032: step 19004, loss 0.562532.
Train: 2018-08-02T00:04:35.146599: step 19005, loss 0.544797.
Train: 2018-08-02T00:04:35.314181: step 19006, loss 0.473948.
Train: 2018-08-02T00:04:35.485722: step 19007, loss 0.580241.
Train: 2018-08-02T00:04:35.661252: step 19008, loss 0.562521.
Train: 2018-08-02T00:04:35.832765: step 19009, loss 0.615675.
Train: 2018-08-02T00:04:35.998357: step 19010, loss 0.580217.
Test: 2018-08-02T00:04:36.521959: step 19010, loss 0.547698.
Train: 2018-08-02T00:04:36.686480: step 19011, loss 0.597872.
Train: 2018-08-02T00:04:36.855055: step 19012, loss 0.544839.
Train: 2018-08-02T00:04:37.018625: step 19013, loss 0.562487.
Train: 2018-08-02T00:04:37.182181: step 19014, loss 0.562479.
Train: 2018-08-02T00:04:37.344752: step 19015, loss 0.527288.
Train: 2018-08-02T00:04:37.510280: step 19016, loss 0.580049.
Train: 2018-08-02T00:04:37.674840: step 19017, loss 0.632721.
Train: 2018-08-02T00:04:37.841393: step 19018, loss 0.597519.
Train: 2018-08-02T00:04:38.021912: step 19019, loss 0.562443.
Train: 2018-08-02T00:04:38.185474: step 19020, loss 0.632274.
Test: 2018-08-02T00:04:38.725057: step 19020, loss 0.547844.
Train: 2018-08-02T00:04:38.892583: step 19021, loss 0.579836.
Train: 2018-08-02T00:04:39.066152: step 19022, loss 0.562416.
Train: 2018-08-02T00:04:39.239656: step 19023, loss 0.579728.
Train: 2018-08-02T00:04:39.414219: step 19024, loss 0.562405.
Train: 2018-08-02T00:04:39.585731: step 19025, loss 0.596869.
Train: 2018-08-02T00:04:39.754306: step 19026, loss 0.493644.
Train: 2018-08-02T00:04:39.919837: step 19027, loss 0.49374.
Train: 2018-08-02T00:04:40.083407: step 19028, loss 0.596718.
Train: 2018-08-02T00:04:40.247961: step 19029, loss 0.510961.
Train: 2018-08-02T00:04:40.410526: step 19030, loss 0.648131.
Test: 2018-08-02T00:04:40.943103: step 19030, loss 0.548058.
Train: 2018-08-02T00:04:41.110655: step 19031, loss 0.476768.
Train: 2018-08-02T00:04:41.277209: step 19032, loss 0.528142.
Train: 2018-08-02T00:04:41.440773: step 19033, loss 0.528121.
Train: 2018-08-02T00:04:41.605332: step 19034, loss 0.665338.
Train: 2018-08-02T00:04:41.770889: step 19035, loss 0.493808.
Train: 2018-08-02T00:04:41.937445: step 19036, loss 0.579556.
Train: 2018-08-02T00:04:42.103033: step 19037, loss 0.596719.
Train: 2018-08-02T00:04:42.271575: step 19038, loss 0.510934.
Train: 2018-08-02T00:04:42.442125: step 19039, loss 0.510911.
Train: 2018-08-02T00:04:42.606686: step 19040, loss 0.613945.
Test: 2018-08-02T00:04:43.132252: step 19040, loss 0.548008.
Train: 2018-08-02T00:04:43.297808: step 19041, loss 0.528026.
Train: 2018-08-02T00:04:43.465386: step 19042, loss 0.596799.
Train: 2018-08-02T00:04:43.630917: step 19043, loss 0.545198.
Train: 2018-08-02T00:04:43.796474: step 19044, loss 0.527982.
Train: 2018-08-02T00:04:43.967019: step 19045, loss 0.49351.
Train: 2018-08-02T00:04:44.130582: step 19046, loss 0.407142.
Train: 2018-08-02T00:04:44.293147: step 19047, loss 0.631659.
Train: 2018-08-02T00:04:44.458735: step 19048, loss 0.614468.
Train: 2018-08-02T00:04:44.630277: step 19049, loss 0.545044.
Train: 2018-08-02T00:04:44.796802: step 19050, loss 0.684199.
Test: 2018-08-02T00:04:45.327383: step 19050, loss 0.547859.
Train: 2018-08-02T00:04:45.488981: step 19051, loss 0.56242.
Train: 2018-08-02T00:04:45.660516: step 19052, loss 0.492894.
Train: 2018-08-02T00:04:45.826080: step 19053, loss 0.492862.
Train: 2018-08-02T00:04:45.992618: step 19054, loss 0.562424.
Train: 2018-08-02T00:04:46.166170: step 19055, loss 0.66702.
Train: 2018-08-02T00:04:46.331728: step 19056, loss 0.562427.
Train: 2018-08-02T00:04:46.502241: step 19057, loss 0.475337.
Train: 2018-08-02T00:04:46.667800: step 19058, loss 0.7193.
Train: 2018-08-02T00:04:46.832383: step 19059, loss 0.562423.
Train: 2018-08-02T00:04:46.999911: step 19060, loss 0.527656.
Test: 2018-08-02T00:04:47.544456: step 19060, loss 0.547873.
Train: 2018-08-02T00:04:47.724997: step 19061, loss 0.527682.
Train: 2018-08-02T00:04:47.890555: step 19062, loss 0.631863.
Train: 2018-08-02T00:04:48.063094: step 19063, loss 0.579752.
Train: 2018-08-02T00:04:48.239598: step 19064, loss 0.562409.
Train: 2018-08-02T00:04:48.405155: step 19065, loss 0.441343.
Train: 2018-08-02T00:04:48.570738: step 19066, loss 0.683525.
Train: 2018-08-02T00:04:48.736296: step 19067, loss 0.510561.
Train: 2018-08-02T00:04:48.902854: step 19068, loss 0.527857.
Train: 2018-08-02T00:04:49.075362: step 19069, loss 0.54513.
Train: 2018-08-02T00:04:49.242931: step 19070, loss 0.63152.
Test: 2018-08-02T00:04:49.775491: step 19070, loss 0.547945.
Train: 2018-08-02T00:04:49.941048: step 19071, loss 0.57967.
Train: 2018-08-02T00:04:50.106637: step 19072, loss 0.596904.
Train: 2018-08-02T00:04:50.269205: step 19073, loss 0.579629.
Train: 2018-08-02T00:04:50.432765: step 19074, loss 0.476379.
Train: 2018-08-02T00:04:50.599288: step 19075, loss 0.5624.
Train: 2018-08-02T00:04:50.777826: step 19076, loss 0.476399.
Train: 2018-08-02T00:04:50.943369: step 19077, loss 0.493527.
Train: 2018-08-02T00:04:51.112915: step 19078, loss 0.51065.
Train: 2018-08-02T00:04:51.275512: step 19079, loss 0.579697.
Train: 2018-08-02T00:04:51.443059: step 19080, loss 0.493115.
Test: 2018-08-02T00:04:51.978602: step 19080, loss 0.547872.
Train: 2018-08-02T00:04:52.147152: step 19081, loss 0.510311.
Train: 2018-08-02T00:04:52.309747: step 19082, loss 0.510166.
Train: 2018-08-02T00:04:52.478266: step 19083, loss 0.544961.
Train: 2018-08-02T00:04:52.644845: step 19084, loss 0.615049.
Train: 2018-08-02T00:04:52.810409: step 19085, loss 0.597601.
Train: 2018-08-02T00:04:52.980921: step 19086, loss 0.615246.
Train: 2018-08-02T00:04:53.146505: step 19087, loss 0.65047.
Train: 2018-08-02T00:04:53.313062: step 19088, loss 0.5273.
Train: 2018-08-02T00:04:53.477593: step 19089, loss 0.492154.
Train: 2018-08-02T00:04:53.658112: step 19090, loss 0.580057.
Test: 2018-08-02T00:04:54.188693: step 19090, loss 0.547742.
Train: 2018-08-02T00:04:54.353254: step 19091, loss 0.509701.
Train: 2018-08-02T00:04:54.519809: step 19092, loss 0.527269.
Train: 2018-08-02T00:04:54.684396: step 19093, loss 0.474373.
Train: 2018-08-02T00:04:54.846934: step 19094, loss 0.686094.
Train: 2018-08-02T00:04:55.009500: step 19095, loss 0.580155.
Train: 2018-08-02T00:04:55.174084: step 19096, loss 0.456561.
Train: 2018-08-02T00:04:55.335627: step 19097, loss 0.509477.
Train: 2018-08-02T00:04:55.502183: step 19098, loss 0.597919.
Train: 2018-08-02T00:04:55.668767: step 19099, loss 0.527083.
Train: 2018-08-02T00:04:55.834301: step 19100, loss 0.509314.
Test: 2018-08-02T00:04:56.359935: step 19100, loss 0.547664.
Train: 2018-08-02T00:04:57.133632: step 19101, loss 0.651374.
Train: 2018-08-02T00:04:57.308141: step 19102, loss 0.509232.
Train: 2018-08-02T00:04:57.472701: step 19103, loss 0.704818.
Train: 2018-08-02T00:04:57.641281: step 19104, loss 0.509254.
Train: 2018-08-02T00:04:57.803844: step 19105, loss 0.598039.
Train: 2018-08-02T00:04:57.967409: step 19106, loss 0.651195.
Train: 2018-08-02T00:04:58.129975: step 19107, loss 0.544814.
Train: 2018-08-02T00:04:58.299491: step 19108, loss 0.58016.
Train: 2018-08-02T00:04:58.469065: step 19109, loss 0.597741.
Train: 2018-08-02T00:04:58.635593: step 19110, loss 0.615235.
Test: 2018-08-02T00:04:59.170163: step 19110, loss 0.547768.
Train: 2018-08-02T00:04:59.347689: step 19111, loss 0.597531.
Train: 2018-08-02T00:04:59.507287: step 19112, loss 0.579924.
Train: 2018-08-02T00:04:59.681795: step 19113, loss 0.579858.
Train: 2018-08-02T00:04:59.852340: step 19114, loss 0.527662.
Train: 2018-08-02T00:05:00.018894: step 19115, loss 0.614422.
Train: 2018-08-02T00:05:00.184452: step 19116, loss 0.648845.
Train: 2018-08-02T00:05:00.356991: step 19117, loss 0.61407.
Train: 2018-08-02T00:05:00.528531: step 19118, loss 0.579551.
Train: 2018-08-02T00:05:00.693118: step 19119, loss 0.750342.
Train: 2018-08-02T00:05:00.873609: step 19120, loss 0.579392.
Test: 2018-08-02T00:05:01.415161: step 19120, loss 0.548288.
Train: 2018-08-02T00:05:01.585706: step 19121, loss 0.596195.
Train: 2018-08-02T00:05:01.754281: step 19122, loss 0.428134.
Train: 2018-08-02T00:05:01.920835: step 19123, loss 0.612679.
Train: 2018-08-02T00:05:02.086368: step 19124, loss 0.529126.
Train: 2018-08-02T00:05:02.249930: step 19125, loss 0.57914.
Train: 2018-08-02T00:05:02.420500: step 19126, loss 0.545922.
Train: 2018-08-02T00:05:02.584038: step 19127, loss 0.645367.
Train: 2018-08-02T00:05:02.750620: step 19128, loss 0.512992.
Train: 2018-08-02T00:05:02.917178: step 19129, loss 0.579062.
Train: 2018-08-02T00:05:03.082732: step 19130, loss 0.546115.
Test: 2018-08-02T00:05:03.622263: step 19130, loss 0.548817.
Train: 2018-08-02T00:05:03.795797: step 19131, loss 0.496793.
Train: 2018-08-02T00:05:03.962384: step 19132, loss 0.579042.
Train: 2018-08-02T00:05:04.127910: step 19133, loss 0.611947.
Train: 2018-08-02T00:05:04.292495: step 19134, loss 0.546153.
Train: 2018-08-02T00:05:04.464034: step 19135, loss 0.431076.
Train: 2018-08-02T00:05:04.633558: step 19136, loss 0.513163.
Train: 2018-08-02T00:05:04.811084: step 19137, loss 0.579072.
Train: 2018-08-02T00:05:04.978662: step 19138, loss 0.529436.
Train: 2018-08-02T00:05:05.143223: step 19139, loss 0.529331.
Train: 2018-08-02T00:05:05.308779: step 19140, loss 0.5625.
Test: 2018-08-02T00:05:05.847313: step 19140, loss 0.548509.
Train: 2018-08-02T00:05:06.022871: step 19141, loss 0.579171.
Train: 2018-08-02T00:05:06.195409: step 19142, loss 0.495566.
Train: 2018-08-02T00:05:06.362961: step 19143, loss 0.596008.
Train: 2018-08-02T00:05:06.528495: step 19144, loss 0.562443.
Train: 2018-08-02T00:05:06.692091: step 19145, loss 0.579287.
Train: 2018-08-02T00:05:06.859608: step 19146, loss 0.663729.
Train: 2018-08-02T00:05:07.025166: step 19147, loss 0.579313.
Train: 2018-08-02T00:05:07.187757: step 19148, loss 0.596201.
Train: 2018-08-02T00:05:07.370243: step 19149, loss 0.528666.
Train: 2018-08-02T00:05:07.542782: step 19150, loss 0.596194.
Test: 2018-08-02T00:05:08.073363: step 19150, loss 0.548294.
Train: 2018-08-02T00:05:08.238947: step 19151, loss 0.562428.
Train: 2018-08-02T00:05:08.408467: step 19152, loss 0.646804.
Train: 2018-08-02T00:05:08.578048: step 19153, loss 0.528728.
Train: 2018-08-02T00:05:08.740610: step 19154, loss 0.579278.
Train: 2018-08-02T00:05:08.908131: step 19155, loss 0.545611.
Train: 2018-08-02T00:05:09.070722: step 19156, loss 0.579263.
Train: 2018-08-02T00:05:09.241272: step 19157, loss 0.629695.
Train: 2018-08-02T00:05:09.411785: step 19158, loss 0.629609.
Train: 2018-08-02T00:05:09.584323: step 19159, loss 0.56246.
Train: 2018-08-02T00:05:09.754868: step 19160, loss 0.66281.
Test: 2018-08-02T00:05:10.299412: step 19160, loss 0.548527.
Train: 2018-08-02T00:05:10.463000: step 19161, loss 0.579161.
Train: 2018-08-02T00:05:10.638506: step 19162, loss 0.529258.
Train: 2018-08-02T00:05:10.801071: step 19163, loss 0.512749.
Train: 2018-08-02T00:05:10.965632: step 19164, loss 0.49623.
Train: 2018-08-02T00:05:11.127230: step 19165, loss 0.562527.
Train: 2018-08-02T00:05:11.299764: step 19166, loss 0.512778.
Train: 2018-08-02T00:05:11.467321: step 19167, loss 0.545917.
Train: 2018-08-02T00:05:11.637835: step 19168, loss 0.612376.
Train: 2018-08-02T00:05:11.799428: step 19169, loss 0.6124.
Train: 2018-08-02T00:05:11.963962: step 19170, loss 0.629025.
Test: 2018-08-02T00:05:12.505515: step 19170, loss 0.5486.
Train: 2018-08-02T00:05:12.673103: step 19171, loss 0.579126.
Train: 2018-08-02T00:05:12.847600: step 19172, loss 0.512729.
Train: 2018-08-02T00:05:13.011191: step 19173, loss 0.59571.
Train: 2018-08-02T00:05:13.180734: step 19174, loss 0.562523.
Train: 2018-08-02T00:05:13.352251: step 19175, loss 0.579108.
Train: 2018-08-02T00:05:13.524790: step 19176, loss 0.545952.
Train: 2018-08-02T00:05:13.704310: step 19177, loss 0.545953.
Train: 2018-08-02T00:05:13.874855: step 19178, loss 0.529363.
Train: 2018-08-02T00:05:14.040439: step 19179, loss 0.579116.
Train: 2018-08-02T00:05:14.206992: step 19180, loss 0.678766.
Test: 2018-08-02T00:05:14.742535: step 19180, loss 0.548629.
Train: 2018-08-02T00:05:14.915073: step 19181, loss 0.579112.
Train: 2018-08-02T00:05:15.081653: step 19182, loss 0.512808.
Train: 2018-08-02T00:05:15.245222: step 19183, loss 0.512813.
Train: 2018-08-02T00:05:15.427703: step 19184, loss 0.512769.
Train: 2018-08-02T00:05:15.598246: step 19185, loss 0.529293.
Train: 2018-08-02T00:05:15.765799: step 19186, loss 0.545857.
Train: 2018-08-02T00:05:15.933352: step 19187, loss 0.712584.
Train: 2018-08-02T00:05:16.104893: step 19188, loss 0.595836.
Train: 2018-08-02T00:05:16.267489: step 19189, loss 0.512498.
Train: 2018-08-02T00:05:16.438999: step 19190, loss 0.579159.
Test: 2018-08-02T00:05:16.979555: step 19190, loss 0.548528.
Train: 2018-08-02T00:05:17.148104: step 19191, loss 0.512474.
Train: 2018-08-02T00:05:17.313678: step 19192, loss 0.529109.
Train: 2018-08-02T00:05:17.478221: step 19193, loss 0.595895.
Train: 2018-08-02T00:05:17.641814: step 19194, loss 0.56247.
Train: 2018-08-02T00:05:17.810334: step 19195, loss 0.62942.
Train: 2018-08-02T00:05:17.975917: step 19196, loss 0.662886.
Train: 2018-08-02T00:05:18.143443: step 19197, loss 0.52905.
Train: 2018-08-02T00:05:18.309000: step 19198, loss 0.562478.
Train: 2018-08-02T00:05:18.472588: step 19199, loss 0.462346.
Train: 2018-08-02T00:05:18.636150: step 19200, loss 0.529064.
Test: 2018-08-02T00:05:19.172691: step 19200, loss 0.548458.
Train: 2018-08-02T00:05:19.951687: step 19201, loss 0.395164.
Train: 2018-08-02T00:05:20.116247: step 19202, loss 0.562448.
Train: 2018-08-02T00:05:20.278781: step 19203, loss 0.629841.
Train: 2018-08-02T00:05:20.445361: step 19204, loss 0.545536.
Train: 2018-08-02T00:05:20.606905: step 19205, loss 0.6132.
Train: 2018-08-02T00:05:20.780466: step 19206, loss 0.579364.
Train: 2018-08-02T00:05:20.947020: step 19207, loss 0.579379.
Train: 2018-08-02T00:05:21.119535: step 19208, loss 0.494488.
Train: 2018-08-02T00:05:21.285118: step 19209, loss 0.460358.
Train: 2018-08-02T00:05:21.449652: step 19210, loss 0.562401.
Test: 2018-08-02T00:05:21.981230: step 19210, loss 0.548077.
Train: 2018-08-02T00:05:22.156787: step 19211, loss 0.528194.
Train: 2018-08-02T00:05:22.320324: step 19212, loss 0.631001.
Train: 2018-08-02T00:05:22.485910: step 19213, loss 0.562398.
Train: 2018-08-02T00:05:22.653460: step 19214, loss 0.579602.
Train: 2018-08-02T00:05:22.818992: step 19215, loss 0.68295.
Train: 2018-08-02T00:05:22.991556: step 19216, loss 0.579608.
Train: 2018-08-02T00:05:23.157087: step 19217, loss 0.510814.
Train: 2018-08-02T00:05:23.321673: step 19218, loss 0.631171.
Train: 2018-08-02T00:05:23.496212: step 19219, loss 0.476519.
Train: 2018-08-02T00:05:23.665753: step 19220, loss 0.562398.
Test: 2018-08-02T00:05:24.204289: step 19220, loss 0.548008.
Train: 2018-08-02T00:05:24.368872: step 19221, loss 0.562398.
Train: 2018-08-02T00:05:24.535427: step 19222, loss 0.596772.
Train: 2018-08-02T00:05:24.701982: step 19223, loss 0.545216.
Train: 2018-08-02T00:05:24.867546: step 19224, loss 0.562398.
Train: 2018-08-02T00:05:25.029083: step 19225, loss 0.648299.
Train: 2018-08-02T00:05:25.197657: step 19226, loss 0.528078.
Train: 2018-08-02T00:05:25.362192: step 19227, loss 0.476654.
Train: 2018-08-02T00:05:25.525813: step 19228, loss 0.528079.
Train: 2018-08-02T00:05:25.688352: step 19229, loss 0.596751.
Train: 2018-08-02T00:05:25.853879: step 19230, loss 0.528029.
Test: 2018-08-02T00:05:26.385500: step 19230, loss 0.547996.
Train: 2018-08-02T00:05:26.555032: step 19231, loss 0.562398.
Train: 2018-08-02T00:05:26.725573: step 19232, loss 0.596823.
Train: 2018-08-02T00:05:26.901080: step 19233, loss 0.527966.
Train: 2018-08-02T00:05:27.065665: step 19234, loss 0.631308.
Train: 2018-08-02T00:05:27.236183: step 19235, loss 0.545178.
Train: 2018-08-02T00:05:27.399777: step 19236, loss 0.476303.
Train: 2018-08-02T00:05:27.564337: step 19237, loss 0.5624.
Train: 2018-08-02T00:05:27.728866: step 19238, loss 0.665924.
Train: 2018-08-02T00:05:27.898414: step 19239, loss 0.510668.
Train: 2018-08-02T00:05:28.064973: step 19240, loss 0.527907.
Test: 2018-08-02T00:05:28.603559: step 19240, loss 0.54795.
Train: 2018-08-02T00:05:28.783047: step 19241, loss 0.476115.
Train: 2018-08-02T00:05:28.951611: step 19242, loss 0.63155.
Train: 2018-08-02T00:05:29.113166: step 19243, loss 0.562405.
Train: 2018-08-02T00:05:29.280745: step 19244, loss 0.57971.
Train: 2018-08-02T00:05:29.447272: step 19245, loss 0.614329.
Train: 2018-08-02T00:05:29.612830: step 19246, loss 0.527809.
Train: 2018-08-02T00:05:29.780408: step 19247, loss 0.458625.
Train: 2018-08-02T00:05:29.956910: step 19248, loss 0.631685.
Train: 2018-08-02T00:05:30.130446: step 19249, loss 0.510439.
Train: 2018-08-02T00:05:30.309966: step 19250, loss 0.56241.
Test: 2018-08-02T00:05:30.844539: step 19250, loss 0.547882.
Train: 2018-08-02T00:05:31.013093: step 19251, loss 0.597114.
Train: 2018-08-02T00:05:31.173657: step 19252, loss 0.527704.
Train: 2018-08-02T00:05:31.339253: step 19253, loss 0.510322.
Train: 2018-08-02T00:05:31.508787: step 19254, loss 0.614571.
Train: 2018-08-02T00:05:31.687314: step 19255, loss 0.579808.
Train: 2018-08-02T00:05:31.861848: step 19256, loss 0.527637.
Train: 2018-08-02T00:05:32.034355: step 19257, loss 0.597216.
Train: 2018-08-02T00:05:32.200937: step 19258, loss 0.475437.
Train: 2018-08-02T00:05:32.364473: step 19259, loss 0.545008.
Train: 2018-08-02T00:05:32.530031: step 19260, loss 0.544993.
Test: 2018-08-02T00:05:33.070617: step 19260, loss 0.547815.
Train: 2018-08-02T00:05:33.238168: step 19261, loss 0.597339.
Train: 2018-08-02T00:05:33.401735: step 19262, loss 0.49258.
Train: 2018-08-02T00:05:33.564303: step 19263, loss 0.527465.
Train: 2018-08-02T00:05:33.727859: step 19264, loss 0.506397.
Train: 2018-08-02T00:05:33.900368: step 19265, loss 0.58001.
Train: 2018-08-02T00:05:34.062972: step 19266, loss 0.650373.
Train: 2018-08-02T00:05:34.228492: step 19267, loss 0.509711.
Train: 2018-08-02T00:05:34.393052: step 19268, loss 0.544872.
Train: 2018-08-02T00:05:34.556645: step 19269, loss 0.597709.
Train: 2018-08-02T00:05:34.728189: step 19270, loss 0.544859.
Test: 2018-08-02T00:05:35.260732: step 19270, loss 0.547721.
Train: 2018-08-02T00:05:35.428309: step 19271, loss 0.544855.
Train: 2018-08-02T00:05:35.597861: step 19272, loss 0.527214.
Train: 2018-08-02T00:05:35.766410: step 19273, loss 0.61544.
Train: 2018-08-02T00:05:35.928945: step 19274, loss 0.544841.
Train: 2018-08-02T00:05:36.095500: step 19275, loss 0.491886.
Train: 2018-08-02T00:05:36.257093: step 19276, loss 0.6155.
Train: 2018-08-02T00:05:36.424651: step 19277, loss 0.509491.
Train: 2018-08-02T00:05:36.590202: step 19278, loss 0.491781.
Train: 2018-08-02T00:05:36.756732: step 19279, loss 0.562512.
Train: 2018-08-02T00:05:36.925281: step 19280, loss 0.580247.
Test: 2018-08-02T00:05:37.462845: step 19280, loss 0.547672.
Train: 2018-08-02T00:05:37.628402: step 19281, loss 0.544788.
Train: 2018-08-02T00:05:37.797973: step 19282, loss 0.562533.
Train: 2018-08-02T00:05:37.967521: step 19283, loss 0.68688.
Train: 2018-08-02T00:05:38.133086: step 19284, loss 0.491558.
Train: 2018-08-02T00:05:38.300630: step 19285, loss 0.52705.
Train: 2018-08-02T00:05:38.466196: step 19286, loss 0.544787.
Train: 2018-08-02T00:05:38.630722: step 19287, loss 0.580275.
Train: 2018-08-02T00:05:38.794284: step 19288, loss 0.580272.
Train: 2018-08-02T00:05:38.960871: step 19289, loss 0.668942.
Train: 2018-08-02T00:05:39.133405: step 19290, loss 0.420885.
Test: 2018-08-02T00:05:39.659972: step 19290, loss 0.547686.
Train: 2018-08-02T00:05:39.828521: step 19291, loss 0.580216.
Train: 2018-08-02T00:05:39.994108: step 19292, loss 0.54481.
Train: 2018-08-02T00:05:40.158637: step 19293, loss 0.597905.
Train: 2018-08-02T00:05:40.324195: step 19294, loss 0.580191.
Train: 2018-08-02T00:05:40.495762: step 19295, loss 0.63318.
Train: 2018-08-02T00:05:40.664317: step 19296, loss 0.580124.
Train: 2018-08-02T00:05:40.828846: step 19297, loss 0.54487.
Train: 2018-08-02T00:05:40.993407: step 19298, loss 0.439437.
Train: 2018-08-02T00:05:41.156995: step 19299, loss 0.439419.
Train: 2018-08-02T00:05:41.334547: step 19300, loss 0.509546.
Test: 2018-08-02T00:05:41.862110: step 19300, loss 0.547687.
Train: 2018-08-02T00:05:42.619215: step 19301, loss 0.526957.
Train: 2018-08-02T00:05:42.787738: step 19302, loss 0.598527.
Train: 2018-08-02T00:05:42.954323: step 19303, loss 0.544433.
Train: 2018-08-02T00:05:43.126831: step 19304, loss 0.707332.
Train: 2018-08-02T00:05:43.304356: step 19305, loss 0.474239.
Train: 2018-08-02T00:05:43.471933: step 19306, loss 0.562872.
Train: 2018-08-02T00:05:43.636468: step 19307, loss 0.544756.
Train: 2018-08-02T00:05:43.804020: step 19308, loss 0.61576.
Train: 2018-08-02T00:05:43.970607: step 19309, loss 0.473897.
Train: 2018-08-02T00:05:44.137130: step 19310, loss 0.527056.
Test: 2018-08-02T00:05:44.659737: step 19310, loss 0.547669.
Train: 2018-08-02T00:05:44.825322: step 19311, loss 0.527032.
Train: 2018-08-02T00:05:44.996832: step 19312, loss 0.527.
Train: 2018-08-02T00:05:45.163387: step 19313, loss 0.562554.
Train: 2018-08-02T00:05:45.330944: step 19314, loss 0.473486.
Train: 2018-08-02T00:05:45.495499: step 19315, loss 0.509027.
Train: 2018-08-02T00:05:45.660084: step 19316, loss 0.491027.
Train: 2018-08-02T00:05:45.821659: step 19317, loss 0.616478.
Train: 2018-08-02T00:05:46.002144: step 19318, loss 0.526697.
Train: 2018-08-02T00:05:46.179669: step 19319, loss 0.508635.
Train: 2018-08-02T00:05:46.345227: step 19320, loss 0.526595.
Test: 2018-08-02T00:05:46.875808: step 19320, loss 0.547581.
Train: 2018-08-02T00:05:47.043362: step 19321, loss 0.52654.
Train: 2018-08-02T00:05:47.205974: step 19322, loss 0.508344.
Train: 2018-08-02T00:05:47.369489: step 19323, loss 0.453669.
Train: 2018-08-02T00:05:47.535047: step 19324, loss 0.63587.
Train: 2018-08-02T00:05:47.699613: step 19325, loss 0.617764.
Train: 2018-08-02T00:05:47.865164: step 19326, loss 0.45304.
Train: 2018-08-02T00:05:48.028726: step 19327, loss 0.489544.
Train: 2018-08-02T00:05:48.192311: step 19328, loss 0.471006.
Train: 2018-08-02T00:05:48.355881: step 19329, loss 0.655304.
Train: 2018-08-02T00:05:48.526396: step 19330, loss 0.544583.
Test: 2018-08-02T00:05:49.050994: step 19330, loss 0.547593.
Train: 2018-08-02T00:05:49.214588: step 19331, loss 0.618612.
Train: 2018-08-02T00:05:49.379117: step 19332, loss 0.507556.
Train: 2018-08-02T00:05:49.552694: step 19333, loss 0.507528.
Train: 2018-08-02T00:05:49.715244: step 19334, loss 0.526036.
Train: 2018-08-02T00:05:49.882795: step 19335, loss 0.581728.
Train: 2018-08-02T00:05:50.049351: step 19336, loss 0.581752.
Train: 2018-08-02T00:05:50.213910: step 19337, loss 0.488832.
Train: 2018-08-02T00:05:50.377477: step 19338, loss 0.451593.
Train: 2018-08-02T00:05:50.546029: step 19339, loss 0.507328.
Train: 2018-08-02T00:05:50.707566: step 19340, loss 0.525927.
Test: 2018-08-02T00:05:51.232163: step 19340, loss 0.547643.
Train: 2018-08-02T00:05:51.401736: step 19341, loss 0.600716.
Train: 2018-08-02T00:05:51.571257: step 19342, loss 0.413528.
Train: 2018-08-02T00:05:51.735842: step 19343, loss 0.563384.
Train: 2018-08-02T00:05:51.908355: step 19344, loss 0.582236.
Train: 2018-08-02T00:05:52.072941: step 19345, loss 0.582289.
Train: 2018-08-02T00:05:52.234484: step 19346, loss 0.582317.
Train: 2018-08-02T00:05:52.400041: step 19347, loss 0.601166.
Train: 2018-08-02T00:05:52.566596: step 19348, loss 0.544628.
Train: 2018-08-02T00:05:52.731156: step 19349, loss 0.619913.
Train: 2018-08-02T00:05:52.897737: step 19350, loss 0.563413.
Test: 2018-08-02T00:05:53.426298: step 19350, loss 0.547663.
Train: 2018-08-02T00:05:53.597840: step 19351, loss 0.600906.
Train: 2018-08-02T00:05:53.760428: step 19352, loss 0.544604.
Train: 2018-08-02T00:05:53.920004: step 19353, loss 0.600663.
Train: 2018-08-02T00:05:54.084552: step 19354, loss 0.507307.
Train: 2018-08-02T00:05:54.259071: step 19355, loss 0.581807.
Train: 2018-08-02T00:05:54.422652: step 19356, loss 0.526016.
Train: 2018-08-02T00:05:54.591215: step 19357, loss 0.507506.
Train: 2018-08-02T00:05:54.751755: step 19358, loss 0.600142.
Train: 2018-08-02T00:05:54.919332: step 19359, loss 0.600051.
Train: 2018-08-02T00:05:55.083873: step 19360, loss 0.544584.
Test: 2018-08-02T00:05:55.607467: step 19360, loss 0.54758.
Train: 2018-08-02T00:05:55.772080: step 19361, loss 0.470927.
Train: 2018-08-02T00:05:55.940577: step 19362, loss 0.489389.
Train: 2018-08-02T00:05:56.104169: step 19363, loss 0.618177.
Train: 2018-08-02T00:05:56.268699: step 19364, loss 0.710012.
Train: 2018-08-02T00:05:56.433259: step 19365, loss 0.507935.
Train: 2018-08-02T00:05:56.605812: step 19366, loss 0.41656.
Train: 2018-08-02T00:05:56.770358: step 19367, loss 0.48974.
Train: 2018-08-02T00:05:56.941899: step 19368, loss 0.709243.
Train: 2018-08-02T00:05:57.107484: step 19369, loss 0.562867.
Train: 2018-08-02T00:05:57.272044: step 19370, loss 0.544606.
Test: 2018-08-02T00:05:57.813569: step 19370, loss 0.547573.
Train: 2018-08-02T00:05:57.980150: step 19371, loss 0.562822.
Train: 2018-08-02T00:05:58.145733: step 19372, loss 0.508247.
Train: 2018-08-02T00:05:58.308269: step 19373, loss 0.508279.
Train: 2018-08-02T00:05:58.473806: step 19374, loss 0.580954.
Train: 2018-08-02T00:05:58.639388: step 19375, loss 0.562779.
Train: 2018-08-02T00:05:58.813896: step 19376, loss 0.653485.
Train: 2018-08-02T00:05:58.994413: step 19377, loss 0.454095.
Train: 2018-08-02T00:05:59.166952: step 19378, loss 0.562737.
Train: 2018-08-02T00:05:59.335501: step 19379, loss 0.653162.
Train: 2018-08-02T00:05:59.502088: step 19380, loss 0.5266.
Test: 2018-08-02T00:06:00.043609: step 19380, loss 0.547592.
Train: 2018-08-02T00:06:00.210163: step 19381, loss 0.508605.
Train: 2018-08-02T00:06:00.377715: step 19382, loss 0.544665.
Train: 2018-08-02T00:06:00.544269: step 19383, loss 0.454645.
Train: 2018-08-02T00:06:00.706835: step 19384, loss 0.544665.
Train: 2018-08-02T00:06:00.874387: step 19385, loss 0.490571.
Train: 2018-08-02T00:06:01.041970: step 19386, loss 0.598816.
Train: 2018-08-02T00:06:01.207527: step 19387, loss 0.562714.
Train: 2018-08-02T00:06:01.379038: step 19388, loss 0.454278.
Train: 2018-08-02T00:06:01.546591: step 19389, loss 0.617045.
Train: 2018-08-02T00:06:01.721123: step 19390, loss 0.562747.
Test: 2018-08-02T00:06:02.256699: step 19390, loss 0.547579.
Train: 2018-08-02T00:06:02.421277: step 19391, loss 0.508395.
Train: 2018-08-02T00:06:02.599774: step 19392, loss 0.453959.
Train: 2018-08-02T00:06:02.764335: step 19393, loss 0.471945.
Train: 2018-08-02T00:06:02.933882: step 19394, loss 0.581044.
Train: 2018-08-02T00:06:03.098475: step 19395, loss 0.654124.
Train: 2018-08-02T00:06:03.270981: step 19396, loss 0.617652.
Train: 2018-08-02T00:06:03.500399: step 19397, loss 0.581113.
Train: 2018-08-02T00:06:03.667000: step 19398, loss 0.581088.
Train: 2018-08-02T00:06:03.831540: step 19399, loss 0.635715.
Train: 2018-08-02T00:06:03.991100: step 19400, loss 0.617352.
Test: 2018-08-02T00:06:04.536660: step 19400, loss 0.547578.
Train: 2018-08-02T00:06:05.320324: step 19401, loss 0.508359.
Train: 2018-08-02T00:06:05.490843: step 19402, loss 0.671337.
Train: 2018-08-02T00:06:05.656400: step 19403, loss 0.688977.
Train: 2018-08-02T00:06:05.827974: step 19404, loss 0.562608.
Train: 2018-08-02T00:06:05.997488: step 19405, loss 0.616237.
Train: 2018-08-02T00:06:06.175039: step 19406, loss 0.579824.
Train: 2018-08-02T00:06:06.336582: step 19407, loss 0.545136.
Train: 2018-08-02T00:06:06.510145: step 19408, loss 0.681212.
Train: 2018-08-02T00:06:06.676672: step 19409, loss 0.47552.
Train: 2018-08-02T00:06:06.842229: step 19410, loss 0.560445.
Test: 2018-08-02T00:06:07.377828: step 19410, loss 0.548456.
Train: 2018-08-02T00:06:07.547372: step 19411, loss 0.609844.
Train: 2018-08-02T00:06:07.710939: step 19412, loss 0.495403.
Train: 2018-08-02T00:06:07.885466: step 19413, loss 0.595796.
Train: 2018-08-02T00:06:08.060981: step 19414, loss 0.548923.
Train: 2018-08-02T00:06:08.223537: step 19415, loss 0.558632.
Train: 2018-08-02T00:06:08.383111: step 19416, loss 0.57988.
Train: 2018-08-02T00:06:08.547706: step 19417, loss 0.569218.
Train: 2018-08-02T00:06:08.712230: step 19418, loss 0.513945.
Train: 2018-08-02T00:06:08.877789: step 19419, loss 0.548517.
Train: 2018-08-02T00:06:09.049330: step 19420, loss 0.612784.
Test: 2018-08-02T00:06:09.588887: step 19420, loss 0.548041.
Train: 2018-08-02T00:06:09.753472: step 19421, loss 0.511183.
Train: 2018-08-02T00:06:09.921030: step 19422, loss 0.52835.
Train: 2018-08-02T00:06:10.087587: step 19423, loss 0.528227.
Train: 2018-08-02T00:06:10.252115: step 19424, loss 0.545339.
Train: 2018-08-02T00:06:10.413683: step 19425, loss 0.527963.
Train: 2018-08-02T00:06:10.581235: step 19426, loss 0.636538.
Train: 2018-08-02T00:06:10.745825: step 19427, loss 0.50976.
Train: 2018-08-02T00:06:10.909385: step 19428, loss 0.564439.
Train: 2018-08-02T00:06:11.074946: step 19429, loss 0.47978.
Train: 2018-08-02T00:06:11.250476: step 19430, loss 0.655555.
Test: 2018-08-02T00:06:11.790026: step 19430, loss 0.548347.
Train: 2018-08-02T00:06:11.954563: step 19431, loss 0.530168.
Train: 2018-08-02T00:06:12.126105: step 19432, loss 0.440743.
Train: 2018-08-02T00:06:12.294679: step 19433, loss 0.546445.
Train: 2018-08-02T00:06:12.460242: step 19434, loss 0.60152.
Train: 2018-08-02T00:06:12.632783: step 19435, loss 0.617521.
Train: 2018-08-02T00:06:12.799330: step 19436, loss 0.70057.
Train: 2018-08-02T00:06:12.969850: step 19437, loss 0.618788.
Train: 2018-08-02T00:06:13.138429: step 19438, loss 0.682464.
Train: 2018-08-02T00:06:13.307975: step 19439, loss 0.675153.
Train: 2018-08-02T00:06:13.478521: step 19440, loss 0.510167.
Test: 2018-08-02T00:06:14.016052: step 19440, loss 0.553809.
Train: 2018-08-02T00:06:14.180638: step 19441, loss 0.656927.
Train: 2018-08-02T00:06:14.343178: step 19442, loss 0.650477.
Train: 2018-08-02T00:06:14.506771: step 19443, loss 0.42447.
Train: 2018-08-02T00:06:14.675290: step 19444, loss 0.596.
Train: 2018-08-02T00:06:14.839849: step 19445, loss 0.560308.
Train: 2018-08-02T00:06:15.012423: step 19446, loss 0.583621.
Train: 2018-08-02T00:06:15.177973: step 19447, loss 0.709233.
Train: 2018-08-02T00:06:15.341541: step 19448, loss 0.499533.
Train: 2018-08-02T00:06:15.516091: step 19449, loss 0.57081.
Train: 2018-08-02T00:06:15.681599: step 19450, loss 0.655702.
Test: 2018-08-02T00:06:16.228138: step 19450, loss 0.554904.
Train: 2018-08-02T00:06:16.393697: step 19451, loss 0.576938.
Train: 2018-08-02T00:06:16.561279: step 19452, loss 0.526237.
Train: 2018-08-02T00:06:16.732789: step 19453, loss 0.544626.
Train: 2018-08-02T00:06:16.896352: step 19454, loss 0.604101.
Train: 2018-08-02T00:06:17.059946: step 19455, loss 0.623305.
Train: 2018-08-02T00:06:17.224475: step 19456, loss 0.533893.
Train: 2018-08-02T00:06:17.388039: step 19457, loss 0.596618.
Train: 2018-08-02T00:06:17.557585: step 19458, loss 0.5961.
Train: 2018-08-02T00:06:17.724151: step 19459, loss 0.503149.
Train: 2018-08-02T00:06:17.889728: step 19460, loss 0.617783.
Test: 2018-08-02T00:06:18.427275: step 19460, loss 0.550443.
Train: 2018-08-02T00:06:18.593846: step 19461, loss 0.582333.
Train: 2018-08-02T00:06:18.766384: step 19462, loss 0.601732.
Train: 2018-08-02T00:06:18.943911: step 19463, loss 0.598764.
Train: 2018-08-02T00:06:19.112429: step 19464, loss 0.482283.
Train: 2018-08-02T00:06:19.279008: step 19465, loss 0.599903.
Train: 2018-08-02T00:06:19.451546: step 19466, loss 0.615814.
Train: 2018-08-02T00:06:19.615116: step 19467, loss 0.495821.
Train: 2018-08-02T00:06:19.780642: step 19468, loss 0.51384.
Train: 2018-08-02T00:06:19.953212: step 19469, loss 0.581254.
Train: 2018-08-02T00:06:20.119736: step 19470, loss 0.530534.
Test: 2018-08-02T00:06:20.651314: step 19470, loss 0.550544.
Train: 2018-08-02T00:06:20.813880: step 19471, loss 0.598101.
Train: 2018-08-02T00:06:20.980435: step 19472, loss 0.563256.
Train: 2018-08-02T00:06:21.149010: step 19473, loss 0.563998.
Train: 2018-08-02T00:06:21.316537: step 19474, loss 0.654316.
Train: 2018-08-02T00:06:21.479102: step 19475, loss 0.658386.
Train: 2018-08-02T00:06:21.642665: step 19476, loss 0.561908.
Train: 2018-08-02T00:06:21.805273: step 19477, loss 0.600132.
Train: 2018-08-02T00:06:21.970787: step 19478, loss 0.559207.
Train: 2018-08-02T00:06:22.137343: step 19479, loss 0.668202.
Train: 2018-08-02T00:06:22.305916: step 19480, loss 0.494281.
Test: 2018-08-02T00:06:22.837487: step 19480, loss 0.549883.
Train: 2018-08-02T00:06:23.002030: step 19481, loss 0.672115.
Train: 2018-08-02T00:06:23.170633: step 19482, loss 0.623296.
Train: 2018-08-02T00:06:23.344147: step 19483, loss 0.488572.
Train: 2018-08-02T00:06:23.508702: step 19484, loss 0.652255.
Train: 2018-08-02T00:06:23.673236: step 19485, loss 0.669917.
Train: 2018-08-02T00:06:23.837831: step 19486, loss 0.561958.
Train: 2018-08-02T00:06:24.003380: step 19487, loss 0.528672.
Train: 2018-08-02T00:06:24.169941: step 19488, loss 0.594334.
Train: 2018-08-02T00:06:24.334468: step 19489, loss 0.53136.
Train: 2018-08-02T00:06:24.499054: step 19490, loss 0.54629.
Test: 2018-08-02T00:06:25.026645: step 19490, loss 0.551297.
Train: 2018-08-02T00:06:25.190212: step 19491, loss 0.547827.
Train: 2018-08-02T00:06:25.356762: step 19492, loss 0.55221.
Train: 2018-08-02T00:06:25.523290: step 19493, loss 0.501487.
Train: 2018-08-02T00:06:25.686854: step 19494, loss 0.550074.
Train: 2018-08-02T00:06:25.857398: step 19495, loss 0.599588.
Train: 2018-08-02T00:06:26.020985: step 19496, loss 0.629276.
Train: 2018-08-02T00:06:26.188544: step 19497, loss 0.468166.
Train: 2018-08-02T00:06:26.359057: step 19498, loss 0.614586.
Train: 2018-08-02T00:06:26.530599: step 19499, loss 0.580607.
Train: 2018-08-02T00:06:26.691169: step 19500, loss 0.563894.
Test: 2018-08-02T00:06:27.216764: step 19500, loss 0.55097.
Train: 2018-08-02T00:06:28.001844: step 19501, loss 0.515659.
Train: 2018-08-02T00:06:28.167396: step 19502, loss 0.597285.
Train: 2018-08-02T00:06:28.328939: step 19503, loss 0.533612.
Train: 2018-08-02T00:06:28.496492: step 19504, loss 0.500441.
Train: 2018-08-02T00:06:28.669030: step 19505, loss 0.564035.
Train: 2018-08-02T00:06:28.846573: step 19506, loss 0.63073.
Train: 2018-08-02T00:06:29.012145: step 19507, loss 0.613569.
Train: 2018-08-02T00:06:29.177697: step 19508, loss 0.598011.
Train: 2018-08-02T00:06:29.346247: step 19509, loss 0.58063.
Train: 2018-08-02T00:06:29.515767: step 19510, loss 0.630485.
Test: 2018-08-02T00:06:30.046348: step 19510, loss 0.55059.
Train: 2018-08-02T00:06:30.215921: step 19511, loss 0.58072.
Train: 2018-08-02T00:06:30.382476: step 19512, loss 0.613891.
Train: 2018-08-02T00:06:30.550999: step 19513, loss 0.580898.
Train: 2018-08-02T00:06:30.721544: step 19514, loss 0.564595.
Train: 2018-08-02T00:06:30.885106: step 19515, loss 0.579593.
Train: 2018-08-02T00:06:31.054679: step 19516, loss 0.581101.
Train: 2018-08-02T00:06:31.221208: step 19517, loss 0.548596.
Train: 2018-08-02T00:06:31.387763: step 19518, loss 0.548338.
Train: 2018-08-02T00:06:31.558348: step 19519, loss 0.629766.
Train: 2018-08-02T00:06:31.723897: step 19520, loss 0.498928.
Test: 2018-08-02T00:06:32.261460: step 19520, loss 0.55055.
Train: 2018-08-02T00:06:32.431005: step 19521, loss 0.613365.
Train: 2018-08-02T00:06:32.598526: step 19522, loss 0.61317.
Train: 2018-08-02T00:06:32.768100: step 19523, loss 0.547832.
Train: 2018-08-02T00:06:32.935624: step 19524, loss 0.678756.
Train: 2018-08-02T00:06:33.104175: step 19525, loss 0.548041.
Train: 2018-08-02T00:06:33.270759: step 19526, loss 0.531883.
Train: 2018-08-02T00:06:33.441304: step 19527, loss 0.564175.
Train: 2018-08-02T00:06:33.603869: step 19528, loss 0.515333.
Train: 2018-08-02T00:06:33.766415: step 19529, loss 0.515059.
Train: 2018-08-02T00:06:33.930964: step 19530, loss 0.531243.
Test: 2018-08-02T00:06:34.467530: step 19530, loss 0.550373.
Train: 2018-08-02T00:06:34.684948: step 19531, loss 0.498659.
Train: 2018-08-02T00:06:34.848542: step 19532, loss 0.548836.
Train: 2018-08-02T00:06:35.013102: step 19533, loss 0.579593.
Train: 2018-08-02T00:06:35.178655: step 19534, loss 0.447985.
Train: 2018-08-02T00:06:35.342228: step 19535, loss 0.528662.
Train: 2018-08-02T00:06:35.512736: step 19536, loss 0.630493.
Train: 2018-08-02T00:06:35.686271: step 19537, loss 0.578864.
Train: 2018-08-02T00:06:35.847840: step 19538, loss 0.544451.
Train: 2018-08-02T00:06:36.015418: step 19539, loss 0.579693.
Train: 2018-08-02T00:06:36.179952: step 19540, loss 0.58164.
Test: 2018-08-02T00:06:36.711541: step 19540, loss 0.549458.
Train: 2018-08-02T00:06:36.877121: step 19541, loss 0.562037.
Train: 2018-08-02T00:06:37.045637: step 19542, loss 0.555643.
Train: 2018-08-02T00:06:37.210228: step 19543, loss 0.581579.
Train: 2018-08-02T00:06:37.373792: step 19544, loss 0.569696.
Train: 2018-08-02T00:06:37.547327: step 19545, loss 0.560521.
Train: 2018-08-02T00:06:37.713859: step 19546, loss 0.605746.
Train: 2018-08-02T00:06:37.879408: step 19547, loss 0.637252.
Train: 2018-08-02T00:06:38.045964: step 19548, loss 0.599177.
Train: 2018-08-02T00:06:38.216540: step 19549, loss 0.647953.
Train: 2018-08-02T00:06:38.383093: step 19550, loss 0.613675.
Test: 2018-08-02T00:06:38.906664: step 19550, loss 0.549991.
Train: 2018-08-02T00:06:39.072219: step 19551, loss 0.480147.
Train: 2018-08-02T00:06:39.235783: step 19552, loss 0.463788.
Train: 2018-08-02T00:06:39.401340: step 19553, loss 0.665304.
Train: 2018-08-02T00:06:39.567895: step 19554, loss 0.613497.
Train: 2018-08-02T00:06:39.733452: step 19555, loss 0.54771.
Train: 2018-08-02T00:06:39.909014: step 19556, loss 0.563935.
Train: 2018-08-02T00:06:40.077533: step 19557, loss 0.663832.
Train: 2018-08-02T00:06:40.248076: step 19558, loss 0.49661.
Train: 2018-08-02T00:06:40.417655: step 19559, loss 0.547423.
Train: 2018-08-02T00:06:40.591188: step 19560, loss 0.53119.
Test: 2018-08-02T00:06:41.115758: step 19560, loss 0.551418.
Train: 2018-08-02T00:06:41.286302: step 19561, loss 0.5613.
Train: 2018-08-02T00:06:41.463827: step 19562, loss 0.599135.
Train: 2018-08-02T00:06:41.634370: step 19563, loss 0.522526.
Train: 2018-08-02T00:06:41.799929: step 19564, loss 0.567828.
Train: 2018-08-02T00:06:41.962494: step 19565, loss 0.52953.
Train: 2018-08-02T00:06:42.126084: step 19566, loss 0.481897.
Train: 2018-08-02T00:06:42.288622: step 19567, loss 0.664113.
Train: 2018-08-02T00:06:42.456174: step 19568, loss 0.547541.
Train: 2018-08-02T00:06:42.627714: step 19569, loss 0.530819.
Train: 2018-08-02T00:06:42.798259: step 19570, loss 0.513851.
Test: 2018-08-02T00:06:43.341806: step 19570, loss 0.550224.
Train: 2018-08-02T00:06:43.511354: step 19571, loss 0.581215.
Train: 2018-08-02T00:06:43.681896: step 19572, loss 0.513518.
Train: 2018-08-02T00:06:43.850447: step 19573, loss 0.564528.
Train: 2018-08-02T00:06:44.012045: step 19574, loss 0.546677.
Train: 2018-08-02T00:06:44.174579: step 19575, loss 0.63301.
Train: 2018-08-02T00:06:44.337170: step 19576, loss 0.634709.
Train: 2018-08-02T00:06:44.513674: step 19577, loss 0.598498.
Train: 2018-08-02T00:06:44.679232: step 19578, loss 0.683738.
Train: 2018-08-02T00:06:44.846811: step 19579, loss 0.460947.
Train: 2018-08-02T00:06:45.010376: step 19580, loss 0.58128.
Test: 2018-08-02T00:06:45.548907: step 19580, loss 0.549926.
Train: 2018-08-02T00:06:45.721444: step 19581, loss 0.563498.
Train: 2018-08-02T00:06:45.889030: step 19582, loss 0.545722.
Train: 2018-08-02T00:06:46.053556: step 19583, loss 0.599638.
Train: 2018-08-02T00:06:46.226096: step 19584, loss 0.683132.
Train: 2018-08-02T00:06:46.389658: step 19585, loss 0.563522.
Train: 2018-08-02T00:06:46.559239: step 19586, loss 0.566463.
Train: 2018-08-02T00:06:46.730771: step 19587, loss 0.48106.
Train: 2018-08-02T00:06:46.896330: step 19588, loss 0.530398.
Train: 2018-08-02T00:06:47.059893: step 19589, loss 0.581314.
Train: 2018-08-02T00:06:47.226421: step 19590, loss 0.699667.
Test: 2018-08-02T00:06:47.753046: step 19590, loss 0.549935.
Train: 2018-08-02T00:06:47.929568: step 19591, loss 0.512945.
Train: 2018-08-02T00:06:48.094103: step 19592, loss 0.479269.
Train: 2018-08-02T00:06:48.258663: step 19593, loss 0.497012.
Train: 2018-08-02T00:06:48.426214: step 19594, loss 0.446571.
Train: 2018-08-02T00:06:48.589777: step 19595, loss 0.530501.
Train: 2018-08-02T00:06:48.755334: step 19596, loss 0.598074.
Train: 2018-08-02T00:06:48.936850: step 19597, loss 0.716036.
Train: 2018-08-02T00:06:49.102407: step 19598, loss 0.478923.
Train: 2018-08-02T00:06:49.266968: step 19599, loss 0.615081.
Train: 2018-08-02T00:06:49.430530: step 19600, loss 0.547245.
Test: 2018-08-02T00:06:49.962151: step 19600, loss 0.549511.
Train: 2018-08-02T00:06:50.704476: step 19601, loss 0.512719.
Train: 2018-08-02T00:06:50.879009: step 19602, loss 0.546706.
Train: 2018-08-02T00:06:51.055537: step 19603, loss 0.615039.
Train: 2018-08-02T00:06:51.233094: step 19604, loss 0.529394.
Train: 2018-08-02T00:06:51.396626: step 19605, loss 0.529553.
Train: 2018-08-02T00:06:51.569164: step 19606, loss 0.580726.
Train: 2018-08-02T00:06:51.737731: step 19607, loss 0.580795.
Train: 2018-08-02T00:06:51.913244: step 19608, loss 0.528829.
Train: 2018-08-02T00:06:52.081795: step 19609, loss 0.649615.
Train: 2018-08-02T00:06:52.252362: step 19610, loss 0.512292.
Test: 2018-08-02T00:06:52.785911: step 19610, loss 0.549181.
Train: 2018-08-02T00:06:52.950472: step 19611, loss 0.666477.
Train: 2018-08-02T00:06:53.127997: step 19612, loss 0.546153.
Train: 2018-08-02T00:06:53.293568: step 19613, loss 0.460792.
Train: 2018-08-02T00:06:53.461139: step 19614, loss 0.563967.
Train: 2018-08-02T00:06:53.627667: step 19615, loss 0.580882.
Train: 2018-08-02T00:06:53.793253: step 19616, loss 0.666473.
Train: 2018-08-02T00:06:53.956808: step 19617, loss 0.529121.
Train: 2018-08-02T00:06:54.123362: step 19618, loss 0.563122.
Train: 2018-08-02T00:06:54.290919: step 19619, loss 0.511823.
Train: 2018-08-02T00:06:54.455449: step 19620, loss 0.5292.
Test: 2018-08-02T00:06:54.981044: step 19620, loss 0.549022.
Train: 2018-08-02T00:06:55.145640: step 19621, loss 0.563808.
Train: 2018-08-02T00:06:55.310188: step 19622, loss 0.56345.
Train: 2018-08-02T00:06:55.484731: step 19623, loss 0.597885.
Train: 2018-08-02T00:06:55.665216: step 19624, loss 0.597754.
Train: 2018-08-02T00:06:55.833765: step 19625, loss 0.597582.
Train: 2018-08-02T00:06:56.003311: step 19626, loss 0.580678.
Train: 2018-08-02T00:06:56.173855: step 19627, loss 0.562894.
Train: 2018-08-02T00:06:56.338463: step 19628, loss 0.477915.
Train: 2018-08-02T00:06:56.502011: step 19629, loss 0.529557.
Train: 2018-08-02T00:06:56.668557: step 19630, loss 0.666679.
Test: 2018-08-02T00:06:57.214074: step 19630, loss 0.548872.
Train: 2018-08-02T00:06:57.393594: step 19631, loss 0.494713.
Train: 2018-08-02T00:06:57.556184: step 19632, loss 0.579449.
Train: 2018-08-02T00:06:57.735679: step 19633, loss 0.615741.
Train: 2018-08-02T00:06:57.903232: step 19634, loss 0.456764.
Train: 2018-08-02T00:06:58.069786: step 19635, loss 0.636666.
Train: 2018-08-02T00:06:58.235370: step 19636, loss 0.619311.
Train: 2018-08-02T00:06:58.401923: step 19637, loss 0.548413.
Train: 2018-08-02T00:06:58.565462: step 19638, loss 0.525508.
Train: 2018-08-02T00:06:58.731019: step 19639, loss 0.651564.
Train: 2018-08-02T00:06:58.895579: step 19640, loss 0.582206.
Test: 2018-08-02T00:06:59.438155: step 19640, loss 0.548999.
Train: 2018-08-02T00:06:59.608673: step 19641, loss 0.51236.
Train: 2018-08-02T00:06:59.775258: step 19642, loss 0.614456.
Train: 2018-08-02T00:06:59.947766: step 19643, loss 0.495325.
Train: 2018-08-02T00:07:00.118311: step 19644, loss 0.562849.
Train: 2018-08-02T00:07:00.292843: step 19645, loss 0.630835.
Train: 2018-08-02T00:07:00.461427: step 19646, loss 0.562853.
Train: 2018-08-02T00:07:00.627973: step 19647, loss 0.547422.
Train: 2018-08-02T00:07:00.805500: step 19648, loss 0.546171.
Train: 2018-08-02T00:07:00.973056: step 19649, loss 0.596225.
Train: 2018-08-02T00:07:01.140578: step 19650, loss 0.530021.
Test: 2018-08-02T00:07:01.670163: step 19650, loss 0.549169.
Train: 2018-08-02T00:07:01.847687: step 19651, loss 0.510861.
Train: 2018-08-02T00:07:02.016265: step 19652, loss 0.647221.
Train: 2018-08-02T00:07:02.182792: step 19653, loss 0.5302.
Train: 2018-08-02T00:07:02.355330: step 19654, loss 0.576972.
Train: 2018-08-02T00:07:02.518921: step 19655, loss 0.561162.
Train: 2018-08-02T00:07:02.684451: step 19656, loss 0.667505.
Train: 2018-08-02T00:07:02.850042: step 19657, loss 0.471938.
Train: 2018-08-02T00:07:03.011602: step 19658, loss 0.601644.
Train: 2018-08-02T00:07:03.176165: step 19659, loss 0.613001.
Train: 2018-08-02T00:07:03.342723: step 19660, loss 0.540625.
Test: 2018-08-02T00:07:03.868286: step 19660, loss 0.54859.
Train: 2018-08-02T00:07:04.034840: step 19661, loss 0.485308.
Train: 2018-08-02T00:07:04.199426: step 19662, loss 0.536291.
Train: 2018-08-02T00:07:04.363961: step 19663, loss 0.63497.
Train: 2018-08-02T00:07:04.528522: step 19664, loss 0.57171.
Train: 2018-08-02T00:07:04.695076: step 19665, loss 0.601216.
Train: 2018-08-02T00:07:04.860633: step 19666, loss 0.650637.
Train: 2018-08-02T00:07:05.026197: step 19667, loss 0.545075.
Train: 2018-08-02T00:07:05.193742: step 19668, loss 0.440389.
Train: 2018-08-02T00:07:05.361295: step 19669, loss 0.562345.
Train: 2018-08-02T00:07:05.528871: step 19670, loss 0.577342.
Test: 2018-08-02T00:07:06.058431: step 19670, loss 0.549163.
Train: 2018-08-02T00:07:06.227978: step 19671, loss 0.514735.
Train: 2018-08-02T00:07:06.396555: step 19672, loss 0.598973.
Train: 2018-08-02T00:07:06.566105: step 19673, loss 0.564713.
Train: 2018-08-02T00:07:06.730635: step 19674, loss 0.563414.
Train: 2018-08-02T00:07:06.894226: step 19675, loss 0.595477.
Train: 2018-08-02T00:07:07.056788: step 19676, loss 0.696117.
Train: 2018-08-02T00:07:07.221356: step 19677, loss 0.612672.
Train: 2018-08-02T00:07:07.389898: step 19678, loss 0.54845.
Train: 2018-08-02T00:07:07.554432: step 19679, loss 0.661789.
Train: 2018-08-02T00:07:07.725008: step 19680, loss 0.561154.
Test: 2018-08-02T00:07:08.267526: step 19680, loss 0.550119.
Train: 2018-08-02T00:07:08.436075: step 19681, loss 0.64441.
Train: 2018-08-02T00:07:08.601632: step 19682, loss 0.565561.
Train: 2018-08-02T00:07:08.772176: step 19683, loss 0.562442.
Train: 2018-08-02T00:07:08.938765: step 19684, loss 0.512464.
Train: 2018-08-02T00:07:09.116264: step 19685, loss 0.49814.
Train: 2018-08-02T00:07:09.282856: step 19686, loss 0.495153.
Train: 2018-08-02T00:07:09.464326: step 19687, loss 0.605078.
Train: 2018-08-02T00:07:09.634896: step 19688, loss 0.526478.
Train: 2018-08-02T00:07:09.801425: step 19689, loss 0.633536.
Train: 2018-08-02T00:07:09.965012: step 19690, loss 0.495377.
Test: 2018-08-02T00:07:10.503562: step 19690, loss 0.550092.
Train: 2018-08-02T00:07:10.671101: step 19691, loss 0.60628.
Train: 2018-08-02T00:07:10.834689: step 19692, loss 0.538391.
Train: 2018-08-02T00:07:11.002215: step 19693, loss 0.61027.
Train: 2018-08-02T00:07:11.167772: step 19694, loss 0.644745.
Train: 2018-08-02T00:07:11.345297: step 19695, loss 0.516574.
Train: 2018-08-02T00:07:11.513876: step 19696, loss 0.563047.
Train: 2018-08-02T00:07:11.676413: step 19697, loss 0.580052.
Train: 2018-08-02T00:07:11.842993: step 19698, loss 0.645762.
Train: 2018-08-02T00:07:12.010520: step 19699, loss 0.612468.
Train: 2018-08-02T00:07:12.173113: step 19700, loss 0.530171.
Test: 2018-08-02T00:07:12.702703: step 19700, loss 0.54982.
Train: 2018-08-02T00:07:13.486476: step 19701, loss 0.529543.
Train: 2018-08-02T00:07:13.654054: step 19702, loss 0.563316.
Train: 2018-08-02T00:07:13.819587: step 19703, loss 0.631572.
Train: 2018-08-02T00:07:13.983148: step 19704, loss 0.482274.
Train: 2018-08-02T00:07:14.145744: step 19705, loss 0.594468.
Train: 2018-08-02T00:07:14.314264: step 19706, loss 0.527121.
Train: 2018-08-02T00:07:14.479821: step 19707, loss 0.465389.
Train: 2018-08-02T00:07:14.647398: step 19708, loss 0.54861.
Train: 2018-08-02T00:07:14.813959: step 19709, loss 0.545869.
Train: 2018-08-02T00:07:14.983474: step 19710, loss 0.481123.
Test: 2018-08-02T00:07:15.507075: step 19710, loss 0.549289.
Train: 2018-08-02T00:07:15.686594: step 19711, loss 0.546279.
Train: 2018-08-02T00:07:15.857138: step 19712, loss 0.483301.
Train: 2018-08-02T00:07:16.021699: step 19713, loss 0.665141.
Train: 2018-08-02T00:07:16.186259: step 19714, loss 0.494394.
Train: 2018-08-02T00:07:16.349835: step 19715, loss 0.511382.
Train: 2018-08-02T00:07:16.515378: step 19716, loss 0.512898.
Train: 2018-08-02T00:07:16.684957: step 19717, loss 0.492889.
Train: 2018-08-02T00:07:16.854473: step 19718, loss 0.617448.
Train: 2018-08-02T00:07:17.018036: step 19719, loss 0.509838.
Train: 2018-08-02T00:07:17.185618: step 19720, loss 0.705826.
Test: 2018-08-02T00:07:17.717166: step 19720, loss 0.548867.
Train: 2018-08-02T00:07:17.885728: step 19721, loss 0.510931.
Train: 2018-08-02T00:07:18.051273: step 19722, loss 0.545994.
Train: 2018-08-02T00:07:18.215834: step 19723, loss 0.511821.
Train: 2018-08-02T00:07:18.381434: step 19724, loss 0.422875.
Train: 2018-08-02T00:07:18.551966: step 19725, loss 0.493535.
Train: 2018-08-02T00:07:18.719488: step 19726, loss 0.527925.
Train: 2018-08-02T00:07:18.888036: step 19727, loss 0.545467.
Train: 2018-08-02T00:07:19.056622: step 19728, loss 0.545095.
Train: 2018-08-02T00:07:19.219151: step 19729, loss 0.492219.
Train: 2018-08-02T00:07:19.391719: step 19730, loss 0.599491.
Test: 2018-08-02T00:07:19.919305: step 19730, loss 0.548553.
Train: 2018-08-02T00:07:20.083840: step 19731, loss 0.581146.
Train: 2018-08-02T00:07:20.251423: step 19732, loss 0.545554.
Train: 2018-08-02T00:07:20.415982: step 19733, loss 0.49241.
Train: 2018-08-02T00:07:20.590510: step 19734, loss 0.492439.
Train: 2018-08-02T00:07:20.753052: step 19735, loss 0.491214.
Train: 2018-08-02T00:07:20.918639: step 19736, loss 0.436859.
Train: 2018-08-02T00:07:21.081199: step 19737, loss 0.490704.
Train: 2018-08-02T00:07:21.246731: step 19738, loss 0.691792.
Train: 2018-08-02T00:07:21.411325: step 19739, loss 0.63725.
Train: 2018-08-02T00:07:21.583855: step 19740, loss 0.60042.
Test: 2018-08-02T00:07:22.111450: step 19740, loss 0.548468.
Train: 2018-08-02T00:07:22.281964: step 19741, loss 0.58177.
Train: 2018-08-02T00:07:22.448549: step 19742, loss 0.45341.
Train: 2018-08-02T00:07:22.615101: step 19743, loss 0.563479.
Train: 2018-08-02T00:07:22.781659: step 19744, loss 0.619229.
Train: 2018-08-02T00:07:22.955194: step 19745, loss 0.544926.
Train: 2018-08-02T00:07:23.120722: step 19746, loss 0.489809.
Train: 2018-08-02T00:07:23.289302: step 19747, loss 0.582801.
Train: 2018-08-02T00:07:23.452834: step 19748, loss 0.582074.
Train: 2018-08-02T00:07:23.617393: step 19749, loss 0.545112.
Train: 2018-08-02T00:07:23.791927: step 19750, loss 0.619074.
Test: 2018-08-02T00:07:24.328493: step 19750, loss 0.548428.
Train: 2018-08-02T00:07:24.497042: step 19751, loss 0.655627.
Train: 2018-08-02T00:07:24.661603: step 19752, loss 0.508985.
Train: 2018-08-02T00:07:24.834167: step 19753, loss 0.527061.
Train: 2018-08-02T00:07:25.006681: step 19754, loss 0.69185.
Train: 2018-08-02T00:07:25.170243: step 19755, loss 0.654924.
Train: 2018-08-02T00:07:25.331811: step 19756, loss 0.56416.
Train: 2018-08-02T00:07:25.496370: step 19757, loss 0.545738.
Train: 2018-08-02T00:07:25.668909: step 19758, loss 0.436952.
Train: 2018-08-02T00:07:25.849427: step 19759, loss 0.41923.
Train: 2018-08-02T00:07:26.012990: step 19760, loss 0.563435.
Test: 2018-08-02T00:07:26.545608: step 19760, loss 0.548417.
Train: 2018-08-02T00:07:26.719133: step 19761, loss 0.617565.
Train: 2018-08-02T00:07:26.894658: step 19762, loss 0.563365.
Train: 2018-08-02T00:07:27.064210: step 19763, loss 0.689443.
Train: 2018-08-02T00:07:27.228739: step 19764, loss 0.563952.
Train: 2018-08-02T00:07:27.392303: step 19765, loss 0.563625.
Train: 2018-08-02T00:07:27.561849: step 19766, loss 0.545426.
Train: 2018-08-02T00:07:27.740372: step 19767, loss 0.527649.
Train: 2018-08-02T00:07:27.909919: step 19768, loss 0.634597.
Train: 2018-08-02T00:07:28.072486: step 19769, loss 0.52814.
Train: 2018-08-02T00:07:28.249013: step 19770, loss 0.509446.
Test: 2018-08-02T00:07:28.774637: step 19770, loss 0.548473.
Train: 2018-08-02T00:07:28.941193: step 19771, loss 0.439127.
Train: 2018-08-02T00:07:29.108715: step 19772, loss 0.528888.
Train: 2018-08-02T00:07:29.271280: step 19773, loss 0.651934.
Train: 2018-08-02T00:07:29.433875: step 19774, loss 0.616434.
Train: 2018-08-02T00:07:29.597439: step 19775, loss 0.527856.
Train: 2018-08-02T00:07:29.770970: step 19776, loss 0.652074.
Train: 2018-08-02T00:07:29.940515: step 19777, loss 0.528629.
Train: 2018-08-02T00:07:30.108043: step 19778, loss 0.562991.
Train: 2018-08-02T00:07:30.275595: step 19779, loss 0.511341.
Train: 2018-08-02T00:07:30.437162: step 19780, loss 0.563112.
Test: 2018-08-02T00:07:30.958777: step 19780, loss 0.548485.
Train: 2018-08-02T00:07:31.125349: step 19781, loss 0.510261.
Train: 2018-08-02T00:07:31.291877: step 19782, loss 0.492694.
Train: 2018-08-02T00:07:31.454444: step 19783, loss 0.598483.
Train: 2018-08-02T00:07:31.618042: step 19784, loss 0.56305.
Train: 2018-08-02T00:07:31.781614: step 19785, loss 0.598434.
Train: 2018-08-02T00:07:31.951140: step 19786, loss 0.598255.
Train: 2018-08-02T00:07:32.113713: step 19787, loss 0.422424.
Train: 2018-08-02T00:07:32.277272: step 19788, loss 0.581051.
Train: 2018-08-02T00:07:32.441842: step 19789, loss 0.563201.
Train: 2018-08-02T00:07:32.614369: step 19790, loss 0.58053.
Test: 2018-08-02T00:07:33.129963: step 19790, loss 0.548422.
Train: 2018-08-02T00:07:33.295521: step 19791, loss 0.52773.
Train: 2018-08-02T00:07:33.459086: step 19792, loss 0.686508.
Train: 2018-08-02T00:07:33.621682: step 19793, loss 0.546097.
Train: 2018-08-02T00:07:33.792220: step 19794, loss 0.580762.
Train: 2018-08-02T00:07:33.959746: step 19795, loss 0.687347.
Train: 2018-08-02T00:07:34.125304: step 19796, loss 0.543568.
Train: 2018-08-02T00:07:34.286904: step 19797, loss 0.580143.
Train: 2018-08-02T00:07:34.462433: step 19798, loss 0.509063.
Train: 2018-08-02T00:07:34.627959: step 19799, loss 0.562716.
Train: 2018-08-02T00:07:34.789528: step 19800, loss 0.601567.
Test: 2018-08-02T00:07:35.304153: step 19800, loss 0.548551.
Train: 2018-08-02T00:07:36.117221: step 19801, loss 0.474779.
Train: 2018-08-02T00:07:36.278814: step 19802, loss 0.638272.
Train: 2018-08-02T00:07:36.441354: step 19803, loss 0.527748.
Train: 2018-08-02T00:07:36.604940: step 19804, loss 0.562482.
Train: 2018-08-02T00:07:36.772469: step 19805, loss 0.5803.
Train: 2018-08-02T00:07:36.936032: step 19806, loss 0.493654.
Train: 2018-08-02T00:07:37.111563: step 19807, loss 0.788758.
Train: 2018-08-02T00:07:37.275125: step 19808, loss 0.459516.
Train: 2018-08-02T00:07:37.437691: step 19809, loss 0.580184.
Train: 2018-08-02T00:07:37.607268: step 19810, loss 0.54587.
Test: 2018-08-02T00:07:38.125851: step 19810, loss 0.548677.
Train: 2018-08-02T00:07:38.287438: step 19811, loss 0.42492.
Train: 2018-08-02T00:07:38.454010: step 19812, loss 0.511541.
Train: 2018-08-02T00:07:38.617568: step 19813, loss 0.632908.
Train: 2018-08-02T00:07:38.783121: step 19814, loss 0.528493.
Train: 2018-08-02T00:07:38.944687: step 19815, loss 0.494041.
Train: 2018-08-02T00:07:39.111243: step 19816, loss 0.51133.
Train: 2018-08-02T00:07:39.273783: step 19817, loss 0.58022.
Train: 2018-08-02T00:07:39.436397: step 19818, loss 0.580815.
Train: 2018-08-02T00:07:39.608919: step 19819, loss 0.528355.
Train: 2018-08-02T00:07:39.776466: step 19820, loss 0.562726.
Test: 2018-08-02T00:07:40.306023: step 19820, loss 0.548542.
Train: 2018-08-02T00:07:40.477564: step 19821, loss 0.754437.
Train: 2018-08-02T00:07:40.643122: step 19822, loss 0.476757.
Train: 2018-08-02T00:07:40.819650: step 19823, loss 0.615045.
Train: 2018-08-02T00:07:40.992189: step 19824, loss 0.631761.
Train: 2018-08-02T00:07:41.162735: step 19825, loss 0.645767.
Train: 2018-08-02T00:07:41.328321: step 19826, loss 0.577164.
Train: 2018-08-02T00:07:41.495867: step 19827, loss 0.584572.
Train: 2018-08-02T00:07:41.669408: step 19828, loss 0.552282.
Train: 2018-08-02T00:07:41.846903: step 19829, loss 0.559593.
Train: 2018-08-02T00:07:42.011463: step 19830, loss 0.514396.
Test: 2018-08-02T00:07:42.552018: step 19830, loss 0.549018.
Train: 2018-08-02T00:07:42.720590: step 19831, loss 0.597436.
Train: 2018-08-02T00:07:42.889143: step 19832, loss 0.529033.
Train: 2018-08-02T00:07:43.054700: step 19833, loss 0.647945.
Train: 2018-08-02T00:07:43.220233: step 19834, loss 0.565571.
Train: 2018-08-02T00:07:43.394766: step 19835, loss 0.614397.
Train: 2018-08-02T00:07:43.561321: step 19836, loss 0.562984.
Train: 2018-08-02T00:07:43.727900: step 19837, loss 0.494715.
Train: 2018-08-02T00:07:43.891469: step 19838, loss 0.494361.
Train: 2018-08-02T00:07:44.066006: step 19839, loss 0.563015.
Train: 2018-08-02T00:07:44.230561: step 19840, loss 0.529283.
Test: 2018-08-02T00:07:44.774094: step 19840, loss 0.548662.
Train: 2018-08-02T00:07:44.939662: step 19841, loss 0.700763.
Train: 2018-08-02T00:07:45.109183: step 19842, loss 0.494429.
Train: 2018-08-02T00:07:45.280725: step 19843, loss 0.528787.
Train: 2018-08-02T00:07:45.444288: step 19844, loss 0.545768.
Train: 2018-08-02T00:07:45.615862: step 19845, loss 0.631423.
Train: 2018-08-02T00:07:45.779423: step 19846, loss 0.562693.
Train: 2018-08-02T00:07:45.957945: step 19847, loss 0.596868.
Train: 2018-08-02T00:07:46.123472: step 19848, loss 0.528348.
Train: 2018-08-02T00:07:46.305984: step 19849, loss 0.54513.
Train: 2018-08-02T00:07:46.469549: step 19850, loss 0.562217.
Test: 2018-08-02T00:07:47.003132: step 19850, loss 0.548584.
Train: 2018-08-02T00:07:47.169675: step 19851, loss 0.494353.
Train: 2018-08-02T00:07:47.339254: step 19852, loss 0.562321.
Train: 2018-08-02T00:07:47.516747: step 19853, loss 0.527273.
Train: 2018-08-02T00:07:47.681338: step 19854, loss 0.49489.
Train: 2018-08-02T00:07:47.839927: step 19855, loss 0.564797.
Train: 2018-08-02T00:07:48.005441: step 19856, loss 0.58021.
Train: 2018-08-02T00:07:48.167042: step 19857, loss 0.612759.
Train: 2018-08-02T00:07:48.330572: step 19858, loss 0.563393.
Train: 2018-08-02T00:07:48.491170: step 19859, loss 0.621198.
Train: 2018-08-02T00:07:48.653750: step 19860, loss 0.559369.
Test: 2018-08-02T00:07:49.185286: step 19860, loss 0.548268.
Train: 2018-08-02T00:07:49.355860: step 19861, loss 0.558684.
Train: 2018-08-02T00:07:49.521414: step 19862, loss 0.560453.
Train: 2018-08-02T00:07:49.684998: step 19863, loss 0.561115.
Train: 2018-08-02T00:07:49.849510: step 19864, loss 0.527955.
Train: 2018-08-02T00:07:50.008125: step 19865, loss 0.695869.
Train: 2018-08-02T00:07:50.183618: step 19866, loss 0.640894.
Train: 2018-08-02T00:07:50.352167: step 19867, loss 0.578604.
Train: 2018-08-02T00:07:50.513735: step 19868, loss 0.56743.
Train: 2018-08-02T00:07:50.678295: step 19869, loss 0.509889.
Train: 2018-08-02T00:07:50.844878: step 19870, loss 0.617728.
Test: 2018-08-02T00:07:51.373437: step 19870, loss 0.548609.
Train: 2018-08-02T00:07:51.540017: step 19871, loss 0.736024.
Train: 2018-08-02T00:07:51.708590: step 19872, loss 0.511715.
Train: 2018-08-02T00:07:51.871107: step 19873, loss 0.51286.
Train: 2018-08-02T00:07:52.033672: step 19874, loss 0.512606.
Train: 2018-08-02T00:07:52.215187: step 19875, loss 0.646032.
Train: 2018-08-02T00:07:52.375757: step 19876, loss 0.563239.
Train: 2018-08-02T00:07:52.541314: step 19877, loss 0.678865.
Train: 2018-08-02T00:07:52.705877: step 19878, loss 0.563054.
Train: 2018-08-02T00:07:52.866446: step 19879, loss 0.465481.
Train: 2018-08-02T00:07:53.037016: step 19880, loss 0.579184.
Test: 2018-08-02T00:07:53.555604: step 19880, loss 0.549537.
Train: 2018-08-02T00:07:53.721187: step 19881, loss 0.594885.
Train: 2018-08-02T00:07:53.885722: step 19882, loss 0.5481.
Train: 2018-08-02T00:07:54.051292: step 19883, loss 0.547565.
Train: 2018-08-02T00:07:54.217833: step 19884, loss 0.579877.
Train: 2018-08-02T00:07:54.382394: step 19885, loss 0.596266.
Train: 2018-08-02T00:07:54.551971: step 19886, loss 0.513605.
Train: 2018-08-02T00:07:54.716501: step 19887, loss 0.643982.
Train: 2018-08-02T00:07:54.880063: step 19888, loss 0.546286.
Train: 2018-08-02T00:07:55.041660: step 19889, loss 0.5458.
Train: 2018-08-02T00:07:55.205218: step 19890, loss 0.499609.
Test: 2018-08-02T00:07:55.731787: step 19890, loss 0.549555.
Train: 2018-08-02T00:07:55.894382: step 19891, loss 0.498575.
Train: 2018-08-02T00:07:56.064927: step 19892, loss 0.561795.
Train: 2018-08-02T00:07:56.229481: step 19893, loss 0.59646.
Train: 2018-08-02T00:07:56.401995: step 19894, loss 0.645008.
Train: 2018-08-02T00:07:56.574565: step 19895, loss 0.563797.
Train: 2018-08-02T00:07:56.740093: step 19896, loss 0.661757.
Train: 2018-08-02T00:07:56.900662: step 19897, loss 0.51421.
Train: 2018-08-02T00:07:57.065222: step 19898, loss 0.546386.
Train: 2018-08-02T00:07:57.233796: step 19899, loss 0.497605.
Train: 2018-08-02T00:07:57.400326: step 19900, loss 0.579534.
Test: 2018-08-02T00:07:57.945898: step 19900, loss 0.549182.
Train: 2018-08-02T00:07:58.726319: step 19901, loss 0.46363.
Train: 2018-08-02T00:07:58.890889: step 19902, loss 0.596085.
Train: 2018-08-02T00:07:59.054446: step 19903, loss 0.596138.
Train: 2018-08-02T00:07:59.228980: step 19904, loss 0.463282.
Train: 2018-08-02T00:07:59.396531: step 19905, loss 0.612698.
Train: 2018-08-02T00:07:59.578021: step 19906, loss 0.629884.
Train: 2018-08-02T00:07:59.737626: step 19907, loss 0.545821.
Train: 2018-08-02T00:07:59.904151: step 19908, loss 0.529082.
Train: 2018-08-02T00:08:00.070705: step 19909, loss 0.545927.
Train: 2018-08-02T00:08:00.235290: step 19910, loss 0.579567.
Test: 2018-08-02T00:08:00.769835: step 19910, loss 0.548803.
Train: 2018-08-02T00:08:00.937389: step 19911, loss 0.579798.
Train: 2018-08-02T00:08:01.102971: step 19912, loss 0.496021.
Train: 2018-08-02T00:08:01.265545: step 19913, loss 0.613969.
Train: 2018-08-02T00:08:01.438050: step 19914, loss 0.562975.
Train: 2018-08-02T00:08:01.603607: step 19915, loss 0.596912.
Train: 2018-08-02T00:08:01.766197: step 19916, loss 0.512365.
Train: 2018-08-02T00:08:01.935749: step 19917, loss 0.49505.
Train: 2018-08-02T00:08:02.097323: step 19918, loss 0.545831.
Train: 2018-08-02T00:08:02.261848: step 19919, loss 0.545946.
Train: 2018-08-02T00:08:02.433389: step 19920, loss 0.477544.
Test: 2018-08-02T00:08:02.975938: step 19920, loss 0.548518.
Train: 2018-08-02T00:08:03.152497: step 19921, loss 0.597067.
Train: 2018-08-02T00:08:03.316053: step 19922, loss 0.493771.
Train: 2018-08-02T00:08:03.505524: step 19923, loss 0.665582.
Train: 2018-08-02T00:08:03.679059: step 19924, loss 0.597138.
Train: 2018-08-02T00:08:03.850611: step 19925, loss 0.580015.
Train: 2018-08-02T00:08:04.021146: step 19926, loss 0.460268.
Train: 2018-08-02T00:08:04.183735: step 19927, loss 0.511314.
Train: 2018-08-02T00:08:04.349298: step 19928, loss 0.455272.
Train: 2018-08-02T00:08:04.527790: step 19929, loss 0.599861.
Train: 2018-08-02T00:08:04.696340: step 19930, loss 0.491627.
Test: 2018-08-02T00:08:05.233903: step 19930, loss 0.548303.
Train: 2018-08-02T00:08:05.405443: step 19931, loss 0.543063.
Train: 2018-08-02T00:08:05.567038: step 19932, loss 0.605634.
Train: 2018-08-02T00:08:05.732600: step 19933, loss 0.413103.
Train: 2018-08-02T00:08:05.896174: step 19934, loss 0.548813.
Train: 2018-08-02T00:08:06.056734: step 19935, loss 0.588776.
Train: 2018-08-02T00:08:06.222259: step 19936, loss 0.710457.
Train: 2018-08-02T00:08:06.390811: step 19937, loss 0.5662.
Train: 2018-08-02T00:08:06.558376: step 19938, loss 0.525705.
Train: 2018-08-02T00:08:06.725945: step 19939, loss 0.563495.
Train: 2018-08-02T00:08:06.891470: step 19940, loss 0.564098.
Test: 2018-08-02T00:08:07.423051: step 19940, loss 0.548172.
Train: 2018-08-02T00:08:07.590628: step 19941, loss 0.687519.
Train: 2018-08-02T00:08:07.758184: step 19942, loss 0.580548.
Train: 2018-08-02T00:08:07.922745: step 19943, loss 0.615743.
Train: 2018-08-02T00:08:08.087275: step 19944, loss 0.650818.
Train: 2018-08-02T00:08:08.255854: step 19945, loss 0.49216.
Train: 2018-08-02T00:08:08.418420: step 19946, loss 0.510594.
Train: 2018-08-02T00:08:08.593920: step 19947, loss 0.493307.
Train: 2018-08-02T00:08:08.758481: step 19948, loss 0.615312.
Train: 2018-08-02T00:08:08.922068: step 19949, loss 0.493739.
Train: 2018-08-02T00:08:09.085632: step 19950, loss 0.65015.
Test: 2018-08-02T00:08:09.617185: step 19950, loss 0.548285.
Train: 2018-08-02T00:08:09.783739: step 19951, loss 0.545474.
Train: 2018-08-02T00:08:09.950324: step 19952, loss 0.597065.
Train: 2018-08-02T00:08:10.112891: step 19953, loss 0.56257.
Train: 2018-08-02T00:08:10.276447: step 19954, loss 0.562919.
Train: 2018-08-02T00:08:10.442005: step 19955, loss 0.545903.
Train: 2018-08-02T00:08:10.605570: step 19956, loss 0.545453.
Train: 2018-08-02T00:08:10.770135: step 19957, loss 0.528528.
Train: 2018-08-02T00:08:10.934690: step 19958, loss 0.528325.
Train: 2018-08-02T00:08:11.098251: step 19959, loss 0.545515.
Train: 2018-08-02T00:08:11.263815: step 19960, loss 0.666851.
Test: 2018-08-02T00:08:11.788379: step 19960, loss 0.548336.
Train: 2018-08-02T00:08:11.953966: step 19961, loss 0.493564.
Train: 2018-08-02T00:08:12.117531: step 19962, loss 0.545517.
Train: 2018-08-02T00:08:12.282060: step 19963, loss 0.49355.
Train: 2018-08-02T00:08:12.448643: step 19964, loss 0.545492.
Train: 2018-08-02T00:08:12.616167: step 19965, loss 0.580199.
Train: 2018-08-02T00:08:12.783735: step 19966, loss 0.441193.
Train: 2018-08-02T00:08:12.952300: step 19967, loss 0.597673.
Train: 2018-08-02T00:08:13.111869: step 19968, loss 0.527952.
Train: 2018-08-02T00:08:13.278398: step 19969, loss 0.597797.
Train: 2018-08-02T00:08:13.442958: step 19970, loss 0.597838.
Test: 2018-08-02T00:08:13.971552: step 19970, loss 0.548188.
Train: 2018-08-02T00:08:14.135137: step 19971, loss 0.545342.
Train: 2018-08-02T00:08:14.311635: step 19972, loss 0.615389.
Train: 2018-08-02T00:08:14.476196: step 19973, loss 0.545331.
Train: 2018-08-02T00:08:14.638760: step 19974, loss 0.492797.
Train: 2018-08-02T00:08:14.809304: step 19975, loss 0.545315.
Train: 2018-08-02T00:08:14.974863: step 19976, loss 0.633007.
Train: 2018-08-02T00:08:15.146403: step 19977, loss 0.527762.
Train: 2018-08-02T00:08:15.312960: step 19978, loss 0.527752.
Train: 2018-08-02T00:08:15.481507: step 19979, loss 0.562839.
Train: 2018-08-02T00:08:15.654047: step 19980, loss 0.492591.
Test: 2018-08-02T00:08:16.193604: step 19980, loss 0.548119.
Train: 2018-08-02T00:08:16.361158: step 19981, loss 0.51009.
Train: 2018-08-02T00:08:16.524719: step 19982, loss 0.527621.
Train: 2018-08-02T00:08:16.689304: step 19983, loss 0.527562.
Train: 2018-08-02T00:08:16.852861: step 19984, loss 0.633624.
Train: 2018-08-02T00:08:17.028372: step 19985, loss 0.509768.
Train: 2018-08-02T00:08:17.191961: step 19986, loss 0.633801.
Train: 2018-08-02T00:08:17.360497: step 19987, loss 0.687018.
Train: 2018-08-02T00:08:17.525047: step 19988, loss 0.598289.
Train: 2018-08-02T00:08:17.687644: step 19989, loss 0.562859.
Train: 2018-08-02T00:08:17.860158: step 19990, loss 0.492271.
Test: 2018-08-02T00:08:18.396715: step 19990, loss 0.548075.
Train: 2018-08-02T00:08:18.563268: step 19991, loss 0.527578.
Train: 2018-08-02T00:08:18.729855: step 19992, loss 0.509961.
Train: 2018-08-02T00:08:18.895410: step 19993, loss 0.509944.
Train: 2018-08-02T00:08:19.059943: step 19994, loss 0.527547.
Train: 2018-08-02T00:08:19.222515: step 19995, loss 0.509845.
Train: 2018-08-02T00:08:19.400033: step 19996, loss 0.580548.
Train: 2018-08-02T00:08:19.579552: step 19997, loss 0.598291.
Train: 2018-08-02T00:08:19.743115: step 19998, loss 0.545137.
Train: 2018-08-02T00:08:19.905681: step 19999, loss 0.54513.
Train: 2018-08-02T00:08:20.085201: step 20000, loss 0.580613.
Test: 2018-08-02T00:08:20.618803: step 20000, loss 0.548004.
Train: 2018-08-02T00:08:21.400669: step 20001, loss 0.456367.
Train: 2018-08-02T00:08:21.571204: step 20002, loss 0.545102.
Train: 2018-08-02T00:08:21.735789: step 20003, loss 0.651899.
Train: 2018-08-02T00:08:21.904312: step 20004, loss 0.580691.
Train: 2018-08-02T00:08:22.078848: step 20005, loss 0.545085.
Train: 2018-08-02T00:08:22.245429: step 20006, loss 0.740839.
Train: 2018-08-02T00:08:22.416973: step 20007, loss 0.669367.
Train: 2018-08-02T00:08:22.586515: step 20008, loss 0.562828.
Train: 2018-08-02T00:08:22.751078: step 20009, loss 0.492302.
Train: 2018-08-02T00:08:22.915610: step 20010, loss 0.457276.
Test: 2018-08-02T00:08:23.446191: step 20010, loss 0.548068.
Train: 2018-08-02T00:08:23.621752: step 20011, loss 0.597927.
Train: 2018-08-02T00:08:23.787296: step 20012, loss 0.685631.
Train: 2018-08-02T00:08:23.951839: step 20013, loss 0.562759.
Train: 2018-08-02T00:08:24.114404: step 20014, loss 0.615121.
Train: 2018-08-02T00:08:24.278996: step 20015, loss 0.61495.
Train: 2018-08-02T00:08:24.443551: step 20016, loss 0.545378.
Train: 2018-08-02T00:08:24.611109: step 20017, loss 0.597302.
Train: 2018-08-02T00:08:24.780649: step 20018, loss 0.51099.
Train: 2018-08-02T00:08:24.952190: step 20019, loss 0.493896.
Train: 2018-08-02T00:08:25.119735: step 20020, loss 0.597073.
Test: 2018-08-02T00:08:25.649328: step 20020, loss 0.548331.
Train: 2018-08-02T00:08:25.814884: step 20021, loss 0.52838.
Train: 2018-08-02T00:08:25.984407: step 20022, loss 0.5627.
Train: 2018-08-02T00:08:26.150960: step 20023, loss 0.494158.
Train: 2018-08-02T00:08:26.321529: step 20024, loss 0.511276.
Train: 2018-08-02T00:08:26.501025: step 20025, loss 0.545539.
Train: 2018-08-02T00:08:26.664618: step 20026, loss 0.682928.
Train: 2018-08-02T00:08:26.828179: step 20027, loss 0.562694.
Train: 2018-08-02T00:08:26.990742: step 20028, loss 0.562692.
Train: 2018-08-02T00:08:27.157304: step 20029, loss 0.545545.
Train: 2018-08-02T00:08:27.322856: step 20030, loss 0.699829.
Test: 2018-08-02T00:08:27.846457: step 20030, loss 0.548364.
Train: 2018-08-02T00:08:28.011014: step 20031, loss 0.61401.
Train: 2018-08-02T00:08:28.183558: step 20032, loss 0.545627.
Train: 2018-08-02T00:08:28.361078: step 20033, loss 0.596748.
Train: 2018-08-02T00:08:28.527636: step 20034, loss 0.664628.
Train: 2018-08-02T00:08:28.690172: step 20035, loss 0.393403.
Train: 2018-08-02T00:08:28.850768: step 20036, loss 0.495033.
Train: 2018-08-02T00:08:29.025276: step 20037, loss 0.528855.
Train: 2018-08-02T00:08:29.191832: step 20038, loss 0.664322.
Train: 2018-08-02T00:08:29.369387: step 20039, loss 0.630405.
Train: 2018-08-02T00:08:29.534914: step 20040, loss 0.613407.
Test: 2018-08-02T00:08:30.076467: step 20040, loss 0.548583.
Train: 2018-08-02T00:08:30.244019: step 20041, loss 0.528972.
Train: 2018-08-02T00:08:30.411571: step 20042, loss 0.427938.
Train: 2018-08-02T00:08:30.575133: step 20043, loss 0.630148.
Train: 2018-08-02T00:08:30.752659: step 20044, loss 0.680709.
Train: 2018-08-02T00:08:30.923228: step 20045, loss 0.512229.
Train: 2018-08-02T00:08:31.084784: step 20046, loss 0.680417.
Train: 2018-08-02T00:08:31.249332: step 20047, loss 0.562725.
Train: 2018-08-02T00:08:31.417907: step 20048, loss 0.495767.
Train: 2018-08-02T00:08:31.589424: step 20049, loss 0.495823.
Train: 2018-08-02T00:08:31.753982: step 20050, loss 0.646404.
Test: 2018-08-02T00:08:32.297534: step 20050, loss 0.548737.
Train: 2018-08-02T00:08:32.468104: step 20051, loss 0.445699.
Train: 2018-08-02T00:08:32.632634: step 20052, loss 0.529259.
Train: 2018-08-02T00:08:32.797192: step 20053, loss 0.629766.
Train: 2018-08-02T00:08:32.973722: step 20054, loss 0.512416.
Train: 2018-08-02T00:08:33.143299: step 20055, loss 0.596287.
Train: 2018-08-02T00:08:33.306831: step 20056, loss 0.697092.
Train: 2018-08-02T00:08:33.471425: step 20057, loss 0.663376.
Train: 2018-08-02T00:08:33.637976: step 20058, loss 0.479046.
Train: 2018-08-02T00:08:33.802506: step 20059, loss 0.596169.
Train: 2018-08-02T00:08:33.968063: step 20060, loss 0.462544.
Test: 2018-08-02T00:08:34.508618: step 20060, loss 0.548744.
Train: 2018-08-02T00:08:34.738638: step 20061, loss 0.462505.
Train: 2018-08-02T00:08:34.905192: step 20062, loss 0.545986.
Train: 2018-08-02T00:08:35.069756: step 20063, loss 0.579478.
Train: 2018-08-02T00:08:35.236309: step 20064, loss 0.646674.
Train: 2018-08-02T00:08:35.404836: step 20065, loss 0.461899.
Train: 2018-08-02T00:08:35.575372: step 20066, loss 0.646838.
Train: 2018-08-02T00:08:35.747942: step 20067, loss 0.630037.
Train: 2018-08-02T00:08:35.929458: step 20068, loss 0.663666.
Train: 2018-08-02T00:08:36.098973: step 20069, loss 0.579495.
Train: 2018-08-02T00:08:36.264529: step 20070, loss 0.51239.
Test: 2018-08-02T00:08:36.792134: step 20070, loss 0.548673.
Train: 2018-08-02T00:08:36.955682: step 20071, loss 0.545948.
Train: 2018-08-02T00:08:37.121240: step 20072, loss 0.512456.
Train: 2018-08-02T00:08:37.293779: step 20073, loss 0.529189.
Train: 2018-08-02T00:08:37.464323: step 20074, loss 0.696877.
Train: 2018-08-02T00:08:37.640878: step 20075, loss 0.512435.
Train: 2018-08-02T00:08:37.803465: step 20076, loss 0.495687.
Train: 2018-08-02T00:08:37.966014: step 20077, loss 0.529159.
Train: 2018-08-02T00:08:38.132561: step 20078, loss 0.461939.
Train: 2018-08-02T00:08:38.298124: step 20079, loss 0.613186.
Train: 2018-08-02T00:08:38.464673: step 20080, loss 0.512069.
Test: 2018-08-02T00:08:38.999220: step 20080, loss 0.548502.
Train: 2018-08-02T00:08:39.163778: step 20081, loss 0.630282.
Train: 2018-08-02T00:08:39.329335: step 20082, loss 0.596507.
Train: 2018-08-02T00:08:39.502872: step 20083, loss 0.461025.
Train: 2018-08-02T00:08:39.678434: step 20084, loss 0.579617.
Train: 2018-08-02T00:08:39.841002: step 20085, loss 0.57964.
Train: 2018-08-02T00:08:40.002536: step 20086, loss 0.647743.
Train: 2018-08-02T00:08:40.169123: step 20087, loss 0.647743.
Train: 2018-08-02T00:08:40.340659: step 20088, loss 0.57964.
Train: 2018-08-02T00:08:40.503232: step 20089, loss 0.630565.
Train: 2018-08-02T00:08:40.679757: step 20090, loss 0.596539.
Test: 2018-08-02T00:08:41.213326: step 20090, loss 0.54849.
Train: 2018-08-02T00:08:41.380885: step 20091, loss 0.630289.
Train: 2018-08-02T00:08:41.548438: step 20092, loss 0.528937.
Train: 2018-08-02T00:08:41.712964: step 20093, loss 0.562667.
Train: 2018-08-02T00:08:41.882510: step 20094, loss 0.461904.
Train: 2018-08-02T00:08:42.051086: step 20095, loss 0.52909.
Train: 2018-08-02T00:08:42.220608: step 20096, loss 0.663461.
Train: 2018-08-02T00:08:42.388185: step 20097, loss 0.512331.
Train: 2018-08-02T00:08:42.557733: step 20098, loss 0.545896.
Train: 2018-08-02T00:08:42.741216: step 20099, loss 0.579456.
Train: 2018-08-02T00:08:42.906773: step 20100, loss 0.54589.
Test: 2018-08-02T00:08:43.437387: step 20100, loss 0.548613.
Train: 2018-08-02T00:08:44.224928: step 20101, loss 0.545883.
Train: 2018-08-02T00:08:44.392514: step 20102, loss 0.562667.
Train: 2018-08-02T00:08:44.560033: step 20103, loss 0.545857.
Train: 2018-08-02T00:08:44.728607: step 20104, loss 0.495377.
Train: 2018-08-02T00:08:44.896134: step 20105, loss 0.545801.
Train: 2018-08-02T00:08:45.059721: step 20106, loss 0.49512.
Train: 2018-08-02T00:08:45.229269: step 20107, loss 0.494932.
Train: 2018-08-02T00:08:45.398791: step 20108, loss 0.545642.
Train: 2018-08-02T00:08:45.565345: step 20109, loss 0.562617.
Train: 2018-08-02T00:08:45.733920: step 20110, loss 0.596788.
Test: 2018-08-02T00:08:46.260489: step 20110, loss 0.54827.
Train: 2018-08-02T00:08:46.426070: step 20111, loss 0.511235.
Train: 2018-08-02T00:08:46.597586: step 20112, loss 0.390901.
Train: 2018-08-02T00:08:46.763143: step 20113, loss 0.476356.
Train: 2018-08-02T00:08:46.927731: step 20114, loss 0.649334.
Train: 2018-08-02T00:08:47.092275: step 20115, loss 0.510416.
Train: 2018-08-02T00:08:47.257821: step 20116, loss 0.632531.
Train: 2018-08-02T00:08:47.426394: step 20117, loss 0.632716.
Train: 2018-08-02T00:08:47.598909: step 20118, loss 0.545122.
Train: 2018-08-02T00:08:47.779425: step 20119, loss 0.615349.
Train: 2018-08-02T00:08:47.952962: step 20120, loss 0.738354.
Test: 2018-08-02T00:08:48.486540: step 20120, loss 0.547974.
Train: 2018-08-02T00:08:48.653123: step 20121, loss 0.510052.
Train: 2018-08-02T00:08:48.823663: step 20122, loss 0.52762.
Train: 2018-08-02T00:08:48.986227: step 20123, loss 0.562647.
Train: 2018-08-02T00:08:49.153752: step 20124, loss 0.475172.
Train: 2018-08-02T00:08:49.325324: step 20125, loss 0.510136.
Train: 2018-08-02T00:08:49.492876: step 20126, loss 0.545128.
Train: 2018-08-02T00:08:49.664387: step 20127, loss 0.527572.
Train: 2018-08-02T00:08:49.831972: step 20128, loss 0.509964.
Train: 2018-08-02T00:08:50.000519: step 20129, loss 0.527474.
Train: 2018-08-02T00:08:50.168042: step 20130, loss 0.615586.
Test: 2018-08-02T00:08:50.703609: step 20130, loss 0.547907.
Train: 2018-08-02T00:08:50.873181: step 20131, loss 0.615649.
Train: 2018-08-02T00:08:51.035746: step 20132, loss 0.598005.
Train: 2018-08-02T00:08:51.197314: step 20133, loss 0.509734.
Train: 2018-08-02T00:08:51.366836: step 20134, loss 0.598001.
Train: 2018-08-02T00:08:51.531396: step 20135, loss 0.527383.
Train: 2018-08-02T00:08:51.699970: step 20136, loss 0.509726.
Train: 2018-08-02T00:08:51.865503: step 20137, loss 0.527359.
Train: 2018-08-02T00:08:52.026107: step 20138, loss 0.509648.
Train: 2018-08-02T00:08:52.191662: step 20139, loss 0.580415.
Train: 2018-08-02T00:08:52.356191: step 20140, loss 0.527261.
Test: 2018-08-02T00:08:52.899739: step 20140, loss 0.54786.
Train: 2018-08-02T00:08:53.067290: step 20141, loss 0.544974.
Train: 2018-08-02T00:08:53.233845: step 20142, loss 0.509421.
Train: 2018-08-02T00:08:53.399403: step 20143, loss 0.598345.
Train: 2018-08-02T00:08:53.570944: step 20144, loss 0.669654.
Train: 2018-08-02T00:08:53.741512: step 20145, loss 0.491522.
Train: 2018-08-02T00:08:53.906048: step 20146, loss 0.56275.
Train: 2018-08-02T00:08:54.076592: step 20147, loss 0.616191.
Train: 2018-08-02T00:08:54.245142: step 20148, loss 0.50934.
Train: 2018-08-02T00:08:54.412693: step 20149, loss 0.580543.
Train: 2018-08-02T00:08:54.580276: step 20150, loss 0.562739.
Test: 2018-08-02T00:08:55.113839: step 20150, loss 0.547841.
Train: 2018-08-02T00:08:55.279401: step 20151, loss 0.651664.
Train: 2018-08-02T00:08:55.442939: step 20152, loss 0.544963.
Train: 2018-08-02T00:08:55.604533: step 20153, loss 0.580443.
Train: 2018-08-02T00:08:55.771088: step 20154, loss 0.54499.
Train: 2018-08-02T00:08:55.934626: step 20155, loss 0.668796.
Train: 2018-08-02T00:08:56.099185: step 20156, loss 0.42155.
Train: 2018-08-02T00:08:56.261750: step 20157, loss 0.615553.
Train: 2018-08-02T00:08:56.429302: step 20158, loss 0.545049.
Train: 2018-08-02T00:08:56.604833: step 20159, loss 0.509883.
Train: 2018-08-02T00:08:56.768396: step 20160, loss 0.580231.
Test: 2018-08-02T00:08:57.300972: step 20160, loss 0.547927.
Train: 2018-08-02T00:08:57.469548: step 20161, loss 0.685658.
Train: 2018-08-02T00:08:57.644055: step 20162, loss 0.52756.
Train: 2018-08-02T00:08:57.807618: step 20163, loss 0.580131.
Train: 2018-08-02T00:08:57.990132: step 20164, loss 0.667487.
Train: 2018-08-02T00:08:58.157683: step 20165, loss 0.492893.
Train: 2018-08-02T00:08:58.323264: step 20166, loss 0.562596.
Train: 2018-08-02T00:08:58.488798: step 20167, loss 0.673743.
Train: 2018-08-02T00:08:58.655357: step 20168, loss 0.614531.
Train: 2018-08-02T00:08:58.815923: step 20169, loss 0.683377.
Train: 2018-08-02T00:08:58.991454: step 20170, loss 0.631279.
Test: 2018-08-02T00:08:59.534003: step 20170, loss 0.548263.
Train: 2018-08-02T00:08:59.704572: step 20171, loss 0.511307.
Train: 2018-08-02T00:08:59.870105: step 20172, loss 0.59662.
Train: 2018-08-02T00:09:00.036659: step 20173, loss 0.664304.
Train: 2018-08-02T00:09:00.214184: step 20174, loss 0.511994.
Train: 2018-08-02T00:09:00.375784: step 20175, loss 0.54581.
Train: 2018-08-02T00:09:00.543336: step 20176, loss 0.529122.
Train: 2018-08-02T00:09:00.705870: step 20177, loss 0.428911.
Train: 2018-08-02T00:09:00.875442: step 20178, loss 0.579357.
Train: 2018-08-02T00:09:01.041005: step 20179, loss 0.579354.
Train: 2018-08-02T00:09:01.206532: step 20180, loss 0.545938.
Test: 2018-08-02T00:09:01.738114: step 20180, loss 0.548654.
Train: 2018-08-02T00:09:01.912644: step 20181, loss 0.529232.
Train: 2018-08-02T00:09:02.079200: step 20182, loss 0.545925.
Train: 2018-08-02T00:09:02.248773: step 20183, loss 0.579363.
Train: 2018-08-02T00:09:02.413307: step 20184, loss 0.612846.
Train: 2018-08-02T00:09:02.580858: step 20185, loss 0.545895.
Train: 2018-08-02T00:09:02.744421: step 20186, loss 0.562629.
Train: 2018-08-02T00:09:02.907001: step 20187, loss 0.663094.
Train: 2018-08-02T00:09:03.076537: step 20188, loss 0.562633.
Train: 2018-08-02T00:09:03.248075: step 20189, loss 0.512505.
Train: 2018-08-02T00:09:03.424631: step 20190, loss 0.529217.
Test: 2018-08-02T00:09:03.961169: step 20190, loss 0.548634.
Train: 2018-08-02T00:09:04.127752: step 20191, loss 0.596073.
Train: 2018-08-02T00:09:04.296299: step 20192, loss 0.545913.
Train: 2018-08-02T00:09:04.460832: step 20193, loss 0.662993.
Train: 2018-08-02T00:09:04.625417: step 20194, loss 0.545925.
Train: 2018-08-02T00:09:04.801920: step 20195, loss 0.529237.
Train: 2018-08-02T00:09:04.976453: step 20196, loss 0.562638.
Train: 2018-08-02T00:09:05.146025: step 20197, loss 0.545933.
Train: 2018-08-02T00:09:05.311559: step 20198, loss 0.579345.
Train: 2018-08-02T00:09:05.489084: step 20199, loss 0.562632.
Train: 2018-08-02T00:09:05.653644: step 20200, loss 0.512471.
Test: 2018-08-02T00:09:06.179252: step 20200, loss 0.548608.
Train: 2018-08-02T00:09:06.913314: step 20201, loss 0.545886.
Train: 2018-08-02T00:09:07.076844: step 20202, loss 0.596132.
Train: 2018-08-02T00:09:07.244421: step 20203, loss 0.596152.
Train: 2018-08-02T00:09:07.410983: step 20204, loss 0.529061.
Train: 2018-08-02T00:09:07.584518: step 20205, loss 0.545819.
Train: 2018-08-02T00:09:07.762041: step 20206, loss 0.596208.
Train: 2018-08-02T00:09:07.929565: step 20207, loss 0.461725.
Train: 2018-08-02T00:09:08.094157: step 20208, loss 0.528901.
Train: 2018-08-02T00:09:08.265666: step 20209, loss 0.630108.
Train: 2018-08-02T00:09:08.432220: step 20210, loss 0.562576.
Test: 2018-08-02T00:09:08.972776: step 20210, loss 0.548404.
Train: 2018-08-02T00:09:09.142350: step 20211, loss 0.545653.
Train: 2018-08-02T00:09:09.309902: step 20212, loss 0.46093.
Train: 2018-08-02T00:09:09.474468: step 20213, loss 0.596526.
Train: 2018-08-02T00:09:09.637998: step 20214, loss 0.562556.
Train: 2018-08-02T00:09:09.803585: step 20215, loss 0.54551.
Train: 2018-08-02T00:09:09.973132: step 20216, loss 0.494259.
Train: 2018-08-02T00:09:10.137662: step 20217, loss 0.528316.
Train: 2018-08-02T00:09:10.304216: step 20218, loss 0.596872.
Train: 2018-08-02T00:09:10.471768: step 20219, loss 0.579744.
Train: 2018-08-02T00:09:10.647323: step 20220, loss 0.648666.
Test: 2018-08-02T00:09:11.179904: step 20220, loss 0.548121.
Train: 2018-08-02T00:09:11.345459: step 20221, loss 0.648687.
Train: 2018-08-02T00:09:11.508996: step 20222, loss 0.424852.
Train: 2018-08-02T00:09:11.674552: step 20223, loss 0.579775.
Train: 2018-08-02T00:09:11.849087: step 20224, loss 0.528068.
Train: 2018-08-02T00:09:12.014644: step 20225, loss 0.648832.
Train: 2018-08-02T00:09:12.180201: step 20226, loss 0.510789.
Train: 2018-08-02T00:09:12.343765: step 20227, loss 0.648848.
Train: 2018-08-02T00:09:12.515336: step 20228, loss 0.579792.
Train: 2018-08-02T00:09:12.689864: step 20229, loss 0.493619.
Train: 2018-08-02T00:09:12.858419: step 20230, loss 0.545311.
Test: 2018-08-02T00:09:13.380992: step 20230, loss 0.548109.
Train: 2018-08-02T00:09:13.546574: step 20231, loss 0.562543.
Train: 2018-08-02T00:09:13.710143: step 20232, loss 0.528061.
Train: 2018-08-02T00:09:13.875669: step 20233, loss 0.648805.
Train: 2018-08-02T00:09:14.045246: step 20234, loss 0.648754.
Train: 2018-08-02T00:09:14.216785: step 20235, loss 0.545327.
Train: 2018-08-02T00:09:14.384309: step 20236, loss 0.682878.
Train: 2018-08-02T00:09:14.559868: step 20237, loss 0.699688.
Train: 2018-08-02T00:09:14.725402: step 20238, loss 0.494263.
Train: 2018-08-02T00:09:14.889958: step 20239, loss 0.596582.
Train: 2018-08-02T00:09:15.060527: step 20240, loss 0.613447.
Test: 2018-08-02T00:09:15.605046: step 20240, loss 0.548403.
Train: 2018-08-02T00:09:15.777625: step 20241, loss 0.579467.
Train: 2018-08-02T00:09:15.946133: step 20242, loss 0.613122.
Train: 2018-08-02T00:09:16.109696: step 20243, loss 0.529011.
Train: 2018-08-02T00:09:16.289241: step 20244, loss 0.579343.
Train: 2018-08-02T00:09:16.455771: step 20245, loss 0.545919.
Train: 2018-08-02T00:09:16.628320: step 20246, loss 0.645942.
Train: 2018-08-02T00:09:16.793896: step 20247, loss 0.546035.
Train: 2018-08-02T00:09:16.960423: step 20248, loss 0.595813.
Train: 2018-08-02T00:09:17.130987: step 20249, loss 0.513084.
Train: 2018-08-02T00:09:17.300513: step 20250, loss 0.645244.
Test: 2018-08-02T00:09:17.839074: step 20250, loss 0.548921.
Train: 2018-08-02T00:09:18.005674: step 20251, loss 0.579185.
Train: 2018-08-02T00:09:18.170225: step 20252, loss 0.529865.
Train: 2018-08-02T00:09:18.341763: step 20253, loss 0.480688.
Train: 2018-08-02T00:09:18.508310: step 20254, loss 0.562745.
Train: 2018-08-02T00:09:18.674870: step 20255, loss 0.579159.
Train: 2018-08-02T00:09:18.848406: step 20256, loss 0.529906.
Train: 2018-08-02T00:09:19.014964: step 20257, loss 0.612023.
Train: 2018-08-02T00:09:19.179518: step 20258, loss 0.628455.
Train: 2018-08-02T00:09:19.347042: step 20259, loss 0.546323.
Train: 2018-08-02T00:09:19.515623: step 20260, loss 0.546331.
Test: 2018-08-02T00:09:20.047171: step 20260, loss 0.548998.
Train: 2018-08-02T00:09:20.216752: step 20261, loss 0.579156.
Train: 2018-08-02T00:09:20.380280: step 20262, loss 0.513503.
Train: 2018-08-02T00:09:20.546834: step 20263, loss 0.579161.
Train: 2018-08-02T00:09:20.710423: step 20264, loss 0.54629.
Train: 2018-08-02T00:09:20.889952: step 20265, loss 0.562718.
Train: 2018-08-02T00:09:21.066477: step 20266, loss 0.645057.
Train: 2018-08-02T00:09:21.239015: step 20267, loss 0.529777.
Train: 2018-08-02T00:09:21.412521: step 20268, loss 0.513284.
Train: 2018-08-02T00:09:21.583064: step 20269, loss 0.463727.
Train: 2018-08-02T00:09:21.744664: step 20270, loss 0.579211.
Test: 2018-08-02T00:09:22.290206: step 20270, loss 0.548773.
Train: 2018-08-02T00:09:22.454759: step 20271, loss 0.612385.
Train: 2018-08-02T00:09:22.621324: step 20272, loss 0.546043.
Train: 2018-08-02T00:09:22.787845: step 20273, loss 0.479492.
Train: 2018-08-02T00:09:22.952406: step 20274, loss 0.595961.
Train: 2018-08-02T00:09:23.124974: step 20275, loss 0.479057.
Train: 2018-08-02T00:09:23.290525: step 20276, loss 0.612867.
Train: 2018-08-02T00:09:23.450074: step 20277, loss 0.629763.
Train: 2018-08-02T00:09:23.621616: step 20278, loss 0.579383.
Train: 2018-08-02T00:09:23.787172: step 20279, loss 0.596222.
Train: 2018-08-02T00:09:23.950761: step 20280, loss 0.512056.
Test: 2018-08-02T00:09:24.490325: step 20280, loss 0.548444.
Train: 2018-08-02T00:09:24.659871: step 20281, loss 0.663671.
Train: 2018-08-02T00:09:24.843380: step 20282, loss 0.47833.
Train: 2018-08-02T00:09:25.010901: step 20283, loss 0.478254.
Train: 2018-08-02T00:09:25.174499: step 20284, loss 0.647014.
Train: 2018-08-02T00:09:25.339063: step 20285, loss 0.579449.
Train: 2018-08-02T00:09:25.511596: step 20286, loss 0.579454.
Train: 2018-08-02T00:09:25.683136: step 20287, loss 0.461051.
Train: 2018-08-02T00:09:25.847690: step 20288, loss 0.579478.
Train: 2018-08-02T00:09:26.018210: step 20289, loss 0.562532.
Train: 2018-08-02T00:09:26.185785: step 20290, loss 0.562529.
Test: 2018-08-02T00:09:26.721328: step 20290, loss 0.54829.
Train: 2018-08-02T00:09:26.891874: step 20291, loss 0.545525.
Train: 2018-08-02T00:09:27.054478: step 20292, loss 0.52848.
Train: 2018-08-02T00:09:27.218033: step 20293, loss 0.528423.
Train: 2018-08-02T00:09:27.383590: step 20294, loss 0.460027.
Train: 2018-08-02T00:09:27.550148: step 20295, loss 0.511111.
Train: 2018-08-02T00:09:27.718663: step 20296, loss 0.596907.
Train: 2018-08-02T00:09:27.896220: step 20297, loss 0.597001.
Train: 2018-08-02T00:09:28.064738: step 20298, loss 0.545247.
Train: 2018-08-02T00:09:28.228300: step 20299, loss 0.458678.
Train: 2018-08-02T00:09:28.389893: step 20300, loss 0.527809.
Test: 2018-08-02T00:09:28.918457: step 20300, loss 0.547956.
Train: 2018-08-02T00:09:29.711056: step 20301, loss 0.440626.
Train: 2018-08-02T00:09:29.875617: step 20302, loss 0.597544.
Train: 2018-08-02T00:09:30.045170: step 20303, loss 0.580127.
Train: 2018-08-02T00:09:30.208734: step 20304, loss 0.580191.
Train: 2018-08-02T00:09:30.371301: step 20305, loss 0.597884.
Train: 2018-08-02T00:09:30.535826: step 20306, loss 0.509613.
Train: 2018-08-02T00:09:30.701409: step 20307, loss 0.580325.
Train: 2018-08-02T00:09:30.866967: step 20308, loss 0.633531.
Train: 2018-08-02T00:09:31.034494: step 20309, loss 0.63355.
Train: 2018-08-02T00:09:31.199079: step 20310, loss 0.56263.
Test: 2018-08-02T00:09:31.730632: step 20310, loss 0.547801.
Train: 2018-08-02T00:09:31.896190: step 20311, loss 0.651128.
Train: 2018-08-02T00:09:32.061774: step 20312, loss 0.580277.
Train: 2018-08-02T00:09:32.234287: step 20313, loss 0.738904.
Train: 2018-08-02T00:09:32.403865: step 20314, loss 0.580129.
Train: 2018-08-02T00:09:32.574377: step 20315, loss 0.562551.
Train: 2018-08-02T00:09:32.740932: step 20316, loss 0.475428.
Train: 2018-08-02T00:09:32.907518: step 20317, loss 0.66682.
Train: 2018-08-02T00:09:33.074073: step 20318, loss 0.631807.
Train: 2018-08-02T00:09:33.241619: step 20319, loss 0.562512.
Train: 2018-08-02T00:09:33.404190: step 20320, loss 0.596885.
Test: 2018-08-02T00:09:33.931749: step 20320, loss 0.548169.
Train: 2018-08-02T00:09:34.100321: step 20321, loss 0.494015.
Train: 2018-08-02T00:09:34.272869: step 20322, loss 0.733315.
Train: 2018-08-02T00:09:34.437430: step 20323, loss 0.511506.
Train: 2018-08-02T00:09:34.605977: step 20324, loss 0.579469.
Train: 2018-08-02T00:09:34.770506: step 20325, loss 0.680769.
Train: 2018-08-02T00:09:34.936063: step 20326, loss 0.512107.
Train: 2018-08-02T00:09:35.108633: step 20327, loss 0.529049.
Train: 2018-08-02T00:09:35.274192: step 20328, loss 0.56258.
Train: 2018-08-02T00:09:35.438752: step 20329, loss 0.579274.
Train: 2018-08-02T00:09:35.607270: step 20330, loss 0.479367.
Test: 2018-08-02T00:09:36.130871: step 20330, loss 0.548676.
Train: 2018-08-02T00:09:36.294432: step 20331, loss 0.562608.
Train: 2018-08-02T00:09:36.462015: step 20332, loss 0.479459.
Train: 2018-08-02T00:09:36.628570: step 20333, loss 0.51267.
Train: 2018-08-02T00:09:36.791106: step 20334, loss 0.495908.
Train: 2018-08-02T00:09:36.961674: step 20335, loss 0.696281.
Train: 2018-08-02T00:09:37.132193: step 20336, loss 0.579296.
Train: 2018-08-02T00:09:37.298747: step 20337, loss 0.596022.
Train: 2018-08-02T00:09:37.465334: step 20338, loss 0.629456.
Train: 2018-08-02T00:09:37.631883: step 20339, loss 0.495771.
Train: 2018-08-02T00:09:37.796443: step 20340, loss 0.529173.
Test: 2018-08-02T00:09:38.326002: step 20340, loss 0.54858.
Train: 2018-08-02T00:09:38.494552: step 20341, loss 0.612721.
Train: 2018-08-02T00:09:38.662104: step 20342, loss 0.579291.
Train: 2018-08-02T00:09:38.835639: step 20343, loss 0.629419.
Train: 2018-08-02T00:09:39.004189: step 20344, loss 0.629355.
Train: 2018-08-02T00:09:39.171741: step 20345, loss 0.495943.
Train: 2018-08-02T00:09:39.339327: step 20346, loss 0.512639.
Train: 2018-08-02T00:09:39.506845: step 20347, loss 0.495966.
Train: 2018-08-02T00:09:39.673401: step 20348, loss 0.52923.
Train: 2018-08-02T00:09:39.842977: step 20349, loss 0.562577.
Train: 2018-08-02T00:09:40.009535: step 20350, loss 0.596036.
Test: 2018-08-02T00:09:40.552065: step 20350, loss 0.548534.
Train: 2018-08-02T00:09:40.716612: step 20351, loss 0.395041.
Train: 2018-08-02T00:09:40.895164: step 20352, loss 0.428078.
Train: 2018-08-02T00:09:41.060702: step 20353, loss 0.63009.
Train: 2018-08-02T00:09:41.230238: step 20354, loss 0.562514.
Train: 2018-08-02T00:09:41.394823: step 20355, loss 0.494484.
Train: 2018-08-02T00:09:41.562351: step 20356, loss 0.460077.
Train: 2018-08-02T00:09:41.726911: step 20357, loss 0.545344.
Train: 2018-08-02T00:09:41.891496: step 20358, loss 0.631422.
Train: 2018-08-02T00:09:42.060054: step 20359, loss 0.510648.
Train: 2018-08-02T00:09:42.225608: step 20360, loss 0.527819.
Test: 2018-08-02T00:09:42.759152: step 20360, loss 0.547944.
Train: 2018-08-02T00:09:42.926703: step 20361, loss 0.545113.
Train: 2018-08-02T00:09:43.097281: step 20362, loss 0.492671.
Train: 2018-08-02T00:09:43.268789: step 20363, loss 0.474886.
Train: 2018-08-02T00:09:43.436365: step 20364, loss 0.544962.
Train: 2018-08-02T00:09:43.602926: step 20365, loss 0.474175.
Train: 2018-08-02T00:09:43.768484: step 20366, loss 0.633717.
Train: 2018-08-02T00:09:43.946009: step 20367, loss 0.633979.
Train: 2018-08-02T00:09:44.119515: step 20368, loss 0.669863.
Train: 2018-08-02T00:09:44.289062: step 20369, loss 0.616294.
Train: 2018-08-02T00:09:44.454645: step 20370, loss 0.544822.
Test: 2018-08-02T00:09:44.983206: step 20370, loss 0.547728.
Train: 2018-08-02T00:09:45.146795: step 20371, loss 0.509115.
Train: 2018-08-02T00:09:45.314351: step 20372, loss 0.544822.
Train: 2018-08-02T00:09:45.480875: step 20373, loss 0.50909.
Train: 2018-08-02T00:09:45.644464: step 20374, loss 0.580571.
Train: 2018-08-02T00:09:45.809029: step 20375, loss 0.687902.
Train: 2018-08-02T00:09:45.972592: step 20376, loss 0.59841.
Train: 2018-08-02T00:09:46.140113: step 20377, loss 0.509167.
Train: 2018-08-02T00:09:46.304672: step 20378, loss 0.544842.
Train: 2018-08-02T00:09:46.470230: step 20379, loss 0.509246.
Train: 2018-08-02T00:09:46.634827: step 20380, loss 0.491455.
Test: 2018-08-02T00:09:47.164406: step 20380, loss 0.54774.
Train: 2018-08-02T00:09:47.333951: step 20381, loss 0.562653.
Train: 2018-08-02T00:09:47.502492: step 20382, loss 0.562657.
Train: 2018-08-02T00:09:47.669060: step 20383, loss 0.562659.
Train: 2018-08-02T00:09:47.832619: step 20384, loss 0.598312.
Train: 2018-08-02T00:09:47.996185: step 20385, loss 0.509202.
Train: 2018-08-02T00:09:48.165698: step 20386, loss 0.616123.
Train: 2018-08-02T00:09:48.337240: step 20387, loss 0.527031.
Train: 2018-08-02T00:09:48.503819: step 20388, loss 0.651684.
Train: 2018-08-02T00:09:48.666394: step 20389, loss 0.615983.
Train: 2018-08-02T00:09:48.838927: step 20390, loss 0.562621.
Test: 2018-08-02T00:09:49.375464: step 20390, loss 0.547774.
Train: 2018-08-02T00:09:49.539063: step 20391, loss 0.633451.
Train: 2018-08-02T00:09:49.704585: step 20392, loss 0.509596.
Train: 2018-08-02T00:09:49.874131: step 20393, loss 0.456793.
Train: 2018-08-02T00:09:50.037694: step 20394, loss 0.597815.
Train: 2018-08-02T00:09:50.206277: step 20395, loss 0.474533.
Train: 2018-08-02T00:09:50.368839: step 20396, loss 0.492126.
Train: 2018-08-02T00:09:50.533397: step 20397, loss 0.615454.
Train: 2018-08-02T00:09:50.698951: step 20398, loss 0.439161.
Train: 2018-08-02T00:09:50.862522: step 20399, loss 0.5979.
Train: 2018-08-02T00:09:51.031069: step 20400, loss 0.668638.
Test: 2018-08-02T00:09:51.568627: step 20400, loss 0.547792.
Train: 2018-08-02T00:09:52.335936: step 20401, loss 0.54492.
Train: 2018-08-02T00:09:52.511498: step 20402, loss 0.597896.
Train: 2018-08-02T00:09:52.678022: step 20403, loss 0.509649.
Train: 2018-08-02T00:09:52.845574: step 20404, loss 0.509659.
Train: 2018-08-02T00:09:53.011169: step 20405, loss 0.615511.
Train: 2018-08-02T00:09:53.178683: step 20406, loss 0.580211.
Train: 2018-08-02T00:09:53.352244: step 20407, loss 0.527315.
Train: 2018-08-02T00:09:53.516780: step 20408, loss 0.527322.
Train: 2018-08-02T00:09:53.682370: step 20409, loss 0.580193.
Train: 2018-08-02T00:09:53.848893: step 20410, loss 0.439208.
Test: 2018-08-02T00:09:54.371527: step 20410, loss 0.547798.
Train: 2018-08-02T00:09:54.541041: step 20411, loss 0.544929.
Train: 2018-08-02T00:09:54.703635: step 20412, loss 0.491905.
Train: 2018-08-02T00:09:54.870162: step 20413, loss 0.544893.
Train: 2018-08-02T00:09:55.036717: step 20414, loss 0.722266.
Train: 2018-08-02T00:09:55.201309: step 20415, loss 0.615809.
Train: 2018-08-02T00:09:55.367857: step 20416, loss 0.438603.
Train: 2018-08-02T00:09:55.537379: step 20417, loss 0.598049.
Train: 2018-08-02T00:09:55.698947: step 20418, loss 0.474.
Train: 2018-08-02T00:09:55.865525: step 20419, loss 0.491662.
Train: 2018-08-02T00:09:56.029063: step 20420, loss 0.562623.
Test: 2018-08-02T00:09:56.563633: step 20420, loss 0.547737.
Train: 2018-08-02T00:09:56.732184: step 20421, loss 0.580423.
Train: 2018-08-02T00:09:56.896775: step 20422, loss 0.527031.
Train: 2018-08-02T00:09:57.061304: step 20423, loss 0.544826.
Train: 2018-08-02T00:09:57.225895: step 20424, loss 0.66973.
Train: 2018-08-02T00:09:57.407436: step 20425, loss 0.509145.
Train: 2018-08-02T00:09:57.572936: step 20426, loss 0.491298.
Train: 2018-08-02T00:09:57.740489: step 20427, loss 0.526954.
Train: 2018-08-02T00:09:57.903078: step 20428, loss 0.616307.
Train: 2018-08-02T00:09:58.082574: step 20429, loss 0.509036.
Train: 2018-08-02T00:09:58.261127: step 20430, loss 0.562685.
Test: 2018-08-02T00:09:58.795668: step 20430, loss 0.547698.
Train: 2018-08-02T00:09:58.967241: step 20431, loss 0.562683.
Train: 2018-08-02T00:09:59.133765: step 20432, loss 0.580645.
Train: 2018-08-02T00:09:59.314281: step 20433, loss 0.509008.
Train: 2018-08-02T00:09:59.481834: step 20434, loss 0.52684.
Train: 2018-08-02T00:09:59.655369: step 20435, loss 0.634466.
Train: 2018-08-02T00:09:59.817937: step 20436, loss 0.454985.
Train: 2018-08-02T00:09:59.981528: step 20437, loss 0.598996.
Train: 2018-08-02T00:10:00.148068: step 20438, loss 0.509471.
Train: 2018-08-02T00:10:00.322617: step 20439, loss 0.598709.
Train: 2018-08-02T00:10:00.489141: step 20440, loss 0.508863.
Test: 2018-08-02T00:10:01.023716: step 20440, loss 0.547699.
Train: 2018-08-02T00:10:01.191263: step 20441, loss 0.526818.
Train: 2018-08-02T00:10:01.357819: step 20442, loss 0.562733.
Train: 2018-08-02T00:10:01.538360: step 20443, loss 0.526623.
Train: 2018-08-02T00:10:01.703926: step 20444, loss 0.669817.
Train: 2018-08-02T00:10:01.867456: step 20445, loss 0.58169.
Train: 2018-08-02T00:10:02.031019: step 20446, loss 0.507212.
Train: 2018-08-02T00:10:02.197573: step 20447, loss 0.702534.
Train: 2018-08-02T00:10:02.364160: step 20448, loss 0.512109.
Train: 2018-08-02T00:10:02.537667: step 20449, loss 0.650138.
Train: 2018-08-02T00:10:02.702250: step 20450, loss 0.574439.
Test: 2018-08-02T00:10:03.228845: step 20450, loss 0.55075.
Train: 2018-08-02T00:10:03.395401: step 20451, loss 0.550264.
Train: 2018-08-02T00:10:03.592037: step 20452, loss 0.485513.
Train: 2018-08-02T00:10:03.755600: step 20453, loss 0.551207.
Train: 2018-08-02T00:10:03.924180: step 20454, loss 0.544873.
Train: 2018-08-02T00:10:04.090748: step 20455, loss 0.58216.
Train: 2018-08-02T00:10:04.267258: step 20456, loss 0.564055.
Train: 2018-08-02T00:10:04.432789: step 20457, loss 0.631547.
Train: 2018-08-02T00:10:04.607358: step 20458, loss 0.509319.
Train: 2018-08-02T00:10:04.770916: step 20459, loss 0.633645.
Train: 2018-08-02T00:10:04.934479: step 20460, loss 0.527713.
Test: 2018-08-02T00:10:05.475004: step 20460, loss 0.547845.
Train: 2018-08-02T00:10:05.645547: step 20461, loss 0.668254.
Train: 2018-08-02T00:10:05.809145: step 20462, loss 0.616025.
Train: 2018-08-02T00:10:05.972674: step 20463, loss 0.528928.
Train: 2018-08-02T00:10:06.139283: step 20464, loss 0.473532.
Train: 2018-08-02T00:10:06.304785: step 20465, loss 0.579309.
Train: 2018-08-02T00:10:06.477355: step 20466, loss 0.68642.
Train: 2018-08-02T00:10:06.645916: step 20467, loss 0.562866.
Train: 2018-08-02T00:10:06.808472: step 20468, loss 0.486569.
Train: 2018-08-02T00:10:06.973023: step 20469, loss 0.476789.
Train: 2018-08-02T00:10:07.138584: step 20470, loss 0.651117.
Test: 2018-08-02T00:10:07.673128: step 20470, loss 0.547923.
Train: 2018-08-02T00:10:07.838684: step 20471, loss 0.490537.
Train: 2018-08-02T00:10:08.002247: step 20472, loss 0.526827.
Train: 2018-08-02T00:10:08.162817: step 20473, loss 0.615364.
Train: 2018-08-02T00:10:08.333363: step 20474, loss 0.545769.
Train: 2018-08-02T00:10:08.499950: step 20475, loss 0.543535.
Train: 2018-08-02T00:10:08.677442: step 20476, loss 0.522895.
Train: 2018-08-02T00:10:08.841040: step 20477, loss 0.614434.
Train: 2018-08-02T00:10:09.004603: step 20478, loss 0.587698.
Train: 2018-08-02T00:10:09.168131: step 20479, loss 0.563147.
Train: 2018-08-02T00:10:09.340669: step 20480, loss 0.512389.
Test: 2018-08-02T00:10:09.864303: step 20480, loss 0.547927.
Train: 2018-08-02T00:10:10.027868: step 20481, loss 0.629817.
Train: 2018-08-02T00:10:10.196382: step 20482, loss 0.542082.
Train: 2018-08-02T00:10:10.362967: step 20483, loss 0.543472.
Train: 2018-08-02T00:10:10.526526: step 20484, loss 0.619616.
Train: 2018-08-02T00:10:10.691060: step 20485, loss 0.602096.
Train: 2018-08-02T00:10:10.855620: step 20486, loss 0.566348.
Train: 2018-08-02T00:10:11.022175: step 20487, loss 0.509788.
Train: 2018-08-02T00:10:11.184739: step 20488, loss 0.563471.
Train: 2018-08-02T00:10:11.354311: step 20489, loss 0.512827.
Train: 2018-08-02T00:10:11.517875: step 20490, loss 0.558849.
Test: 2018-08-02T00:10:12.048436: step 20490, loss 0.548098.
Train: 2018-08-02T00:10:12.210996: step 20491, loss 0.629051.
Train: 2018-08-02T00:10:12.387524: step 20492, loss 0.563045.
Train: 2018-08-02T00:10:12.555077: step 20493, loss 0.561315.
Train: 2018-08-02T00:10:12.714650: step 20494, loss 0.494411.
Train: 2018-08-02T00:10:12.877240: step 20495, loss 0.651383.
Train: 2018-08-02T00:10:13.039814: step 20496, loss 0.650928.
Train: 2018-08-02T00:10:13.205373: step 20497, loss 0.684178.
Train: 2018-08-02T00:10:13.369928: step 20498, loss 0.594938.
Train: 2018-08-02T00:10:13.538479: step 20499, loss 0.56258.
Train: 2018-08-02T00:10:13.704006: step 20500, loss 0.512432.
Test: 2018-08-02T00:10:14.233588: step 20500, loss 0.548545.
Train: 2018-08-02T00:10:15.001149: step 20501, loss 0.495929.
Train: 2018-08-02T00:10:15.164685: step 20502, loss 0.495981.
Train: 2018-08-02T00:10:15.328248: step 20503, loss 0.530495.
Train: 2018-08-02T00:10:15.490828: step 20504, loss 0.528244.
Train: 2018-08-02T00:10:15.655373: step 20505, loss 0.529678.
Train: 2018-08-02T00:10:15.821929: step 20506, loss 0.545602.
Train: 2018-08-02T00:10:15.982526: step 20507, loss 0.528114.
Train: 2018-08-02T00:10:16.148077: step 20508, loss 0.442173.
Train: 2018-08-02T00:10:16.310623: step 20509, loss 0.545364.
Train: 2018-08-02T00:10:16.473218: step 20510, loss 0.545266.
Test: 2018-08-02T00:10:17.019736: step 20510, loss 0.547995.
Train: 2018-08-02T00:10:17.184286: step 20511, loss 0.510116.
Train: 2018-08-02T00:10:17.349845: step 20512, loss 0.527662.
Train: 2018-08-02T00:10:17.515402: step 20513, loss 0.614495.
Train: 2018-08-02T00:10:17.682972: step 20514, loss 0.545028.
Train: 2018-08-02T00:10:17.852500: step 20515, loss 0.580526.
Train: 2018-08-02T00:10:18.014100: step 20516, loss 0.580129.
Train: 2018-08-02T00:10:18.182618: step 20517, loss 0.615799.
Train: 2018-08-02T00:10:18.344186: step 20518, loss 0.544356.
Train: 2018-08-02T00:10:18.511738: step 20519, loss 0.597992.
Train: 2018-08-02T00:10:18.677296: step 20520, loss 0.527674.
Test: 2018-08-02T00:10:19.215870: step 20520, loss 0.54782.
Train: 2018-08-02T00:10:19.379444: step 20521, loss 0.544963.
Train: 2018-08-02T00:10:19.543978: step 20522, loss 0.527925.
Train: 2018-08-02T00:10:19.717547: step 20523, loss 0.632942.
Train: 2018-08-02T00:10:19.884070: step 20524, loss 0.52742.
Train: 2018-08-02T00:10:20.059600: step 20525, loss 0.616136.
Train: 2018-08-02T00:10:20.224161: step 20526, loss 0.598002.
Train: 2018-08-02T00:10:20.403680: step 20527, loss 0.509898.
Train: 2018-08-02T00:10:20.578244: step 20528, loss 0.615915.
Train: 2018-08-02T00:10:20.741785: step 20529, loss 0.615375.
Train: 2018-08-02T00:10:20.909329: step 20530, loss 0.509988.
Test: 2018-08-02T00:10:21.446894: step 20530, loss 0.5479.
Train: 2018-08-02T00:10:21.616463: step 20531, loss 0.475033.
Train: 2018-08-02T00:10:21.781026: step 20532, loss 0.545072.
Train: 2018-08-02T00:10:21.952541: step 20533, loss 0.527509.
Train: 2018-08-02T00:10:22.128070: step 20534, loss 0.597614.
Train: 2018-08-02T00:10:22.287676: step 20535, loss 0.562559.
Train: 2018-08-02T00:10:22.452235: step 20536, loss 0.597605.
Train: 2018-08-02T00:10:22.616763: step 20537, loss 0.650123.
Train: 2018-08-02T00:10:22.789328: step 20538, loss 0.562545.
Train: 2018-08-02T00:10:22.956854: step 20539, loss 0.475253.
Train: 2018-08-02T00:10:23.124407: step 20540, loss 0.579987.
Test: 2018-08-02T00:10:23.650003: step 20540, loss 0.547927.
Train: 2018-08-02T00:10:23.813591: step 20541, loss 0.492768.
Train: 2018-08-02T00:10:23.983112: step 20542, loss 0.562532.
Train: 2018-08-02T00:10:24.145677: step 20543, loss 0.579984.
Train: 2018-08-02T00:10:24.309240: step 20544, loss 0.597432.
Train: 2018-08-02T00:10:24.471805: step 20545, loss 0.492764.
Train: 2018-08-02T00:10:24.640355: step 20546, loss 0.579976.
Train: 2018-08-02T00:10:24.803948: step 20547, loss 0.579976.
Train: 2018-08-02T00:10:24.972495: step 20548, loss 0.632302.
Train: 2018-08-02T00:10:25.138024: step 20549, loss 0.579944.
Train: 2018-08-02T00:10:25.307595: step 20550, loss 0.475513.
Test: 2018-08-02T00:10:25.831172: step 20550, loss 0.547945.
Train: 2018-08-02T00:10:25.996749: step 20551, loss 0.510318.
Train: 2018-08-02T00:10:26.160336: step 20552, loss 0.458065.
Train: 2018-08-02T00:10:26.331833: step 20553, loss 0.492761.
Train: 2018-08-02T00:10:26.499410: step 20554, loss 0.545047.
Train: 2018-08-02T00:10:26.665940: step 20555, loss 0.562541.
Train: 2018-08-02T00:10:26.833493: step 20556, loss 0.562551.
Train: 2018-08-02T00:10:27.000057: step 20557, loss 0.4394.
Train: 2018-08-02T00:10:27.170590: step 20558, loss 0.615528.
Train: 2018-08-02T00:10:27.337178: step 20559, loss 0.49185.
Train: 2018-08-02T00:10:27.507720: step 20560, loss 0.598077.
Test: 2018-08-02T00:10:28.036283: step 20560, loss 0.54775.
Train: 2018-08-02T00:10:28.213832: step 20561, loss 0.473801.
Train: 2018-08-02T00:10:28.383379: step 20562, loss 0.705145.
Train: 2018-08-02T00:10:28.555918: step 20563, loss 0.491368.
Train: 2018-08-02T00:10:28.720482: step 20564, loss 0.580505.
Train: 2018-08-02T00:10:28.885007: step 20565, loss 0.544813.
Train: 2018-08-02T00:10:29.050566: step 20566, loss 0.526935.
Train: 2018-08-02T00:10:29.217153: step 20567, loss 0.562685.
Train: 2018-08-02T00:10:29.380682: step 20568, loss 0.616402.
Train: 2018-08-02T00:10:29.543279: step 20569, loss 0.491084.
Train: 2018-08-02T00:10:29.707808: step 20570, loss 0.526869.
Test: 2018-08-02T00:10:30.248380: step 20570, loss 0.547692.
Train: 2018-08-02T00:10:30.414918: step 20571, loss 0.508913.
Train: 2018-08-02T00:10:30.579478: step 20572, loss 0.616591.
Train: 2018-08-02T00:10:30.751050: step 20573, loss 0.526798.
Train: 2018-08-02T00:10:30.931537: step 20574, loss 0.508803.
Train: 2018-08-02T00:10:31.097094: step 20575, loss 0.562746.
Train: 2018-08-02T00:10:31.261655: step 20576, loss 0.490699.
Train: 2018-08-02T00:10:31.425217: step 20577, loss 0.508648.
Train: 2018-08-02T00:10:31.597774: step 20578, loss 0.580874.
Train: 2018-08-02T00:10:31.761318: step 20579, loss 0.436108.
Train: 2018-08-02T00:10:31.926875: step 20580, loss 0.490258.
Test: 2018-08-02T00:10:32.465444: step 20580, loss 0.547647.
Train: 2018-08-02T00:10:32.630993: step 20581, loss 0.744914.
Train: 2018-08-02T00:10:32.798576: step 20582, loss 0.490058.
Train: 2018-08-02T00:10:32.962108: step 20583, loss 0.562909.
Train: 2018-08-02T00:10:33.144621: step 20584, loss 0.471713.
Train: 2018-08-02T00:10:33.308184: step 20585, loss 0.745643.
Train: 2018-08-02T00:10:33.474757: step 20586, loss 0.745459.
Train: 2018-08-02T00:10:33.641324: step 20587, loss 0.4901.
Train: 2018-08-02T00:10:33.806876: step 20588, loss 0.490226.
Train: 2018-08-02T00:10:33.972440: step 20589, loss 0.363363.
Train: 2018-08-02T00:10:34.137996: step 20590, loss 0.508388.
Test: 2018-08-02T00:10:34.671539: step 20590, loss 0.547644.
Train: 2018-08-02T00:10:34.858286: step 20591, loss 0.671948.
Train: 2018-08-02T00:10:35.022814: step 20592, loss 0.52651.
Train: 2018-08-02T00:10:35.194357: step 20593, loss 0.526507.
Train: 2018-08-02T00:10:35.357953: step 20594, loss 0.562872.
Train: 2018-08-02T00:10:35.519530: step 20595, loss 0.435541.
Train: 2018-08-02T00:10:35.683051: step 20596, loss 0.599331.
Train: 2018-08-02T00:10:35.847642: step 20597, loss 0.47175.
Train: 2018-08-02T00:10:36.018155: step 20598, loss 0.52641.
Train: 2018-08-02T00:10:36.181753: step 20599, loss 0.562955.
Train: 2018-08-02T00:10:36.351264: step 20600, loss 0.562975.
Test: 2018-08-02T00:10:36.872870: step 20600, loss 0.54764.
Train: 2018-08-02T00:10:37.633402: step 20601, loss 0.544658.
Train: 2018-08-02T00:10:37.797936: step 20602, loss 0.526307.
Train: 2018-08-02T00:10:37.961530: step 20603, loss 0.654865.
Train: 2018-08-02T00:10:38.126091: step 20604, loss 0.618106.
Train: 2018-08-02T00:10:38.298599: step 20605, loss 0.581341.
Train: 2018-08-02T00:10:38.474130: step 20606, loss 0.581292.
Train: 2018-08-02T00:10:38.651656: step 20607, loss 0.709238.
Train: 2018-08-02T00:10:38.816214: step 20608, loss 0.599346.
Train: 2018-08-02T00:10:38.982770: step 20609, loss 0.490214.
Train: 2018-08-02T00:10:39.150325: step 20610, loss 0.580919.
Test: 2018-08-02T00:10:39.686886: step 20610, loss 0.54765.
Train: 2018-08-02T00:10:39.851448: step 20611, loss 0.544713.
Train: 2018-08-02T00:10:40.028998: step 20612, loss 0.580762.
Train: 2018-08-02T00:10:40.196525: step 20613, loss 0.562716.
Train: 2018-08-02T00:10:40.363079: step 20614, loss 0.616476.
Train: 2018-08-02T00:10:40.523651: step 20615, loss 0.491156.
Train: 2018-08-02T00:10:40.685218: step 20616, loss 0.616165.
Train: 2018-08-02T00:10:40.846787: step 20617, loss 0.580415.
Train: 2018-08-02T00:10:41.016362: step 20618, loss 0.527093.
Train: 2018-08-02T00:10:41.179896: step 20619, loss 0.633444.
Train: 2018-08-02T00:10:41.345453: step 20620, loss 0.456559.
Test: 2018-08-02T00:10:41.871083: step 20620, loss 0.547776.
Train: 2018-08-02T00:10:42.035608: step 20621, loss 0.562551.
Train: 2018-08-02T00:10:42.205155: step 20622, loss 0.59779.
Train: 2018-08-02T00:10:42.373704: step 20623, loss 0.474559.
Train: 2018-08-02T00:10:42.536270: step 20624, loss 0.439419.
Train: 2018-08-02T00:10:42.713821: step 20625, loss 0.632963.
Train: 2018-08-02T00:10:42.879378: step 20626, loss 0.685787.
Train: 2018-08-02T00:10:43.043914: step 20627, loss 0.50979.
Train: 2018-08-02T00:10:43.206479: step 20628, loss 0.50983.
Train: 2018-08-02T00:10:43.381012: step 20629, loss 0.615204.
Train: 2018-08-02T00:10:43.554573: step 20630, loss 0.597607.
Test: 2018-08-02T00:10:44.102085: step 20630, loss 0.547836.
Train: 2018-08-02T00:10:44.271662: step 20631, loss 0.544986.
Train: 2018-08-02T00:10:44.435225: step 20632, loss 0.597513.
Train: 2018-08-02T00:10:44.598759: step 20633, loss 0.579977.
Train: 2018-08-02T00:10:44.761322: step 20634, loss 0.6323.
Train: 2018-08-02T00:10:44.927877: step 20635, loss 0.492836.
Train: 2018-08-02T00:10:45.093443: step 20636, loss 0.51031.
Train: 2018-08-02T00:10:45.257028: step 20637, loss 0.579856.
Train: 2018-08-02T00:10:45.423552: step 20638, loss 0.562473.
Train: 2018-08-02T00:10:45.593129: step 20639, loss 0.579826.
Train: 2018-08-02T00:10:45.769658: step 20640, loss 0.61449.
Test: 2018-08-02T00:10:46.295221: step 20640, loss 0.547966.
Train: 2018-08-02T00:10:46.462775: step 20641, loss 0.579779.
Train: 2018-08-02T00:10:46.628357: step 20642, loss 0.527888.
Train: 2018-08-02T00:10:46.793889: step 20643, loss 0.527922.
Train: 2018-08-02T00:10:46.966428: step 20644, loss 0.579719.
Train: 2018-08-02T00:10:47.129024: step 20645, loss 0.52796.
Train: 2018-08-02T00:10:47.298562: step 20646, loss 0.683177.
Train: 2018-08-02T00:10:47.467090: step 20647, loss 0.751824.
Train: 2018-08-02T00:10:47.628701: step 20648, loss 0.665324.
Train: 2018-08-02T00:10:47.794245: step 20649, loss 0.494224.
Train: 2018-08-02T00:10:47.957778: step 20650, loss 0.545467.
Test: 2018-08-02T00:10:48.486371: step 20650, loss 0.548281.
Train: 2018-08-02T00:10:48.649927: step 20651, loss 0.66414.
Train: 2018-08-02T00:10:48.812492: step 20652, loss 0.49498.
Train: 2018-08-02T00:10:48.978081: step 20653, loss 0.528833.
Train: 2018-08-02T00:10:49.148595: step 20654, loss 0.629694.
Train: 2018-08-02T00:10:49.312156: step 20655, loss 0.596023.
Train: 2018-08-02T00:10:49.480707: step 20656, loss 0.646072.
Train: 2018-08-02T00:10:49.645267: step 20657, loss 0.645797.
Train: 2018-08-02T00:10:49.810824: step 20658, loss 0.628892.
Train: 2018-08-02T00:10:49.974417: step 20659, loss 0.463621.
Train: 2018-08-02T00:10:50.139970: step 20660, loss 0.644916.
Test: 2018-08-02T00:10:50.677516: step 20660, loss 0.548944.
Train: 2018-08-02T00:10:50.844092: step 20661, loss 0.579072.
Train: 2018-08-02T00:10:51.023581: step 20662, loss 0.513676.
Train: 2018-08-02T00:10:51.189139: step 20663, loss 0.595348.
Train: 2018-08-02T00:10:51.353725: step 20664, loss 0.660395.
Train: 2018-08-02T00:10:51.518259: step 20665, loss 0.579011.
Train: 2018-08-02T00:10:51.682834: step 20666, loss 0.578999.
Train: 2018-08-02T00:10:51.849374: step 20667, loss 0.433843.
Train: 2018-08-02T00:10:52.013965: step 20668, loss 0.546745.
Train: 2018-08-02T00:10:52.176530: step 20669, loss 0.578988.
Train: 2018-08-02T00:10:52.341060: step 20670, loss 0.578988.
Test: 2018-08-02T00:10:52.881616: step 20670, loss 0.549362.
Train: 2018-08-02T00:10:53.045208: step 20671, loss 0.578988.
Train: 2018-08-02T00:10:53.211731: step 20672, loss 0.627356.
Train: 2018-08-02T00:10:53.390256: step 20673, loss 0.433998.
Train: 2018-08-02T00:10:53.555846: step 20674, loss 0.578989.
Train: 2018-08-02T00:10:53.725359: step 20675, loss 0.514387.
Train: 2018-08-02T00:10:53.889919: step 20676, loss 0.498082.
Train: 2018-08-02T00:10:54.054479: step 20677, loss 0.660167.
Train: 2018-08-02T00:10:54.225055: step 20678, loss 0.562765.
Train: 2018-08-02T00:10:54.389584: step 20679, loss 0.562749.
Train: 2018-08-02T00:10:54.553147: step 20680, loss 0.660527.
Test: 2018-08-02T00:10:55.086719: step 20680, loss 0.549084.
Train: 2018-08-02T00:10:55.261253: step 20681, loss 0.497537.
Train: 2018-08-02T00:10:55.425814: step 20682, loss 0.579038.
Train: 2018-08-02T00:10:55.591371: step 20683, loss 0.595375.
Train: 2018-08-02T00:10:55.758922: step 20684, loss 0.546365.
Train: 2018-08-02T00:10:55.920491: step 20685, loss 0.644468.
Train: 2018-08-02T00:10:56.084054: step 20686, loss 0.595399.
Train: 2018-08-02T00:10:56.247645: step 20687, loss 0.464664.
Train: 2018-08-02T00:10:56.411211: step 20688, loss 0.660839.
Train: 2018-08-02T00:10:56.582721: step 20689, loss 0.546345.
Train: 2018-08-02T00:10:56.752267: step 20690, loss 0.611757.
Test: 2018-08-02T00:10:57.284844: step 20690, loss 0.549013.
Train: 2018-08-02T00:10:57.455387: step 20691, loss 0.480972.
Train: 2018-08-02T00:10:57.623938: step 20692, loss 0.51361.
Train: 2018-08-02T00:10:57.792517: step 20693, loss 0.480735.
Train: 2018-08-02T00:10:57.962064: step 20694, loss 0.61195.
Train: 2018-08-02T00:10:58.123602: step 20695, loss 0.612029.
Train: 2018-08-02T00:10:58.289159: step 20696, loss 0.529648.
Train: 2018-08-02T00:10:58.448731: step 20697, loss 0.529579.
Train: 2018-08-02T00:10:58.613292: step 20698, loss 0.562589.
Train: 2018-08-02T00:10:58.780844: step 20699, loss 0.562573.
Train: 2018-08-02T00:10:58.958401: step 20700, loss 0.678838.
Test: 2018-08-02T00:10:59.505906: step 20700, loss 0.548647.
Train: 2018-08-02T00:11:00.286485: step 20701, loss 0.595786.
Train: 2018-08-02T00:11:00.453037: step 20702, loss 0.56256.
Train: 2018-08-02T00:11:00.629593: step 20703, loss 0.529345.
Train: 2018-08-02T00:11:00.803102: step 20704, loss 0.595789.
Train: 2018-08-02T00:11:00.967665: step 20705, loss 0.479474.
Train: 2018-08-02T00:11:01.134247: step 20706, loss 0.595825.
Train: 2018-08-02T00:11:01.304762: step 20707, loss 0.512587.
Train: 2018-08-02T00:11:01.469322: step 20708, loss 0.629241.
Train: 2018-08-02T00:11:01.630890: step 20709, loss 0.545843.
Train: 2018-08-02T00:11:01.805423: step 20710, loss 0.595922.
Test: 2018-08-02T00:11:02.348970: step 20710, loss 0.548534.
Train: 2018-08-02T00:11:02.517545: step 20711, loss 0.562522.
Train: 2018-08-02T00:11:02.683076: step 20712, loss 0.595941.
Train: 2018-08-02T00:11:02.851625: step 20713, loss 0.495682.
Train: 2018-08-02T00:11:03.017183: step 20714, loss 0.595966.
Train: 2018-08-02T00:11:03.188725: step 20715, loss 0.595979.
Train: 2018-08-02T00:11:03.351290: step 20716, loss 0.729856.
Train: 2018-08-02T00:11:03.516847: step 20717, loss 0.595919.
Train: 2018-08-02T00:11:03.679439: step 20718, loss 0.595854.
Train: 2018-08-02T00:11:03.843974: step 20719, loss 0.629019.
Train: 2018-08-02T00:11:04.007537: step 20720, loss 0.49632.
Test: 2018-08-02T00:11:04.552081: step 20720, loss 0.548745.
Train: 2018-08-02T00:11:04.719633: step 20721, loss 0.661803.
Train: 2018-08-02T00:11:04.885189: step 20722, loss 0.546127.
Train: 2018-08-02T00:11:05.049749: step 20723, loss 0.562635.
Train: 2018-08-02T00:11:05.213312: step 20724, loss 0.61191.
Train: 2018-08-02T00:11:05.390863: step 20725, loss 0.61182.
Train: 2018-08-02T00:11:05.559387: step 20726, loss 0.530023.
Train: 2018-08-02T00:11:05.731926: step 20727, loss 0.481167.
Train: 2018-08-02T00:11:05.899479: step 20728, loss 0.464881.
Train: 2018-08-02T00:11:06.062043: step 20729, loss 0.530048.
Train: 2018-08-02T00:11:06.237605: step 20730, loss 0.628126.
Test: 2018-08-02T00:11:06.763177: step 20730, loss 0.548966.
Train: 2018-08-02T00:11:06.932716: step 20731, loss 0.562678.
Train: 2018-08-02T00:11:07.097290: step 20732, loss 0.464332.
Train: 2018-08-02T00:11:07.261867: step 20733, loss 0.49693.
Train: 2018-08-02T00:11:07.428392: step 20734, loss 0.496688.
Train: 2018-08-02T00:11:07.598936: step 20735, loss 0.512942.
Train: 2018-08-02T00:11:07.763494: step 20736, loss 0.695509.
Train: 2018-08-02T00:11:07.928055: step 20737, loss 0.579192.
Train: 2018-08-02T00:11:08.094610: step 20738, loss 0.595896.
Train: 2018-08-02T00:11:08.258204: step 20739, loss 0.562518.
Train: 2018-08-02T00:11:08.424728: step 20740, loss 0.495611.
Test: 2018-08-02T00:11:08.956307: step 20740, loss 0.548466.
Train: 2018-08-02T00:11:09.121864: step 20741, loss 0.57926.
Train: 2018-08-02T00:11:09.287421: step 20742, loss 0.57928.
Train: 2018-08-02T00:11:09.453005: step 20743, loss 0.579296.
Train: 2018-08-02T00:11:09.616542: step 20744, loss 0.511995.
Train: 2018-08-02T00:11:09.782130: step 20745, loss 0.545617.
Train: 2018-08-02T00:11:09.953641: step 20746, loss 0.461142.
Train: 2018-08-02T00:11:10.118201: step 20747, loss 0.562457.
Train: 2018-08-02T00:11:10.278772: step 20748, loss 0.528478.
Train: 2018-08-02T00:11:10.440340: step 20749, loss 0.477261.
Train: 2018-08-02T00:11:10.609918: step 20750, loss 0.545338.
Test: 2018-08-02T00:11:11.142462: step 20750, loss 0.548065.
Train: 2018-08-02T00:11:11.307022: step 20751, loss 0.631098.
Train: 2018-08-02T00:11:11.473577: step 20752, loss 0.665674.
Train: 2018-08-02T00:11:11.637140: step 20753, loss 0.631317.
Train: 2018-08-02T00:11:11.801706: step 20754, loss 0.596871.
Train: 2018-08-02T00:11:11.968255: step 20755, loss 0.476417.
Train: 2018-08-02T00:11:12.135807: step 20756, loss 0.596866.
Train: 2018-08-02T00:11:12.303360: step 20757, loss 0.528011.
Train: 2018-08-02T00:11:12.476926: step 20758, loss 0.493552.
Train: 2018-08-02T00:11:12.649434: step 20759, loss 0.648662.
Train: 2018-08-02T00:11:12.815989: step 20760, loss 0.63142.
Test: 2018-08-02T00:11:13.336597: step 20760, loss 0.548012.
Train: 2018-08-02T00:11:13.501157: step 20761, loss 0.579669.
Train: 2018-08-02T00:11:13.668709: step 20762, loss 0.596862.
Train: 2018-08-02T00:11:13.830304: step 20763, loss 0.54525.
Train: 2018-08-02T00:11:13.998826: step 20764, loss 0.613947.
Train: 2018-08-02T00:11:14.169371: step 20765, loss 0.511011.
Train: 2018-08-02T00:11:14.333931: step 20766, loss 0.493922.
Train: 2018-08-02T00:11:14.495524: step 20767, loss 0.528172.
Train: 2018-08-02T00:11:14.662084: step 20768, loss 0.596724.
Train: 2018-08-02T00:11:14.824618: step 20769, loss 0.416125.
Train: 2018-08-02T00:11:14.992171: step 20770, loss 0.61398.
Test: 2018-08-02T00:11:15.520788: step 20770, loss 0.548033.
Train: 2018-08-02T00:11:15.699311: step 20771, loss 0.510834.
Train: 2018-08-02T00:11:15.861875: step 20772, loss 0.493516.
Train: 2018-08-02T00:11:16.025409: step 20773, loss 0.545169.
Train: 2018-08-02T00:11:16.193958: step 20774, loss 0.527818.
Train: 2018-08-02T00:11:16.371508: step 20775, loss 0.475662.
Train: 2018-08-02T00:11:16.534060: step 20776, loss 0.614712.
Train: 2018-08-02T00:11:16.701629: step 20777, loss 0.56247.
Train: 2018-08-02T00:11:16.869154: step 20778, loss 0.70242.
Train: 2018-08-02T00:11:17.032716: step 20779, loss 0.632449.
Train: 2018-08-02T00:11:17.198300: step 20780, loss 0.49257.
Test: 2018-08-02T00:11:17.744812: step 20780, loss 0.54784.
Train: 2018-08-02T00:11:17.915387: step 20781, loss 0.45762.
Train: 2018-08-02T00:11:18.082940: step 20782, loss 0.527485.
Train: 2018-08-02T00:11:18.251505: step 20783, loss 0.615059.
Train: 2018-08-02T00:11:18.414023: step 20784, loss 0.59756.
Train: 2018-08-02T00:11:18.581576: step 20785, loss 0.650172.
Train: 2018-08-02T00:11:18.752120: step 20786, loss 0.632553.
Train: 2018-08-02T00:11:18.916679: step 20787, loss 0.597441.
Train: 2018-08-02T00:11:19.081240: step 20788, loss 0.562466.
Train: 2018-08-02T00:11:19.254776: step 20789, loss 0.545051.
Train: 2018-08-02T00:11:19.425346: step 20790, loss 0.684098.
Test: 2018-08-02T00:11:19.963880: step 20790, loss 0.547939.
Train: 2018-08-02T00:11:20.133428: step 20791, loss 0.545121.
Train: 2018-08-02T00:11:20.307989: step 20792, loss 0.683386.
Train: 2018-08-02T00:11:20.473517: step 20793, loss 0.493596.
Train: 2018-08-02T00:11:20.649080: step 20794, loss 0.49378.
Train: 2018-08-02T00:11:20.812642: step 20795, loss 0.562434.
Train: 2018-08-02T00:11:20.980195: step 20796, loss 0.528207.
Train: 2018-08-02T00:11:21.143758: step 20797, loss 0.630838.
Train: 2018-08-02T00:11:21.319282: step 20798, loss 0.562436.
Train: 2018-08-02T00:11:21.481854: step 20799, loss 0.596537.
Train: 2018-08-02T00:11:21.648377: step 20800, loss 0.528401.
Test: 2018-08-02T00:11:22.179956: step 20800, loss 0.548206.
Train: 2018-08-02T00:11:22.979048: step 20801, loss 0.511437.
Train: 2018-08-02T00:11:23.151587: step 20802, loss 0.613436.
Train: 2018-08-02T00:11:23.334068: step 20803, loss 0.52848.
Train: 2018-08-02T00:11:23.496659: step 20804, loss 0.545468.
Train: 2018-08-02T00:11:23.663191: step 20805, loss 0.579422.
Train: 2018-08-02T00:11:23.830772: step 20806, loss 0.630339.
Train: 2018-08-02T00:11:24.003286: step 20807, loss 0.715044.
Train: 2018-08-02T00:11:24.171860: step 20808, loss 0.596266.
Train: 2018-08-02T00:11:24.338408: step 20809, loss 0.579321.
Train: 2018-08-02T00:11:24.500950: step 20810, loss 0.562481.
Test: 2018-08-02T00:11:25.028547: step 20810, loss 0.548462.
Train: 2018-08-02T00:11:25.194142: step 20811, loss 0.528979.
Train: 2018-08-02T00:11:25.376607: step 20812, loss 0.595957.
Train: 2018-08-02T00:11:25.557125: step 20813, loss 0.495746.
Train: 2018-08-02T00:11:25.723679: step 20814, loss 0.579199.
Train: 2018-08-02T00:11:25.898213: step 20815, loss 0.479209.
Train: 2018-08-02T00:11:26.065796: step 20816, loss 0.545852.
Train: 2018-08-02T00:11:26.228333: step 20817, loss 0.51247.
Train: 2018-08-02T00:11:26.394916: step 20818, loss 0.51239.
Train: 2018-08-02T00:11:26.557450: step 20819, loss 0.612722.
Train: 2018-08-02T00:11:26.722036: step 20820, loss 0.612778.
Test: 2018-08-02T00:11:27.258576: step 20820, loss 0.548447.
Train: 2018-08-02T00:11:27.427126: step 20821, loss 0.54572.
Train: 2018-08-02T00:11:27.591711: step 20822, loss 0.629615.
Train: 2018-08-02T00:11:27.759238: step 20823, loss 0.629603.
Train: 2018-08-02T00:11:27.925792: step 20824, loss 0.596015.
Train: 2018-08-02T00:11:28.089387: step 20825, loss 0.545759.
Train: 2018-08-02T00:11:28.257906: step 20826, loss 0.562503.
Train: 2018-08-02T00:11:28.429473: step 20827, loss 0.612645.
Train: 2018-08-02T00:11:28.602983: step 20828, loss 0.545823.
Train: 2018-08-02T00:11:28.780539: step 20829, loss 0.679263.
Train: 2018-08-02T00:11:28.956071: step 20830, loss 0.629092.
Test: 2018-08-02T00:11:29.499589: step 20830, loss 0.548662.
Train: 2018-08-02T00:11:29.671128: step 20831, loss 0.728466.
Train: 2018-08-02T00:11:29.834691: step 20832, loss 0.51306.
Train: 2018-08-02T00:11:29.998254: step 20833, loss 0.562623.
Train: 2018-08-02T00:11:30.168832: step 20834, loss 0.480646.
Train: 2018-08-02T00:11:30.334385: step 20835, loss 0.480781.
Train: 2018-08-02T00:11:30.514903: step 20836, loss 0.529912.
Train: 2018-08-02T00:11:30.681427: step 20837, loss 0.480724.
Train: 2018-08-02T00:11:30.847015: step 20838, loss 0.628314.
Train: 2018-08-02T00:11:31.012541: step 20839, loss 0.496904.
Train: 2018-08-02T00:11:31.176132: step 20840, loss 0.57908.
Test: 2018-08-02T00:11:31.711682: step 20840, loss 0.548795.
Train: 2018-08-02T00:11:31.880221: step 20841, loss 0.529625.
Train: 2018-08-02T00:11:32.047805: step 20842, loss 0.59563.
Train: 2018-08-02T00:11:32.216324: step 20843, loss 0.562574.
Train: 2018-08-02T00:11:32.379887: step 20844, loss 0.512856.
Train: 2018-08-02T00:11:32.546465: step 20845, loss 0.612359.
Train: 2018-08-02T00:11:32.711032: step 20846, loss 0.52929.
Train: 2018-08-02T00:11:32.876558: step 20847, loss 0.579181.
Train: 2018-08-02T00:11:33.049124: step 20848, loss 0.512492.
Train: 2018-08-02T00:11:33.215652: step 20849, loss 0.529087.
Train: 2018-08-02T00:11:33.382238: step 20850, loss 0.662989.
Test: 2018-08-02T00:11:33.907803: step 20850, loss 0.548452.
Train: 2018-08-02T00:11:34.076351: step 20851, loss 0.596015.
Train: 2018-08-02T00:11:34.242906: step 20852, loss 0.478646.
Train: 2018-08-02T00:11:34.406495: step 20853, loss 0.56248.
Train: 2018-08-02T00:11:34.579033: step 20854, loss 0.59611.
Train: 2018-08-02T00:11:34.748585: step 20855, loss 0.646635.
Train: 2018-08-02T00:11:34.922090: step 20856, loss 0.511986.
Train: 2018-08-02T00:11:35.084657: step 20857, loss 0.612978.
Train: 2018-08-02T00:11:35.248219: step 20858, loss 0.579302.
Train: 2018-08-02T00:11:35.409812: step 20859, loss 0.612952.
Train: 2018-08-02T00:11:35.575344: step 20860, loss 0.444797.
Test: 2018-08-02T00:11:36.107929: step 20860, loss 0.548382.
Train: 2018-08-02T00:11:36.274475: step 20861, loss 0.579296.
Train: 2018-08-02T00:11:36.440063: step 20862, loss 0.579303.
Train: 2018-08-02T00:11:36.606613: step 20863, loss 0.511946.
Train: 2018-08-02T00:11:36.775168: step 20864, loss 0.545605.
Train: 2018-08-02T00:11:36.939728: step 20865, loss 0.427429.
Train: 2018-08-02T00:11:37.102261: step 20866, loss 0.562448.
Train: 2018-08-02T00:11:37.266847: step 20867, loss 0.528491.
Train: 2018-08-02T00:11:37.432411: step 20868, loss 0.68161.
Train: 2018-08-02T00:11:37.591978: step 20869, loss 0.562433.
Train: 2018-08-02T00:11:37.756513: step 20870, loss 0.562432.
Test: 2018-08-02T00:11:38.281111: step 20870, loss 0.548132.
Train: 2018-08-02T00:11:38.445671: step 20871, loss 0.579505.
Train: 2018-08-02T00:11:38.619206: step 20872, loss 0.579515.
Train: 2018-08-02T00:11:38.785787: step 20873, loss 0.528249.
Train: 2018-08-02T00:11:38.951352: step 20874, loss 0.511119.
Train: 2018-08-02T00:11:39.128876: step 20875, loss 0.648061.
Train: 2018-08-02T00:11:39.296396: step 20876, loss 0.596684.
Train: 2018-08-02T00:11:39.467939: step 20877, loss 0.562428.
Train: 2018-08-02T00:11:39.634492: step 20878, loss 0.682244.
Train: 2018-08-02T00:11:39.803094: step 20879, loss 0.52826.
Train: 2018-08-02T00:11:39.963613: step 20880, loss 0.494175.
Test: 2018-08-02T00:11:40.496193: step 20880, loss 0.548144.
Train: 2018-08-02T00:11:40.659752: step 20881, loss 0.460064.
Train: 2018-08-02T00:11:40.827327: step 20882, loss 0.596593.
Train: 2018-08-02T00:11:40.992862: step 20883, loss 0.613709.
Train: 2018-08-02T00:11:41.158418: step 20884, loss 0.562429.
Train: 2018-08-02T00:11:41.331987: step 20885, loss 0.528246.
Train: 2018-08-02T00:11:41.504518: step 20886, loss 0.596625.
Train: 2018-08-02T00:11:41.673074: step 20887, loss 0.528234.
Train: 2018-08-02T00:11:41.844585: step 20888, loss 0.494012.
Train: 2018-08-02T00:11:42.010169: step 20889, loss 0.579553.
Train: 2018-08-02T00:11:42.177720: step 20890, loss 0.528143.
Test: 2018-08-02T00:11:42.717253: step 20890, loss 0.548054.
Train: 2018-08-02T00:11:42.882842: step 20891, loss 0.648246.
Train: 2018-08-02T00:11:43.045374: step 20892, loss 0.579591.
Train: 2018-08-02T00:11:43.219939: step 20893, loss 0.476622.
Train: 2018-08-02T00:11:43.384488: step 20894, loss 0.425009.
Train: 2018-08-02T00:11:43.551023: step 20895, loss 0.614096.
Train: 2018-08-02T00:11:43.715609: step 20896, loss 0.596934.
Train: 2018-08-02T00:11:43.882168: step 20897, loss 0.493351.
Train: 2018-08-02T00:11:44.051685: step 20898, loss 0.441322.
Train: 2018-08-02T00:11:44.215247: step 20899, loss 0.683939.
Train: 2018-08-02T00:11:44.380807: step 20900, loss 0.614581.
Test: 2018-08-02T00:11:44.918367: step 20900, loss 0.547887.
Train: 2018-08-02T00:11:45.728478: step 20901, loss 0.510288.
Train: 2018-08-02T00:11:45.894996: step 20902, loss 0.510238.
Train: 2018-08-02T00:11:46.057562: step 20903, loss 0.545024.
Train: 2018-08-02T00:11:46.230126: step 20904, loss 0.545003.
Train: 2018-08-02T00:11:46.396679: step 20905, loss 0.527497.
Train: 2018-08-02T00:11:46.569224: step 20906, loss 0.544959.
Train: 2018-08-02T00:11:46.734776: step 20907, loss 0.562484.
Train: 2018-08-02T00:11:46.901306: step 20908, loss 0.544918.
Train: 2018-08-02T00:11:47.063899: step 20909, loss 0.597703.
Train: 2018-08-02T00:11:47.229455: step 20910, loss 0.386345.
Test: 2018-08-02T00:11:47.766992: step 20910, loss 0.547731.
Train: 2018-08-02T00:11:47.938561: step 20911, loss 0.580192.
Train: 2018-08-02T00:11:48.102121: step 20912, loss 0.456297.
Train: 2018-08-02T00:11:48.282645: step 20913, loss 0.509271.
Train: 2018-08-02T00:11:48.455151: step 20914, loss 0.562597.
Train: 2018-08-02T00:11:48.629687: step 20915, loss 0.508982.
Train: 2018-08-02T00:11:48.802224: step 20916, loss 0.580597.
Train: 2018-08-02T00:11:48.969802: step 20917, loss 0.580671.
Train: 2018-08-02T00:11:49.134362: step 20918, loss 0.70687.
Train: 2018-08-02T00:11:49.298896: step 20919, loss 0.508653.
Train: 2018-08-02T00:11:49.466473: step 20920, loss 0.562712.
Test: 2018-08-02T00:11:49.990080: step 20920, loss 0.547617.
Train: 2018-08-02T00:11:50.160619: step 20921, loss 0.688922.
Train: 2018-08-02T00:11:50.331137: step 20922, loss 0.562698.
Train: 2018-08-02T00:11:50.494730: step 20923, loss 0.562681.
Train: 2018-08-02T00:11:50.664271: step 20924, loss 0.59857.
Train: 2018-08-02T00:11:50.835814: step 20925, loss 0.580565.
Train: 2018-08-02T00:11:51.008327: step 20926, loss 0.669923.
Train: 2018-08-02T00:11:51.168924: step 20927, loss 0.562595.
Train: 2018-08-02T00:11:51.341436: step 20928, loss 0.58034.
Train: 2018-08-02T00:11:51.506994: step 20929, loss 0.509393.
Train: 2018-08-02T00:11:51.668589: step 20930, loss 0.509493.
Test: 2018-08-02T00:11:52.207122: step 20930, loss 0.547736.
Train: 2018-08-02T00:11:52.379662: step 20931, loss 0.527212.
Train: 2018-08-02T00:11:52.549208: step 20932, loss 0.615424.
Train: 2018-08-02T00:11:52.717757: step 20933, loss 0.562503.
Train: 2018-08-02T00:11:52.881321: step 20934, loss 0.49216.
Train: 2018-08-02T00:11:53.051888: step 20935, loss 0.509768.
Train: 2018-08-02T00:11:53.214460: step 20936, loss 0.61522.
Train: 2018-08-02T00:11:53.379021: step 20937, loss 0.632748.
Train: 2018-08-02T00:11:53.543575: step 20938, loss 0.56248.
Train: 2018-08-02T00:11:53.707112: step 20939, loss 0.422365.
Train: 2018-08-02T00:11:53.877683: step 20940, loss 0.615036.
Test: 2018-08-02T00:11:54.411231: step 20940, loss 0.547806.
Train: 2018-08-02T00:11:54.589778: step 20941, loss 0.562473.
Train: 2018-08-02T00:11:54.760297: step 20942, loss 0.509947.
Train: 2018-08-02T00:11:54.926852: step 20943, loss 0.650036.
Train: 2018-08-02T00:11:55.095426: step 20944, loss 0.544971.
Train: 2018-08-02T00:11:55.255002: step 20945, loss 0.579949.
Train: 2018-08-02T00:11:55.425519: step 20946, loss 0.684747.
Train: 2018-08-02T00:11:55.593071: step 20947, loss 0.562451.
Train: 2018-08-02T00:11:55.771593: step 20948, loss 0.632001.
Train: 2018-08-02T00:11:55.938148: step 20949, loss 0.579774.
Train: 2018-08-02T00:11:56.107720: step 20950, loss 0.441412.
Test: 2018-08-02T00:11:56.639274: step 20950, loss 0.547966.
Train: 2018-08-02T00:11:56.804832: step 20951, loss 0.510618.
Train: 2018-08-02T00:11:56.971412: step 20952, loss 0.614221.
Train: 2018-08-02T00:11:57.145920: step 20953, loss 0.510682.
Train: 2018-08-02T00:11:57.314470: step 20954, loss 0.648647.
Train: 2018-08-02T00:11:57.484016: step 20955, loss 0.527982.
Train: 2018-08-02T00:11:57.650595: step 20956, loss 0.579632.
Train: 2018-08-02T00:11:57.813164: step 20957, loss 0.614.
Train: 2018-08-02T00:11:57.978724: step 20958, loss 0.562423.
Train: 2018-08-02T00:11:58.145249: step 20959, loss 0.579566.
Train: 2018-08-02T00:11:58.309833: step 20960, loss 0.459714.
Test: 2018-08-02T00:11:58.840390: step 20960, loss 0.548088.
Train: 2018-08-02T00:11:59.011932: step 20961, loss 0.545304.
Train: 2018-08-02T00:11:59.179523: step 20962, loss 0.545299.
Train: 2018-08-02T00:11:59.355039: step 20963, loss 0.562423.
Train: 2018-08-02T00:11:59.514613: step 20964, loss 0.648122.
Train: 2018-08-02T00:11:59.698097: step 20965, loss 0.562423.
Train: 2018-08-02T00:11:59.862683: step 20966, loss 0.596655.
Train: 2018-08-02T00:12:00.030209: step 20967, loss 0.494029.
Train: 2018-08-02T00:12:00.202748: step 20968, loss 0.545325.
Train: 2018-08-02T00:12:00.367339: step 20969, loss 0.630832.
Train: 2018-08-02T00:12:00.528904: step 20970, loss 0.494063.
Test: 2018-08-02T00:12:01.071426: step 20970, loss 0.548108.
Train: 2018-08-02T00:12:01.237008: step 20971, loss 0.596614.
Train: 2018-08-02T00:12:01.401544: step 20972, loss 0.545331.
Train: 2018-08-02T00:12:01.571090: step 20973, loss 0.54533.
Train: 2018-08-02T00:12:01.736648: step 20974, loss 0.511127.
Train: 2018-08-02T00:12:01.919159: step 20975, loss 0.528192.
Train: 2018-08-02T00:12:02.085753: step 20976, loss 0.408179.
Train: 2018-08-02T00:12:02.252302: step 20977, loss 0.545229.
Train: 2018-08-02T00:12:02.416857: step 20978, loss 0.562425.
Train: 2018-08-02T00:12:02.585388: step 20979, loss 0.666199.
Train: 2018-08-02T00:12:02.750936: step 20980, loss 0.631689.
Test: 2018-08-02T00:12:03.291491: step 20980, loss 0.547931.
Train: 2018-08-02T00:12:03.458047: step 20981, loss 0.562431.
Train: 2018-08-02T00:12:03.639586: step 20982, loss 0.493163.
Train: 2018-08-02T00:12:03.806146: step 20983, loss 0.649099.
Train: 2018-08-02T00:12:03.970676: step 20984, loss 0.666397.
Train: 2018-08-02T00:12:04.142217: step 20985, loss 0.54513.
Train: 2018-08-02T00:12:04.312792: step 20986, loss 0.458769.
Train: 2018-08-02T00:12:04.478318: step 20987, loss 0.51059.
Train: 2018-08-02T00:12:04.646867: step 20988, loss 0.527843.
Train: 2018-08-02T00:12:04.816427: step 20989, loss 0.579743.
Train: 2018-08-02T00:12:04.980009: step 20990, loss 0.631734.
Test: 2018-08-02T00:12:05.511556: step 20990, loss 0.547926.
Train: 2018-08-02T00:12:05.676147: step 20991, loss 0.475823.
Train: 2018-08-02T00:12:05.838682: step 20992, loss 0.545096.
Train: 2018-08-02T00:12:06.001273: step 20993, loss 0.614498.
Train: 2018-08-02T00:12:06.168825: step 20994, loss 0.562436.
Train: 2018-08-02T00:12:06.335385: step 20995, loss 0.597155.
Train: 2018-08-02T00:12:06.502931: step 20996, loss 0.42361.
Train: 2018-08-02T00:12:06.666499: step 20997, loss 0.527684.
Train: 2018-08-02T00:12:06.831060: step 20998, loss 0.52763.
Train: 2018-08-02T00:12:06.996586: step 20999, loss 0.51013.
Train: 2018-08-02T00:12:07.160180: step 21000, loss 0.597424.
Test: 2018-08-02T00:12:07.692726: step 21000, loss 0.547806.
Train: 2018-08-02T00:12:08.476343: step 21001, loss 0.457411.
Train: 2018-08-02T00:12:08.642897: step 21002, loss 0.562482.
Train: 2018-08-02T00:12:08.817432: step 21003, loss 0.562495.
Train: 2018-08-02T00:12:08.983986: step 21004, loss 0.580142.
Train: 2018-08-02T00:12:09.149577: step 21005, loss 0.509533.
Train: 2018-08-02T00:12:09.315101: step 21006, loss 0.668709.
Train: 2018-08-02T00:12:09.481686: step 21007, loss 0.509426.
Train: 2018-08-02T00:12:09.647214: step 21008, loss 0.597977.
Train: 2018-08-02T00:12:09.810776: step 21009, loss 0.456205.
Train: 2018-08-02T00:12:09.985341: step 21010, loss 0.6158.
Test: 2018-08-02T00:12:10.521876: step 21010, loss 0.547688.
Train: 2018-08-02T00:12:10.693454: step 21011, loss 0.580317.
Train: 2018-08-02T00:12:10.868978: step 21012, loss 0.580322.
Train: 2018-08-02T00:12:11.032510: step 21013, loss 0.509281.
Train: 2018-08-02T00:12:11.201059: step 21014, loss 0.598095.
Train: 2018-08-02T00:12:11.366648: step 21015, loss 0.615851.
Train: 2018-08-02T00:12:11.532200: step 21016, loss 0.438321.
Train: 2018-08-02T00:12:11.696734: step 21017, loss 0.615833.
Train: 2018-08-02T00:12:11.863315: step 21018, loss 0.598066.
Train: 2018-08-02T00:12:12.029844: step 21019, loss 0.615777.
Train: 2018-08-02T00:12:12.193407: step 21020, loss 0.615692.
Test: 2018-08-02T00:12:12.724985: step 21020, loss 0.547719.
Train: 2018-08-02T00:12:12.892538: step 21021, loss 0.580208.
Train: 2018-08-02T00:12:13.059093: step 21022, loss 0.562512.
Train: 2018-08-02T00:12:13.223683: step 21023, loss 0.562499.
Train: 2018-08-02T00:12:13.388238: step 21024, loss 0.544912.
Train: 2018-08-02T00:12:13.554767: step 21025, loss 0.509833.
Train: 2018-08-02T00:12:13.717359: step 21026, loss 0.562476.
Train: 2018-08-02T00:12:13.882890: step 21027, loss 0.52743.
Train: 2018-08-02T00:12:14.049445: step 21028, loss 0.474892.
Train: 2018-08-02T00:12:14.218992: step 21029, loss 0.509887.
Train: 2018-08-02T00:12:14.383552: step 21030, loss 0.457171.
Test: 2018-08-02T00:12:14.909172: step 21030, loss 0.547762.
Train: 2018-08-02T00:12:15.071712: step 21031, loss 0.615277.
Train: 2018-08-02T00:12:15.235276: step 21032, loss 0.527265.
Train: 2018-08-02T00:12:15.402827: step 21033, loss 0.580163.
Train: 2018-08-02T00:12:15.568415: step 21034, loss 0.580192.
Train: 2018-08-02T00:12:15.732944: step 21035, loss 0.562527.
Train: 2018-08-02T00:12:15.895535: step 21036, loss 0.474066.
Train: 2018-08-02T00:12:16.061068: step 21037, loss 0.473941.
Train: 2018-08-02T00:12:16.226625: step 21038, loss 0.544799.
Train: 2018-08-02T00:12:16.385226: step 21039, loss 0.526975.
Train: 2018-08-02T00:12:16.547792: step 21040, loss 0.669673.
Test: 2018-08-02T00:12:17.065409: step 21040, loss 0.547655.
Train: 2018-08-02T00:12:17.234943: step 21041, loss 0.509036.
Train: 2018-08-02T00:12:17.399491: step 21042, loss 0.580498.
Train: 2018-08-02T00:12:17.565046: step 21043, loss 0.526845.
Train: 2018-08-02T00:12:17.728633: step 21044, loss 0.544728.
Train: 2018-08-02T00:12:17.894166: step 21045, loss 0.580571.
Train: 2018-08-02T00:12:18.069698: step 21046, loss 0.455045.
Train: 2018-08-02T00:12:18.239244: step 21047, loss 0.61656.
Train: 2018-08-02T00:12:18.403836: step 21048, loss 0.5447.
Train: 2018-08-02T00:12:18.570360: step 21049, loss 0.544695.
Train: 2018-08-02T00:12:18.736914: step 21050, loss 0.634697.
Test: 2018-08-02T00:12:19.261513: step 21050, loss 0.547619.
Train: 2018-08-02T00:12:19.428098: step 21051, loss 0.616672.
Train: 2018-08-02T00:12:19.595619: step 21052, loss 0.5447.
Train: 2018-08-02T00:12:19.761209: step 21053, loss 0.706328.
Train: 2018-08-02T00:12:19.926765: step 21054, loss 0.508907.
Train: 2018-08-02T00:12:20.092322: step 21055, loss 0.616243.
Train: 2018-08-02T00:12:20.253859: step 21056, loss 0.651748.
Train: 2018-08-02T00:12:20.417422: step 21057, loss 0.544793.
Train: 2018-08-02T00:12:20.586969: step 21058, loss 0.615688.
Train: 2018-08-02T00:12:20.755517: step 21059, loss 0.650789.
Train: 2018-08-02T00:12:20.921076: step 21060, loss 0.632803.
Test: 2018-08-02T00:12:21.444675: step 21060, loss 0.547814.
Train: 2018-08-02T00:12:21.609235: step 21061, loss 0.562463.
Train: 2018-08-02T00:12:21.784793: step 21062, loss 0.579861.
Train: 2018-08-02T00:12:21.959331: step 21063, loss 0.614456.
Train: 2018-08-02T00:12:22.121896: step 21064, loss 0.510637.
Train: 2018-08-02T00:12:22.287454: step 21065, loss 0.510814.
Train: 2018-08-02T00:12:22.459962: step 21066, loss 0.579578.
Train: 2018-08-02T00:12:22.625519: step 21067, loss 0.528185.
Train: 2018-08-02T00:12:22.791077: step 21068, loss 0.494073.
Train: 2018-08-02T00:12:22.951678: step 21069, loss 0.511194.
Train: 2018-08-02T00:12:23.112217: step 21070, loss 0.580637.
Test: 2018-08-02T00:12:23.660751: step 21070, loss 0.548123.
Train: 2018-08-02T00:12:23.828304: step 21071, loss 0.579496.
Train: 2018-08-02T00:12:23.993860: step 21072, loss 0.579491.
Train: 2018-08-02T00:12:24.163433: step 21073, loss 0.664784.
Train: 2018-08-02T00:12:24.340934: step 21074, loss 0.579454.
Train: 2018-08-02T00:12:24.508485: step 21075, loss 0.63042.
Train: 2018-08-02T00:12:24.673045: step 21076, loss 0.528525.
Train: 2018-08-02T00:12:24.849605: step 21077, loss 0.528591.
Train: 2018-08-02T00:12:25.019121: step 21078, loss 0.461014.
Train: 2018-08-02T00:12:25.187670: step 21079, loss 0.562441.
Train: 2018-08-02T00:12:25.350235: step 21080, loss 0.545521.
Test: 2018-08-02T00:12:25.883836: step 21080, loss 0.548261.
Train: 2018-08-02T00:12:26.046405: step 21081, loss 0.47779.
Train: 2018-08-02T00:12:26.218935: step 21082, loss 0.494595.
Train: 2018-08-02T00:12:26.381509: step 21083, loss 0.528422.
Train: 2018-08-02T00:12:26.549089: step 21084, loss 0.596522.
Train: 2018-08-02T00:12:26.718611: step 21085, loss 0.545335.
Train: 2018-08-02T00:12:26.888124: step 21086, loss 0.613779.
Train: 2018-08-02T00:12:27.051689: step 21087, loss 0.408151.
Train: 2018-08-02T00:12:27.228240: step 21088, loss 0.52803.
Train: 2018-08-02T00:12:27.392807: step 21089, loss 0.493421.
Train: 2018-08-02T00:12:27.558376: step 21090, loss 0.579742.
Test: 2018-08-02T00:12:28.087920: step 21090, loss 0.547889.
Train: 2018-08-02T00:12:28.252476: step 21091, loss 0.597175.
Train: 2018-08-02T00:12:28.417038: step 21092, loss 0.51021.
Train: 2018-08-02T00:12:28.581624: step 21093, loss 0.492619.
Train: 2018-08-02T00:12:28.748152: step 21094, loss 0.527435.
Train: 2018-08-02T00:12:28.913745: step 21095, loss 0.54491.
Train: 2018-08-02T00:12:29.086275: step 21096, loss 0.439095.
Train: 2018-08-02T00:12:29.260813: step 21097, loss 0.544827.
Train: 2018-08-02T00:12:29.426338: step 21098, loss 0.509236.
Train: 2018-08-02T00:12:29.594914: step 21099, loss 0.598297.
Train: 2018-08-02T00:12:29.757454: step 21100, loss 0.491011.
Test: 2018-08-02T00:12:30.291028: step 21100, loss 0.547622.
Train: 2018-08-02T00:12:31.087214: step 21101, loss 0.598606.
Train: 2018-08-02T00:12:31.255763: step 21102, loss 0.580714.
Train: 2018-08-02T00:12:31.426335: step 21103, loss 0.562722.
Train: 2018-08-02T00:12:31.596881: step 21104, loss 0.580824.
Train: 2018-08-02T00:12:31.760414: step 21105, loss 0.490348.
Train: 2018-08-02T00:12:31.927965: step 21106, loss 0.653446.
Train: 2018-08-02T00:12:32.092556: step 21107, loss 0.562783.
Train: 2018-08-02T00:12:32.266061: step 21108, loss 0.599066.
Train: 2018-08-02T00:12:32.435633: step 21109, loss 0.544646.
Train: 2018-08-02T00:12:32.601191: step 21110, loss 0.562773.
Test: 2018-08-02T00:12:33.131747: step 21110, loss 0.547597.
Train: 2018-08-02T00:12:33.296339: step 21111, loss 0.435955.
Train: 2018-08-02T00:12:33.466886: step 21112, loss 0.580911.
Train: 2018-08-02T00:12:33.630447: step 21113, loss 0.508364.
Train: 2018-08-02T00:12:33.797967: step 21114, loss 0.526484.
Train: 2018-08-02T00:12:33.963549: step 21115, loss 0.599161.
Train: 2018-08-02T00:12:34.143044: step 21116, loss 0.580997.
Train: 2018-08-02T00:12:34.316581: step 21117, loss 0.508273.
Train: 2018-08-02T00:12:34.493108: step 21118, loss 0.599197.
Train: 2018-08-02T00:12:34.656671: step 21119, loss 0.61737.
Train: 2018-08-02T00:12:34.825247: step 21120, loss 0.653633.
Test: 2018-08-02T00:12:35.349818: step 21120, loss 0.547595.
Train: 2018-08-02T00:12:35.570032: step 21121, loss 0.580901.
Train: 2018-08-02T00:12:35.750543: step 21122, loss 0.472319.
Train: 2018-08-02T00:12:35.928068: step 21123, loss 0.508537.
Train: 2018-08-02T00:12:36.090640: step 21124, loss 0.544668.
Train: 2018-08-02T00:12:36.255200: step 21125, loss 0.65295.
Train: 2018-08-02T00:12:36.428729: step 21126, loss 0.634769.
Train: 2018-08-02T00:12:36.595286: step 21127, loss 0.72443.
Train: 2018-08-02T00:12:36.775808: step 21128, loss 0.616313.
Train: 2018-08-02T00:12:36.943329: step 21129, loss 0.473517.
Train: 2018-08-02T00:12:37.109883: step 21130, loss 0.633573.
Test: 2018-08-02T00:12:37.645454: step 21130, loss 0.547713.
Train: 2018-08-02T00:12:37.810049: step 21131, loss 0.456406.
Train: 2018-08-02T00:12:37.979560: step 21132, loss 0.544861.
Train: 2018-08-02T00:12:38.151125: step 21133, loss 0.562495.
Train: 2018-08-02T00:12:38.315692: step 21134, loss 0.457004.
Train: 2018-08-02T00:12:38.481248: step 21135, loss 0.597629.
Train: 2018-08-02T00:12:38.644782: step 21136, loss 0.562477.
Train: 2018-08-02T00:12:38.810338: step 21137, loss 0.615104.
Train: 2018-08-02T00:12:38.980910: step 21138, loss 0.650055.
Train: 2018-08-02T00:12:39.147436: step 21139, loss 0.45761.
Train: 2018-08-02T00:12:39.315986: step 21140, loss 0.510079.
Test: 2018-08-02T00:12:39.842610: step 21140, loss 0.547835.
Train: 2018-08-02T00:12:40.007140: step 21141, loss 0.597353.
Train: 2018-08-02T00:12:40.173720: step 21142, loss 0.562446.
Train: 2018-08-02T00:12:40.339250: step 21143, loss 0.545016.
Train: 2018-08-02T00:12:40.512787: step 21144, loss 0.666955.
Train: 2018-08-02T00:12:40.677347: step 21145, loss 0.562436.
Train: 2018-08-02T00:12:40.837918: step 21146, loss 0.579789.
Train: 2018-08-02T00:12:41.000483: step 21147, loss 0.527771.
Train: 2018-08-02T00:12:41.181032: step 21148, loss 0.579731.
Train: 2018-08-02T00:12:41.345560: step 21149, loss 0.562421.
Train: 2018-08-02T00:12:41.513113: step 21150, loss 0.545155.
Test: 2018-08-02T00:12:42.055662: step 21150, loss 0.547974.
Train: 2018-08-02T00:12:42.221221: step 21151, loss 0.493421.
Train: 2018-08-02T00:12:42.382814: step 21152, loss 0.545167.
Train: 2018-08-02T00:12:42.546352: step 21153, loss 0.527906.
Train: 2018-08-02T00:12:42.715929: step 21154, loss 0.510614.
Train: 2018-08-02T00:12:42.880458: step 21155, loss 0.545131.
Train: 2018-08-02T00:12:43.045044: step 21156, loss 0.70094.
Train: 2018-08-02T00:12:43.214564: step 21157, loss 0.597031.
Train: 2018-08-02T00:12:43.390121: step 21158, loss 0.631567.
Train: 2018-08-02T00:12:43.553691: step 21159, loss 0.527909.
Train: 2018-08-02T00:12:43.722208: step 21160, loss 0.596884.
Test: 2018-08-02T00:12:44.250799: step 21160, loss 0.548007.
Train: 2018-08-02T00:12:44.416371: step 21161, loss 0.528003.
Train: 2018-08-02T00:12:44.581909: step 21162, loss 0.596794.
Train: 2018-08-02T00:12:44.745472: step 21163, loss 0.579581.
Train: 2018-08-02T00:12:44.908037: step 21164, loss 0.648124.
Train: 2018-08-02T00:12:45.077618: step 21165, loss 0.511115.
Train: 2018-08-02T00:12:45.247131: step 21166, loss 0.596566.
Train: 2018-08-02T00:12:45.414683: step 21167, loss 0.613551.
Train: 2018-08-02T00:12:45.583245: step 21168, loss 0.613438.
Train: 2018-08-02T00:12:45.747827: step 21169, loss 0.732016.
Train: 2018-08-02T00:12:45.911356: step 21170, loss 0.562444.
Test: 2018-08-02T00:12:46.452908: step 21170, loss 0.548387.
Train: 2018-08-02T00:12:46.619493: step 21171, loss 0.562461.
Train: 2018-08-02T00:12:46.784049: step 21172, loss 0.528991.
Train: 2018-08-02T00:12:46.951574: step 21173, loss 0.545799.
Train: 2018-08-02T00:12:47.115137: step 21174, loss 0.629146.
Train: 2018-08-02T00:12:47.287693: step 21175, loss 0.545918.
Train: 2018-08-02T00:12:47.452236: step 21176, loss 0.612266.
Train: 2018-08-02T00:12:47.613804: step 21177, loss 0.546037.
Train: 2018-08-02T00:12:47.783351: step 21178, loss 0.480104.
Train: 2018-08-02T00:12:47.948933: step 21179, loss 0.694484.
Train: 2018-08-02T00:12:48.120482: step 21180, loss 0.513257.
Test: 2018-08-02T00:12:48.653027: step 21180, loss 0.548859.
Train: 2018-08-02T00:12:48.817587: step 21181, loss 0.480465.
Train: 2018-08-02T00:12:48.983176: step 21182, loss 0.529746.
Train: 2018-08-02T00:12:49.146707: step 21183, loss 0.496814.
Train: 2018-08-02T00:12:49.314258: step 21184, loss 0.595549.
Train: 2018-08-02T00:12:49.475827: step 21185, loss 0.59558.
Train: 2018-08-02T00:12:49.645374: step 21186, loss 0.612114.
Train: 2018-08-02T00:12:49.809964: step 21187, loss 0.546057.
Train: 2018-08-02T00:12:49.976489: step 21188, loss 0.496474.
Train: 2018-08-02T00:12:50.143077: step 21189, loss 0.397074.
Train: 2018-08-02T00:12:50.312599: step 21190, loss 0.712029.
Test: 2018-08-02T00:12:50.838218: step 21190, loss 0.54859.
Train: 2018-08-02T00:12:51.004765: step 21191, loss 0.52925.
Train: 2018-08-02T00:12:51.170298: step 21192, loss 0.562507.
Train: 2018-08-02T00:12:51.342836: step 21193, loss 0.595883.
Train: 2018-08-02T00:12:51.513380: step 21194, loss 0.712902.
Train: 2018-08-02T00:12:51.677974: step 21195, loss 0.645977.
Train: 2018-08-02T00:12:51.845524: step 21196, loss 0.562507.
Train: 2018-08-02T00:12:52.012101: step 21197, loss 0.562518.
Train: 2018-08-02T00:12:52.173647: step 21198, loss 0.612366.
Train: 2018-08-02T00:12:52.338176: step 21199, loss 0.479633.
Train: 2018-08-02T00:12:52.505727: step 21200, loss 0.47967.
Test: 2018-08-02T00:12:53.038303: step 21200, loss 0.548645.
Train: 2018-08-02T00:12:53.824583: step 21201, loss 0.612306.
Train: 2018-08-02T00:12:53.995123: step 21202, loss 0.612316.
Train: 2018-08-02T00:12:54.159654: step 21203, loss 0.595712.
Train: 2018-08-02T00:12:54.327238: step 21204, loss 0.645416.
Train: 2018-08-02T00:12:54.494792: step 21205, loss 0.612195.
Train: 2018-08-02T00:12:54.664321: step 21206, loss 0.562575.
Train: 2018-08-02T00:12:54.827868: step 21207, loss 0.56259.
Train: 2018-08-02T00:12:55.002433: step 21208, loss 0.546148.
Train: 2018-08-02T00:12:55.166003: step 21209, loss 0.529735.
Train: 2018-08-02T00:12:55.331552: step 21210, loss 0.562615.
Test: 2018-08-02T00:12:55.861106: step 21210, loss 0.548857.
Train: 2018-08-02T00:12:56.025693: step 21211, loss 0.546185.
Train: 2018-08-02T00:12:56.189259: step 21212, loss 0.562614.
Train: 2018-08-02T00:12:56.355835: step 21213, loss 0.562611.
Train: 2018-08-02T00:12:56.524358: step 21214, loss 0.529708.
Train: 2018-08-02T00:12:56.691886: step 21215, loss 0.562598.
Train: 2018-08-02T00:12:56.862429: step 21216, loss 0.645002.
Train: 2018-08-02T00:12:57.023998: step 21217, loss 0.546109.
Train: 2018-08-02T00:12:57.184599: step 21218, loss 0.595557.
Train: 2018-08-02T00:12:57.351158: step 21219, loss 0.546106.
Train: 2018-08-02T00:12:57.517709: step 21220, loss 0.49664.
Test: 2018-08-02T00:12:58.050255: step 21220, loss 0.548752.
Train: 2018-08-02T00:12:58.219802: step 21221, loss 0.546067.
Train: 2018-08-02T00:12:58.384393: step 21222, loss 0.562563.
Train: 2018-08-02T00:12:58.564909: step 21223, loss 0.62878.
Train: 2018-08-02T00:12:58.736451: step 21224, loss 0.579113.
Train: 2018-08-02T00:12:58.899019: step 21225, loss 0.628828.
Train: 2018-08-02T00:12:59.063576: step 21226, loss 0.446618.
Train: 2018-08-02T00:12:59.229131: step 21227, loss 0.56254.
Train: 2018-08-02T00:12:59.406628: step 21228, loss 0.413104.
Train: 2018-08-02T00:12:59.571219: step 21229, loss 0.495876.
Train: 2018-08-02T00:12:59.745721: step 21230, loss 0.562484.
Test: 2018-08-02T00:13:00.274323: step 21230, loss 0.548409.
Train: 2018-08-02T00:13:00.437909: step 21231, loss 0.528893.
Train: 2018-08-02T00:13:00.603428: step 21232, loss 0.545598.
Train: 2018-08-02T00:13:00.768022: step 21233, loss 0.46096.
Train: 2018-08-02T00:13:00.932549: step 21234, loss 0.54543.
Train: 2018-08-02T00:13:01.096111: step 21235, loss 0.613623.
Train: 2018-08-02T00:13:01.257689: step 21236, loss 0.476778.
Train: 2018-08-02T00:13:01.421273: step 21237, loss 0.528016.
Train: 2018-08-02T00:13:01.591787: step 21238, loss 0.527877.
Train: 2018-08-02T00:13:01.756347: step 21239, loss 0.493056.
Train: 2018-08-02T00:13:01.923925: step 21240, loss 0.614705.
Test: 2018-08-02T00:13:02.449495: step 21240, loss 0.547813.
Train: 2018-08-02T00:13:02.620038: step 21241, loss 0.684824.
Train: 2018-08-02T00:13:02.786602: step 21242, loss 0.527445.
Train: 2018-08-02T00:13:02.950170: step 21243, loss 0.527394.
Train: 2018-08-02T00:13:03.115744: step 21244, loss 0.562477.
Train: 2018-08-02T00:13:03.284288: step 21245, loss 0.597683.
Train: 2018-08-02T00:13:03.447854: step 21246, loss 0.615337.
Train: 2018-08-02T00:13:03.609427: step 21247, loss 0.509642.
Train: 2018-08-02T00:13:03.775957: step 21248, loss 0.509609.
Train: 2018-08-02T00:13:03.944498: step 21249, loss 0.686058.
Train: 2018-08-02T00:13:04.109088: step 21250, loss 0.580142.
Test: 2018-08-02T00:13:04.648629: step 21250, loss 0.547736.
Train: 2018-08-02T00:13:04.814201: step 21251, loss 0.580123.
Train: 2018-08-02T00:13:04.982722: step 21252, loss 0.72097.
Train: 2018-08-02T00:13:05.157284: step 21253, loss 0.580029.
Train: 2018-08-02T00:13:05.321817: step 21254, loss 0.614971.
Train: 2018-08-02T00:13:05.487374: step 21255, loss 0.597333.
Train: 2018-08-02T00:13:05.652956: step 21256, loss 0.475518.
Train: 2018-08-02T00:13:05.816493: step 21257, loss 0.493044.
Train: 2018-08-02T00:13:05.976091: step 21258, loss 0.631729.
Train: 2018-08-02T00:13:06.139660: step 21259, loss 0.6143.
Train: 2018-08-02T00:13:06.317154: step 21260, loss 0.614172.
Test: 2018-08-02T00:13:06.858710: step 21260, loss 0.548007.
Train: 2018-08-02T00:13:07.026290: step 21261, loss 0.545209.
Train: 2018-08-02T00:13:07.189822: step 21262, loss 0.579574.
Train: 2018-08-02T00:13:07.360367: step 21263, loss 0.54529.
Train: 2018-08-02T00:13:07.527949: step 21264, loss 0.494056.
Train: 2018-08-02T00:13:07.699459: step 21265, loss 0.596565.
Train: 2018-08-02T00:13:07.864019: step 21266, loss 0.528303.
Train: 2018-08-02T00:13:08.026616: step 21267, loss 0.596509.
Train: 2018-08-02T00:13:08.187156: step 21268, loss 0.545387.
Train: 2018-08-02T00:13:08.346760: step 21269, loss 0.528378.
Train: 2018-08-02T00:13:08.517304: step 21270, loss 0.52838.
Test: 2018-08-02T00:13:09.054866: step 21270, loss 0.54816.
Train: 2018-08-02T00:13:09.223385: step 21271, loss 0.579444.
Train: 2018-08-02T00:13:09.387976: step 21272, loss 0.596476.
Train: 2018-08-02T00:13:09.551508: step 21273, loss 0.630516.
Train: 2018-08-02T00:13:09.717067: step 21274, loss 0.579425.
Train: 2018-08-02T00:13:09.881657: step 21275, loss 0.494485.
Train: 2018-08-02T00:13:10.052171: step 21276, loss 0.562423.
Train: 2018-08-02T00:13:10.218725: step 21277, loss 0.460545.
Train: 2018-08-02T00:13:10.381291: step 21278, loss 0.528416.
Train: 2018-08-02T00:13:10.557818: step 21279, loss 0.647568.
Train: 2018-08-02T00:13:10.723400: step 21280, loss 0.494271.
Test: 2018-08-02T00:13:11.260940: step 21280, loss 0.54813.
Train: 2018-08-02T00:13:11.423503: step 21281, loss 0.494182.
Train: 2018-08-02T00:13:11.600058: step 21282, loss 0.545319.
Train: 2018-08-02T00:13:11.759648: step 21283, loss 0.511023.
Train: 2018-08-02T00:13:11.926160: step 21284, loss 0.562411.
Train: 2018-08-02T00:13:12.092715: step 21285, loss 0.631264.
Train: 2018-08-02T00:13:12.257306: step 21286, loss 0.562413.
Train: 2018-08-02T00:13:12.436823: step 21287, loss 0.476161.
Train: 2018-08-02T00:13:12.608362: step 21288, loss 0.648846.
Train: 2018-08-02T00:13:12.776919: step 21289, loss 0.545121.
Train: 2018-08-02T00:13:12.942475: step 21290, loss 0.666289.
Test: 2018-08-02T00:13:13.466074: step 21290, loss 0.547931.
Train: 2018-08-02T00:13:13.634624: step 21291, loss 0.562418.
Train: 2018-08-02T00:13:13.797192: step 21292, loss 0.614287.
Train: 2018-08-02T00:13:13.961720: step 21293, loss 0.562415.
Train: 2018-08-02T00:13:14.127308: step 21294, loss 0.562414.
Train: 2018-08-02T00:13:14.289841: step 21295, loss 0.665795.
Train: 2018-08-02T00:13:14.449415: step 21296, loss 0.562411.
Train: 2018-08-02T00:13:14.621985: step 21297, loss 0.579565.
Train: 2018-08-02T00:13:14.796512: step 21298, loss 0.562411.
Train: 2018-08-02T00:13:14.966034: step 21299, loss 0.494069.
Train: 2018-08-02T00:13:15.132589: step 21300, loss 0.681919.
Test: 2018-08-02T00:13:15.664198: step 21300, loss 0.548153.
Train: 2018-08-02T00:13:16.477083: step 21301, loss 0.460217.
Train: 2018-08-02T00:13:16.647633: step 21302, loss 0.477309.
Train: 2018-08-02T00:13:16.813184: step 21303, loss 0.47726.
Train: 2018-08-02T00:13:16.984726: step 21304, loss 0.579474.
Train: 2018-08-02T00:13:17.149286: step 21305, loss 0.562412.
Train: 2018-08-02T00:13:17.312849: step 21306, loss 0.494005.
Train: 2018-08-02T00:13:17.476417: step 21307, loss 0.596682.
Train: 2018-08-02T00:13:17.641970: step 21308, loss 0.442302.
Train: 2018-08-02T00:13:17.807501: step 21309, loss 0.459177.
Train: 2018-08-02T00:13:17.971091: step 21310, loss 0.614234.
Test: 2018-08-02T00:13:18.499682: step 21310, loss 0.547917.
Train: 2018-08-02T00:13:18.662226: step 21311, loss 0.631713.
Train: 2018-08-02T00:13:18.829795: step 21312, loss 0.527729.
Train: 2018-08-02T00:13:18.996361: step 21313, loss 0.579809.
Train: 2018-08-02T00:13:19.159917: step 21314, loss 0.597234.
Train: 2018-08-02T00:13:19.323474: step 21315, loss 0.545022.
Train: 2018-08-02T00:13:19.487011: step 21316, loss 0.510156.
Train: 2018-08-02T00:13:19.659581: step 21317, loss 0.544992.
Train: 2018-08-02T00:13:19.823139: step 21318, loss 0.562449.
Train: 2018-08-02T00:13:20.003664: step 21319, loss 0.544957.
Train: 2018-08-02T00:13:20.173208: step 21320, loss 0.650057.
Test: 2018-08-02T00:13:20.719728: step 21320, loss 0.547791.
Train: 2018-08-02T00:13:20.889263: step 21321, loss 0.562461.
Train: 2018-08-02T00:13:21.057813: step 21322, loss 0.66756.
Train: 2018-08-02T00:13:21.239327: step 21323, loss 0.544963.
Train: 2018-08-02T00:13:21.404885: step 21324, loss 0.579918.
Train: 2018-08-02T00:13:21.571464: step 21325, loss 0.57989.
Train: 2018-08-02T00:13:21.748964: step 21326, loss 0.562438.
Train: 2018-08-02T00:13:21.918536: step 21327, loss 0.492846.
Train: 2018-08-02T00:13:22.082101: step 21328, loss 0.614602.
Train: 2018-08-02T00:13:22.259635: step 21329, loss 0.527685.
Train: 2018-08-02T00:13:22.432139: step 21330, loss 0.545065.
Test: 2018-08-02T00:13:22.969703: step 21330, loss 0.547892.
Train: 2018-08-02T00:13:23.146256: step 21331, loss 0.527712.
Train: 2018-08-02T00:13:23.316774: step 21332, loss 0.45827.
Train: 2018-08-02T00:13:23.483360: step 21333, loss 0.527662.
Train: 2018-08-02T00:13:23.657863: step 21334, loss 0.632093.
Train: 2018-08-02T00:13:23.835391: step 21335, loss 0.475315.
Train: 2018-08-02T00:13:24.001973: step 21336, loss 0.579897.
Train: 2018-08-02T00:13:24.173514: step 21337, loss 0.492557.
Train: 2018-08-02T00:13:24.338074: step 21338, loss 0.474927.
Train: 2018-08-02T00:13:24.513606: step 21339, loss 0.615134.
Train: 2018-08-02T00:13:24.680137: step 21340, loss 0.544896.
Test: 2018-08-02T00:13:25.212705: step 21340, loss 0.54774.
Train: 2018-08-02T00:13:25.383268: step 21341, loss 0.544876.
Train: 2018-08-02T00:13:25.551799: step 21342, loss 0.527212.
Train: 2018-08-02T00:13:25.724391: step 21343, loss 0.491803.
Train: 2018-08-02T00:13:25.897874: step 21344, loss 0.456203.
Train: 2018-08-02T00:13:26.067427: step 21345, loss 0.56256.
Train: 2018-08-02T00:13:26.233975: step 21346, loss 0.526915.
Train: 2018-08-02T00:13:26.412498: step 21347, loss 0.616284.
Train: 2018-08-02T00:13:26.579053: step 21348, loss 0.544711.
Train: 2018-08-02T00:13:26.751592: step 21349, loss 0.616524.
Train: 2018-08-02T00:13:26.918147: step 21350, loss 0.52672.
Test: 2018-08-02T00:13:27.460696: step 21350, loss 0.54761.
Train: 2018-08-02T00:13:27.624259: step 21351, loss 0.652631.
Train: 2018-08-02T00:13:27.790840: step 21352, loss 0.634618.
Train: 2018-08-02T00:13:27.959363: step 21353, loss 0.61655.
Train: 2018-08-02T00:13:28.121953: step 21354, loss 0.706075.
Train: 2018-08-02T00:13:28.292497: step 21355, loss 0.49114.
Train: 2018-08-02T00:13:28.452046: step 21356, loss 0.616042.
Train: 2018-08-02T00:13:28.623587: step 21357, loss 0.58032.
Train: 2018-08-02T00:13:28.789145: step 21358, loss 0.509388.
Train: 2018-08-02T00:13:28.952708: step 21359, loss 0.562513.
Train: 2018-08-02T00:13:29.119288: step 21360, loss 0.615415.
Test: 2018-08-02T00:13:29.656838: step 21360, loss 0.547751.
Train: 2018-08-02T00:13:29.830361: step 21361, loss 0.580076.
Train: 2018-08-02T00:13:29.997939: step 21362, loss 0.544922.
Train: 2018-08-02T00:13:30.164485: step 21363, loss 0.527442.
Train: 2018-08-02T00:13:30.334045: step 21364, loss 0.614887.
Train: 2018-08-02T00:13:30.496611: step 21365, loss 0.667086.
Train: 2018-08-02T00:13:30.659173: step 21366, loss 0.510283.
Train: 2018-08-02T00:13:30.831716: step 21367, loss 0.545082.
Train: 2018-08-02T00:13:31.004228: step 21368, loss 0.545112.
Train: 2018-08-02T00:13:31.179754: step 21369, loss 0.579694.
Train: 2018-08-02T00:13:31.345342: step 21370, loss 0.545163.
Test: 2018-08-02T00:13:31.868911: step 21370, loss 0.547986.
Train: 2018-08-02T00:13:32.042448: step 21371, loss 0.599163.
Train: 2018-08-02T00:13:32.207033: step 21372, loss 0.493616.
Train: 2018-08-02T00:13:32.379546: step 21373, loss 0.579598.
Train: 2018-08-02T00:13:32.545129: step 21374, loss 0.56241.
Train: 2018-08-02T00:13:32.706703: step 21375, loss 0.545247.
Train: 2018-08-02T00:13:32.874251: step 21376, loss 0.596722.
Train: 2018-08-02T00:13:33.036799: step 21377, loss 0.596693.
Train: 2018-08-02T00:13:33.203363: step 21378, loss 0.630896.
Train: 2018-08-02T00:13:33.372918: step 21379, loss 0.630761.
Train: 2018-08-02T00:13:33.539446: step 21380, loss 0.613539.
Test: 2018-08-02T00:13:34.072022: step 21380, loss 0.548195.
Train: 2018-08-02T00:13:34.237606: step 21381, loss 0.681339.
Train: 2018-08-02T00:13:34.407152: step 21382, loss 0.562432.
Train: 2018-08-02T00:13:34.572684: step 21383, loss 0.612989.
Train: 2018-08-02T00:13:34.739240: step 21384, loss 0.545688.
Train: 2018-08-02T00:13:34.908812: step 21385, loss 0.579202.
Train: 2018-08-02T00:13:35.073345: step 21386, loss 0.54584.
Train: 2018-08-02T00:13:35.239901: step 21387, loss 0.429568.
Train: 2018-08-02T00:13:35.408450: step 21388, loss 0.545911.
Train: 2018-08-02T00:13:35.585975: step 21389, loss 0.512689.
Train: 2018-08-02T00:13:35.750535: step 21390, loss 0.529269.
Test: 2018-08-02T00:13:36.289096: step 21390, loss 0.548571.
Train: 2018-08-02T00:13:36.464653: step 21391, loss 0.529219.
Train: 2018-08-02T00:13:36.625200: step 21392, loss 0.495803.
Train: 2018-08-02T00:13:36.787815: step 21393, loss 0.445466.
Train: 2018-08-02T00:13:36.957310: step 21394, loss 0.67995.
Train: 2018-08-02T00:13:37.127853: step 21395, loss 0.511999.
Train: 2018-08-02T00:13:37.302386: step 21396, loss 0.596165.
Train: 2018-08-02T00:13:37.469970: step 21397, loss 0.596223.
Train: 2018-08-02T00:13:37.637491: step 21398, loss 0.49477.
Train: 2018-08-02T00:13:37.804046: step 21399, loss 0.477667.
Train: 2018-08-02T00:13:37.967608: step 21400, loss 0.562418.
Test: 2018-08-02T00:13:38.499187: step 21400, loss 0.548134.
Train: 2018-08-02T00:13:39.298648: step 21401, loss 0.528309.
Train: 2018-08-02T00:13:39.464206: step 21402, loss 0.5111.
Train: 2018-08-02T00:13:39.643725: step 21403, loss 0.545247.
Train: 2018-08-02T00:13:39.805327: step 21404, loss 0.459106.
Train: 2018-08-02T00:13:39.981821: step 21405, loss 0.545124.
Train: 2018-08-02T00:13:40.146409: step 21406, loss 0.631869.
Train: 2018-08-02T00:13:40.323908: step 21407, loss 0.457986.
Train: 2018-08-02T00:13:40.498441: step 21408, loss 0.562447.
Train: 2018-08-02T00:13:40.665022: step 21409, loss 0.579992.
Train: 2018-08-02T00:13:40.828585: step 21410, loss 0.632781.
Test: 2018-08-02T00:13:41.361134: step 21410, loss 0.547745.
Train: 2018-08-02T00:13:41.529709: step 21411, loss 0.492079.
Train: 2018-08-02T00:13:41.693247: step 21412, loss 0.527221.
Train: 2018-08-02T00:13:41.859841: step 21413, loss 0.597868.
Train: 2018-08-02T00:13:42.027354: step 21414, loss 0.615635.
Train: 2018-08-02T00:13:42.190952: step 21415, loss 0.562527.
Train: 2018-08-02T00:13:42.353482: step 21416, loss 0.615695.
Train: 2018-08-02T00:13:42.517075: step 21417, loss 0.562527.
Train: 2018-08-02T00:13:42.692576: step 21418, loss 0.633353.
Train: 2018-08-02T00:13:42.858132: step 21419, loss 0.474099.
Train: 2018-08-02T00:13:43.019726: step 21420, loss 0.544833.
Test: 2018-08-02T00:13:43.560256: step 21420, loss 0.547709.
Train: 2018-08-02T00:13:43.724817: step 21421, loss 0.597869.
Train: 2018-08-02T00:13:43.897354: step 21422, loss 0.491837.
Train: 2018-08-02T00:13:44.061947: step 21423, loss 0.580183.
Train: 2018-08-02T00:13:44.233456: step 21424, loss 0.527166.
Train: 2018-08-02T00:13:44.401034: step 21425, loss 0.544835.
Train: 2018-08-02T00:13:44.567564: step 21426, loss 0.474094.
Train: 2018-08-02T00:13:44.730159: step 21427, loss 0.49169.
Train: 2018-08-02T00:13:44.898677: step 21428, loss 0.473817.
Train: 2018-08-02T00:13:45.070250: step 21429, loss 0.651539.
Train: 2018-08-02T00:13:45.237803: step 21430, loss 0.616032.
Test: 2018-08-02T00:13:45.762395: step 21430, loss 0.547652.
Train: 2018-08-02T00:13:45.928924: step 21431, loss 0.562579.
Train: 2018-08-02T00:13:46.107447: step 21432, loss 0.633907.
Train: 2018-08-02T00:13:46.270039: step 21433, loss 0.616029.
Train: 2018-08-02T00:13:46.436566: step 21434, loss 0.54477.
Train: 2018-08-02T00:13:46.607117: step 21435, loss 0.704722.
Train: 2018-08-02T00:13:46.768706: step 21436, loss 0.597967.
Train: 2018-08-02T00:13:46.931275: step 21437, loss 0.509518.
Train: 2018-08-02T00:13:47.098821: step 21438, loss 0.47438.
Train: 2018-08-02T00:13:47.267346: step 21439, loss 0.597692.
Train: 2018-08-02T00:13:47.430936: step 21440, loss 0.562476.
Test: 2018-08-02T00:13:47.966476: step 21440, loss 0.547769.
Train: 2018-08-02T00:13:48.132059: step 21441, loss 0.527361.
Train: 2018-08-02T00:13:48.303576: step 21442, loss 0.527387.
Train: 2018-08-02T00:13:48.469158: step 21443, loss 0.597524.
Train: 2018-08-02T00:13:48.633694: step 21444, loss 0.492393.
Train: 2018-08-02T00:13:48.807273: step 21445, loss 0.49239.
Train: 2018-08-02T00:13:48.970818: step 21446, loss 0.457268.
Train: 2018-08-02T00:13:49.131388: step 21447, loss 0.615178.
Train: 2018-08-02T00:13:49.299938: step 21448, loss 0.65042.
Train: 2018-08-02T00:13:49.474445: step 21449, loss 0.580063.
Train: 2018-08-02T00:13:49.640004: step 21450, loss 0.439438.
Test: 2018-08-02T00:13:50.181557: step 21450, loss 0.547747.
Train: 2018-08-02T00:13:50.351129: step 21451, loss 0.580077.
Train: 2018-08-02T00:13:50.517687: step 21452, loss 0.439234.
Train: 2018-08-02T00:13:50.685208: step 21453, loss 0.580142.
Train: 2018-08-02T00:13:50.850794: step 21454, loss 0.633195.
Train: 2018-08-02T00:13:51.014360: step 21455, loss 0.527154.
Train: 2018-08-02T00:13:51.180909: step 21456, loss 0.491749.
Train: 2018-08-02T00:13:51.349434: step 21457, loss 0.615682.
Train: 2018-08-02T00:13:51.522970: step 21458, loss 0.597988.
Train: 2018-08-02T00:13:51.689526: step 21459, loss 0.597986.
Train: 2018-08-02T00:13:51.851112: step 21460, loss 0.562528.
Test: 2018-08-02T00:13:52.388661: step 21460, loss 0.547696.
Train: 2018-08-02T00:13:52.553242: step 21461, loss 0.52711.
Train: 2018-08-02T00:13:52.720767: step 21462, loss 0.544819.
Train: 2018-08-02T00:13:52.892315: step 21463, loss 0.580223.
Train: 2018-08-02T00:13:53.055896: step 21464, loss 0.633301.
Train: 2018-08-02T00:13:53.222452: step 21465, loss 0.509491.
Train: 2018-08-02T00:13:53.389979: step 21466, loss 0.43887.
Train: 2018-08-02T00:13:53.557561: step 21467, loss 0.58019.
Train: 2018-08-02T00:13:53.720129: step 21468, loss 0.509451.
Train: 2018-08-02T00:13:53.889673: step 21469, loss 0.52711.
Train: 2018-08-02T00:13:54.060212: step 21470, loss 0.509344.
Test: 2018-08-02T00:13:54.595755: step 21470, loss 0.547674.
Train: 2018-08-02T00:13:54.761313: step 21471, loss 0.544786.
Train: 2018-08-02T00:13:54.933851: step 21472, loss 0.580351.
Train: 2018-08-02T00:13:55.102432: step 21473, loss 0.580382.
Train: 2018-08-02T00:13:55.270990: step 21474, loss 0.598226.
Train: 2018-08-02T00:13:55.437505: step 21475, loss 0.544752.
Train: 2018-08-02T00:13:55.608049: step 21476, loss 0.56258.
Train: 2018-08-02T00:13:55.771622: step 21477, loss 0.669566.
Train: 2018-08-02T00:13:55.940168: step 21478, loss 0.615988.
Train: 2018-08-02T00:13:56.105753: step 21479, loss 0.615863.
Train: 2018-08-02T00:13:56.269307: step 21480, loss 0.509356.
Test: 2018-08-02T00:13:56.801858: step 21480, loss 0.547701.
Train: 2018-08-02T00:13:56.969410: step 21481, loss 0.562517.
Train: 2018-08-02T00:13:57.143942: step 21482, loss 0.544841.
Train: 2018-08-02T00:13:57.311494: step 21483, loss 0.509577.
Train: 2018-08-02T00:13:57.478077: step 21484, loss 0.527236.
Train: 2018-08-02T00:13:57.655606: step 21485, loss 0.527242.
Train: 2018-08-02T00:13:57.820166: step 21486, loss 0.474355.
Train: 2018-08-02T00:13:57.986722: step 21487, loss 0.668389.
Train: 2018-08-02T00:13:58.157266: step 21488, loss 0.527213.
Train: 2018-08-02T00:13:58.319800: step 21489, loss 0.527212.
Train: 2018-08-02T00:13:58.487383: step 21490, loss 0.5272.
Test: 2018-08-02T00:13:59.023917: step 21490, loss 0.547714.
Train: 2018-08-02T00:13:59.192467: step 21491, loss 0.544842.
Train: 2018-08-02T00:13:59.360019: step 21492, loss 0.56251.
Train: 2018-08-02T00:13:59.527571: step 21493, loss 0.474076.
Train: 2018-08-02T00:13:59.695149: step 21494, loss 0.79283.
Train: 2018-08-02T00:13:59.870679: step 21495, loss 0.650955.
Train: 2018-08-02T00:14:00.042196: step 21496, loss 0.580139.
Train: 2018-08-02T00:14:00.209748: step 21497, loss 0.632863.
Train: 2018-08-02T00:14:00.375338: step 21498, loss 0.632609.
Train: 2018-08-02T00:14:00.541860: step 21499, loss 0.57991.
Train: 2018-08-02T00:14:00.709429: step 21500, loss 0.614627.
Test: 2018-08-02T00:14:01.252960: step 21500, loss 0.547909.
Train: 2018-08-02T00:14:02.081225: step 21501, loss 0.441138.
Train: 2018-08-02T00:14:02.244818: step 21502, loss 0.510553.
Train: 2018-08-02T00:14:02.413363: step 21503, loss 0.614199.
Train: 2018-08-02T00:14:02.581886: step 21504, loss 0.596867.
Train: 2018-08-02T00:14:02.759442: step 21505, loss 0.61398.
Train: 2018-08-02T00:14:02.928990: step 21506, loss 0.562407.
Train: 2018-08-02T00:14:03.098536: step 21507, loss 0.511099.
Train: 2018-08-02T00:14:03.271077: step 21508, loss 0.596564.
Train: 2018-08-02T00:14:03.435633: step 21509, loss 0.579458.
Train: 2018-08-02T00:14:03.602190: step 21510, loss 0.477336.
Test: 2018-08-02T00:14:04.196696: step 21510, loss 0.548173.
Train: 2018-08-02T00:14:04.365239: step 21511, loss 0.494384.
Train: 2018-08-02T00:14:04.531769: step 21512, loss 0.477331.
Train: 2018-08-02T00:14:04.703336: step 21513, loss 0.545367.
Train: 2018-08-02T00:14:04.873855: step 21514, loss 0.579483.
Train: 2018-08-02T00:14:05.040411: step 21515, loss 0.562408.
Train: 2018-08-02T00:14:05.204006: step 21516, loss 0.459701.
Train: 2018-08-02T00:14:05.367566: step 21517, loss 0.493767.
Train: 2018-08-02T00:14:05.545091: step 21518, loss 0.493553.
Train: 2018-08-02T00:14:05.711640: step 21519, loss 0.614244.
Train: 2018-08-02T00:14:05.877204: step 21520, loss 0.510449.
Test: 2018-08-02T00:14:06.416764: step 21520, loss 0.547875.
Train: 2018-08-02T00:14:06.586277: step 21521, loss 0.579799.
Train: 2018-08-02T00:14:06.762836: step 21522, loss 0.545015.
Train: 2018-08-02T00:14:06.928363: step 21523, loss 0.544983.
Train: 2018-08-02T00:14:07.093920: step 21524, loss 0.614946.
Train: 2018-08-02T00:14:07.268453: step 21525, loss 0.544936.
Train: 2018-08-02T00:14:07.440992: step 21526, loss 0.580009.
Train: 2018-08-02T00:14:07.616547: step 21527, loss 0.632716.
Train: 2018-08-02T00:14:07.791088: step 21528, loss 0.509789.
Train: 2018-08-02T00:14:07.956648: step 21529, loss 0.580039.
Train: 2018-08-02T00:14:08.132173: step 21530, loss 0.632755.
Test: 2018-08-02T00:14:08.664752: step 21530, loss 0.547766.
Train: 2018-08-02T00:14:08.836295: step 21531, loss 0.527355.
Train: 2018-08-02T00:14:09.000822: step 21532, loss 0.632664.
Train: 2018-08-02T00:14:09.167379: step 21533, loss 0.544932.
Train: 2018-08-02T00:14:09.342933: step 21534, loss 0.509926.
Train: 2018-08-02T00:14:09.507494: step 21535, loss 0.632469.
Train: 2018-08-02T00:14:09.679034: step 21536, loss 0.509999.
Train: 2018-08-02T00:14:09.840609: step 21537, loss 0.597393.
Train: 2018-08-02T00:14:10.007153: step 21538, loss 0.475151.
Train: 2018-08-02T00:14:10.171693: step 21539, loss 0.562442.
Train: 2018-08-02T00:14:10.333261: step 21540, loss 0.457655.
Test: 2018-08-02T00:14:10.858895: step 21540, loss 0.547803.
Train: 2018-08-02T00:14:11.026480: step 21541, loss 0.527469.
Train: 2018-08-02T00:14:11.192031: step 21542, loss 0.632532.
Train: 2018-08-02T00:14:11.357593: step 21543, loss 0.685159.
Train: 2018-08-02T00:14:11.529129: step 21544, loss 0.47491.
Train: 2018-08-02T00:14:11.696681: step 21545, loss 0.597472.
Train: 2018-08-02T00:14:11.861248: step 21546, loss 0.527449.
Train: 2018-08-02T00:14:12.034778: step 21547, loss 0.614952.
Train: 2018-08-02T00:14:12.203303: step 21548, loss 0.632395.
Train: 2018-08-02T00:14:12.374874: step 21549, loss 0.527525.
Train: 2018-08-02T00:14:12.540401: step 21550, loss 0.562436.
Test: 2018-08-02T00:14:13.078960: step 21550, loss 0.547844.
Train: 2018-08-02T00:14:13.256494: step 21551, loss 0.510169.
Train: 2018-08-02T00:14:13.422075: step 21552, loss 0.666928.
Train: 2018-08-02T00:14:13.590594: step 21553, loss 0.545039.
Train: 2018-08-02T00:14:13.775100: step 21554, loss 0.597151.
Train: 2018-08-02T00:14:13.942652: step 21555, loss 0.510412.
Train: 2018-08-02T00:14:14.110204: step 21556, loss 0.68366.
Train: 2018-08-02T00:14:14.286757: step 21557, loss 0.59697.
Train: 2018-08-02T00:14:14.455295: step 21558, loss 0.614109.
Train: 2018-08-02T00:14:14.625856: step 21559, loss 0.510863.
Train: 2018-08-02T00:14:14.795373: step 21560, loss 0.510971.
Test: 2018-08-02T00:14:15.328947: step 21560, loss 0.548067.
Train: 2018-08-02T00:14:15.493507: step 21561, loss 0.562407.
Train: 2018-08-02T00:14:15.665047: step 21562, loss 0.545301.
Train: 2018-08-02T00:14:15.833628: step 21563, loss 0.494033.
Train: 2018-08-02T00:14:15.998157: step 21564, loss 0.562408.
Train: 2018-08-02T00:14:16.160734: step 21565, loss 0.665016.
Train: 2018-08-02T00:14:16.327278: step 21566, loss 0.562382.
Train: 2018-08-02T00:14:16.490840: step 21567, loss 0.477173.
Train: 2018-08-02T00:14:16.653405: step 21568, loss 0.579372.
Train: 2018-08-02T00:14:16.816999: step 21569, loss 0.528526.
Train: 2018-08-02T00:14:16.981529: step 21570, loss 0.562294.
Test: 2018-08-02T00:14:17.518095: step 21570, loss 0.548149.
Train: 2018-08-02T00:14:17.693661: step 21571, loss 0.647882.
Train: 2018-08-02T00:14:17.867160: step 21572, loss 0.579175.
Train: 2018-08-02T00:14:18.032718: step 21573, loss 0.562227.
Train: 2018-08-02T00:14:18.196281: step 21574, loss 0.527129.
Train: 2018-08-02T00:14:18.369848: step 21575, loss 0.560506.
Train: 2018-08-02T00:14:18.534408: step 21576, loss 0.580132.
Train: 2018-08-02T00:14:18.698969: step 21577, loss 0.610661.
Train: 2018-08-02T00:14:18.860505: step 21578, loss 0.52992.
Train: 2018-08-02T00:14:19.025101: step 21579, loss 0.484812.
Train: 2018-08-02T00:14:19.194620: step 21580, loss 0.531515.
Test: 2018-08-02T00:14:19.725221: step 21580, loss 0.549331.
Train: 2018-08-02T00:14:19.891773: step 21581, loss 0.529339.
Train: 2018-08-02T00:14:20.058328: step 21582, loss 0.579241.
Train: 2018-08-02T00:14:20.225856: step 21583, loss 0.427336.
Train: 2018-08-02T00:14:20.392439: step 21584, loss 0.663203.
Train: 2018-08-02T00:14:20.555980: step 21585, loss 0.578269.
Train: 2018-08-02T00:14:20.724547: step 21586, loss 0.595124.
Train: 2018-08-02T00:14:20.895068: step 21587, loss 0.547144.
Train: 2018-08-02T00:14:21.061622: step 21588, loss 0.511676.
Train: 2018-08-02T00:14:21.227179: step 21589, loss 0.56199.
Train: 2018-08-02T00:14:21.390743: step 21590, loss 0.563883.
Test: 2018-08-02T00:14:21.925313: step 21590, loss 0.547985.
Train: 2018-08-02T00:14:22.091900: step 21591, loss 0.493311.
Train: 2018-08-02T00:14:22.252469: step 21592, loss 0.527561.
Train: 2018-08-02T00:14:22.412012: step 21593, loss 0.474907.
Train: 2018-08-02T00:14:22.582555: step 21594, loss 0.439949.
Train: 2018-08-02T00:14:22.750139: step 21595, loss 0.65091.
Train: 2018-08-02T00:14:22.920678: step 21596, loss 0.633071.
Train: 2018-08-02T00:14:23.086241: step 21597, loss 0.597955.
Train: 2018-08-02T00:14:23.248774: step 21598, loss 0.562511.
Train: 2018-08-02T00:14:23.414368: step 21599, loss 0.491545.
Train: 2018-08-02T00:14:23.574934: step 21600, loss 0.562558.
Test: 2018-08-02T00:14:24.106482: step 21600, loss 0.54766.
Train: 2018-08-02T00:14:24.999717: step 21601, loss 0.651558.
Train: 2018-08-02T00:14:25.175248: step 21602, loss 0.544768.
Train: 2018-08-02T00:14:25.337786: step 21603, loss 0.59815.
Train: 2018-08-02T00:14:25.500351: step 21604, loss 0.740389.
Train: 2018-08-02T00:14:25.668931: step 21605, loss 0.544801.
Train: 2018-08-02T00:14:25.846451: step 21606, loss 0.615582.
Train: 2018-08-02T00:14:26.016007: step 21607, loss 0.491949.
Train: 2018-08-02T00:14:26.180533: step 21608, loss 0.632898.
Train: 2018-08-02T00:14:26.359054: step 21609, loss 0.544912.
Train: 2018-08-02T00:14:26.526608: step 21610, loss 0.439835.
Test: 2018-08-02T00:14:27.061178: step 21610, loss 0.547793.
Train: 2018-08-02T00:14:27.226737: step 21611, loss 0.562455.
Train: 2018-08-02T00:14:27.393290: step 21612, loss 0.509954.
Train: 2018-08-02T00:14:27.564831: step 21613, loss 0.492446.
Train: 2018-08-02T00:14:27.731421: step 21614, loss 0.562458.
Train: 2018-08-02T00:14:27.892955: step 21615, loss 0.650125.
Train: 2018-08-02T00:14:28.063532: step 21616, loss 0.544934.
Train: 2018-08-02T00:14:28.226065: step 21617, loss 0.615019.
Train: 2018-08-02T00:14:28.390649: step 21618, loss 0.614964.
Train: 2018-08-02T00:14:28.559174: step 21619, loss 0.579922.
Train: 2018-08-02T00:14:28.722737: step 21620, loss 0.702003.
Test: 2018-08-02T00:14:29.251324: step 21620, loss 0.547868.
Train: 2018-08-02T00:14:29.415910: step 21621, loss 0.649359.
Train: 2018-08-02T00:14:29.582438: step 21622, loss 0.545103.
Train: 2018-08-02T00:14:29.746026: step 21623, loss 0.648661.
Train: 2018-08-02T00:14:29.912605: step 21624, loss 0.562407.
Train: 2018-08-02T00:14:30.077141: step 21625, loss 0.493987.
Train: 2018-08-02T00:14:30.244703: step 21626, loss 0.562411.
Train: 2018-08-02T00:14:30.407233: step 21627, loss 0.528378.
Train: 2018-08-02T00:14:30.571819: step 21628, loss 0.528439.
Train: 2018-08-02T00:14:30.743334: step 21629, loss 0.579392.
Train: 2018-08-02T00:14:30.905922: step 21630, loss 0.562423.
Test: 2018-08-02T00:14:31.437479: step 21630, loss 0.548244.
Train: 2018-08-02T00:14:31.606038: step 21631, loss 0.562426.
Train: 2018-08-02T00:14:31.771596: step 21632, loss 0.596271.
Train: 2018-08-02T00:14:31.946156: step 21633, loss 0.596235.
Train: 2018-08-02T00:14:32.111677: step 21634, loss 0.646819.
Train: 2018-08-02T00:14:32.277235: step 21635, loss 0.410931.
Train: 2018-08-02T00:14:32.440830: step 21636, loss 0.511944.
Train: 2018-08-02T00:14:32.606389: step 21637, loss 0.612982.
Train: 2018-08-02T00:14:32.773907: step 21638, loss 0.562444.
Train: 2018-08-02T00:14:32.944497: step 21639, loss 0.511904.
Train: 2018-08-02T00:14:33.111035: step 21640, loss 0.596161.
Test: 2018-08-02T00:14:33.634607: step 21640, loss 0.548316.
Train: 2018-08-02T00:14:33.803190: step 21641, loss 0.579304.
Train: 2018-08-02T00:14:33.969735: step 21642, loss 0.613039.
Train: 2018-08-02T00:14:34.134300: step 21643, loss 0.562441.
Train: 2018-08-02T00:14:34.299828: step 21644, loss 0.646692.
Train: 2018-08-02T00:14:34.464419: step 21645, loss 0.545624.
Train: 2018-08-02T00:14:34.633965: step 21646, loss 0.528841.
Train: 2018-08-02T00:14:34.800524: step 21647, loss 0.545657.
Train: 2018-08-02T00:14:34.962057: step 21648, loss 0.545659.
Train: 2018-08-02T00:14:35.125650: step 21649, loss 0.512052.
Train: 2018-08-02T00:14:35.290211: step 21650, loss 0.562451.
Test: 2018-08-02T00:14:35.834724: step 21650, loss 0.548351.
Train: 2018-08-02T00:14:36.024653: step 21651, loss 0.579279.
Train: 2018-08-02T00:14:36.196164: step 21652, loss 0.612975.
Train: 2018-08-02T00:14:36.355768: step 21653, loss 0.612972.
Train: 2018-08-02T00:14:36.520328: step 21654, loss 0.478291.
Train: 2018-08-02T00:14:36.687885: step 21655, loss 0.562444.
Train: 2018-08-02T00:14:36.866403: step 21656, loss 0.528739.
Train: 2018-08-02T00:14:37.032926: step 21657, loss 0.562438.
Train: 2018-08-02T00:14:37.199482: step 21658, loss 0.528662.
Train: 2018-08-02T00:14:37.360079: step 21659, loss 0.66389.
Train: 2018-08-02T00:14:37.533590: step 21660, loss 0.613156.
Test: 2018-08-02T00:14:38.073152: step 21660, loss 0.548283.
Train: 2018-08-02T00:14:38.238735: step 21661, loss 0.579328.
Train: 2018-08-02T00:14:38.401268: step 21662, loss 0.646847.
Train: 2018-08-02T00:14:38.571813: step 21663, loss 0.629845.
Train: 2018-08-02T00:14:38.737370: step 21664, loss 0.495222.
Train: 2018-08-02T00:14:38.902929: step 21665, loss 0.512102.
Train: 2018-08-02T00:14:39.066521: step 21666, loss 0.596018.
Train: 2018-08-02T00:14:39.237066: step 21667, loss 0.595996.
Train: 2018-08-02T00:14:39.409598: step 21668, loss 0.579218.
Train: 2018-08-02T00:14:39.582142: step 21669, loss 0.612661.
Train: 2018-08-02T00:14:39.757668: step 21670, loss 0.478985.
Test: 2018-08-02T00:14:40.294209: step 21670, loss 0.548506.
Train: 2018-08-02T00:14:40.460788: step 21671, loss 0.59588.
Train: 2018-08-02T00:14:40.636322: step 21672, loss 0.615887.
Train: 2018-08-02T00:14:40.803847: step 21673, loss 0.562498.
Train: 2018-08-02T00:14:40.971399: step 21674, loss 0.529206.
Train: 2018-08-02T00:14:41.139972: step 21675, loss 0.612438.
Train: 2018-08-02T00:14:41.316475: step 21676, loss 0.579142.
Train: 2018-08-02T00:14:41.481035: step 21677, loss 0.56252.
Train: 2018-08-02T00:14:41.644598: step 21678, loss 0.562525.
Train: 2018-08-02T00:14:41.811155: step 21679, loss 0.579119.
Train: 2018-08-02T00:14:41.982723: step 21680, loss 0.678585.
Test: 2018-08-02T00:14:42.518263: step 21680, loss 0.548698.
Train: 2018-08-02T00:14:42.693819: step 21681, loss 0.529465.
Train: 2018-08-02T00:14:42.867357: step 21682, loss 0.579082.
Train: 2018-08-02T00:14:43.034885: step 21683, loss 0.612067.
Train: 2018-08-02T00:14:43.198482: step 21684, loss 0.562589.
Train: 2018-08-02T00:14:43.362038: step 21685, loss 0.579046.
Train: 2018-08-02T00:14:43.533560: step 21686, loss 0.595455.
Train: 2018-08-02T00:14:43.697142: step 21687, loss 0.644592.
Train: 2018-08-02T00:14:43.863667: step 21688, loss 0.595357.
Train: 2018-08-02T00:14:44.028272: step 21689, loss 0.546378.
Train: 2018-08-02T00:14:44.194781: step 21690, loss 0.660354.
Test: 2018-08-02T00:14:44.728354: step 21690, loss 0.549156.
Train: 2018-08-02T00:14:44.894934: step 21691, loss 0.578967.
Train: 2018-08-02T00:14:45.064457: step 21692, loss 0.546599.
Train: 2018-08-02T00:14:45.226032: step 21693, loss 0.514373.
Train: 2018-08-02T00:14:45.393588: step 21694, loss 0.514434.
Train: 2018-08-02T00:14:45.565147: step 21695, loss 0.562816.
Train: 2018-08-02T00:14:45.735693: step 21696, loss 0.546682.
Train: 2018-08-02T00:14:45.902217: step 21697, loss 0.482095.
Train: 2018-08-02T00:14:46.068772: step 21698, loss 0.595128.
Train: 2018-08-02T00:14:46.240313: step 21699, loss 0.611357.
Train: 2018-08-02T00:14:46.406892: step 21700, loss 0.514117.
Test: 2018-08-02T00:14:46.940442: step 21700, loss 0.549135.
Train: 2018-08-02T00:14:47.705896: step 21701, loss 0.546495.
Train: 2018-08-02T00:14:47.871453: step 21702, loss 0.562711.
Train: 2018-08-02T00:14:48.049950: step 21703, loss 0.546392.
Train: 2018-08-02T00:14:48.217504: step 21704, loss 0.546336.
Train: 2018-08-02T00:14:48.383060: step 21705, loss 0.546275.
Train: 2018-08-02T00:14:48.549640: step 21706, loss 0.57903.
Train: 2018-08-02T00:14:48.718195: step 21707, loss 0.579045.
Train: 2018-08-02T00:14:48.882750: step 21708, loss 0.612003.
Train: 2018-08-02T00:14:49.045330: step 21709, loss 0.513112.
Train: 2018-08-02T00:14:49.207856: step 21710, loss 0.529529.
Test: 2018-08-02T00:14:49.747414: step 21710, loss 0.548685.
Train: 2018-08-02T00:14:49.920987: step 21711, loss 0.545993.
Train: 2018-08-02T00:14:50.087505: step 21712, loss 0.512756.
Train: 2018-08-02T00:14:50.254059: step 21713, loss 0.629063.
Train: 2018-08-02T00:14:50.430618: step 21714, loss 0.579165.
Train: 2018-08-02T00:14:50.598140: step 21715, loss 0.512416.
Train: 2018-08-02T00:14:50.762725: step 21716, loss 0.495575.
Train: 2018-08-02T00:14:50.925314: step 21717, loss 0.495367.
Train: 2018-08-02T00:14:51.092841: step 21718, loss 0.629781.
Train: 2018-08-02T00:14:51.259404: step 21719, loss 0.545562.
Train: 2018-08-02T00:14:51.427921: step 21720, loss 0.630083.
Test: 2018-08-02T00:14:51.968475: step 21720, loss 0.548242.
Train: 2018-08-02T00:14:52.133062: step 21721, loss 0.579359.
Train: 2018-08-02T00:14:52.298593: step 21722, loss 0.596323.
Train: 2018-08-02T00:14:52.459199: step 21723, loss 0.511546.
Train: 2018-08-02T00:14:52.630704: step 21724, loss 0.579394.
Train: 2018-08-02T00:14:52.801249: step 21725, loss 0.511445.
Train: 2018-08-02T00:14:52.970797: step 21726, loss 0.545398.
Train: 2018-08-02T00:14:53.133362: step 21727, loss 0.681699.
Train: 2018-08-02T00:14:53.301944: step 21728, loss 0.596485.
Train: 2018-08-02T00:14:53.469497: step 21729, loss 0.477277.
Train: 2018-08-02T00:14:53.642028: step 21730, loss 0.613522.
Test: 2018-08-02T00:14:54.172614: step 21730, loss 0.548145.
Train: 2018-08-02T00:14:54.340135: step 21731, loss 0.647586.
Train: 2018-08-02T00:14:54.505718: step 21732, loss 0.528385.
Train: 2018-08-02T00:14:54.668259: step 21733, loss 0.613422.
Train: 2018-08-02T00:14:54.830823: step 21734, loss 0.664306.
Train: 2018-08-02T00:14:54.995385: step 21735, loss 0.545484.
Train: 2018-08-02T00:14:55.160969: step 21736, loss 0.545523.
Train: 2018-08-02T00:14:55.326498: step 21737, loss 0.596194.
Train: 2018-08-02T00:14:55.491059: step 21738, loss 0.612991.
Train: 2018-08-02T00:14:55.660636: step 21739, loss 0.663322.
Train: 2018-08-02T00:14:55.825196: step 21740, loss 0.595976.
Test: 2018-08-02T00:14:56.355748: step 21740, loss 0.548504.
Train: 2018-08-02T00:14:56.532303: step 21741, loss 0.662667.
Train: 2018-08-02T00:14:56.692846: step 21742, loss 0.579137.
Train: 2018-08-02T00:14:56.855412: step 21743, loss 0.545993.
Train: 2018-08-02T00:14:57.018973: step 21744, loss 0.628556.
Train: 2018-08-02T00:14:57.189519: step 21745, loss 0.496886.
Train: 2018-08-02T00:14:57.357096: step 21746, loss 0.46429.
Train: 2018-08-02T00:14:57.521655: step 21747, loss 0.595398.
Train: 2018-08-02T00:14:57.687189: step 21748, loss 0.546281.
Train: 2018-08-02T00:14:57.852746: step 21749, loss 0.546292.
Train: 2018-08-02T00:14:58.018329: step 21750, loss 0.529931.
Test: 2018-08-02T00:14:58.551882: step 21750, loss 0.548936.
Train: 2018-08-02T00:14:58.718432: step 21751, loss 0.579014.
Train: 2018-08-02T00:14:58.889001: step 21752, loss 0.431611.
Train: 2018-08-02T00:14:59.056545: step 21753, loss 0.496934.
Train: 2018-08-02T00:14:59.222110: step 21754, loss 0.562584.
Train: 2018-08-02T00:14:59.389662: step 21755, loss 0.579085.
Train: 2018-08-02T00:14:59.566196: step 21756, loss 0.479673.
Train: 2018-08-02T00:14:59.730760: step 21757, loss 0.612413.
Train: 2018-08-02T00:14:59.911255: step 21758, loss 0.595852.
Train: 2018-08-02T00:15:00.074805: step 21759, loss 0.612624.
Train: 2018-08-02T00:15:00.252331: step 21760, loss 0.495531.
Test: 2018-08-02T00:15:00.792887: step 21760, loss 0.548418.
Train: 2018-08-02T00:15:00.960439: step 21761, loss 0.545691.
Train: 2018-08-02T00:15:01.124999: step 21762, loss 0.579259.
Train: 2018-08-02T00:15:01.301556: step 21763, loss 0.562443.
Train: 2018-08-02T00:15:01.470076: step 21764, loss 0.461232.
Train: 2018-08-02T00:15:01.632642: step 21765, loss 0.545508.
Train: 2018-08-02T00:15:01.806208: step 21766, loss 0.494546.
Train: 2018-08-02T00:15:01.972731: step 21767, loss 0.460233.
Train: 2018-08-02T00:15:02.138289: step 21768, loss 0.45975.
Train: 2018-08-02T00:15:02.302875: step 21769, loss 0.51079.
Train: 2018-08-02T00:15:02.477408: step 21770, loss 0.562412.
Test: 2018-08-02T00:15:03.017937: step 21770, loss 0.547863.
Train: 2018-08-02T00:15:03.183521: step 21771, loss 0.614595.
Train: 2018-08-02T00:15:03.350081: step 21772, loss 0.527526.
Train: 2018-08-02T00:15:03.517602: step 21773, loss 0.527409.
Train: 2018-08-02T00:15:03.685201: step 21774, loss 0.509706.
Train: 2018-08-02T00:15:03.856702: step 21775, loss 0.438876.
Train: 2018-08-02T00:15:04.023275: step 21776, loss 0.527037.
Train: 2018-08-02T00:15:04.185822: step 21777, loss 0.509068.
Train: 2018-08-02T00:15:04.354396: step 21778, loss 0.526777.
Train: 2018-08-02T00:15:04.524910: step 21779, loss 0.490637.
Train: 2018-08-02T00:15:04.698476: step 21780, loss 0.50844.
Test: 2018-08-02T00:15:05.233015: step 21780, loss 0.547578.
Train: 2018-08-02T00:15:05.404583: step 21781, loss 0.490046.
Train: 2018-08-02T00:15:05.571137: step 21782, loss 0.544602.
Train: 2018-08-02T00:15:05.731714: step 21783, loss 0.709924.
Train: 2018-08-02T00:15:05.907245: step 21784, loss 0.563001.
Train: 2018-08-02T00:15:06.070803: step 21785, loss 0.526143.
Train: 2018-08-02T00:15:06.238356: step 21786, loss 0.618501.
Train: 2018-08-02T00:15:06.402888: step 21787, loss 0.544587.
Train: 2018-08-02T00:15:06.575453: step 21788, loss 0.544587.
Train: 2018-08-02T00:15:06.741016: step 21789, loss 0.581624.
Train: 2018-08-02T00:15:06.907540: step 21790, loss 0.60015.
Test: 2018-08-02T00:15:07.429146: step 21790, loss 0.547598.
Train: 2018-08-02T00:15:07.603724: step 21791, loss 0.674165.
Train: 2018-08-02T00:15:07.764280: step 21792, loss 0.489164.
Train: 2018-08-02T00:15:07.940808: step 21793, loss 0.581495.
Train: 2018-08-02T00:15:08.106365: step 21794, loss 0.470882.
Train: 2018-08-02T00:15:08.282879: step 21795, loss 0.507751.
Train: 2018-08-02T00:15:08.442437: step 21796, loss 0.599848.
Train: 2018-08-02T00:15:08.613014: step 21797, loss 0.673451.
Train: 2018-08-02T00:15:08.782558: step 21798, loss 0.618078.
Train: 2018-08-02T00:15:08.950111: step 21799, loss 0.507952.
Train: 2018-08-02T00:15:09.112671: step 21800, loss 0.526315.
Test: 2018-08-02T00:15:09.654222: step 21800, loss 0.547575.
Train: 2018-08-02T00:15:10.450507: step 21801, loss 0.562865.
Train: 2018-08-02T00:15:10.625041: step 21802, loss 0.581072.
Train: 2018-08-02T00:15:10.797578: step 21803, loss 0.544618.
Train: 2018-08-02T00:15:10.962163: step 21804, loss 0.562793.
Train: 2018-08-02T00:15:11.128740: step 21805, loss 0.580912.
Train: 2018-08-02T00:15:11.297267: step 21806, loss 0.490317.
Train: 2018-08-02T00:15:11.464826: step 21807, loss 0.61701.
Train: 2018-08-02T00:15:11.639328: step 21808, loss 0.436284.
Train: 2018-08-02T00:15:11.805883: step 21809, loss 0.562714.
Train: 2018-08-02T00:15:11.964483: step 21810, loss 0.526599.
Test: 2018-08-02T00:15:12.497034: step 21810, loss 0.547591.
Train: 2018-08-02T00:15:12.666607: step 21811, loss 0.562712.
Train: 2018-08-02T00:15:12.831169: step 21812, loss 0.526599.
Train: 2018-08-02T00:15:12.996699: step 21813, loss 0.544654.
Train: 2018-08-02T00:15:13.177216: step 21814, loss 0.598849.
Train: 2018-08-02T00:15:13.342775: step 21815, loss 0.436299.
Train: 2018-08-02T00:15:13.509361: step 21816, loss 0.616963.
Train: 2018-08-02T00:15:13.677878: step 21817, loss 0.490403.
Train: 2018-08-02T00:15:13.844434: step 21818, loss 0.454157.
Train: 2018-08-02T00:15:14.009026: step 21819, loss 0.526501.
Train: 2018-08-02T00:15:14.177543: step 21820, loss 0.6173.
Test: 2018-08-02T00:15:14.718124: step 21820, loss 0.547577.
Train: 2018-08-02T00:15:14.889671: step 21821, loss 0.580994.
Train: 2018-08-02T00:15:15.056194: step 21822, loss 0.617403.
Train: 2018-08-02T00:15:15.221785: step 21823, loss 0.562809.
Train: 2018-08-02T00:15:15.391332: step 21824, loss 0.617342.
Train: 2018-08-02T00:15:15.568823: step 21825, loss 0.671724.
Train: 2018-08-02T00:15:15.736376: step 21826, loss 0.68951.
Train: 2018-08-02T00:15:15.896947: step 21827, loss 0.580732.
Train: 2018-08-02T00:15:16.063528: step 21828, loss 0.652461.
Train: 2018-08-02T00:15:16.242024: step 21829, loss 0.508977.
Train: 2018-08-02T00:15:16.406615: step 21830, loss 0.598173.
Test: 2018-08-02T00:15:16.941155: step 21830, loss 0.547681.
Train: 2018-08-02T00:15:17.105715: step 21831, loss 0.615727.
Train: 2018-08-02T00:15:17.270275: step 21832, loss 0.509537.
Train: 2018-08-02T00:15:17.434867: step 21833, loss 0.456918.
Train: 2018-08-02T00:15:17.606380: step 21834, loss 0.580027.
Train: 2018-08-02T00:15:17.771934: step 21835, loss 0.492347.
Train: 2018-08-02T00:15:17.938489: step 21836, loss 0.527429.
Train: 2018-08-02T00:15:18.119006: step 21837, loss 0.667474.
Train: 2018-08-02T00:15:18.283566: step 21838, loss 0.562441.
Train: 2018-08-02T00:15:18.445160: step 21839, loss 0.597321.
Train: 2018-08-02T00:15:18.610691: step 21840, loss 0.579836.
Test: 2018-08-02T00:15:19.136317: step 21840, loss 0.547874.
Train: 2018-08-02T00:15:19.298886: step 21841, loss 0.649279.
Train: 2018-08-02T00:15:19.466429: step 21842, loss 0.527776.
Train: 2018-08-02T00:15:19.628970: step 21843, loss 0.648798.
Train: 2018-08-02T00:15:19.797520: step 21844, loss 0.665732.
Train: 2018-08-02T00:15:19.960109: step 21845, loss 0.596696.
Train: 2018-08-02T00:15:20.126640: step 21846, loss 0.613619.
Train: 2018-08-02T00:15:20.294192: step 21847, loss 0.664362.
Train: 2018-08-02T00:15:20.462772: step 21848, loss 0.545533.
Train: 2018-08-02T00:15:20.627302: step 21849, loss 0.495184.
Train: 2018-08-02T00:15:20.789897: step 21850, loss 0.495423.
Test: 2018-08-02T00:15:21.321446: step 21850, loss 0.548467.
Train: 2018-08-02T00:15:21.486031: step 21851, loss 0.52902.
Train: 2018-08-02T00:15:21.650590: step 21852, loss 0.445532.
Train: 2018-08-02T00:15:21.812159: step 21853, loss 0.595913.
Train: 2018-08-02T00:15:21.978713: step 21854, loss 0.529035.
Train: 2018-08-02T00:15:22.140287: step 21855, loss 0.529007.
Train: 2018-08-02T00:15:22.305814: step 21856, loss 0.579219.
Train: 2018-08-02T00:15:22.470374: step 21857, loss 0.629535.
Train: 2018-08-02T00:15:22.635963: step 21858, loss 0.478624.
Train: 2018-08-02T00:15:22.803485: step 21859, loss 0.545668.
Train: 2018-08-02T00:15:22.965052: step 21860, loss 0.680127.
Test: 2018-08-02T00:15:23.492642: step 21860, loss 0.548378.
Train: 2018-08-02T00:15:23.657234: step 21861, loss 0.394396.
Train: 2018-08-02T00:15:23.824753: step 21862, loss 0.596123.
Train: 2018-08-02T00:15:23.987345: step 21863, loss 0.444375.
Train: 2018-08-02T00:15:24.154896: step 21864, loss 0.613176.
Train: 2018-08-02T00:15:24.320429: step 21865, loss 0.647177.
Train: 2018-08-02T00:15:24.485986: step 21866, loss 0.596345.
Train: 2018-08-02T00:15:24.659546: step 21867, loss 0.528482.
Train: 2018-08-02T00:15:24.821108: step 21868, loss 0.681269.
Train: 2018-08-02T00:15:25.000610: step 21869, loss 0.545455.
Train: 2018-08-02T00:15:25.171155: step 21870, loss 0.647181.
Test: 2018-08-02T00:15:25.711710: step 21870, loss 0.548252.
Train: 2018-08-02T00:15:25.880258: step 21871, loss 0.5455.
Train: 2018-08-02T00:15:26.045841: step 21872, loss 0.596234.
Train: 2018-08-02T00:15:26.211401: step 21873, loss 0.46117.
Train: 2018-08-02T00:15:26.381918: step 21874, loss 0.59619.
Train: 2018-08-02T00:15:26.561438: step 21875, loss 0.579307.
Train: 2018-08-02T00:15:26.738994: step 21876, loss 0.49498.
Train: 2018-08-02T00:15:26.902559: step 21877, loss 0.444323.
Train: 2018-08-02T00:15:27.072097: step 21878, loss 0.494788.
Train: 2018-08-02T00:15:27.233667: step 21879, loss 0.511541.
Train: 2018-08-02T00:15:27.401223: step 21880, loss 0.477332.
Test: 2018-08-02T00:15:27.936761: step 21880, loss 0.548096.
Train: 2018-08-02T00:15:28.101363: step 21881, loss 0.528231.
Train: 2018-08-02T00:15:28.264908: step 21882, loss 0.562403.
Train: 2018-08-02T00:15:28.431463: step 21883, loss 0.562405.
Train: 2018-08-02T00:15:28.593007: step 21884, loss 0.666104.
Train: 2018-08-02T00:15:28.768538: step 21885, loss 0.545101.
Train: 2018-08-02T00:15:28.944069: step 21886, loss 0.545076.
Train: 2018-08-02T00:15:29.108654: step 21887, loss 0.631899.
Train: 2018-08-02T00:15:29.272192: step 21888, loss 0.631941.
Train: 2018-08-02T00:15:29.434783: step 21889, loss 0.527676.
Train: 2018-08-02T00:15:29.604303: step 21890, loss 0.701408.
Test: 2018-08-02T00:15:30.140900: step 21890, loss 0.547894.
Train: 2018-08-02T00:15:30.305457: step 21891, loss 0.5971.
Train: 2018-08-02T00:15:30.472016: step 21892, loss 0.614327.
Train: 2018-08-02T00:15:30.639561: step 21893, loss 0.562407.
Train: 2018-08-02T00:15:30.809107: step 21894, loss 0.579623.
Train: 2018-08-02T00:15:30.974641: step 21895, loss 0.545225.
Train: 2018-08-02T00:15:31.144211: step 21896, loss 0.613837.
Train: 2018-08-02T00:15:31.316756: step 21897, loss 0.545301.
Train: 2018-08-02T00:15:31.483294: step 21898, loss 0.545336.
Train: 2018-08-02T00:15:31.645872: step 21899, loss 0.477188.
Train: 2018-08-02T00:15:31.816390: step 21900, loss 0.596485.
Test: 2018-08-02T00:15:32.346991: step 21900, loss 0.548136.
Train: 2018-08-02T00:15:33.134187: step 21901, loss 0.664691.
Train: 2018-08-02T00:15:33.298715: step 21902, loss 0.545411.
Train: 2018-08-02T00:15:33.480255: step 21903, loss 0.545443.
Train: 2018-08-02T00:15:33.649809: step 21904, loss 0.579373.
Train: 2018-08-02T00:15:33.816364: step 21905, loss 0.443877.
Train: 2018-08-02T00:15:33.980920: step 21906, loss 0.460746.
Train: 2018-08-02T00:15:34.154459: step 21907, loss 0.579394.
Train: 2018-08-02T00:15:34.316994: step 21908, loss 0.647446.
Train: 2018-08-02T00:15:34.486566: step 21909, loss 0.528388.
Train: 2018-08-02T00:15:34.653096: step 21910, loss 0.56241.
Test: 2018-08-02T00:15:35.184675: step 21910, loss 0.548143.
Train: 2018-08-02T00:15:35.350233: step 21911, loss 0.545372.
Train: 2018-08-02T00:15:35.515789: step 21912, loss 0.562408.
Train: 2018-08-02T00:15:35.683341: step 21913, loss 0.545341.
Train: 2018-08-02T00:15:35.845907: step 21914, loss 0.511158.
Train: 2018-08-02T00:15:36.013490: step 21915, loss 0.596624.
Train: 2018-08-02T00:15:36.188018: step 21916, loss 0.613783.
Train: 2018-08-02T00:15:36.354547: step 21917, loss 0.613793.
Train: 2018-08-02T00:15:36.519107: step 21918, loss 0.579526.
Train: 2018-08-02T00:15:36.687657: step 21919, loss 0.579514.
Train: 2018-08-02T00:15:36.853246: step 21920, loss 0.528216.
Test: 2018-08-02T00:15:37.381812: step 21920, loss 0.548095.
Train: 2018-08-02T00:15:37.547383: step 21921, loss 0.767485.
Train: 2018-08-02T00:15:37.713914: step 21922, loss 0.613526.
Train: 2018-08-02T00:15:37.879507: step 21923, loss 0.494492.
Train: 2018-08-02T00:15:38.046056: step 21924, loss 0.562422.
Train: 2018-08-02T00:15:38.219562: step 21925, loss 0.528599.
Train: 2018-08-02T00:15:38.397118: step 21926, loss 0.680692.
Train: 2018-08-02T00:15:38.567655: step 21927, loss 0.528741.
Train: 2018-08-02T00:15:38.742190: step 21928, loss 0.612905.
Train: 2018-08-02T00:15:38.909747: step 21929, loss 0.596019.
Train: 2018-08-02T00:15:39.072313: step 21930, loss 0.445303.
Test: 2018-08-02T00:15:39.616827: step 21930, loss 0.548466.
Train: 2018-08-02T00:15:39.783407: step 21931, loss 0.545745.
Train: 2018-08-02T00:15:39.949967: step 21932, loss 0.579203.
Train: 2018-08-02T00:15:40.117513: step 21933, loss 0.66281.
Train: 2018-08-02T00:15:40.284042: step 21934, loss 0.529096.
Train: 2018-08-02T00:15:40.449601: step 21935, loss 0.545811.
Train: 2018-08-02T00:15:40.619147: step 21936, loss 0.579168.
Train: 2018-08-02T00:15:40.786699: step 21937, loss 0.529172.
Train: 2018-08-02T00:15:40.951259: step 21938, loss 0.495843.
Train: 2018-08-02T00:15:41.123828: step 21939, loss 0.612536.
Train: 2018-08-02T00:15:41.292347: step 21940, loss 0.579177.
Test: 2018-08-02T00:15:41.824924: step 21940, loss 0.548512.
Train: 2018-08-02T00:15:41.991490: step 21941, loss 0.595868.
Train: 2018-08-02T00:15:42.157035: step 21942, loss 0.56249.
Train: 2018-08-02T00:15:42.321595: step 21943, loss 0.562491.
Train: 2018-08-02T00:15:42.500119: step 21944, loss 0.679274.
Train: 2018-08-02T00:15:42.668699: step 21945, loss 0.662441.
Train: 2018-08-02T00:15:42.842230: step 21946, loss 0.479465.
Train: 2018-08-02T00:15:43.007770: step 21947, loss 0.545937.
Train: 2018-08-02T00:15:43.179303: step 21948, loss 0.562533.
Train: 2018-08-02T00:15:43.351874: step 21949, loss 0.562538.
Train: 2018-08-02T00:15:43.519393: step 21950, loss 0.579103.
Test: 2018-08-02T00:15:44.047980: step 21950, loss 0.548682.
Train: 2018-08-02T00:15:44.211575: step 21951, loss 0.479774.
Train: 2018-08-02T00:15:44.383085: step 21952, loss 0.446559.
Train: 2018-08-02T00:15:44.549664: step 21953, loss 0.579129.
Train: 2018-08-02T00:15:44.716219: step 21954, loss 0.595796.
Train: 2018-08-02T00:15:44.881752: step 21955, loss 0.445793.
Train: 2018-08-02T00:15:45.045314: step 21956, loss 0.646095.
Train: 2018-08-02T00:15:45.210903: step 21957, loss 0.629476.
Train: 2018-08-02T00:15:45.381416: step 21958, loss 0.579227.
Train: 2018-08-02T00:15:45.544012: step 21959, loss 0.596003.
Train: 2018-08-02T00:15:45.718515: step 21960, loss 0.528919.
Test: 2018-08-02T00:15:46.240120: step 21960, loss 0.548407.
Train: 2018-08-02T00:15:46.409682: step 21961, loss 0.461779.
Train: 2018-08-02T00:15:46.576222: step 21962, loss 0.461574.
Train: 2018-08-02T00:15:46.740808: step 21963, loss 0.663635.
Train: 2018-08-02T00:15:46.905342: step 21964, loss 0.63.
Train: 2018-08-02T00:15:47.071897: step 21965, loss 0.528627.
Train: 2018-08-02T00:15:47.240447: step 21966, loss 0.545508.
Train: 2018-08-02T00:15:47.407001: step 21967, loss 0.42693.
Train: 2018-08-02T00:15:47.574584: step 21968, loss 0.528443.
Train: 2018-08-02T00:15:47.743133: step 21969, loss 0.494255.
Train: 2018-08-02T00:15:47.914645: step 21970, loss 0.528201.
Test: 2018-08-02T00:15:48.461183: step 21970, loss 0.548028.
Train: 2018-08-02T00:15:48.637711: step 21971, loss 0.562403.
Train: 2018-08-02T00:15:48.804297: step 21972, loss 0.493503.
Train: 2018-08-02T00:15:48.974840: step 21973, loss 0.580857.
Train: 2018-08-02T00:15:49.149356: step 21974, loss 0.475666.
Train: 2018-08-02T00:15:49.313934: step 21975, loss 0.649532.
Train: 2018-08-02T00:15:49.483450: step 21976, loss 0.562439.
Train: 2018-08-02T00:15:49.649007: step 21977, loss 0.562448.
Train: 2018-08-02T00:15:49.810576: step 21978, loss 0.632591.
Train: 2018-08-02T00:15:49.974157: step 21979, loss 0.720375.
Train: 2018-08-02T00:15:50.137701: step 21980, loss 0.49237.
Test: 2018-08-02T00:15:50.673270: step 21980, loss 0.547786.
Train: 2018-08-02T00:15:50.839857: step 21981, loss 0.614991.
Train: 2018-08-02T00:15:51.006380: step 21982, loss 0.52746.
Train: 2018-08-02T00:15:51.168975: step 21983, loss 0.405104.
Train: 2018-08-02T00:15:51.341509: step 21984, loss 0.56245.
Train: 2018-08-02T00:15:51.520037: step 21985, loss 0.439766.
Train: 2018-08-02T00:15:51.686561: step 21986, loss 0.562469.
Train: 2018-08-02T00:15:51.856107: step 21987, loss 0.615314.
Train: 2018-08-02T00:15:52.018674: step 21988, loss 0.56249.
Train: 2018-08-02T00:15:52.193240: step 21989, loss 0.527193.
Train: 2018-08-02T00:15:52.359761: step 21990, loss 0.544831.
Test: 2018-08-02T00:15:52.897355: step 21990, loss 0.547695.
Train: 2018-08-02T00:15:53.061884: step 21991, loss 0.633304.
Train: 2018-08-02T00:15:53.225447: step 21992, loss 0.527114.
Train: 2018-08-02T00:15:53.390007: step 21993, loss 0.509389.
Train: 2018-08-02T00:15:53.564571: step 21994, loss 0.597985.
Train: 2018-08-02T00:15:53.742104: step 21995, loss 0.598003.
Train: 2018-08-02T00:15:53.927602: step 21996, loss 0.580262.
Train: 2018-08-02T00:15:54.102104: step 21997, loss 0.615699.
Train: 2018-08-02T00:15:54.266694: step 21998, loss 0.52711.
Train: 2018-08-02T00:15:54.437239: step 21999, loss 0.633275.
Train: 2018-08-02T00:15:54.609773: step 22000, loss 0.438865.
Test: 2018-08-02T00:15:55.144318: step 22000, loss 0.54771.
Train: 2018-08-02T00:15:55.901113: step 22001, loss 0.438865.
Train: 2018-08-02T00:15:56.064702: step 22002, loss 0.456382.
Train: 2018-08-02T00:15:56.241234: step 22003, loss 0.633462.
Train: 2018-08-02T00:15:56.409753: step 22004, loss 0.633565.
Train: 2018-08-02T00:15:56.575311: step 22005, loss 0.651341.
Train: 2018-08-02T00:15:56.740892: step 22006, loss 0.72221.
Train: 2018-08-02T00:15:56.903463: step 22007, loss 0.5802.
Train: 2018-08-02T00:15:57.069988: step 22008, loss 0.527218.
Train: 2018-08-02T00:15:57.234579: step 22009, loss 0.562476.
Train: 2018-08-02T00:15:57.400106: step 22010, loss 0.615132.
Test: 2018-08-02T00:15:57.927698: step 22010, loss 0.547789.
Train: 2018-08-02T00:15:58.097279: step 22011, loss 0.685009.
Train: 2018-08-02T00:15:58.263796: step 22012, loss 0.57987.
Train: 2018-08-02T00:15:58.425396: step 22013, loss 0.510312.
Train: 2018-08-02T00:15:58.593941: step 22014, loss 0.475821.
Train: 2018-08-02T00:15:58.761498: step 22015, loss 0.579701.
Train: 2018-08-02T00:15:58.927024: step 22016, loss 0.47609.
Train: 2018-08-02T00:15:59.098596: step 22017, loss 0.579663.
Train: 2018-08-02T00:15:59.279110: step 22018, loss 0.545158.
Train: 2018-08-02T00:15:59.446636: step 22019, loss 0.527922.
Train: 2018-08-02T00:15:59.611195: step 22020, loss 0.441695.
Test: 2018-08-02T00:16:00.137835: step 22020, loss 0.547942.
Train: 2018-08-02T00:16:00.304341: step 22021, loss 0.596958.
Train: 2018-08-02T00:16:00.466907: step 22022, loss 0.47598.
Train: 2018-08-02T00:16:00.641471: step 22023, loss 0.406464.
Train: 2018-08-02T00:16:00.820961: step 22024, loss 0.562423.
Train: 2018-08-02T00:16:00.983558: step 22025, loss 0.562472.
Train: 2018-08-02T00:16:01.149085: step 22026, loss 0.579953.
Train: 2018-08-02T00:16:01.310652: step 22027, loss 0.632646.
Train: 2018-08-02T00:16:01.481196: step 22028, loss 0.597602.
Train: 2018-08-02T00:16:01.651739: step 22029, loss 0.597626.
Train: 2018-08-02T00:16:01.819291: step 22030, loss 0.72067.
Test: 2018-08-02T00:16:02.363836: step 22030, loss 0.547771.
Train: 2018-08-02T00:16:02.529394: step 22031, loss 0.422126.
Train: 2018-08-02T00:16:02.701933: step 22032, loss 0.615077.
Train: 2018-08-02T00:16:02.871504: step 22033, loss 0.544931.
Train: 2018-08-02T00:16:03.035073: step 22034, loss 0.597479.
Train: 2018-08-02T00:16:03.202594: step 22035, loss 0.492468.
Train: 2018-08-02T00:16:03.366182: step 22036, loss 0.527462.
Train: 2018-08-02T00:16:03.530750: step 22037, loss 0.509956.
Train: 2018-08-02T00:16:03.697271: step 22038, loss 0.509915.
Train: 2018-08-02T00:16:03.909799: step 22039, loss 0.615069.
Train: 2018-08-02T00:16:04.087364: step 22040, loss 0.667734.
Test: 2018-08-02T00:16:04.622898: step 22040, loss 0.547779.
Train: 2018-08-02T00:16:04.792439: step 22041, loss 0.597515.
Train: 2018-08-02T00:16:04.957997: step 22042, loss 0.544946.
Train: 2018-08-02T00:16:05.120586: step 22043, loss 0.59742.
Train: 2018-08-02T00:16:05.288114: step 22044, loss 0.632287.
Train: 2018-08-02T00:16:05.458658: step 22045, loss 0.6147.
Train: 2018-08-02T00:16:05.624240: step 22046, loss 0.510298.
Train: 2018-08-02T00:16:05.788810: step 22047, loss 0.510389.
Train: 2018-08-02T00:16:05.954333: step 22048, loss 0.562416.
Train: 2018-08-02T00:16:06.121886: step 22049, loss 0.614341.
Train: 2018-08-02T00:16:06.284481: step 22050, loss 0.648823.
Test: 2018-08-02T00:16:06.812051: step 22050, loss 0.547972.
Train: 2018-08-02T00:16:06.976600: step 22051, loss 0.579646.
Train: 2018-08-02T00:16:07.141170: step 22052, loss 0.562406.
Train: 2018-08-02T00:16:07.320680: step 22053, loss 0.459471.
Train: 2018-08-02T00:16:07.488249: step 22054, loss 0.425257.
Train: 2018-08-02T00:16:07.651795: step 22055, loss 0.510915.
Train: 2018-08-02T00:16:07.816386: step 22056, loss 0.682757.
Train: 2018-08-02T00:16:07.996900: step 22057, loss 0.579598.
Train: 2018-08-02T00:16:08.157444: step 22058, loss 0.648346.
Train: 2018-08-02T00:16:08.322035: step 22059, loss 0.613899.
Train: 2018-08-02T00:16:08.490566: step 22060, loss 0.545273.
Test: 2018-08-02T00:16:09.025124: step 22060, loss 0.548081.
Train: 2018-08-02T00:16:09.190681: step 22061, loss 0.613726.
Train: 2018-08-02T00:16:09.357267: step 22062, loss 0.596552.
Train: 2018-08-02T00:16:09.517831: step 22063, loss 0.528343.
Train: 2018-08-02T00:16:09.680400: step 22064, loss 0.477376.
Train: 2018-08-02T00:16:09.843966: step 22065, loss 0.613424.
Train: 2018-08-02T00:16:10.012484: step 22066, loss 0.545427.
Train: 2018-08-02T00:16:10.181065: step 22067, loss 0.443559.
Train: 2018-08-02T00:16:10.346617: step 22068, loss 0.596412.
Train: 2018-08-02T00:16:10.509188: step 22069, loss 0.579422.
Train: 2018-08-02T00:16:10.674715: step 22070, loss 0.477336.
Test: 2018-08-02T00:16:11.212278: step 22070, loss 0.548141.
Train: 2018-08-02T00:16:11.379845: step 22071, loss 0.54537.
Train: 2018-08-02T00:16:11.545387: step 22072, loss 0.545341.
Train: 2018-08-02T00:16:11.710945: step 22073, loss 0.528216.
Train: 2018-08-02T00:16:11.875530: step 22074, loss 0.61379.
Train: 2018-08-02T00:16:12.042091: step 22075, loss 0.545258.
Train: 2018-08-02T00:16:12.208613: step 22076, loss 0.407899.
Train: 2018-08-02T00:16:12.377163: step 22077, loss 0.631287.
Train: 2018-08-02T00:16:12.544747: step 22078, loss 0.545155.
Train: 2018-08-02T00:16:12.706308: step 22079, loss 0.579695.
Train: 2018-08-02T00:16:12.872839: step 22080, loss 0.545103.
Test: 2018-08-02T00:16:13.412395: step 22080, loss 0.5479.
Train: 2018-08-02T00:16:13.577979: step 22081, loss 0.579752.
Train: 2018-08-02T00:16:13.745505: step 22082, loss 0.545064.
Train: 2018-08-02T00:16:13.919042: step 22083, loss 0.510297.
Train: 2018-08-02T00:16:14.092577: step 22084, loss 0.545023.
Train: 2018-08-02T00:16:14.252150: step 22085, loss 0.614735.
Train: 2018-08-02T00:16:14.430682: step 22086, loss 0.492644.
Train: 2018-08-02T00:16:14.597258: step 22087, loss 0.527492.
Train: 2018-08-02T00:16:14.767800: step 22088, loss 0.597464.
Train: 2018-08-02T00:16:14.936322: step 22089, loss 0.667614.
Train: 2018-08-02T00:16:15.111853: step 22090, loss 0.457337.
Test: 2018-08-02T00:16:15.646454: step 22090, loss 0.547775.
Train: 2018-08-02T00:16:15.812007: step 22091, loss 0.544923.
Train: 2018-08-02T00:16:15.991501: step 22092, loss 0.492253.
Train: 2018-08-02T00:16:16.156060: step 22093, loss 0.597638.
Train: 2018-08-02T00:16:16.318651: step 22094, loss 0.580079.
Train: 2018-08-02T00:16:16.479223: step 22095, loss 0.439205.
Train: 2018-08-02T00:16:16.652759: step 22096, loss 0.509556.
Train: 2018-08-02T00:16:16.825272: step 22097, loss 0.5802.
Train: 2018-08-02T00:16:16.993847: step 22098, loss 0.509362.
Train: 2018-08-02T00:16:17.168355: step 22099, loss 0.598063.
Train: 2018-08-02T00:16:17.336933: step 22100, loss 0.651484.
Test: 2018-08-02T00:16:17.863498: step 22100, loss 0.54766.
Train: 2018-08-02T00:16:18.677291: step 22101, loss 0.562555.
Train: 2018-08-02T00:16:18.839891: step 22102, loss 0.562555.
Train: 2018-08-02T00:16:19.014421: step 22103, loss 0.509197.
Train: 2018-08-02T00:16:19.183967: step 22104, loss 0.544764.
Train: 2018-08-02T00:16:19.350525: step 22105, loss 0.651578.
Train: 2018-08-02T00:16:19.517046: step 22106, loss 0.562556.
Train: 2018-08-02T00:16:19.679612: step 22107, loss 0.56255.
Train: 2018-08-02T00:16:19.846191: step 22108, loss 0.562543.
Train: 2018-08-02T00:16:20.017741: step 22109, loss 0.562537.
Train: 2018-08-02T00:16:20.187255: step 22110, loss 0.527063.
Test: 2018-08-02T00:16:20.711864: step 22110, loss 0.547683.
Train: 2018-08-02T00:16:20.892395: step 22111, loss 0.562527.
Train: 2018-08-02T00:16:21.063942: step 22112, loss 0.456216.
Train: 2018-08-02T00:16:21.233489: step 22113, loss 0.615726.
Train: 2018-08-02T00:16:21.398048: step 22114, loss 0.491602.
Train: 2018-08-02T00:16:21.560614: step 22115, loss 0.615771.
Train: 2018-08-02T00:16:21.741107: step 22116, loss 0.562535.
Train: 2018-08-02T00:16:21.903698: step 22117, loss 0.651237.
Train: 2018-08-02T00:16:22.068227: step 22118, loss 0.544806.
Train: 2018-08-02T00:16:22.248769: step 22119, loss 0.474025.
Train: 2018-08-02T00:16:22.414302: step 22120, loss 0.61561.
Test: 2018-08-02T00:16:22.956851: step 22120, loss 0.5477.
Train: 2018-08-02T00:16:23.122424: step 22121, loss 0.527138.
Train: 2018-08-02T00:16:23.293949: step 22122, loss 0.615549.
Train: 2018-08-02T00:16:23.457543: step 22123, loss 0.650813.
Train: 2018-08-02T00:16:23.626087: step 22124, loss 0.632989.
Train: 2018-08-02T00:16:23.804615: step 22125, loss 0.615196.
Train: 2018-08-02T00:16:23.969171: step 22126, loss 0.474866.
Train: 2018-08-02T00:16:24.133705: step 22127, loss 0.667343.
Train: 2018-08-02T00:16:24.307266: step 22128, loss 0.475288.
Train: 2018-08-02T00:16:24.477811: step 22129, loss 0.545027.
Train: 2018-08-02T00:16:24.651351: step 22130, loss 0.527674.
Test: 2018-08-02T00:16:25.189881: step 22130, loss 0.547882.
Train: 2018-08-02T00:16:25.357460: step 22131, loss 0.579777.
Train: 2018-08-02T00:16:25.522024: step 22132, loss 0.510394.
Train: 2018-08-02T00:16:25.689578: step 22133, loss 0.562415.
Train: 2018-08-02T00:16:25.862115: step 22134, loss 0.614406.
Train: 2018-08-02T00:16:26.031632: step 22135, loss 0.614353.
Train: 2018-08-02T00:16:26.195225: step 22136, loss 0.562409.
Train: 2018-08-02T00:16:26.365737: step 22137, loss 0.579669.
Train: 2018-08-02T00:16:26.532323: step 22138, loss 0.545169.
Train: 2018-08-02T00:16:26.704831: step 22139, loss 0.579619.
Train: 2018-08-02T00:16:26.868394: step 22140, loss 0.596788.
Test: 2018-08-02T00:16:27.392992: step 22140, loss 0.548029.
Train: 2018-08-02T00:16:27.557577: step 22141, loss 0.545238.
Train: 2018-08-02T00:16:27.729103: step 22142, loss 0.596689.
Train: 2018-08-02T00:16:27.893653: step 22143, loss 0.579519.
Train: 2018-08-02T00:16:28.057216: step 22144, loss 0.562404.
Train: 2018-08-02T00:16:28.218818: step 22145, loss 0.528279.
Train: 2018-08-02T00:16:28.384341: step 22146, loss 0.562407.
Train: 2018-08-02T00:16:28.550923: step 22147, loss 0.59648.
Train: 2018-08-02T00:16:28.715482: step 22148, loss 0.579427.
Train: 2018-08-02T00:16:28.879050: step 22149, loss 0.596406.
Train: 2018-08-02T00:16:29.050592: step 22150, loss 0.579387.
Test: 2018-08-02T00:16:29.579161: step 22150, loss 0.54823.
Train: 2018-08-02T00:16:29.741714: step 22151, loss 0.52853.
Train: 2018-08-02T00:16:29.914279: step 22152, loss 0.596281.
Train: 2018-08-02T00:16:30.079809: step 22153, loss 0.630059.
Train: 2018-08-02T00:16:30.245391: step 22154, loss 0.461186.
Train: 2018-08-02T00:16:30.410923: step 22155, loss 0.528698.
Train: 2018-08-02T00:16:30.579474: step 22156, loss 0.646791.
Train: 2018-08-02T00:16:30.741041: step 22157, loss 0.596148.
Train: 2018-08-02T00:16:30.905627: step 22158, loss 0.528773.
Train: 2018-08-02T00:16:31.068166: step 22159, loss 0.528796.
Train: 2018-08-02T00:16:31.231730: step 22160, loss 0.596092.
Test: 2018-08-02T00:16:31.753335: step 22160, loss 0.548365.
Train: 2018-08-02T00:16:31.916929: step 22161, loss 0.629713.
Train: 2018-08-02T00:16:32.083453: step 22162, loss 0.562452.
Train: 2018-08-02T00:16:32.255019: step 22163, loss 0.495346.
Train: 2018-08-02T00:16:32.418558: step 22164, loss 0.495343.
Train: 2018-08-02T00:16:32.592124: step 22165, loss 0.596044.
Train: 2018-08-02T00:16:32.758648: step 22166, loss 0.596059.
Train: 2018-08-02T00:16:32.927197: step 22167, loss 0.512031.
Train: 2018-08-02T00:16:33.087794: step 22168, loss 0.495166.
Train: 2018-08-02T00:16:33.252329: step 22169, loss 0.579288.
Train: 2018-08-02T00:16:33.416919: step 22170, loss 0.562433.
Test: 2018-08-02T00:16:33.932510: step 22170, loss 0.548281.
Train: 2018-08-02T00:16:34.107081: step 22171, loss 0.545534.
Train: 2018-08-02T00:16:34.273598: step 22172, loss 0.511667.
Train: 2018-08-02T00:16:34.439156: step 22173, loss 0.613277.
Train: 2018-08-02T00:16:34.611725: step 22174, loss 0.579388.
Train: 2018-08-02T00:16:34.776285: step 22175, loss 0.5794.
Train: 2018-08-02T00:16:34.937848: step 22176, loss 0.477425.
Train: 2018-08-02T00:16:35.105375: step 22177, loss 0.494304.
Train: 2018-08-02T00:16:35.276941: step 22178, loss 0.511201.
Train: 2018-08-02T00:16:35.440479: step 22179, loss 0.596638.
Train: 2018-08-02T00:16:35.609029: step 22180, loss 0.528092.
Test: 2018-08-02T00:16:36.152589: step 22180, loss 0.548002.
Train: 2018-08-02T00:16:36.380526: step 22181, loss 0.562402.
Train: 2018-08-02T00:16:36.546085: step 22182, loss 0.493471.
Train: 2018-08-02T00:16:36.710649: step 22183, loss 0.510562.
Train: 2018-08-02T00:16:36.877170: step 22184, loss 0.6491.
Train: 2018-08-02T00:16:37.051703: step 22185, loss 0.545052.
Train: 2018-08-02T00:16:37.220253: step 22186, loss 0.614614.
Train: 2018-08-02T00:16:37.388827: step 22187, loss 0.562426.
Train: 2018-08-02T00:16:37.550400: step 22188, loss 0.562429.
Train: 2018-08-02T00:16:37.722934: step 22189, loss 0.527564.
Train: 2018-08-02T00:16:37.897473: step 22190, loss 0.475185.
Test: 2018-08-02T00:16:38.432039: step 22190, loss 0.547802.
Train: 2018-08-02T00:16:38.605573: step 22191, loss 0.562442.
Train: 2018-08-02T00:16:38.767143: step 22192, loss 0.509913.
Train: 2018-08-02T00:16:38.929709: step 22193, loss 0.615108.
Train: 2018-08-02T00:16:39.102246: step 22194, loss 0.509758.
Train: 2018-08-02T00:16:39.263788: step 22195, loss 0.580074.
Train: 2018-08-02T00:16:39.432363: step 22196, loss 0.527246.
Train: 2018-08-02T00:16:39.602909: step 22197, loss 0.668356.
Train: 2018-08-02T00:16:39.770435: step 22198, loss 0.633057.
Train: 2018-08-02T00:16:39.935992: step 22199, loss 0.456752.
Train: 2018-08-02T00:16:40.103578: step 22200, loss 0.668246.
Test: 2018-08-02T00:16:40.640109: step 22200, loss 0.547736.
Train: 2018-08-02T00:16:41.431777: step 22201, loss 0.509661.
Train: 2018-08-02T00:16:41.598301: step 22202, loss 0.615269.
Train: 2018-08-02T00:16:41.767873: step 22203, loss 0.632778.
Train: 2018-08-02T00:16:41.937425: step 22204, loss 0.527375.
Train: 2018-08-02T00:16:42.103985: step 22205, loss 0.457351.
Train: 2018-08-02T00:16:42.282498: step 22206, loss 0.492385.
Train: 2018-08-02T00:16:42.447063: step 22207, loss 0.685171.
Train: 2018-08-02T00:16:42.612589: step 22208, loss 0.52742.
Train: 2018-08-02T00:16:42.783134: step 22209, loss 0.562449.
Train: 2018-08-02T00:16:42.949719: step 22210, loss 0.544946.
Test: 2018-08-02T00:16:43.486285: step 22210, loss 0.547794.
Train: 2018-08-02T00:16:43.656824: step 22211, loss 0.579942.
Train: 2018-08-02T00:16:43.824372: step 22212, loss 0.509979.
Train: 2018-08-02T00:16:44.004868: step 22213, loss 0.562444.
Train: 2018-08-02T00:16:44.171422: step 22214, loss 0.632417.
Train: 2018-08-02T00:16:44.334031: step 22215, loss 0.510007.
Train: 2018-08-02T00:16:44.502568: step 22216, loss 0.544965.
Train: 2018-08-02T00:16:44.667129: step 22217, loss 0.597392.
Train: 2018-08-02T00:16:44.834649: step 22218, loss 0.597373.
Train: 2018-08-02T00:16:45.015200: step 22219, loss 0.510081.
Train: 2018-08-02T00:16:45.181752: step 22220, loss 0.562434.
Test: 2018-08-02T00:16:45.715296: step 22220, loss 0.547826.
Train: 2018-08-02T00:16:45.882847: step 22221, loss 0.527547.
Train: 2018-08-02T00:16:46.046410: step 22222, loss 0.544987.
Train: 2018-08-02T00:16:46.216986: step 22223, loss 0.614791.
Train: 2018-08-02T00:16:46.383543: step 22224, loss 0.544988.
Train: 2018-08-02T00:16:46.556065: step 22225, loss 0.510109.
Train: 2018-08-02T00:16:46.722603: step 22226, loss 0.597331.
Train: 2018-08-02T00:16:46.885199: step 22227, loss 0.492646.
Train: 2018-08-02T00:16:47.057740: step 22228, loss 0.527516.
Train: 2018-08-02T00:16:47.225276: step 22229, loss 0.510004.
Train: 2018-08-02T00:16:47.395828: step 22230, loss 0.509927.
Test: 2018-08-02T00:16:47.939351: step 22230, loss 0.547768.
Train: 2018-08-02T00:16:48.105936: step 22231, loss 0.615087.
Train: 2018-08-02T00:16:48.277495: step 22232, loss 0.615151.
Train: 2018-08-02T00:16:48.441009: step 22233, loss 0.615165.
Train: 2018-08-02T00:16:48.610556: step 22234, loss 0.562462.
Train: 2018-08-02T00:16:48.780102: step 22235, loss 0.667746.
Train: 2018-08-02T00:16:48.946682: step 22236, loss 0.457363.
Train: 2018-08-02T00:16:49.113237: step 22237, loss 0.527434.
Train: 2018-08-02T00:16:49.275778: step 22238, loss 0.527434.
Train: 2018-08-02T00:16:49.443329: step 22239, loss 0.597478.
Train: 2018-08-02T00:16:49.613904: step 22240, loss 0.544938.
Test: 2018-08-02T00:16:50.138471: step 22240, loss 0.547785.
Train: 2018-08-02T00:16:50.305054: step 22241, loss 0.579961.
Train: 2018-08-02T00:16:50.476597: step 22242, loss 0.649984.
Train: 2018-08-02T00:16:50.642124: step 22243, loss 0.649853.
Train: 2018-08-02T00:16:50.821645: step 22244, loss 0.527553.
Train: 2018-08-02T00:16:50.994215: step 22245, loss 0.701684.
Train: 2018-08-02T00:16:51.156781: step 22246, loss 0.493028.
Train: 2018-08-02T00:16:51.328321: step 22247, loss 0.579717.
Train: 2018-08-02T00:16:51.492850: step 22248, loss 0.562406.
Train: 2018-08-02T00:16:51.656444: step 22249, loss 0.562403.
Train: 2018-08-02T00:16:51.824963: step 22250, loss 0.562401.
Test: 2018-08-02T00:16:52.350558: step 22250, loss 0.548025.
Train: 2018-08-02T00:16:52.515117: step 22251, loss 0.459393.
Train: 2018-08-02T00:16:52.681682: step 22252, loss 0.648225.
Train: 2018-08-02T00:16:52.845261: step 22253, loss 0.613828.
Train: 2018-08-02T00:16:53.025786: step 22254, loss 0.511071.
Train: 2018-08-02T00:16:53.190312: step 22255, loss 0.579495.
Train: 2018-08-02T00:16:53.358862: step 22256, loss 0.664849.
Train: 2018-08-02T00:16:53.524450: step 22257, loss 0.613508.
Train: 2018-08-02T00:16:53.691998: step 22258, loss 0.562412.
Train: 2018-08-02T00:16:53.856532: step 22259, loss 0.647135.
Train: 2018-08-02T00:16:54.024115: step 22260, loss 0.511771.
Test: 2018-08-02T00:16:54.563642: step 22260, loss 0.54833.
Train: 2018-08-02T00:16:54.734186: step 22261, loss 0.545591.
Train: 2018-08-02T00:16:54.904755: step 22262, loss 0.680164.
Train: 2018-08-02T00:16:55.072282: step 22263, loss 0.579224.
Train: 2018-08-02T00:16:55.241829: step 22264, loss 0.412067.
Train: 2018-08-02T00:16:55.402400: step 22265, loss 0.478965.
Train: 2018-08-02T00:16:55.568985: step 22266, loss 0.579192.
Train: 2018-08-02T00:16:55.734537: step 22267, loss 0.629372.
Train: 2018-08-02T00:16:55.898099: step 22268, loss 0.529038.
Train: 2018-08-02T00:16:56.066655: step 22269, loss 0.478865.
Train: 2018-08-02T00:16:56.233211: step 22270, loss 0.562466.
Test: 2018-08-02T00:16:56.765755: step 22270, loss 0.548417.
Train: 2018-08-02T00:16:56.932344: step 22271, loss 0.663071.
Train: 2018-08-02T00:16:57.104873: step 22272, loss 0.528925.
Train: 2018-08-02T00:16:57.276390: step 22273, loss 0.562457.
Train: 2018-08-02T00:16:57.447962: step 22274, loss 0.562455.
Train: 2018-08-02T00:16:57.616511: step 22275, loss 0.596033.
Train: 2018-08-02T00:16:57.780044: step 22276, loss 0.579243.
Train: 2018-08-02T00:16:57.944634: step 22277, loss 0.629608.
Train: 2018-08-02T00:16:58.112155: step 22278, loss 0.528913.
Train: 2018-08-02T00:16:58.281734: step 22279, loss 0.595992.
Train: 2018-08-02T00:16:58.458256: step 22280, loss 0.528954.
Test: 2018-08-02T00:16:58.988826: step 22280, loss 0.548435.
Train: 2018-08-02T00:16:59.153372: step 22281, loss 0.4787.
Train: 2018-08-02T00:16:59.316935: step 22282, loss 0.596002.
Train: 2018-08-02T00:16:59.476508: step 22283, loss 0.59602.
Train: 2018-08-02T00:16:59.655062: step 22284, loss 0.495311.
Train: 2018-08-02T00:16:59.821585: step 22285, loss 0.730499.
Train: 2018-08-02T00:16:59.992130: step 22286, loss 0.629592.
Train: 2018-08-02T00:17:00.158717: step 22287, loss 0.612718.
Train: 2018-08-02T00:17:00.321274: step 22288, loss 0.42879.
Train: 2018-08-02T00:17:00.483841: step 22289, loss 0.529064.
Train: 2018-08-02T00:17:00.655387: step 22290, loss 0.612616.
Test: 2018-08-02T00:17:01.181950: step 22290, loss 0.548487.
Train: 2018-08-02T00:17:01.347506: step 22291, loss 0.529062.
Train: 2018-08-02T00:17:01.513095: step 22292, loss 0.595902.
Train: 2018-08-02T00:17:01.679653: step 22293, loss 0.629319.
Train: 2018-08-02T00:17:01.843182: step 22294, loss 0.579177.
Train: 2018-08-02T00:17:02.004750: step 22295, loss 0.562489.
Train: 2018-08-02T00:17:02.171335: step 22296, loss 0.579157.
Train: 2018-08-02T00:17:02.336895: step 22297, loss 0.695682.
Train: 2018-08-02T00:17:02.501421: step 22298, loss 0.579123.
Train: 2018-08-02T00:17:02.659997: step 22299, loss 0.512848.
Train: 2018-08-02T00:17:02.825556: step 22300, loss 0.546008.
Test: 2018-08-02T00:17:03.354143: step 22300, loss 0.548719.
Train: 2018-08-02T00:17:04.164157: step 22301, loss 0.529508.
Train: 2018-08-02T00:17:04.327710: step 22302, loss 0.612117.
Train: 2018-08-02T00:17:04.496254: step 22303, loss 0.546058.
Train: 2018-08-02T00:17:04.659816: step 22304, loss 0.480068.
Train: 2018-08-02T00:17:04.833352: step 22305, loss 0.47998.
Train: 2018-08-02T00:17:04.998919: step 22306, loss 0.612197.
Train: 2018-08-02T00:17:05.174441: step 22307, loss 0.645402.
Train: 2018-08-02T00:17:05.339998: step 22308, loss 0.678566.
Train: 2018-08-02T00:17:05.505556: step 22309, loss 0.595647.
Train: 2018-08-02T00:17:05.682084: step 22310, loss 0.496452.
Test: 2018-08-02T00:17:06.213663: step 22310, loss 0.548725.
Train: 2018-08-02T00:17:06.379245: step 22311, loss 0.562558.
Train: 2018-08-02T00:17:06.544778: step 22312, loss 0.546045.
Train: 2018-08-02T00:17:06.710361: step 22313, loss 0.579075.
Train: 2018-08-02T00:17:06.871903: step 22314, loss 0.645141.
Train: 2018-08-02T00:17:07.036464: step 22315, loss 0.645061.
Train: 2018-08-02T00:17:07.212025: step 22316, loss 0.463795.
Train: 2018-08-02T00:17:07.378549: step 22317, loss 0.595509.
Train: 2018-08-02T00:17:07.540138: step 22318, loss 0.562593.
Train: 2018-08-02T00:17:07.704678: step 22319, loss 0.562597.
Train: 2018-08-02T00:17:07.868240: step 22320, loss 0.496839.
Test: 2018-08-02T00:17:08.397837: step 22320, loss 0.548813.
Train: 2018-08-02T00:17:08.564404: step 22321, loss 0.546137.
Train: 2018-08-02T00:17:08.730960: step 22322, loss 0.628474.
Train: 2018-08-02T00:17:08.894534: step 22323, loss 0.480195.
Train: 2018-08-02T00:17:09.059057: step 22324, loss 0.694579.
Train: 2018-08-02T00:17:09.225611: step 22325, loss 0.56257.
Train: 2018-08-02T00:17:09.389174: step 22326, loss 0.595553.
Train: 2018-08-02T00:17:09.556759: step 22327, loss 0.546097.
Train: 2018-08-02T00:17:09.720322: step 22328, loss 0.48019.
Train: 2018-08-02T00:17:09.883851: step 22329, loss 0.579066.
Train: 2018-08-02T00:17:10.048411: step 22330, loss 0.446975.
Test: 2018-08-02T00:17:10.576999: step 22330, loss 0.548677.
Train: 2018-08-02T00:17:10.743552: step 22331, loss 0.579095.
Train: 2018-08-02T00:17:10.913125: step 22332, loss 0.545931.
Train: 2018-08-02T00:17:11.087633: step 22333, loss 0.595769.
Train: 2018-08-02T00:17:11.257181: step 22334, loss 0.545836.
Train: 2018-08-02T00:17:11.420743: step 22335, loss 0.462341.
Train: 2018-08-02T00:17:11.588327: step 22336, loss 0.612696.
Train: 2018-08-02T00:17:11.754883: step 22337, loss 0.545674.
Train: 2018-08-02T00:17:11.915421: step 22338, loss 0.545625.
Train: 2018-08-02T00:17:12.090952: step 22339, loss 0.511859.
Train: 2018-08-02T00:17:12.252520: step 22340, loss 0.579331.
Test: 2018-08-02T00:17:12.798087: step 22340, loss 0.548226.
Train: 2018-08-02T00:17:12.967608: step 22341, loss 0.511575.
Train: 2018-08-02T00:17:13.132199: step 22342, loss 0.494427.
Train: 2018-08-02T00:17:13.297724: step 22343, loss 0.562404.
Train: 2018-08-02T00:17:13.469267: step 22344, loss 0.562401.
Train: 2018-08-02T00:17:13.638845: step 22345, loss 0.545241.
Train: 2018-08-02T00:17:13.804371: step 22346, loss 0.476365.
Train: 2018-08-02T00:17:13.971954: step 22347, loss 0.579675.
Train: 2018-08-02T00:17:14.136483: step 22348, loss 0.510445.
Train: 2018-08-02T00:17:14.301074: step 22349, loss 0.527659.
Train: 2018-08-02T00:17:14.464607: step 22350, loss 0.562431.
Test: 2018-08-02T00:17:15.004181: step 22350, loss 0.547795.
Train: 2018-08-02T00:17:15.168755: step 22351, loss 0.614921.
Train: 2018-08-02T00:17:15.332287: step 22352, loss 0.57998.
Train: 2018-08-02T00:17:15.498872: step 22353, loss 0.527353.
Train: 2018-08-02T00:17:15.666419: step 22354, loss 0.65039.
Train: 2018-08-02T00:17:15.829983: step 22355, loss 0.509701.
Train: 2018-08-02T00:17:15.993519: step 22356, loss 0.70333.
Train: 2018-08-02T00:17:16.159077: step 22357, loss 0.386588.
Train: 2018-08-02T00:17:16.328648: step 22358, loss 0.527259.
Train: 2018-08-02T00:17:16.500164: step 22359, loss 0.650666.
Train: 2018-08-02T00:17:16.666719: step 22360, loss 0.562489.
Test: 2018-08-02T00:17:17.206303: step 22360, loss 0.547719.
Train: 2018-08-02T00:17:17.371834: step 22361, loss 0.615404.
Train: 2018-08-02T00:17:17.548400: step 22362, loss 0.439103.
Train: 2018-08-02T00:17:17.713920: step 22363, loss 0.544849.
Train: 2018-08-02T00:17:17.877482: step 22364, loss 0.527183.
Train: 2018-08-02T00:17:18.057034: step 22365, loss 0.650886.
Train: 2018-08-02T00:17:18.222560: step 22366, loss 0.474134.
Train: 2018-08-02T00:17:18.384127: step 22367, loss 0.52713.
Train: 2018-08-02T00:17:18.549685: step 22368, loss 0.45626.
Train: 2018-08-02T00:17:18.714246: step 22369, loss 0.598033.
Train: 2018-08-02T00:17:18.884816: step 22370, loss 0.491444.
Test: 2018-08-02T00:17:19.402407: step 22370, loss 0.547648.
Train: 2018-08-02T00:17:19.570974: step 22371, loss 0.526938.
Train: 2018-08-02T00:17:19.735516: step 22372, loss 0.580438.
Train: 2018-08-02T00:17:19.898081: step 22373, loss 0.544718.
Train: 2018-08-02T00:17:20.062642: step 22374, loss 0.544706.
Train: 2018-08-02T00:17:20.230194: step 22375, loss 0.634387.
Train: 2018-08-02T00:17:20.396748: step 22376, loss 0.652362.
Train: 2018-08-02T00:17:20.565299: step 22377, loss 0.437123.
Train: 2018-08-02T00:17:20.736871: step 22378, loss 0.580574.
Train: 2018-08-02T00:17:20.899404: step 22379, loss 0.490863.
Train: 2018-08-02T00:17:21.060973: step 22380, loss 0.490807.
Test: 2018-08-02T00:17:21.594545: step 22380, loss 0.5476.
Train: 2018-08-02T00:17:21.757142: step 22381, loss 0.526688.
Train: 2018-08-02T00:17:21.927656: step 22382, loss 0.49061.
Train: 2018-08-02T00:17:22.096205: step 22383, loss 0.544651.
Train: 2018-08-02T00:17:22.260765: step 22384, loss 0.580833.
Train: 2018-08-02T00:17:22.429314: step 22385, loss 0.562756.
Train: 2018-08-02T00:17:22.594878: step 22386, loss 0.617209.
Train: 2018-08-02T00:17:22.762424: step 22387, loss 0.653528.
Train: 2018-08-02T00:17:22.927981: step 22388, loss 0.54463.
Train: 2018-08-02T00:17:23.097549: step 22389, loss 0.598985.
Train: 2018-08-02T00:17:23.263085: step 22390, loss 0.508455.
Test: 2018-08-02T00:17:23.793691: step 22390, loss 0.547584.
Train: 2018-08-02T00:17:23.962247: step 22391, loss 0.526565.
Train: 2018-08-02T00:17:24.129809: step 22392, loss 0.580793.
Train: 2018-08-02T00:17:24.294328: step 22393, loss 0.454347.
Train: 2018-08-02T00:17:24.458915: step 22394, loss 0.580787.
Train: 2018-08-02T00:17:24.624446: step 22395, loss 0.616935.
Train: 2018-08-02T00:17:24.788010: step 22396, loss 0.490473.
Train: 2018-08-02T00:17:24.955561: step 22397, loss 0.653015.
Train: 2018-08-02T00:17:25.124136: step 22398, loss 0.634856.
Train: 2018-08-02T00:17:25.290697: step 22399, loss 0.634684.
Train: 2018-08-02T00:17:25.457252: step 22400, loss 0.526738.
Test: 2018-08-02T00:17:25.996779: step 22400, loss 0.547619.
Train: 2018-08-02T00:17:26.791069: step 22401, loss 0.598437.
Train: 2018-08-02T00:17:26.954635: step 22402, loss 0.562591.
Train: 2018-08-02T00:17:27.119192: step 22403, loss 0.526929.
Train: 2018-08-02T00:17:27.287741: step 22404, loss 0.52698.
Train: 2018-08-02T00:17:27.456265: step 22405, loss 0.562539.
Train: 2018-08-02T00:17:27.623817: step 22406, loss 0.651211.
Train: 2018-08-02T00:17:27.792366: step 22407, loss 0.615591.
Train: 2018-08-02T00:17:27.957955: step 22408, loss 0.544848.
Train: 2018-08-02T00:17:28.137444: step 22409, loss 0.615266.
Train: 2018-08-02T00:17:28.303002: step 22410, loss 0.457191.
Test: 2018-08-02T00:17:28.839593: step 22410, loss 0.54778.
Train: 2018-08-02T00:17:29.013103: step 22411, loss 0.597488.
Train: 2018-08-02T00:17:29.177697: step 22412, loss 0.632391.
Train: 2018-08-02T00:17:29.352196: step 22413, loss 0.579873.
Train: 2018-08-02T00:17:29.519750: step 22414, loss 0.545025.
Train: 2018-08-02T00:17:29.684308: step 22415, loss 0.64921.
Train: 2018-08-02T00:17:29.851862: step 22416, loss 0.597018.
Train: 2018-08-02T00:17:30.017419: step 22417, loss 0.545155.
Train: 2018-08-02T00:17:30.198966: step 22418, loss 0.4248.
Train: 2018-08-02T00:17:30.367514: step 22419, loss 0.631154.
Train: 2018-08-02T00:17:30.540021: step 22420, loss 0.648211.
Test: 2018-08-02T00:17:31.077584: step 22420, loss 0.548065.
Train: 2018-08-02T00:17:31.254112: step 22421, loss 0.528162.
Train: 2018-08-02T00:17:31.424656: step 22422, loss 0.66493.
Train: 2018-08-02T00:17:31.604176: step 22423, loss 0.511294.
Train: 2018-08-02T00:17:31.771728: step 22424, loss 0.528403.
Train: 2018-08-02T00:17:31.939307: step 22425, loss 0.596373.
Train: 2018-08-02T00:17:32.110822: step 22426, loss 0.511555.
Train: 2018-08-02T00:17:32.277403: step 22427, loss 0.5963.
Train: 2018-08-02T00:17:32.447921: step 22428, loss 0.528574.
Train: 2018-08-02T00:17:32.614476: step 22429, loss 0.596253.
Train: 2018-08-02T00:17:32.786023: step 22430, loss 0.511721.
Test: 2018-08-02T00:17:33.316612: step 22430, loss 0.548272.
Train: 2018-08-02T00:17:33.484179: step 22431, loss 0.528623.
Train: 2018-08-02T00:17:33.649722: step 22432, loss 0.528605.
Train: 2018-08-02T00:17:33.816263: step 22433, loss 0.596272.
Train: 2018-08-02T00:17:33.981846: step 22434, loss 0.51162.
Train: 2018-08-02T00:17:34.149373: step 22435, loss 0.613272.
Train: 2018-08-02T00:17:34.323906: step 22436, loss 0.562415.
Train: 2018-08-02T00:17:34.487502: step 22437, loss 0.596343.
Train: 2018-08-02T00:17:34.655046: step 22438, loss 0.545452.
Train: 2018-08-02T00:17:34.816627: step 22439, loss 0.630275.
Train: 2018-08-02T00:17:34.985164: step 22440, loss 0.698038.
Test: 2018-08-02T00:17:35.506745: step 22440, loss 0.548262.
Train: 2018-08-02T00:17:35.672301: step 22441, loss 0.613155.
Train: 2018-08-02T00:17:35.836861: step 22442, loss 0.613019.
Train: 2018-08-02T00:17:36.004414: step 22443, loss 0.646482.
Train: 2018-08-02T00:17:36.178947: step 22444, loss 0.595945.
Train: 2018-08-02T00:17:36.341544: step 22445, loss 0.612505.
Train: 2018-08-02T00:17:36.509064: step 22446, loss 0.529319.
Train: 2018-08-02T00:17:36.677628: step 22447, loss 0.512907.
Train: 2018-08-02T00:17:36.843205: step 22448, loss 0.628602.
Train: 2018-08-02T00:17:37.010724: step 22449, loss 0.546121.
Train: 2018-08-02T00:17:37.175312: step 22450, loss 0.579034.
Test: 2018-08-02T00:17:37.686916: step 22450, loss 0.548894.
Train: 2018-08-02T00:17:37.865438: step 22451, loss 0.628209.
Train: 2018-08-02T00:17:38.033016: step 22452, loss 0.464533.
Train: 2018-08-02T00:17:38.198574: step 22453, loss 0.546318.
Train: 2018-08-02T00:17:38.367098: step 22454, loss 0.578996.
Train: 2018-08-02T00:17:38.538640: step 22455, loss 0.530009.
Train: 2018-08-02T00:17:38.702235: step 22456, loss 0.627996.
Train: 2018-08-02T00:17:38.867785: step 22457, loss 0.611642.
Train: 2018-08-02T00:17:39.032345: step 22458, loss 0.513758.
Train: 2018-08-02T00:17:39.194885: step 22459, loss 0.546374.
Train: 2018-08-02T00:17:39.363465: step 22460, loss 0.644235.
Test: 2018-08-02T00:17:39.904986: step 22460, loss 0.549035.
Train: 2018-08-02T00:17:40.082512: step 22461, loss 0.546385.
Train: 2018-08-02T00:17:40.244111: step 22462, loss 0.595278.
Train: 2018-08-02T00:17:40.406681: step 22463, loss 0.595266.
Train: 2018-08-02T00:17:40.572234: step 22464, loss 0.578976.
Train: 2018-08-02T00:17:40.733805: step 22465, loss 0.530193.
Train: 2018-08-02T00:17:40.911327: step 22466, loss 0.49768.
Train: 2018-08-02T00:17:41.078875: step 22467, loss 0.546425.
Train: 2018-08-02T00:17:41.246400: step 22468, loss 0.53009.
Train: 2018-08-02T00:17:41.409989: step 22469, loss 0.578994.
Train: 2018-08-02T00:17:41.576543: step 22470, loss 0.611712.
Test: 2018-08-02T00:17:42.111115: step 22470, loss 0.548934.
Train: 2018-08-02T00:17:42.280636: step 22471, loss 0.546272.
Train: 2018-08-02T00:17:42.443226: step 22472, loss 0.579016.
Train: 2018-08-02T00:17:42.609764: step 22473, loss 0.56262.
Train: 2018-08-02T00:17:42.772357: step 22474, loss 0.595449.
Train: 2018-08-02T00:17:42.935912: step 22475, loss 0.579034.
Train: 2018-08-02T00:17:43.100444: step 22476, loss 0.546166.
Train: 2018-08-02T00:17:43.274978: step 22477, loss 0.595489.
Train: 2018-08-02T00:17:43.440554: step 22478, loss 0.6284.
Train: 2018-08-02T00:17:43.611079: step 22479, loss 0.562597.
Train: 2018-08-02T00:17:43.777634: step 22480, loss 0.628348.
Test: 2018-08-02T00:17:44.316193: step 22480, loss 0.548863.
Train: 2018-08-02T00:17:44.481782: step 22481, loss 0.496939.
Train: 2018-08-02T00:17:44.649330: step 22482, loss 0.464096.
Train: 2018-08-02T00:17:44.812897: step 22483, loss 0.595487.
Train: 2018-08-02T00:17:44.977457: step 22484, loss 0.579049.
Train: 2018-08-02T00:17:45.145986: step 22485, loss 0.661452.
Train: 2018-08-02T00:17:45.313553: step 22486, loss 0.496698.
Train: 2018-08-02T00:17:45.476126: step 22487, loss 0.546095.
Train: 2018-08-02T00:17:45.650653: step 22488, loss 0.513078.
Train: 2018-08-02T00:17:45.813192: step 22489, loss 0.54603.
Train: 2018-08-02T00:17:45.981767: step 22490, loss 0.529429.
Test: 2018-08-02T00:17:46.497363: step 22490, loss 0.548628.
Train: 2018-08-02T00:17:46.663917: step 22491, loss 0.678672.
Train: 2018-08-02T00:17:46.828517: step 22492, loss 0.529318.
Train: 2018-08-02T00:17:46.997029: step 22493, loss 0.612364.
Train: 2018-08-02T00:17:47.164580: step 22494, loss 0.579132.
Train: 2018-08-02T00:17:47.331135: step 22495, loss 0.562509.
Train: 2018-08-02T00:17:47.510655: step 22496, loss 0.54588.
Train: 2018-08-02T00:17:47.680202: step 22497, loss 0.429415.
Train: 2018-08-02T00:17:47.846756: step 22498, loss 0.512457.
Train: 2018-08-02T00:17:48.008324: step 22499, loss 0.595925.
Train: 2018-08-02T00:17:48.172909: step 22500, loss 0.495395.
Test: 2018-08-02T00:17:48.701471: step 22500, loss 0.548361.
Train: 2018-08-02T00:17:49.464805: step 22501, loss 0.444721.
Train: 2018-08-02T00:17:49.631359: step 22502, loss 0.545534.
Train: 2018-08-02T00:17:49.796915: step 22503, loss 0.443659.
Train: 2018-08-02T00:17:49.958484: step 22504, loss 0.477113.
Train: 2018-08-02T00:17:50.123044: step 22505, loss 0.579561.
Train: 2018-08-02T00:17:50.286607: step 22506, loss 0.562403.
Train: 2018-08-02T00:17:50.464183: step 22507, loss 0.54508.
Train: 2018-08-02T00:17:50.628727: step 22508, loss 0.614646.
Train: 2018-08-02T00:17:50.795248: step 22509, loss 0.667213.
Train: 2018-08-02T00:17:50.969807: step 22510, loss 0.667372.
Test: 2018-08-02T00:17:51.509338: step 22510, loss 0.547798.
Train: 2018-08-02T00:17:51.673930: step 22511, loss 0.492495.
Train: 2018-08-02T00:17:51.835467: step 22512, loss 0.562445.
Train: 2018-08-02T00:17:52.001058: step 22513, loss 0.614979.
Train: 2018-08-02T00:17:52.171602: step 22514, loss 0.562446.
Train: 2018-08-02T00:17:52.339150: step 22515, loss 0.492431.
Train: 2018-08-02T00:17:52.514681: step 22516, loss 0.439845.
Train: 2018-08-02T00:17:52.683200: step 22517, loss 0.562459.
Train: 2018-08-02T00:17:52.845767: step 22518, loss 0.580052.
Train: 2018-08-02T00:17:53.012351: step 22519, loss 0.509658.
Train: 2018-08-02T00:17:53.175909: step 22520, loss 0.597764.
Test: 2018-08-02T00:17:53.708459: step 22520, loss 0.54771.
Train: 2018-08-02T00:17:53.873019: step 22521, loss 0.456554.
Train: 2018-08-02T00:17:54.038578: step 22522, loss 0.527119.
Train: 2018-08-02T00:17:54.203163: step 22523, loss 0.544791.
Train: 2018-08-02T00:17:54.372714: step 22524, loss 0.580324.
Train: 2018-08-02T00:17:54.546220: step 22525, loss 0.473525.
Train: 2018-08-02T00:17:54.716788: step 22526, loss 0.455473.
Train: 2018-08-02T00:17:54.885313: step 22527, loss 0.616359.
Train: 2018-08-02T00:17:55.064868: step 22528, loss 0.544688.
Train: 2018-08-02T00:17:55.239368: step 22529, loss 0.508685.
Train: 2018-08-02T00:17:55.413908: step 22530, loss 0.526619.
Test: 2018-08-02T00:17:55.949473: step 22530, loss 0.547584.
Train: 2018-08-02T00:17:56.122010: step 22531, loss 0.418062.
Train: 2018-08-02T00:17:56.299557: step 22532, loss 0.762451.
Train: 2018-08-02T00:17:56.466087: step 22533, loss 0.526452.
Train: 2018-08-02T00:17:56.638651: step 22534, loss 0.544616.
Train: 2018-08-02T00:17:56.803188: step 22535, loss 0.599229.
Train: 2018-08-02T00:17:56.968744: step 22536, loss 0.489977.
Train: 2018-08-02T00:17:57.131311: step 22537, loss 0.471693.
Train: 2018-08-02T00:17:57.294902: step 22538, loss 0.581123.
Train: 2018-08-02T00:17:57.466445: step 22539, loss 0.508033.
Train: 2018-08-02T00:17:57.638953: step 22540, loss 0.544591.
Test: 2018-08-02T00:17:58.176516: step 22540, loss 0.547573.
Train: 2018-08-02T00:17:58.343101: step 22541, loss 0.691312.
Train: 2018-08-02T00:17:58.512616: step 22542, loss 0.489607.
Train: 2018-08-02T00:17:58.678174: step 22543, loss 0.63626.
Train: 2018-08-02T00:17:58.844759: step 22544, loss 0.526274.
Train: 2018-08-02T00:17:59.023282: step 22545, loss 0.489663.
Train: 2018-08-02T00:17:59.190803: step 22546, loss 0.581224.
Train: 2018-08-02T00:17:59.357393: step 22547, loss 0.489659.
Train: 2018-08-02T00:17:59.528899: step 22548, loss 0.526272.
Train: 2018-08-02T00:17:59.702436: step 22549, loss 0.526258.
Train: 2018-08-02T00:17:59.871983: step 22550, loss 0.673038.
Test: 2018-08-02T00:18:00.408548: step 22550, loss 0.547573.
Train: 2018-08-02T00:18:00.585077: step 22551, loss 0.489581.
Train: 2018-08-02T00:18:00.752660: step 22552, loss 0.544592.
Train: 2018-08-02T00:18:00.924169: step 22553, loss 0.507914.
Train: 2018-08-02T00:18:01.085738: step 22554, loss 0.526243.
Train: 2018-08-02T00:18:01.248304: step 22555, loss 0.654752.
Train: 2018-08-02T00:18:01.423859: step 22556, loss 0.581288.
Train: 2018-08-02T00:18:01.598392: step 22557, loss 0.636245.
Train: 2018-08-02T00:18:01.758954: step 22558, loss 0.636073.
Train: 2018-08-02T00:18:01.926516: step 22559, loss 0.599338.
Train: 2018-08-02T00:18:02.099060: step 22560, loss 0.653741.
Test: 2018-08-02T00:18:02.634598: step 22560, loss 0.54758.
Train: 2018-08-02T00:18:02.801201: step 22561, loss 0.598976.
Train: 2018-08-02T00:18:02.966740: step 22562, loss 0.508585.
Train: 2018-08-02T00:18:03.135261: step 22563, loss 0.598609.
Train: 2018-08-02T00:18:03.298822: step 22564, loss 0.544706.
Train: 2018-08-02T00:18:03.463407: step 22565, loss 0.47331.
Train: 2018-08-02T00:18:03.628964: step 22566, loss 0.616036.
Train: 2018-08-02T00:18:03.794522: step 22567, loss 0.669219.
Train: 2018-08-02T00:18:03.976053: step 22568, loss 0.580233.
Train: 2018-08-02T00:18:04.139637: step 22569, loss 0.562493.
Train: 2018-08-02T00:18:04.308153: step 22570, loss 0.492107.
Test: 2018-08-02T00:18:04.847710: step 22570, loss 0.547762.
Train: 2018-08-02T00:18:05.030223: step 22571, loss 0.474697.
Train: 2018-08-02T00:18:05.195808: step 22572, loss 0.527383.
Train: 2018-08-02T00:18:05.365362: step 22573, loss 0.49234.
Train: 2018-08-02T00:18:05.532879: step 22574, loss 0.615062.
Train: 2018-08-02T00:18:05.702426: step 22575, loss 0.69334.
Train: 2018-08-02T00:18:05.874991: step 22576, loss 0.527456.
Train: 2018-08-02T00:18:06.038528: step 22577, loss 0.597373.
Train: 2018-08-02T00:18:06.203088: step 22578, loss 0.475249.
Train: 2018-08-02T00:18:06.363690: step 22579, loss 0.492723.
Train: 2018-08-02T00:18:06.538199: step 22580, loss 0.579861.
Test: 2018-08-02T00:18:07.071766: step 22580, loss 0.547831.
Train: 2018-08-02T00:18:07.245332: step 22581, loss 0.544996.
Train: 2018-08-02T00:18:07.411857: step 22582, loss 0.544993.
Train: 2018-08-02T00:18:07.580406: step 22583, loss 0.44033.
Train: 2018-08-02T00:18:07.748969: step 22584, loss 0.597389.
Train: 2018-08-02T00:18:07.916507: step 22585, loss 0.544948.
Train: 2018-08-02T00:18:08.082096: step 22586, loss 0.632516.
Train: 2018-08-02T00:18:08.245627: step 22587, loss 0.544931.
Train: 2018-08-02T00:18:08.413211: step 22588, loss 0.544928.
Train: 2018-08-02T00:18:08.579735: step 22589, loss 0.685171.
Train: 2018-08-02T00:18:08.749280: step 22590, loss 0.492411.
Test: 2018-08-02T00:18:09.282855: step 22590, loss 0.547788.
Train: 2018-08-02T00:18:09.449441: step 22591, loss 0.49243.
Train: 2018-08-02T00:18:09.621949: step 22592, loss 0.509907.
Train: 2018-08-02T00:18:09.796516: step 22593, loss 0.562454.
Train: 2018-08-02T00:18:09.969053: step 22594, loss 0.474709.
Train: 2018-08-02T00:18:10.137602: step 22595, loss 0.632802.
Train: 2018-08-02T00:18:10.313127: step 22596, loss 0.562473.
Train: 2018-08-02T00:18:10.479655: step 22597, loss 0.615297.
Train: 2018-08-02T00:18:10.644216: step 22598, loss 0.52727.
Train: 2018-08-02T00:18:10.811798: step 22599, loss 0.580083.
Train: 2018-08-02T00:18:10.977325: step 22600, loss 0.404044.
Test: 2018-08-02T00:18:11.519878: step 22600, loss 0.54772.
Train: 2018-08-02T00:18:12.289098: step 22601, loss 0.491946.
Train: 2018-08-02T00:18:12.456651: step 22602, loss 0.580181.
Train: 2018-08-02T00:18:12.624203: step 22603, loss 0.544806.
Train: 2018-08-02T00:18:12.790758: step 22604, loss 0.615759.
Train: 2018-08-02T00:18:12.953324: step 22605, loss 0.63357.
Train: 2018-08-02T00:18:13.119910: step 22606, loss 0.438255.
Train: 2018-08-02T00:18:13.288427: step 22607, loss 0.54477.
Train: 2018-08-02T00:18:13.457008: step 22608, loss 0.598154.
Train: 2018-08-02T00:18:13.623558: step 22609, loss 0.526944.
Train: 2018-08-02T00:18:13.789089: step 22610, loss 0.544745.
Test: 2018-08-02T00:18:14.313727: step 22610, loss 0.547639.
Train: 2018-08-02T00:18:14.481264: step 22611, loss 0.651775.
Train: 2018-08-02T00:18:14.649788: step 22612, loss 0.544742.
Train: 2018-08-02T00:18:14.817365: step 22613, loss 0.562571.
Train: 2018-08-02T00:18:14.977911: step 22614, loss 0.491286.
Train: 2018-08-02T00:18:15.141498: step 22615, loss 0.633884.
Train: 2018-08-02T00:18:15.308059: step 22616, loss 0.633836.
Train: 2018-08-02T00:18:15.471590: step 22617, loss 0.704876.
Train: 2018-08-02T00:18:15.636152: step 22618, loss 0.580259.
Train: 2018-08-02T00:18:15.801731: step 22619, loss 0.615529.
Train: 2018-08-02T00:18:15.969292: step 22620, loss 0.544867.
Test: 2018-08-02T00:18:16.497847: step 22620, loss 0.54776.
Train: 2018-08-02T00:18:16.669414: step 22621, loss 0.56246.
Train: 2018-08-02T00:18:16.832986: step 22622, loss 0.579948.
Train: 2018-08-02T00:18:17.007529: step 22623, loss 0.649692.
Train: 2018-08-02T00:18:17.179027: step 22624, loss 0.562419.
Train: 2018-08-02T00:18:17.342590: step 22625, loss 0.493112.
Train: 2018-08-02T00:18:17.518145: step 22626, loss 0.614261.
Train: 2018-08-02T00:18:17.681684: step 22627, loss 0.545164.
Train: 2018-08-02T00:18:17.845245: step 22628, loss 0.5624.
Train: 2018-08-02T00:18:18.013795: step 22629, loss 0.510911.
Train: 2018-08-02T00:18:18.178356: step 22630, loss 0.562399.
Test: 2018-08-02T00:18:18.718936: step 22630, loss 0.54806.
Train: 2018-08-02T00:18:18.896469: step 22631, loss 0.511026.
Train: 2018-08-02T00:18:19.065016: step 22632, loss 0.52816.
Train: 2018-08-02T00:18:19.227582: step 22633, loss 0.511028.
Train: 2018-08-02T00:18:19.394140: step 22634, loss 0.648096.
Train: 2018-08-02T00:18:19.561665: step 22635, loss 0.545266.
Train: 2018-08-02T00:18:19.728243: step 22636, loss 0.63093.
Train: 2018-08-02T00:18:19.891775: step 22637, loss 0.459704.
Train: 2018-08-02T00:18:20.058354: step 22638, loss 0.596649.
Train: 2018-08-02T00:18:20.223922: step 22639, loss 0.5624.
Train: 2018-08-02T00:18:20.388448: step 22640, loss 0.545275.
Test: 2018-08-02T00:18:20.932991: step 22640, loss 0.548056.
Train: 2018-08-02T00:18:21.101575: step 22641, loss 0.528141.
Train: 2018-08-02T00:18:21.268127: step 22642, loss 0.630962.
Train: 2018-08-02T00:18:21.430660: step 22643, loss 0.630942.
Train: 2018-08-02T00:18:21.606223: step 22644, loss 0.5624.
Train: 2018-08-02T00:18:21.774742: step 22645, loss 0.459817.
Train: 2018-08-02T00:18:21.949275: step 22646, loss 0.545295.
Train: 2018-08-02T00:18:22.124805: step 22647, loss 0.5624.
Train: 2018-08-02T00:18:22.289372: step 22648, loss 0.5624.
Train: 2018-08-02T00:18:22.452959: step 22649, loss 0.476729.
Train: 2018-08-02T00:18:22.618523: step 22650, loss 0.631047.
Test: 2018-08-02T00:18:23.151093: step 22650, loss 0.54802.
Train: 2018-08-02T00:18:23.315623: step 22651, loss 0.562399.
Train: 2018-08-02T00:18:23.483175: step 22652, loss 0.562399.
Train: 2018-08-02T00:18:23.659735: step 22653, loss 0.528026.
Train: 2018-08-02T00:18:23.830281: step 22654, loss 0.493595.
Train: 2018-08-02T00:18:24.005777: step 22655, loss 0.562401.
Train: 2018-08-02T00:18:24.184299: step 22656, loss 0.596913.
Train: 2018-08-02T00:18:24.350871: step 22657, loss 0.562404.
Train: 2018-08-02T00:18:24.515439: step 22658, loss 0.631537.
Train: 2018-08-02T00:18:24.692966: step 22659, loss 0.666076.
Train: 2018-08-02T00:18:24.858528: step 22660, loss 0.510651.
Test: 2018-08-02T00:18:25.393068: step 22660, loss 0.547968.
Train: 2018-08-02T00:18:25.563613: step 22661, loss 0.614112.
Train: 2018-08-02T00:18:25.729205: step 22662, loss 0.545188.
Train: 2018-08-02T00:18:25.895756: step 22663, loss 0.579594.
Train: 2018-08-02T00:18:26.060309: step 22664, loss 0.493703.
Train: 2018-08-02T00:18:26.226870: step 22665, loss 0.562399.
Train: 2018-08-02T00:18:26.399378: step 22666, loss 0.493723.
Train: 2018-08-02T00:18:26.564935: step 22667, loss 0.596764.
Train: 2018-08-02T00:18:26.730520: step 22668, loss 0.648333.
Train: 2018-08-02T00:18:26.899043: step 22669, loss 0.665424.
Train: 2018-08-02T00:18:27.071607: step 22670, loss 0.596665.
Test: 2018-08-02T00:18:27.611170: step 22670, loss 0.548089.
Train: 2018-08-02T00:18:27.784702: step 22671, loss 0.562401.
Train: 2018-08-02T00:18:27.952256: step 22672, loss 0.511243.
Train: 2018-08-02T00:18:28.119779: step 22673, loss 0.647563.
Train: 2018-08-02T00:18:28.286366: step 22674, loss 0.545417.
Train: 2018-08-02T00:18:28.449896: step 22675, loss 0.545451.
Train: 2018-08-02T00:18:28.628419: step 22676, loss 0.477721.
Train: 2018-08-02T00:18:28.793976: step 22677, loss 0.443846.
Train: 2018-08-02T00:18:28.969534: step 22678, loss 0.511513.
Train: 2018-08-02T00:18:29.134067: step 22679, loss 0.596416.
Train: 2018-08-02T00:18:29.299656: step 22680, loss 0.528344.
Test: 2018-08-02T00:18:29.832217: step 22680, loss 0.548115.
Train: 2018-08-02T00:18:29.995789: step 22681, loss 0.54534.
Train: 2018-08-02T00:18:30.164316: step 22682, loss 0.511114.
Train: 2018-08-02T00:18:30.332864: step 22683, loss 0.510988.
Train: 2018-08-02T00:18:30.499418: step 22684, loss 0.51084.
Train: 2018-08-02T00:18:30.665004: step 22685, loss 0.579644.
Train: 2018-08-02T00:18:30.829560: step 22686, loss 0.614272.
Train: 2018-08-02T00:18:30.994095: step 22687, loss 0.510454.
Train: 2018-08-02T00:18:31.159681: step 22688, loss 0.597127.
Train: 2018-08-02T00:18:31.326208: step 22689, loss 0.492891.
Train: 2018-08-02T00:18:31.492762: step 22690, loss 0.597266.
Test: 2018-08-02T00:18:32.030356: step 22690, loss 0.547822.
Train: 2018-08-02T00:18:32.199907: step 22691, loss 0.475202.
Train: 2018-08-02T00:18:32.366427: step 22692, loss 0.49249.
Train: 2018-08-02T00:18:32.530987: step 22693, loss 0.509835.
Train: 2018-08-02T00:18:32.696571: step 22694, loss 0.509679.
Train: 2018-08-02T00:18:32.863099: step 22695, loss 0.544835.
Train: 2018-08-02T00:18:33.025696: step 22696, loss 0.56252.
Train: 2018-08-02T00:18:33.193241: step 22697, loss 0.509231.
Train: 2018-08-02T00:18:33.361796: step 22698, loss 0.669531.
Train: 2018-08-02T00:18:33.526351: step 22699, loss 0.544731.
Train: 2018-08-02T00:18:33.693909: step 22700, loss 0.491093.
Test: 2018-08-02T00:18:34.218495: step 22700, loss 0.547617.
Train: 2018-08-02T00:18:35.020269: step 22701, loss 0.544705.
Train: 2018-08-02T00:18:35.184830: step 22702, loss 0.526746.
Train: 2018-08-02T00:18:35.355373: step 22703, loss 0.490735.
Train: 2018-08-02T00:18:35.525887: step 22704, loss 0.562687.
Train: 2018-08-02T00:18:35.690479: step 22705, loss 0.508519.
Train: 2018-08-02T00:18:35.855038: step 22706, loss 0.526527.
Train: 2018-08-02T00:18:36.017603: step 22707, loss 0.490166.
Train: 2018-08-02T00:18:36.188117: step 22708, loss 0.508201.
Train: 2018-08-02T00:18:36.353674: step 22709, loss 0.581124.
Train: 2018-08-02T00:18:36.518235: step 22710, loss 0.48968.
Test: 2018-08-02T00:18:37.054799: step 22710, loss 0.547574.
Train: 2018-08-02T00:18:37.270553: step 22711, loss 0.599657.
Train: 2018-08-02T00:18:37.438075: step 22712, loss 0.562977.
Train: 2018-08-02T00:18:37.602665: step 22713, loss 0.728772.
Train: 2018-08-02T00:18:37.771209: step 22714, loss 0.562989.
Train: 2018-08-02T00:18:37.937769: step 22715, loss 0.544586.
Train: 2018-08-02T00:18:38.102343: step 22716, loss 0.507841.
Train: 2018-08-02T00:18:38.266891: step 22717, loss 0.544587.
Train: 2018-08-02T00:18:38.435409: step 22718, loss 0.618042.
Train: 2018-08-02T00:18:38.607035: step 22719, loss 0.489556.
Train: 2018-08-02T00:18:38.771561: step 22720, loss 0.507911.
Test: 2018-08-02T00:18:39.315109: step 22720, loss 0.547573.
Train: 2018-08-02T00:18:39.486649: step 22721, loss 0.54459.
Train: 2018-08-02T00:18:39.653235: step 22722, loss 0.544589.
Train: 2018-08-02T00:18:39.818761: step 22723, loss 0.452835.
Train: 2018-08-02T00:18:39.989306: step 22724, loss 0.507838.
Train: 2018-08-02T00:18:40.153897: step 22725, loss 0.562989.
Train: 2018-08-02T00:18:40.319451: step 22726, loss 0.56301.
Train: 2018-08-02T00:18:40.484980: step 22727, loss 0.563027.
Train: 2018-08-02T00:18:40.653561: step 22728, loss 0.618402.
Train: 2018-08-02T00:18:40.816096: step 22729, loss 0.61838.
Train: 2018-08-02T00:18:40.979659: step 22730, loss 0.636726.
Test: 2018-08-02T00:18:41.511244: step 22730, loss 0.547576.
Train: 2018-08-02T00:18:41.679817: step 22731, loss 0.654927.
Train: 2018-08-02T00:18:41.855349: step 22732, loss 0.599588.
Train: 2018-08-02T00:18:42.021880: step 22733, loss 0.599406.
Train: 2018-08-02T00:18:42.187456: step 22734, loss 0.544612.
Train: 2018-08-02T00:18:42.352021: step 22735, loss 0.599046.
Train: 2018-08-02T00:18:42.518545: step 22736, loss 0.635016.
Train: 2018-08-02T00:18:42.686122: step 22737, loss 0.526674.
Train: 2018-08-02T00:18:42.850669: step 22738, loss 0.490899.
Train: 2018-08-02T00:18:43.023227: step 22739, loss 0.526827.
Train: 2018-08-02T00:18:43.197729: step 22740, loss 0.633997.
Test: 2018-08-02T00:18:43.727313: step 22740, loss 0.54765.
Train: 2018-08-02T00:18:43.895894: step 22741, loss 0.651582.
Train: 2018-08-02T00:18:44.066448: step 22742, loss 0.580267.
Train: 2018-08-02T00:18:44.231998: step 22743, loss 0.438783.
Train: 2018-08-02T00:18:44.395526: step 22744, loss 0.580131.
Train: 2018-08-02T00:18:44.568067: step 22745, loss 0.544869.
Train: 2018-08-02T00:18:44.737670: step 22746, loss 0.580046.
Train: 2018-08-02T00:18:44.909196: step 22747, loss 0.6502.
Train: 2018-08-02T00:18:45.087710: step 22748, loss 0.544944.
Train: 2018-08-02T00:18:45.261213: step 22749, loss 0.597352.
Train: 2018-08-02T00:18:45.435771: step 22750, loss 0.632077.
Test: 2018-08-02T00:18:45.958350: step 22750, loss 0.54788.
Train: 2018-08-02T00:18:46.119918: step 22751, loss 0.562414.
Train: 2018-08-02T00:18:46.285501: step 22752, loss 0.562407.
Train: 2018-08-02T00:18:46.450035: step 22753, loss 0.579658.
Train: 2018-08-02T00:18:46.618609: step 22754, loss 0.459151.
Train: 2018-08-02T00:18:46.783145: step 22755, loss 0.562399.
Train: 2018-08-02T00:18:46.948759: step 22756, loss 0.579571.
Train: 2018-08-02T00:18:47.126226: step 22757, loss 0.579551.
Train: 2018-08-02T00:18:47.290813: step 22758, loss 0.476742.
Train: 2018-08-02T00:18:47.458365: step 22759, loss 0.596663.
Train: 2018-08-02T00:18:47.622934: step 22760, loss 0.613773.
Test: 2018-08-02T00:18:48.167448: step 22760, loss 0.548075.
Train: 2018-08-02T00:18:48.335993: step 22761, loss 0.408443.
Train: 2018-08-02T00:18:48.498559: step 22762, loss 0.613776.
Train: 2018-08-02T00:18:48.662146: step 22763, loss 0.613792.
Train: 2018-08-02T00:18:48.832700: step 22764, loss 0.682269.
Train: 2018-08-02T00:18:49.007230: step 22765, loss 0.682035.
Train: 2018-08-02T00:18:49.186743: step 22766, loss 0.664603.
Train: 2018-08-02T00:18:49.353299: step 22767, loss 0.562414.
Train: 2018-08-02T00:18:49.521841: step 22768, loss 0.579317.
Train: 2018-08-02T00:18:49.692392: step 22769, loss 0.596095.
Train: 2018-08-02T00:18:49.857925: step 22770, loss 0.579222.
Test: 2018-08-02T00:18:50.400475: step 22770, loss 0.548491.
Train: 2018-08-02T00:18:50.569051: step 22771, loss 0.562478.
Train: 2018-08-02T00:18:50.742560: step 22772, loss 0.629097.
Train: 2018-08-02T00:18:50.907148: step 22773, loss 0.529348.
Train: 2018-08-02T00:18:51.068687: step 22774, loss 0.546003.
Train: 2018-08-02T00:18:51.234273: step 22775, loss 0.595574.
Train: 2018-08-02T00:18:51.408791: step 22776, loss 0.57905.
Train: 2018-08-02T00:18:51.575358: step 22777, loss 0.595466.
Train: 2018-08-02T00:18:51.747873: step 22778, loss 0.595411.
Train: 2018-08-02T00:18:51.913454: step 22779, loss 0.595356.
Train: 2018-08-02T00:18:52.085001: step 22780, loss 0.546363.
Test: 2018-08-02T00:18:52.607575: step 22780, loss 0.549062.
Train: 2018-08-02T00:18:52.786098: step 22781, loss 0.562696.
Train: 2018-08-02T00:18:52.954680: step 22782, loss 0.627739.
Train: 2018-08-02T00:18:53.123227: step 22783, loss 0.611401.
Train: 2018-08-02T00:18:53.288754: step 22784, loss 0.57895.
Train: 2018-08-02T00:18:53.458300: step 22785, loss 0.562799.
Train: 2018-08-02T00:18:53.622895: step 22786, loss 0.578934.
Train: 2018-08-02T00:18:53.783430: step 22787, loss 0.611095.
Train: 2018-08-02T00:18:53.948016: step 22788, loss 0.578925.
Train: 2018-08-02T00:18:54.114579: step 22789, loss 0.466803.
Train: 2018-08-02T00:18:54.280103: step 22790, loss 0.546889.
Test: 2018-08-02T00:18:54.810685: step 22790, loss 0.549482.
Train: 2018-08-02T00:18:54.974254: step 22791, loss 0.578921.
Train: 2018-08-02T00:18:55.144825: step 22792, loss 0.514809.
Train: 2018-08-02T00:18:55.310374: step 22793, loss 0.627071.
Train: 2018-08-02T00:18:55.473946: step 22794, loss 0.562871.
Train: 2018-08-02T00:18:55.642492: step 22795, loss 0.59499.
Train: 2018-08-02T00:18:55.812033: step 22796, loss 0.611061.
Train: 2018-08-02T00:18:55.976603: step 22797, loss 0.562865.
Train: 2018-08-02T00:18:56.141128: step 22798, loss 0.514687.
Train: 2018-08-02T00:18:56.307682: step 22799, loss 0.562855.
Train: 2018-08-02T00:18:56.473241: step 22800, loss 0.643289.
Test: 2018-08-02T00:18:57.000862: step 22800, loss 0.549372.
Train: 2018-08-02T00:18:57.780860: step 22801, loss 0.611105.
Train: 2018-08-02T00:18:57.950408: step 22802, loss 0.562852.
Train: 2018-08-02T00:18:58.111976: step 22803, loss 0.627139.
Train: 2018-08-02T00:18:58.277533: step 22804, loss 0.498663.
Train: 2018-08-02T00:18:58.449075: step 22805, loss 0.56287.
Train: 2018-08-02T00:18:58.614651: step 22806, loss 0.466495.
Train: 2018-08-02T00:18:58.780214: step 22807, loss 0.562837.
Train: 2018-08-02T00:18:58.947741: step 22808, loss 0.659574.
Train: 2018-08-02T00:18:59.119308: step 22809, loss 0.578939.
Train: 2018-08-02T00:18:59.289827: step 22810, loss 0.578941.
Test: 2018-08-02T00:18:59.821407: step 22810, loss 0.54927.
Train: 2018-08-02T00:19:00.006935: step 22811, loss 0.498197.
Train: 2018-08-02T00:19:00.165510: step 22812, loss 0.659813.
Train: 2018-08-02T00:19:00.335032: step 22813, loss 0.562773.
Train: 2018-08-02T00:19:00.500616: step 22814, loss 0.595126.
Train: 2018-08-02T00:19:00.678116: step 22815, loss 0.546595.
Train: 2018-08-02T00:19:00.851682: step 22816, loss 0.595132.
Train: 2018-08-02T00:19:01.024216: step 22817, loss 0.498037.
Train: 2018-08-02T00:19:01.204733: step 22818, loss 0.514148.
Train: 2018-08-02T00:19:01.373258: step 22819, loss 0.514028.
Train: 2018-08-02T00:19:01.538815: step 22820, loss 0.595253.
Test: 2018-08-02T00:19:02.068429: step 22820, loss 0.549016.
Train: 2018-08-02T00:19:02.237977: step 22821, loss 0.61161.
Train: 2018-08-02T00:19:02.400542: step 22822, loss 0.546328.
Train: 2018-08-02T00:19:02.567096: step 22823, loss 0.562645.
Train: 2018-08-02T00:19:02.730660: step 22824, loss 0.52986.
Train: 2018-08-02T00:19:02.897209: step 22825, loss 0.546191.
Train: 2018-08-02T00:19:03.062754: step 22826, loss 0.611953.
Train: 2018-08-02T00:19:03.227337: step 22827, loss 0.579055.
Train: 2018-08-02T00:19:03.395863: step 22828, loss 0.612057.
Train: 2018-08-02T00:19:03.558442: step 22829, loss 0.562564.
Train: 2018-08-02T00:19:03.723977: step 22830, loss 0.579071.
Test: 2018-08-02T00:19:04.265550: step 22830, loss 0.548725.
Train: 2018-08-02T00:19:04.433077: step 22831, loss 0.430416.
Train: 2018-08-02T00:19:04.610628: step 22832, loss 0.529425.
Train: 2018-08-02T00:19:04.775195: step 22833, loss 0.512714.
Train: 2018-08-02T00:19:04.941744: step 22834, loss 0.512529.
Train: 2018-08-02T00:19:05.109270: step 22835, loss 0.612626.
Train: 2018-08-02T00:19:05.274827: step 22836, loss 0.612747.
Train: 2018-08-02T00:19:05.439387: step 22837, loss 0.56245.
Train: 2018-08-02T00:19:05.606971: step 22838, loss 0.545621.
Train: 2018-08-02T00:19:05.769505: step 22839, loss 0.495028.
Train: 2018-08-02T00:19:05.938072: step 22840, loss 0.545529.
Test: 2018-08-02T00:19:06.469665: step 22840, loss 0.548231.
Train: 2018-08-02T00:19:06.640202: step 22841, loss 0.613238.
Train: 2018-08-02T00:19:06.810752: step 22842, loss 0.545443.
Train: 2018-08-02T00:19:06.976329: step 22843, loss 0.494408.
Train: 2018-08-02T00:19:07.144853: step 22844, loss 0.460139.
Train: 2018-08-02T00:19:07.309389: step 22845, loss 0.47686.
Train: 2018-08-02T00:19:07.476966: step 22846, loss 0.545213.
Train: 2018-08-02T00:19:07.647511: step 22847, loss 0.527884.
Train: 2018-08-02T00:19:07.812045: step 22848, loss 0.56241.
Train: 2018-08-02T00:19:07.985612: step 22849, loss 0.545021.
Train: 2018-08-02T00:19:08.150189: step 22850, loss 0.667209.
Test: 2018-08-02T00:19:08.681719: step 22850, loss 0.547793.
Train: 2018-08-02T00:19:08.847278: step 22851, loss 0.544948.
Train: 2018-08-02T00:19:09.010879: step 22852, loss 0.650078.
Train: 2018-08-02T00:19:09.174439: step 22853, loss 0.509858.
Train: 2018-08-02T00:19:09.334973: step 22854, loss 0.527357.
Train: 2018-08-02T00:19:09.511502: step 22855, loss 0.439442.
Train: 2018-08-02T00:19:09.676062: step 22856, loss 0.54493.
Train: 2018-08-02T00:19:09.840653: step 22857, loss 0.580166.
Train: 2018-08-02T00:19:10.003212: step 22858, loss 0.562499.
Train: 2018-08-02T00:19:10.171736: step 22859, loss 0.633401.
Train: 2018-08-02T00:19:10.335316: step 22860, loss 0.544883.
Test: 2018-08-02T00:19:10.864914: step 22860, loss 0.547665.
Train: 2018-08-02T00:19:11.026477: step 22861, loss 0.615875.
Train: 2018-08-02T00:19:11.191038: step 22862, loss 0.633496.
Train: 2018-08-02T00:19:11.363581: step 22863, loss 0.52708.
Train: 2018-08-02T00:19:11.531136: step 22864, loss 0.49169.
Train: 2018-08-02T00:19:11.697683: step 22865, loss 0.580223.
Train: 2018-08-02T00:19:11.858257: step 22866, loss 0.509401.
Train: 2018-08-02T00:19:12.027806: step 22867, loss 0.651073.
Train: 2018-08-02T00:19:12.200313: step 22868, loss 0.509423.
Train: 2018-08-02T00:19:12.374878: step 22869, loss 0.704047.
Train: 2018-08-02T00:19:12.547416: step 22870, loss 0.491873.
Test: 2018-08-02T00:19:13.083954: step 22870, loss 0.547719.
Train: 2018-08-02T00:19:13.254527: step 22871, loss 0.544851.
Train: 2018-08-02T00:19:13.422074: step 22872, loss 0.544861.
Train: 2018-08-02T00:19:13.588636: step 22873, loss 0.632929.
Train: 2018-08-02T00:19:13.755184: step 22874, loss 0.597642.
Train: 2018-08-02T00:19:13.918720: step 22875, loss 0.580013.
Train: 2018-08-02T00:19:14.091259: step 22876, loss 0.581136.
Train: 2018-08-02T00:19:14.267812: step 22877, loss 0.47504.
Train: 2018-08-02T00:19:14.431350: step 22878, loss 0.562438.
Train: 2018-08-02T00:19:14.600896: step 22879, loss 0.6846.
Train: 2018-08-02T00:19:14.767452: step 22880, loss 0.457961.
Test: 2018-08-02T00:19:15.304021: step 22880, loss 0.547856.
Train: 2018-08-02T00:19:15.472567: step 22881, loss 0.597218.
Train: 2018-08-02T00:19:15.635131: step 22882, loss 0.614547.
Train: 2018-08-02T00:19:15.805706: step 22883, loss 0.545071.
Train: 2018-08-02T00:19:15.972230: step 22884, loss 0.545093.
Train: 2018-08-02T00:19:16.142799: step 22885, loss 0.579709.
Train: 2018-08-02T00:19:16.316310: step 22886, loss 0.476019.
Train: 2018-08-02T00:19:16.477910: step 22887, loss 0.562408.
Train: 2018-08-02T00:19:16.646455: step 22888, loss 0.54513.
Train: 2018-08-02T00:19:16.817998: step 22889, loss 0.59697.
Train: 2018-08-02T00:19:16.982530: step 22890, loss 0.510581.
Test: 2018-08-02T00:19:17.518099: step 22890, loss 0.547936.
Train: 2018-08-02T00:19:17.684653: step 22891, loss 0.614255.
Train: 2018-08-02T00:19:17.853203: step 22892, loss 0.562407.
Train: 2018-08-02T00:19:18.018792: step 22893, loss 0.683297.
Train: 2018-08-02T00:19:18.184317: step 22894, loss 0.562404.
Train: 2018-08-02T00:19:18.355885: step 22895, loss 0.527991.
Train: 2018-08-02T00:19:18.518455: step 22896, loss 0.596774.
Train: 2018-08-02T00:19:18.682018: step 22897, loss 0.510921.
Train: 2018-08-02T00:19:18.846580: step 22898, loss 0.528104.
Train: 2018-08-02T00:19:19.013126: step 22899, loss 0.476668.
Train: 2018-08-02T00:19:19.178659: step 22900, loss 0.510907.
Test: 2018-08-02T00:19:19.716259: step 22900, loss 0.548005.
Train: 2018-08-02T00:19:20.480580: step 22901, loss 0.545209.
Train: 2018-08-02T00:19:20.644144: step 22902, loss 0.614071.
Train: 2018-08-02T00:19:20.806734: step 22903, loss 0.596878.
Train: 2018-08-02T00:19:20.968278: step 22904, loss 0.596887.
Train: 2018-08-02T00:19:21.136828: step 22905, loss 0.614116.
Train: 2018-08-02T00:19:21.300390: step 22906, loss 0.51074.
Train: 2018-08-02T00:19:21.468970: step 22907, loss 0.59684.
Train: 2018-08-02T00:19:21.635494: step 22908, loss 0.527987.
Train: 2018-08-02T00:19:21.798059: step 22909, loss 0.579609.
Train: 2018-08-02T00:19:21.968602: step 22910, loss 0.579603.
Test: 2018-08-02T00:19:22.503181: step 22910, loss 0.548006.
Train: 2018-08-02T00:19:22.675712: step 22911, loss 0.493636.
Train: 2018-08-02T00:19:22.843293: step 22912, loss 0.493605.
Train: 2018-08-02T00:19:23.004863: step 22913, loss 0.596847.
Train: 2018-08-02T00:19:23.169394: step 22914, loss 0.545169.
Train: 2018-08-02T00:19:23.347916: step 22915, loss 0.527906.
Train: 2018-08-02T00:19:23.523446: step 22916, loss 0.545136.
Train: 2018-08-02T00:19:23.690033: step 22917, loss 0.545116.
Train: 2018-08-02T00:19:23.853590: step 22918, loss 0.614362.
Train: 2018-08-02T00:19:24.018151: step 22919, loss 0.52776.
Train: 2018-08-02T00:19:24.186700: step 22920, loss 0.666465.
Test: 2018-08-02T00:19:24.718253: step 22920, loss 0.5479.
Train: 2018-08-02T00:19:24.883811: step 22921, loss 0.510419.
Train: 2018-08-02T00:19:25.047398: step 22922, loss 0.562413.
Train: 2018-08-02T00:19:25.208967: step 22923, loss 0.423738.
Train: 2018-08-02T00:19:25.376524: step 22924, loss 0.510317.
Train: 2018-08-02T00:19:25.545042: step 22925, loss 0.562425.
Train: 2018-08-02T00:19:25.711597: step 22926, loss 0.527545.
Train: 2018-08-02T00:19:25.875193: step 22927, loss 0.492508.
Train: 2018-08-02T00:19:26.054680: step 22928, loss 0.527387.
Train: 2018-08-02T00:19:26.219271: step 22929, loss 0.492125.
Train: 2018-08-02T00:19:26.385824: step 22930, loss 0.527196.
Test: 2018-08-02T00:19:26.921363: step 22930, loss 0.547687.
Train: 2018-08-02T00:19:27.083954: step 22931, loss 0.544807.
Train: 2018-08-02T00:19:27.250508: step 22932, loss 0.544775.
Train: 2018-08-02T00:19:27.421054: step 22933, loss 0.526923.
Train: 2018-08-02T00:19:27.588610: step 22934, loss 0.526841.
Train: 2018-08-02T00:19:27.755159: step 22935, loss 0.634372.
Train: 2018-08-02T00:19:27.928670: step 22936, loss 0.45485.
Train: 2018-08-02T00:19:28.094258: step 22937, loss 0.526649.
Train: 2018-08-02T00:19:28.260804: step 22938, loss 0.689188.
Train: 2018-08-02T00:19:28.432324: step 22939, loss 0.616972.
Train: 2018-08-02T00:19:28.596909: step 22940, loss 0.598885.
Test: 2018-08-02T00:19:29.115498: step 22940, loss 0.547587.
Train: 2018-08-02T00:19:29.292025: step 22941, loss 0.454311.
Train: 2018-08-02T00:19:29.455588: step 22942, loss 0.598879.
Train: 2018-08-02T00:19:29.623140: step 22943, loss 0.580798.
Train: 2018-08-02T00:19:29.787726: step 22944, loss 0.616919.
Train: 2018-08-02T00:19:29.952288: step 22945, loss 0.562701.
Train: 2018-08-02T00:19:30.117844: step 22946, loss 0.562686.
Train: 2018-08-02T00:19:30.280415: step 22947, loss 0.598669.
Train: 2018-08-02T00:19:30.444974: step 22948, loss 0.526716.
Train: 2018-08-02T00:19:30.621472: step 22949, loss 0.454968.
Train: 2018-08-02T00:19:30.783070: step 22950, loss 0.59853.
Test: 2018-08-02T00:19:31.308635: step 22950, loss 0.547612.
Train: 2018-08-02T00:19:31.487157: step 22951, loss 0.562632.
Train: 2018-08-02T00:19:31.652716: step 22952, loss 0.598477.
Train: 2018-08-02T00:19:31.816302: step 22953, loss 0.616331.
Train: 2018-08-02T00:19:31.980848: step 22954, loss 0.491103.
Train: 2018-08-02T00:19:32.142432: step 22955, loss 0.580446.
Train: 2018-08-02T00:19:32.308998: step 22956, loss 0.580415.
Train: 2018-08-02T00:19:32.481500: step 22957, loss 0.705076.
Train: 2018-08-02T00:19:32.653040: step 22958, loss 0.580296.
Train: 2018-08-02T00:19:32.816604: step 22959, loss 0.61562.
Train: 2018-08-02T00:19:32.995126: step 22960, loss 0.685953.
Test: 2018-08-02T00:19:33.534685: step 22960, loss 0.547763.
Train: 2018-08-02T00:19:33.702237: step 22961, loss 0.474707.
Train: 2018-08-02T00:19:33.865825: step 22962, loss 0.544952.
Train: 2018-08-02T00:19:34.042326: step 22963, loss 0.544991.
Train: 2018-08-02T00:19:34.207929: step 22964, loss 0.632013.
Train: 2018-08-02T00:19:34.371473: step 22965, loss 0.510386.
Train: 2018-08-02T00:19:34.549970: step 22966, loss 0.527799.
Train: 2018-08-02T00:19:34.722509: step 22967, loss 0.510572.
Train: 2018-08-02T00:19:34.890061: step 22968, loss 0.614201.
Train: 2018-08-02T00:19:35.063597: step 22969, loss 0.752063.
Train: 2018-08-02T00:19:35.231175: step 22970, loss 0.699812.
Test: 2018-08-02T00:19:35.761762: step 22970, loss 0.548094.
Train: 2018-08-02T00:19:35.929309: step 22971, loss 0.562402.
Train: 2018-08-02T00:19:36.096868: step 22972, loss 0.613422.
Train: 2018-08-02T00:19:36.263390: step 22973, loss 0.697779.
Train: 2018-08-02T00:19:36.428948: step 22974, loss 0.528821.
Train: 2018-08-02T00:19:36.592510: step 22975, loss 0.595921.
Train: 2018-08-02T00:19:36.765048: step 22976, loss 0.512586.
Train: 2018-08-02T00:19:36.930630: step 22977, loss 0.56253.
Train: 2018-08-02T00:19:37.094169: step 22978, loss 0.446913.
Train: 2018-08-02T00:19:37.260751: step 22979, loss 0.612068.
Train: 2018-08-02T00:19:37.422323: step 22980, loss 0.546108.
Test: 2018-08-02T00:19:37.956870: step 22980, loss 0.548812.
Train: 2018-08-02T00:19:38.126409: step 22981, loss 0.562591.
Train: 2018-08-02T00:19:38.292964: step 22982, loss 0.480403.
Train: 2018-08-02T00:19:38.462510: step 22983, loss 0.480354.
Train: 2018-08-02T00:19:38.627101: step 22984, loss 0.644968.
Train: 2018-08-02T00:19:38.794629: step 22985, loss 0.595548.
Train: 2018-08-02T00:19:38.959222: step 22986, loss 0.562572.
Train: 2018-08-02T00:19:39.124765: step 22987, loss 0.612051.
Train: 2018-08-02T00:19:39.292319: step 22988, loss 0.57906.
Train: 2018-08-02T00:19:39.458878: step 22989, loss 0.628498.
Train: 2018-08-02T00:19:39.631386: step 22990, loss 0.595507.
Test: 2018-08-02T00:19:40.158976: step 22990, loss 0.548838.
Train: 2018-08-02T00:19:40.326558: step 22991, loss 0.67765.
Train: 2018-08-02T00:19:40.490117: step 22992, loss 0.562628.
Train: 2018-08-02T00:19:40.654651: step 22993, loss 0.546305.
Train: 2018-08-02T00:19:40.822202: step 22994, loss 0.481083.
Train: 2018-08-02T00:19:40.989780: step 22995, loss 0.595302.
Train: 2018-08-02T00:19:41.153348: step 22996, loss 0.546377.
Train: 2018-08-02T00:19:41.329870: step 22997, loss 0.562679.
Train: 2018-08-02T00:19:41.498395: step 22998, loss 0.530078.
Train: 2018-08-02T00:19:41.663952: step 22999, loss 0.595304.
Train: 2018-08-02T00:19:41.840481: step 23000, loss 0.579022.
Test: 2018-08-02T00:19:42.382033: step 23000, loss 0.548986.
Train: 2018-08-02T00:19:43.163337: step 23001, loss 0.611685.
Train: 2018-08-02T00:19:43.328863: step 23002, loss 0.578988.
Train: 2018-08-02T00:19:43.495450: step 23003, loss 0.546368.
Train: 2018-08-02T00:19:43.659013: step 23004, loss 0.562678.
Train: 2018-08-02T00:19:43.824569: step 23005, loss 0.578988.
Train: 2018-08-02T00:19:43.997102: step 23006, loss 0.530049.
Train: 2018-08-02T00:19:44.156650: step 23007, loss 0.595318.
Train: 2018-08-02T00:19:44.320229: step 23008, loss 0.481015.
Train: 2018-08-02T00:19:44.487766: step 23009, loss 0.513577.
Train: 2018-08-02T00:19:44.659353: step 23010, loss 0.497041.
Test: 2018-08-02T00:19:45.182909: step 23010, loss 0.54882.
Train: 2018-08-02T00:19:45.351482: step 23011, loss 0.595493.
Train: 2018-08-02T00:19:45.526989: step 23012, loss 0.496606.
Train: 2018-08-02T00:19:45.692569: step 23013, loss 0.446711.
Train: 2018-08-02T00:19:45.863120: step 23014, loss 0.662277.
Train: 2018-08-02T00:19:46.039618: step 23015, loss 0.495779.
Train: 2018-08-02T00:19:46.203210: step 23016, loss 0.462029.
Train: 2018-08-02T00:19:46.370732: step 23017, loss 0.62973.
Train: 2018-08-02T00:19:46.536289: step 23018, loss 0.494914.
Train: 2018-08-02T00:19:46.700849: step 23019, loss 0.477676.
Train: 2018-08-02T00:19:46.867404: step 23020, loss 0.545377.
Test: 2018-08-02T00:19:47.407971: step 23020, loss 0.548075.
Train: 2018-08-02T00:19:47.583491: step 23021, loss 0.545293.
Train: 2018-08-02T00:19:47.754065: step 23022, loss 0.648328.
Train: 2018-08-02T00:19:47.917596: step 23023, loss 0.47624.
Train: 2018-08-02T00:19:48.088171: step 23024, loss 0.579705.
Train: 2018-08-02T00:19:48.268689: step 23025, loss 0.631805.
Train: 2018-08-02T00:19:48.434217: step 23026, loss 0.545043.
Train: 2018-08-02T00:19:48.601768: step 23027, loss 0.562426.
Train: 2018-08-02T00:19:48.780315: step 23028, loss 0.579866.
Train: 2018-08-02T00:19:48.946846: step 23029, loss 0.544982.
Train: 2018-08-02T00:19:49.115419: step 23030, loss 0.492545.
Test: 2018-08-02T00:19:49.650995: step 23030, loss 0.547789.
Train: 2018-08-02T00:19:49.822504: step 23031, loss 0.509927.
Train: 2018-08-02T00:19:49.988081: step 23032, loss 0.597557.
Train: 2018-08-02T00:19:50.156611: step 23033, loss 0.474591.
Train: 2018-08-02T00:19:50.320174: step 23034, loss 0.544865.
Train: 2018-08-02T00:19:50.480744: step 23035, loss 0.597821.
Train: 2018-08-02T00:19:50.662260: step 23036, loss 0.615577.
Train: 2018-08-02T00:19:50.828840: step 23037, loss 0.704121.
Train: 2018-08-02T00:19:51.008365: step 23038, loss 0.52715.
Train: 2018-08-02T00:19:51.187855: step 23039, loss 0.50951.
Train: 2018-08-02T00:19:51.358399: step 23040, loss 0.509516.
Test: 2018-08-02T00:19:51.892016: step 23040, loss 0.547706.
Train: 2018-08-02T00:19:52.063514: step 23041, loss 0.562502.
Train: 2018-08-02T00:19:52.237081: step 23042, loss 0.615529.
Train: 2018-08-02T00:19:52.400612: step 23043, loss 0.544834.
Train: 2018-08-02T00:19:52.569166: step 23044, loss 0.597822.
Train: 2018-08-02T00:19:52.734719: step 23045, loss 0.527199.
Train: 2018-08-02T00:19:52.896312: step 23046, loss 0.580132.
Train: 2018-08-02T00:19:53.058853: step 23047, loss 0.474336.
Train: 2018-08-02T00:19:53.228431: step 23048, loss 0.580129.
Train: 2018-08-02T00:19:53.393002: step 23049, loss 0.580132.
Train: 2018-08-02T00:19:53.557553: step 23050, loss 0.56249.
Test: 2018-08-02T00:19:54.099072: step 23050, loss 0.547722.
Train: 2018-08-02T00:19:54.265668: step 23051, loss 0.562488.
Train: 2018-08-02T00:19:54.432182: step 23052, loss 0.580113.
Train: 2018-08-02T00:19:54.597739: step 23053, loss 0.509631.
Train: 2018-08-02T00:19:54.764319: step 23054, loss 0.597719.
Train: 2018-08-02T00:19:54.930847: step 23055, loss 0.580089.
Train: 2018-08-02T00:19:55.102389: step 23056, loss 0.580072.
Train: 2018-08-02T00:19:55.274929: step 23057, loss 0.580049.
Train: 2018-08-02T00:19:55.445472: step 23058, loss 0.544903.
Train: 2018-08-02T00:19:55.611030: step 23059, loss 0.615089.
Train: 2018-08-02T00:19:55.775590: step 23060, loss 0.685064.
Test: 2018-08-02T00:19:56.312209: step 23060, loss 0.547813.
Train: 2018-08-02T00:19:56.493671: step 23061, loss 0.632287.
Train: 2018-08-02T00:19:56.662219: step 23062, loss 0.597217.
Train: 2018-08-02T00:19:56.835758: step 23063, loss 0.527753.
Train: 2018-08-02T00:19:57.014305: step 23064, loss 0.493301.
Train: 2018-08-02T00:19:57.180834: step 23065, loss 0.510669.
Train: 2018-08-02T00:19:57.347389: step 23066, loss 0.596895.
Train: 2018-08-02T00:19:57.509954: step 23067, loss 0.579605.
Train: 2018-08-02T00:19:57.674514: step 23068, loss 0.510869.
Train: 2018-08-02T00:19:57.847053: step 23069, loss 0.510903.
Train: 2018-08-02T00:19:58.013607: step 23070, loss 0.476561.
Test: 2018-08-02T00:19:58.546184: step 23070, loss 0.548007.
Train: 2018-08-02T00:19:58.717751: step 23071, loss 0.61397.
Train: 2018-08-02T00:19:58.884312: step 23072, loss 0.528007.
Train: 2018-08-02T00:19:59.050835: step 23073, loss 0.510766.
Train: 2018-08-02T00:19:59.222409: step 23074, loss 0.614114.
Train: 2018-08-02T00:19:59.393917: step 23075, loss 0.614146.
Train: 2018-08-02T00:19:59.556482: step 23076, loss 0.527916.
Train: 2018-08-02T00:19:59.731041: step 23077, loss 0.510657.
Train: 2018-08-02T00:19:59.898568: step 23078, loss 0.51061.
Train: 2018-08-02T00:20:00.075097: step 23079, loss 0.596992.
Train: 2018-08-02T00:20:00.239683: step 23080, loss 0.545103.
Test: 2018-08-02T00:20:00.769250: step 23080, loss 0.547906.
Train: 2018-08-02T00:20:00.932803: step 23081, loss 0.493116.
Train: 2018-08-02T00:20:01.102376: step 23082, loss 0.510351.
Train: 2018-08-02T00:20:01.265938: step 23083, loss 0.614608.
Train: 2018-08-02T00:20:01.431502: step 23084, loss 0.597263.
Train: 2018-08-02T00:20:01.602015: step 23085, loss 0.56243.
Train: 2018-08-02T00:20:01.767572: step 23086, loss 0.562432.
Train: 2018-08-02T00:20:01.929171: step 23087, loss 0.579881.
Train: 2018-08-02T00:20:02.100681: step 23088, loss 0.579883.
Train: 2018-08-02T00:20:02.263248: step 23089, loss 0.57988.
Train: 2018-08-02T00:20:02.438778: step 23090, loss 0.527556.
Test: 2018-08-02T00:20:02.978367: step 23090, loss 0.54783.
Train: 2018-08-02T00:20:03.144921: step 23091, loss 0.544994.
Train: 2018-08-02T00:20:03.308452: step 23092, loss 0.579875.
Train: 2018-08-02T00:20:03.472016: step 23093, loss 0.562432.
Train: 2018-08-02T00:20:03.634612: step 23094, loss 0.632181.
Train: 2018-08-02T00:20:03.799141: step 23095, loss 0.701773.
Train: 2018-08-02T00:20:04.019638: step 23096, loss 0.579786.
Train: 2018-08-02T00:20:04.182176: step 23097, loss 0.597047.
Train: 2018-08-02T00:20:04.346763: step 23098, loss 0.476085.
Train: 2018-08-02T00:20:04.513331: step 23099, loss 0.614111.
Train: 2018-08-02T00:20:04.678847: step 23100, loss 0.631194.
Test: 2018-08-02T00:20:05.212420: step 23100, loss 0.548042.
Train: 2018-08-02T00:20:06.018161: step 23101, loss 0.476664.
Train: 2018-08-02T00:20:06.184748: step 23102, loss 0.596647.
Train: 2018-08-02T00:20:06.347282: step 23103, loss 0.647862.
Train: 2018-08-02T00:20:06.517852: step 23104, loss 0.630584.
Train: 2018-08-02T00:20:06.683415: step 23105, loss 0.647342.
Train: 2018-08-02T00:20:06.852960: step 23106, loss 0.528596.
Train: 2018-08-02T00:20:07.018495: step 23107, loss 0.562435.
Train: 2018-08-02T00:20:07.191038: step 23108, loss 0.579256.
Train: 2018-08-02T00:20:07.356609: step 23109, loss 0.51218.
Train: 2018-08-02T00:20:07.524137: step 23110, loss 0.529009.
Test: 2018-08-02T00:20:08.055746: step 23110, loss 0.548481.
Train: 2018-08-02T00:20:08.222300: step 23111, loss 0.428767.
Train: 2018-08-02T00:20:08.390874: step 23112, loss 0.595934.
Train: 2018-08-02T00:20:08.562391: step 23113, loss 0.462019.
Train: 2018-08-02T00:20:08.728914: step 23114, loss 0.495355.
Train: 2018-08-02T00:20:08.890510: step 23115, loss 0.54562.
Train: 2018-08-02T00:20:09.059033: step 23116, loss 0.629917.
Train: 2018-08-02T00:20:09.221623: step 23117, loss 0.646923.
Train: 2018-08-02T00:20:09.399154: step 23118, loss 0.494803.
Train: 2018-08-02T00:20:09.562685: step 23119, loss 0.596276.
Train: 2018-08-02T00:20:09.729266: step 23120, loss 0.511597.
Test: 2018-08-02T00:20:10.260820: step 23120, loss 0.548208.
Train: 2018-08-02T00:20:10.428372: step 23121, loss 0.596343.
Train: 2018-08-02T00:20:10.595923: step 23122, loss 0.613349.
Train: 2018-08-02T00:20:10.757524: step 23123, loss 0.579392.
Train: 2018-08-02T00:20:10.925079: step 23124, loss 0.57939.
Train: 2018-08-02T00:20:11.088638: step 23125, loss 0.511495.
Train: 2018-08-02T00:20:11.254196: step 23126, loss 0.562412.
Train: 2018-08-02T00:20:11.419746: step 23127, loss 0.49446.
Train: 2018-08-02T00:20:11.586306: step 23128, loss 0.698497.
Train: 2018-08-02T00:20:11.747870: step 23129, loss 0.61341.
Train: 2018-08-02T00:20:11.923376: step 23130, loss 0.545433.
Test: 2018-08-02T00:20:12.451962: step 23130, loss 0.548208.
Train: 2018-08-02T00:20:12.619546: step 23131, loss 0.630273.
Train: 2018-08-02T00:20:12.783109: step 23132, loss 0.596291.
Train: 2018-08-02T00:20:12.951652: step 23133, loss 0.59623.
Train: 2018-08-02T00:20:13.126160: step 23134, loss 0.562432.
Train: 2018-08-02T00:20:13.305714: step 23135, loss 0.511939.
Train: 2018-08-02T00:20:13.474229: step 23136, loss 0.495174.
Train: 2018-08-02T00:20:13.639813: step 23137, loss 0.562443.
Train: 2018-08-02T00:20:13.818310: step 23138, loss 0.579266.
Train: 2018-08-02T00:20:13.987882: step 23139, loss 0.511971.
Train: 2018-08-02T00:20:14.151419: step 23140, loss 0.646623.
Test: 2018-08-02T00:20:14.687018: step 23140, loss 0.548348.
Train: 2018-08-02T00:20:14.854565: step 23141, loss 0.5961.
Train: 2018-08-02T00:20:15.025113: step 23142, loss 0.461548.
Train: 2018-08-02T00:20:15.189654: step 23143, loss 0.596099.
Train: 2018-08-02T00:20:15.360220: step 23144, loss 0.478271.
Train: 2018-08-02T00:20:15.526744: step 23145, loss 0.62987.
Train: 2018-08-02T00:20:15.694325: step 23146, loss 0.528698.
Train: 2018-08-02T00:20:15.856891: step 23147, loss 0.71437.
Train: 2018-08-02T00:20:16.023459: step 23148, loss 0.427541.
Train: 2018-08-02T00:20:16.188008: step 23149, loss 0.494928.
Train: 2018-08-02T00:20:16.356555: step 23150, loss 0.596233.
Test: 2018-08-02T00:20:16.893122: step 23150, loss 0.548248.
Train: 2018-08-02T00:20:17.059645: step 23151, loss 0.511647.
Train: 2018-08-02T00:20:17.241185: step 23152, loss 0.630232.
Train: 2018-08-02T00:20:17.407715: step 23153, loss 0.528481.
Train: 2018-08-02T00:20:17.587266: step 23154, loss 0.52844.
Train: 2018-08-02T00:20:17.756812: step 23155, loss 0.579418.
Train: 2018-08-02T00:20:17.923336: step 23156, loss 0.494282.
Train: 2018-08-02T00:20:18.083933: step 23157, loss 0.579467.
Train: 2018-08-02T00:20:18.249464: step 23158, loss 0.545309.
Train: 2018-08-02T00:20:18.411059: step 23159, loss 0.59664.
Train: 2018-08-02T00:20:18.575592: step 23160, loss 0.528123.
Test: 2018-08-02T00:20:19.116178: step 23160, loss 0.548027.
Train: 2018-08-02T00:20:19.282732: step 23161, loss 0.425102.
Train: 2018-08-02T00:20:19.453271: step 23162, loss 0.631263.
Train: 2018-08-02T00:20:19.628808: step 23163, loss 0.510659.
Train: 2018-08-02T00:20:19.794359: step 23164, loss 0.562405.
Train: 2018-08-02T00:20:19.962915: step 23165, loss 0.631707.
Train: 2018-08-02T00:20:20.126473: step 23166, loss 0.545071.
Train: 2018-08-02T00:20:20.292003: step 23167, loss 0.649206.
Train: 2018-08-02T00:20:20.457562: step 23168, loss 0.597121.
Train: 2018-08-02T00:20:20.627140: step 23169, loss 0.545071.
Train: 2018-08-02T00:20:20.794685: step 23170, loss 0.597076.
Test: 2018-08-02T00:20:21.323278: step 23170, loss 0.547907.
Train: 2018-08-02T00:20:21.487833: step 23171, loss 0.475823.
Train: 2018-08-02T00:20:21.651402: step 23172, loss 0.597054.
Train: 2018-08-02T00:20:21.821945: step 23173, loss 0.562409.
Train: 2018-08-02T00:20:21.988481: step 23174, loss 0.562408.
Train: 2018-08-02T00:20:22.153054: step 23175, loss 0.510475.
Train: 2018-08-02T00:20:22.320612: step 23176, loss 0.475814.
Train: 2018-08-02T00:20:22.493146: step 23177, loss 0.562412.
Train: 2018-08-02T00:20:22.662667: step 23178, loss 0.545047.
Train: 2018-08-02T00:20:22.829253: step 23179, loss 0.56242.
Train: 2018-08-02T00:20:22.992814: step 23180, loss 0.527594.
Test: 2018-08-02T00:20:23.522368: step 23180, loss 0.547824.
Train: 2018-08-02T00:20:23.687959: step 23181, loss 0.510105.
Train: 2018-08-02T00:20:23.854511: step 23182, loss 0.510008.
Train: 2018-08-02T00:20:24.018075: step 23183, loss 0.579968.
Train: 2018-08-02T00:20:24.186624: step 23184, loss 0.457144.
Train: 2018-08-02T00:20:24.353173: step 23185, loss 0.562475.
Train: 2018-08-02T00:20:24.517741: step 23186, loss 0.527189.
Train: 2018-08-02T00:20:24.693267: step 23187, loss 0.615609.
Train: 2018-08-02T00:20:24.858797: step 23188, loss 0.491607.
Train: 2018-08-02T00:20:25.030369: step 23189, loss 0.580312.
Train: 2018-08-02T00:20:25.197889: step 23190, loss 0.598158.
Test: 2018-08-02T00:20:25.735483: step 23190, loss 0.547644.
Train: 2018-08-02T00:20:25.901034: step 23191, loss 0.491291.
Train: 2018-08-02T00:20:26.068594: step 23192, loss 0.526883.
Train: 2018-08-02T00:20:26.235143: step 23193, loss 0.491071.
Train: 2018-08-02T00:20:26.400674: step 23194, loss 0.580549.
Train: 2018-08-02T00:20:26.568226: step 23195, loss 0.59856.
Train: 2018-08-02T00:20:26.736781: step 23196, loss 0.562655.
Train: 2018-08-02T00:20:26.902332: step 23197, loss 0.634633.
Train: 2018-08-02T00:20:27.067921: step 23198, loss 0.670581.
Train: 2018-08-02T00:20:27.236471: step 23199, loss 0.634462.
Train: 2018-08-02T00:20:27.402994: step 23200, loss 0.419347.
Test: 2018-08-02T00:20:27.941555: step 23200, loss 0.547621.
Train: 2018-08-02T00:20:28.718655: step 23201, loss 0.598398.
Train: 2018-08-02T00:20:28.887204: step 23202, loss 0.562595.
Train: 2018-08-02T00:20:29.054756: step 23203, loss 0.54473.
Train: 2018-08-02T00:20:29.216326: step 23204, loss 0.491231.
Train: 2018-08-02T00:20:29.383907: step 23205, loss 0.562573.
Train: 2018-08-02T00:20:29.547451: step 23206, loss 0.651723.
Train: 2018-08-02T00:20:29.722995: step 23207, loss 0.615973.
Train: 2018-08-02T00:20:29.889555: step 23208, loss 0.509236.
Train: 2018-08-02T00:20:30.058073: step 23209, loss 0.52704.
Train: 2018-08-02T00:20:30.225626: step 23210, loss 0.491601.
Test: 2018-08-02T00:20:30.764187: step 23210, loss 0.547676.
Train: 2018-08-02T00:20:30.929762: step 23211, loss 0.562524.
Train: 2018-08-02T00:20:31.094330: step 23212, loss 0.527061.
Train: 2018-08-02T00:20:31.259892: step 23213, loss 0.544789.
Train: 2018-08-02T00:20:31.423455: step 23214, loss 0.544785.
Train: 2018-08-02T00:20:31.588982: step 23215, loss 0.54478.
Train: 2018-08-02T00:20:31.754564: step 23216, loss 0.544775.
Train: 2018-08-02T00:20:31.924085: step 23217, loss 0.509219.
Train: 2018-08-02T00:20:32.106598: step 23218, loss 0.473576.
Train: 2018-08-02T00:20:32.270160: step 23219, loss 0.509076.
Train: 2018-08-02T00:20:32.435718: step 23220, loss 0.634095.
Test: 2018-08-02T00:20:32.974279: step 23220, loss 0.547621.
Train: 2018-08-02T00:20:33.142828: step 23221, loss 0.526815.
Train: 2018-08-02T00:20:33.308384: step 23222, loss 0.5447.
Train: 2018-08-02T00:20:33.473943: step 23223, loss 0.634406.
Train: 2018-08-02T00:20:33.638502: step 23224, loss 0.58058.
Train: 2018-08-02T00:20:33.804060: step 23225, loss 0.52675.
Train: 2018-08-02T00:20:33.981616: step 23226, loss 0.670293.
Train: 2018-08-02T00:20:34.151157: step 23227, loss 0.616374.
Train: 2018-08-02T00:20:34.315692: step 23228, loss 0.508954.
Train: 2018-08-02T00:20:34.477291: step 23229, loss 0.580444.
Train: 2018-08-02T00:20:34.639825: step 23230, loss 0.598234.
Test: 2018-08-02T00:20:35.178386: step 23230, loss 0.547652.
Train: 2018-08-02T00:20:35.347934: step 23231, loss 0.509165.
Train: 2018-08-02T00:20:35.512493: step 23232, loss 0.473667.
Train: 2018-08-02T00:20:35.687057: step 23233, loss 0.615868.
Train: 2018-08-02T00:20:35.859596: step 23234, loss 0.473735.
Train: 2018-08-02T00:20:36.024157: step 23235, loss 0.633602.
Train: 2018-08-02T00:20:36.190680: step 23236, loss 0.562533.
Train: 2018-08-02T00:20:36.355273: step 23237, loss 0.598004.
Train: 2018-08-02T00:20:36.521796: step 23238, loss 0.651098.
Train: 2018-08-02T00:20:36.686390: step 23239, loss 0.527152.
Train: 2018-08-02T00:20:36.849947: step 23240, loss 0.580131.
Test: 2018-08-02T00:20:37.379501: step 23240, loss 0.54773.
Train: 2018-08-02T00:20:37.625466: step 23241, loss 0.580085.
Train: 2018-08-02T00:20:37.788063: step 23242, loss 0.49217.
Train: 2018-08-02T00:20:37.949599: step 23243, loss 0.544903.
Train: 2018-08-02T00:20:38.114159: step 23244, loss 0.597542.
Train: 2018-08-02T00:20:38.280713: step 23245, loss 0.650062.
Train: 2018-08-02T00:20:38.445273: step 23246, loss 0.579921.
Train: 2018-08-02T00:20:38.610834: step 23247, loss 0.562429.
Train: 2018-08-02T00:20:38.776388: step 23248, loss 0.614632.
Train: 2018-08-02T00:20:38.942943: step 23249, loss 0.597126.
Train: 2018-08-02T00:20:39.106506: step 23250, loss 0.510489.
Test: 2018-08-02T00:20:39.641077: step 23250, loss 0.54794.
Train: 2018-08-02T00:20:39.807631: step 23251, loss 0.683302.
Train: 2018-08-02T00:20:39.975185: step 23252, loss 0.459127.
Train: 2018-08-02T00:20:40.141747: step 23253, loss 0.631128.
Train: 2018-08-02T00:20:40.315306: step 23254, loss 0.528118.
Train: 2018-08-02T00:20:40.479835: step 23255, loss 0.562398.
Train: 2018-08-02T00:20:40.645393: step 23256, loss 0.5624.
Train: 2018-08-02T00:20:40.811947: step 23257, loss 0.57946.
Train: 2018-08-02T00:20:40.974512: step 23258, loss 0.477239.
Train: 2018-08-02T00:20:41.141067: step 23259, loss 0.647556.
Train: 2018-08-02T00:20:41.315626: step 23260, loss 0.681466.
Test: 2018-08-02T00:20:41.836209: step 23260, loss 0.548209.
Train: 2018-08-02T00:20:42.008777: step 23261, loss 0.494568.
Train: 2018-08-02T00:20:42.179291: step 23262, loss 0.680962.
Train: 2018-08-02T00:20:42.349835: step 23263, loss 0.511773.
Train: 2018-08-02T00:20:42.515393: step 23264, loss 0.562434.
Train: 2018-08-02T00:20:42.677958: step 23265, loss 0.579263.
Train: 2018-08-02T00:20:42.844539: step 23266, loss 0.545654.
Train: 2018-08-02T00:20:43.007079: step 23267, loss 0.629548.
Train: 2018-08-02T00:20:43.171639: step 23268, loss 0.462026.
Train: 2018-08-02T00:20:43.339192: step 23269, loss 0.579201.
Train: 2018-08-02T00:20:43.506743: step 23270, loss 0.612653.
Test: 2018-08-02T00:20:44.044332: step 23270, loss 0.548481.
Train: 2018-08-02T00:20:44.209864: step 23271, loss 0.579185.
Train: 2018-08-02T00:20:44.376418: step 23272, loss 0.662639.
Train: 2018-08-02T00:20:44.554972: step 23273, loss 0.479224.
Train: 2018-08-02T00:20:44.721496: step 23274, loss 0.512578.
Train: 2018-08-02T00:20:44.884061: step 23275, loss 0.5625.
Train: 2018-08-02T00:20:45.048622: step 23276, loss 0.47927.
Train: 2018-08-02T00:20:45.216173: step 23277, loss 0.645841.
Train: 2018-08-02T00:20:45.383750: step 23278, loss 0.52914.
Train: 2018-08-02T00:20:45.548319: step 23279, loss 0.595856.
Train: 2018-08-02T00:20:45.719852: step 23280, loss 0.579173.
Test: 2018-08-02T00:20:46.238441: step 23280, loss 0.548502.
Train: 2018-08-02T00:20:46.403998: step 23281, loss 0.495708.
Train: 2018-08-02T00:20:46.579562: step 23282, loss 0.545761.
Train: 2018-08-02T00:20:46.745087: step 23283, loss 0.562466.
Train: 2018-08-02T00:20:46.909646: step 23284, loss 0.545703.
Train: 2018-08-02T00:20:47.075204: step 23285, loss 0.596015.
Train: 2018-08-02T00:20:47.241789: step 23286, loss 0.596041.
Train: 2018-08-02T00:20:47.414298: step 23287, loss 0.57925.
Train: 2018-08-02T00:20:47.577860: step 23288, loss 0.562445.
Train: 2018-08-02T00:20:47.740456: step 23289, loss 0.579255.
Train: 2018-08-02T00:20:47.903022: step 23290, loss 0.562442.
Test: 2018-08-02T00:20:48.425625: step 23290, loss 0.548364.
Train: 2018-08-02T00:20:48.593176: step 23291, loss 0.495189.
Train: 2018-08-02T00:20:48.754716: step 23292, loss 0.596101.
Train: 2018-08-02T00:20:48.919305: step 23293, loss 0.545595.
Train: 2018-08-02T00:20:49.084832: step 23294, loss 0.579292.
Train: 2018-08-02T00:20:49.250420: step 23295, loss 0.511833.
Train: 2018-08-02T00:20:49.414950: step 23296, loss 0.562426.
Train: 2018-08-02T00:20:49.579532: step 23297, loss 0.477881.
Train: 2018-08-02T00:20:49.750067: step 23298, loss 0.562407.
Train: 2018-08-02T00:20:49.910623: step 23299, loss 0.596354.
Train: 2018-08-02T00:20:50.077209: step 23300, loss 0.494311.
Test: 2018-08-02T00:20:50.607760: step 23300, loss 0.548089.
Train: 2018-08-02T00:20:51.387610: step 23301, loss 0.545094.
Train: 2018-08-02T00:20:51.555128: step 23302, loss 0.527813.
Train: 2018-08-02T00:20:51.721684: step 23303, loss 0.544411.
Train: 2018-08-02T00:20:51.884273: step 23304, loss 0.527064.
Train: 2018-08-02T00:20:52.047837: step 23305, loss 0.436083.
Train: 2018-08-02T00:20:52.215388: step 23306, loss 0.507825.
Train: 2018-08-02T00:20:52.377929: step 23307, loss 0.603404.
Train: 2018-08-02T00:20:52.545511: step 23308, loss 0.525338.
Train: 2018-08-02T00:20:52.713064: step 23309, loss 0.602756.
Train: 2018-08-02T00:20:52.876596: step 23310, loss 0.602153.
Test: 2018-08-02T00:20:53.406180: step 23310, loss 0.547779.
Train: 2018-08-02T00:20:53.568770: step 23311, loss 0.581015.
Train: 2018-08-02T00:20:53.734303: step 23312, loss 0.545255.
Train: 2018-08-02T00:20:53.897898: step 23313, loss 0.599702.
Train: 2018-08-02T00:20:54.065446: step 23314, loss 0.695194.
Train: 2018-08-02T00:20:54.239976: step 23315, loss 0.524927.
Train: 2018-08-02T00:20:54.403519: step 23316, loss 0.64952.
Train: 2018-08-02T00:20:54.578073: step 23317, loss 0.598419.
Train: 2018-08-02T00:20:54.741646: step 23318, loss 0.613932.
Train: 2018-08-02T00:20:54.906170: step 23319, loss 0.495951.
Train: 2018-08-02T00:20:55.072724: step 23320, loss 0.59666.
Test: 2018-08-02T00:20:55.604309: step 23320, loss 0.548353.
Train: 2018-08-02T00:20:55.767867: step 23321, loss 0.545401.
Train: 2018-08-02T00:20:55.936416: step 23322, loss 0.663791.
Train: 2018-08-02T00:20:56.098982: step 23323, loss 0.477347.
Train: 2018-08-02T00:20:56.266533: step 23324, loss 0.595129.
Train: 2018-08-02T00:20:56.440096: step 23325, loss 0.575545.
Train: 2018-08-02T00:20:56.619615: step 23326, loss 0.66242.
Train: 2018-08-02T00:20:56.789147: step 23327, loss 0.520929.
Train: 2018-08-02T00:20:56.953696: step 23328, loss 0.541015.
Train: 2018-08-02T00:20:57.126262: step 23329, loss 0.627571.
Train: 2018-08-02T00:20:57.295781: step 23330, loss 0.463517.
Test: 2018-08-02T00:20:57.826363: step 23330, loss 0.547872.
Train: 2018-08-02T00:20:57.991922: step 23331, loss 0.590247.
Train: 2018-08-02T00:20:58.159504: step 23332, loss 0.511572.
Train: 2018-08-02T00:20:58.328056: step 23333, loss 0.580594.
Train: 2018-08-02T00:20:58.506546: step 23334, loss 0.678711.
Train: 2018-08-02T00:20:58.678113: step 23335, loss 0.649081.
Train: 2018-08-02T00:20:58.839688: step 23336, loss 0.544939.
Train: 2018-08-02T00:20:59.006209: step 23337, loss 0.477933.
Train: 2018-08-02T00:20:59.170795: step 23338, loss 0.548233.
Train: 2018-08-02T00:20:59.348295: step 23339, loss 0.478954.
Train: 2018-08-02T00:20:59.509864: step 23340, loss 0.545215.
Test: 2018-08-02T00:21:00.037453: step 23340, loss 0.549095.
Train: 2018-08-02T00:21:00.200043: step 23341, loss 0.613912.
Train: 2018-08-02T00:21:00.364578: step 23342, loss 0.529522.
Train: 2018-08-02T00:21:00.527174: step 23343, loss 0.563261.
Train: 2018-08-02T00:21:00.694695: step 23344, loss 0.546111.
Train: 2018-08-02T00:21:00.857292: step 23345, loss 0.579616.
Train: 2018-08-02T00:21:01.025810: step 23346, loss 0.530264.
Train: 2018-08-02T00:21:01.194360: step 23347, loss 0.597574.
Train: 2018-08-02T00:21:01.361912: step 23348, loss 0.545753.
Train: 2018-08-02T00:21:01.526502: step 23349, loss 0.512495.
Train: 2018-08-02T00:21:01.703000: step 23350, loss 0.512172.
Test: 2018-08-02T00:21:02.245575: step 23350, loss 0.54905.
Train: 2018-08-02T00:21:02.421081: step 23351, loss 0.495279.
Train: 2018-08-02T00:21:02.595644: step 23352, loss 0.512027.
Train: 2018-08-02T00:21:02.766159: step 23353, loss 0.528871.
Train: 2018-08-02T00:21:02.936703: step 23354, loss 0.613575.
Train: 2018-08-02T00:21:03.103257: step 23355, loss 0.476586.
Train: 2018-08-02T00:21:03.265823: step 23356, loss 0.632479.
Train: 2018-08-02T00:21:03.427424: step 23357, loss 0.561563.
Train: 2018-08-02T00:21:03.592948: step 23358, loss 0.510141.
Train: 2018-08-02T00:21:03.758531: step 23359, loss 0.529937.
Train: 2018-08-02T00:21:03.924088: step 23360, loss 0.471371.
Test: 2018-08-02T00:21:04.461628: step 23360, loss 0.548394.
Train: 2018-08-02T00:21:04.639182: step 23361, loss 0.614554.
Train: 2018-08-02T00:21:04.812712: step 23362, loss 0.596326.
Train: 2018-08-02T00:21:04.976250: step 23363, loss 0.638152.
Train: 2018-08-02T00:21:05.139813: step 23364, loss 0.599703.
Train: 2018-08-02T00:21:05.303406: step 23365, loss 0.617074.
Train: 2018-08-02T00:21:05.473920: step 23366, loss 0.523302.
Train: 2018-08-02T00:21:05.643492: step 23367, loss 0.580778.
Train: 2018-08-02T00:21:05.807060: step 23368, loss 0.613227.
Train: 2018-08-02T00:21:05.966635: step 23369, loss 0.440628.
Train: 2018-08-02T00:21:06.133158: step 23370, loss 0.528699.
Test: 2018-08-02T00:21:06.664735: step 23370, loss 0.548814.
Train: 2018-08-02T00:21:06.832314: step 23371, loss 0.648712.
Train: 2018-08-02T00:21:06.997890: step 23372, loss 0.549267.
Train: 2018-08-02T00:21:07.165398: step 23373, loss 0.648083.
Train: 2018-08-02T00:21:07.335981: step 23374, loss 0.531469.
Train: 2018-08-02T00:21:07.512471: step 23375, loss 0.670251.
Train: 2018-08-02T00:21:07.678028: step 23376, loss 0.563247.
Train: 2018-08-02T00:21:07.844610: step 23377, loss 0.631621.
Train: 2018-08-02T00:21:08.010164: step 23378, loss 0.63139.
Train: 2018-08-02T00:21:08.183706: step 23379, loss 0.47806.
Train: 2018-08-02T00:21:08.356268: step 23380, loss 0.631503.
Test: 2018-08-02T00:21:08.900759: step 23380, loss 0.54916.
Train: 2018-08-02T00:21:09.072331: step 23381, loss 0.595653.
Train: 2018-08-02T00:21:09.242870: step 23382, loss 0.582891.
Train: 2018-08-02T00:21:09.416414: step 23383, loss 0.582449.
Train: 2018-08-02T00:21:09.578947: step 23384, loss 0.54653.
Train: 2018-08-02T00:21:09.743539: step 23385, loss 0.582452.
Train: 2018-08-02T00:21:09.918065: step 23386, loss 0.56318.
Train: 2018-08-02T00:21:10.082606: step 23387, loss 0.513006.
Train: 2018-08-02T00:21:10.244168: step 23388, loss 0.580414.
Train: 2018-08-02T00:21:10.410723: step 23389, loss 0.579957.
Train: 2018-08-02T00:21:10.575283: step 23390, loss 0.52994.
Test: 2018-08-02T00:21:11.108887: step 23390, loss 0.551355.
Train: 2018-08-02T00:21:11.275441: step 23391, loss 0.513119.
Train: 2018-08-02T00:21:11.442987: step 23392, loss 0.524203.
Train: 2018-08-02T00:21:11.609517: step 23393, loss 0.501361.
Train: 2018-08-02T00:21:11.773114: step 23394, loss 0.624407.
Train: 2018-08-02T00:21:11.940666: step 23395, loss 0.580433.
Train: 2018-08-02T00:21:12.105220: step 23396, loss 0.572755.
Train: 2018-08-02T00:21:12.275770: step 23397, loss 0.588081.
Train: 2018-08-02T00:21:12.442322: step 23398, loss 0.620011.
Train: 2018-08-02T00:21:12.605854: step 23399, loss 0.480385.
Train: 2018-08-02T00:21:12.771443: step 23400, loss 0.583303.
Test: 2018-08-02T00:21:13.309971: step 23400, loss 0.549849.
Train: 2018-08-02T00:21:14.102179: step 23401, loss 0.563795.
Train: 2018-08-02T00:21:14.264741: step 23402, loss 0.563846.
Train: 2018-08-02T00:21:14.430300: step 23403, loss 0.580754.
Train: 2018-08-02T00:21:14.596853: step 23404, loss 0.597781.
Train: 2018-08-02T00:21:14.773414: step 23405, loss 0.581139.
Train: 2018-08-02T00:21:14.934950: step 23406, loss 0.478519.
Train: 2018-08-02T00:21:15.102503: step 23407, loss 0.496776.
Train: 2018-08-02T00:21:15.267063: step 23408, loss 0.513971.
Train: 2018-08-02T00:21:15.439601: step 23409, loss 0.409179.
Train: 2018-08-02T00:21:15.610176: step 23410, loss 0.567924.
Test: 2018-08-02T00:21:16.147718: step 23410, loss 0.54978.
Train: 2018-08-02T00:21:16.313296: step 23411, loss 0.513233.
Train: 2018-08-02T00:21:16.481840: step 23412, loss 0.597338.
Train: 2018-08-02T00:21:16.654387: step 23413, loss 0.58191.
Train: 2018-08-02T00:21:16.818913: step 23414, loss 0.633437.
Train: 2018-08-02T00:21:16.985496: step 23415, loss 0.545579.
Train: 2018-08-02T00:21:17.161997: step 23416, loss 0.543068.
Train: 2018-08-02T00:21:17.334535: step 23417, loss 0.530136.
Train: 2018-08-02T00:21:17.501091: step 23418, loss 0.636076.
Train: 2018-08-02T00:21:17.674653: step 23419, loss 0.604977.
Train: 2018-08-02T00:21:17.842179: step 23420, loss 0.531527.
Test: 2018-08-02T00:21:18.363786: step 23420, loss 0.549837.
Train: 2018-08-02T00:21:18.528344: step 23421, loss 0.516445.
Train: 2018-08-02T00:21:18.693901: step 23422, loss 0.493374.
Train: 2018-08-02T00:21:18.856482: step 23423, loss 0.421485.
Train: 2018-08-02T00:21:19.033993: step 23424, loss 0.56321.
Train: 2018-08-02T00:21:19.201544: step 23425, loss 0.565489.
Train: 2018-08-02T00:21:19.365108: step 23426, loss 0.582514.
Train: 2018-08-02T00:21:19.527673: step 23427, loss 0.616778.
Train: 2018-08-02T00:21:19.692233: step 23428, loss 0.601204.
Train: 2018-08-02T00:21:19.856792: step 23429, loss 0.529145.
Train: 2018-08-02T00:21:20.026371: step 23430, loss 0.601933.
Test: 2018-08-02T00:21:20.553942: step 23430, loss 0.549669.
Train: 2018-08-02T00:21:20.716494: step 23431, loss 0.653251.
Train: 2018-08-02T00:21:20.880057: step 23432, loss 0.583772.
Train: 2018-08-02T00:21:21.044641: step 23433, loss 0.509762.
Train: 2018-08-02T00:21:21.211173: step 23434, loss 0.462514.
Train: 2018-08-02T00:21:21.378724: step 23435, loss 0.68752.
Train: 2018-08-02T00:21:21.540293: step 23436, loss 0.511427.
Train: 2018-08-02T00:21:21.701861: step 23437, loss 0.565279.
Train: 2018-08-02T00:21:21.865448: step 23438, loss 0.564121.
Train: 2018-08-02T00:21:22.031978: step 23439, loss 0.511091.
Train: 2018-08-02T00:21:22.201556: step 23440, loss 0.600593.
Test: 2018-08-02T00:21:22.724128: step 23440, loss 0.549596.
Train: 2018-08-02T00:21:22.884730: step 23441, loss 0.528314.
Train: 2018-08-02T00:21:23.052251: step 23442, loss 0.522351.
Train: 2018-08-02T00:21:23.219803: step 23443, loss 0.453463.
Train: 2018-08-02T00:21:23.380373: step 23444, loss 0.58107.
Train: 2018-08-02T00:21:23.555937: step 23445, loss 0.627421.
Train: 2018-08-02T00:21:23.738442: step 23446, loss 0.588341.
Train: 2018-08-02T00:21:23.906019: step 23447, loss 0.552487.
Train: 2018-08-02T00:21:24.067537: step 23448, loss 0.518694.
Train: 2018-08-02T00:21:24.238080: step 23449, loss 0.564178.
Train: 2018-08-02T00:21:24.401644: step 23450, loss 0.630502.
Test: 2018-08-02T00:21:24.941202: step 23450, loss 0.549573.
Train: 2018-08-02T00:21:25.110778: step 23451, loss 0.622147.
Train: 2018-08-02T00:21:25.280320: step 23452, loss 0.619411.
Train: 2018-08-02T00:21:25.447873: step 23453, loss 0.548708.
Train: 2018-08-02T00:21:25.614435: step 23454, loss 0.493802.
Train: 2018-08-02T00:21:25.780958: step 23455, loss 0.545523.
Train: 2018-08-02T00:21:25.945515: step 23456, loss 0.619008.
Train: 2018-08-02T00:21:26.110076: step 23457, loss 0.511454.
Train: 2018-08-02T00:21:26.273639: step 23458, loss 0.565672.
Train: 2018-08-02T00:21:26.438200: step 23459, loss 0.530933.
Train: 2018-08-02T00:21:26.607747: step 23460, loss 0.581664.
Test: 2018-08-02T00:21:27.146306: step 23460, loss 0.549719.
Train: 2018-08-02T00:21:27.314856: step 23461, loss 0.5656.
Train: 2018-08-02T00:21:27.478418: step 23462, loss 0.546533.
Train: 2018-08-02T00:21:27.644973: step 23463, loss 0.546656.
Train: 2018-08-02T00:21:27.825516: step 23464, loss 0.616071.
Train: 2018-08-02T00:21:28.001052: step 23465, loss 0.668146.
Train: 2018-08-02T00:21:28.167575: step 23466, loss 0.598898.
Train: 2018-08-02T00:21:28.342109: step 23467, loss 0.684971.
Train: 2018-08-02T00:21:28.505701: step 23468, loss 0.563739.
Train: 2018-08-02T00:21:28.669236: step 23469, loss 0.701643.
Train: 2018-08-02T00:21:28.837783: step 23470, loss 0.479547.
Test: 2018-08-02T00:21:29.369372: step 23470, loss 0.549946.
Train: 2018-08-02T00:21:29.533924: step 23471, loss 0.462105.
Train: 2018-08-02T00:21:29.707486: step 23472, loss 0.632268.
Train: 2018-08-02T00:21:29.869029: step 23473, loss 0.46275.
Train: 2018-08-02T00:21:30.033588: step 23474, loss 0.580996.
Train: 2018-08-02T00:21:30.198148: step 23475, loss 0.631594.
Train: 2018-08-02T00:21:30.368692: step 23476, loss 0.563954.
Train: 2018-08-02T00:21:30.533283: step 23477, loss 0.580862.
Train: 2018-08-02T00:21:30.695817: step 23478, loss 0.672044.
Train: 2018-08-02T00:21:30.863370: step 23479, loss 0.547291.
Train: 2018-08-02T00:21:31.027954: step 23480, loss 0.697759.
Test: 2018-08-02T00:21:31.558512: step 23480, loss 0.550103.
Train: 2018-08-02T00:21:31.726094: step 23481, loss 0.563974.
Train: 2018-08-02T00:21:31.905614: step 23482, loss 0.564051.
Train: 2018-08-02T00:21:32.070143: step 23483, loss 0.530892.
Train: 2018-08-02T00:21:32.231711: step 23484, loss 0.613591.
Train: 2018-08-02T00:21:32.396272: step 23485, loss 0.51464.
Train: 2018-08-02T00:21:32.559835: step 23486, loss 0.646303.
Train: 2018-08-02T00:21:32.725391: step 23487, loss 0.531233.
Train: 2018-08-02T00:21:32.888954: step 23488, loss 0.547589.
Train: 2018-08-02T00:21:33.053516: step 23489, loss 0.629536.
Train: 2018-08-02T00:21:33.222064: step 23490, loss 0.547662.
Test: 2018-08-02T00:21:33.753644: step 23490, loss 0.550372.
Train: 2018-08-02T00:21:33.916208: step 23491, loss 0.564046.
Train: 2018-08-02T00:21:34.080769: step 23492, loss 0.515094.
Train: 2018-08-02T00:21:34.245329: step 23493, loss 0.466116.
Train: 2018-08-02T00:21:34.407893: step 23494, loss 0.564184.
Train: 2018-08-02T00:21:34.577441: step 23495, loss 0.449287.
Train: 2018-08-02T00:21:34.742025: step 23496, loss 0.563913.
Train: 2018-08-02T00:21:34.909553: step 23497, loss 0.497833.
Train: 2018-08-02T00:21:35.089074: step 23498, loss 0.530688.
Train: 2018-08-02T00:21:35.254630: step 23499, loss 0.513911.
Train: 2018-08-02T00:21:35.424209: step 23500, loss 0.513629.
Test: 2018-08-02T00:21:35.941795: step 23500, loss 0.549647.
Train: 2018-08-02T00:21:36.764140: step 23501, loss 0.580483.
Train: 2018-08-02T00:21:36.929722: step 23502, loss 0.647892.
Train: 2018-08-02T00:21:37.099244: step 23503, loss 0.546749.
Train: 2018-08-02T00:21:37.261809: step 23504, loss 0.580631.
Train: 2018-08-02T00:21:37.427367: step 23505, loss 0.563612.
Train: 2018-08-02T00:21:37.597941: step 23506, loss 0.614632.
Train: 2018-08-02T00:21:37.767459: step 23507, loss 0.682744.
Train: 2018-08-02T00:21:37.933045: step 23508, loss 0.512499.
Train: 2018-08-02T00:21:38.099569: step 23509, loss 0.682937.
Train: 2018-08-02T00:21:38.270114: step 23510, loss 0.631562.
Test: 2018-08-02T00:21:38.800697: step 23510, loss 0.549342.
Train: 2018-08-02T00:21:38.967279: step 23511, loss 0.563547.
Train: 2018-08-02T00:21:39.135826: step 23512, loss 0.495825.
Train: 2018-08-02T00:21:39.303377: step 23513, loss 0.495836.
Train: 2018-08-02T00:21:39.480878: step 23514, loss 0.648124.
Train: 2018-08-02T00:21:39.651421: step 23515, loss 0.665056.
Train: 2018-08-02T00:21:39.818998: step 23516, loss 0.630984.
Train: 2018-08-02T00:21:39.982537: step 23517, loss 0.580336.
Train: 2018-08-02T00:21:40.149091: step 23518, loss 0.580278.
Train: 2018-08-02T00:21:40.312682: step 23519, loss 0.463169.
Train: 2018-08-02T00:21:40.484221: step 23520, loss 0.647079.
Test: 2018-08-02T00:21:41.023784: step 23520, loss 0.549558.
Train: 2018-08-02T00:21:41.190335: step 23521, loss 0.563525.
Train: 2018-08-02T00:21:41.361850: step 23522, loss 0.496921.
Train: 2018-08-02T00:21:41.529403: step 23523, loss 0.56352.
Train: 2018-08-02T00:21:41.698972: step 23524, loss 0.496935.
Train: 2018-08-02T00:21:41.878498: step 23525, loss 0.5635.
Train: 2018-08-02T00:21:42.044055: step 23526, loss 0.630153.
Train: 2018-08-02T00:21:42.209608: step 23527, loss 0.513487.
Train: 2018-08-02T00:21:42.384116: step 23528, loss 0.530116.
Train: 2018-08-02T00:21:42.548702: step 23529, loss 0.713717.
Train: 2018-08-02T00:21:42.730219: step 23530, loss 0.59681.
Test: 2018-08-02T00:21:43.267753: step 23530, loss 0.549504.
Train: 2018-08-02T00:21:43.432342: step 23531, loss 0.663397.
Train: 2018-08-02T00:21:43.597872: step 23532, loss 0.563462.
Train: 2018-08-02T00:21:43.768415: step 23533, loss 0.530297.
Train: 2018-08-02T00:21:43.941982: step 23534, loss 0.480695.
Train: 2018-08-02T00:21:44.118480: step 23535, loss 0.596584.
Train: 2018-08-02T00:21:44.283073: step 23536, loss 0.464132.
Train: 2018-08-02T00:21:44.451620: step 23537, loss 0.580023.
Train: 2018-08-02T00:21:44.620153: step 23538, loss 0.580026.
Train: 2018-08-02T00:21:44.786720: step 23539, loss 0.580028.
Train: 2018-08-02T00:21:44.949258: step 23540, loss 0.596739.
Test: 2018-08-02T00:21:45.478843: step 23540, loss 0.54949.
Train: 2018-08-02T00:21:45.643434: step 23541, loss 0.679726.
Train: 2018-08-02T00:21:45.816965: step 23542, loss 0.596589.
Train: 2018-08-02T00:21:45.981530: step 23543, loss 0.695908.
Train: 2018-08-02T00:21:46.153063: step 23544, loss 0.51391.
Train: 2018-08-02T00:21:46.318629: step 23545, loss 0.579922.
Train: 2018-08-02T00:21:46.485152: step 23546, loss 0.5799.
Train: 2018-08-02T00:21:46.659687: step 23547, loss 0.481487.
Train: 2018-08-02T00:21:46.828269: step 23548, loss 0.530675.
Train: 2018-08-02T00:21:46.999808: step 23549, loss 0.59627.
Train: 2018-08-02T00:21:47.164368: step 23550, loss 0.530627.
Test: 2018-08-02T00:21:47.694921: step 23550, loss 0.549708.
Train: 2018-08-02T00:21:47.866460: step 23551, loss 0.547039.
Train: 2018-08-02T00:21:48.032049: step 23552, loss 0.563435.
Train: 2018-08-02T00:21:48.201565: step 23553, loss 0.596298.
Train: 2018-08-02T00:21:48.367152: step 23554, loss 0.596373.
Train: 2018-08-02T00:21:48.531683: step 23555, loss 0.579851.
Train: 2018-08-02T00:21:48.698237: step 23556, loss 0.612739.
Train: 2018-08-02T00:21:48.871798: step 23557, loss 0.612696.
Train: 2018-08-02T00:21:49.043341: step 23558, loss 0.497737.
Train: 2018-08-02T00:21:49.206908: step 23559, loss 0.596224.
Train: 2018-08-02T00:21:49.371466: step 23560, loss 0.629047.
Test: 2018-08-02T00:21:49.909009: step 23560, loss 0.549672.
Train: 2018-08-02T00:21:50.072563: step 23561, loss 0.481426.
Train: 2018-08-02T00:21:50.251086: step 23562, loss 0.579792.
Train: 2018-08-02T00:21:50.417671: step 23563, loss 0.514169.
Train: 2018-08-02T00:21:50.595191: step 23564, loss 0.481253.
Train: 2018-08-02T00:21:50.758729: step 23565, loss 0.448108.
Train: 2018-08-02T00:21:50.937252: step 23566, loss 0.546769.
Train: 2018-08-02T00:21:51.103831: step 23567, loss 0.530087.
Train: 2018-08-02T00:21:51.274350: step 23568, loss 0.563236.
Train: 2018-08-02T00:21:51.446889: step 23569, loss 0.529744.
Train: 2018-08-02T00:21:51.617464: step 23570, loss 0.613548.
Test: 2018-08-02T00:21:52.149012: step 23570, loss 0.549067.
Train: 2018-08-02T00:21:52.313572: step 23571, loss 0.495819.
Train: 2018-08-02T00:21:52.492126: step 23572, loss 0.596942.
Train: 2018-08-02T00:21:52.663669: step 23573, loss 0.47841.
Train: 2018-08-02T00:21:52.832186: step 23574, loss 0.546113.
Train: 2018-08-02T00:21:53.000737: step 23575, loss 0.614322.
Train: 2018-08-02T00:21:53.172307: step 23576, loss 0.511764.
Train: 2018-08-02T00:21:53.348812: step 23577, loss 0.460109.
Train: 2018-08-02T00:21:53.514361: step 23578, loss 0.47692.
Train: 2018-08-02T00:21:53.684906: step 23579, loss 0.441905.
Train: 2018-08-02T00:21:53.848500: step 23580, loss 0.6328.
Test: 2018-08-02T00:21:54.381045: step 23580, loss 0.548476.
Train: 2018-08-02T00:21:54.545636: step 23581, loss 0.528134.
Train: 2018-08-02T00:21:54.710196: step 23582, loss 0.61586.
Train: 2018-08-02T00:21:54.876751: step 23583, loss 0.616036.
Train: 2018-08-02T00:21:55.044297: step 23584, loss 0.633811.
Train: 2018-08-02T00:21:55.205846: step 23585, loss 0.633868.
Train: 2018-08-02T00:21:55.372426: step 23586, loss 0.563163.
Train: 2018-08-02T00:21:55.543937: step 23587, loss 0.527829.
Train: 2018-08-02T00:21:55.708530: step 23588, loss 0.545488.
Train: 2018-08-02T00:21:55.875082: step 23589, loss 0.545472.
Train: 2018-08-02T00:21:56.043634: step 23590, loss 0.598494.
Test: 2018-08-02T00:21:56.585178: step 23590, loss 0.548347.
Train: 2018-08-02T00:21:56.760683: step 23591, loss 0.563142.
Train: 2018-08-02T00:21:56.926241: step 23592, loss 0.616123.
Train: 2018-08-02T00:21:57.094791: step 23593, loss 0.527838.
Train: 2018-08-02T00:21:57.272317: step 23594, loss 0.527847.
Train: 2018-08-02T00:21:57.437874: step 23595, loss 0.598373.
Train: 2018-08-02T00:21:57.607454: step 23596, loss 0.545486.
Train: 2018-08-02T00:21:57.774971: step 23597, loss 0.439829.
Train: 2018-08-02T00:21:57.941574: step 23598, loss 0.457325.
Train: 2018-08-02T00:21:58.127050: step 23599, loss 0.545443.
Train: 2018-08-02T00:21:58.296607: step 23600, loss 0.492292.
Test: 2018-08-02T00:21:58.835139: step 23600, loss 0.548273.
Train: 2018-08-02T00:21:59.632719: step 23601, loss 0.509871.
Train: 2018-08-02T00:21:59.804251: step 23602, loss 0.634413.
Train: 2018-08-02T00:21:59.967819: step 23603, loss 0.527492.
Train: 2018-08-02T00:22:00.150327: step 23604, loss 0.509562.
Train: 2018-08-02T00:22:00.319849: step 23605, loss 0.527377.
Train: 2018-08-02T00:22:00.489424: step 23606, loss 0.65303.
Train: 2018-08-02T00:22:00.660968: step 23607, loss 0.545265.
Train: 2018-08-02T00:22:00.826530: step 23608, loss 0.653194.
Train: 2018-08-02T00:22:01.007043: step 23609, loss 0.473333.
Train: 2018-08-02T00:22:01.174595: step 23610, loss 0.581229.
Test: 2018-08-02T00:22:01.705145: step 23610, loss 0.548171.
Train: 2018-08-02T00:22:01.869734: step 23611, loss 0.491268.
Train: 2018-08-02T00:22:02.050251: step 23612, loss 0.563244.
Train: 2018-08-02T00:22:02.220800: step 23613, loss 0.563245.
Train: 2018-08-02T00:22:02.382334: step 23614, loss 0.455092.
Train: 2018-08-02T00:22:02.548890: step 23615, loss 0.472993.
Train: 2018-08-02T00:22:02.713480: step 23616, loss 0.617585.
Train: 2018-08-02T00:22:02.878043: step 23617, loss 0.490798.
Train: 2018-08-02T00:22:03.059540: step 23618, loss 0.599686.
Train: 2018-08-02T00:22:03.223087: step 23619, loss 0.490919.
Train: 2018-08-02T00:22:03.389673: step 23620, loss 0.654356.
Test: 2018-08-02T00:22:03.935184: step 23620, loss 0.548127.
Train: 2018-08-02T00:22:04.102736: step 23621, loss 0.599643.
Train: 2018-08-02T00:22:04.316578: step 23622, loss 0.581917.
Train: 2018-08-02T00:22:04.492085: step 23623, loss 0.472999.
Train: 2018-08-02T00:22:04.660633: step 23624, loss 0.490282.
Train: 2018-08-02T00:22:04.828186: step 23625, loss 0.638272.
Train: 2018-08-02T00:22:05.000751: step 23626, loss 0.562923.
Train: 2018-08-02T00:22:05.179248: step 23627, loss 0.563012.
Train: 2018-08-02T00:22:05.341812: step 23628, loss 0.563325.
Train: 2018-08-02T00:22:05.505410: step 23629, loss 0.601538.
Train: 2018-08-02T00:22:05.670933: step 23630, loss 0.562181.
Test: 2018-08-02T00:22:06.207498: step 23630, loss 0.548164.
Train: 2018-08-02T00:22:06.373055: step 23631, loss 0.544194.
Train: 2018-08-02T00:22:06.537617: step 23632, loss 0.675724.
Train: 2018-08-02T00:22:06.706165: step 23633, loss 0.43346.
Train: 2018-08-02T00:22:06.867733: step 23634, loss 0.562602.
Train: 2018-08-02T00:22:07.036299: step 23635, loss 0.581576.
Train: 2018-08-02T00:22:07.198878: step 23636, loss 0.561836.
Train: 2018-08-02T00:22:07.365427: step 23637, loss 0.542718.
Train: 2018-08-02T00:22:07.529963: step 23638, loss 0.602339.
Train: 2018-08-02T00:22:07.690565: step 23639, loss 0.47406.
Train: 2018-08-02T00:22:07.859083: step 23640, loss 0.564901.
Test: 2018-08-02T00:22:08.389665: step 23640, loss 0.548226.
Train: 2018-08-02T00:22:08.561207: step 23641, loss 0.489922.
Train: 2018-08-02T00:22:08.725767: step 23642, loss 0.551471.
Train: 2018-08-02T00:22:08.892349: step 23643, loss 0.511472.
Train: 2018-08-02T00:22:09.059876: step 23644, loss 0.420525.
Train: 2018-08-02T00:22:09.223471: step 23645, loss 0.671165.
Train: 2018-08-02T00:22:09.386999: step 23646, loss 0.437763.
Train: 2018-08-02T00:22:09.551584: step 23647, loss 0.563574.
Train: 2018-08-02T00:22:09.716119: step 23648, loss 0.545238.
Train: 2018-08-02T00:22:09.879712: step 23649, loss 0.671101.
Train: 2018-08-02T00:22:10.048256: step 23650, loss 0.581025.
Test: 2018-08-02T00:22:10.580839: step 23650, loss 0.548263.
Train: 2018-08-02T00:22:10.745368: step 23651, loss 0.474318.
Train: 2018-08-02T00:22:10.906935: step 23652, loss 0.652853.
Train: 2018-08-02T00:22:11.070499: step 23653, loss 0.52801.
Train: 2018-08-02T00:22:11.242040: step 23654, loss 0.491899.
Train: 2018-08-02T00:22:11.408595: step 23655, loss 0.491719.
Train: 2018-08-02T00:22:11.575149: step 23656, loss 0.545294.
Train: 2018-08-02T00:22:11.743699: step 23657, loss 0.599117.
Train: 2018-08-02T00:22:11.913271: step 23658, loss 0.563073.
Train: 2018-08-02T00:22:12.077861: step 23659, loss 0.562409.
Train: 2018-08-02T00:22:12.242393: step 23660, loss 0.600048.
Test: 2018-08-02T00:22:12.771951: step 23660, loss 0.548225.
Train: 2018-08-02T00:22:12.935515: step 23661, loss 0.543508.
Train: 2018-08-02T00:22:13.103066: step 23662, loss 0.582141.
Train: 2018-08-02T00:22:13.267624: step 23663, loss 0.63463.
Train: 2018-08-02T00:22:13.432213: step 23664, loss 0.634879.
Train: 2018-08-02T00:22:13.594781: step 23665, loss 0.52748.
Train: 2018-08-02T00:22:13.763300: step 23666, loss 0.492255.
Train: 2018-08-02T00:22:13.937850: step 23667, loss 0.598881.
Train: 2018-08-02T00:22:14.102419: step 23668, loss 0.580955.
Train: 2018-08-02T00:22:14.267950: step 23669, loss 0.456594.
Train: 2018-08-02T00:22:14.431539: step 23670, loss 0.634174.
Test: 2018-08-02T00:22:14.968104: step 23670, loss 0.548291.
Train: 2018-08-02T00:22:15.137626: step 23671, loss 0.580875.
Train: 2018-08-02T00:22:15.302187: step 23672, loss 0.474582.
Train: 2018-08-02T00:22:15.465782: step 23673, loss 0.492294.
Train: 2018-08-02T00:22:15.635296: step 23674, loss 0.527725.
Train: 2018-08-02T00:22:15.799856: step 23675, loss 0.598593.
Train: 2018-08-02T00:22:15.966444: step 23676, loss 0.616335.
Train: 2018-08-02T00:22:16.136955: step 23677, loss 0.704934.
Train: 2018-08-02T00:22:16.303518: step 23678, loss 0.633831.
Train: 2018-08-02T00:22:16.472090: step 23679, loss 0.510184.
Train: 2018-08-02T00:22:16.642628: step 23680, loss 0.650989.
Test: 2018-08-02T00:22:17.182167: step 23680, loss 0.548342.
Train: 2018-08-02T00:22:17.352704: step 23681, loss 0.510428.
Train: 2018-08-02T00:22:17.535248: step 23682, loss 0.632968.
Train: 2018-08-02T00:22:17.704793: step 23683, loss 0.632735.
Train: 2018-08-02T00:22:17.869324: step 23684, loss 0.580341.
Train: 2018-08-02T00:22:18.033884: step 23685, loss 0.493697.
Train: 2018-08-02T00:22:18.206447: step 23686, loss 0.597436.
Train: 2018-08-02T00:22:18.371013: step 23687, loss 0.493631.
Train: 2018-08-02T00:22:18.549505: step 23688, loss 0.648901.
Train: 2018-08-02T00:22:18.718056: step 23689, loss 0.580687.
Train: 2018-08-02T00:22:18.884610: step 23690, loss 0.604827.
Test: 2018-08-02T00:22:19.413196: step 23690, loss 0.548259.
Train: 2018-08-02T00:22:19.581746: step 23691, loss 0.60019.
Train: 2018-08-02T00:22:19.747338: step 23692, loss 0.63156.
Train: 2018-08-02T00:22:19.925851: step 23693, loss 0.579771.
Train: 2018-08-02T00:22:20.091383: step 23694, loss 0.461759.
Train: 2018-08-02T00:22:20.251953: step 23695, loss 0.546164.
Train: 2018-08-02T00:22:20.433469: step 23696, loss 0.630255.
Train: 2018-08-02T00:22:20.604013: step 23697, loss 0.630125.
Train: 2018-08-02T00:22:20.765609: step 23698, loss 0.546315.
Train: 2018-08-02T00:22:20.931138: step 23699, loss 0.546366.
Train: 2018-08-02T00:22:21.095698: step 23700, loss 0.546404.
Test: 2018-08-02T00:22:21.631267: step 23700, loss 0.549136.
Train: 2018-08-02T00:22:22.384932: step 23701, loss 0.612995.
Train: 2018-08-02T00:22:22.554503: step 23702, loss 0.480015.
Train: 2018-08-02T00:22:22.719073: step 23703, loss 0.579687.
Train: 2018-08-02T00:22:22.881630: step 23704, loss 0.612887.
Train: 2018-08-02T00:22:23.047191: step 23705, loss 0.529903.
Train: 2018-08-02T00:22:23.214739: step 23706, loss 0.446988.
Train: 2018-08-02T00:22:23.388275: step 23707, loss 0.463398.
Train: 2018-08-02T00:22:23.561810: step 23708, loss 0.579693.
Train: 2018-08-02T00:22:23.727368: step 23709, loss 0.629818.
Train: 2018-08-02T00:22:23.890906: step 23710, loss 0.663347.
Test: 2018-08-02T00:22:24.423528: step 23710, loss 0.548989.
Train: 2018-08-02T00:22:24.586077: step 23711, loss 0.54627.
Train: 2018-08-02T00:22:24.751604: step 23712, loss 0.579712.
Train: 2018-08-02T00:22:24.921151: step 23713, loss 0.579706.
Train: 2018-08-02T00:22:25.095711: step 23714, loss 0.546248.
Train: 2018-08-02T00:22:25.267257: step 23715, loss 0.562966.
Train: 2018-08-02T00:22:25.431821: step 23716, loss 0.546225.
Train: 2018-08-02T00:22:25.600338: step 23717, loss 0.462497.
Train: 2018-08-02T00:22:25.772874: step 23718, loss 0.546157.
Train: 2018-08-02T00:22:25.937435: step 23719, loss 0.69741.
Train: 2018-08-02T00:22:26.101028: step 23720, loss 0.579724.
Test: 2018-08-02T00:22:26.635599: step 23720, loss 0.54883.
Train: 2018-08-02T00:22:26.800154: step 23721, loss 0.462045.
Train: 2018-08-02T00:22:26.963691: step 23722, loss 0.562896.
Train: 2018-08-02T00:22:27.135264: step 23723, loss 0.596598.
Train: 2018-08-02T00:22:27.301819: step 23724, loss 0.54601.
Train: 2018-08-02T00:22:27.470362: step 23725, loss 0.529102.
Train: 2018-08-02T00:22:27.632933: step 23726, loss 0.512137.
Train: 2018-08-02T00:22:27.797489: step 23727, loss 0.579794.
Train: 2018-08-02T00:22:27.960052: step 23728, loss 0.613752.
Train: 2018-08-02T00:22:28.138581: step 23729, loss 0.579821.
Train: 2018-08-02T00:22:28.309124: step 23730, loss 0.494869.
Test: 2018-08-02T00:22:28.833691: step 23730, loss 0.548578.
Train: 2018-08-02T00:22:29.002242: step 23731, loss 0.698947.
Train: 2018-08-02T00:22:29.166803: step 23732, loss 0.460793.
Train: 2018-08-02T00:22:29.328395: step 23733, loss 0.596859.
Train: 2018-08-02T00:22:29.490935: step 23734, loss 0.562814.
Train: 2018-08-02T00:22:29.652538: step 23735, loss 0.460612.
Train: 2018-08-02T00:22:29.816092: step 23736, loss 0.59693.
Train: 2018-08-02T00:22:29.984640: step 23737, loss 0.665293.
Train: 2018-08-02T00:22:30.145217: step 23738, loss 0.596949.
Train: 2018-08-02T00:22:30.306754: step 23739, loss 0.579857.
Train: 2018-08-02T00:22:30.481313: step 23740, loss 0.477563.
Test: 2018-08-02T00:22:31.007906: step 23740, loss 0.548511.
Train: 2018-08-02T00:22:31.182419: step 23741, loss 0.56279.
Train: 2018-08-02T00:22:31.344979: step 23742, loss 0.68219.
Train: 2018-08-02T00:22:31.509538: step 23743, loss 0.494645.
Train: 2018-08-02T00:22:31.690083: step 23744, loss 0.511689.
Train: 2018-08-02T00:22:31.861624: step 23745, loss 0.528699.
Train: 2018-08-02T00:22:32.027155: step 23746, loss 0.528662.
Train: 2018-08-02T00:22:32.192737: step 23747, loss 0.57985.
Train: 2018-08-02T00:22:32.358303: step 23748, loss 0.59696.
Train: 2018-08-02T00:22:32.523858: step 23749, loss 0.511453.
Train: 2018-08-02T00:22:32.690413: step 23750, loss 0.648375.
Test: 2018-08-02T00:22:33.230938: step 23750, loss 0.548423.
Train: 2018-08-02T00:22:33.398508: step 23751, loss 0.596999.
Train: 2018-08-02T00:22:33.563068: step 23752, loss 0.631194.
Train: 2018-08-02T00:22:33.726613: step 23753, loss 0.528588.
Train: 2018-08-02T00:22:33.894164: step 23754, loss 0.511547.
Train: 2018-08-02T00:22:34.057752: step 23755, loss 0.596889.
Train: 2018-08-02T00:22:34.223315: step 23756, loss 0.59687.
Train: 2018-08-02T00:22:34.389865: step 23757, loss 0.545705.
Train: 2018-08-02T00:22:34.564403: step 23758, loss 0.511643.
Train: 2018-08-02T00:22:34.730928: step 23759, loss 0.562746.
Train: 2018-08-02T00:22:34.894525: step 23760, loss 0.511615.
Test: 2018-08-02T00:22:35.425085: step 23760, loss 0.548455.
Train: 2018-08-02T00:22:35.591655: step 23761, loss 0.545681.
Train: 2018-08-02T00:22:35.759204: step 23762, loss 0.596891.
Train: 2018-08-02T00:22:35.929749: step 23763, loss 0.545648.
Train: 2018-08-02T00:22:36.096306: step 23764, loss 0.614027.
Train: 2018-08-02T00:22:36.257877: step 23765, loss 0.511436.
Train: 2018-08-02T00:22:36.425423: step 23766, loss 0.682492.
Train: 2018-08-02T00:22:36.592980: step 23767, loss 0.528543.
Train: 2018-08-02T00:22:36.767514: step 23768, loss 0.562725.
Train: 2018-08-02T00:22:36.935066: step 23769, loss 0.699342.
Train: 2018-08-02T00:22:37.108605: step 23770, loss 0.545685.
Test: 2018-08-02T00:22:37.639190: step 23770, loss 0.548482.
Train: 2018-08-02T00:22:37.868539: step 23771, loss 0.613755.
Train: 2018-08-02T00:22:38.037090: step 23772, loss 0.511814.
Train: 2018-08-02T00:22:38.203670: step 23773, loss 0.54578.
Train: 2018-08-02T00:22:38.368251: step 23774, loss 0.562731.
Train: 2018-08-02T00:22:38.531798: step 23775, loss 0.715026.
Train: 2018-08-02T00:22:38.699350: step 23776, loss 0.562739.
Train: 2018-08-02T00:22:38.862881: step 23777, loss 0.545914.
Train: 2018-08-02T00:22:39.038413: step 23778, loss 0.445146.
Train: 2018-08-02T00:22:39.206996: step 23779, loss 0.598595.
Train: 2018-08-02T00:22:39.375544: step 23780, loss 0.613135.
Test: 2018-08-02T00:22:39.923049: step 23780, loss 0.548707.
Train: 2018-08-02T00:22:40.094615: step 23781, loss 0.596309.
Train: 2018-08-02T00:22:40.264136: step 23782, loss 0.546007.
Train: 2018-08-02T00:22:40.430691: step 23783, loss 0.579503.
Train: 2018-08-02T00:22:40.595275: step 23784, loss 0.428986.
Train: 2018-08-02T00:22:40.761839: step 23785, loss 0.596244.
Train: 2018-08-02T00:22:40.930380: step 23786, loss 0.529252.
Train: 2018-08-02T00:22:41.101898: step 23787, loss 0.66337.
Train: 2018-08-02T00:22:41.264487: step 23788, loss 0.512457.
Train: 2018-08-02T00:22:41.428071: step 23789, loss 0.680146.
Train: 2018-08-02T00:22:41.593610: step 23790, loss 0.596253.
Test: 2018-08-02T00:22:42.131145: step 23790, loss 0.548749.
Train: 2018-08-02T00:22:42.299694: step 23791, loss 0.612937.
Train: 2018-08-02T00:22:42.469266: step 23792, loss 0.562766.
Train: 2018-08-02T00:22:42.633831: step 23793, loss 0.546109.
Train: 2018-08-02T00:22:42.800387: step 23794, loss 0.546136.
Train: 2018-08-02T00:22:42.965913: step 23795, loss 0.546151.
Train: 2018-08-02T00:22:43.140473: step 23796, loss 0.662554.
Train: 2018-08-02T00:22:43.305019: step 23797, loss 0.612598.
Train: 2018-08-02T00:22:43.474579: step 23798, loss 0.64564.
Train: 2018-08-02T00:22:43.640139: step 23799, loss 0.513283.
Train: 2018-08-02T00:22:43.809688: step 23800, loss 0.645275.
Test: 2018-08-02T00:22:44.339242: step 23800, loss 0.549103.
Train: 2018-08-02T00:22:45.170613: step 23801, loss 0.529989.
Train: 2018-08-02T00:22:45.335149: step 23802, loss 0.513657.
Train: 2018-08-02T00:22:45.498742: step 23803, loss 0.612081.
Train: 2018-08-02T00:22:45.671275: step 23804, loss 0.530155.
Train: 2018-08-02T00:22:45.835811: step 23805, loss 0.530177.
Train: 2018-08-02T00:22:46.002365: step 23806, loss 0.497433.
Train: 2018-08-02T00:22:46.173907: step 23807, loss 0.46456.
Train: 2018-08-02T00:22:46.337470: step 23808, loss 0.645033.
Train: 2018-08-02T00:22:46.502060: step 23809, loss 0.529938.
Train: 2018-08-02T00:22:46.672599: step 23810, loss 0.513372.
Test: 2018-08-02T00:22:47.208142: step 23810, loss 0.54897.
Train: 2018-08-02T00:22:47.383673: step 23811, loss 0.628926.
Train: 2018-08-02T00:22:47.548261: step 23812, loss 0.562799.
Train: 2018-08-02T00:22:47.717805: step 23813, loss 0.529637.
Train: 2018-08-02T00:22:47.886373: step 23814, loss 0.546167.
Train: 2018-08-02T00:22:48.064886: step 23815, loss 0.52948.
Train: 2018-08-02T00:22:48.232403: step 23816, loss 0.529384.
Train: 2018-08-02T00:22:48.394970: step 23817, loss 0.562724.
Train: 2018-08-02T00:22:48.560527: step 23818, loss 0.59624.
Train: 2018-08-02T00:22:48.723116: step 23819, loss 0.529107.
Train: 2018-08-02T00:22:48.890674: step 23820, loss 0.444856.
Test: 2018-08-02T00:22:49.405281: step 23820, loss 0.548525.
Train: 2018-08-02T00:22:49.576810: step 23821, loss 0.596464.
Train: 2018-08-02T00:22:49.742395: step 23822, loss 0.545719.
Train: 2018-08-02T00:22:49.908949: step 23823, loss 0.545662.
Train: 2018-08-02T00:22:50.074480: step 23824, loss 0.443359.
Train: 2018-08-02T00:22:50.242031: step 23825, loss 0.528416.
Train: 2018-08-02T00:22:50.406592: step 23826, loss 0.545455.
Train: 2018-08-02T00:22:50.573147: step 23827, loss 0.648921.
Train: 2018-08-02T00:22:50.737737: step 23828, loss 0.579944.
Train: 2018-08-02T00:22:50.903264: step 23829, loss 0.666637.
Train: 2018-08-02T00:22:51.066859: step 23830, loss 0.597323.
Test: 2018-08-02T00:22:51.596410: step 23830, loss 0.548133.
Train: 2018-08-02T00:22:51.759974: step 23831, loss 0.597314.
Train: 2018-08-02T00:22:51.923535: step 23832, loss 0.528002.
Train: 2018-08-02T00:22:52.096124: step 23833, loss 0.545325.
Train: 2018-08-02T00:22:52.266652: step 23834, loss 0.476047.
Train: 2018-08-02T00:22:52.433204: step 23835, loss 0.597323.
Train: 2018-08-02T00:22:52.596737: step 23836, loss 0.510597.
Train: 2018-08-02T00:22:52.760335: step 23837, loss 0.49317.
Train: 2018-08-02T00:22:52.927876: step 23838, loss 0.458236.
Train: 2018-08-02T00:22:53.092466: step 23839, loss 0.545206.
Train: 2018-08-02T00:22:53.259964: step 23840, loss 0.597688.
Test: 2018-08-02T00:22:53.775586: step 23840, loss 0.547992.
Train: 2018-08-02T00:22:53.940146: step 23841, loss 0.474962.
Train: 2018-08-02T00:22:54.112710: step 23842, loss 0.65068.
Train: 2018-08-02T00:22:54.279238: step 23843, loss 0.562706.
Train: 2018-08-02T00:22:54.454770: step 23844, loss 0.474496.
Train: 2018-08-02T00:22:54.620355: step 23845, loss 0.545045.
Train: 2018-08-02T00:22:54.786882: step 23846, loss 0.562741.
Train: 2018-08-02T00:22:54.954434: step 23847, loss 0.545004.
Train: 2018-08-02T00:22:55.123001: step 23848, loss 0.598325.
Train: 2018-08-02T00:22:55.289537: step 23849, loss 0.527181.
Train: 2018-08-02T00:22:55.464096: step 23850, loss 0.634057.
Test: 2018-08-02T00:22:56.005625: step 23850, loss 0.547862.
Train: 2018-08-02T00:22:56.182152: step 23851, loss 0.580603.
Train: 2018-08-02T00:22:56.350702: step 23852, loss 0.438075.
Train: 2018-08-02T00:22:56.515288: step 23853, loss 0.50928.
Train: 2018-08-02T00:22:56.681847: step 23854, loss 0.544939.
Train: 2018-08-02T00:22:56.847404: step 23855, loss 0.67019.
Train: 2018-08-02T00:22:57.018940: step 23856, loss 0.544924.
Train: 2018-08-02T00:22:57.189492: step 23857, loss 0.544923.
Train: 2018-08-02T00:22:57.361025: step 23858, loss 0.544921.
Train: 2018-08-02T00:22:57.524596: step 23859, loss 0.527016.
Train: 2018-08-02T00:22:57.690152: step 23860, loss 0.706122.
Test: 2018-08-02T00:22:58.228681: step 23860, loss 0.547832.
Train: 2018-08-02T00:22:58.394239: step 23861, loss 0.580696.
Train: 2018-08-02T00:22:58.563817: step 23862, loss 0.473508.
Train: 2018-08-02T00:22:58.725355: step 23863, loss 0.616333.
Train: 2018-08-02T00:22:58.899888: step 23864, loss 0.544949.
Train: 2018-08-02T00:22:59.063450: step 23865, loss 0.634001.
Train: 2018-08-02T00:22:59.239977: step 23866, loss 0.580523.
Train: 2018-08-02T00:22:59.419498: step 23867, loss 0.527254.
Train: 2018-08-02T00:22:59.593059: step 23868, loss 0.598142.
Train: 2018-08-02T00:22:59.760611: step 23869, loss 0.598061.
Train: 2018-08-02T00:22:59.924180: step 23870, loss 0.562688.
Test: 2018-08-02T00:23:00.446752: step 23870, loss 0.547935.
Train: 2018-08-02T00:23:00.609345: step 23871, loss 0.527472.
Train: 2018-08-02T00:23:00.772879: step 23872, loss 0.632961.
Train: 2018-08-02T00:23:00.937440: step 23873, loss 0.527586.
Train: 2018-08-02T00:23:01.104023: step 23874, loss 0.562642.
Train: 2018-08-02T00:23:01.272591: step 23875, loss 0.667473.
Train: 2018-08-02T00:23:01.438132: step 23876, loss 0.562622.
Train: 2018-08-02T00:23:01.611637: step 23877, loss 0.545236.
Train: 2018-08-02T00:23:01.780213: step 23878, loss 0.52793.
Train: 2018-08-02T00:23:01.943781: step 23879, loss 0.614538.
Train: 2018-08-02T00:23:02.108341: step 23880, loss 0.545339.
Test: 2018-08-02T00:23:02.622950: step 23880, loss 0.548154.
Train: 2018-08-02T00:23:02.788492: step 23881, loss 0.562593.
Train: 2018-08-02T00:23:02.956045: step 23882, loss 0.579808.
Train: 2018-08-02T00:23:03.117612: step 23883, loss 0.511027.
Train: 2018-08-02T00:23:03.283169: step 23884, loss 0.596937.
Train: 2018-08-02T00:23:03.459723: step 23885, loss 0.459667.
Train: 2018-08-02T00:23:03.626251: step 23886, loss 0.648384.
Train: 2018-08-02T00:23:03.788818: step 23887, loss 0.45972.
Train: 2018-08-02T00:23:03.951382: step 23888, loss 0.47681.
Train: 2018-08-02T00:23:04.118935: step 23889, loss 0.614141.
Train: 2018-08-02T00:23:04.286518: step 23890, loss 0.579785.
Test: 2018-08-02T00:23:04.800123: step 23890, loss 0.548175.
Train: 2018-08-02T00:23:04.970683: step 23891, loss 0.528168.
Train: 2018-08-02T00:23:05.135248: step 23892, loss 0.562585.
Train: 2018-08-02T00:23:05.299805: step 23893, loss 0.666018.
Train: 2018-08-02T00:23:05.468352: step 23894, loss 0.579811.
Train: 2018-08-02T00:23:05.634882: step 23895, loss 0.47652.
Train: 2018-08-02T00:23:05.804456: step 23896, loss 0.562582.
Train: 2018-08-02T00:23:05.969017: step 23897, loss 0.614256.
Train: 2018-08-02T00:23:06.135597: step 23898, loss 0.579798.
Train: 2018-08-02T00:23:06.299107: step 23899, loss 0.545372.
Train: 2018-08-02T00:23:06.465662: step 23900, loss 0.57978.
Test: 2018-08-02T00:23:06.999236: step 23900, loss 0.54818.
Train: 2018-08-02T00:23:07.831044: step 23901, loss 0.459421.
Train: 2018-08-02T00:23:07.995603: step 23902, loss 0.596994.
Train: 2018-08-02T00:23:08.164153: step 23903, loss 0.597005.
Train: 2018-08-02T00:23:08.330746: step 23904, loss 0.510941.
Train: 2018-08-02T00:23:08.494302: step 23905, loss 0.510912.
Train: 2018-08-02T00:23:08.665840: step 23906, loss 0.562575.
Train: 2018-08-02T00:23:08.829374: step 23907, loss 0.631612.
Train: 2018-08-02T00:23:08.996963: step 23908, loss 0.528057.
Train: 2018-08-02T00:23:09.168497: step 23909, loss 0.545309.
Train: 2018-08-02T00:23:09.333029: step 23910, loss 0.597129.
Test: 2018-08-02T00:23:09.873616: step 23910, loss 0.548106.
Train: 2018-08-02T00:23:10.046148: step 23911, loss 0.648964.
Train: 2018-08-02T00:23:10.217694: step 23912, loss 0.597089.
Train: 2018-08-02T00:23:10.385215: step 23913, loss 0.56257.
Train: 2018-08-02T00:23:10.549807: step 23914, loss 0.665819.
Train: 2018-08-02T00:23:10.718326: step 23915, loss 0.52824.
Train: 2018-08-02T00:23:10.882915: step 23916, loss 0.459787.
Train: 2018-08-02T00:23:11.049471: step 23917, loss 0.562565.
Train: 2018-08-02T00:23:11.215028: step 23918, loss 0.631043.
Train: 2018-08-02T00:23:11.382574: step 23919, loss 0.562565.
Train: 2018-08-02T00:23:11.554120: step 23920, loss 0.579646.
Test: 2018-08-02T00:23:12.095643: step 23920, loss 0.548279.
Train: 2018-08-02T00:23:12.261232: step 23921, loss 0.613749.
Train: 2018-08-02T00:23:12.428778: step 23922, loss 0.562568.
Train: 2018-08-02T00:23:12.588326: step 23923, loss 0.56257.
Train: 2018-08-02T00:23:12.753912: step 23924, loss 0.494648.
Train: 2018-08-02T00:23:12.921468: step 23925, loss 0.545596.
Train: 2018-08-02T00:23:13.084999: step 23926, loss 0.494662.
Train: 2018-08-02T00:23:13.249559: step 23927, loss 0.596558.
Train: 2018-08-02T00:23:13.413121: step 23928, loss 0.477554.
Train: 2018-08-02T00:23:13.575686: step 23929, loss 0.630685.
Train: 2018-08-02T00:23:13.741275: step 23930, loss 0.460323.
Test: 2018-08-02T00:23:14.271856: step 23930, loss 0.548262.
Train: 2018-08-02T00:23:14.438381: step 23931, loss 0.511341.
Train: 2018-08-02T00:23:14.608926: step 23932, loss 0.665244.
Train: 2018-08-02T00:23:14.780466: step 23933, loss 0.562555.
Train: 2018-08-02T00:23:14.946024: step 23934, loss 0.545414.
Train: 2018-08-02T00:23:15.114576: step 23935, loss 0.562554.
Train: 2018-08-02T00:23:15.278166: step 23936, loss 0.579723.
Train: 2018-08-02T00:23:15.445689: step 23937, loss 0.459484.
Train: 2018-08-02T00:23:15.613271: step 23938, loss 0.579763.
Train: 2018-08-02T00:23:15.780817: step 23939, loss 0.562554.
Train: 2018-08-02T00:23:15.948371: step 23940, loss 0.631583.
Test: 2018-08-02T00:23:16.484910: step 23940, loss 0.548099.
Train: 2018-08-02T00:23:16.647476: step 23941, loss 0.666079.
Train: 2018-08-02T00:23:16.809069: step 23942, loss 0.597116.
Train: 2018-08-02T00:23:16.974628: step 23943, loss 0.545269.
Train: 2018-08-02T00:23:17.142196: step 23944, loss 0.47671.
Train: 2018-08-02T00:23:17.308707: step 23945, loss 0.596917.
Train: 2018-08-02T00:23:17.473267: step 23946, loss 0.545332.
Train: 2018-08-02T00:23:17.645832: step 23947, loss 0.562503.
Train: 2018-08-02T00:23:17.810367: step 23948, loss 0.579651.
Train: 2018-08-02T00:23:17.976954: step 23949, loss 0.52809.
Train: 2018-08-02T00:23:18.149490: step 23950, loss 0.510916.
Test: 2018-08-02T00:23:18.680041: step 23950, loss 0.548167.
Train: 2018-08-02T00:23:18.847595: step 23951, loss 0.492946.
Train: 2018-08-02T00:23:19.014166: step 23952, loss 0.719854.
Train: 2018-08-02T00:23:19.177712: step 23953, loss 0.52658.
Train: 2018-08-02T00:23:19.344267: step 23954, loss 0.578549.
Train: 2018-08-02T00:23:19.506832: step 23955, loss 0.619407.
Train: 2018-08-02T00:23:19.674384: step 23956, loss 0.492145.
Train: 2018-08-02T00:23:19.838943: step 23957, loss 0.474985.
Train: 2018-08-02T00:23:20.004501: step 23958, loss 0.510149.
Train: 2018-08-02T00:23:20.171055: step 23959, loss 0.600185.
Train: 2018-08-02T00:23:20.331627: step 23960, loss 0.580659.
Test: 2018-08-02T00:23:20.861255: step 23960, loss 0.547969.
Train: 2018-08-02T00:23:21.028762: step 23961, loss 0.51033.
Train: 2018-08-02T00:23:21.194353: step 23962, loss 0.611841.
Train: 2018-08-02T00:23:21.359911: step 23963, loss 0.582268.
Train: 2018-08-02T00:23:21.522471: step 23964, loss 0.527501.
Train: 2018-08-02T00:23:21.692020: step 23965, loss 0.526676.
Train: 2018-08-02T00:23:21.855578: step 23966, loss 0.563265.
Train: 2018-08-02T00:23:22.025134: step 23967, loss 0.511435.
Train: 2018-08-02T00:23:22.190688: step 23968, loss 0.542441.
Train: 2018-08-02T00:23:22.354251: step 23969, loss 0.580022.
Train: 2018-08-02T00:23:22.524797: step 23970, loss 0.602313.
Test: 2018-08-02T00:23:23.060332: step 23970, loss 0.547877.
Train: 2018-08-02T00:23:23.224918: step 23971, loss 0.576745.
Train: 2018-08-02T00:23:23.391447: step 23972, loss 0.509523.
Train: 2018-08-02T00:23:23.555010: step 23973, loss 0.522857.
Train: 2018-08-02T00:23:23.728546: step 23974, loss 0.540817.
Train: 2018-08-02T00:23:23.892133: step 23975, loss 0.580539.
Train: 2018-08-02T00:23:24.063675: step 23976, loss 0.630511.
Train: 2018-08-02T00:23:24.232224: step 23977, loss 0.640489.
Train: 2018-08-02T00:23:24.395793: step 23978, loss 0.562959.
Train: 2018-08-02T00:23:24.559353: step 23979, loss 0.489028.
Train: 2018-08-02T00:23:24.722918: step 23980, loss 0.505663.
Test: 2018-08-02T00:23:25.261447: step 23980, loss 0.547931.
Train: 2018-08-02T00:23:25.431992: step 23981, loss 0.462225.
Train: 2018-08-02T00:23:25.595581: step 23982, loss 0.612237.
Train: 2018-08-02T00:23:25.757147: step 23983, loss 0.507687.
Train: 2018-08-02T00:23:25.924709: step 23984, loss 0.443241.
Train: 2018-08-02T00:23:26.095219: step 23985, loss 0.628986.
Train: 2018-08-02T00:23:26.259804: step 23986, loss 0.551696.
Train: 2018-08-02T00:23:26.428353: step 23987, loss 0.636232.
Train: 2018-08-02T00:23:26.589927: step 23988, loss 0.624746.
Train: 2018-08-02T00:23:26.756484: step 23989, loss 0.512497.
Train: 2018-08-02T00:23:26.920014: step 23990, loss 0.648231.
Test: 2018-08-02T00:23:27.463590: step 23990, loss 0.548156.
Train: 2018-08-02T00:23:27.639111: step 23991, loss 0.665489.
Train: 2018-08-02T00:23:27.807641: step 23992, loss 0.596638.
Train: 2018-08-02T00:23:27.975203: step 23993, loss 0.477844.
Train: 2018-08-02T00:23:28.141777: step 23994, loss 0.513532.
Train: 2018-08-02T00:23:28.309314: step 23995, loss 0.562514.
Train: 2018-08-02T00:23:28.474858: step 23996, loss 0.613671.
Train: 2018-08-02T00:23:28.639417: step 23997, loss 0.562893.
Train: 2018-08-02T00:23:28.806997: step 23998, loss 0.544575.
Train: 2018-08-02T00:23:28.974540: step 23999, loss 0.580305.
Train: 2018-08-02T00:23:29.142102: step 24000, loss 0.712664.
Test: 2018-08-02T00:23:29.683642: step 24000, loss 0.549001.
Train: 2018-08-02T00:23:30.489644: step 24001, loss 0.496606.
Train: 2018-08-02T00:23:30.657194: step 24002, loss 0.579234.
Train: 2018-08-02T00:23:30.822752: step 24003, loss 0.694602.
Train: 2018-08-02T00:23:30.997290: step 24004, loss 0.513327.
Train: 2018-08-02T00:23:31.164812: step 24005, loss 0.564198.
Train: 2018-08-02T00:23:31.330368: step 24006, loss 0.578686.
Train: 2018-08-02T00:23:31.502908: step 24007, loss 0.466734.
Train: 2018-08-02T00:23:31.668496: step 24008, loss 0.562789.
Train: 2018-08-02T00:23:31.833025: step 24009, loss 0.515863.
Train: 2018-08-02T00:23:32.002604: step 24010, loss 0.628494.
Test: 2018-08-02T00:23:32.540135: step 24010, loss 0.548919.
Train: 2018-08-02T00:23:32.706731: step 24011, loss 0.579271.
Train: 2018-08-02T00:23:32.867285: step 24012, loss 0.480241.
Train: 2018-08-02T00:23:33.041793: step 24013, loss 0.52859.
Train: 2018-08-02T00:23:33.207351: step 24014, loss 0.478357.
Train: 2018-08-02T00:23:33.372915: step 24015, loss 0.595695.
Train: 2018-08-02T00:23:33.538491: step 24016, loss 0.61239.
Train: 2018-08-02T00:23:33.706017: step 24017, loss 0.546036.
Train: 2018-08-02T00:23:33.868610: step 24018, loss 0.544864.
Train: 2018-08-02T00:23:34.037133: step 24019, loss 0.662118.
Train: 2018-08-02T00:23:34.204685: step 24020, loss 0.513107.
Test: 2018-08-02T00:23:34.736264: step 24020, loss 0.548599.
Train: 2018-08-02T00:23:34.900824: step 24021, loss 0.579931.
Train: 2018-08-02T00:23:35.066406: step 24022, loss 0.529107.
Train: 2018-08-02T00:23:35.228974: step 24023, loss 0.512249.
Train: 2018-08-02T00:23:35.395501: step 24024, loss 0.613969.
Train: 2018-08-02T00:23:35.559065: step 24025, loss 0.69584.
Train: 2018-08-02T00:23:35.725644: step 24026, loss 0.49476.
Train: 2018-08-02T00:23:35.891228: step 24027, loss 0.614454.
Train: 2018-08-02T00:23:36.056758: step 24028, loss 0.563224.
Train: 2018-08-02T00:23:36.222319: step 24029, loss 0.595707.
Train: 2018-08-02T00:23:36.396843: step 24030, loss 0.478402.
Test: 2018-08-02T00:23:36.920424: step 24030, loss 0.548549.
Train: 2018-08-02T00:23:37.083019: step 24031, loss 0.612423.
Train: 2018-08-02T00:23:37.245586: step 24032, loss 0.52891.
Train: 2018-08-02T00:23:37.410141: step 24033, loss 0.597719.
Train: 2018-08-02T00:23:37.575699: step 24034, loss 0.561626.
Train: 2018-08-02T00:23:37.742261: step 24035, loss 0.461939.
Train: 2018-08-02T00:23:37.903829: step 24036, loss 0.47809.
Train: 2018-08-02T00:23:38.066362: step 24037, loss 0.528266.
Train: 2018-08-02T00:23:38.229925: step 24038, loss 0.528674.
Train: 2018-08-02T00:23:38.393515: step 24039, loss 0.496377.
Train: 2018-08-02T00:23:38.557050: step 24040, loss 0.562213.
Test: 2018-08-02T00:23:39.084639: step 24040, loss 0.548288.
Train: 2018-08-02T00:23:39.248203: step 24041, loss 0.546046.
Train: 2018-08-02T00:23:39.413793: step 24042, loss 0.667433.
Train: 2018-08-02T00:23:39.577350: step 24043, loss 0.47775.
Train: 2018-08-02T00:23:39.740923: step 24044, loss 0.580585.
Train: 2018-08-02T00:23:39.903481: step 24045, loss 0.66695.
Train: 2018-08-02T00:23:40.068048: step 24046, loss 0.701031.
Train: 2018-08-02T00:23:40.230607: step 24047, loss 0.510777.
Train: 2018-08-02T00:23:40.397132: step 24048, loss 0.545451.
Train: 2018-08-02T00:23:40.557727: step 24049, loss 0.458938.
Train: 2018-08-02T00:23:40.724275: step 24050, loss 0.562648.
Test: 2018-08-02T00:23:41.250880: step 24050, loss 0.54816.
Train: 2018-08-02T00:23:41.424418: step 24051, loss 0.510741.
Train: 2018-08-02T00:23:41.591963: step 24052, loss 0.562652.
Train: 2018-08-02T00:23:41.757521: step 24053, loss 0.545309.
Train: 2018-08-02T00:23:41.919062: step 24054, loss 0.562657.
Train: 2018-08-02T00:23:42.080662: step 24055, loss 0.59743.
Train: 2018-08-02T00:23:42.244219: step 24056, loss 0.632236.
Train: 2018-08-02T00:23:42.404789: step 24057, loss 0.597431.
Train: 2018-08-02T00:23:42.568351: step 24058, loss 0.493164.
Train: 2018-08-02T00:23:42.730893: step 24059, loss 0.441039.
Train: 2018-08-02T00:23:42.895460: step 24060, loss 0.562654.
Test: 2018-08-02T00:23:43.426035: step 24060, loss 0.548069.
Train: 2018-08-02T00:23:43.592590: step 24061, loss 0.632341.
Train: 2018-08-02T00:23:43.758179: step 24062, loss 0.492952.
Train: 2018-08-02T00:23:43.922734: step 24063, loss 0.632433.
Train: 2018-08-02T00:23:44.088295: step 24064, loss 0.597545.
Train: 2018-08-02T00:23:44.249864: step 24065, loss 0.614965.
Train: 2018-08-02T00:23:44.423368: step 24066, loss 0.562645.
Train: 2018-08-02T00:23:44.591918: step 24067, loss 0.545236.
Train: 2018-08-02T00:23:44.757505: step 24068, loss 0.562634.
Train: 2018-08-02T00:23:44.922048: step 24069, loss 0.597387.
Train: 2018-08-02T00:23:45.082605: step 24070, loss 0.510542.
Test: 2018-08-02T00:23:45.603214: step 24070, loss 0.548087.
Train: 2018-08-02T00:23:45.766808: step 24071, loss 0.718806.
Train: 2018-08-02T00:23:45.927373: step 24072, loss 0.510665.
Train: 2018-08-02T00:23:46.090941: step 24073, loss 0.597188.
Train: 2018-08-02T00:23:46.257464: step 24074, loss 0.528078.
Train: 2018-08-02T00:23:46.419058: step 24075, loss 0.562596.
Train: 2018-08-02T00:23:46.586585: step 24076, loss 0.476475.
Train: 2018-08-02T00:23:46.746184: step 24077, loss 0.614266.
Train: 2018-08-02T00:23:46.908724: step 24078, loss 0.562588.
Train: 2018-08-02T00:23:47.068297: step 24079, loss 0.614206.
Train: 2018-08-02T00:23:47.229865: step 24080, loss 0.635919.
Test: 2018-08-02T00:23:47.758452: step 24080, loss 0.548214.
Train: 2018-08-02T00:23:47.922014: step 24081, loss 0.614047.
Train: 2018-08-02T00:23:48.085609: step 24082, loss 0.511232.
Train: 2018-08-02T00:23:48.250163: step 24083, loss 0.630939.
Train: 2018-08-02T00:23:48.411706: step 24084, loss 0.630792.
Train: 2018-08-02T00:23:48.587237: step 24085, loss 0.562581.
Train: 2018-08-02T00:23:48.747838: step 24086, loss 0.545621.
Train: 2018-08-02T00:23:48.911395: step 24087, loss 0.511798.
Train: 2018-08-02T00:23:49.071941: step 24088, loss 0.494946.
Train: 2018-08-02T00:23:49.233542: step 24089, loss 0.54568.
Train: 2018-08-02T00:23:49.403056: step 24090, loss 0.613322.
Test: 2018-08-02T00:23:49.925661: step 24090, loss 0.548432.
Train: 2018-08-02T00:23:50.089222: step 24091, loss 0.478068.
Train: 2018-08-02T00:23:50.256799: step 24092, loss 0.579499.
Train: 2018-08-02T00:23:50.418342: step 24093, loss 0.528734.
Train: 2018-08-02T00:23:50.584897: step 24094, loss 0.579515.
Train: 2018-08-02T00:23:50.747494: step 24095, loss 0.59647.
Train: 2018-08-02T00:23:50.908059: step 24096, loss 0.51172.
Train: 2018-08-02T00:23:51.070629: step 24097, loss 0.52864.
Train: 2018-08-02T00:23:51.235189: step 24098, loss 0.681459.
Train: 2018-08-02T00:23:51.396751: step 24099, loss 0.528609.
Train: 2018-08-02T00:23:51.560290: step 24100, loss 0.545584.
Test: 2018-08-02T00:23:52.096855: step 24100, loss 0.548338.
Train: 2018-08-02T00:23:52.861534: step 24101, loss 0.511605.
Train: 2018-08-02T00:23:53.023105: step 24102, loss 0.579561.
Train: 2018-08-02T00:23:53.184669: step 24103, loss 0.57957.
Train: 2018-08-02T00:23:53.345244: step 24104, loss 0.562555.
Train: 2018-08-02T00:23:53.509798: step 24105, loss 0.494442.
Train: 2018-08-02T00:23:53.670376: step 24106, loss 0.545501.
Train: 2018-08-02T00:23:53.830939: step 24107, loss 0.528404.
Train: 2018-08-02T00:23:54.007442: step 24108, loss 0.579645.
Train: 2018-08-02T00:23:54.177987: step 24109, loss 0.494057.
Train: 2018-08-02T00:23:54.351522: step 24110, loss 0.459606.
Test: 2018-08-02T00:23:54.881106: step 24110, loss 0.548132.
Train: 2018-08-02T00:23:55.044700: step 24111, loss 0.579751.
Train: 2018-08-02T00:23:55.220200: step 24112, loss 0.510787.
Train: 2018-08-02T00:23:55.384786: step 24113, loss 0.527944.
Train: 2018-08-02T00:23:55.546328: step 24114, loss 0.510492.
Train: 2018-08-02T00:23:55.708895: step 24115, loss 0.684439.
Train: 2018-08-02T00:23:55.871486: step 24116, loss 0.580003.
Train: 2018-08-02T00:23:56.027068: step 24117, loss 0.545116.
Train: 2018-08-02T00:23:56.188640: step 24118, loss 0.492681.
Train: 2018-08-02T00:23:56.346217: step 24119, loss 0.615094.
Train: 2018-08-02T00:23:56.506786: step 24120, loss 0.597625.
Test: 2018-08-02T00:23:57.033354: step 24120, loss 0.54791.
Train: 2018-08-02T00:23:57.196945: step 24121, loss 0.720322.
Train: 2018-08-02T00:23:57.359482: step 24122, loss 0.492585.
Train: 2018-08-02T00:23:57.519081: step 24123, loss 0.615033.
Train: 2018-08-02T00:23:57.683616: step 24124, loss 0.492712.
Train: 2018-08-02T00:23:57.844218: step 24125, loss 0.59748.
Train: 2018-08-02T00:23:58.006751: step 24126, loss 0.614892.
Train: 2018-08-02T00:23:58.168320: step 24127, loss 0.510299.
Train: 2018-08-02T00:23:58.327919: step 24128, loss 0.458108.
Train: 2018-08-02T00:23:58.489460: step 24129, loss 0.579972.
Train: 2018-08-02T00:23:58.654025: step 24130, loss 0.440593.
Test: 2018-08-02T00:23:59.181611: step 24130, loss 0.547946.
Train: 2018-08-02T00:23:59.344202: step 24131, loss 0.649822.
Train: 2018-08-02T00:23:59.508736: step 24132, loss 0.614941.
Train: 2018-08-02T00:23:59.671302: step 24133, loss 0.754561.
Train: 2018-08-02T00:23:59.842842: step 24134, loss 0.597369.
Train: 2018-08-02T00:23:59.995435: step 24135, loss 0.545179.
Train: 2018-08-02T00:24:00.163985: step 24136, loss 0.562534.
Train: 2018-08-02T00:24:00.321588: step 24137, loss 0.458826.
Train: 2018-08-02T00:24:00.478171: step 24138, loss 0.597072.
Train: 2018-08-02T00:24:00.648688: step 24139, loss 0.510765.
Train: 2018-08-02T00:24:00.809290: step 24140, loss 0.700508.
Test: 2018-08-02T00:24:01.332860: step 24140, loss 0.548108.
Train: 2018-08-02T00:24:01.494456: step 24141, loss 0.751871.
Train: 2018-08-02T00:24:01.659014: step 24142, loss 0.61395.
Train: 2018-08-02T00:24:01.821581: step 24143, loss 0.664939.
Train: 2018-08-02T00:24:01.983121: step 24144, loss 0.54555.
Train: 2018-08-02T00:24:02.143725: step 24145, loss 0.461103.
Train: 2018-08-02T00:24:02.316240: step 24146, loss 0.545691.
Train: 2018-08-02T00:24:02.479836: step 24147, loss 0.545735.
Train: 2018-08-02T00:24:02.643357: step 24148, loss 0.646552.
Train: 2018-08-02T00:24:02.812903: step 24149, loss 0.579334.
Train: 2018-08-02T00:24:02.978489: step 24150, loss 0.629448.
Test: 2018-08-02T00:24:03.505054: step 24150, loss 0.548655.
Train: 2018-08-02T00:24:03.669637: step 24151, loss 0.579274.
Train: 2018-08-02T00:24:03.828213: step 24152, loss 0.54601.
Train: 2018-08-02T00:24:03.989788: step 24153, loss 0.645546.
Train: 2018-08-02T00:24:04.150329: step 24154, loss 0.46348.
Train: 2018-08-02T00:24:04.366497: step 24155, loss 0.595695.
Train: 2018-08-02T00:24:04.527068: step 24156, loss 0.612142.
Train: 2018-08-02T00:24:04.688663: step 24157, loss 0.529797.
Train: 2018-08-02T00:24:04.853196: step 24158, loss 0.595584.
Train: 2018-08-02T00:24:05.018779: step 24159, loss 0.562726.
Train: 2018-08-02T00:24:05.182317: step 24160, loss 0.546341.
Test: 2018-08-02T00:24:05.704919: step 24160, loss 0.549022.
Train: 2018-08-02T00:24:05.867515: step 24161, loss 0.529973.
Train: 2018-08-02T00:24:06.031080: step 24162, loss 0.579126.
Train: 2018-08-02T00:24:06.190621: step 24163, loss 0.611891.
Train: 2018-08-02T00:24:06.352189: step 24164, loss 0.57912.
Train: 2018-08-02T00:24:06.518775: step 24165, loss 0.562753.
Train: 2018-08-02T00:24:06.680349: step 24166, loss 0.51369.
Train: 2018-08-02T00:24:06.839885: step 24167, loss 0.595477.
Train: 2018-08-02T00:24:07.005443: step 24168, loss 0.546386.
Train: 2018-08-02T00:24:07.166014: step 24169, loss 0.497249.
Train: 2018-08-02T00:24:07.330604: step 24170, loss 0.529845.
Test: 2018-08-02T00:24:07.856168: step 24170, loss 0.548844.
Train: 2018-08-02T00:24:08.014745: step 24171, loss 0.562186.
Train: 2018-08-02T00:24:08.179305: step 24172, loss 0.578937.
Train: 2018-08-02T00:24:08.351844: step 24173, loss 0.580884.
Train: 2018-08-02T00:24:08.515440: step 24174, loss 0.513464.
Train: 2018-08-02T00:24:08.677007: step 24175, loss 0.494228.
Train: 2018-08-02T00:24:08.838573: step 24176, loss 0.510534.
Train: 2018-08-02T00:24:09.002130: step 24177, loss 0.456623.
Train: 2018-08-02T00:24:09.161678: step 24178, loss 0.54092.
Train: 2018-08-02T00:24:09.323246: step 24179, loss 0.622522.
Train: 2018-08-02T00:24:09.483818: step 24180, loss 0.506534.
Test: 2018-08-02T00:24:10.012405: step 24180, loss 0.547788.
Train: 2018-08-02T00:24:10.182977: step 24181, loss 0.508506.
Train: 2018-08-02T00:24:10.346537: step 24182, loss 0.599291.
Train: 2018-08-02T00:24:10.512099: step 24183, loss 0.514347.
Train: 2018-08-02T00:24:10.680617: step 24184, loss 0.628755.
Train: 2018-08-02T00:24:10.840222: step 24185, loss 0.636688.
Train: 2018-08-02T00:24:11.001788: step 24186, loss 0.51429.
Train: 2018-08-02T00:24:11.168343: step 24187, loss 0.60082.
Train: 2018-08-02T00:24:11.330880: step 24188, loss 0.584628.
Train: 2018-08-02T00:24:11.494473: step 24189, loss 0.527535.
Train: 2018-08-02T00:24:11.661031: step 24190, loss 0.597907.
Test: 2018-08-02T00:24:12.194572: step 24190, loss 0.5483.
Train: 2018-08-02T00:24:12.353146: step 24191, loss 0.528457.
Train: 2018-08-02T00:24:12.518729: step 24192, loss 0.511639.
Train: 2018-08-02T00:24:12.691268: step 24193, loss 0.545677.
Train: 2018-08-02T00:24:12.852844: step 24194, loss 0.545612.
Train: 2018-08-02T00:24:13.022383: step 24195, loss 0.495046.
Train: 2018-08-02T00:24:13.189941: step 24196, loss 0.630132.
Train: 2018-08-02T00:24:13.354470: step 24197, loss 0.613329.
Train: 2018-08-02T00:24:13.520061: step 24198, loss 0.562672.
Train: 2018-08-02T00:24:13.690600: step 24199, loss 0.494625.
Train: 2018-08-02T00:24:13.861176: step 24200, loss 0.51173.
Test: 2018-08-02T00:24:14.388706: step 24200, loss 0.548305.
Train: 2018-08-02T00:24:15.155367: step 24201, loss 0.579298.
Train: 2018-08-02T00:24:15.316968: step 24202, loss 0.57954.
Train: 2018-08-02T00:24:15.484512: step 24203, loss 0.494439.
Train: 2018-08-02T00:24:15.654064: step 24204, loss 0.613668.
Train: 2018-08-02T00:24:15.814606: step 24205, loss 0.528279.
Train: 2018-08-02T00:24:15.979190: step 24206, loss 0.59672.
Train: 2018-08-02T00:24:16.150706: step 24207, loss 0.562768.
Train: 2018-08-02T00:24:16.318259: step 24208, loss 0.477014.
Train: 2018-08-02T00:24:16.481854: step 24209, loss 0.631121.
Train: 2018-08-02T00:24:16.646405: step 24210, loss 0.648283.
Test: 2018-08-02T00:24:17.179956: step 24210, loss 0.548167.
Train: 2018-08-02T00:24:17.341548: step 24211, loss 0.476792.
Train: 2018-08-02T00:24:17.500099: step 24212, loss 0.545262.
Train: 2018-08-02T00:24:17.659673: step 24213, loss 0.596799.
Train: 2018-08-02T00:24:17.821240: step 24214, loss 0.562765.
Train: 2018-08-02T00:24:17.981845: step 24215, loss 0.528153.
Train: 2018-08-02T00:24:18.148365: step 24216, loss 0.545441.
Train: 2018-08-02T00:24:18.312925: step 24217, loss 0.614145.
Train: 2018-08-02T00:24:18.473498: step 24218, loss 0.545289.
Train: 2018-08-02T00:24:18.637060: step 24219, loss 0.545176.
Train: 2018-08-02T00:24:18.796664: step 24220, loss 0.493872.
Test: 2018-08-02T00:24:19.327246: step 24220, loss 0.548087.
Train: 2018-08-02T00:24:19.487820: step 24221, loss 0.441885.
Train: 2018-08-02T00:24:19.652346: step 24222, loss 0.545092.
Train: 2018-08-02T00:24:19.811951: step 24223, loss 0.562562.
Train: 2018-08-02T00:24:19.972520: step 24224, loss 0.545067.
Train: 2018-08-02T00:24:20.135086: step 24225, loss 0.510514.
Train: 2018-08-02T00:24:20.299616: step 24226, loss 0.56253.
Train: 2018-08-02T00:24:20.463208: step 24227, loss 0.545193.
Train: 2018-08-02T00:24:20.623775: step 24228, loss 0.510022.
Train: 2018-08-02T00:24:20.782362: step 24229, loss 0.527383.
Train: 2018-08-02T00:24:20.940919: step 24230, loss 0.580289.
Test: 2018-08-02T00:24:21.476484: step 24230, loss 0.547831.
Train: 2018-08-02T00:24:21.642061: step 24231, loss 0.527308.
Train: 2018-08-02T00:24:21.799605: step 24232, loss 0.47416.
Train: 2018-08-02T00:24:21.958199: step 24233, loss 0.580396.
Train: 2018-08-02T00:24:22.128755: step 24234, loss 0.527097.
Train: 2018-08-02T00:24:22.291322: step 24235, loss 0.473548.
Train: 2018-08-02T00:24:22.453886: step 24236, loss 0.65213.
Train: 2018-08-02T00:24:22.616452: step 24237, loss 0.598556.
Train: 2018-08-02T00:24:22.788997: step 24238, loss 0.491031.
Train: 2018-08-02T00:24:22.951525: step 24239, loss 0.419117.
Train: 2018-08-02T00:24:23.110102: step 24240, loss 0.50877.
Test: 2018-08-02T00:24:23.637692: step 24240, loss 0.5477.
Train: 2018-08-02T00:24:23.798296: step 24241, loss 0.580886.
Train: 2018-08-02T00:24:23.959855: step 24242, loss 0.562854.
Train: 2018-08-02T00:24:24.134364: step 24243, loss 0.435897.
Train: 2018-08-02T00:24:24.301965: step 24244, loss 0.581117.
Train: 2018-08-02T00:24:24.475452: step 24245, loss 0.49.
Train: 2018-08-02T00:24:24.637019: step 24246, loss 0.709313.
Train: 2018-08-02T00:24:24.800604: step 24247, loss 0.544705.
Train: 2018-08-02T00:24:24.961187: step 24248, loss 0.544703.
Train: 2018-08-02T00:24:25.135718: step 24249, loss 0.618009.
Train: 2018-08-02T00:24:25.297286: step 24250, loss 0.526378.
Test: 2018-08-02T00:24:25.836812: step 24250, loss 0.54768.
Train: 2018-08-02T00:24:25.996423: step 24251, loss 0.508052.
Train: 2018-08-02T00:24:26.154990: step 24252, loss 0.526366.
Train: 2018-08-02T00:24:26.322549: step 24253, loss 0.673108.
Train: 2018-08-02T00:24:26.487108: step 24254, loss 0.544698.
Train: 2018-08-02T00:24:26.656652: step 24255, loss 0.599651.
Train: 2018-08-02T00:24:26.816221: step 24256, loss 0.544701.
Train: 2018-08-02T00:24:26.976766: step 24257, loss 0.56298.
Train: 2018-08-02T00:24:27.139331: step 24258, loss 0.562963.
Train: 2018-08-02T00:24:27.296941: step 24259, loss 0.708824.
Train: 2018-08-02T00:24:27.455514: step 24260, loss 0.508352.
Test: 2018-08-02T00:24:27.974143: step 24260, loss 0.547679.
Train: 2018-08-02T00:24:28.136699: step 24261, loss 0.508436.
Train: 2018-08-02T00:24:28.299255: step 24262, loss 0.580975.
Train: 2018-08-02T00:24:28.455846: step 24263, loss 0.562833.
Train: 2018-08-02T00:24:28.619374: step 24264, loss 0.490566.
Train: 2018-08-02T00:24:28.782972: step 24265, loss 0.418424.
Train: 2018-08-02T00:24:28.942545: step 24266, loss 0.616993.
Train: 2018-08-02T00:24:29.103080: step 24267, loss 0.45445.
Train: 2018-08-02T00:24:29.271655: step 24268, loss 0.508587.
Train: 2018-08-02T00:24:29.430233: step 24269, loss 0.490427.
Train: 2018-08-02T00:24:29.602746: step 24270, loss 0.599137.
Test: 2018-08-02T00:24:30.126346: step 24270, loss 0.547673.
Train: 2018-08-02T00:24:30.284955: step 24271, loss 0.508405.
Train: 2018-08-02T00:24:30.448485: step 24272, loss 0.526529.
Train: 2018-08-02T00:24:30.616037: step 24273, loss 0.599348.
Train: 2018-08-02T00:24:30.774637: step 24274, loss 0.544704.
Train: 2018-08-02T00:24:30.932200: step 24275, loss 0.489969.
Train: 2018-08-02T00:24:31.094787: step 24276, loss 0.581233.
Train: 2018-08-02T00:24:31.263306: step 24277, loss 0.562978.
Train: 2018-08-02T00:24:31.419918: step 24278, loss 0.617872.
Train: 2018-08-02T00:24:31.586474: step 24279, loss 0.471527.
Train: 2018-08-02T00:24:31.759016: step 24280, loss 0.508084.
Test: 2018-08-02T00:24:32.283583: step 24280, loss 0.547666.
Train: 2018-08-02T00:24:32.443185: step 24281, loss 0.508046.
Train: 2018-08-02T00:24:32.605746: step 24282, loss 0.581372.
Train: 2018-08-02T00:24:32.768311: step 24283, loss 0.636477.
Train: 2018-08-02T00:24:32.931846: step 24284, loss 0.563036.
Train: 2018-08-02T00:24:33.094442: step 24285, loss 0.563029.
Train: 2018-08-02T00:24:33.254010: step 24286, loss 0.471335.
Train: 2018-08-02T00:24:33.411595: step 24287, loss 0.544682.
Train: 2018-08-02T00:24:33.569173: step 24288, loss 0.636424.
Train: 2018-08-02T00:24:33.729742: step 24289, loss 0.728042.
Train: 2018-08-02T00:24:33.888315: step 24290, loss 0.508109.
Test: 2018-08-02T00:24:34.419869: step 24290, loss 0.547661.
Train: 2018-08-02T00:24:34.579441: step 24291, loss 0.562947.
Train: 2018-08-02T00:24:34.749019: step 24292, loss 0.526478.
Train: 2018-08-02T00:24:34.914576: step 24293, loss 0.52651.
Train: 2018-08-02T00:24:35.074119: step 24294, loss 0.581056.
Train: 2018-08-02T00:24:35.230701: step 24295, loss 0.562863.
Train: 2018-08-02T00:24:35.406256: step 24296, loss 0.599098.
Train: 2018-08-02T00:24:35.577801: step 24297, loss 0.599011.
Train: 2018-08-02T00:24:35.744328: step 24298, loss 0.598907.
Train: 2018-08-02T00:24:35.905896: step 24299, loss 0.706862.
Train: 2018-08-02T00:24:36.066491: step 24300, loss 0.508897.
Test: 2018-08-02T00:24:36.602034: step 24300, loss 0.547711.
Train: 2018-08-02T00:24:37.362073: step 24301, loss 0.59846.
Train: 2018-08-02T00:24:37.521617: step 24302, loss 0.491349.
Train: 2018-08-02T00:24:37.687206: step 24303, loss 0.59821.
Train: 2018-08-02T00:24:37.850737: step 24304, loss 0.527133.
Train: 2018-08-02T00:24:38.013301: step 24305, loss 0.5626.
Train: 2018-08-02T00:24:38.174871: step 24306, loss 0.580259.
Train: 2018-08-02T00:24:38.345414: step 24307, loss 0.527296.
Train: 2018-08-02T00:24:38.505020: step 24308, loss 0.6154.
Train: 2018-08-02T00:24:38.667552: step 24309, loss 0.632859.
Train: 2018-08-02T00:24:38.830154: step 24310, loss 0.527476.
Test: 2018-08-02T00:24:39.339755: step 24310, loss 0.547877.
Train: 2018-08-02T00:24:39.555668: step 24311, loss 0.649993.
Train: 2018-08-02T00:24:39.722223: step 24312, loss 0.562513.
Train: 2018-08-02T00:24:39.887781: step 24313, loss 0.632081.
Train: 2018-08-02T00:24:40.048386: step 24314, loss 0.597167.
Train: 2018-08-02T00:24:40.208922: step 24315, loss 0.631594.
Train: 2018-08-02T00:24:40.380488: step 24316, loss 0.631315.
Train: 2018-08-02T00:24:40.540036: step 24317, loss 0.631008.
Train: 2018-08-02T00:24:40.704624: step 24318, loss 0.579533.
Train: 2018-08-02T00:24:40.871152: step 24319, loss 0.613405.
Train: 2018-08-02T00:24:41.033718: step 24320, loss 0.596287.
Test: 2018-08-02T00:24:41.569287: step 24320, loss 0.548448.
Train: 2018-08-02T00:24:41.730854: step 24321, loss 0.629768.
Train: 2018-08-02T00:24:41.896440: step 24322, loss 0.596002.
Train: 2018-08-02T00:24:42.055985: step 24323, loss 0.612511.
Train: 2018-08-02T00:24:42.215558: step 24324, loss 0.496377.
Train: 2018-08-02T00:24:42.379151: step 24325, loss 0.546144.
Train: 2018-08-02T00:24:42.538694: step 24326, loss 0.562669.
Train: 2018-08-02T00:24:42.706277: step 24327, loss 0.595516.
Train: 2018-08-02T00:24:42.877812: step 24328, loss 0.497237.
Train: 2018-08-02T00:24:43.037362: step 24329, loss 0.546381.
Train: 2018-08-02T00:24:43.198929: step 24330, loss 0.660753.
Test: 2018-08-02T00:24:43.728548: step 24330, loss 0.549104.
Train: 2018-08-02T00:24:43.910064: step 24331, loss 0.644281.
Train: 2018-08-02T00:24:44.070630: step 24332, loss 0.4327.
Train: 2018-08-02T00:24:44.233164: step 24333, loss 0.546539.
Train: 2018-08-02T00:24:44.394763: step 24334, loss 0.481532.
Train: 2018-08-02T00:24:44.555304: step 24335, loss 0.530235.
Train: 2018-08-02T00:24:44.725848: step 24336, loss 0.53016.
Train: 2018-08-02T00:24:44.888413: step 24337, loss 0.579071.
Train: 2018-08-02T00:24:45.050012: step 24338, loss 0.628173.
Train: 2018-08-02T00:24:45.211558: step 24339, loss 0.497195.
Train: 2018-08-02T00:24:45.374145: step 24340, loss 0.529875.
Test: 2018-08-02T00:24:45.904696: step 24340, loss 0.5489.
Train: 2018-08-02T00:24:46.065298: step 24341, loss 0.595558.
Train: 2018-08-02T00:24:46.235811: step 24342, loss 0.562655.
Train: 2018-08-02T00:24:46.397410: step 24343, loss 0.612135.
Train: 2018-08-02T00:24:46.560942: step 24344, loss 0.562633.
Train: 2018-08-02T00:24:46.723535: step 24345, loss 0.645265.
Train: 2018-08-02T00:24:46.887071: step 24346, loss 0.463474.
Train: 2018-08-02T00:24:47.046643: step 24347, loss 0.628802.
Train: 2018-08-02T00:24:47.217218: step 24348, loss 0.579165.
Train: 2018-08-02T00:24:47.377758: step 24349, loss 0.562611.
Train: 2018-08-02T00:24:47.540325: step 24350, loss 0.546047.
Test: 2018-08-02T00:24:48.073898: step 24350, loss 0.548727.
Train: 2018-08-02T00:24:48.231475: step 24351, loss 0.463177.
Train: 2018-08-02T00:24:48.392078: step 24352, loss 0.545986.
Train: 2018-08-02T00:24:48.552617: step 24353, loss 0.545937.
Train: 2018-08-02T00:24:48.730150: step 24354, loss 0.545887.
Train: 2018-08-02T00:24:48.892708: step 24355, loss 0.495705.
Train: 2018-08-02T00:24:49.059264: step 24356, loss 0.713367.
Train: 2018-08-02T00:24:49.219834: step 24357, loss 0.596071.
Train: 2018-08-02T00:24:49.378409: step 24358, loss 0.612858.
Train: 2018-08-02T00:24:49.548954: step 24359, loss 0.545753.
Train: 2018-08-02T00:24:49.713514: step 24360, loss 0.629617.
Test: 2018-08-02T00:24:50.234123: step 24360, loss 0.548495.
Train: 2018-08-02T00:24:50.394724: step 24361, loss 0.713367.
Train: 2018-08-02T00:24:50.556295: step 24362, loss 0.629409.
Train: 2018-08-02T00:24:50.719825: step 24363, loss 0.495904.
Train: 2018-08-02T00:24:50.894358: step 24364, loss 0.629107.
Train: 2018-08-02T00:24:51.055926: step 24365, loss 0.579183.
Train: 2018-08-02T00:24:51.217493: step 24366, loss 0.645375.
Train: 2018-08-02T00:24:51.380059: step 24367, loss 0.562633.
Train: 2018-08-02T00:24:51.544618: step 24368, loss 0.52974.
Train: 2018-08-02T00:24:51.705190: step 24369, loss 0.677661.
Train: 2018-08-02T00:24:51.866792: step 24370, loss 0.595454.
Test: 2018-08-02T00:24:52.394359: step 24370, loss 0.549063.
Train: 2018-08-02T00:24:52.554945: step 24371, loss 0.513758.
Train: 2018-08-02T00:24:52.730480: step 24372, loss 0.448707.
Train: 2018-08-02T00:24:52.896039: step 24373, loss 0.513883.
Train: 2018-08-02T00:24:53.067549: step 24374, loss 0.644262.
Train: 2018-08-02T00:24:53.228118: step 24375, loss 0.481262.
Train: 2018-08-02T00:24:53.389686: step 24376, loss 0.56274.
Train: 2018-08-02T00:24:53.550257: step 24377, loss 0.59539.
Train: 2018-08-02T00:24:53.719804: step 24378, loss 0.513701.
Train: 2018-08-02T00:24:53.880376: step 24379, loss 0.546346.
Train: 2018-08-02T00:24:54.042940: step 24380, loss 0.595469.
Test: 2018-08-02T00:24:54.563574: step 24380, loss 0.548943.
Train: 2018-08-02T00:24:54.721128: step 24381, loss 0.527681.
Train: 2018-08-02T00:24:54.889677: step 24382, loss 0.529801.
Train: 2018-08-02T00:24:55.061219: step 24383, loss 0.595579.
Train: 2018-08-02T00:24:55.223785: step 24384, loss 0.612105.
Train: 2018-08-02T00:24:55.382391: step 24385, loss 0.562628.
Train: 2018-08-02T00:24:55.559886: step 24386, loss 0.496557.
Train: 2018-08-02T00:24:55.722450: step 24387, loss 0.529518.
Train: 2018-08-02T00:24:55.892997: step 24388, loss 0.595749.
Train: 2018-08-02T00:24:56.053595: step 24389, loss 0.545976.
Train: 2018-08-02T00:24:56.221117: step 24390, loss 0.595832.
Test: 2018-08-02T00:24:56.747739: step 24390, loss 0.548616.
Train: 2018-08-02T00:24:56.910275: step 24391, loss 0.529257.
Train: 2018-08-02T00:24:57.067885: step 24392, loss 0.545872.
Train: 2018-08-02T00:24:57.236430: step 24393, loss 0.612659.
Train: 2018-08-02T00:24:57.398997: step 24394, loss 0.562533.
Train: 2018-08-02T00:24:57.559570: step 24395, loss 0.462099.
Train: 2018-08-02T00:24:57.719144: step 24396, loss 0.461866.
Train: 2018-08-02T00:24:57.877718: step 24397, loss 0.612994.
Train: 2018-08-02T00:24:58.039257: step 24398, loss 0.44439.
Train: 2018-08-02T00:24:58.197861: step 24399, loss 0.579413.
Train: 2018-08-02T00:24:58.357434: step 24400, loss 0.545483.
Test: 2018-08-02T00:24:58.882005: step 24400, loss 0.548196.
Train: 2018-08-02T00:24:59.628332: step 24401, loss 0.545426.
Train: 2018-08-02T00:24:59.793895: step 24402, loss 0.494102.
Train: 2018-08-02T00:24:59.967400: step 24403, loss 0.493859.
Train: 2018-08-02T00:25:00.129991: step 24404, loss 0.579679.
Train: 2018-08-02T00:25:00.307491: step 24405, loss 0.52791.
Train: 2018-08-02T00:25:00.465070: step 24406, loss 0.545135.
Train: 2018-08-02T00:25:00.631625: step 24407, loss 0.597264.
Train: 2018-08-02T00:25:00.798180: step 24408, loss 0.527617.
Train: 2018-08-02T00:25:00.962740: step 24409, loss 0.579978.
Train: 2018-08-02T00:25:01.127300: step 24410, loss 0.544991.
Test: 2018-08-02T00:25:01.664863: step 24410, loss 0.54782.
Train: 2018-08-02T00:25:01.827461: step 24411, loss 0.59762.
Train: 2018-08-02T00:25:01.990024: step 24412, loss 0.615249.
Train: 2018-08-02T00:25:02.154579: step 24413, loss 0.650445.
Train: 2018-08-02T00:25:02.317150: step 24414, loss 0.54495.
Train: 2018-08-02T00:25:02.486678: step 24415, loss 0.580085.
Train: 2018-08-02T00:25:02.652223: step 24416, loss 0.597623.
Train: 2018-08-02T00:25:02.813822: step 24417, loss 0.59758.
Train: 2018-08-02T00:25:02.979375: step 24418, loss 0.527485.
Train: 2018-08-02T00:25:03.144932: step 24419, loss 0.562499.
Train: 2018-08-02T00:25:03.314453: step 24420, loss 0.545018.
Test: 2018-08-02T00:25:03.853014: step 24420, loss 0.547867.
Train: 2018-08-02T00:25:04.015611: step 24421, loss 0.475164.
Train: 2018-08-02T00:25:04.182159: step 24422, loss 0.649856.
Train: 2018-08-02T00:25:04.344699: step 24423, loss 0.57995.
Train: 2018-08-02T00:25:04.519232: step 24424, loss 0.527599.
Train: 2018-08-02T00:25:04.682826: step 24425, loss 0.527613.
Train: 2018-08-02T00:25:04.845376: step 24426, loss 0.61479.
Train: 2018-08-02T00:25:05.007949: step 24427, loss 0.492789.
Train: 2018-08-02T00:25:05.172521: step 24428, loss 0.579908.
Train: 2018-08-02T00:25:05.335085: step 24429, loss 0.579906.
Train: 2018-08-02T00:25:05.500609: step 24430, loss 0.510226.
Test: 2018-08-02T00:25:06.044157: step 24430, loss 0.547891.
Train: 2018-08-02T00:25:06.204752: step 24431, loss 0.562481.
Train: 2018-08-02T00:25:06.369288: step 24432, loss 0.597334.
Train: 2018-08-02T00:25:06.528891: step 24433, loss 0.579902.
Train: 2018-08-02T00:25:06.691456: step 24434, loss 0.614716.
Train: 2018-08-02T00:25:06.850999: step 24435, loss 0.614654.
Train: 2018-08-02T00:25:07.017580: step 24436, loss 0.545105.
Train: 2018-08-02T00:25:07.179122: step 24437, loss 0.666511.
Train: 2018-08-02T00:25:07.344679: step 24438, loss 0.510567.
Train: 2018-08-02T00:25:07.523233: step 24439, loss 0.545188.
Train: 2018-08-02T00:25:07.695740: step 24440, loss 0.59695.
Test: 2018-08-02T00:25:08.232316: step 24440, loss 0.548034.
Train: 2018-08-02T00:25:08.395918: step 24441, loss 0.459135.
Train: 2018-08-02T00:25:08.569404: step 24442, loss 0.510803.
Train: 2018-08-02T00:25:08.732968: step 24443, loss 0.665801.
Train: 2018-08-02T00:25:08.897558: step 24444, loss 0.648501.
Train: 2018-08-02T00:25:09.078045: step 24445, loss 0.528095.
Train: 2018-08-02T00:25:09.239643: step 24446, loss 0.545294.
Train: 2018-08-02T00:25:09.406194: step 24447, loss 0.699581.
Train: 2018-08-02T00:25:09.571750: step 24448, loss 0.562452.
Train: 2018-08-02T00:25:09.732334: step 24449, loss 0.733053.
Train: 2018-08-02T00:25:09.908824: step 24450, loss 0.511493.
Test: 2018-08-02T00:25:10.438409: step 24450, loss 0.548286.
Train: 2018-08-02T00:25:10.599977: step 24451, loss 0.528594.
Train: 2018-08-02T00:25:10.770521: step 24452, loss 0.528679.
Train: 2018-08-02T00:25:10.934084: step 24453, loss 0.54561.
Train: 2018-08-02T00:25:11.100639: step 24454, loss 0.528783.
Train: 2018-08-02T00:25:11.263238: step 24455, loss 0.596171.
Train: 2018-08-02T00:25:11.429759: step 24456, loss 0.579317.
Train: 2018-08-02T00:25:11.593352: step 24457, loss 0.444829.
Train: 2018-08-02T00:25:11.759902: step 24458, loss 0.562492.
Train: 2018-08-02T00:25:11.919450: step 24459, loss 0.49516.
Train: 2018-08-02T00:25:12.087999: step 24460, loss 0.495052.
Test: 2018-08-02T00:25:12.627557: step 24460, loss 0.548326.
Train: 2018-08-02T00:25:12.789125: step 24461, loss 0.545579.
Train: 2018-08-02T00:25:12.949696: step 24462, loss 0.528598.
Train: 2018-08-02T00:25:13.115261: step 24463, loss 0.545483.
Train: 2018-08-02T00:25:13.280837: step 24464, loss 0.494374.
Train: 2018-08-02T00:25:13.443375: step 24465, loss 0.630745.
Train: 2018-08-02T00:25:13.613946: step 24466, loss 0.579556.
Train: 2018-08-02T00:25:13.779511: step 24467, loss 0.579582.
Train: 2018-08-02T00:25:13.940074: step 24468, loss 0.613911.
Train: 2018-08-02T00:25:14.100626: step 24469, loss 0.51096.
Train: 2018-08-02T00:25:14.269168: step 24470, loss 0.528087.
Test: 2018-08-02T00:25:14.804736: step 24470, loss 0.548042.
Train: 2018-08-02T00:25:14.968325: step 24471, loss 0.596854.
Train: 2018-08-02T00:25:15.140839: step 24472, loss 0.614098.
Train: 2018-08-02T00:25:15.305423: step 24473, loss 0.562448.
Train: 2018-08-02T00:25:15.466967: step 24474, loss 0.441918.
Train: 2018-08-02T00:25:15.632554: step 24475, loss 0.52796.
Train: 2018-08-02T00:25:15.805061: step 24476, loss 0.562452.
Train: 2018-08-02T00:25:15.970646: step 24477, loss 0.562454.
Train: 2018-08-02T00:25:16.133186: step 24478, loss 0.701063.
Train: 2018-08-02T00:25:16.304727: step 24479, loss 0.52782.
Train: 2018-08-02T00:25:16.468289: step 24480, loss 0.527821.
Test: 2018-08-02T00:25:16.995878: step 24480, loss 0.547949.
Train: 2018-08-02T00:25:17.158470: step 24481, loss 0.631752.
Train: 2018-08-02T00:25:17.320037: step 24482, loss 0.527825.
Train: 2018-08-02T00:25:17.487565: step 24483, loss 0.579768.
Train: 2018-08-02T00:25:17.657122: step 24484, loss 0.475914.
Train: 2018-08-02T00:25:17.816711: step 24485, loss 0.735664.
Train: 2018-08-02T00:25:17.983240: step 24486, loss 0.527861.
Train: 2018-08-02T00:25:18.150792: step 24487, loss 0.631571.
Train: 2018-08-02T00:25:18.322333: step 24488, loss 0.614202.
Train: 2018-08-02T00:25:18.489886: step 24489, loss 0.528017.
Train: 2018-08-02T00:25:18.653479: step 24490, loss 0.545257.
Test: 2018-08-02T00:25:19.177048: step 24490, loss 0.548068.
Train: 2018-08-02T00:25:19.350584: step 24491, loss 0.66545.
Train: 2018-08-02T00:25:19.510159: step 24492, loss 0.562444.
Train: 2018-08-02T00:25:19.687683: step 24493, loss 0.579538.
Train: 2018-08-02T00:25:19.852270: step 24494, loss 0.630673.
Train: 2018-08-02T00:25:20.020793: step 24495, loss 0.545442.
Train: 2018-08-02T00:25:20.185379: step 24496, loss 0.681264.
Train: 2018-08-02T00:25:20.340937: step 24497, loss 0.49481.
Train: 2018-08-02T00:25:20.500535: step 24498, loss 0.596226.
Train: 2018-08-02T00:25:20.670057: step 24499, loss 0.478303.
Train: 2018-08-02T00:25:20.834618: step 24500, loss 0.52885.
Test: 2018-08-02T00:25:21.362208: step 24500, loss 0.548411.
Train: 2018-08-02T00:25:22.144829: step 24501, loss 0.629734.
Train: 2018-08-02T00:25:22.308392: step 24502, loss 0.562494.
Train: 2018-08-02T00:25:22.467990: step 24503, loss 0.629595.
Train: 2018-08-02T00:25:22.631528: step 24504, loss 0.595998.
Train: 2018-08-02T00:25:22.808089: step 24505, loss 0.662797.
Train: 2018-08-02T00:25:22.971619: step 24506, loss 0.512544.
Train: 2018-08-02T00:25:23.145155: step 24507, loss 0.512653.
Train: 2018-08-02T00:25:23.311734: step 24508, loss 0.57917.
Train: 2018-08-02T00:25:23.472313: step 24509, loss 0.462972.
Train: 2018-08-02T00:25:23.639866: step 24510, loss 0.529346.
Test: 2018-08-02T00:25:24.177404: step 24510, loss 0.548633.
Train: 2018-08-02T00:25:24.340959: step 24511, loss 0.529307.
Train: 2018-08-02T00:25:24.499534: step 24512, loss 0.529252.
Train: 2018-08-02T00:25:24.672101: step 24513, loss 0.479158.
Train: 2018-08-02T00:25:24.837630: step 24514, loss 0.495633.
Train: 2018-08-02T00:25:25.006210: step 24515, loss 0.596051.
Train: 2018-08-02T00:25:25.168745: step 24516, loss 0.562484.
Train: 2018-08-02T00:25:25.336296: step 24517, loss 0.478155.
Train: 2018-08-02T00:25:25.499886: step 24518, loss 0.562462.
Train: 2018-08-02T00:25:25.659448: step 24519, loss 0.51154.
Train: 2018-08-02T00:25:25.824991: step 24520, loss 0.630561.
Test: 2018-08-02T00:25:26.352605: step 24520, loss 0.548152.
Train: 2018-08-02T00:25:26.516173: step 24521, loss 0.494178.
Train: 2018-08-02T00:25:26.678739: step 24522, loss 0.6309.
Train: 2018-08-02T00:25:26.842296: step 24523, loss 0.56244.
Train: 2018-08-02T00:25:27.013820: step 24524, loss 0.545269.
Train: 2018-08-02T00:25:27.181395: step 24525, loss 0.64843.
Train: 2018-08-02T00:25:27.347945: step 24526, loss 0.545236.
Train: 2018-08-02T00:25:27.509487: step 24527, loss 0.528013.
Train: 2018-08-02T00:25:27.673076: step 24528, loss 0.751964.
Train: 2018-08-02T00:25:27.843620: step 24529, loss 0.493626.
Train: 2018-08-02T00:25:28.011147: step 24530, loss 0.56244.
Test: 2018-08-02T00:25:28.540761: step 24530, loss 0.548048.
Train: 2018-08-02T00:25:28.704294: step 24531, loss 0.528067.
Train: 2018-08-02T00:25:28.869852: step 24532, loss 0.528068.
Train: 2018-08-02T00:25:29.030453: step 24533, loss 0.579632.
Train: 2018-08-02T00:25:29.198971: step 24534, loss 0.596829.
Train: 2018-08-02T00:25:29.368551: step 24535, loss 0.528059.
Train: 2018-08-02T00:25:29.534109: step 24536, loss 0.682786.
Train: 2018-08-02T00:25:29.698635: step 24537, loss 0.596777.
Train: 2018-08-02T00:25:29.866187: step 24538, loss 0.511014.
Train: 2018-08-02T00:25:30.028752: step 24539, loss 0.528184.
Train: 2018-08-02T00:25:30.189355: step 24540, loss 0.613803.
Test: 2018-08-02T00:25:30.720902: step 24540, loss 0.548115.
Train: 2018-08-02T00:25:30.882504: step 24541, loss 0.59665.
Train: 2018-08-02T00:25:31.061024: step 24542, loss 0.56244.
Train: 2018-08-02T00:25:31.221564: step 24543, loss 0.630703.
Train: 2018-08-02T00:25:31.382137: step 24544, loss 0.613546.
Train: 2018-08-02T00:25:31.545697: step 24545, loss 0.647426.
Train: 2018-08-02T00:25:31.712252: step 24546, loss 0.494681.
Train: 2018-08-02T00:25:31.872854: step 24547, loss 0.613194.
Train: 2018-08-02T00:25:32.045387: step 24548, loss 0.545596.
Train: 2018-08-02T00:25:32.207927: step 24549, loss 0.596159.
Train: 2018-08-02T00:25:32.372487: step 24550, loss 0.562484.
Test: 2018-08-02T00:25:32.904066: step 24550, loss 0.548442.
Train: 2018-08-02T00:25:33.069648: step 24551, loss 0.478601.
Train: 2018-08-02T00:25:33.234188: step 24552, loss 0.629577.
Train: 2018-08-02T00:25:33.404729: step 24553, loss 0.579251.
Train: 2018-08-02T00:25:33.570286: step 24554, loss 0.512317.
Train: 2018-08-02T00:25:33.734876: step 24555, loss 0.562509.
Train: 2018-08-02T00:25:33.897410: step 24556, loss 0.612666.
Train: 2018-08-02T00:25:34.073973: step 24557, loss 0.462285.
Train: 2018-08-02T00:25:34.237527: step 24558, loss 0.612657.
Train: 2018-08-02T00:25:34.409069: step 24559, loss 0.662802.
Train: 2018-08-02T00:25:34.570645: step 24560, loss 0.579213.
Test: 2018-08-02T00:25:35.096220: step 24560, loss 0.548563.
Train: 2018-08-02T00:25:35.259797: step 24561, loss 0.562526.
Train: 2018-08-02T00:25:35.421337: step 24562, loss 0.629158.
Train: 2018-08-02T00:25:35.581908: step 24563, loss 0.479404.
Train: 2018-08-02T00:25:35.753480: step 24564, loss 0.595789.
Train: 2018-08-02T00:25:35.917011: step 24565, loss 0.545941.
Train: 2018-08-02T00:25:36.079577: step 24566, loss 0.662183.
Train: 2018-08-02T00:25:36.239150: step 24567, loss 0.562563.
Train: 2018-08-02T00:25:36.400747: step 24568, loss 0.562573.
Train: 2018-08-02T00:25:36.564312: step 24569, loss 0.463323.
Train: 2018-08-02T00:25:36.729870: step 24570, loss 0.46327.
Test: 2018-08-02T00:25:37.258457: step 24570, loss 0.548677.
Train: 2018-08-02T00:25:37.421989: step 24571, loss 0.545981.
Train: 2018-08-02T00:25:37.580565: step 24572, loss 0.612391.
Train: 2018-08-02T00:25:37.745157: step 24573, loss 0.512636.
Train: 2018-08-02T00:25:37.909686: step 24574, loss 0.512532.
Train: 2018-08-02T00:25:38.079258: step 24575, loss 0.629332.
Train: 2018-08-02T00:25:38.256811: step 24576, loss 0.579232.
Train: 2018-08-02T00:25:38.422346: step 24577, loss 0.495519.
Train: 2018-08-02T00:25:38.592885: step 24578, loss 0.512159.
Train: 2018-08-02T00:25:38.755425: step 24579, loss 0.562478.
Train: 2018-08-02T00:25:38.918987: step 24580, loss 0.579323.
Test: 2018-08-02T00:25:39.442592: step 24580, loss 0.548323.
Train: 2018-08-02T00:25:39.613132: step 24581, loss 0.494922.
Train: 2018-08-02T00:25:39.791683: step 24582, loss 0.562454.
Train: 2018-08-02T00:25:39.961201: step 24583, loss 0.630316.
Train: 2018-08-02T00:25:40.128784: step 24584, loss 0.579433.
Train: 2018-08-02T00:25:40.299321: step 24585, loss 0.596452.
Train: 2018-08-02T00:25:40.462884: step 24586, loss 0.596466.
Train: 2018-08-02T00:25:40.629448: step 24587, loss 0.596466.
Train: 2018-08-02T00:25:40.793974: step 24588, loss 0.647471.
Train: 2018-08-02T00:25:40.956541: step 24589, loss 0.579427.
Train: 2018-08-02T00:25:41.121152: step 24590, loss 0.426785.
Test: 2018-08-02T00:25:41.662652: step 24590, loss 0.54824.
Train: 2018-08-02T00:25:41.830205: step 24591, loss 0.596377.
Train: 2018-08-02T00:25:41.991772: step 24592, loss 0.562446.
Train: 2018-08-02T00:25:42.156333: step 24593, loss 0.579412.
Train: 2018-08-02T00:25:42.317934: step 24594, loss 0.52852.
Train: 2018-08-02T00:25:42.481494: step 24595, loss 0.494571.
Train: 2018-08-02T00:25:42.644029: step 24596, loss 0.647386.
Train: 2018-08-02T00:25:42.818596: step 24597, loss 0.630398.
Train: 2018-08-02T00:25:42.982151: step 24598, loss 0.630345.
Train: 2018-08-02T00:25:43.147683: step 24599, loss 0.714989.
Train: 2018-08-02T00:25:43.311272: step 24600, loss 0.596245.
Test: 2018-08-02T00:25:43.848821: step 24600, loss 0.548372.
Train: 2018-08-02T00:25:44.595500: step 24601, loss 0.478285.
Train: 2018-08-02T00:25:44.759036: step 24602, loss 0.512063.
Train: 2018-08-02T00:25:44.931601: step 24603, loss 0.528907.
Train: 2018-08-02T00:25:45.096136: step 24604, loss 0.562486.
Train: 2018-08-02T00:25:45.258735: step 24605, loss 0.59604.
Train: 2018-08-02T00:25:45.421268: step 24606, loss 0.679849.
Train: 2018-08-02T00:25:45.583867: step 24607, loss 0.612696.
Train: 2018-08-02T00:25:45.749400: step 24608, loss 0.562514.
Train: 2018-08-02T00:25:45.910957: step 24609, loss 0.629155.
Train: 2018-08-02T00:25:46.078511: step 24610, loss 0.562545.
Test: 2018-08-02T00:25:46.607097: step 24610, loss 0.548683.
Train: 2018-08-02T00:25:46.771658: step 24611, loss 0.662007.
Train: 2018-08-02T00:25:46.936218: step 24612, loss 0.612147.
Train: 2018-08-02T00:25:47.097797: step 24613, loss 0.513233.
Train: 2018-08-02T00:25:47.261347: step 24614, loss 0.64475.
Train: 2018-08-02T00:25:47.424945: step 24615, loss 0.480817.
Train: 2018-08-02T00:25:47.594457: step 24616, loss 0.562686.
Train: 2018-08-02T00:25:47.756025: step 24617, loss 0.579023.
Train: 2018-08-02T00:25:47.914603: step 24618, loss 0.497501.
Train: 2018-08-02T00:25:48.078164: step 24619, loss 0.51381.
Train: 2018-08-02T00:25:48.241759: step 24620, loss 0.464827.
Test: 2018-08-02T00:25:48.772309: step 24620, loss 0.548994.
Train: 2018-08-02T00:25:48.936902: step 24621, loss 0.513641.
Train: 2018-08-02T00:25:49.101448: step 24622, loss 0.562656.
Train: 2018-08-02T00:25:49.267015: step 24623, loss 0.546199.
Train: 2018-08-02T00:25:49.441545: step 24624, loss 0.595562.
Train: 2018-08-02T00:25:49.602090: step 24625, loss 0.628633.
Train: 2018-08-02T00:25:49.768646: step 24626, loss 0.645223.
Train: 2018-08-02T00:25:49.932208: step 24627, loss 0.529526.
Train: 2018-08-02T00:25:50.105769: step 24628, loss 0.529504.
Train: 2018-08-02T00:25:50.280306: step 24629, loss 0.479802.
Train: 2018-08-02T00:25:50.450851: step 24630, loss 0.512789.
Test: 2018-08-02T00:25:50.973426: step 24630, loss 0.548608.
Train: 2018-08-02T00:25:51.143969: step 24631, loss 0.662327.
Train: 2018-08-02T00:25:51.310524: step 24632, loss 0.545876.
Train: 2018-08-02T00:25:51.480070: step 24633, loss 0.645885.
Train: 2018-08-02T00:25:51.647622: step 24634, loss 0.512488.
Train: 2018-08-02T00:25:51.809191: step 24635, loss 0.529128.
Train: 2018-08-02T00:25:51.972753: step 24636, loss 0.512362.
Train: 2018-08-02T00:25:52.134356: step 24637, loss 0.512256.
Train: 2018-08-02T00:25:52.296887: step 24638, loss 0.545694.
Train: 2018-08-02T00:25:52.458454: step 24639, loss 0.680265.
Train: 2018-08-02T00:25:52.625020: step 24640, loss 0.612989.
Test: 2018-08-02T00:25:53.149608: step 24640, loss 0.548363.
Train: 2018-08-02T00:25:53.315178: step 24641, loss 0.596151.
Train: 2018-08-02T00:25:53.479725: step 24642, loss 0.54563.
Train: 2018-08-02T00:25:53.646280: step 24643, loss 0.495116.
Train: 2018-08-02T00:25:53.809843: step 24644, loss 0.427638.
Train: 2018-08-02T00:25:53.974402: step 24645, loss 0.57935.
Train: 2018-08-02T00:25:54.138963: step 24646, loss 0.613245.
Train: 2018-08-02T00:25:54.306518: step 24647, loss 0.562443.
Train: 2018-08-02T00:25:54.471075: step 24648, loss 0.579416.
Train: 2018-08-02T00:25:54.635634: step 24649, loss 0.511464.
Train: 2018-08-02T00:25:54.799197: step 24650, loss 0.460335.
Test: 2018-08-02T00:25:55.322798: step 24650, loss 0.548144.
Train: 2018-08-02T00:25:55.502322: step 24651, loss 0.528308.
Train: 2018-08-02T00:25:55.679844: step 24652, loss 0.52821.
Train: 2018-08-02T00:25:55.850412: step 24653, loss 0.49379.
Train: 2018-08-02T00:25:56.022926: step 24654, loss 0.648522.
Train: 2018-08-02T00:25:56.201449: step 24655, loss 0.493414.
Train: 2018-08-02T00:25:56.366064: step 24656, loss 0.527835.
Train: 2018-08-02T00:25:56.529572: step 24657, loss 0.406306.
Train: 2018-08-02T00:25:56.710089: step 24658, loss 0.5973.
Train: 2018-08-02T00:25:56.871658: step 24659, loss 0.649881.
Train: 2018-08-02T00:25:57.046190: step 24660, loss 0.562476.
Test: 2018-08-02T00:25:57.585767: step 24660, loss 0.547791.
Train: 2018-08-02T00:25:57.751306: step 24661, loss 0.544937.
Train: 2018-08-02T00:25:57.917891: step 24662, loss 0.650383.
Train: 2018-08-02T00:25:58.081424: step 24663, loss 0.50974.
Train: 2018-08-02T00:25:58.245984: step 24664, loss 0.5449.
Train: 2018-08-02T00:25:58.421514: step 24665, loss 0.50965.
Train: 2018-08-02T00:25:58.587071: step 24666, loss 0.580161.
Train: 2018-08-02T00:25:58.751632: step 24667, loss 0.562524.
Train: 2018-08-02T00:25:58.918217: step 24668, loss 0.580206.
Train: 2018-08-02T00:25:59.080787: step 24669, loss 0.491794.
Train: 2018-08-02T00:25:59.246310: step 24670, loss 0.544836.
Test: 2018-08-02T00:25:59.773902: step 24670, loss 0.547706.
Train: 2018-08-02T00:25:59.959403: step 24671, loss 0.580274.
Train: 2018-08-02T00:26:00.127952: step 24672, loss 0.527078.
Train: 2018-08-02T00:26:00.304506: step 24673, loss 0.562562.
Train: 2018-08-02T00:26:00.464053: step 24674, loss 0.491491.
Train: 2018-08-02T00:26:00.629643: step 24675, loss 0.544786.
Train: 2018-08-02T00:26:00.797163: step 24676, loss 0.455676.
Train: 2018-08-02T00:26:00.972726: step 24677, loss 0.544753.
Train: 2018-08-02T00:26:01.155235: step 24678, loss 0.598438.
Train: 2018-08-02T00:26:01.322759: step 24679, loss 0.473016.
Train: 2018-08-02T00:26:01.493303: step 24680, loss 0.436909.
Test: 2018-08-02T00:26:02.017925: step 24680, loss 0.547618.
Train: 2018-08-02T00:26:02.183482: step 24681, loss 0.598762.
Train: 2018-08-02T00:26:02.354020: step 24682, loss 0.524196.
Train: 2018-08-02T00:26:02.524570: step 24683, loss 0.598996.
Train: 2018-08-02T00:26:02.688109: step 24684, loss 0.490227.
Train: 2018-08-02T00:26:02.861675: step 24685, loss 0.526462.
Train: 2018-08-02T00:26:03.027202: step 24686, loss 0.544634.
Train: 2018-08-02T00:26:03.198744: step 24687, loss 0.508117.
Train: 2018-08-02T00:26:03.375305: step 24688, loss 0.526326.
Train: 2018-08-02T00:26:03.543821: step 24689, loss 0.52628.
Train: 2018-08-02T00:26:03.706417: step 24690, loss 0.562988.
Test: 2018-08-02T00:26:04.251928: step 24690, loss 0.547603.
Train: 2018-08-02T00:26:04.436461: step 24691, loss 0.54461.
Train: 2018-08-02T00:26:04.624074: step 24692, loss 0.507731.
Train: 2018-08-02T00:26:04.795621: step 24693, loss 0.507662.
Train: 2018-08-02T00:26:04.962181: step 24694, loss 0.544608.
Train: 2018-08-02T00:26:05.124741: step 24695, loss 0.655879.
Train: 2018-08-02T00:26:05.289302: step 24696, loss 0.470393.
Train: 2018-08-02T00:26:05.452834: step 24697, loss 0.581763.
Train: 2018-08-02T00:26:05.614402: step 24698, loss 0.656145.
Train: 2018-08-02T00:26:05.779989: step 24699, loss 0.507453.
Train: 2018-08-02T00:26:05.940555: step 24700, loss 0.507458.
Test: 2018-08-02T00:26:06.477095: step 24700, loss 0.547632.
Train: 2018-08-02T00:26:07.299509: step 24701, loss 0.618932.
Train: 2018-08-02T00:26:07.464044: step 24702, loss 0.61889.
Train: 2018-08-02T00:26:07.627605: step 24703, loss 0.600246.
Train: 2018-08-02T00:26:07.793192: step 24704, loss 0.544607.
Train: 2018-08-02T00:26:07.958752: step 24705, loss 0.618543.
Train: 2018-08-02T00:26:08.126298: step 24706, loss 0.56305.
Train: 2018-08-02T00:26:08.291862: step 24707, loss 0.581415.
Train: 2018-08-02T00:26:08.457413: step 24708, loss 0.599691.
Train: 2018-08-02T00:26:08.629927: step 24709, loss 0.581238.
Train: 2018-08-02T00:26:08.799475: step 24710, loss 0.690694.
Test: 2018-08-02T00:26:09.333057: step 24710, loss 0.547597.
Train: 2018-08-02T00:26:09.496611: step 24711, loss 0.599193.
Train: 2018-08-02T00:26:09.663190: step 24712, loss 0.490337.
Train: 2018-08-02T00:26:09.829744: step 24713, loss 0.508576.
Train: 2018-08-02T00:26:09.996299: step 24714, loss 0.544691.
Train: 2018-08-02T00:26:10.160865: step 24715, loss 0.616581.
Train: 2018-08-02T00:26:10.327414: step 24716, loss 0.59849.
Train: 2018-08-02T00:26:10.496935: step 24717, loss 0.562617.
Train: 2018-08-02T00:26:10.663491: step 24718, loss 0.562591.
Train: 2018-08-02T00:26:10.825059: step 24719, loss 0.491463.
Train: 2018-08-02T00:26:10.992611: step 24720, loss 0.527059.
Test: 2018-08-02T00:26:11.528180: step 24720, loss 0.547701.
Train: 2018-08-02T00:26:11.692765: step 24721, loss 0.544818.
Train: 2018-08-02T00:26:11.863316: step 24722, loss 0.633392.
Train: 2018-08-02T00:26:12.028871: step 24723, loss 0.562528.
Train: 2018-08-02T00:26:12.195427: step 24724, loss 0.509554.
Train: 2018-08-02T00:26:12.356964: step 24725, loss 0.474321.
Train: 2018-08-02T00:26:12.523538: step 24726, loss 0.562511.
Train: 2018-08-02T00:26:12.691101: step 24727, loss 0.52723.
Train: 2018-08-02T00:26:12.857651: step 24728, loss 0.544866.
Train: 2018-08-02T00:26:13.025208: step 24729, loss 0.491896.
Train: 2018-08-02T00:26:13.193726: step 24730, loss 0.633226.
Test: 2018-08-02T00:26:13.727319: step 24730, loss 0.547722.
Train: 2018-08-02T00:26:13.897850: step 24731, loss 0.474136.
Train: 2018-08-02T00:26:14.064400: step 24732, loss 0.580229.
Train: 2018-08-02T00:26:14.228994: step 24733, loss 0.580246.
Train: 2018-08-02T00:26:14.392547: step 24734, loss 0.65111.
Train: 2018-08-02T00:26:14.556104: step 24735, loss 0.438638.
Train: 2018-08-02T00:26:14.724635: step 24736, loss 0.580248.
Train: 2018-08-02T00:26:14.889195: step 24737, loss 0.56254.
Train: 2018-08-02T00:26:15.060761: step 24738, loss 0.491673.
Train: 2018-08-02T00:26:15.228288: step 24739, loss 0.562547.
Train: 2018-08-02T00:26:15.399830: step 24740, loss 0.580296.
Test: 2018-08-02T00:26:15.936396: step 24740, loss 0.547692.
Train: 2018-08-02T00:26:16.112923: step 24741, loss 0.509306.
Train: 2018-08-02T00:26:16.277484: step 24742, loss 0.527034.
Train: 2018-08-02T00:26:16.439051: step 24743, loss 0.633699.
Train: 2018-08-02T00:26:16.612587: step 24744, loss 0.544787.
Train: 2018-08-02T00:26:16.777155: step 24745, loss 0.527001.
Train: 2018-08-02T00:26:16.941733: step 24746, loss 0.580367.
Train: 2018-08-02T00:26:17.116241: step 24747, loss 0.651544.
Train: 2018-08-02T00:26:17.283794: step 24748, loss 0.544791.
Train: 2018-08-02T00:26:17.450375: step 24749, loss 0.6336.
Train: 2018-08-02T00:26:17.615905: step 24750, loss 0.66893.
Test: 2018-08-02T00:26:18.150507: step 24750, loss 0.547719.
Train: 2018-08-02T00:26:18.328001: step 24751, loss 0.562525.
Train: 2018-08-02T00:26:18.493585: step 24752, loss 0.509604.
Train: 2018-08-02T00:26:18.666098: step 24753, loss 0.650508.
Train: 2018-08-02T00:26:18.837643: step 24754, loss 0.597589.
Train: 2018-08-02T00:26:19.000230: step 24755, loss 0.59747.
Train: 2018-08-02T00:26:19.165761: step 24756, loss 0.51011.
Train: 2018-08-02T00:26:19.330323: step 24757, loss 0.527628.
Train: 2018-08-02T00:26:19.495880: step 24758, loss 0.545059.
Train: 2018-08-02T00:26:19.666424: step 24759, loss 0.579791.
Train: 2018-08-02T00:26:19.831985: step 24760, loss 0.493105.
Test: 2018-08-02T00:26:20.371545: step 24760, loss 0.547924.
Train: 2018-08-02T00:26:20.535102: step 24761, loss 0.56243.
Train: 2018-08-02T00:26:20.699662: step 24762, loss 0.59706.
Train: 2018-08-02T00:26:20.862257: step 24763, loss 0.562427.
Train: 2018-08-02T00:26:21.023795: step 24764, loss 0.493279.
Train: 2018-08-02T00:26:21.186361: step 24765, loss 0.614291.
Train: 2018-08-02T00:26:21.351918: step 24766, loss 0.49331.
Train: 2018-08-02T00:26:21.515497: step 24767, loss 0.527857.
Train: 2018-08-02T00:26:21.692008: step 24768, loss 0.545131.
Train: 2018-08-02T00:26:21.864548: step 24769, loss 0.545118.
Train: 2018-08-02T00:26:22.030105: step 24770, loss 0.527779.
Test: 2018-08-02T00:26:22.569662: step 24770, loss 0.547907.
Train: 2018-08-02T00:26:22.734223: step 24771, loss 0.510393.
Train: 2018-08-02T00:26:22.897786: step 24772, loss 0.579813.
Train: 2018-08-02T00:26:23.062371: step 24773, loss 0.719022.
Train: 2018-08-02T00:26:23.237908: step 24774, loss 0.649361.
Train: 2018-08-02T00:26:23.402436: step 24775, loss 0.597141.
Train: 2018-08-02T00:26:23.569016: step 24776, loss 0.545111.
Train: 2018-08-02T00:26:23.741529: step 24777, loss 0.666159.
Train: 2018-08-02T00:26:23.909083: step 24778, loss 0.596903.
Train: 2018-08-02T00:26:24.070651: step 24779, loss 0.665559.
Train: 2018-08-02T00:26:24.240197: step 24780, loss 0.562419.
Test: 2018-08-02T00:26:24.769781: step 24780, loss 0.548135.
Train: 2018-08-02T00:26:24.932374: step 24781, loss 0.528298.
Train: 2018-08-02T00:26:25.093940: step 24782, loss 0.579441.
Train: 2018-08-02T00:26:25.268447: step 24783, loss 0.511525.
Train: 2018-08-02T00:26:25.436997: step 24784, loss 0.596315.
Train: 2018-08-02T00:26:25.607542: step 24785, loss 0.663878.
Train: 2018-08-02T00:26:25.786065: step 24786, loss 0.495034.
Train: 2018-08-02T00:26:25.951622: step 24787, loss 0.579285.
Train: 2018-08-02T00:26:26.116183: step 24788, loss 0.612851.
Train: 2018-08-02T00:26:26.281763: step 24789, loss 0.595993.
Train: 2018-08-02T00:26:26.449317: step 24790, loss 0.545775.
Test: 2018-08-02T00:26:26.977879: step 24790, loss 0.54853.
Train: 2018-08-02T00:26:27.141443: step 24791, loss 0.495757.
Train: 2018-08-02T00:26:27.304034: step 24792, loss 0.562508.
Train: 2018-08-02T00:26:27.469589: step 24793, loss 0.645838.
Train: 2018-08-02T00:26:27.635146: step 24794, loss 0.612442.
Train: 2018-08-02T00:26:27.806663: step 24795, loss 0.579142.
Train: 2018-08-02T00:26:27.971223: step 24796, loss 0.628857.
Train: 2018-08-02T00:26:28.140770: step 24797, loss 0.529493.
Train: 2018-08-02T00:26:28.309320: step 24798, loss 0.529562.
Train: 2018-08-02T00:26:28.470887: step 24799, loss 0.513109.
Train: 2018-08-02T00:26:28.641432: step 24800, loss 0.628555.
Test: 2018-08-02T00:26:29.177997: step 24800, loss 0.548798.
Train: 2018-08-02T00:26:29.981474: step 24801, loss 0.612029.
Train: 2018-08-02T00:26:30.146009: step 24802, loss 0.529695.
Train: 2018-08-02T00:26:30.308574: step 24803, loss 0.463937.
Train: 2018-08-02T00:26:30.477170: step 24804, loss 0.562605.
Train: 2018-08-02T00:26:30.644675: step 24805, loss 0.579072.
Train: 2018-08-02T00:26:30.807273: step 24806, loss 0.579078.
Train: 2018-08-02T00:26:30.975790: step 24807, loss 0.579082.
Train: 2018-08-02T00:26:31.142377: step 24808, loss 0.661601.
Train: 2018-08-02T00:26:31.306906: step 24809, loss 0.579078.
Train: 2018-08-02T00:26:31.478447: step 24810, loss 0.546123.
Test: 2018-08-02T00:26:32.023990: step 24810, loss 0.548812.
Train: 2018-08-02T00:26:32.188549: step 24811, loss 0.463801.
Train: 2018-08-02T00:26:32.364079: step 24812, loss 0.529625.
Train: 2018-08-02T00:26:32.544627: step 24813, loss 0.546072.
Train: 2018-08-02T00:26:32.718163: step 24814, loss 0.612172.
Train: 2018-08-02T00:26:32.881695: step 24815, loss 0.413599.
Train: 2018-08-02T00:26:33.044260: step 24816, loss 0.496139.
Train: 2018-08-02T00:26:33.205856: step 24817, loss 0.579171.
Train: 2018-08-02T00:26:33.373413: step 24818, loss 0.612624.
Train: 2018-08-02T00:26:33.544923: step 24819, loss 0.545734.
Train: 2018-08-02T00:26:33.708523: step 24820, loss 0.612824.
Test: 2018-08-02T00:26:34.249043: step 24820, loss 0.548388.
Train: 2018-08-02T00:26:34.415614: step 24821, loss 0.579272.
Train: 2018-08-02T00:26:34.588157: step 24822, loss 0.646597.
Train: 2018-08-02T00:26:34.752694: step 24823, loss 0.663425.
Train: 2018-08-02T00:26:34.915258: step 24824, loss 0.612884.
Train: 2018-08-02T00:26:35.081839: step 24825, loss 0.478581.
Train: 2018-08-02T00:26:35.245376: step 24826, loss 0.512159.
Train: 2018-08-02T00:26:35.412929: step 24827, loss 0.629582.
Train: 2018-08-02T00:26:35.578486: step 24828, loss 0.562474.
Train: 2018-08-02T00:26:35.744093: step 24829, loss 0.612765.
Train: 2018-08-02T00:26:35.907607: step 24830, loss 0.528984.
Test: 2018-08-02T00:26:36.445170: step 24830, loss 0.548462.
Train: 2018-08-02T00:26:36.611749: step 24831, loss 0.445284.
Train: 2018-08-02T00:26:36.775286: step 24832, loss 0.562475.
Train: 2018-08-02T00:26:36.940844: step 24833, loss 0.49533.
Train: 2018-08-02T00:26:37.107419: step 24834, loss 0.528822.
Train: 2018-08-02T00:26:37.270961: step 24835, loss 0.596166.
Train: 2018-08-02T00:26:37.440540: step 24836, loss 0.646876.
Train: 2018-08-02T00:26:37.617069: step 24837, loss 0.545547.
Train: 2018-08-02T00:26:37.787581: step 24838, loss 0.579346.
Train: 2018-08-02T00:26:37.957128: step 24839, loss 0.579352.
Train: 2018-08-02T00:26:38.119692: step 24840, loss 0.579356.
Test: 2018-08-02T00:26:38.648312: step 24840, loss 0.548269.
Train: 2018-08-02T00:26:38.813837: step 24841, loss 0.630114.
Train: 2018-08-02T00:26:38.981389: step 24842, loss 0.461004.
Train: 2018-08-02T00:26:39.153960: step 24843, loss 0.579354.
Train: 2018-08-02T00:26:39.319487: step 24844, loss 0.630128.
Train: 2018-08-02T00:26:39.485075: step 24845, loss 0.579353.
Train: 2018-08-02T00:26:39.654590: step 24846, loss 0.579345.
Train: 2018-08-02T00:26:39.819149: step 24847, loss 0.663799.
Train: 2018-08-02T00:26:39.982744: step 24848, loss 0.511866.
Train: 2018-08-02T00:26:40.149298: step 24849, loss 0.596141.
Train: 2018-08-02T00:26:40.318813: step 24850, loss 0.495169.
Test: 2018-08-02T00:26:40.849397: step 24850, loss 0.548376.
Train: 2018-08-02T00:26:41.067143: step 24851, loss 0.579277.
Train: 2018-08-02T00:26:41.232706: step 24852, loss 0.528837.
Train: 2018-08-02T00:26:41.408205: step 24853, loss 0.663347.
Train: 2018-08-02T00:26:41.570801: step 24854, loss 0.545669.
Train: 2018-08-02T00:26:41.741340: step 24855, loss 0.579252.
Train: 2018-08-02T00:26:41.904878: step 24856, loss 0.562472.
Train: 2018-08-02T00:26:42.071436: step 24857, loss 0.629514.
Train: 2018-08-02T00:26:42.236990: step 24858, loss 0.545747.
Train: 2018-08-02T00:26:42.399580: step 24859, loss 0.529048.
Train: 2018-08-02T00:26:42.561149: step 24860, loss 0.595918.
Test: 2018-08-02T00:26:43.105667: step 24860, loss 0.548508.
Train: 2018-08-02T00:26:43.277263: step 24861, loss 0.512387.
Train: 2018-08-02T00:26:43.441800: step 24862, loss 0.428859.
Train: 2018-08-02T00:26:43.606329: step 24863, loss 0.612692.
Train: 2018-08-02T00:26:43.776873: step 24864, loss 0.562477.
Train: 2018-08-02T00:26:43.939439: step 24865, loss 0.579245.
Train: 2018-08-02T00:26:44.104029: step 24866, loss 0.629617.
Train: 2018-08-02T00:26:44.273600: step 24867, loss 0.512107.
Train: 2018-08-02T00:26:44.441128: step 24868, loss 0.495273.
Train: 2018-08-02T00:26:44.606680: step 24869, loss 0.562457.
Train: 2018-08-02T00:26:44.779218: step 24870, loss 0.478215.
Test: 2018-08-02T00:26:45.313765: step 24870, loss 0.5483.
Train: 2018-08-02T00:26:45.477361: step 24871, loss 0.596217.
Train: 2018-08-02T00:26:45.647871: step 24872, loss 0.562436.
Train: 2018-08-02T00:26:45.816421: step 24873, loss 0.562431.
Train: 2018-08-02T00:26:45.981006: step 24874, loss 0.596369.
Train: 2018-08-02T00:26:46.145541: step 24875, loss 0.494477.
Train: 2018-08-02T00:26:46.310132: step 24876, loss 0.511372.
Train: 2018-08-02T00:26:46.484667: step 24877, loss 0.579472.
Train: 2018-08-02T00:26:46.653215: step 24878, loss 0.596587.
Train: 2018-08-02T00:26:46.823781: step 24879, loss 0.630839.
Train: 2018-08-02T00:26:46.996267: step 24880, loss 0.511084.
Test: 2018-08-02T00:26:47.528844: step 24880, loss 0.548074.
Train: 2018-08-02T00:26:47.703431: step 24881, loss 0.630914.
Train: 2018-08-02T00:26:47.868933: step 24882, loss 0.562414.
Train: 2018-08-02T00:26:48.029535: step 24883, loss 0.545291.
Train: 2018-08-02T00:26:48.204063: step 24884, loss 0.562414.
Train: 2018-08-02T00:26:48.367634: step 24885, loss 0.579542.
Train: 2018-08-02T00:26:48.531213: step 24886, loss 0.442522.
Train: 2018-08-02T00:26:48.697718: step 24887, loss 0.545262.
Train: 2018-08-02T00:26:48.861311: step 24888, loss 0.579591.
Train: 2018-08-02T00:26:49.026839: step 24889, loss 0.579611.
Train: 2018-08-02T00:26:49.188437: step 24890, loss 0.510782.
Test: 2018-08-02T00:26:49.714035: step 24890, loss 0.547984.
Train: 2018-08-02T00:26:49.889566: step 24891, loss 0.562416.
Train: 2018-08-02T00:26:50.052097: step 24892, loss 0.545162.
Train: 2018-08-02T00:26:50.221644: step 24893, loss 0.562419.
Train: 2018-08-02T00:26:50.393204: step 24894, loss 0.475939.
Train: 2018-08-02T00:26:50.556749: step 24895, loss 0.597088.
Train: 2018-08-02T00:26:50.721308: step 24896, loss 0.562428.
Train: 2018-08-02T00:26:50.888861: step 24897, loss 0.545055.
Train: 2018-08-02T00:26:51.060401: step 24898, loss 0.597233.
Train: 2018-08-02T00:26:51.229974: step 24899, loss 0.597259.
Train: 2018-08-02T00:26:51.399529: step 24900, loss 0.510198.
Test: 2018-08-02T00:26:51.939053: step 24900, loss 0.547848.
Train: 2018-08-02T00:26:52.758531: step 24901, loss 0.579867.
Train: 2018-08-02T00:26:52.926082: step 24902, loss 0.579875.
Train: 2018-08-02T00:26:53.092605: step 24903, loss 0.597312.
Train: 2018-08-02T00:26:53.255172: step 24904, loss 0.510156.
Train: 2018-08-02T00:26:53.422755: step 24905, loss 0.545009.
Train: 2018-08-02T00:26:53.589313: step 24906, loss 0.492685.
Train: 2018-08-02T00:26:53.756855: step 24907, loss 0.527529.
Train: 2018-08-02T00:26:53.924415: step 24908, loss 0.614906.
Train: 2018-08-02T00:26:54.089973: step 24909, loss 0.632434.
Train: 2018-08-02T00:26:54.255498: step 24910, loss 0.579942.
Test: 2018-08-02T00:26:54.800075: step 24910, loss 0.547817.
Train: 2018-08-02T00:26:54.974575: step 24911, loss 0.597407.
Train: 2018-08-02T00:26:55.140132: step 24912, loss 0.562448.
Train: 2018-08-02T00:26:55.306713: step 24913, loss 0.684546.
Train: 2018-08-02T00:26:55.484244: step 24914, loss 0.40582.
Train: 2018-08-02T00:26:55.659775: step 24915, loss 0.527641.
Train: 2018-08-02T00:26:55.823306: step 24916, loss 0.49284.
Train: 2018-08-02T00:26:55.994847: step 24917, loss 0.68434.
Train: 2018-08-02T00:26:56.162400: step 24918, loss 0.423214.
Train: 2018-08-02T00:26:56.330955: step 24919, loss 0.6147.
Train: 2018-08-02T00:26:56.509497: step 24920, loss 0.527591.
Test: 2018-08-02T00:26:57.036064: step 24920, loss 0.547842.
Train: 2018-08-02T00:26:57.202632: step 24921, loss 0.492702.
Train: 2018-08-02T00:26:57.370197: step 24922, loss 0.579905.
Train: 2018-08-02T00:26:57.536751: step 24923, loss 0.544976.
Train: 2018-08-02T00:26:57.701286: step 24924, loss 0.492488.
Train: 2018-08-02T00:26:57.866869: step 24925, loss 0.650068.
Train: 2018-08-02T00:26:58.030406: step 24926, loss 0.61505.
Train: 2018-08-02T00:26:58.197958: step 24927, loss 0.650077.
Train: 2018-08-02T00:26:58.362519: step 24928, loss 0.649949.
Train: 2018-08-02T00:26:58.533093: step 24929, loss 0.527531.
Train: 2018-08-02T00:26:58.693632: step 24930, loss 0.492731.
Test: 2018-08-02T00:26:59.218231: step 24930, loss 0.547855.
Train: 2018-08-02T00:26:59.383819: step 24931, loss 0.510196.
Train: 2018-08-02T00:26:59.549372: step 24932, loss 0.457966.
Train: 2018-08-02T00:26:59.721915: step 24933, loss 0.70189.
Train: 2018-08-02T00:26:59.885448: step 24934, loss 0.579857.
Train: 2018-08-02T00:27:00.069953: step 24935, loss 0.579838.
Train: 2018-08-02T00:27:00.235538: step 24936, loss 0.527662.
Train: 2018-08-02T00:27:00.405057: step 24937, loss 0.475555.
Train: 2018-08-02T00:27:00.569617: step 24938, loss 0.562431.
Train: 2018-08-02T00:27:00.735202: step 24939, loss 0.666777.
Train: 2018-08-02T00:27:00.899760: step 24940, loss 0.49293.
Test: 2018-08-02T00:27:01.422338: step 24940, loss 0.54788.
Train: 2018-08-02T00:27:01.589924: step 24941, loss 0.56243.
Train: 2018-08-02T00:27:01.760436: step 24942, loss 0.562429.
Train: 2018-08-02T00:27:01.935001: step 24943, loss 0.597173.
Train: 2018-08-02T00:27:02.100526: step 24944, loss 0.475614.
Train: 2018-08-02T00:27:02.267111: step 24945, loss 0.597173.
Train: 2018-08-02T00:27:02.433666: step 24946, loss 0.510312.
Train: 2018-08-02T00:27:02.602209: step 24947, loss 0.597197.
Train: 2018-08-02T00:27:02.772755: step 24948, loss 0.597201.
Train: 2018-08-02T00:27:02.939284: step 24949, loss 0.68408.
Train: 2018-08-02T00:27:03.105837: step 24950, loss 0.545078.
Test: 2018-08-02T00:27:03.644399: step 24950, loss 0.547917.
Train: 2018-08-02T00:27:03.811982: step 24951, loss 0.510457.
Train: 2018-08-02T00:27:03.977507: step 24952, loss 0.545111.
Train: 2018-08-02T00:27:04.149049: step 24953, loss 0.57972.
Train: 2018-08-02T00:27:04.314633: step 24954, loss 0.458679.
Train: 2018-08-02T00:27:04.494160: step 24955, loss 0.40671.
Train: 2018-08-02T00:27:04.659684: step 24956, loss 0.597111.
Train: 2018-08-02T00:27:04.826239: step 24957, loss 0.545055.
Train: 2018-08-02T00:27:04.992825: step 24958, loss 0.579838.
Train: 2018-08-02T00:27:05.170350: step 24959, loss 0.597293.
Train: 2018-08-02T00:27:05.334910: step 24960, loss 0.510122.
Test: 2018-08-02T00:27:05.864465: step 24960, loss 0.547825.
Train: 2018-08-02T00:27:06.030046: step 24961, loss 0.614831.
Train: 2018-08-02T00:27:06.194615: step 24962, loss 0.510041.
Train: 2018-08-02T00:27:06.363164: step 24963, loss 0.562452.
Train: 2018-08-02T00:27:06.528728: step 24964, loss 0.562456.
Train: 2018-08-02T00:27:06.693248: step 24965, loss 0.614996.
Train: 2018-08-02T00:27:06.860800: step 24966, loss 0.457392.
Train: 2018-08-02T00:27:07.029349: step 24967, loss 0.579995.
Train: 2018-08-02T00:27:07.198896: step 24968, loss 0.6151.
Train: 2018-08-02T00:27:07.381440: step 24969, loss 0.667731.
Train: 2018-08-02T00:27:07.547994: step 24970, loss 0.562461.
Test: 2018-08-02T00:27:08.085531: step 24970, loss 0.547802.
Train: 2018-08-02T00:27:08.253078: step 24971, loss 0.579954.
Train: 2018-08-02T00:27:08.419667: step 24972, loss 0.475074.
Train: 2018-08-02T00:27:08.585190: step 24973, loss 0.527507.
Train: 2018-08-02T00:27:08.747756: step 24974, loss 0.527503.
Train: 2018-08-02T00:27:08.911319: step 24975, loss 0.544969.
Train: 2018-08-02T00:27:09.077924: step 24976, loss 0.474993.
Train: 2018-08-02T00:27:09.242435: step 24977, loss 0.615016.
Train: 2018-08-02T00:27:09.405997: step 24978, loss 0.562464.
Train: 2018-08-02T00:27:09.569559: step 24979, loss 0.615088.
Train: 2018-08-02T00:27:09.732149: step 24980, loss 0.492316.
Test: 2018-08-02T00:27:10.268714: step 24980, loss 0.547774.
Train: 2018-08-02T00:27:10.437265: step 24981, loss 0.580017.
Train: 2018-08-02T00:27:10.612779: step 24982, loss 0.527364.
Train: 2018-08-02T00:27:10.776333: step 24983, loss 0.487536.
Train: 2018-08-02T00:27:10.940927: step 24984, loss 0.615243.
Train: 2018-08-02T00:27:11.105452: step 24985, loss 0.650467.
Train: 2018-08-02T00:27:11.275025: step 24986, loss 0.492137.
Train: 2018-08-02T00:27:11.436568: step 24987, loss 0.615249.
Train: 2018-08-02T00:27:11.600131: step 24988, loss 0.527318.
Train: 2018-08-02T00:27:11.769708: step 24989, loss 0.439429.
Train: 2018-08-02T00:27:11.933266: step 24990, loss 0.580086.
Test: 2018-08-02T00:27:12.451854: step 24990, loss 0.547739.
Train: 2018-08-02T00:27:12.617442: step 24991, loss 0.456793.
Train: 2018-08-02T00:27:12.782002: step 24992, loss 0.4566.
Train: 2018-08-02T00:27:12.948526: step 24993, loss 0.597927.
Train: 2018-08-02T00:27:13.126076: step 24994, loss 0.615757.
Train: 2018-08-02T00:27:13.290613: step 24995, loss 0.509267.
Train: 2018-08-02T00:27:13.461182: step 24996, loss 0.704871.
Train: 2018-08-02T00:27:13.627710: step 24997, loss 0.687042.
Train: 2018-08-02T00:27:13.792297: step 24998, loss 0.615799.
Train: 2018-08-02T00:27:13.957858: step 24999, loss 0.597947.
Train: 2018-08-02T00:27:14.119397: step 25000, loss 0.491849.
Test: 2018-08-02T00:27:14.643030: step 25000, loss 0.547729.
Train: 2018-08-02T00:27:15.399572: step 25001, loss 0.509588.
Train: 2018-08-02T00:27:15.565127: step 25002, loss 0.474388.
Train: 2018-08-02T00:27:15.737666: step 25003, loss 0.527246.
Train: 2018-08-02T00:27:15.904265: step 25004, loss 0.527232.
Train: 2018-08-02T00:27:16.072771: step 25005, loss 0.580145.
Train: 2018-08-02T00:27:16.243315: step 25006, loss 0.509547.
Train: 2018-08-02T00:27:16.408903: step 25007, loss 0.456501.
Train: 2018-08-02T00:27:16.571481: step 25008, loss 0.491713.
Train: 2018-08-02T00:27:16.736996: step 25009, loss 0.633528.
Train: 2018-08-02T00:27:16.897597: step 25010, loss 0.562552.
Test: 2018-08-02T00:27:17.436126: step 25010, loss 0.547665.
Train: 2018-08-02T00:27:17.607669: step 25011, loss 0.526982.
Train: 2018-08-02T00:27:17.770264: step 25012, loss 0.598198.
Train: 2018-08-02T00:27:17.951781: step 25013, loss 0.562579.
Train: 2018-08-02T00:27:18.115337: step 25014, loss 0.473418.
Train: 2018-08-02T00:27:18.278879: step 25015, loss 0.651882.
Train: 2018-08-02T00:27:18.460388: step 25016, loss 0.544738.
Train: 2018-08-02T00:27:18.623977: step 25017, loss 0.49115.
Train: 2018-08-02T00:27:18.792501: step 25018, loss 0.598361.
Train: 2018-08-02T00:27:18.955092: step 25019, loss 0.598373.
Train: 2018-08-02T00:27:19.124613: step 25020, loss 0.651991.
Test: 2018-08-02T00:27:19.655220: step 25020, loss 0.547644.
Train: 2018-08-02T00:27:19.821775: step 25021, loss 0.687559.
Train: 2018-08-02T00:27:19.990299: step 25022, loss 0.544765.
Train: 2018-08-02T00:27:20.154859: step 25023, loss 0.544787.
Train: 2018-08-02T00:27:20.318452: step 25024, loss 0.615703.
Train: 2018-08-02T00:27:20.480020: step 25025, loss 0.562513.
Train: 2018-08-02T00:27:20.657515: step 25026, loss 0.59777.
Train: 2018-08-02T00:27:20.823072: step 25027, loss 0.56248.
Train: 2018-08-02T00:27:21.001595: step 25028, loss 0.580014.
Train: 2018-08-02T00:27:21.167182: step 25029, loss 0.579958.
Train: 2018-08-02T00:27:21.328748: step 25030, loss 0.527523.
Test: 2018-08-02T00:27:21.851323: step 25030, loss 0.547843.
Train: 2018-08-02T00:27:22.016911: step 25031, loss 0.579863.
Train: 2018-08-02T00:27:22.182469: step 25032, loss 0.56243.
Train: 2018-08-02T00:27:22.350988: step 25033, loss 0.597148.
Train: 2018-08-02T00:27:22.529510: step 25034, loss 0.614397.
Train: 2018-08-02T00:27:22.693086: step 25035, loss 0.562414.
Train: 2018-08-02T00:27:22.862620: step 25036, loss 0.493441.
Train: 2018-08-02T00:27:23.033164: step 25037, loss 0.459084.
Train: 2018-08-02T00:27:23.197725: step 25038, loss 0.579633.
Train: 2018-08-02T00:27:23.362285: step 25039, loss 0.476302.
Train: 2018-08-02T00:27:23.526875: step 25040, loss 0.579648.
Test: 2018-08-02T00:27:24.072417: step 25040, loss 0.547967.
Train: 2018-08-02T00:27:24.248913: step 25041, loss 0.596908.
Train: 2018-08-02T00:27:24.414472: step 25042, loss 0.614164.
Train: 2018-08-02T00:27:24.578035: step 25043, loss 0.56241.
Train: 2018-08-02T00:27:24.746584: step 25044, loss 0.56241.
Train: 2018-08-02T00:27:24.910148: step 25045, loss 0.562409.
Train: 2018-08-02T00:27:25.075717: step 25046, loss 0.545191.
Train: 2018-08-02T00:27:25.241292: step 25047, loss 0.510763.
Train: 2018-08-02T00:27:25.402829: step 25048, loss 0.493516.
Train: 2018-08-02T00:27:25.573405: step 25049, loss 0.527921.
Train: 2018-08-02T00:27:25.744940: step 25050, loss 0.545141.
Test: 2018-08-02T00:27:26.284473: step 25050, loss 0.54793.
Train: 2018-08-02T00:27:26.445068: step 25051, loss 0.631609.
Train: 2018-08-02T00:27:26.607635: step 25052, loss 0.545108.
Train: 2018-08-02T00:27:26.774195: step 25053, loss 0.527778.
Train: 2018-08-02T00:27:26.937726: step 25054, loss 0.614431.
Train: 2018-08-02T00:27:27.102317: step 25055, loss 0.475714.
Train: 2018-08-02T00:27:27.265875: step 25056, loss 0.527698.
Train: 2018-08-02T00:27:27.426420: step 25057, loss 0.579817.
Train: 2018-08-02T00:27:27.593005: step 25058, loss 0.545024.
Train: 2018-08-02T00:27:27.762522: step 25059, loss 0.492723.
Train: 2018-08-02T00:27:27.930099: step 25060, loss 0.684664.
Test: 2018-08-02T00:27:28.467636: step 25060, loss 0.54782.
Train: 2018-08-02T00:27:28.636211: step 25061, loss 0.54498.
Train: 2018-08-02T00:27:28.803769: step 25062, loss 0.510041.
Train: 2018-08-02T00:27:28.974307: step 25063, loss 0.475037.
Train: 2018-08-02T00:27:29.155796: step 25064, loss 0.667531.
Train: 2018-08-02T00:27:29.317391: step 25065, loss 0.474872.
Train: 2018-08-02T00:27:29.488939: step 25066, loss 0.562463.
Train: 2018-08-02T00:27:29.656488: step 25067, loss 0.45713.
Train: 2018-08-02T00:27:29.827999: step 25068, loss 0.580073.
Train: 2018-08-02T00:27:29.989597: step 25069, loss 0.580111.
Train: 2018-08-02T00:27:30.164101: step 25070, loss 0.50957.
Test: 2018-08-02T00:27:30.693688: step 25070, loss 0.547711.
Train: 2018-08-02T00:27:30.873205: step 25071, loss 0.527167.
Train: 2018-08-02T00:27:31.033810: step 25072, loss 0.597924.
Train: 2018-08-02T00:27:31.206315: step 25073, loss 0.633412.
Train: 2018-08-02T00:27:31.372871: step 25074, loss 0.49164.
Train: 2018-08-02T00:27:31.534464: step 25075, loss 0.580271.
Train: 2018-08-02T00:27:31.708972: step 25076, loss 0.651257.
Train: 2018-08-02T00:27:31.877521: step 25077, loss 0.597994.
Train: 2018-08-02T00:27:32.046070: step 25078, loss 0.562524.
Train: 2018-08-02T00:27:32.216615: step 25079, loss 0.615585.
Train: 2018-08-02T00:27:32.378209: step 25080, loss 0.527185.
Test: 2018-08-02T00:27:32.917756: step 25080, loss 0.547726.
Train: 2018-08-02T00:27:33.091276: step 25081, loss 0.562495.
Train: 2018-08-02T00:27:33.255867: step 25082, loss 0.597721.
Train: 2018-08-02T00:27:33.423389: step 25083, loss 0.668016.
Train: 2018-08-02T00:27:33.585954: step 25084, loss 0.667716.
Train: 2018-08-02T00:27:33.759489: step 25085, loss 0.632351.
Train: 2018-08-02T00:27:33.934024: step 25086, loss 0.527626.
Train: 2018-08-02T00:27:34.104594: step 25087, loss 0.597103.
Train: 2018-08-02T00:27:34.267163: step 25088, loss 0.596971.
Train: 2018-08-02T00:27:34.428727: step 25089, loss 0.717348.
Train: 2018-08-02T00:27:34.590299: step 25090, loss 0.442527.
Test: 2018-08-02T00:27:35.133817: step 25090, loss 0.548115.
Train: 2018-08-02T00:27:35.298402: step 25091, loss 0.61362.
Train: 2018-08-02T00:27:35.461964: step 25092, loss 0.579425.
Train: 2018-08-02T00:27:35.624504: step 25093, loss 0.562422.
Train: 2018-08-02T00:27:35.786097: step 25094, loss 0.477928.
Train: 2018-08-02T00:27:35.952658: step 25095, loss 0.579311.
Train: 2018-08-02T00:27:36.114195: step 25096, loss 0.528752.
Train: 2018-08-02T00:27:36.276794: step 25097, loss 0.545618.
Train: 2018-08-02T00:27:36.440349: step 25098, loss 0.495174.
Train: 2018-08-02T00:27:36.602888: step 25099, loss 0.629749.
Train: 2018-08-02T00:27:36.780414: step 25100, loss 0.495183.
Test: 2018-08-02T00:27:37.323961: step 25100, loss 0.548359.
Train: 2018-08-02T00:27:38.081546: step 25101, loss 0.562449.
Train: 2018-08-02T00:27:38.252065: step 25102, loss 0.57928.
Train: 2018-08-02T00:27:38.433610: step 25103, loss 0.579283.
Train: 2018-08-02T00:27:38.598171: step 25104, loss 0.579285.
Train: 2018-08-02T00:27:38.761702: step 25105, loss 0.629799.
Train: 2018-08-02T00:27:38.929255: step 25106, loss 0.478331.
Train: 2018-08-02T00:27:39.103813: step 25107, loss 0.478301.
Train: 2018-08-02T00:27:39.270342: step 25108, loss 0.444467.
Train: 2018-08-02T00:27:39.429920: step 25109, loss 0.646938.
Train: 2018-08-02T00:27:39.595498: step 25110, loss 0.545499.
Test: 2018-08-02T00:27:40.134062: step 25110, loss 0.548224.
Train: 2018-08-02T00:27:40.298593: step 25111, loss 0.545467.
Train: 2018-08-02T00:27:40.462157: step 25112, loss 0.494484.
Train: 2018-08-02T00:27:40.626753: step 25113, loss 0.732657.
Train: 2018-08-02T00:27:40.802272: step 25114, loss 0.545387.
Train: 2018-08-02T00:27:40.966808: step 25115, loss 0.596473.
Train: 2018-08-02T00:27:41.132365: step 25116, loss 0.545384.
Train: 2018-08-02T00:27:41.297953: step 25117, loss 0.630531.
Train: 2018-08-02T00:27:41.472455: step 25118, loss 0.545396.
Train: 2018-08-02T00:27:41.643025: step 25119, loss 0.477365.
Train: 2018-08-02T00:27:41.803601: step 25120, loss 0.579435.
Test: 2018-08-02T00:27:42.341137: step 25120, loss 0.548152.
Train: 2018-08-02T00:27:42.507688: step 25121, loss 0.596471.
Train: 2018-08-02T00:27:42.679230: step 25122, loss 0.579441.
Train: 2018-08-02T00:27:42.847779: step 25123, loss 0.528361.
Train: 2018-08-02T00:27:43.015363: step 25124, loss 0.647564.
Train: 2018-08-02T00:27:43.181887: step 25125, loss 0.494346.
Train: 2018-08-02T00:27:43.347444: step 25126, loss 0.63049.
Train: 2018-08-02T00:27:43.509011: step 25127, loss 0.613437.
Train: 2018-08-02T00:27:43.672574: step 25128, loss 0.562417.
Train: 2018-08-02T00:27:43.834173: step 25129, loss 0.613324.
Train: 2018-08-02T00:27:43.998703: step 25130, loss 0.562424.
Test: 2018-08-02T00:27:44.529297: step 25130, loss 0.548261.
Train: 2018-08-02T00:27:44.690852: step 25131, loss 0.460922.
Train: 2018-08-02T00:27:44.852454: step 25132, loss 0.579346.
Train: 2018-08-02T00:27:45.017978: step 25133, loss 0.477844.
Train: 2018-08-02T00:27:45.185529: step 25134, loss 0.511624.
Train: 2018-08-02T00:27:45.347099: step 25135, loss 0.748985.
Train: 2018-08-02T00:27:45.511663: step 25136, loss 0.545477.
Train: 2018-08-02T00:27:45.672259: step 25137, loss 0.562424.
Train: 2018-08-02T00:27:45.839780: step 25138, loss 0.528569.
Train: 2018-08-02T00:27:46.004340: step 25139, loss 0.49471.
Train: 2018-08-02T00:27:46.168901: step 25140, loss 0.545478.
Test: 2018-08-02T00:27:46.703506: step 25140, loss 0.548215.
Train: 2018-08-02T00:27:46.864042: step 25141, loss 0.56242.
Train: 2018-08-02T00:27:47.025610: step 25142, loss 0.528457.
Train: 2018-08-02T00:27:47.191169: step 25143, loss 0.528408.
Train: 2018-08-02T00:27:47.362734: step 25144, loss 0.477253.
Train: 2018-08-02T00:27:47.527294: step 25145, loss 0.579483.
Train: 2018-08-02T00:27:47.687840: step 25146, loss 0.613742.
Train: 2018-08-02T00:27:47.846450: step 25147, loss 0.579539.
Train: 2018-08-02T00:27:48.011007: step 25148, loss 0.528105.
Train: 2018-08-02T00:27:48.171582: step 25149, loss 0.545233.
Train: 2018-08-02T00:27:48.336139: step 25150, loss 0.459236.
Test: 2018-08-02T00:27:48.871720: step 25150, loss 0.547973.
Train: 2018-08-02T00:27:49.033278: step 25151, loss 0.596883.
Train: 2018-08-02T00:27:49.191819: step 25152, loss 0.579678.
Train: 2018-08-02T00:27:49.354387: step 25153, loss 0.510536.
Train: 2018-08-02T00:27:49.514956: step 25154, loss 0.475797.
Train: 2018-08-02T00:27:49.675528: step 25155, loss 0.579793.
Train: 2018-08-02T00:27:49.841085: step 25156, loss 0.545022.
Train: 2018-08-02T00:27:50.003649: step 25157, loss 0.597325.
Train: 2018-08-02T00:27:50.181206: step 25158, loss 0.544973.
Train: 2018-08-02T00:27:50.345736: step 25159, loss 0.667415.
Train: 2018-08-02T00:27:50.505309: step 25160, loss 0.614932.
Test: 2018-08-02T00:27:51.038889: step 25160, loss 0.547806.
Train: 2018-08-02T00:27:51.202495: step 25161, loss 0.52748.
Train: 2018-08-02T00:27:51.369030: step 25162, loss 0.544966.
Train: 2018-08-02T00:27:51.537550: step 25163, loss 0.440102.
Train: 2018-08-02T00:27:51.708119: step 25164, loss 0.544949.
Train: 2018-08-02T00:27:51.868697: step 25165, loss 0.474834.
Train: 2018-08-02T00:27:52.043230: step 25166, loss 0.580031.
Train: 2018-08-02T00:27:52.206760: step 25167, loss 0.492106.
Train: 2018-08-02T00:27:52.370348: step 25168, loss 0.597758.
Train: 2018-08-02T00:27:52.529896: step 25169, loss 0.650806.
Train: 2018-08-02T00:27:52.694487: step 25170, loss 0.544838.
Test: 2018-08-02T00:27:53.229028: step 25170, loss 0.547707.
Train: 2018-08-02T00:27:53.395608: step 25171, loss 0.633199.
Train: 2018-08-02T00:27:53.561165: step 25172, loss 0.59783.
Train: 2018-08-02T00:27:53.726697: step 25173, loss 0.615435.
Train: 2018-08-02T00:27:53.889291: step 25174, loss 0.509631.
Train: 2018-08-02T00:27:54.049834: step 25175, loss 0.544878.
Train: 2018-08-02T00:27:54.209437: step 25176, loss 0.580069.
Train: 2018-08-02T00:27:54.372003: step 25177, loss 0.597625.
Train: 2018-08-02T00:27:54.535534: step 25178, loss 0.615129.
Train: 2018-08-02T00:27:54.701124: step 25179, loss 0.562456.
Train: 2018-08-02T00:27:54.861663: step 25180, loss 0.667407.
Test: 2018-08-02T00:27:55.394273: step 25180, loss 0.547829.
Train: 2018-08-02T00:27:55.554837: step 25181, loss 0.544992.
Train: 2018-08-02T00:27:55.719371: step 25182, loss 0.527623.
Train: 2018-08-02T00:27:55.894901: step 25183, loss 0.492937.
Train: 2018-08-02T00:27:56.070432: step 25184, loss 0.510347.
Train: 2018-08-02T00:27:56.233994: step 25185, loss 0.735973.
Train: 2018-08-02T00:27:56.403541: step 25186, loss 0.545098.
Train: 2018-08-02T00:27:56.573088: step 25187, loss 0.475985.
Train: 2018-08-02T00:27:56.732691: step 25188, loss 0.545135.
Train: 2018-08-02T00:27:56.893267: step 25189, loss 0.579677.
Train: 2018-08-02T00:27:57.058790: step 25190, loss 0.614184.
Test: 2018-08-02T00:27:57.592364: step 25190, loss 0.547972.
Train: 2018-08-02T00:27:57.752933: step 25191, loss 0.545168.
Train: 2018-08-02T00:27:57.913504: step 25192, loss 0.614079.
Train: 2018-08-02T00:27:58.072111: step 25193, loss 0.545205.
Train: 2018-08-02T00:27:58.231685: step 25194, loss 0.442131.
Train: 2018-08-02T00:27:58.394220: step 25195, loss 0.545214.
Train: 2018-08-02T00:27:58.553792: step 25196, loss 0.579607.
Train: 2018-08-02T00:27:58.715361: step 25197, loss 0.527987.
Train: 2018-08-02T00:27:58.889895: step 25198, loss 0.545184.
Train: 2018-08-02T00:27:59.052491: step 25199, loss 0.493458.
Train: 2018-08-02T00:27:59.216023: step 25200, loss 0.631469.
Test: 2018-08-02T00:27:59.751636: step 25200, loss 0.547943.
Train: 2018-08-02T00:28:00.573918: step 25201, loss 0.527858.
Train: 2018-08-02T00:28:00.733493: step 25202, loss 0.631578.
Train: 2018-08-02T00:28:00.906056: step 25203, loss 0.631577.
Train: 2018-08-02T00:28:01.083589: step 25204, loss 0.493304.
Train: 2018-08-02T00:28:01.243154: step 25205, loss 0.476024.
Train: 2018-08-02T00:28:01.409684: step 25206, loss 0.666184.
Train: 2018-08-02T00:28:01.571277: step 25207, loss 0.527831.
Train: 2018-08-02T00:28:01.737835: step 25208, loss 0.648869.
Train: 2018-08-02T00:28:01.895417: step 25209, loss 0.52786.
Train: 2018-08-02T00:28:02.056957: step 25210, loss 0.545142.
Test: 2018-08-02T00:28:02.595514: step 25210, loss 0.547953.
Train: 2018-08-02T00:28:02.764064: step 25211, loss 0.493359.
Train: 2018-08-02T00:28:02.934609: step 25212, loss 0.545136.
Train: 2018-08-02T00:28:03.095178: step 25213, loss 0.631552.
Train: 2018-08-02T00:28:03.253783: step 25214, loss 0.579692.
Train: 2018-08-02T00:28:03.414326: step 25215, loss 0.59696.
Train: 2018-08-02T00:28:03.590879: step 25216, loss 0.631453.
Train: 2018-08-02T00:28:03.755414: step 25217, loss 0.49347.
Train: 2018-08-02T00:28:03.933936: step 25218, loss 0.614077.
Train: 2018-08-02T00:28:04.096530: step 25219, loss 0.562405.
Train: 2018-08-02T00:28:04.258104: step 25220, loss 0.579589.
Test: 2018-08-02T00:28:04.781669: step 25220, loss 0.54803.
Train: 2018-08-02T00:28:04.949222: step 25221, loss 0.47658.
Train: 2018-08-02T00:28:05.161748: step 25222, loss 0.579569.
Train: 2018-08-02T00:28:05.322319: step 25223, loss 0.596726.
Train: 2018-08-02T00:28:05.477933: step 25224, loss 0.528103.
Train: 2018-08-02T00:28:05.645479: step 25225, loss 0.648144.
Train: 2018-08-02T00:28:05.806026: step 25226, loss 0.648043.
Train: 2018-08-02T00:28:05.964633: step 25227, loss 0.596589.
Train: 2018-08-02T00:28:06.127193: step 25228, loss 0.425992.
Train: 2018-08-02T00:28:06.284774: step 25229, loss 0.579454.
Train: 2018-08-02T00:28:06.450304: step 25230, loss 0.528336.
Test: 2018-08-02T00:28:06.977943: step 25230, loss 0.548144.
Train: 2018-08-02T00:28:07.138492: step 25231, loss 0.596481.
Train: 2018-08-02T00:28:07.299065: step 25232, loss 0.579439.
Train: 2018-08-02T00:28:07.461600: step 25233, loss 0.596448.
Train: 2018-08-02T00:28:07.625163: step 25234, loss 0.562413.
Train: 2018-08-02T00:28:07.784764: step 25235, loss 0.596392.
Train: 2018-08-02T00:28:07.948298: step 25236, loss 0.579387.
Train: 2018-08-02T00:28:08.111862: step 25237, loss 0.57937.
Train: 2018-08-02T00:28:08.272432: step 25238, loss 0.477784.
Train: 2018-08-02T00:28:08.438015: step 25239, loss 0.562424.
Train: 2018-08-02T00:28:08.601583: step 25240, loss 0.528568.
Test: 2018-08-02T00:28:09.141138: step 25240, loss 0.548241.
Train: 2018-08-02T00:28:09.303676: step 25241, loss 0.460808.
Train: 2018-08-02T00:28:09.463249: step 25242, loss 0.596348.
Train: 2018-08-02T00:28:09.629849: step 25243, loss 0.5794.
Train: 2018-08-02T00:28:09.794364: step 25244, loss 0.579413.
Train: 2018-08-02T00:28:09.954963: step 25245, loss 0.528391.
Train: 2018-08-02T00:28:10.115522: step 25246, loss 0.579437.
Train: 2018-08-02T00:28:10.274081: step 25247, loss 0.562409.
Train: 2018-08-02T00:28:10.438642: step 25248, loss 0.579458.
Train: 2018-08-02T00:28:10.600240: step 25249, loss 0.562407.
Train: 2018-08-02T00:28:10.767793: step 25250, loss 0.511219.
Test: 2018-08-02T00:28:11.304328: step 25250, loss 0.548104.
Train: 2018-08-02T00:28:11.467890: step 25251, loss 0.630723.
Train: 2018-08-02T00:28:11.640428: step 25252, loss 0.596566.
Train: 2018-08-02T00:28:11.801997: step 25253, loss 0.459963.
Train: 2018-08-02T00:28:11.963564: step 25254, loss 0.545315.
Train: 2018-08-02T00:28:12.127152: step 25255, loss 0.596621.
Train: 2018-08-02T00:28:12.283740: step 25256, loss 0.528168.
Train: 2018-08-02T00:28:12.447321: step 25257, loss 0.511001.
Train: 2018-08-02T00:28:12.610866: step 25258, loss 0.545244.
Train: 2018-08-02T00:28:12.782402: step 25259, loss 0.596775.
Train: 2018-08-02T00:28:12.940951: step 25260, loss 0.579606.
Test: 2018-08-02T00:28:13.471541: step 25260, loss 0.54799.
Train: 2018-08-02T00:28:13.638088: step 25261, loss 0.527978.
Train: 2018-08-02T00:28:13.801651: step 25262, loss 0.562405.
Train: 2018-08-02T00:28:13.961251: step 25263, loss 0.545161.
Train: 2018-08-02T00:28:14.124814: step 25264, loss 0.64872.
Train: 2018-08-02T00:28:14.286355: step 25265, loss 0.562407.
Train: 2018-08-02T00:28:14.449923: step 25266, loss 0.45887.
Train: 2018-08-02T00:28:14.620494: step 25267, loss 0.527859.
Train: 2018-08-02T00:28:14.783028: step 25268, loss 0.666203.
Train: 2018-08-02T00:28:14.944621: step 25269, loss 0.527815.
Train: 2018-08-02T00:28:15.106189: step 25270, loss 0.700844.
Test: 2018-08-02T00:28:15.641732: step 25270, loss 0.547939.
Train: 2018-08-02T00:28:15.804322: step 25271, loss 0.527849.
Train: 2018-08-02T00:28:15.966863: step 25272, loss 0.596939.
Train: 2018-08-02T00:28:16.127434: step 25273, loss 0.545161.
Train: 2018-08-02T00:28:16.288005: step 25274, loss 0.545176.
Train: 2018-08-02T00:28:16.462537: step 25275, loss 0.527966.
Train: 2018-08-02T00:28:16.636099: step 25276, loss 0.614055.
Train: 2018-08-02T00:28:16.799662: step 25277, loss 0.476385.
Train: 2018-08-02T00:28:16.962202: step 25278, loss 0.579614.
Train: 2018-08-02T00:28:17.121776: step 25279, loss 0.545192.
Train: 2018-08-02T00:28:17.292347: step 25280, loss 0.579621.
Test: 2018-08-02T00:28:17.827887: step 25280, loss 0.547986.
Train: 2018-08-02T00:28:17.992448: step 25281, loss 0.476314.
Train: 2018-08-02T00:28:18.165008: step 25282, loss 0.596879.
Train: 2018-08-02T00:28:18.328549: step 25283, loss 0.614145.
Train: 2018-08-02T00:28:18.495129: step 25284, loss 0.52562.
Train: 2018-08-02T00:28:18.665679: step 25285, loss 0.614152.
Train: 2018-08-02T00:28:18.840183: step 25286, loss 0.59689.
Train: 2018-08-02T00:28:19.003776: step 25287, loss 0.510719.
Train: 2018-08-02T00:28:19.174320: step 25288, loss 0.510723.
Train: 2018-08-02T00:28:19.334859: step 25289, loss 0.407275.
Train: 2018-08-02T00:28:19.493435: step 25290, loss 0.648797.
Test: 2018-08-02T00:28:20.028006: step 25290, loss 0.547927.
Train: 2018-08-02T00:28:20.189575: step 25291, loss 0.597003.
Train: 2018-08-02T00:28:20.353138: step 25292, loss 0.562411.
Train: 2018-08-02T00:28:20.513709: step 25293, loss 0.527787.
Train: 2018-08-02T00:28:20.682258: step 25294, loss 0.631719.
Train: 2018-08-02T00:28:20.846848: step 25295, loss 0.579737.
Train: 2018-08-02T00:28:21.009383: step 25296, loss 0.597047.
Train: 2018-08-02T00:28:21.166963: step 25297, loss 0.579714.
Train: 2018-08-02T00:28:21.330524: step 25298, loss 0.527835.
Train: 2018-08-02T00:28:21.491096: step 25299, loss 0.52785.
Train: 2018-08-02T00:28:21.663649: step 25300, loss 0.631525.
Test: 2018-08-02T00:28:22.196219: step 25300, loss 0.54795.
Train: 2018-08-02T00:28:22.942330: step 25301, loss 0.579671.
Train: 2018-08-02T00:28:23.103915: step 25302, loss 0.527912.
Train: 2018-08-02T00:28:23.264486: step 25303, loss 0.510691.
Train: 2018-08-02T00:28:23.429049: step 25304, loss 0.562405.
Train: 2018-08-02T00:28:23.590593: step 25305, loss 0.596893.
Train: 2018-08-02T00:28:23.754150: step 25306, loss 0.596882.
Train: 2018-08-02T00:28:23.928715: step 25307, loss 0.682985.
Train: 2018-08-02T00:28:24.093243: step 25308, loss 0.648348.
Train: 2018-08-02T00:28:24.251821: step 25309, loss 0.51099.
Train: 2018-08-02T00:28:24.415415: step 25310, loss 0.528199.
Test: 2018-08-02T00:28:24.944966: step 25310, loss 0.548104.
Train: 2018-08-02T00:28:25.108562: step 25311, loss 0.699028.
Train: 2018-08-02T00:28:25.266135: step 25312, loss 0.596463.
Train: 2018-08-02T00:28:25.427677: step 25313, loss 0.596364.
Train: 2018-08-02T00:28:25.588247: step 25314, loss 0.663946.
Train: 2018-08-02T00:28:25.759821: step 25315, loss 0.562439.
Train: 2018-08-02T00:28:25.919395: step 25316, loss 0.495301.
Train: 2018-08-02T00:28:26.078937: step 25317, loss 0.545719.
Train: 2018-08-02T00:28:26.238542: step 25318, loss 0.595911.
Train: 2018-08-02T00:28:26.399111: step 25319, loss 0.695949.
Train: 2018-08-02T00:28:26.570621: step 25320, loss 0.579136.
Test: 2018-08-02T00:28:27.096248: step 25320, loss 0.548661.
Train: 2018-08-02T00:28:27.261806: step 25321, loss 0.595674.
Train: 2018-08-02T00:28:27.422345: step 25322, loss 0.546048.
Train: 2018-08-02T00:28:27.583913: step 25323, loss 0.447296.
Train: 2018-08-02T00:28:27.744499: step 25324, loss 0.513219.
Train: 2018-08-02T00:28:27.906052: step 25325, loss 0.513212.
Train: 2018-08-02T00:28:28.066653: step 25326, loss 0.546109.
Train: 2018-08-02T00:28:28.235171: step 25327, loss 0.628543.
Train: 2018-08-02T00:28:28.394770: step 25328, loss 0.480087.
Train: 2018-08-02T00:28:28.557344: step 25329, loss 0.562559.
Train: 2018-08-02T00:28:28.719908: step 25330, loss 0.595635.
Test: 2018-08-02T00:28:29.256453: step 25330, loss 0.548674.
Train: 2018-08-02T00:28:29.427983: step 25331, loss 0.529423.
Train: 2018-08-02T00:28:29.596532: step 25332, loss 0.612275.
Train: 2018-08-02T00:28:29.755134: step 25333, loss 0.512747.
Train: 2018-08-02T00:28:29.929676: step 25334, loss 0.612362.
Train: 2018-08-02T00:28:30.092239: step 25335, loss 0.61239.
Train: 2018-08-02T00:28:30.260789: step 25336, loss 0.496004.
Train: 2018-08-02T00:28:30.420329: step 25337, loss 0.512575.
Train: 2018-08-02T00:28:30.594896: step 25338, loss 0.662515.
Train: 2018-08-02T00:28:30.755434: step 25339, loss 0.595841.
Train: 2018-08-02T00:28:30.917003: step 25340, loss 0.512477.
Test: 2018-08-02T00:28:31.444591: step 25340, loss 0.54852.
Train: 2018-08-02T00:28:31.610150: step 25341, loss 0.445715.
Train: 2018-08-02T00:28:31.770747: step 25342, loss 0.646066.
Train: 2018-08-02T00:28:31.940268: step 25343, loss 0.595938.
Train: 2018-08-02T00:28:32.101838: step 25344, loss 0.562469.
Train: 2018-08-02T00:28:32.272410: step 25345, loss 0.512219.
Train: 2018-08-02T00:28:32.433947: step 25346, loss 0.495386.
Train: 2018-08-02T00:28:32.595540: step 25347, loss 0.596054.
Train: 2018-08-02T00:28:32.757084: step 25348, loss 0.528794.
Train: 2018-08-02T00:28:32.920645: step 25349, loss 0.714128.
Train: 2018-08-02T00:28:33.089221: step 25350, loss 0.52874.
Test: 2018-08-02T00:28:33.614801: step 25350, loss 0.548326.
Train: 2018-08-02T00:28:33.779350: step 25351, loss 0.495029.
Train: 2018-08-02T00:28:33.943911: step 25352, loss 0.613045.
Train: 2018-08-02T00:28:34.106507: step 25353, loss 0.545555.
Train: 2018-08-02T00:28:34.271037: step 25354, loss 0.663752.
Train: 2018-08-02T00:28:34.435629: step 25355, loss 0.511807.
Train: 2018-08-02T00:28:34.597165: step 25356, loss 0.545556.
Train: 2018-08-02T00:28:34.759730: step 25357, loss 0.511787.
Train: 2018-08-02T00:28:34.917340: step 25358, loss 0.613121.
Train: 2018-08-02T00:28:35.079874: step 25359, loss 0.545525.
Train: 2018-08-02T00:28:35.241443: step 25360, loss 0.511693.
Test: 2018-08-02T00:28:35.766039: step 25360, loss 0.548245.
Train: 2018-08-02T00:28:35.933623: step 25361, loss 0.613212.
Train: 2018-08-02T00:28:36.096156: step 25362, loss 0.579358.
Train: 2018-08-02T00:28:36.260718: step 25363, loss 0.613241.
Train: 2018-08-02T00:28:36.428270: step 25364, loss 0.630154.
Train: 2018-08-02T00:28:36.593827: step 25365, loss 0.613163.
Train: 2018-08-02T00:28:36.759385: step 25366, loss 0.629967.
Train: 2018-08-02T00:28:36.920952: step 25367, loss 0.545595.
Train: 2018-08-02T00:28:37.085512: step 25368, loss 0.52882.
Train: 2018-08-02T00:28:37.246108: step 25369, loss 0.646422.
Train: 2018-08-02T00:28:37.406654: step 25370, loss 0.562463.
Test: 2018-08-02T00:28:37.941238: step 25370, loss 0.548464.
Train: 2018-08-02T00:28:38.103791: step 25371, loss 0.629387.
Train: 2018-08-02T00:28:38.267353: step 25372, loss 0.595863.
Train: 2018-08-02T00:28:38.435904: step 25373, loss 0.479275.
Train: 2018-08-02T00:28:38.598468: step 25374, loss 0.56251.
Train: 2018-08-02T00:28:38.762055: step 25375, loss 0.512682.
Train: 2018-08-02T00:28:38.933573: step 25376, loss 0.562517.
Train: 2018-08-02T00:28:39.096169: step 25377, loss 0.545906.
Train: 2018-08-02T00:28:39.257705: step 25378, loss 0.645595.
Train: 2018-08-02T00:28:39.419273: step 25379, loss 0.529311.
Train: 2018-08-02T00:28:39.589843: step 25380, loss 0.529315.
Test: 2018-08-02T00:28:40.107445: step 25380, loss 0.548608.
Train: 2018-08-02T00:28:40.277008: step 25381, loss 0.529297.
Train: 2018-08-02T00:28:40.441567: step 25382, loss 0.545885.
Train: 2018-08-02T00:28:40.609119: step 25383, loss 0.529215.
Train: 2018-08-02T00:28:40.771690: step 25384, loss 0.512483.
Train: 2018-08-02T00:28:40.932229: step 25385, loss 0.56248.
Train: 2018-08-02T00:28:41.092831: step 25386, loss 0.612686.
Train: 2018-08-02T00:28:41.257361: step 25387, loss 0.579222.
Train: 2018-08-02T00:28:41.420923: step 25388, loss 0.612782.
Train: 2018-08-02T00:28:41.585483: step 25389, loss 0.478561.
Train: 2018-08-02T00:28:41.758047: step 25390, loss 0.579252.
Test: 2018-08-02T00:28:42.271648: step 25390, loss 0.54836.
Train: 2018-08-02T00:28:42.486443: step 25391, loss 0.511984.
Train: 2018-08-02T00:28:42.648010: step 25392, loss 0.596137.
Train: 2018-08-02T00:28:42.811598: step 25393, loss 0.646771.
Train: 2018-08-02T00:28:42.973166: step 25394, loss 0.545567.
Train: 2018-08-02T00:28:43.134710: step 25395, loss 0.511819.
Train: 2018-08-02T00:28:43.298273: step 25396, loss 0.494883.
Train: 2018-08-02T00:28:43.461835: step 25397, loss 0.613172.
Train: 2018-08-02T00:28:43.622406: step 25398, loss 0.562421.
Train: 2018-08-02T00:28:43.781979: step 25399, loss 0.545471.
Train: 2018-08-02T00:28:43.941552: step 25400, loss 0.528485.
Test: 2018-08-02T00:28:44.472160: step 25400, loss 0.548186.
Train: 2018-08-02T00:28:45.262785: step 25401, loss 0.61338.
Train: 2018-08-02T00:28:45.424354: step 25402, loss 0.613411.
Train: 2018-08-02T00:28:45.588915: step 25403, loss 0.528412.
Train: 2018-08-02T00:28:45.753469: step 25404, loss 0.528397.
Train: 2018-08-02T00:28:45.914044: step 25405, loss 0.596451.
Train: 2018-08-02T00:28:46.077578: step 25406, loss 0.562408.
Train: 2018-08-02T00:28:46.240174: step 25407, loss 0.51131.
Train: 2018-08-02T00:28:46.404727: step 25408, loss 0.545357.
Train: 2018-08-02T00:28:46.568266: step 25409, loss 0.511202.
Train: 2018-08-02T00:28:46.732850: step 25410, loss 0.596595.
Test: 2018-08-02T00:28:47.267396: step 25410, loss 0.548071.
Train: 2018-08-02T00:28:47.432953: step 25411, loss 0.442604.
Train: 2018-08-02T00:28:47.598536: step 25412, loss 0.562402.
Train: 2018-08-02T00:28:47.764069: step 25413, loss 0.476433.
Train: 2018-08-02T00:28:47.925667: step 25414, loss 0.527912.
Train: 2018-08-02T00:28:48.086206: step 25415, loss 0.475909.
Train: 2018-08-02T00:28:48.250766: step 25416, loss 0.718719.
Train: 2018-08-02T00:28:48.413333: step 25417, loss 0.54503.
Train: 2018-08-02T00:28:48.576926: step 25418, loss 0.492749.
Train: 2018-08-02T00:28:48.738463: step 25419, loss 0.614809.
Train: 2018-08-02T00:28:48.908035: step 25420, loss 0.527482.
Test: 2018-08-02T00:28:49.437595: step 25420, loss 0.547789.
Train: 2018-08-02T00:28:49.610158: step 25421, loss 0.474917.
Train: 2018-08-02T00:28:49.774720: step 25422, loss 0.527365.
Train: 2018-08-02T00:28:49.937290: step 25423, loss 0.509702.
Train: 2018-08-02T00:28:50.101837: step 25424, loss 0.474295.
Train: 2018-08-02T00:28:50.264416: step 25425, loss 0.597914.
Train: 2018-08-02T00:28:50.425977: step 25426, loss 0.598025.
Train: 2018-08-02T00:28:50.586522: step 25427, loss 0.580326.
Train: 2018-08-02T00:28:50.753103: step 25428, loss 0.473553.
Train: 2018-08-02T00:28:50.921652: step 25429, loss 0.455544.
Train: 2018-08-02T00:28:51.085220: step 25430, loss 0.634181.
Test: 2018-08-02T00:28:51.604801: step 25430, loss 0.547616.
Train: 2018-08-02T00:28:51.770358: step 25431, loss 0.723951.
Train: 2018-08-02T00:28:51.933951: step 25432, loss 0.473029.
Train: 2018-08-02T00:28:52.100512: step 25433, loss 0.472982.
Train: 2018-08-02T00:28:52.265037: step 25434, loss 0.490825.
Train: 2018-08-02T00:28:52.426604: step 25435, loss 0.670606.
Train: 2018-08-02T00:28:52.589200: step 25436, loss 0.472683.
Train: 2018-08-02T00:28:52.751735: step 25437, loss 0.634766.
Train: 2018-08-02T00:28:52.925270: step 25438, loss 0.652807.
Train: 2018-08-02T00:28:53.091826: step 25439, loss 0.580683.
Train: 2018-08-02T00:28:53.258381: step 25440, loss 0.490723.
Test: 2018-08-02T00:28:53.799934: step 25440, loss 0.547604.
Train: 2018-08-02T00:28:53.970502: step 25441, loss 0.670525.
Train: 2018-08-02T00:28:54.137063: step 25442, loss 0.544692.
Train: 2018-08-02T00:28:54.298632: step 25443, loss 0.544703.
Train: 2018-08-02T00:28:54.470140: step 25444, loss 0.526812.
Train: 2018-08-02T00:28:54.637724: step 25445, loss 0.562604.
Train: 2018-08-02T00:28:54.803250: step 25446, loss 0.598336.
Train: 2018-08-02T00:28:54.963827: step 25447, loss 0.544736.
Train: 2018-08-02T00:28:55.125390: step 25448, loss 0.61606.
Train: 2018-08-02T00:28:55.291970: step 25449, loss 0.52696.
Train: 2018-08-02T00:28:55.456530: step 25450, loss 0.598105.
Test: 2018-08-02T00:28:55.992073: step 25450, loss 0.547672.
Train: 2018-08-02T00:28:56.159624: step 25451, loss 0.598035.
Train: 2018-08-02T00:28:56.333161: step 25452, loss 0.473942.
Train: 2018-08-02T00:28:56.494775: step 25453, loss 0.491708.
Train: 2018-08-02T00:28:56.663310: step 25454, loss 0.509409.
Train: 2018-08-02T00:28:56.831844: step 25455, loss 0.527096.
Train: 2018-08-02T00:28:56.992424: step 25456, loss 0.527073.
Train: 2018-08-02T00:28:57.171919: step 25457, loss 0.615767.
Train: 2018-08-02T00:28:57.340493: step 25458, loss 0.47379.
Train: 2018-08-02T00:28:57.507048: step 25459, loss 0.562544.
Train: 2018-08-02T00:28:57.672614: step 25460, loss 0.562552.
Test: 2018-08-02T00:28:58.203162: step 25460, loss 0.547654.
Train: 2018-08-02T00:28:58.365726: step 25461, loss 0.473562.
Train: 2018-08-02T00:28:58.531285: step 25462, loss 0.491263.
Train: 2018-08-02T00:28:58.691881: step 25463, loss 0.526861.
Train: 2018-08-02T00:28:58.859408: step 25464, loss 0.544709.
Train: 2018-08-02T00:28:59.023992: step 25465, loss 0.634407.
Train: 2018-08-02T00:28:59.188528: step 25466, loss 0.490809.
Train: 2018-08-02T00:28:59.360094: step 25467, loss 0.508703.
Train: 2018-08-02T00:28:59.522665: step 25468, loss 0.526644.
Train: 2018-08-02T00:28:59.692208: step 25469, loss 0.526598.
Train: 2018-08-02T00:28:59.857769: step 25470, loss 0.436093.
Test: 2018-08-02T00:29:00.384331: step 25470, loss 0.547578.
Train: 2018-08-02T00:29:00.561856: step 25471, loss 0.490186.
Train: 2018-08-02T00:29:00.726440: step 25472, loss 0.544613.
Train: 2018-08-02T00:29:00.893969: step 25473, loss 0.635928.
Train: 2018-08-02T00:29:01.055537: step 25474, loss 0.581194.
Train: 2018-08-02T00:29:01.214143: step 25475, loss 0.452989.
Train: 2018-08-02T00:29:01.378700: step 25476, loss 0.618032.
Train: 2018-08-02T00:29:01.545259: step 25477, loss 0.507825.
Train: 2018-08-02T00:29:01.708791: step 25478, loss 0.581403.
Train: 2018-08-02T00:29:01.871356: step 25479, loss 0.544586.
Train: 2018-08-02T00:29:02.038938: step 25480, loss 0.618348.
Test: 2018-08-02T00:29:02.565540: step 25480, loss 0.547584.
Train: 2018-08-02T00:29:02.728066: step 25481, loss 0.636785.
Train: 2018-08-02T00:29:02.896641: step 25482, loss 0.526164.
Train: 2018-08-02T00:29:03.068155: step 25483, loss 0.618221.
Train: 2018-08-02T00:29:03.232747: step 25484, loss 0.434293.
Train: 2018-08-02T00:29:03.399272: step 25485, loss 0.654876.
Train: 2018-08-02T00:29:03.565831: step 25486, loss 0.562949.
Train: 2018-08-02T00:29:03.725430: step 25487, loss 0.562928.
Train: 2018-08-02T00:29:03.890956: step 25488, loss 0.526285.
Train: 2018-08-02T00:29:04.054519: step 25489, loss 0.508013.
Train: 2018-08-02T00:29:04.227059: step 25490, loss 0.599452.
Test: 2018-08-02T00:29:04.754649: step 25490, loss 0.547573.
Train: 2018-08-02T00:29:04.919209: step 25491, loss 0.526336.
Train: 2018-08-02T00:29:05.091778: step 25492, loss 0.581113.
Train: 2018-08-02T00:29:05.256339: step 25493, loss 0.617554.
Train: 2018-08-02T00:29:05.428883: step 25494, loss 0.635649.
Train: 2018-08-02T00:29:05.594434: step 25495, loss 0.635439.
Train: 2018-08-02T00:29:05.759960: step 25496, loss 0.653276.
Train: 2018-08-02T00:29:05.919564: step 25497, loss 0.454488.
Train: 2018-08-02T00:29:06.089106: step 25498, loss 0.508697.
Train: 2018-08-02T00:29:06.249701: step 25499, loss 0.562645.
Train: 2018-08-02T00:29:06.415239: step 25500, loss 0.544701.
Test: 2018-08-02T00:29:06.952771: step 25500, loss 0.547623.
Train: 2018-08-02T00:29:07.749315: step 25501, loss 0.58051.
Train: 2018-08-02T00:29:07.912879: step 25502, loss 0.526856.
Train: 2018-08-02T00:29:08.079432: step 25503, loss 0.544735.
Train: 2018-08-02T00:29:08.242024: step 25504, loss 0.562574.
Train: 2018-08-02T00:29:08.408586: step 25505, loss 0.580378.
Train: 2018-08-02T00:29:08.572146: step 25506, loss 0.598138.
Train: 2018-08-02T00:29:08.737672: step 25507, loss 0.562541.
Train: 2018-08-02T00:29:08.906222: step 25508, loss 0.544793.
Train: 2018-08-02T00:29:09.067821: step 25509, loss 0.491664.
Train: 2018-08-02T00:29:09.240330: step 25510, loss 0.65105.
Test: 2018-08-02T00:29:09.768917: step 25510, loss 0.5477.
Train: 2018-08-02T00:29:09.936469: step 25511, loss 0.491783.
Train: 2018-08-02T00:29:10.100032: step 25512, loss 0.668522.
Train: 2018-08-02T00:29:10.264597: step 25513, loss 0.580127.
Train: 2018-08-02T00:29:10.431147: step 25514, loss 0.456858.
Train: 2018-08-02T00:29:10.596729: step 25515, loss 0.562473.
Train: 2018-08-02T00:29:10.761289: step 25516, loss 0.544888.
Train: 2018-08-02T00:29:10.923860: step 25517, loss 0.527317.
Train: 2018-08-02T00:29:11.097365: step 25518, loss 0.562468.
Train: 2018-08-02T00:29:11.271898: step 25519, loss 0.527319.
Train: 2018-08-02T00:29:11.436459: step 25520, loss 0.562469.
Test: 2018-08-02T00:29:11.976017: step 25520, loss 0.547747.
Train: 2018-08-02T00:29:12.142597: step 25521, loss 0.580052.
Train: 2018-08-02T00:29:12.310123: step 25522, loss 0.544889.
Train: 2018-08-02T00:29:12.475701: step 25523, loss 0.456983.
Train: 2018-08-02T00:29:12.639269: step 25524, loss 0.562477.
Train: 2018-08-02T00:29:12.800811: step 25525, loss 0.597724.
Train: 2018-08-02T00:29:12.963376: step 25526, loss 0.61537.
Train: 2018-08-02T00:29:13.125976: step 25527, loss 0.544861.
Train: 2018-08-02T00:29:13.293524: step 25528, loss 0.527242.
Train: 2018-08-02T00:29:13.455087: step 25529, loss 0.650604.
Train: 2018-08-02T00:29:13.621617: step 25530, loss 0.580086.
Test: 2018-08-02T00:29:14.160179: step 25530, loss 0.547744.
Train: 2018-08-02T00:29:14.333713: step 25531, loss 0.632825.
Train: 2018-08-02T00:29:14.499304: step 25532, loss 0.562461.
Train: 2018-08-02T00:29:14.663841: step 25533, loss 0.50988.
Train: 2018-08-02T00:29:14.825399: step 25534, loss 0.544941.
Train: 2018-08-02T00:29:14.986979: step 25535, loss 0.562444.
Train: 2018-08-02T00:29:15.149559: step 25536, loss 0.579919.
Train: 2018-08-02T00:29:15.322071: step 25537, loss 0.527511.
Train: 2018-08-02T00:29:15.487629: step 25538, loss 0.632251.
Train: 2018-08-02T00:29:15.652190: step 25539, loss 0.632154.
Train: 2018-08-02T00:29:15.820739: step 25540, loss 0.510237.
Test: 2018-08-02T00:29:16.359299: step 25540, loss 0.547871.
Train: 2018-08-02T00:29:16.527849: step 25541, loss 0.492926.
Train: 2018-08-02T00:29:16.694427: step 25542, loss 0.597152.
Train: 2018-08-02T00:29:16.866966: step 25543, loss 0.597123.
Train: 2018-08-02T00:29:17.030536: step 25544, loss 0.631749.
Train: 2018-08-02T00:29:17.193103: step 25545, loss 0.631613.
Train: 2018-08-02T00:29:17.361620: step 25546, loss 0.458866.
Train: 2018-08-02T00:29:17.527176: step 25547, loss 0.545165.
Train: 2018-08-02T00:29:17.690764: step 25548, loss 0.596851.
Train: 2018-08-02T00:29:17.855333: step 25549, loss 0.49358.
Train: 2018-08-02T00:29:18.020857: step 25550, loss 0.682817.
Test: 2018-08-02T00:29:18.564404: step 25550, loss 0.548019.
Train: 2018-08-02T00:29:18.728994: step 25551, loss 0.631096.
Train: 2018-08-02T00:29:18.896516: step 25552, loss 0.579534.
Train: 2018-08-02T00:29:19.074041: step 25553, loss 0.545308.
Train: 2018-08-02T00:29:19.241594: step 25554, loss 0.494157.
Train: 2018-08-02T00:29:19.408174: step 25555, loss 0.562405.
Train: 2018-08-02T00:29:19.574730: step 25556, loss 0.511299.
Train: 2018-08-02T00:29:19.741284: step 25557, loss 0.528334.
Train: 2018-08-02T00:29:19.912844: step 25558, loss 0.579449.
Train: 2018-08-02T00:29:20.080377: step 25559, loss 0.545357.
Train: 2018-08-02T00:29:20.248903: step 25560, loss 0.579459.
Test: 2018-08-02T00:29:20.782476: step 25560, loss 0.54812.
Train: 2018-08-02T00:29:20.951057: step 25561, loss 0.562404.
Train: 2018-08-02T00:29:21.131541: step 25562, loss 0.613589.
Train: 2018-08-02T00:29:21.300118: step 25563, loss 0.562404.
Train: 2018-08-02T00:29:21.469668: step 25564, loss 0.579452.
Train: 2018-08-02T00:29:21.634231: step 25565, loss 0.52833.
Train: 2018-08-02T00:29:21.798788: step 25566, loss 0.613516.
Train: 2018-08-02T00:29:21.963343: step 25567, loss 0.528356.
Train: 2018-08-02T00:29:22.128876: step 25568, loss 0.596453.
Train: 2018-08-02T00:29:22.301413: step 25569, loss 0.613449.
Train: 2018-08-02T00:29:22.471958: step 25570, loss 0.477437.
Test: 2018-08-02T00:29:23.005567: step 25570, loss 0.548178.
Train: 2018-08-02T00:29:23.174081: step 25571, loss 0.630396.
Train: 2018-08-02T00:29:23.339671: step 25572, loss 0.562412.
Train: 2018-08-02T00:29:23.508219: step 25573, loss 0.528467.
Train: 2018-08-02T00:29:23.673776: step 25574, loss 0.613326.
Train: 2018-08-02T00:29:23.841328: step 25575, loss 0.647209.
Train: 2018-08-02T00:29:24.009875: step 25576, loss 0.630139.
Train: 2018-08-02T00:29:24.177399: step 25577, loss 0.613097.
Train: 2018-08-02T00:29:24.343953: step 25578, loss 0.579282.
Train: 2018-08-02T00:29:24.512531: step 25579, loss 0.478454.
Train: 2018-08-02T00:29:24.683047: step 25580, loss 0.596012.
Test: 2018-08-02T00:29:25.222605: step 25580, loss 0.548434.
Train: 2018-08-02T00:29:25.388165: step 25581, loss 0.629478.
Train: 2018-08-02T00:29:25.553720: step 25582, loss 0.462167.
Train: 2018-08-02T00:29:25.715288: step 25583, loss 0.646022.
Train: 2018-08-02T00:29:25.888824: step 25584, loss 0.479068.
Train: 2018-08-02T00:29:26.056407: step 25585, loss 0.544695.
Train: 2018-08-02T00:29:26.222958: step 25586, loss 0.445703.
Train: 2018-08-02T00:29:26.401453: step 25587, loss 0.612614.
Train: 2018-08-02T00:29:26.581970: step 25588, loss 0.512283.
Train: 2018-08-02T00:29:26.751518: step 25589, loss 0.545705.
Train: 2018-08-02T00:29:26.922089: step 25590, loss 0.562453.
Test: 2018-08-02T00:29:27.459624: step 25590, loss 0.548367.
Train: 2018-08-02T00:29:27.633161: step 25591, loss 0.545632.
Train: 2018-08-02T00:29:27.799740: step 25592, loss 0.528753.
Train: 2018-08-02T00:29:27.966271: step 25593, loss 0.545553.
Train: 2018-08-02T00:29:28.133822: step 25594, loss 0.562423.
Train: 2018-08-02T00:29:28.298409: step 25595, loss 0.494635.
Train: 2018-08-02T00:29:28.468957: step 25596, loss 0.579401.
Train: 2018-08-02T00:29:28.641491: step 25597, loss 0.613489.
Train: 2018-08-02T00:29:28.806052: step 25598, loss 0.562404.
Train: 2018-08-02T00:29:28.972608: step 25599, loss 0.630687.
Train: 2018-08-02T00:29:29.138138: step 25600, loss 0.477026.
Test: 2018-08-02T00:29:29.670714: step 25600, loss 0.548084.
Train: 2018-08-02T00:29:30.429893: step 25601, loss 0.494011.
Train: 2018-08-02T00:29:30.598475: step 25602, loss 0.630931.
Train: 2018-08-02T00:29:30.768988: step 25603, loss 0.699597.
Train: 2018-08-02T00:29:30.938561: step 25604, loss 0.613807.
Train: 2018-08-02T00:29:31.113067: step 25605, loss 0.493953.
Train: 2018-08-02T00:29:31.278657: step 25606, loss 0.545296.
Train: 2018-08-02T00:29:31.445210: step 25607, loss 0.476884.
Train: 2018-08-02T00:29:31.612757: step 25608, loss 0.5624.
Train: 2018-08-02T00:29:31.774330: step 25609, loss 0.476725.
Train: 2018-08-02T00:29:31.939857: step 25610, loss 0.5624.
Test: 2018-08-02T00:29:32.471434: step 25610, loss 0.548001.
Train: 2018-08-02T00:29:32.642976: step 25611, loss 0.579596.
Train: 2018-08-02T00:29:32.812580: step 25612, loss 0.596837.
Train: 2018-08-02T00:29:32.979078: step 25613, loss 0.510711.
Train: 2018-08-02T00:29:33.157600: step 25614, loss 0.579656.
Train: 2018-08-02T00:29:33.328175: step 25615, loss 0.579673.
Train: 2018-08-02T00:29:33.493733: step 25616, loss 0.458736.
Train: 2018-08-02T00:29:33.664246: step 25617, loss 0.597027.
Train: 2018-08-02T00:29:33.829837: step 25618, loss 0.649057.
Train: 2018-08-02T00:29:33.996358: step 25619, loss 0.475769.
Train: 2018-08-02T00:29:34.160949: step 25620, loss 0.475686.
Test: 2018-08-02T00:29:34.692498: step 25620, loss 0.547866.
Train: 2018-08-02T00:29:34.856068: step 25621, loss 0.597176.
Train: 2018-08-02T00:29:35.023641: step 25622, loss 0.597224.
Train: 2018-08-02T00:29:35.190198: step 25623, loss 0.562425.
Train: 2018-08-02T00:29:35.358745: step 25624, loss 0.527582.
Train: 2018-08-02T00:29:35.521283: step 25625, loss 0.579868.
Train: 2018-08-02T00:29:35.690877: step 25626, loss 0.614774.
Train: 2018-08-02T00:29:35.863368: step 25627, loss 0.667094.
Train: 2018-08-02T00:29:36.026955: step 25628, loss 0.562426.
Train: 2018-08-02T00:29:36.193511: step 25629, loss 0.614604.
Train: 2018-08-02T00:29:36.356079: step 25630, loss 0.614498.
Test: 2018-08-02T00:29:36.878653: step 25630, loss 0.547907.
Train: 2018-08-02T00:29:37.047236: step 25631, loss 0.700965.
Train: 2018-08-02T00:29:37.216776: step 25632, loss 0.631413.
Train: 2018-08-02T00:29:37.382332: step 25633, loss 0.528045.
Train: 2018-08-02T00:29:37.549859: step 25634, loss 0.545284.
Train: 2018-08-02T00:29:37.715450: step 25635, loss 0.545337.
Train: 2018-08-02T00:29:37.884996: step 25636, loss 0.511333.
Train: 2018-08-02T00:29:38.049524: step 25637, loss 0.545411.
Train: 2018-08-02T00:29:38.216110: step 25638, loss 0.59637.
Train: 2018-08-02T00:29:38.386648: step 25639, loss 0.57937.
Train: 2018-08-02T00:29:38.560184: step 25640, loss 0.579349.
Test: 2018-08-02T00:29:39.096730: step 25640, loss 0.548269.
Train: 2018-08-02T00:29:39.266302: step 25641, loss 0.54552.
Train: 2018-08-02T00:29:39.429858: step 25642, loss 0.461115.
Train: 2018-08-02T00:29:39.594394: step 25643, loss 0.613102.
Train: 2018-08-02T00:29:39.756991: step 25644, loss 0.477994.
Train: 2018-08-02T00:29:39.922550: step 25645, loss 0.444114.
Train: 2018-08-02T00:29:40.089071: step 25646, loss 0.630182.
Train: 2018-08-02T00:29:40.254662: step 25647, loss 0.579377.
Train: 2018-08-02T00:29:40.417194: step 25648, loss 0.477516.
Train: 2018-08-02T00:29:40.580756: step 25649, loss 0.545395.
Train: 2018-08-02T00:29:40.754292: step 25650, loss 0.698779.
Test: 2018-08-02T00:29:41.298839: step 25650, loss 0.548128.
Train: 2018-08-02T00:29:41.468415: step 25651, loss 0.443063.
Train: 2018-08-02T00:29:41.639957: step 25652, loss 0.613631.
Train: 2018-08-02T00:29:41.807497: step 25653, loss 0.562401.
Train: 2018-08-02T00:29:41.981044: step 25654, loss 0.61371.
Train: 2018-08-02T00:29:42.146590: step 25655, loss 0.545297.
Train: 2018-08-02T00:29:42.311156: step 25656, loss 0.528184.
Train: 2018-08-02T00:29:42.474724: step 25657, loss 0.493922.
Train: 2018-08-02T00:29:42.642245: step 25658, loss 0.648123.
Train: 2018-08-02T00:29:42.809798: step 25659, loss 0.528102.
Train: 2018-08-02T00:29:42.975358: step 25660, loss 0.57956.
Test: 2018-08-02T00:29:43.516908: step 25660, loss 0.548025.
Train: 2018-08-02T00:29:43.689471: step 25661, loss 0.579566.
Train: 2018-08-02T00:29:43.856999: step 25662, loss 0.5624.
Train: 2018-08-02T00:29:44.035552: step 25663, loss 0.5624.
Train: 2018-08-02T00:29:44.203098: step 25664, loss 0.596744.
Train: 2018-08-02T00:29:44.364642: step 25665, loss 0.5624.
Train: 2018-08-02T00:29:44.532194: step 25666, loss 0.665365.
Train: 2018-08-02T00:29:44.701769: step 25667, loss 0.665204.
Train: 2018-08-02T00:29:44.879266: step 25668, loss 0.511137.
Train: 2018-08-02T00:29:45.045820: step 25669, loss 0.545345.
Train: 2018-08-02T00:29:45.215400: step 25670, loss 0.613509.
Test: 2018-08-02T00:29:45.739964: step 25670, loss 0.54817.
Train: 2018-08-02T00:29:45.905522: step 25671, loss 0.494396.
Train: 2018-08-02T00:29:46.074120: step 25672, loss 0.54542.
Train: 2018-08-02T00:29:46.238633: step 25673, loss 0.596375.
Train: 2018-08-02T00:29:46.419196: step 25674, loss 0.579381.
Train: 2018-08-02T00:29:46.592685: step 25675, loss 0.59632.
Train: 2018-08-02T00:29:46.754254: step 25676, loss 0.562419.
Train: 2018-08-02T00:29:46.917841: step 25677, loss 0.731557.
Train: 2018-08-02T00:29:47.085400: step 25678, loss 0.613017.
Train: 2018-08-02T00:29:47.253948: step 25679, loss 0.461621.
Train: 2018-08-02T00:29:47.420473: step 25680, loss 0.545679.
Test: 2018-08-02T00:29:47.942079: step 25680, loss 0.548431.
Train: 2018-08-02T00:29:48.118607: step 25681, loss 0.612727.
Train: 2018-08-02T00:29:48.283198: step 25682, loss 0.529018.
Train: 2018-08-02T00:29:48.447757: step 25683, loss 0.529058.
Train: 2018-08-02T00:29:48.620290: step 25684, loss 0.478967.
Train: 2018-08-02T00:29:48.785869: step 25685, loss 0.595906.
Train: 2018-08-02T00:29:48.952409: step 25686, loss 0.579194.
Train: 2018-08-02T00:29:49.118933: step 25687, loss 0.428674.
Train: 2018-08-02T00:29:49.284522: step 25688, loss 0.612734.
Train: 2018-08-02T00:29:49.451045: step 25689, loss 0.495345.
Train: 2018-08-02T00:29:49.616628: step 25690, loss 0.612879.
Test: 2018-08-02T00:29:50.160179: step 25690, loss 0.548346.
Train: 2018-08-02T00:29:50.334683: step 25691, loss 0.528778.
Train: 2018-08-02T00:29:50.500270: step 25692, loss 0.66358.
Train: 2018-08-02T00:29:50.665797: step 25693, loss 0.562433.
Train: 2018-08-02T00:29:50.835344: step 25694, loss 0.629888.
Train: 2018-08-02T00:29:50.997928: step 25695, loss 0.629847.
Train: 2018-08-02T00:29:51.166490: step 25696, loss 0.562441.
Train: 2018-08-02T00:29:51.342021: step 25697, loss 0.512018.
Train: 2018-08-02T00:29:51.512534: step 25698, loss 0.444825.
Train: 2018-08-02T00:29:51.685097: step 25699, loss 0.478322.
Train: 2018-08-02T00:29:51.851627: step 25700, loss 0.444399.
Test: 2018-08-02T00:29:52.380214: step 25700, loss 0.54825.
Train: 2018-08-02T00:29:53.156463: step 25701, loss 0.545498.
Train: 2018-08-02T00:29:53.321047: step 25702, loss 0.562411.
Train: 2018-08-02T00:29:53.490569: step 25703, loss 0.681643.
Train: 2018-08-02T00:29:53.671088: step 25704, loss 0.613574.
Train: 2018-08-02T00:29:53.839636: step 25705, loss 0.425869.
Train: 2018-08-02T00:29:54.003224: step 25706, loss 0.545296.
Train: 2018-08-02T00:29:54.165792: step 25707, loss 0.562399.
Train: 2018-08-02T00:29:54.338328: step 25708, loss 0.528045.
Train: 2018-08-02T00:29:54.506853: step 25709, loss 0.545185.
Train: 2018-08-02T00:29:54.679392: step 25710, loss 0.510644.
Test: 2018-08-02T00:29:55.212984: step 25710, loss 0.547922.
Train: 2018-08-02T00:29:55.380517: step 25711, loss 0.579705.
Train: 2018-08-02T00:29:55.544104: step 25712, loss 0.579745.
Train: 2018-08-02T00:29:55.709637: step 25713, loss 0.701313.
Train: 2018-08-02T00:29:55.886196: step 25714, loss 0.545057.
Train: 2018-08-02T00:29:56.053751: step 25715, loss 0.579772.
Train: 2018-08-02T00:29:56.222300: step 25716, loss 0.475649.
Train: 2018-08-02T00:29:56.388852: step 25717, loss 0.527683.
Train: 2018-08-02T00:29:56.552415: step 25718, loss 0.510264.
Train: 2018-08-02T00:29:56.712986: step 25719, loss 0.492776.
Train: 2018-08-02T00:29:56.883513: step 25720, loss 0.649683.
Test: 2018-08-02T00:29:57.416075: step 25720, loss 0.54781.
Train: 2018-08-02T00:29:57.582630: step 25721, loss 0.54497.
Train: 2018-08-02T00:29:57.749216: step 25722, loss 0.544958.
Train: 2018-08-02T00:29:57.913746: step 25723, loss 0.492445.
Train: 2018-08-02T00:29:58.092268: step 25724, loss 0.579982.
Train: 2018-08-02T00:29:58.257856: step 25725, loss 0.650215.
Train: 2018-08-02T00:29:58.430364: step 25726, loss 0.527355.
Train: 2018-08-02T00:29:58.602921: step 25727, loss 0.527344.
Train: 2018-08-02T00:29:58.771452: step 25728, loss 0.562465.
Train: 2018-08-02T00:29:58.943024: step 25729, loss 0.492147.
Train: 2018-08-02T00:29:59.115533: step 25730, loss 0.562475.
Test: 2018-08-02T00:29:59.645126: step 25730, loss 0.547726.
Train: 2018-08-02T00:29:59.809689: step 25731, loss 0.615344.
Train: 2018-08-02T00:29:59.982215: step 25732, loss 0.562483.
Train: 2018-08-02T00:30:00.162760: step 25733, loss 0.650626.
Train: 2018-08-02T00:30:00.328291: step 25734, loss 0.492033.
Train: 2018-08-02T00:30:00.506813: step 25735, loss 0.509649.
Train: 2018-08-02T00:30:00.675401: step 25736, loss 0.527246.
Train: 2018-08-02T00:30:00.839947: step 25737, loss 0.685898.
Train: 2018-08-02T00:30:00.999526: step 25738, loss 0.509633.
Train: 2018-08-02T00:30:01.181046: step 25739, loss 0.562478.
Train: 2018-08-02T00:30:01.358537: step 25740, loss 0.597691.
Test: 2018-08-02T00:30:01.900098: step 25740, loss 0.547739.
Train: 2018-08-02T00:30:02.063677: step 25741, loss 0.597661.
Train: 2018-08-02T00:30:02.231203: step 25742, loss 0.632763.
Train: 2018-08-02T00:30:02.395789: step 25743, loss 0.492293.
Train: 2018-08-02T00:30:02.562318: step 25744, loss 0.632546.
Train: 2018-08-02T00:30:02.735880: step 25745, loss 0.614922.
Train: 2018-08-02T00:30:02.909420: step 25746, loss 0.527525.
Train: 2018-08-02T00:30:03.077940: step 25747, loss 0.527577.
Train: 2018-08-02T00:30:03.239533: step 25748, loss 0.527613.
Train: 2018-08-02T00:30:03.402099: step 25749, loss 0.631996.
Train: 2018-08-02T00:30:03.569652: step 25750, loss 0.545048.
Test: 2018-08-02T00:30:04.106191: step 25750, loss 0.547886.
Train: 2018-08-02T00:30:04.274769: step 25751, loss 0.458319.
Train: 2018-08-02T00:30:04.451269: step 25752, loss 0.493003.
Train: 2018-08-02T00:30:04.612872: step 25753, loss 0.527677.
Train: 2018-08-02T00:30:04.783406: step 25754, loss 0.597204.
Train: 2018-08-02T00:30:04.942980: step 25755, loss 0.492806.
Train: 2018-08-02T00:30:05.109540: step 25756, loss 0.510141.
Train: 2018-08-02T00:30:05.329921: step 25757, loss 0.527512.
Train: 2018-08-02T00:30:05.495505: step 25758, loss 0.597437.
Train: 2018-08-02T00:30:05.668017: step 25759, loss 0.615011.
Train: 2018-08-02T00:30:05.832606: step 25760, loss 0.509862.
Test: 2018-08-02T00:30:06.373131: step 25760, loss 0.547763.
Train: 2018-08-02T00:30:06.543676: step 25761, loss 0.6502.
Train: 2018-08-02T00:30:06.708263: step 25762, loss 0.63264.
Train: 2018-08-02T00:30:06.873823: step 25763, loss 0.457286.
Train: 2018-08-02T00:30:07.039381: step 25764, loss 0.650114.
Train: 2018-08-02T00:30:07.203941: step 25765, loss 0.49238.
Train: 2018-08-02T00:30:07.369492: step 25766, loss 0.650034.
Train: 2018-08-02T00:30:07.535051: step 25767, loss 0.527448.
Train: 2018-08-02T00:30:07.699586: step 25768, loss 0.614901.
Train: 2018-08-02T00:30:07.865143: step 25769, loss 0.562435.
Train: 2018-08-02T00:30:08.025739: step 25770, loss 0.492652.
Test: 2018-08-02T00:30:08.565288: step 25770, loss 0.547826.
Train: 2018-08-02T00:30:08.732854: step 25771, loss 0.59731.
Train: 2018-08-02T00:30:08.899378: step 25772, loss 0.597283.
Train: 2018-08-02T00:30:09.066955: step 25773, loss 0.45797.
Train: 2018-08-02T00:30:09.231521: step 25774, loss 0.510185.
Train: 2018-08-02T00:30:09.395077: step 25775, loss 0.49272.
Train: 2018-08-02T00:30:09.564599: step 25776, loss 0.597338.
Train: 2018-08-02T00:30:09.735169: step 25777, loss 0.457626.
Train: 2018-08-02T00:30:09.901724: step 25778, loss 0.509936.
Train: 2018-08-02T00:30:10.069302: step 25779, loss 0.562457.
Train: 2018-08-02T00:30:10.234808: step 25780, loss 0.703111.
Test: 2018-08-02T00:30:10.768382: step 25780, loss 0.547745.
Train: 2018-08-02T00:30:10.932968: step 25781, loss 0.562468.
Train: 2018-08-02T00:30:11.095532: step 25782, loss 0.474551.
Train: 2018-08-02T00:30:11.259100: step 25783, loss 0.439266.
Train: 2018-08-02T00:30:11.429614: step 25784, loss 0.668335.
Train: 2018-08-02T00:30:11.606142: step 25785, loss 0.650764.
Train: 2018-08-02T00:30:11.769735: step 25786, loss 0.615429.
Train: 2018-08-02T00:30:11.949224: step 25787, loss 0.491975.
Train: 2018-08-02T00:30:12.112813: step 25788, loss 0.685843.
Train: 2018-08-02T00:30:12.279373: step 25789, loss 0.439322.
Train: 2018-08-02T00:30:12.444909: step 25790, loss 0.492112.
Test: 2018-08-02T00:30:12.980468: step 25790, loss 0.547736.
Train: 2018-08-02T00:30:13.148045: step 25791, loss 0.650474.
Train: 2018-08-02T00:30:13.314603: step 25792, loss 0.597651.
Train: 2018-08-02T00:30:13.491103: step 25793, loss 0.597611.
Train: 2018-08-02T00:30:13.667633: step 25794, loss 0.597556.
Train: 2018-08-02T00:30:13.833189: step 25795, loss 0.422296.
Train: 2018-08-02T00:30:13.999769: step 25796, loss 0.527409.
Train: 2018-08-02T00:30:14.172282: step 25797, loss 0.562451.
Train: 2018-08-02T00:30:14.337864: step 25798, loss 0.509855.
Train: 2018-08-02T00:30:14.501432: step 25799, loss 0.562457.
Train: 2018-08-02T00:30:14.672968: step 25800, loss 0.58002.
Test: 2018-08-02T00:30:15.205529: step 25800, loss 0.547754.
Train: 2018-08-02T00:30:15.949189: step 25801, loss 0.597593.
Train: 2018-08-02T00:30:16.111729: step 25802, loss 0.509774.
Train: 2018-08-02T00:30:16.280278: step 25803, loss 0.632743.
Train: 2018-08-02T00:30:16.458812: step 25804, loss 0.597582.
Train: 2018-08-02T00:30:16.624359: step 25805, loss 0.457191.
Train: 2018-08-02T00:30:16.792939: step 25806, loss 0.509808.
Train: 2018-08-02T00:30:16.958496: step 25807, loss 0.544898.
Train: 2018-08-02T00:30:17.128018: step 25808, loss 0.509726.
Train: 2018-08-02T00:30:17.294566: step 25809, loss 0.632893.
Train: 2018-08-02T00:30:17.457133: step 25810, loss 0.580088.
Test: 2018-08-02T00:30:17.995700: step 25810, loss 0.547731.
Train: 2018-08-02T00:30:18.159275: step 25811, loss 0.685753.
Train: 2018-08-02T00:30:18.333820: step 25812, loss 0.615223.
Train: 2018-08-02T00:30:18.500378: step 25813, loss 0.615106.
Train: 2018-08-02T00:30:18.666899: step 25814, loss 0.579951.
Train: 2018-08-02T00:30:18.832457: step 25815, loss 0.527511.
Train: 2018-08-02T00:30:18.997016: step 25816, loss 0.614712.
Train: 2018-08-02T00:30:19.164569: step 25817, loss 0.492871.
Train: 2018-08-02T00:30:19.329128: step 25818, loss 0.527688.
Train: 2018-08-02T00:30:19.494717: step 25819, loss 0.579762.
Train: 2018-08-02T00:30:19.657252: step 25820, loss 0.614406.
Test: 2018-08-02T00:30:20.186835: step 25820, loss 0.547916.
Train: 2018-08-02T00:30:20.357379: step 25821, loss 0.648932.
Train: 2018-08-02T00:30:20.522937: step 25822, loss 0.717765.
Train: 2018-08-02T00:30:20.689491: step 25823, loss 0.562399.
Train: 2018-08-02T00:30:20.856046: step 25824, loss 0.511015.
Train: 2018-08-02T00:30:21.018612: step 25825, loss 0.528238.
Train: 2018-08-02T00:30:21.183173: step 25826, loss 0.443076.
Train: 2018-08-02T00:30:21.350724: step 25827, loss 0.562404.
Train: 2018-08-02T00:30:21.514286: step 25828, loss 0.596477.
Train: 2018-08-02T00:30:21.683833: step 25829, loss 0.528355.
Train: 2018-08-02T00:30:21.855375: step 25830, loss 0.460268.
Test: 2018-08-02T00:30:22.385956: step 25830, loss 0.548132.
Train: 2018-08-02T00:30:22.551514: step 25831, loss 0.528319.
Train: 2018-08-02T00:30:22.713115: step 25832, loss 0.630673.
Train: 2018-08-02T00:30:22.877643: step 25833, loss 0.596554.
Train: 2018-08-02T00:30:23.056196: step 25834, loss 0.613632.
Train: 2018-08-02T00:30:23.225711: step 25835, loss 0.716007.
Train: 2018-08-02T00:30:23.390273: step 25836, loss 0.664563.
Train: 2018-08-02T00:30:23.556838: step 25837, loss 0.596349.
Train: 2018-08-02T00:30:23.723381: step 25838, loss 0.579332.
Train: 2018-08-02T00:30:23.888939: step 25839, loss 0.495017.
Train: 2018-08-02T00:30:24.055494: step 25840, loss 0.495166.
Test: 2018-08-02T00:30:24.586077: step 25840, loss 0.548376.
Train: 2018-08-02T00:30:24.755622: step 25841, loss 0.57925.
Train: 2018-08-02T00:30:24.937137: step 25842, loss 0.596023.
Train: 2018-08-02T00:30:25.113698: step 25843, loss 0.629513.
Train: 2018-08-02T00:30:25.280250: step 25844, loss 0.44535.
Train: 2018-08-02T00:30:25.441788: step 25845, loss 0.595925.
Train: 2018-08-02T00:30:25.606347: step 25846, loss 0.579191.
Train: 2018-08-02T00:30:25.767925: step 25847, loss 0.545767.
Train: 2018-08-02T00:30:25.930482: step 25848, loss 0.545773.
Train: 2018-08-02T00:30:26.093046: step 25849, loss 0.595887.
Train: 2018-08-02T00:30:26.259626: step 25850, loss 0.545779.
Test: 2018-08-02T00:30:26.789186: step 25850, loss 0.548495.
Train: 2018-08-02T00:30:26.956737: step 25851, loss 0.495679.
Train: 2018-08-02T00:30:27.122296: step 25852, loss 0.478894.
Train: 2018-08-02T00:30:27.287879: step 25853, loss 0.579212.
Train: 2018-08-02T00:30:27.452443: step 25854, loss 0.562454.
Train: 2018-08-02T00:30:27.624951: step 25855, loss 0.512039.
Train: 2018-08-02T00:30:27.791530: step 25856, loss 0.562437.
Train: 2018-08-02T00:30:27.965077: step 25857, loss 0.56243.
Train: 2018-08-02T00:30:28.129602: step 25858, loss 0.511724.
Train: 2018-08-02T00:30:28.296158: step 25859, loss 0.697925.
Train: 2018-08-02T00:30:28.458722: step 25860, loss 0.545471.
Test: 2018-08-02T00:30:28.992327: step 25860, loss 0.548216.
Train: 2018-08-02T00:30:29.160876: step 25861, loss 0.647189.
Train: 2018-08-02T00:30:29.327400: step 25862, loss 0.494629.
Train: 2018-08-02T00:30:29.495950: step 25863, loss 0.528506.
Train: 2018-08-02T00:30:29.663528: step 25864, loss 0.494538.
Train: 2018-08-02T00:30:29.828087: step 25865, loss 0.562408.
Train: 2018-08-02T00:30:29.993650: step 25866, loss 0.596449.
Train: 2018-08-02T00:30:30.173165: step 25867, loss 0.647593.
Train: 2018-08-02T00:30:30.338728: step 25868, loss 0.528334.
Train: 2018-08-02T00:30:30.507274: step 25869, loss 0.545364.
Train: 2018-08-02T00:30:30.673827: step 25870, loss 0.647641.
Test: 2018-08-02T00:30:31.205381: step 25870, loss 0.548137.
Train: 2018-08-02T00:30:31.371935: step 25871, loss 0.613516.
Train: 2018-08-02T00:30:31.536503: step 25872, loss 0.545388.
Train: 2018-08-02T00:30:31.701055: step 25873, loss 0.545403.
Train: 2018-08-02T00:30:31.869636: step 25874, loss 0.494426.
Train: 2018-08-02T00:30:32.033168: step 25875, loss 0.562407.
Train: 2018-08-02T00:30:32.208723: step 25876, loss 0.596426.
Train: 2018-08-02T00:30:32.371264: step 25877, loss 0.630444.
Train: 2018-08-02T00:30:32.535823: step 25878, loss 0.562408.
Train: 2018-08-02T00:30:32.699413: step 25879, loss 0.579392.
Train: 2018-08-02T00:30:32.865941: step 25880, loss 0.443635.
Test: 2018-08-02T00:30:33.402508: step 25880, loss 0.548191.
Train: 2018-08-02T00:30:33.569092: step 25881, loss 0.613352.
Train: 2018-08-02T00:30:33.734653: step 25882, loss 0.596374.
Train: 2018-08-02T00:30:33.902171: step 25883, loss 0.511482.
Train: 2018-08-02T00:30:34.067739: step 25884, loss 0.477497.
Train: 2018-08-02T00:30:34.235280: step 25885, loss 0.579413.
Train: 2018-08-02T00:30:34.396849: step 25886, loss 0.598724.
Train: 2018-08-02T00:30:34.578394: step 25887, loss 0.596472.
Train: 2018-08-02T00:30:34.743923: step 25888, loss 0.596475.
Train: 2018-08-02T00:30:34.917457: step 25889, loss 0.528344.
Train: 2018-08-02T00:30:35.088001: step 25890, loss 0.613502.
Test: 2018-08-02T00:30:35.613596: step 25890, loss 0.548149.
Train: 2018-08-02T00:30:35.774197: step 25891, loss 0.528355.
Train: 2018-08-02T00:30:35.940750: step 25892, loss 0.61348.
Train: 2018-08-02T00:30:36.103295: step 25893, loss 0.596435.
Train: 2018-08-02T00:30:36.269868: step 25894, loss 0.562408.
Train: 2018-08-02T00:30:36.431410: step 25895, loss 0.562409.
Train: 2018-08-02T00:30:36.595001: step 25896, loss 0.562411.
Train: 2018-08-02T00:30:36.765551: step 25897, loss 0.596337.
Train: 2018-08-02T00:30:36.930105: step 25898, loss 0.528522.
Train: 2018-08-02T00:30:37.103613: step 25899, loss 0.647115.
Train: 2018-08-02T00:30:37.277149: step 25900, loss 0.545504.
Test: 2018-08-02T00:30:37.814712: step 25900, loss 0.548272.
Train: 2018-08-02T00:30:38.588955: step 25901, loss 0.613122.
Train: 2018-08-02T00:30:38.752509: step 25902, loss 0.613052.
Train: 2018-08-02T00:30:38.921052: step 25903, loss 0.59612.
Train: 2018-08-02T00:30:39.085627: step 25904, loss 0.646475.
Train: 2018-08-02T00:30:39.248158: step 25905, loss 0.545703.
Train: 2018-08-02T00:30:39.416739: step 25906, loss 0.512322.
Train: 2018-08-02T00:30:39.580301: step 25907, loss 0.71272.
Train: 2018-08-02T00:30:39.746852: step 25908, loss 0.629064.
Train: 2018-08-02T00:30:39.913414: step 25909, loss 0.463041.
Train: 2018-08-02T00:30:40.080932: step 25910, loss 0.39704.
Test: 2018-08-02T00:30:40.606527: step 25910, loss 0.548669.
Train: 2018-08-02T00:30:40.772113: step 25911, loss 0.512856.
Train: 2018-08-02T00:30:40.935647: step 25912, loss 0.529363.
Train: 2018-08-02T00:30:41.097217: step 25913, loss 0.579123.
Train: 2018-08-02T00:30:41.260805: step 25914, loss 0.562504.
Train: 2018-08-02T00:30:41.430325: step 25915, loss 0.495877.
Train: 2018-08-02T00:30:41.589898: step 25916, loss 0.462339.
Train: 2018-08-02T00:30:41.753489: step 25917, loss 0.579209.
Train: 2018-08-02T00:30:41.916056: step 25918, loss 0.545657.
Train: 2018-08-02T00:30:42.079620: step 25919, loss 0.528758.
Train: 2018-08-02T00:30:42.244149: step 25920, loss 0.579315.
Test: 2018-08-02T00:30:42.769745: step 25920, loss 0.548238.
Train: 2018-08-02T00:30:43.017539: step 25921, loss 0.545485.
Train: 2018-08-02T00:30:43.188053: step 25922, loss 0.681233.
Train: 2018-08-02T00:30:43.356602: step 25923, loss 0.460479.
Train: 2018-08-02T00:30:43.526149: step 25924, loss 0.477293.
Train: 2018-08-02T00:30:43.690735: step 25925, loss 0.630686.
Train: 2018-08-02T00:30:43.861253: step 25926, loss 0.69921.
Train: 2018-08-02T00:30:44.024844: step 25927, loss 0.630799.
Train: 2018-08-02T00:30:44.189377: step 25928, loss 0.613653.
Train: 2018-08-02T00:30:44.359953: step 25929, loss 0.681821.
Train: 2018-08-02T00:30:44.533457: step 25930, loss 0.579419.
Test: 2018-08-02T00:30:45.068027: step 25930, loss 0.548203.
Train: 2018-08-02T00:30:45.231616: step 25931, loss 0.613316.
Train: 2018-08-02T00:30:45.394156: step 25932, loss 0.460917.
Train: 2018-08-02T00:30:45.558741: step 25933, loss 0.44417.
Train: 2018-08-02T00:30:45.732252: step 25934, loss 0.545526.
Train: 2018-08-02T00:30:45.902824: step 25935, loss 0.680764.
Train: 2018-08-02T00:30:46.066360: step 25936, loss 0.545536.
Train: 2018-08-02T00:30:46.226941: step 25937, loss 0.579306.
Train: 2018-08-02T00:30:46.389527: step 25938, loss 0.680491.
Train: 2018-08-02T00:30:46.567019: step 25939, loss 0.579269.
Train: 2018-08-02T00:30:46.734572: step 25940, loss 0.629631.
Test: 2018-08-02T00:30:47.265185: step 25940, loss 0.548434.
Train: 2018-08-02T00:30:47.427719: step 25941, loss 0.629467.
Train: 2018-08-02T00:30:47.594307: step 25942, loss 0.529082.
Train: 2018-08-02T00:30:47.754845: step 25943, loss 0.579153.
Train: 2018-08-02T00:30:47.919430: step 25944, loss 0.479401.
Train: 2018-08-02T00:30:48.088951: step 25945, loss 0.529301.
Train: 2018-08-02T00:30:48.253537: step 25946, loss 0.446299.
Train: 2018-08-02T00:30:48.417074: step 25947, loss 0.496008.
Train: 2018-08-02T00:30:48.584656: step 25948, loss 0.545832.
Train: 2018-08-02T00:30:48.747193: step 25949, loss 0.595875.
Train: 2018-08-02T00:30:48.913746: step 25950, loss 0.529019.
Test: 2018-08-02T00:30:49.448318: step 25950, loss 0.548426.
Train: 2018-08-02T00:30:49.619859: step 25951, loss 0.579217.
Train: 2018-08-02T00:30:49.784420: step 25952, loss 0.579237.
Train: 2018-08-02T00:30:49.950007: step 25953, loss 0.680101.
Train: 2018-08-02T00:30:50.111573: step 25954, loss 0.512033.
Train: 2018-08-02T00:30:50.271118: step 25955, loss 0.663316.
Train: 2018-08-02T00:30:50.433683: step 25956, loss 0.512051.
Train: 2018-08-02T00:30:50.595256: step 25957, loss 0.545649.
Train: 2018-08-02T00:30:50.765795: step 25958, loss 0.579249.
Train: 2018-08-02T00:30:50.928392: step 25959, loss 0.562446.
Train: 2018-08-02T00:30:51.097908: step 25960, loss 0.528832.
Test: 2018-08-02T00:30:51.630484: step 25960, loss 0.54836.
Train: 2018-08-02T00:30:51.793076: step 25961, loss 0.495176.
Train: 2018-08-02T00:30:51.954642: step 25962, loss 0.461392.
Train: 2018-08-02T00:30:52.115215: step 25963, loss 0.511772.
Train: 2018-08-02T00:30:52.278808: step 25964, loss 0.528545.
Train: 2018-08-02T00:30:52.437355: step 25965, loss 0.647355.
Train: 2018-08-02T00:30:52.598895: step 25966, loss 0.511344.
Train: 2018-08-02T00:30:52.760464: step 25967, loss 0.647696.
Train: 2018-08-02T00:30:52.924055: step 25968, loss 0.511171.
Train: 2018-08-02T00:30:53.088611: step 25969, loss 0.545296.
Train: 2018-08-02T00:30:53.251179: step 25970, loss 0.579529.
Test: 2018-08-02T00:30:53.781732: step 25970, loss 0.548035.
Train: 2018-08-02T00:30:53.944300: step 25971, loss 0.493789.
Train: 2018-08-02T00:30:54.104872: step 25972, loss 0.47647.
Train: 2018-08-02T00:30:54.268433: step 25973, loss 0.6141.
Train: 2018-08-02T00:30:54.430998: step 25974, loss 0.614199.
Train: 2018-08-02T00:30:54.592567: step 25975, loss 0.389568.
Train: 2018-08-02T00:30:54.754158: step 25976, loss 0.52774.
Train: 2018-08-02T00:30:54.913707: step 25977, loss 0.579806.
Train: 2018-08-02T00:30:55.078305: step 25978, loss 0.544996.
Train: 2018-08-02T00:30:55.240863: step 25979, loss 0.597381.
Train: 2018-08-02T00:30:55.406390: step 25980, loss 0.527439.
Test: 2018-08-02T00:30:55.940992: step 25980, loss 0.547769.
Train: 2018-08-02T00:30:56.103557: step 25981, loss 0.562452.
Train: 2018-08-02T00:30:56.265096: step 25982, loss 0.580025.
Train: 2018-08-02T00:30:56.427686: step 25983, loss 0.562468.
Train: 2018-08-02T00:30:56.586261: step 25984, loss 0.544871.
Train: 2018-08-02T00:30:56.751825: step 25985, loss 0.668207.
Train: 2018-08-02T00:30:56.914389: step 25986, loss 0.615323.
Train: 2018-08-02T00:30:57.077922: step 25987, loss 0.509679.
Train: 2018-08-02T00:30:57.240487: step 25988, loss 0.56247.
Train: 2018-08-02T00:30:57.402057: step 25989, loss 0.474544.
Train: 2018-08-02T00:30:57.565618: step 25990, loss 0.580067.
Test: 2018-08-02T00:30:58.099192: step 25990, loss 0.547735.
Train: 2018-08-02T00:30:58.264774: step 25991, loss 0.544873.
Train: 2018-08-02T00:30:58.429338: step 25992, loss 0.615296.
Train: 2018-08-02T00:30:58.589880: step 25993, loss 0.70328.
Train: 2018-08-02T00:30:58.755437: step 25994, loss 0.580028.
Train: 2018-08-02T00:30:58.925982: step 25995, loss 0.474802.
Train: 2018-08-02T00:30:59.085581: step 25996, loss 0.579961.
Train: 2018-08-02T00:30:59.253107: step 25997, loss 0.49246.
Train: 2018-08-02T00:30:59.418697: step 25998, loss 0.579933.
Train: 2018-08-02T00:30:59.588212: step 25999, loss 0.579923.
Train: 2018-08-02T00:30:59.751799: step 26000, loss 0.562436.
Test: 2018-08-02T00:31:00.289337: step 26000, loss 0.547811.
Train: 2018-08-02T00:31:01.021638: step 26001, loss 0.457662.
Train: 2018-08-02T00:31:01.188160: step 26002, loss 0.562436.
Train: 2018-08-02T00:31:01.344760: step 26003, loss 0.562438.
Train: 2018-08-02T00:31:01.512319: step 26004, loss 0.492488.
Train: 2018-08-02T00:31:01.671898: step 26005, loss 0.562445.
Train: 2018-08-02T00:31:01.835430: step 26006, loss 0.632546.
Train: 2018-08-02T00:31:02.010960: step 26007, loss 0.597497.
Train: 2018-08-02T00:31:02.182502: step 26008, loss 0.527416.
Train: 2018-08-02T00:31:02.346098: step 26009, loss 0.614989.
Train: 2018-08-02T00:31:02.506641: step 26010, loss 0.649949.
Test: 2018-08-02T00:31:03.019293: step 26010, loss 0.547806.
Train: 2018-08-02T00:31:03.183825: step 26011, loss 0.492552.
Train: 2018-08-02T00:31:03.345393: step 26012, loss 0.684631.
Train: 2018-08-02T00:31:03.506992: step 26013, loss 0.562424.
Train: 2018-08-02T00:31:03.674514: step 26014, loss 0.52765.
Train: 2018-08-02T00:31:03.835109: step 26015, loss 0.545055.
Train: 2018-08-02T00:31:04.005628: step 26016, loss 0.614427.
Train: 2018-08-02T00:31:04.171185: step 26017, loss 0.648957.
Train: 2018-08-02T00:31:04.342752: step 26018, loss 0.562402.
Train: 2018-08-02T00:31:04.501303: step 26019, loss 0.596851.
Train: 2018-08-02T00:31:04.669853: step 26020, loss 0.648311.
Test: 2018-08-02T00:31:05.198440: step 26020, loss 0.548057.
Train: 2018-08-02T00:31:05.366988: step 26021, loss 0.545272.
Train: 2018-08-02T00:31:05.530577: step 26022, loss 0.5624.
Train: 2018-08-02T00:31:05.703124: step 26023, loss 0.528332.
Train: 2018-08-02T00:31:05.869653: step 26024, loss 0.528397.
Train: 2018-08-02T00:31:06.028222: step 26025, loss 0.664316.
Train: 2018-08-02T00:31:06.191814: step 26026, loss 0.545469.
Train: 2018-08-02T00:31:06.355347: step 26027, loss 0.613163.
Train: 2018-08-02T00:31:06.516962: step 26028, loss 0.579304.
Train: 2018-08-02T00:31:06.680502: step 26029, loss 0.596115.
Train: 2018-08-02T00:31:06.843043: step 26030, loss 0.596046.
Test: 2018-08-02T00:31:07.370633: step 26030, loss 0.548426.
Train: 2018-08-02T00:31:07.534226: step 26031, loss 0.512185.
Train: 2018-08-02T00:31:07.702744: step 26032, loss 0.512271.
Train: 2018-08-02T00:31:07.864313: step 26033, loss 0.529029.
Train: 2018-08-02T00:31:08.030868: step 26034, loss 0.478877.
Train: 2018-08-02T00:31:08.190441: step 26035, loss 0.512257.
Train: 2018-08-02T00:31:08.352038: step 26036, loss 0.478638.
Train: 2018-08-02T00:31:08.516570: step 26037, loss 0.629674.
Train: 2018-08-02T00:31:08.682128: step 26038, loss 0.562438.
Train: 2018-08-02T00:31:08.843707: step 26039, loss 0.478144.
Train: 2018-08-02T00:31:09.010249: step 26040, loss 0.68071.
Test: 2018-08-02T00:31:09.539835: step 26040, loss 0.548261.
Train: 2018-08-02T00:31:09.702432: step 26041, loss 0.545511.
Train: 2018-08-02T00:31:09.865993: step 26042, loss 0.545493.
Train: 2018-08-02T00:31:10.034512: step 26043, loss 0.511585.
Train: 2018-08-02T00:31:10.200097: step 26044, loss 0.54544.
Train: 2018-08-02T00:31:10.363644: step 26045, loss 0.511409.
Train: 2018-08-02T00:31:10.522242: step 26046, loss 0.579439.
Train: 2018-08-02T00:31:10.688762: step 26047, loss 0.460002.
Train: 2018-08-02T00:31:10.848336: step 26048, loss 0.442593.
Train: 2018-08-02T00:31:11.007910: step 26049, loss 0.545214.
Train: 2018-08-02T00:31:11.180448: step 26050, loss 0.510653.
Test: 2018-08-02T00:31:11.715020: step 26050, loss 0.547905.
Train: 2018-08-02T00:31:11.877585: step 26051, loss 0.579726.
Train: 2018-08-02T00:31:12.038180: step 26052, loss 0.579793.
Train: 2018-08-02T00:31:12.200751: step 26053, loss 0.562425.
Train: 2018-08-02T00:31:12.364314: step 26054, loss 0.614839.
Train: 2018-08-02T00:31:12.524854: step 26055, loss 0.649916.
Train: 2018-08-02T00:31:12.690412: step 26056, loss 0.527442.
Train: 2018-08-02T00:31:12.858960: step 26057, loss 0.667517.
Train: 2018-08-02T00:31:13.017562: step 26058, loss 0.492444.
Train: 2018-08-02T00:31:13.182098: step 26059, loss 0.474934.
Train: 2018-08-02T00:31:13.354635: step 26060, loss 0.527407.
Test: 2018-08-02T00:31:13.885217: step 26060, loss 0.547764.
Train: 2018-08-02T00:31:14.046817: step 26061, loss 0.439648.
Train: 2018-08-02T00:31:14.211345: step 26062, loss 0.456938.
Train: 2018-08-02T00:31:14.372941: step 26063, loss 0.491895.
Train: 2018-08-02T00:31:14.536488: step 26064, loss 0.615665.
Train: 2018-08-02T00:31:14.716994: step 26065, loss 0.633595.
Train: 2018-08-02T00:31:14.878562: step 26066, loss 0.687094.
Train: 2018-08-02T00:31:15.042124: step 26067, loss 0.615921.
Train: 2018-08-02T00:31:15.214663: step 26068, loss 0.598096.
Train: 2018-08-02T00:31:15.379224: step 26069, loss 0.50927.
Train: 2018-08-02T00:31:15.540821: step 26070, loss 0.491551.
Test: 2018-08-02T00:31:16.072371: step 26070, loss 0.547669.
Train: 2018-08-02T00:31:16.237929: step 26071, loss 0.491539.
Train: 2018-08-02T00:31:16.411470: step 26072, loss 0.580302.
Train: 2018-08-02T00:31:16.574061: step 26073, loss 0.580315.
Train: 2018-08-02T00:31:16.742580: step 26074, loss 0.45589.
Train: 2018-08-02T00:31:16.904172: step 26075, loss 0.509161.
Train: 2018-08-02T00:31:17.064718: step 26076, loss 0.526915.
Train: 2018-08-02T00:31:17.231273: step 26077, loss 0.526869.
Train: 2018-08-02T00:31:17.393839: step 26078, loss 0.580495.
Train: 2018-08-02T00:31:17.561416: step 26079, loss 0.616365.
Train: 2018-08-02T00:31:17.730950: step 26080, loss 0.526772.
Test: 2018-08-02T00:31:18.255534: step 26080, loss 0.547609.
Train: 2018-08-02T00:31:18.421121: step 26081, loss 0.634385.
Train: 2018-08-02T00:31:18.581688: step 26082, loss 0.634366.
Train: 2018-08-02T00:31:18.744228: step 26083, loss 0.544702.
Train: 2018-08-02T00:31:18.907791: step 26084, loss 0.6163.
Train: 2018-08-02T00:31:19.067398: step 26085, loss 0.598332.
Train: 2018-08-02T00:31:19.225940: step 26086, loss 0.616083.
Train: 2018-08-02T00:31:19.385539: step 26087, loss 0.491379.
Train: 2018-08-02T00:31:19.555061: step 26088, loss 0.527005.
Train: 2018-08-02T00:31:19.716630: step 26089, loss 0.527033.
Train: 2018-08-02T00:31:19.878221: step 26090, loss 0.598003.
Test: 2018-08-02T00:31:20.403793: step 26090, loss 0.54768.
Train: 2018-08-02T00:31:20.567355: step 26091, loss 0.580238.
Train: 2018-08-02T00:31:20.733934: step 26092, loss 0.509415.
Train: 2018-08-02T00:31:20.902459: step 26093, loss 0.580193.
Train: 2018-08-02T00:31:21.066022: step 26094, loss 0.597847.
Train: 2018-08-02T00:31:21.227621: step 26095, loss 0.597796.
Train: 2018-08-02T00:31:21.398135: step 26096, loss 0.491983.
Train: 2018-08-02T00:31:21.558729: step 26097, loss 0.562477.
Train: 2018-08-02T00:31:21.732241: step 26098, loss 0.580073.
Train: 2018-08-02T00:31:21.893834: step 26099, loss 0.509713.
Train: 2018-08-02T00:31:22.068342: step 26100, loss 0.580047.
Test: 2018-08-02T00:31:22.597963: step 26100, loss 0.547749.
Train: 2018-08-02T00:31:23.384274: step 26101, loss 0.597606.
Train: 2018-08-02T00:31:23.553792: step 26102, loss 0.667791.
Train: 2018-08-02T00:31:23.718379: step 26103, loss 0.562447.
Train: 2018-08-02T00:31:23.882913: step 26104, loss 0.52747.
Train: 2018-08-02T00:31:24.046476: step 26105, loss 0.562432.
Train: 2018-08-02T00:31:24.211036: step 26106, loss 0.527555.
Train: 2018-08-02T00:31:24.374630: step 26107, loss 0.492736.
Train: 2018-08-02T00:31:24.536167: step 26108, loss 0.510156.
Train: 2018-08-02T00:31:24.695775: step 26109, loss 0.52756.
Train: 2018-08-02T00:31:24.861322: step 26110, loss 0.544981.
Test: 2018-08-02T00:31:25.396867: step 26110, loss 0.547808.
Train: 2018-08-02T00:31:25.560460: step 26111, loss 0.5799.
Train: 2018-08-02T00:31:25.729975: step 26112, loss 0.510005.
Train: 2018-08-02T00:31:25.893564: step 26113, loss 0.579939.
Train: 2018-08-02T00:31:26.058099: step 26114, loss 0.544935.
Train: 2018-08-02T00:31:26.226667: step 26115, loss 0.5274.
Train: 2018-08-02T00:31:26.393203: step 26116, loss 0.562455.
Train: 2018-08-02T00:31:26.571725: step 26117, loss 0.702943.
Train: 2018-08-02T00:31:26.734322: step 26118, loss 0.615091.
Train: 2018-08-02T00:31:26.895889: step 26119, loss 0.527408.
Train: 2018-08-02T00:31:27.063410: step 26120, loss 0.579947.
Test: 2018-08-02T00:31:27.600974: step 26120, loss 0.547798.
Train: 2018-08-02T00:31:27.765570: step 26121, loss 0.614889.
Train: 2018-08-02T00:31:27.928124: step 26122, loss 0.544977.
Train: 2018-08-02T00:31:28.096648: step 26123, loss 0.597288.
Train: 2018-08-02T00:31:28.260236: step 26124, loss 0.492814.
Train: 2018-08-02T00:31:28.426767: step 26125, loss 0.562418.
Train: 2018-08-02T00:31:28.600303: step 26126, loss 0.527662.
Train: 2018-08-02T00:31:28.769850: step 26127, loss 0.614534.
Train: 2018-08-02T00:31:28.933436: step 26128, loss 0.562413.
Train: 2018-08-02T00:31:29.095011: step 26129, loss 0.631786.
Train: 2018-08-02T00:31:29.261561: step 26130, loss 0.493143.
Test: 2018-08-02T00:31:29.787131: step 26130, loss 0.547915.
Train: 2018-08-02T00:31:29.948698: step 26131, loss 0.614321.
Train: 2018-08-02T00:31:30.116250: step 26132, loss 0.545119.
Train: 2018-08-02T00:31:30.280840: step 26133, loss 0.562402.
Train: 2018-08-02T00:31:30.445399: step 26134, loss 0.510639.
Train: 2018-08-02T00:31:30.624890: step 26135, loss 0.527898.
Train: 2018-08-02T00:31:30.787481: step 26136, loss 0.648682.
Train: 2018-08-02T00:31:30.949024: step 26137, loss 0.493431.
Train: 2018-08-02T00:31:31.110626: step 26138, loss 0.476182.
Train: 2018-08-02T00:31:31.275180: step 26139, loss 0.579664.
Train: 2018-08-02T00:31:31.438714: step 26140, loss 0.493299.
Test: 2018-08-02T00:31:31.966330: step 26140, loss 0.547918.
Train: 2018-08-02T00:31:32.127903: step 26141, loss 0.579707.
Train: 2018-08-02T00:31:32.293463: step 26142, loss 0.614363.
Train: 2018-08-02T00:31:32.455996: step 26143, loss 0.527738.
Train: 2018-08-02T00:31:32.619559: step 26144, loss 0.44119.
Train: 2018-08-02T00:31:32.783149: step 26145, loss 0.597164.
Train: 2018-08-02T00:31:32.944689: step 26146, loss 0.545016.
Train: 2018-08-02T00:31:33.108282: step 26147, loss 0.457844.
Train: 2018-08-02T00:31:33.270817: step 26148, loss 0.544969.
Train: 2018-08-02T00:31:33.435378: step 26149, loss 0.509799.
Train: 2018-08-02T00:31:33.601932: step 26150, loss 0.544771.
Test: 2018-08-02T00:31:34.135505: step 26150, loss 0.547698.
Train: 2018-08-02T00:31:34.298097: step 26151, loss 0.456511.
Train: 2018-08-02T00:31:34.457671: step 26152, loss 0.58045.
Train: 2018-08-02T00:31:34.623201: step 26153, loss 0.545123.
Train: 2018-08-02T00:31:34.784771: step 26154, loss 0.545722.
Train: 2018-08-02T00:31:34.942348: step 26155, loss 0.543909.
Train: 2018-08-02T00:31:35.103947: step 26156, loss 0.490155.
Train: 2018-08-02T00:31:35.266483: step 26157, loss 0.707626.
Train: 2018-08-02T00:31:35.429080: step 26158, loss 0.65379.
Train: 2018-08-02T00:31:35.590640: step 26159, loss 0.544728.
Train: 2018-08-02T00:31:35.760193: step 26160, loss 0.545329.
Test: 2018-08-02T00:31:36.297725: step 26160, loss 0.54761.
Train: 2018-08-02T00:31:36.459294: step 26161, loss 0.562664.
Train: 2018-08-02T00:31:36.637816: step 26162, loss 0.563037.
Train: 2018-08-02T00:31:36.812349: step 26163, loss 0.616154.
Train: 2018-08-02T00:31:36.972952: step 26164, loss 0.598299.
Train: 2018-08-02T00:31:37.137481: step 26165, loss 0.562567.
Train: 2018-08-02T00:31:37.305058: step 26166, loss 0.598117.
Train: 2018-08-02T00:31:37.468626: step 26167, loss 0.491546.
Train: 2018-08-02T00:31:37.630164: step 26168, loss 0.562533.
Train: 2018-08-02T00:31:37.795746: step 26169, loss 0.597955.
Train: 2018-08-02T00:31:37.959284: step 26170, loss 0.63328.
Test: 2018-08-02T00:31:38.488893: step 26170, loss 0.54772.
Train: 2018-08-02T00:31:38.655423: step 26171, loss 0.59781.
Train: 2018-08-02T00:31:38.817016: step 26172, loss 0.650544.
Train: 2018-08-02T00:31:38.978559: step 26173, loss 0.580024.
Train: 2018-08-02T00:31:39.146112: step 26174, loss 0.527459.
Train: 2018-08-02T00:31:39.316655: step 26175, loss 0.597352.
Train: 2018-08-02T00:31:39.479221: step 26176, loss 0.510218.
Train: 2018-08-02T00:31:39.647770: step 26177, loss 0.510311.
Train: 2018-08-02T00:31:39.815323: step 26178, loss 0.57978.
Train: 2018-08-02T00:31:39.980905: step 26179, loss 0.683744.
Train: 2018-08-02T00:31:40.146463: step 26180, loss 0.545132.
Test: 2018-08-02T00:31:40.679014: step 26180, loss 0.547972.
Train: 2018-08-02T00:31:40.845593: step 26181, loss 0.510669.
Train: 2018-08-02T00:31:41.013119: step 26182, loss 0.614092.
Train: 2018-08-02T00:31:41.179675: step 26183, loss 0.751546.
Train: 2018-08-02T00:31:41.350219: step 26184, loss 0.476784.
Train: 2018-08-02T00:31:41.517771: step 26185, loss 0.545332.
Train: 2018-08-02T00:31:41.682330: step 26186, loss 0.596513.
Train: 2018-08-02T00:31:41.846920: step 26187, loss 0.544279.
Train: 2018-08-02T00:31:42.013446: step 26188, loss 0.460554.
Train: 2018-08-02T00:31:42.181996: step 26189, loss 0.562426.
Train: 2018-08-02T00:31:42.347583: step 26190, loss 0.596364.
Test: 2018-08-02T00:31:42.868189: step 26190, loss 0.548227.
Train: 2018-08-02T00:31:43.046713: step 26191, loss 0.562428.
Train: 2018-08-02T00:31:43.220220: step 26192, loss 0.460733.
Train: 2018-08-02T00:31:43.385808: step 26193, loss 0.5285.
Train: 2018-08-02T00:31:43.555351: step 26194, loss 0.613376.
Train: 2018-08-02T00:31:43.717890: step 26195, loss 0.545431.
Train: 2018-08-02T00:31:43.884469: step 26196, loss 0.494409.
Train: 2018-08-02T00:31:44.046037: step 26197, loss 0.647559.
Train: 2018-08-02T00:31:44.219549: step 26198, loss 0.545386.
Train: 2018-08-02T00:31:44.384133: step 26199, loss 0.681706.
Train: 2018-08-02T00:31:44.549690: step 26200, loss 0.494324.
Test: 2018-08-02T00:31:45.093213: step 26200, loss 0.548164.
Train: 2018-08-02T00:31:45.835837: step 26201, loss 0.596465.
Train: 2018-08-02T00:31:46.004393: step 26202, loss 0.647497.
Train: 2018-08-02T00:31:46.168955: step 26203, loss 0.579414.
Train: 2018-08-02T00:31:46.338500: step 26204, loss 0.681188.
Train: 2018-08-02T00:31:46.500065: step 26205, loss 0.545515.
Train: 2018-08-02T00:31:46.667617: step 26206, loss 0.646845.
Train: 2018-08-02T00:31:46.831163: step 26207, loss 0.528794.
Train: 2018-08-02T00:31:46.996740: step 26208, loss 0.612837.
Train: 2018-08-02T00:31:47.159308: step 26209, loss 0.646212.
Train: 2018-08-02T00:31:47.331844: step 26210, loss 0.579186.
Test: 2018-08-02T00:31:47.864390: step 26210, loss 0.548583.
Train: 2018-08-02T00:31:48.029976: step 26211, loss 0.59579.
Train: 2018-08-02T00:31:48.192512: step 26212, loss 0.380114.
Train: 2018-08-02T00:31:48.355077: step 26213, loss 0.612271.
Train: 2018-08-02T00:31:48.518640: step 26214, loss 0.496303.
Train: 2018-08-02T00:31:48.691212: step 26215, loss 0.446605.
Train: 2018-08-02T00:31:48.863749: step 26216, loss 0.562534.
Train: 2018-08-02T00:31:49.037283: step 26217, loss 0.678852.
Train: 2018-08-02T00:31:49.203810: step 26218, loss 0.496045.
Train: 2018-08-02T00:31:49.370364: step 26219, loss 0.495972.
Train: 2018-08-02T00:31:49.542908: step 26220, loss 0.479172.
Test: 2018-08-02T00:31:50.068497: step 26220, loss 0.548491.
Train: 2018-08-02T00:31:50.233057: step 26221, loss 0.595912.
Train: 2018-08-02T00:31:50.398614: step 26222, loss 0.545725.
Train: 2018-08-02T00:31:50.563206: step 26223, loss 0.663178.
Train: 2018-08-02T00:31:50.732722: step 26224, loss 0.612856.
Train: 2018-08-02T00:31:50.896285: step 26225, loss 0.646463.
Train: 2018-08-02T00:31:51.065862: step 26226, loss 0.579248.
Train: 2018-08-02T00:31:51.230423: step 26227, loss 0.579237.
Train: 2018-08-02T00:31:51.395976: step 26228, loss 0.612732.
Train: 2018-08-02T00:31:51.565523: step 26229, loss 0.512294.
Train: 2018-08-02T00:31:51.729091: step 26230, loss 0.545765.
Test: 2018-08-02T00:31:52.267619: step 26230, loss 0.548489.
Train: 2018-08-02T00:31:52.433210: step 26231, loss 0.612625.
Train: 2018-08-02T00:31:52.596738: step 26232, loss 0.595889.
Train: 2018-08-02T00:31:52.756312: step 26233, loss 0.529132.
Train: 2018-08-02T00:31:52.919901: step 26234, loss 0.479128.
Train: 2018-08-02T00:31:53.084436: step 26235, loss 0.612551.
Train: 2018-08-02T00:31:53.246004: step 26236, loss 0.612542.
Train: 2018-08-02T00:31:53.407596: step 26237, loss 0.595823.
Train: 2018-08-02T00:31:53.573160: step 26238, loss 0.562529.
Train: 2018-08-02T00:31:53.735734: step 26239, loss 0.67809.
Train: 2018-08-02T00:31:53.904272: step 26240, loss 0.646399.
Test: 2018-08-02T00:31:54.436838: step 26240, loss 0.548972.
Train: 2018-08-02T00:31:54.609383: step 26241, loss 0.514187.
Train: 2018-08-02T00:31:54.771955: step 26242, loss 0.595589.
Train: 2018-08-02T00:31:54.942493: step 26243, loss 0.562762.
Train: 2018-08-02T00:31:55.109024: step 26244, loss 0.545305.
Train: 2018-08-02T00:31:55.275611: step 26245, loss 0.464873.
Train: 2018-08-02T00:31:55.442158: step 26246, loss 0.54691.
Train: 2018-08-02T00:31:55.601705: step 26247, loss 0.612042.
Train: 2018-08-02T00:31:55.774277: step 26248, loss 0.611864.
Train: 2018-08-02T00:31:55.940800: step 26249, loss 0.579045.
Train: 2018-08-02T00:31:56.112366: step 26250, loss 0.612041.
Test: 2018-08-02T00:31:56.654903: step 26250, loss 0.548791.
Train: 2018-08-02T00:31:56.820479: step 26251, loss 0.546126.
Train: 2018-08-02T00:31:56.983014: step 26252, loss 0.513186.
Train: 2018-08-02T00:31:57.146601: step 26253, loss 0.661442.
Train: 2018-08-02T00:31:57.310170: step 26254, loss 0.496739.
Train: 2018-08-02T00:31:57.469713: step 26255, loss 0.529649.
Train: 2018-08-02T00:31:57.633276: step 26256, loss 0.546109.
Train: 2018-08-02T00:31:57.795841: step 26257, loss 0.546076.
Train: 2018-08-02T00:31:57.957409: step 26258, loss 0.546051.
Train: 2018-08-02T00:31:58.122978: step 26259, loss 0.595616.
Train: 2018-08-02T00:31:58.294507: step 26260, loss 0.54595.
Test: 2018-08-02T00:31:58.837058: step 26260, loss 0.548663.
Train: 2018-08-02T00:31:59.001648: step 26261, loss 0.529391.
Train: 2018-08-02T00:31:59.169201: step 26262, loss 0.479452.
Train: 2018-08-02T00:31:59.342736: step 26263, loss 0.512444.
Train: 2018-08-02T00:31:59.508263: step 26264, loss 0.512086.
Train: 2018-08-02T00:31:59.681799: step 26265, loss 0.596798.
Train: 2018-08-02T00:31:59.846358: step 26266, loss 0.511042.
Train: 2018-08-02T00:32:00.008935: step 26267, loss 0.580467.
Train: 2018-08-02T00:32:00.192433: step 26268, loss 0.543537.
Train: 2018-08-02T00:32:00.354001: step 26269, loss 0.599204.
Train: 2018-08-02T00:32:00.542498: step 26270, loss 0.580378.
Test: 2018-08-02T00:32:01.076084: step 26270, loss 0.547867.
Train: 2018-08-02T00:32:01.240631: step 26271, loss 0.493119.
Train: 2018-08-02T00:32:01.404227: step 26272, loss 0.545166.
Train: 2018-08-02T00:32:01.567788: step 26273, loss 0.545555.
Train: 2018-08-02T00:32:01.730353: step 26274, loss 0.528441.
Train: 2018-08-02T00:32:01.892919: step 26275, loss 0.504891.
Train: 2018-08-02T00:32:02.055454: step 26276, loss 0.545788.
Train: 2018-08-02T00:32:02.230018: step 26277, loss 0.533108.
Train: 2018-08-02T00:32:02.396542: step 26278, loss 0.552311.
Train: 2018-08-02T00:32:02.562099: step 26279, loss 0.652688.
Train: 2018-08-02T00:32:02.726685: step 26280, loss 0.586739.
Test: 2018-08-02T00:32:03.260244: step 26280, loss 0.547653.
Train: 2018-08-02T00:32:03.424821: step 26281, loss 0.548237.
Train: 2018-08-02T00:32:03.586362: step 26282, loss 0.543295.
Train: 2018-08-02T00:32:03.747959: step 26283, loss 0.441046.
Train: 2018-08-02T00:32:03.915481: step 26284, loss 0.667062.
Train: 2018-08-02T00:32:04.078046: step 26285, loss 0.562041.
Train: 2018-08-02T00:32:04.244634: step 26286, loss 0.579762.
Train: 2018-08-02T00:32:04.413178: step 26287, loss 0.597403.
Train: 2018-08-02T00:32:04.575747: step 26288, loss 0.579595.
Train: 2018-08-02T00:32:04.738281: step 26289, loss 0.510516.
Train: 2018-08-02T00:32:04.909823: step 26290, loss 0.683637.
Test: 2018-08-02T00:32:05.441403: step 26290, loss 0.54796.
Train: 2018-08-02T00:32:05.606990: step 26291, loss 0.510596.
Train: 2018-08-02T00:32:05.787509: step 26292, loss 0.54523.
Train: 2018-08-02T00:32:05.951071: step 26293, loss 0.579662.
Train: 2018-08-02T00:32:06.117626: step 26294, loss 0.545236.
Train: 2018-08-02T00:32:06.282155: step 26295, loss 0.562277.
Train: 2018-08-02T00:32:06.450735: step 26296, loss 0.510624.
Train: 2018-08-02T00:32:06.628260: step 26297, loss 0.493715.
Train: 2018-08-02T00:32:06.799801: step 26298, loss 0.562237.
Train: 2018-08-02T00:32:06.963333: step 26299, loss 0.493806.
Train: 2018-08-02T00:32:07.130885: step 26300, loss 0.441453.
Test: 2018-08-02T00:32:07.671441: step 26300, loss 0.547916.
Train: 2018-08-02T00:32:08.457108: step 26301, loss 0.562554.
Train: 2018-08-02T00:32:08.620670: step 26302, loss 0.527659.
Train: 2018-08-02T00:32:08.782264: step 26303, loss 0.492656.
Train: 2018-08-02T00:32:08.948793: step 26304, loss 0.579975.
Train: 2018-08-02T00:32:09.116345: step 26305, loss 0.667801.
Train: 2018-08-02T00:32:09.278910: step 26306, loss 0.562486.
Train: 2018-08-02T00:32:09.445496: step 26307, loss 0.580079.
Train: 2018-08-02T00:32:09.610051: step 26308, loss 0.580092.
Train: 2018-08-02T00:32:09.773588: step 26309, loss 0.509693.
Train: 2018-08-02T00:32:09.938179: step 26310, loss 0.527272.
Test: 2018-08-02T00:32:10.469727: step 26310, loss 0.547741.
Train: 2018-08-02T00:32:10.641269: step 26311, loss 0.597766.
Train: 2018-08-02T00:32:10.808854: step 26312, loss 0.580154.
Train: 2018-08-02T00:32:10.975406: step 26313, loss 0.562519.
Train: 2018-08-02T00:32:11.156889: step 26314, loss 0.544859.
Train: 2018-08-02T00:32:11.322476: step 26315, loss 0.615426.
Train: 2018-08-02T00:32:11.489002: step 26316, loss 0.527197.
Train: 2018-08-02T00:32:11.655557: step 26317, loss 0.562439.
Train: 2018-08-02T00:32:11.819120: step 26318, loss 0.562316.
Train: 2018-08-02T00:32:11.989664: step 26319, loss 0.543958.
Train: 2018-08-02T00:32:12.152237: step 26320, loss 0.614533.
Test: 2018-08-02T00:32:12.689793: step 26320, loss 0.54761.
Train: 2018-08-02T00:32:12.868315: step 26321, loss 0.649911.
Train: 2018-08-02T00:32:13.033898: step 26322, loss 0.585834.
Train: 2018-08-02T00:32:13.202449: step 26323, loss 0.449476.
Train: 2018-08-02T00:32:13.367980: step 26324, loss 0.55037.
Train: 2018-08-02T00:32:13.534534: step 26325, loss 0.680024.
Train: 2018-08-02T00:32:13.701114: step 26326, loss 0.578057.
Train: 2018-08-02T00:32:13.872630: step 26327, loss 0.527768.
Train: 2018-08-02T00:32:14.041180: step 26328, loss 0.617003.
Train: 2018-08-02T00:32:14.213740: step 26329, loss 0.491486.
Train: 2018-08-02T00:32:14.377282: step 26330, loss 0.476289.
Test: 2018-08-02T00:32:14.909861: step 26330, loss 0.547853.
Train: 2018-08-02T00:32:15.076412: step 26331, loss 0.526269.
Train: 2018-08-02T00:32:15.242993: step 26332, loss 0.546288.
Train: 2018-08-02T00:32:15.404561: step 26333, loss 0.614148.
Train: 2018-08-02T00:32:15.575105: step 26334, loss 0.562193.
Train: 2018-08-02T00:32:15.750609: step 26335, loss 0.458357.
Train: 2018-08-02T00:32:15.914173: step 26336, loss 0.597628.
Train: 2018-08-02T00:32:16.078758: step 26337, loss 0.59695.
Train: 2018-08-02T00:32:16.243319: step 26338, loss 0.562503.
Train: 2018-08-02T00:32:16.405859: step 26339, loss 0.614385.
Train: 2018-08-02T00:32:16.572443: step 26340, loss 0.562531.
Test: 2018-08-02T00:32:17.112968: step 26340, loss 0.547999.
Train: 2018-08-02T00:32:17.274562: step 26341, loss 0.527917.
Train: 2018-08-02T00:32:17.438124: step 26342, loss 0.562503.
Train: 2018-08-02T00:32:17.605652: step 26343, loss 0.614219.
Train: 2018-08-02T00:32:17.770211: step 26344, loss 0.579691.
Train: 2018-08-02T00:32:17.932808: step 26345, loss 0.579681.
Train: 2018-08-02T00:32:18.112296: step 26346, loss 0.528049.
Train: 2018-08-02T00:32:18.285834: step 26347, loss 0.648405.
Train: 2018-08-02T00:32:18.457400: step 26348, loss 0.61394.
Train: 2018-08-02T00:32:18.625923: step 26349, loss 0.579579.
Train: 2018-08-02T00:32:18.789493: step 26350, loss 0.545362.
Test: 2018-08-02T00:32:19.319071: step 26350, loss 0.548167.
Train: 2018-08-02T00:32:19.492607: step 26351, loss 0.511267.
Train: 2018-08-02T00:32:19.665146: step 26352, loss 0.52836.
Train: 2018-08-02T00:32:19.832724: step 26353, loss 0.528371.
Train: 2018-08-02T00:32:19.997288: step 26354, loss 0.596546.
Train: 2018-08-02T00:32:20.162815: step 26355, loss 0.545414.
Train: 2018-08-02T00:32:20.328403: step 26356, loss 0.596534.
Train: 2018-08-02T00:32:20.499944: step 26357, loss 0.579487.
Train: 2018-08-02T00:32:20.667465: step 26358, loss 0.375193.
Train: 2018-08-02T00:32:20.833023: step 26359, loss 0.528339.
Train: 2018-08-02T00:32:21.002596: step 26360, loss 0.545355.
Test: 2018-08-02T00:32:21.532154: step 26360, loss 0.548101.
Train: 2018-08-02T00:32:21.706688: step 26361, loss 0.545316.
Train: 2018-08-02T00:32:21.875237: step 26362, loss 0.528124.
Train: 2018-08-02T00:32:22.041823: step 26363, loss 0.562444.
Train: 2018-08-02T00:32:22.214357: step 26364, loss 0.51074.
Train: 2018-08-02T00:32:22.377918: step 26365, loss 0.562446.
Train: 2018-08-02T00:32:22.540459: step 26366, loss 0.441214.
Train: 2018-08-02T00:32:22.707014: step 26367, loss 0.597212.
Train: 2018-08-02T00:32:22.870577: step 26368, loss 0.667005.
Train: 2018-08-02T00:32:23.038129: step 26369, loss 0.614793.
Train: 2018-08-02T00:32:23.205711: step 26370, loss 0.597362.
Test: 2018-08-02T00:32:23.750226: step 26370, loss 0.547861.
Train: 2018-08-02T00:32:23.918801: step 26371, loss 0.492694.
Train: 2018-08-02T00:32:24.082363: step 26372, loss 0.545016.
Train: 2018-08-02T00:32:24.252882: step 26373, loss 0.684733.
Train: 2018-08-02T00:32:24.415447: step 26374, loss 0.562468.
Train: 2018-08-02T00:32:24.593969: step 26375, loss 0.475282.
Train: 2018-08-02T00:32:24.768527: step 26376, loss 0.545023.
Train: 2018-08-02T00:32:24.937083: step 26377, loss 0.545017.
Train: 2018-08-02T00:32:25.106599: step 26378, loss 0.579921.
Train: 2018-08-02T00:32:25.270187: step 26379, loss 0.545006.
Train: 2018-08-02T00:32:25.436742: step 26380, loss 0.597396.
Test: 2018-08-02T00:32:25.975288: step 26380, loss 0.547841.
Train: 2018-08-02T00:32:26.143826: step 26381, loss 0.492614.
Train: 2018-08-02T00:32:26.318359: step 26382, loss 0.579939.
Train: 2018-08-02T00:32:26.483918: step 26383, loss 0.527508.
Train: 2018-08-02T00:32:26.650503: step 26384, loss 0.562468.
Train: 2018-08-02T00:32:26.815032: step 26385, loss 0.492471.
Train: 2018-08-02T00:32:26.979627: step 26386, loss 0.579997.
Train: 2018-08-02T00:32:27.143185: step 26387, loss 0.615089.
Train: 2018-08-02T00:32:27.309736: step 26388, loss 0.580017.
Train: 2018-08-02T00:32:27.477262: step 26389, loss 0.509871.
Train: 2018-08-02T00:32:27.646808: step 26390, loss 0.49231.
Test: 2018-08-02T00:32:28.179392: step 26390, loss 0.547778.
Train: 2018-08-02T00:32:28.342978: step 26391, loss 0.597608.
Train: 2018-08-02T00:32:28.506510: step 26392, loss 0.685484.
Train: 2018-08-02T00:32:28.674088: step 26393, loss 0.544927.
Train: 2018-08-02T00:32:28.846601: step 26394, loss 0.615094.
Train: 2018-08-02T00:32:29.010194: step 26395, loss 0.544953.
Train: 2018-08-02T00:32:29.174750: step 26396, loss 0.579958.
Train: 2018-08-02T00:32:29.338320: step 26397, loss 0.510035.
Train: 2018-08-02T00:32:29.506854: step 26398, loss 0.667239.
Train: 2018-08-02T00:32:29.672424: step 26399, loss 0.492715.
Train: 2018-08-02T00:32:29.834968: step 26400, loss 0.614698.
Test: 2018-08-02T00:32:30.367535: step 26400, loss 0.547873.
Train: 2018-08-02T00:32:31.133057: step 26401, loss 0.52765.
Train: 2018-08-02T00:32:31.304597: step 26402, loss 0.527678.
Train: 2018-08-02T00:32:31.470180: step 26403, loss 0.579804.
Train: 2018-08-02T00:32:31.634746: step 26404, loss 0.527711.
Train: 2018-08-02T00:32:31.800303: step 26405, loss 0.649215.
Train: 2018-08-02T00:32:31.968847: step 26406, loss 0.510421.
Train: 2018-08-02T00:32:32.135378: step 26407, loss 0.527772.
Train: 2018-08-02T00:32:32.306918: step 26408, loss 0.57975.
Train: 2018-08-02T00:32:32.485440: step 26409, loss 0.631704.
Train: 2018-08-02T00:32:32.659973: step 26410, loss 0.510521.
Test: 2018-08-02T00:32:33.195542: step 26410, loss 0.54794.
Train: 2018-08-02T00:32:33.366112: step 26411, loss 0.6143.
Train: 2018-08-02T00:32:33.529674: step 26412, loss 0.527868.
Train: 2018-08-02T00:32:33.703186: step 26413, loss 0.59695.
Train: 2018-08-02T00:32:33.878716: step 26414, loss 0.68317.
Train: 2018-08-02T00:32:34.048264: step 26415, loss 0.579625.
Train: 2018-08-02T00:32:34.215815: step 26416, loss 0.613926.
Train: 2018-08-02T00:32:34.377409: step 26417, loss 0.579537.
Train: 2018-08-02T00:32:34.548926: step 26418, loss 0.545335.
Train: 2018-08-02T00:32:34.715480: step 26419, loss 0.613543.
Train: 2018-08-02T00:32:34.877083: step 26420, loss 0.460434.
Test: 2018-08-02T00:32:35.399657: step 26420, loss 0.548204.
Train: 2018-08-02T00:32:35.564211: step 26421, loss 0.562424.
Train: 2018-08-02T00:32:35.730783: step 26422, loss 0.54546.
Train: 2018-08-02T00:32:35.905300: step 26423, loss 0.494602.
Train: 2018-08-02T00:32:36.077838: step 26424, loss 0.545464.
Train: 2018-08-02T00:32:36.241425: step 26425, loss 0.596371.
Train: 2018-08-02T00:32:36.411945: step 26426, loss 0.698227.
Train: 2018-08-02T00:32:36.579497: step 26427, loss 0.579377.
Train: 2018-08-02T00:32:36.743063: step 26428, loss 0.562432.
Train: 2018-08-02T00:32:36.911628: step 26429, loss 0.444145.
Train: 2018-08-02T00:32:37.076168: step 26430, loss 0.562435.
Test: 2018-08-02T00:32:37.616749: step 26430, loss 0.548275.
Train: 2018-08-02T00:32:37.790260: step 26431, loss 0.613161.
Train: 2018-08-02T00:32:37.961802: step 26432, loss 0.596241.
Train: 2018-08-02T00:32:38.130351: step 26433, loss 0.511761.
Train: 2018-08-02T00:32:38.291918: step 26434, loss 0.630012.
Train: 2018-08-02T00:32:38.457501: step 26435, loss 0.5962.
Train: 2018-08-02T00:32:38.624057: step 26436, loss 0.54558.
Train: 2018-08-02T00:32:38.799562: step 26437, loss 0.596149.
Train: 2018-08-02T00:32:38.965120: step 26438, loss 0.596119.
Train: 2018-08-02T00:32:39.128707: step 26439, loss 0.680146.
Train: 2018-08-02T00:32:39.297232: step 26440, loss 0.461855.
Test: 2018-08-02T00:32:39.831803: step 26440, loss 0.548446.
Train: 2018-08-02T00:32:39.997385: step 26441, loss 0.512217.
Train: 2018-08-02T00:32:40.167904: step 26442, loss 0.528976.
Train: 2018-08-02T00:32:40.340467: step 26443, loss 0.612739.
Train: 2018-08-02T00:32:40.502042: step 26444, loss 0.495472.
Train: 2018-08-02T00:32:40.671588: step 26445, loss 0.612758.
Train: 2018-08-02T00:32:40.838141: step 26446, loss 0.512182.
Train: 2018-08-02T00:32:41.003696: step 26447, loss 0.528916.
Train: 2018-08-02T00:32:41.175243: step 26448, loss 0.680027.
Train: 2018-08-02T00:32:41.341796: step 26449, loss 0.49531.
Train: 2018-08-02T00:32:41.507348: step 26450, loss 0.579258.
Test: 2018-08-02T00:32:42.037904: step 26450, loss 0.548387.
Train: 2018-08-02T00:32:42.205492: step 26451, loss 0.545655.
Train: 2018-08-02T00:32:42.369021: step 26452, loss 0.562456.
Train: 2018-08-02T00:32:42.532583: step 26453, loss 0.478335.
Train: 2018-08-02T00:32:42.694183: step 26454, loss 0.680407.
Train: 2018-08-02T00:32:42.857744: step 26455, loss 0.562445.
Train: 2018-08-02T00:32:43.026288: step 26456, loss 0.511883.
Train: 2018-08-02T00:32:43.198827: step 26457, loss 0.545575.
Train: 2018-08-02T00:32:43.365382: step 26458, loss 0.494906.
Train: 2018-08-02T00:32:43.532935: step 26459, loss 0.528606.
Train: 2018-08-02T00:32:43.705447: step 26460, loss 0.59632.
Test: 2018-08-02T00:32:44.239022: step 26460, loss 0.548209.
Train: 2018-08-02T00:32:44.450115: step 26461, loss 0.647281.
Train: 2018-08-02T00:32:44.613707: step 26462, loss 0.494515.
Train: 2018-08-02T00:32:44.781229: step 26463, loss 0.545422.
Train: 2018-08-02T00:32:44.955762: step 26464, loss 0.664516.
Train: 2018-08-02T00:32:45.128327: step 26465, loss 0.562415.
Train: 2018-08-02T00:32:45.298881: step 26466, loss 0.545403.
Train: 2018-08-02T00:32:45.467424: step 26467, loss 0.630477.
Train: 2018-08-02T00:32:45.647937: step 26468, loss 0.528409.
Train: 2018-08-02T00:32:45.814498: step 26469, loss 0.562417.
Train: 2018-08-02T00:32:45.978059: step 26470, loss 0.613413.
Test: 2018-08-02T00:32:46.520578: step 26470, loss 0.548194.
Train: 2018-08-02T00:32:46.687158: step 26471, loss 0.51146.
Train: 2018-08-02T00:32:46.850695: step 26472, loss 0.613378.
Train: 2018-08-02T00:32:47.013287: step 26473, loss 0.528467.
Train: 2018-08-02T00:32:47.181810: step 26474, loss 0.630319.
Train: 2018-08-02T00:32:47.356345: step 26475, loss 0.511544.
Train: 2018-08-02T00:32:47.520935: step 26476, loss 0.613293.
Train: 2018-08-02T00:32:47.691477: step 26477, loss 0.545479.
Train: 2018-08-02T00:32:47.857006: step 26478, loss 0.494674.
Train: 2018-08-02T00:32:48.023561: step 26479, loss 0.511584.
Train: 2018-08-02T00:32:48.188151: step 26480, loss 0.426693.
Test: 2018-08-02T00:32:48.727679: step 26480, loss 0.548168.
Train: 2018-08-02T00:32:48.897225: step 26481, loss 0.630466.
Train: 2018-08-02T00:32:49.060788: step 26482, loss 0.596494.
Train: 2018-08-02T00:32:49.229368: step 26483, loss 0.54535.
Train: 2018-08-02T00:32:49.396890: step 26484, loss 0.562409.
Train: 2018-08-02T00:32:49.567464: step 26485, loss 0.511111.
Train: 2018-08-02T00:32:49.734014: step 26486, loss 0.4939.
Train: 2018-08-02T00:32:49.901540: step 26487, loss 0.493741.
Train: 2018-08-02T00:32:50.072115: step 26488, loss 0.6175.
Train: 2018-08-02T00:32:50.242659: step 26489, loss 0.562409.
Train: 2018-08-02T00:32:50.423146: step 26490, loss 0.579691.
Test: 2018-08-02T00:32:50.947743: step 26490, loss 0.547925.
Train: 2018-08-02T00:32:51.112304: step 26491, loss 0.631624.
Train: 2018-08-02T00:32:51.290826: step 26492, loss 0.510491.
Train: 2018-08-02T00:32:51.456408: step 26493, loss 0.683681.
Train: 2018-08-02T00:32:51.616955: step 26494, loss 0.666285.
Train: 2018-08-02T00:32:51.790491: step 26495, loss 0.596969.
Train: 2018-08-02T00:32:51.961053: step 26496, loss 0.527925.
Train: 2018-08-02T00:32:52.124597: step 26497, loss 0.459115.
Train: 2018-08-02T00:32:52.298134: step 26498, loss 0.614046.
Train: 2018-08-02T00:32:52.465717: step 26499, loss 0.562406.
Train: 2018-08-02T00:32:52.642239: step 26500, loss 0.596783.
Test: 2018-08-02T00:32:53.186758: step 26500, loss 0.548027.
Train: 2018-08-02T00:32:53.942354: step 26501, loss 0.528065.
Train: 2018-08-02T00:32:54.110862: step 26502, loss 0.476598.
Train: 2018-08-02T00:32:54.280438: step 26503, loss 0.61392.
Train: 2018-08-02T00:32:54.460956: step 26504, loss 0.562405.
Train: 2018-08-02T00:32:54.641443: step 26505, loss 0.562405.
Train: 2018-08-02T00:32:54.807031: step 26506, loss 0.545239.
Train: 2018-08-02T00:32:54.972582: step 26507, loss 0.631079.
Train: 2018-08-02T00:32:55.137149: step 26508, loss 0.631029.
Train: 2018-08-02T00:32:55.312679: step 26509, loss 0.665186.
Train: 2018-08-02T00:32:55.483217: step 26510, loss 0.511151.
Test: 2018-08-02T00:32:56.021753: step 26510, loss 0.548127.
Train: 2018-08-02T00:32:56.186344: step 26511, loss 0.494186.
Train: 2018-08-02T00:32:56.353896: step 26512, loss 0.494233.
Train: 2018-08-02T00:32:56.521416: step 26513, loss 0.511264.
Train: 2018-08-02T00:32:56.684979: step 26514, loss 0.511217.
Train: 2018-08-02T00:32:56.850538: step 26515, loss 0.767466.
Train: 2018-08-02T00:32:57.014100: step 26516, loss 0.562408.
Train: 2018-08-02T00:32:57.177662: step 26517, loss 0.596507.
Train: 2018-08-02T00:32:57.345244: step 26518, loss 0.613487.
Train: 2018-08-02T00:32:57.524735: step 26519, loss 0.545422.
Train: 2018-08-02T00:32:57.690292: step 26520, loss 0.545449.
Test: 2018-08-02T00:32:58.228854: step 26520, loss 0.548227.
Train: 2018-08-02T00:32:58.395440: step 26521, loss 0.443765.
Train: 2018-08-02T00:32:58.562992: step 26522, loss 0.511538.
Train: 2018-08-02T00:32:58.730537: step 26523, loss 0.494494.
Train: 2018-08-02T00:32:58.893076: step 26524, loss 0.443318.
Train: 2018-08-02T00:32:59.055642: step 26525, loss 0.767232.
Train: 2018-08-02T00:32:59.218232: step 26526, loss 0.425803.
Train: 2018-08-02T00:32:59.381802: step 26527, loss 0.579515.
Train: 2018-08-02T00:32:59.547361: step 26528, loss 0.630957.
Train: 2018-08-02T00:32:59.712916: step 26529, loss 0.442361.
Train: 2018-08-02T00:32:59.879440: step 26530, loss 0.545221.
Test: 2018-08-02T00:33:00.421020: step 26530, loss 0.547987.
Train: 2018-08-02T00:33:00.607494: step 26531, loss 0.596843.
Train: 2018-08-02T00:33:00.778063: step 26532, loss 0.562407.
Train: 2018-08-02T00:33:00.941632: step 26533, loss 0.527881.
Train: 2018-08-02T00:33:01.111148: step 26534, loss 0.614281.
Train: 2018-08-02T00:33:01.284714: step 26535, loss 0.527807.
Train: 2018-08-02T00:33:01.452266: step 26536, loss 0.458492.
Train: 2018-08-02T00:33:01.622779: step 26537, loss 0.597134.
Train: 2018-08-02T00:33:01.788368: step 26538, loss 0.579807.
Train: 2018-08-02T00:33:01.951899: step 26539, loss 0.545025.
Train: 2018-08-02T00:33:02.115463: step 26540, loss 0.440476.
Test: 2018-08-02T00:33:02.647068: step 26540, loss 0.547816.
Train: 2018-08-02T00:33:02.814594: step 26541, loss 0.544976.
Train: 2018-08-02T00:33:02.977173: step 26542, loss 0.52744.
Train: 2018-08-02T00:33:03.149728: step 26543, loss 0.527366.
Train: 2018-08-02T00:33:03.318281: step 26544, loss 0.562476.
Train: 2018-08-02T00:33:03.485799: step 26545, loss 0.597754.
Train: 2018-08-02T00:33:03.662345: step 26546, loss 0.491866.
Train: 2018-08-02T00:33:03.827884: step 26547, loss 0.58021.
Train: 2018-08-02T00:33:03.991480: step 26548, loss 0.597974.
Train: 2018-08-02T00:33:04.156009: step 26549, loss 0.527053.
Train: 2018-08-02T00:33:04.319571: step 26550, loss 0.544782.
Test: 2018-08-02T00:33:04.846164: step 26550, loss 0.547663.
Train: 2018-08-02T00:33:05.008754: step 26551, loss 0.544771.
Train: 2018-08-02T00:33:05.174316: step 26552, loss 0.598166.
Train: 2018-08-02T00:33:05.336876: step 26553, loss 0.59819.
Train: 2018-08-02T00:33:05.505401: step 26554, loss 0.473513.
Train: 2018-08-02T00:33:05.674948: step 26555, loss 0.580401.
Train: 2018-08-02T00:33:05.839508: step 26556, loss 0.616084.
Train: 2018-08-02T00:33:06.010080: step 26557, loss 0.598237.
Train: 2018-08-02T00:33:06.175640: step 26558, loss 0.544753.
Train: 2018-08-02T00:33:06.344159: step 26559, loss 0.544758.
Train: 2018-08-02T00:33:06.509749: step 26560, loss 0.615959.
Test: 2018-08-02T00:33:07.045283: step 26560, loss 0.547663.
Train: 2018-08-02T00:33:07.210850: step 26561, loss 0.651453.
Train: 2018-08-02T00:33:07.390390: step 26562, loss 0.491561.
Train: 2018-08-02T00:33:07.554964: step 26563, loss 0.509357.
Train: 2018-08-02T00:33:07.727486: step 26564, loss 0.562522.
Train: 2018-08-02T00:33:07.892055: step 26565, loss 0.491702.
Train: 2018-08-02T00:33:08.054585: step 26566, loss 0.562519.
Train: 2018-08-02T00:33:08.219145: step 26567, loss 0.58023.
Train: 2018-08-02T00:33:08.384703: step 26568, loss 0.61564.
Train: 2018-08-02T00:33:08.553285: step 26569, loss 0.597896.
Train: 2018-08-02T00:33:08.720804: step 26570, loss 0.527164.
Test: 2018-08-02T00:33:09.257399: step 26570, loss 0.547714.
Train: 2018-08-02T00:33:09.427949: step 26571, loss 0.527187.
Train: 2018-08-02T00:33:09.590481: step 26572, loss 0.633088.
Train: 2018-08-02T00:33:09.766040: step 26573, loss 0.474356.
Train: 2018-08-02T00:33:09.940569: step 26574, loss 0.597732.
Train: 2018-08-02T00:33:10.109118: step 26575, loss 0.632928.
Train: 2018-08-02T00:33:10.279637: step 26576, loss 0.562473.
Train: 2018-08-02T00:33:10.447221: step 26577, loss 0.474659.
Train: 2018-08-02T00:33:10.613744: step 26578, loss 0.544907.
Train: 2018-08-02T00:33:10.793265: step 26579, loss 0.492246.
Train: 2018-08-02T00:33:10.955830: step 26580, loss 0.492204.
Test: 2018-08-02T00:33:11.494402: step 26580, loss 0.547745.
Train: 2018-08-02T00:33:11.659973: step 26581, loss 0.544885.
Train: 2018-08-02T00:33:11.829495: step 26582, loss 0.580095.
Train: 2018-08-02T00:33:11.994083: step 26583, loss 0.5096.
Train: 2018-08-02T00:33:12.167621: step 26584, loss 0.597804.
Train: 2018-08-02T00:33:12.333179: step 26585, loss 0.509502.
Train: 2018-08-02T00:33:12.503723: step 26586, loss 0.580198.
Train: 2018-08-02T00:33:12.676262: step 26587, loss 0.509409.
Train: 2018-08-02T00:33:12.842803: step 26588, loss 0.58025.
Train: 2018-08-02T00:33:13.005351: step 26589, loss 0.527054.
Train: 2018-08-02T00:33:13.166954: step 26590, loss 0.491509.
Test: 2018-08-02T00:33:13.695507: step 26590, loss 0.547659.
Train: 2018-08-02T00:33:13.861094: step 26591, loss 0.562554.
Train: 2018-08-02T00:33:14.041606: step 26592, loss 0.633818.
Train: 2018-08-02T00:33:14.204146: step 26593, loss 0.633846.
Train: 2018-08-02T00:33:14.370701: step 26594, loss 0.580373.
Train: 2018-08-02T00:33:14.536259: step 26595, loss 0.615938.
Train: 2018-08-02T00:33:14.700819: step 26596, loss 0.420404.
Train: 2018-08-02T00:33:14.865410: step 26597, loss 0.509232.
Train: 2018-08-02T00:33:15.040910: step 26598, loss 0.59812.
Train: 2018-08-02T00:33:15.210488: step 26599, loss 0.562553.
Train: 2018-08-02T00:33:15.376043: step 26600, loss 0.562554.
Test: 2018-08-02T00:33:15.911632: step 26600, loss 0.54766.
Train: 2018-08-02T00:33:16.701911: step 26601, loss 0.491412.
Train: 2018-08-02T00:33:16.865473: step 26602, loss 0.562558.
Train: 2018-08-02T00:33:17.033050: step 26603, loss 0.509147.
Train: 2018-08-02T00:33:17.194625: step 26604, loss 0.580393.
Train: 2018-08-02T00:33:17.355165: step 26605, loss 0.526912.
Train: 2018-08-02T00:33:17.523715: step 26606, loss 0.598275.
Train: 2018-08-02T00:33:17.694258: step 26607, loss 0.562585.
Train: 2018-08-02T00:33:17.866822: step 26608, loss 0.491183.
Train: 2018-08-02T00:33:18.038364: step 26609, loss 0.580457.
Train: 2018-08-02T00:33:18.202937: step 26610, loss 0.508983.
Test: 2018-08-02T00:33:18.730489: step 26610, loss 0.547627.
Train: 2018-08-02T00:33:18.900066: step 26611, loss 0.562605.
Train: 2018-08-02T00:33:19.074568: step 26612, loss 0.687895.
Train: 2018-08-02T00:33:19.248140: step 26613, loss 0.598363.
Train: 2018-08-02T00:33:19.409673: step 26614, loss 0.580445.
Train: 2018-08-02T00:33:19.576228: step 26615, loss 0.562574.
Train: 2018-08-02T00:33:19.736798: step 26616, loss 0.651569.
Train: 2018-08-02T00:33:19.899394: step 26617, loss 0.757876.
Train: 2018-08-02T00:33:20.064920: step 26618, loss 0.491796.
Train: 2018-08-02T00:33:20.227512: step 26619, loss 0.668189.
Train: 2018-08-02T00:33:20.393075: step 26620, loss 0.562458.
Test: 2018-08-02T00:33:20.912658: step 26620, loss 0.547809.
Train: 2018-08-02T00:33:21.080238: step 26621, loss 0.56244.
Train: 2018-08-02T00:33:21.247789: step 26622, loss 0.562426.
Train: 2018-08-02T00:33:21.412319: step 26623, loss 0.545066.
Train: 2018-08-02T00:33:21.576886: step 26624, loss 0.527808.
Train: 2018-08-02T00:33:21.746426: step 26625, loss 0.493355.
Train: 2018-08-02T00:33:21.911983: step 26626, loss 0.665865.
Train: 2018-08-02T00:33:22.079568: step 26627, loss 0.562403.
Train: 2018-08-02T00:33:22.255067: step 26628, loss 0.57957.
Train: 2018-08-02T00:33:22.421621: step 26629, loss 0.493874.
Train: 2018-08-02T00:33:22.585184: step 26630, loss 0.511056.
Test: 2018-08-02T00:33:23.128761: step 26630, loss 0.548077.
Train: 2018-08-02T00:33:23.295286: step 26631, loss 0.613726.
Train: 2018-08-02T00:33:23.457881: step 26632, loss 0.613667.
Train: 2018-08-02T00:33:23.626429: step 26633, loss 0.715795.
Train: 2018-08-02T00:33:23.791957: step 26634, loss 0.544938.
Train: 2018-08-02T00:33:23.956543: step 26635, loss 0.562766.
Train: 2018-08-02T00:33:24.120080: step 26636, loss 0.49788.
Train: 2018-08-02T00:33:24.285664: step 26637, loss 0.6125.
Train: 2018-08-02T00:33:24.455215: step 26638, loss 0.544823.
Train: 2018-08-02T00:33:24.619745: step 26639, loss 0.579189.
Train: 2018-08-02T00:33:24.785303: step 26640, loss 0.612854.
Test: 2018-08-02T00:33:25.328850: step 26640, loss 0.548574.
Train: 2018-08-02T00:33:25.497429: step 26641, loss 0.580082.
Train: 2018-08-02T00:33:25.675953: step 26642, loss 0.612913.
Train: 2018-08-02T00:33:25.843473: step 26643, loss 0.613004.
Train: 2018-08-02T00:33:26.009031: step 26644, loss 0.562266.
Train: 2018-08-02T00:33:26.175617: step 26645, loss 0.579026.
Train: 2018-08-02T00:33:26.339182: step 26646, loss 0.545796.
Train: 2018-08-02T00:33:26.507729: step 26647, loss 0.579022.
Train: 2018-08-02T00:33:26.678242: step 26648, loss 0.562912.
Train: 2018-08-02T00:33:26.846792: step 26649, loss 0.612135.
Train: 2018-08-02T00:33:27.007394: step 26650, loss 0.496676.
Test: 2018-08-02T00:33:27.538955: step 26650, loss 0.548738.
Train: 2018-08-02T00:33:27.704499: step 26651, loss 0.661591.
Train: 2018-08-02T00:33:27.872052: step 26652, loss 0.562452.
Train: 2018-08-02T00:33:28.037608: step 26653, loss 0.546307.
Train: 2018-08-02T00:33:28.220152: step 26654, loss 0.546328.
Train: 2018-08-02T00:33:28.387704: step 26655, loss 0.529704.
Train: 2018-08-02T00:33:28.557219: step 26656, loss 0.595564.
Train: 2018-08-02T00:33:28.720786: step 26657, loss 0.628517.
Train: 2018-08-02T00:33:28.891366: step 26658, loss 0.595529.
Train: 2018-08-02T00:33:29.064893: step 26659, loss 0.611946.
Train: 2018-08-02T00:33:29.234408: step 26660, loss 0.54617.
Test: 2018-08-02T00:33:29.758009: step 26660, loss 0.548877.
Train: 2018-08-02T00:33:29.931571: step 26661, loss 0.496959.
Train: 2018-08-02T00:33:30.095121: step 26662, loss 0.546198.
Train: 2018-08-02T00:33:30.257674: step 26663, loss 0.529785.
Train: 2018-08-02T00:33:30.422234: step 26664, loss 0.529719.
Train: 2018-08-02T00:33:30.590813: step 26665, loss 0.414322.
Train: 2018-08-02T00:33:30.755353: step 26666, loss 0.57902.
Train: 2018-08-02T00:33:30.929908: step 26667, loss 0.54582.
Train: 2018-08-02T00:33:31.098427: step 26668, loss 0.679411.
Train: 2018-08-02T00:33:31.271961: step 26669, loss 0.596343.
Train: 2018-08-02T00:33:31.435556: step 26670, loss 0.629124.
Test: 2018-08-02T00:33:31.952143: step 26670, loss 0.548507.
Train: 2018-08-02T00:33:32.113738: step 26671, loss 0.478725.
Train: 2018-08-02T00:33:32.283288: step 26672, loss 0.612708.
Train: 2018-08-02T00:33:32.455823: step 26673, loss 0.579281.
Train: 2018-08-02T00:33:32.620388: step 26674, loss 0.529093.
Train: 2018-08-02T00:33:32.787910: step 26675, loss 0.579251.
Train: 2018-08-02T00:33:32.959482: step 26676, loss 0.562049.
Train: 2018-08-02T00:33:33.125009: step 26677, loss 0.528711.
Train: 2018-08-02T00:33:33.290594: step 26678, loss 0.628976.
Train: 2018-08-02T00:33:33.455128: step 26679, loss 0.611795.
Train: 2018-08-02T00:33:33.619686: step 26680, loss 0.613835.
Test: 2018-08-02T00:33:34.151267: step 26680, loss 0.548319.
Train: 2018-08-02T00:33:34.321835: step 26681, loss 0.646565.
Train: 2018-08-02T00:33:34.486401: step 26682, loss 0.613577.
Train: 2018-08-02T00:33:34.648961: step 26683, loss 0.512113.
Train: 2018-08-02T00:33:34.814524: step 26684, loss 0.462011.
Train: 2018-08-02T00:33:34.978086: step 26685, loss 0.495651.
Train: 2018-08-02T00:33:35.144609: step 26686, loss 0.545563.
Train: 2018-08-02T00:33:35.308197: step 26687, loss 0.596174.
Train: 2018-08-02T00:33:35.468743: step 26688, loss 0.596826.
Train: 2018-08-02T00:33:35.632337: step 26689, loss 0.493896.
Train: 2018-08-02T00:33:35.799889: step 26690, loss 0.494069.
Test: 2018-08-02T00:33:36.342422: step 26690, loss 0.548168.
Train: 2018-08-02T00:33:36.508003: step 26691, loss 0.561792.
Train: 2018-08-02T00:33:36.685515: step 26692, loss 0.508958.
Train: 2018-08-02T00:33:36.844093: step 26693, loss 0.614009.
Train: 2018-08-02T00:33:37.006633: step 26694, loss 0.58223.
Train: 2018-08-02T00:33:37.171223: step 26695, loss 0.633755.
Train: 2018-08-02T00:33:37.331762: step 26696, loss 0.614338.
Train: 2018-08-02T00:33:37.493331: step 26697, loss 0.593194.
Train: 2018-08-02T00:33:37.667874: step 26698, loss 0.700967.
Train: 2018-08-02T00:33:37.837438: step 26699, loss 0.560122.
Train: 2018-08-02T00:33:38.001004: step 26700, loss 0.52645.
Test: 2018-08-02T00:33:38.542541: step 26700, loss 0.54806.
Train: 2018-08-02T00:33:39.345644: step 26701, loss 0.494545.
Train: 2018-08-02T00:33:39.515197: step 26702, loss 0.581222.
Train: 2018-08-02T00:33:39.678728: step 26703, loss 0.459532.
Train: 2018-08-02T00:33:39.848275: step 26704, loss 0.531071.
Train: 2018-08-02T00:33:40.012866: step 26705, loss 0.513127.
Train: 2018-08-02T00:33:40.187368: step 26706, loss 0.617679.
Train: 2018-08-02T00:33:40.353924: step 26707, loss 0.495971.
Train: 2018-08-02T00:33:40.520479: step 26708, loss 0.577384.
Train: 2018-08-02T00:33:40.682072: step 26709, loss 0.595813.
Train: 2018-08-02T00:33:40.843647: step 26710, loss 0.511235.
Test: 2018-08-02T00:33:41.380180: step 26710, loss 0.548125.
Train: 2018-08-02T00:33:41.546766: step 26711, loss 0.665261.
Train: 2018-08-02T00:33:41.710325: step 26712, loss 0.561838.
Train: 2018-08-02T00:33:41.872893: step 26713, loss 0.528379.
Train: 2018-08-02T00:33:42.036452: step 26714, loss 0.645526.
Train: 2018-08-02T00:33:42.199022: step 26715, loss 0.562064.
Train: 2018-08-02T00:33:42.359594: step 26716, loss 0.595451.
Train: 2018-08-02T00:33:42.526117: step 26717, loss 0.546023.
Train: 2018-08-02T00:33:42.694667: step 26718, loss 0.546335.
Train: 2018-08-02T00:33:42.864226: step 26719, loss 0.594248.
Train: 2018-08-02T00:33:43.022815: step 26720, loss 0.698874.
Test: 2018-08-02T00:33:43.559356: step 26720, loss 0.548414.
Train: 2018-08-02T00:33:43.725935: step 26721, loss 0.628782.
Train: 2018-08-02T00:33:43.885510: step 26722, loss 0.597767.
Train: 2018-08-02T00:33:44.050068: step 26723, loss 0.512636.
Train: 2018-08-02T00:33:44.218592: step 26724, loss 0.498055.
Train: 2018-08-02T00:33:44.390134: step 26725, loss 0.512912.
Train: 2018-08-02T00:33:44.559682: step 26726, loss 0.529953.
Train: 2018-08-02T00:33:44.735211: step 26727, loss 0.52961.
Train: 2018-08-02T00:33:44.901767: step 26728, loss 0.595186.
Train: 2018-08-02T00:33:45.067354: step 26729, loss 0.628778.
Train: 2018-08-02T00:33:45.228924: step 26730, loss 0.513323.
Test: 2018-08-02T00:33:45.755484: step 26730, loss 0.548849.
Train: 2018-08-02T00:33:45.919047: step 26731, loss 0.546265.
Train: 2018-08-02T00:33:46.096572: step 26732, loss 0.513086.
Train: 2018-08-02T00:33:46.260166: step 26733, loss 0.595671.
Train: 2018-08-02T00:33:46.420711: step 26734, loss 0.446814.
Train: 2018-08-02T00:33:46.596236: step 26735, loss 0.5957.
Train: 2018-08-02T00:33:46.758802: step 26736, loss 0.579228.
Train: 2018-08-02T00:33:46.917378: step 26737, loss 0.56263.
Train: 2018-08-02T00:33:47.082936: step 26738, loss 0.479149.
Train: 2018-08-02T00:33:47.245507: step 26739, loss 0.529153.
Train: 2018-08-02T00:33:47.404102: step 26740, loss 0.680172.
Test: 2018-08-02T00:33:47.939645: step 26740, loss 0.548506.
Train: 2018-08-02T00:33:48.103209: step 26741, loss 0.478537.
Train: 2018-08-02T00:33:48.269763: step 26742, loss 0.478421.
Train: 2018-08-02T00:33:48.431355: step 26743, loss 0.630166.
Train: 2018-08-02T00:33:48.594894: step 26744, loss 0.511773.
Train: 2018-08-02T00:33:48.764440: step 26745, loss 0.460845.
Train: 2018-08-02T00:33:48.927032: step 26746, loss 0.545466.
Train: 2018-08-02T00:33:49.091566: step 26747, loss 0.596695.
Train: 2018-08-02T00:33:49.254132: step 26748, loss 0.52839.
Train: 2018-08-02T00:33:49.417719: step 26749, loss 0.459582.
Train: 2018-08-02T00:33:49.578292: step 26750, loss 0.528102.
Test: 2018-08-02T00:33:50.111838: step 26750, loss 0.548034.
Train: 2018-08-02T00:33:50.273407: step 26751, loss 0.631591.
Train: 2018-08-02T00:33:50.436994: step 26752, loss 0.510517.
Train: 2018-08-02T00:33:50.603523: step 26753, loss 0.562465.
Train: 2018-08-02T00:33:50.767086: step 26754, loss 0.527705.
Train: 2018-08-02T00:33:50.932660: step 26755, loss 0.545021.
Train: 2018-08-02T00:33:51.095235: step 26756, loss 0.545025.
Train: 2018-08-02T00:33:51.257804: step 26757, loss 0.615101.
Train: 2018-08-02T00:33:51.420373: step 26758, loss 0.650378.
Train: 2018-08-02T00:33:51.582906: step 26759, loss 0.58015.
Train: 2018-08-02T00:33:51.746499: step 26760, loss 0.562707.
Test: 2018-08-02T00:33:52.287024: step 26760, loss 0.547848.
Train: 2018-08-02T00:33:52.447624: step 26761, loss 0.527291.
Train: 2018-08-02T00:33:52.611157: step 26762, loss 0.685958.
Train: 2018-08-02T00:33:52.786688: step 26763, loss 0.562412.
Train: 2018-08-02T00:33:52.950255: step 26764, loss 0.527259.
Train: 2018-08-02T00:33:53.119797: step 26765, loss 0.457048.
Train: 2018-08-02T00:33:53.295327: step 26766, loss 0.7024.
Train: 2018-08-02T00:33:53.454934: step 26767, loss 0.579498.
Train: 2018-08-02T00:33:53.618489: step 26768, loss 0.527558.
Train: 2018-08-02T00:33:53.782026: step 26769, loss 0.54572.
Train: 2018-08-02T00:33:53.953568: step 26770, loss 0.598121.
Test: 2018-08-02T00:33:54.491132: step 26770, loss 0.547916.
Train: 2018-08-02T00:33:54.660678: step 26771, loss 0.579661.
Train: 2018-08-02T00:33:54.823274: step 26772, loss 0.723187.
Train: 2018-08-02T00:33:54.989798: step 26773, loss 0.563956.
Train: 2018-08-02T00:33:55.149372: step 26774, loss 0.526839.
Train: 2018-08-02T00:33:55.324902: step 26775, loss 0.545318.
Train: 2018-08-02T00:33:55.493478: step 26776, loss 0.597225.
Train: 2018-08-02T00:33:55.661043: step 26777, loss 0.562161.
Train: 2018-08-02T00:33:55.827559: step 26778, loss 0.630828.
Train: 2018-08-02T00:33:55.987159: step 26779, loss 0.425127.
Train: 2018-08-02T00:33:56.154684: step 26780, loss 0.477115.
Test: 2018-08-02T00:33:56.689256: step 26780, loss 0.548136.
Train: 2018-08-02T00:33:56.858826: step 26781, loss 0.476952.
Train: 2018-08-02T00:33:57.023363: step 26782, loss 0.493914.
Train: 2018-08-02T00:33:57.187923: step 26783, loss 0.614059.
Train: 2018-08-02T00:33:57.349526: step 26784, loss 0.458804.
Train: 2018-08-02T00:33:57.514052: step 26785, loss 0.580192.
Train: 2018-08-02T00:33:57.681603: step 26786, loss 0.562195.
Train: 2018-08-02T00:33:57.851149: step 26787, loss 0.545193.
Train: 2018-08-02T00:33:58.014711: step 26788, loss 0.458264.
Train: 2018-08-02T00:33:58.176280: step 26789, loss 0.563107.
Train: 2018-08-02T00:33:58.340841: step 26790, loss 0.58001.
Test: 2018-08-02T00:33:58.861451: step 26790, loss 0.547879.
Train: 2018-08-02T00:33:59.026009: step 26791, loss 0.545262.
Train: 2018-08-02T00:33:59.185608: step 26792, loss 0.615147.
Train: 2018-08-02T00:33:59.353160: step 26793, loss 0.580047.
Train: 2018-08-02T00:33:59.512734: step 26794, loss 0.702853.
Train: 2018-08-02T00:33:59.675299: step 26795, loss 0.509997.
Train: 2018-08-02T00:33:59.851811: step 26796, loss 0.649981.
Train: 2018-08-02T00:34:00.020351: step 26797, loss 0.649835.
Train: 2018-08-02T00:34:00.197877: step 26798, loss 0.667257.
Train: 2018-08-02T00:34:00.363464: step 26799, loss 0.492817.
Train: 2018-08-02T00:34:00.524029: step 26800, loss 0.632021.
Test: 2018-08-02T00:34:01.053609: step 26800, loss 0.547992.
Train: 2018-08-02T00:34:01.878884: step 26801, loss 0.579687.
Train: 2018-08-02T00:34:02.046436: step 26802, loss 0.579513.
Train: 2018-08-02T00:34:02.209006: step 26803, loss 0.6482.
Train: 2018-08-02T00:34:02.383503: step 26804, loss 0.562778.
Train: 2018-08-02T00:34:02.542105: step 26805, loss 0.579372.
Train: 2018-08-02T00:34:02.716613: step 26806, loss 0.579808.
Train: 2018-08-02T00:34:02.876213: step 26807, loss 0.49468.
Train: 2018-08-02T00:34:03.041775: step 26808, loss 0.528354.
Train: 2018-08-02T00:34:03.205315: step 26809, loss 0.579877.
Train: 2018-08-02T00:34:03.372890: step 26810, loss 0.494091.
Test: 2018-08-02T00:34:03.900480: step 26810, loss 0.548338.
Train: 2018-08-02T00:34:04.068000: step 26811, loss 0.647113.
Train: 2018-08-02T00:34:04.229569: step 26812, loss 0.664079.
Train: 2018-08-02T00:34:04.395126: step 26813, loss 0.528944.
Train: 2018-08-02T00:34:04.558714: step 26814, loss 0.612952.
Train: 2018-08-02T00:34:04.714298: step 26815, loss 0.512083.
Train: 2018-08-02T00:34:04.875873: step 26816, loss 0.59629.
Train: 2018-08-02T00:34:05.037410: step 26817, loss 0.461497.
Train: 2018-08-02T00:34:05.204962: step 26818, loss 0.561914.
Train: 2018-08-02T00:34:05.375505: step 26819, loss 0.629392.
Train: 2018-08-02T00:34:05.542061: step 26820, loss 0.479419.
Test: 2018-08-02T00:34:06.067687: step 26820, loss 0.548507.
Train: 2018-08-02T00:34:06.232242: step 26821, loss 0.579701.
Train: 2018-08-02T00:34:06.453845: step 26822, loss 0.545456.
Train: 2018-08-02T00:34:06.617377: step 26823, loss 0.645651.
Train: 2018-08-02T00:34:06.780967: step 26824, loss 0.529438.
Train: 2018-08-02T00:34:06.941537: step 26825, loss 0.546049.
Train: 2018-08-02T00:34:07.106104: step 26826, loss 0.478886.
Train: 2018-08-02T00:34:07.266669: step 26827, loss 0.595913.
Train: 2018-08-02T00:34:07.433197: step 26828, loss 0.612774.
Train: 2018-08-02T00:34:07.593800: step 26829, loss 0.596439.
Train: 2018-08-02T00:34:07.757356: step 26830, loss 0.562532.
Test: 2018-08-02T00:34:08.297887: step 26830, loss 0.548427.
Train: 2018-08-02T00:34:08.460478: step 26831, loss 0.511837.
Train: 2018-08-02T00:34:08.622051: step 26832, loss 0.629849.
Train: 2018-08-02T00:34:08.786611: step 26833, loss 0.529562.
Train: 2018-08-02T00:34:08.952136: step 26834, loss 0.629443.
Train: 2018-08-02T00:34:09.117725: step 26835, loss 0.49488.
Train: 2018-08-02T00:34:09.282281: step 26836, loss 0.545358.
Train: 2018-08-02T00:34:09.441858: step 26837, loss 0.596436.
Train: 2018-08-02T00:34:09.616361: step 26838, loss 0.545671.
Train: 2018-08-02T00:34:09.777930: step 26839, loss 0.697173.
Train: 2018-08-02T00:34:09.942519: step 26840, loss 0.528854.
Test: 2018-08-02T00:34:10.481049: step 26840, loss 0.54841.
Train: 2018-08-02T00:34:10.647630: step 26841, loss 0.546271.
Train: 2018-08-02T00:34:10.811194: step 26842, loss 0.495465.
Train: 2018-08-02T00:34:10.974731: step 26843, loss 0.680126.
Train: 2018-08-02T00:34:11.139290: step 26844, loss 0.428576.
Train: 2018-08-02T00:34:11.305871: step 26845, loss 0.545772.
Train: 2018-08-02T00:34:11.466417: step 26846, loss 0.562742.
Train: 2018-08-02T00:34:11.627985: step 26847, loss 0.51212.
Train: 2018-08-02T00:34:11.792570: step 26848, loss 0.511898.
Train: 2018-08-02T00:34:11.955110: step 26849, loss 0.698042.
Train: 2018-08-02T00:34:12.117706: step 26850, loss 0.528485.
Test: 2018-08-02T00:34:12.655237: step 26850, loss 0.548259.
Train: 2018-08-02T00:34:12.824785: step 26851, loss 0.596479.
Train: 2018-08-02T00:34:12.986379: step 26852, loss 0.562509.
Train: 2018-08-02T00:34:13.150937: step 26853, loss 0.562385.
Train: 2018-08-02T00:34:13.314476: step 26854, loss 0.392798.
Train: 2018-08-02T00:34:13.481063: step 26855, loss 0.613401.
Train: 2018-08-02T00:34:13.645603: step 26856, loss 0.596526.
Train: 2018-08-02T00:34:13.815137: step 26857, loss 0.494204.
Train: 2018-08-02T00:34:13.977727: step 26858, loss 0.528256.
Train: 2018-08-02T00:34:14.143260: step 26859, loss 0.579561.
Train: 2018-08-02T00:34:14.305826: step 26860, loss 0.5967.
Test: 2018-08-02T00:34:14.844386: step 26860, loss 0.548065.
Train: 2018-08-02T00:34:15.005954: step 26861, loss 0.579618.
Train: 2018-08-02T00:34:15.176498: step 26862, loss 0.528047.
Train: 2018-08-02T00:34:15.345047: step 26863, loss 0.596886.
Train: 2018-08-02T00:34:15.507639: step 26864, loss 0.493649.
Train: 2018-08-02T00:34:15.672172: step 26865, loss 0.614127.
Train: 2018-08-02T00:34:15.835783: step 26866, loss 0.631338.
Train: 2018-08-02T00:34:15.996306: step 26867, loss 0.527985.
Train: 2018-08-02T00:34:16.160866: step 26868, loss 0.493526.
Train: 2018-08-02T00:34:16.325427: step 26869, loss 0.631451.
Train: 2018-08-02T00:34:16.484999: step 26870, loss 0.562564.
Test: 2018-08-02T00:34:17.024558: step 26870, loss 0.548002.
Train: 2018-08-02T00:34:17.191144: step 26871, loss 0.648758.
Train: 2018-08-02T00:34:17.368638: step 26872, loss 0.545219.
Train: 2018-08-02T00:34:17.537187: step 26873, loss 0.596879.
Train: 2018-08-02T00:34:17.704740: step 26874, loss 0.562449.
Train: 2018-08-02T00:34:17.872291: step 26875, loss 0.562436.
Train: 2018-08-02T00:34:18.038872: step 26876, loss 0.476648.
Train: 2018-08-02T00:34:18.205431: step 26877, loss 0.528185.
Train: 2018-08-02T00:34:18.370957: step 26878, loss 0.613951.
Train: 2018-08-02T00:34:18.537512: step 26879, loss 0.476582.
Train: 2018-08-02T00:34:18.710052: step 26880, loss 0.545251.
Test: 2018-08-02T00:34:19.249641: step 26880, loss 0.548031.
Train: 2018-08-02T00:34:19.428132: step 26881, loss 0.476404.
Train: 2018-08-02T00:34:19.597679: step 26882, loss 0.596923.
Train: 2018-08-02T00:34:19.760269: step 26883, loss 0.527903.
Train: 2018-08-02T00:34:19.921837: step 26884, loss 0.631602.
Train: 2018-08-02T00:34:20.084378: step 26885, loss 0.683393.
Train: 2018-08-02T00:34:20.261903: step 26886, loss 0.597023.
Train: 2018-08-02T00:34:20.437434: step 26887, loss 0.527995.
Train: 2018-08-02T00:34:20.607005: step 26888, loss 0.493219.
Train: 2018-08-02T00:34:20.776528: step 26889, loss 0.422346.
Train: 2018-08-02T00:34:20.941087: step 26890, loss 0.598404.
Test: 2018-08-02T00:34:21.468677: step 26890, loss 0.547804.
Train: 2018-08-02T00:34:21.631243: step 26891, loss 0.58038.
Train: 2018-08-02T00:34:21.799792: step 26892, loss 0.442515.
Train: 2018-08-02T00:34:21.964352: step 26893, loss 0.628508.
Train: 2018-08-02T00:34:22.125946: step 26894, loss 0.614427.
Train: 2018-08-02T00:34:22.290481: step 26895, loss 0.403138.
Train: 2018-08-02T00:34:22.454068: step 26896, loss 0.490995.
Train: 2018-08-02T00:34:22.617638: step 26897, loss 0.578076.
Train: 2018-08-02T00:34:22.780202: step 26898, loss 0.639652.
Train: 2018-08-02T00:34:22.944757: step 26899, loss 0.553339.
Train: 2018-08-02T00:34:23.107296: step 26900, loss 0.580867.
Test: 2018-08-02T00:34:23.628906: step 26900, loss 0.547678.
Train: 2018-08-02T00:34:24.409915: step 26901, loss 0.478863.
Train: 2018-08-02T00:34:24.575497: step 26902, loss 0.532413.
Train: 2018-08-02T00:34:24.738039: step 26903, loss 0.617898.
Train: 2018-08-02T00:34:24.902627: step 26904, loss 0.535091.
Train: 2018-08-02T00:34:25.073175: step 26905, loss 0.512593.
Train: 2018-08-02T00:34:25.237730: step 26906, loss 0.53038.
Train: 2018-08-02T00:34:25.401265: step 26907, loss 0.518713.
Train: 2018-08-02T00:34:25.568817: step 26908, loss 0.627635.
Train: 2018-08-02T00:34:25.731382: step 26909, loss 0.606002.
Train: 2018-08-02T00:34:25.894978: step 26910, loss 0.671952.
Test: 2018-08-02T00:34:26.426526: step 26910, loss 0.547929.
Train: 2018-08-02T00:34:26.592114: step 26911, loss 0.526851.
Train: 2018-08-02T00:34:26.760662: step 26912, loss 0.564408.
Train: 2018-08-02T00:34:26.922234: step 26913, loss 0.579367.
Train: 2018-08-02T00:34:27.103740: step 26914, loss 0.510699.
Train: 2018-08-02T00:34:27.267302: step 26915, loss 0.562468.
Train: 2018-08-02T00:34:27.428871: step 26916, loss 0.597141.
Train: 2018-08-02T00:34:27.594402: step 26917, loss 0.527916.
Train: 2018-08-02T00:34:27.755004: step 26918, loss 0.597091.
Train: 2018-08-02T00:34:27.914578: step 26919, loss 0.476173.
Train: 2018-08-02T00:34:28.078110: step 26920, loss 0.476171.
Test: 2018-08-02T00:34:28.609689: step 26920, loss 0.548043.
Train: 2018-08-02T00:34:28.771267: step 26921, loss 0.56252.
Train: 2018-08-02T00:34:28.936814: step 26922, loss 0.545218.
Train: 2018-08-02T00:34:29.098382: step 26923, loss 0.545203.
Train: 2018-08-02T00:34:29.263940: step 26924, loss 0.631885.
Train: 2018-08-02T00:34:29.424541: step 26925, loss 0.527861.
Train: 2018-08-02T00:34:29.587100: step 26926, loss 0.59723.
Train: 2018-08-02T00:34:29.751660: step 26927, loss 0.475766.
Train: 2018-08-02T00:34:29.916195: step 26928, loss 0.545181.
Train: 2018-08-02T00:34:30.078762: step 26929, loss 0.719267.
Train: 2018-08-02T00:34:30.251300: step 26930, loss 0.458307.
Test: 2018-08-02T00:34:30.790886: step 26930, loss 0.547974.
Train: 2018-08-02T00:34:30.954421: step 26931, loss 0.545149.
Train: 2018-08-02T00:34:31.117984: step 26932, loss 0.579913.
Train: 2018-08-02T00:34:31.282544: step 26933, loss 0.649476.
Train: 2018-08-02T00:34:31.449098: step 26934, loss 0.632022.
Train: 2018-08-02T00:34:31.625652: step 26935, loss 0.441095.
Train: 2018-08-02T00:34:31.798196: step 26936, loss 0.493137.
Train: 2018-08-02T00:34:31.964750: step 26937, loss 0.510445.
Train: 2018-08-02T00:34:32.129279: step 26938, loss 0.579899.
Train: 2018-08-02T00:34:32.293864: step 26939, loss 0.527733.
Train: 2018-08-02T00:34:32.462420: step 26940, loss 0.527695.
Test: 2018-08-02T00:34:32.993994: step 26940, loss 0.547924.
Train: 2018-08-02T00:34:33.159552: step 26941, loss 0.562527.
Train: 2018-08-02T00:34:33.322091: step 26942, loss 0.52761.
Train: 2018-08-02T00:34:33.486652: step 26943, loss 0.510077.
Train: 2018-08-02T00:34:33.650270: step 26944, loss 0.562542.
Train: 2018-08-02T00:34:33.812804: step 26945, loss 0.545002.
Train: 2018-08-02T00:34:33.977372: step 26946, loss 0.562557.
Train: 2018-08-02T00:34:34.142898: step 26947, loss 0.527363.
Train: 2018-08-02T00:34:34.310491: step 26948, loss 0.668339.
Train: 2018-08-02T00:34:34.475009: step 26949, loss 0.615462.
Train: 2018-08-02T00:34:34.645554: step 26950, loss 0.474465.
Test: 2018-08-02T00:34:35.176142: step 26950, loss 0.547807.
Train: 2018-08-02T00:34:35.345707: step 26951, loss 0.54494.
Train: 2018-08-02T00:34:35.516270: step 26952, loss 0.597844.
Train: 2018-08-02T00:34:35.688764: step 26953, loss 0.509659.
Train: 2018-08-02T00:34:35.859339: step 26954, loss 0.456695.
Train: 2018-08-02T00:34:36.032845: step 26955, loss 0.509558.
Train: 2018-08-02T00:34:36.195409: step 26956, loss 0.580304.
Train: 2018-08-02T00:34:36.357975: step 26957, loss 0.456191.
Train: 2018-08-02T00:34:36.525528: step 26958, loss 0.527065.
Train: 2018-08-02T00:34:36.694078: step 26959, loss 0.473525.
Train: 2018-08-02T00:34:36.859664: step 26960, loss 0.544793.
Test: 2018-08-02T00:34:37.395234: step 26960, loss 0.547686.
Train: 2018-08-02T00:34:37.567742: step 26961, loss 0.580635.
Train: 2018-08-02T00:34:37.733298: step 26962, loss 0.508805.
Train: 2018-08-02T00:34:37.896888: step 26963, loss 0.490676.
Train: 2018-08-02T00:34:38.060425: step 26964, loss 0.508571.
Train: 2018-08-02T00:34:38.235985: step 26965, loss 0.617217.
Train: 2018-08-02T00:34:38.400540: step 26966, loss 0.581023.
Train: 2018-08-02T00:34:38.563111: step 26967, loss 0.490104.
Train: 2018-08-02T00:34:38.732654: step 26968, loss 0.526447.
Train: 2018-08-02T00:34:38.899183: step 26969, loss 0.709062.
Train: 2018-08-02T00:34:39.076707: step 26970, loss 0.800409.
Test: 2018-08-02T00:34:39.618291: step 26970, loss 0.547639.
Train: 2018-08-02T00:34:39.790826: step 26971, loss 0.562895.
Train: 2018-08-02T00:34:39.955392: step 26972, loss 0.526512.
Train: 2018-08-02T00:34:40.130923: step 26973, loss 0.599102.
Train: 2018-08-02T00:34:40.306421: step 26974, loss 0.580891.
Train: 2018-08-02T00:34:40.479982: step 26975, loss 0.544718.
Train: 2018-08-02T00:34:40.644517: step 26976, loss 0.526722.
Train: 2018-08-02T00:34:40.815086: step 26977, loss 0.526763.
Train: 2018-08-02T00:34:40.981646: step 26978, loss 0.508838.
Train: 2018-08-02T00:34:41.157146: step 26979, loss 0.490922.
Train: 2018-08-02T00:34:41.324729: step 26980, loss 0.598589.
Test: 2018-08-02T00:34:41.858282: step 26980, loss 0.547672.
Train: 2018-08-02T00:34:42.025850: step 26981, loss 0.544756.
Train: 2018-08-02T00:34:42.189386: step 26982, loss 0.508895.
Train: 2018-08-02T00:34:42.352950: step 26983, loss 0.508848.
Train: 2018-08-02T00:34:42.521530: step 26984, loss 0.670534.
Train: 2018-08-02T00:34:42.691076: step 26985, loss 0.473013.
Train: 2018-08-02T00:34:42.856629: step 26986, loss 0.580644.
Train: 2018-08-02T00:34:43.022193: step 26987, loss 0.61657.
Train: 2018-08-02T00:34:43.188745: step 26988, loss 0.544764.
Train: 2018-08-02T00:34:43.353300: step 26989, loss 0.526861.
Train: 2018-08-02T00:34:43.517835: step 26990, loss 0.598487.
Test: 2018-08-02T00:34:44.044428: step 26990, loss 0.547691.
Train: 2018-08-02T00:34:44.210010: step 26991, loss 0.509003.
Train: 2018-08-02T00:34:44.380528: step 26992, loss 0.616326.
Train: 2018-08-02T00:34:44.543122: step 26993, loss 0.669876.
Train: 2018-08-02T00:34:44.709649: step 26994, loss 0.526979.
Train: 2018-08-02T00:34:44.873238: step 26995, loss 0.616033.
Train: 2018-08-02T00:34:45.038770: step 26996, loss 0.633663.
Train: 2018-08-02T00:34:45.205324: step 26997, loss 0.686579.
Train: 2018-08-02T00:34:45.369915: step 26998, loss 0.50963.
Train: 2018-08-02T00:34:45.537436: step 26999, loss 0.597709.
Train: 2018-08-02T00:34:45.710998: step 27000, loss 0.65016.
Test: 2018-08-02T00:34:46.236576: step 27000, loss 0.54788.
Train: 2018-08-02T00:34:47.009733: step 27001, loss 0.527583.
Train: 2018-08-02T00:34:47.177311: step 27002, loss 0.614686.
Train: 2018-08-02T00:34:47.354810: step 27003, loss 0.562475.
Train: 2018-08-02T00:34:47.521391: step 27004, loss 0.614307.
Train: 2018-08-02T00:34:47.693934: step 27005, loss 0.441946.
Train: 2018-08-02T00:34:47.860484: step 27006, loss 0.528091.
Train: 2018-08-02T00:34:48.042007: step 27007, loss 0.493806.
Train: 2018-08-02T00:34:48.208528: step 27008, loss 0.545301.
Train: 2018-08-02T00:34:48.372091: step 27009, loss 0.63109.
Train: 2018-08-02T00:34:48.551611: step 27010, loss 0.562459.
Test: 2018-08-02T00:34:49.076207: step 27010, loss 0.548115.
Train: 2018-08-02T00:34:49.244757: step 27011, loss 0.511071.
Train: 2018-08-02T00:34:49.409346: step 27012, loss 0.596713.
Train: 2018-08-02T00:34:49.578865: step 27013, loss 0.630934.
Train: 2018-08-02T00:34:49.740433: step 27014, loss 0.562457.
Train: 2018-08-02T00:34:49.909017: step 27015, loss 0.562458.
Train: 2018-08-02T00:34:50.075564: step 27016, loss 0.460093.
Train: 2018-08-02T00:34:50.240127: step 27017, loss 0.596592.
Train: 2018-08-02T00:34:50.406652: step 27018, loss 0.613654.
Train: 2018-08-02T00:34:50.573232: step 27019, loss 0.52835.
Train: 2018-08-02T00:34:50.741755: step 27020, loss 0.528355.
Test: 2018-08-02T00:34:51.272349: step 27020, loss 0.548174.
Train: 2018-08-02T00:34:51.440887: step 27021, loss 0.528345.
Train: 2018-08-02T00:34:51.606444: step 27022, loss 0.528321.
Train: 2018-08-02T00:34:51.773000: step 27023, loss 0.613709.
Train: 2018-08-02T00:34:51.936568: step 27024, loss 0.477.
Train: 2018-08-02T00:34:52.101140: step 27025, loss 0.511111.
Train: 2018-08-02T00:34:52.267677: step 27026, loss 0.596741.
Train: 2018-08-02T00:34:52.435229: step 27027, loss 0.476618.
Train: 2018-08-02T00:34:52.600786: step 27028, loss 0.545247.
Train: 2018-08-02T00:34:52.776317: step 27029, loss 0.510732.
Train: 2018-08-02T00:34:52.937910: step 27030, loss 0.579739.
Test: 2018-08-02T00:34:53.480435: step 27030, loss 0.547956.
Train: 2018-08-02T00:34:53.649981: step 27031, loss 0.527821.
Train: 2018-08-02T00:34:53.827507: step 27032, loss 0.64925.
Train: 2018-08-02T00:34:53.990114: step 27033, loss 0.545093.
Train: 2018-08-02T00:34:54.156655: step 27034, loss 0.545078.
Train: 2018-08-02T00:34:54.320220: step 27035, loss 0.510243.
Train: 2018-08-02T00:34:54.487790: step 27036, loss 0.59735.
Train: 2018-08-02T00:34:54.654297: step 27037, loss 0.597386.
Train: 2018-08-02T00:34:54.819880: step 27038, loss 0.527562.
Train: 2018-08-02T00:34:54.984426: step 27039, loss 0.492596.
Train: 2018-08-02T00:34:55.150970: step 27040, loss 0.632476.
Test: 2018-08-02T00:34:55.686537: step 27040, loss 0.547835.
Train: 2018-08-02T00:34:55.857081: step 27041, loss 0.579995.
Train: 2018-08-02T00:34:56.025657: step 27042, loss 0.544987.
Train: 2018-08-02T00:34:56.189216: step 27043, loss 0.61502.
Train: 2018-08-02T00:34:56.352782: step 27044, loss 0.509987.
Train: 2018-08-02T00:34:56.532307: step 27045, loss 0.562491.
Train: 2018-08-02T00:34:56.698832: step 27046, loss 0.562491.
Train: 2018-08-02T00:34:56.867381: step 27047, loss 0.52748.
Train: 2018-08-02T00:34:57.031941: step 27048, loss 0.597516.
Train: 2018-08-02T00:34:57.194526: step 27049, loss 0.66755.
Train: 2018-08-02T00:34:57.366048: step 27050, loss 0.492544.
Test: 2018-08-02T00:34:57.897627: step 27050, loss 0.547848.
Train: 2018-08-02T00:34:58.061190: step 27051, loss 0.579959.
Train: 2018-08-02T00:34:58.226747: step 27052, loss 0.632336.
Train: 2018-08-02T00:34:58.401311: step 27053, loss 0.545035.
Train: 2018-08-02T00:34:58.566862: step 27054, loss 0.579886.
Train: 2018-08-02T00:34:58.731433: step 27055, loss 0.510281.
Train: 2018-08-02T00:34:58.894991: step 27056, loss 0.579845.
Train: 2018-08-02T00:34:59.062512: step 27057, loss 0.527719.
Train: 2018-08-02T00:34:59.227072: step 27058, loss 0.52773.
Train: 2018-08-02T00:34:59.389672: step 27059, loss 0.492996.
Train: 2018-08-02T00:34:59.556193: step 27060, loss 0.423416.
Test: 2018-08-02T00:35:00.090771: step 27060, loss 0.547879.
Train: 2018-08-02T00:35:00.258317: step 27061, loss 0.649579.
Train: 2018-08-02T00:35:00.426890: step 27062, loss 0.52759.
Train: 2018-08-02T00:35:00.595415: step 27063, loss 0.562477.
Train: 2018-08-02T00:35:00.764986: step 27064, loss 0.632418.
Train: 2018-08-02T00:35:00.929553: step 27065, loss 0.63243.
Train: 2018-08-02T00:35:01.094081: step 27066, loss 0.579953.
Train: 2018-08-02T00:35:01.255650: step 27067, loss 0.562474.
Train: 2018-08-02T00:35:01.424198: step 27068, loss 0.562471.
Train: 2018-08-02T00:35:01.590784: step 27069, loss 0.597326.
Train: 2018-08-02T00:35:01.752355: step 27070, loss 0.614688.
Test: 2018-08-02T00:35:02.283902: step 27070, loss 0.547905.
Train: 2018-08-02T00:35:02.453448: step 27071, loss 0.562458.
Train: 2018-08-02T00:35:02.617035: step 27072, loss 0.597153.
Train: 2018-08-02T00:35:02.795534: step 27073, loss 0.666352.
Train: 2018-08-02T00:35:02.959126: step 27074, loss 0.596974.
Train: 2018-08-02T00:35:03.123656: step 27075, loss 0.424742.
Train: 2018-08-02T00:35:03.293229: step 27076, loss 0.614015.
Train: 2018-08-02T00:35:03.465742: step 27077, loss 0.493783.
Train: 2018-08-02T00:35:03.632322: step 27078, loss 0.596741.
Train: 2018-08-02T00:35:03.795884: step 27079, loss 0.579572.
Train: 2018-08-02T00:35:03.970393: step 27080, loss 0.562438.
Test: 2018-08-02T00:35:04.509984: step 27080, loss 0.548121.
Train: 2018-08-02T00:35:04.689496: step 27081, loss 0.579536.
Train: 2018-08-02T00:35:04.864033: step 27082, loss 0.54536.
Train: 2018-08-02T00:35:05.032552: step 27083, loss 0.528308.
Train: 2018-08-02T00:35:05.198142: step 27084, loss 0.664807.
Train: 2018-08-02T00:35:05.367657: step 27085, loss 0.562442.
Train: 2018-08-02T00:35:05.542190: step 27086, loss 0.647511.
Train: 2018-08-02T00:35:05.717722: step 27087, loss 0.545473.
Train: 2018-08-02T00:35:05.892255: step 27088, loss 0.545507.
Train: 2018-08-02T00:35:06.058810: step 27089, loss 0.630149.
Train: 2018-08-02T00:35:06.226362: step 27090, loss 0.562463.
Test: 2018-08-02T00:35:06.755946: step 27090, loss 0.548352.
Train: 2018-08-02T00:35:06.924521: step 27091, loss 0.562469.
Train: 2018-08-02T00:35:07.090081: step 27092, loss 0.629806.
Train: 2018-08-02T00:35:07.260622: step 27093, loss 0.612871.
Train: 2018-08-02T00:35:07.427177: step 27094, loss 0.512241.
Train: 2018-08-02T00:35:07.595702: step 27095, loss 0.545781.
Train: 2018-08-02T00:35:07.761259: step 27096, loss 0.579219.
Train: 2018-08-02T00:35:07.935792: step 27097, loss 0.629264.
Train: 2018-08-02T00:35:08.099379: step 27098, loss 0.562531.
Train: 2018-08-02T00:35:08.262946: step 27099, loss 0.529284.
Train: 2018-08-02T00:35:08.430470: step 27100, loss 0.612388.
Test: 2018-08-02T00:35:08.966051: step 27100, loss 0.548665.
Train: 2018-08-02T00:35:09.721699: step 27101, loss 0.711872.
Train: 2018-08-02T00:35:09.889251: step 27102, loss 0.628737.
Train: 2018-08-02T00:35:10.060820: step 27103, loss 0.56261.
Train: 2018-08-02T00:35:10.228346: step 27104, loss 0.464063.
Train: 2018-08-02T00:35:10.393935: step 27105, loss 0.579058.
Train: 2018-08-02T00:35:10.565470: step 27106, loss 0.579049.
Train: 2018-08-02T00:35:10.731002: step 27107, loss 0.644497.
Train: 2018-08-02T00:35:10.905560: step 27108, loss 0.628016.
Train: 2018-08-02T00:35:11.071092: step 27109, loss 0.693017.
Train: 2018-08-02T00:35:11.236680: step 27110, loss 0.562772.
Test: 2018-08-02T00:35:11.767231: step 27110, loss 0.549277.
Train: 2018-08-02T00:35:11.941797: step 27111, loss 0.562814.
Train: 2018-08-02T00:35:12.107322: step 27112, loss 0.530616.
Train: 2018-08-02T00:35:12.276868: step 27113, loss 0.562878.
Train: 2018-08-02T00:35:12.438437: step 27114, loss 0.578959.
Train: 2018-08-02T00:35:12.606986: step 27115, loss 0.514819.
Train: 2018-08-02T00:35:12.773540: step 27116, loss 0.498824.
Train: 2018-08-02T00:35:12.946079: step 27117, loss 0.530846.
Train: 2018-08-02T00:35:13.112668: step 27118, loss 0.466565.
Train: 2018-08-02T00:35:13.291181: step 27119, loss 0.62726.
Train: 2018-08-02T00:35:13.466719: step 27120, loss 0.514469.
Test: 2018-08-02T00:35:13.992314: step 27120, loss 0.54928.
Train: 2018-08-02T00:35:14.158837: step 27121, loss 0.530489.
Train: 2018-08-02T00:35:14.323398: step 27122, loss 0.497954.
Train: 2018-08-02T00:35:14.487989: step 27123, loss 0.579003.
Train: 2018-08-02T00:35:14.657533: step 27124, loss 0.611645.
Train: 2018-08-02T00:35:14.836028: step 27125, loss 0.562683.
Train: 2018-08-02T00:35:15.001584: step 27126, loss 0.579045.
Train: 2018-08-02T00:35:15.164150: step 27127, loss 0.595472.
Train: 2018-08-02T00:35:15.327713: step 27128, loss 0.628377.
Train: 2018-08-02T00:35:15.490278: step 27129, loss 0.595513.
Train: 2018-08-02T00:35:15.673809: step 27130, loss 0.644858.
Test: 2018-08-02T00:35:16.212392: step 27130, loss 0.548875.
Train: 2018-08-02T00:35:16.380898: step 27131, loss 0.562634.
Train: 2018-08-02T00:35:16.554466: step 27132, loss 0.464122.
Train: 2018-08-02T00:35:16.723014: step 27133, loss 0.464033.
Train: 2018-08-02T00:35:16.893527: step 27134, loss 0.496733.
Train: 2018-08-02T00:35:17.066067: step 27135, loss 0.546073.
Train: 2018-08-02T00:35:17.238605: step 27136, loss 0.645413.
Train: 2018-08-02T00:35:17.415165: step 27137, loss 0.579136.
Train: 2018-08-02T00:35:17.586705: step 27138, loss 0.512644.
Train: 2018-08-02T00:35:17.753258: step 27139, loss 0.662624.
Train: 2018-08-02T00:35:17.920781: step 27140, loss 0.579202.
Test: 2018-08-02T00:35:18.454354: step 27140, loss 0.548568.
Train: 2018-08-02T00:35:18.621939: step 27141, loss 0.629216.
Train: 2018-08-02T00:35:18.790455: step 27142, loss 0.579191.
Train: 2018-08-02T00:35:18.958018: step 27143, loss 0.529223.
Train: 2018-08-02T00:35:19.120573: step 27144, loss 0.529214.
Train: 2018-08-02T00:35:19.284167: step 27145, loss 0.495887.
Train: 2018-08-02T00:35:19.449693: step 27146, loss 0.545902.
Train: 2018-08-02T00:35:19.613288: step 27147, loss 0.562485.
Train: 2018-08-02T00:35:19.777817: step 27148, loss 0.529075.
Train: 2018-08-02T00:35:19.942410: step 27149, loss 0.545741.
Train: 2018-08-02T00:35:20.106961: step 27150, loss 0.62968.
Test: 2018-08-02T00:35:20.639513: step 27150, loss 0.548403.
Train: 2018-08-02T00:35:20.814046: step 27151, loss 0.461597.
Train: 2018-08-02T00:35:20.976613: step 27152, loss 0.528773.
Train: 2018-08-02T00:35:21.141172: step 27153, loss 0.646929.
Train: 2018-08-02T00:35:21.308725: step 27154, loss 0.562464.
Train: 2018-08-02T00:35:21.480266: step 27155, loss 0.579391.
Train: 2018-08-02T00:35:21.645833: step 27156, loss 0.579403.
Train: 2018-08-02T00:35:21.814372: step 27157, loss 0.647226.
Train: 2018-08-02T00:35:21.979955: step 27158, loss 0.477732.
Train: 2018-08-02T00:35:22.143493: step 27159, loss 0.630277.
Train: 2018-08-02T00:35:22.314059: step 27160, loss 0.613308.
Test: 2018-08-02T00:35:22.844644: step 27160, loss 0.548274.
Train: 2018-08-02T00:35:23.012171: step 27161, loss 0.562457.
Train: 2018-08-02T00:35:23.187700: step 27162, loss 0.613232.
Train: 2018-08-02T00:35:23.357247: step 27163, loss 0.64698.
Train: 2018-08-02T00:35:23.523832: step 27164, loss 0.545601.
Train: 2018-08-02T00:35:23.694362: step 27165, loss 0.596153.
Train: 2018-08-02T00:35:23.861925: step 27166, loss 0.612905.
Train: 2018-08-02T00:35:24.031476: step 27167, loss 0.579262.
Train: 2018-08-02T00:35:24.195033: step 27168, loss 0.562504.
Train: 2018-08-02T00:35:24.368545: step 27169, loss 0.445617.
Train: 2018-08-02T00:35:24.545072: step 27170, loss 0.629304.
Test: 2018-08-02T00:35:25.071665: step 27170, loss 0.548551.
Train: 2018-08-02T00:35:25.238250: step 27171, loss 0.479111.
Train: 2018-08-02T00:35:25.403777: step 27172, loss 0.529144.
Train: 2018-08-02T00:35:25.569334: step 27173, loss 0.562513.
Train: 2018-08-02T00:35:25.732898: step 27174, loss 0.579222.
Train: 2018-08-02T00:35:25.899486: step 27175, loss 0.6294.
Train: 2018-08-02T00:35:26.070994: step 27176, loss 0.512348.
Train: 2018-08-02T00:35:26.249542: step 27177, loss 0.478867.
Train: 2018-08-02T00:35:26.414077: step 27178, loss 0.545741.
Train: 2018-08-02T00:35:26.583623: step 27179, loss 0.579268.
Train: 2018-08-02T00:35:26.750178: step 27180, loss 0.612894.
Test: 2018-08-02T00:35:27.293727: step 27180, loss 0.548394.
Train: 2018-08-02T00:35:27.462275: step 27181, loss 0.545659.
Train: 2018-08-02T00:35:27.623842: step 27182, loss 0.511981.
Train: 2018-08-02T00:35:27.786438: step 27183, loss 0.478216.
Train: 2018-08-02T00:35:27.954958: step 27184, loss 0.54556.
Train: 2018-08-02T00:35:28.119517: step 27185, loss 0.511636.
Train: 2018-08-02T00:35:28.287100: step 27186, loss 0.630385.
Train: 2018-08-02T00:35:28.452626: step 27187, loss 0.51139.
Train: 2018-08-02T00:35:28.615191: step 27188, loss 0.630649.
Train: 2018-08-02T00:35:28.790756: step 27189, loss 0.562432.
Train: 2018-08-02T00:35:28.964259: step 27190, loss 0.47697.
Test: 2018-08-02T00:35:29.504840: step 27190, loss 0.548087.
Train: 2018-08-02T00:35:29.678383: step 27191, loss 0.630937.
Train: 2018-08-02T00:35:29.858894: step 27192, loss 0.545285.
Train: 2018-08-02T00:35:30.032432: step 27193, loss 0.407965.
Train: 2018-08-02T00:35:30.194978: step 27194, loss 0.528006.
Train: 2018-08-02T00:35:30.360526: step 27195, loss 0.545169.
Train: 2018-08-02T00:35:30.525087: step 27196, loss 0.527812.
Train: 2018-08-02T00:35:30.691642: step 27197, loss 0.597171.
Train: 2018-08-02T00:35:30.863182: step 27198, loss 0.597255.
Train: 2018-08-02T00:35:31.036724: step 27199, loss 0.545026.
Train: 2018-08-02T00:35:31.205268: step 27200, loss 0.597373.
Test: 2018-08-02T00:35:31.731861: step 27200, loss 0.547834.
Train: 2018-08-02T00:35:32.532138: step 27201, loss 0.562465.
Train: 2018-08-02T00:35:32.702681: step 27202, loss 0.649898.
Train: 2018-08-02T00:35:32.868272: step 27203, loss 0.562466.
Train: 2018-08-02T00:35:33.047786: step 27204, loss 0.579937.
Train: 2018-08-02T00:35:33.214325: step 27205, loss 0.544999.
Train: 2018-08-02T00:35:33.382863: step 27206, loss 0.56246.
Train: 2018-08-02T00:35:33.548421: step 27207, loss 0.614804.
Train: 2018-08-02T00:35:33.720960: step 27208, loss 0.614746.
Train: 2018-08-02T00:35:33.885519: step 27209, loss 0.545046.
Train: 2018-08-02T00:35:34.060052: step 27210, loss 0.649349.
Test: 2018-08-02T00:35:34.576698: step 27210, loss 0.547916.
Train: 2018-08-02T00:35:34.747215: step 27211, loss 0.683836.
Train: 2018-08-02T00:35:34.913800: step 27212, loss 0.63156.
Train: 2018-08-02T00:35:35.091296: step 27213, loss 0.631279.
Train: 2018-08-02T00:35:35.257906: step 27214, loss 0.562425.
Train: 2018-08-02T00:35:35.425434: step 27215, loss 0.494157.
Train: 2018-08-02T00:35:35.593983: step 27216, loss 0.49435.
Train: 2018-08-02T00:35:35.756518: step 27217, loss 0.494465.
Train: 2018-08-02T00:35:35.923073: step 27218, loss 0.545453.
Train: 2018-08-02T00:35:36.090624: step 27219, loss 0.59639.
Train: 2018-08-02T00:35:36.258177: step 27220, loss 0.579403.
Test: 2018-08-02T00:35:36.790759: step 27220, loss 0.548244.
Train: 2018-08-02T00:35:36.955314: step 27221, loss 0.596345.
Train: 2018-08-02T00:35:37.120870: step 27222, loss 0.613244.
Train: 2018-08-02T00:35:37.284464: step 27223, loss 0.494819.
Train: 2018-08-02T00:35:37.448993: step 27224, loss 0.427265.
Train: 2018-08-02T00:35:37.623560: step 27225, loss 0.477848.
Train: 2018-08-02T00:35:37.791079: step 27226, loss 0.61331.
Train: 2018-08-02T00:35:37.957633: step 27227, loss 0.511492.
Train: 2018-08-02T00:35:38.123192: step 27228, loss 0.664512.
Train: 2018-08-02T00:35:38.304715: step 27229, loss 0.494342.
Train: 2018-08-02T00:35:38.476247: step 27230, loss 0.613559.
Test: 2018-08-02T00:35:39.013810: step 27230, loss 0.548146.
Train: 2018-08-02T00:35:39.181362: step 27231, loss 0.528319.
Train: 2018-08-02T00:35:39.347942: step 27232, loss 0.613634.
Train: 2018-08-02T00:35:39.512477: step 27233, loss 0.494131.
Train: 2018-08-02T00:35:39.684018: step 27234, loss 0.579515.
Train: 2018-08-02T00:35:39.851571: step 27235, loss 0.51111.
Train: 2018-08-02T00:35:40.015134: step 27236, loss 0.699438.
Train: 2018-08-02T00:35:40.176728: step 27237, loss 0.511067.
Train: 2018-08-02T00:35:40.345263: step 27238, loss 0.459691.
Train: 2018-08-02T00:35:40.511806: step 27239, loss 0.562421.
Train: 2018-08-02T00:35:40.679358: step 27240, loss 0.528084.
Test: 2018-08-02T00:35:41.218941: step 27240, loss 0.548022.
Train: 2018-08-02T00:35:41.387465: step 27241, loss 0.614009.
Train: 2018-08-02T00:35:41.554020: step 27242, loss 0.596841.
Train: 2018-08-02T00:35:41.721604: step 27243, loss 0.545208.
Train: 2018-08-02T00:35:41.884138: step 27244, loss 0.562422.
Train: 2018-08-02T00:35:42.053684: step 27245, loss 0.665795.
Train: 2018-08-02T00:35:42.220239: step 27246, loss 0.631275.
Train: 2018-08-02T00:35:42.383832: step 27247, loss 0.59679.
Train: 2018-08-02T00:35:42.548361: step 27248, loss 0.66533.
Train: 2018-08-02T00:35:42.714916: step 27249, loss 0.579522.
Train: 2018-08-02T00:35:42.885460: step 27250, loss 0.613577.
Test: 2018-08-02T00:35:43.422056: step 27250, loss 0.548198.
Train: 2018-08-02T00:35:43.590575: step 27251, loss 0.392471.
Train: 2018-08-02T00:35:43.755135: step 27252, loss 0.596397.
Train: 2018-08-02T00:35:43.919695: step 27253, loss 0.562434.
Train: 2018-08-02T00:35:44.083259: step 27254, loss 0.545488.
Train: 2018-08-02T00:35:44.246821: step 27255, loss 0.528563.
Train: 2018-08-02T00:35:44.412379: step 27256, loss 0.443885.
Train: 2018-08-02T00:35:44.592927: step 27257, loss 0.562434.
Train: 2018-08-02T00:35:44.773413: step 27258, loss 0.528466.
Train: 2018-08-02T00:35:44.949967: step 27259, loss 0.528406.
Train: 2018-08-02T00:35:45.111509: step 27260, loss 0.545381.
Test: 2018-08-02T00:35:45.645084: step 27260, loss 0.548122.
Train: 2018-08-02T00:35:45.808646: step 27261, loss 0.699034.
Train: 2018-08-02T00:35:45.972208: step 27262, loss 0.511188.
Train: 2018-08-02T00:35:46.138764: step 27263, loss 0.579511.
Train: 2018-08-02T00:35:46.318284: step 27264, loss 0.528225.
Train: 2018-08-02T00:35:46.488828: step 27265, loss 0.545308.
Train: 2018-08-02T00:35:46.658408: step 27266, loss 0.562419.
Train: 2018-08-02T00:35:46.823932: step 27267, loss 0.682423.
Train: 2018-08-02T00:35:46.989489: step 27268, loss 0.511023.
Train: 2018-08-02T00:35:47.151088: step 27269, loss 0.545287.
Train: 2018-08-02T00:35:47.317612: step 27270, loss 0.545282.
Test: 2018-08-02T00:35:47.862157: step 27270, loss 0.548063.
Train: 2018-08-02T00:35:48.029708: step 27271, loss 0.476701.
Train: 2018-08-02T00:35:48.196294: step 27272, loss 0.510915.
Train: 2018-08-02T00:35:48.363841: step 27273, loss 0.631222.
Train: 2018-08-02T00:35:48.528375: step 27274, loss 0.596851.
Train: 2018-08-02T00:35:48.691939: step 27275, loss 0.441867.
Train: 2018-08-02T00:35:48.855501: step 27276, loss 0.545169.
Train: 2018-08-02T00:35:49.019064: step 27277, loss 0.579707.
Train: 2018-08-02T00:35:49.185622: step 27278, loss 0.59704.
Train: 2018-08-02T00:35:49.347186: step 27279, loss 0.63171.
Train: 2018-08-02T00:35:49.512743: step 27280, loss 0.614382.
Test: 2018-08-02T00:35:50.051318: step 27280, loss 0.547935.
Train: 2018-08-02T00:35:50.225839: step 27281, loss 0.64895.
Train: 2018-08-02T00:35:50.406386: step 27282, loss 0.579697.
Train: 2018-08-02T00:35:50.572910: step 27283, loss 0.476204.
Train: 2018-08-02T00:35:50.739480: step 27284, loss 0.527954.
Train: 2018-08-02T00:35:50.911034: step 27285, loss 0.59688.
Train: 2018-08-02T00:35:51.085571: step 27286, loss 0.562419.
Train: 2018-08-02T00:35:51.252120: step 27287, loss 0.631267.
Train: 2018-08-02T00:35:51.425655: step 27288, loss 0.596797.
Train: 2018-08-02T00:35:51.593183: step 27289, loss 0.528092.
Train: 2018-08-02T00:35:51.756745: step 27290, loss 0.562417.
Test: 2018-08-02T00:35:52.279379: step 27290, loss 0.548074.
Train: 2018-08-02T00:35:52.447923: step 27291, loss 0.51103.
Train: 2018-08-02T00:35:52.612458: step 27292, loss 0.562417.
Train: 2018-08-02T00:35:52.787989: step 27293, loss 0.545294.
Train: 2018-08-02T00:35:52.948558: step 27294, loss 0.511045.
Train: 2018-08-02T00:35:53.114141: step 27295, loss 0.63096.
Train: 2018-08-02T00:35:53.277680: step 27296, loss 0.476756.
Train: 2018-08-02T00:35:53.445264: step 27297, loss 0.579564.
Train: 2018-08-02T00:35:53.617796: step 27298, loss 0.579573.
Train: 2018-08-02T00:35:53.785322: step 27299, loss 0.613901.
Train: 2018-08-02T00:35:53.949882: step 27300, loss 0.493794.
Test: 2018-08-02T00:35:54.476474: step 27300, loss 0.548043.
Train: 2018-08-02T00:35:55.241296: step 27301, loss 0.528088.
Train: 2018-08-02T00:35:55.402839: step 27302, loss 0.545237.
Train: 2018-08-02T00:35:55.568397: step 27303, loss 0.54522.
Train: 2018-08-02T00:35:55.735980: step 27304, loss 0.527987.
Train: 2018-08-02T00:35:55.898516: step 27305, loss 0.545179.
Train: 2018-08-02T00:35:56.073093: step 27306, loss 0.545156.
Train: 2018-08-02T00:35:56.243623: step 27307, loss 0.562423.
Train: 2018-08-02T00:35:56.408184: step 27308, loss 0.579739.
Train: 2018-08-02T00:35:56.576727: step 27309, loss 0.614417.
Train: 2018-08-02T00:35:56.748276: step 27310, loss 0.47576.
Test: 2018-08-02T00:35:57.286830: step 27310, loss 0.547899.
Train: 2018-08-02T00:35:57.453389: step 27311, loss 0.527722.
Train: 2018-08-02T00:35:57.622905: step 27312, loss 0.545055.
Train: 2018-08-02T00:35:57.784473: step 27313, loss 0.545034.
Train: 2018-08-02T00:35:57.949058: step 27314, loss 0.492718.
Train: 2018-08-02T00:35:58.114633: step 27315, loss 0.562453.
Train: 2018-08-02T00:35:58.280176: step 27316, loss 0.579963.
Train: 2018-08-02T00:35:58.442712: step 27317, loss 0.527415.
Train: 2018-08-02T00:35:58.607306: step 27318, loss 0.597586.
Train: 2018-08-02T00:35:58.771834: step 27319, loss 0.667916.
Train: 2018-08-02T00:35:58.933406: step 27320, loss 0.632742.
Test: 2018-08-02T00:35:59.471992: step 27320, loss 0.547783.
Train: 2018-08-02T00:35:59.649489: step 27321, loss 0.580014.
Train: 2018-08-02T00:35:59.814048: step 27322, loss 0.457361.
Train: 2018-08-02T00:35:59.979645: step 27323, loss 0.579979.
Train: 2018-08-02T00:36:00.146185: step 27324, loss 0.492409.
Train: 2018-08-02T00:36:00.319721: step 27325, loss 0.439858.
Train: 2018-08-02T00:36:00.479302: step 27326, loss 0.509827.
Train: 2018-08-02T00:36:00.663777: step 27327, loss 0.544899.
Train: 2018-08-02T00:36:00.828337: step 27328, loss 0.633008.
Train: 2018-08-02T00:36:00.993924: step 27329, loss 0.509577.
Train: 2018-08-02T00:36:01.163449: step 27330, loss 0.562518.
Test: 2018-08-02T00:36:01.691030: step 27330, loss 0.547714.
Train: 2018-08-02T00:36:01.856618: step 27331, loss 0.597908.
Train: 2018-08-02T00:36:02.020174: step 27332, loss 0.580232.
Train: 2018-08-02T00:36:02.185707: step 27333, loss 0.580239.
Train: 2018-08-02T00:36:02.347306: step 27334, loss 0.597941.
Train: 2018-08-02T00:36:02.508843: step 27335, loss 0.633305.
Train: 2018-08-02T00:36:02.675429: step 27336, loss 0.491843.
Train: 2018-08-02T00:36:02.841984: step 27337, loss 0.580175.
Train: 2018-08-02T00:36:03.003554: step 27338, loss 0.403692.
Train: 2018-08-02T00:36:03.181078: step 27339, loss 0.562517.
Train: 2018-08-02T00:36:03.344609: step 27340, loss 0.527161.
Test: 2018-08-02T00:36:03.875191: step 27340, loss 0.547708.
Train: 2018-08-02T00:36:04.044738: step 27341, loss 0.615635.
Train: 2018-08-02T00:36:04.204312: step 27342, loss 0.509412.
Train: 2018-08-02T00:36:04.371863: step 27343, loss 0.491653.
Train: 2018-08-02T00:36:04.537452: step 27344, loss 0.598045.
Train: 2018-08-02T00:36:04.704973: step 27345, loss 0.580318.
Train: 2018-08-02T00:36:04.869533: step 27346, loss 0.527021.
Train: 2018-08-02T00:36:05.034119: step 27347, loss 0.687039.
Train: 2018-08-02T00:36:05.199676: step 27348, loss 0.651389.
Train: 2018-08-02T00:36:05.365235: step 27349, loss 0.562542.
Train: 2018-08-02T00:36:05.535752: step 27350, loss 0.527128.
Test: 2018-08-02T00:36:06.061351: step 27350, loss 0.547716.
Train: 2018-08-02T00:36:06.224935: step 27351, loss 0.544842.
Train: 2018-08-02T00:36:06.389498: step 27352, loss 0.544853.
Train: 2018-08-02T00:36:06.606247: step 27353, loss 0.49193.
Train: 2018-08-02T00:36:06.771811: step 27354, loss 0.544862.
Train: 2018-08-02T00:36:06.936339: step 27355, loss 0.562506.
Train: 2018-08-02T00:36:07.112898: step 27356, loss 0.527213.
Train: 2018-08-02T00:36:07.290393: step 27357, loss 0.580162.
Train: 2018-08-02T00:36:07.453957: step 27358, loss 0.597818.
Train: 2018-08-02T00:36:07.626550: step 27359, loss 0.474274.
Train: 2018-08-02T00:36:07.791088: step 27360, loss 0.491885.
Test: 2018-08-02T00:36:08.328630: step 27360, loss 0.547715.
Train: 2018-08-02T00:36:08.492212: step 27361, loss 0.63323.
Train: 2018-08-02T00:36:08.665717: step 27362, loss 0.580201.
Train: 2018-08-02T00:36:08.829280: step 27363, loss 0.633238.
Train: 2018-08-02T00:36:08.993840: step 27364, loss 0.650824.
Train: 2018-08-02T00:36:09.170399: step 27365, loss 0.580126.
Train: 2018-08-02T00:36:09.333930: step 27366, loss 0.544895.
Train: 2018-08-02T00:36:09.497494: step 27367, loss 0.492235.
Train: 2018-08-02T00:36:09.665046: step 27368, loss 0.544926.
Train: 2018-08-02T00:36:09.827641: step 27369, loss 0.580005.
Train: 2018-08-02T00:36:09.993200: step 27370, loss 0.509898.
Test: 2018-08-02T00:36:10.527738: step 27370, loss 0.547793.
Train: 2018-08-02T00:36:10.696289: step 27371, loss 0.562464.
Train: 2018-08-02T00:36:10.859876: step 27372, loss 0.509914.
Train: 2018-08-02T00:36:11.023442: step 27373, loss 0.562465.
Train: 2018-08-02T00:36:11.201962: step 27374, loss 0.544938.
Train: 2018-08-02T00:36:11.366527: step 27375, loss 0.597539.
Train: 2018-08-02T00:36:11.531057: step 27376, loss 0.5274.
Train: 2018-08-02T00:36:11.693653: step 27377, loss 0.562469.
Train: 2018-08-02T00:36:11.855218: step 27378, loss 0.544928.
Train: 2018-08-02T00:36:12.022775: step 27379, loss 0.580018.
Train: 2018-08-02T00:36:12.188325: step 27380, loss 0.492283.
Test: 2018-08-02T00:36:12.724868: step 27380, loss 0.54777.
Train: 2018-08-02T00:36:12.890449: step 27381, loss 0.597596.
Train: 2018-08-02T00:36:13.053019: step 27382, loss 0.544912.
Train: 2018-08-02T00:36:13.217573: step 27383, loss 0.632759.
Train: 2018-08-02T00:36:13.377122: step 27384, loss 0.597595.
Train: 2018-08-02T00:36:13.553681: step 27385, loss 0.597554.
Train: 2018-08-02T00:36:13.720205: step 27386, loss 0.509909.
Train: 2018-08-02T00:36:13.887757: step 27387, loss 0.597471.
Train: 2018-08-02T00:36:14.056307: step 27388, loss 0.509993.
Train: 2018-08-02T00:36:14.238818: step 27389, loss 0.510013.
Train: 2018-08-02T00:36:14.400386: step 27390, loss 0.510003.
Test: 2018-08-02T00:36:14.927976: step 27390, loss 0.547805.
Train: 2018-08-02T00:36:15.093534: step 27391, loss 0.487804.
Train: 2018-08-02T00:36:15.259116: step 27392, loss 0.544941.
Train: 2018-08-02T00:36:15.425675: step 27393, loss 0.615121.
Train: 2018-08-02T00:36:15.588242: step 27394, loss 0.738091.
Train: 2018-08-02T00:36:15.752796: step 27395, loss 0.492328.
Train: 2018-08-02T00:36:15.918359: step 27396, loss 0.457319.
Train: 2018-08-02T00:36:16.080927: step 27397, loss 0.562467.
Train: 2018-08-02T00:36:16.244457: step 27398, loss 0.597558.
Train: 2018-08-02T00:36:16.405054: step 27399, loss 0.667734.
Train: 2018-08-02T00:36:16.569588: step 27400, loss 0.544942.
Test: 2018-08-02T00:36:17.093219: step 27400, loss 0.547801.
Train: 2018-08-02T00:36:17.849363: step 27401, loss 0.649972.
Train: 2018-08-02T00:36:18.013912: step 27402, loss 0.597384.
Train: 2018-08-02T00:36:18.179470: step 27403, loss 0.527585.
Train: 2018-08-02T00:36:18.347022: step 27404, loss 0.475443.
Train: 2018-08-02T00:36:18.502606: step 27405, loss 0.666767.
Train: 2018-08-02T00:36:18.666200: step 27406, loss 0.510352.
Train: 2018-08-02T00:36:18.832749: step 27407, loss 0.5104.
Train: 2018-08-02T00:36:19.004291: step 27408, loss 0.614433.
Train: 2018-08-02T00:36:19.169831: step 27409, loss 0.527781.
Train: 2018-08-02T00:36:19.328399: step 27410, loss 0.579734.
Test: 2018-08-02T00:36:19.864964: step 27410, loss 0.547931.
Train: 2018-08-02T00:36:20.030553: step 27411, loss 0.597023.
Train: 2018-08-02T00:36:20.192090: step 27412, loss 0.614271.
Train: 2018-08-02T00:36:20.354681: step 27413, loss 0.683218.
Train: 2018-08-02T00:36:20.517220: step 27414, loss 0.596828.
Train: 2018-08-02T00:36:20.679815: step 27415, loss 0.476634.
Train: 2018-08-02T00:36:20.841383: step 27416, loss 0.493905.
Train: 2018-08-02T00:36:21.003946: step 27417, loss 0.579527.
Train: 2018-08-02T00:36:21.167483: step 27418, loss 0.52821.
Train: 2018-08-02T00:36:21.338051: step 27419, loss 0.562413.
Train: 2018-08-02T00:36:21.500591: step 27420, loss 0.528229.
Test: 2018-08-02T00:36:22.033202: step 27420, loss 0.548097.
Train: 2018-08-02T00:36:22.194737: step 27421, loss 0.476935.
Train: 2018-08-02T00:36:22.359322: step 27422, loss 0.545295.
Train: 2018-08-02T00:36:22.532865: step 27423, loss 0.579551.
Train: 2018-08-02T00:36:22.704405: step 27424, loss 0.579568.
Train: 2018-08-02T00:36:22.866939: step 27425, loss 0.510904.
Train: 2018-08-02T00:36:23.027510: step 27426, loss 0.596792.
Train: 2018-08-02T00:36:23.195062: step 27427, loss 0.528007.
Train: 2018-08-02T00:36:23.357628: step 27428, loss 0.614073.
Train: 2018-08-02T00:36:23.520193: step 27429, loss 0.545187.
Train: 2018-08-02T00:36:23.682759: step 27430, loss 0.59688.
Test: 2018-08-02T00:36:24.208382: step 27430, loss 0.547982.
Train: 2018-08-02T00:36:24.370919: step 27431, loss 0.476247.
Train: 2018-08-02T00:36:24.534482: step 27432, loss 0.510661.
Train: 2018-08-02T00:36:24.703031: step 27433, loss 0.562416.
Train: 2018-08-02T00:36:24.873575: step 27434, loss 0.614322.
Train: 2018-08-02T00:36:25.035143: step 27435, loss 0.614352.
Train: 2018-08-02T00:36:25.199703: step 27436, loss 0.562419.
Train: 2018-08-02T00:36:25.360274: step 27437, loss 0.475889.
Train: 2018-08-02T00:36:25.522870: step 27438, loss 0.649028.
Train: 2018-08-02T00:36:25.689394: step 27439, loss 0.45852.
Train: 2018-08-02T00:36:25.855981: step 27440, loss 0.562422.
Test: 2018-08-02T00:36:26.393518: step 27440, loss 0.547897.
Train: 2018-08-02T00:36:26.568077: step 27441, loss 0.475681.
Train: 2018-08-02T00:36:26.736595: step 27442, loss 0.492911.
Train: 2018-08-02T00:36:26.901154: step 27443, loss 0.545016.
Train: 2018-08-02T00:36:27.066712: step 27444, loss 0.667217.
Train: 2018-08-02T00:36:27.229277: step 27445, loss 0.579925.
Train: 2018-08-02T00:36:27.392872: step 27446, loss 0.544967.
Train: 2018-08-02T00:36:27.556405: step 27447, loss 0.667415.
Train: 2018-08-02T00:36:27.723956: step 27448, loss 0.510011.
Train: 2018-08-02T00:36:27.883529: step 27449, loss 0.527495.
Train: 2018-08-02T00:36:28.055071: step 27450, loss 0.579932.
Test: 2018-08-02T00:36:28.580666: step 27450, loss 0.547812.
Train: 2018-08-02T00:36:28.745279: step 27451, loss 0.54497.
Train: 2018-08-02T00:36:28.905827: step 27452, loss 0.527485.
Train: 2018-08-02T00:36:29.067364: step 27453, loss 0.562453.
Train: 2018-08-02T00:36:29.231924: step 27454, loss 0.614946.
Train: 2018-08-02T00:36:29.391497: step 27455, loss 0.492488.
Train: 2018-08-02T00:36:29.555086: step 27456, loss 0.597453.
Train: 2018-08-02T00:36:29.715662: step 27457, loss 0.632448.
Train: 2018-08-02T00:36:29.876233: step 27458, loss 0.614897.
Train: 2018-08-02T00:36:30.042791: step 27459, loss 0.475165.
Train: 2018-08-02T00:36:30.211305: step 27460, loss 0.544993.
Test: 2018-08-02T00:36:30.749868: step 27460, loss 0.547833.
Train: 2018-08-02T00:36:30.913430: step 27461, loss 0.510103.
Train: 2018-08-02T00:36:31.074026: step 27462, loss 0.422814.
Train: 2018-08-02T00:36:31.242550: step 27463, loss 0.492498.
Train: 2018-08-02T00:36:31.408106: step 27464, loss 0.422196.
Train: 2018-08-02T00:36:31.573696: step 27465, loss 0.527282.
Train: 2018-08-02T00:36:31.742247: step 27466, loss 0.544841.
Train: 2018-08-02T00:36:31.905777: step 27467, loss 0.615733.
Train: 2018-08-02T00:36:32.068343: step 27468, loss 0.49145.
Train: 2018-08-02T00:36:32.229909: step 27469, loss 0.580411.
Train: 2018-08-02T00:36:32.392501: step 27470, loss 0.49112.
Test: 2018-08-02T00:36:32.931069: step 27470, loss 0.547625.
Train: 2018-08-02T00:36:33.097622: step 27471, loss 0.52679.
Train: 2018-08-02T00:36:33.263179: step 27472, loss 0.562662.
Train: 2018-08-02T00:36:33.424715: step 27473, loss 0.490631.
Train: 2018-08-02T00:36:33.591271: step 27474, loss 0.454329.
Train: 2018-08-02T00:36:33.756828: step 27475, loss 0.617163.
Train: 2018-08-02T00:36:33.919418: step 27476, loss 0.544628.
Train: 2018-08-02T00:36:34.090962: step 27477, loss 0.489967.
Train: 2018-08-02T00:36:34.252503: step 27478, loss 0.526344.
Train: 2018-08-02T00:36:34.421078: step 27479, loss 0.507975.
Train: 2018-08-02T00:36:34.583618: step 27480, loss 0.526233.
Test: 2018-08-02T00:36:35.120185: step 27480, loss 0.547589.
Train: 2018-08-02T00:36:35.295715: step 27481, loss 0.544594.
Train: 2018-08-02T00:36:35.459278: step 27482, loss 0.526132.
Train: 2018-08-02T00:36:35.623836: step 27483, loss 0.507582.
Train: 2018-08-02T00:36:35.787419: step 27484, loss 0.600251.
Train: 2018-08-02T00:36:35.959939: step 27485, loss 0.526012.
Train: 2018-08-02T00:36:36.124529: step 27486, loss 0.674906.
Train: 2018-08-02T00:36:36.292051: step 27487, loss 0.488745.
Train: 2018-08-02T00:36:36.452650: step 27488, loss 0.600493.
Train: 2018-08-02T00:36:36.624163: step 27489, loss 0.5446.
Train: 2018-08-02T00:36:36.786778: step 27490, loss 0.5446.
Test: 2018-08-02T00:36:37.317326: step 27490, loss 0.54763.
Train: 2018-08-02T00:36:37.480911: step 27491, loss 0.5446.
Train: 2018-08-02T00:36:37.644474: step 27492, loss 0.52597.
Train: 2018-08-02T00:36:37.803026: step 27493, loss 0.563233.
Train: 2018-08-02T00:36:37.966589: step 27494, loss 0.470077.
Train: 2018-08-02T00:36:38.130175: step 27495, loss 0.581888.
Train: 2018-08-02T00:36:38.293739: step 27496, loss 0.563249.
Train: 2018-08-02T00:36:38.464257: step 27497, loss 0.637833.
Train: 2018-08-02T00:36:38.626848: step 27498, loss 0.619107.
Train: 2018-08-02T00:36:38.789389: step 27499, loss 0.581786.
Train: 2018-08-02T00:36:38.950982: step 27500, loss 0.58171.
Test: 2018-08-02T00:36:39.484560: step 27500, loss 0.547604.
Train: 2018-08-02T00:36:40.255304: step 27501, loss 0.489041.
Train: 2018-08-02T00:36:40.418891: step 27502, loss 0.563084.
Train: 2018-08-02T00:36:40.580465: step 27503, loss 0.544592.
Train: 2018-08-02T00:36:40.746990: step 27504, loss 0.581474.
Train: 2018-08-02T00:36:40.905566: step 27505, loss 0.470946.
Train: 2018-08-02T00:36:41.076142: step 27506, loss 0.544595.
Train: 2018-08-02T00:36:41.241714: step 27507, loss 0.636551.
Train: 2018-08-02T00:36:41.411213: step 27508, loss 0.562962.
Train: 2018-08-02T00:36:41.572812: step 27509, loss 0.599611.
Train: 2018-08-02T00:36:41.742330: step 27510, loss 0.599509.
Test: 2018-08-02T00:36:42.277905: step 27510, loss 0.54758.
Train: 2018-08-02T00:36:42.441490: step 27511, loss 0.471573.
Train: 2018-08-02T00:36:42.606019: step 27512, loss 0.599325.
Train: 2018-08-02T00:36:42.770580: step 27513, loss 0.435386.
Train: 2018-08-02T00:36:42.932179: step 27514, loss 0.599224.
Train: 2018-08-02T00:36:43.092750: step 27515, loss 0.47188.
Train: 2018-08-02T00:36:43.256281: step 27516, loss 0.599189.
Train: 2018-08-02T00:36:43.416852: step 27517, loss 0.599167.
Train: 2018-08-02T00:36:43.579417: step 27518, loss 0.508305.
Train: 2018-08-02T00:36:43.754948: step 27519, loss 0.653562.
Train: 2018-08-02T00:36:43.917544: step 27520, loss 0.544639.
Test: 2018-08-02T00:36:44.455077: step 27520, loss 0.54759.
Train: 2018-08-02T00:36:44.619636: step 27521, loss 0.544646.
Train: 2018-08-02T00:36:44.787219: step 27522, loss 0.562734.
Train: 2018-08-02T00:36:44.947759: step 27523, loss 0.43629.
Train: 2018-08-02T00:36:45.117331: step 27524, loss 0.508526.
Train: 2018-08-02T00:36:45.278905: step 27525, loss 0.544653.
Train: 2018-08-02T00:36:45.440468: step 27526, loss 0.52656.
Train: 2018-08-02T00:36:45.606000: step 27527, loss 0.653275.
Train: 2018-08-02T00:36:45.769590: step 27528, loss 0.598941.
Train: 2018-08-02T00:36:45.934154: step 27529, loss 0.52657.
Train: 2018-08-02T00:36:46.099711: step 27530, loss 0.598871.
Test: 2018-08-02T00:36:46.637244: step 27530, loss 0.547596.
Train: 2018-08-02T00:36:46.820778: step 27531, loss 0.526607.
Train: 2018-08-02T00:36:46.984346: step 27532, loss 0.580747.
Train: 2018-08-02T00:36:47.149872: step 27533, loss 0.616769.
Train: 2018-08-02T00:36:47.315451: step 27534, loss 0.634661.
Train: 2018-08-02T00:36:47.484005: step 27535, loss 0.526743.
Train: 2018-08-02T00:36:47.648571: step 27536, loss 0.473029.
Train: 2018-08-02T00:36:47.810138: step 27537, loss 0.544716.
Train: 2018-08-02T00:36:47.973671: step 27538, loss 0.526825.
Train: 2018-08-02T00:36:48.146234: step 27539, loss 0.50894.
Train: 2018-08-02T00:36:48.308776: step 27540, loss 0.687887.
Test: 2018-08-02T00:36:48.851325: step 27540, loss 0.547637.
Train: 2018-08-02T00:36:49.023890: step 27541, loss 0.54473.
Train: 2018-08-02T00:36:49.190457: step 27542, loss 0.616153.
Train: 2018-08-02T00:36:49.353009: step 27543, loss 0.598223.
Train: 2018-08-02T00:36:49.515579: step 27544, loss 0.633706.
Train: 2018-08-02T00:36:49.682104: step 27545, loss 0.651225.
Train: 2018-08-02T00:36:49.844668: step 27546, loss 0.509488.
Train: 2018-08-02T00:36:50.011224: step 27547, loss 0.615368.
Train: 2018-08-02T00:36:50.176782: step 27548, loss 0.667893.
Train: 2018-08-02T00:36:50.341342: step 27549, loss 0.562452.
Train: 2018-08-02T00:36:50.511886: step 27550, loss 0.475284.
Test: 2018-08-02T00:36:51.045458: step 27550, loss 0.547868.
Train: 2018-08-02T00:36:51.209022: step 27551, loss 0.562428.
Train: 2018-08-02T00:36:51.379566: step 27552, loss 0.597119.
Train: 2018-08-02T00:36:51.543159: step 27553, loss 0.527804.
Train: 2018-08-02T00:36:51.709707: step 27554, loss 0.545138.
Train: 2018-08-02T00:36:51.877236: step 27555, loss 0.648656.
Train: 2018-08-02T00:36:52.037807: step 27556, loss 0.527992.
Train: 2018-08-02T00:36:52.199405: step 27557, loss 0.562407.
Train: 2018-08-02T00:36:52.373932: step 27558, loss 0.699614.
Train: 2018-08-02T00:36:52.535477: step 27559, loss 0.596603.
Train: 2018-08-02T00:36:52.704061: step 27560, loss 0.528328.
Test: 2018-08-02T00:36:53.235612: step 27560, loss 0.548181.
Train: 2018-08-02T00:36:53.399177: step 27561, loss 0.613417.
Train: 2018-08-02T00:36:53.572734: step 27562, loss 0.545472.
Train: 2018-08-02T00:36:53.736292: step 27563, loss 0.511693.
Train: 2018-08-02T00:36:53.899828: step 27564, loss 0.562435.
Train: 2018-08-02T00:36:54.073366: step 27565, loss 0.4781.
Train: 2018-08-02T00:36:54.244905: step 27566, loss 0.528702.
Train: 2018-08-02T00:36:54.417469: step 27567, loss 0.562437.
Train: 2018-08-02T00:36:54.580041: step 27568, loss 0.494893.
Train: 2018-08-02T00:36:54.746596: step 27569, loss 0.646973.
Train: 2018-08-02T00:36:54.907135: step 27570, loss 0.6639.
Test: 2018-08-02T00:36:55.418768: step 27570, loss 0.548288.
Train: 2018-08-02T00:36:55.580336: step 27571, loss 0.579327.
Train: 2018-08-02T00:36:55.748916: step 27572, loss 0.562438.
Train: 2018-08-02T00:36:55.913445: step 27573, loss 0.646727.
Train: 2018-08-02T00:36:56.087006: step 27574, loss 0.427852.
Train: 2018-08-02T00:36:56.249547: step 27575, loss 0.545623.
Train: 2018-08-02T00:36:56.412135: step 27576, loss 0.528786.
Train: 2018-08-02T00:36:56.573711: step 27577, loss 0.596133.
Train: 2018-08-02T00:36:56.738270: step 27578, loss 0.495045.
Train: 2018-08-02T00:36:56.899822: step 27579, loss 0.528699.
Train: 2018-08-02T00:36:57.071350: step 27580, loss 0.545537.
Test: 2018-08-02T00:36:57.602929: step 27580, loss 0.548255.
Train: 2018-08-02T00:36:57.776496: step 27581, loss 0.579352.
Train: 2018-08-02T00:36:57.943048: step 27582, loss 0.57937.
Train: 2018-08-02T00:36:58.110573: step 27583, loss 0.528494.
Train: 2018-08-02T00:36:58.274161: step 27584, loss 0.562418.
Train: 2018-08-02T00:36:58.439693: step 27585, loss 0.596431.
Train: 2018-08-02T00:36:58.604251: step 27586, loss 0.562414.
Train: 2018-08-02T00:36:58.765846: step 27587, loss 0.545381.
Train: 2018-08-02T00:36:58.931404: step 27588, loss 0.528319.
Train: 2018-08-02T00:36:59.096966: step 27589, loss 0.545343.
Train: 2018-08-02T00:36:59.267504: step 27590, loss 0.630765.
Test: 2018-08-02T00:36:59.804071: step 27590, loss 0.548093.
Train: 2018-08-02T00:36:59.973611: step 27591, loss 0.613693.
Train: 2018-08-02T00:37:00.142176: step 27592, loss 0.579498.
Train: 2018-08-02T00:37:00.305705: step 27593, loss 0.528247.
Train: 2018-08-02T00:37:00.475257: step 27594, loss 0.613651.
Train: 2018-08-02T00:37:00.636849: step 27595, loss 0.459991.
Train: 2018-08-02T00:37:00.813346: step 27596, loss 0.596573.
Train: 2018-08-02T00:37:00.981927: step 27597, loss 0.562408.
Train: 2018-08-02T00:37:01.153463: step 27598, loss 0.545318.
Train: 2018-08-02T00:37:01.323009: step 27599, loss 0.613699.
Train: 2018-08-02T00:37:01.495562: step 27600, loss 0.528222.
Test: 2018-08-02T00:37:02.031103: step 27600, loss 0.548091.
Train: 2018-08-02T00:37:02.795795: step 27601, loss 0.562408.
Train: 2018-08-02T00:37:02.960324: step 27602, loss 0.630808.
Train: 2018-08-02T00:37:03.123888: step 27603, loss 0.579497.
Train: 2018-08-02T00:37:03.287475: step 27604, loss 0.647784.
Train: 2018-08-02T00:37:03.451043: step 27605, loss 0.562411.
Train: 2018-08-02T00:37:03.614606: step 27606, loss 0.545396.
Train: 2018-08-02T00:37:03.782160: step 27607, loss 0.545419.
Train: 2018-08-02T00:37:03.949705: step 27608, loss 0.511469.
Train: 2018-08-02T00:37:04.115262: step 27609, loss 0.528455.
Train: 2018-08-02T00:37:04.281823: step 27610, loss 0.596394.
Test: 2018-08-02T00:37:04.820359: step 27610, loss 0.548192.
Train: 2018-08-02T00:37:04.987928: step 27611, loss 0.54543.
Train: 2018-08-02T00:37:05.149473: step 27612, loss 0.562417.
Train: 2018-08-02T00:37:05.322040: step 27613, loss 0.579411.
Train: 2018-08-02T00:37:05.485573: step 27614, loss 0.579411.
Train: 2018-08-02T00:37:05.648151: step 27615, loss 0.562417.
Train: 2018-08-02T00:37:05.814693: step 27616, loss 0.579406.
Train: 2018-08-02T00:37:05.979254: step 27617, loss 0.545434.
Train: 2018-08-02T00:37:06.142842: step 27618, loss 0.562418.
Train: 2018-08-02T00:37:06.312394: step 27619, loss 0.562418.
Train: 2018-08-02T00:37:06.485899: step 27620, loss 0.5794.
Test: 2018-08-02T00:37:07.022496: step 27620, loss 0.548199.
Train: 2018-08-02T00:37:07.188054: step 27621, loss 0.562418.
Train: 2018-08-02T00:37:07.354577: step 27622, loss 0.63033.
Train: 2018-08-02T00:37:07.517168: step 27623, loss 0.562421.
Train: 2018-08-02T00:37:07.682731: step 27624, loss 0.494632.
Train: 2018-08-02T00:37:07.846294: step 27625, loss 0.562422.
Train: 2018-08-02T00:37:08.007862: step 27626, loss 0.579375.
Train: 2018-08-02T00:37:08.174416: step 27627, loss 0.511562.
Train: 2018-08-02T00:37:08.342946: step 27628, loss 0.647244.
Train: 2018-08-02T00:37:08.507520: step 27629, loss 0.51155.
Train: 2018-08-02T00:37:08.674055: step 27630, loss 0.596344.
Test: 2018-08-02T00:37:09.213608: step 27630, loss 0.548219.
Train: 2018-08-02T00:37:09.385150: step 27631, loss 0.596339.
Train: 2018-08-02T00:37:09.551730: step 27632, loss 0.630222.
Train: 2018-08-02T00:37:09.728232: step 27633, loss 0.596281.
Train: 2018-08-02T00:37:09.895783: step 27634, loss 0.52863.
Train: 2018-08-02T00:37:10.061340: step 27635, loss 0.54555.
Train: 2018-08-02T00:37:10.233879: step 27636, loss 0.545562.
Train: 2018-08-02T00:37:10.399463: step 27637, loss 0.579307.
Train: 2018-08-02T00:37:10.575965: step 27638, loss 0.545576.
Train: 2018-08-02T00:37:10.742521: step 27639, loss 0.596162.
Train: 2018-08-02T00:37:10.903117: step 27640, loss 0.562441.
Test: 2018-08-02T00:37:11.441652: step 27640, loss 0.548336.
Train: 2018-08-02T00:37:11.610226: step 27641, loss 0.545596.
Train: 2018-08-02T00:37:11.775789: step 27642, loss 0.511907.
Train: 2018-08-02T00:37:11.944341: step 27643, loss 0.444446.
Train: 2018-08-02T00:37:12.107895: step 27644, loss 0.596218.
Train: 2018-08-02T00:37:12.270437: step 27645, loss 0.528588.
Train: 2018-08-02T00:37:12.438019: step 27646, loss 0.562422.
Train: 2018-08-02T00:37:12.608550: step 27647, loss 0.511476.
Train: 2018-08-02T00:37:12.777083: step 27648, loss 0.545394.
Train: 2018-08-02T00:37:12.940675: step 27649, loss 0.443012.
Train: 2018-08-02T00:37:13.117173: step 27650, loss 0.579523.
Test: 2018-08-02T00:37:13.644762: step 27650, loss 0.548029.
Train: 2018-08-02T00:37:13.814335: step 27651, loss 0.596742.
Train: 2018-08-02T00:37:13.978893: step 27652, loss 0.631234.
Train: 2018-08-02T00:37:14.142460: step 27653, loss 0.631313.
Train: 2018-08-02T00:37:14.310014: step 27654, loss 0.562407.
Train: 2018-08-02T00:37:14.473546: step 27655, loss 0.665795.
Train: 2018-08-02T00:37:14.637110: step 27656, loss 0.562406.
Train: 2018-08-02T00:37:14.800697: step 27657, loss 0.493638.
Train: 2018-08-02T00:37:14.970244: step 27658, loss 0.717116.
Train: 2018-08-02T00:37:15.132785: step 27659, loss 0.665334.
Train: 2018-08-02T00:37:15.297369: step 27660, loss 0.562407.
Test: 2018-08-02T00:37:15.819947: step 27660, loss 0.54813.
Train: 2018-08-02T00:37:15.984533: step 27661, loss 0.545357.
Train: 2018-08-02T00:37:16.149092: step 27662, loss 0.5454.
Train: 2018-08-02T00:37:16.314626: step 27663, loss 0.528453.
Train: 2018-08-02T00:37:16.484204: step 27664, loss 0.477613.
Train: 2018-08-02T00:37:16.648733: step 27665, loss 0.528496.
Train: 2018-08-02T00:37:16.825302: step 27666, loss 0.579389.
Train: 2018-08-02T00:37:16.991841: step 27667, loss 0.69822.
Train: 2018-08-02T00:37:17.158395: step 27668, loss 0.579372.
Train: 2018-08-02T00:37:17.323958: step 27669, loss 0.579351.
Train: 2018-08-02T00:37:17.488514: step 27670, loss 0.562431.
Test: 2018-08-02T00:37:18.010093: step 27670, loss 0.548304.
Train: 2018-08-02T00:37:18.185624: step 27671, loss 0.528684.
Train: 2018-08-02T00:37:18.352187: step 27672, loss 0.66362.
Train: 2018-08-02T00:37:18.522722: step 27673, loss 0.545615.
Train: 2018-08-02T00:37:18.693297: step 27674, loss 0.478418.
Train: 2018-08-02T00:37:18.858822: step 27675, loss 0.646474.
Train: 2018-08-02T00:37:19.028396: step 27676, loss 0.663165.
Train: 2018-08-02T00:37:19.198915: step 27677, loss 0.595961.
Train: 2018-08-02T00:37:19.367464: step 27678, loss 0.646006.
Train: 2018-08-02T00:37:19.532050: step 27679, loss 0.562504.
Train: 2018-08-02T00:37:19.700605: step 27680, loss 0.545921.
Test: 2018-08-02T00:37:20.244145: step 27680, loss 0.548669.
Train: 2018-08-02T00:37:20.413692: step 27681, loss 0.529409.
Train: 2018-08-02T00:37:20.578252: step 27682, loss 0.579093.
Train: 2018-08-02T00:37:20.742818: step 27683, loss 0.546046.
Train: 2018-08-02T00:37:20.905354: step 27684, loss 0.480068.
Train: 2018-08-02T00:37:21.067944: step 27685, loss 0.612088.
Train: 2018-08-02T00:37:21.235502: step 27686, loss 0.579073.
Train: 2018-08-02T00:37:21.401059: step 27687, loss 0.612065.
Train: 2018-08-02T00:37:21.571599: step 27688, loss 0.529614.
Train: 2018-08-02T00:37:21.738127: step 27689, loss 0.546102.
Train: 2018-08-02T00:37:21.903683: step 27690, loss 0.579063.
Test: 2018-08-02T00:37:22.446234: step 27690, loss 0.548777.
Train: 2018-08-02T00:37:22.621790: step 27691, loss 0.56258.
Train: 2018-08-02T00:37:22.788319: step 27692, loss 0.597749.
Train: 2018-08-02T00:37:22.968868: step 27693, loss 0.562581.
Train: 2018-08-02T00:37:23.142398: step 27694, loss 0.579062.
Train: 2018-08-02T00:37:23.311950: step 27695, loss 0.480195.
Train: 2018-08-02T00:37:23.477478: step 27696, loss 0.562574.
Train: 2018-08-02T00:37:23.642036: step 27697, loss 0.513025.
Train: 2018-08-02T00:37:23.805625: step 27698, loss 0.62872.
Train: 2018-08-02T00:37:23.971158: step 27699, loss 0.545989.
Train: 2018-08-02T00:37:24.134745: step 27700, loss 0.595682.
Test: 2018-08-02T00:37:24.667295: step 27700, loss 0.548646.
Train: 2018-08-02T00:37:25.412096: step 27701, loss 0.529367.
Train: 2018-08-02T00:37:25.580678: step 27702, loss 0.496122.
Train: 2018-08-02T00:37:25.757174: step 27703, loss 0.562511.
Train: 2018-08-02T00:37:25.922731: step 27704, loss 0.662481.
Train: 2018-08-02T00:37:26.093276: step 27705, loss 0.512484.
Train: 2018-08-02T00:37:26.270803: step 27706, loss 0.562489.
Train: 2018-08-02T00:37:26.438353: step 27707, loss 0.595897.
Train: 2018-08-02T00:37:26.613912: step 27708, loss 0.62935.
Train: 2018-08-02T00:37:26.784459: step 27709, loss 0.595907.
Train: 2018-08-02T00:37:26.950016: step 27710, loss 0.529077.
Test: 2018-08-02T00:37:27.482561: step 27710, loss 0.548497.
Train: 2018-08-02T00:37:27.655100: step 27711, loss 0.545781.
Train: 2018-08-02T00:37:27.822652: step 27712, loss 0.478944.
Train: 2018-08-02T00:37:27.988209: step 27713, loss 0.679597.
Train: 2018-08-02T00:37:28.154764: step 27714, loss 0.49556.
Train: 2018-08-02T00:37:28.318327: step 27715, loss 0.528987.
Train: 2018-08-02T00:37:28.486929: step 27716, loss 0.562464.
Train: 2018-08-02T00:37:28.652435: step 27717, loss 0.528894.
Train: 2018-08-02T00:37:28.822012: step 27718, loss 0.596068.
Train: 2018-08-02T00:37:28.992525: step 27719, loss 0.612923.
Train: 2018-08-02T00:37:29.156088: step 27720, loss 0.545615.
Test: 2018-08-02T00:37:29.685671: step 27720, loss 0.548341.
Train: 2018-08-02T00:37:29.860231: step 27721, loss 0.612963.
Train: 2018-08-02T00:37:30.023769: step 27722, loss 0.545605.
Train: 2018-08-02T00:37:30.191351: step 27723, loss 0.511919.
Train: 2018-08-02T00:37:30.353886: step 27724, loss 0.629863.
Train: 2018-08-02T00:37:30.527453: step 27725, loss 0.511873.
Train: 2018-08-02T00:37:30.701956: step 27726, loss 0.579303.
Train: 2018-08-02T00:37:30.870505: step 27727, loss 0.494939.
Train: 2018-08-02T00:37:31.048061: step 27728, loss 0.494844.
Train: 2018-08-02T00:37:31.217602: step 27729, loss 0.562419.
Train: 2018-08-02T00:37:31.379170: step 27730, loss 0.715167.
Test: 2018-08-02T00:37:31.916708: step 27730, loss 0.548215.
Train: 2018-08-02T00:37:32.083263: step 27731, loss 0.528497.
Train: 2018-08-02T00:37:32.247822: step 27732, loss 0.528486.
Train: 2018-08-02T00:37:32.422356: step 27733, loss 0.494502.
Train: 2018-08-02T00:37:32.586916: step 27734, loss 0.630433.
Train: 2018-08-02T00:37:32.752474: step 27735, loss 0.613455.
Train: 2018-08-02T00:37:32.920025: step 27736, loss 0.579425.
Train: 2018-08-02T00:37:33.092590: step 27737, loss 0.511393.
Train: 2018-08-02T00:37:33.258122: step 27738, loss 0.562413.
Train: 2018-08-02T00:37:33.421684: step 27739, loss 0.40923.
Train: 2018-08-02T00:37:33.588240: step 27740, loss 0.596528.
Test: 2018-08-02T00:37:34.124815: step 27740, loss 0.548098.
Train: 2018-08-02T00:37:34.291384: step 27741, loss 0.579495.
Train: 2018-08-02T00:37:34.465895: step 27742, loss 0.545296.
Train: 2018-08-02T00:37:34.631450: step 27743, loss 0.613808.
Train: 2018-08-02T00:37:34.799002: step 27744, loss 0.596694.
Train: 2018-08-02T00:37:34.964560: step 27745, loss 0.528113.
Train: 2018-08-02T00:37:35.129120: step 27746, loss 0.528094.
Train: 2018-08-02T00:37:35.309663: step 27747, loss 0.613919.
Train: 2018-08-02T00:37:35.490186: step 27748, loss 0.528057.
Train: 2018-08-02T00:37:35.659727: step 27749, loss 0.545222.
Train: 2018-08-02T00:37:35.823265: step 27750, loss 0.613996.
Test: 2018-08-02T00:37:36.358836: step 27750, loss 0.548005.
Train: 2018-08-02T00:37:36.529378: step 27751, loss 0.613997.
Train: 2018-08-02T00:37:36.696929: step 27752, loss 0.579592.
Train: 2018-08-02T00:37:36.867504: step 27753, loss 0.545233.
Train: 2018-08-02T00:37:37.042035: step 27754, loss 0.613895.
Train: 2018-08-02T00:37:37.211554: step 27755, loss 0.699558.
Train: 2018-08-02T00:37:37.374118: step 27756, loss 0.47692.
Train: 2018-08-02T00:37:37.540698: step 27757, loss 0.630704.
Train: 2018-08-02T00:37:37.710247: step 27758, loss 0.56241.
Train: 2018-08-02T00:37:37.876806: step 27759, loss 0.57942.
Train: 2018-08-02T00:37:38.049314: step 27760, loss 0.732166.
Test: 2018-08-02T00:37:38.563943: step 27760, loss 0.548268.
Train: 2018-08-02T00:37:38.725519: step 27761, loss 0.477879.
Train: 2018-08-02T00:37:38.891064: step 27762, loss 0.579307.
Train: 2018-08-02T00:37:39.058641: step 27763, loss 0.562445.
Train: 2018-08-02T00:37:39.220183: step 27764, loss 0.646448.
Train: 2018-08-02T00:37:39.384743: step 27765, loss 0.679726.
Train: 2018-08-02T00:37:39.550338: step 27766, loss 0.545807.
Train: 2018-08-02T00:37:39.715858: step 27767, loss 0.529257.
Train: 2018-08-02T00:37:39.878424: step 27768, loss 0.46301.
Train: 2018-08-02T00:37:40.043981: step 27769, loss 0.529389.
Train: 2018-08-02T00:37:40.212530: step 27770, loss 0.512826.
Test: 2018-08-02T00:37:40.744109: step 27770, loss 0.548648.
Train: 2018-08-02T00:37:40.911662: step 27771, loss 0.612274.
Train: 2018-08-02T00:37:41.076253: step 27772, loss 0.446471.
Train: 2018-08-02T00:37:41.241806: step 27773, loss 0.462877.
Train: 2018-08-02T00:37:41.407338: step 27774, loss 0.595812.
Train: 2018-08-02T00:37:41.572925: step 27775, loss 0.529102.
Train: 2018-08-02T00:37:41.736458: step 27776, loss 0.529003.
Train: 2018-08-02T00:37:41.904010: step 27777, loss 0.512113.
Train: 2018-08-02T00:37:42.073582: step 27778, loss 0.57928.
Train: 2018-08-02T00:37:42.248121: step 27779, loss 0.545551.
Train: 2018-08-02T00:37:42.421625: step 27780, loss 0.562424.
Test: 2018-08-02T00:37:42.956197: step 27780, loss 0.548208.
Train: 2018-08-02T00:37:43.126760: step 27781, loss 0.545448.
Train: 2018-08-02T00:37:43.294293: step 27782, loss 0.596435.
Train: 2018-08-02T00:37:43.461845: step 27783, loss 0.460156.
Train: 2018-08-02T00:37:43.625408: step 27784, loss 0.579498.
Train: 2018-08-02T00:37:43.793983: step 27785, loss 0.562404.
Train: 2018-08-02T00:37:43.960512: step 27786, loss 0.579575.
Train: 2018-08-02T00:37:44.130059: step 27787, loss 0.5108.
Train: 2018-08-02T00:37:44.295647: step 27788, loss 0.510687.
Train: 2018-08-02T00:37:44.462170: step 27789, loss 0.631549.
Train: 2018-08-02T00:37:44.627759: step 27790, loss 0.614344.
Test: 2018-08-02T00:37:45.170304: step 27790, loss 0.54791.
Train: 2018-08-02T00:37:45.336864: step 27791, loss 0.649022.
Train: 2018-08-02T00:37:45.503418: step 27792, loss 0.5451.
Train: 2018-08-02T00:37:45.665978: step 27793, loss 0.527796.
Train: 2018-08-02T00:37:45.827552: step 27794, loss 0.527791.
Train: 2018-08-02T00:37:45.992081: step 27795, loss 0.597055.
Train: 2018-08-02T00:37:46.163622: step 27796, loss 0.597057.
Train: 2018-08-02T00:37:46.331174: step 27797, loss 0.579727.
Train: 2018-08-02T00:37:46.495734: step 27798, loss 0.510502.
Train: 2018-08-02T00:37:46.678247: step 27799, loss 0.510497.
Train: 2018-08-02T00:37:46.841810: step 27800, loss 0.545082.
Test: 2018-08-02T00:37:47.374386: step 27800, loss 0.547896.
Train: 2018-08-02T00:37:48.185339: step 27801, loss 0.614444.
Train: 2018-08-02T00:37:48.351920: step 27802, loss 0.510385.
Train: 2018-08-02T00:37:48.518474: step 27803, loss 0.527792.
Train: 2018-08-02T00:37:48.683033: step 27804, loss 0.614489.
Train: 2018-08-02T00:37:48.853585: step 27805, loss 0.631928.
Train: 2018-08-02T00:37:49.019110: step 27806, loss 0.510351.
Train: 2018-08-02T00:37:49.189658: step 27807, loss 0.683879.
Train: 2018-08-02T00:37:49.369174: step 27808, loss 0.597062.
Train: 2018-08-02T00:37:49.533767: step 27809, loss 0.510551.
Train: 2018-08-02T00:37:49.708267: step 27810, loss 0.562413.
Test: 2018-08-02T00:37:50.239847: step 27810, loss 0.54796.
Train: 2018-08-02T00:37:50.410391: step 27811, loss 0.665945.
Train: 2018-08-02T00:37:50.579965: step 27812, loss 0.493534.
Train: 2018-08-02T00:37:50.744523: step 27813, loss 0.459211.
Train: 2018-08-02T00:37:50.910055: step 27814, loss 0.476386.
Train: 2018-08-02T00:37:51.078605: step 27815, loss 0.579635.
Train: 2018-08-02T00:37:51.240174: step 27816, loss 0.545166.
Train: 2018-08-02T00:37:51.405761: step 27817, loss 0.527887.
Train: 2018-08-02T00:37:51.576274: step 27818, loss 0.545128.
Train: 2018-08-02T00:37:51.744823: step 27819, loss 0.597035.
Train: 2018-08-02T00:37:51.909410: step 27820, loss 0.49312.
Test: 2018-08-02T00:37:52.438968: step 27820, loss 0.547891.
Train: 2018-08-02T00:37:52.602530: step 27821, loss 0.649178.
Train: 2018-08-02T00:37:52.767121: step 27822, loss 0.631849.
Train: 2018-08-02T00:37:52.928686: step 27823, loss 0.666499.
Train: 2018-08-02T00:37:53.094217: step 27824, loss 0.57973.
Train: 2018-08-02T00:37:53.258776: step 27825, loss 0.631538.
Train: 2018-08-02T00:37:53.423349: step 27826, loss 0.579645.
Train: 2018-08-02T00:37:53.592909: step 27827, loss 0.631176.
Train: 2018-08-02T00:37:53.756445: step 27828, loss 0.545269.
Train: 2018-08-02T00:37:53.920009: step 27829, loss 0.562408.
Train: 2018-08-02T00:37:54.088558: step 27830, loss 0.460104.
Test: 2018-08-02T00:37:54.613155: step 27830, loss 0.548145.
Train: 2018-08-02T00:37:54.789683: step 27831, loss 0.596485.
Train: 2018-08-02T00:37:54.955266: step 27832, loss 0.494345.
Train: 2018-08-02T00:37:55.121814: step 27833, loss 0.681514.
Train: 2018-08-02T00:37:55.298324: step 27834, loss 0.64735.
Train: 2018-08-02T00:37:55.467871: step 27835, loss 0.579367.
Train: 2018-08-02T00:37:55.634427: step 27836, loss 0.477922.
Train: 2018-08-02T00:37:55.801008: step 27837, loss 0.748151.
Train: 2018-08-02T00:37:55.967536: step 27838, loss 0.612929.
Train: 2018-08-02T00:37:56.134090: step 27839, loss 0.478631.
Train: 2018-08-02T00:37:56.312613: step 27840, loss 0.595938.
Test: 2018-08-02T00:37:56.850176: step 27840, loss 0.548509.
Train: 2018-08-02T00:37:57.018750: step 27841, loss 0.495712.
Train: 2018-08-02T00:37:57.183311: step 27842, loss 0.612523.
Train: 2018-08-02T00:37:57.346876: step 27843, loss 0.612455.
Train: 2018-08-02T00:37:57.514399: step 27844, loss 0.579135.
Train: 2018-08-02T00:37:57.680983: step 27845, loss 0.496187.
Train: 2018-08-02T00:37:57.847534: step 27846, loss 0.595684.
Train: 2018-08-02T00:37:58.017088: step 27847, loss 0.496315.
Train: 2018-08-02T00:37:58.192612: step 27848, loss 0.562543.
Train: 2018-08-02T00:37:58.355153: step 27849, loss 0.479734.
Train: 2018-08-02T00:37:58.524721: step 27850, loss 0.612284.
Test: 2018-08-02T00:37:59.060299: step 27850, loss 0.54863.
Train: 2018-08-02T00:37:59.233834: step 27851, loss 0.595717.
Train: 2018-08-02T00:37:59.396369: step 27852, loss 0.579124.
Train: 2018-08-02T00:37:59.565917: step 27853, loss 0.64553.
Train: 2018-08-02T00:37:59.728507: step 27854, loss 0.645461.
Train: 2018-08-02T00:37:59.899026: step 27855, loss 0.595655.
Train: 2018-08-02T00:38:00.064591: step 27856, loss 0.579082.
Train: 2018-08-02T00:38:00.229143: step 27857, loss 0.480121.
Train: 2018-08-02T00:38:00.399712: step 27858, loss 0.56258.
Train: 2018-08-02T00:38:00.566272: step 27859, loss 0.529628.
Train: 2018-08-02T00:38:00.740776: step 27860, loss 0.776844.
Test: 2018-08-02T00:38:01.275347: step 27860, loss 0.548836.
Train: 2018-08-02T00:38:01.439908: step 27861, loss 0.628364.
Train: 2018-08-02T00:38:01.606461: step 27862, loss 0.513461.
Train: 2018-08-02T00:38:01.769054: step 27863, loss 0.562651.
Train: 2018-08-02T00:38:01.934609: step 27864, loss 0.660659.
Train: 2018-08-02T00:38:02.100178: step 27865, loss 0.595273.
Train: 2018-08-02T00:38:02.265734: step 27866, loss 0.578971.
Train: 2018-08-02T00:38:02.437264: step 27867, loss 0.497939.
Train: 2018-08-02T00:38:02.607815: step 27868, loss 0.59514.
Train: 2018-08-02T00:38:02.770361: step 27869, loss 0.514297.
Train: 2018-08-02T00:38:02.938927: step 27870, loss 0.627421.
Test: 2018-08-02T00:38:03.476494: step 27870, loss 0.549289.
Train: 2018-08-02T00:38:03.655982: step 27871, loss 0.546664.
Train: 2018-08-02T00:38:03.818578: step 27872, loss 0.707997.
Train: 2018-08-02T00:38:03.982110: step 27873, loss 0.659406.
Train: 2018-08-02T00:38:04.146698: step 27874, loss 0.498723.
Train: 2018-08-02T00:38:04.316250: step 27875, loss 0.546902.
Train: 2018-08-02T00:38:04.477784: step 27876, loss 0.54694.
Train: 2018-08-02T00:38:04.648354: step 27877, loss 0.610884.
Train: 2018-08-02T00:38:04.813911: step 27878, loss 0.594884.
Train: 2018-08-02T00:38:04.985428: step 27879, loss 0.515145.
Train: 2018-08-02T00:38:05.153978: step 27880, loss 0.562979.
Test: 2018-08-02T00:38:05.674597: step 27880, loss 0.54963.
Train: 2018-08-02T00:38:05.838148: step 27881, loss 0.531096.
Train: 2018-08-02T00:38:06.002757: step 27882, loss 0.51511.
Train: 2018-08-02T00:38:06.170261: step 27883, loss 0.515014.
Train: 2018-08-02T00:38:06.336841: step 27884, loss 0.594938.
Train: 2018-08-02T00:38:06.515338: step 27885, loss 0.578928.
Train: 2018-08-02T00:38:06.733968: step 27886, loss 0.578932.
Train: 2018-08-02T00:38:06.905484: step 27887, loss 0.530666.
Train: 2018-08-02T00:38:07.069046: step 27888, loss 0.611181.
Train: 2018-08-02T00:38:07.244602: step 27889, loss 0.498245.
Train: 2018-08-02T00:38:07.408145: step 27890, loss 0.401029.
Test: 2018-08-02T00:38:07.943708: step 27890, loss 0.549122.
Train: 2018-08-02T00:38:08.108268: step 27891, loss 0.562725.
Train: 2018-08-02T00:38:08.281830: step 27892, loss 0.546369.
Train: 2018-08-02T00:38:08.454370: step 27893, loss 0.628146.
Train: 2018-08-02T00:38:08.618928: step 27894, loss 0.51334.
Train: 2018-08-02T00:38:08.785457: step 27895, loss 0.644973.
Train: 2018-08-02T00:38:08.952013: step 27896, loss 0.513029.
Train: 2018-08-02T00:38:09.114578: step 27897, loss 0.463217.
Train: 2018-08-02T00:38:09.282131: step 27898, loss 0.579132.
Train: 2018-08-02T00:38:09.442702: step 27899, loss 0.612503.
Train: 2018-08-02T00:38:09.619254: step 27900, loss 0.612605.
Test: 2018-08-02T00:38:10.157789: step 27900, loss 0.54846.
Train: 2018-08-02T00:38:10.987786: step 27901, loss 0.579205.
Train: 2018-08-02T00:38:11.152321: step 27902, loss 0.512205.
Train: 2018-08-02T00:38:11.330877: step 27903, loss 0.478537.
Train: 2018-08-02T00:38:11.503407: step 27904, loss 0.495123.
Train: 2018-08-02T00:38:11.678939: step 27905, loss 0.511768.
Train: 2018-08-02T00:38:11.843507: step 27906, loss 0.613271.
Train: 2018-08-02T00:38:12.010029: step 27907, loss 0.596407.
Train: 2018-08-02T00:38:12.184562: step 27908, loss 0.528344.
Train: 2018-08-02T00:38:12.355130: step 27909, loss 0.562405.
Train: 2018-08-02T00:38:12.529665: step 27910, loss 0.596617.
Test: 2018-08-02T00:38:13.056231: step 27910, loss 0.548057.
Train: 2018-08-02T00:38:13.220792: step 27911, loss 0.493876.
Train: 2018-08-02T00:38:13.385351: step 27912, loss 0.476559.
Train: 2018-08-02T00:38:13.549943: step 27913, loss 0.424642.
Train: 2018-08-02T00:38:13.716471: step 27914, loss 0.579705.
Train: 2018-08-02T00:38:13.879031: step 27915, loss 0.562418.
Train: 2018-08-02T00:38:14.044620: step 27916, loss 0.492766.
Train: 2018-08-02T00:38:14.208181: step 27917, loss 0.562442.
Train: 2018-08-02T00:38:14.380717: step 27918, loss 0.54492.
Train: 2018-08-02T00:38:14.548268: step 27919, loss 0.650436.
Train: 2018-08-02T00:38:14.713833: step 27920, loss 0.527242.
Test: 2018-08-02T00:38:15.248372: step 27920, loss 0.547714.
Train: 2018-08-02T00:38:15.416939: step 27921, loss 0.597805.
Train: 2018-08-02T00:38:15.584497: step 27922, loss 0.50948.
Train: 2018-08-02T00:38:15.751057: step 27923, loss 0.70414.
Train: 2018-08-02T00:38:15.919608: step 27924, loss 0.456329.
Train: 2018-08-02T00:38:16.084162: step 27925, loss 0.633372.
Train: 2018-08-02T00:38:16.248724: step 27926, loss 0.527098.
Train: 2018-08-02T00:38:16.422265: step 27927, loss 0.562521.
Train: 2018-08-02T00:38:16.587821: step 27928, loss 0.580238.
Train: 2018-08-02T00:38:16.752382: step 27929, loss 0.527094.
Train: 2018-08-02T00:38:16.921928: step 27930, loss 0.562521.
Test: 2018-08-02T00:38:17.461456: step 27930, loss 0.547686.
Train: 2018-08-02T00:38:17.639987: step 27931, loss 0.438507.
Train: 2018-08-02T00:38:17.801547: step 27932, loss 0.527051.
Train: 2018-08-02T00:38:17.970095: step 27933, loss 0.527008.
Train: 2018-08-02T00:38:18.138644: step 27934, loss 0.580359.
Train: 2018-08-02T00:38:18.302234: step 27935, loss 0.437821.
Train: 2018-08-02T00:38:18.482758: step 27936, loss 0.580458.
Train: 2018-08-02T00:38:18.653270: step 27937, loss 0.526815.
Train: 2018-08-02T00:38:18.817830: step 27938, loss 0.598499.
Train: 2018-08-02T00:38:18.987383: step 27939, loss 0.63447.
Train: 2018-08-02T00:38:19.160943: step 27940, loss 0.508768.
Test: 2018-08-02T00:38:19.693515: step 27940, loss 0.547606.
Train: 2018-08-02T00:38:19.857051: step 27941, loss 0.472797.
Train: 2018-08-02T00:38:20.021612: step 27942, loss 0.526675.
Train: 2018-08-02T00:38:20.186202: step 27943, loss 0.490583.
Train: 2018-08-02T00:38:20.355719: step 27944, loss 0.580781.
Train: 2018-08-02T00:38:20.529254: step 27945, loss 0.544642.
Train: 2018-08-02T00:38:20.696807: step 27946, loss 0.635233.
Train: 2018-08-02T00:38:20.861366: step 27947, loss 0.544633.
Train: 2018-08-02T00:38:21.037894: step 27948, loss 0.526498.
Train: 2018-08-02T00:38:21.213425: step 27949, loss 0.490197.
Train: 2018-08-02T00:38:21.382006: step 27950, loss 0.59912.
Test: 2018-08-02T00:38:21.904578: step 27950, loss 0.547576.
Train: 2018-08-02T00:38:22.069138: step 27951, loss 0.61732.
Train: 2018-08-02T00:38:22.233699: step 27952, loss 0.490114.
Train: 2018-08-02T00:38:22.397261: step 27953, loss 0.635503.
Train: 2018-08-02T00:38:22.562818: step 27954, loss 0.562788.
Train: 2018-08-02T00:38:22.736354: step 27955, loss 0.562779.
Train: 2018-08-02T00:38:22.898920: step 27956, loss 0.508353.
Train: 2018-08-02T00:38:23.072485: step 27957, loss 0.544631.
Train: 2018-08-02T00:38:23.239035: step 27958, loss 0.725939.
Train: 2018-08-02T00:38:23.407588: step 27959, loss 0.617012.
Train: 2018-08-02T00:38:23.575113: step 27960, loss 0.562702.
Test: 2018-08-02T00:38:24.103699: step 27960, loss 0.5476.
Train: 2018-08-02T00:38:24.268260: step 27961, loss 0.544673.
Train: 2018-08-02T00:38:24.433841: step 27962, loss 0.652444.
Train: 2018-08-02T00:38:24.596383: step 27963, loss 0.544711.
Train: 2018-08-02T00:38:24.760941: step 27964, loss 0.616151.
Train: 2018-08-02T00:38:24.924505: step 27965, loss 0.580356.
Train: 2018-08-02T00:38:25.088094: step 27966, loss 0.598014.
Train: 2018-08-02T00:38:25.247666: step 27967, loss 0.527144.
Train: 2018-08-02T00:38:25.410206: step 27968, loss 0.474323.
Train: 2018-08-02T00:38:25.571774: step 27969, loss 0.544872.
Train: 2018-08-02T00:38:25.735369: step 27970, loss 0.47455.
Test: 2018-08-02T00:38:26.263925: step 27970, loss 0.547747.
Train: 2018-08-02T00:38:26.430509: step 27971, loss 0.580054.
Train: 2018-08-02T00:38:26.593069: step 27972, loss 0.474587.
Train: 2018-08-02T00:38:26.762622: step 27973, loss 0.562472.
Train: 2018-08-02T00:38:26.927151: step 27974, loss 0.544879.
Train: 2018-08-02T00:38:27.090714: step 27975, loss 0.597691.
Train: 2018-08-02T00:38:27.258265: step 27976, loss 0.615299.
Train: 2018-08-02T00:38:27.432824: step 27977, loss 0.61526.
Train: 2018-08-02T00:38:27.603343: step 27978, loss 0.632757.
Train: 2018-08-02T00:38:27.764912: step 27979, loss 0.457238.
Train: 2018-08-02T00:38:27.930470: step 27980, loss 0.562453.
Test: 2018-08-02T00:38:28.463044: step 27980, loss 0.547785.
Train: 2018-08-02T00:38:28.623647: step 27981, loss 0.597476.
Train: 2018-08-02T00:38:28.800169: step 27982, loss 0.527456.
Train: 2018-08-02T00:38:28.962710: step 27983, loss 0.579926.
Train: 2018-08-02T00:38:29.128296: step 27984, loss 0.492558.
Train: 2018-08-02T00:38:29.295844: step 27985, loss 0.510025.
Train: 2018-08-02T00:38:29.458418: step 27986, loss 0.597407.
Train: 2018-08-02T00:38:29.620980: step 27987, loss 0.527474.
Train: 2018-08-02T00:38:29.786507: step 27988, loss 0.457491.
Train: 2018-08-02T00:38:29.952064: step 27989, loss 0.43981.
Train: 2018-08-02T00:38:30.114630: step 27990, loss 0.580037.
Test: 2018-08-02T00:38:30.646209: step 27990, loss 0.547733.
Train: 2018-08-02T00:38:30.816782: step 27991, loss 0.47443.
Train: 2018-08-02T00:38:30.996304: step 27992, loss 0.597825.
Train: 2018-08-02T00:38:31.160865: step 27993, loss 0.600279.
Train: 2018-08-02T00:38:31.330413: step 27994, loss 0.544798.
Train: 2018-08-02T00:38:31.495938: step 27995, loss 0.562537.
Train: 2018-08-02T00:38:31.656540: step 27996, loss 0.633642.
Train: 2018-08-02T00:38:31.822066: step 27997, loss 0.491441.
Train: 2018-08-02T00:38:31.985660: step 27998, loss 0.562555.
Train: 2018-08-02T00:38:32.149192: step 27999, loss 0.615973.
Train: 2018-08-02T00:38:32.321731: step 28000, loss 0.47355.
Test: 2018-08-02T00:38:32.859294: step 28000, loss 0.547648.
Train: 2018-08-02T00:38:33.613240: step 28001, loss 0.4913.
Train: 2018-08-02T00:38:33.777802: step 28002, loss 0.526895.
Train: 2018-08-02T00:38:33.938402: step 28003, loss 0.526853.
Train: 2018-08-02T00:38:34.109944: step 28004, loss 0.526808.
Train: 2018-08-02T00:38:34.285475: step 28005, loss 0.580568.
Train: 2018-08-02T00:38:34.450037: step 28006, loss 0.562647.
Train: 2018-08-02T00:38:34.621570: step 28007, loss 0.544679.
Train: 2018-08-02T00:38:34.784111: step 28008, loss 0.562671.
Train: 2018-08-02T00:38:34.947706: step 28009, loss 0.544667.
Train: 2018-08-02T00:38:35.114229: step 28010, loss 0.580718.
Test: 2018-08-02T00:38:35.637839: step 28010, loss 0.547592.
Train: 2018-08-02T00:38:35.802389: step 28011, loss 0.54466.
Train: 2018-08-02T00:38:35.963988: step 28012, loss 0.508572.
Train: 2018-08-02T00:38:36.128516: step 28013, loss 0.56271.
Train: 2018-08-02T00:38:36.294105: step 28014, loss 0.598857.
Train: 2018-08-02T00:38:36.458635: step 28015, loss 0.562718.
Train: 2018-08-02T00:38:36.624192: step 28016, loss 0.598852.
Train: 2018-08-02T00:38:36.787787: step 28017, loss 0.47243.
Train: 2018-08-02T00:38:36.954334: step 28018, loss 0.472408.
Train: 2018-08-02T00:38:37.116908: step 28019, loss 0.562726.
Train: 2018-08-02T00:38:37.290443: step 28020, loss 0.635125.
Test: 2018-08-02T00:38:37.813013: step 28020, loss 0.547584.
Train: 2018-08-02T00:38:37.979587: step 28021, loss 0.617017.
Train: 2018-08-02T00:38:38.143158: step 28022, loss 0.580801.
Train: 2018-08-02T00:38:38.311713: step 28023, loss 0.634936.
Train: 2018-08-02T00:38:38.476269: step 28024, loss 0.490602.
Train: 2018-08-02T00:38:38.647813: step 28025, loss 0.580673.
Train: 2018-08-02T00:38:38.819324: step 28026, loss 0.436821.
Train: 2018-08-02T00:38:38.992860: step 28027, loss 0.562658.
Train: 2018-08-02T00:38:39.156423: step 28028, loss 0.652545.
Train: 2018-08-02T00:38:39.319013: step 28029, loss 0.544688.
Train: 2018-08-02T00:38:39.490530: step 28030, loss 0.670265.
Test: 2018-08-02T00:38:40.034077: step 28030, loss 0.547623.
Train: 2018-08-02T00:38:40.205644: step 28031, loss 0.598408.
Train: 2018-08-02T00:38:40.367214: step 28032, loss 0.598292.
Train: 2018-08-02T00:38:40.532772: step 28033, loss 0.509151.
Train: 2018-08-02T00:38:40.710269: step 28034, loss 0.509242.
Train: 2018-08-02T00:38:40.875857: step 28035, loss 0.491557.
Train: 2018-08-02T00:38:41.036398: step 28036, loss 0.65121.
Train: 2018-08-02T00:38:41.205999: step 28037, loss 0.456262.
Train: 2018-08-02T00:38:41.370535: step 28038, loss 0.615632.
Train: 2018-08-02T00:38:41.537058: step 28039, loss 0.52713.
Train: 2018-08-02T00:38:41.707640: step 28040, loss 0.597868.
Test: 2018-08-02T00:38:42.242187: step 28040, loss 0.547707.
Train: 2018-08-02T00:38:42.403741: step 28041, loss 0.597828.
Train: 2018-08-02T00:38:42.567305: step 28042, loss 0.491926.
Train: 2018-08-02T00:38:42.731905: step 28043, loss 0.439055.
Train: 2018-08-02T00:38:42.895457: step 28044, loss 0.703697.
Train: 2018-08-02T00:38:43.061015: step 28045, loss 0.562488.
Train: 2018-08-02T00:38:43.228536: step 28046, loss 0.562483.
Train: 2018-08-02T00:38:43.391103: step 28047, loss 0.703322.
Train: 2018-08-02T00:38:43.552670: step 28048, loss 0.667835.
Train: 2018-08-02T00:38:43.726234: step 28049, loss 0.632445.
Train: 2018-08-02T00:38:43.892761: step 28050, loss 0.527572.
Test: 2018-08-02T00:38:44.423353: step 28050, loss 0.547872.
Train: 2018-08-02T00:38:44.593917: step 28051, loss 0.510306.
Train: 2018-08-02T00:38:44.763434: step 28052, loss 0.475767.
Train: 2018-08-02T00:38:44.935998: step 28053, loss 0.631647.
Train: 2018-08-02T00:38:45.099566: step 28054, loss 0.596959.
Train: 2018-08-02T00:38:45.263129: step 28055, loss 0.562403.
Train: 2018-08-02T00:38:45.431678: step 28056, loss 0.545196.
Train: 2018-08-02T00:38:45.609198: step 28057, loss 0.57958.
Train: 2018-08-02T00:38:45.771771: step 28058, loss 0.579552.
Train: 2018-08-02T00:38:45.934313: step 28059, loss 0.579523.
Train: 2018-08-02T00:38:46.095871: step 28060, loss 0.613681.
Test: 2018-08-02T00:38:46.623493: step 28060, loss 0.548122.
Train: 2018-08-02T00:38:46.793041: step 28061, loss 0.545349.
Train: 2018-08-02T00:38:46.954610: step 28062, loss 0.477275.
Train: 2018-08-02T00:38:47.113152: step 28063, loss 0.494335.
Train: 2018-08-02T00:38:47.277712: step 28064, loss 0.681589.
Train: 2018-08-02T00:38:47.441275: step 28065, loss 0.528393.
Train: 2018-08-02T00:38:47.605862: step 28066, loss 0.511412.
Train: 2018-08-02T00:38:47.767403: step 28067, loss 0.494398.
Train: 2018-08-02T00:38:47.931994: step 28068, loss 0.545386.
Train: 2018-08-02T00:38:48.100513: step 28069, loss 0.579447.
Train: 2018-08-02T00:38:48.266070: step 28070, loss 0.579461.
Test: 2018-08-02T00:38:48.802636: step 28070, loss 0.548112.
Train: 2018-08-02T00:38:49.013454: step 28071, loss 0.57947.
Train: 2018-08-02T00:38:49.185027: step 28072, loss 0.664839.
Train: 2018-08-02T00:38:49.359554: step 28073, loss 0.545348.
Train: 2018-08-02T00:38:49.523123: step 28074, loss 0.443088.
Train: 2018-08-02T00:38:49.693666: step 28075, loss 0.562404.
Train: 2018-08-02T00:38:49.867198: step 28076, loss 0.596555.
Train: 2018-08-02T00:38:50.029770: step 28077, loss 0.545321.
Train: 2018-08-02T00:38:50.195326: step 28078, loss 0.545311.
Train: 2018-08-02T00:38:50.367834: step 28079, loss 0.613715.
Train: 2018-08-02T00:38:50.539383: step 28080, loss 0.528193.
Test: 2018-08-02T00:38:51.083924: step 28080, loss 0.548072.
Train: 2018-08-02T00:38:51.252468: step 28081, loss 0.511066.
Train: 2018-08-02T00:38:51.417060: step 28082, loss 0.54527.
Train: 2018-08-02T00:38:51.594597: step 28083, loss 0.562401.
Train: 2018-08-02T00:38:51.759139: step 28084, loss 0.476562.
Train: 2018-08-02T00:38:51.922704: step 28085, loss 0.545199.
Train: 2018-08-02T00:38:52.085242: step 28086, loss 0.631355.
Train: 2018-08-02T00:38:52.246811: step 28087, loss 0.458887.
Train: 2018-08-02T00:38:52.407414: step 28088, loss 0.545119.
Train: 2018-08-02T00:38:52.568950: step 28089, loss 0.614386.
Train: 2018-08-02T00:38:52.731515: step 28090, loss 0.562415.
Test: 2018-08-02T00:38:53.270076: step 28090, loss 0.547877.
Train: 2018-08-02T00:38:53.435664: step 28091, loss 0.475598.
Train: 2018-08-02T00:38:53.598202: step 28092, loss 0.632016.
Train: 2018-08-02T00:38:53.762759: step 28093, loss 0.632079.
Train: 2018-08-02T00:38:53.932331: step 28094, loss 0.719133.
Train: 2018-08-02T00:38:54.096865: step 28095, loss 0.597173.
Train: 2018-08-02T00:38:54.259461: step 28096, loss 0.562413.
Train: 2018-08-02T00:38:54.424021: step 28097, loss 0.545106.
Train: 2018-08-02T00:38:54.592539: step 28098, loss 0.562406.
Train: 2018-08-02T00:38:54.761115: step 28099, loss 0.52791.
Train: 2018-08-02T00:38:54.931634: step 28100, loss 0.527942.
Test: 2018-08-02T00:38:55.462240: step 28100, loss 0.547981.
Train: 2018-08-02T00:38:56.239476: step 28101, loss 0.476291.
Train: 2018-08-02T00:38:56.405005: step 28102, loss 0.54517.
Train: 2018-08-02T00:38:56.565576: step 28103, loss 0.562404.
Train: 2018-08-02T00:38:56.736145: step 28104, loss 0.648694.
Train: 2018-08-02T00:38:56.897687: step 28105, loss 0.527904.
Train: 2018-08-02T00:38:57.061254: step 28106, loss 0.562404.
Train: 2018-08-02T00:38:57.223816: step 28107, loss 0.527905.
Train: 2018-08-02T00:38:57.384387: step 28108, loss 0.545148.
Train: 2018-08-02T00:38:57.552937: step 28109, loss 0.562405.
Train: 2018-08-02T00:38:57.720489: step 28110, loss 0.562405.
Test: 2018-08-02T00:38:58.258051: step 28110, loss 0.547936.
Train: 2018-08-02T00:38:58.423640: step 28111, loss 0.579686.
Train: 2018-08-02T00:38:58.596151: step 28112, loss 0.631536.
Train: 2018-08-02T00:38:58.759710: step 28113, loss 0.579675.
Train: 2018-08-02T00:38:58.927262: step 28114, loss 0.562404.
Train: 2018-08-02T00:38:59.087866: step 28115, loss 0.527924.
Train: 2018-08-02T00:38:59.250399: step 28116, loss 0.476233.
Train: 2018-08-02T00:38:59.414987: step 28117, loss 0.648637.
Train: 2018-08-02T00:38:59.577555: step 28118, loss 0.614122.
Train: 2018-08-02T00:38:59.738121: step 28119, loss 0.510735.
Train: 2018-08-02T00:38:59.906678: step 28120, loss 0.614053.
Test: 2018-08-02T00:39:00.451189: step 28120, loss 0.547996.
Train: 2018-08-02T00:39:00.630709: step 28121, loss 0.510796.
Train: 2018-08-02T00:39:00.807237: step 28122, loss 0.493607.
Train: 2018-08-02T00:39:00.968804: step 28123, loss 0.527981.
Train: 2018-08-02T00:39:01.132367: step 28124, loss 0.545174.
Train: 2018-08-02T00:39:01.307930: step 28125, loss 0.476168.
Train: 2018-08-02T00:39:01.471461: step 28126, loss 0.596971.
Train: 2018-08-02T00:39:01.634027: step 28127, loss 0.579716.
Train: 2018-08-02T00:39:01.802576: step 28128, loss 0.545086.
Train: 2018-08-02T00:39:01.965141: step 28129, loss 0.649131.
Train: 2018-08-02T00:39:02.126709: step 28130, loss 0.597097.
Test: 2018-08-02T00:39:02.658289: step 28130, loss 0.547898.
Train: 2018-08-02T00:39:02.821852: step 28131, loss 0.649076.
Train: 2018-08-02T00:39:02.998411: step 28132, loss 0.493186.
Train: 2018-08-02T00:39:03.170943: step 28133, loss 0.562407.
Train: 2018-08-02T00:39:03.342460: step 28134, loss 0.596978.
Train: 2018-08-02T00:39:03.509014: step 28135, loss 0.562405.
Train: 2018-08-02T00:39:03.672577: step 28136, loss 0.493385.
Train: 2018-08-02T00:39:03.834145: step 28137, loss 0.510637.
Train: 2018-08-02T00:39:03.999732: step 28138, loss 0.666006.
Train: 2018-08-02T00:39:04.163266: step 28139, loss 0.527894.
Train: 2018-08-02T00:39:04.324864: step 28140, loss 0.476147.
Test: 2018-08-02T00:39:04.860403: step 28140, loss 0.547946.
Train: 2018-08-02T00:39:05.026957: step 28141, loss 0.562405.
Train: 2018-08-02T00:39:05.189522: step 28142, loss 0.648801.
Train: 2018-08-02T00:39:05.359068: step 28143, loss 0.614221.
Train: 2018-08-02T00:39:05.522632: step 28144, loss 0.545149.
Train: 2018-08-02T00:39:05.688188: step 28145, loss 0.683096.
Train: 2018-08-02T00:39:05.862748: step 28146, loss 0.596811.
Train: 2018-08-02T00:39:06.023293: step 28147, loss 0.528072.
Train: 2018-08-02T00:39:06.188851: step 28148, loss 0.648074.
Train: 2018-08-02T00:39:06.352412: step 28149, loss 0.476951.
Train: 2018-08-02T00:39:06.514012: step 28150, loss 0.630677.
Test: 2018-08-02T00:39:07.043566: step 28150, loss 0.548141.
Train: 2018-08-02T00:39:07.206131: step 28151, loss 0.528337.
Train: 2018-08-02T00:39:07.367698: step 28152, loss 0.528385.
Train: 2018-08-02T00:39:07.537258: step 28153, loss 0.681401.
Train: 2018-08-02T00:39:07.704799: step 28154, loss 0.545452.
Train: 2018-08-02T00:39:07.867399: step 28155, loss 0.613219.
Train: 2018-08-02T00:39:08.036929: step 28156, loss 0.613117.
Train: 2018-08-02T00:39:08.202468: step 28157, loss 0.478164.
Train: 2018-08-02T00:39:08.365033: step 28158, loss 0.461432.
Train: 2018-08-02T00:39:08.528627: step 28159, loss 0.478238.
Train: 2018-08-02T00:39:08.697146: step 28160, loss 0.528705.
Test: 2018-08-02T00:39:09.225731: step 28160, loss 0.54828.
Train: 2018-08-02T00:39:09.389294: step 28161, loss 0.52864.
Train: 2018-08-02T00:39:09.551861: step 28162, loss 0.545492.
Train: 2018-08-02T00:39:09.713428: step 28163, loss 0.528487.
Train: 2018-08-02T00:39:09.874030: step 28164, loss 0.630422.
Train: 2018-08-02T00:39:10.040585: step 28165, loss 0.562407.
Train: 2018-08-02T00:39:10.206137: step 28166, loss 0.630584.
Train: 2018-08-02T00:39:10.368677: step 28167, loss 0.511259.
Train: 2018-08-02T00:39:10.532240: step 28168, loss 0.442962.
Train: 2018-08-02T00:39:10.699822: step 28169, loss 0.665012.
Train: 2018-08-02T00:39:10.862387: step 28170, loss 0.63086.
Test: 2018-08-02T00:39:11.397956: step 28170, loss 0.548071.
Train: 2018-08-02T00:39:11.558496: step 28171, loss 0.647963.
Train: 2018-08-02T00:39:11.729059: step 28172, loss 0.613678.
Train: 2018-08-02T00:39:11.895626: step 28173, loss 0.477089.
Train: 2018-08-02T00:39:12.072123: step 28174, loss 0.579459.
Train: 2018-08-02T00:39:12.236713: step 28175, loss 0.494225.
Train: 2018-08-02T00:39:12.406255: step 28176, loss 0.528303.
Train: 2018-08-02T00:39:12.569793: step 28177, loss 0.579466.
Train: 2018-08-02T00:39:12.737371: step 28178, loss 0.545332.
Train: 2018-08-02T00:39:12.899910: step 28179, loss 0.54532.
Train: 2018-08-02T00:39:13.063504: step 28180, loss 0.511113.
Test: 2018-08-02T00:39:13.595083: step 28180, loss 0.548065.
Train: 2018-08-02T00:39:13.762614: step 28181, loss 0.5624.
Train: 2018-08-02T00:39:13.928162: step 28182, loss 0.493837.
Train: 2018-08-02T00:39:14.092720: step 28183, loss 0.5624.
Train: 2018-08-02T00:39:14.259277: step 28184, loss 0.562401.
Train: 2018-08-02T00:39:14.420843: step 28185, loss 0.579634.
Train: 2018-08-02T00:39:14.587398: step 28186, loss 0.545151.
Train: 2018-08-02T00:39:14.758939: step 28187, loss 0.562405.
Train: 2018-08-02T00:39:14.921540: step 28188, loss 0.752643.
Train: 2018-08-02T00:39:15.084070: step 28189, loss 0.424235.
Train: 2018-08-02T00:39:15.249647: step 28190, loss 0.545126.
Test: 2018-08-02T00:39:15.778215: step 28190, loss 0.547928.
Train: 2018-08-02T00:39:15.949772: step 28191, loss 0.614276.
Train: 2018-08-02T00:39:16.113350: step 28192, loss 0.52783.
Train: 2018-08-02T00:39:16.281869: step 28193, loss 0.52782.
Train: 2018-08-02T00:39:16.450418: step 28194, loss 0.614324.
Train: 2018-08-02T00:39:16.621991: step 28195, loss 0.527799.
Train: 2018-08-02T00:39:16.788514: step 28196, loss 0.475856.
Train: 2018-08-02T00:39:16.951080: step 28197, loss 0.493077.
Train: 2018-08-02T00:39:17.115639: step 28198, loss 0.52768.
Train: 2018-08-02T00:39:17.279239: step 28199, loss 0.492796.
Train: 2018-08-02T00:39:17.449766: step 28200, loss 0.544975.
Test: 2018-08-02T00:39:17.980362: step 28200, loss 0.547787.
Train: 2018-08-02T00:39:18.761249: step 28201, loss 0.509933.
Train: 2018-08-02T00:39:18.923816: step 28202, loss 0.562583.
Train: 2018-08-02T00:39:19.088401: step 28203, loss 0.632866.
Train: 2018-08-02T00:39:19.265901: step 28204, loss 0.509616.
Train: 2018-08-02T00:39:19.429464: step 28205, loss 0.615459.
Train: 2018-08-02T00:39:19.595046: step 28206, loss 0.54483.
Train: 2018-08-02T00:39:19.759612: step 28207, loss 0.509443.
Train: 2018-08-02T00:39:19.935142: step 28208, loss 0.491666.
Train: 2018-08-02T00:39:20.114663: step 28209, loss 0.580283.
Train: 2018-08-02T00:39:20.292158: step 28210, loss 0.562547.
Test: 2018-08-02T00:39:20.814761: step 28210, loss 0.547655.
Train: 2018-08-02T00:39:20.977352: step 28211, loss 0.437976.
Train: 2018-08-02T00:39:21.150895: step 28212, loss 0.509057.
Train: 2018-08-02T00:39:21.317442: step 28213, loss 0.508938.
Train: 2018-08-02T00:39:21.488958: step 28214, loss 0.508811.
Train: 2018-08-02T00:39:21.657507: step 28215, loss 0.598667.
Train: 2018-08-02T00:39:21.818104: step 28216, loss 0.670929.
Train: 2018-08-02T00:39:21.985656: step 28217, loss 0.526604.
Train: 2018-08-02T00:39:22.151188: step 28218, loss 0.616917.
Train: 2018-08-02T00:39:22.315747: step 28219, loss 0.472379.
Train: 2018-08-02T00:39:22.482327: step 28220, loss 0.52656.
Test: 2018-08-02T00:39:23.031834: step 28220, loss 0.547583.
Train: 2018-08-02T00:39:23.198388: step 28221, loss 0.562743.
Train: 2018-08-02T00:39:23.364943: step 28222, loss 0.490278.
Train: 2018-08-02T00:39:23.531498: step 28223, loss 0.562772.
Train: 2018-08-02T00:39:23.704036: step 28224, loss 0.562787.
Train: 2018-08-02T00:39:23.868596: step 28225, loss 0.635518.
Train: 2018-08-02T00:39:24.034154: step 28226, loss 0.508267.
Train: 2018-08-02T00:39:24.195747: step 28227, loss 0.599162.
Train: 2018-08-02T00:39:24.362277: step 28228, loss 0.653673.
Train: 2018-08-02T00:39:24.527834: step 28229, loss 0.599074.
Train: 2018-08-02T00:39:24.704362: step 28230, loss 0.544635.
Test: 2018-08-02T00:39:25.236944: step 28230, loss 0.547584.
Train: 2018-08-02T00:39:25.406486: step 28231, loss 0.671251.
Train: 2018-08-02T00:39:25.570049: step 28232, loss 0.670912.
Train: 2018-08-02T00:39:25.732665: step 28233, loss 0.634516.
Train: 2018-08-02T00:39:25.896201: step 28234, loss 0.616265.
Train: 2018-08-02T00:39:26.065723: step 28235, loss 0.509153.
Train: 2018-08-02T00:39:26.239286: step 28236, loss 0.686678.
Train: 2018-08-02T00:39:26.416810: step 28237, loss 0.633087.
Train: 2018-08-02T00:39:26.593312: step 28238, loss 0.492244.
Train: 2018-08-02T00:39:26.755910: step 28239, loss 0.527476.
Train: 2018-08-02T00:39:26.924427: step 28240, loss 0.579853.
Test: 2018-08-02T00:39:27.459996: step 28240, loss 0.547874.
Train: 2018-08-02T00:39:27.628546: step 28241, loss 0.562418.
Train: 2018-08-02T00:39:27.796097: step 28242, loss 0.545094.
Train: 2018-08-02T00:39:27.965645: step 28243, loss 0.545132.
Train: 2018-08-02T00:39:28.130229: step 28244, loss 0.596879.
Train: 2018-08-02T00:39:28.294765: step 28245, loss 0.528004.
Train: 2018-08-02T00:39:28.457330: step 28246, loss 0.596741.
Train: 2018-08-02T00:39:28.623920: step 28247, loss 0.510989.
Train: 2018-08-02T00:39:28.794429: step 28248, loss 0.459689.
Train: 2018-08-02T00:39:28.962978: step 28249, loss 0.459658.
Train: 2018-08-02T00:39:29.129559: step 28250, loss 0.5281.
Test: 2018-08-02T00:39:29.657123: step 28250, loss 0.548013.
Train: 2018-08-02T00:39:29.822705: step 28251, loss 0.510857.
Train: 2018-08-02T00:39:29.983250: step 28252, loss 0.631283.
Train: 2018-08-02T00:39:30.151800: step 28253, loss 0.579643.
Train: 2018-08-02T00:39:30.321377: step 28254, loss 0.545149.
Train: 2018-08-02T00:39:30.487901: step 28255, loss 0.579676.
Train: 2018-08-02T00:39:30.656451: step 28256, loss 0.59697.
Train: 2018-08-02T00:39:30.823030: step 28257, loss 0.527839.
Train: 2018-08-02T00:39:31.002525: step 28258, loss 0.683456.
Train: 2018-08-02T00:39:31.169081: step 28259, loss 0.596955.
Train: 2018-08-02T00:39:31.331676: step 28260, loss 0.562403.
Test: 2018-08-02T00:39:31.844275: step 28260, loss 0.547975.
Train: 2018-08-02T00:39:32.021827: step 28261, loss 0.545173.
Train: 2018-08-02T00:39:32.190376: step 28262, loss 0.596828.
Train: 2018-08-02T00:39:32.358900: step 28263, loss 0.493635.
Train: 2018-08-02T00:39:32.524483: step 28264, loss 0.613959.
Train: 2018-08-02T00:39:32.692010: step 28265, loss 0.5624.
Train: 2018-08-02T00:39:32.854575: step 28266, loss 0.579556.
Train: 2018-08-02T00:39:33.024122: step 28267, loss 0.5624.
Train: 2018-08-02T00:39:33.199652: step 28268, loss 0.6309.
Train: 2018-08-02T00:39:33.364245: step 28269, loss 0.647888.
Train: 2018-08-02T00:39:33.528805: step 28270, loss 0.477133.
Test: 2018-08-02T00:39:34.075342: step 28270, loss 0.548142.
Train: 2018-08-02T00:39:34.240870: step 28271, loss 0.545372.
Train: 2018-08-02T00:39:34.410415: step 28272, loss 0.613467.
Train: 2018-08-02T00:39:34.577968: step 28273, loss 0.596403.
Train: 2018-08-02T00:39:34.744522: step 28274, loss 0.47757.
Train: 2018-08-02T00:39:34.912107: step 28275, loss 0.579377.
Train: 2018-08-02T00:39:35.078630: step 28276, loss 0.613276.
Train: 2018-08-02T00:39:35.250172: step 28277, loss 0.477742.
Train: 2018-08-02T00:39:35.416726: step 28278, loss 0.596294.
Train: 2018-08-02T00:39:35.577296: step 28279, loss 0.545486.
Train: 2018-08-02T00:39:35.749835: step 28280, loss 0.511619.
Test: 2018-08-02T00:39:36.293381: step 28280, loss 0.548227.
Train: 2018-08-02T00:39:36.459952: step 28281, loss 0.562417.
Train: 2018-08-02T00:39:36.622533: step 28282, loss 0.562415.
Train: 2018-08-02T00:39:36.795042: step 28283, loss 0.545447.
Train: 2018-08-02T00:39:36.958628: step 28284, loss 0.562412.
Train: 2018-08-02T00:39:37.122167: step 28285, loss 0.579404.
Train: 2018-08-02T00:39:37.288752: step 28286, loss 0.647419.
Train: 2018-08-02T00:39:37.461260: step 28287, loss 0.732328.
Train: 2018-08-02T00:39:37.625819: step 28288, loss 0.579363.
Train: 2018-08-02T00:39:37.793403: step 28289, loss 0.461006.
Train: 2018-08-02T00:39:37.956959: step 28290, loss 0.562428.
Test: 2018-08-02T00:39:38.493502: step 28290, loss 0.548303.
Train: 2018-08-02T00:39:38.657094: step 28291, loss 0.596175.
Train: 2018-08-02T00:39:38.826611: step 28292, loss 0.579288.
Train: 2018-08-02T00:39:38.991196: step 28293, loss 0.528775.
Train: 2018-08-02T00:39:39.157751: step 28294, loss 0.562443.
Train: 2018-08-02T00:39:39.323314: step 28295, loss 0.612888.
Train: 2018-08-02T00:39:39.492855: step 28296, loss 0.528855.
Train: 2018-08-02T00:39:39.660406: step 28297, loss 0.69677.
Train: 2018-08-02T00:39:39.837937: step 28298, loss 0.545707.
Train: 2018-08-02T00:39:40.003463: step 28299, loss 0.646115.
Train: 2018-08-02T00:39:40.172014: step 28300, loss 0.629237.
Test: 2018-08-02T00:39:40.699604: step 28300, loss 0.548571.
Train: 2018-08-02T00:39:41.488838: step 28301, loss 0.562503.
Train: 2018-08-02T00:39:41.653392: step 28302, loss 0.496142.
Train: 2018-08-02T00:39:41.824909: step 28303, loss 0.479672.
Train: 2018-08-02T00:39:41.993457: step 28304, loss 0.645388.
Train: 2018-08-02T00:39:42.160044: step 28305, loss 0.628756.
Train: 2018-08-02T00:39:42.316594: step 28306, loss 0.61213.
Train: 2018-08-02T00:39:42.481185: step 28307, loss 0.54608.
Train: 2018-08-02T00:39:42.648732: step 28308, loss 0.480258.
Train: 2018-08-02T00:39:42.813292: step 28309, loss 0.44735.
Train: 2018-08-02T00:39:42.978855: step 28310, loss 0.612034.
Test: 2018-08-02T00:39:43.512398: step 28310, loss 0.548749.
Train: 2018-08-02T00:39:43.674994: step 28311, loss 0.529566.
Train: 2018-08-02T00:39:43.845519: step 28312, loss 0.496471.
Train: 2018-08-02T00:39:44.017074: step 28313, loss 0.628765.
Train: 2018-08-02T00:39:44.179623: step 28314, loss 0.678558.
Train: 2018-08-02T00:39:44.342213: step 28315, loss 0.545965.
Train: 2018-08-02T00:39:44.507770: step 28316, loss 0.678508.
Train: 2018-08-02T00:39:44.674320: step 28317, loss 0.562545.
Train: 2018-08-02T00:39:44.838853: step 28318, loss 0.562555.
Train: 2018-08-02T00:39:45.010424: step 28319, loss 0.48003.
Train: 2018-08-02T00:39:45.177944: step 28320, loss 0.46349.
Test: 2018-08-02T00:39:45.709525: step 28320, loss 0.548696.
Train: 2018-08-02T00:39:45.880099: step 28321, loss 0.628706.
Train: 2018-08-02T00:39:46.052608: step 28322, loss 0.56254.
Train: 2018-08-02T00:39:46.216202: step 28323, loss 0.612231.
Train: 2018-08-02T00:39:46.377739: step 28324, loss 0.612237.
Train: 2018-08-02T00:39:46.557283: step 28325, loss 0.595658.
Train: 2018-08-02T00:39:46.722815: step 28326, loss 0.529444.
Train: 2018-08-02T00:39:46.890367: step 28327, loss 0.64528.
Train: 2018-08-02T00:39:47.055924: step 28328, loss 0.496433.
Train: 2018-08-02T00:39:47.218491: step 28329, loss 0.579081.
Train: 2018-08-02T00:39:47.389060: step 28330, loss 0.562552.
Test: 2018-08-02T00:39:47.911668: step 28330, loss 0.548712.
Train: 2018-08-02T00:39:48.082212: step 28331, loss 0.562552.
Train: 2018-08-02T00:39:48.247742: step 28332, loss 0.595611.
Train: 2018-08-02T00:39:48.416317: step 28333, loss 0.595606.
Train: 2018-08-02T00:39:48.581877: step 28334, loss 0.579075.
Train: 2018-08-02T00:39:48.751413: step 28335, loss 0.546054.
Train: 2018-08-02T00:39:48.919967: step 28336, loss 0.463538.
Train: 2018-08-02T00:39:49.093519: step 28337, loss 0.512979.
Train: 2018-08-02T00:39:49.261042: step 28338, loss 0.579095.
Train: 2018-08-02T00:39:49.428613: step 28339, loss 0.579109.
Train: 2018-08-02T00:39:49.600153: step 28340, loss 0.479498.
Test: 2018-08-02T00:39:50.137686: step 28340, loss 0.548565.
Train: 2018-08-02T00:39:50.307262: step 28341, loss 0.545858.
Train: 2018-08-02T00:39:50.478801: step 28342, loss 0.462387.
Train: 2018-08-02T00:39:50.643365: step 28343, loss 0.545723.
Train: 2018-08-02T00:39:50.807894: step 28344, loss 0.512046.
Train: 2018-08-02T00:39:50.971482: step 28345, loss 0.680485.
Train: 2018-08-02T00:39:51.143997: step 28346, loss 0.528629.
Train: 2018-08-02T00:39:51.316535: step 28347, loss 0.545482.
Train: 2018-08-02T00:39:51.479131: step 28348, loss 0.511489.
Train: 2018-08-02T00:39:51.641699: step 28349, loss 0.511346.
Train: 2018-08-02T00:39:51.809219: step 28350, loss 0.579474.
Test: 2018-08-02T00:39:52.348776: step 28350, loss 0.548067.
Train: 2018-08-02T00:39:52.523339: step 28351, loss 0.579516.
Train: 2018-08-02T00:39:52.697869: step 28352, loss 0.631009.
Train: 2018-08-02T00:39:52.863399: step 28353, loss 0.59674.
Train: 2018-08-02T00:39:53.028958: step 28354, loss 0.682654.
Train: 2018-08-02T00:39:53.203491: step 28355, loss 0.682531.
Train: 2018-08-02T00:39:53.380044: step 28356, loss 0.5624.
Train: 2018-08-02T00:39:53.547571: step 28357, loss 0.528233.
Train: 2018-08-02T00:39:53.719146: step 28358, loss 0.528287.
Train: 2018-08-02T00:39:53.883673: step 28359, loss 0.528319.
Train: 2018-08-02T00:39:54.045271: step 28360, loss 0.528333.
Test: 2018-08-02T00:39:54.577817: step 28360, loss 0.548137.
Train: 2018-08-02T00:39:54.760357: step 28361, loss 0.494254.
Train: 2018-08-02T00:39:54.930872: step 28362, loss 0.630619.
Train: 2018-08-02T00:39:55.113419: step 28363, loss 0.681784.
Train: 2018-08-02T00:39:55.278973: step 28364, loss 0.613495.
Train: 2018-08-02T00:39:55.441509: step 28365, loss 0.562409.
Train: 2018-08-02T00:39:55.613079: step 28366, loss 0.596349.
Train: 2018-08-02T00:39:55.779629: step 28367, loss 0.562418.
Train: 2018-08-02T00:39:55.957155: step 28368, loss 0.545516.
Train: 2018-08-02T00:39:56.132692: step 28369, loss 0.596197.
Train: 2018-08-02T00:39:56.299243: step 28370, loss 0.511856.
Test: 2018-08-02T00:39:56.839795: step 28370, loss 0.548328.
Train: 2018-08-02T00:39:57.006351: step 28371, loss 0.4782.
Train: 2018-08-02T00:39:57.170916: step 28372, loss 0.596145.
Train: 2018-08-02T00:39:57.333451: step 28373, loss 0.511863.
Train: 2018-08-02T00:39:57.513001: step 28374, loss 0.646779.
Train: 2018-08-02T00:39:57.679526: step 28375, loss 0.714207.
Train: 2018-08-02T00:39:57.851093: step 28376, loss 0.545614.
Train: 2018-08-02T00:39:58.016625: step 28377, loss 0.596045.
Train: 2018-08-02T00:39:58.186202: step 28378, loss 0.478627.
Train: 2018-08-02T00:39:58.355745: step 28379, loss 0.662999.
Train: 2018-08-02T00:39:58.521306: step 28380, loss 0.545743.
Test: 2018-08-02T00:39:59.052854: step 28380, loss 0.548488.
Train: 2018-08-02T00:39:59.217442: step 28381, loss 0.646006.
Train: 2018-08-02T00:39:59.381973: step 28382, loss 0.629168.
Train: 2018-08-02T00:39:59.547532: step 28383, loss 0.61238.
Train: 2018-08-02T00:39:59.718076: step 28384, loss 0.545957.
Train: 2018-08-02T00:39:59.883664: step 28385, loss 0.51295.
Train: 2018-08-02T00:40:00.052182: step 28386, loss 0.57907.
Train: 2018-08-02T00:40:00.222758: step 28387, loss 0.710952.
Train: 2018-08-02T00:40:00.391302: step 28388, loss 0.5626.
Train: 2018-08-02T00:40:00.572792: step 28389, loss 0.628191.
Train: 2018-08-02T00:40:00.737350: step 28390, loss 0.578997.
Test: 2018-08-02T00:40:01.269956: step 28390, loss 0.549048.
Train: 2018-08-02T00:40:01.436507: step 28391, loss 0.57898.
Train: 2018-08-02T00:40:01.613035: step 28392, loss 0.578967.
Train: 2018-08-02T00:40:01.779600: step 28393, loss 0.56275.
Train: 2018-08-02T00:40:01.946159: step 28394, loss 0.546605.
Train: 2018-08-02T00:40:02.118685: step 28395, loss 0.562795.
Train: 2018-08-02T00:40:02.290230: step 28396, loss 0.611192.
Train: 2018-08-02T00:40:02.455758: step 28397, loss 0.530627.
Train: 2018-08-02T00:40:02.622327: step 28398, loss 0.562841.
Train: 2018-08-02T00:40:02.793882: step 28399, loss 0.498524.
Train: 2018-08-02T00:40:02.961415: step 28400, loss 0.546749.
Test: 2018-08-02T00:40:03.493004: step 28400, loss 0.549339.
Train: 2018-08-02T00:40:04.287481: step 28401, loss 0.578934.
Train: 2018-08-02T00:40:04.464992: step 28402, loss 0.546695.
Train: 2018-08-02T00:40:04.630549: step 28403, loss 0.562801.
Train: 2018-08-02T00:40:04.797104: step 28404, loss 0.546626.
Train: 2018-08-02T00:40:04.963660: step 28405, loss 0.611317.
Train: 2018-08-02T00:40:05.129244: step 28406, loss 0.514165.
Train: 2018-08-02T00:40:05.297766: step 28407, loss 0.676298.
Train: 2018-08-02T00:40:05.467337: step 28408, loss 0.660078.
Train: 2018-08-02T00:40:05.632871: step 28409, loss 0.53034.
Train: 2018-08-02T00:40:05.799459: step 28410, loss 0.659944.
Test: 2018-08-02T00:40:06.330009: step 28410, loss 0.549231.
Train: 2018-08-02T00:40:06.497560: step 28411, loss 0.546601.
Train: 2018-08-02T00:40:06.669101: step 28412, loss 0.611257.
Train: 2018-08-02T00:40:06.892643: step 28413, loss 0.546671.
Train: 2018-08-02T00:40:07.058184: step 28414, loss 0.627297.
Train: 2018-08-02T00:40:07.234702: step 28415, loss 0.595028.
Train: 2018-08-02T00:40:07.399288: step 28416, loss 0.627139.
Train: 2018-08-02T00:40:07.566846: step 28417, loss 0.594958.
Train: 2018-08-02T00:40:07.745337: step 28418, loss 0.498918.
Train: 2018-08-02T00:40:07.908900: step 28419, loss 0.658848.
Train: 2018-08-02T00:40:08.077449: step 28420, loss 0.562961.
Test: 2018-08-02T00:40:08.603055: step 28420, loss 0.549644.
Train: 2018-08-02T00:40:08.764637: step 28421, loss 0.547053.
Train: 2018-08-02T00:40:08.933162: step 28422, loss 0.626661.
Train: 2018-08-02T00:40:09.100714: step 28423, loss 0.515349.
Train: 2018-08-02T00:40:09.262307: step 28424, loss 0.547146.
Train: 2018-08-02T00:40:09.425845: step 28425, loss 0.515377.
Train: 2018-08-02T00:40:09.602399: step 28426, loss 0.467621.
Train: 2018-08-02T00:40:09.769925: step 28427, loss 0.61079.
Train: 2018-08-02T00:40:09.935482: step 28428, loss 0.531021.
Train: 2018-08-02T00:40:10.100068: step 28429, loss 0.59492.
Train: 2018-08-02T00:40:10.269590: step 28430, loss 0.562894.
Test: 2018-08-02T00:40:10.804181: step 28430, loss 0.549423.
Train: 2018-08-02T00:40:10.969717: step 28431, loss 0.450472.
Train: 2018-08-02T00:40:11.136297: step 28432, loss 0.482271.
Train: 2018-08-02T00:40:11.306817: step 28433, loss 0.562769.
Train: 2018-08-02T00:40:11.478382: step 28434, loss 0.497739.
Train: 2018-08-02T00:40:11.650897: step 28435, loss 0.481062.
Train: 2018-08-02T00:40:11.825430: step 28436, loss 0.579024.
Train: 2018-08-02T00:40:11.988022: step 28437, loss 0.447153.
Train: 2018-08-02T00:40:12.150562: step 28438, loss 0.57911.
Train: 2018-08-02T00:40:12.311156: step 28439, loss 0.562489.
Train: 2018-08-02T00:40:12.488657: step 28440, loss 0.612713.
Test: 2018-08-02T00:40:13.009266: step 28440, loss 0.548367.
Train: 2018-08-02T00:40:13.173826: step 28441, loss 0.427956.
Train: 2018-08-02T00:40:13.354343: step 28442, loss 0.545529.
Train: 2018-08-02T00:40:13.516908: step 28443, loss 0.477529.
Train: 2018-08-02T00:40:13.699420: step 28444, loss 0.494132.
Train: 2018-08-02T00:40:13.863014: step 28445, loss 0.562399.
Train: 2018-08-02T00:40:14.036519: step 28446, loss 0.51065.
Train: 2018-08-02T00:40:14.210071: step 28447, loss 0.545072.
Train: 2018-08-02T00:40:14.389575: step 28448, loss 0.475312.
Train: 2018-08-02T00:40:14.556161: step 28449, loss 0.579963.
Train: 2018-08-02T00:40:14.727697: step 28450, loss 0.527286.
Test: 2018-08-02T00:40:15.265270: step 28450, loss 0.547704.
Train: 2018-08-02T00:40:15.441762: step 28451, loss 0.58017.
Train: 2018-08-02T00:40:15.607345: step 28452, loss 0.509326.
Train: 2018-08-02T00:40:15.772903: step 28453, loss 0.491357.
Train: 2018-08-02T00:40:15.937438: step 28454, loss 0.634087.
Train: 2018-08-02T00:40:16.104024: step 28455, loss 0.634311.
Train: 2018-08-02T00:40:16.267587: step 28456, loss 0.598538.
Train: 2018-08-02T00:40:16.434111: step 28457, loss 0.508754.
Train: 2018-08-02T00:40:16.604653: step 28458, loss 0.598637.
Train: 2018-08-02T00:40:16.775199: step 28459, loss 0.544671.
Train: 2018-08-02T00:40:16.940756: step 28460, loss 0.70675.
Test: 2018-08-02T00:40:17.476325: step 28460, loss 0.5476.
Train: 2018-08-02T00:40:17.640909: step 28461, loss 0.454736.
Train: 2018-08-02T00:40:17.808462: step 28462, loss 0.634615.
Train: 2018-08-02T00:40:17.972997: step 28463, loss 0.544681.
Train: 2018-08-02T00:40:18.141585: step 28464, loss 0.490812.
Train: 2018-08-02T00:40:18.314084: step 28465, loss 0.401021.
Train: 2018-08-02T00:40:18.480670: step 28466, loss 0.490709.
Train: 2018-08-02T00:40:18.644233: step 28467, loss 0.616771.
Train: 2018-08-02T00:40:18.823722: step 28468, loss 0.508553.
Train: 2018-08-02T00:40:18.988282: step 28469, loss 0.580801.
Train: 2018-08-02T00:40:19.155859: step 28470, loss 0.544638.
Test: 2018-08-02T00:40:19.685449: step 28470, loss 0.547579.
Train: 2018-08-02T00:40:19.865963: step 28471, loss 0.526517.
Train: 2018-08-02T00:40:20.031523: step 28472, loss 0.508352.
Train: 2018-08-02T00:40:20.198049: step 28473, loss 0.58095.
Train: 2018-08-02T00:40:20.361611: step 28474, loss 0.617349.
Train: 2018-08-02T00:40:20.526199: step 28475, loss 0.508244.
Train: 2018-08-02T00:40:20.700735: step 28476, loss 0.635596.
Train: 2018-08-02T00:40:20.866266: step 28477, loss 0.562804.
Train: 2018-08-02T00:40:21.033814: step 28478, loss 0.580975.
Train: 2018-08-02T00:40:21.197408: step 28479, loss 0.562784.
Train: 2018-08-02T00:40:21.360965: step 28480, loss 0.490183.
Test: 2018-08-02T00:40:21.893532: step 28480, loss 0.547576.
Train: 2018-08-02T00:40:22.062096: step 28481, loss 0.544625.
Train: 2018-08-02T00:40:22.230615: step 28482, loss 0.599052.
Train: 2018-08-02T00:40:22.396202: step 28483, loss 0.490237.
Train: 2018-08-02T00:40:22.564722: step 28484, loss 0.580892.
Train: 2018-08-02T00:40:22.740252: step 28485, loss 0.580883.
Train: 2018-08-02T00:40:22.907829: step 28486, loss 0.598981.
Train: 2018-08-02T00:40:23.071395: step 28487, loss 0.562734.
Train: 2018-08-02T00:40:23.238948: step 28488, loss 0.598873.
Train: 2018-08-02T00:40:23.403480: step 28489, loss 0.490508.
Train: 2018-08-02T00:40:23.568039: step 28490, loss 0.562692.
Test: 2018-08-02T00:40:24.101613: step 28490, loss 0.547592.
Train: 2018-08-02T00:40:24.269195: step 28491, loss 0.616745.
Train: 2018-08-02T00:40:24.437739: step 28492, loss 0.670635.
Train: 2018-08-02T00:40:24.608258: step 28493, loss 0.652374.
Train: 2018-08-02T00:40:24.773816: step 28494, loss 0.526831.
Train: 2018-08-02T00:40:24.938375: step 28495, loss 0.49124.
Train: 2018-08-02T00:40:25.105952: step 28496, loss 0.669353.
Train: 2018-08-02T00:40:25.272507: step 28497, loss 0.598023.
Train: 2018-08-02T00:40:25.436071: step 28498, loss 0.544818.
Train: 2018-08-02T00:40:25.600636: step 28499, loss 0.562487.
Train: 2018-08-02T00:40:25.768182: step 28500, loss 0.527284.
Test: 2018-08-02T00:40:26.311704: step 28500, loss 0.547757.
Train: 2018-08-02T00:40:27.142666: step 28501, loss 0.580019.
Train: 2018-08-02T00:40:27.311217: step 28502, loss 0.597494.
Train: 2018-08-02T00:40:27.477771: step 28503, loss 0.544957.
Train: 2018-08-02T00:40:27.646290: step 28504, loss 0.544983.
Train: 2018-08-02T00:40:27.811882: step 28505, loss 0.562424.
Train: 2018-08-02T00:40:27.982419: step 28506, loss 0.545026.
Train: 2018-08-02T00:40:28.144957: step 28507, loss 0.440807.
Train: 2018-08-02T00:40:28.313532: step 28508, loss 0.458142.
Train: 2018-08-02T00:40:28.479088: step 28509, loss 0.52761.
Train: 2018-08-02T00:40:28.659581: step 28510, loss 0.544991.
Test: 2018-08-02T00:40:29.184179: step 28510, loss 0.547808.
Train: 2018-08-02T00:40:29.349744: step 28511, loss 0.544968.
Train: 2018-08-02T00:40:29.516297: step 28512, loss 0.632433.
Train: 2018-08-02T00:40:29.692818: step 28513, loss 0.527427.
Train: 2018-08-02T00:40:29.856408: step 28514, loss 0.579977.
Train: 2018-08-02T00:40:30.019944: step 28515, loss 0.492306.
Train: 2018-08-02T00:40:30.179542: step 28516, loss 0.58002.
Train: 2018-08-02T00:40:30.344078: step 28517, loss 0.527314.
Train: 2018-08-02T00:40:30.507640: step 28518, loss 0.509684.
Train: 2018-08-02T00:40:30.672231: step 28519, loss 0.527233.
Train: 2018-08-02T00:40:30.844740: step 28520, loss 0.42125.
Test: 2018-08-02T00:40:31.377326: step 28520, loss 0.547684.
Train: 2018-08-02T00:40:31.541901: step 28521, loss 0.562515.
Train: 2018-08-02T00:40:31.714415: step 28522, loss 0.527018.
Train: 2018-08-02T00:40:31.881003: step 28523, loss 0.491325.
Train: 2018-08-02T00:40:32.057498: step 28524, loss 0.687652.
Train: 2018-08-02T00:40:32.225068: step 28525, loss 0.491043.
Train: 2018-08-02T00:40:32.396592: step 28526, loss 0.59847.
Train: 2018-08-02T00:40:32.574141: step 28527, loss 0.490854.
Train: 2018-08-02T00:40:32.749647: step 28528, loss 0.688492.
Train: 2018-08-02T00:40:32.914238: step 28529, loss 0.526699.
Train: 2018-08-02T00:40:33.082789: step 28530, loss 0.562658.
Test: 2018-08-02T00:40:33.627302: step 28530, loss 0.547599.
Train: 2018-08-02T00:40:33.802832: step 28531, loss 0.580646.
Train: 2018-08-02T00:40:33.971381: step 28532, loss 0.472744.
Train: 2018-08-02T00:40:34.134975: step 28533, loss 0.490684.
Train: 2018-08-02T00:40:34.296544: step 28534, loss 0.508624.
Train: 2018-08-02T00:40:34.461100: step 28535, loss 0.544652.
Train: 2018-08-02T00:40:34.625665: step 28536, loss 0.490412.
Train: 2018-08-02T00:40:34.791190: step 28537, loss 0.472176.
Train: 2018-08-02T00:40:34.951760: step 28538, loss 0.635436.
Train: 2018-08-02T00:40:35.115357: step 28539, loss 0.562804.
Train: 2018-08-02T00:40:35.276922: step 28540, loss 0.56282.
Test: 2018-08-02T00:40:35.817447: step 28540, loss 0.54757.
Train: 2018-08-02T00:40:35.984001: step 28541, loss 0.617516.
Train: 2018-08-02T00:40:36.148561: step 28542, loss 0.617519.
Train: 2018-08-02T00:40:36.312152: step 28543, loss 0.61747.
Train: 2018-08-02T00:40:36.476717: step 28544, loss 0.617374.
Train: 2018-08-02T00:40:36.641244: step 28545, loss 0.472009.
Train: 2018-08-02T00:40:36.805836: step 28546, loss 0.472084.
Train: 2018-08-02T00:40:36.981335: step 28547, loss 0.580897.
Train: 2018-08-02T00:40:37.146918: step 28548, loss 0.544628.
Train: 2018-08-02T00:40:37.316439: step 28549, loss 0.599004.
Train: 2018-08-02T00:40:37.493964: step 28550, loss 0.653291.
Test: 2018-08-02T00:40:38.032551: step 28550, loss 0.547583.
Train: 2018-08-02T00:40:38.204066: step 28551, loss 0.526568.
Train: 2018-08-02T00:40:38.368653: step 28552, loss 0.544652.
Train: 2018-08-02T00:40:38.537201: step 28553, loss 0.490575.
Train: 2018-08-02T00:40:38.703731: step 28554, loss 0.580703.
Train: 2018-08-02T00:40:38.865331: step 28555, loss 0.544667.
Train: 2018-08-02T00:40:39.029895: step 28556, loss 0.598663.
Train: 2018-08-02T00:40:39.197411: step 28557, loss 0.724464.
Train: 2018-08-02T00:40:39.363996: step 28558, loss 0.59848.
Train: 2018-08-02T00:40:39.527561: step 28559, loss 0.65195.
Train: 2018-08-02T00:40:39.698100: step 28560, loss 0.687164.
Test: 2018-08-02T00:40:40.236645: step 28560, loss 0.547687.
Train: 2018-08-02T00:40:40.400223: step 28561, loss 0.473964.
Train: 2018-08-02T00:40:40.565778: step 28562, loss 0.597784.
Train: 2018-08-02T00:40:40.728318: step 28563, loss 0.421833.
Train: 2018-08-02T00:40:40.898863: step 28564, loss 0.580006.
Train: 2018-08-02T00:40:41.064420: step 28565, loss 0.597476.
Train: 2018-08-02T00:40:41.231990: step 28566, loss 0.475064.
Train: 2018-08-02T00:40:41.408500: step 28567, loss 0.562434.
Train: 2018-08-02T00:40:41.581039: step 28568, loss 0.562431.
Train: 2018-08-02T00:40:41.742631: step 28569, loss 0.562429.
Train: 2018-08-02T00:40:41.906194: step 28570, loss 0.527605.
Test: 2018-08-02T00:40:42.437780: step 28570, loss 0.54785.
Train: 2018-08-02T00:40:42.602310: step 28571, loss 0.527616.
Train: 2018-08-02T00:40:42.781829: step 28572, loss 0.527617.
Train: 2018-08-02T00:40:42.948383: step 28573, loss 0.562427.
Train: 2018-08-02T00:40:43.113965: step 28574, loss 0.54501.
Train: 2018-08-02T00:40:43.275509: step 28575, loss 0.614697.
Train: 2018-08-02T00:40:43.439082: step 28576, loss 0.527589.
Train: 2018-08-02T00:40:43.606650: step 28577, loss 0.61469.
Train: 2018-08-02T00:40:43.771210: step 28578, loss 0.597248.
Train: 2018-08-02T00:40:43.935775: step 28579, loss 0.475444.
Train: 2018-08-02T00:40:44.098342: step 28580, loss 0.545024.
Test: 2018-08-02T00:40:44.626904: step 28580, loss 0.547849.
Train: 2018-08-02T00:40:44.792454: step 28581, loss 0.545019.
Train: 2018-08-02T00:40:44.952028: step 28582, loss 0.632068.
Train: 2018-08-02T00:40:45.114623: step 28583, loss 0.492811.
Train: 2018-08-02T00:40:45.279184: step 28584, loss 0.545014.
Train: 2018-08-02T00:40:45.443713: step 28585, loss 0.614679.
Train: 2018-08-02T00:40:45.606304: step 28586, loss 0.510182.
Train: 2018-08-02T00:40:45.771836: step 28587, loss 0.527584.
Train: 2018-08-02T00:40:45.945371: step 28588, loss 0.544995.
Train: 2018-08-02T00:40:46.106972: step 28589, loss 0.562431.
Train: 2018-08-02T00:40:46.271526: step 28590, loss 0.562434.
Test: 2018-08-02T00:40:46.795101: step 28590, loss 0.547808.
Train: 2018-08-02T00:40:46.962679: step 28591, loss 0.544967.
Train: 2018-08-02T00:40:47.127243: step 28592, loss 0.527477.
Train: 2018-08-02T00:40:47.295763: step 28593, loss 0.632436.
Train: 2018-08-02T00:40:47.462342: step 28594, loss 0.562443.
Train: 2018-08-02T00:40:47.626877: step 28595, loss 0.581106.
Train: 2018-08-02T00:40:47.793460: step 28596, loss 0.52746.
Train: 2018-08-02T00:40:47.959014: step 28597, loss 0.509966.
Train: 2018-08-02T00:40:48.120587: step 28598, loss 0.59745.
Train: 2018-08-02T00:40:48.288134: step 28599, loss 0.544941.
Train: 2018-08-02T00:40:48.467629: step 28600, loss 0.474904.
Test: 2018-08-02T00:40:49.001203: step 28600, loss 0.547773.
Train: 2018-08-02T00:40:49.723703: step 28601, loss 0.544922.
Train: 2018-08-02T00:40:49.893249: step 28602, loss 0.580009.
Train: 2018-08-02T00:40:50.063794: step 28603, loss 0.562462.
Train: 2018-08-02T00:40:50.230348: step 28604, loss 0.474581.
Train: 2018-08-02T00:40:50.408870: step 28605, loss 0.61529.
Train: 2018-08-02T00:40:50.574455: step 28606, loss 0.685804.
Train: 2018-08-02T00:40:50.742005: step 28607, loss 0.562474.
Train: 2018-08-02T00:40:50.905543: step 28608, loss 0.580057.
Train: 2018-08-02T00:40:51.082071: step 28609, loss 0.457046.
Train: 2018-08-02T00:40:51.248627: step 28610, loss 0.650334.
Test: 2018-08-02T00:40:51.782203: step 28610, loss 0.547757.
Train: 2018-08-02T00:40:51.947767: step 28611, loss 0.56246.
Train: 2018-08-02T00:40:52.125282: step 28612, loss 0.544912.
Train: 2018-08-02T00:40:52.287874: step 28613, loss 0.492323.
Train: 2018-08-02T00:40:52.452408: step 28614, loss 0.544918.
Train: 2018-08-02T00:40:52.616969: step 28615, loss 0.667705.
Train: 2018-08-02T00:40:52.790504: step 28616, loss 0.527402.
Train: 2018-08-02T00:40:52.955095: step 28617, loss 0.404817.
Train: 2018-08-02T00:40:53.120650: step 28618, loss 0.579991.
Train: 2018-08-02T00:40:53.282191: step 28619, loss 0.509798.
Train: 2018-08-02T00:40:53.448769: step 28620, loss 0.615199.
Test: 2018-08-02T00:40:53.977366: step 28620, loss 0.547742.
Train: 2018-08-02T00:40:54.139928: step 28621, loss 0.527295.
Train: 2018-08-02T00:40:54.302494: step 28622, loss 0.52727.
Train: 2018-08-02T00:40:54.464055: step 28623, loss 0.491995.
Train: 2018-08-02T00:40:54.628616: step 28624, loss 0.54484.
Train: 2018-08-02T00:40:54.804122: step 28625, loss 0.597868.
Train: 2018-08-02T00:40:54.967684: step 28626, loss 0.580211.
Train: 2018-08-02T00:40:55.129283: step 28627, loss 0.597938.
Train: 2018-08-02T00:40:55.294820: step 28628, loss 0.527091.
Train: 2018-08-02T00:40:55.457406: step 28629, loss 0.473921.
Train: 2018-08-02T00:40:55.629914: step 28630, loss 0.598014.
Test: 2018-08-02T00:40:56.154512: step 28630, loss 0.547667.
Train: 2018-08-02T00:40:56.321092: step 28631, loss 0.45601.
Train: 2018-08-02T00:40:56.484663: step 28632, loss 0.580333.
Train: 2018-08-02T00:40:56.650221: step 28633, loss 0.61598.
Train: 2018-08-02T00:40:56.827712: step 28634, loss 0.526935.
Train: 2018-08-02T00:40:56.993303: step 28635, loss 0.50909.
Train: 2018-08-02T00:40:57.156831: step 28636, loss 0.616117.
Train: 2018-08-02T00:40:57.325381: step 28637, loss 0.616135.
Train: 2018-08-02T00:40:57.485983: step 28638, loss 0.651791.
Train: 2018-08-02T00:40:57.652535: step 28639, loss 0.562563.
Train: 2018-08-02T00:40:57.818095: step 28640, loss 0.526973.
Test: 2018-08-02T00:40:58.343660: step 28640, loss 0.547661.
Train: 2018-08-02T00:40:58.509217: step 28641, loss 0.527001.
Train: 2018-08-02T00:40:58.675797: step 28642, loss 0.633574.
Train: 2018-08-02T00:40:58.842327: step 28643, loss 0.491592.
Train: 2018-08-02T00:40:59.003895: step 28644, loss 0.597965.
Train: 2018-08-02T00:40:59.177431: step 28645, loss 0.615625.
Train: 2018-08-02T00:40:59.340994: step 28646, loss 0.650878.
Train: 2018-08-02T00:40:59.514529: step 28647, loss 0.509594.
Train: 2018-08-02T00:40:59.681085: step 28648, loss 0.509677.
Train: 2018-08-02T00:40:59.854620: step 28649, loss 0.544886.
Train: 2018-08-02T00:41:00.020202: step 28650, loss 0.562461.
Test: 2018-08-02T00:41:00.548789: step 28650, loss 0.54776.
Train: 2018-08-02T00:41:00.715320: step 28651, loss 0.492252.
Train: 2018-08-02T00:41:00.884905: step 28652, loss 0.597562.
Train: 2018-08-02T00:41:01.044464: step 28653, loss 0.457188.
Train: 2018-08-02T00:41:01.205010: step 28654, loss 0.492226.
Train: 2018-08-02T00:41:01.370568: step 28655, loss 0.527299.
Train: 2018-08-02T00:41:01.539143: step 28656, loss 0.562477.
Train: 2018-08-02T00:41:01.714648: step 28657, loss 0.66832.
Train: 2018-08-02T00:41:01.878227: step 28658, loss 0.562486.
Train: 2018-08-02T00:41:02.043793: step 28659, loss 0.562486.
Train: 2018-08-02T00:41:02.210348: step 28660, loss 0.597752.
Test: 2018-08-02T00:41:02.723963: step 28660, loss 0.547724.
Train: 2018-08-02T00:41:02.888510: step 28661, loss 0.580103.
Train: 2018-08-02T00:41:03.059087: step 28662, loss 0.580083.
Train: 2018-08-02T00:41:03.218628: step 28663, loss 0.527291.
Train: 2018-08-02T00:41:03.382216: step 28664, loss 0.527309.
Train: 2018-08-02T00:41:03.546783: step 28665, loss 0.65034.
Train: 2018-08-02T00:41:03.711345: step 28666, loss 0.58001.
Train: 2018-08-02T00:41:03.875898: step 28667, loss 0.579978.
Train: 2018-08-02T00:41:04.046414: step 28668, loss 0.49244.
Train: 2018-08-02T00:41:04.210975: step 28669, loss 0.52746.
Train: 2018-08-02T00:41:04.383514: step 28670, loss 0.527466.
Test: 2018-08-02T00:41:04.928058: step 28670, loss 0.547794.
Train: 2018-08-02T00:41:05.096637: step 28671, loss 0.70236.
Train: 2018-08-02T00:41:05.258201: step 28672, loss 0.632285.
Train: 2018-08-02T00:41:05.431745: step 28673, loss 0.510155.
Train: 2018-08-02T00:41:05.595299: step 28674, loss 0.614613.
Train: 2018-08-02T00:41:05.756873: step 28675, loss 0.492961.
Train: 2018-08-02T00:41:05.921427: step 28676, loss 0.527719.
Train: 2018-08-02T00:41:06.086960: step 28677, loss 0.458387.
Train: 2018-08-02T00:41:06.251520: step 28678, loss 0.597113.
Train: 2018-08-02T00:41:06.419098: step 28679, loss 0.631833.
Train: 2018-08-02T00:41:06.581662: step 28680, loss 0.458348.
Test: 2018-08-02T00:41:07.117223: step 28680, loss 0.54788.
Train: 2018-08-02T00:41:07.286753: step 28681, loss 0.579767.
Train: 2018-08-02T00:41:07.450315: step 28682, loss 0.49297.
Train: 2018-08-02T00:41:07.625845: step 28683, loss 0.492896.
Train: 2018-08-02T00:41:07.790431: step 28684, loss 0.562422.
Train: 2018-08-02T00:41:07.956992: step 28685, loss 0.632184.
Train: 2018-08-02T00:41:08.127506: step 28686, loss 0.544982.
Train: 2018-08-02T00:41:08.296054: step 28687, loss 0.492594.
Train: 2018-08-02T00:41:08.466599: step 28688, loss 0.579922.
Train: 2018-08-02T00:41:08.626206: step 28689, loss 0.544942.
Train: 2018-08-02T00:41:08.796716: step 28690, loss 0.527411.
Test: 2018-08-02T00:41:09.336274: step 28690, loss 0.547765.
Train: 2018-08-02T00:41:09.502859: step 28691, loss 0.509832.
Train: 2018-08-02T00:41:09.669383: step 28692, loss 0.439468.
Train: 2018-08-02T00:41:09.834971: step 28693, loss 0.615347.
Train: 2018-08-02T00:41:09.999525: step 28694, loss 0.597805.
Train: 2018-08-02T00:41:10.167053: step 28695, loss 0.527145.
Train: 2018-08-02T00:41:10.333608: step 28696, loss 0.544808.
Train: 2018-08-02T00:41:10.497170: step 28697, loss 0.420693.
Train: 2018-08-02T00:41:10.665719: step 28698, loss 0.704776.
Train: 2018-08-02T00:41:10.827287: step 28699, loss 0.562552.
Train: 2018-08-02T00:41:10.989854: step 28700, loss 0.598166.
Test: 2018-08-02T00:41:11.518440: step 28700, loss 0.547648.
Train: 2018-08-02T00:41:12.340052: step 28701, loss 0.562558.
Train: 2018-08-02T00:41:12.515553: step 28702, loss 0.509145.
Train: 2018-08-02T00:41:12.680139: step 28703, loss 0.562561.
Train: 2018-08-02T00:41:12.841713: step 28704, loss 0.562564.
Train: 2018-08-02T00:41:13.009234: step 28705, loss 0.509102.
Train: 2018-08-02T00:41:13.172822: step 28706, loss 0.562572.
Train: 2018-08-02T00:41:13.344337: step 28707, loss 0.687471.
Train: 2018-08-02T00:41:13.515878: step 28708, loss 0.526921.
Train: 2018-08-02T00:41:13.678444: step 28709, loss 0.580371.
Train: 2018-08-02T00:41:13.842034: step 28710, loss 0.509169.
Test: 2018-08-02T00:41:14.373585: step 28710, loss 0.547654.
Train: 2018-08-02T00:41:14.534182: step 28711, loss 0.598128.
Train: 2018-08-02T00:41:14.697751: step 28712, loss 0.562543.
Train: 2018-08-02T00:41:14.864274: step 28713, loss 0.580298.
Train: 2018-08-02T00:41:15.026839: step 28714, loss 0.651242.
Train: 2018-08-02T00:41:15.206359: step 28715, loss 0.491686.
Train: 2018-08-02T00:41:15.373938: step 28716, loss 0.527131.
Train: 2018-08-02T00:41:15.538472: step 28717, loss 0.456448.
Train: 2018-08-02T00:41:15.710043: step 28718, loss 0.580189.
Train: 2018-08-02T00:41:15.878563: step 28719, loss 0.562506.
Train: 2018-08-02T00:41:16.043122: step 28720, loss 0.562506.
Test: 2018-08-02T00:41:16.572707: step 28720, loss 0.547692.
Train: 2018-08-02T00:41:16.743282: step 28721, loss 0.491742.
Train: 2018-08-02T00:41:16.907836: step 28722, loss 0.509399.
Train: 2018-08-02T00:41:17.068381: step 28723, loss 0.562521.
Train: 2018-08-02T00:41:17.233939: step 28724, loss 0.527042.
Train: 2018-08-02T00:41:17.395507: step 28725, loss 0.580303.
Train: 2018-08-02T00:41:17.560067: step 28726, loss 0.509208.
Train: 2018-08-02T00:41:17.724627: step 28727, loss 0.562556.
Train: 2018-08-02T00:41:17.888215: step 28728, loss 0.580383.
Train: 2018-08-02T00:41:18.052782: step 28729, loss 0.491252.
Train: 2018-08-02T00:41:18.220333: step 28730, loss 0.616137.
Test: 2018-08-02T00:41:18.744900: step 28730, loss 0.547631.
Train: 2018-08-02T00:41:18.912479: step 28731, loss 0.562585.
Train: 2018-08-02T00:41:19.074051: step 28732, loss 0.508998.
Train: 2018-08-02T00:41:19.243567: step 28733, loss 0.491089.
Train: 2018-08-02T00:41:19.403172: step 28734, loss 0.544708.
Train: 2018-08-02T00:41:19.567725: step 28735, loss 0.544698.
Train: 2018-08-02T00:41:19.735278: step 28736, loss 0.544688.
Train: 2018-08-02T00:41:19.896845: step 28737, loss 0.652499.
Train: 2018-08-02T00:41:20.057391: step 28738, loss 0.526712.
Train: 2018-08-02T00:41:20.223971: step 28739, loss 0.490761.
Train: 2018-08-02T00:41:20.382553: step 28740, loss 0.454728.
Test: 2018-08-02T00:41:20.910144: step 28740, loss 0.547592.
Train: 2018-08-02T00:41:21.074703: step 28741, loss 0.562684.
Train: 2018-08-02T00:41:21.245216: step 28742, loss 0.580755.
Train: 2018-08-02T00:41:21.407807: step 28743, loss 0.490432.
Train: 2018-08-02T00:41:21.568383: step 28744, loss 0.689442.
Train: 2018-08-02T00:41:21.742885: step 28745, loss 0.653232.
Train: 2018-08-02T00:41:21.911450: step 28746, loss 0.562721.
Train: 2018-08-02T00:41:22.082976: step 28747, loss 0.472433.
Train: 2018-08-02T00:41:22.260501: step 28748, loss 0.526602.
Train: 2018-08-02T00:41:22.426060: step 28749, loss 0.689055.
Train: 2018-08-02T00:41:22.600592: step 28750, loss 0.634776.
Test: 2018-08-02T00:41:23.122226: step 28750, loss 0.547599.
Train: 2018-08-02T00:41:23.289750: step 28751, loss 0.544675.
Train: 2018-08-02T00:41:23.451319: step 28752, loss 0.616472.
Train: 2018-08-02T00:41:23.615911: step 28753, loss 0.580508.
Train: 2018-08-02T00:41:23.782434: step 28754, loss 0.598289.
Train: 2018-08-02T00:41:23.949985: step 28755, loss 0.544754.
Train: 2018-08-02T00:41:24.111584: step 28756, loss 0.651325.
Train: 2018-08-02T00:41:24.278109: step 28757, loss 0.597906.
Train: 2018-08-02T00:41:24.444688: step 28758, loss 0.580121.
Train: 2018-08-02T00:41:24.610220: step 28759, loss 0.615187.
Train: 2018-08-02T00:41:24.771788: step 28760, loss 0.562445.
Test: 2018-08-02T00:41:25.302371: step 28760, loss 0.547821.
Train: 2018-08-02T00:41:25.463966: step 28761, loss 0.475203.
Train: 2018-08-02T00:41:25.629520: step 28762, loss 0.632051.
Train: 2018-08-02T00:41:25.794088: step 28763, loss 0.597129.
Train: 2018-08-02T00:41:25.959613: step 28764, loss 0.545101.
Train: 2018-08-02T00:41:26.122209: step 28765, loss 0.510617.
Train: 2018-08-02T00:41:26.291727: step 28766, loss 0.5624.
Train: 2018-08-02T00:41:26.452296: step 28767, loss 0.579605.
Train: 2018-08-02T00:41:26.612899: step 28768, loss 0.579577.
Train: 2018-08-02T00:41:26.775433: step 28769, loss 0.631001.
Train: 2018-08-02T00:41:26.938995: step 28770, loss 0.528177.
Test: 2018-08-02T00:41:27.468580: step 28770, loss 0.548096.
Train: 2018-08-02T00:41:27.636176: step 28771, loss 0.630728.
Train: 2018-08-02T00:41:27.801721: step 28772, loss 0.528319.
Train: 2018-08-02T00:41:27.966282: step 28773, loss 0.545392.
Train: 2018-08-02T00:41:28.129838: step 28774, loss 0.545417.
Train: 2018-08-02T00:41:28.293386: step 28775, loss 0.596362.
Train: 2018-08-02T00:41:28.463919: step 28776, loss 0.613279.
Train: 2018-08-02T00:41:28.632500: step 28777, loss 0.477786.
Train: 2018-08-02T00:41:28.799023: step 28778, loss 0.647015.
Train: 2018-08-02T00:41:28.963608: step 28779, loss 0.494843.
Train: 2018-08-02T00:41:29.128174: step 28780, loss 0.545536.
Test: 2018-08-02T00:41:29.662720: step 28780, loss 0.548284.
Train: 2018-08-02T00:41:29.837278: step 28781, loss 0.511762.
Train: 2018-08-02T00:41:30.003829: step 28782, loss 0.562423.
Train: 2018-08-02T00:41:30.170357: step 28783, loss 0.528604.
Train: 2018-08-02T00:41:30.332948: step 28784, loss 0.59627.
Train: 2018-08-02T00:41:30.500475: step 28785, loss 0.511612.
Train: 2018-08-02T00:41:30.671019: step 28786, loss 0.579368.
Train: 2018-08-02T00:41:30.834582: step 28787, loss 0.545442.
Train: 2018-08-02T00:41:30.998170: step 28788, loss 0.579395.
Train: 2018-08-02T00:41:31.164735: step 28789, loss 0.545408.
Train: 2018-08-02T00:41:31.329289: step 28790, loss 0.647475.
Test: 2018-08-02T00:41:31.873803: step 28790, loss 0.548162.
Train: 2018-08-02T00:41:32.037416: step 28791, loss 0.562406.
Train: 2018-08-02T00:41:32.198960: step 28792, loss 0.511387.
Train: 2018-08-02T00:41:32.361500: step 28793, loss 0.511361.
Train: 2018-08-02T00:41:32.535036: step 28794, loss 0.409099.
Train: 2018-08-02T00:41:32.700593: step 28795, loss 0.562399.
Train: 2018-08-02T00:41:32.863190: step 28796, loss 0.630916.
Train: 2018-08-02T00:41:33.027719: step 28797, loss 0.442305.
Train: 2018-08-02T00:41:33.194298: step 28798, loss 0.66563.
Train: 2018-08-02T00:41:33.357868: step 28799, loss 0.562399.
Train: 2018-08-02T00:41:33.521399: step 28800, loss 0.596896.
Test: 2018-08-02T00:41:34.058968: step 28800, loss 0.54795.
Train: 2018-08-02T00:41:34.805978: step 28801, loss 0.631432.
Train: 2018-08-02T00:41:34.969541: step 28802, loss 0.510644.
Train: 2018-08-02T00:41:35.142081: step 28803, loss 0.407081.
Train: 2018-08-02T00:41:35.306609: step 28804, loss 0.596996.
Train: 2018-08-02T00:41:35.465211: step 28805, loss 0.683659.
Train: 2018-08-02T00:41:35.632738: step 28806, loss 0.683641.
Train: 2018-08-02T00:41:35.800289: step 28807, loss 0.458658.
Train: 2018-08-02T00:41:35.960860: step 28808, loss 0.596978.
Train: 2018-08-02T00:41:36.120433: step 28809, loss 0.579679.
Train: 2018-08-02T00:41:36.295996: step 28810, loss 0.579664.
Test: 2018-08-02T00:41:36.824552: step 28810, loss 0.547959.
Train: 2018-08-02T00:41:36.995096: step 28811, loss 0.5624.
Train: 2018-08-02T00:41:37.159687: step 28812, loss 0.596861.
Train: 2018-08-02T00:41:37.322221: step 28813, loss 0.631237.
Train: 2018-08-02T00:41:37.487778: step 28814, loss 0.579573.
Train: 2018-08-02T00:41:37.651342: step 28815, loss 0.442404.
Train: 2018-08-02T00:41:37.814905: step 28816, loss 0.510987.
Train: 2018-08-02T00:41:37.976503: step 28817, loss 0.476681.
Train: 2018-08-02T00:41:38.140068: step 28818, loss 0.528063.
Train: 2018-08-02T00:41:38.312605: step 28819, loss 0.528006.
Train: 2018-08-02T00:41:38.474167: step 28820, loss 0.54517.
Test: 2018-08-02T00:41:39.004724: step 28820, loss 0.547947.
Train: 2018-08-02T00:41:39.170314: step 28821, loss 0.665975.
Train: 2018-08-02T00:41:39.343844: step 28822, loss 0.579672.
Train: 2018-08-02T00:41:39.506413: step 28823, loss 0.510583.
Train: 2018-08-02T00:41:39.678968: step 28824, loss 0.631549.
Train: 2018-08-02T00:41:39.850463: step 28825, loss 0.510553.
Train: 2018-08-02T00:41:40.016046: step 28826, loss 0.579696.
Train: 2018-08-02T00:41:40.187592: step 28827, loss 0.579699.
Train: 2018-08-02T00:41:40.352153: step 28828, loss 0.493231.
Train: 2018-08-02T00:41:40.521699: step 28829, loss 0.545099.
Train: 2018-08-02T00:41:40.690253: step 28830, loss 0.631693.
Test: 2018-08-02T00:41:41.231808: step 28830, loss 0.547904.
Train: 2018-08-02T00:41:41.397328: step 28831, loss 0.527768.
Train: 2018-08-02T00:41:41.563917: step 28832, loss 0.579733.
Train: 2018-08-02T00:41:41.732474: step 28833, loss 0.683692.
Train: 2018-08-02T00:41:41.897044: step 28834, loss 0.614308.
Train: 2018-08-02T00:41:42.060583: step 28835, loss 0.562402.
Train: 2018-08-02T00:41:42.229161: step 28836, loss 0.5624.
Train: 2018-08-02T00:41:42.398700: step 28837, loss 0.54519.
Train: 2018-08-02T00:41:42.566235: step 28838, loss 0.613957.
Train: 2018-08-02T00:41:42.740767: step 28839, loss 0.699643.
Train: 2018-08-02T00:41:42.904324: step 28840, loss 0.511101.
Test: 2018-08-02T00:41:43.446850: step 28840, loss 0.548116.
Train: 2018-08-02T00:41:43.611408: step 28841, loss 0.545342.
Train: 2018-08-02T00:41:43.781979: step 28842, loss 0.477266.
Train: 2018-08-02T00:41:43.942523: step 28843, loss 0.579422.
Train: 2018-08-02T00:41:44.109079: step 28844, loss 0.579412.
Train: 2018-08-02T00:41:44.270675: step 28845, loss 0.630369.
Train: 2018-08-02T00:41:44.451190: step 28846, loss 0.528485.
Train: 2018-08-02T00:41:44.623728: step 28847, loss 0.596308.
Train: 2018-08-02T00:41:44.797276: step 28848, loss 0.596268.
Train: 2018-08-02T00:41:44.960832: step 28849, loss 0.545524.
Train: 2018-08-02T00:41:45.133371: step 28850, loss 0.596187.
Test: 2018-08-02T00:41:45.668908: step 28850, loss 0.548317.
Train: 2018-08-02T00:41:45.836461: step 28851, loss 0.52872.
Train: 2018-08-02T00:41:45.998060: step 28852, loss 0.511906.
Train: 2018-08-02T00:41:46.171581: step 28853, loss 0.596121.
Train: 2018-08-02T00:41:46.345125: step 28854, loss 0.562437.
Train: 2018-08-02T00:41:46.509695: step 28855, loss 0.612933.
Train: 2018-08-02T00:41:46.675243: step 28856, loss 0.495173.
Train: 2018-08-02T00:41:46.842803: step 28857, loss 0.562441.
Train: 2018-08-02T00:41:47.010323: step 28858, loss 0.612909.
Train: 2018-08-02T00:41:47.176878: step 28859, loss 0.629706.
Train: 2018-08-02T00:41:47.336481: step 28860, loss 0.495264.
Test: 2018-08-02T00:41:47.863043: step 28860, loss 0.548386.
Train: 2018-08-02T00:41:48.026605: step 28861, loss 0.528861.
Train: 2018-08-02T00:41:48.189172: step 28862, loss 0.495248.
Train: 2018-08-02T00:41:48.350740: step 28863, loss 0.54562.
Train: 2018-08-02T00:41:48.518292: step 28864, loss 0.579279.
Train: 2018-08-02T00:41:48.683849: step 28865, loss 0.663608.
Train: 2018-08-02T00:41:48.852398: step 28866, loss 0.494998.
Train: 2018-08-02T00:41:49.013967: step 28867, loss 0.494949.
Train: 2018-08-02T00:41:49.181545: step 28868, loss 0.596215.
Train: 2018-08-02T00:41:49.346079: step 28869, loss 0.545508.
Train: 2018-08-02T00:41:49.508644: step 28870, loss 0.579348.
Test: 2018-08-02T00:41:50.036234: step 28870, loss 0.548224.
Train: 2018-08-02T00:41:50.209770: step 28871, loss 0.57936.
Train: 2018-08-02T00:41:50.383306: step 28872, loss 0.715013.
Train: 2018-08-02T00:41:50.552877: step 28873, loss 0.545484.
Train: 2018-08-02T00:41:50.716415: step 28874, loss 0.579335.
Train: 2018-08-02T00:41:50.881973: step 28875, loss 0.57932.
Train: 2018-08-02T00:41:51.045536: step 28876, loss 0.562427.
Train: 2018-08-02T00:41:51.212091: step 28877, loss 0.596154.
Train: 2018-08-02T00:41:51.374656: step 28878, loss 0.511915.
Train: 2018-08-02T00:41:51.537246: step 28879, loss 0.528771.
Train: 2018-08-02T00:41:51.702785: step 28880, loss 0.562437.
Test: 2018-08-02T00:41:52.232362: step 28880, loss 0.548337.
Train: 2018-08-02T00:41:52.400945: step 28881, loss 0.59611.
Train: 2018-08-02T00:41:52.567499: step 28882, loss 0.596103.
Train: 2018-08-02T00:41:52.739036: step 28883, loss 0.56244.
Train: 2018-08-02T00:41:52.904591: step 28884, loss 0.612885.
Train: 2018-08-02T00:41:53.069162: step 28885, loss 0.528854.
Train: 2018-08-02T00:41:53.237706: step 28886, loss 0.629606.
Train: 2018-08-02T00:41:53.401271: step 28887, loss 0.612762.
Train: 2018-08-02T00:41:53.572780: step 28888, loss 0.545723.
Train: 2018-08-02T00:41:53.739335: step 28889, loss 0.612632.
Train: 2018-08-02T00:41:53.900930: step 28890, loss 0.512404.
Test: 2018-08-02T00:41:54.422507: step 28890, loss 0.548518.
Train: 2018-08-02T00:41:54.594074: step 28891, loss 0.562485.
Train: 2018-08-02T00:41:54.761632: step 28892, loss 0.67917.
Train: 2018-08-02T00:41:54.925189: step 28893, loss 0.678948.
Train: 2018-08-02T00:41:55.091750: step 28894, loss 0.562525.
Train: 2018-08-02T00:41:55.263293: step 28895, loss 0.512946.
Train: 2018-08-02T00:41:55.423860: step 28896, loss 0.580166.
Train: 2018-08-02T00:41:55.593409: step 28897, loss 0.480202.
Train: 2018-08-02T00:41:55.762957: step 28898, loss 0.56258.
Train: 2018-08-02T00:41:55.932505: step 28899, loss 0.546116.
Train: 2018-08-02T00:41:56.098028: step 28900, loss 0.628455.
Test: 2018-08-02T00:41:56.637615: step 28900, loss 0.548804.
Train: 2018-08-02T00:41:57.431963: step 28901, loss 0.611961.
Train: 2018-08-02T00:41:57.603535: step 28902, loss 0.546155.
Train: 2018-08-02T00:41:57.769090: step 28903, loss 0.579031.
Train: 2018-08-02T00:41:57.933653: step 28904, loss 0.595445.
Train: 2018-08-02T00:41:58.102195: step 28905, loss 0.546214.
Train: 2018-08-02T00:41:58.269724: step 28906, loss 0.513437.
Train: 2018-08-02T00:41:58.434283: step 28907, loss 0.464212.
Train: 2018-08-02T00:41:58.600864: step 28908, loss 0.562601.
Train: 2018-08-02T00:41:58.766395: step 28909, loss 0.579044.
Train: 2018-08-02T00:41:58.933948: step 28910, loss 0.562573.
Test: 2018-08-02T00:41:59.472509: step 28910, loss 0.548738.
Train: 2018-08-02T00:41:59.637093: step 28911, loss 0.51304.
Train: 2018-08-02T00:41:59.800631: step 28912, loss 0.579085.
Train: 2018-08-02T00:41:59.969220: step 28913, loss 0.54596.
Train: 2018-08-02T00:42:00.137730: step 28914, loss 0.612319.
Train: 2018-08-02T00:42:00.320255: step 28915, loss 0.612368.
Train: 2018-08-02T00:42:00.484802: step 28916, loss 0.612387.
Train: 2018-08-02T00:42:00.668312: step 28917, loss 0.662252.
Train: 2018-08-02T00:42:00.832871: step 28918, loss 0.479511.
Train: 2018-08-02T00:42:01.015383: step 28919, loss 0.612318.
Train: 2018-08-02T00:42:01.176977: step 28920, loss 0.595701.
Test: 2018-08-02T00:42:01.719500: step 28920, loss 0.548648.
Train: 2018-08-02T00:42:01.887082: step 28921, loss 0.612253.
Train: 2018-08-02T00:42:02.053636: step 28922, loss 0.496328.
Train: 2018-08-02T00:42:02.226147: step 28923, loss 0.512895.
Train: 2018-08-02T00:42:02.389709: step 28924, loss 0.446634.
Train: 2018-08-02T00:42:02.553273: step 28925, loss 0.595706.
Train: 2018-08-02T00:42:02.719826: step 28926, loss 0.579128.
Train: 2018-08-02T00:42:02.884418: step 28927, loss 0.57914.
Train: 2018-08-02T00:42:03.050942: step 28928, loss 0.579151.
Train: 2018-08-02T00:42:03.218507: step 28929, loss 0.562488.
Train: 2018-08-02T00:42:03.392030: step 28930, loss 0.529115.
Test: 2018-08-02T00:42:03.924621: step 28930, loss 0.548488.
Train: 2018-08-02T00:42:04.094184: step 28931, loss 0.562476.
Train: 2018-08-02T00:42:04.263729: step 28932, loss 0.478846.
Train: 2018-08-02T00:42:04.430255: step 28933, loss 0.562457.
Train: 2018-08-02T00:42:04.593818: step 28934, loss 0.579244.
Train: 2018-08-02T00:42:04.757411: step 28935, loss 0.646567.
Train: 2018-08-02T00:42:04.921966: step 28936, loss 0.629773.
Train: 2018-08-02T00:42:05.089492: step 28937, loss 0.562439.
Train: 2018-08-02T00:42:05.254077: step 28938, loss 0.495148.
Train: 2018-08-02T00:42:05.433572: step 28939, loss 0.730782.
Train: 2018-08-02T00:42:05.597161: step 28940, loss 0.495207.
Test: 2018-08-02T00:42:06.126720: step 28940, loss 0.548376.
Train: 2018-08-02T00:42:06.296267: step 28941, loss 0.545644.
Train: 2018-08-02T00:42:06.457862: step 28942, loss 0.528846.
Train: 2018-08-02T00:42:06.623416: step 28943, loss 0.427992.
Train: 2018-08-02T00:42:06.787951: step 28944, loss 0.59612.
Train: 2018-08-02T00:42:06.983429: step 28945, loss 0.562429.
Train: 2018-08-02T00:42:07.148015: step 28946, loss 0.64689.
Train: 2018-08-02T00:42:07.315572: step 28947, loss 0.562423.
Train: 2018-08-02T00:42:07.481099: step 28948, loss 0.596227.
Train: 2018-08-02T00:42:07.648676: step 28949, loss 0.579322.
Train: 2018-08-02T00:42:07.812239: step 28950, loss 0.613108.
Test: 2018-08-02T00:42:08.350805: step 28950, loss 0.548291.
Train: 2018-08-02T00:42:08.518357: step 28951, loss 0.511786.
Train: 2018-08-02T00:42:08.680916: step 28952, loss 0.511791.
Train: 2018-08-02T00:42:08.842460: step 28953, loss 0.613091.
Train: 2018-08-02T00:42:09.007019: step 28954, loss 0.545537.
Train: 2018-08-02T00:42:09.173575: step 28955, loss 0.528641.
Train: 2018-08-02T00:42:09.354092: step 28956, loss 0.562422.
Train: 2018-08-02T00:42:09.535606: step 28957, loss 0.613161.
Train: 2018-08-02T00:42:09.698197: step 28958, loss 0.528593.
Train: 2018-08-02T00:42:09.869747: step 28959, loss 0.528576.
Train: 2018-08-02T00:42:10.033302: step 28960, loss 0.630159.
Test: 2018-08-02T00:42:10.572835: step 28960, loss 0.548235.
Train: 2018-08-02T00:42:10.741409: step 28961, loss 0.460806.
Train: 2018-08-02T00:42:10.907938: step 28962, loss 0.528498.
Train: 2018-08-02T00:42:11.072499: step 28963, loss 0.630347.
Train: 2018-08-02T00:42:11.239053: step 28964, loss 0.579402.
Train: 2018-08-02T00:42:11.410619: step 28965, loss 0.59641.
Train: 2018-08-02T00:42:11.573191: step 28966, loss 0.596408.
Train: 2018-08-02T00:42:11.738753: step 28967, loss 0.460449.
Train: 2018-08-02T00:42:11.904308: step 28968, loss 0.596422.
Train: 2018-08-02T00:42:12.074818: step 28969, loss 0.528374.
Train: 2018-08-02T00:42:12.240407: step 28970, loss 0.613491.
Test: 2018-08-02T00:42:12.778937: step 28970, loss 0.548141.
Train: 2018-08-02T00:42:12.955475: step 28971, loss 0.562403.
Train: 2018-08-02T00:42:13.120053: step 28972, loss 0.44317.
Train: 2018-08-02T00:42:13.294559: step 28973, loss 0.494156.
Train: 2018-08-02T00:42:13.463107: step 28974, loss 0.579499.
Train: 2018-08-02T00:42:13.625708: step 28975, loss 0.562397.
Train: 2018-08-02T00:42:13.794271: step 28976, loss 0.510908.
Train: 2018-08-02T00:42:13.964792: step 28977, loss 0.493596.
Train: 2018-08-02T00:42:14.139300: step 28978, loss 0.579649.
Train: 2018-08-02T00:42:14.316854: step 28979, loss 0.59698.
Train: 2018-08-02T00:42:14.484406: step 28980, loss 0.458511.
Test: 2018-08-02T00:42:15.018955: step 28980, loss 0.547874.
Train: 2018-08-02T00:42:15.186511: step 28981, loss 0.579776.
Train: 2018-08-02T00:42:15.351086: step 28982, loss 0.527619.
Train: 2018-08-02T00:42:15.517641: step 28983, loss 0.510105.
Train: 2018-08-02T00:42:15.690180: step 28984, loss 0.509976.
Train: 2018-08-02T00:42:15.856722: step 28985, loss 0.544913.
Train: 2018-08-02T00:42:16.021269: step 28986, loss 0.52729.
Train: 2018-08-02T00:42:16.190829: step 28987, loss 0.633046.
Train: 2018-08-02T00:42:16.356372: step 28988, loss 0.491819.
Train: 2018-08-02T00:42:16.524923: step 28989, loss 0.509385.
Train: 2018-08-02T00:42:16.690480: step 28990, loss 0.509269.
Test: 2018-08-02T00:42:17.230038: step 28990, loss 0.547648.
Train: 2018-08-02T00:42:17.396623: step 28991, loss 0.562557.
Train: 2018-08-02T00:42:17.565142: step 28992, loss 0.544731.
Train: 2018-08-02T00:42:17.733691: step 28993, loss 0.59838.
Train: 2018-08-02T00:42:17.912213: step 28994, loss 0.616367.
Train: 2018-08-02T00:42:18.076814: step 28995, loss 0.526767.
Train: 2018-08-02T00:42:18.245348: step 28996, loss 0.59852.
Train: 2018-08-02T00:42:18.412900: step 28997, loss 0.508791.
Train: 2018-08-02T00:42:18.579455: step 28998, loss 0.580604.
Train: 2018-08-02T00:42:18.747980: step 28999, loss 0.54468.
Train: 2018-08-02T00:42:18.911544: step 29000, loss 0.562651.
Test: 2018-08-02T00:42:19.450103: step 29000, loss 0.5476.
Train: 2018-08-02T00:42:20.240705: step 29001, loss 0.634559.
Train: 2018-08-02T00:42:20.411220: step 29002, loss 0.634495.
Train: 2018-08-02T00:42:20.585784: step 29003, loss 0.63436.
Train: 2018-08-02T00:42:20.757295: step 29004, loss 0.544712.
Train: 2018-08-02T00:42:20.927865: step 29005, loss 0.509024.
Train: 2018-08-02T00:42:21.093395: step 29006, loss 0.580396.
Train: 2018-08-02T00:42:21.258982: step 29007, loss 0.651555.
Train: 2018-08-02T00:42:21.428525: step 29008, loss 0.562533.
Train: 2018-08-02T00:42:21.596083: step 29009, loss 0.704216.
Train: 2018-08-02T00:42:21.762632: step 29010, loss 0.509556.
Test: 2018-08-02T00:42:22.302164: step 29010, loss 0.547738.
Train: 2018-08-02T00:42:22.467722: step 29011, loss 0.527287.
Train: 2018-08-02T00:42:22.634277: step 29012, loss 0.492255.
Train: 2018-08-02T00:42:22.800862: step 29013, loss 0.544922.
Train: 2018-08-02T00:42:22.970378: step 29014, loss 0.527427.
Train: 2018-08-02T00:42:23.137957: step 29015, loss 0.57994.
Train: 2018-08-02T00:42:23.304519: step 29016, loss 0.527469.
Train: 2018-08-02T00:42:23.475054: step 29017, loss 0.63235.
Train: 2018-08-02T00:42:23.639623: step 29018, loss 0.579889.
Train: 2018-08-02T00:42:23.821105: step 29019, loss 0.632162.
Train: 2018-08-02T00:42:23.987659: step 29020, loss 0.49283.
Test: 2018-08-02T00:42:24.508268: step 29020, loss 0.547863.
Train: 2018-08-02T00:42:24.679809: step 29021, loss 0.579794.
Train: 2018-08-02T00:42:24.846389: step 29022, loss 0.440912.
Train: 2018-08-02T00:42:25.023921: step 29023, loss 0.527687.
Train: 2018-08-02T00:42:25.191442: step 29024, loss 0.562415.
Train: 2018-08-02T00:42:25.359990: step 29025, loss 0.492874.
Train: 2018-08-02T00:42:25.523583: step 29026, loss 0.440557.
Train: 2018-08-02T00:42:25.692103: step 29027, loss 0.632252.
Train: 2018-08-02T00:42:25.855691: step 29028, loss 0.562437.
Train: 2018-08-02T00:42:26.022276: step 29029, loss 0.667464.
Train: 2018-08-02T00:42:26.188809: step 29030, loss 0.597443.
Test: 2018-08-02T00:42:26.711383: step 29030, loss 0.547794.
Train: 2018-08-02T00:42:26.876935: step 29031, loss 0.50997.
Train: 2018-08-02T00:42:27.041520: step 29032, loss 0.68487.
Train: 2018-08-02T00:42:27.208050: step 29033, loss 0.59736.
Train: 2018-08-02T00:42:27.375607: step 29034, loss 0.492699.
Train: 2018-08-02T00:42:27.538193: step 29035, loss 0.562423.
Train: 2018-08-02T00:42:27.710706: step 29036, loss 0.579823.
Train: 2018-08-02T00:42:27.876298: step 29037, loss 0.562417.
Train: 2018-08-02T00:42:28.040852: step 29038, loss 0.510305.
Train: 2018-08-02T00:42:28.207378: step 29039, loss 0.614511.
Train: 2018-08-02T00:42:28.373958: step 29040, loss 0.510359.
Test: 2018-08-02T00:42:28.908504: step 29040, loss 0.547884.
Train: 2018-08-02T00:42:29.075059: step 29041, loss 0.649148.
Train: 2018-08-02T00:42:29.250591: step 29042, loss 0.510431.
Train: 2018-08-02T00:42:29.416179: step 29043, loss 0.54509.
Train: 2018-08-02T00:42:29.583700: step 29044, loss 0.562406.
Train: 2018-08-02T00:42:29.756271: step 29045, loss 0.683553.
Train: 2018-08-02T00:42:29.922793: step 29046, loss 0.52785.
Train: 2018-08-02T00:42:30.088350: step 29047, loss 0.545145.
Train: 2018-08-02T00:42:30.250916: step 29048, loss 0.562399.
Train: 2018-08-02T00:42:30.414528: step 29049, loss 0.545172.
Train: 2018-08-02T00:42:30.580061: step 29050, loss 0.648489.
Test: 2018-08-02T00:42:31.119593: step 29050, loss 0.548001.
Train: 2018-08-02T00:42:31.286174: step 29051, loss 0.579589.
Train: 2018-08-02T00:42:31.451736: step 29052, loss 0.596724.
Train: 2018-08-02T00:42:31.615269: step 29053, loss 0.579528.
Train: 2018-08-02T00:42:31.791808: step 29054, loss 0.494003.
Train: 2018-08-02T00:42:31.963362: step 29055, loss 0.52823.
Train: 2018-08-02T00:42:32.128929: step 29056, loss 0.596556.
Train: 2018-08-02T00:42:32.295492: step 29057, loss 0.545333.
Train: 2018-08-02T00:42:32.463033: step 29058, loss 0.528281.
Train: 2018-08-02T00:42:32.629610: step 29059, loss 0.545339.
Train: 2018-08-02T00:42:32.792146: step 29060, loss 0.630664.
Test: 2018-08-02T00:42:33.331680: step 29060, loss 0.548118.
Train: 2018-08-02T00:42:33.501227: step 29061, loss 0.664738.
Train: 2018-08-02T00:42:33.668813: step 29062, loss 0.630506.
Train: 2018-08-02T00:42:33.834336: step 29063, loss 0.647327.
Train: 2018-08-02T00:42:34.000921: step 29064, loss 0.562417.
Train: 2018-08-02T00:42:34.165484: step 29065, loss 0.545548.
Train: 2018-08-02T00:42:34.335028: step 29066, loss 0.461404.
Train: 2018-08-02T00:42:34.502550: step 29067, loss 0.411009.
Train: 2018-08-02T00:42:34.676086: step 29068, loss 0.528738.
Train: 2018-08-02T00:42:34.840646: step 29069, loss 0.528675.
Train: 2018-08-02T00:42:35.014208: step 29070, loss 0.56242.
Test: 2018-08-02T00:42:35.543807: step 29070, loss 0.548229.
Train: 2018-08-02T00:42:35.720325: step 29071, loss 0.528533.
Train: 2018-08-02T00:42:35.887847: step 29072, loss 0.596362.
Train: 2018-08-02T00:42:36.051410: step 29073, loss 0.681419.
Train: 2018-08-02T00:42:36.217989: step 29074, loss 0.596405.
Train: 2018-08-02T00:42:36.382555: step 29075, loss 0.511436.
Train: 2018-08-02T00:42:36.547115: step 29076, loss 0.494432.
Train: 2018-08-02T00:42:36.713667: step 29077, loss 0.511369.
Train: 2018-08-02T00:42:36.881191: step 29078, loss 0.579441.
Train: 2018-08-02T00:42:37.049771: step 29079, loss 0.5624.
Train: 2018-08-02T00:42:37.215324: step 29080, loss 0.476994.
Test: 2018-08-02T00:42:37.742888: step 29080, loss 0.548064.
Train: 2018-08-02T00:42:37.908476: step 29081, loss 0.579515.
Train: 2018-08-02T00:42:38.077019: step 29082, loss 0.528104.
Train: 2018-08-02T00:42:38.238588: step 29083, loss 0.510856.
Train: 2018-08-02T00:42:38.415121: step 29084, loss 0.510734.
Train: 2018-08-02T00:42:38.581676: step 29085, loss 0.510594.
Train: 2018-08-02T00:42:38.749197: step 29086, loss 0.57973.
Train: 2018-08-02T00:42:38.911763: step 29087, loss 0.527679.
Train: 2018-08-02T00:42:39.076354: step 29088, loss 0.527594.
Train: 2018-08-02T00:42:39.239886: step 29089, loss 0.527507.
Train: 2018-08-02T00:42:39.406473: step 29090, loss 0.544932.
Test: 2018-08-02T00:42:39.929044: step 29090, loss 0.547754.
Train: 2018-08-02T00:42:40.100585: step 29091, loss 0.527337.
Train: 2018-08-02T00:42:40.265176: step 29092, loss 0.597697.
Train: 2018-08-02T00:42:40.430702: step 29093, loss 0.633075.
Train: 2018-08-02T00:42:40.601272: step 29094, loss 0.59782.
Train: 2018-08-02T00:42:40.764810: step 29095, loss 0.650839.
Train: 2018-08-02T00:42:40.934384: step 29096, loss 0.527183.
Train: 2018-08-02T00:42:41.103902: step 29097, loss 0.509549.
Train: 2018-08-02T00:42:41.269461: step 29098, loss 0.633088.
Train: 2018-08-02T00:42:41.434046: step 29099, loss 0.474302.
Train: 2018-08-02T00:42:41.603598: step 29100, loss 0.544845.
Test: 2018-08-02T00:42:42.148112: step 29100, loss 0.54771.
Train: 2018-08-02T00:42:42.942386: step 29101, loss 0.580137.
Train: 2018-08-02T00:42:43.109907: step 29102, loss 0.54484.
Train: 2018-08-02T00:42:43.275494: step 29103, loss 0.562491.
Train: 2018-08-02T00:42:43.443016: step 29104, loss 0.562491.
Train: 2018-08-02T00:42:43.608604: step 29105, loss 0.63311.
Train: 2018-08-02T00:42:43.774156: step 29106, loss 0.580124.
Train: 2018-08-02T00:42:43.940686: step 29107, loss 0.597716.
Train: 2018-08-02T00:42:44.107265: step 29108, loss 0.492098.
Train: 2018-08-02T00:42:44.274823: step 29109, loss 0.615214.
Train: 2018-08-02T00:42:44.441379: step 29110, loss 0.509775.
Test: 2018-08-02T00:42:44.977925: step 29110, loss 0.547759.
Train: 2018-08-02T00:42:45.145489: step 29111, loss 0.422041.
Train: 2018-08-02T00:42:45.309028: step 29112, loss 0.685466.
Train: 2018-08-02T00:42:45.484589: step 29113, loss 0.544897.
Train: 2018-08-02T00:42:45.648153: step 29114, loss 0.474671.
Train: 2018-08-02T00:42:45.813679: step 29115, loss 0.650311.
Train: 2018-08-02T00:42:45.980264: step 29116, loss 0.597582.
Train: 2018-08-02T00:42:46.147812: step 29117, loss 0.544909.
Train: 2018-08-02T00:42:46.314340: step 29118, loss 0.66765.
Train: 2018-08-02T00:42:46.478932: step 29119, loss 0.632439.
Train: 2018-08-02T00:42:46.644488: step 29120, loss 0.56243.
Test: 2018-08-02T00:42:47.178032: step 29120, loss 0.547841.
Train: 2018-08-02T00:42:47.346606: step 29121, loss 0.527599.
Train: 2018-08-02T00:42:47.511140: step 29122, loss 0.562415.
Train: 2018-08-02T00:42:47.675732: step 29123, loss 0.614462.
Train: 2018-08-02T00:42:47.843284: step 29124, loss 0.77016.
Train: 2018-08-02T00:42:48.008811: step 29125, loss 0.562399.
Train: 2018-08-02T00:42:48.177360: step 29126, loss 0.545232.
Train: 2018-08-02T00:42:48.343946: step 29127, loss 0.49398.
Train: 2018-08-02T00:42:48.507521: step 29128, loss 0.59653.
Train: 2018-08-02T00:42:48.668047: step 29129, loss 0.528357.
Train: 2018-08-02T00:42:48.835600: step 29130, loss 0.749328.
Test: 2018-08-02T00:42:49.369176: step 29130, loss 0.548243.
Train: 2018-08-02T00:42:49.589262: step 29131, loss 0.613195.
Train: 2018-08-02T00:42:49.754821: step 29132, loss 0.52872.
Train: 2018-08-02T00:42:49.925390: step 29133, loss 0.545645.
Train: 2018-08-02T00:42:50.090922: step 29134, loss 0.579214.
Train: 2018-08-02T00:42:50.258474: step 29135, loss 0.646033.
Train: 2018-08-02T00:42:50.421038: step 29136, loss 0.562493.
Train: 2018-08-02T00:42:50.586598: step 29137, loss 0.545906.
Train: 2018-08-02T00:42:50.752154: step 29138, loss 0.711647.
Train: 2018-08-02T00:42:50.916741: step 29139, loss 0.595567.
Train: 2018-08-02T00:42:51.085294: step 29140, loss 0.579034.
Test: 2018-08-02T00:42:51.615846: step 29140, loss 0.548917.
Train: 2018-08-02T00:42:51.777414: step 29141, loss 0.595387.
Train: 2018-08-02T00:42:51.942971: step 29142, loss 0.481075.
Train: 2018-08-02T00:42:52.108528: step 29143, loss 0.497546.
Train: 2018-08-02T00:42:52.271095: step 29144, loss 0.481329.
Train: 2018-08-02T00:42:52.434656: step 29145, loss 0.611543.
Train: 2018-08-02T00:42:52.598219: step 29146, loss 0.562692.
Train: 2018-08-02T00:42:52.764774: step 29147, loss 0.513829.
Train: 2018-08-02T00:42:52.929334: step 29148, loss 0.546376.
Train: 2018-08-02T00:42:53.089904: step 29149, loss 0.513696.
Train: 2018-08-02T00:42:53.254465: step 29150, loss 0.628065.
Test: 2018-08-02T00:42:53.783056: step 29150, loss 0.548927.
Train: 2018-08-02T00:42:53.948610: step 29151, loss 0.513523.
Train: 2018-08-02T00:42:54.112205: step 29152, loss 0.562619.
Train: 2018-08-02T00:42:54.281751: step 29153, loss 0.513326.
Train: 2018-08-02T00:42:54.451266: step 29154, loss 0.579046.
Train: 2018-08-02T00:42:54.612862: step 29155, loss 0.595556.
Train: 2018-08-02T00:42:54.777394: step 29156, loss 0.595591.
Train: 2018-08-02T00:42:54.937996: step 29157, loss 0.612149.
Train: 2018-08-02T00:42:55.108509: step 29158, loss 0.529466.
Train: 2018-08-02T00:42:55.272072: step 29159, loss 0.529433.
Train: 2018-08-02T00:42:55.436632: step 29160, loss 0.562528.
Test: 2018-08-02T00:42:55.970214: step 29160, loss 0.548623.
Train: 2018-08-02T00:42:56.133793: step 29161, loss 0.512736.
Train: 2018-08-02T00:42:56.297330: step 29162, loss 0.529255.
Train: 2018-08-02T00:42:56.464882: step 29163, loss 0.462517.
Train: 2018-08-02T00:42:56.627449: step 29164, loss 0.545752.
Train: 2018-08-02T00:42:56.791044: step 29165, loss 0.528904.
Train: 2018-08-02T00:42:56.969534: step 29166, loss 0.646602.
Train: 2018-08-02T00:42:57.133096: step 29167, loss 0.511823.
Train: 2018-08-02T00:42:57.296690: step 29168, loss 0.528596.
Train: 2018-08-02T00:42:57.475183: step 29169, loss 0.630248.
Train: 2018-08-02T00:42:57.647755: step 29170, loss 0.528432.
Test: 2018-08-02T00:42:58.173350: step 29170, loss 0.54815.
Train: 2018-08-02T00:42:58.340926: step 29171, loss 0.528361.
Train: 2018-08-02T00:42:58.507456: step 29172, loss 0.511224.
Train: 2018-08-02T00:42:58.673014: step 29173, loss 0.511085.
Train: 2018-08-02T00:42:58.846580: step 29174, loss 0.528085.
Train: 2018-08-02T00:42:59.016121: step 29175, loss 0.476348.
Train: 2018-08-02T00:42:59.180657: step 29176, loss 0.545125.
Train: 2018-08-02T00:42:59.345242: step 29177, loss 0.614429.
Train: 2018-08-02T00:42:59.516790: step 29178, loss 0.492874.
Train: 2018-08-02T00:42:59.689299: step 29179, loss 0.492665.
Train: 2018-08-02T00:42:59.859866: step 29180, loss 0.614954.
Test: 2018-08-02T00:43:00.397404: step 29180, loss 0.54776.
Train: 2018-08-02T00:43:00.569943: step 29181, loss 0.632651.
Train: 2018-08-02T00:43:00.749494: step 29182, loss 0.580037.
Train: 2018-08-02T00:43:00.914023: step 29183, loss 0.562469.
Train: 2018-08-02T00:43:01.089553: step 29184, loss 0.562473.
Train: 2018-08-02T00:43:01.250155: step 29185, loss 0.527242.
Train: 2018-08-02T00:43:01.415681: step 29186, loss 0.544849.
Train: 2018-08-02T00:43:01.582237: step 29187, loss 0.597794.
Train: 2018-08-02T00:43:01.743805: step 29188, loss 0.438881.
Train: 2018-08-02T00:43:01.911356: step 29189, loss 0.509437.
Train: 2018-08-02T00:43:02.073923: step 29190, loss 0.527068.
Test: 2018-08-02T00:43:02.613481: step 29190, loss 0.547661.
Train: 2018-08-02T00:43:02.777073: step 29191, loss 0.544772.
Train: 2018-08-02T00:43:02.944595: step 29192, loss 0.580358.
Train: 2018-08-02T00:43:03.116167: step 29193, loss 0.61606.
Train: 2018-08-02T00:43:03.294660: step 29194, loss 0.723157.
Train: 2018-08-02T00:43:03.457249: step 29195, loss 0.633837.
Train: 2018-08-02T00:43:03.625805: step 29196, loss 0.615887.
Train: 2018-08-02T00:43:03.790364: step 29197, loss 0.657104.
Train: 2018-08-02T00:43:03.957887: step 29198, loss 0.474144.
Train: 2018-08-02T00:43:04.122447: step 29199, loss 0.562482.
Train: 2018-08-02T00:43:04.290028: step 29200, loss 0.50969.
Test: 2018-08-02T00:43:04.817588: step 29200, loss 0.547749.
Train: 2018-08-02T00:43:05.603970: step 29201, loss 0.615169.
Train: 2018-08-02T00:43:05.768556: step 29202, loss 0.457237.
Train: 2018-08-02T00:43:05.945058: step 29203, loss 0.527396.
Train: 2018-08-02T00:43:06.117597: step 29204, loss 0.5274.
Train: 2018-08-02T00:43:06.285193: step 29205, loss 0.579978.
Train: 2018-08-02T00:43:06.445747: step 29206, loss 0.579977.
Train: 2018-08-02T00:43:06.611278: step 29207, loss 0.667581.
Train: 2018-08-02T00:43:06.773868: step 29208, loss 0.422484.
Train: 2018-08-02T00:43:06.936439: step 29209, loss 0.579938.
Train: 2018-08-02T00:43:07.102964: step 29210, loss 0.579936.
Test: 2018-08-02T00:43:07.634541: step 29210, loss 0.547793.
Train: 2018-08-02T00:43:07.798105: step 29211, loss 0.492479.
Train: 2018-08-02T00:43:07.969646: step 29212, loss 0.47495.
Train: 2018-08-02T00:43:08.135229: step 29213, loss 0.544925.
Train: 2018-08-02T00:43:08.299794: step 29214, loss 0.580002.
Train: 2018-08-02T00:43:08.467347: step 29215, loss 0.615156.
Train: 2018-08-02T00:43:08.629908: step 29216, loss 0.632737.
Train: 2018-08-02T00:43:08.797433: step 29217, loss 0.492231.
Train: 2018-08-02T00:43:08.961021: step 29218, loss 0.492223.
Train: 2018-08-02T00:43:09.136526: step 29219, loss 0.544888.
Train: 2018-08-02T00:43:09.303082: step 29220, loss 0.59765.
Test: 2018-08-02T00:43:09.841641: step 29220, loss 0.547735.
Train: 2018-08-02T00:43:10.015178: step 29221, loss 0.527276.
Train: 2018-08-02T00:43:10.190708: step 29222, loss 0.580084.
Train: 2018-08-02T00:43:10.368234: step 29223, loss 0.544862.
Train: 2018-08-02T00:43:10.533823: step 29224, loss 0.650591.
Train: 2018-08-02T00:43:10.704336: step 29225, loss 0.580084.
Train: 2018-08-02T00:43:10.865928: step 29226, loss 0.668028.
Train: 2018-08-02T00:43:11.029497: step 29227, loss 0.43957.
Train: 2018-08-02T00:43:11.192058: step 29228, loss 0.685273.
Train: 2018-08-02T00:43:11.367562: step 29229, loss 0.509913.
Train: 2018-08-02T00:43:11.533153: step 29230, loss 0.66737.
Test: 2018-08-02T00:43:12.069716: step 29230, loss 0.547819.
Train: 2018-08-02T00:43:12.236266: step 29231, loss 0.579875.
Train: 2018-08-02T00:43:12.400801: step 29232, loss 0.527612.
Train: 2018-08-02T00:43:12.565361: step 29233, loss 0.475554.
Train: 2018-08-02T00:43:12.730919: step 29234, loss 0.545051.
Train: 2018-08-02T00:43:12.892511: step 29235, loss 0.579765.
Train: 2018-08-02T00:43:13.052085: step 29236, loss 0.56241.
Train: 2018-08-02T00:43:13.215653: step 29237, loss 0.562408.
Train: 2018-08-02T00:43:13.381180: step 29238, loss 0.562407.
Train: 2018-08-02T00:43:13.549755: step 29239, loss 0.597038.
Train: 2018-08-02T00:43:13.715287: step 29240, loss 0.614305.
Test: 2018-08-02T00:43:14.245868: step 29240, loss 0.547936.
Train: 2018-08-02T00:43:14.413446: step 29241, loss 0.631501.
Train: 2018-08-02T00:43:14.575015: step 29242, loss 0.527925.
Train: 2018-08-02T00:43:14.742540: step 29243, loss 0.579607.
Train: 2018-08-02T00:43:14.910094: step 29244, loss 0.562396.
Train: 2018-08-02T00:43:15.073682: step 29245, loss 0.528083.
Train: 2018-08-02T00:43:15.244223: step 29246, loss 0.562396.
Train: 2018-08-02T00:43:15.418733: step 29247, loss 0.630904.
Train: 2018-08-02T00:43:15.581299: step 29248, loss 0.528198.
Train: 2018-08-02T00:43:15.745884: step 29249, loss 0.442818.
Train: 2018-08-02T00:43:15.911416: step 29250, loss 0.476931.
Test: 2018-08-02T00:43:16.450974: step 29250, loss 0.54806.
Train: 2018-08-02T00:43:16.616532: step 29251, loss 0.562397.
Train: 2018-08-02T00:43:16.780094: step 29252, loss 0.476663.
Train: 2018-08-02T00:43:16.943706: step 29253, loss 0.699896.
Train: 2018-08-02T00:43:17.114201: step 29254, loss 0.545202.
Train: 2018-08-02T00:43:17.282750: step 29255, loss 0.70004.
Train: 2018-08-02T00:43:17.449305: step 29256, loss 0.596768.
Train: 2018-08-02T00:43:17.611901: step 29257, loss 0.562396.
Train: 2018-08-02T00:43:17.788398: step 29258, loss 0.613809.
Train: 2018-08-02T00:43:17.953981: step 29259, loss 0.511079.
Train: 2018-08-02T00:43:18.120510: step 29260, loss 0.545309.
Test: 2018-08-02T00:43:18.658074: step 29260, loss 0.548098.
Train: 2018-08-02T00:43:18.826648: step 29261, loss 0.408702.
Train: 2018-08-02T00:43:18.996170: step 29262, loss 0.545298.
Train: 2018-08-02T00:43:19.161727: step 29263, loss 0.511025.
Train: 2018-08-02T00:43:19.326288: step 29264, loss 0.49377.
Train: 2018-08-02T00:43:19.493870: step 29265, loss 0.493596.
Train: 2018-08-02T00:43:19.657401: step 29266, loss 0.545146.
Train: 2018-08-02T00:43:19.825951: step 29267, loss 0.562405.
Train: 2018-08-02T00:43:20.001483: step 29268, loss 0.406241.
Train: 2018-08-02T00:43:20.166069: step 29269, loss 0.52757.
Train: 2018-08-02T00:43:20.329605: step 29270, loss 0.61498.
Test: 2018-08-02T00:43:20.860187: step 29270, loss 0.547756.
Train: 2018-08-02T00:43:21.028766: step 29271, loss 0.580019.
Train: 2018-08-02T00:43:21.189335: step 29272, loss 0.474492.
Train: 2018-08-02T00:43:21.354890: step 29273, loss 0.580142.
Train: 2018-08-02T00:43:21.518426: step 29274, loss 0.686389.
Train: 2018-08-02T00:43:21.680026: step 29275, loss 0.562514.
Train: 2018-08-02T00:43:21.844598: step 29276, loss 0.615687.
Train: 2018-08-02T00:43:22.011143: step 29277, loss 0.651119.
Train: 2018-08-02T00:43:22.177691: step 29278, loss 0.544812.
Train: 2018-08-02T00:43:22.352198: step 29279, loss 0.544823.
Train: 2018-08-02T00:43:22.516787: step 29280, loss 0.527164.
Test: 2018-08-02T00:43:23.044379: step 29280, loss 0.547707.
Train: 2018-08-02T00:43:23.208909: step 29281, loss 0.544835.
Train: 2018-08-02T00:43:23.372488: step 29282, loss 0.633123.
Train: 2018-08-02T00:43:23.538028: step 29283, loss 0.597764.
Train: 2018-08-02T00:43:23.696630: step 29284, loss 0.562479.
Train: 2018-08-02T00:43:23.868179: step 29285, loss 0.527295.
Train: 2018-08-02T00:43:24.044674: step 29286, loss 0.580037.
Train: 2018-08-02T00:43:24.210236: step 29287, loss 0.580012.
Train: 2018-08-02T00:43:24.383766: step 29288, loss 0.544922.
Train: 2018-08-02T00:43:24.545371: step 29289, loss 0.614989.
Train: 2018-08-02T00:43:24.717875: step 29290, loss 0.422552.
Test: 2018-08-02T00:43:25.248459: step 29290, loss 0.547796.
Train: 2018-08-02T00:43:25.413022: step 29291, loss 0.597421.
Train: 2018-08-02T00:43:25.573612: step 29292, loss 0.492502.
Train: 2018-08-02T00:43:25.741138: step 29293, loss 0.492468.
Train: 2018-08-02T00:43:25.914674: step 29294, loss 0.685058.
Train: 2018-08-02T00:43:26.082227: step 29295, loss 0.562447.
Train: 2018-08-02T00:43:26.243821: step 29296, loss 0.632452.
Train: 2018-08-02T00:43:26.408395: step 29297, loss 0.579919.
Train: 2018-08-02T00:43:26.569923: step 29298, loss 0.719535.
Train: 2018-08-02T00:43:26.736478: step 29299, loss 0.545021.
Train: 2018-08-02T00:43:26.903032: step 29300, loss 0.649184.
Test: 2018-08-02T00:43:27.436605: step 29300, loss 0.547925.
Train: 2018-08-02T00:43:28.220657: step 29301, loss 0.527819.
Train: 2018-08-02T00:43:28.385188: step 29302, loss 0.458927.
Train: 2018-08-02T00:43:28.550745: step 29303, loss 0.562401.
Train: 2018-08-02T00:43:28.716332: step 29304, loss 0.49358.
Train: 2018-08-02T00:43:28.890834: step 29305, loss 0.631207.
Train: 2018-08-02T00:43:29.055395: step 29306, loss 0.545215.
Train: 2018-08-02T00:43:29.229928: step 29307, loss 0.510883.
Train: 2018-08-02T00:43:29.409448: step 29308, loss 0.596743.
Train: 2018-08-02T00:43:29.587971: step 29309, loss 0.631056.
Train: 2018-08-02T00:43:29.749570: step 29310, loss 0.493825.
Test: 2018-08-02T00:43:30.278129: step 29310, loss 0.548047.
Train: 2018-08-02T00:43:30.441715: step 29311, loss 0.510982.
Train: 2018-08-02T00:43:30.617220: step 29312, loss 0.648126.
Train: 2018-08-02T00:43:30.780783: step 29313, loss 0.545265.
Train: 2018-08-02T00:43:30.946340: step 29314, loss 0.528145.
Train: 2018-08-02T00:43:31.109903: step 29315, loss 0.613781.
Train: 2018-08-02T00:43:31.274463: step 29316, loss 0.47681.
Train: 2018-08-02T00:43:31.437060: step 29317, loss 0.630906.
Train: 2018-08-02T00:43:31.600592: step 29318, loss 0.545278.
Train: 2018-08-02T00:43:31.763189: step 29319, loss 0.596637.
Train: 2018-08-02T00:43:31.930709: step 29320, loss 0.562399.
Test: 2018-08-02T00:43:32.458299: step 29320, loss 0.548077.
Train: 2018-08-02T00:43:32.620882: step 29321, loss 0.562399.
Train: 2018-08-02T00:43:32.780437: step 29322, loss 0.699167.
Train: 2018-08-02T00:43:32.948987: step 29323, loss 0.613585.
Train: 2018-08-02T00:43:33.111553: step 29324, loss 0.630482.
Train: 2018-08-02T00:43:33.284121: step 29325, loss 0.562411.
Train: 2018-08-02T00:43:33.450671: step 29326, loss 0.494733.
Train: 2018-08-02T00:43:33.609252: step 29327, loss 0.494843.
Train: 2018-08-02T00:43:33.780763: step 29328, loss 0.562425.
Train: 2018-08-02T00:43:33.944351: step 29329, loss 0.629945.
Train: 2018-08-02T00:43:34.107889: step 29330, loss 0.444413.
Test: 2018-08-02T00:43:34.630492: step 29330, loss 0.548304.
Train: 2018-08-02T00:43:34.794054: step 29331, loss 0.629901.
Train: 2018-08-02T00:43:34.965596: step 29332, loss 0.596154.
Train: 2018-08-02T00:43:35.135143: step 29333, loss 0.612982.
Train: 2018-08-02T00:43:35.300701: step 29334, loss 0.461466.
Train: 2018-08-02T00:43:35.467256: step 29335, loss 0.663433.
Train: 2018-08-02T00:43:35.632812: step 29336, loss 0.562442.
Train: 2018-08-02T00:43:35.807346: step 29337, loss 0.495243.
Train: 2018-08-02T00:43:35.972903: step 29338, loss 0.562445.
Train: 2018-08-02T00:43:36.141477: step 29339, loss 0.545638.
Train: 2018-08-02T00:43:36.309005: step 29340, loss 0.512002.
Test: 2018-08-02T00:43:36.827618: step 29340, loss 0.548343.
Train: 2018-08-02T00:43:36.990212: step 29341, loss 0.612933.
Train: 2018-08-02T00:43:37.156763: step 29342, loss 0.629789.
Train: 2018-08-02T00:43:37.320302: step 29343, loss 0.478284.
Train: 2018-08-02T00:43:37.480873: step 29344, loss 0.562435.
Train: 2018-08-02T00:43:37.645461: step 29345, loss 0.596141.
Train: 2018-08-02T00:43:37.819996: step 29346, loss 0.629866.
Train: 2018-08-02T00:43:37.990510: step 29347, loss 0.596131.
Train: 2018-08-02T00:43:38.163073: step 29348, loss 0.562437.
Train: 2018-08-02T00:43:38.323653: step 29349, loss 0.612903.
Train: 2018-08-02T00:43:38.488178: step 29350, loss 0.596045.
Test: 2018-08-02T00:43:39.021753: step 29350, loss 0.548407.
Train: 2018-08-02T00:43:39.186343: step 29351, loss 0.495356.
Train: 2018-08-02T00:43:39.363839: step 29352, loss 0.528921.
Train: 2018-08-02T00:43:39.529421: step 29353, loss 0.562455.
Train: 2018-08-02T00:43:39.690988: step 29354, loss 0.528912.
Train: 2018-08-02T00:43:39.852543: step 29355, loss 0.444979.
Train: 2018-08-02T00:43:40.014131: step 29356, loss 0.478353.
Train: 2018-08-02T00:43:40.178659: step 29357, loss 0.596166.
Train: 2018-08-02T00:43:40.342223: step 29358, loss 0.56242.
Train: 2018-08-02T00:43:40.514762: step 29359, loss 0.613248.
Train: 2018-08-02T00:43:40.677327: step 29360, loss 0.528475.
Test: 2018-08-02T00:43:41.216886: step 29360, loss 0.548175.
Train: 2018-08-02T00:43:41.385459: step 29361, loss 0.562406.
Train: 2018-08-02T00:43:41.547999: step 29362, loss 0.545382.
Train: 2018-08-02T00:43:41.715552: step 29363, loss 0.664691.
Train: 2018-08-02T00:43:41.882146: step 29364, loss 0.528298.
Train: 2018-08-02T00:43:42.044702: step 29365, loss 0.596523.
Train: 2018-08-02T00:43:42.206270: step 29366, loss 0.460016.
Train: 2018-08-02T00:43:42.371797: step 29367, loss 0.511134.
Train: 2018-08-02T00:43:42.537380: step 29368, loss 0.613759.
Train: 2018-08-02T00:43:42.699940: step 29369, loss 0.579536.
Train: 2018-08-02T00:43:42.875465: step 29370, loss 0.44233.
Test: 2018-08-02T00:43:43.416006: step 29370, loss 0.548003.
Train: 2018-08-02T00:43:43.581593: step 29371, loss 0.510828.
Train: 2018-08-02T00:43:43.747151: step 29372, loss 0.527931.
Train: 2018-08-02T00:43:43.911690: step 29373, loss 0.527841.
Train: 2018-08-02T00:43:44.073248: step 29374, loss 0.562408.
Train: 2018-08-02T00:43:44.241823: step 29375, loss 0.562414.
Train: 2018-08-02T00:43:44.417329: step 29376, loss 0.64948.
Train: 2018-08-02T00:43:44.591862: step 29377, loss 0.492714.
Train: 2018-08-02T00:43:44.757446: step 29378, loss 0.579886.
Train: 2018-08-02T00:43:44.918016: step 29379, loss 0.579912.
Train: 2018-08-02T00:43:45.088535: step 29380, loss 0.649894.
Test: 2018-08-02T00:43:45.621110: step 29380, loss 0.547796.
Train: 2018-08-02T00:43:45.788691: step 29381, loss 0.544953.
Train: 2018-08-02T00:43:45.953248: step 29382, loss 0.562437.
Train: 2018-08-02T00:43:46.113793: step 29383, loss 0.527479.
Train: 2018-08-02T00:43:46.276384: step 29384, loss 0.527473.
Train: 2018-08-02T00:43:46.446935: step 29385, loss 0.527457.
Train: 2018-08-02T00:43:46.618445: step 29386, loss 0.667478.
Train: 2018-08-02T00:43:46.783006: step 29387, loss 0.457462.
Train: 2018-08-02T00:43:46.943576: step 29388, loss 0.562444.
Train: 2018-08-02T00:43:47.108163: step 29389, loss 0.527409.
Train: 2018-08-02T00:43:47.274721: step 29390, loss 0.42218.
Test: 2018-08-02T00:43:47.807266: step 29390, loss 0.547746.
Train: 2018-08-02T00:43:47.974820: step 29391, loss 0.562463.
Train: 2018-08-02T00:43:48.138407: step 29392, loss 0.580087.
Train: 2018-08-02T00:43:48.303964: step 29393, loss 0.456648.
Train: 2018-08-02T00:43:48.468500: step 29394, loss 0.527133.
Train: 2018-08-02T00:43:48.634090: step 29395, loss 0.509328.
Train: 2018-08-02T00:43:48.798643: step 29396, loss 0.509198.
Train: 2018-08-02T00:43:48.962180: step 29397, loss 0.651762.
Train: 2018-08-02T00:43:49.125767: step 29398, loss 0.526854.
Train: 2018-08-02T00:43:49.289306: step 29399, loss 0.526808.
Train: 2018-08-02T00:43:49.451896: step 29400, loss 0.508825.
Test: 2018-08-02T00:43:49.987439: step 29400, loss 0.5476.
Train: 2018-08-02T00:43:50.789524: step 29401, loss 0.616569.
Train: 2018-08-02T00:43:50.954068: step 29402, loss 0.544669.
Train: 2018-08-02T00:43:51.116653: step 29403, loss 0.616728.
Train: 2018-08-02T00:43:51.278233: step 29404, loss 0.61675.
Train: 2018-08-02T00:43:51.444757: step 29405, loss 0.526647.
Train: 2018-08-02T00:43:51.611337: step 29406, loss 0.562675.
Train: 2018-08-02T00:43:51.773876: step 29407, loss 0.634707.
Train: 2018-08-02T00:43:51.941429: step 29408, loss 0.562659.
Train: 2018-08-02T00:43:52.116959: step 29409, loss 0.544679.
Train: 2018-08-02T00:43:52.285535: step 29410, loss 0.598535.
Test: 2018-08-02T00:43:52.824070: step 29410, loss 0.547611.
Train: 2018-08-02T00:43:52.994639: step 29411, loss 0.59847.
Train: 2018-08-02T00:43:53.157210: step 29412, loss 0.526817.
Train: 2018-08-02T00:43:53.320742: step 29413, loss 0.491113.
Train: 2018-08-02T00:43:53.486330: step 29414, loss 0.651888.
Train: 2018-08-02T00:43:53.663853: step 29415, loss 0.740897.
Train: 2018-08-02T00:43:53.832374: step 29416, loss 0.49146.
Train: 2018-08-02T00:43:54.005935: step 29417, loss 0.686602.
Train: 2018-08-02T00:43:54.168475: step 29418, loss 0.544833.
Train: 2018-08-02T00:43:54.335061: step 29419, loss 0.509665.
Train: 2018-08-02T00:43:54.500587: step 29420, loss 0.544898.
Test: 2018-08-02T00:43:55.029222: step 29420, loss 0.547773.
Train: 2018-08-02T00:43:55.192763: step 29421, loss 0.667598.
Train: 2018-08-02T00:43:55.365302: step 29422, loss 0.527493.
Train: 2018-08-02T00:43:55.531861: step 29423, loss 0.544996.
Train: 2018-08-02T00:43:55.708365: step 29424, loss 0.475455.
Train: 2018-08-02T00:43:55.873941: step 29425, loss 0.562415.
Train: 2018-08-02T00:43:56.034487: step 29426, loss 0.579777.
Train: 2018-08-02T00:43:56.202070: step 29427, loss 0.545063.
Train: 2018-08-02T00:43:56.366630: step 29428, loss 0.562408.
Train: 2018-08-02T00:43:56.535150: step 29429, loss 0.718327.
Train: 2018-08-02T00:43:56.699739: step 29430, loss 0.545121.
Test: 2018-08-02T00:43:57.230310: step 29430, loss 0.547957.
Train: 2018-08-02T00:43:57.398865: step 29431, loss 0.458922.
Train: 2018-08-02T00:43:57.571378: step 29432, loss 0.59687.
Train: 2018-08-02T00:43:57.735974: step 29433, loss 0.527962.
Train: 2018-08-02T00:43:57.906482: step 29434, loss 0.562398.
Train: 2018-08-02T00:43:58.070047: step 29435, loss 0.545193.
Train: 2018-08-02T00:43:58.248594: step 29436, loss 0.545197.
Train: 2018-08-02T00:43:58.422139: step 29437, loss 0.527995.
Train: 2018-08-02T00:43:58.587662: step 29438, loss 0.527981.
Train: 2018-08-02T00:43:58.757243: step 29439, loss 0.596842.
Train: 2018-08-02T00:43:58.923772: step 29440, loss 0.579626.
Test: 2018-08-02T00:43:59.464319: step 29440, loss 0.547972.
Train: 2018-08-02T00:43:59.632899: step 29441, loss 0.493485.
Train: 2018-08-02T00:43:59.800420: step 29442, loss 0.476183.
Train: 2018-08-02T00:43:59.979941: step 29443, loss 0.59695.
Train: 2018-08-02T00:44:00.154473: step 29444, loss 0.545108.
Train: 2018-08-02T00:44:00.317038: step 29445, loss 0.579723.
Train: 2018-08-02T00:44:00.483593: step 29446, loss 0.562408.
Train: 2018-08-02T00:44:00.662117: step 29447, loss 0.510372.
Train: 2018-08-02T00:44:00.824682: step 29448, loss 0.59715.
Train: 2018-08-02T00:44:00.996223: step 29449, loss 0.562415.
Train: 2018-08-02T00:44:01.163776: step 29450, loss 0.5972.
Test: 2018-08-02T00:44:01.700345: step 29450, loss 0.547853.
Train: 2018-08-02T00:44:01.863929: step 29451, loss 0.492845.
Train: 2018-08-02T00:44:02.032453: step 29452, loss 0.475379.
Train: 2018-08-02T00:44:02.202996: step 29453, loss 0.544987.
Train: 2018-08-02T00:44:02.372543: step 29454, loss 0.527493.
Train: 2018-08-02T00:44:02.535141: step 29455, loss 0.544938.
Train: 2018-08-02T00:44:02.706652: step 29456, loss 0.57999.
Train: 2018-08-02T00:44:02.870245: step 29457, loss 0.650277.
Train: 2018-08-02T00:44:03.033777: step 29458, loss 0.562461.
Train: 2018-08-02T00:44:03.196341: step 29459, loss 0.667877.
Train: 2018-08-02T00:44:03.360927: step 29460, loss 0.580003.
Test: 2018-08-02T00:44:03.896471: step 29460, loss 0.547773.
Train: 2018-08-02T00:44:04.063049: step 29461, loss 0.544923.
Train: 2018-08-02T00:44:04.229580: step 29462, loss 0.649973.
Train: 2018-08-02T00:44:04.398164: step 29463, loss 0.649785.
Train: 2018-08-02T00:44:04.570696: step 29464, loss 0.492746.
Train: 2018-08-02T00:44:04.733263: step 29465, loss 0.562416.
Train: 2018-08-02T00:44:04.896827: step 29466, loss 0.51034.
Train: 2018-08-02T00:44:05.071360: step 29467, loss 0.458365.
Train: 2018-08-02T00:44:05.237917: step 29468, loss 0.631798.
Train: 2018-08-02T00:44:05.402475: step 29469, loss 0.562409.
Train: 2018-08-02T00:44:05.570993: step 29470, loss 0.579737.
Test: 2018-08-02T00:44:06.101601: step 29470, loss 0.547905.
Train: 2018-08-02T00:44:06.279101: step 29471, loss 0.527772.
Train: 2018-08-02T00:44:06.443676: step 29472, loss 0.614344.
Train: 2018-08-02T00:44:06.610241: step 29473, loss 0.475914.
Train: 2018-08-02T00:44:06.773809: step 29474, loss 0.510498.
Train: 2018-08-02T00:44:06.950307: step 29475, loss 0.614356.
Train: 2018-08-02T00:44:07.191234: step 29476, loss 0.614361.
Train: 2018-08-02T00:44:07.362775: step 29477, loss 0.631638.
Train: 2018-08-02T00:44:07.529329: step 29478, loss 0.63154.
Train: 2018-08-02T00:44:07.693902: step 29479, loss 0.545151.
Train: 2018-08-02T00:44:07.859478: step 29480, loss 0.614054.
Test: 2018-08-02T00:44:08.398013: step 29480, loss 0.548009.
Train: 2018-08-02T00:44:08.578524: step 29481, loss 0.545215.
Train: 2018-08-02T00:44:08.746103: step 29482, loss 0.61385.
Train: 2018-08-02T00:44:08.908674: step 29483, loss 0.57951.
Train: 2018-08-02T00:44:09.076229: step 29484, loss 0.528247.
Train: 2018-08-02T00:44:09.249761: step 29485, loss 0.647647.
Train: 2018-08-02T00:44:09.410327: step 29486, loss 0.715473.
Train: 2018-08-02T00:44:09.571901: step 29487, loss 0.579353.
Train: 2018-08-02T00:44:09.737461: step 29488, loss 0.545555.
Train: 2018-08-02T00:44:09.901021: step 29489, loss 0.596076.
Train: 2018-08-02T00:44:10.067577: step 29490, loss 0.512169.
Test: 2018-08-02T00:44:10.590148: step 29490, loss 0.548463.
Train: 2018-08-02T00:44:10.756733: step 29491, loss 0.595917.
Train: 2018-08-02T00:44:10.921296: step 29492, loss 0.595851.
Train: 2018-08-02T00:44:11.084860: step 29493, loss 0.612425.
Train: 2018-08-02T00:44:11.249410: step 29494, loss 0.496139.
Train: 2018-08-02T00:44:11.422947: step 29495, loss 0.529394.
Train: 2018-08-02T00:44:11.592469: step 29496, loss 0.52943.
Train: 2018-08-02T00:44:11.765006: step 29497, loss 0.529442.
Train: 2018-08-02T00:44:11.937571: step 29498, loss 0.633165.
Train: 2018-08-02T00:44:12.111082: step 29499, loss 0.628718.
Train: 2018-08-02T00:44:12.279639: step 29500, loss 0.463414.
Test: 2018-08-02T00:44:12.802233: step 29500, loss 0.54871.
Train: 2018-08-02T00:44:13.569317: step 29501, loss 0.512967.
Train: 2018-08-02T00:44:13.742824: step 29502, loss 0.645261.
Train: 2018-08-02T00:44:13.906387: step 29503, loss 0.628712.
Train: 2018-08-02T00:44:14.070974: step 29504, loss 0.562549.
Train: 2018-08-02T00:44:14.234510: step 29505, loss 0.579072.
Train: 2018-08-02T00:44:14.406052: step 29506, loss 0.513042.
Train: 2018-08-02T00:44:14.570638: step 29507, loss 0.480017.
Train: 2018-08-02T00:44:14.738189: step 29508, loss 0.595608.
Train: 2018-08-02T00:44:14.909705: step 29509, loss 0.562541.
Train: 2018-08-02T00:44:15.072302: step 29510, loss 0.430061.
Test: 2018-08-02T00:44:15.608836: step 29510, loss 0.548612.
Train: 2018-08-02T00:44:15.773427: step 29511, loss 0.512706.
Train: 2018-08-02T00:44:15.938954: step 29512, loss 0.562494.
Train: 2018-08-02T00:44:16.103547: step 29513, loss 0.579178.
Train: 2018-08-02T00:44:16.273060: step 29514, loss 0.629435.
Train: 2018-08-02T00:44:16.449621: step 29515, loss 0.495388.
Train: 2018-08-02T00:44:16.624122: step 29516, loss 0.512034.
Train: 2018-08-02T00:44:16.785691: step 29517, loss 0.511888.
Train: 2018-08-02T00:44:16.951248: step 29518, loss 0.579322.
Train: 2018-08-02T00:44:17.116833: step 29519, loss 0.579358.
Train: 2018-08-02T00:44:17.288347: step 29520, loss 0.613351.
Test: 2018-08-02T00:44:17.812953: step 29520, loss 0.548167.
Train: 2018-08-02T00:44:17.986480: step 29521, loss 0.579409.
Train: 2018-08-02T00:44:18.147051: step 29522, loss 0.511342.
Train: 2018-08-02T00:44:18.313606: step 29523, loss 0.511259.
Train: 2018-08-02T00:44:18.478191: step 29524, loss 0.613645.
Train: 2018-08-02T00:44:18.648710: step 29525, loss 0.493984.
Train: 2018-08-02T00:44:18.817286: step 29526, loss 0.493842.
Train: 2018-08-02T00:44:18.981820: step 29527, loss 0.59678.
Train: 2018-08-02T00:44:19.153385: step 29528, loss 0.493511.
Train: 2018-08-02T00:44:19.315957: step 29529, loss 0.527872.
Train: 2018-08-02T00:44:19.484476: step 29530, loss 0.492964.
Test: 2018-08-02T00:44:20.012082: step 29530, loss 0.547824.
Train: 2018-08-02T00:44:20.179617: step 29531, loss 0.545066.
Train: 2018-08-02T00:44:20.354176: step 29532, loss 0.544703.
Train: 2018-08-02T00:44:20.516716: step 29533, loss 0.615286.
Train: 2018-08-02T00:44:20.692247: step 29534, loss 0.563856.
Train: 2018-08-02T00:44:20.855836: step 29535, loss 0.596469.
Train: 2018-08-02T00:44:21.028348: step 29536, loss 0.544312.
Train: 2018-08-02T00:44:21.193906: step 29537, loss 0.525963.
Train: 2018-08-02T00:44:21.362457: step 29538, loss 0.565197.
Train: 2018-08-02T00:44:21.525045: step 29539, loss 0.509298.
Train: 2018-08-02T00:44:21.689612: step 29540, loss 0.455588.
Test: 2018-08-02T00:44:22.233129: step 29540, loss 0.547591.
Train: 2018-08-02T00:44:22.405667: step 29541, loss 0.527199.
Train: 2018-08-02T00:44:22.572246: step 29542, loss 0.692546.
Train: 2018-08-02T00:44:22.733815: step 29543, loss 0.509699.
Train: 2018-08-02T00:44:22.900345: step 29544, loss 0.526737.
Train: 2018-08-02T00:44:23.066931: step 29545, loss 0.509261.
Train: 2018-08-02T00:44:23.230499: step 29546, loss 0.597787.
Train: 2018-08-02T00:44:23.393063: step 29547, loss 0.563214.
Train: 2018-08-02T00:44:23.566564: step 29548, loss 0.544542.
Train: 2018-08-02T00:44:23.740100: step 29549, loss 0.455433.
Train: 2018-08-02T00:44:23.909674: step 29550, loss 0.437511.
Test: 2018-08-02T00:44:24.449235: step 29550, loss 0.547616.
Train: 2018-08-02T00:44:24.614793: step 29551, loss 0.580496.
Train: 2018-08-02T00:44:24.781316: step 29552, loss 0.670381.
Train: 2018-08-02T00:44:24.952858: step 29553, loss 0.490807.
Train: 2018-08-02T00:44:25.117443: step 29554, loss 0.598655.
Train: 2018-08-02T00:44:25.283973: step 29555, loss 0.472715.
Train: 2018-08-02T00:44:25.448558: step 29556, loss 0.562659.
Train: 2018-08-02T00:44:25.627056: step 29557, loss 0.580722.
Train: 2018-08-02T00:44:25.796633: step 29558, loss 0.5266.
Train: 2018-08-02T00:44:25.961188: step 29559, loss 0.562722.
Train: 2018-08-02T00:44:26.127744: step 29560, loss 0.580798.
Test: 2018-08-02T00:44:26.661292: step 29560, loss 0.547583.
Train: 2018-08-02T00:44:26.837818: step 29561, loss 0.544642.
Train: 2018-08-02T00:44:27.004405: step 29562, loss 0.526552.
Train: 2018-08-02T00:44:27.173954: step 29563, loss 0.472253.
Train: 2018-08-02T00:44:27.343491: step 29564, loss 0.653354.
Train: 2018-08-02T00:44:27.509024: step 29565, loss 0.508387.
Train: 2018-08-02T00:44:27.677604: step 29566, loss 0.544629.
Train: 2018-08-02T00:44:27.841137: step 29567, loss 0.490203.
Train: 2018-08-02T00:44:28.007717: step 29568, loss 0.580951.
Train: 2018-08-02T00:44:28.174271: step 29569, loss 0.562794.
Train: 2018-08-02T00:44:28.340807: step 29570, loss 0.599157.
Test: 2018-08-02T00:44:28.873395: step 29570, loss 0.547572.
Train: 2018-08-02T00:44:29.037938: step 29571, loss 0.508261.
Train: 2018-08-02T00:44:29.219483: step 29572, loss 0.471882.
Train: 2018-08-02T00:44:29.395980: step 29573, loss 0.471794.
Train: 2018-08-02T00:44:29.564560: step 29574, loss 0.508128.
Train: 2018-08-02T00:44:29.741061: step 29575, loss 0.508046.
Train: 2018-08-02T00:44:29.903649: step 29576, loss 0.471319.
Train: 2018-08-02T00:44:30.070178: step 29577, loss 0.618069.
Train: 2018-08-02T00:44:30.238751: step 29578, loss 0.599793.
Train: 2018-08-02T00:44:30.401318: step 29579, loss 0.544578.
Train: 2018-08-02T00:44:30.569867: step 29580, loss 0.581486.
Test: 2018-08-02T00:44:31.112391: step 29580, loss 0.547581.
Train: 2018-08-02T00:44:31.281939: step 29581, loss 0.563033.
Train: 2018-08-02T00:44:31.447496: step 29582, loss 0.729121.
Train: 2018-08-02T00:44:31.614050: step 29583, loss 0.655112.
Train: 2018-08-02T00:44:31.785639: step 29584, loss 0.636439.
Train: 2018-08-02T00:44:31.953170: step 29585, loss 0.471363.
Train: 2018-08-02T00:44:32.122690: step 29586, loss 0.581128.
Train: 2018-08-02T00:44:32.287265: step 29587, loss 0.526386.
Train: 2018-08-02T00:44:32.452809: step 29588, loss 0.508245.
Train: 2018-08-02T00:44:32.626344: step 29589, loss 0.526459.
Train: 2018-08-02T00:44:32.794894: step 29590, loss 0.562767.
Test: 2018-08-02T00:44:33.332457: step 29590, loss 0.547576.
Train: 2018-08-02T00:44:33.511977: step 29591, loss 0.508379.
Train: 2018-08-02T00:44:33.682551: step 29592, loss 0.598981.
Train: 2018-08-02T00:44:33.845086: step 29593, loss 0.544635.
Train: 2018-08-02T00:44:34.005658: step 29594, loss 0.689319.
Train: 2018-08-02T00:44:34.172247: step 29595, loss 0.490521.
Train: 2018-08-02T00:44:34.335805: step 29596, loss 0.598718.
Train: 2018-08-02T00:44:34.514297: step 29597, loss 0.616624.
Train: 2018-08-02T00:44:34.682847: step 29598, loss 0.544689.
Train: 2018-08-02T00:44:34.849414: step 29599, loss 0.616339.
Train: 2018-08-02T00:44:35.013987: step 29600, loss 0.616176.
Test: 2018-08-02T00:44:35.543553: step 29600, loss 0.547646.
Train: 2018-08-02T00:44:36.347626: step 29601, loss 0.615987.
Train: 2018-08-02T00:44:36.513185: step 29602, loss 0.580278.
Train: 2018-08-02T00:44:36.680736: step 29603, loss 0.615567.
Train: 2018-08-02T00:44:36.860256: step 29604, loss 0.650581.
Train: 2018-08-02T00:44:37.024843: step 29605, loss 0.422134.
Train: 2018-08-02T00:44:37.200348: step 29606, loss 0.527446.
Train: 2018-08-02T00:44:37.362938: step 29607, loss 0.492574.
Train: 2018-08-02T00:44:37.530512: step 29608, loss 0.667124.
Train: 2018-08-02T00:44:37.693063: step 29609, loss 0.632071.
Train: 2018-08-02T00:44:37.857622: step 29610, loss 0.54505.
Test: 2018-08-02T00:44:38.380194: step 29610, loss 0.547901.
Train: 2018-08-02T00:44:38.546782: step 29611, loss 0.614378.
Train: 2018-08-02T00:44:38.716320: step 29612, loss 0.683332.
Train: 2018-08-02T00:44:38.884844: step 29613, loss 0.66564.
Train: 2018-08-02T00:44:39.049429: step 29614, loss 0.545276.
Train: 2018-08-02T00:44:39.210006: step 29615, loss 0.579451.
Train: 2018-08-02T00:44:39.375534: step 29616, loss 0.596369.
Train: 2018-08-02T00:44:39.538097: step 29617, loss 0.494802.
Train: 2018-08-02T00:44:39.701661: step 29618, loss 0.562413.
Train: 2018-08-02T00:44:39.865233: step 29619, loss 0.612903.
Train: 2018-08-02T00:44:40.032777: step 29620, loss 0.562452.
Test: 2018-08-02T00:44:40.564354: step 29620, loss 0.548473.
Train: 2018-08-02T00:44:40.732905: step 29621, loss 0.595945.
Train: 2018-08-02T00:44:40.895469: step 29622, loss 0.595914.
Train: 2018-08-02T00:44:41.060054: step 29623, loss 0.6788.
Train: 2018-08-02T00:44:41.226616: step 29624, loss 0.645118.
Train: 2018-08-02T00:44:41.399134: step 29625, loss 0.693603.
Train: 2018-08-02T00:44:41.561688: step 29626, loss 0.436794.
Train: 2018-08-02T00:44:41.726274: step 29627, loss 0.513896.
Train: 2018-08-02T00:44:41.893801: step 29628, loss 0.644581.
Train: 2018-08-02T00:44:42.061379: step 29629, loss 0.611871.
Train: 2018-08-02T00:44:42.225939: step 29630, loss 0.561989.
Test: 2018-08-02T00:44:42.744527: step 29630, loss 0.549335.
Train: 2018-08-02T00:44:42.918065: step 29631, loss 0.595538.
Train: 2018-08-02T00:44:43.093624: step 29632, loss 0.643374.
Train: 2018-08-02T00:44:43.259176: step 29633, loss 0.546808.
Train: 2018-08-02T00:44:43.423710: step 29634, loss 0.531002.
Train: 2018-08-02T00:44:43.590297: step 29635, loss 0.498755.
Train: 2018-08-02T00:44:43.761806: step 29636, loss 0.627146.
Train: 2018-08-02T00:44:43.926392: step 29637, loss 0.466864.
Train: 2018-08-02T00:44:44.093920: step 29638, loss 0.498615.
Train: 2018-08-02T00:44:44.259477: step 29639, loss 0.514394.
Train: 2018-08-02T00:44:44.433045: step 29640, loss 0.594668.
Test: 2018-08-02T00:44:44.968589: step 29640, loss 0.549013.
Train: 2018-08-02T00:44:45.141121: step 29641, loss 0.530067.
Train: 2018-08-02T00:44:45.306678: step 29642, loss 0.647604.
Train: 2018-08-02T00:44:45.479220: step 29643, loss 0.6119.
Train: 2018-08-02T00:44:45.643776: step 29644, loss 0.529196.
Train: 2018-08-02T00:44:45.812326: step 29645, loss 0.546316.
Train: 2018-08-02T00:44:45.978912: step 29646, loss 0.513212.
Train: 2018-08-02T00:44:46.150447: step 29647, loss 0.596096.
Train: 2018-08-02T00:44:46.317974: step 29648, loss 0.67733.
Train: 2018-08-02T00:44:46.481537: step 29649, loss 0.644377.
Train: 2018-08-02T00:44:46.649115: step 29650, loss 0.611348.
Test: 2018-08-02T00:44:47.189670: step 29650, loss 0.549018.
Train: 2018-08-02T00:44:47.365203: step 29651, loss 0.546571.
Train: 2018-08-02T00:44:47.531730: step 29652, loss 0.660728.
Train: 2018-08-02T00:44:47.700309: step 29653, loss 0.692299.
Train: 2018-08-02T00:44:47.866833: step 29654, loss 0.643766.
Train: 2018-08-02T00:44:48.033421: step 29655, loss 0.48261.
Train: 2018-08-02T00:44:48.200940: step 29656, loss 0.482817.
Train: 2018-08-02T00:44:48.367495: step 29657, loss 0.466934.
Train: 2018-08-02T00:44:48.532055: step 29658, loss 0.67506.
Train: 2018-08-02T00:44:48.696646: step 29659, loss 0.610968.
Train: 2018-08-02T00:44:48.860209: step 29660, loss 0.658907.
Test: 2018-08-02T00:44:49.396775: step 29660, loss 0.549626.
Train: 2018-08-02T00:44:49.611196: step 29661, loss 0.499157.
Train: 2018-08-02T00:44:49.775730: step 29662, loss 0.53111.
Train: 2018-08-02T00:44:49.955277: step 29663, loss 0.642721.
Train: 2018-08-02T00:44:50.118832: step 29664, loss 0.642658.
Train: 2018-08-02T00:44:50.285369: step 29665, loss 0.706145.
Train: 2018-08-02T00:44:50.449960: step 29666, loss 0.578943.
Train: 2018-08-02T00:44:50.616510: step 29667, loss 0.594742.
Train: 2018-08-02T00:44:50.783069: step 29668, loss 0.6262.
Train: 2018-08-02T00:44:50.948619: step 29669, loss 0.516161.
Train: 2018-08-02T00:44:51.112185: step 29670, loss 0.64162.
Test: 2018-08-02T00:44:51.652712: step 29670, loss 0.550261.
Train: 2018-08-02T00:44:51.830269: step 29671, loss 0.657078.
Train: 2018-08-02T00:44:52.003800: step 29672, loss 0.625678.
Train: 2018-08-02T00:44:52.171357: step 29673, loss 0.594496.
Train: 2018-08-02T00:44:52.337919: step 29674, loss 0.532637.
Train: 2018-08-02T00:44:52.503438: step 29675, loss 0.594559.
Train: 2018-08-02T00:44:52.678000: step 29676, loss 0.579001.
Train: 2018-08-02T00:44:52.848516: step 29677, loss 0.609741.
Train: 2018-08-02T00:44:53.016068: step 29678, loss 0.548359.
Train: 2018-08-02T00:44:53.180628: step 29679, loss 0.517732.
Train: 2018-08-02T00:44:53.347209: step 29680, loss 0.548679.
Test: 2018-08-02T00:44:53.878778: step 29680, loss 0.550755.
Train: 2018-08-02T00:44:54.044319: step 29681, loss 0.578794.
Train: 2018-08-02T00:44:54.210902: step 29682, loss 0.486779.
Train: 2018-08-02T00:44:54.376444: step 29683, loss 0.641393.
Train: 2018-08-02T00:44:54.540991: step 29684, loss 0.533345.
Train: 2018-08-02T00:44:54.705551: step 29685, loss 0.624762.
Train: 2018-08-02T00:44:54.868143: step 29686, loss 0.561333.
Train: 2018-08-02T00:44:55.032677: step 29687, loss 0.530868.
Train: 2018-08-02T00:44:55.203221: step 29688, loss 0.564494.
Train: 2018-08-02T00:44:55.375791: step 29689, loss 0.461243.
Train: 2018-08-02T00:44:55.540321: step 29690, loss 0.588042.
Test: 2018-08-02T00:44:56.077884: step 29690, loss 0.548073.
Train: 2018-08-02T00:44:56.246472: step 29691, loss 0.418513.
Train: 2018-08-02T00:44:56.415980: step 29692, loss 0.561292.
Train: 2018-08-02T00:44:56.581537: step 29693, loss 0.598687.
Train: 2018-08-02T00:44:56.751084: step 29694, loss 0.686462.
Train: 2018-08-02T00:44:56.916641: step 29695, loss 0.572202.
Train: 2018-08-02T00:44:57.083227: step 29696, loss 0.524494.
Train: 2018-08-02T00:44:57.253765: step 29697, loss 0.45852.
Train: 2018-08-02T00:44:57.422290: step 29698, loss 0.683385.
Train: 2018-08-02T00:44:57.584855: step 29699, loss 0.556012.
Train: 2018-08-02T00:44:57.756421: step 29700, loss 0.581033.
Test: 2018-08-02T00:44:58.276007: step 29700, loss 0.550259.
Train: 2018-08-02T00:44:59.048796: step 29701, loss 0.625612.
Train: 2018-08-02T00:44:59.217346: step 29702, loss 0.5164.
Train: 2018-08-02T00:44:59.387890: step 29703, loss 0.547588.
Train: 2018-08-02T00:44:59.556440: step 29704, loss 0.532883.
Train: 2018-08-02T00:44:59.727982: step 29705, loss 0.643549.
Train: 2018-08-02T00:44:59.896550: step 29706, loss 0.56273.
Train: 2018-08-02T00:45:00.080040: step 29707, loss 0.562667.
Train: 2018-08-02T00:45:00.270532: step 29708, loss 0.45078.
Train: 2018-08-02T00:45:00.445090: step 29709, loss 0.549361.
Train: 2018-08-02T00:45:00.609624: step 29710, loss 0.510515.
Test: 2018-08-02T00:45:01.136217: step 29710, loss 0.548156.
Train: 2018-08-02T00:45:01.299779: step 29711, loss 0.626502.
Train: 2018-08-02T00:45:01.466334: step 29712, loss 0.55006.
Train: 2018-08-02T00:45:01.628900: step 29713, loss 0.549616.
Train: 2018-08-02T00:45:01.793491: step 29714, loss 0.589808.
Train: 2018-08-02T00:45:01.961042: step 29715, loss 0.56085.
Train: 2018-08-02T00:45:02.128563: step 29716, loss 0.45335.
Train: 2018-08-02T00:45:02.295144: step 29717, loss 0.456512.
Train: 2018-08-02T00:45:02.477631: step 29718, loss 0.539439.
Train: 2018-08-02T00:45:02.644186: step 29719, loss 0.480515.
Train: 2018-08-02T00:45:02.816751: step 29720, loss 0.721397.
Test: 2018-08-02T00:45:03.336366: step 29720, loss 0.548302.
Train: 2018-08-02T00:45:03.505882: step 29721, loss 0.61703.
Train: 2018-08-02T00:45:03.678458: step 29722, loss 0.56298.
Train: 2018-08-02T00:45:03.843016: step 29723, loss 0.612129.
Train: 2018-08-02T00:45:04.011556: step 29724, loss 0.600419.
Train: 2018-08-02T00:45:04.173098: step 29725, loss 0.628585.
Train: 2018-08-02T00:45:04.346635: step 29726, loss 0.646237.
Train: 2018-08-02T00:45:04.516195: step 29727, loss 0.499093.
Train: 2018-08-02T00:45:04.680762: step 29728, loss 0.515667.
Train: 2018-08-02T00:45:04.846298: step 29729, loss 0.563644.
Train: 2018-08-02T00:45:05.019872: step 29730, loss 0.64331.
Test: 2018-08-02T00:45:05.560389: step 29730, loss 0.550545.
Train: 2018-08-02T00:45:05.730960: step 29731, loss 0.627304.
Train: 2018-08-02T00:45:05.895525: step 29732, loss 0.57955.
Train: 2018-08-02T00:45:06.064096: step 29733, loss 0.532449.
Train: 2018-08-02T00:45:06.231596: step 29734, loss 0.608907.
Train: 2018-08-02T00:45:06.402165: step 29735, loss 0.565174.
Train: 2018-08-02T00:45:06.573705: step 29736, loss 0.668738.
Train: 2018-08-02T00:45:06.736245: step 29737, loss 0.562466.
Train: 2018-08-02T00:45:06.903798: step 29738, loss 0.534206.
Train: 2018-08-02T00:45:07.065397: step 29739, loss 0.470357.
Train: 2018-08-02T00:45:07.237936: step 29740, loss 0.536653.
Test: 2018-08-02T00:45:07.775495: step 29740, loss 0.550837.
Train: 2018-08-02T00:45:07.944045: step 29741, loss 0.596726.
Train: 2018-08-02T00:45:08.108591: step 29742, loss 0.499229.
Train: 2018-08-02T00:45:08.275133: step 29743, loss 0.532711.
Train: 2018-08-02T00:45:08.446675: step 29744, loss 0.53171.
Train: 2018-08-02T00:45:08.612257: step 29745, loss 0.515297.
Train: 2018-08-02T00:45:08.784802: step 29746, loss 0.578361.
Train: 2018-08-02T00:45:08.949362: step 29747, loss 0.582256.
Train: 2018-08-02T00:45:09.112892: step 29748, loss 0.561548.
Train: 2018-08-02T00:45:09.286454: step 29749, loss 0.479338.
Train: 2018-08-02T00:45:09.447997: step 29750, loss 0.542011.
Test: 2018-08-02T00:45:09.985561: step 29750, loss 0.548482.
Train: 2018-08-02T00:45:10.152116: step 29751, loss 0.60124.
Train: 2018-08-02T00:45:10.318669: step 29752, loss 0.512632.
Train: 2018-08-02T00:45:10.483230: step 29753, loss 0.588457.
Train: 2018-08-02T00:45:10.652777: step 29754, loss 0.655099.
Train: 2018-08-02T00:45:10.825315: step 29755, loss 0.661938.
Train: 2018-08-02T00:45:10.990872: step 29756, loss 0.497651.
Train: 2018-08-02T00:45:11.159422: step 29757, loss 0.470196.
Train: 2018-08-02T00:45:11.327972: step 29758, loss 0.609186.
Train: 2018-08-02T00:45:11.504531: step 29759, loss 0.495754.
Train: 2018-08-02T00:45:11.673070: step 29760, loss 0.489477.
Test: 2018-08-02T00:45:12.199668: step 29760, loss 0.548358.
Train: 2018-08-02T00:45:12.369219: step 29761, loss 0.592821.
Train: 2018-08-02T00:45:12.544719: step 29762, loss 0.584879.
Train: 2018-08-02T00:45:12.711300: step 29763, loss 0.50927.
Train: 2018-08-02T00:45:12.879823: step 29764, loss 0.524372.
Train: 2018-08-02T00:45:13.044384: step 29765, loss 0.585405.
Train: 2018-08-02T00:45:13.208947: step 29766, loss 0.48857.
Train: 2018-08-02T00:45:13.370542: step 29767, loss 0.562863.
Train: 2018-08-02T00:45:13.546071: step 29768, loss 0.49868.
Train: 2018-08-02T00:45:13.712597: step 29769, loss 0.49452.
Train: 2018-08-02T00:45:13.879153: step 29770, loss 0.576469.
Test: 2018-08-02T00:45:14.425690: step 29770, loss 0.548778.
Train: 2018-08-02T00:45:14.598229: step 29771, loss 0.507271.
Train: 2018-08-02T00:45:14.773760: step 29772, loss 0.528589.
Train: 2018-08-02T00:45:14.937323: step 29773, loss 0.581487.
Train: 2018-08-02T00:45:15.100913: step 29774, loss 0.586282.
Train: 2018-08-02T00:45:15.263482: step 29775, loss 0.468713.
Train: 2018-08-02T00:45:15.438982: step 29776, loss 0.538401.
Train: 2018-08-02T00:45:15.610523: step 29777, loss 0.566803.
Train: 2018-08-02T00:45:15.793036: step 29778, loss 0.524046.
Train: 2018-08-02T00:45:15.974551: step 29779, loss 0.631749.
Train: 2018-08-02T00:45:16.141105: step 29780, loss 0.588528.
Test: 2018-08-02T00:45:16.663707: step 29780, loss 0.548444.
Train: 2018-08-02T00:45:16.828269: step 29781, loss 0.61897.
Train: 2018-08-02T00:45:16.996843: step 29782, loss 0.608853.
Train: 2018-08-02T00:45:17.163372: step 29783, loss 0.604558.
Train: 2018-08-02T00:45:17.329954: step 29784, loss 0.471741.
Train: 2018-08-02T00:45:17.496506: step 29785, loss 0.525481.
Train: 2018-08-02T00:45:17.659080: step 29786, loss 0.531365.
Train: 2018-08-02T00:45:17.825602: step 29787, loss 0.513323.
Train: 2018-08-02T00:45:17.994184: step 29788, loss 0.411363.
Train: 2018-08-02T00:45:18.158743: step 29789, loss 0.481924.
Train: 2018-08-02T00:45:18.332248: step 29790, loss 0.527415.
Test: 2018-08-02T00:45:18.865820: step 29790, loss 0.549014.
Train: 2018-08-02T00:45:19.040379: step 29791, loss 0.580596.
Train: 2018-08-02T00:45:19.208929: step 29792, loss 0.60044.
Train: 2018-08-02T00:45:19.369475: step 29793, loss 0.614691.
Train: 2018-08-02T00:45:19.532060: step 29794, loss 0.564328.
Train: 2018-08-02T00:45:19.698595: step 29795, loss 0.494853.
Train: 2018-08-02T00:45:19.870137: step 29796, loss 0.479875.
Train: 2018-08-02T00:45:20.041709: step 29797, loss 0.647972.
Train: 2018-08-02T00:45:20.205241: step 29798, loss 0.57961.
Train: 2018-08-02T00:45:20.374788: step 29799, loss 0.508629.
Train: 2018-08-02T00:45:20.542339: step 29800, loss 0.597063.
Test: 2018-08-02T00:45:21.068958: step 29800, loss 0.548898.
Train: 2018-08-02T00:45:21.831169: step 29801, loss 0.646976.
Train: 2018-08-02T00:45:21.999720: step 29802, loss 0.528792.
Train: 2018-08-02T00:45:22.176247: step 29803, loss 0.545779.
Train: 2018-08-02T00:45:22.342832: step 29804, loss 0.581446.
Train: 2018-08-02T00:45:22.504394: step 29805, loss 0.460536.
Train: 2018-08-02T00:45:22.668930: step 29806, loss 0.649131.
Train: 2018-08-02T00:45:22.828528: step 29807, loss 0.529317.
Train: 2018-08-02T00:45:22.995089: step 29808, loss 0.562723.
Train: 2018-08-02T00:45:23.163635: step 29809, loss 0.512543.
Train: 2018-08-02T00:45:23.327201: step 29810, loss 0.63068.
Test: 2018-08-02T00:45:23.848775: step 29810, loss 0.548789.
Train: 2018-08-02T00:45:24.011367: step 29811, loss 0.631865.
Train: 2018-08-02T00:45:24.178919: step 29812, loss 0.57956.
Train: 2018-08-02T00:45:24.340487: step 29813, loss 0.512877.
Train: 2018-08-02T00:45:24.512003: step 29814, loss 0.632185.
Train: 2018-08-02T00:45:24.679580: step 29815, loss 0.580079.
Train: 2018-08-02T00:45:24.847107: step 29816, loss 0.546661.
Train: 2018-08-02T00:45:25.015687: step 29817, loss 0.564943.
Train: 2018-08-02T00:45:25.181214: step 29818, loss 0.495287.
Train: 2018-08-02T00:45:25.349764: step 29819, loss 0.646729.
Train: 2018-08-02T00:45:25.511331: step 29820, loss 0.563411.
Test: 2018-08-02T00:45:26.039918: step 29820, loss 0.548914.
Train: 2018-08-02T00:45:26.207502: step 29821, loss 0.562978.
Train: 2018-08-02T00:45:26.368041: step 29822, loss 0.547245.
Train: 2018-08-02T00:45:26.543571: step 29823, loss 0.563191.
Train: 2018-08-02T00:45:26.707165: step 29824, loss 0.597512.
Train: 2018-08-02T00:45:26.873720: step 29825, loss 0.63049.
Train: 2018-08-02T00:45:27.039290: step 29826, loss 0.530971.
Train: 2018-08-02T00:45:27.201812: step 29827, loss 0.412985.
Train: 2018-08-02T00:45:27.365408: step 29828, loss 0.579136.
Train: 2018-08-02T00:45:27.542900: step 29829, loss 0.597453.
Train: 2018-08-02T00:45:27.719428: step 29830, loss 0.495536.
Test: 2018-08-02T00:45:28.245023: step 29830, loss 0.548741.
Train: 2018-08-02T00:45:28.407615: step 29831, loss 0.613307.
Train: 2018-08-02T00:45:28.573147: step 29832, loss 0.596936.
Train: 2018-08-02T00:45:28.734715: step 29833, loss 0.546236.
Train: 2018-08-02T00:45:28.902298: step 29834, loss 0.630953.
Train: 2018-08-02T00:45:29.070816: step 29835, loss 0.528872.
Train: 2018-08-02T00:45:29.238368: step 29836, loss 0.597413.
Train: 2018-08-02T00:45:29.398970: step 29837, loss 0.546383.
Train: 2018-08-02T00:45:29.573472: step 29838, loss 0.648714.
Train: 2018-08-02T00:45:29.743044: step 29839, loss 0.614252.
Train: 2018-08-02T00:45:29.906582: step 29840, loss 0.664554.
Test: 2018-08-02T00:45:30.436166: step 29840, loss 0.548788.
Train: 2018-08-02T00:45:30.604742: step 29841, loss 0.64801.
Train: 2018-08-02T00:45:30.770302: step 29842, loss 0.546358.
Train: 2018-08-02T00:45:30.930843: step 29843, loss 0.64724.
Train: 2018-08-02T00:45:31.094445: step 29844, loss 0.479317.
Train: 2018-08-02T00:45:31.259990: step 29845, loss 0.528993.
Train: 2018-08-02T00:45:31.425522: step 29846, loss 0.563029.
Train: 2018-08-02T00:45:31.589108: step 29847, loss 0.446262.
Train: 2018-08-02T00:45:31.754666: step 29848, loss 0.59701.
Train: 2018-08-02T00:45:31.918235: step 29849, loss 0.529612.
Train: 2018-08-02T00:45:32.088776: step 29850, loss 0.512801.
Test: 2018-08-02T00:45:32.614343: step 29850, loss 0.548977.
Train: 2018-08-02T00:45:32.775942: step 29851, loss 0.63012.
Train: 2018-08-02T00:45:32.938505: step 29852, loss 0.580018.
Train: 2018-08-02T00:45:33.103063: step 29853, loss 0.529368.
Train: 2018-08-02T00:45:33.273581: step 29854, loss 0.529491.
Train: 2018-08-02T00:45:33.450132: step 29855, loss 0.529557.
Train: 2018-08-02T00:45:33.611709: step 29856, loss 0.495497.
Train: 2018-08-02T00:45:33.777235: step 29857, loss 0.545875.
Train: 2018-08-02T00:45:33.948776: step 29858, loss 0.495508.
Train: 2018-08-02T00:45:34.114364: step 29859, loss 0.59652.
Train: 2018-08-02T00:45:34.283880: step 29860, loss 0.478325.
Test: 2018-08-02T00:45:34.811471: step 29860, loss 0.548664.
Train: 2018-08-02T00:45:34.978024: step 29861, loss 0.580184.
Train: 2018-08-02T00:45:35.152557: step 29862, loss 0.494606.
Train: 2018-08-02T00:45:35.322136: step 29863, loss 0.49446.
Train: 2018-08-02T00:45:35.491651: step 29864, loss 0.528612.
Train: 2018-08-02T00:45:35.656217: step 29865, loss 0.459297.
Train: 2018-08-02T00:45:35.820772: step 29866, loss 0.597604.
Train: 2018-08-02T00:45:35.981387: step 29867, loss 0.632439.
Train: 2018-08-02T00:45:36.144934: step 29868, loss 0.685622.
Train: 2018-08-02T00:45:36.315450: step 29869, loss 0.562874.
Train: 2018-08-02T00:45:36.479043: step 29870, loss 0.54541.
Test: 2018-08-02T00:45:37.011589: step 29870, loss 0.548315.
Train: 2018-08-02T00:45:37.179167: step 29871, loss 0.580555.
Train: 2018-08-02T00:45:37.344698: step 29872, loss 0.475569.
Train: 2018-08-02T00:45:37.511254: step 29873, loss 0.668013.
Train: 2018-08-02T00:45:37.678829: step 29874, loss 0.510434.
Train: 2018-08-02T00:45:37.841403: step 29875, loss 0.475129.
Train: 2018-08-02T00:45:38.005956: step 29876, loss 0.580564.
Train: 2018-08-02T00:45:38.178468: step 29877, loss 0.562815.
Train: 2018-08-02T00:45:38.343028: step 29878, loss 0.528036.
Train: 2018-08-02T00:45:38.503599: step 29879, loss 0.563299.
Train: 2018-08-02T00:45:38.670180: step 29880, loss 0.704483.
Test: 2018-08-02T00:45:39.206750: step 29880, loss 0.548232.
Train: 2018-08-02T00:45:39.378286: step 29881, loss 0.668649.
Train: 2018-08-02T00:45:39.540839: step 29882, loss 0.58048.
Train: 2018-08-02T00:45:39.704423: step 29883, loss 0.492748.
Train: 2018-08-02T00:45:39.868976: step 29884, loss 0.633159.
Train: 2018-08-02T00:45:40.033544: step 29885, loss 0.527816.
Train: 2018-08-02T00:45:40.198098: step 29886, loss 0.493202.
Train: 2018-08-02T00:45:40.364624: step 29887, loss 0.580602.
Train: 2018-08-02T00:45:40.526219: step 29888, loss 0.615223.
Train: 2018-08-02T00:45:40.696738: step 29889, loss 0.510786.
Train: 2018-08-02T00:45:40.864298: step 29890, loss 0.580261.
Test: 2018-08-02T00:45:41.399870: step 29890, loss 0.548333.
Train: 2018-08-02T00:45:41.564442: step 29891, loss 0.528187.
Train: 2018-08-02T00:45:41.733964: step 29892, loss 0.528137.
Train: 2018-08-02T00:45:41.895564: step 29893, loss 0.562972.
Train: 2018-08-02T00:45:42.062114: step 29894, loss 0.63252.
Train: 2018-08-02T00:45:42.226660: step 29895, loss 0.597617.
Train: 2018-08-02T00:45:42.395196: step 29896, loss 0.528781.
Train: 2018-08-02T00:45:42.573719: step 29897, loss 0.511148.
Train: 2018-08-02T00:45:42.738308: step 29898, loss 0.441799.
Train: 2018-08-02T00:45:42.902868: step 29899, loss 0.510983.
Train: 2018-08-02T00:45:43.074381: step 29900, loss 0.510818.
Test: 2018-08-02T00:45:43.606988: step 29900, loss 0.548306.
Train: 2018-08-02T00:45:44.413217: step 29901, loss 0.562839.
Train: 2018-08-02T00:45:44.585780: step 29902, loss 0.5104.
Train: 2018-08-02T00:45:44.755302: step 29903, loss 0.647134.
Train: 2018-08-02T00:45:44.920890: step 29904, loss 0.612062.
Train: 2018-08-02T00:45:45.081430: step 29905, loss 0.531767.
Train: 2018-08-02T00:45:45.257985: step 29906, loss 0.536324.
Train: 2018-08-02T00:45:45.421552: step 29907, loss 0.563541.
Train: 2018-08-02T00:45:45.602038: step 29908, loss 0.509816.
Train: 2018-08-02T00:45:45.766598: step 29909, loss 0.581957.
Train: 2018-08-02T00:45:45.930162: step 29910, loss 0.579787.
Test: 2018-08-02T00:45:46.466728: step 29910, loss 0.5485.
Train: 2018-08-02T00:45:46.630289: step 29911, loss 0.579879.
Train: 2018-08-02T00:45:46.796845: step 29912, loss 0.598159.
Train: 2018-08-02T00:45:46.963400: step 29913, loss 0.647923.
Train: 2018-08-02T00:45:47.139958: step 29914, loss 0.547974.
Train: 2018-08-02T00:45:47.304518: step 29915, loss 0.560619.
Train: 2018-08-02T00:45:47.472064: step 29916, loss 0.544866.
Train: 2018-08-02T00:45:47.645575: step 29917, loss 0.581808.
Train: 2018-08-02T00:45:47.815153: step 29918, loss 0.630612.
Train: 2018-08-02T00:45:47.986696: step 29919, loss 0.498335.
Train: 2018-08-02T00:45:48.152246: step 29920, loss 0.562829.
Test: 2018-08-02T00:45:48.679810: step 29920, loss 0.548446.
Train: 2018-08-02T00:45:48.843387: step 29921, loss 0.4597.
Train: 2018-08-02T00:45:49.007965: step 29922, loss 0.63221.
Train: 2018-08-02T00:45:49.172495: step 29923, loss 0.476923.
Train: 2018-08-02T00:45:49.335091: step 29924, loss 0.615276.
Train: 2018-08-02T00:45:49.500648: step 29925, loss 0.564077.
Train: 2018-08-02T00:45:49.666205: step 29926, loss 0.6332.
Train: 2018-08-02T00:45:49.830735: step 29927, loss 0.475491.
Train: 2018-08-02T00:45:49.993299: step 29928, loss 0.580149.
Train: 2018-08-02T00:45:50.155864: step 29929, loss 0.597858.
Train: 2018-08-02T00:45:50.318429: step 29930, loss 0.457424.
Test: 2018-08-02T00:45:50.838040: step 29930, loss 0.548134.
Train: 2018-08-02T00:45:51.001605: step 29931, loss 0.563543.
Train: 2018-08-02T00:45:51.176162: step 29932, loss 0.510308.
Train: 2018-08-02T00:45:51.347709: step 29933, loss 0.492313.
Train: 2018-08-02T00:45:51.515261: step 29934, loss 0.492279.
Train: 2018-08-02T00:45:51.677796: step 29935, loss 0.562701.
Train: 2018-08-02T00:45:51.840362: step 29936, loss 0.474144.
Train: 2018-08-02T00:45:52.001929: step 29937, loss 0.598369.
Train: 2018-08-02T00:45:52.169507: step 29938, loss 0.562918.
Train: 2018-08-02T00:45:52.339029: step 29939, loss 0.509479.
Train: 2018-08-02T00:45:52.502623: step 29940, loss 0.419969.
Test: 2018-08-02T00:45:53.030212: step 29940, loss 0.547985.
Train: 2018-08-02T00:45:53.192746: step 29941, loss 0.491281.
Train: 2018-08-02T00:45:53.361296: step 29942, loss 0.652729.
Train: 2018-08-02T00:45:53.526900: step 29943, loss 0.599227.
Train: 2018-08-02T00:45:53.690442: step 29944, loss 0.563296.
Train: 2018-08-02T00:45:53.861958: step 29945, loss 0.599301.
Train: 2018-08-02T00:45:54.022559: step 29946, loss 0.526962.
Train: 2018-08-02T00:45:54.193089: step 29947, loss 0.617513.
Train: 2018-08-02T00:45:54.355638: step 29948, loss 0.581224.
Train: 2018-08-02T00:45:54.515210: step 29949, loss 0.562887.
Train: 2018-08-02T00:45:54.680768: step 29950, loss 0.56307.
Test: 2018-08-02T00:45:55.205367: step 29950, loss 0.54793.
Train: 2018-08-02T00:45:55.371920: step 29951, loss 0.599178.
Train: 2018-08-02T00:45:55.532517: step 29952, loss 0.599051.
Train: 2018-08-02T00:45:55.706030: step 29953, loss 0.526797.
Train: 2018-08-02T00:45:55.876603: step 29954, loss 0.545086.
Train: 2018-08-02T00:45:56.039138: step 29955, loss 0.436954.
Train: 2018-08-02T00:45:56.202700: step 29956, loss 0.616501.
Train: 2018-08-02T00:45:56.370283: step 29957, loss 0.616824.
Train: 2018-08-02T00:45:56.533846: step 29958, loss 0.562964.
Train: 2018-08-02T00:45:56.699373: step 29959, loss 0.598689.
Train: 2018-08-02T00:45:56.865927: step 29960, loss 0.580924.
Test: 2018-08-02T00:45:57.388530: step 29960, loss 0.547955.
Train: 2018-08-02T00:45:57.555117: step 29961, loss 0.634082.
Train: 2018-08-02T00:45:57.718678: step 29962, loss 0.599389.
Train: 2018-08-02T00:45:57.882212: step 29963, loss 0.580134.
Train: 2018-08-02T00:45:58.046770: step 29964, loss 0.509108.
Train: 2018-08-02T00:45:58.210387: step 29965, loss 0.651314.
Train: 2018-08-02T00:45:58.382904: step 29966, loss 0.509793.
Train: 2018-08-02T00:45:58.546459: step 29967, loss 0.650932.
Train: 2018-08-02T00:45:58.709997: step 29968, loss 0.492111.
Train: 2018-08-02T00:45:58.873560: step 29969, loss 0.579893.
Train: 2018-08-02T00:45:59.038151: step 29970, loss 0.579355.
Test: 2018-08-02T00:45:59.566753: step 29970, loss 0.548244.
Train: 2018-08-02T00:45:59.738279: step 29971, loss 0.68345.
Train: 2018-08-02T00:45:59.903839: step 29972, loss 0.665398.
Train: 2018-08-02T00:46:00.067370: step 29973, loss 0.561526.
Train: 2018-08-02T00:46:00.231928: step 29974, loss 0.546558.
Train: 2018-08-02T00:46:00.397486: step 29975, loss 0.545785.
Train: 2018-08-02T00:46:00.565069: step 29976, loss 0.594688.
Train: 2018-08-02T00:46:00.737590: step 29977, loss 0.544149.
Train: 2018-08-02T00:46:00.905129: step 29978, loss 0.680766.
Train: 2018-08-02T00:46:01.074676: step 29979, loss 0.593787.
Train: 2018-08-02T00:46:01.237241: step 29980, loss 0.627875.
Test: 2018-08-02T00:46:01.776800: step 29980, loss 0.549773.
Train: 2018-08-02T00:46:01.940363: step 29981, loss 0.563516.
Train: 2018-08-02T00:46:02.107935: step 29982, loss 0.532444.
Train: 2018-08-02T00:46:02.274468: step 29983, loss 0.533981.
Train: 2018-08-02T00:46:02.436064: step 29984, loss 0.595331.
Train: 2018-08-02T00:46:02.598634: step 29985, loss 0.529896.
Train: 2018-08-02T00:46:02.774166: step 29986, loss 0.567335.
Train: 2018-08-02T00:46:02.944677: step 29987, loss 0.547322.
Train: 2018-08-02T00:46:03.109237: step 29988, loss 0.597729.
Train: 2018-08-02T00:46:03.273834: step 29989, loss 0.579435.
Train: 2018-08-02T00:46:03.438383: step 29990, loss 0.481902.
Test: 2018-08-02T00:46:03.972929: step 29990, loss 0.548846.
Train: 2018-08-02T00:46:04.139535: step 29991, loss 0.562963.
Train: 2018-08-02T00:46:04.304044: step 29992, loss 0.66005.
Train: 2018-08-02T00:46:04.473624: step 29993, loss 0.71239.
Train: 2018-08-02T00:46:04.638150: step 29994, loss 0.531299.
Train: 2018-08-02T00:46:04.803707: step 29995, loss 0.464278.
Train: 2018-08-02T00:46:04.966304: step 29996, loss 0.612447.
Train: 2018-08-02T00:46:05.141834: step 29997, loss 0.612825.
Train: 2018-08-02T00:46:05.319328: step 29998, loss 0.530257.
Train: 2018-08-02T00:46:05.482923: step 29999, loss 0.529899.
Train: 2018-08-02T00:46:05.653467: step 30000, loss 0.578312.
Test: 2018-08-02T00:46:06.190998: step 30000, loss 0.548744.
Train: 2018-08-02T00:46:06.969175: step 30001, loss 0.478734.
Train: 2018-08-02T00:46:07.168856: step 30002, loss 0.629549.
Train: 2018-08-02T00:46:07.330425: step 30003, loss 0.462445.
Train: 2018-08-02T00:46:07.507950: step 30004, loss 0.713863.
Train: 2018-08-02T00:46:07.670547: step 30005, loss 0.479222.
Train: 2018-08-02T00:46:07.837070: step 30006, loss 0.444464.
Train: 2018-08-02T00:46:08.000659: step 30007, loss 0.546475.
Train: 2018-08-02T00:46:08.167187: step 30008, loss 0.564236.
Train: 2018-08-02T00:46:08.325764: step 30009, loss 0.57975.
Train: 2018-08-02T00:46:08.490349: step 30010, loss 0.6637.
Test: 2018-08-02T00:46:09.025891: step 30010, loss 0.548469.
Train: 2018-08-02T00:46:09.191474: step 30011, loss 0.529246.
Train: 2018-08-02T00:46:09.355012: step 30012, loss 0.512088.
Train: 2018-08-02T00:46:09.517584: step 30013, loss 0.630798.
Train: 2018-08-02T00:46:09.690148: step 30014, loss 0.57915.
Train: 2018-08-02T00:46:09.864681: step 30015, loss 0.511025.
Train: 2018-08-02T00:46:10.031204: step 30016, loss 0.562617.
Train: 2018-08-02T00:46:10.196761: step 30017, loss 0.528478.
Train: 2018-08-02T00:46:10.365311: step 30018, loss 0.461481.
Train: 2018-08-02T00:46:10.531889: step 30019, loss 0.529866.
Train: 2018-08-02T00:46:10.699419: step 30020, loss 0.580158.
Test: 2018-08-02T00:46:11.225015: step 30020, loss 0.548336.
Train: 2018-08-02T00:46:11.399547: step 30021, loss 0.544553.
Train: 2018-08-02T00:46:11.573083: step 30022, loss 0.527876.
Train: 2018-08-02T00:46:11.742629: step 30023, loss 0.476855.
Train: 2018-08-02T00:46:11.905194: step 30024, loss 0.493467.
Train: 2018-08-02T00:46:12.066762: step 30025, loss 0.493557.
Train: 2018-08-02T00:46:12.229329: step 30026, loss 0.562012.
Train: 2018-08-02T00:46:12.388933: step 30027, loss 0.562948.
Train: 2018-08-02T00:46:12.557450: step 30028, loss 0.474775.
Train: 2018-08-02T00:46:12.722011: step 30029, loss 0.615746.
Train: 2018-08-02T00:46:12.895548: step 30030, loss 0.475072.
Test: 2018-08-02T00:46:13.425132: step 30030, loss 0.547897.
Train: 2018-08-02T00:46:13.590689: step 30031, loss 0.49088.
Train: 2018-08-02T00:46:13.753255: step 30032, loss 0.597952.
Train: 2018-08-02T00:46:13.917814: step 30033, loss 0.674453.
Train: 2018-08-02T00:46:14.083371: step 30034, loss 0.436606.
Train: 2018-08-02T00:46:14.263920: step 30035, loss 0.48527.
Train: 2018-08-02T00:46:14.435431: step 30036, loss 0.641724.
Train: 2018-08-02T00:46:14.601987: step 30037, loss 0.585364.
Train: 2018-08-02T00:46:14.771533: step 30038, loss 0.583388.
Train: 2018-08-02T00:46:14.936093: step 30039, loss 0.491762.
Train: 2018-08-02T00:46:15.111624: step 30040, loss 0.63669.
Test: 2018-08-02T00:46:15.652177: step 30040, loss 0.547846.
Train: 2018-08-02T00:46:15.832695: step 30041, loss 0.600082.
Train: 2018-08-02T00:46:15.995287: step 30042, loss 0.617101.
Train: 2018-08-02T00:46:16.159820: step 30043, loss 0.669753.
Train: 2018-08-02T00:46:16.327398: step 30044, loss 0.615557.
Train: 2018-08-02T00:46:16.496921: step 30045, loss 0.580063.
Train: 2018-08-02T00:46:16.660483: step 30046, loss 0.545854.
Train: 2018-08-02T00:46:16.825074: step 30047, loss 0.492685.
Train: 2018-08-02T00:46:16.992621: step 30048, loss 0.527837.
Train: 2018-08-02T00:46:17.158203: step 30049, loss 0.544972.
Train: 2018-08-02T00:46:17.324707: step 30050, loss 0.491581.
Test: 2018-08-02T00:46:17.867256: step 30050, loss 0.547994.
Train: 2018-08-02T00:46:18.037827: step 30051, loss 0.527939.
Train: 2018-08-02T00:46:18.204386: step 30052, loss 0.578995.
Train: 2018-08-02T00:46:18.380884: step 30053, loss 0.564112.
Train: 2018-08-02T00:46:18.548435: step 30054, loss 0.616318.
Train: 2018-08-02T00:46:18.719004: step 30055, loss 0.614421.
Train: 2018-08-02T00:46:18.891519: step 30056, loss 0.495571.
Train: 2018-08-02T00:46:19.058104: step 30057, loss 0.597132.
Train: 2018-08-02T00:46:19.231609: step 30058, loss 0.509275.
Train: 2018-08-02T00:46:19.393178: step 30059, loss 0.668409.
Train: 2018-08-02T00:46:19.559764: step 30060, loss 0.545285.
Test: 2018-08-02T00:46:20.088350: step 30060, loss 0.548085.
Train: 2018-08-02T00:46:20.250884: step 30061, loss 0.441001.
Train: 2018-08-02T00:46:20.424451: step 30062, loss 0.595825.
Train: 2018-08-02T00:46:20.584991: step 30063, loss 0.583165.
Train: 2018-08-02T00:46:20.749552: step 30064, loss 0.527355.
Train: 2018-08-02T00:46:20.919097: step 30065, loss 0.57963.
Train: 2018-08-02T00:46:21.095627: step 30066, loss 0.58192.
Train: 2018-08-02T00:46:21.259190: step 30067, loss 0.424936.
Train: 2018-08-02T00:46:21.433749: step 30068, loss 0.52972.
Train: 2018-08-02T00:46:21.598283: step 30069, loss 0.493261.
Train: 2018-08-02T00:46:21.769823: step 30070, loss 0.580736.
Test: 2018-08-02T00:46:22.301428: step 30070, loss 0.548029.
Train: 2018-08-02T00:46:22.465963: step 30071, loss 0.580401.
Train: 2018-08-02T00:46:22.630537: step 30072, loss 0.456753.
Train: 2018-08-02T00:46:22.795084: step 30073, loss 0.509865.
Train: 2018-08-02T00:46:22.961639: step 30074, loss 0.616121.
Train: 2018-08-02T00:46:23.133180: step 30075, loss 0.634125.
Train: 2018-08-02T00:46:23.296742: step 30076, loss 0.580731.
Train: 2018-08-02T00:46:23.462300: step 30077, loss 0.634105.
Train: 2018-08-02T00:46:23.625889: step 30078, loss 0.56295.
Train: 2018-08-02T00:46:23.801424: step 30079, loss 0.598401.
Train: 2018-08-02T00:46:23.967949: step 30080, loss 0.65177.
Test: 2018-08-02T00:46:24.486564: step 30080, loss 0.547969.
Train: 2018-08-02T00:46:24.650151: step 30081, loss 0.421244.
Train: 2018-08-02T00:46:24.815683: step 30082, loss 0.615992.
Train: 2018-08-02T00:46:24.986252: step 30083, loss 0.527466.
Train: 2018-08-02T00:46:25.149788: step 30084, loss 0.598277.
Train: 2018-08-02T00:46:25.312385: step 30085, loss 0.474506.
Train: 2018-08-02T00:46:25.491874: step 30086, loss 0.562583.
Train: 2018-08-02T00:46:25.660434: step 30087, loss 0.633276.
Train: 2018-08-02T00:46:25.833990: step 30088, loss 0.386302.
Train: 2018-08-02T00:46:26.005501: step 30089, loss 0.509689.
Train: 2018-08-02T00:46:26.172055: step 30090, loss 0.633481.
Test: 2018-08-02T00:46:26.708621: step 30090, loss 0.547953.
Train: 2018-08-02T00:46:26.874210: step 30091, loss 0.580462.
Train: 2018-08-02T00:46:27.038770: step 30092, loss 0.598019.
Train: 2018-08-02T00:46:27.202302: step 30093, loss 0.56283.
Train: 2018-08-02T00:46:27.368882: step 30094, loss 0.598148.
Train: 2018-08-02T00:46:27.538404: step 30095, loss 0.633517.
Train: 2018-08-02T00:46:27.701995: step 30096, loss 0.59792.
Train: 2018-08-02T00:46:27.866526: step 30097, loss 0.615292.
Train: 2018-08-02T00:46:28.032083: step 30098, loss 0.580137.
Train: 2018-08-02T00:46:28.193677: step 30099, loss 0.667554.
Train: 2018-08-02T00:46:28.357215: step 30100, loss 0.636821.
Test: 2018-08-02T00:46:28.894776: step 30100, loss 0.548096.
