Train: 2018-07-31T11:12:12.735022: step 1, loss 1.649.
Train: 2018-07-31T11:12:12.953721: step 2, loss 4.27576.
Train: 2018-07-31T11:12:13.156798: step 3, loss 4.36723.
Train: 2018-07-31T11:12:13.344280: step 4, loss 2.6129.
Train: 2018-07-31T11:12:13.547363: step 5, loss 2.77957.
Train: 2018-07-31T11:12:13.734813: step 6, loss 2.34224.
Train: 2018-07-31T11:12:13.937867: step 7, loss 1.40397.
Train: 2018-07-31T11:12:14.125355: step 8, loss 1.17663.
Train: 2018-07-31T11:12:14.312809: step 9, loss 0.886486.
Train: 2018-07-31T11:12:14.515855: step 10, loss 0.906768.
Test: 2018-07-31T11:12:15.171952: step 10, loss 0.71379.
Train: 2018-07-31T11:12:15.375030: step 11, loss 0.925582.
Train: 2018-07-31T11:12:15.562516: step 12, loss 0.9125.
Train: 2018-07-31T11:12:15.765563: step 13, loss 0.843209.
Train: 2018-07-31T11:12:15.953020: step 14, loss 0.775483.
Train: 2018-07-31T11:12:16.140506: step 15, loss 0.741275.
Train: 2018-07-31T11:12:16.327931: step 16, loss 0.853172.
Train: 2018-07-31T11:12:16.531009: step 17, loss 0.77693.
Train: 2018-07-31T11:12:16.718465: step 18, loss 0.754829.
Train: 2018-07-31T11:12:16.905922: step 19, loss 0.705373.
Train: 2018-07-31T11:12:17.108998: step 20, loss 0.779387.
Test: 2018-07-31T11:12:17.562047: step 20, loss 0.728699.
Train: 2018-07-31T11:12:17.765095: step 21, loss 0.728323.
Train: 2018-07-31T11:12:17.952551: step 22, loss 0.730654.
Train: 2018-07-31T11:12:18.140006: step 23, loss 0.709618.
Train: 2018-07-31T11:12:18.327463: step 24, loss 0.673041.
Train: 2018-07-31T11:12:18.530571: step 25, loss 0.721226.
Train: 2018-07-31T11:12:18.733619: step 26, loss 0.688262.
Train: 2018-07-31T11:12:18.921075: step 27, loss 0.708481.
Train: 2018-07-31T11:12:19.108531: step 28, loss 0.683843.
Train: 2018-07-31T11:12:19.311608: step 29, loss 0.64377.
Train: 2018-07-31T11:12:19.499064: step 30, loss 0.71895.
Test: 2018-07-31T11:12:19.967704: step 30, loss 0.65051.
Train: 2018-07-31T11:12:20.155190: step 31, loss 0.637641.
Train: 2018-07-31T11:12:20.342616: step 32, loss 0.680744.
Train: 2018-07-31T11:12:20.545723: step 33, loss 0.634714.
Train: 2018-07-31T11:12:20.733149: step 34, loss 0.695371.
Train: 2018-07-31T11:12:20.936256: step 35, loss 0.690176.
Train: 2018-07-31T11:12:21.123683: step 36, loss 0.638085.
Train: 2018-07-31T11:12:21.326761: step 37, loss 0.667937.
Train: 2018-07-31T11:12:21.514217: step 38, loss 0.60161.
Train: 2018-07-31T11:12:21.701706: step 39, loss 0.619405.
Train: 2018-07-31T11:12:21.904775: step 40, loss 0.626196.
Test: 2018-07-31T11:12:22.357794: step 40, loss 0.615351.
Train: 2018-07-31T11:12:22.545226: step 41, loss 0.603132.
Train: 2018-07-31T11:12:22.748302: step 42, loss 0.663184.
Train: 2018-07-31T11:12:22.935789: step 43, loss 0.616679.
Train: 2018-07-31T11:12:23.138836: step 44, loss 0.570849.
Train: 2018-07-31T11:12:23.326323: step 45, loss 0.546919.
Train: 2018-07-31T11:12:23.529400: step 46, loss 0.658369.
Train: 2018-07-31T11:12:23.716827: step 47, loss 0.619822.
Train: 2018-07-31T11:12:23.919933: step 48, loss 0.541598.
Train: 2018-07-31T11:12:24.123014: step 49, loss 0.68705.
Train: 2018-07-31T11:12:24.310467: step 50, loss 0.633288.
Test: 2018-07-31T11:12:24.779107: step 50, loss 0.60059.
Train: 2018-07-31T11:12:24.966534: step 51, loss 0.584376.
Train: 2018-07-31T11:12:25.169611: step 52, loss 0.634508.
Train: 2018-07-31T11:12:25.357098: step 53, loss 0.585367.
Train: 2018-07-31T11:12:25.544553: step 54, loss 0.619923.
Train: 2018-07-31T11:12:25.747600: step 55, loss 0.662347.
Train: 2018-07-31T11:12:25.950678: step 56, loss 0.556856.
Train: 2018-07-31T11:12:26.138164: step 57, loss 0.633127.
Train: 2018-07-31T11:12:26.325620: step 58, loss 0.578661.
Train: 2018-07-31T11:12:26.528699: step 59, loss 0.670415.
Train: 2018-07-31T11:12:26.731769: step 60, loss 0.56035.
Test: 2018-07-31T11:12:27.184794: step 60, loss 0.591143.
Train: 2018-07-31T11:12:27.387842: step 61, loss 0.613374.
Train: 2018-07-31T11:12:27.575298: step 62, loss 0.629646.
Train: 2018-07-31T11:12:27.762784: step 63, loss 0.651638.
Train: 2018-07-31T11:12:27.965830: step 64, loss 0.614426.
Train: 2018-07-31T11:12:28.153289: step 65, loss 0.649085.
Train: 2018-07-31T11:12:28.356364: step 66, loss 0.62929.
Train: 2018-07-31T11:12:28.543820: step 67, loss 0.613291.
Train: 2018-07-31T11:12:28.746899: step 68, loss 0.661258.
Train: 2018-07-31T11:12:28.934353: step 69, loss 0.603263.
Train: 2018-07-31T11:12:29.137432: step 70, loss 0.563343.
Test: 2018-07-31T11:12:29.606072: step 70, loss 0.604052.
Train: 2018-07-31T11:12:29.809174: step 71, loss 0.662018.
Train: 2018-07-31T11:12:29.996639: step 72, loss 0.607158.
Train: 2018-07-31T11:12:30.199707: step 73, loss 0.594897.
Train: 2018-07-31T11:12:30.387138: step 74, loss 0.647298.
Train: 2018-07-31T11:12:30.574595: step 75, loss 0.721557.
Train: 2018-07-31T11:12:30.777672: step 76, loss 0.665869.
Train: 2018-07-31T11:12:30.965129: step 77, loss 0.638194.
Train: 2018-07-31T11:12:31.152585: step 78, loss 0.549179.
Train: 2018-07-31T11:12:31.355692: step 79, loss 0.600137.
Train: 2018-07-31T11:12:31.543148: step 80, loss 0.560636.
Test: 2018-07-31T11:12:32.011759: step 80, loss 0.600387.
Train: 2018-07-31T11:12:32.199215: step 81, loss 0.63259.
Train: 2018-07-31T11:12:32.402293: step 82, loss 0.623366.
Train: 2018-07-31T11:12:32.589780: step 83, loss 0.657575.
Train: 2018-07-31T11:12:32.792855: step 84, loss 0.582031.
Train: 2018-07-31T11:12:32.995937: step 85, loss 0.596545.
Train: 2018-07-31T11:12:33.183389: step 86, loss 0.56793.
Train: 2018-07-31T11:12:33.386436: step 87, loss 0.670893.
Train: 2018-07-31T11:12:33.573918: step 88, loss 0.608261.
Train: 2018-07-31T11:12:33.776969: step 89, loss 0.625822.
Train: 2018-07-31T11:12:33.980048: step 90, loss 0.589158.
Test: 2018-07-31T11:12:34.448718: step 90, loss 0.597056.
Train: 2018-07-31T11:12:34.636143: step 91, loss 0.712858.
Train: 2018-07-31T11:12:34.839254: step 92, loss 0.606484.
Train: 2018-07-31T11:12:35.042329: step 93, loss 0.644708.
Train: 2018-07-31T11:12:35.229785: step 94, loss 0.697711.
Train: 2018-07-31T11:12:35.432862: step 95, loss 0.579011.
Train: 2018-07-31T11:12:35.635912: step 96, loss 0.601168.
Train: 2018-07-31T11:12:35.823390: step 97, loss 0.622524.
Train: 2018-07-31T11:12:36.010853: step 98, loss 0.58809.
Train: 2018-07-31T11:12:36.213930: step 99, loss 0.614565.
Train: 2018-07-31T11:12:36.401356: step 100, loss 0.596152.
Test: 2018-07-31T11:12:36.869995: step 100, loss 0.587101.
Train: 2018-07-31T11:12:37.635472: step 101, loss 0.620139.
Train: 2018-07-31T11:12:37.822928: step 102, loss 0.535153.
Train: 2018-07-31T11:12:38.026006: step 103, loss 0.585255.
Train: 2018-07-31T11:12:38.213431: step 104, loss 0.597872.
Train: 2018-07-31T11:12:38.400918: step 105, loss 0.610277.
Train: 2018-07-31T11:12:38.603965: step 106, loss 0.607692.
Train: 2018-07-31T11:12:38.791451: step 107, loss 0.571029.
Train: 2018-07-31T11:12:38.994498: step 108, loss 0.593563.
Train: 2018-07-31T11:12:39.181955: step 109, loss 0.663622.
Train: 2018-07-31T11:12:39.385032: step 110, loss 0.604386.
Test: 2018-07-31T11:12:39.853702: step 110, loss 0.580402.
Train: 2018-07-31T11:12:40.041159: step 111, loss 0.61221.
Train: 2018-07-31T11:12:40.244205: step 112, loss 0.623464.
Train: 2018-07-31T11:12:40.431692: step 113, loss 0.574322.
Train: 2018-07-31T11:12:40.634739: step 114, loss 0.702178.
Train: 2018-07-31T11:12:40.822195: step 115, loss 0.610232.
Train: 2018-07-31T11:12:41.025272: step 116, loss 0.621965.
Train: 2018-07-31T11:12:41.212729: step 117, loss 0.642148.
Train: 2018-07-31T11:12:41.415806: step 118, loss 0.613153.
Train: 2018-07-31T11:12:41.603263: step 119, loss 0.68488.
Train: 2018-07-31T11:12:41.790719: step 120, loss 0.582171.
Test: 2018-07-31T11:12:42.259359: step 120, loss 0.597526.
Train: 2018-07-31T11:12:42.446815: step 121, loss 0.620174.
Train: 2018-07-31T11:12:42.649922: step 122, loss 0.650639.
Train: 2018-07-31T11:12:42.837380: step 123, loss 0.554128.
Train: 2018-07-31T11:12:43.040456: step 124, loss 0.613408.
Train: 2018-07-31T11:12:43.227906: step 125, loss 0.586634.
Train: 2018-07-31T11:12:43.430961: step 126, loss 0.552092.
Train: 2018-07-31T11:12:43.618449: step 127, loss 0.645215.
Train: 2018-07-31T11:12:43.821523: step 128, loss 0.544113.
Train: 2018-07-31T11:12:44.008949: step 129, loss 0.527272.
Train: 2018-07-31T11:12:44.212056: step 130, loss 0.617759.
Test: 2018-07-31T11:12:44.680667: step 130, loss 0.586294.
Train: 2018-07-31T11:12:44.868156: step 131, loss 0.571356.
Train: 2018-07-31T11:12:45.071231: step 132, loss 0.648966.
Train: 2018-07-31T11:12:45.258656: step 133, loss 0.598909.
Train: 2018-07-31T11:12:45.461758: step 134, loss 0.596819.
Train: 2018-07-31T11:12:45.649220: step 135, loss 0.630806.
Train: 2018-07-31T11:12:45.836647: step 136, loss 0.59227.
Train: 2018-07-31T11:12:46.039753: step 137, loss 0.576933.
Train: 2018-07-31T11:12:46.227210: step 138, loss 0.628731.
Train: 2018-07-31T11:12:46.430262: step 139, loss 0.578672.
Train: 2018-07-31T11:12:46.617747: step 140, loss 0.610848.
Test: 2018-07-31T11:12:47.070762: step 140, loss 0.577458.
Train: 2018-07-31T11:12:47.273840: step 141, loss 0.637705.
Train: 2018-07-31T11:12:47.476920: step 142, loss 0.594316.
Train: 2018-07-31T11:12:47.664344: step 143, loss 0.551075.
Train: 2018-07-31T11:12:47.867454: step 144, loss 0.558035.
Train: 2018-07-31T11:12:48.054906: step 145, loss 0.546144.
Train: 2018-07-31T11:12:48.257955: step 146, loss 0.599335.
Train: 2018-07-31T11:12:48.445411: step 147, loss 0.558909.
Train: 2018-07-31T11:12:48.648488: step 148, loss 0.524463.
Train: 2018-07-31T11:12:48.835974: step 149, loss 0.630096.
Train: 2018-07-31T11:12:49.039021: step 150, loss 0.618118.
Test: 2018-07-31T11:12:49.507662: step 150, loss 0.576372.
Train: 2018-07-31T11:12:49.695117: step 151, loss 0.508169.
Train: 2018-07-31T11:12:49.898194: step 152, loss 0.594907.
Train: 2018-07-31T11:12:50.085681: step 153, loss 0.564847.
Train: 2018-07-31T11:12:50.288762: step 154, loss 0.605975.
Train: 2018-07-31T11:12:50.476214: step 155, loss 0.68189.
Train: 2018-07-31T11:12:50.679291: step 156, loss 0.638046.
Train: 2018-07-31T11:12:50.882366: step 157, loss 0.650451.
Train: 2018-07-31T11:12:51.069826: step 158, loss 0.547725.
Train: 2018-07-31T11:12:51.272904: step 159, loss 0.602672.
Train: 2018-07-31T11:12:51.460329: step 160, loss 0.580307.
Test: 2018-07-31T11:12:51.928970: step 160, loss 0.578975.
Train: 2018-07-31T11:12:52.116459: step 161, loss 0.598762.
Train: 2018-07-31T11:12:52.319503: step 162, loss 0.585074.
Train: 2018-07-31T11:12:52.506959: step 163, loss 0.588451.
Train: 2018-07-31T11:12:52.710067: step 164, loss 0.656528.
Train: 2018-07-31T11:12:52.897522: step 165, loss 0.59287.
Train: 2018-07-31T11:12:53.100600: step 166, loss 0.627655.
Train: 2018-07-31T11:12:53.288027: step 167, loss 0.637763.
Train: 2018-07-31T11:12:53.491128: step 168, loss 0.559845.
Train: 2018-07-31T11:12:53.678590: step 169, loss 0.642316.
Train: 2018-07-31T11:12:53.866049: step 170, loss 0.637304.
Test: 2018-07-31T11:12:54.350308: step 170, loss 0.579613.
Train: 2018-07-31T11:12:54.537733: step 171, loss 0.543922.
Train: 2018-07-31T11:12:54.740811: step 172, loss 0.59789.
Train: 2018-07-31T11:12:54.943920: step 173, loss 0.572499.
Train: 2018-07-31T11:12:55.131374: step 174, loss 0.591256.
Train: 2018-07-31T11:12:55.334452: step 175, loss 0.612387.
Train: 2018-07-31T11:12:55.521879: step 176, loss 0.616404.
Train: 2018-07-31T11:12:55.724956: step 177, loss 0.571026.
Train: 2018-07-31T11:12:55.912442: step 178, loss 0.613556.
Train: 2018-07-31T11:12:56.115522: step 179, loss 0.598729.
Train: 2018-07-31T11:12:56.302945: step 180, loss 0.623483.
Test: 2018-07-31T11:12:56.771586: step 180, loss 0.576175.
Train: 2018-07-31T11:12:56.974663: step 181, loss 0.500964.
Train: 2018-07-31T11:12:57.162119: step 182, loss 0.65718.
Train: 2018-07-31T11:12:57.349575: step 183, loss 0.572316.
Train: 2018-07-31T11:12:57.552683: step 184, loss 0.611349.
Train: 2018-07-31T11:12:57.740142: step 185, loss 0.539374.
Train: 2018-07-31T11:12:57.943216: step 186, loss 0.597272.
Train: 2018-07-31T11:12:58.130672: step 187, loss 0.580311.
Train: 2018-07-31T11:12:58.333750: step 188, loss 0.501809.
Train: 2018-07-31T11:12:58.521207: step 189, loss 0.538417.
Train: 2018-07-31T11:12:58.724254: step 190, loss 0.598298.
Test: 2018-07-31T11:12:59.192894: step 190, loss 0.574317.
Train: 2018-07-31T11:12:59.395970: step 191, loss 0.601604.
Train: 2018-07-31T11:12:59.599074: step 192, loss 0.6032.
Train: 2018-07-31T11:12:59.786535: step 193, loss 0.689807.
Train: 2018-07-31T11:12:59.989616: step 194, loss 0.586997.
Train: 2018-07-31T11:13:00.192659: step 195, loss 0.62701.
Train: 2018-07-31T11:13:00.380116: step 196, loss 0.618371.
Train: 2018-07-31T11:13:00.583217: step 197, loss 0.534694.
Train: 2018-07-31T11:13:00.770679: step 198, loss 0.640083.
Train: 2018-07-31T11:13:00.958130: step 199, loss 0.541429.
Train: 2018-07-31T11:13:01.161183: step 200, loss 0.57355.
Test: 2018-07-31T11:13:01.629853: step 200, loss 0.573893.
Train: 2018-07-31T11:13:02.364026: step 201, loss 0.585531.
Train: 2018-07-31T11:13:02.551512: step 202, loss 0.616167.
Train: 2018-07-31T11:13:02.754589: step 203, loss 0.563622.
Train: 2018-07-31T11:13:02.942015: step 204, loss 0.577464.
Train: 2018-07-31T11:13:03.129501: step 205, loss 0.536015.
Train: 2018-07-31T11:13:03.348204: step 206, loss 0.601969.
Train: 2018-07-31T11:13:03.535627: step 207, loss 0.597294.
Train: 2018-07-31T11:13:03.738737: step 208, loss 0.634053.
Train: 2018-07-31T11:13:03.926190: step 209, loss 0.631928.
Train: 2018-07-31T11:13:04.129238: step 210, loss 0.604432.
Test: 2018-07-31T11:13:04.597878: step 210, loss 0.572651.
Train: 2018-07-31T11:13:04.962584: step 211, loss 0.567199.
Train: 2018-07-31T11:13:05.212525: step 212, loss 0.540655.
Train: 2018-07-31T11:13:05.415601: step 213, loss 0.57082.
Train: 2018-07-31T11:13:05.603060: step 214, loss 0.634464.
Train: 2018-07-31T11:13:05.806135: step 215, loss 0.605313.
Train: 2018-07-31T11:13:05.993590: step 216, loss 0.60488.
Train: 2018-07-31T11:13:06.196698: step 217, loss 0.588456.
Train: 2018-07-31T11:13:06.384157: step 218, loss 0.647012.
Train: 2018-07-31T11:13:06.587203: step 219, loss 0.61408.
Train: 2018-07-31T11:13:06.790313: step 220, loss 0.55897.
Test: 2018-07-31T11:13:07.258920: step 220, loss 0.571584.
Train: 2018-07-31T11:13:07.446375: step 221, loss 0.565137.
Train: 2018-07-31T11:13:07.649484: step 222, loss 0.573774.
Train: 2018-07-31T11:13:07.852561: step 223, loss 0.632543.
Train: 2018-07-31T11:13:08.040017: step 224, loss 0.61171.
Train: 2018-07-31T11:13:08.258710: step 225, loss 0.586091.
Train: 2018-07-31T11:13:08.446172: step 226, loss 0.550845.
Train: 2018-07-31T11:13:08.649218: step 227, loss 0.611845.
Train: 2018-07-31T11:13:08.852321: step 228, loss 0.529631.
Train: 2018-07-31T11:13:09.039752: step 229, loss 0.585296.
Train: 2018-07-31T11:13:09.242829: step 230, loss 0.592525.
Test: 2018-07-31T11:13:09.711471: step 230, loss 0.570135.
Train: 2018-07-31T11:13:09.914578: step 231, loss 0.602868.
Train: 2018-07-31T11:13:10.117655: step 232, loss 0.543372.
Train: 2018-07-31T11:13:10.305082: step 233, loss 0.567771.
Train: 2018-07-31T11:13:10.492567: step 234, loss 0.602719.
Train: 2018-07-31T11:13:10.695615: step 235, loss 0.612998.
Train: 2018-07-31T11:13:10.898692: step 236, loss 0.592137.
Train: 2018-07-31T11:13:11.086149: step 237, loss 0.555566.
Train: 2018-07-31T11:13:11.289227: step 238, loss 0.591987.
Train: 2018-07-31T11:13:11.492334: step 239, loss 0.635547.
Train: 2018-07-31T11:13:11.679760: step 240, loss 0.545523.
Test: 2018-07-31T11:13:12.164020: step 240, loss 0.569853.
Train: 2018-07-31T11:13:12.351507: step 241, loss 0.62833.
Train: 2018-07-31T11:13:12.554555: step 242, loss 0.584527.
Train: 2018-07-31T11:13:12.757665: step 243, loss 0.527293.
Train: 2018-07-31T11:13:12.945088: step 244, loss 0.602919.
Train: 2018-07-31T11:13:13.132577: step 245, loss 0.609927.
Train: 2018-07-31T11:13:13.335652: step 246, loss 0.660564.
Train: 2018-07-31T11:13:13.523078: step 247, loss 0.59355.
Train: 2018-07-31T11:13:13.726156: step 248, loss 0.618621.
Train: 2018-07-31T11:13:13.929233: step 249, loss 0.565718.
Train: 2018-07-31T11:13:14.116720: step 250, loss 0.598311.
Test: 2018-07-31T11:13:14.600981: step 250, loss 0.570203.
Train: 2018-07-31T11:13:14.804028: step 251, loss 0.599284.
Train: 2018-07-31T11:13:14.991514: step 252, loss 0.545953.
Train: 2018-07-31T11:13:15.194562: step 253, loss 0.553308.
Train: 2018-07-31T11:13:15.382018: step 254, loss 0.545101.
Train: 2018-07-31T11:13:15.585125: step 255, loss 0.609724.
Train: 2018-07-31T11:13:15.772581: step 256, loss 0.518096.
Train: 2018-07-31T11:13:15.975629: step 257, loss 0.508443.
Train: 2018-07-31T11:13:16.163110: step 258, loss 0.590057.
Train: 2018-07-31T11:13:16.366186: step 259, loss 0.536384.
Train: 2018-07-31T11:13:16.553619: step 260, loss 0.642689.
Test: 2018-07-31T11:13:17.022289: step 260, loss 0.567861.
Train: 2018-07-31T11:13:17.209715: step 261, loss 0.576589.
Train: 2018-07-31T11:13:17.412822: step 262, loss 0.517832.
Train: 2018-07-31T11:13:17.600249: step 263, loss 0.531364.
Train: 2018-07-31T11:13:17.803356: step 264, loss 0.606988.
Train: 2018-07-31T11:13:17.990806: step 265, loss 0.565168.
Train: 2018-07-31T11:13:18.178268: step 266, loss 0.622235.
Train: 2018-07-31T11:13:18.381348: step 267, loss 0.551462.
Train: 2018-07-31T11:13:18.568802: step 268, loss 0.546704.
Train: 2018-07-31T11:13:18.771849: step 269, loss 0.583771.
Train: 2018-07-31T11:13:18.959304: step 270, loss 0.572649.
Test: 2018-07-31T11:13:19.443566: step 270, loss 0.567373.
Train: 2018-07-31T11:13:19.631023: step 271, loss 0.608672.
Train: 2018-07-31T11:13:19.818509: step 272, loss 0.598481.
Train: 2018-07-31T11:13:20.021557: step 273, loss 0.507938.
Train: 2018-07-31T11:13:20.209042: step 274, loss 0.591293.
Train: 2018-07-31T11:13:20.412089: step 275, loss 0.607261.
Train: 2018-07-31T11:13:20.599576: step 276, loss 0.529334.
Train: 2018-07-31T11:13:20.802623: step 277, loss 0.573734.
Train: 2018-07-31T11:13:20.990079: step 278, loss 0.581875.
Train: 2018-07-31T11:13:21.177536: step 279, loss 0.547727.
Train: 2018-07-31T11:13:21.364992: step 280, loss 0.57358.
Test: 2018-07-31T11:13:21.833632: step 280, loss 0.566957.
Train: 2018-07-31T11:13:22.036739: step 281, loss 0.555303.
Train: 2018-07-31T11:13:22.224167: step 282, loss 0.58981.
Train: 2018-07-31T11:13:22.411651: step 283, loss 0.564866.
Train: 2018-07-31T11:13:22.599107: step 284, loss 0.512796.
Train: 2018-07-31T11:13:22.802156: step 285, loss 0.573689.
Train: 2018-07-31T11:13:22.989611: step 286, loss 0.522023.
Train: 2018-07-31T11:13:23.192719: step 287, loss 0.615449.
Train: 2018-07-31T11:13:23.380144: step 288, loss 0.4854.
Train: 2018-07-31T11:13:23.583247: step 289, loss 0.599392.
Train: 2018-07-31T11:13:23.786299: step 290, loss 0.569611.
Test: 2018-07-31T11:13:24.254972: step 290, loss 0.565962.
Train: 2018-07-31T11:13:24.442396: step 291, loss 0.569993.
Train: 2018-07-31T11:13:24.645473: step 292, loss 0.573806.
Train: 2018-07-31T11:13:24.832960: step 293, loss 0.621506.
Train: 2018-07-31T11:13:25.036037: step 294, loss 0.643499.
Train: 2018-07-31T11:13:25.223493: step 295, loss 0.547181.
Train: 2018-07-31T11:13:25.410919: step 296, loss 0.538159.
Train: 2018-07-31T11:13:25.614028: step 297, loss 0.546556.
Train: 2018-07-31T11:13:25.801483: step 298, loss 0.562454.
Train: 2018-07-31T11:13:25.988939: step 299, loss 0.554413.
Train: 2018-07-31T11:13:26.176395: step 300, loss 0.572656.
Test: 2018-07-31T11:13:26.660626: step 300, loss 0.565777.
Train: 2018-07-31T11:13:27.347996: step 301, loss 0.5453.
Train: 2018-07-31T11:13:27.519801: step 302, loss 0.544117.
Train: 2018-07-31T11:13:27.722908: step 303, loss 0.579771.
Train: 2018-07-31T11:13:27.910359: step 304, loss 0.684097.
Train: 2018-07-31T11:13:28.097820: step 305, loss 0.563461.
Train: 2018-07-31T11:13:28.285246: step 306, loss 0.571166.
Train: 2018-07-31T11:13:28.472733: step 307, loss 0.639615.
Train: 2018-07-31T11:13:28.675779: step 308, loss 0.554185.
Train: 2018-07-31T11:13:28.863236: step 309, loss 0.554759.
Train: 2018-07-31T11:13:29.050691: step 310, loss 0.493752.
Test: 2018-07-31T11:13:29.519331: step 310, loss 0.56548.
Train: 2018-07-31T11:13:29.706818: step 311, loss 0.66557.
Train: 2018-07-31T11:13:29.894245: step 312, loss 0.553276.
Train: 2018-07-31T11:13:30.081730: step 313, loss 0.562438.
Train: 2018-07-31T11:13:30.269186: step 314, loss 0.553692.
Train: 2018-07-31T11:13:30.472236: step 315, loss 0.535909.
Train: 2018-07-31T11:13:30.659689: step 316, loss 0.572498.
Train: 2018-07-31T11:13:30.847147: step 317, loss 0.571626.
Train: 2018-07-31T11:13:31.034601: step 318, loss 0.570269.
Train: 2018-07-31T11:13:31.222059: step 319, loss 0.67556.
Train: 2018-07-31T11:13:31.425135: step 320, loss 0.535621.
Test: 2018-07-31T11:13:31.893776: step 320, loss 0.56497.
Train: 2018-07-31T11:13:32.081232: step 321, loss 0.614259.
Train: 2018-07-31T11:13:32.268712: step 322, loss 0.570823.
Train: 2018-07-31T11:13:32.456174: step 323, loss 0.6244.
Train: 2018-07-31T11:13:32.643601: step 324, loss 0.511327.
Train: 2018-07-31T11:13:32.831057: step 325, loss 0.510369.
Train: 2018-07-31T11:13:33.034134: step 326, loss 0.554102.
Train: 2018-07-31T11:13:33.221620: step 327, loss 0.570523.
Train: 2018-07-31T11:13:33.409071: step 328, loss 0.602056.
Train: 2018-07-31T11:13:33.612157: step 329, loss 0.613428.
Train: 2018-07-31T11:13:33.799604: step 330, loss 0.578821.
Test: 2018-07-31T11:13:34.268219: step 330, loss 0.56438.
Train: 2018-07-31T11:13:34.471322: step 331, loss 0.623606.
Train: 2018-07-31T11:13:34.658754: step 332, loss 0.535647.
Train: 2018-07-31T11:13:34.846234: step 333, loss 0.589163.
Train: 2018-07-31T11:13:35.033666: step 334, loss 0.60556.
Train: 2018-07-31T11:13:35.221146: step 335, loss 0.581009.
Train: 2018-07-31T11:13:35.424200: step 336, loss 0.59104.
Train: 2018-07-31T11:13:35.611655: step 337, loss 0.492053.
Train: 2018-07-31T11:13:35.814764: step 338, loss 0.520264.
Train: 2018-07-31T11:13:36.002213: step 339, loss 0.589124.
Train: 2018-07-31T11:13:36.189644: step 340, loss 0.591553.
Test: 2018-07-31T11:13:36.673906: step 340, loss 0.563834.
Train: 2018-07-31T11:13:36.861393: step 341, loss 0.532148.
Train: 2018-07-31T11:13:37.048849: step 342, loss 0.515343.
Train: 2018-07-31T11:13:37.251896: step 343, loss 0.570194.
Train: 2018-07-31T11:13:37.439353: step 344, loss 0.598507.
Train: 2018-07-31T11:13:37.626841: step 345, loss 0.590824.
Train: 2018-07-31T11:13:37.814265: step 346, loss 0.586498.
Train: 2018-07-31T11:13:38.001720: step 347, loss 0.577771.
Train: 2018-07-31T11:13:38.204831: step 348, loss 0.543474.
Train: 2018-07-31T11:13:38.392284: step 349, loss 0.560938.
Train: 2018-07-31T11:13:38.579740: step 350, loss 0.5253.
Test: 2018-07-31T11:13:39.064002: step 350, loss 0.563696.
Train: 2018-07-31T11:13:39.251427: step 351, loss 0.560668.
Train: 2018-07-31T11:13:39.438914: step 352, loss 0.569509.
Train: 2018-07-31T11:13:39.626340: step 353, loss 0.587101.
Train: 2018-07-31T11:13:39.813827: step 354, loss 0.577873.
Train: 2018-07-31T11:13:40.016907: step 355, loss 0.587072.
Train: 2018-07-31T11:13:40.204329: step 356, loss 0.507111.
Train: 2018-07-31T11:13:40.391787: step 357, loss 0.54263.
Train: 2018-07-31T11:13:40.579266: step 358, loss 0.560527.
Train: 2018-07-31T11:13:40.782320: step 359, loss 0.560772.
Train: 2018-07-31T11:13:40.969806: step 360, loss 0.542539.
Test: 2018-07-31T11:13:41.438446: step 360, loss 0.563368.
Train: 2018-07-31T11:13:41.625871: step 361, loss 0.623129.
Train: 2018-07-31T11:13:41.828975: step 362, loss 0.603951.
Train: 2018-07-31T11:13:42.016405: step 363, loss 0.619713.
Train: 2018-07-31T11:13:42.203861: step 364, loss 0.624778.
Train: 2018-07-31T11:13:42.406969: step 365, loss 0.616493.
Train: 2018-07-31T11:13:42.594425: step 366, loss 0.60278.
Train: 2018-07-31T11:13:42.781852: step 367, loss 0.572486.
Train: 2018-07-31T11:13:42.984928: step 368, loss 0.586725.
Train: 2018-07-31T11:13:43.172416: step 369, loss 0.622544.
Train: 2018-07-31T11:13:43.359872: step 370, loss 0.600309.
Test: 2018-07-31T11:13:43.828511: step 370, loss 0.563356.
Train: 2018-07-31T11:13:44.015967: step 371, loss 0.609833.
Train: 2018-07-31T11:13:44.203393: step 372, loss 0.555415.
Train: 2018-07-31T11:13:44.406501: step 373, loss 0.548865.
Train: 2018-07-31T11:13:44.593958: step 374, loss 0.587841.
Train: 2018-07-31T11:13:44.781384: step 375, loss 0.550179.
Train: 2018-07-31T11:13:44.984490: step 376, loss 0.526439.
Train: 2018-07-31T11:13:45.171916: step 377, loss 0.583054.
Train: 2018-07-31T11:13:45.359405: step 378, loss 0.649335.
Train: 2018-07-31T11:13:45.546829: step 379, loss 0.563259.
Train: 2018-07-31T11:13:45.734315: step 380, loss 0.63442.
Test: 2018-07-31T11:13:46.218546: step 380, loss 0.564045.
Train: 2018-07-31T11:13:46.406033: step 381, loss 0.533933.
Train: 2018-07-31T11:13:46.593458: step 382, loss 0.667147.
Train: 2018-07-31T11:13:46.780948: step 383, loss 0.56052.
Train: 2018-07-31T11:13:46.984022: step 384, loss 0.588062.
Train: 2018-07-31T11:13:47.171447: step 385, loss 0.544475.
Train: 2018-07-31T11:13:47.374553: step 386, loss 0.552792.
Train: 2018-07-31T11:13:47.561981: step 387, loss 0.58732.
Train: 2018-07-31T11:13:47.749463: step 388, loss 0.612005.
Train: 2018-07-31T11:13:47.936894: step 389, loss 0.682132.
Train: 2018-07-31T11:13:48.124383: step 390, loss 0.569439.
Test: 2018-07-31T11:13:48.608642: step 390, loss 0.563502.
Train: 2018-07-31T11:13:48.796068: step 391, loss 0.586755.
Train: 2018-07-31T11:13:48.999178: step 392, loss 0.60264.
Train: 2018-07-31T11:13:49.186601: step 393, loss 0.568741.
Train: 2018-07-31T11:13:49.374058: step 394, loss 0.544018.
Train: 2018-07-31T11:13:49.561543: step 395, loss 0.534601.
Train: 2018-07-31T11:13:49.764621: step 396, loss 0.578222.
Train: 2018-07-31T11:13:49.952048: step 397, loss 0.594868.
Train: 2018-07-31T11:13:50.155125: step 398, loss 0.585111.
Train: 2018-07-31T11:13:50.342581: step 399, loss 0.57795.
Train: 2018-07-31T11:13:50.530067: step 400, loss 0.637082.
Test: 2018-07-31T11:13:51.001247: step 400, loss 0.563284.
Train: 2018-07-31T11:13:51.704207: step 401, loss 0.518723.
Train: 2018-07-31T11:13:51.891633: step 402, loss 0.577239.
Train: 2018-07-31T11:13:52.079118: step 403, loss 0.602761.
Train: 2018-07-31T11:13:52.266578: step 404, loss 0.593197.
Train: 2018-07-31T11:13:52.469647: step 405, loss 0.577503.
Train: 2018-07-31T11:13:52.657078: step 406, loss 0.503032.
Train: 2018-07-31T11:13:52.844535: step 407, loss 0.54253.
Train: 2018-07-31T11:13:53.047642: step 408, loss 0.619041.
Train: 2018-07-31T11:13:53.250689: step 409, loss 0.559451.
Train: 2018-07-31T11:13:53.438177: step 410, loss 0.568597.
Test: 2018-07-31T11:13:53.906786: step 410, loss 0.56251.
Train: 2018-07-31T11:13:54.109894: step 411, loss 0.560491.
Train: 2018-07-31T11:13:54.297352: step 412, loss 0.526428.
Train: 2018-07-31T11:13:54.484776: step 413, loss 0.586303.
Train: 2018-07-31T11:13:54.672231: step 414, loss 0.525035.
Train: 2018-07-31T11:13:54.875309: step 415, loss 0.636626.
Train: 2018-07-31T11:13:55.062798: step 416, loss 0.559526.
Train: 2018-07-31T11:13:55.250252: step 417, loss 0.628569.
Train: 2018-07-31T11:13:55.453300: step 418, loss 0.670194.
Train: 2018-07-31T11:13:55.640754: step 419, loss 0.551388.
Train: 2018-07-31T11:13:55.843832: step 420, loss 0.507755.
Test: 2018-07-31T11:13:56.312472: step 420, loss 0.561863.
Train: 2018-07-31T11:13:56.515581: step 421, loss 0.601647.
Train: 2018-07-31T11:13:56.718628: step 422, loss 0.618465.
Train: 2018-07-31T11:13:56.906108: step 423, loss 0.577823.
Train: 2018-07-31T11:13:57.093570: step 424, loss 0.556326.
Train: 2018-07-31T11:13:57.296648: step 425, loss 0.613719.
Train: 2018-07-31T11:13:57.484074: step 426, loss 0.537726.
Train: 2018-07-31T11:13:57.687159: step 427, loss 0.560022.
Train: 2018-07-31T11:13:57.874637: step 428, loss 0.559953.
Train: 2018-07-31T11:13:58.062095: step 429, loss 0.566857.
Train: 2018-07-31T11:13:58.249544: step 430, loss 0.662277.
Test: 2018-07-31T11:13:58.718189: step 430, loss 0.561339.
Train: 2018-07-31T11:13:58.905616: step 431, loss 0.60136.
Train: 2018-07-31T11:13:59.108694: step 432, loss 0.567385.
Train: 2018-07-31T11:13:59.296179: step 433, loss 0.515997.
Train: 2018-07-31T11:13:59.483636: step 434, loss 0.593309.
Train: 2018-07-31T11:13:59.686715: step 435, loss 0.541179.
Train: 2018-07-31T11:13:59.874169: step 436, loss 0.66219.
Train: 2018-07-31T11:14:00.061627: step 437, loss 0.610144.
Train: 2018-07-31T11:14:00.249081: step 438, loss 0.489458.
Train: 2018-07-31T11:14:00.452159: step 439, loss 0.558621.
Train: 2018-07-31T11:14:00.639615: step 440, loss 0.635851.
Test: 2018-07-31T11:14:01.123857: step 440, loss 0.561155.
Train: 2018-07-31T11:14:01.326924: step 441, loss 0.567422.
Train: 2018-07-31T11:14:01.519944: step 442, loss 0.634911.
Train: 2018-07-31T11:14:01.707376: step 443, loss 0.592905.
Train: 2018-07-31T11:14:01.894831: step 444, loss 0.52422.
Train: 2018-07-31T11:14:02.082288: step 445, loss 0.557932.
Train: 2018-07-31T11:14:02.285395: step 446, loss 0.558702.
Train: 2018-07-31T11:14:02.488444: step 447, loss 0.592512.
Train: 2018-07-31T11:14:02.675899: step 448, loss 0.549524.
Train: 2018-07-31T11:14:02.863386: step 449, loss 0.58384.
Train: 2018-07-31T11:14:03.050842: step 450, loss 0.489286.
Test: 2018-07-31T11:14:03.519451: step 450, loss 0.560729.
Train: 2018-07-31T11:14:03.706908: step 451, loss 0.574722.
Train: 2018-07-31T11:14:03.894363: step 452, loss 0.584081.
Train: 2018-07-31T11:14:04.081853: step 453, loss 0.666016.
Train: 2018-07-31T11:14:04.269305: step 454, loss 0.519002.
Train: 2018-07-31T11:14:04.472354: step 455, loss 0.546987.
Train: 2018-07-31T11:14:04.659839: step 456, loss 0.620892.
Train: 2018-07-31T11:14:04.862887: step 457, loss 0.558104.
Train: 2018-07-31T11:14:05.050373: step 458, loss 0.660029.
Train: 2018-07-31T11:14:05.237798: step 459, loss 0.541924.
Train: 2018-07-31T11:14:05.440876: step 460, loss 0.617554.
Test: 2018-07-31T11:14:05.893895: step 460, loss 0.560403.
Train: 2018-07-31T11:14:06.094361: step 461, loss 0.566384.
Train: 2018-07-31T11:14:06.328712: step 462, loss 0.515101.
Train: 2018-07-31T11:14:06.531760: step 463, loss 0.557434.
Train: 2018-07-31T11:14:06.719215: step 464, loss 0.557254.
Train: 2018-07-31T11:14:06.906672: step 465, loss 0.659689.
Train: 2018-07-31T11:14:07.094164: step 466, loss 0.541019.
Train: 2018-07-31T11:14:07.281614: step 467, loss 0.549042.
Train: 2018-07-31T11:14:07.469041: step 468, loss 0.497516.
Train: 2018-07-31T11:14:07.656526: step 469, loss 0.557644.
Train: 2018-07-31T11:14:07.843953: step 470, loss 0.574696.
Test: 2018-07-31T11:14:08.312622: step 470, loss 0.560156.
Train: 2018-07-31T11:14:08.531292: step 471, loss 0.566022.
Train: 2018-07-31T11:14:08.718779: step 472, loss 0.55754.
Train: 2018-07-31T11:14:08.890612: step 473, loss 0.54051.
Train: 2018-07-31T11:14:09.093691: step 474, loss 0.574732.
Train: 2018-07-31T11:14:09.281115: step 475, loss 0.591702.
Train: 2018-07-31T11:14:09.468571: step 476, loss 0.565943.
Train: 2018-07-31T11:14:09.656053: step 477, loss 0.565544.
Train: 2018-07-31T11:14:09.843485: step 478, loss 0.583009.
Train: 2018-07-31T11:14:10.030941: step 479, loss 0.521674.
Train: 2018-07-31T11:14:10.218420: step 480, loss 0.477362.
Test: 2018-07-31T11:14:10.702692: step 480, loss 0.55977.
Train: 2018-07-31T11:14:10.890115: step 481, loss 0.52359.
Train: 2018-07-31T11:14:11.077596: step 482, loss 0.516501.
Train: 2018-07-31T11:14:11.265027: step 483, loss 0.545899.
Train: 2018-07-31T11:14:11.452483: step 484, loss 0.610234.
Train: 2018-07-31T11:14:11.639938: step 485, loss 0.592104.
Train: 2018-07-31T11:14:11.827394: step 486, loss 0.575485.
Train: 2018-07-31T11:14:12.014850: step 487, loss 0.558466.
Train: 2018-07-31T11:14:12.202309: step 488, loss 0.563377.
Train: 2018-07-31T11:14:12.389763: step 489, loss 0.682265.
Train: 2018-07-31T11:14:12.577249: step 490, loss 0.568855.
Test: 2018-07-31T11:14:13.045859: step 490, loss 0.559297.
Train: 2018-07-31T11:14:13.233316: step 491, loss 0.584713.
Train: 2018-07-31T11:14:13.420772: step 492, loss 0.592028.
Train: 2018-07-31T11:14:13.608228: step 493, loss 0.556391.
Train: 2018-07-31T11:14:13.795683: step 494, loss 0.565455.
Train: 2018-07-31T11:14:13.998760: step 495, loss 0.618627.
Train: 2018-07-31T11:14:14.186247: step 496, loss 0.529702.
Train: 2018-07-31T11:14:14.373674: step 497, loss 0.662974.
Train: 2018-07-31T11:14:14.561128: step 498, loss 0.565257.
Train: 2018-07-31T11:14:14.748586: step 499, loss 0.547617.
Train: 2018-07-31T11:14:14.936072: step 500, loss 0.547586.
Test: 2018-07-31T11:14:15.404683: step 500, loss 0.559334.
Train: 2018-07-31T11:14:16.154537: step 501, loss 0.538781.
Train: 2018-07-31T11:14:16.341963: step 502, loss 0.627022.
Train: 2018-07-31T11:14:16.513827: step 503, loss 0.626923.
Train: 2018-07-31T11:14:16.701283: step 504, loss 0.565275.
Train: 2018-07-31T11:14:16.888739: step 505, loss 0.582786.
Train: 2018-07-31T11:14:17.076195: step 506, loss 0.600286.
Train: 2018-07-31T11:14:17.263646: step 507, loss 0.600202.
Train: 2018-07-31T11:14:17.435486: step 508, loss 0.591368.
Train: 2018-07-31T11:14:17.622942: step 509, loss 0.626051.
Train: 2018-07-31T11:14:17.810368: step 510, loss 0.573831.
Test: 2018-07-31T11:14:18.279009: step 510, loss 0.559333.
Train: 2018-07-31T11:14:18.466464: step 511, loss 0.547877.
Train: 2018-07-31T11:14:18.653920: step 512, loss 0.556487.
Train: 2018-07-31T11:14:18.856999: step 513, loss 0.513514.
Train: 2018-07-31T11:14:19.028834: step 514, loss 0.582288.
Train: 2018-07-31T11:14:19.216320: step 515, loss 0.547858.
Train: 2018-07-31T11:14:19.403776: step 516, loss 0.487677.
Train: 2018-07-31T11:14:19.591202: step 517, loss 0.52195.
Train: 2018-07-31T11:14:19.763036: step 518, loss 0.599383.
Train: 2018-07-31T11:14:19.950523: step 519, loss 0.530291.
Train: 2018-07-31T11:14:20.137980: step 520, loss 0.556146.
Test: 2018-07-31T11:14:20.606588: step 520, loss 0.558873.
Train: 2018-07-31T11:14:20.794075: step 521, loss 0.55605.
Train: 2018-07-31T11:14:20.965880: step 522, loss 0.529943.
Train: 2018-07-31T11:14:21.153335: step 523, loss 0.616789.
Train: 2018-07-31T11:14:21.340823: step 524, loss 0.503576.
Train: 2018-07-31T11:14:21.512627: step 525, loss 0.599467.
Train: 2018-07-31T11:14:21.700113: step 526, loss 0.581947.
Train: 2018-07-31T11:14:21.887569: step 527, loss 0.485755.
Train: 2018-07-31T11:14:22.059404: step 528, loss 0.556011.
Train: 2018-07-31T11:14:22.246861: step 529, loss 0.617034.
Train: 2018-07-31T11:14:22.434286: step 530, loss 0.643417.
Test: 2018-07-31T11:14:22.887304: step 530, loss 0.558307.
Train: 2018-07-31T11:14:23.074760: step 531, loss 0.51283.
Train: 2018-07-31T11:14:23.277838: step 532, loss 0.556259.
Train: 2018-07-31T11:14:23.449673: step 533, loss 0.546387.
Train: 2018-07-31T11:14:23.637128: step 534, loss 0.555349.
Train: 2018-07-31T11:14:23.824615: step 535, loss 0.528432.
Train: 2018-07-31T11:14:23.996446: step 536, loss 0.653199.
Train: 2018-07-31T11:14:24.183901: step 537, loss 0.591069.
Train: 2018-07-31T11:14:24.371332: step 538, loss 0.529539.
Train: 2018-07-31T11:14:24.543197: step 539, loss 0.635621.
Train: 2018-07-31T11:14:24.730653: step 540, loss 0.608326.
Test: 2018-07-31T11:14:25.214885: step 540, loss 0.558236.
Train: 2018-07-31T11:14:25.386753: step 541, loss 0.545679.
Train: 2018-07-31T11:14:25.574175: step 542, loss 0.520012.
Train: 2018-07-31T11:14:25.761656: step 543, loss 0.517222.
Train: 2018-07-31T11:14:25.933496: step 544, loss 0.557542.
Train: 2018-07-31T11:14:26.120922: step 545, loss 0.633348.
Train: 2018-07-31T11:14:26.292781: step 546, loss 0.605058.
Train: 2018-07-31T11:14:26.480244: step 547, loss 0.625511.
Train: 2018-07-31T11:14:26.667693: step 548, loss 0.581878.
Train: 2018-07-31T11:14:26.839504: step 549, loss 0.604224.
Train: 2018-07-31T11:14:27.026960: step 550, loss 0.621373.
Test: 2018-07-31T11:14:27.495630: step 550, loss 0.559062.
Train: 2018-07-31T11:14:27.667465: step 551, loss 0.511416.
Train: 2018-07-31T11:14:27.854892: step 552, loss 0.625503.
Train: 2018-07-31T11:14:28.042377: step 553, loss 0.53833.
Train: 2018-07-31T11:14:28.229834: step 554, loss 0.590087.
Train: 2018-07-31T11:14:28.417259: step 555, loss 0.581679.
Train: 2018-07-31T11:14:28.604716: step 556, loss 0.624148.
Train: 2018-07-31T11:14:28.776551: step 557, loss 0.558951.
Train: 2018-07-31T11:14:28.964006: step 558, loss 0.597837.
Train: 2018-07-31T11:14:29.151463: step 559, loss 0.602708.
Train: 2018-07-31T11:14:29.323328: step 560, loss 0.618479.
Test: 2018-07-31T11:14:29.791938: step 560, loss 0.558982.
Train: 2018-07-31T11:14:29.979394: step 561, loss 0.590427.
Train: 2018-07-31T11:14:30.166850: step 562, loss 0.554136.
Train: 2018-07-31T11:14:30.354307: step 563, loss 0.513774.
Train: 2018-07-31T11:14:30.526140: step 564, loss 0.565404.
Train: 2018-07-31T11:14:30.713628: step 565, loss 0.608186.
Train: 2018-07-31T11:14:30.901077: step 566, loss 0.566057.
Train: 2018-07-31T11:14:31.088508: step 567, loss 0.608011.
Train: 2018-07-31T11:14:31.275965: step 568, loss 0.607808.
Train: 2018-07-31T11:14:31.447830: step 569, loss 0.574221.
Train: 2018-07-31T11:14:31.635255: step 570, loss 0.548247.
Test: 2018-07-31T11:14:32.119548: step 570, loss 0.559144.
Train: 2018-07-31T11:14:32.291383: step 571, loss 0.615941.
Train: 2018-07-31T11:14:32.478809: step 572, loss 0.54797.
Train: 2018-07-31T11:14:32.666265: step 573, loss 0.590193.
Train: 2018-07-31T11:14:32.838130: step 574, loss 0.555357.
Train: 2018-07-31T11:14:33.025556: step 575, loss 0.599139.
Train: 2018-07-31T11:14:33.213012: step 576, loss 0.530773.
Train: 2018-07-31T11:14:33.384846: step 577, loss 0.544622.
Train: 2018-07-31T11:14:33.572332: step 578, loss 0.582382.
Train: 2018-07-31T11:14:33.744138: step 579, loss 0.581672.
Train: 2018-07-31T11:14:33.931623: step 580, loss 0.513817.
Test: 2018-07-31T11:14:34.415884: step 580, loss 0.558941.
Train: 2018-07-31T11:14:34.587721: step 581, loss 0.556267.
Train: 2018-07-31T11:14:34.775177: step 582, loss 0.513579.
Train: 2018-07-31T11:14:34.962602: step 583, loss 0.590306.
Train: 2018-07-31T11:14:35.134436: step 584, loss 0.521758.
Train: 2018-07-31T11:14:35.321922: step 585, loss 0.547426.
Train: 2018-07-31T11:14:35.509379: step 586, loss 0.633384.
Train: 2018-07-31T11:14:35.696830: step 587, loss 0.57313.
Train: 2018-07-31T11:14:35.884285: step 588, loss 0.598953.
Train: 2018-07-31T11:14:36.071747: step 589, loss 0.642405.
Train: 2018-07-31T11:14:36.243577: step 590, loss 0.52989.
Test: 2018-07-31T11:14:36.712192: step 590, loss 0.55853.
Train: 2018-07-31T11:14:36.899678: step 591, loss 0.616377.
Train: 2018-07-31T11:14:37.087135: step 592, loss 0.607606.
Train: 2018-07-31T11:14:37.258969: step 593, loss 0.633278.
Train: 2018-07-31T11:14:37.462017: step 594, loss 0.607366.
Train: 2018-07-31T11:14:37.633882: step 595, loss 0.564314.
Train: 2018-07-31T11:14:37.821307: step 596, loss 0.589985.
Train: 2018-07-31T11:14:37.993175: step 597, loss 0.555571.
Train: 2018-07-31T11:14:38.180598: step 598, loss 0.675219.
Train: 2018-07-31T11:14:38.352433: step 599, loss 0.615677.
Train: 2018-07-31T11:14:38.524299: step 600, loss 0.60746.
Test: 2018-07-31T11:14:38.992938: step 600, loss 0.558458.
Train: 2018-07-31T11:14:39.742763: step 601, loss 0.515706.
Train: 2018-07-31T11:14:39.914597: step 602, loss 0.598254.
Train: 2018-07-31T11:14:40.102054: step 603, loss 0.555765.
Train: 2018-07-31T11:14:40.258266: step 604, loss 0.611652.
Train: 2018-07-31T11:14:40.445692: step 605, loss 0.640479.
Train: 2018-07-31T11:14:40.633179: step 606, loss 0.597746.
Train: 2018-07-31T11:14:40.820604: step 607, loss 0.587408.
Train: 2018-07-31T11:14:40.992441: step 608, loss 0.547812.
Train: 2018-07-31T11:14:41.179926: step 609, loss 0.624919.
Train: 2018-07-31T11:14:41.367351: step 610, loss 0.530269.
Test: 2018-07-31T11:14:41.851613: step 610, loss 0.558313.
Train: 2018-07-31T11:14:42.039071: step 611, loss 0.604793.
Train: 2018-07-31T11:14:42.210905: step 612, loss 0.622707.
Train: 2018-07-31T11:14:42.382769: step 613, loss 0.617507.
Train: 2018-07-31T11:14:42.570225: step 614, loss 0.605158.
Train: 2018-07-31T11:14:42.757675: step 615, loss 0.574397.
Train: 2018-07-31T11:14:42.929487: step 616, loss 0.580793.
Train: 2018-07-31T11:14:43.116974: step 617, loss 0.621185.
Train: 2018-07-31T11:14:43.288807: step 618, loss 0.620914.
Train: 2018-07-31T11:14:43.476233: step 619, loss 0.5487.
Train: 2018-07-31T11:14:43.663690: step 620, loss 0.500545.
Test: 2018-07-31T11:14:44.132330: step 620, loss 0.559159.
Train: 2018-07-31T11:14:44.319785: step 621, loss 0.516433.
Train: 2018-07-31T11:14:44.491620: step 622, loss 0.588869.
Train: 2018-07-31T11:14:44.679076: step 623, loss 0.55654.
Train: 2018-07-31T11:14:44.850936: step 624, loss 0.628913.
Train: 2018-07-31T11:14:45.038368: step 625, loss 0.580588.
Train: 2018-07-31T11:14:45.210202: step 626, loss 0.532232.
Train: 2018-07-31T11:14:45.382068: step 627, loss 0.57258.
Train: 2018-07-31T11:14:45.569493: step 628, loss 0.548109.
Train: 2018-07-31T11:14:45.756979: step 629, loss 0.531777.
Train: 2018-07-31T11:14:45.928808: step 630, loss 0.556177.
Test: 2018-07-31T11:14:46.397424: step 630, loss 0.558567.
Train: 2018-07-31T11:14:46.584911: step 631, loss 0.555931.
Train: 2018-07-31T11:14:46.756745: step 632, loss 0.596817.
Train: 2018-07-31T11:14:46.944204: step 633, loss 0.572161.
Train: 2018-07-31T11:14:47.116036: step 634, loss 0.555713.
Train: 2018-07-31T11:14:47.303492: step 635, loss 0.563715.
Train: 2018-07-31T11:14:47.475296: step 636, loss 0.580153.
Train: 2018-07-31T11:14:47.647131: step 637, loss 0.464027.
Train: 2018-07-31T11:14:47.834619: step 638, loss 0.471774.
Train: 2018-07-31T11:14:48.022074: step 639, loss 0.454275.
Train: 2018-07-31T11:14:48.209531: step 640, loss 0.520904.
Test: 2018-07-31T11:14:48.678140: step 640, loss 0.557292.
Train: 2018-07-31T11:14:48.865626: step 641, loss 0.656779.
Train: 2018-07-31T11:14:49.037455: step 642, loss 0.588642.
Train: 2018-07-31T11:14:49.224918: step 643, loss 0.55427.
Train: 2018-07-31T11:14:49.396722: step 644, loss 0.56278.
Train: 2018-07-31T11:14:49.568587: step 645, loss 0.623428.
Train: 2018-07-31T11:14:49.740391: step 646, loss 0.51915.
Train: 2018-07-31T11:14:49.927847: step 647, loss 0.545174.
Train: 2018-07-31T11:14:50.099682: step 648, loss 0.562687.
Train: 2018-07-31T11:14:50.287139: step 649, loss 0.544937.
Train: 2018-07-31T11:14:50.458973: step 650, loss 0.676892.
Test: 2018-07-31T11:14:50.927643: step 650, loss 0.55646.
Train: 2018-07-31T11:14:51.115100: step 651, loss 0.544788.
Train: 2018-07-31T11:14:51.286905: step 652, loss 0.553606.
Train: 2018-07-31T11:14:51.458770: step 653, loss 0.615558.
Train: 2018-07-31T11:14:51.646225: step 654, loss 0.615384.
Train: 2018-07-31T11:14:51.833682: step 655, loss 0.579932.
Train: 2018-07-31T11:14:52.005516: step 656, loss 0.606356.
Train: 2018-07-31T11:14:52.177351: step 657, loss 0.527103.
Train: 2018-07-31T11:14:52.349180: step 658, loss 0.509321.
Train: 2018-07-31T11:14:52.520991: step 659, loss 0.623768.
Train: 2018-07-31T11:14:52.692826: step 660, loss 0.544583.
Test: 2018-07-31T11:14:53.161465: step 660, loss 0.55619.
Train: 2018-07-31T11:14:53.333301: step 661, loss 0.632201.
Train: 2018-07-31T11:14:53.520781: step 662, loss 0.518673.
Train: 2018-07-31T11:14:53.692621: step 663, loss 0.545026.
Train: 2018-07-31T11:14:53.864426: step 664, loss 0.492763.
Train: 2018-07-31T11:14:54.051882: step 665, loss 0.60471.
Train: 2018-07-31T11:14:54.223717: step 666, loss 0.480787.
Train: 2018-07-31T11:14:54.395581: step 667, loss 0.58122.
Train: 2018-07-31T11:14:54.567387: step 668, loss 0.561833.
Train: 2018-07-31T11:14:54.754843: step 669, loss 0.533765.
Train: 2018-07-31T11:14:54.942328: step 670, loss 0.729763.
Test: 2018-07-31T11:14:55.410968: step 670, loss 0.556543.
Train: 2018-07-31T11:14:55.582805: step 671, loss 0.61709.
Train: 2018-07-31T11:14:55.770259: step 672, loss 0.596282.
Train: 2018-07-31T11:14:55.942098: step 673, loss 0.56026.
Train: 2018-07-31T11:14:56.129545: step 674, loss 0.550779.
Train: 2018-07-31T11:14:56.301356: step 675, loss 0.613801.
Train: 2018-07-31T11:14:56.473214: step 676, loss 0.539937.
Train: 2018-07-31T11:14:56.660676: step 677, loss 0.560652.
Train: 2018-07-31T11:14:56.832512: step 678, loss 0.632555.
Train: 2018-07-31T11:14:57.004315: step 679, loss 0.544672.
Train: 2018-07-31T11:14:57.191772: step 680, loss 0.571594.
Test: 2018-07-31T11:14:57.676044: step 680, loss 0.556491.
Train: 2018-07-31T11:14:57.847868: step 681, loss 0.536443.
Train: 2018-07-31T11:14:58.035324: step 682, loss 0.571401.
Train: 2018-07-31T11:14:58.207158: step 683, loss 0.513217.
Train: 2018-07-31T11:14:58.394645: step 684, loss 0.518675.
Train: 2018-07-31T11:14:58.566479: step 685, loss 0.53645.
Train: 2018-07-31T11:14:58.753905: step 686, loss 0.54232.
Train: 2018-07-31T11:14:58.941392: step 687, loss 0.62046.
Train: 2018-07-31T11:14:59.128848: step 688, loss 0.566749.
Train: 2018-07-31T11:14:59.300682: step 689, loss 0.573166.
Train: 2018-07-31T11:14:59.488139: step 690, loss 0.628434.
Test: 2018-07-31T11:14:59.972401: step 690, loss 0.556585.
Train: 2018-07-31T11:15:00.159856: step 691, loss 0.490941.
Train: 2018-07-31T11:15:00.347283: step 692, loss 0.526774.
Train: 2018-07-31T11:15:00.519147: step 693, loss 0.561606.
Train: 2018-07-31T11:15:00.706573: step 694, loss 0.587253.
Train: 2018-07-31T11:15:00.894059: step 695, loss 0.563253.
Train: 2018-07-31T11:15:01.081517: step 696, loss 0.546044.
Train: 2018-07-31T11:15:01.268941: step 697, loss 0.597848.
Train: 2018-07-31T11:15:01.456429: step 698, loss 0.559043.
Train: 2018-07-31T11:15:01.628232: step 699, loss 0.58293.
Train: 2018-07-31T11:15:01.815689: step 700, loss 0.572143.
Test: 2018-07-31T11:15:02.299999: step 700, loss 0.556639.
Train: 2018-07-31T11:15:03.034184: step 701, loss 0.605968.
Train: 2018-07-31T11:15:03.221640: step 702, loss 0.571512.
Train: 2018-07-31T11:15:03.409064: step 703, loss 0.615514.
Train: 2018-07-31T11:15:03.580900: step 704, loss 0.539172.
Train: 2018-07-31T11:15:03.768356: step 705, loss 0.563234.
Train: 2018-07-31T11:15:03.955812: step 706, loss 0.534076.
Train: 2018-07-31T11:15:04.127648: step 707, loss 0.562171.
Train: 2018-07-31T11:15:04.315128: step 708, loss 0.483042.
Train: 2018-07-31T11:15:04.502590: step 709, loss 0.606297.
Train: 2018-07-31T11:15:04.674394: step 710, loss 0.615419.
Test: 2018-07-31T11:15:05.143064: step 710, loss 0.55632.
Train: 2018-07-31T11:15:05.346143: step 711, loss 0.491521.
Train: 2018-07-31T11:15:05.533568: step 712, loss 0.553504.
Train: 2018-07-31T11:15:05.721024: step 713, loss 0.552863.
Train: 2018-07-31T11:15:05.892888: step 714, loss 0.660808.
Train: 2018-07-31T11:15:06.080346: step 715, loss 0.581008.
Train: 2018-07-31T11:15:06.267801: step 716, loss 0.598722.
Train: 2018-07-31T11:15:06.455227: step 717, loss 0.543667.
Train: 2018-07-31T11:15:06.627061: step 718, loss 0.642069.
Train: 2018-07-31T11:15:06.814519: step 719, loss 0.606539.
Train: 2018-07-31T11:15:07.017626: step 720, loss 0.530051.
Test: 2018-07-31T11:15:07.486235: step 720, loss 0.556335.
Train: 2018-07-31T11:15:07.673723: step 721, loss 0.526018.
Train: 2018-07-31T11:15:07.845527: step 722, loss 0.588904.
Train: 2018-07-31T11:15:08.033007: step 723, loss 0.65176.
Train: 2018-07-31T11:15:08.220470: step 724, loss 0.529313.
Train: 2018-07-31T11:15:08.407926: step 725, loss 0.529654.
Train: 2018-07-31T11:15:08.579761: step 726, loss 0.610355.
Train: 2018-07-31T11:15:08.767186: step 727, loss 0.62383.
Train: 2018-07-31T11:15:08.954678: step 728, loss 0.520499.
Train: 2018-07-31T11:15:09.142128: step 729, loss 0.622898.
Train: 2018-07-31T11:15:09.313963: step 730, loss 0.536588.
Test: 2018-07-31T11:15:09.782603: step 730, loss 0.557455.
Train: 2018-07-31T11:15:09.970029: step 731, loss 0.604159.
Train: 2018-07-31T11:15:10.157485: step 732, loss 0.649267.
Train: 2018-07-31T11:15:10.344966: step 733, loss 0.562088.
Train: 2018-07-31T11:15:10.516777: step 734, loss 0.58809.
Train: 2018-07-31T11:15:10.704257: step 735, loss 0.553349.
Train: 2018-07-31T11:15:10.907340: step 736, loss 0.570732.
Train: 2018-07-31T11:15:11.079145: step 737, loss 0.631132.
Train: 2018-07-31T11:15:11.266600: step 738, loss 0.639684.
Train: 2018-07-31T11:15:11.454088: step 739, loss 0.579168.
Train: 2018-07-31T11:15:11.641512: step 740, loss 0.58765.
Test: 2018-07-31T11:15:12.125804: step 740, loss 0.556372.
Train: 2018-07-31T11:15:12.313231: step 741, loss 0.562096.
Train: 2018-07-31T11:15:12.485065: step 742, loss 0.536537.
Train: 2018-07-31T11:15:12.672546: step 743, loss 0.570062.
Train: 2018-07-31T11:15:12.860008: step 744, loss 0.544057.
Train: 2018-07-31T11:15:13.047458: step 745, loss 0.564691.
Train: 2018-07-31T11:15:13.234920: step 746, loss 0.601695.
Train: 2018-07-31T11:15:13.406725: step 747, loss 0.608575.
Train: 2018-07-31T11:15:13.594181: step 748, loss 0.587537.
Train: 2018-07-31T11:15:13.766016: step 749, loss 0.579007.
Train: 2018-07-31T11:15:13.953471: step 750, loss 0.461355.
Test: 2018-07-31T11:15:14.422142: step 750, loss 0.556572.
Train: 2018-07-31T11:15:14.609597: step 751, loss 0.612643.
Train: 2018-07-31T11:15:14.797023: step 752, loss 0.562206.
Train: 2018-07-31T11:15:14.984511: step 753, loss 0.604347.
Train: 2018-07-31T11:15:15.156315: step 754, loss 0.562347.
Train: 2018-07-31T11:15:15.328181: step 755, loss 0.499063.
Train: 2018-07-31T11:15:15.515636: step 756, loss 0.562294.
Train: 2018-07-31T11:15:15.703062: step 757, loss 0.511852.
Train: 2018-07-31T11:15:15.890548: step 758, loss 0.612959.
Train: 2018-07-31T11:15:16.078005: step 759, loss 0.511592.
Train: 2018-07-31T11:15:16.249808: step 760, loss 0.63021.
Test: 2018-07-31T11:15:16.734100: step 760, loss 0.556553.
Train: 2018-07-31T11:15:16.921556: step 761, loss 0.596187.
Train: 2018-07-31T11:15:17.109013: step 762, loss 0.604613.
Train: 2018-07-31T11:15:17.296468: step 763, loss 0.57917.
Train: 2018-07-31T11:15:17.468304: step 764, loss 0.56228.
Train: 2018-07-31T11:15:17.655729: step 765, loss 0.579096.
Train: 2018-07-31T11:15:17.827564: step 766, loss 0.570655.
Train: 2018-07-31T11:15:18.015050: step 767, loss 0.536593.
Train: 2018-07-31T11:15:18.202477: step 768, loss 0.655539.
Train: 2018-07-31T11:15:18.374341: step 769, loss 0.519696.
Train: 2018-07-31T11:15:18.561791: step 770, loss 0.519624.
Test: 2018-07-31T11:15:19.030437: step 770, loss 0.556246.
Train: 2018-07-31T11:15:19.217863: step 771, loss 0.511021.
Train: 2018-07-31T11:15:19.405352: step 772, loss 0.553407.
Train: 2018-07-31T11:15:19.577154: step 773, loss 0.621513.
Train: 2018-07-31T11:15:19.764610: step 774, loss 0.527688.
Train: 2018-07-31T11:15:19.952067: step 775, loss 0.578868.
Train: 2018-07-31T11:15:20.139522: step 776, loss 0.544624.
Train: 2018-07-31T11:15:20.327004: step 777, loss 0.596173.
Train: 2018-07-31T11:15:20.498843: step 778, loss 0.587294.
Train: 2018-07-31T11:15:20.686270: step 779, loss 0.621824.
Train: 2018-07-31T11:15:20.858105: step 780, loss 0.518515.
Test: 2018-07-31T11:15:21.326777: step 780, loss 0.555618.
Train: 2018-07-31T11:15:21.514200: step 781, loss 0.587245.
Train: 2018-07-31T11:15:21.686066: step 782, loss 0.561361.
Train: 2018-07-31T11:15:21.873491: step 783, loss 0.56135.
Train: 2018-07-31T11:15:22.045326: step 784, loss 0.492355.
Train: 2018-07-31T11:15:22.232783: step 785, loss 0.639098.
Train: 2018-07-31T11:15:22.404617: step 786, loss 0.612985.
Train: 2018-07-31T11:15:22.592104: step 787, loss 0.457612.
Train: 2018-07-31T11:15:22.763938: step 788, loss 0.630176.
Train: 2018-07-31T11:15:22.951388: step 789, loss 0.561158.
Train: 2018-07-31T11:15:23.123229: step 790, loss 0.561028.
Test: 2018-07-31T11:15:23.607490: step 790, loss 0.555252.
Train: 2018-07-31T11:15:23.794947: step 791, loss 0.57896.
Train: 2018-07-31T11:15:23.966782: step 792, loss 0.491786.
Train: 2018-07-31T11:15:24.154238: step 793, loss 0.552183.
Train: 2018-07-31T11:15:24.341694: step 794, loss 0.517839.
Train: 2018-07-31T11:15:24.513499: step 795, loss 0.578123.
Train: 2018-07-31T11:15:24.700954: step 796, loss 0.508213.
Train: 2018-07-31T11:15:24.872813: step 797, loss 0.614159.
Train: 2018-07-31T11:15:25.060276: step 798, loss 0.595878.
Train: 2018-07-31T11:15:25.232079: step 799, loss 0.621312.
Train: 2018-07-31T11:15:25.403914: step 800, loss 0.604743.
Test: 2018-07-31T11:15:25.872585: step 800, loss 0.555019.
Train: 2018-07-31T11:15:26.669244: step 801, loss 0.551259.
Train: 2018-07-31T11:15:26.872348: step 802, loss 0.622538.
Train: 2018-07-31T11:15:27.075399: step 803, loss 0.546094.
Train: 2018-07-31T11:15:27.262879: step 804, loss 0.562061.
Train: 2018-07-31T11:15:27.434690: step 805, loss 0.611409.
Train: 2018-07-31T11:15:27.606524: step 806, loss 0.527479.
Train: 2018-07-31T11:15:27.794005: step 807, loss 0.511864.
Train: 2018-07-31T11:15:27.965814: step 808, loss 0.550342.
Train: 2018-07-31T11:15:28.153295: step 809, loss 0.496655.
Train: 2018-07-31T11:15:28.325105: step 810, loss 0.552451.
Test: 2018-07-31T11:15:28.809368: step 810, loss 0.555178.
Train: 2018-07-31T11:15:28.981203: step 811, loss 0.596311.
Train: 2018-07-31T11:15:29.168658: step 812, loss 0.674448.
Train: 2018-07-31T11:15:29.356115: step 813, loss 0.569479.
Train: 2018-07-31T11:15:29.527979: step 814, loss 0.543927.
Train: 2018-07-31T11:15:29.715405: step 815, loss 0.579452.
Train: 2018-07-31T11:15:29.902862: step 816, loss 0.508793.
Train: 2018-07-31T11:15:30.074727: step 817, loss 0.657266.
Train: 2018-07-31T11:15:30.262176: step 818, loss 0.561288.
Train: 2018-07-31T11:15:30.434017: step 819, loss 0.605086.
Train: 2018-07-31T11:15:30.605846: step 820, loss 0.569882.
Test: 2018-07-31T11:15:31.074492: step 820, loss 0.555385.
Train: 2018-07-31T11:15:31.261918: step 821, loss 0.535125.
Train: 2018-07-31T11:15:31.449399: step 822, loss 0.613694.
Train: 2018-07-31T11:15:31.621239: step 823, loss 0.570015.
Train: 2018-07-31T11:15:31.793077: step 824, loss 0.517982.
Train: 2018-07-31T11:15:31.980500: step 825, loss 0.56146.
Train: 2018-07-31T11:15:32.152335: step 826, loss 0.578654.
Train: 2018-07-31T11:15:32.339821: step 827, loss 0.535408.
Train: 2018-07-31T11:15:32.511626: step 828, loss 0.535364.
Train: 2018-07-31T11:15:32.699082: step 829, loss 0.561301.
Train: 2018-07-31T11:15:32.870917: step 830, loss 0.543916.
Test: 2018-07-31T11:15:33.355178: step 830, loss 0.555425.
Train: 2018-07-31T11:15:33.527012: step 831, loss 0.543948.
Train: 2018-07-31T11:15:33.698878: step 832, loss 0.552471.
Train: 2018-07-31T11:15:33.886334: step 833, loss 0.63105.
Train: 2018-07-31T11:15:34.058138: step 834, loss 0.561354.
Train: 2018-07-31T11:15:34.245594: step 835, loss 0.569916.
Train: 2018-07-31T11:15:34.417428: step 836, loss 0.657214.
Train: 2018-07-31T11:15:34.589263: step 837, loss 0.561263.
Train: 2018-07-31T11:15:34.776720: step 838, loss 0.569779.
Train: 2018-07-31T11:15:34.948586: step 839, loss 0.508945.
Train: 2018-07-31T11:15:35.120421: step 840, loss 0.552348.
Test: 2018-07-31T11:15:35.604651: step 840, loss 0.555177.
Train: 2018-07-31T11:15:35.792133: step 841, loss 0.560966.
Train: 2018-07-31T11:15:35.963974: step 842, loss 0.51734.
Train: 2018-07-31T11:15:36.151428: step 843, loss 0.545041.
Train: 2018-07-31T11:15:36.338878: step 844, loss 0.631128.
Train: 2018-07-31T11:15:36.510719: step 845, loss 0.525925.
Train: 2018-07-31T11:15:36.682523: step 846, loss 0.630785.
Train: 2018-07-31T11:15:36.869979: step 847, loss 0.569762.
Train: 2018-07-31T11:15:37.041814: step 848, loss 0.534457.
Train: 2018-07-31T11:15:37.213649: step 849, loss 0.587105.
Train: 2018-07-31T11:15:37.401105: step 850, loss 0.604382.
Test: 2018-07-31T11:15:37.869745: step 850, loss 0.555046.
Train: 2018-07-31T11:15:38.057232: step 851, loss 0.551612.
Train: 2018-07-31T11:15:38.229067: step 852, loss 0.560349.
Train: 2018-07-31T11:15:38.416523: step 853, loss 0.665964.
Train: 2018-07-31T11:15:38.588327: step 854, loss 0.605435.
Train: 2018-07-31T11:15:38.760192: step 855, loss 0.549863.
Train: 2018-07-31T11:15:38.947619: step 856, loss 0.549865.
Train: 2018-07-31T11:15:39.135104: step 857, loss 0.528404.
Train: 2018-07-31T11:15:39.322532: step 858, loss 0.562701.
Train: 2018-07-31T11:15:39.494395: step 859, loss 0.532583.
Train: 2018-07-31T11:15:39.681851: step 860, loss 0.551251.
Test: 2018-07-31T11:15:40.150463: step 860, loss 0.555004.
Train: 2018-07-31T11:15:40.337918: step 861, loss 0.586856.
Train: 2018-07-31T11:15:40.509752: step 862, loss 0.561894.
Train: 2018-07-31T11:15:40.697209: step 863, loss 0.543558.
Train: 2018-07-31T11:15:40.884667: step 864, loss 0.543758.
Train: 2018-07-31T11:15:41.056499: step 865, loss 0.604282.
Train: 2018-07-31T11:15:41.243980: step 866, loss 0.483458.
Train: 2018-07-31T11:15:41.431412: step 867, loss 0.543771.
Train: 2018-07-31T11:15:41.603246: step 868, loss 0.629581.
Train: 2018-07-31T11:15:41.790732: step 869, loss 0.596902.
Train: 2018-07-31T11:15:41.962537: step 870, loss 0.647509.
Test: 2018-07-31T11:15:42.431176: step 870, loss 0.555179.
Train: 2018-07-31T11:15:42.618634: step 871, loss 0.569946.
Train: 2018-07-31T11:15:42.790501: step 872, loss 0.561592.
Train: 2018-07-31T11:15:42.977949: step 873, loss 0.595727.
Train: 2018-07-31T11:15:43.165380: step 874, loss 0.561078.
Train: 2018-07-31T11:15:43.337215: step 875, loss 0.535422.
Train: 2018-07-31T11:15:43.509079: step 876, loss 0.569679.
Train: 2018-07-31T11:15:43.696505: step 877, loss 0.646234.
Train: 2018-07-31T11:15:43.868371: step 878, loss 0.620944.
Train: 2018-07-31T11:15:44.055827: step 879, loss 0.543919.
Train: 2018-07-31T11:15:44.227662: step 880, loss 0.586447.
Test: 2018-07-31T11:15:44.711924: step 880, loss 0.555137.
Train: 2018-07-31T11:15:44.883728: step 881, loss 0.594731.
Train: 2018-07-31T11:15:45.055594: step 882, loss 0.569544.
Train: 2018-07-31T11:15:45.243019: step 883, loss 0.552183.
Train: 2018-07-31T11:15:45.414887: step 884, loss 0.552366.
Train: 2018-07-31T11:15:45.602309: step 885, loss 0.619803.
Train: 2018-07-31T11:15:45.774144: step 886, loss 0.484739.
Train: 2018-07-31T11:15:45.946004: step 887, loss 0.611603.
Train: 2018-07-31T11:15:46.133463: step 888, loss 0.526789.
Train: 2018-07-31T11:15:46.305270: step 889, loss 0.525785.
Train: 2018-07-31T11:15:46.492727: step 890, loss 0.473532.
Test: 2018-07-31T11:15:46.961397: step 890, loss 0.554223.
Train: 2018-07-31T11:15:47.148853: step 891, loss 0.580495.
Train: 2018-07-31T11:15:47.336309: step 892, loss 0.663904.
Train: 2018-07-31T11:15:47.508143: step 893, loss 0.591902.
Train: 2018-07-31T11:15:47.695601: step 894, loss 0.567012.
Train: 2018-07-31T11:15:47.867404: step 895, loss 0.549362.
Train: 2018-07-31T11:15:48.054861: step 896, loss 0.622838.
Train: 2018-07-31T11:15:48.242317: step 897, loss 0.603587.
Train: 2018-07-31T11:15:48.414151: step 898, loss 0.537641.
Train: 2018-07-31T11:15:48.601607: step 899, loss 0.611585.
Train: 2018-07-31T11:15:48.789094: step 900, loss 0.674529.
Test: 2018-07-31T11:15:49.257704: step 900, loss 0.554963.
Train: 2018-07-31T11:15:49.960695: step 901, loss 0.533777.
Train: 2018-07-31T11:15:50.148120: step 902, loss 0.540718.
Train: 2018-07-31T11:15:50.319954: step 903, loss 0.589207.
Train: 2018-07-31T11:15:50.507412: step 904, loss 0.584375.
Train: 2018-07-31T11:15:50.679277: step 905, loss 0.59131.
Train: 2018-07-31T11:15:50.851081: step 906, loss 0.428724.
Train: 2018-07-31T11:15:51.022916: step 907, loss 0.550923.
Train: 2018-07-31T11:15:51.210371: step 908, loss 0.57913.
Train: 2018-07-31T11:15:51.382206: step 909, loss 0.612945.
Train: 2018-07-31T11:15:51.569694: step 910, loss 0.600192.
Test: 2018-07-31T11:15:52.053923: step 910, loss 0.55544.
Train: 2018-07-31T11:15:52.225758: step 911, loss 0.512347.
Train: 2018-07-31T11:15:52.397624: step 912, loss 0.572422.
Train: 2018-07-31T11:15:52.585049: step 913, loss 0.553691.
Train: 2018-07-31T11:15:52.756914: step 914, loss 0.662654.
Train: 2018-07-31T11:15:52.928719: step 915, loss 0.579317.
Train: 2018-07-31T11:15:53.116176: step 916, loss 0.543707.
Train: 2018-07-31T11:15:53.303631: step 917, loss 0.51004.
Train: 2018-07-31T11:15:53.475496: step 918, loss 0.585669.
Train: 2018-07-31T11:15:53.647331: step 919, loss 0.543937.
Train: 2018-07-31T11:15:53.834757: step 920, loss 0.510103.
Test: 2018-07-31T11:15:54.303398: step 920, loss 0.555442.
Train: 2018-07-31T11:15:54.490853: step 921, loss 0.571925.
Train: 2018-07-31T11:15:54.662688: step 922, loss 0.617126.
Train: 2018-07-31T11:15:54.850145: step 923, loss 0.509389.
Train: 2018-07-31T11:15:55.022009: step 924, loss 0.58573.
Train: 2018-07-31T11:15:55.209435: step 925, loss 0.519224.
Train: 2018-07-31T11:15:55.396921: step 926, loss 0.612608.
Train: 2018-07-31T11:15:55.568757: step 927, loss 0.59611.
Train: 2018-07-31T11:15:55.740591: step 928, loss 0.542849.
Train: 2018-07-31T11:15:55.912426: step 929, loss 0.543641.
Train: 2018-07-31T11:15:56.099881: step 930, loss 0.509162.
Test: 2018-07-31T11:15:56.584144: step 930, loss 0.554907.
Train: 2018-07-31T11:15:56.755973: step 931, loss 0.594179.
Train: 2018-07-31T11:15:56.927783: step 932, loss 0.612563.
Train: 2018-07-31T11:15:57.115238: step 933, loss 0.501858.
Train: 2018-07-31T11:15:57.302726: step 934, loss 0.560502.
Train: 2018-07-31T11:15:57.490151: step 935, loss 0.552765.
Train: 2018-07-31T11:15:57.661985: step 936, loss 0.52566.
Train: 2018-07-31T11:15:57.849443: step 937, loss 0.543214.
Train: 2018-07-31T11:15:58.021307: step 938, loss 0.525646.
Train: 2018-07-31T11:15:58.208732: step 939, loss 0.657011.
Train: 2018-07-31T11:15:58.380567: step 940, loss 0.612861.
Test: 2018-07-31T11:15:58.849208: step 940, loss 0.554419.
Train: 2018-07-31T11:15:59.036694: step 941, loss 0.55939.
Train: 2018-07-31T11:15:59.224121: step 942, loss 0.543296.
Train: 2018-07-31T11:15:59.395955: step 943, loss 0.674498.
Train: 2018-07-31T11:15:59.583410: step 944, loss 0.586859.
Train: 2018-07-31T11:15:59.755276: step 945, loss 0.568602.
Train: 2018-07-31T11:15:59.942701: step 946, loss 0.551498.
Train: 2018-07-31T11:16:00.114537: step 947, loss 0.594286.
Train: 2018-07-31T11:16:00.302017: step 948, loss 0.674059.
Train: 2018-07-31T11:16:00.489479: step 949, loss 0.55147.
Train: 2018-07-31T11:16:00.661284: step 950, loss 0.62171.
Test: 2018-07-31T11:16:01.145575: step 950, loss 0.554371.
Train: 2018-07-31T11:16:01.317411: step 951, loss 0.500006.
Train: 2018-07-31T11:16:01.504837: step 952, loss 0.517118.
Train: 2018-07-31T11:16:01.676671: step 953, loss 0.541646.
Train: 2018-07-31T11:16:01.848535: step 954, loss 0.594763.
Train: 2018-07-31T11:16:02.035964: step 955, loss 0.551502.
Train: 2018-07-31T11:16:02.207827: step 956, loss 0.594293.
Train: 2018-07-31T11:16:02.379630: step 957, loss 0.524526.
Train: 2018-07-31T11:16:02.567088: step 958, loss 0.611812.
Train: 2018-07-31T11:16:02.738921: step 959, loss 0.576117.
Train: 2018-07-31T11:16:02.926377: step 960, loss 0.502191.
Test: 2018-07-31T11:16:03.395048: step 960, loss 0.554367.
Train: 2018-07-31T11:16:03.566852: step 961, loss 0.592956.
Train: 2018-07-31T11:16:03.754339: step 962, loss 0.499738.
Train: 2018-07-31T11:16:03.926145: step 963, loss 0.614561.
Train: 2018-07-31T11:16:04.098009: step 964, loss 0.530769.
Train: 2018-07-31T11:16:04.285465: step 965, loss 0.622533.
Train: 2018-07-31T11:16:04.457269: step 966, loss 0.649734.
Train: 2018-07-31T11:16:04.629105: step 967, loss 0.560013.
Train: 2018-07-31T11:16:04.800939: step 968, loss 0.603424.
Train: 2018-07-31T11:16:04.988426: step 969, loss 0.508518.
Train: 2018-07-31T11:16:05.160260: step 970, loss 0.58438.
Test: 2018-07-31T11:16:05.628870: step 970, loss 0.554338.
Train: 2018-07-31T11:16:05.816327: step 971, loss 0.590004.
Train: 2018-07-31T11:16:06.003782: step 972, loss 0.510179.
Train: 2018-07-31T11:16:06.191270: step 973, loss 0.537059.
Train: 2018-07-31T11:16:06.409968: step 974, loss 0.603454.
Train: 2018-07-31T11:16:06.581771: step 975, loss 0.54042.
Train: 2018-07-31T11:16:06.769227: step 976, loss 0.569573.
Train: 2018-07-31T11:16:06.941063: step 977, loss 0.551554.
Train: 2018-07-31T11:16:07.112898: step 978, loss 0.577282.
Train: 2018-07-31T11:16:07.284733: step 979, loss 0.517383.
Train: 2018-07-31T11:16:07.472213: step 980, loss 0.594823.
Test: 2018-07-31T11:16:07.956450: step 980, loss 0.554459.
Train: 2018-07-31T11:16:08.128309: step 981, loss 0.577291.
Train: 2018-07-31T11:16:08.300120: step 982, loss 0.508554.
Train: 2018-07-31T11:16:08.487606: step 983, loss 0.552752.
Train: 2018-07-31T11:16:08.675031: step 984, loss 0.510506.
Train: 2018-07-31T11:16:08.846900: step 985, loss 0.649477.
Train: 2018-07-31T11:16:09.018731: step 986, loss 0.628826.
Train: 2018-07-31T11:16:09.190562: step 987, loss 0.596697.
Train: 2018-07-31T11:16:09.377993: step 988, loss 0.535071.
Train: 2018-07-31T11:16:09.549827: step 989, loss 0.510153.
Train: 2018-07-31T11:16:09.721662: step 990, loss 0.655378.
Test: 2018-07-31T11:16:10.205954: step 990, loss 0.554562.
Train: 2018-07-31T11:16:10.424652: step 991, loss 0.603636.
Train: 2018-07-31T11:16:10.612109: step 992, loss 0.568999.
Train: 2018-07-31T11:16:10.783912: step 993, loss 0.551778.
Train: 2018-07-31T11:16:10.955748: step 994, loss 0.577957.
Train: 2018-07-31T11:16:11.143234: step 995, loss 0.542924.
Train: 2018-07-31T11:16:11.315038: step 996, loss 0.551512.
Train: 2018-07-31T11:16:11.502495: step 997, loss 0.482418.
Train: 2018-07-31T11:16:11.674330: step 998, loss 0.534561.
Train: 2018-07-31T11:16:11.846165: step 999, loss 0.551814.
Train: 2018-07-31T11:16:12.017999: step 1000, loss 0.612072.
Test: 2018-07-31T11:16:12.502290: step 1000, loss 0.554363.
Train: 2018-07-31T11:16:13.252085: step 1001, loss 0.611903.
Train: 2018-07-31T11:16:13.439542: step 1002, loss 0.576155.
Train: 2018-07-31T11:16:13.611406: step 1003, loss 0.569771.
Train: 2018-07-31T11:16:13.798831: step 1004, loss 0.562473.
Train: 2018-07-31T11:16:13.970667: step 1005, loss 0.679306.
Train: 2018-07-31T11:16:14.142531: step 1006, loss 0.560067.
Train: 2018-07-31T11:16:14.329988: step 1007, loss 0.499736.
Train: 2018-07-31T11:16:14.501823: step 1008, loss 0.577296.
Train: 2018-07-31T11:16:14.673627: step 1009, loss 0.525617.
Train: 2018-07-31T11:16:14.861083: step 1010, loss 0.499761.
Test: 2018-07-31T11:16:15.345359: step 1010, loss 0.554258.
Train: 2018-07-31T11:16:15.517179: step 1011, loss 0.577344.
Train: 2018-07-31T11:16:15.689046: step 1012, loss 0.542808.
Train: 2018-07-31T11:16:15.860880: step 1013, loss 0.534125.
Train: 2018-07-31T11:16:16.032684: step 1014, loss 0.560098.
Train: 2018-07-31T11:16:16.204519: step 1015, loss 0.499258.
Train: 2018-07-31T11:16:16.391975: step 1016, loss 0.560094.
Train: 2018-07-31T11:16:16.563810: step 1017, loss 0.533864.
Train: 2018-07-31T11:16:16.735675: step 1018, loss 0.638984.
Train: 2018-07-31T11:16:16.907479: step 1019, loss 0.551294.
Train: 2018-07-31T11:16:17.094935: step 1020, loss 0.489728.
Test: 2018-07-31T11:16:17.563606: step 1020, loss 0.554104.
Train: 2018-07-31T11:16:17.735440: step 1021, loss 0.612939.
Train: 2018-07-31T11:16:17.922868: step 1022, loss 0.66569.
Train: 2018-07-31T11:16:18.094700: step 1023, loss 0.578785.
Train: 2018-07-31T11:16:18.282157: step 1024, loss 0.533363.
Train: 2018-07-31T11:16:18.454023: step 1025, loss 0.561682.
Train: 2018-07-31T11:16:18.625827: step 1026, loss 0.568878.
Train: 2018-07-31T11:16:18.797692: step 1027, loss 0.507312.
Train: 2018-07-31T11:16:18.985118: step 1028, loss 0.56004.
Train: 2018-07-31T11:16:19.156982: step 1029, loss 0.560017.
Train: 2018-07-31T11:16:19.328787: step 1030, loss 0.621856.
Test: 2018-07-31T11:16:19.813080: step 1030, loss 0.553925.
Train: 2018-07-31T11:16:19.984914: step 1031, loss 0.559727.
Train: 2018-07-31T11:16:20.172364: step 1032, loss 0.568593.
Train: 2018-07-31T11:16:20.344205: step 1033, loss 0.568619.
Train: 2018-07-31T11:16:20.516010: step 1034, loss 0.595411.
Train: 2018-07-31T11:16:20.703466: step 1035, loss 0.612374.
Train: 2018-07-31T11:16:20.890921: step 1036, loss 0.603524.
Train: 2018-07-31T11:16:21.062787: step 1037, loss 0.559817.
Train: 2018-07-31T11:16:21.250236: step 1038, loss 0.472078.
Train: 2018-07-31T11:16:21.422047: step 1039, loss 0.63786.
Train: 2018-07-31T11:16:21.593911: step 1040, loss 0.603296.
Test: 2018-07-31T11:16:22.078174: step 1040, loss 0.553782.
Train: 2018-07-31T11:16:22.250008: step 1041, loss 0.595728.
Train: 2018-07-31T11:16:22.421813: step 1042, loss 0.525413.
Train: 2018-07-31T11:16:22.609269: step 1043, loss 0.602588.
Train: 2018-07-31T11:16:22.781104: step 1044, loss 0.57608.
Train: 2018-07-31T11:16:22.952969: step 1045, loss 0.558771.
Train: 2018-07-31T11:16:23.140396: step 1046, loss 0.594068.
Train: 2018-07-31T11:16:23.312261: step 1047, loss 0.518433.
Train: 2018-07-31T11:16:23.499686: step 1048, loss 0.602206.
Train: 2018-07-31T11:16:23.671520: step 1049, loss 0.611898.
Train: 2018-07-31T11:16:23.843355: step 1050, loss 0.61984.
Test: 2018-07-31T11:16:24.327617: step 1050, loss 0.554238.
Train: 2018-07-31T11:16:24.515103: step 1051, loss 0.559996.
Train: 2018-07-31T11:16:24.686938: step 1052, loss 0.568662.
Train: 2018-07-31T11:16:24.858742: step 1053, loss 0.577324.
Train: 2018-07-31T11:16:25.046228: step 1054, loss 0.577436.
Train: 2018-07-31T11:16:25.218034: step 1055, loss 0.611237.
Train: 2018-07-31T11:16:25.389867: step 1056, loss 0.569301.
Train: 2018-07-31T11:16:25.561702: step 1057, loss 0.497942.
Train: 2018-07-31T11:16:25.733567: step 1058, loss 0.543899.
Train: 2018-07-31T11:16:25.905372: step 1059, loss 0.578082.
Train: 2018-07-31T11:16:26.092827: step 1060, loss 0.544167.
Test: 2018-07-31T11:16:26.561468: step 1060, loss 0.555352.
Train: 2018-07-31T11:16:26.748925: step 1061, loss 0.628104.
Train: 2018-07-31T11:16:26.920760: step 1062, loss 0.519033.
Train: 2018-07-31T11:16:27.108215: step 1063, loss 0.518885.
Train: 2018-07-31T11:16:27.280049: step 1064, loss 0.543914.
Train: 2018-07-31T11:16:27.451884: step 1065, loss 0.53698.
Train: 2018-07-31T11:16:27.623719: step 1066, loss 0.552869.
Train: 2018-07-31T11:16:27.811176: step 1067, loss 0.52768.
Train: 2018-07-31T11:16:27.983041: step 1068, loss 0.52677.
Train: 2018-07-31T11:16:28.154876: step 1069, loss 0.510209.
Train: 2018-07-31T11:16:28.326680: step 1070, loss 0.588012.
Test: 2018-07-31T11:16:28.795353: step 1070, loss 0.554853.
Train: 2018-07-31T11:16:28.982777: step 1071, loss 0.528326.
Train: 2018-07-31T11:16:29.154641: step 1072, loss 0.664105.
Train: 2018-07-31T11:16:29.326477: step 1073, loss 0.655703.
Train: 2018-07-31T11:16:29.513902: step 1074, loss 0.638335.
Train: 2018-07-31T11:16:29.685737: step 1075, loss 0.543283.
Train: 2018-07-31T11:16:29.857572: step 1076, loss 0.56055.
Train: 2018-07-31T11:16:30.029436: step 1077, loss 0.517449.
Train: 2018-07-31T11:16:30.216892: step 1078, loss 0.612277.
Train: 2018-07-31T11:16:30.388696: step 1079, loss 0.517438.
Train: 2018-07-31T11:16:30.576185: step 1080, loss 0.517398.
Test: 2018-07-31T11:16:31.044824: step 1080, loss 0.554656.
Train: 2018-07-31T11:16:31.247872: step 1081, loss 0.612296.
Train: 2018-07-31T11:16:31.450948: step 1082, loss 0.560454.
Train: 2018-07-31T11:16:31.654058: step 1083, loss 0.569064.
Train: 2018-07-31T11:16:31.872725: step 1084, loss 0.57768.
Train: 2018-07-31T11:16:32.075850: step 1085, loss 0.690091.
Train: 2018-07-31T11:16:32.247637: step 1086, loss 0.534416.
Train: 2018-07-31T11:16:32.419472: step 1087, loss 0.594758.
Train: 2018-07-31T11:16:32.606928: step 1088, loss 0.517198.
Train: 2018-07-31T11:16:32.778763: step 1089, loss 0.543013.
Train: 2018-07-31T11:16:32.966218: step 1090, loss 0.62033.
Test: 2018-07-31T11:16:33.434890: step 1090, loss 0.554282.
Train: 2018-07-31T11:16:33.606694: step 1091, loss 0.542885.
Train: 2018-07-31T11:16:33.778558: step 1092, loss 0.491278.
Train: 2018-07-31T11:16:33.966014: step 1093, loss 0.516961.
Train: 2018-07-31T11:16:34.137843: step 1094, loss 0.507748.
Train: 2018-07-31T11:16:34.309685: step 1095, loss 0.613351.
Train: 2018-07-31T11:16:34.481518: step 1096, loss 0.508659.
Train: 2018-07-31T11:16:34.653353: step 1097, loss 0.611451.
Train: 2018-07-31T11:16:34.840783: step 1098, loss 0.603004.
Train: 2018-07-31T11:16:35.012614: step 1099, loss 0.611784.
Train: 2018-07-31T11:16:35.184449: step 1100, loss 0.541414.
Test: 2018-07-31T11:16:35.653119: step 1100, loss 0.553697.
Train: 2018-07-31T11:16:36.402944: step 1101, loss 0.568191.
Train: 2018-07-31T11:16:36.574779: step 1102, loss 0.560396.
Train: 2018-07-31T11:16:36.746607: step 1103, loss 0.689664.
Train: 2018-07-31T11:16:36.918442: step 1104, loss 0.628326.
Train: 2018-07-31T11:16:37.105875: step 1105, loss 0.613637.
Train: 2018-07-31T11:16:37.277708: step 1106, loss 0.601885.
Train: 2018-07-31T11:16:37.449574: step 1107, loss 0.541946.
Train: 2018-07-31T11:16:37.637000: step 1108, loss 0.542702.
Train: 2018-07-31T11:16:37.808835: step 1109, loss 0.593187.
Train: 2018-07-31T11:16:37.980670: step 1110, loss 0.593955.
Test: 2018-07-31T11:16:38.464941: step 1110, loss 0.553749.
Train: 2018-07-31T11:16:38.636796: step 1111, loss 0.555445.
Train: 2018-07-31T11:16:38.808625: step 1112, loss 0.566458.
Train: 2018-07-31T11:16:38.980434: step 1113, loss 0.550692.
Train: 2018-07-31T11:16:39.152270: step 1114, loss 0.567389.
Train: 2018-07-31T11:16:39.324105: step 1115, loss 0.643344.
Train: 2018-07-31T11:16:39.495940: step 1116, loss 0.508587.
Train: 2018-07-31T11:16:39.683396: step 1117, loss 0.626367.
Train: 2018-07-31T11:16:39.855260: step 1118, loss 0.565053.
Train: 2018-07-31T11:16:40.027065: step 1119, loss 0.541753.
Train: 2018-07-31T11:16:40.198899: step 1120, loss 0.558294.
Test: 2018-07-31T11:16:40.667540: step 1120, loss 0.553704.
Train: 2018-07-31T11:16:40.839405: step 1121, loss 0.585197.
Train: 2018-07-31T11:16:41.026842: step 1122, loss 0.525988.
Train: 2018-07-31T11:16:41.198666: step 1123, loss 0.559194.
Train: 2018-07-31T11:16:41.386122: step 1124, loss 0.592761.
Train: 2018-07-31T11:16:41.557957: step 1125, loss 0.584487.
Train: 2018-07-31T11:16:41.745412: step 1126, loss 0.567673.
Train: 2018-07-31T11:16:41.917249: step 1127, loss 0.567556.
Train: 2018-07-31T11:16:42.089083: step 1128, loss 0.626242.
Train: 2018-07-31T11:16:42.260916: step 1129, loss 0.575163.
Train: 2018-07-31T11:16:42.432751: step 1130, loss 0.634162.
Test: 2018-07-31T11:16:42.917044: step 1130, loss 0.553576.
Train: 2018-07-31T11:16:43.104494: step 1131, loss 0.559538.
Train: 2018-07-31T11:16:43.291926: step 1132, loss 0.520479.
Train: 2018-07-31T11:16:43.463761: step 1133, loss 0.626969.
Train: 2018-07-31T11:16:43.635625: step 1134, loss 0.55225.
Train: 2018-07-31T11:16:43.823052: step 1135, loss 0.515612.
Train: 2018-07-31T11:16:43.994887: step 1136, loss 0.613752.
Train: 2018-07-31T11:16:44.182341: step 1137, loss 0.650396.
Train: 2018-07-31T11:16:44.354207: step 1138, loss 0.583433.
Train: 2018-07-31T11:16:44.526045: step 1139, loss 0.559733.
Train: 2018-07-31T11:16:44.713497: step 1140, loss 0.566671.
Test: 2018-07-31T11:16:45.182139: step 1140, loss 0.553694.
Train: 2018-07-31T11:16:45.369589: step 1141, loss 0.636333.
Train: 2018-07-31T11:16:45.541399: step 1142, loss 0.607418.
Train: 2018-07-31T11:16:45.713258: step 1143, loss 0.469056.
Train: 2018-07-31T11:16:45.900689: step 1144, loss 0.511014.
Train: 2018-07-31T11:16:46.072555: step 1145, loss 0.551106.
Train: 2018-07-31T11:16:46.244360: step 1146, loss 0.589919.
Train: 2018-07-31T11:16:46.431814: step 1147, loss 0.583476.
Train: 2018-07-31T11:16:46.603680: step 1148, loss 0.552052.
Train: 2018-07-31T11:16:46.775514: step 1149, loss 0.643938.
Train: 2018-07-31T11:16:46.947350: step 1150, loss 0.501522.
Test: 2018-07-31T11:16:47.431580: step 1150, loss 0.553613.
Train: 2018-07-31T11:16:47.603416: step 1151, loss 0.57585.
Train: 2018-07-31T11:16:47.775251: step 1152, loss 0.592594.
Train: 2018-07-31T11:16:47.947116: step 1153, loss 0.575967.
Train: 2018-07-31T11:16:48.134541: step 1154, loss 0.592841.
Train: 2018-07-31T11:16:48.321998: step 1155, loss 0.659487.
Train: 2018-07-31T11:16:48.493866: step 1156, loss 0.575785.
Train: 2018-07-31T11:16:48.681319: step 1157, loss 0.541816.
Train: 2018-07-31T11:16:48.853124: step 1158, loss 0.561289.
Train: 2018-07-31T11:16:49.040609: step 1159, loss 0.531524.
Train: 2018-07-31T11:16:49.212438: step 1160, loss 0.551887.
Test: 2018-07-31T11:16:49.681085: step 1160, loss 0.553304.
Train: 2018-07-31T11:16:49.852889: step 1161, loss 0.638807.
Train: 2018-07-31T11:16:50.040375: step 1162, loss 0.55923.
Train: 2018-07-31T11:16:50.227801: step 1163, loss 0.585198.
Train: 2018-07-31T11:16:50.399635: step 1164, loss 0.521768.
Train: 2018-07-31T11:16:50.587123: step 1165, loss 0.551804.
Train: 2018-07-31T11:16:50.758927: step 1166, loss 0.534035.
Train: 2018-07-31T11:16:50.946415: step 1167, loss 0.558898.
Train: 2018-07-31T11:16:51.133840: step 1168, loss 0.535627.
Train: 2018-07-31T11:16:51.305674: step 1169, loss 0.495069.
Train: 2018-07-31T11:16:51.493131: step 1170, loss 0.635252.
Test: 2018-07-31T11:16:51.977421: step 1170, loss 0.553797.
Train: 2018-07-31T11:16:52.149257: step 1171, loss 0.591135.
Train: 2018-07-31T11:16:52.352334: step 1172, loss 0.655793.
Train: 2018-07-31T11:16:52.524169: step 1173, loss 0.536419.
Train: 2018-07-31T11:16:52.711624: step 1174, loss 0.534158.
Train: 2018-07-31T11:16:52.899081: step 1175, loss 0.559715.
Train: 2018-07-31T11:16:53.086507: step 1176, loss 0.542825.
Train: 2018-07-31T11:16:53.273993: step 1177, loss 0.533078.
Train: 2018-07-31T11:16:53.461420: step 1178, loss 0.559766.
Train: 2018-07-31T11:16:53.633253: step 1179, loss 0.595591.
Train: 2018-07-31T11:16:53.820741: step 1180, loss 0.577565.
Test: 2018-07-31T11:16:54.304970: step 1180, loss 0.554227.
Train: 2018-07-31T11:16:54.492428: step 1181, loss 0.552313.
Train: 2018-07-31T11:16:54.679883: step 1182, loss 0.560456.
Train: 2018-07-31T11:16:54.867370: step 1183, loss 0.52501.
Train: 2018-07-31T11:16:55.039175: step 1184, loss 0.605923.
Train: 2018-07-31T11:16:55.226631: step 1185, loss 0.611355.
Train: 2018-07-31T11:16:55.414087: step 1186, loss 0.515504.
Train: 2018-07-31T11:16:55.601543: step 1187, loss 0.553739.
Train: 2018-07-31T11:16:55.773378: step 1188, loss 0.585834.
Train: 2018-07-31T11:16:55.960864: step 1189, loss 0.516163.
Train: 2018-07-31T11:16:56.132698: step 1190, loss 0.551145.
Test: 2018-07-31T11:16:56.616960: step 1190, loss 0.553994.
Train: 2018-07-31T11:16:56.804385: step 1191, loss 0.533728.
Train: 2018-07-31T11:16:56.976221: step 1192, loss 0.559466.
Train: 2018-07-31T11:16:57.163677: step 1193, loss 0.533484.
Train: 2018-07-31T11:16:57.351133: step 1194, loss 0.595376.
Train: 2018-07-31T11:16:57.522968: step 1195, loss 0.569619.
Train: 2018-07-31T11:16:57.710454: step 1196, loss 0.480631.
Train: 2018-07-31T11:16:57.882259: step 1197, loss 0.622747.
Train: 2018-07-31T11:16:58.069715: step 1198, loss 0.524746.
Train: 2018-07-31T11:16:58.257201: step 1199, loss 0.585939.
Train: 2018-07-31T11:16:58.444686: step 1200, loss 0.586227.
Test: 2018-07-31T11:16:58.913266: step 1200, loss 0.553614.
Train: 2018-07-31T11:16:59.725578: step 1201, loss 0.533026.
Train: 2018-07-31T11:16:59.928687: step 1202, loss 0.595065.
Train: 2018-07-31T11:17:00.116140: step 1203, loss 0.515283.
Train: 2018-07-31T11:17:00.303598: step 1204, loss 0.577356.
Train: 2018-07-31T11:17:00.491053: step 1205, loss 0.541943.
Train: 2018-07-31T11:17:00.662887: step 1206, loss 0.54206.
Train: 2018-07-31T11:17:00.850314: step 1207, loss 0.603522.
Train: 2018-07-31T11:17:01.006526: step 1208, loss 0.474028.
Train: 2018-07-31T11:17:01.193982: step 1209, loss 0.54175.
Train: 2018-07-31T11:17:01.381440: step 1210, loss 0.630931.
Test: 2018-07-31T11:17:01.850110: step 1210, loss 0.553515.
Train: 2018-07-31T11:17:02.037566: step 1211, loss 0.532736.
Train: 2018-07-31T11:17:02.224992: step 1212, loss 0.604363.
Train: 2018-07-31T11:17:02.396857: step 1213, loss 0.53223.
Train: 2018-07-31T11:17:02.584313: step 1214, loss 0.541258.
Train: 2018-07-31T11:17:02.756117: step 1215, loss 0.558714.
Train: 2018-07-31T11:17:02.943573: step 1216, loss 0.514487.
Train: 2018-07-31T11:17:03.115432: step 1217, loss 0.575461.
Train: 2018-07-31T11:17:03.287244: step 1218, loss 0.57911.
Train: 2018-07-31T11:17:03.474699: step 1219, loss 0.612421.
Train: 2018-07-31T11:17:03.646534: step 1220, loss 0.597719.
Test: 2018-07-31T11:17:04.130825: step 1220, loss 0.553212.
Train: 2018-07-31T11:17:04.318253: step 1221, loss 0.608423.
Train: 2018-07-31T11:17:04.505708: step 1222, loss 0.544593.
Train: 2018-07-31T11:17:04.677574: step 1223, loss 0.513585.
Train: 2018-07-31T11:17:04.865023: step 1224, loss 0.568119.
Train: 2018-07-31T11:17:05.036834: step 1225, loss 0.631087.
Train: 2018-07-31T11:17:05.224289: step 1226, loss 0.611553.
Train: 2018-07-31T11:17:05.396124: step 1227, loss 0.603277.
Train: 2018-07-31T11:17:05.583605: step 1228, loss 0.541772.
Train: 2018-07-31T11:17:05.771061: step 1229, loss 0.597667.
Train: 2018-07-31T11:17:05.942902: step 1230, loss 0.516127.
Test: 2018-07-31T11:17:06.427164: step 1230, loss 0.553066.
Train: 2018-07-31T11:17:06.594882: step 1231, loss 0.610667.
Train: 2018-07-31T11:17:06.782340: step 1232, loss 0.470834.
Train: 2018-07-31T11:17:06.969771: step 1233, loss 0.54696.
Train: 2018-07-31T11:17:07.141635: step 1234, loss 0.617459.
Train: 2018-07-31T11:17:07.329061: step 1235, loss 0.514862.
Train: 2018-07-31T11:17:07.516548: step 1236, loss 0.581483.
Train: 2018-07-31T11:17:07.688352: step 1237, loss 0.44451.
Train: 2018-07-31T11:17:07.860186: step 1238, loss 0.524538.
Train: 2018-07-31T11:17:08.047672: step 1239, loss 0.524942.
Train: 2018-07-31T11:17:08.219511: step 1240, loss 0.55041.
Test: 2018-07-31T11:17:08.688118: step 1240, loss 0.553862.
Train: 2018-07-31T11:17:08.875574: step 1241, loss 0.559822.
Train: 2018-07-31T11:17:09.047408: step 1242, loss 0.622096.
Train: 2018-07-31T11:17:09.250511: step 1243, loss 0.468417.
Train: 2018-07-31T11:17:09.422346: step 1244, loss 0.622562.
Train: 2018-07-31T11:17:09.609808: step 1245, loss 0.533843.
Train: 2018-07-31T11:17:09.781612: step 1246, loss 0.544511.
Train: 2018-07-31T11:17:09.969102: step 1247, loss 0.587077.
Train: 2018-07-31T11:17:10.140933: step 1248, loss 0.531115.
Train: 2018-07-31T11:17:10.328391: step 1249, loss 0.629881.
Train: 2018-07-31T11:17:10.515845: step 1250, loss 0.476374.
Test: 2018-07-31T11:17:10.984485: step 1250, loss 0.554579.
Train: 2018-07-31T11:17:11.171944: step 1251, loss 0.590685.
Train: 2018-07-31T11:17:11.343780: step 1252, loss 0.609159.
Train: 2018-07-31T11:17:11.531202: step 1253, loss 0.539386.
Train: 2018-07-31T11:17:11.703067: step 1254, loss 0.523934.
Train: 2018-07-31T11:17:11.890494: step 1255, loss 0.596895.
Train: 2018-07-31T11:17:12.062353: step 1256, loss 0.572478.
Train: 2018-07-31T11:17:12.249809: step 1257, loss 0.535497.
Train: 2018-07-31T11:17:12.421618: step 1258, loss 0.59679.
Train: 2018-07-31T11:17:12.593454: step 1259, loss 0.631734.
Train: 2018-07-31T11:17:12.780939: step 1260, loss 0.632053.
Test: 2018-07-31T11:17:13.249580: step 1260, loss 0.554632.
Train: 2018-07-31T11:17:13.437007: step 1261, loss 0.515145.
Train: 2018-07-31T11:17:13.608871: step 1262, loss 0.651033.
Train: 2018-07-31T11:17:13.796327: step 1263, loss 0.632147.
Train: 2018-07-31T11:17:13.968133: step 1264, loss 0.551637.
Train: 2018-07-31T11:17:14.171208: step 1265, loss 0.54275.
Train: 2018-07-31T11:17:14.343077: step 1266, loss 0.569144.
Train: 2018-07-31T11:17:14.514879: step 1267, loss 0.570602.
Train: 2018-07-31T11:17:14.702365: step 1268, loss 0.603958.
Train: 2018-07-31T11:17:14.874170: step 1269, loss 0.595453.
Train: 2018-07-31T11:17:15.061659: step 1270, loss 0.499306.
Test: 2018-07-31T11:17:15.545886: step 1270, loss 0.554412.
Train: 2018-07-31T11:17:15.733374: step 1271, loss 0.551564.
Train: 2018-07-31T11:17:15.905208: step 1272, loss 0.621333.
Train: 2018-07-31T11:17:16.092668: step 1273, loss 0.568919.
Train: 2018-07-31T11:17:16.264469: step 1274, loss 0.481527.
Train: 2018-07-31T11:17:16.451924: step 1275, loss 0.603561.
Train: 2018-07-31T11:17:16.623790: step 1276, loss 0.540575.
Train: 2018-07-31T11:17:16.795594: step 1277, loss 0.560207.
Train: 2018-07-31T11:17:16.983081: step 1278, loss 0.525409.
Train: 2018-07-31T11:17:17.170507: step 1279, loss 0.514701.
Train: 2018-07-31T11:17:17.342375: step 1280, loss 0.507333.
Test: 2018-07-31T11:17:17.811012: step 1280, loss 0.554024.
Train: 2018-07-31T11:17:17.998438: step 1281, loss 0.535066.
Train: 2018-07-31T11:17:18.170273: step 1282, loss 0.620381.
Train: 2018-07-31T11:17:18.342141: step 1283, loss 0.568207.
Train: 2018-07-31T11:17:18.529588: step 1284, loss 0.545576.
Train: 2018-07-31T11:17:18.701398: step 1285, loss 0.593813.
Train: 2018-07-31T11:17:18.873259: step 1286, loss 0.655216.
Train: 2018-07-31T11:17:19.060719: step 1287, loss 0.577452.
Train: 2018-07-31T11:17:19.232523: step 1288, loss 0.585744.
Train: 2018-07-31T11:17:19.420010: step 1289, loss 0.581089.
Train: 2018-07-31T11:17:19.591848: step 1290, loss 0.671905.
Test: 2018-07-31T11:17:20.076106: step 1290, loss 0.553746.
Train: 2018-07-31T11:17:20.263532: step 1291, loss 0.588624.
Train: 2018-07-31T11:17:20.435367: step 1292, loss 0.566975.
Train: 2018-07-31T11:17:20.622853: step 1293, loss 0.61849.
Train: 2018-07-31T11:17:20.810279: step 1294, loss 0.559908.
Train: 2018-07-31T11:17:20.982113: step 1295, loss 0.533996.
Train: 2018-07-31T11:17:21.169570: step 1296, loss 0.602178.
Train: 2018-07-31T11:17:21.341405: step 1297, loss 0.576728.
Train: 2018-07-31T11:17:21.528862: step 1298, loss 0.577758.
Train: 2018-07-31T11:17:21.700725: step 1299, loss 0.610542.
Train: 2018-07-31T11:17:21.888182: step 1300, loss 0.568569.
Test: 2018-07-31T11:17:22.356792: step 1300, loss 0.554243.
Train: 2018-07-31T11:17:23.106647: step 1301, loss 0.576755.
Train: 2018-07-31T11:17:23.294102: step 1302, loss 0.534805.
Train: 2018-07-31T11:17:23.465907: step 1303, loss 0.49301.
Train: 2018-07-31T11:17:23.653364: step 1304, loss 0.601879.
Train: 2018-07-31T11:17:23.825199: step 1305, loss 0.610248.
Train: 2018-07-31T11:17:23.997033: step 1306, loss 0.593488.
Train: 2018-07-31T11:17:24.184490: step 1307, loss 0.559969.
Train: 2018-07-31T11:17:24.356324: step 1308, loss 0.593034.
Train: 2018-07-31T11:17:24.528190: step 1309, loss 0.576424.
Train: 2018-07-31T11:17:24.715648: step 1310, loss 0.622622.
Test: 2018-07-31T11:17:25.184286: step 1310, loss 0.5545.
Train: 2018-07-31T11:17:25.356091: step 1311, loss 0.554277.
Train: 2018-07-31T11:17:25.543546: step 1312, loss 0.510343.
Train: 2018-07-31T11:17:25.715380: step 1313, loss 0.535253.
Train: 2018-07-31T11:17:25.902836: step 1314, loss 0.526954.
Train: 2018-07-31T11:17:26.074702: step 1315, loss 0.585169.
Train: 2018-07-31T11:17:26.262130: step 1316, loss 0.476892.
Train: 2018-07-31T11:17:26.433987: step 1317, loss 0.551817.
Train: 2018-07-31T11:17:26.605827: step 1318, loss 0.594094.
Train: 2018-07-31T11:17:26.793283: step 1319, loss 0.577393.
Train: 2018-07-31T11:17:26.965121: step 1320, loss 0.593947.
Test: 2018-07-31T11:17:27.449383: step 1320, loss 0.554585.
Train: 2018-07-31T11:17:27.621185: step 1321, loss 0.534963.
Train: 2018-07-31T11:17:27.808640: step 1322, loss 0.5095.
Train: 2018-07-31T11:17:27.980475: step 1323, loss 0.577233.
Train: 2018-07-31T11:17:28.152340: step 1324, loss 0.611315.
Train: 2018-07-31T11:17:28.324176: step 1325, loss 0.517542.
Train: 2018-07-31T11:17:28.511631: step 1326, loss 0.645626.
Train: 2018-07-31T11:17:28.683435: step 1327, loss 0.671338.
Train: 2018-07-31T11:17:28.855270: step 1328, loss 0.57745.
Train: 2018-07-31T11:17:29.042727: step 1329, loss 0.568551.
Train: 2018-07-31T11:17:29.214561: step 1330, loss 0.551468.
Test: 2018-07-31T11:17:29.683200: step 1330, loss 0.554198.
Train: 2018-07-31T11:17:29.870658: step 1331, loss 0.568465.
Train: 2018-07-31T11:17:30.042522: step 1332, loss 0.483269.
Train: 2018-07-31T11:17:30.214361: step 1333, loss 0.585394.
Train: 2018-07-31T11:17:30.401813: step 1334, loss 0.610937.
Train: 2018-07-31T11:17:30.573619: step 1335, loss 0.627928.
Train: 2018-07-31T11:17:30.761104: step 1336, loss 0.542603.
Train: 2018-07-31T11:17:30.932934: step 1337, loss 0.568099.
Train: 2018-07-31T11:17:31.104743: step 1338, loss 0.576547.
Train: 2018-07-31T11:17:31.276609: step 1339, loss 0.533959.
Train: 2018-07-31T11:17:31.464068: step 1340, loss 0.593436.
Test: 2018-07-31T11:17:31.948297: step 1340, loss 0.553618.
Train: 2018-07-31T11:17:32.120161: step 1341, loss 0.584867.
Train: 2018-07-31T11:17:32.291965: step 1342, loss 0.542308.
Train: 2018-07-31T11:17:32.463800: step 1343, loss 0.542253.
Train: 2018-07-31T11:17:32.635666: step 1344, loss 0.627224.
Train: 2018-07-31T11:17:32.807500: step 1345, loss 0.610141.
Train: 2018-07-31T11:17:32.994927: step 1346, loss 0.593064.
Train: 2018-07-31T11:17:33.166790: step 1347, loss 0.576035.
Train: 2018-07-31T11:17:33.338596: step 1348, loss 0.550606.
Train: 2018-07-31T11:17:33.510460: step 1349, loss 0.550586.
Train: 2018-07-31T11:17:33.682266: step 1350, loss 0.609675.
Test: 2018-07-31T11:17:34.166556: step 1350, loss 0.553291.
Train: 2018-07-31T11:17:34.338395: step 1351, loss 0.474638.
Train: 2018-07-31T11:17:34.510221: step 1352, loss 0.516732.
Train: 2018-07-31T11:17:34.697652: step 1353, loss 0.516602.
Train: 2018-07-31T11:17:34.869486: step 1354, loss 0.533384.
Train: 2018-07-31T11:17:35.041321: step 1355, loss 0.550252.
Train: 2018-07-31T11:17:35.213190: step 1356, loss 0.618426.
Train: 2018-07-31T11:17:35.384991: step 1357, loss 0.575749.
Train: 2018-07-31T11:17:35.556826: step 1358, loss 0.592859.
Train: 2018-07-31T11:17:35.728661: step 1359, loss 0.585431.
Train: 2018-07-31T11:17:35.916116: step 1360, loss 0.541407.
Test: 2018-07-31T11:17:36.384757: step 1360, loss 0.552733.
Train: 2018-07-31T11:17:36.556622: step 1361, loss 0.558523.
Train: 2018-07-31T11:17:36.744078: step 1362, loss 0.506957.
Train: 2018-07-31T11:17:36.915882: step 1363, loss 0.635919.
Train: 2018-07-31T11:17:37.087751: step 1364, loss 0.541211.
Train: 2018-07-31T11:17:37.259582: step 1365, loss 0.53255.
Train: 2018-07-31T11:17:37.431417: step 1366, loss 0.592905.
Train: 2018-07-31T11:17:37.603252: step 1367, loss 0.506522.
Train: 2018-07-31T11:17:37.775081: step 1368, loss 0.636217.
Train: 2018-07-31T11:17:37.962537: step 1369, loss 0.549649.
Train: 2018-07-31T11:17:38.134372: step 1370, loss 0.488989.
Test: 2018-07-31T11:17:38.603018: step 1370, loss 0.552402.
Train: 2018-07-31T11:17:38.790443: step 1371, loss 0.523542.
Train: 2018-07-31T11:17:38.962312: step 1372, loss 0.58433.
Train: 2018-07-31T11:17:39.134143: step 1373, loss 0.558203.
Train: 2018-07-31T11:17:39.321599: step 1374, loss 0.549455.
Train: 2018-07-31T11:17:39.493403: step 1375, loss 0.505665.
Train: 2018-07-31T11:17:39.665269: step 1376, loss 0.58445.
Train: 2018-07-31T11:17:39.837107: step 1377, loss 0.549321.
Train: 2018-07-31T11:17:39.993317: step 1378, loss 0.514048.
Train: 2018-07-31T11:17:40.180773: step 1379, loss 0.531577.
Train: 2018-07-31T11:17:40.336957: step 1380, loss 0.58454.
Test: 2018-07-31T11:17:40.821248: step 1380, loss 0.552057.
Train: 2018-07-31T11:17:41.008674: step 1381, loss 0.584714.
Train: 2018-07-31T11:17:41.180510: step 1382, loss 0.522231.
Train: 2018-07-31T11:17:41.352374: step 1383, loss 0.584317.
Train: 2018-07-31T11:17:41.539830: step 1384, loss 0.495616.
Train: 2018-07-31T11:17:41.711668: step 1385, loss 0.6475.
Train: 2018-07-31T11:17:41.883493: step 1386, loss 0.486835.
Train: 2018-07-31T11:17:42.055335: step 1387, loss 0.504153.
Train: 2018-07-31T11:17:42.227169: step 1388, loss 0.59233.
Train: 2018-07-31T11:17:42.398973: step 1389, loss 0.48119.
Train: 2018-07-31T11:17:42.570833: step 1390, loss 0.633578.
Test: 2018-07-31T11:17:43.055081: step 1390, loss 0.552057.
Train: 2018-07-31T11:17:43.226904: step 1391, loss 0.594111.
Train: 2018-07-31T11:17:43.398765: step 1392, loss 0.5791.
Train: 2018-07-31T11:17:43.570575: step 1393, loss 0.558129.
Train: 2018-07-31T11:17:43.742434: step 1394, loss 0.531556.
Train: 2018-07-31T11:17:43.914273: step 1395, loss 0.594.
Train: 2018-07-31T11:17:44.086078: step 1396, loss 0.56711.
Train: 2018-07-31T11:17:44.257913: step 1397, loss 0.630514.
Train: 2018-07-31T11:17:44.429749: step 1398, loss 0.477349.
Train: 2018-07-31T11:17:44.601613: step 1399, loss 0.683659.
Train: 2018-07-31T11:17:44.789038: step 1400, loss 0.584975.
Test: 2018-07-31T11:17:45.257709: step 1400, loss 0.552147.
Train: 2018-07-31T11:17:46.007535: step 1401, loss 0.567082.
Train: 2018-07-31T11:17:46.179368: step 1402, loss 0.575975.
Train: 2018-07-31T11:17:46.351205: step 1403, loss 0.584809.
Train: 2018-07-31T11:17:46.538630: step 1404, loss 0.540706.
Train: 2018-07-31T11:17:46.710464: step 1405, loss 0.59352.
Train: 2018-07-31T11:17:46.882298: step 1406, loss 0.663618.
Train: 2018-07-31T11:17:47.069785: step 1407, loss 0.558283.
Train: 2018-07-31T11:17:47.241589: step 1408, loss 0.628217.
Train: 2018-07-31T11:17:47.413455: step 1409, loss 0.514956.
Train: 2018-07-31T11:17:47.569638: step 1410, loss 0.56708.
Test: 2018-07-31T11:17:48.053900: step 1410, loss 0.552573.
Train: 2018-07-31T11:17:48.225765: step 1411, loss 0.540701.
Train: 2018-07-31T11:17:48.397569: step 1412, loss 0.634142.
Train: 2018-07-31T11:17:48.569403: step 1413, loss 0.571695.
Train: 2018-07-31T11:17:48.741271: step 1414, loss 0.533727.
Train: 2018-07-31T11:17:48.928721: step 1415, loss 0.592637.
Train: 2018-07-31T11:17:49.100561: step 1416, loss 0.56693.
Train: 2018-07-31T11:17:49.272395: step 1417, loss 0.566951.
Train: 2018-07-31T11:17:49.444198: step 1418, loss 0.55007.
Train: 2018-07-31T11:17:49.616034: step 1419, loss 0.558883.
Train: 2018-07-31T11:17:49.787869: step 1420, loss 0.626356.
Test: 2018-07-31T11:17:50.256509: step 1420, loss 0.552821.
Train: 2018-07-31T11:17:50.443965: step 1421, loss 0.541639.
Train: 2018-07-31T11:17:50.600209: step 1422, loss 0.524791.
Train: 2018-07-31T11:17:50.772043: step 1423, loss 0.566979.
Train: 2018-07-31T11:17:50.943848: step 1424, loss 0.609243.
Train: 2018-07-31T11:17:51.131334: step 1425, loss 0.583866.
Train: 2018-07-31T11:17:51.303139: step 1426, loss 0.55005.
Train: 2018-07-31T11:17:51.474973: step 1427, loss 0.55842.
Train: 2018-07-31T11:17:51.646808: step 1428, loss 0.66812.
Train: 2018-07-31T11:17:51.818667: step 1429, loss 0.491246.
Train: 2018-07-31T11:17:51.990508: step 1430, loss 0.583691.
Test: 2018-07-31T11:17:52.474769: step 1430, loss 0.55281.
Train: 2018-07-31T11:17:52.646575: step 1431, loss 0.591724.
Train: 2018-07-31T11:17:52.818439: step 1432, loss 0.533276.
Train: 2018-07-31T11:17:52.990244: step 1433, loss 0.626998.
Train: 2018-07-31T11:17:53.162077: step 1434, loss 0.533011.
Train: 2018-07-31T11:17:53.333913: step 1435, loss 0.577603.
Train: 2018-07-31T11:17:53.505784: step 1436, loss 0.574927.
Train: 2018-07-31T11:17:53.677616: step 1437, loss 0.566859.
Train: 2018-07-31T11:17:53.849417: step 1438, loss 0.633637.
Train: 2018-07-31T11:17:54.021253: step 1439, loss 0.624587.
Train: 2018-07-31T11:17:54.193117: step 1440, loss 0.590928.
Test: 2018-07-31T11:17:54.677378: step 1440, loss 0.552868.
Train: 2018-07-31T11:17:54.849184: step 1441, loss 0.558405.
Train: 2018-07-31T11:17:55.021043: step 1442, loss 0.525292.
Train: 2018-07-31T11:17:55.192882: step 1443, loss 0.625024.
Train: 2018-07-31T11:17:55.364713: step 1444, loss 0.53377.
Train: 2018-07-31T11:17:55.520936: step 1445, loss 0.608499.
Train: 2018-07-31T11:17:55.692765: step 1446, loss 0.550616.
Train: 2018-07-31T11:17:55.864570: step 1447, loss 0.575646.
Train: 2018-07-31T11:17:56.036405: step 1448, loss 0.575545.
Train: 2018-07-31T11:17:56.208239: step 1449, loss 0.550844.
Train: 2018-07-31T11:17:56.380105: step 1450, loss 0.542642.
Test: 2018-07-31T11:17:56.864336: step 1450, loss 0.553618.
Train: 2018-07-31T11:17:57.036201: step 1451, loss 0.625255.
Train: 2018-07-31T11:17:57.208005: step 1452, loss 0.592305.
Train: 2018-07-31T11:17:57.379865: step 1453, loss 0.59205.
Train: 2018-07-31T11:17:57.551675: step 1454, loss 0.567579.
Train: 2018-07-31T11:17:57.707889: step 1455, loss 0.575749.
Train: 2018-07-31T11:17:57.879724: step 1456, loss 0.551164.
Train: 2018-07-31T11:17:58.051558: step 1457, loss 0.551165.
Train: 2018-07-31T11:17:58.223418: step 1458, loss 0.526444.
Train: 2018-07-31T11:17:58.395258: step 1459, loss 0.542859.
Train: 2018-07-31T11:17:58.567095: step 1460, loss 0.542766.
Test: 2018-07-31T11:17:59.051324: step 1460, loss 0.553664.
Train: 2018-07-31T11:17:59.223158: step 1461, loss 0.501367.
Train: 2018-07-31T11:17:59.394994: step 1462, loss 0.54215.
Train: 2018-07-31T11:17:59.566861: step 1463, loss 0.542391.
Train: 2018-07-31T11:17:59.738694: step 1464, loss 0.533818.
Train: 2018-07-31T11:17:59.910528: step 1465, loss 0.466196.
Train: 2018-07-31T11:18:00.082332: step 1466, loss 0.524911.
Train: 2018-07-31T11:18:00.254168: step 1467, loss 0.567246.
Train: 2018-07-31T11:18:00.426026: step 1468, loss 0.555356.
Train: 2018-07-31T11:18:00.597837: step 1469, loss 0.535472.
Train: 2018-07-31T11:18:00.754050: step 1470, loss 0.555272.
Test: 2018-07-31T11:18:01.238337: step 1470, loss 0.552523.
Train: 2018-07-31T11:18:01.425769: step 1471, loss 0.558487.
Train: 2018-07-31T11:18:01.597603: step 1472, loss 0.567628.
Train: 2018-07-31T11:18:01.769437: step 1473, loss 0.575943.
Train: 2018-07-31T11:18:01.941303: step 1474, loss 0.567064.
Train: 2018-07-31T11:18:02.128729: step 1475, loss 0.58493.
Train: 2018-07-31T11:18:02.300563: step 1476, loss 0.522627.
Train: 2018-07-31T11:18:02.472398: step 1477, loss 0.52251.
Train: 2018-07-31T11:18:02.644233: step 1478, loss 0.522424.
Train: 2018-07-31T11:18:02.816098: step 1479, loss 0.540135.
Train: 2018-07-31T11:18:02.987932: step 1480, loss 0.585049.
Test: 2018-07-31T11:18:03.472194: step 1480, loss 0.551947.
Train: 2018-07-31T11:18:03.644023: step 1481, loss 0.576073.
Train: 2018-07-31T11:18:03.800211: step 1482, loss 0.58516.
Train: 2018-07-31T11:18:03.972046: step 1483, loss 0.612048.
Train: 2018-07-31T11:18:04.143911: step 1484, loss 0.56639.
Train: 2018-07-31T11:18:04.315741: step 1485, loss 0.539847.
Train: 2018-07-31T11:18:04.487551: step 1486, loss 0.550059.
Train: 2018-07-31T11:18:04.675008: step 1487, loss 0.606013.
Train: 2018-07-31T11:18:04.831254: step 1488, loss 0.576009.
Train: 2018-07-31T11:18:05.003086: step 1489, loss 0.584847.
Train: 2018-07-31T11:18:05.174890: step 1490, loss 0.566763.
Test: 2018-07-31T11:18:05.643531: step 1490, loss 0.551683.
Train: 2018-07-31T11:18:05.815365: step 1491, loss 0.512732.
Train: 2018-07-31T11:18:06.002846: step 1492, loss 0.584699.
Train: 2018-07-31T11:18:06.174657: step 1493, loss 0.593657.
Train: 2018-07-31T11:18:06.330869: step 1494, loss 0.530771.
Train: 2018-07-31T11:18:06.565223: step 1495, loss 0.548706.
Train: 2018-07-31T11:18:06.737025: step 1496, loss 0.575555.
Train: 2018-07-31T11:18:06.908889: step 1497, loss 0.638159.
Train: 2018-07-31T11:18:07.080694: step 1498, loss 0.530815.
Train: 2018-07-31T11:18:07.236937: step 1499, loss 0.557476.
Train: 2018-07-31T11:18:07.408772: step 1500, loss 0.610871.
Test: 2018-07-31T11:18:07.893033: step 1500, loss 0.551578.
Train: 2018-07-31T11:18:08.627207: step 1501, loss 0.539694.
Train: 2018-07-31T11:18:08.799042: step 1502, loss 0.593019.
Train: 2018-07-31T11:18:08.986498: step 1503, loss 0.672584.
Train: 2018-07-31T11:18:09.158332: step 1504, loss 0.627937.
Train: 2018-07-31T11:18:09.330168: step 1505, loss 0.548829.
Train: 2018-07-31T11:18:09.502002: step 1506, loss 0.531399.
Train: 2018-07-31T11:18:09.673861: step 1507, loss 0.557467.
Train: 2018-07-31T11:18:09.845670: step 1508, loss 0.592204.
Train: 2018-07-31T11:18:10.001914: step 1509, loss 0.635289.
Train: 2018-07-31T11:18:10.158099: step 1510, loss 0.601649.
Test: 2018-07-31T11:18:10.626740: step 1510, loss 0.551854.
Train: 2018-07-31T11:18:10.845438: step 1511, loss 0.591714.
Train: 2018-07-31T11:18:11.001652: step 1512, loss 0.531886.
Train: 2018-07-31T11:18:11.173487: step 1513, loss 0.566465.
Train: 2018-07-31T11:18:11.345321: step 1514, loss 0.514576.
Train: 2018-07-31T11:18:11.532800: step 1515, loss 0.549664.
Train: 2018-07-31T11:18:11.704636: step 1516, loss 0.576177.
Train: 2018-07-31T11:18:11.876445: step 1517, loss 0.583337.
Train: 2018-07-31T11:18:12.048281: step 1518, loss 0.650058.
Train: 2018-07-31T11:18:12.220115: step 1519, loss 0.540584.
Train: 2018-07-31T11:18:12.391950: step 1520, loss 0.607696.
Test: 2018-07-31T11:18:12.860606: step 1520, loss 0.552226.
Train: 2018-07-31T11:18:13.016804: step 1521, loss 0.600789.
Train: 2018-07-31T11:18:13.188640: step 1522, loss 0.541026.
Train: 2018-07-31T11:18:13.360503: step 1523, loss 0.517658.
Train: 2018-07-31T11:18:13.532332: step 1524, loss 0.51638.
Train: 2018-07-31T11:18:13.688551: step 1525, loss 0.516338.
Train: 2018-07-31T11:18:13.860386: step 1526, loss 0.566266.
Train: 2018-07-31T11:18:14.032191: step 1527, loss 0.616323.
Train: 2018-07-31T11:18:14.204027: step 1528, loss 0.52452.
Train: 2018-07-31T11:18:14.375861: step 1529, loss 0.582978.
Train: 2018-07-31T11:18:14.547696: step 1530, loss 0.532486.
Test: 2018-07-31T11:18:15.016336: step 1530, loss 0.552156.
Train: 2018-07-31T11:18:15.203816: step 1531, loss 0.574715.
Train: 2018-07-31T11:18:15.360031: step 1532, loss 0.591428.
Train: 2018-07-31T11:18:15.531870: step 1533, loss 0.616696.
Train: 2018-07-31T11:18:15.703708: step 1534, loss 0.540942.
Train: 2018-07-31T11:18:15.875539: step 1535, loss 0.599923.
Train: 2018-07-31T11:18:16.047374: step 1536, loss 0.540836.
Train: 2018-07-31T11:18:16.219209: step 1537, loss 0.590819.
Train: 2018-07-31T11:18:16.391014: step 1538, loss 0.587604.
Train: 2018-07-31T11:18:16.562881: step 1539, loss 0.571886.
Train: 2018-07-31T11:18:16.734684: step 1540, loss 0.61366.
Test: 2018-07-31T11:18:17.203322: step 1540, loss 0.552172.
Train: 2018-07-31T11:18:17.375158: step 1541, loss 0.575151.
Train: 2018-07-31T11:18:17.562613: step 1542, loss 0.541649.
Train: 2018-07-31T11:18:17.734449: step 1543, loss 0.549545.
Train: 2018-07-31T11:18:17.906319: step 1544, loss 0.533626.
Train: 2018-07-31T11:18:18.078143: step 1545, loss 0.558226.
Train: 2018-07-31T11:18:18.249954: step 1546, loss 0.59225.
Train: 2018-07-31T11:18:18.406196: step 1547, loss 0.568156.
Train: 2018-07-31T11:18:18.593623: step 1548, loss 0.575564.
Train: 2018-07-31T11:18:18.765458: step 1549, loss 0.634633.
Train: 2018-07-31T11:18:18.937322: step 1550, loss 0.508789.
Test: 2018-07-31T11:18:19.405962: step 1550, loss 0.553741.
Train: 2018-07-31T11:18:19.593419: step 1551, loss 0.559595.
Train: 2018-07-31T11:18:19.749602: step 1552, loss 0.534337.
Train: 2018-07-31T11:18:19.921436: step 1553, loss 0.509079.
Train: 2018-07-31T11:18:20.093271: step 1554, loss 0.526229.
Train: 2018-07-31T11:18:20.265107: step 1555, loss 0.56867.
Train: 2018-07-31T11:18:20.452589: step 1556, loss 0.500271.
Train: 2018-07-31T11:18:20.624422: step 1557, loss 0.559908.
Train: 2018-07-31T11:18:20.796262: step 1558, loss 0.542517.
Train: 2018-07-31T11:18:20.952445: step 1559, loss 0.58484.
Train: 2018-07-31T11:18:21.124311: step 1560, loss 0.602973.
Test: 2018-07-31T11:18:21.608542: step 1560, loss 0.553872.
Train: 2018-07-31T11:18:21.780378: step 1561, loss 0.583119.
Train: 2018-07-31T11:18:21.952212: step 1562, loss 0.57949.
Train: 2018-07-31T11:18:22.139668: step 1563, loss 0.554477.
Train: 2018-07-31T11:18:22.311527: step 1564, loss 0.544479.
Train: 2018-07-31T11:18:22.467715: step 1565, loss 0.587661.
Train: 2018-07-31T11:18:22.655171: step 1566, loss 0.605213.
Train: 2018-07-31T11:18:22.827040: step 1567, loss 0.582574.
Train: 2018-07-31T11:18:22.998866: step 1568, loss 0.528047.
Train: 2018-07-31T11:18:23.186298: step 1569, loss 0.527808.
Train: 2018-07-31T11:18:23.358132: step 1570, loss 0.524978.
Test: 2018-07-31T11:18:23.826771: step 1570, loss 0.554209.
Train: 2018-07-31T11:18:24.014259: step 1571, loss 0.551841.
Train: 2018-07-31T11:18:24.201686: step 1572, loss 0.630637.
Train: 2018-07-31T11:18:24.373519: step 1573, loss 0.515173.
Train: 2018-07-31T11:18:24.545355: step 1574, loss 0.638368.
Train: 2018-07-31T11:18:24.717189: step 1575, loss 0.569799.
Train: 2018-07-31T11:18:24.904645: step 1576, loss 0.586727.
Train: 2018-07-31T11:18:25.076513: step 1577, loss 0.60439.
Train: 2018-07-31T11:18:25.263935: step 1578, loss 0.546407.
Train: 2018-07-31T11:18:25.435801: step 1579, loss 0.583372.
Train: 2018-07-31T11:18:25.607607: step 1580, loss 0.579871.
Test: 2018-07-31T11:18:26.076279: step 1580, loss 0.555321.
Train: 2018-07-31T11:18:26.263702: step 1581, loss 0.552605.
Train: 2018-07-31T11:18:26.435561: step 1582, loss 0.524231.
Train: 2018-07-31T11:18:26.607401: step 1583, loss 0.578918.
Train: 2018-07-31T11:18:26.779237: step 1584, loss 0.569561.
Train: 2018-07-31T11:18:26.966692: step 1585, loss 0.570419.
Train: 2018-07-31T11:18:27.138527: step 1586, loss 0.649581.
Train: 2018-07-31T11:18:27.310332: step 1587, loss 0.554592.
Train: 2018-07-31T11:18:27.497788: step 1588, loss 0.569038.
Train: 2018-07-31T11:18:27.669652: step 1589, loss 0.553044.
Train: 2018-07-31T11:18:27.841487: step 1590, loss 0.535421.
Test: 2018-07-31T11:18:28.325719: step 1590, loss 0.555851.
Train: 2018-07-31T11:18:28.497578: step 1591, loss 0.596922.
Train: 2018-07-31T11:18:28.669389: step 1592, loss 0.596916.
Train: 2018-07-31T11:18:28.841254: step 1593, loss 0.526701.
Train: 2018-07-31T11:18:29.013058: step 1594, loss 0.553019.
Train: 2018-07-31T11:18:29.184893: step 1595, loss 0.561767.
Train: 2018-07-31T11:18:29.372350: step 1596, loss 0.649533.
Train: 2018-07-31T11:18:29.544182: step 1597, loss 0.569906.
Train: 2018-07-31T11:18:29.716018: step 1598, loss 0.631709.
Train: 2018-07-31T11:18:29.903505: step 1599, loss 0.648726.
Train: 2018-07-31T11:18:30.075343: step 1600, loss 0.492387.
Test: 2018-07-31T11:18:30.559569: step 1600, loss 0.555652.
Train: 2018-07-31T11:18:31.262561: step 1601, loss 0.543816.
Train: 2018-07-31T11:18:31.450021: step 1602, loss 0.508877.
Train: 2018-07-31T11:18:31.606230: step 1603, loss 0.648042.
Train: 2018-07-31T11:18:31.793687: step 1604, loss 0.544771.
Train: 2018-07-31T11:18:31.965492: step 1605, loss 0.581949.
Train: 2018-07-31T11:18:32.137351: step 1606, loss 0.602385.
Train: 2018-07-31T11:18:32.309192: step 1607, loss 0.532437.
Train: 2018-07-31T11:18:32.480996: step 1608, loss 0.474597.
Train: 2018-07-31T11:18:32.652831: step 1609, loss 0.491555.
Train: 2018-07-31T11:18:32.840317: step 1610, loss 0.569294.
Test: 2018-07-31T11:18:33.308957: step 1610, loss 0.5549.
Train: 2018-07-31T11:18:33.480763: step 1611, loss 0.552602.
Train: 2018-07-31T11:18:33.668217: step 1612, loss 0.614318.
Train: 2018-07-31T11:18:33.824432: step 1613, loss 0.525035.
Train: 2018-07-31T11:18:34.011918: step 1614, loss 0.57906.
Train: 2018-07-31T11:18:34.183756: step 1615, loss 0.585208.
Train: 2018-07-31T11:18:34.355557: step 1616, loss 0.499212.
Train: 2018-07-31T11:18:34.543014: step 1617, loss 0.543873.
Train: 2018-07-31T11:18:34.714847: step 1618, loss 0.620955.
Train: 2018-07-31T11:18:34.886713: step 1619, loss 0.617792.
Train: 2018-07-31T11:18:35.074139: step 1620, loss 0.532637.
Test: 2018-07-31T11:18:35.542780: step 1620, loss 0.554775.
Train: 2018-07-31T11:18:35.714644: step 1621, loss 0.50666.
Train: 2018-07-31T11:18:35.886474: step 1622, loss 0.58216.
Train: 2018-07-31T11:18:36.073938: step 1623, loss 0.614039.
Train: 2018-07-31T11:18:36.245765: step 1624, loss 0.525834.
Train: 2018-07-31T11:18:36.417575: step 1625, loss 0.605774.
Train: 2018-07-31T11:18:36.605030: step 1626, loss 0.472581.
Train: 2018-07-31T11:18:36.776865: step 1627, loss 0.534217.
Train: 2018-07-31T11:18:36.948733: step 1628, loss 0.604967.
Train: 2018-07-31T11:18:37.120565: step 1629, loss 0.552184.
Train: 2018-07-31T11:18:37.292370: step 1630, loss 0.525262.
Test: 2018-07-31T11:18:37.761008: step 1630, loss 0.55464.
Train: 2018-07-31T11:18:37.932874: step 1631, loss 0.613753.
Train: 2018-07-31T11:18:38.104679: step 1632, loss 0.533958.
Train: 2018-07-31T11:18:38.276514: step 1633, loss 0.578596.
Train: 2018-07-31T11:18:38.448348: step 1634, loss 0.560079.
Train: 2018-07-31T11:18:38.620218: step 1635, loss 0.577806.
Train: 2018-07-31T11:18:38.807672: step 1636, loss 0.544574.
Train: 2018-07-31T11:18:38.979474: step 1637, loss 0.634183.
Train: 2018-07-31T11:18:39.151309: step 1638, loss 0.537617.
Train: 2018-07-31T11:18:39.323176: step 1639, loss 0.649034.
Train: 2018-07-31T11:18:39.495010: step 1640, loss 0.578014.
Test: 2018-07-31T11:18:39.963617: step 1640, loss 0.554301.
Train: 2018-07-31T11:18:40.135478: step 1641, loss 0.595588.
Train: 2018-07-31T11:18:40.307288: step 1642, loss 0.533774.
Train: 2018-07-31T11:18:40.479122: step 1643, loss 0.604133.
Train: 2018-07-31T11:18:40.666579: step 1644, loss 0.551339.
Train: 2018-07-31T11:18:40.838443: step 1645, loss 0.568814.
Train: 2018-07-31T11:18:40.994657: step 1646, loss 0.603683.
Train: 2018-07-31T11:18:41.182084: step 1647, loss 0.594807.
Train: 2018-07-31T11:18:41.353918: step 1648, loss 0.568631.
Train: 2018-07-31T11:18:41.525782: step 1649, loss 0.637918.
Train: 2018-07-31T11:18:41.681967: step 1650, loss 0.585743.
Test: 2018-07-31T11:18:42.166257: step 1650, loss 0.553984.
Train: 2018-07-31T11:18:42.338093: step 1651, loss 0.620036.
Train: 2018-07-31T11:18:42.525519: step 1652, loss 0.559765.
Train: 2018-07-31T11:18:42.681764: step 1653, loss 0.619489.
Train: 2018-07-31T11:18:42.869219: step 1654, loss 0.551225.
Train: 2018-07-31T11:18:43.041053: step 1655, loss 0.508866.
Train: 2018-07-31T11:18:43.212858: step 1656, loss 0.559666.
Train: 2018-07-31T11:18:43.384693: step 1657, loss 0.601772.
Train: 2018-07-31T11:18:43.556557: step 1658, loss 0.576179.
Train: 2018-07-31T11:18:43.728392: step 1659, loss 0.643505.
Train: 2018-07-31T11:18:43.915820: step 1660, loss 0.626338.
Test: 2018-07-31T11:18:44.384488: step 1660, loss 0.553941.
Train: 2018-07-31T11:18:44.540701: step 1661, loss 0.585644.
Train: 2018-07-31T11:18:44.712537: step 1662, loss 0.668098.
Train: 2018-07-31T11:18:44.899963: step 1663, loss 0.608777.
Train: 2018-07-31T11:18:45.071831: step 1664, loss 0.534604.
Train: 2018-07-31T11:18:45.243664: step 1665, loss 0.533352.
Train: 2018-07-31T11:18:45.415497: step 1666, loss 0.619922.
Train: 2018-07-31T11:18:45.587332: step 1667, loss 0.483544.
Train: 2018-07-31T11:18:45.759136: step 1668, loss 0.586874.
Train: 2018-07-31T11:18:45.946593: step 1669, loss 0.574939.
Train: 2018-07-31T11:18:46.118457: step 1670, loss 0.584387.
Test: 2018-07-31T11:18:46.587066: step 1670, loss 0.554718.
Train: 2018-07-31T11:18:46.758903: step 1671, loss 0.592951.
Train: 2018-07-31T11:18:46.946392: step 1672, loss 0.62105.
Train: 2018-07-31T11:18:47.118220: step 1673, loss 0.547981.
Train: 2018-07-31T11:18:47.290027: step 1674, loss 0.561659.
Train: 2018-07-31T11:18:47.446275: step 1675, loss 0.470037.
Train: 2018-07-31T11:18:47.633697: step 1676, loss 0.573627.
Train: 2018-07-31T11:18:47.805563: step 1677, loss 0.600852.
Train: 2018-07-31T11:18:47.992989: step 1678, loss 0.611039.
Train: 2018-07-31T11:18:48.164854: step 1679, loss 0.558388.
Train: 2018-07-31T11:18:48.336682: step 1680, loss 0.570565.
Test: 2018-07-31T11:18:48.805298: step 1680, loss 0.556612.
Train: 2018-07-31T11:18:48.977163: step 1681, loss 0.518585.
Train: 2018-07-31T11:18:49.164588: step 1682, loss 0.580734.
Train: 2018-07-31T11:18:49.336453: step 1683, loss 0.590639.
Train: 2018-07-31T11:18:49.508258: step 1684, loss 0.544502.
Train: 2018-07-31T11:18:49.680124: step 1685, loss 0.548087.
Train: 2018-07-31T11:18:49.867550: step 1686, loss 0.582564.
Train: 2018-07-31T11:18:50.039383: step 1687, loss 0.652792.
Train: 2018-07-31T11:18:50.211249: step 1688, loss 0.562076.
Train: 2018-07-31T11:18:50.383083: step 1689, loss 0.551832.
Train: 2018-07-31T11:18:50.570510: step 1690, loss 0.594731.
Test: 2018-07-31T11:18:51.039151: step 1690, loss 0.557121.
Train: 2018-07-31T11:18:51.211009: step 1691, loss 0.577488.
Train: 2018-07-31T11:18:51.382850: step 1692, loss 0.515836.
Train: 2018-07-31T11:18:51.554655: step 1693, loss 0.488233.
Train: 2018-07-31T11:18:51.726519: step 1694, loss 0.520308.
Train: 2018-07-31T11:18:51.898354: step 1695, loss 0.645982.
Train: 2018-07-31T11:18:52.070192: step 1696, loss 0.587546.
Train: 2018-07-31T11:18:52.241993: step 1697, loss 0.528644.
Train: 2018-07-31T11:18:52.413827: step 1698, loss 0.427337.
Train: 2018-07-31T11:18:52.585662: step 1699, loss 0.587595.
Train: 2018-07-31T11:18:52.757521: step 1700, loss 0.655869.
Test: 2018-07-31T11:18:53.241789: step 1700, loss 0.556221.
Train: 2018-07-31T11:18:53.960371: step 1701, loss 0.519224.
Train: 2018-07-31T11:18:54.132201: step 1702, loss 0.553328.
Train: 2018-07-31T11:18:54.304011: step 1703, loss 0.553214.
Train: 2018-07-31T11:18:54.475876: step 1704, loss 0.594423.
Train: 2018-07-31T11:18:54.647709: step 1705, loss 0.570341.
Train: 2018-07-31T11:18:54.819515: step 1706, loss 0.605055.
Train: 2018-07-31T11:18:54.991356: step 1707, loss 0.60504.
Train: 2018-07-31T11:18:55.163214: step 1708, loss 0.636121.
Train: 2018-07-31T11:18:55.335019: step 1709, loss 0.578722.
Train: 2018-07-31T11:18:55.506853: step 1710, loss 0.543942.
Test: 2018-07-31T11:18:55.975494: step 1710, loss 0.55522.
Train: 2018-07-31T11:18:56.147329: step 1711, loss 0.578489.
Train: 2018-07-31T11:18:56.319197: step 1712, loss 0.574813.
Train: 2018-07-31T11:18:56.490998: step 1713, loss 0.621744.
Train: 2018-07-31T11:18:56.662863: step 1714, loss 0.526053.
Train: 2018-07-31T11:18:56.834698: step 1715, loss 0.543375.
Train: 2018-07-31T11:18:57.006503: step 1716, loss 0.569391.
Train: 2018-07-31T11:18:57.162717: step 1717, loss 0.560682.
Train: 2018-07-31T11:18:57.334551: step 1718, loss 0.50846.
Train: 2018-07-31T11:18:57.506386: step 1719, loss 0.588705.
Train: 2018-07-31T11:18:57.678254: step 1720, loss 0.6042.
Test: 2018-07-31T11:18:58.162511: step 1720, loss 0.554745.
Train: 2018-07-31T11:18:58.334347: step 1721, loss 0.54322.
Train: 2018-07-31T11:18:58.506152: step 1722, loss 0.569317.
Train: 2018-07-31T11:18:58.678010: step 1723, loss 0.612874.
Train: 2018-07-31T11:18:58.834200: step 1724, loss 0.534456.
Train: 2018-07-31T11:18:59.021686: step 1725, loss 0.4996.
Train: 2018-07-31T11:18:59.177899: step 1726, loss 0.639005.
Train: 2018-07-31T11:18:59.365326: step 1727, loss 0.534288.
Train: 2018-07-31T11:18:59.521539: step 1728, loss 0.595346.
Train: 2018-07-31T11:18:59.693373: step 1729, loss 0.49939.
Train: 2018-07-31T11:18:59.880854: step 1730, loss 0.604022.
Test: 2018-07-31T11:19:00.349469: step 1730, loss 0.554442.
Train: 2018-07-31T11:19:00.536928: step 1731, loss 0.569048.
Train: 2018-07-31T11:19:00.693139: step 1732, loss 0.621396.
Train: 2018-07-31T11:19:00.864975: step 1733, loss 0.638842.
Train: 2018-07-31T11:19:01.036810: step 1734, loss 0.534021.
Train: 2018-07-31T11:19:01.208674: step 1735, loss 0.638124.
Train: 2018-07-31T11:19:01.380513: step 1736, loss 0.516433.
Train: 2018-07-31T11:19:01.552345: step 1737, loss 0.595144.
Train: 2018-07-31T11:19:01.724179: step 1738, loss 0.556651.
Train: 2018-07-31T11:19:01.895983: step 1739, loss 0.570383.
Train: 2018-07-31T11:19:02.052240: step 1740, loss 0.594412.
Test: 2018-07-31T11:19:02.536488: step 1740, loss 0.554126.
Train: 2018-07-31T11:19:02.708293: step 1741, loss 0.614402.
Train: 2018-07-31T11:19:02.895774: step 1742, loss 0.509329.
Train: 2018-07-31T11:19:03.051986: step 1743, loss 0.576898.
Train: 2018-07-31T11:19:03.223797: step 1744, loss 0.602529.
Train: 2018-07-31T11:19:03.395662: step 1745, loss 0.653594.
Train: 2018-07-31T11:19:03.567466: step 1746, loss 0.559806.
Train: 2018-07-31T11:19:03.739301: step 1747, loss 0.534518.
Train: 2018-07-31T11:19:03.895515: step 1748, loss 0.568079.
Train: 2018-07-31T11:19:04.067383: step 1749, loss 0.661051.
Train: 2018-07-31T11:19:04.239214: step 1750, loss 0.694413.
Test: 2018-07-31T11:19:04.707824: step 1750, loss 0.553997.
Train: 2018-07-31T11:19:04.879659: step 1751, loss 0.467488.
Train: 2018-07-31T11:19:05.051493: step 1752, loss 0.608804.
Train: 2018-07-31T11:19:05.223358: step 1753, loss 0.534777.
Train: 2018-07-31T11:19:05.379566: step 1754, loss 0.559674.
Train: 2018-07-31T11:19:05.551377: step 1755, loss 0.559619.
Train: 2018-07-31T11:19:05.723212: step 1756, loss 0.518375.
Train: 2018-07-31T11:19:05.895047: step 1757, loss 0.609798.
Train: 2018-07-31T11:19:06.082502: step 1758, loss 0.5181.
Train: 2018-07-31T11:19:06.254336: step 1759, loss 0.54277.
Train: 2018-07-31T11:19:06.426202: step 1760, loss 0.586705.
Test: 2018-07-31T11:19:06.894842: step 1760, loss 0.553819.
Train: 2018-07-31T11:19:07.066680: step 1761, loss 0.551118.
Train: 2018-07-31T11:19:07.238482: step 1762, loss 0.492486.
Train: 2018-07-31T11:19:07.394725: step 1763, loss 0.49246.
Train: 2018-07-31T11:19:07.566529: step 1764, loss 0.634427.
Train: 2018-07-31T11:19:07.738394: step 1765, loss 0.584535.
Train: 2018-07-31T11:19:07.910200: step 1766, loss 0.567231.
Train: 2018-07-31T11:19:08.082033: step 1767, loss 0.54265.
Train: 2018-07-31T11:19:08.238247: step 1768, loss 0.57418.
Train: 2018-07-31T11:19:08.410112: step 1769, loss 0.59581.
Train: 2018-07-31T11:19:08.581947: step 1770, loss 0.529066.
Test: 2018-07-31T11:19:09.050556: step 1770, loss 0.553293.
Train: 2018-07-31T11:19:09.222422: step 1771, loss 0.558931.
Train: 2018-07-31T11:19:09.394256: step 1772, loss 0.584013.
Train: 2018-07-31T11:19:09.566095: step 1773, loss 0.63988.
Train: 2018-07-31T11:19:09.737926: step 1774, loss 0.598942.
Train: 2018-07-31T11:19:09.925385: step 1775, loss 0.505463.
Train: 2018-07-31T11:19:10.097217: step 1776, loss 0.516732.
Train: 2018-07-31T11:19:10.269021: step 1777, loss 0.594535.
Train: 2018-07-31T11:19:10.440881: step 1778, loss 0.584687.
Train: 2018-07-31T11:19:10.612716: step 1779, loss 0.587065.
Train: 2018-07-31T11:19:10.784556: step 1780, loss 0.507987.
Test: 2018-07-31T11:19:11.253166: step 1780, loss 0.553546.
Train: 2018-07-31T11:19:11.425000: step 1781, loss 0.636789.
Train: 2018-07-31T11:19:11.596837: step 1782, loss 0.525169.
Train: 2018-07-31T11:19:11.753079: step 1783, loss 0.628273.
Train: 2018-07-31T11:19:11.924884: step 1784, loss 0.593847.
Train: 2018-07-31T11:19:12.096719: step 1785, loss 0.533506.
Train: 2018-07-31T11:19:12.268553: step 1786, loss 0.57676.
Train: 2018-07-31T11:19:12.440388: step 1787, loss 0.499729.
Train: 2018-07-31T11:19:12.612254: step 1788, loss 0.533956.
Train: 2018-07-31T11:19:12.768461: step 1789, loss 0.559236.
Train: 2018-07-31T11:19:12.940271: step 1790, loss 0.594054.
Test: 2018-07-31T11:19:13.408941: step 1790, loss 0.553828.
Train: 2018-07-31T11:19:13.580747: step 1791, loss 0.541376.
Train: 2018-07-31T11:19:13.752581: step 1792, loss 0.567185.
Train: 2018-07-31T11:19:13.924416: step 1793, loss 0.60049.
Train: 2018-07-31T11:19:14.096280: step 1794, loss 0.634333.
Train: 2018-07-31T11:19:14.252494: step 1795, loss 0.559548.
Train: 2018-07-31T11:19:14.424331: step 1796, loss 0.576755.
Train: 2018-07-31T11:19:14.596164: step 1797, loss 0.55949.
Train: 2018-07-31T11:19:14.767968: step 1798, loss 0.550869.
Train: 2018-07-31T11:19:14.939803: step 1799, loss 0.54222.
Train: 2018-07-31T11:19:15.111637: step 1800, loss 0.628238.
Test: 2018-07-31T11:19:15.595900: step 1800, loss 0.55356.
Train: 2018-07-31T11:19:16.314481: step 1801, loss 0.576557.
Train: 2018-07-31T11:19:16.486342: step 1802, loss 0.593772.
Train: 2018-07-31T11:19:16.658151: step 1803, loss 0.499278.
Train: 2018-07-31T11:19:16.829985: step 1804, loss 0.516501.
Train: 2018-07-31T11:19:17.001821: step 1805, loss 0.499151.
Train: 2018-07-31T11:19:17.173655: step 1806, loss 0.550611.
Train: 2018-07-31T11:19:17.345489: step 1807, loss 0.64545.
Train: 2018-07-31T11:19:17.517348: step 1808, loss 0.524631.
Train: 2018-07-31T11:19:17.673568: step 1809, loss 0.559131.
Train: 2018-07-31T11:19:17.845372: step 1810, loss 0.52448.
Test: 2018-07-31T11:19:18.314013: step 1810, loss 0.553221.
Train: 2018-07-31T11:19:18.485848: step 1811, loss 0.567748.
Train: 2018-07-31T11:19:18.642060: step 1812, loss 0.567733.
Train: 2018-07-31T11:19:18.813896: step 1813, loss 0.593826.
Train: 2018-07-31T11:19:18.985730: step 1814, loss 0.480595.
Train: 2018-07-31T11:19:19.157565: step 1815, loss 0.480373.
Train: 2018-07-31T11:19:19.329399: step 1816, loss 0.576434.
Train: 2018-07-31T11:19:19.501235: step 1817, loss 0.567658.
Train: 2018-07-31T11:19:19.657448: step 1818, loss 0.506286.
Train: 2018-07-31T11:19:19.829308: step 1819, loss 0.594166.
Train: 2018-07-31T11:19:20.016739: step 1820, loss 0.53216.
Test: 2018-07-31T11:19:20.469787: step 1820, loss 0.552724.
Train: 2018-07-31T11:19:20.641623: step 1821, loss 0.576502.
Train: 2018-07-31T11:19:20.813461: step 1822, loss 0.576508.
Train: 2018-07-31T11:19:20.985262: step 1823, loss 0.55866.
Train: 2018-07-31T11:19:21.141500: step 1824, loss 0.567564.
Train: 2018-07-31T11:19:21.313311: step 1825, loss 0.6391.
Train: 2018-07-31T11:19:21.485146: step 1826, loss 0.603266.
Train: 2018-07-31T11:19:21.657010: step 1827, loss 0.567462.
Train: 2018-07-31T11:19:21.813193: step 1828, loss 0.549585.
Train: 2018-07-31T11:19:21.985029: step 1829, loss 0.603002.
Train: 2018-07-31T11:19:22.156893: step 1830, loss 0.531767.
Test: 2018-07-31T11:19:22.625502: step 1830, loss 0.552415.
Train: 2018-07-31T11:19:22.797370: step 1831, loss 0.585049.
Train: 2018-07-31T11:19:22.984825: step 1832, loss 0.611583.
Train: 2018-07-31T11:19:23.156653: step 1833, loss 0.514115.
Train: 2018-07-31T11:19:23.328499: step 1834, loss 0.602502.
Train: 2018-07-31T11:19:23.484676: step 1835, loss 0.584754.
Train: 2018-07-31T11:19:23.656512: step 1836, loss 0.558273.
Train: 2018-07-31T11:19:23.828377: step 1837, loss 0.575817.
Train: 2018-07-31T11:19:24.000212: step 1838, loss 0.602058.
Train: 2018-07-31T11:19:24.156424: step 1839, loss 0.566955.
Train: 2018-07-31T11:19:24.328230: step 1840, loss 0.645418.
Test: 2018-07-31T11:19:24.812524: step 1840, loss 0.552327.
Train: 2018-07-31T11:19:24.984325: step 1841, loss 0.514744.
Train: 2018-07-31T11:19:25.140569: step 1842, loss 0.540844.
Train: 2018-07-31T11:19:25.312375: step 1843, loss 0.566819.
Train: 2018-07-31T11:19:25.484239: step 1844, loss 0.59271.
Train: 2018-07-31T11:19:25.656074: step 1845, loss 0.566766.
Train: 2018-07-31T11:19:25.827908: step 1846, loss 0.575348.
Train: 2018-07-31T11:19:25.984121: step 1847, loss 0.566718.
Train: 2018-07-31T11:19:26.155951: step 1848, loss 0.601001.
Train: 2018-07-31T11:19:26.327792: step 1849, loss 0.566676.
Train: 2018-07-31T11:19:26.499595: step 1850, loss 0.489793.
Test: 2018-07-31T11:19:26.968237: step 1850, loss 0.552336.
Train: 2018-07-31T11:19:27.140072: step 1851, loss 0.592254.
Train: 2018-07-31T11:19:27.296317: step 1852, loss 0.575152.
Train: 2018-07-31T11:19:27.468149: step 1853, loss 0.506913.
Train: 2018-07-31T11:19:27.639954: step 1854, loss 0.575113.
Train: 2018-07-31T11:19:27.796197: step 1855, loss 0.558028.
Train: 2018-07-31T11:19:27.968003: step 1856, loss 0.61778.
Train: 2018-07-31T11:19:28.139836: step 1857, loss 0.600666.
Train: 2018-07-31T11:19:28.311705: step 1858, loss 0.575037.
Train: 2018-07-31T11:19:28.483537: step 1859, loss 0.600556.
Train: 2018-07-31T11:19:28.655342: step 1860, loss 0.574985.
Test: 2018-07-31T11:19:29.124008: step 1860, loss 0.552264.
Train: 2018-07-31T11:19:29.295846: step 1861, loss 0.591929.
Train: 2018-07-31T11:19:29.467681: step 1862, loss 0.566464.
Train: 2018-07-31T11:19:29.639485: step 1863, loss 0.574908.
Train: 2018-07-31T11:19:29.795700: step 1864, loss 0.532691.
Train: 2018-07-31T11:19:29.967559: step 1865, loss 0.617029.
Train: 2018-07-31T11:19:30.139399: step 1866, loss 0.541171.
Train: 2018-07-31T11:19:30.311203: step 1867, loss 0.499128.
Train: 2018-07-31T11:19:30.483068: step 1868, loss 0.541149.
Train: 2018-07-31T11:19:30.654873: step 1869, loss 0.57481.
Train: 2018-07-31T11:19:30.811087: step 1870, loss 0.541054.
Test: 2018-07-31T11:19:31.295347: step 1870, loss 0.552195.
Train: 2018-07-31T11:19:31.467182: step 1871, loss 0.591696.
Train: 2018-07-31T11:19:31.623397: step 1872, loss 0.540958.
Train: 2018-07-31T11:19:31.795231: step 1873, loss 0.574784.
Train: 2018-07-31T11:19:31.951445: step 1874, loss 0.540859.
Train: 2018-07-31T11:19:32.123279: step 1875, loss 0.557789.
Train: 2018-07-31T11:19:32.295114: step 1876, loss 0.540743.
Train: 2018-07-31T11:19:32.451328: step 1877, loss 0.515097.
Train: 2018-07-31T11:19:32.623193: step 1878, loss 0.557686.
Train: 2018-07-31T11:19:32.794997: step 1879, loss 0.506198.
Train: 2018-07-31T11:19:32.966831: step 1880, loss 0.540392.
Test: 2018-07-31T11:19:33.451092: step 1880, loss 0.551736.
Train: 2018-07-31T11:19:33.607307: step 1881, loss 0.58349.
Train: 2018-07-31T11:19:33.779172: step 1882, loss 0.574871.
Train: 2018-07-31T11:19:33.951006: step 1883, loss 0.514058.
Train: 2018-07-31T11:19:34.122811: step 1884, loss 0.540043.
Train: 2018-07-31T11:19:34.294645: step 1885, loss 0.53121.
Train: 2018-07-31T11:19:34.450858: step 1886, loss 0.49597.
Train: 2018-07-31T11:19:34.622695: step 1887, loss 0.583866.
Train: 2018-07-31T11:19:34.794561: step 1888, loss 0.539687.
Train: 2018-07-31T11:19:34.966394: step 1889, loss 0.592908.
Train: 2018-07-31T11:19:35.122613: step 1890, loss 0.56627.
Test: 2018-07-31T11:19:35.606838: step 1890, loss 0.551333.
Train: 2018-07-31T11:19:35.778703: step 1891, loss 0.485959.
Train: 2018-07-31T11:19:35.934886: step 1892, loss 0.584208.
Train: 2018-07-31T11:19:36.106751: step 1893, loss 0.61119.
Train: 2018-07-31T11:19:36.278556: step 1894, loss 0.494441.
Train: 2018-07-31T11:19:36.450419: step 1895, loss 0.674377.
Train: 2018-07-31T11:19:36.606605: step 1896, loss 0.521315.
Train: 2018-07-31T11:19:36.778441: step 1897, loss 0.521294.
Train: 2018-07-31T11:19:36.950274: step 1898, loss 0.530266.
Train: 2018-07-31T11:19:37.122108: step 1899, loss 0.566321.
Train: 2018-07-31T11:19:37.278353: step 1900, loss 0.629547.
Test: 2018-07-31T11:19:37.778230: step 1900, loss 0.551188.
Train: 2018-07-31T11:19:38.512407: step 1901, loss 0.530198.
Train: 2018-07-31T11:19:38.668622: step 1902, loss 0.647535.
Train: 2018-07-31T11:19:38.840483: step 1903, loss 0.611318.
Train: 2018-07-31T11:19:39.012321: step 1904, loss 0.602173.
Train: 2018-07-31T11:19:39.168504: step 1905, loss 0.548268.
Train: 2018-07-31T11:19:39.340339: step 1906, loss 0.530416.
Train: 2018-07-31T11:19:39.512174: step 1907, loss 0.619603.
Train: 2018-07-31T11:19:39.684042: step 1908, loss 0.557195.
Train: 2018-07-31T11:19:39.855843: step 1909, loss 0.583778.
Train: 2018-07-31T11:19:40.012057: step 1910, loss 0.601371.
Test: 2018-07-31T11:19:40.496348: step 1910, loss 0.551241.
Train: 2018-07-31T11:19:40.652531: step 1911, loss 0.574797.
Train: 2018-07-31T11:19:40.824366: step 1912, loss 0.565962.
Train: 2018-07-31T11:19:40.996232: step 1913, loss 0.504696.
Train: 2018-07-31T11:19:41.152445: step 1914, loss 0.592121.
Train: 2018-07-31T11:19:41.324283: step 1915, loss 0.548476.
Train: 2018-07-31T11:19:41.496085: step 1916, loss 0.583289.
Train: 2018-07-31T11:19:41.652299: step 1917, loss 0.635336.
Train: 2018-07-31T11:19:41.824163: step 1918, loss 0.496621.
Train: 2018-07-31T11:19:41.995968: step 1919, loss 0.557207.
Train: 2018-07-31T11:19:42.167832: step 1920, loss 0.600362.
Test: 2018-07-31T11:19:42.636441: step 1920, loss 0.5514.
Train: 2018-07-31T11:19:42.808310: step 1921, loss 0.591673.
Train: 2018-07-31T11:19:42.980136: step 1922, loss 0.557222.
Train: 2018-07-31T11:19:43.136355: step 1923, loss 0.522908.
Train: 2018-07-31T11:19:43.308193: step 1924, loss 0.514359.
Train: 2018-07-31T11:19:43.480025: step 1925, loss 0.57437.
Train: 2018-07-31T11:19:43.636208: step 1926, loss 0.565785.
Train: 2018-07-31T11:19:43.808043: step 1927, loss 0.608673.
Train: 2018-07-31T11:19:43.979908: step 1928, loss 0.557197.
Train: 2018-07-31T11:19:44.151712: step 1929, loss 0.540056.
Train: 2018-07-31T11:19:44.307926: step 1930, loss 0.60003.
Test: 2018-07-31T11:19:44.792188: step 1930, loss 0.551404.
Train: 2018-07-31T11:19:44.964052: step 1931, loss 0.540055.
Train: 2018-07-31T11:19:45.135887: step 1932, loss 0.557175.
Train: 2018-07-31T11:19:45.307691: step 1933, loss 0.59999.
Train: 2018-07-31T11:19:45.463905: step 1934, loss 0.565723.
Train: 2018-07-31T11:19:45.635740: step 1935, loss 0.582827.
Train: 2018-07-31T11:19:45.807574: step 1936, loss 0.591356.
Train: 2018-07-31T11:19:45.979410: step 1937, loss 0.540086.
Train: 2018-07-31T11:19:46.151245: step 1938, loss 0.574229.
Train: 2018-07-31T11:19:46.307459: step 1939, loss 0.633918.
Train: 2018-07-31T11:19:46.479323: step 1940, loss 0.523124.
Test: 2018-07-31T11:19:46.963554: step 1940, loss 0.551435.
Train: 2018-07-31T11:19:47.151009: step 1941, loss 0.591192.
Train: 2018-07-31T11:19:47.322846: step 1942, loss 0.548685.
Train: 2018-07-31T11:19:47.494679: step 1943, loss 0.514741.
Train: 2018-07-31T11:19:47.650893: step 1944, loss 0.642084.
Train: 2018-07-31T11:19:47.822728: step 1945, loss 0.574137.
Train: 2018-07-31T11:19:47.978941: step 1946, loss 0.599539.
Train: 2018-07-31T11:19:48.150775: step 1947, loss 0.574106.
Train: 2018-07-31T11:19:48.322641: step 1948, loss 0.582534.
Train: 2018-07-31T11:19:48.494475: step 1949, loss 0.557217.
Train: 2018-07-31T11:19:48.650658: step 1950, loss 0.523554.
Test: 2018-07-31T11:19:49.134950: step 1950, loss 0.551542.
Train: 2018-07-31T11:19:49.306789: step 1951, loss 0.540389.
Train: 2018-07-31T11:19:49.478590: step 1952, loss 0.515109.
Train: 2018-07-31T11:19:49.650424: step 1953, loss 0.506584.
Train: 2018-07-31T11:19:49.806638: step 1954, loss 0.599423.
Train: 2018-07-31T11:19:49.978497: step 1955, loss 0.616414.
Train: 2018-07-31T11:19:50.150338: step 1956, loss 0.531672.
Train: 2018-07-31T11:19:50.306554: step 1957, loss 0.55708.
Train: 2018-07-31T11:19:50.462735: step 1958, loss 0.633555.
Train: 2018-07-31T11:19:50.634599: step 1959, loss 0.531556.
Train: 2018-07-31T11:19:50.806438: step 1960, loss 0.591056.
Test: 2018-07-31T11:19:51.275074: step 1960, loss 0.551292.
Train: 2018-07-31T11:19:51.446909: step 1961, loss 0.557031.
Train: 2018-07-31T11:19:51.618713: step 1962, loss 0.548513.
Train: 2018-07-31T11:19:51.774927: step 1963, loss 0.638169.
Train: 2018-07-31T11:19:51.931174: step 1964, loss 0.505973.
Train: 2018-07-31T11:19:52.103005: step 1965, loss 0.591038.
Train: 2018-07-31T11:19:52.274809: step 1966, loss 0.556991.
Train: 2018-07-31T11:19:52.446675: step 1967, loss 0.548472.
Train: 2018-07-31T11:19:52.618492: step 1968, loss 0.574002.
Train: 2018-07-31T11:19:52.774723: step 1969, loss 0.514367.
Train: 2018-07-31T11:19:52.946552: step 1970, loss 0.591063.
Test: 2018-07-31T11:19:53.415198: step 1970, loss 0.551165.
Train: 2018-07-31T11:19:53.571412: step 1971, loss 0.599612.
Train: 2018-07-31T11:19:53.743217: step 1972, loss 0.548382.
Train: 2018-07-31T11:19:53.915052: step 1973, loss 0.582531.
Train: 2018-07-31T11:19:54.071295: step 1974, loss 0.539822.
Train: 2018-07-31T11:19:54.243123: step 1975, loss 0.582527.
Train: 2018-07-31T11:19:54.414933: step 1976, loss 0.531244.
Train: 2018-07-31T11:19:54.571148: step 1977, loss 0.556869.
Train: 2018-07-31T11:19:54.742983: step 1978, loss 0.556853.
Train: 2018-07-31T11:19:54.914818: step 1979, loss 0.573981.
Train: 2018-07-31T11:19:55.071060: step 1980, loss 0.53109.
Test: 2018-07-31T11:19:55.555325: step 1980, loss 0.55101.
Train: 2018-07-31T11:19:55.727126: step 1981, loss 0.539626.
Train: 2018-07-31T11:19:55.898991: step 1982, loss 0.470728.
Train: 2018-07-31T11:19:56.070827: step 1983, loss 0.556753.
Train: 2018-07-31T11:19:56.227010: step 1984, loss 0.608713.
Train: 2018-07-31T11:19:56.398844: step 1985, loss 0.548023.
Train: 2018-07-31T11:19:56.570709: step 1986, loss 0.574089.
Train: 2018-07-31T11:19:56.726922: step 1987, loss 0.643829.
Train: 2018-07-31T11:19:56.898728: step 1988, loss 0.530523.
Train: 2018-07-31T11:19:57.054942: step 1989, loss 0.608972.
Train: 2018-07-31T11:19:57.226775: step 1990, loss 0.600228.
Test: 2018-07-31T11:19:57.711037: step 1990, loss 0.550782.
Train: 2018-07-31T11:19:57.882872: step 1991, loss 0.608879.
Train: 2018-07-31T11:19:58.039086: step 1992, loss 0.530601.
Train: 2018-07-31T11:19:58.210950: step 1993, loss 0.565337.
Train: 2018-07-31T11:19:58.367164: step 1994, loss 0.617327.
Train: 2018-07-31T11:19:58.538968: step 1995, loss 0.573966.
Train: 2018-07-31T11:19:58.695182: step 1996, loss 0.539419.
Train: 2018-07-31T11:19:58.867016: step 1997, loss 0.539448.
Train: 2018-07-31T11:19:59.038876: step 1998, loss 0.565294.
Train: 2018-07-31T11:19:59.210686: step 1999, loss 0.573893.
Train: 2018-07-31T11:19:59.398151: step 2000, loss 0.539488.
Test: 2018-07-31T11:19:59.880831: step 2000, loss 0.550879.
Train: 2018-07-31T11:20:00.660529: step 2001, loss 0.5137.
Train: 2018-07-31T11:20:00.840520: step 2002, loss 0.530857.
Train: 2018-07-31T11:20:01.010496: step 2003, loss 0.573881.
Train: 2018-07-31T11:20:01.174911: step 2004, loss 0.565261.
Train: 2018-07-31T11:20:01.340474: step 2005, loss 0.556623.
Train: 2018-07-31T11:20:01.510875: step 2006, loss 0.62575.
Train: 2018-07-31T11:20:01.690582: step 2007, loss 0.530688.
Train: 2018-07-31T11:20:01.860580: step 2008, loss 0.556598.
Train: 2018-07-31T11:20:02.029836: step 2009, loss 0.556589.
Train: 2018-07-31T11:20:02.186071: step 2010, loss 0.539276.
Test: 2018-07-31T11:20:02.661279: step 2010, loss 0.550724.
Train: 2018-07-31T11:20:02.833115: step 2011, loss 0.565227.
Train: 2018-07-31T11:20:03.004920: step 2012, loss 0.625896.
Train: 2018-07-31T11:20:03.176754: step 2013, loss 0.547891.
Train: 2018-07-31T11:20:03.332978: step 2014, loss 0.565211.
Train: 2018-07-31T11:20:03.504803: step 2015, loss 0.617164.
Train: 2018-07-31T11:20:03.676667: step 2016, loss 0.547899.
Train: 2018-07-31T11:20:03.848496: step 2017, loss 0.547906.
Train: 2018-07-31T11:20:04.004686: step 2018, loss 0.504717.
Train: 2018-07-31T11:20:04.176519: step 2019, loss 0.547889.
Train: 2018-07-31T11:20:04.363986: step 2020, loss 0.643063.
Test: 2018-07-31T11:20:04.863859: step 2020, loss 0.550685.
Train: 2018-07-31T11:20:05.113800: step 2021, loss 0.625707.
Train: 2018-07-31T11:20:05.394985: step 2022, loss 0.539264.
Train: 2018-07-31T11:20:05.582445: step 2023, loss 0.522045.
Train: 2018-07-31T11:20:05.801140: step 2024, loss 0.599626.
Train: 2018-07-31T11:20:05.957354: step 2025, loss 0.573753.
Train: 2018-07-31T11:20:06.129188: step 2026, loss 0.633957.
Train: 2018-07-31T11:20:06.301049: step 2027, loss 0.539381.
Train: 2018-07-31T11:20:06.472887: step 2028, loss 0.470868.
Train: 2018-07-31T11:20:06.691555: step 2029, loss 0.470813.
Train: 2018-07-31T11:20:06.863421: step 2030, loss 0.573707.
Test: 2018-07-31T11:20:07.332031: step 2030, loss 0.550688.
Train: 2018-07-31T11:20:07.503867: step 2031, loss 0.522054.
Train: 2018-07-31T11:20:07.675700: step 2032, loss 0.521937.
Train: 2018-07-31T11:20:07.847565: step 2033, loss 0.582431.
Train: 2018-07-31T11:20:08.113098: step 2034, loss 0.513008.
Train: 2018-07-31T11:20:08.558921: step 2035, loss 0.565113.
Train: 2018-07-31T11:20:08.777619: step 2036, loss 0.53891.
Train: 2018-07-31T11:20:08.933861: step 2037, loss 0.53884.
Train: 2018-07-31T11:20:09.121320: step 2038, loss 0.54756.
Train: 2018-07-31T11:20:09.277501: step 2039, loss 0.565152.
Train: 2018-07-31T11:20:09.449335: step 2040, loss 0.494452.
Test: 2018-07-31T11:20:09.918005: step 2040, loss 0.550329.
Train: 2018-07-31T11:20:10.089810: step 2041, loss 0.582927.
Train: 2018-07-31T11:20:10.261646: step 2042, loss 0.547413.
Train: 2018-07-31T11:20:10.417889: step 2043, loss 0.583056.
Train: 2018-07-31T11:20:10.589723: step 2044, loss 0.484827.
Train: 2018-07-31T11:20:10.745937: step 2045, loss 0.511494.
Train: 2018-07-31T11:20:10.917772: step 2046, loss 0.556298.
Train: 2018-07-31T11:20:11.089577: step 2047, loss 0.538262.
Train: 2018-07-31T11:20:11.261410: step 2048, loss 0.610592.
Train: 2018-07-31T11:20:11.433245: step 2049, loss 0.556308.
Train: 2018-07-31T11:20:11.589458: step 2050, loss 0.55631.
Test: 2018-07-31T11:20:12.073750: step 2050, loss 0.550179.
Train: 2018-07-31T11:20:12.292420: step 2051, loss 0.656259.
Train: 2018-07-31T11:20:12.464284: step 2052, loss 0.583536.
Train: 2018-07-31T11:20:12.636089: step 2053, loss 0.637883.
Train: 2018-07-31T11:20:12.792303: step 2054, loss 0.610515.
Train: 2018-07-31T11:20:12.964167: step 2055, loss 0.58328.
Train: 2018-07-31T11:20:13.136002: step 2056, loss 0.583162.
Train: 2018-07-31T11:20:13.292186: step 2057, loss 0.726035.
Train: 2018-07-31T11:20:13.464021: step 2058, loss 0.61836.
Train: 2018-07-31T11:20:13.651501: step 2059, loss 0.582668.
Train: 2018-07-31T11:20:13.823310: step 2060, loss 0.634985.
Test: 2018-07-31T11:20:14.291951: step 2060, loss 0.550436.
Train: 2018-07-31T11:20:14.448165: step 2061, loss 0.530257.
Train: 2018-07-31T11:20:14.635646: step 2062, loss 0.521847.
Train: 2018-07-31T11:20:14.791833: step 2063, loss 0.607832.
Train: 2018-07-31T11:20:14.948047: step 2064, loss 0.556414.
Train: 2018-07-31T11:20:15.119882: step 2065, loss 0.615885.
Train: 2018-07-31T11:20:15.276126: step 2066, loss 0.522736.
Train: 2018-07-31T11:20:15.447960: step 2067, loss 0.539733.
Train: 2018-07-31T11:20:15.619765: step 2068, loss 0.539813.
Train: 2018-07-31T11:20:15.776009: step 2069, loss 0.481264.
Train: 2018-07-31T11:20:15.947813: step 2070, loss 0.52311.
Test: 2018-07-31T11:20:16.432074: step 2070, loss 0.550926.
Train: 2018-07-31T11:20:16.603910: step 2071, loss 0.590115.
Train: 2018-07-31T11:20:16.760124: step 2072, loss 0.581735.
Train: 2018-07-31T11:20:16.931959: step 2073, loss 0.55656.
Train: 2018-07-31T11:20:17.103793: step 2074, loss 0.598528.
Train: 2018-07-31T11:20:17.275627: step 2075, loss 0.606916.
Train: 2018-07-31T11:20:17.447493: step 2076, loss 0.606878.
Train: 2018-07-31T11:20:17.619321: step 2077, loss 0.623562.
Train: 2018-07-31T11:20:17.775541: step 2078, loss 0.556601.
Train: 2018-07-31T11:20:17.947377: step 2079, loss 0.598302.
Train: 2018-07-31T11:20:18.119210: step 2080, loss 0.531711.
Test: 2018-07-31T11:20:18.603441: step 2080, loss 0.551069.
Train: 2018-07-31T11:20:18.775306: step 2081, loss 0.639715.
Train: 2018-07-31T11:20:18.931520: step 2082, loss 0.473888.
Train: 2018-07-31T11:20:19.103356: step 2083, loss 0.523587.
Train: 2018-07-31T11:20:19.275184: step 2084, loss 0.540115.
Train: 2018-07-31T11:20:19.446994: step 2085, loss 0.564962.
Train: 2018-07-31T11:20:19.618859: step 2086, loss 0.581572.
Train: 2018-07-31T11:20:19.775072: step 2087, loss 0.5899.
Train: 2018-07-31T11:20:19.946908: step 2088, loss 0.58158.
Train: 2018-07-31T11:20:20.118741: step 2089, loss 0.51494.
Train: 2018-07-31T11:20:20.290577: step 2090, loss 0.598276.
Test: 2018-07-31T11:20:20.759187: step 2090, loss 0.550913.
Train: 2018-07-31T11:20:20.931052: step 2091, loss 0.589944.
Train: 2018-07-31T11:20:21.102886: step 2092, loss 0.581593.
Train: 2018-07-31T11:20:21.274721: step 2093, loss 0.473015.
Train: 2018-07-31T11:20:21.430935: step 2094, loss 0.615088.
Train: 2018-07-31T11:20:21.602738: step 2095, loss 0.548104.
Train: 2018-07-31T11:20:21.790194: step 2096, loss 0.556459.
Train: 2018-07-31T11:20:21.946433: step 2097, loss 0.573238.
Train: 2018-07-31T11:20:22.118243: step 2098, loss 0.548003.
Train: 2018-07-31T11:20:22.290078: step 2099, loss 0.564816.
Train: 2018-07-31T11:20:22.446292: step 2100, loss 0.606995.
Test: 2018-07-31T11:20:22.930553: step 2100, loss 0.550661.
Train: 2018-07-31T11:20:23.742893: step 2101, loss 0.547915.
Train: 2018-07-31T11:20:23.914697: step 2102, loss 0.640843.
Train: 2018-07-31T11:20:24.086563: step 2103, loss 0.539458.
Train: 2018-07-31T11:20:24.258398: step 2104, loss 0.539455.
Train: 2018-07-31T11:20:24.430232: step 2105, loss 0.590126.
Train: 2018-07-31T11:20:24.586415: step 2106, loss 0.497183.
Train: 2018-07-31T11:20:24.758280: step 2107, loss 0.632464.
Train: 2018-07-31T11:20:24.930109: step 2108, loss 0.463212.
Train: 2018-07-31T11:20:25.101920: step 2109, loss 0.530826.
Train: 2018-07-31T11:20:25.273784: step 2110, loss 0.573246.
Test: 2018-07-31T11:20:25.742395: step 2110, loss 0.550456.
Train: 2018-07-31T11:20:25.914259: step 2111, loss 0.590307.
Train: 2018-07-31T11:20:26.070472: step 2112, loss 0.556188.
Train: 2018-07-31T11:20:26.242310: step 2113, loss 0.590378.
Train: 2018-07-31T11:20:26.398521: step 2114, loss 0.601242.
Train: 2018-07-31T11:20:26.570325: step 2115, loss 0.530465.
Train: 2018-07-31T11:20:26.742161: step 2116, loss 0.530432.
Train: 2018-07-31T11:20:26.913996: step 2117, loss 0.530378.
Train: 2018-07-31T11:20:27.070239: step 2118, loss 0.556101.
Train: 2018-07-31T11:20:27.242044: step 2119, loss 0.52162.
Train: 2018-07-31T11:20:27.398281: step 2120, loss 0.538782.
Test: 2018-07-31T11:20:27.882519: step 2120, loss 0.550189.
Train: 2018-07-31T11:20:28.038732: step 2121, loss 0.512721.
Train: 2018-07-31T11:20:28.210597: step 2122, loss 0.564703.
Train: 2018-07-31T11:20:28.382432: step 2123, loss 0.625756.
Train: 2018-07-31T11:20:28.538615: step 2124, loss 0.494838.
Train: 2018-07-31T11:20:28.710480: step 2125, loss 0.599749.
Train: 2018-07-31T11:20:28.882315: step 2126, loss 0.555952.
Train: 2018-07-31T11:20:29.054143: step 2127, loss 0.573512.
Train: 2018-07-31T11:20:29.210334: step 2128, loss 0.520757.
Train: 2018-07-31T11:20:29.382168: step 2129, loss 0.555926.
Train: 2018-07-31T11:20:29.554002: step 2130, loss 0.591213.
Test: 2018-07-31T11:20:30.018619: step 2130, loss 0.549954.
Train: 2018-07-31T11:20:30.190452: step 2131, loss 0.591235.
Train: 2018-07-31T11:20:30.362256: step 2132, loss 0.591234.
Train: 2018-07-31T11:20:30.518470: step 2133, loss 0.59121.
Train: 2018-07-31T11:20:30.690304: step 2134, loss 0.520641.
Train: 2018-07-31T11:20:30.862140: step 2135, loss 0.591153.
Train: 2018-07-31T11:20:31.033975: step 2136, loss 0.564704.
Train: 2018-07-31T11:20:31.205809: step 2137, loss 0.643861.
Train: 2018-07-31T11:20:31.362022: step 2138, loss 0.555903.
Train: 2018-07-31T11:20:31.533856: step 2139, loss 0.538398.
Train: 2018-07-31T11:20:31.705693: step 2140, loss 0.634577.
Test: 2018-07-31T11:20:32.189954: step 2140, loss 0.550039.
Train: 2018-07-31T11:20:32.346197: step 2141, loss 0.564635.
Train: 2018-07-31T11:20:32.518002: step 2142, loss 0.573314.
Train: 2018-07-31T11:20:32.689866: step 2143, loss 0.503928.
Train: 2018-07-31T11:20:32.846080: step 2144, loss 0.616555.
Train: 2018-07-31T11:20:33.017885: step 2145, loss 0.495481.
Train: 2018-07-31T11:20:33.189744: step 2146, loss 0.495518.
Train: 2018-07-31T11:20:33.345933: step 2147, loss 0.590513.
Train: 2018-07-31T11:20:33.517799: step 2148, loss 0.52137.
Train: 2018-07-31T11:20:33.689638: step 2149, loss 0.564583.
Train: 2018-07-31T11:20:33.861467: step 2150, loss 0.599218.
Test: 2018-07-31T11:20:34.345698: step 2150, loss 0.550075.
Train: 2018-07-31T11:20:34.501937: step 2151, loss 0.547259.
Train: 2018-07-31T11:20:34.673777: step 2152, loss 0.590562.
Train: 2018-07-31T11:20:34.845612: step 2153, loss 0.52993.
Train: 2018-07-31T11:20:35.017416: step 2154, loss 0.590559.
Train: 2018-07-31T11:20:35.189281: step 2155, loss 0.555901.
Train: 2018-07-31T11:20:35.345495: step 2156, loss 0.573221.
Train: 2018-07-31T11:20:35.517331: step 2157, loss 0.547236.
Train: 2018-07-31T11:20:35.689164: step 2158, loss 0.599192.
Train: 2018-07-31T11:20:35.860999: step 2159, loss 0.547238.
Train: 2018-07-31T11:20:36.032834: step 2160, loss 0.564542.
Test: 2018-07-31T11:20:36.517064: step 2160, loss 0.550053.
Train: 2018-07-31T11:20:36.688924: step 2161, loss 0.495347.
Train: 2018-07-31T11:20:36.860735: step 2162, loss 0.512586.
Train: 2018-07-31T11:20:37.032570: step 2163, loss 0.607914.
Train: 2018-07-31T11:20:37.204434: step 2164, loss 0.573218.
Train: 2018-07-31T11:20:37.360618: step 2165, loss 0.53847.
Train: 2018-07-31T11:20:37.532482: step 2166, loss 0.686275.
Train: 2018-07-31T11:20:37.704288: step 2167, loss 0.642658.
Train: 2018-07-31T11:20:37.876122: step 2168, loss 0.547204.
Train: 2018-07-31T11:20:38.047987: step 2169, loss 0.599036.
Train: 2018-07-31T11:20:38.204179: step 2170, loss 0.61614.
Test: 2018-07-31T11:20:38.688461: step 2170, loss 0.550133.
Train: 2018-07-31T11:20:38.860268: step 2171, loss 0.538769.
Train: 2018-07-31T11:20:39.032132: step 2172, loss 0.547391.
Train: 2018-07-31T11:20:39.203966: step 2173, loss 0.632736.
Train: 2018-07-31T11:20:39.360149: step 2174, loss 0.598498.
Train: 2018-07-31T11:20:39.531983: step 2175, loss 0.496737.
Train: 2018-07-31T11:20:39.703849: step 2176, loss 0.530683.
Train: 2018-07-31T11:20:39.875678: step 2177, loss 0.530714.
Train: 2018-07-31T11:20:40.031868: step 2178, loss 0.547603.
Train: 2018-07-31T11:20:40.203737: step 2179, loss 0.589838.
Train: 2018-07-31T11:20:40.375562: step 2180, loss 0.539142.
Test: 2018-07-31T11:20:40.859797: step 2180, loss 0.550325.
Train: 2018-07-31T11:20:41.031663: step 2181, loss 0.598292.
Train: 2018-07-31T11:20:41.187871: step 2182, loss 0.547576.
Train: 2018-07-31T11:20:41.359681: step 2183, loss 0.589834.
Train: 2018-07-31T11:20:41.531517: step 2184, loss 0.522215.
Train: 2018-07-31T11:20:41.703381: step 2185, loss 0.640585.
Train: 2018-07-31T11:20:41.875186: step 2186, loss 0.522212.
Train: 2018-07-31T11:20:42.031423: step 2187, loss 0.496844.
Train: 2018-07-31T11:20:42.203265: step 2188, loss 0.58985.
Train: 2018-07-31T11:20:42.375069: step 2189, loss 0.57292.
Train: 2018-07-31T11:20:42.531283: step 2190, loss 0.547476.
Test: 2018-07-31T11:20:43.015575: step 2190, loss 0.550213.
Train: 2018-07-31T11:20:43.187403: step 2191, loss 0.555942.
Train: 2018-07-31T11:20:43.359213: step 2192, loss 0.479407.
Train: 2018-07-31T11:20:43.515459: step 2193, loss 0.50473.
Train: 2018-07-31T11:20:43.687298: step 2194, loss 0.615775.
Train: 2018-07-31T11:20:43.859127: step 2195, loss 0.504338.
Train: 2018-07-31T11:20:44.015334: step 2196, loss 0.59885.
Train: 2018-07-31T11:20:44.187174: step 2197, loss 0.52988.
Train: 2018-07-31T11:20:44.358980: step 2198, loss 0.512474.
Train: 2018-07-31T11:20:44.515193: step 2199, loss 0.625214.
Train: 2018-07-31T11:20:44.687057: step 2200, loss 0.581819.
Test: 2018-07-31T11:20:45.155668: step 2200, loss 0.549823.
Train: 2018-07-31T11:20:45.858628: step 2201, loss 0.529562.
Train: 2018-07-31T11:20:46.014841: step 2202, loss 0.529504.
Train: 2018-07-31T11:20:46.186676: step 2203, loss 0.590669.
Train: 2018-07-31T11:20:46.358535: step 2204, loss 0.581948.
Train: 2018-07-31T11:20:46.530378: step 2205, loss 0.625796.
Train: 2018-07-31T11:20:46.702205: step 2206, loss 0.555661.
Train: 2018-07-31T11:20:46.874042: step 2207, loss 0.546902.
Train: 2018-07-31T11:20:47.030258: step 2208, loss 0.616948.
Train: 2018-07-31T11:20:47.202064: step 2209, loss 0.538169.
Train: 2018-07-31T11:20:47.389520: step 2210, loss 0.564397.
Test: 2018-07-31T11:20:47.858160: step 2210, loss 0.549766.
Train: 2018-07-31T11:20:48.029995: step 2211, loss 0.581854.
Train: 2018-07-31T11:20:48.201846: step 2212, loss 0.47716.
Train: 2018-07-31T11:20:48.358073: step 2213, loss 0.520741.
Train: 2018-07-31T11:20:48.529877: step 2214, loss 0.564385.
Train: 2018-07-31T11:20:48.701738: step 2215, loss 0.47688.
Train: 2018-07-31T11:20:48.873576: step 2216, loss 0.590719.
Train: 2018-07-31T11:20:49.029760: step 2217, loss 0.502873.
Train: 2018-07-31T11:20:49.201625: step 2218, loss 0.537975.
Train: 2018-07-31T11:20:49.357808: step 2219, loss 0.617464.
Train: 2018-07-31T11:20:49.529643: step 2220, loss 0.573289.
Test: 2018-07-31T11:20:49.998313: step 2220, loss 0.549605.
Train: 2018-07-31T11:20:50.170119: step 2221, loss 0.58216.
Train: 2018-07-31T11:20:50.341954: step 2222, loss 0.528994.
Train: 2018-07-31T11:20:50.498199: step 2223, loss 0.502359.
Train: 2018-07-31T11:20:50.670026: step 2224, loss 0.635539.
Train: 2018-07-31T11:20:50.841866: step 2225, loss 0.50225.
Train: 2018-07-31T11:20:50.998079: step 2226, loss 0.519979.
Train: 2018-07-31T11:20:51.169908: step 2227, loss 0.573382.
Train: 2018-07-31T11:20:51.341720: step 2228, loss 0.546636.
Train: 2018-07-31T11:20:51.513585: step 2229, loss 0.555555.
Train: 2018-07-31T11:20:51.685419: step 2230, loss 0.510841.
Test: 2018-07-31T11:20:52.154058: step 2230, loss 0.549507.
Train: 2018-07-31T11:20:52.325894: step 2231, loss 0.618261.
Train: 2018-07-31T11:20:52.497697: step 2232, loss 0.654132.
Train: 2018-07-31T11:20:52.653911: step 2233, loss 0.51975.
Train: 2018-07-31T11:20:52.825747: step 2234, loss 0.591308.
Train: 2018-07-31T11:20:52.997581: step 2235, loss 0.573396.
Train: 2018-07-31T11:20:53.153795: step 2236, loss 0.502034.
Train: 2018-07-31T11:20:53.325629: step 2237, loss 0.609005.
Train: 2018-07-31T11:20:53.497463: step 2238, loss 0.537724.
Train: 2018-07-31T11:20:53.653702: step 2239, loss 0.564415.
Train: 2018-07-31T11:20:53.825542: step 2240, loss 0.582172.
Test: 2018-07-31T11:20:54.297223: step 2240, loss 0.549532.
Train: 2018-07-31T11:20:54.469089: step 2241, loss 0.573262.
Train: 2018-07-31T11:20:54.640895: step 2242, loss 0.590948.
Train: 2018-07-31T11:20:54.812759: step 2243, loss 0.635073.
Train: 2018-07-31T11:20:54.984564: step 2244, loss 0.58195.
Train: 2018-07-31T11:20:55.140809: step 2245, loss 0.573086.
Train: 2018-07-31T11:20:55.312642: step 2246, loss 0.573036.
Train: 2018-07-31T11:20:55.484471: step 2247, loss 0.555551.
Train: 2018-07-31T11:20:55.656280: step 2248, loss 0.581643.
Train: 2018-07-31T11:20:55.828140: step 2249, loss 0.572912.
Train: 2018-07-31T11:20:55.984328: step 2250, loss 0.564237.
Test: 2018-07-31T11:20:56.468621: step 2250, loss 0.5498.
Train: 2018-07-31T11:20:56.624835: step 2251, loss 0.598695.
Train: 2018-07-31T11:20:56.796669: step 2252, loss 0.555636.
Train: 2018-07-31T11:20:56.968474: step 2253, loss 0.581352.
Train: 2018-07-31T11:20:57.140340: step 2254, loss 0.564221.
Train: 2018-07-31T11:20:57.296552: step 2255, loss 0.649417.
Train: 2018-07-31T11:20:57.468387: step 2256, loss 0.606652.
Train: 2018-07-31T11:20:57.640191: step 2257, loss 0.623367.
Train: 2018-07-31T11:20:57.812056: step 2258, loss 0.497031.
Train: 2018-07-31T11:20:57.968270: step 2259, loss 0.547515.
Train: 2018-07-31T11:20:58.140106: step 2260, loss 0.589345.
Test: 2018-07-31T11:20:58.608745: step 2260, loss 0.550332.
Train: 2018-07-31T11:20:58.780581: step 2261, loss 0.56429.
Train: 2018-07-31T11:20:58.952409: step 2262, loss 0.597565.
Train: 2018-07-31T11:20:59.124251: step 2263, loss 0.556022.
Train: 2018-07-31T11:20:59.280456: step 2264, loss 0.564328.
Train: 2018-07-31T11:20:59.436676: step 2265, loss 0.529075.
Train: 2018-07-31T11:20:59.608482: step 2266, loss 0.547817.
Train: 2018-07-31T11:20:59.780315: step 2267, loss 0.564335.
Train: 2018-07-31T11:20:59.936529: step 2268, loss 0.630443.
Train: 2018-07-31T11:21:00.108364: step 2269, loss 0.564336.
Train: 2018-07-31T11:21:00.280199: step 2270, loss 0.506593.
Test: 2018-07-31T11:21:00.764460: step 2270, loss 0.550502.
Train: 2018-07-31T11:21:00.936295: step 2271, loss 0.605609.
Train: 2018-07-31T11:21:01.108130: step 2272, loss 0.547815.
Train: 2018-07-31T11:21:01.279994: step 2273, loss 0.531278.
Train: 2018-07-31T11:21:01.467450: step 2274, loss 0.539489.
Train: 2018-07-31T11:21:01.623634: step 2275, loss 0.54771.
Train: 2018-07-31T11:21:01.795469: step 2276, loss 0.572575.
Train: 2018-07-31T11:21:01.967303: step 2277, loss 0.530951.
Train: 2018-07-31T11:21:02.139170: step 2278, loss 0.597621.
Train: 2018-07-31T11:21:02.310997: step 2279, loss 0.522401.
Train: 2018-07-31T11:21:02.467217: step 2280, loss 0.5642.
Test: 2018-07-31T11:21:02.951474: step 2280, loss 0.550106.
Train: 2018-07-31T11:21:03.123307: step 2281, loss 0.572592.
Train: 2018-07-31T11:21:03.279521: step 2282, loss 0.597875.
Train: 2018-07-31T11:21:03.451361: step 2283, loss 0.530419.
Train: 2018-07-31T11:21:03.623195: step 2284, loss 0.589515.
Train: 2018-07-31T11:21:03.779380: step 2285, loss 0.581078.
Train: 2018-07-31T11:21:03.951244: step 2286, loss 0.547199.
Train: 2018-07-31T11:21:04.107457: step 2287, loss 0.598067.
Train: 2018-07-31T11:21:04.279261: step 2288, loss 0.513217.
Train: 2018-07-31T11:21:04.435500: step 2289, loss 0.496136.
Train: 2018-07-31T11:21:04.622932: step 2290, loss 0.564119.
Test: 2018-07-31T11:21:05.092675: step 2290, loss 0.549802.
Train: 2018-07-31T11:21:05.264479: step 2291, loss 0.658111.
Train: 2018-07-31T11:21:05.436346: step 2292, loss 0.51282.
Train: 2018-07-31T11:21:05.592529: step 2293, loss 0.564107.
Train: 2018-07-31T11:21:05.764363: step 2294, loss 0.564104.
Train: 2018-07-31T11:21:05.936199: step 2295, loss 0.495448.
Train: 2018-07-31T11:21:06.108045: step 2296, loss 0.624318.
Train: 2018-07-31T11:21:06.279897: step 2297, loss 0.521042.
Train: 2018-07-31T11:21:06.436112: step 2298, loss 0.495089.
Train: 2018-07-31T11:21:06.607940: step 2299, loss 0.581401.
Train: 2018-07-31T11:21:06.779751: step 2300, loss 0.512075.
Test: 2018-07-31T11:21:07.264012: step 2300, loss 0.54954.
Train: 2018-07-31T11:21:07.998215: step 2301, loss 0.590197.
Train: 2018-07-31T11:21:08.170050: step 2302, loss 0.686128.
Train: 2018-07-31T11:21:08.341886: step 2303, loss 0.598946.
Train: 2018-07-31T11:21:08.498098: step 2304, loss 0.494509.
Train: 2018-07-31T11:21:08.654312: step 2305, loss 0.529297.
Train: 2018-07-31T11:21:08.826145: step 2306, loss 0.598922.
Train: 2018-07-31T11:21:08.997981: step 2307, loss 0.503152.
Train: 2018-07-31T11:21:09.169846: step 2308, loss 0.598955.
Train: 2018-07-31T11:21:09.354675: step 2309, loss 0.555375.
Train: 2018-07-31T11:21:09.510915: step 2310, loss 0.537932.
Test: 2018-07-31T11:21:09.995176: step 2310, loss 0.549477.
Train: 2018-07-31T11:21:10.167011: step 2311, loss 0.537914.
Train: 2018-07-31T11:21:10.323228: step 2312, loss 0.468008.
Train: 2018-07-31T11:21:10.495059: step 2313, loss 0.494027.
Train: 2018-07-31T11:21:10.666894: step 2314, loss 0.546534.
Train: 2018-07-31T11:21:10.838731: step 2315, loss 0.555315.
Train: 2018-07-31T11:21:10.994911: step 2316, loss 0.555306.
Train: 2018-07-31T11:21:11.166747: step 2317, loss 0.5553.
Train: 2018-07-31T11:21:11.322961: step 2318, loss 0.617629.
Train: 2018-07-31T11:21:11.494830: step 2319, loss 0.671198.
Train: 2018-07-31T11:21:11.666660: step 2320, loss 0.555292.
Test: 2018-07-31T11:21:12.150891: step 2320, loss 0.549286.
Train: 2018-07-31T11:21:12.322726: step 2321, loss 0.573083.
Train: 2018-07-31T11:21:12.478940: step 2322, loss 0.537516.
Train: 2018-07-31T11:21:12.650804: step 2323, loss 0.617437.
Train: 2018-07-31T11:21:12.806988: step 2324, loss 0.519835.
Train: 2018-07-31T11:21:12.978847: step 2325, loss 0.564136.
Train: 2018-07-31T11:21:13.150688: step 2326, loss 0.572971.
Train: 2018-07-31T11:21:13.322491: step 2327, loss 0.511114.
Train: 2018-07-31T11:21:13.478739: step 2328, loss 0.493458.
Train: 2018-07-31T11:21:13.650540: step 2329, loss 0.475706.
Train: 2018-07-31T11:21:13.822376: step 2330, loss 0.564132.
Test: 2018-07-31T11:21:14.306666: step 2330, loss 0.549271.
Train: 2018-07-31T11:21:14.478495: step 2331, loss 0.564146.
Train: 2018-07-31T11:21:14.634715: step 2332, loss 0.608646.
Train: 2018-07-31T11:21:14.806549: step 2333, loss 0.573063.
Train: 2018-07-31T11:21:14.962733: step 2334, loss 0.501833.
Train: 2018-07-31T11:21:15.134598: step 2335, loss 0.510682.
Train: 2018-07-31T11:21:15.306403: step 2336, loss 0.510596.
Train: 2018-07-31T11:21:15.478237: step 2337, loss 0.546297.
Train: 2018-07-31T11:21:15.650072: step 2338, loss 0.483448.
Train: 2018-07-31T11:21:15.806309: step 2339, loss 0.564265.
Train: 2018-07-31T11:21:15.978145: step 2340, loss 0.555264.
Test: 2018-07-31T11:21:16.446759: step 2340, loss 0.54916.
Train: 2018-07-31T11:21:16.618594: step 2341, loss 0.591497.
Train: 2018-07-31T11:21:16.790429: step 2342, loss 0.609698.
Train: 2018-07-31T11:21:16.946674: step 2343, loss 0.582494.
Train: 2018-07-31T11:21:17.118508: step 2344, loss 0.573411.
Train: 2018-07-31T11:21:17.290314: step 2345, loss 0.627777.
Train: 2018-07-31T11:21:17.446556: step 2346, loss 0.627618.
Train: 2018-07-31T11:21:17.618360: step 2347, loss 0.564257.
Train: 2018-07-31T11:21:17.774604: step 2348, loss 0.600167.
Train: 2018-07-31T11:21:17.946439: step 2349, loss 0.519407.
Train: 2018-07-31T11:21:18.118279: step 2350, loss 0.644507.
Test: 2018-07-31T11:21:18.586913: step 2350, loss 0.549213.
Train: 2018-07-31T11:21:18.758754: step 2351, loss 0.564105.
Train: 2018-07-31T11:21:18.914957: step 2352, loss 0.564071.
Train: 2018-07-31T11:21:19.086768: step 2353, loss 0.58169.
Train: 2018-07-31T11:21:19.274253: step 2354, loss 0.572807.
Train: 2018-07-31T11:21:19.430437: step 2355, loss 0.432635.
Train: 2018-07-31T11:21:19.602271: step 2356, loss 0.590237.
Train: 2018-07-31T11:21:19.758516: step 2357, loss 0.563979.
Train: 2018-07-31T11:21:19.930319: step 2358, loss 0.590149.
Train: 2018-07-31T11:21:20.102188: step 2359, loss 0.624933.
Train: 2018-07-31T11:21:20.274019: step 2360, loss 0.537897.
Test: 2018-07-31T11:21:20.742659: step 2360, loss 0.549429.
Train: 2018-07-31T11:21:20.914464: step 2361, loss 0.563942.
Train: 2018-07-31T11:21:21.086329: step 2362, loss 0.503397.
Train: 2018-07-31T11:21:21.258163: step 2363, loss 0.589862.
Train: 2018-07-31T11:21:21.414346: step 2364, loss 0.503494.
Train: 2018-07-31T11:21:21.586211: step 2365, loss 0.555291.
Train: 2018-07-31T11:21:21.742395: step 2366, loss 0.503458.
Train: 2018-07-31T11:21:21.914260: step 2367, loss 0.676386.
Train: 2018-07-31T11:21:22.070474: step 2368, loss 0.624417.
Train: 2018-07-31T11:21:22.242279: step 2369, loss 0.572539.
Train: 2018-07-31T11:21:22.414113: step 2370, loss 0.589727.
Test: 2018-07-31T11:21:22.882753: step 2370, loss 0.54953.
Train: 2018-07-31T11:21:23.054587: step 2371, loss 0.52957.
Train: 2018-07-31T11:21:23.226456: step 2372, loss 0.572476.
Train: 2018-07-31T11:21:23.382636: step 2373, loss 0.623808.
Train: 2018-07-31T11:21:23.554471: step 2374, loss 0.504155.
Train: 2018-07-31T11:21:23.726305: step 2375, loss 0.623584.
Train: 2018-07-31T11:21:23.898170: step 2376, loss 0.563903.
Train: 2018-07-31T11:21:24.069975: step 2377, loss 0.580886.
Train: 2018-07-31T11:21:24.226188: step 2378, loss 0.68253.
Train: 2018-07-31T11:21:24.398024: step 2379, loss 0.563919.
Train: 2018-07-31T11:21:24.569859: step 2380, loss 0.530314.
Test: 2018-07-31T11:21:25.038528: step 2380, loss 0.549904.
Train: 2018-07-31T11:21:25.210334: step 2381, loss 0.580708.
Train: 2018-07-31T11:21:25.382197: step 2382, loss 0.589036.
Train: 2018-07-31T11:21:25.554038: step 2383, loss 0.597318.
Train: 2018-07-31T11:21:25.710241: step 2384, loss 0.547364.
Train: 2018-07-31T11:21:25.882051: step 2385, loss 0.588878.
Train: 2018-07-31T11:21:26.038264: step 2386, loss 0.555745.
Train: 2018-07-31T11:21:26.210123: step 2387, loss 0.5888.
Train: 2018-07-31T11:21:26.366312: step 2388, loss 0.547566.
Train: 2018-07-31T11:21:26.538177: step 2389, loss 0.522908.
Train: 2018-07-31T11:21:26.709983: step 2390, loss 0.531137.
Test: 2018-07-31T11:21:27.194273: step 2390, loss 0.550248.
Train: 2018-07-31T11:21:27.366109: step 2391, loss 0.564042.
Train: 2018-07-31T11:21:27.537913: step 2392, loss 0.58052.
Train: 2018-07-31T11:21:27.709777: step 2393, loss 0.597025.
Train: 2018-07-31T11:21:27.865991: step 2394, loss 0.531012.
Train: 2018-07-31T11:21:28.037796: step 2395, loss 0.522708.
Train: 2018-07-31T11:21:28.209633: step 2396, loss 0.630204.
Train: 2018-07-31T11:21:28.365874: step 2397, loss 0.61367.
Train: 2018-07-31T11:21:28.537703: step 2398, loss 0.48949.
Train: 2018-07-31T11:21:28.693893: step 2399, loss 0.58055.
Train: 2018-07-31T11:21:28.865728: step 2400, loss 0.580557.
Test: 2018-07-31T11:21:29.349989: step 2400, loss 0.550048.
Train: 2018-07-31T11:21:30.068600: step 2401, loss 0.547347.
Train: 2018-07-31T11:21:30.240407: step 2402, loss 0.580572.
Train: 2018-07-31T11:21:30.396643: step 2403, loss 0.630503.
Train: 2018-07-31T11:21:30.568483: step 2404, loss 0.547305.
Train: 2018-07-31T11:21:30.740318: step 2405, loss 0.622151.
Train: 2018-07-31T11:21:30.912154: step 2406, loss 0.547326.
Train: 2018-07-31T11:21:31.083957: step 2407, loss 0.563942.
Train: 2018-07-31T11:21:31.240202: step 2408, loss 0.622049.
Train: 2018-07-31T11:21:31.412005: step 2409, loss 0.588818.
Train: 2018-07-31T11:21:31.583871: step 2410, loss 0.597064.
Test: 2018-07-31T11:21:32.052481: step 2410, loss 0.550141.
Train: 2018-07-31T11:21:32.208724: step 2411, loss 0.539195.
Train: 2018-07-31T11:21:32.380530: step 2412, loss 0.497977.
Train: 2018-07-31T11:21:32.552388: step 2413, loss 0.530949.
Train: 2018-07-31T11:21:32.724198: step 2414, loss 0.539152.
Train: 2018-07-31T11:21:32.896063: step 2415, loss 0.580512.
Train: 2018-07-31T11:21:33.052271: step 2416, loss 0.493099.
Train: 2018-07-31T11:21:33.208460: step 2417, loss 0.530594.
Train: 2018-07-31T11:21:33.380325: step 2418, loss 0.58895.
Train: 2018-07-31T11:21:33.552166: step 2419, loss 0.572241.
Train: 2018-07-31T11:21:33.723964: step 2420, loss 0.639485.
Test: 2018-07-31T11:21:34.208257: step 2420, loss 0.549751.
Train: 2018-07-31T11:21:34.364469: step 2421, loss 0.639539.
Train: 2018-07-31T11:21:34.536273: step 2422, loss 0.589052.
Train: 2018-07-31T11:21:34.692519: step 2423, loss 0.505081.
Train: 2018-07-31T11:21:34.864353: step 2424, loss 0.58903.
Train: 2018-07-31T11:21:35.036187: step 2425, loss 0.589024.
Train: 2018-07-31T11:21:35.208021: step 2426, loss 0.597401.
Train: 2018-07-31T11:21:35.379826: step 2427, loss 0.530318.
Train: 2018-07-31T11:21:35.551662: step 2428, loss 0.555461.
Train: 2018-07-31T11:21:35.707907: step 2429, loss 0.64764.
Train: 2018-07-31T11:21:35.879734: step 2430, loss 0.614046.
Test: 2018-07-31T11:21:36.363970: step 2430, loss 0.549877.
Train: 2018-07-31T11:21:36.520184: step 2431, loss 0.538816.
Train: 2018-07-31T11:21:36.692050: step 2432, loss 0.5972.
Train: 2018-07-31T11:21:36.863886: step 2433, loss 0.472376.
Train: 2018-07-31T11:21:37.035689: step 2434, loss 0.638761.
Train: 2018-07-31T11:21:37.207524: step 2435, loss 0.58881.
Train: 2018-07-31T11:21:37.363737: step 2436, loss 0.538982.
Train: 2018-07-31T11:21:37.535572: step 2437, loss 0.547294.
Train: 2018-07-31T11:21:37.691786: step 2438, loss 0.646837.
Train: 2018-07-31T11:21:37.863620: step 2439, loss 0.654992.
Train: 2018-07-31T11:21:38.035484: step 2440, loss 0.539154.
Test: 2018-07-31T11:21:38.504125: step 2440, loss 0.550142.
Train: 2018-07-31T11:21:38.675930: step 2441, loss 0.530992.
Train: 2018-07-31T11:21:38.847765: step 2442, loss 0.539263.
Train: 2018-07-31T11:21:39.019625: step 2443, loss 0.662632.
Train: 2018-07-31T11:21:39.175812: step 2444, loss 0.473697.
Train: 2018-07-31T11:21:39.347681: step 2445, loss 0.613203.
Train: 2018-07-31T11:21:39.519514: step 2446, loss 0.55576.
Train: 2018-07-31T11:21:39.675725: step 2447, loss 0.572162.
Train: 2018-07-31T11:21:39.847530: step 2448, loss 0.563961.
Train: 2018-07-31T11:21:40.019365: step 2449, loss 0.522962.
Train: 2018-07-31T11:21:40.175579: step 2450, loss 0.506481.
Test: 2018-07-31T11:21:40.644249: step 2450, loss 0.550136.
Train: 2018-07-31T11:21:40.816084: step 2451, loss 0.481617.
Train: 2018-07-31T11:21:40.987918: step 2452, loss 0.530818.
Train: 2018-07-31T11:21:41.159724: step 2453, loss 0.530622.
Train: 2018-07-31T11:21:41.315966: step 2454, loss 0.572156.
Train: 2018-07-31T11:21:41.487771: step 2455, loss 0.471532.
Train: 2018-07-31T11:21:41.659636: step 2456, loss 0.589066.
Train: 2018-07-31T11:21:41.831474: step 2457, loss 0.580696.
Train: 2018-07-31T11:21:41.987654: step 2458, loss 0.529658.
Train: 2018-07-31T11:21:42.159489: step 2459, loss 0.606491.
Train: 2018-07-31T11:21:42.331324: step 2460, loss 0.529385.
Test: 2018-07-31T11:21:42.815585: step 2460, loss 0.549289.
Train: 2018-07-31T11:21:42.971799: step 2461, loss 0.537875.
Train: 2018-07-31T11:21:43.143663: step 2462, loss 0.494559.
Train: 2018-07-31T11:21:43.315469: step 2463, loss 0.528985.
Train: 2018-07-31T11:21:43.471715: step 2464, loss 0.555013.
Train: 2018-07-31T11:21:43.643517: step 2465, loss 0.625116.
Train: 2018-07-31T11:21:43.815351: step 2466, loss 0.519826.
Train: 2018-07-31T11:21:43.971598: step 2467, loss 0.537338.
Train: 2018-07-31T11:21:44.143399: step 2468, loss 0.519584.
Train: 2018-07-31T11:21:44.315265: step 2469, loss 0.546079.
Train: 2018-07-31T11:21:44.487070: step 2470, loss 0.474796.
Test: 2018-07-31T11:21:44.955739: step 2470, loss 0.548915.
Train: 2018-07-31T11:21:45.127544: step 2471, loss 0.572846.
Train: 2018-07-31T11:21:45.299408: step 2472, loss 0.528015.
Train: 2018-07-31T11:21:45.471247: step 2473, loss 0.491862.
Train: 2018-07-31T11:21:45.643078: step 2474, loss 0.582143.
Train: 2018-07-31T11:21:45.799262: step 2475, loss 0.518643.
Train: 2018-07-31T11:21:45.971097: step 2476, loss 0.555009.
Train: 2018-07-31T11:21:46.142967: step 2477, loss 0.472678.
Train: 2018-07-31T11:21:46.299144: step 2478, loss 0.536678.
Train: 2018-07-31T11:21:46.471013: step 2479, loss 0.610439.
Train: 2018-07-31T11:21:46.642844: step 2480, loss 0.527362.
Test: 2018-07-31T11:21:47.111484: step 2480, loss 0.548873.
Train: 2018-07-31T11:21:47.283290: step 2481, loss 0.601485.
Train: 2018-07-31T11:21:47.455158: step 2482, loss 0.592272.
Train: 2018-07-31T11:21:47.626959: step 2483, loss 0.666574.
Train: 2018-07-31T11:21:47.783172: step 2484, loss 0.518044.
Train: 2018-07-31T11:21:47.955008: step 2485, loss 0.545849.
Train: 2018-07-31T11:21:48.126842: step 2486, loss 0.52735.
Train: 2018-07-31T11:21:48.283055: step 2487, loss 0.564329.
Train: 2018-07-31T11:21:48.454889: step 2488, loss 0.555077.
Train: 2018-07-31T11:21:48.626754: step 2489, loss 0.545841.
Train: 2018-07-31T11:21:48.798595: step 2490, loss 0.591924.
Test: 2018-07-31T11:21:49.267230: step 2490, loss 0.548832.
Train: 2018-07-31T11:21:49.439065: step 2491, loss 0.564244.
Train: 2018-07-31T11:21:49.610869: step 2492, loss 0.610144.
Train: 2018-07-31T11:21:49.782704: step 2493, loss 0.500035.
Train: 2018-07-31T11:21:49.938918: step 2494, loss 0.619023.
Train: 2018-07-31T11:21:50.110752: step 2495, loss 0.545851.
Train: 2018-07-31T11:21:50.282617: step 2496, loss 0.591364.
Train: 2018-07-31T11:21:50.454422: step 2497, loss 0.591243.
Train: 2018-07-31T11:21:50.626286: step 2498, loss 0.527793.
Train: 2018-07-31T11:21:50.782469: step 2499, loss 0.527853.
Train: 2018-07-31T11:21:50.954304: step 2500, loss 0.545905.
Test: 2018-07-31T11:21:51.438596: step 2500, loss 0.548837.
Train: 2018-07-31T11:21:52.188420: step 2501, loss 0.599851.
Train: 2018-07-31T11:21:52.360256: step 2502, loss 0.554897.
Train: 2018-07-31T11:21:52.547711: step 2503, loss 0.581741.
Train: 2018-07-31T11:21:52.703928: step 2504, loss 0.501321.
Train: 2018-07-31T11:21:52.875760: step 2505, loss 0.63514.
Train: 2018-07-31T11:21:53.047594: step 2506, loss 0.572672.
Train: 2018-07-31T11:21:53.219399: step 2507, loss 0.501663.
Train: 2018-07-31T11:21:53.391234: step 2508, loss 0.643462.
Train: 2018-07-31T11:21:53.547447: step 2509, loss 0.590215.
Train: 2018-07-31T11:21:53.719281: step 2510, loss 0.581302.
Test: 2018-07-31T11:21:54.187955: step 2510, loss 0.548982.
Train: 2018-07-31T11:21:54.359756: step 2511, loss 0.607539.
Train: 2018-07-31T11:21:54.531592: step 2512, loss 0.511229.
Train: 2018-07-31T11:21:54.687836: step 2513, loss 0.563641.
Train: 2018-07-31T11:21:54.859669: step 2514, loss 0.537556.
Train: 2018-07-31T11:21:55.015884: step 2515, loss 0.589652.
Train: 2018-07-31T11:21:55.187713: step 2516, loss 0.503019.
Train: 2018-07-31T11:21:55.343903: step 2517, loss 0.529014.
Train: 2018-07-31T11:21:55.515767: step 2518, loss 0.563612.
Train: 2018-07-31T11:21:55.671982: step 2519, loss 0.580906.
Train: 2018-07-31T11:21:55.843818: step 2520, loss 0.580895.
Test: 2018-07-31T11:21:56.328076: step 2520, loss 0.549139.
Train: 2018-07-31T11:21:56.484259: step 2521, loss 0.537693.
Train: 2018-07-31T11:21:56.656094: step 2522, loss 0.503157.
Train: 2018-07-31T11:21:56.827928: step 2523, loss 0.624105.
Train: 2018-07-31T11:21:56.999764: step 2524, loss 0.606799.
Train: 2018-07-31T11:21:57.156007: step 2525, loss 0.51182.
Train: 2018-07-31T11:21:57.327812: step 2526, loss 0.572222.
Train: 2018-07-31T11:21:57.499676: step 2527, loss 0.615346.
Train: 2018-07-31T11:21:57.655860: step 2528, loss 0.537746.
Train: 2018-07-31T11:21:57.827725: step 2529, loss 0.64106.
Train: 2018-07-31T11:21:57.999530: step 2530, loss 0.50346.
Test: 2018-07-31T11:21:58.483822: step 2530, loss 0.549208.
Train: 2018-07-31T11:21:58.655625: step 2531, loss 0.563581.
Train: 2018-07-31T11:21:58.811864: step 2532, loss 0.494969.
Train: 2018-07-31T11:21:58.983673: step 2533, loss 0.597909.
Train: 2018-07-31T11:21:59.155509: step 2534, loss 0.623655.
Train: 2018-07-31T11:21:59.327376: step 2535, loss 0.632151.
Train: 2018-07-31T11:21:59.483587: step 2536, loss 0.520821.
Train: 2018-07-31T11:21:59.655422: step 2537, loss 0.546496.
Train: 2018-07-31T11:21:59.811643: step 2538, loss 0.555041.
Train: 2018-07-31T11:21:59.983439: step 2539, loss 0.572098.
Train: 2018-07-31T11:22:00.155274: step 2540, loss 0.572091.
Test: 2018-07-31T11:22:00.623915: step 2540, loss 0.549311.
Train: 2018-07-31T11:22:00.795774: step 2541, loss 0.529513.
Train: 2018-07-31T11:22:00.967618: step 2542, loss 0.606142.
Train: 2018-07-31T11:22:01.139420: step 2543, loss 0.580584.
Train: 2018-07-31T11:22:01.295632: step 2544, loss 0.580567.
Train: 2018-07-31T11:22:01.467468: step 2545, loss 0.606013.
Train: 2018-07-31T11:22:01.639326: step 2546, loss 0.495795.
Train: 2018-07-31T11:22:01.795546: step 2547, loss 0.546632.
Train: 2018-07-31T11:22:01.967351: step 2548, loss 0.563569.
Train: 2018-07-31T11:22:02.139210: step 2549, loss 0.682188.
Train: 2018-07-31T11:22:02.295398: step 2550, loss 0.639657.
Test: 2018-07-31T11:22:02.779690: step 2550, loss 0.549475.
Train: 2018-07-31T11:22:02.935904: step 2551, loss 0.605703.
Train: 2018-07-31T11:22:03.107739: step 2552, loss 0.62233.
Train: 2018-07-31T11:22:03.279543: step 2553, loss 0.563624.
Train: 2018-07-31T11:22:03.451377: step 2554, loss 0.4805.
Train: 2018-07-31T11:22:03.623250: step 2555, loss 0.571959.
Train: 2018-07-31T11:22:03.795078: step 2556, loss 0.56367.
Train: 2018-07-31T11:22:03.951291: step 2557, loss 0.522307.
Train: 2018-07-31T11:22:04.123126: step 2558, loss 0.547132.
Train: 2018-07-31T11:22:04.294930: step 2559, loss 0.571951.
Train: 2018-07-31T11:22:04.451169: step 2560, loss 0.53883.
Test: 2018-07-31T11:22:04.935435: step 2560, loss 0.549779.
Train: 2018-07-31T11:22:05.091619: step 2561, loss 0.580236.
Train: 2018-07-31T11:22:05.263483: step 2562, loss 0.580242.
Train: 2018-07-31T11:22:05.435318: step 2563, loss 0.513858.
Train: 2018-07-31T11:22:05.607122: step 2564, loss 0.571946.
Train: 2018-07-31T11:22:05.778986: step 2565, loss 0.488706.
Train: 2018-07-31T11:22:05.935196: step 2566, loss 0.580302.
Train: 2018-07-31T11:22:06.091384: step 2567, loss 0.617159.
Train: 2018-07-31T11:22:06.247598: step 2568, loss 0.538435.
Train: 2018-07-31T11:22:06.419433: step 2569, loss 0.471213.
Train: 2018-07-31T11:22:06.591301: step 2570, loss 0.622536.
Test: 2018-07-31T11:22:07.122423: step 2570, loss 0.549399.
Train: 2018-07-31T11:22:07.294259: step 2571, loss 0.614206.
Train: 2018-07-31T11:22:07.466088: step 2572, loss 0.588892.
Train: 2018-07-31T11:22:07.637927: step 2573, loss 0.538169.
Train: 2018-07-31T11:22:07.809733: step 2574, loss 0.588916.
Train: 2018-07-31T11:22:07.965946: step 2575, loss 0.605853.
Train: 2018-07-31T11:22:08.137781: step 2576, loss 0.580448.
Train: 2018-07-31T11:22:08.309615: step 2577, loss 0.538163.
Train: 2018-07-31T11:22:08.465862: step 2578, loss 0.470532.
Train: 2018-07-31T11:22:08.637693: step 2579, loss 0.546577.
Train: 2018-07-31T11:22:08.809523: step 2580, loss 0.555022.
Test: 2018-07-31T11:22:09.278139: step 2580, loss 0.549257.
Train: 2018-07-31T11:22:09.449973: step 2581, loss 0.589027.
Train: 2018-07-31T11:22:09.606186: step 2582, loss 0.563501.
Train: 2018-07-31T11:22:09.778021: step 2583, loss 0.503778.
Train: 2018-07-31T11:22:09.949856: step 2584, loss 0.546389.
Train: 2018-07-31T11:22:10.121722: step 2585, loss 0.649235.
Train: 2018-07-31T11:22:10.277935: step 2586, loss 0.537751.
Train: 2018-07-31T11:22:10.449769: step 2587, loss 0.580664.
Train: 2018-07-31T11:22:10.621574: step 2588, loss 0.5463.
Train: 2018-07-31T11:22:10.777812: step 2589, loss 0.537684.
Train: 2018-07-31T11:22:10.965244: step 2590, loss 0.520427.
Test: 2018-07-31T11:22:11.433913: step 2590, loss 0.549034.
Train: 2018-07-31T11:22:11.590097: step 2591, loss 0.520341.
Train: 2018-07-31T11:22:11.761962: step 2592, loss 0.572139.
Train: 2018-07-31T11:22:11.933797: step 2593, loss 0.624184.
Train: 2018-07-31T11:22:12.105601: step 2594, loss 0.572167.
Train: 2018-07-31T11:22:12.261845: step 2595, loss 0.511412.
Train: 2018-07-31T11:22:12.433681: step 2596, loss 0.598258.
Train: 2018-07-31T11:22:12.621136: step 2597, loss 0.520016.
Train: 2018-07-31T11:22:12.777319: step 2598, loss 0.519966.
Train: 2018-07-31T11:22:12.964808: step 2599, loss 0.5635.
Train: 2018-07-31T11:22:13.121014: step 2600, loss 0.467407.
Test: 2018-07-31T11:22:13.605279: step 2600, loss 0.54884.
Train: 2018-07-31T11:22:14.386318: step 2601, loss 0.572283.
Train: 2018-07-31T11:22:14.558152: step 2602, loss 0.537167.
Train: 2018-07-31T11:22:14.729987: step 2603, loss 0.581171.
Train: 2018-07-31T11:22:14.886201: step 2604, loss 0.581219.
Train: 2018-07-31T11:22:15.058035: step 2605, loss 0.572408.
Train: 2018-07-31T11:22:15.229869: step 2606, loss 0.643226.
Train: 2018-07-31T11:22:15.386084: step 2607, loss 0.634308.
Train: 2018-07-31T11:22:15.557948: step 2608, loss 0.519431.
Train: 2018-07-31T11:22:15.714131: step 2609, loss 0.440175.
Train: 2018-07-31T11:22:15.885996: step 2610, loss 0.510615.
Test: 2018-07-31T11:22:16.370228: step 2610, loss 0.548752.
Train: 2018-07-31T11:22:16.542092: step 2611, loss 0.545878.
Train: 2018-07-31T11:22:16.698277: step 2612, loss 0.537002.
Train: 2018-07-31T11:22:16.870135: step 2613, loss 0.590196.
Train: 2018-07-31T11:22:17.041946: step 2614, loss 0.643532.
Train: 2018-07-31T11:22:17.198184: step 2615, loss 0.474806.
Train: 2018-07-31T11:22:17.370023: step 2616, loss 0.563587.
Train: 2018-07-31T11:22:17.526208: step 2617, loss 0.679196.
Train: 2018-07-31T11:22:17.698075: step 2618, loss 0.625719.
Train: 2018-07-31T11:22:17.869907: step 2619, loss 0.510439.
Train: 2018-07-31T11:22:18.041742: step 2620, loss 0.519352.
Test: 2018-07-31T11:22:18.510351: step 2620, loss 0.548743.
Train: 2018-07-31T11:22:18.682216: step 2621, loss 0.598851.
Train: 2018-07-31T11:22:18.838400: step 2622, loss 0.572336.
Train: 2018-07-31T11:22:19.010234: step 2623, loss 0.572309.
Train: 2018-07-31T11:22:19.182068: step 2624, loss 0.545925.
Train: 2018-07-31T11:22:19.353903: step 2625, loss 0.510848.
Train: 2018-07-31T11:22:19.525768: step 2626, loss 0.563484.
Train: 2018-07-31T11:22:19.681952: step 2627, loss 0.519647.
Train: 2018-07-31T11:22:19.853787: step 2628, loss 0.5284.
Train: 2018-07-31T11:22:20.025622: step 2629, loss 0.493258.
Train: 2018-07-31T11:22:20.197456: step 2630, loss 0.589884.
Test: 2018-07-31T11:22:20.666096: step 2630, loss 0.54875.
Train: 2018-07-31T11:22:20.837931: step 2631, loss 0.686808.
Train: 2018-07-31T11:22:21.009767: step 2632, loss 0.581082.
Train: 2018-07-31T11:22:21.181602: step 2633, loss 0.545918.
Train: 2018-07-31T11:22:21.353436: step 2634, loss 0.581011.
Train: 2018-07-31T11:22:21.525300: step 2635, loss 0.537195.
Train: 2018-07-31T11:22:21.681515: step 2636, loss 0.563456.
Train: 2018-07-31T11:22:21.853349: step 2637, loss 0.545974.
Train: 2018-07-31T11:22:22.025154: step 2638, loss 0.615841.
Train: 2018-07-31T11:22:22.181393: step 2639, loss 0.563438.
Train: 2018-07-31T11:22:22.353202: step 2640, loss 0.554727.
Test: 2018-07-31T11:22:22.837497: step 2640, loss 0.548868.
Train: 2018-07-31T11:22:23.009323: step 2641, loss 0.598194.
Train: 2018-07-31T11:22:23.165541: step 2642, loss 0.537392.
Train: 2018-07-31T11:22:23.337346: step 2643, loss 0.528757.
Train: 2018-07-31T11:22:23.509211: step 2644, loss 0.485475.
Train: 2018-07-31T11:22:23.681046: step 2645, loss 0.537406.
Train: 2018-07-31T11:22:23.852851: step 2646, loss 0.667583.
Train: 2018-07-31T11:22:24.024686: step 2647, loss 0.546064.
Train: 2018-07-31T11:22:24.180898: step 2648, loss 0.598084.
Train: 2018-07-31T11:22:24.352764: step 2649, loss 0.546085.
Train: 2018-07-31T11:22:24.508946: step 2650, loss 0.537443.
Test: 2018-07-31T11:22:24.977586: step 2650, loss 0.548911.
Train: 2018-07-31T11:22:25.149423: step 2651, loss 0.606653.
Train: 2018-07-31T11:22:25.321287: step 2652, loss 0.502904.
Train: 2018-07-31T11:22:25.493121: step 2653, loss 0.563394.
Train: 2018-07-31T11:22:25.664926: step 2654, loss 0.572038.
Train: 2018-07-31T11:22:25.821164: step 2655, loss 0.554748.
Train: 2018-07-31T11:22:25.993005: step 2656, loss 0.537456.
Train: 2018-07-31T11:22:26.149189: step 2657, loss 0.597987.
Train: 2018-07-31T11:22:26.336677: step 2658, loss 0.494209.
Train: 2018-07-31T11:22:26.508509: step 2659, loss 0.520103.
Train: 2018-07-31T11:22:26.664693: step 2660, loss 0.624104.
Test: 2018-07-31T11:22:27.148983: step 2660, loss 0.54886.
Train: 2018-07-31T11:22:27.305168: step 2661, loss 0.520008.
Train: 2018-07-31T11:22:27.477033: step 2662, loss 0.554706.
Train: 2018-07-31T11:22:27.648866: step 2663, loss 0.537305.
Train: 2018-07-31T11:22:27.820695: step 2664, loss 0.519849.
Train: 2018-07-31T11:22:27.976915: step 2665, loss 0.589591.
Train: 2018-07-31T11:22:28.148750: step 2666, loss 0.615851.
Train: 2018-07-31T11:22:28.320585: step 2667, loss 0.598372.
Train: 2018-07-31T11:22:28.492390: step 2668, loss 0.51973.
Train: 2018-07-31T11:22:28.664249: step 2669, loss 0.61582.
Train: 2018-07-31T11:22:28.820437: step 2670, loss 0.51976.
Test: 2018-07-31T11:22:29.289109: step 2670, loss 0.548781.
Train: 2018-07-31T11:22:29.460912: step 2671, loss 0.589576.
Train: 2018-07-31T11:22:29.617156: step 2672, loss 0.511068.
Train: 2018-07-31T11:22:29.788990: step 2673, loss 0.563392.
Train: 2018-07-31T11:22:29.960829: step 2674, loss 0.563392.
Train: 2018-07-31T11:22:30.132660: step 2675, loss 0.563391.
Train: 2018-07-31T11:22:30.304465: step 2676, loss 0.537208.
Train: 2018-07-31T11:22:30.460711: step 2677, loss 0.537196.
Train: 2018-07-31T11:22:30.632543: step 2678, loss 0.545915.
Train: 2018-07-31T11:22:30.804348: step 2679, loss 0.589639.
Train: 2018-07-31T11:22:30.960594: step 2680, loss 0.572146.
Test: 2018-07-31T11:22:31.444852: step 2680, loss 0.548739.
Train: 2018-07-31T11:22:31.616657: step 2681, loss 0.633401.
Train: 2018-07-31T11:22:31.772901: step 2682, loss 0.519695.
Train: 2018-07-31T11:22:31.944740: step 2683, loss 0.589584.
Train: 2018-07-31T11:22:32.116541: step 2684, loss 0.580826.
Train: 2018-07-31T11:22:32.272753: step 2685, loss 0.615645.
Train: 2018-07-31T11:22:32.444588: step 2686, loss 0.563362.
Train: 2018-07-31T11:22:32.616449: step 2687, loss 0.493971.
Train: 2018-07-31T11:22:32.788257: step 2688, loss 0.546017.
Train: 2018-07-31T11:22:32.960126: step 2689, loss 0.606675.
Train: 2018-07-31T11:22:33.116306: step 2690, loss 0.615276.
Test: 2018-07-31T11:22:33.600598: step 2690, loss 0.548876.
Train: 2018-07-31T11:22:33.772433: step 2691, loss 0.589251.
Train: 2018-07-31T11:22:33.928647: step 2692, loss 0.597798.
Train: 2018-07-31T11:22:34.100476: step 2693, loss 0.554743.
Train: 2018-07-31T11:22:34.272285: step 2694, loss 0.51192.
Train: 2018-07-31T11:22:34.444119: step 2695, loss 0.597564.
Train: 2018-07-31T11:22:34.615985: step 2696, loss 0.597504.
Train: 2018-07-31T11:22:34.787823: step 2697, loss 0.588903.
Train: 2018-07-31T11:22:34.959625: step 2698, loss 0.588841.
Train: 2018-07-31T11:22:35.131477: step 2699, loss 0.588775.
Train: 2018-07-31T11:22:35.287673: step 2700, loss 0.563347.
Test: 2018-07-31T11:22:35.771965: step 2700, loss 0.549237.
Train: 2018-07-31T11:22:36.490516: step 2701, loss 0.580217.
Train: 2018-07-31T11:22:36.646759: step 2702, loss 0.588589.
Train: 2018-07-31T11:22:36.818596: step 2703, loss 0.613682.
Train: 2018-07-31T11:22:36.990400: step 2704, loss 0.588459.
Train: 2018-07-31T11:22:37.162258: step 2705, loss 0.588389.
Train: 2018-07-31T11:22:37.334102: step 2706, loss 0.521973.
Train: 2018-07-31T11:22:37.505904: step 2707, loss 0.555181.
Train: 2018-07-31T11:22:37.662116: step 2708, loss 0.579991.
Train: 2018-07-31T11:22:37.833985: step 2709, loss 0.571728.
Train: 2018-07-31T11:22:38.005785: step 2710, loss 0.555258.
Test: 2018-07-31T11:22:38.490077: step 2710, loss 0.549723.
Train: 2018-07-31T11:22:38.646262: step 2711, loss 0.497688.
Train: 2018-07-31T11:22:38.818129: step 2712, loss 0.547026.
Train: 2018-07-31T11:22:38.989960: step 2713, loss 0.621183.
Train: 2018-07-31T11:22:39.161766: step 2714, loss 0.612941.
Train: 2018-07-31T11:22:39.317979: step 2715, loss 0.530538.
Train: 2018-07-31T11:22:39.489814: step 2716, loss 0.596433.
Train: 2018-07-31T11:22:39.661649: step 2717, loss 0.55525.
Train: 2018-07-31T11:22:39.802240: step 2718, loss 0.422948.
Train: 2018-07-31T11:22:39.974075: step 2719, loss 0.522145.
Train: 2018-07-31T11:22:40.145934: step 2720, loss 0.662966.
Test: 2018-07-31T11:22:40.614580: step 2720, loss 0.549493.
Train: 2018-07-31T11:22:40.786415: step 2721, loss 0.480319.
Train: 2018-07-31T11:22:40.942631: step 2722, loss 0.538373.
Train: 2018-07-31T11:22:41.114457: step 2723, loss 0.529888.
Train: 2018-07-31T11:22:41.286269: step 2724, loss 0.61376.
Train: 2018-07-31T11:22:41.458136: step 2725, loss 0.571753.
Train: 2018-07-31T11:22:41.614347: step 2726, loss 0.588657.
Train: 2018-07-31T11:22:41.786150: step 2727, loss 0.487158.
Train: 2018-07-31T11:22:41.957985: step 2728, loss 0.486907.
Train: 2018-07-31T11:22:42.114233: step 2729, loss 0.56329.
Train: 2018-07-31T11:22:42.286035: step 2730, loss 0.451997.
Test: 2018-07-31T11:22:42.754673: step 2730, loss 0.548863.
Train: 2018-07-31T11:22:42.926534: step 2731, loss 0.554674.
Train: 2018-07-31T11:22:43.098376: step 2732, loss 0.485371.
Train: 2018-07-31T11:22:43.254588: step 2733, loss 0.53717.
Train: 2018-07-31T11:22:43.426422: step 2734, loss 0.580867.
Train: 2018-07-31T11:22:43.582639: step 2735, loss 0.598604.
Train: 2018-07-31T11:22:43.754441: step 2736, loss 0.563385.
Train: 2018-07-31T11:22:43.926274: step 2737, loss 0.519047.
Train: 2018-07-31T11:22:44.082522: step 2738, loss 0.563436.
Train: 2018-07-31T11:22:44.254353: step 2739, loss 0.545601.
Train: 2018-07-31T11:22:44.426188: step 2740, loss 0.608272.
Test: 2018-07-31T11:22:44.894798: step 2740, loss 0.548482.
Train: 2018-07-31T11:22:45.066632: step 2741, loss 0.554535.
Train: 2018-07-31T11:22:45.222880: step 2742, loss 0.563519.
Train: 2018-07-31T11:22:45.394680: step 2743, loss 0.518569.
Train: 2018-07-31T11:22:45.550924: step 2744, loss 0.55454.
Train: 2018-07-31T11:22:45.722759: step 2745, loss 0.581596.
Train: 2018-07-31T11:22:45.878968: step 2746, loss 0.509424.
Train: 2018-07-31T11:22:46.050777: step 2747, loss 0.536474.
Train: 2018-07-31T11:22:46.222642: step 2748, loss 0.581699.
Train: 2018-07-31T11:22:46.394477: step 2749, loss 0.590779.
Train: 2018-07-31T11:22:46.566311: step 2750, loss 0.536439.
Test: 2018-07-31T11:22:47.034952: step 2750, loss 0.548438.
Train: 2018-07-31T11:22:47.206781: step 2751, loss 0.527376.
Train: 2018-07-31T11:22:47.378622: step 2752, loss 0.491105.
Train: 2018-07-31T11:22:47.534835: step 2753, loss 0.563637.
Train: 2018-07-31T11:22:47.706639: step 2754, loss 0.609101.
Train: 2018-07-31T11:22:47.878474: step 2755, loss 0.581831.
Train: 2018-07-31T11:22:48.050339: step 2756, loss 0.490965.
Train: 2018-07-31T11:22:48.206523: step 2757, loss 0.500021.
Train: 2018-07-31T11:22:48.393993: step 2758, loss 0.627385.
Train: 2018-07-31T11:22:48.550217: step 2759, loss 0.509058.
Train: 2018-07-31T11:22:48.706406: step 2760, loss 0.49082.
Test: 2018-07-31T11:22:49.190698: step 2760, loss 0.548418.
Train: 2018-07-31T11:22:49.362501: step 2761, loss 0.645781.
Train: 2018-07-31T11:22:49.518745: step 2762, loss 0.627512.
Train: 2018-07-31T11:22:49.690580: step 2763, loss 0.60007.
Train: 2018-07-31T11:22:49.846794: step 2764, loss 0.509146.
Train: 2018-07-31T11:22:50.003007: step 2765, loss 0.500139.
Train: 2018-07-31T11:22:50.174811: step 2766, loss 0.554534.
Train: 2018-07-31T11:22:50.331024: step 2767, loss 0.518298.
Train: 2018-07-31T11:22:50.502860: step 2768, loss 0.536411.
Train: 2018-07-31T11:22:50.659074: step 2769, loss 0.590786.
Train: 2018-07-31T11:22:50.830907: step 2770, loss 0.617952.
Test: 2018-07-31T11:22:51.299578: step 2770, loss 0.548417.
Train: 2018-07-31T11:22:51.471412: step 2771, loss 0.572614.
Train: 2018-07-31T11:22:51.643248: step 2772, loss 0.509365.
Train: 2018-07-31T11:22:51.799431: step 2773, loss 0.617664.
Train: 2018-07-31T11:22:51.971297: step 2774, loss 0.599523.
Train: 2018-07-31T11:22:52.143131: step 2775, loss 0.572455.
Train: 2018-07-31T11:22:52.314965: step 2776, loss 0.473897.
Train: 2018-07-31T11:22:52.471150: step 2777, loss 0.67078.
Train: 2018-07-31T11:22:52.643014: step 2778, loss 0.590155.
Train: 2018-07-31T11:22:52.814848: step 2779, loss 0.527824.
Train: 2018-07-31T11:22:52.971032: step 2780, loss 0.607663.
Test: 2018-07-31T11:22:53.439701: step 2780, loss 0.548529.
Train: 2018-07-31T11:22:53.611536: step 2781, loss 0.536822.
Train: 2018-07-31T11:22:53.783356: step 2782, loss 0.536875.
Train: 2018-07-31T11:22:53.939554: step 2783, loss 0.56329.
Train: 2018-07-31T11:22:54.111420: step 2784, loss 0.572053.
Train: 2018-07-31T11:22:54.283251: step 2785, loss 0.589538.
Train: 2018-07-31T11:22:54.439477: step 2786, loss 0.606934.
Train: 2018-07-31T11:22:54.611297: step 2787, loss 0.537116.
Train: 2018-07-31T11:22:54.767510: step 2788, loss 0.571921.
Train: 2018-07-31T11:22:54.939320: step 2789, loss 0.545891.
Train: 2018-07-31T11:22:55.111156: step 2790, loss 0.537267.
Test: 2018-07-31T11:22:55.579829: step 2790, loss 0.548746.
Train: 2018-07-31T11:22:55.751630: step 2791, loss 0.571858.
Train: 2018-07-31T11:22:55.907843: step 2792, loss 0.528694.
Train: 2018-07-31T11:22:56.079679: step 2793, loss 0.606341.
Train: 2018-07-31T11:22:56.235922: step 2794, loss 0.589053.
Train: 2018-07-31T11:22:56.407727: step 2795, loss 0.580406.
Train: 2018-07-31T11:22:56.579561: step 2796, loss 0.631868.
Train: 2018-07-31T11:22:56.751396: step 2797, loss 0.580317.
Train: 2018-07-31T11:22:56.923231: step 2798, loss 0.588793.
Train: 2018-07-31T11:22:57.079478: step 2799, loss 0.563212.
Train: 2018-07-31T11:22:57.251309: step 2800, loss 0.52932.
Test: 2018-07-31T11:22:57.719950: step 2800, loss 0.549057.
Train: 2018-07-31T11:22:58.454147: step 2801, loss 0.597054.
Train: 2018-07-31T11:22:58.610366: step 2802, loss 0.546354.
Train: 2018-07-31T11:22:58.782171: step 2803, loss 0.529542.
Train: 2018-07-31T11:22:58.954039: step 2804, loss 0.52957.
Train: 2018-07-31T11:22:59.110219: step 2805, loss 0.596906.
Train: 2018-07-31T11:22:59.282054: step 2806, loss 0.437035.
Train: 2018-07-31T11:22:59.453922: step 2807, loss 0.571658.
Train: 2018-07-31T11:22:59.625723: step 2808, loss 0.563218.
Train: 2018-07-31T11:22:59.797557: step 2809, loss 0.554746.
Train: 2018-07-31T11:22:59.953805: step 2810, loss 0.597127.
Test: 2018-07-31T11:23:00.422411: step 2810, loss 0.548983.
Train: 2018-07-31T11:23:00.594277: step 2811, loss 0.461325.
Train: 2018-07-31T11:23:00.766112: step 2812, loss 0.631321.
Train: 2018-07-31T11:23:00.937916: step 2813, loss 0.60583.
Train: 2018-07-31T11:23:01.094154: step 2814, loss 0.614378.
Train: 2018-07-31T11:23:01.265994: step 2815, loss 0.605818.
Train: 2018-07-31T11:23:01.422178: step 2816, loss 0.597244.
Train: 2018-07-31T11:23:01.594012: step 2817, loss 0.588683.
Train: 2018-07-31T11:23:01.765877: step 2818, loss 0.571674.
Train: 2018-07-31T11:23:01.937682: step 2819, loss 0.563202.
Train: 2018-07-31T11:23:02.109541: step 2820, loss 0.639199.
Test: 2018-07-31T11:23:02.578157: step 2820, loss 0.549123.
Train: 2018-07-31T11:23:02.750022: step 2821, loss 0.571634.
Train: 2018-07-31T11:23:02.921857: step 2822, loss 0.580013.
Train: 2018-07-31T11:23:03.093661: step 2823, loss 0.579978.
Train: 2018-07-31T11:23:03.249905: step 2824, loss 0.488183.
Train: 2018-07-31T11:23:03.421740: step 2825, loss 0.588269.
Train: 2018-07-31T11:23:03.593574: step 2826, loss 0.604895.
Train: 2018-07-31T11:23:03.765379: step 2827, loss 0.530045.
Train: 2018-07-31T11:23:03.921593: step 2828, loss 0.530076.
Train: 2018-07-31T11:23:04.093457: step 2829, loss 0.596502.
Train: 2018-07-31T11:23:04.265263: step 2830, loss 0.579889.
Test: 2018-07-31T11:23:04.733932: step 2830, loss 0.549396.
Train: 2018-07-31T11:23:04.905761: step 2831, loss 0.571586.
Train: 2018-07-31T11:23:05.077605: step 2832, loss 0.513544.
Train: 2018-07-31T11:23:05.249436: step 2833, loss 0.596477.
Train: 2018-07-31T11:23:05.405650: step 2834, loss 0.554986.
Train: 2018-07-31T11:23:05.577455: step 2835, loss 0.513466.
Train: 2018-07-31T11:23:05.749321: step 2836, loss 0.513372.
Train: 2018-07-31T11:23:05.921123: step 2837, loss 0.638295.
Train: 2018-07-31T11:23:06.077337: step 2838, loss 0.521512.
Train: 2018-07-31T11:23:06.249173: step 2839, loss 0.571592.
Train: 2018-07-31T11:23:06.421037: step 2840, loss 0.54647.
Test: 2018-07-31T11:23:06.889647: step 2840, loss 0.549156.
Train: 2018-07-31T11:23:07.061481: step 2841, loss 0.521253.
Train: 2018-07-31T11:23:07.233348: step 2842, loss 0.563196.
Train: 2018-07-31T11:23:07.389531: step 2843, loss 0.580057.
Train: 2018-07-31T11:23:07.561395: step 2844, loss 0.57163.
Train: 2018-07-31T11:23:07.733200: step 2845, loss 0.486969.
Train: 2018-07-31T11:23:07.889414: step 2846, loss 0.529187.
Train: 2018-07-31T11:23:08.061278: step 2847, loss 0.546105.
Train: 2018-07-31T11:23:08.233082: step 2848, loss 0.588815.
Train: 2018-07-31T11:23:08.389297: step 2849, loss 0.580305.
Train: 2018-07-31T11:23:08.561163: step 2850, loss 0.571744.
Test: 2018-07-31T11:23:09.045422: step 2850, loss 0.548726.
Train: 2018-07-31T11:23:09.201640: step 2851, loss 0.537314.
Train: 2018-07-31T11:23:09.373471: step 2852, loss 0.563148.
Train: 2018-07-31T11:23:09.545301: step 2853, loss 0.589083.
Train: 2018-07-31T11:23:09.717110: step 2854, loss 0.580458.
Train: 2018-07-31T11:23:09.873335: step 2855, loss 0.632417.
Train: 2018-07-31T11:23:10.045188: step 2856, loss 0.519897.
Train: 2018-07-31T11:23:10.216992: step 2857, loss 0.545847.
Train: 2018-07-31T11:23:10.373231: step 2858, loss 0.5718.
Train: 2018-07-31T11:23:10.545066: step 2859, loss 0.632374.
Train: 2018-07-31T11:23:10.701255: step 2860, loss 0.571784.
Test: 2018-07-31T11:23:11.185546: step 2860, loss 0.548687.
Train: 2018-07-31T11:23:11.341764: step 2861, loss 0.571768.
Train: 2018-07-31T11:23:11.513594: step 2862, loss 0.537288.
Train: 2018-07-31T11:23:11.685429: step 2863, loss 0.588961.
Train: 2018-07-31T11:23:11.841643: step 2864, loss 0.554535.
Train: 2018-07-31T11:23:12.013478: step 2865, loss 0.528776.
Train: 2018-07-31T11:23:12.185282: step 2866, loss 0.631825.
Train: 2018-07-31T11:23:12.372762: step 2867, loss 0.614567.
Train: 2018-07-31T11:23:12.528982: step 2868, loss 0.657197.
Train: 2018-07-31T11:23:12.685166: step 2869, loss 0.544966.
Train: 2018-07-31T11:23:12.857031: step 2870, loss 0.571626.
Test: 2018-07-31T11:23:13.341260: step 2870, loss 0.548977.
Train: 2018-07-31T11:23:13.513097: step 2871, loss 0.580067.
Train: 2018-07-31T11:23:13.669334: step 2872, loss 0.622192.
Train: 2018-07-31T11:23:13.841145: step 2873, loss 0.504373.
Train: 2018-07-31T11:23:13.997359: step 2874, loss 0.563182.
Train: 2018-07-31T11:23:14.169193: step 2875, loss 0.546462.
Train: 2018-07-31T11:23:14.341053: step 2876, loss 0.613321.
Train: 2018-07-31T11:23:14.497265: step 2877, loss 0.571546.
Train: 2018-07-31T11:23:14.669076: step 2878, loss 0.54658.
Train: 2018-07-31T11:23:14.825289: step 2879, loss 0.546609.
Train: 2018-07-31T11:23:14.997148: step 2880, loss 0.563232.
Test: 2018-07-31T11:23:15.465763: step 2880, loss 0.54933.
Train: 2018-07-31T11:23:15.637598: step 2881, loss 0.496821.
Train: 2018-07-31T11:23:15.809433: step 2882, loss 0.563223.
Train: 2018-07-31T11:23:15.965671: step 2883, loss 0.554891.
Train: 2018-07-31T11:23:16.137515: step 2884, loss 0.554869.
Train: 2018-07-31T11:23:16.309318: step 2885, loss 0.529798.
Train: 2018-07-31T11:23:16.496802: step 2886, loss 0.496234.
Train: 2018-07-31T11:23:16.653016: step 2887, loss 0.537963.
Train: 2018-07-31T11:23:16.824821: step 2888, loss 0.61373.
Train: 2018-07-31T11:23:16.996686: step 2889, loss 0.571584.
Train: 2018-07-31T11:23:17.152899: step 2890, loss 0.503828.
Test: 2018-07-31T11:23:17.637131: step 2890, loss 0.548882.
Train: 2018-07-31T11:23:17.808995: step 2891, loss 0.580111.
Train: 2018-07-31T11:23:17.980826: step 2892, loss 0.537548.
Train: 2018-07-31T11:23:18.152659: step 2893, loss 0.528925.
Train: 2018-07-31T11:23:18.324470: step 2894, loss 0.588822.
Train: 2018-07-31T11:23:18.480684: step 2895, loss 0.545913.
Train: 2018-07-31T11:23:18.652548: step 2896, loss 0.537254.
Train: 2018-07-31T11:23:18.824352: step 2897, loss 0.571745.
Train: 2018-07-31T11:23:18.980590: step 2898, loss 0.519808.
Train: 2018-07-31T11:23:19.152431: step 2899, loss 0.493633.
Train: 2018-07-31T11:23:19.324269: step 2900, loss 0.598004.
Test: 2018-07-31T11:23:19.792876: step 2900, loss 0.548494.
Train: 2018-07-31T11:23:20.511458: step 2901, loss 0.51942.
Train: 2018-07-31T11:23:20.683322: step 2902, loss 0.536836.
Train: 2018-07-31T11:23:20.839536: step 2903, loss 0.563169.
Train: 2018-07-31T11:23:21.011371: step 2904, loss 0.536707.
Train: 2018-07-31T11:23:21.183205: step 2905, loss 0.607469.
Train: 2018-07-31T11:23:21.339418: step 2906, loss 0.492279.
Train: 2018-07-31T11:23:21.511248: step 2907, loss 0.57213.
Train: 2018-07-31T11:23:21.683057: step 2908, loss 0.527616.
Train: 2018-07-31T11:23:21.839271: step 2909, loss 0.58114.
Train: 2018-07-31T11:23:22.011137: step 2910, loss 0.527509.
Test: 2018-07-31T11:23:22.479777: step 2910, loss 0.5483.
Train: 2018-07-31T11:23:22.651607: step 2911, loss 0.473677.
Train: 2018-07-31T11:23:22.823449: step 2912, loss 0.527376.
Train: 2018-07-31T11:23:22.995275: step 2913, loss 0.599476.
Train: 2018-07-31T11:23:23.167087: step 2914, loss 0.572452.
Train: 2018-07-31T11:23:23.338920: step 2915, loss 0.608707.
Train: 2018-07-31T11:23:23.510756: step 2916, loss 0.608722.
Train: 2018-07-31T11:23:23.666968: step 2917, loss 0.581521.
Train: 2018-07-31T11:23:23.838803: step 2918, loss 0.608595.
Train: 2018-07-31T11:23:23.995017: step 2919, loss 0.572391.
Train: 2018-07-31T11:23:24.166851: step 2920, loss 0.500369.
Test: 2018-07-31T11:23:24.651113: step 2920, loss 0.54828.
Train: 2018-07-31T11:23:24.822948: step 2921, loss 0.590284.
Train: 2018-07-31T11:23:24.994813: step 2922, loss 0.617121.
Train: 2018-07-31T11:23:25.150996: step 2923, loss 0.599051.
Train: 2018-07-31T11:23:25.322830: step 2924, loss 0.509768.
Train: 2018-07-31T11:23:25.494690: step 2925, loss 0.500976.
Train: 2018-07-31T11:23:25.650879: step 2926, loss 0.536563.
Train: 2018-07-31T11:23:25.822747: step 2927, loss 0.589843.
Train: 2018-07-31T11:23:25.994579: step 2928, loss 0.55433.
Train: 2018-07-31T11:23:26.150792: step 2929, loss 0.545471.
Train: 2018-07-31T11:23:26.322630: step 2930, loss 0.536625.
Test: 2018-07-31T11:23:26.806858: step 2930, loss 0.548358.
Train: 2018-07-31T11:23:26.963102: step 2931, loss 0.563179.
Train: 2018-07-31T11:23:27.134931: step 2932, loss 0.554329.
Train: 2018-07-31T11:23:27.291151: step 2933, loss 0.572015.
Train: 2018-07-31T11:23:27.462985: step 2934, loss 0.598518.
Train: 2018-07-31T11:23:27.634819: step 2935, loss 0.563156.
Train: 2018-07-31T11:23:27.806625: step 2936, loss 0.642468.
Train: 2018-07-31T11:23:27.962838: step 2937, loss 0.642219.
Train: 2018-07-31T11:23:28.134697: step 2938, loss 0.536855.
Train: 2018-07-31T11:23:28.306509: step 2939, loss 0.59797.
Train: 2018-07-31T11:23:28.462722: step 2940, loss 0.563076.
Test: 2018-07-31T11:23:28.947015: step 2940, loss 0.548571.
Train: 2018-07-31T11:23:29.118848: step 2941, loss 0.589032.
Train: 2018-07-31T11:23:29.290681: step 2942, loss 0.571682.
Train: 2018-07-31T11:23:29.462511: step 2943, loss 0.563056.
Train: 2018-07-31T11:23:29.618724: step 2944, loss 0.614432.
Train: 2018-07-31T11:23:29.790564: step 2945, loss 0.597171.
Train: 2018-07-31T11:23:29.946748: step 2946, loss 0.53759.
Train: 2018-07-31T11:23:30.118613: step 2947, loss 0.596926.
Train: 2018-07-31T11:23:30.274796: step 2948, loss 0.579949.
Train: 2018-07-31T11:23:30.446632: step 2949, loss 0.546294.
Train: 2018-07-31T11:23:30.618495: step 2950, loss 0.563111.
Test: 2018-07-31T11:23:31.087136: step 2950, loss 0.549124.
Train: 2018-07-31T11:23:31.243346: step 2951, loss 0.51297.
Train: 2018-07-31T11:23:31.415185: step 2952, loss 0.554776.
Train: 2018-07-31T11:23:31.587019: step 2953, loss 0.529744.
Train: 2018-07-31T11:23:31.758848: step 2954, loss 0.513029.
Train: 2018-07-31T11:23:31.915062: step 2955, loss 0.512941.
Train: 2018-07-31T11:23:32.086897: step 2956, loss 0.579871.
Train: 2018-07-31T11:23:32.258707: step 2957, loss 0.571493.
Train: 2018-07-31T11:23:32.414920: step 2958, loss 0.588334.
Train: 2018-07-31T11:23:32.586755: step 2959, loss 0.630492.
Train: 2018-07-31T11:23:32.758589: step 2960, loss 0.579926.
Test: 2018-07-31T11:23:33.227260: step 2960, loss 0.548977.
Train: 2018-07-31T11:23:33.399098: step 2961, loss 0.5294.
Train: 2018-07-31T11:23:33.570933: step 2962, loss 0.588343.
Train: 2018-07-31T11:23:33.742735: step 2963, loss 0.537814.
Train: 2018-07-31T11:23:33.898948: step 2964, loss 0.546224.
Train: 2018-07-31T11:23:34.070816: step 2965, loss 0.605229.
Train: 2018-07-31T11:23:34.242618: step 2966, loss 0.563068.
Train: 2018-07-31T11:23:34.414488: step 2967, loss 0.504042.
Train: 2018-07-31T11:23:34.570699: step 2968, loss 0.60528.
Train: 2018-07-31T11:23:34.742524: step 2969, loss 0.563059.
Train: 2018-07-31T11:23:34.898713: step 2970, loss 0.5208.
Test: 2018-07-31T11:23:35.382974: step 2970, loss 0.548878.
Train: 2018-07-31T11:23:35.554810: step 2971, loss 0.495348.
Train: 2018-07-31T11:23:35.726645: step 2972, loss 0.571529.
Train: 2018-07-31T11:23:35.898509: step 2973, loss 0.605567.
Train: 2018-07-31T11:23:36.054723: step 2974, loss 0.537484.
Train: 2018-07-31T11:23:36.226558: step 2975, loss 0.597154.
Train: 2018-07-31T11:23:36.382767: step 2976, loss 0.580105.
Train: 2018-07-31T11:23:36.554610: step 2977, loss 0.58011.
Train: 2018-07-31T11:23:36.726442: step 2978, loss 0.520318.
Train: 2018-07-31T11:23:36.898276: step 2979, loss 0.528825.
Train: 2018-07-31T11:23:37.054491: step 2980, loss 0.443136.
Test: 2018-07-31T11:23:37.538749: step 2980, loss 0.548625.
Train: 2018-07-31T11:23:37.710580: step 2981, loss 0.606007.
Train: 2018-07-31T11:23:37.866798: step 2982, loss 0.571643.
Train: 2018-07-31T11:23:38.038603: step 2983, loss 0.459371.
Train: 2018-07-31T11:23:38.194817: step 2984, loss 0.55436.
Train: 2018-07-31T11:23:38.366650: step 2985, loss 0.597862.
Train: 2018-07-31T11:23:38.538487: step 2986, loss 0.580505.
Train: 2018-07-31T11:23:38.694700: step 2987, loss 0.641749.
Train: 2018-07-31T11:23:38.866534: step 2988, loss 0.61552.
Train: 2018-07-31T11:23:39.038369: step 2989, loss 0.545584.
Train: 2018-07-31T11:23:39.194608: step 2990, loss 0.589231.
Test: 2018-07-31T11:23:39.678874: step 2990, loss 0.548444.
Train: 2018-07-31T11:23:39.850709: step 2991, loss 0.502026.
Train: 2018-07-31T11:23:40.022544: step 2992, loss 0.528175.
Train: 2018-07-31T11:23:40.178759: step 2993, loss 0.615377.
Train: 2018-07-31T11:23:40.350562: step 2994, loss 0.606628.
Train: 2018-07-31T11:23:40.522427: step 2995, loss 0.53692.
Train: 2018-07-31T11:23:40.694231: step 2996, loss 0.606518.
Train: 2018-07-31T11:23:40.850445: step 2997, loss 0.510929.
Train: 2018-07-31T11:23:41.022280: step 2998, loss 0.606413.
Train: 2018-07-31T11:23:41.178493: step 2999, loss 0.528355.
Train: 2018-07-31T11:23:41.350328: step 3000, loss 0.545695.
Test: 2018-07-31T11:23:41.819001: step 3000, loss 0.548514.
Train: 2018-07-31T11:23:42.537549: step 3001, loss 0.528382.
Train: 2018-07-31T11:23:42.693764: step 3002, loss 0.571677.
Train: 2018-07-31T11:23:42.865630: step 3003, loss 0.606332.
Train: 2018-07-31T11:23:43.021835: step 3004, loss 0.580325.
Train: 2018-07-31T11:23:43.193679: step 3005, loss 0.571656.
Train: 2018-07-31T11:23:43.365511: step 3006, loss 0.545732.
Train: 2018-07-31T11:23:43.521694: step 3007, loss 0.528482.
Train: 2018-07-31T11:23:43.693529: step 3008, loss 0.623416.
Train: 2018-07-31T11:23:43.865363: step 3009, loss 0.554381.
Train: 2018-07-31T11:23:44.021576: step 3010, loss 0.631887.
Test: 2018-07-31T11:23:44.490217: step 3010, loss 0.548609.
Train: 2018-07-31T11:23:44.662052: step 3011, loss 0.554406.
Train: 2018-07-31T11:23:44.833886: step 3012, loss 0.64874.
Train: 2018-07-31T11:23:45.005752: step 3013, loss 0.580085.
Train: 2018-07-31T11:23:45.177589: step 3014, loss 0.648143.
Train: 2018-07-31T11:23:45.333770: step 3015, loss 0.571483.
Train: 2018-07-31T11:23:45.505629: step 3016, loss 0.546157.
Train: 2018-07-31T11:23:45.661850: step 3017, loss 0.529429.
Train: 2018-07-31T11:23:45.833678: step 3018, loss 0.571432.
Train: 2018-07-31T11:23:46.005517: step 3019, loss 0.554701.
Train: 2018-07-31T11:23:46.161731: step 3020, loss 0.527459.
Test: 2018-07-31T11:23:46.630371: step 3020, loss 0.549107.
Train: 2018-07-31T11:23:46.802205: step 3021, loss 0.513031.
Train: 2018-07-31T11:23:46.974041: step 3022, loss 0.571417.
Train: 2018-07-31T11:23:47.145875: step 3023, loss 0.579767.
Train: 2018-07-31T11:23:47.317680: step 3024, loss 0.512958.
Train: 2018-07-31T11:23:47.473924: step 3025, loss 0.546331.
Train: 2018-07-31T11:23:47.645753: step 3026, loss 0.604934.
Train: 2018-07-31T11:23:47.817597: step 3027, loss 0.521123.
Train: 2018-07-31T11:23:47.989428: step 3028, loss 0.554635.
Train: 2018-07-31T11:23:48.161262: step 3029, loss 0.621918.
Train: 2018-07-31T11:23:48.317479: step 3030, loss 0.554604.
Test: 2018-07-31T11:23:48.801715: step 3030, loss 0.548911.
Train: 2018-07-31T11:23:48.957921: step 3031, loss 0.655672.
Train: 2018-07-31T11:23:49.129786: step 3032, loss 0.529373.
Train: 2018-07-31T11:23:49.301623: step 3033, loss 0.647116.
Train: 2018-07-31T11:23:49.457834: step 3034, loss 0.462324.
Train: 2018-07-31T11:23:49.629670: step 3035, loss 0.596613.
Train: 2018-07-31T11:23:49.785882: step 3036, loss 0.596606.
Train: 2018-07-31T11:23:49.957717: step 3037, loss 0.529482.
Train: 2018-07-31T11:23:50.129552: step 3038, loss 0.512701.
Train: 2018-07-31T11:23:50.285736: step 3039, loss 0.529427.
Train: 2018-07-31T11:23:50.457569: step 3040, loss 0.588261.
Test: 2018-07-31T11:23:50.941831: step 3040, loss 0.548895.
Train: 2018-07-31T11:23:51.113696: step 3041, loss 0.571434.
Train: 2018-07-31T11:23:51.269904: step 3042, loss 0.537695.
Train: 2018-07-31T11:23:51.441714: step 3043, loss 0.562996.
Train: 2018-07-31T11:23:51.613580: step 3044, loss 0.613765.
Train: 2018-07-31T11:23:51.785385: step 3045, loss 0.59685.
Train: 2018-07-31T11:23:51.941597: step 3046, loss 0.554527.
Train: 2018-07-31T11:23:52.113432: step 3047, loss 0.588374.
Train: 2018-07-31T11:23:52.269676: step 3048, loss 0.554532.
Train: 2018-07-31T11:23:52.425889: step 3049, loss 0.537623.
Train: 2018-07-31T11:23:52.597724: step 3050, loss 0.554529.
Test: 2018-07-31T11:23:53.081985: step 3050, loss 0.54881.
Train: 2018-07-31T11:23:53.238193: step 3051, loss 0.656087.
Train: 2018-07-31T11:23:53.410004: step 3052, loss 0.588345.
Train: 2018-07-31T11:23:53.581838: step 3053, loss 0.605182.
Train: 2018-07-31T11:23:53.753673: step 3054, loss 0.621927.
Train: 2018-07-31T11:23:53.925540: step 3055, loss 0.504287.
Train: 2018-07-31T11:23:54.081752: step 3056, loss 0.596529.
Train: 2018-07-31T11:23:54.253555: step 3057, loss 0.537957.
Train: 2018-07-31T11:23:54.425392: step 3058, loss 0.529644.
Train: 2018-07-31T11:23:54.581635: step 3059, loss 0.512957.
Train: 2018-07-31T11:23:54.753470: step 3060, loss 0.563034.
Test: 2018-07-31T11:23:55.237731: step 3060, loss 0.549018.
Train: 2018-07-31T11:23:55.409560: step 3061, loss 0.537933.
Train: 2018-07-31T11:23:55.565749: step 3062, loss 0.579774.
Train: 2018-07-31T11:23:55.737613: step 3063, loss 0.512681.
Train: 2018-07-31T11:23:55.893796: step 3064, loss 0.588222.
Train: 2018-07-31T11:23:56.065665: step 3065, loss 0.588254.
Train: 2018-07-31T11:23:56.237497: step 3066, loss 0.546129.
Train: 2018-07-31T11:23:56.393704: step 3067, loss 0.52078.
Train: 2018-07-31T11:23:56.565515: step 3068, loss 0.596808.
Train: 2018-07-31T11:23:56.721758: step 3069, loss 0.53756.
Train: 2018-07-31T11:23:56.893596: step 3070, loss 0.630839.
Test: 2018-07-31T11:23:57.362232: step 3070, loss 0.54875.
Train: 2018-07-31T11:23:57.534069: step 3071, loss 0.554476.
Train: 2018-07-31T11:23:57.705872: step 3072, loss 0.545981.
Train: 2018-07-31T11:23:57.862118: step 3073, loss 0.571454.
Train: 2018-07-31T11:23:58.033951: step 3074, loss 0.579958.
Train: 2018-07-31T11:23:58.205785: step 3075, loss 0.554452.
Train: 2018-07-31T11:23:58.377591: step 3076, loss 0.511917.
Train: 2018-07-31T11:23:58.549459: step 3077, loss 0.511838.
Train: 2018-07-31T11:23:58.721290: step 3078, loss 0.554407.
Train: 2018-07-31T11:23:58.877472: step 3079, loss 0.588623.
Train: 2018-07-31T11:23:59.049307: step 3080, loss 0.58009.
Test: 2018-07-31T11:23:59.517948: step 3080, loss 0.548567.
Train: 2018-07-31T11:23:59.674191: step 3081, loss 0.520026.
Train: 2018-07-31T11:23:59.846026: step 3082, loss 0.537144.
Train: 2018-07-31T11:24:00.017865: step 3083, loss 0.519852.
Train: 2018-07-31T11:24:00.174044: step 3084, loss 0.502446.
Train: 2018-07-31T11:24:00.345879: step 3085, loss 0.597654.
Train: 2018-07-31T11:24:00.502092: step 3086, loss 0.615148.
Train: 2018-07-31T11:24:00.673957: step 3087, loss 0.51942.
Train: 2018-07-31T11:24:00.845762: step 3088, loss 0.493167.
Train: 2018-07-31T11:24:01.017596: step 3089, loss 0.55423.
Train: 2018-07-31T11:24:01.173835: step 3090, loss 0.448901.
Test: 2018-07-31T11:24:01.658074: step 3090, loss 0.548254.
Train: 2018-07-31T11:24:01.814319: step 3091, loss 0.545387.
Train: 2018-07-31T11:24:01.986121: step 3092, loss 0.571908.
Train: 2018-07-31T11:24:02.142365: step 3093, loss 0.651965.
Train: 2018-07-31T11:24:02.329793: step 3094, loss 0.536388.
Train: 2018-07-31T11:24:02.486033: step 3095, loss 0.51853.
Train: 2018-07-31T11:24:02.657868: step 3096, loss 0.563123.
Train: 2018-07-31T11:24:02.829673: step 3097, loss 0.55419.
Train: 2018-07-31T11:24:03.001539: step 3098, loss 0.563154.
Train: 2018-07-31T11:24:03.173374: step 3099, loss 0.527275.
Train: 2018-07-31T11:24:03.329555: step 3100, loss 0.527236.
Test: 2018-07-31T11:24:03.813850: step 3100, loss 0.548124.
Train: 2018-07-31T11:24:04.579262: step 3101, loss 0.545196.
Train: 2018-07-31T11:24:04.751128: step 3102, loss 0.518128.
Train: 2018-07-31T11:24:04.922965: step 3103, loss 0.527093.
Train: 2018-07-31T11:24:05.079147: step 3104, loss 0.536096.
Train: 2018-07-31T11:24:05.251011: step 3105, loss 0.563311.
Train: 2018-07-31T11:24:05.422815: step 3106, loss 0.554237.
Train: 2018-07-31T11:24:05.594650: step 3107, loss 0.517784.
Train: 2018-07-31T11:24:05.829001: step 3108, loss 0.581659.
Train: 2018-07-31T11:24:06.078942: step 3109, loss 0.581699.
Train: 2018-07-31T11:24:06.281989: step 3110, loss 0.590864.
Test: 2018-07-31T11:24:06.766251: step 3110, loss 0.548091.
Train: 2018-07-31T11:24:06.978114: step 3111, loss 0.590848.
Train: 2018-07-31T11:24:07.201731: step 3112, loss 0.627343.
Train: 2018-07-31T11:24:07.372467: step 3113, loss 0.526899.
Train: 2018-07-31T11:24:07.541542: step 3114, loss 0.536036.
Train: 2018-07-31T11:24:07.711714: step 3115, loss 0.508801.
Train: 2018-07-31T11:24:07.881994: step 3116, loss 0.554218.
Train: 2018-07-31T11:24:08.051748: step 3117, loss 0.508843.
Train: 2018-07-31T11:24:08.218048: step 3118, loss 0.490678.
Train: 2018-07-31T11:24:08.387840: step 3119, loss 0.599666.
Train: 2018-07-31T11:24:08.561539: step 3120, loss 0.654228.
Test: 2018-07-31T11:24:09.042110: step 3120, loss 0.548089.
Train: 2018-07-31T11:24:09.207959: step 3121, loss 0.536061.
Train: 2018-07-31T11:24:09.372232: step 3122, loss 0.563271.
Train: 2018-07-31T11:24:09.543537: step 3123, loss 0.527046.
Train: 2018-07-31T11:24:09.699751: step 3124, loss 0.56324.
Train: 2018-07-31T11:24:09.871585: step 3125, loss 0.626472.
Train: 2018-07-31T11:24:10.027799: step 3126, loss 0.626296.
Train: 2018-07-31T11:24:10.199635: step 3127, loss 0.563157.
Train: 2018-07-31T11:24:10.371468: step 3128, loss 0.589977.
Train: 2018-07-31T11:24:10.543304: step 3129, loss 0.589839.
Train: 2018-07-31T11:24:10.715168: step 3130, loss 0.500872.
Test: 2018-07-31T11:24:11.183809: step 3130, loss 0.548189.
Train: 2018-07-31T11:24:11.355644: step 3131, loss 0.625036.
Train: 2018-07-31T11:24:11.543100: step 3132, loss 0.571824.
Train: 2018-07-31T11:24:11.714904: step 3133, loss 0.615732.
Train: 2018-07-31T11:24:11.871117: step 3134, loss 0.53669.
Train: 2018-07-31T11:24:12.042952: step 3135, loss 0.554212.
Train: 2018-07-31T11:24:12.214789: step 3136, loss 0.589004.
Train: 2018-07-31T11:24:12.386622: step 3137, loss 0.510929.
Train: 2018-07-31T11:24:12.542871: step 3138, loss 0.545616.
Train: 2018-07-31T11:24:12.714700: step 3139, loss 0.606063.
Train: 2018-07-31T11:24:12.886539: step 3140, loss 0.562901.
Test: 2018-07-31T11:24:13.355175: step 3140, loss 0.548507.
Train: 2018-07-31T11:24:13.526979: step 3141, loss 0.571492.
Train: 2018-07-31T11:24:13.698845: step 3142, loss 0.605772.
Train: 2018-07-31T11:24:13.870679: step 3143, loss 0.562898.
Train: 2018-07-31T11:24:14.026863: step 3144, loss 0.579959.
Train: 2018-07-31T11:24:14.198729: step 3145, loss 0.579918.
Train: 2018-07-31T11:24:14.370533: step 3146, loss 0.605331.
Train: 2018-07-31T11:24:14.526750: step 3147, loss 0.47835.
Train: 2018-07-31T11:24:14.698580: step 3148, loss 0.520681.
Train: 2018-07-31T11:24:14.870448: step 3149, loss 0.537575.
Train: 2018-07-31T11:24:15.042250: step 3150, loss 0.554463.
Test: 2018-07-31T11:24:15.510890: step 3150, loss 0.548744.
Train: 2018-07-31T11:24:15.729620: step 3151, loss 0.554453.
Train: 2018-07-31T11:24:15.901454: step 3152, loss 0.562909.
Train: 2018-07-31T11:24:16.073261: step 3153, loss 0.554429.
Train: 2018-07-31T11:24:16.245093: step 3154, loss 0.511985.
Train: 2018-07-31T11:24:16.401338: step 3155, loss 0.681958.
Train: 2018-07-31T11:24:16.573173: step 3156, loss 0.588396.
Train: 2018-07-31T11:24:16.744977: step 3157, loss 0.630825.
Train: 2018-07-31T11:24:16.916810: step 3158, loss 0.579847.
Train: 2018-07-31T11:24:17.073024: step 3159, loss 0.571362.
Train: 2018-07-31T11:24:17.244889: step 3160, loss 0.62194.
Test: 2018-07-31T11:24:17.713530: step 3160, loss 0.548856.
Train: 2018-07-31T11:24:17.885334: step 3161, loss 0.504101.
Train: 2018-07-31T11:24:18.057169: step 3162, loss 0.579721.
Train: 2018-07-31T11:24:18.229034: step 3163, loss 0.579701.
Train: 2018-07-31T11:24:18.385217: step 3164, loss 0.529503.
Train: 2018-07-31T11:24:18.557051: step 3165, loss 0.537887.
Train: 2018-07-31T11:24:18.713265: step 3166, loss 0.604743.
Train: 2018-07-31T11:24:18.885101: step 3167, loss 0.579662.
Train: 2018-07-31T11:24:19.056966: step 3168, loss 0.587995.
Train: 2018-07-31T11:24:19.228800: step 3169, loss 0.537975.
Train: 2018-07-31T11:24:19.385016: step 3170, loss 0.571304.
Test: 2018-07-31T11:24:19.869245: step 3170, loss 0.549034.
Train: 2018-07-31T11:24:20.009837: step 3171, loss 0.616259.
Train: 2018-07-31T11:24:20.181671: step 3172, loss 0.59624.
Train: 2018-07-31T11:24:20.353539: step 3173, loss 0.521509.
Train: 2018-07-31T11:24:20.525372: step 3174, loss 0.621052.
Train: 2018-07-31T11:24:20.681579: step 3175, loss 0.496783.
Train: 2018-07-31T11:24:20.853391: step 3176, loss 0.554732.
Train: 2018-07-31T11:24:21.025225: step 3177, loss 0.604429.
Train: 2018-07-31T11:24:21.181437: step 3178, loss 0.587855.
Train: 2018-07-31T11:24:21.353302: step 3179, loss 0.48853.
Train: 2018-07-31T11:24:21.525137: step 3180, loss 0.546432.
Test: 2018-07-31T11:24:21.993777: step 3180, loss 0.549091.
Train: 2018-07-31T11:24:22.181229: step 3181, loss 0.513192.
Train: 2018-07-31T11:24:22.337447: step 3182, loss 0.58794.
Train: 2018-07-31T11:24:22.509277: step 3183, loss 0.529598.
Train: 2018-07-31T11:24:22.665464: step 3184, loss 0.58803.
Train: 2018-07-31T11:24:22.837325: step 3185, loss 0.546171.
Train: 2018-07-31T11:24:23.009135: step 3186, loss 0.520924.
Train: 2018-07-31T11:24:23.180999: step 3187, loss 0.529205.
Train: 2018-07-31T11:24:23.352813: step 3188, loss 0.605165.
Train: 2018-07-31T11:24:23.509051: step 3189, loss 0.469659.
Train: 2018-07-31T11:24:23.680853: step 3190, loss 0.562873.
Test: 2018-07-31T11:24:24.165144: step 3190, loss 0.548561.
Train: 2018-07-31T11:24:24.336950: step 3191, loss 0.614122.
Train: 2018-07-31T11:24:24.493187: step 3192, loss 0.597114.
Train: 2018-07-31T11:24:24.665028: step 3193, loss 0.511419.
Train: 2018-07-31T11:24:24.836856: step 3194, loss 0.648805.
Train: 2018-07-31T11:24:25.008667: step 3195, loss 0.537074.
Train: 2018-07-31T11:24:25.164904: step 3196, loss 0.631689.
Train: 2018-07-31T11:24:25.336715: step 3197, loss 0.519877.
Train: 2018-07-31T11:24:25.508584: step 3198, loss 0.476873.
Train: 2018-07-31T11:24:25.680419: step 3199, loss 0.623173.
Train: 2018-07-31T11:24:25.852219: step 3200, loss 0.519762.
Test: 2018-07-31T11:24:26.305238: step 3200, loss 0.548407.
Train: 2018-07-31T11:24:27.180034: step 3201, loss 0.5456.
Train: 2018-07-31T11:24:27.351898: step 3202, loss 0.571509.
Train: 2018-07-31T11:24:27.523703: step 3203, loss 0.666719.
Train: 2018-07-31T11:24:27.695539: step 3204, loss 0.545575.
Train: 2018-07-31T11:24:27.851752: step 3205, loss 0.493754.
Train: 2018-07-31T11:24:28.023615: step 3206, loss 0.597442.
Train: 2018-07-31T11:24:28.195451: step 3207, loss 0.606083.
Train: 2018-07-31T11:24:28.351663: step 3208, loss 0.588767.
Train: 2018-07-31T11:24:28.523467: step 3209, loss 0.54561.
Train: 2018-07-31T11:24:28.679706: step 3210, loss 0.5887.
Test: 2018-07-31T11:24:29.148352: step 3210, loss 0.548446.
Train: 2018-07-31T11:24:29.320181: step 3211, loss 0.614469.
Train: 2018-07-31T11:24:29.507613: step 3212, loss 0.528523.
Train: 2018-07-31T11:24:29.663826: step 3213, loss 0.451441.
Train: 2018-07-31T11:24:29.835661: step 3214, loss 0.519956.
Train: 2018-07-31T11:24:29.991875: step 3215, loss 0.597228.
Train: 2018-07-31T11:24:30.163739: step 3216, loss 0.554248.
Train: 2018-07-31T11:24:30.335574: step 3217, loss 0.511185.
Train: 2018-07-31T11:24:30.507412: step 3218, loss 0.623244.
Train: 2018-07-31T11:24:30.663623: step 3219, loss 0.519694.
Train: 2018-07-31T11:24:30.851076: step 3220, loss 0.580139.
Test: 2018-07-31T11:24:31.319718: step 3220, loss 0.548369.
Train: 2018-07-31T11:24:31.491523: step 3221, loss 0.588802.
Train: 2018-07-31T11:24:31.647767: step 3222, loss 0.545553.
Train: 2018-07-31T11:24:31.819601: step 3223, loss 0.623431.
Train: 2018-07-31T11:24:31.991406: step 3224, loss 0.536913.
Train: 2018-07-31T11:24:32.147650: step 3225, loss 0.571494.
Train: 2018-07-31T11:24:32.319455: step 3226, loss 0.528293.
Train: 2018-07-31T11:24:32.491320: step 3227, loss 0.502361.
Train: 2018-07-31T11:24:32.663154: step 3228, loss 0.545544.
Train: 2018-07-31T11:24:32.819367: step 3229, loss 0.54552.
Train: 2018-07-31T11:24:33.006827: step 3230, loss 0.545494.
Test: 2018-07-31T11:24:33.475434: step 3230, loss 0.548295.
Train: 2018-07-31T11:24:33.631647: step 3231, loss 0.545467.
Train: 2018-07-31T11:24:33.803511: step 3232, loss 0.641301.
Train: 2018-07-31T11:24:33.975348: step 3233, loss 0.571583.
Train: 2018-07-31T11:24:34.131563: step 3234, loss 0.519296.
Train: 2018-07-31T11:24:34.303365: step 3235, loss 0.571588.
Train: 2018-07-31T11:24:34.475200: step 3236, loss 0.527978.
Train: 2018-07-31T11:24:34.647064: step 3237, loss 0.658912.
Train: 2018-07-31T11:24:34.818894: step 3238, loss 0.519264.
Train: 2018-07-31T11:24:34.990736: step 3239, loss 0.554147.
Train: 2018-07-31T11:24:35.162572: step 3240, loss 0.536712.
Test: 2018-07-31T11:24:35.631227: step 3240, loss 0.548261.
Train: 2018-07-31T11:24:35.787393: step 3241, loss 0.623907.
Train: 2018-07-31T11:24:35.959257: step 3242, loss 0.458323.
Train: 2018-07-31T11:24:36.131095: step 3243, loss 0.554143.
Train: 2018-07-31T11:24:36.302897: step 3244, loss 0.554137.
Train: 2018-07-31T11:24:36.459111: step 3245, loss 0.562872.
Train: 2018-07-31T11:24:36.630945: step 3246, loss 0.597869.
Train: 2018-07-31T11:24:36.802779: step 3247, loss 0.589121.
Train: 2018-07-31T11:24:36.958994: step 3248, loss 0.562872.
Train: 2018-07-31T11:24:37.126248: step 3249, loss 0.589082.
Train: 2018-07-31T11:24:37.313675: step 3250, loss 0.53668.
Test: 2018-07-31T11:24:37.782312: step 3250, loss 0.548252.
Train: 2018-07-31T11:24:37.954177: step 3251, loss 0.545415.
Train: 2018-07-31T11:24:38.126014: step 3252, loss 0.545419.
Train: 2018-07-31T11:24:38.282196: step 3253, loss 0.589018.
Train: 2018-07-31T11:24:38.454030: step 3254, loss 0.53671.
Train: 2018-07-31T11:24:38.625894: step 3255, loss 0.55414.
Train: 2018-07-31T11:24:38.797730: step 3256, loss 0.484422.
Train: 2018-07-31T11:24:38.953913: step 3257, loss 0.597772.
Train: 2018-07-31T11:24:39.125780: step 3258, loss 0.606528.
Train: 2018-07-31T11:24:39.297613: step 3259, loss 0.554129.
Train: 2018-07-31T11:24:39.453828: step 3260, loss 0.55413.
Test: 2018-07-31T11:24:39.938088: step 3260, loss 0.548242.
Train: 2018-07-31T11:24:40.109893: step 3261, loss 0.615211.
Train: 2018-07-31T11:24:40.281751: step 3262, loss 0.554136.
Train: 2018-07-31T11:24:40.453562: step 3263, loss 0.597669.
Train: 2018-07-31T11:24:40.625396: step 3264, loss 0.519386.
Train: 2018-07-31T11:24:40.781610: step 3265, loss 0.528099.
Train: 2018-07-31T11:24:40.953475: step 3266, loss 0.519415.
Train: 2018-07-31T11:24:41.125279: step 3267, loss 0.493306.
Train: 2018-07-31T11:24:41.297144: step 3268, loss 0.597684.
Train: 2018-07-31T11:24:41.468949: step 3269, loss 0.580287.
Train: 2018-07-31T11:24:41.640808: step 3270, loss 0.510505.
Test: 2018-07-31T11:24:42.125046: step 3270, loss 0.548223.
Train: 2018-07-31T11:24:42.296904: step 3271, loss 0.510433.
Train: 2018-07-31T11:24:42.453124: step 3272, loss 0.571622.
Train: 2018-07-31T11:24:42.624960: step 3273, loss 0.571646.
Train: 2018-07-31T11:24:42.796797: step 3274, loss 0.527742.
Train: 2018-07-31T11:24:42.952977: step 3275, loss 0.483687.
Train: 2018-07-31T11:24:43.124812: step 3276, loss 0.571736.
Train: 2018-07-31T11:24:43.296676: step 3277, loss 0.589471.
Train: 2018-07-31T11:24:43.468511: step 3278, loss 0.598384.
Train: 2018-07-31T11:24:43.640340: step 3279, loss 0.518605.
Train: 2018-07-31T11:24:43.812152: step 3280, loss 0.562948.
Test: 2018-07-31T11:24:44.296412: step 3280, loss 0.548074.
Train: 2018-07-31T11:24:44.468246: step 3281, loss 0.562955.
Train: 2018-07-31T11:24:44.640106: step 3282, loss 0.545177.
Train: 2018-07-31T11:24:44.811946: step 3283, loss 0.580762.
Train: 2018-07-31T11:24:44.983752: step 3284, loss 0.643065.
Train: 2018-07-31T11:24:45.139965: step 3285, loss 0.598497.
Train: 2018-07-31T11:24:45.311799: step 3286, loss 0.554069.
Train: 2018-07-31T11:24:45.483664: step 3287, loss 0.527527.
Train: 2018-07-31T11:24:45.655468: step 3288, loss 0.580583.
Train: 2018-07-31T11:24:45.827303: step 3289, loss 0.580542.
Train: 2018-07-31T11:24:45.999139: step 3290, loss 0.606911.
Test: 2018-07-31T11:24:46.467779: step 3290, loss 0.548162.
Train: 2018-07-31T11:24:46.655234: step 3291, loss 0.58043.
Train: 2018-07-31T11:24:46.827099: step 3292, loss 0.510321.
Train: 2018-07-31T11:24:46.998936: step 3293, loss 0.615287.
Train: 2018-07-31T11:24:47.170769: step 3294, loss 0.606419.
Train: 2018-07-31T11:24:47.326985: step 3295, loss 0.571509.
Train: 2018-07-31T11:24:47.498819: step 3296, loss 0.510857.
Train: 2018-07-31T11:24:47.670646: step 3297, loss 0.493668.
Train: 2018-07-31T11:24:47.842457: step 3298, loss 0.545528.
Train: 2018-07-31T11:24:47.998699: step 3299, loss 0.640562.
Train: 2018-07-31T11:24:48.170536: step 3300, loss 0.554178.
Test: 2018-07-31T11:24:48.654773: step 3300, loss 0.548375.
Train: 2018-07-31T11:24:49.373380: step 3301, loss 0.588644.
Train: 2018-07-31T11:24:49.545183: step 3302, loss 0.5628.
Train: 2018-07-31T11:24:49.717019: step 3303, loss 0.545628.
Train: 2018-07-31T11:24:49.888885: step 3304, loss 0.502765.
Train: 2018-07-31T11:24:50.060720: step 3305, loss 0.60569.
Train: 2018-07-31T11:24:50.232522: step 3306, loss 0.605663.
Train: 2018-07-31T11:24:50.404358: step 3307, loss 0.699765.
Train: 2018-07-31T11:24:50.576191: step 3308, loss 0.630999.
Train: 2018-07-31T11:24:50.748026: step 3309, loss 0.545848.
Train: 2018-07-31T11:24:50.919890: step 3310, loss 0.571267.
Test: 2018-07-31T11:24:51.388532: step 3310, loss 0.548751.
Train: 2018-07-31T11:24:51.560360: step 3311, loss 0.588071.
Train: 2018-07-31T11:24:51.716549: step 3312, loss 0.512593.
Train: 2018-07-31T11:24:51.888383: step 3313, loss 0.579583.
Train: 2018-07-31T11:24:52.058625: step 3314, loss 0.529537.
Train: 2018-07-31T11:24:52.230459: step 3315, loss 0.579542.
Train: 2018-07-31T11:24:52.386703: step 3316, loss 0.604468.
Train: 2018-07-31T11:24:52.558508: step 3317, loss 0.57121.
Train: 2018-07-31T11:24:52.730373: step 3318, loss 0.529803.
Train: 2018-07-31T11:24:52.902177: step 3319, loss 0.587756.
Train: 2018-07-31T11:24:53.074042: step 3320, loss 0.546409.
Test: 2018-07-31T11:24:53.542652: step 3320, loss 0.549106.
Train: 2018-07-31T11:24:53.714486: step 3321, loss 0.562944.
Train: 2018-07-31T11:24:53.870700: step 3322, loss 0.598189.
Train: 2018-07-31T11:24:54.026944: step 3323, loss 0.496919.
Train: 2018-07-31T11:24:54.198772: step 3324, loss 0.55468.
Train: 2018-07-31T11:24:54.354986: step 3325, loss 0.48021.
Train: 2018-07-31T11:24:54.526827: step 3326, loss 0.529715.
Train: 2018-07-31T11:24:54.698661: step 3327, loss 0.596197.
Train: 2018-07-31T11:24:54.870474: step 3328, loss 0.571216.
Train: 2018-07-31T11:24:55.026713: step 3329, loss 0.596337.
Train: 2018-07-31T11:24:55.198515: step 3330, loss 0.495765.
Test: 2018-07-31T11:24:55.682801: step 3330, loss 0.548744.
Train: 2018-07-31T11:24:55.854611: step 3331, loss 0.52919.
Train: 2018-07-31T11:24:56.010854: step 3332, loss 0.571252.
Train: 2018-07-31T11:24:56.182683: step 3333, loss 0.579731.
Train: 2018-07-31T11:24:56.354520: step 3334, loss 0.681579.
Train: 2018-07-31T11:24:56.526359: step 3335, loss 0.545832.
Train: 2018-07-31T11:24:56.682542: step 3336, loss 0.579761.
Train: 2018-07-31T11:24:56.869997: step 3337, loss 0.588239.
Train: 2018-07-31T11:24:57.026210: step 3338, loss 0.579748.
Train: 2018-07-31T11:24:57.198076: step 3339, loss 0.605139.
Train: 2018-07-31T11:24:57.369880: step 3340, loss 0.579711.
Test: 2018-07-31T11:24:57.838520: step 3340, loss 0.548674.
Train: 2018-07-31T11:24:58.010356: step 3341, loss 0.495295.
Train: 2018-07-31T11:24:58.182221: step 3342, loss 0.596562.
Train: 2018-07-31T11:24:58.354058: step 3343, loss 0.579675.
Train: 2018-07-31T11:24:58.510238: step 3344, loss 0.638635.
Train: 2018-07-31T11:24:58.682105: step 3345, loss 0.554418.
Train: 2018-07-31T11:24:58.853908: step 3346, loss 0.554442.
Train: 2018-07-31T11:24:59.025744: step 3347, loss 0.512575.
Train: 2018-07-31T11:24:59.181957: step 3348, loss 0.571214.
Train: 2018-07-31T11:24:59.353821: step 3349, loss 0.59634.
Train: 2018-07-31T11:24:59.525626: step 3350, loss 0.554471.
Test: 2018-07-31T11:25:00.009888: step 3350, loss 0.54883.
Train: 2018-07-31T11:25:00.181723: step 3351, loss 0.62141.
Train: 2018-07-31T11:25:00.353557: step 3352, loss 0.537786.
Train: 2018-07-31T11:25:00.509770: step 3353, loss 0.512759.
Train: 2018-07-31T11:25:00.681604: step 3354, loss 0.612976.
Train: 2018-07-31T11:25:00.853439: step 3355, loss 0.496041.
Train: 2018-07-31T11:25:01.025305: step 3356, loss 0.604651.
Train: 2018-07-31T11:25:01.197134: step 3357, loss 0.579569.
Train: 2018-07-31T11:25:01.368945: step 3358, loss 0.613028.
Train: 2018-07-31T11:25:01.540778: step 3359, loss 0.612986.
Train: 2018-07-31T11:25:01.712645: step 3360, loss 0.521147.
Test: 2018-07-31T11:25:02.181279: step 3360, loss 0.548896.
Train: 2018-07-31T11:25:02.353089: step 3361, loss 0.529511.
Train: 2018-07-31T11:25:02.518862: step 3362, loss 0.529497.
Train: 2018-07-31T11:25:02.675074: step 3363, loss 0.546149.
Train: 2018-07-31T11:25:02.846939: step 3364, loss 0.529388.
Train: 2018-07-31T11:25:03.018744: step 3365, loss 0.596351.
Train: 2018-07-31T11:25:03.190580: step 3366, loss 0.546032.
Train: 2018-07-31T11:25:03.346792: step 3367, loss 0.588033.
Train: 2018-07-31T11:25:03.518657: step 3368, loss 0.537553.
Train: 2018-07-31T11:25:03.690462: step 3369, loss 0.520643.
Train: 2018-07-31T11:25:03.846705: step 3370, loss 0.528982.
Test: 2018-07-31T11:25:04.315316: step 3370, loss 0.548582.
Train: 2018-07-31T11:25:04.487181: step 3371, loss 0.58821.
Train: 2018-07-31T11:25:04.659016: step 3372, loss 0.613747.
Train: 2018-07-31T11:25:04.830850: step 3373, loss 0.545763.
Train: 2018-07-31T11:25:05.002685: step 3374, loss 0.571282.
Train: 2018-07-31T11:25:05.174491: step 3375, loss 0.579811.
Train: 2018-07-31T11:25:05.346355: step 3376, loss 0.528654.
Train: 2018-07-31T11:25:05.502569: step 3377, loss 0.579839.
Train: 2018-07-31T11:25:05.674402: step 3378, loss 0.656756.
Train: 2018-07-31T11:25:05.830586: step 3379, loss 0.596899.
Train: 2018-07-31T11:25:06.002445: step 3380, loss 0.486095.
Test: 2018-07-31T11:25:06.471091: step 3380, loss 0.548496.
Train: 2018-07-31T11:25:06.642929: step 3381, loss 0.554245.
Train: 2018-07-31T11:25:06.799144: step 3382, loss 0.5372.
Train: 2018-07-31T11:25:06.970974: step 3383, loss 0.528652.
Train: 2018-07-31T11:25:07.141739: step 3384, loss 0.605457.
Train: 2018-07-31T11:25:07.313579: step 3385, loss 0.511499.
Train: 2018-07-31T11:25:07.485414: step 3386, loss 0.451531.
Train: 2018-07-31T11:25:07.641597: step 3387, loss 0.528407.
Train: 2018-07-31T11:25:07.813432: step 3388, loss 0.605864.
Train: 2018-07-31T11:25:07.985296: step 3389, loss 0.536828.
Train: 2018-07-31T11:25:08.157134: step 3390, loss 0.562766.
Test: 2018-07-31T11:25:08.610150: step 3390, loss 0.548217.
Train: 2018-07-31T11:25:08.781956: step 3391, loss 0.571464.
Train: 2018-07-31T11:25:08.953821: step 3392, loss 0.54536.
Train: 2018-07-31T11:25:09.125655: step 3393, loss 0.475509.
Train: 2018-07-31T11:25:09.297461: step 3394, loss 0.510248.
Train: 2018-07-31T11:25:09.453674: step 3395, loss 0.545232.
Train: 2018-07-31T11:25:09.625507: step 3396, loss 0.589338.
Train: 2018-07-31T11:25:09.797343: step 3397, loss 0.58058.
Train: 2018-07-31T11:25:09.969177: step 3398, loss 0.545134.
Train: 2018-07-31T11:25:10.141014: step 3399, loss 0.598473.
Train: 2018-07-31T11:25:10.297227: step 3400, loss 0.562909.
Test: 2018-07-31T11:25:10.765895: step 3400, loss 0.547995.
Train: 2018-07-31T11:25:11.453235: step 3401, loss 0.518371.
Train: 2018-07-31T11:25:11.625040: step 3402, loss 0.482638.
Train: 2018-07-31T11:25:11.796874: step 3403, loss 0.491399.
Train: 2018-07-31T11:25:11.953118: step 3404, loss 0.607864.
Train: 2018-07-31T11:25:12.124922: step 3405, loss 0.56301.
Train: 2018-07-31T11:25:12.296757: step 3406, loss 0.526992.
Train: 2018-07-31T11:25:12.453001: step 3407, loss 0.590131.
Train: 2018-07-31T11:25:12.624835: step 3408, loss 0.59017.
Train: 2018-07-31T11:25:12.796642: step 3409, loss 0.572102.
Train: 2018-07-31T11:25:12.968501: step 3410, loss 0.499817.
Test: 2018-07-31T11:25:13.437145: step 3410, loss 0.547927.
Train: 2018-07-31T11:25:13.593328: step 3411, loss 0.599235.
Train: 2018-07-31T11:25:13.765194: step 3412, loss 0.52691.
Train: 2018-07-31T11:25:13.937028: step 3413, loss 0.563068.
Train: 2018-07-31T11:25:14.093241: step 3414, loss 0.499785.
Train: 2018-07-31T11:25:14.265046: step 3415, loss 0.463546.
Train: 2018-07-31T11:25:14.421259: step 3416, loss 0.617531.
Train: 2018-07-31T11:25:14.593125: step 3417, loss 0.608514.
Train: 2018-07-31T11:25:14.749307: step 3418, loss 0.535889.
Train: 2018-07-31T11:25:14.921174: step 3419, loss 0.55404.
Train: 2018-07-31T11:25:15.093002: step 3420, loss 0.554039.
Test: 2018-07-31T11:25:15.561647: step 3420, loss 0.547915.
Train: 2018-07-31T11:25:15.733484: step 3421, loss 0.608482.
Train: 2018-07-31T11:25:15.905312: step 3422, loss 0.608408.
Train: 2018-07-31T11:25:16.061500: step 3423, loss 0.617321.
Train: 2018-07-31T11:25:16.233360: step 3424, loss 0.644144.
Train: 2018-07-31T11:25:16.405203: step 3425, loss 0.589885.
Train: 2018-07-31T11:25:16.561408: step 3426, loss 0.491509.
Train: 2018-07-31T11:25:16.733219: step 3427, loss 0.527302.
Train: 2018-07-31T11:25:16.905053: step 3428, loss 0.580621.
Train: 2018-07-31T11:25:17.076912: step 3429, loss 0.589403.
Train: 2018-07-31T11:25:17.233132: step 3430, loss 0.598122.
Test: 2018-07-31T11:25:17.701741: step 3430, loss 0.548079.
Train: 2018-07-31T11:25:17.873577: step 3431, loss 0.554012.
Train: 2018-07-31T11:25:18.029789: step 3432, loss 0.545259.
Train: 2018-07-31T11:25:18.201624: step 3433, loss 0.527812.
Train: 2018-07-31T11:25:18.373460: step 3434, loss 0.588939.
Train: 2018-07-31T11:25:18.545326: step 3435, loss 0.588872.
Train: 2018-07-31T11:25:18.717129: step 3436, loss 0.545383.
Train: 2018-07-31T11:25:18.873373: step 3437, loss 0.588737.
Train: 2018-07-31T11:25:19.045209: step 3438, loss 0.519518.
Train: 2018-07-31T11:25:19.201391: step 3439, loss 0.536838.
Train: 2018-07-31T11:25:19.373225: step 3440, loss 0.614491.
Test: 2018-07-31T11:25:19.841896: step 3440, loss 0.548309.
Train: 2018-07-31T11:25:20.013700: step 3441, loss 0.597176.
Train: 2018-07-31T11:25:20.169914: step 3442, loss 0.588504.
Train: 2018-07-31T11:25:20.341748: step 3443, loss 0.588436.
Train: 2018-07-31T11:25:20.497986: step 3444, loss 0.554185.
Train: 2018-07-31T11:25:20.669821: step 3445, loss 0.588304.
Train: 2018-07-31T11:25:20.841661: step 3446, loss 0.613743.
Train: 2018-07-31T11:25:20.997869: step 3447, loss 0.537329.
Train: 2018-07-31T11:25:21.169680: step 3448, loss 0.621903.
Train: 2018-07-31T11:25:21.341514: step 3449, loss 0.537502.
Train: 2018-07-31T11:25:21.497752: step 3450, loss 0.512377.
Test: 2018-07-31T11:25:21.966366: step 3450, loss 0.548727.
Train: 2018-07-31T11:25:22.138202: step 3451, loss 0.554388.
Train: 2018-07-31T11:25:22.310036: step 3452, loss 0.537627.
Train: 2018-07-31T11:25:22.481873: step 3453, loss 0.60471.
Train: 2018-07-31T11:25:22.638116: step 3454, loss 0.529267.
Train: 2018-07-31T11:25:22.809952: step 3455, loss 0.621448.
Train: 2018-07-31T11:25:22.966163: step 3456, loss 0.587905.
Train: 2018-07-31T11:25:23.137970: step 3457, loss 0.487548.
Train: 2018-07-31T11:25:23.309823: step 3458, loss 0.629716.
Train: 2018-07-31T11:25:23.466042: step 3459, loss 0.587871.
Train: 2018-07-31T11:25:23.637851: step 3460, loss 0.596196.
Test: 2018-07-31T11:25:24.122144: step 3460, loss 0.548854.
Train: 2018-07-31T11:25:24.278358: step 3461, loss 0.529474.
Train: 2018-07-31T11:25:24.450191: step 3462, loss 0.662766.
Train: 2018-07-31T11:25:24.621995: step 3463, loss 0.554526.
Train: 2018-07-31T11:25:24.793830: step 3464, loss 0.562847.
Train: 2018-07-31T11:25:24.950075: step 3465, loss 0.56286.
Train: 2018-07-31T11:25:25.121909: step 3466, loss 0.505013.
Train: 2018-07-31T11:25:25.293746: step 3467, loss 0.571135.
Train: 2018-07-31T11:25:25.465573: step 3468, loss 0.513248.
Train: 2018-07-31T11:25:25.637414: step 3469, loss 0.637391.
Train: 2018-07-31T11:25:25.793597: step 3470, loss 0.513168.
Test: 2018-07-31T11:25:26.277857: step 3470, loss 0.54896.
Train: 2018-07-31T11:25:26.449692: step 3471, loss 0.554554.
Train: 2018-07-31T11:25:26.605936: step 3472, loss 0.521324.
Train: 2018-07-31T11:25:26.762120: step 3473, loss 0.562817.
Train: 2018-07-31T11:25:26.933954: step 3474, loss 0.637862.
Train: 2018-07-31T11:25:27.105819: step 3475, loss 0.579487.
Train: 2018-07-31T11:25:27.262002: step 3476, loss 0.504377.
Train: 2018-07-31T11:25:27.433837: step 3477, loss 0.546069.
Train: 2018-07-31T11:25:27.590050: step 3478, loss 0.571152.
Train: 2018-07-31T11:25:27.761915: step 3479, loss 0.571157.
Train: 2018-07-31T11:25:27.933745: step 3480, loss 0.545958.
Test: 2018-07-31T11:25:28.402391: step 3480, loss 0.548657.
Train: 2018-07-31T11:25:28.558604: step 3481, loss 0.537504.
Train: 2018-07-31T11:25:28.730439: step 3482, loss 0.512135.
Train: 2018-07-31T11:25:28.902244: step 3483, loss 0.571193.
Train: 2018-07-31T11:25:29.074079: step 3484, loss 0.460913.
Train: 2018-07-31T11:25:29.230291: step 3485, loss 0.59681.
Train: 2018-07-31T11:25:29.402126: step 3486, loss 0.571264.
Train: 2018-07-31T11:25:29.558371: step 3487, loss 0.571288.
Train: 2018-07-31T11:25:29.730175: step 3488, loss 0.511109.
Train: 2018-07-31T11:25:29.886413: step 3489, loss 0.648995.
Train: 2018-07-31T11:25:30.058253: step 3490, loss 0.605904.
Test: 2018-07-31T11:25:30.526893: step 3490, loss 0.548245.
Train: 2018-07-31T11:25:30.698698: step 3491, loss 0.571352.
Train: 2018-07-31T11:25:30.854941: step 3492, loss 0.493609.
Train: 2018-07-31T11:25:31.026746: step 3493, loss 0.588661.
Train: 2018-07-31T11:25:31.214201: step 3494, loss 0.554061.
Train: 2018-07-31T11:25:31.370415: step 3495, loss 0.545396.
Train: 2018-07-31T11:25:31.557873: step 3496, loss 0.536715.
Train: 2018-07-31T11:25:31.714086: step 3497, loss 0.510648.
Train: 2018-07-31T11:25:31.885950: step 3498, loss 0.597516.
Train: 2018-07-31T11:25:32.057784: step 3499, loss 0.527898.
Train: 2018-07-31T11:25:32.213998: step 3500, loss 0.571458.
Test: 2018-07-31T11:25:32.682632: step 3500, loss 0.548113.
Train: 2018-07-31T11:25:33.401221: step 3501, loss 0.510338.
Train: 2018-07-31T11:25:33.573057: step 3502, loss 0.562749.
Train: 2018-07-31T11:25:33.744860: step 3503, loss 0.527685.
Train: 2018-07-31T11:25:33.901103: step 3504, loss 0.527619.
Train: 2018-07-31T11:25:34.072938: step 3505, loss 0.509923.
Train: 2018-07-31T11:25:34.244744: step 3506, loss 0.59816.
Train: 2018-07-31T11:25:34.416577: step 3507, loss 0.589391.
Train: 2018-07-31T11:25:34.572820: step 3508, loss 0.598299.
Train: 2018-07-31T11:25:34.744625: step 3509, loss 0.651527.
Train: 2018-07-31T11:25:34.900872: step 3510, loss 0.598235.
Test: 2018-07-31T11:25:35.385099: step 3510, loss 0.548004.
Train: 2018-07-31T11:25:35.556935: step 3511, loss 0.571633.
Train: 2018-07-31T11:25:35.713148: step 3512, loss 0.51872.
Train: 2018-07-31T11:25:35.884983: step 3513, loss 0.518774.
Train: 2018-07-31T11:25:36.056817: step 3514, loss 0.571564.
Train: 2018-07-31T11:25:36.213063: step 3515, loss 0.659432.
Train: 2018-07-31T11:25:36.384867: step 3516, loss 0.483874.
Train: 2018-07-31T11:25:36.556700: step 3517, loss 0.457682.
Train: 2018-07-31T11:25:36.712915: step 3518, loss 0.553984.
Train: 2018-07-31T11:25:36.884750: step 3519, loss 0.571528.
Train: 2018-07-31T11:25:37.056617: step 3520, loss 0.536417.
Test: 2018-07-31T11:25:37.525254: step 3520, loss 0.548043.
Train: 2018-07-31T11:25:37.697083: step 3521, loss 0.606709.
Train: 2018-07-31T11:25:37.868894: step 3522, loss 0.518818.
Train: 2018-07-31T11:25:38.040762: step 3523, loss 0.597946.
Train: 2018-07-31T11:25:38.212562: step 3524, loss 0.589145.
Train: 2018-07-31T11:25:38.368776: step 3525, loss 0.518827.
Train: 2018-07-31T11:25:38.540615: step 3526, loss 0.562759.
Train: 2018-07-31T11:25:38.728092: step 3527, loss 0.562758.
Train: 2018-07-31T11:25:38.884281: step 3528, loss 0.624251.
Train: 2018-07-31T11:25:39.056115: step 3529, loss 0.492575.
Train: 2018-07-31T11:25:39.227950: step 3530, loss 0.527663.
Test: 2018-07-31T11:25:39.696621: step 3530, loss 0.548052.
Train: 2018-07-31T11:25:39.868455: step 3531, loss 0.580303.
Train: 2018-07-31T11:25:40.040260: step 3532, loss 0.536421.
Train: 2018-07-31T11:25:40.212096: step 3533, loss 0.580314.
Train: 2018-07-31T11:25:40.368308: step 3534, loss 0.553972.
Train: 2018-07-31T11:25:40.540144: step 3535, loss 0.597873.
Train: 2018-07-31T11:25:40.712011: step 3536, loss 0.562747.
Train: 2018-07-31T11:25:40.883851: step 3537, loss 0.64164.
Train: 2018-07-31T11:25:41.040027: step 3538, loss 0.606457.
Train: 2018-07-31T11:25:41.211861: step 3539, loss 0.458102.
Train: 2018-07-31T11:25:41.368105: step 3540, loss 0.562714.
Test: 2018-07-31T11:25:41.852366: step 3540, loss 0.548133.
Train: 2018-07-31T11:25:42.024170: step 3541, loss 0.597527.
Train: 2018-07-31T11:25:42.196006: step 3542, loss 0.536632.
Train: 2018-07-31T11:25:42.367864: step 3543, loss 0.484549.
Train: 2018-07-31T11:25:42.524084: step 3544, loss 0.545324.
Train: 2018-07-31T11:25:42.695889: step 3545, loss 0.571406.
Train: 2018-07-31T11:25:42.867753: step 3546, loss 0.545301.
Train: 2018-07-31T11:25:43.023936: step 3547, loss 0.510442.
Train: 2018-07-31T11:25:43.195772: step 3548, loss 0.545264.
Train: 2018-07-31T11:25:43.367605: step 3549, loss 0.58021.
Train: 2018-07-31T11:25:43.539440: step 3550, loss 0.483949.
Test: 2018-07-31T11:25:44.008111: step 3550, loss 0.548045.
Train: 2018-07-31T11:25:44.179946: step 3551, loss 0.492534.
Train: 2018-07-31T11:25:44.351751: step 3552, loss 0.642031.
Train: 2018-07-31T11:25:44.523585: step 3553, loss 0.580414.
Train: 2018-07-31T11:25:44.679829: step 3554, loss 0.580436.
Train: 2018-07-31T11:25:44.851664: step 3555, loss 0.651101.
Train: 2018-07-31T11:25:45.023498: step 3556, loss 0.55395.
Train: 2018-07-31T11:25:45.195304: step 3557, loss 0.606792.
Train: 2018-07-31T11:25:45.351546: step 3558, loss 0.606681.
Train: 2018-07-31T11:25:45.523381: step 3559, loss 0.536448.
Train: 2018-07-31T11:25:45.695186: step 3560, loss 0.615165.
Test: 2018-07-31T11:25:46.163825: step 3560, loss 0.548114.
Train: 2018-07-31T11:25:46.335691: step 3561, loss 0.623699.
Train: 2018-07-31T11:25:46.507496: step 3562, loss 0.527986.
Train: 2018-07-31T11:25:46.663741: step 3563, loss 0.536736.
Train: 2018-07-31T11:25:46.835545: step 3564, loss 0.571311.
Train: 2018-07-31T11:25:47.007379: step 3565, loss 0.4938.
Train: 2018-07-31T11:25:47.163617: step 3566, loss 0.554074.
Train: 2018-07-31T11:25:47.335457: step 3567, loss 0.64868.
Train: 2018-07-31T11:25:47.507262: step 3568, loss 0.588421.
Train: 2018-07-31T11:25:47.663475: step 3569, loss 0.571236.
Train: 2018-07-31T11:25:47.835309: step 3570, loss 0.605374.
Test: 2018-07-31T11:25:48.303980: step 3570, loss 0.548424.
Train: 2018-07-31T11:25:48.475786: step 3571, loss 0.571194.
Train: 2018-07-31T11:25:48.647620: step 3572, loss 0.59664.
Train: 2018-07-31T11:25:48.803863: step 3573, loss 0.503474.
Train: 2018-07-31T11:25:48.960046: step 3574, loss 0.562701.
Train: 2018-07-31T11:25:49.131880: step 3575, loss 0.663917.
Train: 2018-07-31T11:25:49.303741: step 3576, loss 0.604749.
Train: 2018-07-31T11:25:49.475582: step 3577, loss 0.554364.
Train: 2018-07-31T11:25:49.631765: step 3578, loss 0.60449.
Train: 2018-07-31T11:25:49.803635: step 3579, loss 0.537828.
Train: 2018-07-31T11:25:49.975434: step 3580, loss 0.488141.
Test: 2018-07-31T11:25:50.444074: step 3580, loss 0.548914.
Train: 2018-07-31T11:25:50.615909: step 3581, loss 0.554509.
Train: 2018-07-31T11:25:50.787743: step 3582, loss 0.604247.
Train: 2018-07-31T11:25:50.943957: step 3583, loss 0.554524.
Train: 2018-07-31T11:25:51.115822: step 3584, loss 0.455186.
Train: 2018-07-31T11:25:51.287651: step 3585, loss 0.562792.
Train: 2018-07-31T11:25:51.459488: step 3586, loss 0.554463.
Train: 2018-07-31T11:25:51.646917: step 3587, loss 0.537764.
Train: 2018-07-31T11:25:51.803131: step 3588, loss 0.562745.
Train: 2018-07-31T11:25:51.959344: step 3589, loss 0.596237.
Train: 2018-07-31T11:25:52.115588: step 3590, loss 0.503989.
Test: 2018-07-31T11:25:52.599818: step 3590, loss 0.548616.
Train: 2018-07-31T11:25:52.771654: step 3591, loss 0.537465.
Train: 2018-07-31T11:25:52.927898: step 3592, loss 0.579579.
Train: 2018-07-31T11:25:53.099733: step 3593, loss 0.554223.
Train: 2018-07-31T11:25:53.271537: step 3594, loss 0.613593.
Train: 2018-07-31T11:25:53.443401: step 3595, loss 0.630648.
Train: 2018-07-31T11:25:53.615206: step 3596, loss 0.579665.
Train: 2018-07-31T11:25:53.787040: step 3597, loss 0.562677.
Train: 2018-07-31T11:25:53.943255: step 3598, loss 0.562678.
Train: 2018-07-31T11:25:54.115089: step 3599, loss 0.605097.
Train: 2018-07-31T11:25:54.271333: step 3600, loss 0.494888.
Test: 2018-07-31T11:25:54.755595: step 3600, loss 0.54848.
Train: 2018-07-31T11:25:55.458525: step 3601, loss 0.579635.
Train: 2018-07-31T11:25:55.630384: step 3602, loss 0.46093.
Train: 2018-07-31T11:25:55.802224: step 3603, loss 0.503182.
Train: 2018-07-31T11:25:55.974028: step 3604, loss 0.588248.
Train: 2018-07-31T11:25:56.130273: step 3605, loss 0.494273.
Train: 2018-07-31T11:25:56.302108: step 3606, loss 0.639886.
Train: 2018-07-31T11:25:56.489533: step 3607, loss 0.588448.
Train: 2018-07-31T11:25:56.645780: step 3608, loss 0.554058.
Train: 2018-07-31T11:25:56.817581: step 3609, loss 0.648794.
Train: 2018-07-31T11:25:56.989447: step 3610, loss 0.536843.
Test: 2018-07-31T11:25:57.458055: step 3610, loss 0.548251.
Train: 2018-07-31T11:25:57.629915: step 3611, loss 0.536849.
Train: 2018-07-31T11:25:57.817348: step 3612, loss 0.528234.
Train: 2018-07-31T11:25:57.973561: step 3613, loss 0.605737.
Train: 2018-07-31T11:25:58.145395: step 3614, loss 0.605735.
Train: 2018-07-31T11:25:58.317261: step 3615, loss 0.545446.
Train: 2018-07-31T11:25:58.489089: step 3616, loss 0.579864.
Train: 2018-07-31T11:25:58.660924: step 3617, loss 0.485298.
Train: 2018-07-31T11:25:58.817144: step 3618, loss 0.571261.
Train: 2018-07-31T11:25:58.988972: step 3619, loss 0.605697.
Train: 2018-07-31T11:25:59.145162: step 3620, loss 0.60568.
Test: 2018-07-31T11:25:59.613800: step 3620, loss 0.548264.
Train: 2018-07-31T11:25:59.785669: step 3621, loss 0.588438.
Train: 2018-07-31T11:25:59.957501: step 3622, loss 0.562656.
Train: 2018-07-31T11:26:00.113715: step 3623, loss 0.511256.
Train: 2018-07-31T11:26:00.269931: step 3624, loss 0.580926.
Train: 2018-07-31T11:26:00.441763: step 3625, loss 0.528419.
Train: 2018-07-31T11:26:00.597977: step 3626, loss 0.519853.
Train: 2018-07-31T11:26:00.769811: step 3627, loss 0.596931.
Train: 2018-07-31T11:26:00.941646: step 3628, loss 0.562654.
Train: 2018-07-31T11:26:01.113476: step 3629, loss 0.528361.
Train: 2018-07-31T11:26:01.269664: step 3630, loss 0.53691.
Test: 2018-07-31T11:26:01.738303: step 3630, loss 0.548263.
Train: 2018-07-31T11:26:01.910171: step 3631, loss 0.571245.
Train: 2018-07-31T11:26:02.082009: step 3632, loss 0.545451.
Train: 2018-07-31T11:26:02.253833: step 3633, loss 0.502369.
Train: 2018-07-31T11:26:02.441267: step 3634, loss 0.657613.
Train: 2018-07-31T11:26:02.597478: step 3635, loss 0.562656.
Train: 2018-07-31T11:26:02.769346: step 3636, loss 0.588559.
Train: 2018-07-31T11:26:02.941178: step 3637, loss 0.571285.
Train: 2018-07-31T11:26:03.113008: step 3638, loss 0.605779.
Train: 2018-07-31T11:26:03.269226: step 3639, loss 0.666.
Train: 2018-07-31T11:26:03.441030: step 3640, loss 0.511154.
Test: 2018-07-31T11:26:03.909671: step 3640, loss 0.548305.
Train: 2018-07-31T11:26:04.081506: step 3641, loss 0.571214.
Train: 2018-07-31T11:26:04.253370: step 3642, loss 0.519913.
Train: 2018-07-31T11:26:04.409555: step 3643, loss 0.605348.
Train: 2018-07-31T11:26:04.581418: step 3644, loss 0.494454.
Train: 2018-07-31T11:26:04.737626: step 3645, loss 0.579701.
Train: 2018-07-31T11:26:04.909436: step 3646, loss 0.460403.
Train: 2018-07-31T11:26:05.081271: step 3647, loss 0.571186.
Train: 2018-07-31T11:26:05.253137: step 3648, loss 0.511356.
Train: 2018-07-31T11:26:05.409352: step 3649, loss 0.5198.
Train: 2018-07-31T11:26:05.581184: step 3650, loss 0.554051.
Test: 2018-07-31T11:26:06.049793: step 3650, loss 0.548211.
Train: 2018-07-31T11:26:06.221629: step 3651, loss 0.554028.
Train: 2018-07-31T11:26:06.393489: step 3652, loss 0.588584.
Train: 2018-07-31T11:26:06.549702: step 3653, loss 0.553995.
Train: 2018-07-31T11:26:06.721512: step 3654, loss 0.571336.
Train: 2018-07-31T11:26:06.893377: step 3655, loss 0.623488.
Train: 2018-07-31T11:26:07.049561: step 3656, loss 0.580041.
Train: 2018-07-31T11:26:07.283914: step 3657, loss 0.588719.
Train: 2018-07-31T11:26:07.455746: step 3658, loss 0.536626.
Train: 2018-07-31T11:26:07.627551: step 3659, loss 0.606028.
Train: 2018-07-31T11:26:07.799409: step 3660, loss 0.571317.
Test: 2018-07-31T11:26:08.252404: step 3660, loss 0.548161.
Train: 2018-07-31T11:26:08.439884: step 3661, loss 0.56265.
Train: 2018-07-31T11:26:08.611695: step 3662, loss 0.571288.
Train: 2018-07-31T11:26:08.767939: step 3663, loss 0.536759.
Train: 2018-07-31T11:26:08.939773: step 3664, loss 0.485042.
Train: 2018-07-31T11:26:09.111579: step 3665, loss 0.605797.
Train: 2018-07-31T11:26:09.283445: step 3666, loss 0.597163.
Train: 2018-07-31T11:26:09.439626: step 3667, loss 0.554021.
Train: 2018-07-31T11:26:09.611490: step 3668, loss 0.536789.
Train: 2018-07-31T11:26:09.783328: step 3669, loss 0.60573.
Train: 2018-07-31T11:26:09.955131: step 3670, loss 0.605689.
Test: 2018-07-31T11:26:10.439391: step 3670, loss 0.548245.
Train: 2018-07-31T11:26:10.611226: step 3671, loss 0.597018.
Train: 2018-07-31T11:26:10.767439: step 3672, loss 0.562639.
Train: 2018-07-31T11:26:10.939305: step 3673, loss 0.554081.
Train: 2018-07-31T11:26:11.111139: step 3674, loss 0.554096.
Train: 2018-07-31T11:26:11.267353: step 3675, loss 0.545576.
Train: 2018-07-31T11:26:11.439158: step 3676, loss 0.502963.
Train: 2018-07-31T11:26:11.610991: step 3677, loss 0.54558.
Train: 2018-07-31T11:26:11.767206: step 3678, loss 0.588253.
Train: 2018-07-31T11:26:11.939074: step 3679, loss 0.571179.
Train: 2018-07-31T11:26:12.110876: step 3680, loss 0.579721.
Test: 2018-07-31T11:26:12.579545: step 3680, loss 0.548336.
Train: 2018-07-31T11:26:12.751380: step 3681, loss 0.571177.
Train: 2018-07-31T11:26:12.907596: step 3682, loss 0.571174.
Train: 2018-07-31T11:26:13.079428: step 3683, loss 0.52851.
Train: 2018-07-31T11:26:13.251234: step 3684, loss 0.588241.
Train: 2018-07-31T11:26:13.407471: step 3685, loss 0.519975.
Train: 2018-07-31T11:26:13.579312: step 3686, loss 0.554098.
Train: 2018-07-31T11:26:13.735525: step 3687, loss 0.545544.
Train: 2018-07-31T11:26:13.907329: step 3688, loss 0.459972.
Train: 2018-07-31T11:26:14.079194: step 3689, loss 0.571215.
Train: 2018-07-31T11:26:14.250999: step 3690, loss 0.562634.
Test: 2018-07-31T11:26:14.719671: step 3690, loss 0.54819.
Train: 2018-07-31T11:26:14.891504: step 3691, loss 0.605764.
Train: 2018-07-31T11:26:15.047687: step 3692, loss 0.554001.
Train: 2018-07-31T11:26:15.219547: step 3693, loss 0.493464.
Train: 2018-07-31T11:26:15.391357: step 3694, loss 0.562644.
Train: 2018-07-31T11:26:15.563191: step 3695, loss 0.606086.
Train: 2018-07-31T11:26:15.735027: step 3696, loss 0.553956.
Train: 2018-07-31T11:26:15.891240: step 3697, loss 0.623593.
Train: 2018-07-31T11:26:16.063074: step 3698, loss 0.640974.
Train: 2018-07-31T11:26:16.234939: step 3699, loss 0.501852.
Train: 2018-07-31T11:26:16.391124: step 3700, loss 0.536606.
Test: 2018-07-31T11:26:16.875385: step 3700, loss 0.548109.
Train: 2018-07-31T11:26:17.640829: step 3701, loss 0.588682.
Train: 2018-07-31T11:26:17.797068: step 3702, loss 0.597336.
Train: 2018-07-31T11:26:17.968908: step 3703, loss 0.571301.
Train: 2018-07-31T11:26:18.125116: step 3704, loss 0.476142.
Train: 2018-07-31T11:26:18.296951: step 3705, loss 0.579943.
Train: 2018-07-31T11:26:18.468792: step 3706, loss 0.597249.
Train: 2018-07-31T11:26:18.640621: step 3707, loss 0.597221.
Train: 2018-07-31T11:26:18.796839: step 3708, loss 0.588535.
Train: 2018-07-31T11:26:18.968644: step 3709, loss 0.631578.
Train: 2018-07-31T11:26:19.124857: step 3710, loss 0.545443.
Test: 2018-07-31T11:26:19.609119: step 3710, loss 0.548272.
Train: 2018-07-31T11:26:19.765333: step 3711, loss 0.588338.
Train: 2018-07-31T11:26:19.937168: step 3712, loss 0.588269.
Train: 2018-07-31T11:26:20.109033: step 3713, loss 0.673412.
Train: 2018-07-31T11:26:20.280836: step 3714, loss 0.647431.
Train: 2018-07-31T11:26:20.437081: step 3715, loss 0.545808.
Train: 2018-07-31T11:26:20.608885: step 3716, loss 0.54592.
Train: 2018-07-31T11:26:20.780719: step 3717, loss 0.537668.
Train: 2018-07-31T11:26:20.936933: step 3718, loss 0.546087.
Train: 2018-07-31T11:26:21.108767: step 3719, loss 0.579344.
Train: 2018-07-31T11:26:21.280635: step 3720, loss 0.546197.
Test: 2018-07-31T11:26:21.749273: step 3720, loss 0.548924.
Train: 2018-07-31T11:26:21.921108: step 3721, loss 0.546235.
Train: 2018-07-31T11:26:22.092937: step 3722, loss 0.521472.
Train: 2018-07-31T11:26:22.249164: step 3723, loss 0.521455.
Train: 2018-07-31T11:26:22.420960: step 3724, loss 0.546212.
Train: 2018-07-31T11:26:22.592826: step 3725, loss 0.579331.
Train: 2018-07-31T11:26:22.749039: step 3726, loss 0.620855.
Train: 2018-07-31T11:26:22.920875: step 3727, loss 0.537827.
Train: 2018-07-31T11:26:23.092678: step 3728, loss 0.521179.
Train: 2018-07-31T11:26:23.248892: step 3729, loss 0.512764.
Train: 2018-07-31T11:26:23.420751: step 3730, loss 0.5961.
Test: 2018-07-31T11:26:23.905020: step 3730, loss 0.548677.
Train: 2018-07-31T11:26:24.061232: step 3731, loss 0.545956.
Train: 2018-07-31T11:26:24.233037: step 3732, loss 0.596218.
Train: 2018-07-31T11:26:24.404903: step 3733, loss 0.520683.
Train: 2018-07-31T11:26:24.561115: step 3734, loss 0.554242.
Train: 2018-07-31T11:26:24.732920: step 3735, loss 0.545777.
Train: 2018-07-31T11:26:24.904786: step 3736, loss 0.545725.
Train: 2018-07-31T11:26:25.076625: step 3737, loss 0.571113.
Train: 2018-07-31T11:26:25.232803: step 3738, loss 0.639127.
Train: 2018-07-31T11:26:25.404669: step 3739, loss 0.579634.
Train: 2018-07-31T11:26:25.560880: step 3740, loss 0.562625.
Test: 2018-07-31T11:26:26.045142: step 3740, loss 0.548378.
Train: 2018-07-31T11:26:26.201326: step 3741, loss 0.545611.
Train: 2018-07-31T11:26:26.373191: step 3742, loss 0.53709.
Train: 2018-07-31T11:26:26.545019: step 3743, loss 0.468901.
Train: 2018-07-31T11:26:26.701239: step 3744, loss 0.579708.
Train: 2018-07-31T11:26:26.873073: step 3745, loss 0.57118.
Train: 2018-07-31T11:26:27.044902: step 3746, loss 0.588359.
Train: 2018-07-31T11:26:27.201122: step 3747, loss 0.64853.
Train: 2018-07-31T11:26:27.372927: step 3748, loss 0.554029.
Train: 2018-07-31T11:26:27.529173: step 3749, loss 0.545452.
Train: 2018-07-31T11:26:27.701004: step 3750, loss 0.562614.
Test: 2018-07-31T11:26:28.185262: step 3750, loss 0.548244.
Train: 2018-07-31T11:26:28.357096: step 3751, loss 0.493975.
Train: 2018-07-31T11:26:28.528905: step 3752, loss 0.588388.
Train: 2018-07-31T11:26:28.700770: step 3753, loss 0.545419.
Train: 2018-07-31T11:26:28.856954: step 3754, loss 0.51098.
Train: 2018-07-31T11:26:29.028820: step 3755, loss 0.588481.
Train: 2018-07-31T11:26:29.200624: step 3756, loss 0.571248.
Train: 2018-07-31T11:26:29.356837: step 3757, loss 0.553978.
Train: 2018-07-31T11:26:29.528671: step 3758, loss 0.502089.
Train: 2018-07-31T11:26:29.716159: step 3759, loss 0.536629.
Train: 2018-07-31T11:26:29.872340: step 3760, loss 0.484468.
Test: 2018-07-31T11:26:30.356632: step 3760, loss 0.548043.
Train: 2018-07-31T11:26:30.512846: step 3761, loss 0.571355.
Train: 2018-07-31T11:26:30.684651: step 3762, loss 0.545168.
Train: 2018-07-31T11:26:30.856521: step 3763, loss 0.597733.
Train: 2018-07-31T11:26:31.012723: step 3764, loss 0.545107.
Train: 2018-07-31T11:26:31.184564: step 3765, loss 0.615482.
Train: 2018-07-31T11:26:31.340748: step 3766, loss 0.562687.
Train: 2018-07-31T11:26:31.512581: step 3767, loss 0.527459.
Train: 2018-07-31T11:26:31.684447: step 3768, loss 0.562694.
Train: 2018-07-31T11:26:31.840629: step 3769, loss 0.633268.
Train: 2018-07-31T11:26:32.012495: step 3770, loss 0.553879.
Test: 2018-07-31T11:26:32.496727: step 3770, loss 0.547938.
Train: 2018-07-31T11:26:32.652972: step 3771, loss 0.465816.
Train: 2018-07-31T11:26:32.840426: step 3772, loss 0.589142.
Train: 2018-07-31T11:26:32.996609: step 3773, loss 0.580334.
Train: 2018-07-31T11:26:33.168475: step 3774, loss 0.518606.
Train: 2018-07-31T11:26:33.324687: step 3775, loss 0.637986.
Train: 2018-07-31T11:26:33.480902: step 3776, loss 0.58913.
Train: 2018-07-31T11:26:33.652705: step 3777, loss 0.553881.
Train: 2018-07-31T11:26:33.824553: step 3778, loss 0.571458.
Train: 2018-07-31T11:26:33.980753: step 3779, loss 0.536344.
Train: 2018-07-31T11:26:34.152621: step 3780, loss 0.545129.
Test: 2018-07-31T11:26:34.621228: step 3780, loss 0.547985.
Train: 2018-07-31T11:26:34.793064: step 3781, loss 0.59769.
Train: 2018-07-31T11:26:34.964898: step 3782, loss 0.615129.
Train: 2018-07-31T11:26:35.136758: step 3783, loss 0.588816.
Train: 2018-07-31T11:26:35.308569: step 3784, loss 0.580031.
Train: 2018-07-31T11:26:35.464812: step 3785, loss 0.579972.
Train: 2018-07-31T11:26:35.636646: step 3786, loss 0.605864.
Train: 2018-07-31T11:26:35.792860: step 3787, loss 0.605699.
Train: 2018-07-31T11:26:35.964666: step 3788, loss 0.605515.
Train: 2018-07-31T11:26:36.136531: step 3789, loss 0.639482.
Train: 2018-07-31T11:26:36.308333: step 3790, loss 0.596581.
Test: 2018-07-31T11:26:36.776974: step 3790, loss 0.548494.
Train: 2018-07-31T11:26:36.948810: step 3791, loss 0.511984.
Train: 2018-07-31T11:26:37.105022: step 3792, loss 0.587863.
Train: 2018-07-31T11:26:37.276857: step 3793, loss 0.596146.
Train: 2018-07-31T11:26:37.448716: step 3794, loss 0.604353.
Train: 2018-07-31T11:26:37.620553: step 3795, loss 0.554437.
Train: 2018-07-31T11:26:37.792391: step 3796, loss 0.471922.
Train: 2018-07-31T11:26:37.979853: step 3797, loss 0.4968.
Train: 2018-07-31T11:26:38.136031: step 3798, loss 0.587516.
Train: 2018-07-31T11:26:38.292244: step 3799, loss 0.595765.
Train: 2018-07-31T11:26:38.464109: step 3800, loss 0.52979.
Test: 2018-07-31T11:26:38.932749: step 3800, loss 0.548952.
Train: 2018-07-31T11:26:39.651300: step 3801, loss 0.47203.
Train: 2018-07-31T11:26:39.823136: step 3802, loss 0.587556.
Train: 2018-07-31T11:26:39.979348: step 3803, loss 0.529582.
Train: 2018-07-31T11:26:40.151217: step 3804, loss 0.521165.
Train: 2018-07-31T11:26:40.307422: step 3805, loss 0.587701.
Train: 2018-07-31T11:26:40.479232: step 3806, loss 0.495785.
Train: 2018-07-31T11:26:40.651067: step 3807, loss 0.50389.
Train: 2018-07-31T11:26:40.822925: step 3808, loss 0.554192.
Train: 2018-07-31T11:26:40.979114: step 3809, loss 0.63042.
Train: 2018-07-31T11:26:41.150982: step 3810, loss 0.554106.
Test: 2018-07-31T11:26:41.619589: step 3810, loss 0.548326.
Train: 2018-07-31T11:26:41.791455: step 3811, loss 0.588171.
Train: 2018-07-31T11:26:41.947663: step 3812, loss 0.502815.
Train: 2018-07-31T11:26:42.119472: step 3813, loss 0.476931.
Train: 2018-07-31T11:26:42.291308: step 3814, loss 0.553991.
Train: 2018-07-31T11:26:42.447552: step 3815, loss 0.588526.
Train: 2018-07-31T11:26:42.619386: step 3816, loss 0.510591.
Train: 2018-07-31T11:26:42.791190: step 3817, loss 0.501696.
Train: 2018-07-31T11:26:42.947405: step 3818, loss 0.536405.
Train: 2018-07-31T11:26:43.119269: step 3819, loss 0.615362.
Train: 2018-07-31T11:26:43.291105: step 3820, loss 0.501006.
Test: 2018-07-31T11:26:43.775334: step 3820, loss 0.54789.
Train: 2018-07-31T11:26:43.931550: step 3821, loss 0.527328.
Train: 2018-07-31T11:26:44.103383: step 3822, loss 0.491708.
Train: 2018-07-31T11:26:44.275242: step 3823, loss 0.562771.
Train: 2018-07-31T11:26:44.447083: step 3824, loss 0.544898.
Train: 2018-07-31T11:26:44.618887: step 3825, loss 0.59881.
Train: 2018-07-31T11:26:44.790722: step 3826, loss 0.571891.
Train: 2018-07-31T11:26:44.946966: step 3827, loss 0.57193.
Train: 2018-07-31T11:26:45.118795: step 3828, loss 0.544835.
Train: 2018-07-31T11:26:45.275009: step 3829, loss 0.517673.
Train: 2018-07-31T11:26:45.446844: step 3830, loss 0.535752.
Test: 2018-07-31T11:26:45.915458: step 3830, loss 0.547766.
Train: 2018-07-31T11:26:46.087325: step 3831, loss 0.544811.
Train: 2018-07-31T11:26:46.259161: step 3832, loss 0.535705.
Train: 2018-07-31T11:26:46.430964: step 3833, loss 0.544797.
Train: 2018-07-31T11:26:46.587207: step 3834, loss 0.553922.
Train: 2018-07-31T11:26:46.759044: step 3835, loss 0.526502.
Train: 2018-07-31T11:26:46.915255: step 3836, loss 0.553941.
Train: 2018-07-31T11:26:47.087060: step 3837, loss 0.51727.
Train: 2018-07-31T11:26:47.243304: step 3838, loss 0.599892.
Train: 2018-07-31T11:26:47.415132: step 3839, loss 0.590732.
Train: 2018-07-31T11:26:47.586943: step 3840, loss 0.563153.
Test: 2018-07-31T11:26:48.055583: step 3840, loss 0.547763.
Train: 2018-07-31T11:26:48.227448: step 3841, loss 0.563143.
Train: 2018-07-31T11:26:48.383631: step 3842, loss 0.609009.
Train: 2018-07-31T11:26:48.555506: step 3843, loss 0.581416.
Train: 2018-07-31T11:26:48.727331: step 3844, loss 0.627025.
Train: 2018-07-31T11:26:48.883514: step 3845, loss 0.544798.
Train: 2018-07-31T11:26:49.055349: step 3846, loss 0.535734.
Train: 2018-07-31T11:26:49.227214: step 3847, loss 0.490504.
Train: 2018-07-31T11:26:49.399018: step 3848, loss 0.653334.
Train: 2018-07-31T11:26:49.555256: step 3849, loss 0.490758.
Train: 2018-07-31T11:26:49.727096: step 3850, loss 0.562857.
Test: 2018-07-31T11:26:50.195707: step 3850, loss 0.547789.
Train: 2018-07-31T11:26:50.367541: step 3851, loss 0.598779.
Train: 2018-07-31T11:26:50.539377: step 3852, loss 0.598667.
Train: 2018-07-31T11:26:50.711212: step 3853, loss 0.464476.
Train: 2018-07-31T11:26:50.867449: step 3854, loss 0.643117.
Train: 2018-07-31T11:26:51.039293: step 3855, loss 0.571649.
Train: 2018-07-31T11:26:51.211095: step 3856, loss 0.491688.
Train: 2018-07-31T11:26:51.382929: step 3857, loss 0.607052.
Train: 2018-07-31T11:26:51.554763: step 3858, loss 0.553846.
Train: 2018-07-31T11:26:51.711007: step 3859, loss 0.527353.
Train: 2018-07-31T11:26:51.882845: step 3860, loss 0.571496.
Test: 2018-07-31T11:26:52.351452: step 3860, loss 0.54791.
Train: 2018-07-31T11:26:52.523286: step 3861, loss 0.580285.
Train: 2018-07-31T11:26:52.695122: step 3862, loss 0.545064.
Train: 2018-07-31T11:26:52.851366: step 3863, loss 0.61535.
Train: 2018-07-31T11:26:53.023170: step 3864, loss 0.597689.
Train: 2018-07-31T11:26:53.179413: step 3865, loss 0.492714.
Train: 2018-07-31T11:26:53.351248: step 3866, loss 0.597523.
Train: 2018-07-31T11:26:53.523085: step 3867, loss 0.580028.
Train: 2018-07-31T11:26:53.694888: step 3868, loss 0.5626.
Train: 2018-07-31T11:26:53.866723: step 3869, loss 0.5279.
Train: 2018-07-31T11:26:54.022936: step 3870, loss 0.579918.
Test: 2018-07-31T11:26:54.491574: step 3870, loss 0.5481.
Train: 2018-07-31T11:26:54.663410: step 3871, loss 0.527985.
Train: 2018-07-31T11:26:54.835271: step 3872, loss 0.545296.
Train: 2018-07-31T11:26:54.991458: step 3873, loss 0.553943.
Train: 2018-07-31T11:26:55.163293: step 3874, loss 0.562585.
Train: 2018-07-31T11:26:55.335128: step 3875, loss 0.657638.
Train: 2018-07-31T11:26:55.491375: step 3876, loss 0.545335.
Train: 2018-07-31T11:26:55.663177: step 3877, loss 0.493702.
Train: 2018-07-31T11:26:55.835035: step 3878, loss 0.571189.
Train: 2018-07-31T11:26:55.991225: step 3879, loss 0.622836.
Train: 2018-07-31T11:26:56.163090: step 3880, loss 0.631344.
Test: 2018-07-31T11:26:56.631700: step 3880, loss 0.548221.
Train: 2018-07-31T11:26:56.803567: step 3881, loss 0.468282.
Train: 2018-07-31T11:26:56.975399: step 3882, loss 0.528305.
Train: 2018-07-31T11:26:57.147236: step 3883, loss 0.596859.
Train: 2018-07-31T11:26:57.303442: step 3884, loss 0.511177.
Train: 2018-07-31T11:26:57.475252: step 3885, loss 0.53686.
Train: 2018-07-31T11:26:57.647119: step 3886, loss 0.528248.
Train: 2018-07-31T11:26:57.818952: step 3887, loss 0.536786.
Train: 2018-07-31T11:26:57.975165: step 3888, loss 0.605651.
Train: 2018-07-31T11:26:58.146970: step 3889, loss 0.622939.
Train: 2018-07-31T11:26:58.318835: step 3890, loss 0.631534.
Test: 2018-07-31T11:26:58.803065: step 3890, loss 0.548166.
Train: 2018-07-31T11:26:58.974931: step 3891, loss 0.622804.
Train: 2018-07-31T11:26:59.146760: step 3892, loss 0.596891.
Train: 2018-07-31T11:26:59.318600: step 3893, loss 0.630982.
Train: 2018-07-31T11:26:59.490406: step 3894, loss 0.528528.
Train: 2018-07-31T11:26:59.662270: step 3895, loss 0.588044.
Train: 2018-07-31T11:26:59.834106: step 3896, loss 0.562596.
Train: 2018-07-31T11:26:59.990287: step 3897, loss 0.562607.
Train: 2018-07-31T11:27:00.162122: step 3898, loss 0.562617.
Train: 2018-07-31T11:27:00.333958: step 3899, loss 0.612967.
Train: 2018-07-31T11:27:00.490170: step 3900, loss 0.512457.
Test: 2018-07-31T11:27:00.974464: step 3900, loss 0.548665.
Train: 2018-07-31T11:27:01.677423: step 3901, loss 0.5543.
Train: 2018-07-31T11:27:01.849228: step 3902, loss 0.50426.
Train: 2018-07-31T11:27:02.021086: step 3903, loss 0.50423.
Train: 2018-07-31T11:27:02.177308: step 3904, loss 0.529198.
Train: 2018-07-31T11:27:02.349110: step 3905, loss 0.604542.
Train: 2018-07-31T11:27:02.520944: step 3906, loss 0.579412.
Train: 2018-07-31T11:27:02.692780: step 3907, loss 0.571021.
Train: 2018-07-31T11:27:02.848993: step 3908, loss 0.545794.
Train: 2018-07-31T11:27:03.020829: step 3909, loss 0.60471.
Train: 2018-07-31T11:27:03.192663: step 3910, loss 0.545762.
Test: 2018-07-31T11:27:03.661303: step 3910, loss 0.548489.
Train: 2018-07-31T11:27:03.833137: step 3911, loss 0.689032.
Train: 2018-07-31T11:27:03.989351: step 3912, loss 0.587847.
Train: 2018-07-31T11:27:04.161219: step 3913, loss 0.545837.
Train: 2018-07-31T11:27:04.333020: step 3914, loss 0.587768.
Train: 2018-07-31T11:27:04.489234: step 3915, loss 0.504093.
Train: 2018-07-31T11:27:04.661069: step 3916, loss 0.545918.
Train: 2018-07-31T11:27:04.832934: step 3917, loss 0.529185.
Train: 2018-07-31T11:27:05.004771: step 3918, loss 0.529143.
Train: 2018-07-31T11:27:05.176573: step 3919, loss 0.50391.
Train: 2018-07-31T11:27:05.348439: step 3920, loss 0.722456.
Test: 2018-07-31T11:27:05.832669: step 3920, loss 0.548532.
Train: 2018-07-31T11:27:06.004529: step 3921, loss 0.57102.
Train: 2018-07-31T11:27:06.160748: step 3922, loss 0.587821.
Train: 2018-07-31T11:27:06.332554: step 3923, loss 0.604585.
Train: 2018-07-31T11:27:06.504388: step 3924, loss 0.487224.
Train: 2018-07-31T11:27:06.660631: step 3925, loss 0.529109.
Train: 2018-07-31T11:27:06.816815: step 3926, loss 0.562622.
Train: 2018-07-31T11:27:06.988674: step 3927, loss 0.604599.
Train: 2018-07-31T11:27:07.160514: step 3928, loss 0.545822.
Train: 2018-07-31T11:27:07.332348: step 3929, loss 0.571015.
Train: 2018-07-31T11:27:07.488533: step 3930, loss 0.621447.
Test: 2018-07-31T11:27:07.972824: step 3930, loss 0.54855.
Train: 2018-07-31T11:27:08.144629: step 3931, loss 0.53742.
Train: 2018-07-31T11:27:08.316464: step 3932, loss 0.638194.
Train: 2018-07-31T11:27:08.472707: step 3933, loss 0.579391.
Train: 2018-07-31T11:27:08.644544: step 3934, loss 0.571001.
Train: 2018-07-31T11:27:08.816346: step 3935, loss 0.512499.
Train: 2018-07-31T11:27:08.972584: step 3936, loss 0.637835.
Train: 2018-07-31T11:27:09.144395: step 3937, loss 0.562651.
Train: 2018-07-31T11:27:09.316253: step 3938, loss 0.562659.
Train: 2018-07-31T11:27:09.488065: step 3939, loss 0.570986.
Train: 2018-07-31T11:27:09.644310: step 3940, loss 0.487876.
Test: 2018-07-31T11:27:10.112917: step 3940, loss 0.548737.
Train: 2018-07-31T11:27:10.284783: step 3941, loss 0.537714.
Train: 2018-07-31T11:27:10.456588: step 3942, loss 0.487687.
Train: 2018-07-31T11:27:10.612800: step 3943, loss 0.562639.
Train: 2018-07-31T11:27:10.784665: step 3944, loss 0.587763.
Train: 2018-07-31T11:27:10.956500: step 3945, loss 0.545816.
Train: 2018-07-31T11:27:11.128337: step 3946, loss 0.604693.
Train: 2018-07-31T11:27:11.300165: step 3947, loss 0.587885.
Train: 2018-07-31T11:27:11.471999: step 3948, loss 0.571028.
Train: 2018-07-31T11:27:11.628188: step 3949, loss 0.511944.
Train: 2018-07-31T11:27:11.800053: step 3950, loss 0.596405.
Test: 2018-07-31T11:27:12.268695: step 3950, loss 0.548408.
Train: 2018-07-31T11:27:12.440497: step 3951, loss 0.554119.
Train: 2018-07-31T11:27:12.612357: step 3952, loss 0.630346.
Train: 2018-07-31T11:27:12.784168: step 3953, loss 0.520243.
Train: 2018-07-31T11:27:12.940405: step 3954, loss 0.587992.
Train: 2018-07-31T11:27:13.112245: step 3955, loss 0.571049.
Train: 2018-07-31T11:27:13.284081: step 3956, loss 0.511752.
Train: 2018-07-31T11:27:13.440263: step 3957, loss 0.511696.
Train: 2018-07-31T11:27:13.612129: step 3958, loss 0.605054.
Train: 2018-07-31T11:27:13.783935: step 3959, loss 0.588083.
Train: 2018-07-31T11:27:13.940176: step 3960, loss 0.511512.
Test: 2018-07-31T11:27:14.424407: step 3960, loss 0.548291.
Train: 2018-07-31T11:27:14.580646: step 3961, loss 0.579606.
Train: 2018-07-31T11:27:14.752457: step 3962, loss 0.596682.
Train: 2018-07-31T11:27:14.924291: step 3963, loss 0.571093.
Train: 2018-07-31T11:27:15.080534: step 3964, loss 0.545496.
Train: 2018-07-31T11:27:15.252372: step 3965, loss 0.588167.
Train: 2018-07-31T11:27:15.424174: step 3966, loss 0.46868.
Train: 2018-07-31T11:27:15.596008: step 3967, loss 0.528356.
Train: 2018-07-31T11:27:15.752221: step 3968, loss 0.519702.
Train: 2018-07-31T11:27:15.924087: step 3969, loss 0.579751.
Train: 2018-07-31T11:27:16.095922: step 3970, loss 0.579793.
Test: 2018-07-31T11:27:16.564561: step 3970, loss 0.548103.
Train: 2018-07-31T11:27:16.736397: step 3971, loss 0.605723.
Train: 2018-07-31T11:27:16.908232: step 3972, loss 0.536646.
Train: 2018-07-31T11:27:17.064415: step 3973, loss 0.519324.
Train: 2018-07-31T11:27:17.236280: step 3974, loss 0.579893.
Train: 2018-07-31T11:27:17.408084: step 3975, loss 0.536546.
Train: 2018-07-31T11:27:17.564328: step 3976, loss 0.545197.
Train: 2018-07-31T11:27:17.736163: step 3977, loss 0.588687.
Train: 2018-07-31T11:27:17.892347: step 3978, loss 0.640984.
Train: 2018-07-31T11:27:18.064182: step 3979, loss 0.510351.
Train: 2018-07-31T11:27:18.236015: step 3980, loss 0.562579.
Test: 2018-07-31T11:27:18.720276: step 3980, loss 0.547995.
Train: 2018-07-31T11:27:18.892144: step 3981, loss 0.527746.
Train: 2018-07-31T11:27:19.048326: step 3982, loss 0.588728.
Train: 2018-07-31T11:27:19.220190: step 3983, loss 0.562582.
Train: 2018-07-31T11:27:19.376374: step 3984, loss 0.553865.
Train: 2018-07-31T11:27:19.548238: step 3985, loss 0.623607.
Train: 2018-07-31T11:27:19.720043: step 3986, loss 0.606115.
Train: 2018-07-31T11:27:19.891902: step 3987, loss 0.588641.
Train: 2018-07-31T11:27:20.063742: step 3988, loss 0.510551.
Train: 2018-07-31T11:27:20.235578: step 3989, loss 0.562562.
Train: 2018-07-31T11:27:20.407413: step 3990, loss 0.579858.
Test: 2018-07-31T11:27:20.876022: step 3990, loss 0.548091.
Train: 2018-07-31T11:27:21.032262: step 3991, loss 0.519369.
Train: 2018-07-31T11:27:21.204096: step 3992, loss 0.519384.
Train: 2018-07-31T11:27:21.375904: step 3993, loss 0.614388.
Train: 2018-07-31T11:27:21.547740: step 3994, loss 0.605721.
Train: 2018-07-31T11:27:21.719575: step 3995, loss 0.597034.
Train: 2018-07-31T11:27:21.891441: step 3996, loss 0.622768.
Train: 2018-07-31T11:27:22.047623: step 3997, loss 0.553974.
Train: 2018-07-31T11:27:22.219488: step 3998, loss 0.545445.
Train: 2018-07-31T11:27:22.375701: step 3999, loss 0.545481.
Train: 2018-07-31T11:27:22.547505: step 4000, loss 0.579601.
Test: 2018-07-31T11:27:23.016178: step 4000, loss 0.548305.
Train: 2018-07-31T11:27:23.719136: step 4001, loss 0.622124.
Train: 2018-07-31T11:27:23.875320: step 4002, loss 0.545588.
Train: 2018-07-31T11:27:24.047154: step 4003, loss 0.613385.
Train: 2018-07-31T11:27:24.218990: step 4004, loss 0.469673.
Train: 2018-07-31T11:27:24.390823: step 4005, loss 0.520369.
Train: 2018-07-31T11:27:24.547037: step 4006, loss 0.545683.
Train: 2018-07-31T11:27:24.718873: step 4007, loss 0.587933.
Train: 2018-07-31T11:27:24.890707: step 4008, loss 0.647139.
Train: 2018-07-31T11:27:25.046951: step 4009, loss 0.55413.
Train: 2018-07-31T11:27:25.218780: step 4010, loss 0.562578.
Test: 2018-07-31T11:27:25.687426: step 4010, loss 0.548465.
Train: 2018-07-31T11:27:25.859260: step 4011, loss 0.545723.
Train: 2018-07-31T11:27:26.031095: step 4012, loss 0.528877.
Train: 2018-07-31T11:27:26.187279: step 4013, loss 0.545718.
Train: 2018-07-31T11:27:26.359144: step 4014, loss 0.596331.
Train: 2018-07-31T11:27:26.530978: step 4015, loss 0.613217.
Train: 2018-07-31T11:27:26.702783: step 4016, loss 0.587878.
Train: 2018-07-31T11:27:26.859027: step 4017, loss 0.478356.
Train: 2018-07-31T11:27:27.030830: step 4018, loss 0.587868.
Train: 2018-07-31T11:27:27.187044: step 4019, loss 0.579442.
Train: 2018-07-31T11:27:27.358879: step 4020, loss 0.613166.
Test: 2018-07-31T11:27:27.827549: step 4020, loss 0.548477.
Train: 2018-07-31T11:27:27.999384: step 4021, loss 0.621539.
Train: 2018-07-31T11:27:28.171219: step 4022, loss 0.50376.
Train: 2018-07-31T11:27:28.343023: step 4023, loss 0.537394.
Train: 2018-07-31T11:27:28.514891: step 4024, loss 0.579395.
Train: 2018-07-31T11:27:28.671102: step 4025, loss 0.562593.
Train: 2018-07-31T11:27:28.842907: step 4026, loss 0.528993.
Train: 2018-07-31T11:27:29.014774: step 4027, loss 0.537369.
Train: 2018-07-31T11:27:29.186607: step 4028, loss 0.495236.
Train: 2018-07-31T11:27:29.342790: step 4029, loss 0.579455.
Train: 2018-07-31T11:27:29.514648: step 4030, loss 0.554104.
Test: 2018-07-31T11:27:29.983294: step 4030, loss 0.548357.
Train: 2018-07-31T11:27:30.155100: step 4031, loss 0.503204.
Train: 2018-07-31T11:27:30.326935: step 4032, loss 0.588072.
Train: 2018-07-31T11:27:30.483148: step 4033, loss 0.485791.
Train: 2018-07-31T11:27:30.654982: step 4034, loss 0.605349.
Train: 2018-07-31T11:27:30.826817: step 4035, loss 0.545375.
Train: 2018-07-31T11:27:30.998652: step 4036, loss 0.528118.
Train: 2018-07-31T11:27:31.154865: step 4037, loss 0.60571.
Train: 2018-07-31T11:27:31.326730: step 4038, loss 0.527954.
Train: 2018-07-31T11:27:31.498536: step 4039, loss 0.588561.
Train: 2018-07-31T11:27:31.670399: step 4040, loss 0.579922.
Test: 2018-07-31T11:27:32.139041: step 4040, loss 0.548005.
Train: 2018-07-31T11:27:32.310844: step 4041, loss 0.614704.
Train: 2018-07-31T11:27:32.482679: step 4042, loss 0.606001.
Train: 2018-07-31T11:27:32.638893: step 4043, loss 0.519164.
Train: 2018-07-31T11:27:32.810757: step 4044, loss 0.519173.
Train: 2018-07-31T11:27:32.966971: step 4045, loss 0.605965.
Train: 2018-07-31T11:27:33.138808: step 4046, loss 0.614626.
Train: 2018-07-31T11:27:33.310640: step 4047, loss 0.597215.
Train: 2018-07-31T11:27:33.466823: step 4048, loss 0.553899.
Train: 2018-07-31T11:27:33.638691: step 4049, loss 0.52801.
Train: 2018-07-31T11:27:33.810494: step 4050, loss 0.502162.
Test: 2018-07-31T11:27:34.279133: step 4050, loss 0.54809.
Train: 2018-07-31T11:27:34.466590: step 4051, loss 0.579802.
Train: 2018-07-31T11:27:34.638424: step 4052, loss 0.579802.
Train: 2018-07-31T11:27:34.794638: step 4053, loss 0.6143.
Train: 2018-07-31T11:27:34.966473: step 4054, loss 0.622838.
Train: 2018-07-31T11:27:35.122710: step 4055, loss 0.519578.
Train: 2018-07-31T11:27:35.294556: step 4056, loss 0.476739.
Train: 2018-07-31T11:27:35.450733: step 4057, loss 0.545369.
Train: 2018-07-31T11:27:35.622595: step 4058, loss 0.588311.
Train: 2018-07-31T11:27:35.794434: step 4059, loss 0.562537.
Train: 2018-07-31T11:27:35.966238: step 4060, loss 0.510973.
Test: 2018-07-31T11:27:36.434908: step 4060, loss 0.548127.
Train: 2018-07-31T11:27:36.606743: step 4061, loss 0.510912.
Train: 2018-07-31T11:27:36.762960: step 4062, loss 0.605653.
Train: 2018-07-31T11:27:36.934761: step 4063, loss 0.528015.
Train: 2018-07-31T11:27:37.106596: step 4064, loss 0.519318.
Train: 2018-07-31T11:27:37.262809: step 4065, loss 0.657858.
Train: 2018-07-31T11:27:37.434675: step 4066, loss 0.527887.
Train: 2018-07-31T11:27:37.606478: step 4067, loss 0.562549.
Train: 2018-07-31T11:27:37.778315: step 4068, loss 0.545198.
Train: 2018-07-31T11:27:37.950149: step 4069, loss 0.545185.
Train: 2018-07-31T11:27:38.122009: step 4070, loss 0.501709.
Test: 2018-07-31T11:27:38.606244: step 4070, loss 0.547973.
Train: 2018-07-31T11:27:38.778079: step 4071, loss 0.53643.
Train: 2018-07-31T11:27:38.934292: step 4072, loss 0.571301.
Train: 2018-07-31T11:27:39.106158: step 4073, loss 0.52759.
Train: 2018-07-31T11:27:39.277988: step 4074, loss 0.509987.
Train: 2018-07-31T11:27:39.434177: step 4075, loss 0.536225.
Train: 2018-07-31T11:27:39.606041: step 4076, loss 0.562623.
Train: 2018-07-31T11:27:39.762256: step 4077, loss 0.638105.
Train: 2018-07-31T11:27:39.918469: step 4078, loss 0.536096.
Train: 2018-07-31T11:27:40.090273: step 4079, loss 0.527216.
Train: 2018-07-31T11:27:40.262107: step 4080, loss 0.607028.
Test: 2018-07-31T11:27:40.730746: step 4080, loss 0.547803.
Train: 2018-07-31T11:27:40.902582: step 4081, loss 0.553792.
Train: 2018-07-31T11:27:41.074443: step 4082, loss 0.580426.
Train: 2018-07-31T11:27:41.230661: step 4083, loss 0.509411.
Train: 2018-07-31T11:27:41.402466: step 4084, loss 0.544909.
Train: 2018-07-31T11:27:41.574299: step 4085, loss 0.571569.
Train: 2018-07-31T11:27:41.730513: step 4086, loss 0.527112.
Train: 2018-07-31T11:27:41.902349: step 4087, loss 0.589391.
Train: 2018-07-31T11:27:42.074182: step 4088, loss 0.589394.
Train: 2018-07-31T11:27:42.246017: step 4089, loss 0.580477.
Train: 2018-07-31T11:27:42.417852: step 4090, loss 0.65154.
Test: 2018-07-31T11:27:42.886492: step 4090, loss 0.547813.
Train: 2018-07-31T11:27:43.042732: step 4091, loss 0.518347.
Train: 2018-07-31T11:27:43.230198: step 4092, loss 0.474191.
Train: 2018-07-31T11:27:43.401997: step 4093, loss 0.580327.
Train: 2018-07-31T11:27:43.558210: step 4094, loss 0.651028.
Train: 2018-07-31T11:27:43.730075: step 4095, loss 0.527344.
Train: 2018-07-31T11:27:43.886291: step 4096, loss 0.536194.
Train: 2018-07-31T11:27:44.058118: step 4097, loss 0.58899.
Train: 2018-07-31T11:27:44.229953: step 4098, loss 0.641634.
Train: 2018-07-31T11:27:44.386141: step 4099, loss 0.615109.
Train: 2018-07-31T11:27:44.557976: step 4100, loss 0.56256.
Test: 2018-07-31T11:27:45.026647: step 4100, loss 0.547995.
Train: 2018-07-31T11:27:45.729607: step 4101, loss 0.597307.
Train: 2018-07-31T11:27:45.901442: step 4102, loss 0.597157.
Train: 2018-07-31T11:27:46.073277: step 4103, loss 0.493596.
Train: 2018-07-31T11:27:46.245111: step 4104, loss 0.562529.
Train: 2018-07-31T11:27:46.401295: step 4105, loss 0.468206.
Train: 2018-07-31T11:27:46.573130: step 4106, loss 0.605396.
Train: 2018-07-31T11:27:46.744989: step 4107, loss 0.519703.
Train: 2018-07-31T11:27:46.901177: step 4108, loss 0.562528.
Train: 2018-07-31T11:27:47.073044: step 4109, loss 0.596787.
Train: 2018-07-31T11:27:47.244848: step 4110, loss 0.588204.
Test: 2018-07-31T11:27:47.713517: step 4110, loss 0.548211.
Train: 2018-07-31T11:27:47.869731: step 4111, loss 0.596724.
Train: 2018-07-31T11:27:48.041567: step 4112, loss 0.613734.
Train: 2018-07-31T11:27:48.213396: step 4113, loss 0.54551.
Train: 2018-07-31T11:27:48.385206: step 4114, loss 0.622002.
Train: 2018-07-31T11:27:48.557040: step 4115, loss 0.571014.
Train: 2018-07-31T11:27:48.728874: step 4116, loss 0.58789.
Train: 2018-07-31T11:27:48.885120: step 4117, loss 0.604666.
Train: 2018-07-31T11:27:49.056954: step 4118, loss 0.520628.
Train: 2018-07-31T11:27:49.213137: step 4119, loss 0.470498.
Train: 2018-07-31T11:27:49.385004: step 4120, loss 0.56259.
Test: 2018-07-31T11:27:49.869262: step 4120, loss 0.548561.
Train: 2018-07-31T11:27:50.025476: step 4121, loss 0.57934.
Train: 2018-07-31T11:27:50.197282: step 4122, loss 0.512336.
Train: 2018-07-31T11:27:50.369116: step 4123, loss 0.604509.
Train: 2018-07-31T11:27:50.540981: step 4124, loss 0.562581.
Train: 2018-07-31T11:27:50.697188: step 4125, loss 0.545798.
Train: 2018-07-31T11:27:50.869028: step 4126, loss 0.562575.
Train: 2018-07-31T11:27:51.040834: step 4127, loss 0.545763.
Train: 2018-07-31T11:27:51.212698: step 4128, loss 0.503668.
Train: 2018-07-31T11:27:51.368881: step 4129, loss 0.537253.
Train: 2018-07-31T11:27:51.540747: step 4130, loss 0.621752.
Test: 2018-07-31T11:27:52.025008: step 4130, loss 0.548358.
Train: 2018-07-31T11:27:52.212433: step 4131, loss 0.562542.
Train: 2018-07-31T11:27:52.415543: step 4132, loss 0.537099.
Train: 2018-07-31T11:27:52.602994: step 4133, loss 0.588017.
Train: 2018-07-31T11:27:52.806044: step 4134, loss 0.588041.
Train: 2018-07-31T11:27:52.993530: step 4135, loss 0.588052.
Train: 2018-07-31T11:27:53.180957: step 4136, loss 0.605062.
Train: 2018-07-31T11:27:53.337200: step 4137, loss 0.511543.
Train: 2018-07-31T11:27:53.509004: step 4138, loss 0.54553.
Train: 2018-07-31T11:27:53.680870: step 4139, loss 0.554024.
Train: 2018-07-31T11:27:53.852675: step 4140, loss 0.571041.
Test: 2018-07-31T11:27:54.321345: step 4140, loss 0.548262.
Train: 2018-07-31T11:27:54.493180: step 4141, loss 0.596598.
Train: 2018-07-31T11:27:54.665009: step 4142, loss 0.596593.
Train: 2018-07-31T11:27:54.821228: step 4143, loss 0.554019.
Train: 2018-07-31T11:27:54.993062: step 4144, loss 0.528511.
Train: 2018-07-31T11:27:55.164868: step 4145, loss 0.579543.
Train: 2018-07-31T11:27:55.336732: step 4146, loss 0.545515.
Train: 2018-07-31T11:27:55.508536: step 4147, loss 0.468924.
Train: 2018-07-31T11:27:55.664781: step 4148, loss 0.562524.
Train: 2018-07-31T11:27:55.852237: step 4149, loss 0.502691.
Train: 2018-07-31T11:27:56.008419: step 4150, loss 0.553946.
Test: 2018-07-31T11:27:56.477059: step 4150, loss 0.548117.
Train: 2018-07-31T11:27:56.648895: step 4151, loss 0.588321.
Train: 2018-07-31T11:27:56.820729: step 4152, loss 0.571141.
Train: 2018-07-31T11:27:56.992565: step 4153, loss 0.588426.
Train: 2018-07-31T11:27:57.164399: step 4154, loss 0.597097.
Train: 2018-07-31T11:27:57.336234: step 4155, loss 0.545237.
Train: 2018-07-31T11:27:57.492447: step 4156, loss 0.614415.
Train: 2018-07-31T11:27:57.664282: step 4157, loss 0.536598.
Train: 2018-07-31T11:27:57.820496: step 4158, loss 0.562525.
Train: 2018-07-31T11:27:57.992330: step 4159, loss 0.553884.
Train: 2018-07-31T11:27:58.164196: step 4160, loss 0.588444.
Test: 2018-07-31T11:27:58.632838: step 4160, loss 0.548061.
Train: 2018-07-31T11:27:58.789049: step 4161, loss 0.519346.
Train: 2018-07-31T11:27:58.960854: step 4162, loss 0.519331.
Train: 2018-07-31T11:27:59.132689: step 4163, loss 0.519282.
Train: 2018-07-31T11:27:59.288931: step 4164, loss 0.562529.
Train: 2018-07-31T11:27:59.460737: step 4165, loss 0.536494.
Train: 2018-07-31T11:27:59.632570: step 4166, loss 0.571236.
Train: 2018-07-31T11:27:59.788784: step 4167, loss 0.606095.
Train: 2018-07-31T11:27:59.960644: step 4168, loss 0.579973.
Train: 2018-07-31T11:28:00.132487: step 4169, loss 0.632254.
Train: 2018-07-31T11:28:00.288667: step 4170, loss 0.553839.
Test: 2018-07-31T11:28:00.772959: step 4170, loss 0.547983.
Train: 2018-07-31T11:28:00.929143: step 4171, loss 0.484331.
Train: 2018-07-31T11:28:01.101010: step 4172, loss 0.649464.
Train: 2018-07-31T11:28:01.272812: step 4173, loss 0.510452.
Train: 2018-07-31T11:28:01.444648: step 4174, loss 0.50179.
Train: 2018-07-31T11:28:01.600891: step 4175, loss 0.562533.
Train: 2018-07-31T11:28:01.772726: step 4176, loss 0.553844.
Train: 2018-07-31T11:28:01.944530: step 4177, loss 0.484253.
Train: 2018-07-31T11:28:02.100774: step 4178, loss 0.562545.
Train: 2018-07-31T11:28:02.272579: step 4179, loss 0.588756.
Train: 2018-07-31T11:28:02.444437: step 4180, loss 0.580046.
Test: 2018-07-31T11:28:02.913084: step 4180, loss 0.547906.
Train: 2018-07-31T11:28:03.084887: step 4181, loss 0.588807.
Train: 2018-07-31T11:28:03.256755: step 4182, loss 0.527565.
Train: 2018-07-31T11:28:03.412966: step 4183, loss 0.606325.
Train: 2018-07-31T11:28:03.584772: step 4184, loss 0.55381.
Train: 2018-07-31T11:28:03.756606: step 4185, loss 0.518831.
Train: 2018-07-31T11:28:03.928440: step 4186, loss 0.457569.
Train: 2018-07-31T11:28:04.100280: step 4187, loss 0.623962.
Train: 2018-07-31T11:28:04.256513: step 4188, loss 0.588907.
Train: 2018-07-31T11:28:04.428323: step 4189, loss 0.545017.
Train: 2018-07-31T11:28:04.600158: step 4190, loss 0.457193.
Test: 2018-07-31T11:28:05.068800: step 4190, loss 0.547848.
Train: 2018-07-31T11:28:05.240632: step 4191, loss 0.553787.
Train: 2018-07-31T11:28:05.396877: step 4192, loss 0.500856.
Train: 2018-07-31T11:28:05.568682: step 4193, loss 0.482996.
Train: 2018-07-31T11:28:05.724894: step 4194, loss 0.571539.
Train: 2018-07-31T11:28:05.896730: step 4195, loss 0.544857.
Train: 2018-07-31T11:28:06.068565: step 4196, loss 0.607422.
Train: 2018-07-31T11:28:06.240432: step 4197, loss 0.562732.
Train: 2018-07-31T11:28:06.396643: step 4198, loss 0.589658.
Train: 2018-07-31T11:28:06.568477: step 4199, loss 0.562752.
Train: 2018-07-31T11:28:06.740283: step 4200, loss 0.58969.
Test: 2018-07-31T11:28:07.208922: step 4200, loss 0.54772.
Train: 2018-07-31T11:28:07.939076: step 4201, loss 0.589673.
Train: 2018-07-31T11:28:08.126530: step 4202, loss 0.580666.
Train: 2018-07-31T11:28:08.298334: step 4203, loss 0.589572.
Train: 2018-07-31T11:28:08.470170: step 4204, loss 0.509113.
Train: 2018-07-31T11:28:08.626413: step 4205, loss 0.535925.
Train: 2018-07-31T11:28:08.798247: step 4206, loss 0.607266.
Train: 2018-07-31T11:28:08.954464: step 4207, loss 0.527062.
Train: 2018-07-31T11:28:09.126290: step 4208, loss 0.633813.
Train: 2018-07-31T11:28:09.298100: step 4209, loss 0.642498.
Train: 2018-07-31T11:28:09.469934: step 4210, loss 0.562613.
Test: 2018-07-31T11:28:09.938575: step 4210, loss 0.54784.
Train: 2018-07-31T11:28:10.110440: step 4211, loss 0.483329.
Train: 2018-07-31T11:28:10.282244: step 4212, loss 0.641695.
Train: 2018-07-31T11:28:10.454079: step 4213, loss 0.509991.
Train: 2018-07-31T11:28:10.625939: step 4214, loss 0.53632.
Train: 2018-07-31T11:28:10.782158: step 4215, loss 0.545082.
Train: 2018-07-31T11:28:10.953996: step 4216, loss 0.579986.
Train: 2018-07-31T11:28:11.125798: step 4217, loss 0.536403.
Train: 2018-07-31T11:28:11.297658: step 4218, loss 0.510306.
Train: 2018-07-31T11:28:11.469497: step 4219, loss 0.562534.
Train: 2018-07-31T11:28:11.625681: step 4220, loss 0.649627.
Test: 2018-07-31T11:28:12.109968: step 4220, loss 0.547967.
Train: 2018-07-31T11:28:12.297432: step 4221, loss 0.571224.
Train: 2018-07-31T11:28:12.484886: step 4222, loss 0.519123.
Train: 2018-07-31T11:28:12.687932: step 4223, loss 0.501804.
Train: 2018-07-31T11:28:12.891041: step 4224, loss 0.571202.
Train: 2018-07-31T11:28:13.078465: step 4225, loss 0.571204.
Train: 2018-07-31T11:28:13.250299: step 4226, loss 0.605927.
Train: 2018-07-31T11:28:13.422135: step 4227, loss 0.623229.
Train: 2018-07-31T11:28:13.578382: step 4228, loss 0.580978.
Train: 2018-07-31T11:28:13.750183: step 4229, loss 0.579779.
Train: 2018-07-31T11:28:13.922017: step 4230, loss 0.579733.
Test: 2018-07-31T11:28:14.390687: step 4230, loss 0.548121.
Train: 2018-07-31T11:28:14.562521: step 4231, loss 0.485201.
Train: 2018-07-31T11:28:14.718707: step 4232, loss 0.519592.
Train: 2018-07-31T11:28:14.906189: step 4233, loss 0.519583.
Train: 2018-07-31T11:28:15.062405: step 4234, loss 0.528132.
Train: 2018-07-31T11:28:15.234211: step 4235, loss 0.614155.
Train: 2018-07-31T11:28:15.406075: step 4236, loss 0.510839.
Train: 2018-07-31T11:28:15.562283: step 4237, loss 0.61425.
Train: 2018-07-31T11:28:15.734118: step 4238, loss 0.605635.
Train: 2018-07-31T11:28:15.905952: step 4239, loss 0.510795.
Train: 2018-07-31T11:28:16.062174: step 4240, loss 0.502154.
Test: 2018-07-31T11:28:16.530811: step 4240, loss 0.548048.
Train: 2018-07-31T11:28:16.702616: step 4241, loss 0.553874.
Train: 2018-07-31T11:28:16.874452: step 4242, loss 0.571161.
Train: 2018-07-31T11:28:17.030665: step 4243, loss 0.536538.
Train: 2018-07-31T11:28:17.202499: step 4244, loss 0.562518.
Train: 2018-07-31T11:28:17.358713: step 4245, loss 0.519103.
Train: 2018-07-31T11:28:17.530578: step 4246, loss 0.562527.
Train: 2018-07-31T11:28:17.702381: step 4247, loss 0.527667.
Train: 2018-07-31T11:28:17.858627: step 4248, loss 0.571277.
Train: 2018-07-31T11:28:18.030430: step 4249, loss 0.615053.
Train: 2018-07-31T11:28:18.202266: step 4250, loss 0.606321.
Test: 2018-07-31T11:28:18.670906: step 4250, loss 0.547896.
Train: 2018-07-31T11:28:18.889634: step 4251, loss 0.588794.
Train: 2018-07-31T11:28:19.061470: step 4252, loss 0.58002.
Train: 2018-07-31T11:28:19.233273: step 4253, loss 0.571262.
Train: 2018-07-31T11:28:19.389518: step 4254, loss 0.632223.
Train: 2018-07-31T11:28:19.561321: step 4255, loss 0.510404.
Train: 2018-07-31T11:28:19.733195: step 4256, loss 0.57986.
Train: 2018-07-31T11:28:19.905021: step 4257, loss 0.536545.
Train: 2018-07-31T11:28:20.076827: step 4258, loss 0.553864.
Train: 2018-07-31T11:28:20.233070: step 4259, loss 0.545233.
Train: 2018-07-31T11:28:20.404904: step 4260, loss 0.614301.
Test: 2018-07-31T11:28:20.873513: step 4260, loss 0.54807.
Train: 2018-07-31T11:28:21.045379: step 4261, loss 0.493553.
Train: 2018-07-31T11:28:21.217185: step 4262, loss 0.605598.
Train: 2018-07-31T11:28:21.389019: step 4263, loss 0.502227.
Train: 2018-07-31T11:28:21.545232: step 4264, loss 0.579732.
Train: 2018-07-31T11:28:21.717067: step 4265, loss 0.571117.
Train: 2018-07-31T11:28:21.873310: step 4266, loss 0.596952.
Train: 2018-07-31T11:28:22.045145: step 4267, loss 0.571106.
Train: 2018-07-31T11:28:22.201362: step 4268, loss 0.528119.
Train: 2018-07-31T11:28:22.373194: step 4269, loss 0.614062.
Train: 2018-07-31T11:28:22.529407: step 4270, loss 0.596831.
Test: 2018-07-31T11:28:23.013669: step 4270, loss 0.548154.
Train: 2018-07-31T11:28:23.169881: step 4271, loss 0.5625.
Train: 2018-07-31T11:28:23.341716: step 4272, loss 0.562501.
Train: 2018-07-31T11:28:23.513546: step 4273, loss 0.511267.
Train: 2018-07-31T11:28:23.685381: step 4274, loss 0.579576.
Train: 2018-07-31T11:28:23.841569: step 4275, loss 0.596632.
Train: 2018-07-31T11:28:24.013429: step 4276, loss 0.511373.
Train: 2018-07-31T11:28:24.185264: step 4277, loss 0.588071.
Train: 2018-07-31T11:28:24.341477: step 4278, loss 0.562505.
Train: 2018-07-31T11:28:24.513287: step 4279, loss 0.639137.
Train: 2018-07-31T11:28:24.685122: step 4280, loss 0.554011.
Test: 2018-07-31T11:28:25.153761: step 4280, loss 0.548302.
Train: 2018-07-31T11:28:25.325631: step 4281, loss 0.570997.
Train: 2018-07-31T11:28:25.481835: step 4282, loss 0.570988.
Train: 2018-07-31T11:28:25.653646: step 4283, loss 0.57098.
Train: 2018-07-31T11:28:25.825514: step 4284, loss 0.494947.
Train: 2018-07-31T11:28:25.997315: step 4285, loss 0.554075.
Train: 2018-07-31T11:28:26.153528: step 4286, loss 0.537164.
Train: 2018-07-31T11:28:26.325397: step 4287, loss 0.537134.
Train: 2018-07-31T11:28:26.481607: step 4288, loss 0.587938.
Train: 2018-07-31T11:28:26.653435: step 4289, loss 0.520102.
Train: 2018-07-31T11:28:26.825245: step 4290, loss 0.520023.
Test: 2018-07-31T11:28:27.309537: step 4290, loss 0.548237.
Train: 2018-07-31T11:28:27.465752: step 4291, loss 0.562504.
Train: 2018-07-31T11:28:27.637566: step 4292, loss 0.519809.
Train: 2018-07-31T11:28:27.809390: step 4293, loss 0.528243.
Train: 2018-07-31T11:28:27.965634: step 4294, loss 0.528126.
Train: 2018-07-31T11:28:28.137438: step 4295, loss 0.588379.
Train: 2018-07-31T11:28:28.309272: step 4296, loss 0.614365.
Train: 2018-07-31T11:28:28.481133: step 4297, loss 0.501847.
Train: 2018-07-31T11:28:28.637322: step 4298, loss 0.571226.
Train: 2018-07-31T11:28:28.809186: step 4299, loss 0.588564.
Train: 2018-07-31T11:28:28.980991: step 4300, loss 0.501742.
Test: 2018-07-31T11:28:29.465252: step 4300, loss 0.548034.
Train: 2018-07-31T11:28:30.199490: step 4301, loss 0.623341.
Train: 2018-07-31T11:28:30.371320: step 4302, loss 0.606394.
Train: 2018-07-31T11:28:30.527528: step 4303, loss 0.52062.
Train: 2018-07-31T11:28:30.699373: step 4304, loss 0.553819.
Train: 2018-07-31T11:28:30.871174: step 4305, loss 0.588645.
Train: 2018-07-31T11:28:31.027417: step 4306, loss 0.519298.
Train: 2018-07-31T11:28:31.199223: step 4307, loss 0.49413.
Train: 2018-07-31T11:28:31.371081: step 4308, loss 0.545328.
Train: 2018-07-31T11:28:31.542921: step 4309, loss 0.483975.
Train: 2018-07-31T11:28:31.699138: step 4310, loss 0.536559.
Test: 2018-07-31T11:28:32.183366: step 4310, loss 0.548032.
Train: 2018-07-31T11:28:32.339609: step 4311, loss 0.554285.
Train: 2018-07-31T11:28:32.511445: step 4312, loss 0.589717.
Train: 2018-07-31T11:28:32.683248: step 4313, loss 0.491654.
Train: 2018-07-31T11:28:32.855114: step 4314, loss 0.581178.
Train: 2018-07-31T11:28:33.011323: step 4315, loss 0.590821.
Train: 2018-07-31T11:28:33.183164: step 4316, loss 0.599118.
Train: 2018-07-31T11:28:33.354968: step 4317, loss 0.516658.
Train: 2018-07-31T11:28:33.526801: step 4318, loss 0.530407.
Train: 2018-07-31T11:28:33.698670: step 4319, loss 0.518381.
Train: 2018-07-31T11:28:33.870470: step 4320, loss 0.554367.
Test: 2018-07-31T11:28:34.354762: step 4320, loss 0.548075.
Train: 2018-07-31T11:28:34.526568: step 4321, loss 0.653724.
Train: 2018-07-31T11:28:34.698402: step 4322, loss 0.608476.
Train: 2018-07-31T11:28:34.870238: step 4323, loss 0.54529.
Train: 2018-07-31T11:28:35.042072: step 4324, loss 0.66242.
Train: 2018-07-31T11:28:35.213936: step 4325, loss 0.572114.
Train: 2018-07-31T11:28:35.385742: step 4326, loss 0.61697.
Train: 2018-07-31T11:28:35.557606: step 4327, loss 0.58106.
Train: 2018-07-31T11:28:35.729411: step 4328, loss 0.590046.
Train: 2018-07-31T11:28:35.916867: step 4329, loss 0.6331.
Train: 2018-07-31T11:28:36.073080: step 4330, loss 0.545574.
Test: 2018-07-31T11:28:36.557342: step 4330, loss 0.54875.
Train: 2018-07-31T11:28:36.729209: step 4331, loss 0.511088.
Train: 2018-07-31T11:28:36.901011: step 4332, loss 0.588701.
Train: 2018-07-31T11:28:37.072846: step 4333, loss 0.579752.
Train: 2018-07-31T11:28:37.244682: step 4334, loss 0.526064.
Train: 2018-07-31T11:28:37.416516: step 4335, loss 0.579256.
Train: 2018-07-31T11:28:37.588375: step 4336, loss 0.537644.
Train: 2018-07-31T11:28:37.760185: step 4337, loss 0.504098.
Train: 2018-07-31T11:28:37.932046: step 4338, loss 0.580781.
Train: 2018-07-31T11:28:38.103885: step 4339, loss 0.63164.
Train: 2018-07-31T11:28:38.291312: step 4340, loss 0.530014.
Test: 2018-07-31T11:28:38.759981: step 4340, loss 0.549061.
Train: 2018-07-31T11:28:38.947408: step 4341, loss 0.468567.
Train: 2018-07-31T11:28:39.119272: step 4342, loss 0.520762.
Train: 2018-07-31T11:28:39.306698: step 4343, loss 0.529194.
Train: 2018-07-31T11:28:39.478533: step 4344, loss 0.563703.
Train: 2018-07-31T11:28:39.650392: step 4345, loss 0.554363.
Train: 2018-07-31T11:28:39.822203: step 4346, loss 0.560697.
Train: 2018-07-31T11:28:40.009688: step 4347, loss 0.528865.
Train: 2018-07-31T11:28:40.181493: step 4348, loss 0.634548.
Train: 2018-07-31T11:28:40.353358: step 4349, loss 0.607624.
Train: 2018-07-31T11:28:40.540783: step 4350, loss 0.563925.
Test: 2018-07-31T11:28:41.025046: step 4350, loss 0.549023.
Train: 2018-07-31T11:28:41.212532: step 4351, loss 0.581292.
Train: 2018-07-31T11:28:41.384336: step 4352, loss 0.625279.
Train: 2018-07-31T11:28:41.571817: step 4353, loss 0.642444.
Train: 2018-07-31T11:28:41.743657: step 4354, loss 0.572353.
Train: 2018-07-31T11:28:41.915461: step 4355, loss 0.519808.
Train: 2018-07-31T11:28:42.087296: step 4356, loss 0.537117.
Train: 2018-07-31T11:28:42.274753: step 4357, loss 0.543929.
Train: 2018-07-31T11:28:42.446612: step 4358, loss 0.530518.
Train: 2018-07-31T11:28:42.618424: step 4359, loss 0.554285.
Train: 2018-07-31T11:28:42.805880: step 4360, loss 0.607597.
Test: 2018-07-31T11:28:43.290170: step 4360, loss 0.54916.
Train: 2018-07-31T11:28:43.462000: step 4361, loss 0.590432.
Train: 2018-07-31T11:28:43.649433: step 4362, loss 0.625688.
Train: 2018-07-31T11:28:43.821266: step 4363, loss 0.589316.
Train: 2018-07-31T11:28:44.008746: step 4364, loss 0.622416.
Train: 2018-07-31T11:28:44.180560: step 4365, loss 0.513516.
Train: 2018-07-31T11:28:44.352422: step 4366, loss 0.874827.
Train: 2018-07-31T11:28:44.539847: step 4367, loss 0.55612.
Train: 2018-07-31T11:28:44.711682: step 4368, loss 0.521725.
Train: 2018-07-31T11:28:44.883517: step 4369, loss 0.648542.
Train: 2018-07-31T11:28:45.070972: step 4370, loss 0.573536.
Test: 2018-07-31T11:28:45.555265: step 4370, loss 0.55072.
Train: 2018-07-31T11:28:45.727069: step 4371, loss 0.576518.
Train: 2018-07-31T11:28:45.914559: step 4372, loss 0.666841.
Train: 2018-07-31T11:28:46.086359: step 4373, loss 0.531305.
Train: 2018-07-31T11:28:46.273817: step 4374, loss 0.622827.
Train: 2018-07-31T11:28:46.461273: step 4375, loss 0.543674.
Train: 2018-07-31T11:28:46.633107: step 4376, loss 0.5409.
Train: 2018-07-31T11:28:46.820597: step 4377, loss 0.503959.
Train: 2018-07-31T11:28:47.008019: step 4378, loss 0.601615.
Train: 2018-07-31T11:28:47.164263: step 4379, loss 0.635242.
Train: 2018-07-31T11:28:47.351719: step 4380, loss 0.543145.
Test: 2018-07-31T11:28:47.820363: step 4380, loss 0.557989.
Train: 2018-07-31T11:28:48.007785: step 4381, loss 0.552624.
Train: 2018-07-31T11:28:48.195275: step 4382, loss 0.511826.
Train: 2018-07-31T11:28:48.382697: step 4383, loss 0.562963.
Train: 2018-07-31T11:28:48.570188: step 4384, loss 0.624047.
Train: 2018-07-31T11:28:48.757641: step 4385, loss 0.597908.
Train: 2018-07-31T11:28:48.929478: step 4386, loss 0.53325.
Train: 2018-07-31T11:28:49.116931: step 4387, loss 0.52384.
Train: 2018-07-31T11:28:49.304357: step 4388, loss 0.606456.
Train: 2018-07-31T11:28:49.491843: step 4389, loss 0.587579.
Train: 2018-07-31T11:28:49.663677: step 4390, loss 0.518979.
Test: 2018-07-31T11:28:50.147908: step 4390, loss 0.563178.
Train: 2018-07-31T11:28:50.335390: step 4391, loss 0.57946.
Train: 2018-07-31T11:28:50.522821: step 4392, loss 0.463946.
Train: 2018-07-31T11:28:50.710308: step 4393, loss 0.535044.
Train: 2018-07-31T11:28:50.882112: step 4394, loss 0.547829.
Train: 2018-07-31T11:28:51.069569: step 4395, loss 0.69705.
Train: 2018-07-31T11:28:51.257024: step 4396, loss 0.535276.
Train: 2018-07-31T11:28:51.444514: step 4397, loss 0.595145.
Train: 2018-07-31T11:28:51.616340: step 4398, loss 0.603335.
Train: 2018-07-31T11:28:51.803801: step 4399, loss 0.648552.
Train: 2018-07-31T11:28:51.991252: step 4400, loss 0.534872.
Test: 2018-07-31T11:28:52.459868: step 4400, loss 0.563878.
Train: 2018-07-31T11:28:53.209723: step 4401, loss 0.553392.
Train: 2018-07-31T11:28:53.397179: step 4402, loss 0.604279.
Train: 2018-07-31T11:28:53.568983: step 4403, loss 0.511728.
Train: 2018-07-31T11:28:53.772091: step 4404, loss 0.624342.
Train: 2018-07-31T11:28:53.959516: step 4405, loss 0.614286.
Train: 2018-07-31T11:28:54.146973: step 4406, loss 0.660903.
Train: 2018-07-31T11:28:54.334459: step 4407, loss 0.560732.
Train: 2018-07-31T11:28:54.521884: step 4408, loss 0.625349.
Train: 2018-07-31T11:28:54.709340: step 4409, loss 0.643362.
Train: 2018-07-31T11:28:54.896825: step 4410, loss 0.524178.
Test: 2018-07-31T11:28:55.365439: step 4410, loss 0.564728.
Train: 2018-07-31T11:28:55.552893: step 4411, loss 0.65422.
Train: 2018-07-31T11:28:55.740349: step 4412, loss 0.592095.
Train: 2018-07-31T11:28:55.927806: step 4413, loss 0.602442.
Train: 2018-07-31T11:28:56.115291: step 4414, loss 0.557011.
Train: 2018-07-31T11:28:56.287127: step 4415, loss 0.600554.
Train: 2018-07-31T11:28:56.474552: step 4416, loss 0.607721.
Train: 2018-07-31T11:28:56.662039: step 4417, loss 0.619441.
Train: 2018-07-31T11:28:56.849464: step 4418, loss 0.69346.
Train: 2018-07-31T11:28:57.036946: step 4419, loss 0.616999.
Train: 2018-07-31T11:28:57.224408: step 4420, loss 0.582642.
Test: 2018-07-31T11:28:57.693051: step 4420, loss 0.567493.
Train: 2018-07-31T11:28:57.880504: step 4421, loss 0.580703.
Train: 2018-07-31T11:28:58.067930: step 4422, loss 0.613306.
Train: 2018-07-31T11:28:58.255415: step 4423, loss 0.505943.
Train: 2018-07-31T11:28:58.442842: step 4424, loss 0.581588.
Train: 2018-07-31T11:28:58.630297: step 4425, loss 0.522657.
Train: 2018-07-31T11:28:58.802162: step 4426, loss 0.555895.
Train: 2018-07-31T11:28:59.005211: step 4427, loss 0.525429.
Train: 2018-07-31T11:28:59.177069: step 4428, loss 0.73293.
Train: 2018-07-31T11:28:59.380121: step 4429, loss 0.614478.
Train: 2018-07-31T11:28:59.567613: step 4430, loss 0.574753.
Test: 2018-07-31T11:29:00.036218: step 4430, loss 0.573807.
Train: 2018-07-31T11:29:00.223705: step 4431, loss 0.592429.
Train: 2018-07-31T11:29:00.411131: step 4432, loss 0.490398.
Train: 2018-07-31T11:29:00.598618: step 4433, loss 0.690234.
Train: 2018-07-31T11:29:00.786073: step 4434, loss 0.537273.
Train: 2018-07-31T11:29:00.973499: step 4435, loss 0.550474.
Train: 2018-07-31T11:29:01.176606: step 4436, loss 0.536562.
Train: 2018-07-31T11:29:01.364063: step 4437, loss 0.582027.
Train: 2018-07-31T11:29:01.551519: step 4438, loss 0.570404.
Train: 2018-07-31T11:29:01.738978: step 4439, loss 0.638338.
Train: 2018-07-31T11:29:01.910810: step 4440, loss 0.619604.
Test: 2018-07-31T11:29:02.395040: step 4440, loss 0.58162.
Train: 2018-07-31T11:29:02.582498: step 4441, loss 0.575321.
Train: 2018-07-31T11:29:02.769984: step 4442, loss 0.690703.
Train: 2018-07-31T11:29:02.957440: step 4443, loss 0.537349.
Train: 2018-07-31T11:29:03.144897: step 4444, loss 0.582882.
Train: 2018-07-31T11:29:03.332322: step 4445, loss 0.568425.
Train: 2018-07-31T11:29:03.519778: step 4446, loss 0.491999.
Train: 2018-07-31T11:29:03.707264: step 4447, loss 0.588471.
Train: 2018-07-31T11:29:03.894720: step 4448, loss 0.63233.
Train: 2018-07-31T11:29:04.082176: step 4449, loss 0.673372.
Train: 2018-07-31T11:29:04.269602: step 4450, loss 0.624612.
Test: 2018-07-31T11:29:04.753863: step 4450, loss 0.582735.
Train: 2018-07-31T11:29:04.925728: step 4451, loss 0.535315.
Train: 2018-07-31T11:29:05.128776: step 4452, loss 0.625782.
Train: 2018-07-31T11:29:05.316272: step 4453, loss 0.613374.
Train: 2018-07-31T11:29:05.488097: step 4454, loss 0.532087.
Train: 2018-07-31T11:29:05.675553: step 4455, loss 0.653631.
Train: 2018-07-31T11:29:05.863010: step 4456, loss 0.702805.
Train: 2018-07-31T11:29:06.050461: step 4457, loss 0.601204.
Train: 2018-07-31T11:29:06.237916: step 4458, loss 0.609245.
Train: 2018-07-31T11:29:06.425348: step 4459, loss 0.620444.
Train: 2018-07-31T11:29:06.612834: step 4460, loss 0.651857.
Test: 2018-07-31T11:29:07.081474: step 4460, loss 0.57573.
Train: 2018-07-31T11:29:07.315766: step 4461, loss 0.595869.
Train: 2018-07-31T11:29:07.550085: step 4462, loss 0.63282.
Train: 2018-07-31T11:29:07.784405: step 4463, loss 0.598053.
Train: 2018-07-31T11:29:08.018727: step 4464, loss 0.597675.
Train: 2018-07-31T11:29:08.221832: step 4465, loss 0.622542.
Train: 2018-07-31T11:29:08.409289: step 4466, loss 0.624382.
Train: 2018-07-31T11:29:08.596713: step 4467, loss 0.656044.
Train: 2018-07-31T11:29:08.784194: step 4468, loss 0.642814.
Train: 2018-07-31T11:29:08.987272: step 4469, loss 0.571426.
Train: 2018-07-31T11:29:09.174734: step 4470, loss 0.595771.
Test: 2018-07-31T11:29:09.658965: step 4470, loss 0.572503.
Train: 2018-07-31T11:29:09.846453: step 4471, loss 0.544916.
Train: 2018-07-31T11:29:10.033878: step 4472, loss 0.574426.
Train: 2018-07-31T11:29:10.221334: step 4473, loss 0.630518.
Train: 2018-07-31T11:29:10.408790: step 4474, loss 0.551795.
Train: 2018-07-31T11:29:10.580660: step 4475, loss 0.59265.
Train: 2018-07-31T11:29:10.768111: step 4476, loss 0.609092.
Train: 2018-07-31T11:29:10.955537: step 4477, loss 0.54646.
Train: 2018-07-31T11:29:11.142992: step 4478, loss 0.707314.
Train: 2018-07-31T11:29:11.330449: step 4479, loss 0.516801.
Train: 2018-07-31T11:29:11.517935: step 4480, loss 0.535647.
Test: 2018-07-31T11:29:12.002165: step 4480, loss 0.571894.
Train: 2018-07-31T11:29:12.189657: step 4481, loss 0.585804.
Train: 2018-07-31T11:29:12.377079: step 4482, loss 0.60563.
Train: 2018-07-31T11:29:12.548944: step 4483, loss 0.788901.
Train: 2018-07-31T11:29:12.752021: step 4484, loss 0.567495.
Train: 2018-07-31T11:29:12.939481: step 4485, loss 0.620513.
Train: 2018-07-31T11:29:13.111312: step 4486, loss 0.549115.
Train: 2018-07-31T11:29:13.298768: step 4487, loss 0.566593.
Train: 2018-07-31T11:29:13.486194: step 4488, loss 0.566685.
Train: 2018-07-31T11:29:13.673651: step 4489, loss 0.696794.
Train: 2018-07-31T11:29:13.861137: step 4490, loss 0.792385.
Test: 2018-07-31T11:29:14.345393: step 4490, loss 0.575695.
Train: 2018-07-31T11:29:14.517233: step 4491, loss 0.556706.
Train: 2018-07-31T11:29:14.704689: step 4492, loss 0.705534.
Train: 2018-07-31T11:29:14.892114: step 4493, loss 0.655156.
Train: 2018-07-31T11:29:15.079571: step 4494, loss 0.542566.
Train: 2018-07-31T11:29:15.267027: step 4495, loss 0.576276.
Train: 2018-07-31T11:29:15.454508: step 4496, loss 0.601959.
Train: 2018-07-31T11:29:15.641963: step 4497, loss 0.587115.
Train: 2018-07-31T11:29:15.829394: step 4498, loss 0.536101.
Train: 2018-07-31T11:29:16.032472: step 4499, loss 0.629108.
Train: 2018-07-31T11:29:16.219959: step 4500, loss 0.603303.
Test: 2018-07-31T11:29:16.688568: step 4500, loss 0.580141.
Train: 2018-07-31T11:29:17.469666: step 4501, loss 0.637132.
Train: 2018-07-31T11:29:17.657122: step 4502, loss 0.507069.
Train: 2018-07-31T11:29:17.844548: step 4503, loss 0.565541.
Train: 2018-07-31T11:29:18.016414: step 4504, loss 0.608324.
Train: 2018-07-31T11:29:18.203870: step 4505, loss 0.593638.
Train: 2018-07-31T11:29:18.391320: step 4506, loss 0.593828.
Train: 2018-07-31T11:29:18.578782: step 4507, loss 0.645956.
Train: 2018-07-31T11:29:18.766237: step 4508, loss 0.619893.
Train: 2018-07-31T11:29:18.953663: step 4509, loss 0.540768.
Train: 2018-07-31T11:29:19.125499: step 4510, loss 0.602535.
Test: 2018-07-31T11:29:19.609790: step 4510, loss 0.579258.
Train: 2018-07-31T11:29:19.797217: step 4511, loss 0.559007.
Train: 2018-07-31T11:29:19.969076: step 4512, loss 0.639702.
Train: 2018-07-31T11:29:20.156537: step 4513, loss 0.654864.
Train: 2018-07-31T11:29:20.343993: step 4514, loss 0.593972.
Train: 2018-07-31T11:29:20.531420: step 4515, loss 0.594439.
Train: 2018-07-31T11:29:20.703284: step 4516, loss 0.56695.
Train: 2018-07-31T11:29:20.890740: step 4517, loss 0.557994.
Train: 2018-07-31T11:29:21.062544: step 4518, loss 0.988031.
Train: 2018-07-31T11:29:21.250031: step 4519, loss 0.6476.
Train: 2018-07-31T11:29:21.437456: step 4520, loss 0.67726.
Test: 2018-07-31T11:29:21.906127: step 4520, loss 0.57991.
Train: 2018-07-31T11:29:22.093554: step 4521, loss 0.888925.
Train: 2018-07-31T11:29:22.281040: step 4522, loss 0.645216.
Train: 2018-07-31T11:29:22.452875: step 4523, loss 0.599382.
Train: 2018-07-31T11:29:22.640331: step 4524, loss 0.558976.
Train: 2018-07-31T11:29:22.827757: step 4525, loss 0.637716.
Train: 2018-07-31T11:29:23.015213: step 4526, loss 0.648523.
Train: 2018-07-31T11:29:23.202699: step 4527, loss 0.58752.
Train: 2018-07-31T11:29:23.390155: step 4528, loss 0.844907.
Train: 2018-07-31T11:29:23.577611: step 4529, loss 0.631865.
Train: 2018-07-31T11:29:23.733794: step 4530, loss 0.624346.
Test: 2018-07-31T11:29:24.218055: step 4530, loss 0.591858.
Train: 2018-07-31T11:29:24.405512: step 4531, loss 0.572021.
Train: 2018-07-31T11:29:24.577376: step 4532, loss 0.674394.
Train: 2018-07-31T11:29:24.764833: step 4533, loss 0.623336.
Train: 2018-07-31T11:29:24.952258: step 4534, loss 0.538648.
Train: 2018-07-31T11:29:25.139745: step 4535, loss 0.615064.
Train: 2018-07-31T11:29:25.311550: step 4536, loss 0.531078.
Train: 2018-07-31T11:29:25.499007: step 4537, loss 0.581339.
Train: 2018-07-31T11:29:25.686496: step 4538, loss 0.538898.
Train: 2018-07-31T11:29:25.873919: step 4539, loss 0.606699.
Train: 2018-07-31T11:29:26.061375: step 4540, loss 0.589508.
Test: 2018-07-31T11:29:26.545666: step 4540, loss 0.592017.
Train: 2018-07-31T11:29:26.733122: step 4541, loss 0.640534.
Train: 2018-07-31T11:29:26.920549: step 4542, loss 0.597455.
Train: 2018-07-31T11:29:27.092414: step 4543, loss 0.63993.
Train: 2018-07-31T11:29:27.279839: step 4544, loss 0.62231.
Train: 2018-07-31T11:29:27.451673: step 4545, loss 0.638953.
Train: 2018-07-31T11:29:27.639160: step 4546, loss 0.595425.
Train: 2018-07-31T11:29:27.826616: step 4547, loss 0.5948.
Train: 2018-07-31T11:29:27.998451: step 4548, loss 0.679986.
Train: 2018-07-31T11:29:28.185902: step 4549, loss 0.610634.
Train: 2018-07-31T11:29:28.373333: step 4550, loss 0.627056.
Test: 2018-07-31T11:29:28.857595: step 4550, loss 0.586405.
Train: 2018-07-31T11:29:29.045075: step 4551, loss 0.617787.
Train: 2018-07-31T11:29:29.232532: step 4552, loss 0.531859.
Train: 2018-07-31T11:29:29.404342: step 4553, loss 0.571545.
Train: 2018-07-31T11:29:29.591797: step 4554, loss 0.624257.
Train: 2018-07-31T11:29:29.779284: step 4555, loss 0.589535.
Train: 2018-07-31T11:29:29.951118: step 4556, loss 0.614443.
Train: 2018-07-31T11:29:30.138569: step 4557, loss 0.579772.
Train: 2018-07-31T11:29:30.326030: step 4558, loss 0.553635.
Train: 2018-07-31T11:29:30.497836: step 4559, loss 0.57638.
Train: 2018-07-31T11:29:30.669670: step 4560, loss 0.646199.
Test: 2018-07-31T11:29:31.153961: step 4560, loss 0.580192.
Train: 2018-07-31T11:29:31.341388: step 4561, loss 0.654178.
Train: 2018-07-31T11:29:31.528875: step 4562, loss 0.6195.
Train: 2018-07-31T11:29:31.700709: step 4563, loss 0.669978.
Train: 2018-07-31T11:29:31.888165: step 4564, loss 0.575986.
Train: 2018-07-31T11:29:32.059970: step 4565, loss 0.558636.
Train: 2018-07-31T11:29:32.247426: step 4566, loss 0.60047.
Train: 2018-07-31T11:29:32.419290: step 4567, loss 0.730409.
Train: 2018-07-31T11:29:32.606717: step 4568, loss 0.549216.
Train: 2018-07-31T11:29:32.794172: step 4569, loss 0.574558.
Train: 2018-07-31T11:29:32.981653: step 4570, loss 0.600092.
Test: 2018-07-31T11:29:33.465890: step 4570, loss 0.578037.
Train: 2018-07-31T11:29:33.637755: step 4571, loss 0.541665.
Train: 2018-07-31T11:29:33.809594: step 4572, loss 0.659968.
Train: 2018-07-31T11:29:33.997015: step 4573, loss 0.685716.
Train: 2018-07-31T11:29:34.184472: step 4574, loss 0.618909.
Train: 2018-07-31T11:29:34.371958: step 4575, loss 0.619353.
Train: 2018-07-31T11:29:34.543794: step 4576, loss 0.569657.
Train: 2018-07-31T11:29:34.731249: step 4577, loss 0.551339.
Train: 2018-07-31T11:29:34.903054: step 4578, loss 0.653632.
Train: 2018-07-31T11:29:35.090511: step 4579, loss 0.578939.
Train: 2018-07-31T11:29:35.277965: step 4580, loss 0.602008.
Test: 2018-07-31T11:29:35.762226: step 4580, loss 0.581831.
Train: 2018-07-31T11:29:35.934062: step 4581, loss 0.604182.
Train: 2018-07-31T11:29:36.121519: step 4582, loss 0.619466.
Train: 2018-07-31T11:29:36.293353: step 4583, loss 0.604069.
Train: 2018-07-31T11:29:36.480839: step 4584, loss 0.595868.
Train: 2018-07-31T11:29:36.652675: step 4585, loss 0.612469.
Train: 2018-07-31T11:29:36.840100: step 4586, loss 0.587744.
Train: 2018-07-31T11:29:37.011960: step 4587, loss 0.55475.
Train: 2018-07-31T11:29:37.199421: step 4588, loss 0.505118.
Train: 2018-07-31T11:29:37.371255: step 4589, loss 0.579271.
Train: 2018-07-31T11:29:37.543091: step 4590, loss 0.579033.
Test: 2018-07-31T11:29:38.027321: step 4590, loss 0.581445.
Train: 2018-07-31T11:29:38.214803: step 4591, loss 0.578734.
Train: 2018-07-31T11:29:38.386643: step 4592, loss 0.603473.
Train: 2018-07-31T11:29:38.574069: step 4593, loss 0.594763.
Train: 2018-07-31T11:29:38.761550: step 4594, loss 0.619624.
Train: 2018-07-31T11:29:38.933394: step 4595, loss 0.577094.
Train: 2018-07-31T11:29:39.105219: step 4596, loss 0.551248.
Train: 2018-07-31T11:29:39.292651: step 4597, loss 0.601524.
Train: 2018-07-31T11:29:39.464487: step 4598, loss 0.550056.
Train: 2018-07-31T11:29:39.651941: step 4599, loss 0.566485.
Train: 2018-07-31T11:29:39.823777: step 4600, loss 0.514544.
Test: 2018-07-31T11:29:40.308072: step 4600, loss 0.576695.
Train: 2018-07-31T11:29:41.073514: step 4601, loss 0.625505.
Train: 2018-07-31T11:29:41.260965: step 4602, loss 0.547464.
Train: 2018-07-31T11:29:41.432809: step 4603, loss 0.555491.
Train: 2018-07-31T11:29:41.620260: step 4604, loss 0.624529.
Train: 2018-07-31T11:29:41.792065: step 4605, loss 0.545618.
Train: 2018-07-31T11:29:41.963899: step 4606, loss 0.582767.
Train: 2018-07-31T11:29:42.151357: step 4607, loss 0.614786.
Train: 2018-07-31T11:29:42.338843: step 4608, loss 0.632061.
Train: 2018-07-31T11:29:42.510677: step 4609, loss 0.596438.
Train: 2018-07-31T11:29:42.682516: step 4610, loss 0.543118.
Test: 2018-07-31T11:29:43.166774: step 4610, loss 0.57212.
Train: 2018-07-31T11:29:43.354200: step 4611, loss 0.595764.
Train: 2018-07-31T11:29:43.526035: step 4612, loss 0.621999.
Train: 2018-07-31T11:29:43.713521: step 4613, loss 0.639388.
Train: 2018-07-31T11:29:43.900971: step 4614, loss 0.692046.
Train: 2018-07-31T11:29:44.072811: step 4615, loss 0.594511.
Train: 2018-07-31T11:29:44.244617: step 4616, loss 0.55908.
Train: 2018-07-31T11:29:44.416482: step 4617, loss 0.576389.
Train: 2018-07-31T11:29:44.588285: step 4618, loss 0.576149.
Train: 2018-07-31T11:29:44.775772: step 4619, loss 0.514873.
Train: 2018-07-31T11:29:44.947576: step 4620, loss 0.601836.
Test: 2018-07-31T11:29:45.431872: step 4620, loss 0.569584.
Train: 2018-07-31T11:29:45.603698: step 4621, loss 0.566751.
Train: 2018-07-31T11:29:45.791130: step 4622, loss 0.63619.
Train: 2018-07-31T11:29:45.978615: step 4623, loss 0.540264.
Train: 2018-07-31T11:29:46.150444: step 4624, loss 0.566145.
Train: 2018-07-31T11:29:46.322255: step 4625, loss 0.531239.
Train: 2018-07-31T11:29:46.494090: step 4626, loss 0.600519.
Train: 2018-07-31T11:29:46.665924: step 4627, loss 0.574246.
Train: 2018-07-31T11:29:46.853411: step 4628, loss 0.634969.
Train: 2018-07-31T11:29:47.025214: step 4629, loss 0.617364.
Train: 2018-07-31T11:29:47.197080: step 4630, loss 0.625822.
Test: 2018-07-31T11:29:47.681346: step 4630, loss 0.567737.
Train: 2018-07-31T11:29:47.853146: step 4631, loss 0.573582.
Train: 2018-07-31T11:29:48.025011: step 4632, loss 0.564791.
Train: 2018-07-31T11:29:48.196845: step 4633, loss 0.650998.
Train: 2018-07-31T11:29:48.384296: step 4634, loss 0.616218.
Train: 2018-07-31T11:29:48.556107: step 4635, loss 0.564466.
Train: 2018-07-31T11:29:48.743587: step 4636, loss 0.607179.
Train: 2018-07-31T11:29:48.915422: step 4637, loss 0.581363.
Train: 2018-07-31T11:29:49.087232: step 4638, loss 0.598269.
Train: 2018-07-31T11:29:49.259097: step 4639, loss 0.513163.
Train: 2018-07-31T11:29:49.446553: step 4640, loss 0.580995.
Test: 2018-07-31T11:29:49.915162: step 4640, loss 0.566674.
Train: 2018-07-31T11:29:50.087028: step 4641, loss 0.59784.
Train: 2018-07-31T11:29:50.258833: step 4642, loss 0.521454.
Train: 2018-07-31T11:29:50.430668: step 4643, loss 0.606083.
Train: 2018-07-31T11:29:50.602533: step 4644, loss 0.580543.
Train: 2018-07-31T11:29:50.774338: step 4645, loss 0.537036.
Train: 2018-07-31T11:29:50.961818: step 4646, loss 0.699296.
Train: 2018-07-31T11:29:51.133653: step 4647, loss 0.572423.
Train: 2018-07-31T11:29:51.305463: step 4648, loss 0.564574.
Train: 2018-07-31T11:29:51.477298: step 4649, loss 0.589389.
Train: 2018-07-31T11:29:51.649156: step 4650, loss 0.659112.
Test: 2018-07-31T11:29:52.133424: step 4650, loss 0.569847.
Train: 2018-07-31T11:29:52.305263: step 4651, loss 0.560152.
Train: 2018-07-31T11:29:52.492685: step 4652, loss 0.593391.
Train: 2018-07-31T11:29:52.664520: step 4653, loss 0.62824.
Train: 2018-07-31T11:29:52.836384: step 4654, loss 0.722478.
Train: 2018-07-31T11:29:53.023834: step 4655, loss 0.546971.
Train: 2018-07-31T11:29:53.195645: step 4656, loss 0.607249.
Train: 2018-07-31T11:29:53.367512: step 4657, loss 0.58388.
Train: 2018-07-31T11:29:53.554970: step 4658, loss 0.610698.
Train: 2018-07-31T11:29:53.726771: step 4659, loss 0.578989.
Train: 2018-07-31T11:29:53.898605: step 4660, loss 0.530521.
Test: 2018-07-31T11:29:54.367276: step 4660, loss 0.584718.
Train: 2018-07-31T11:29:54.554702: step 4661, loss 0.582001.
Train: 2018-07-31T11:29:54.726567: step 4662, loss 0.599992.
Train: 2018-07-31T11:29:54.898372: step 4663, loss 0.609487.
Train: 2018-07-31T11:29:55.070236: step 4664, loss 0.627228.
Train: 2018-07-31T11:29:55.242072: step 4665, loss 0.661621.
Train: 2018-07-31T11:29:55.429531: step 4666, loss 0.527888.
Train: 2018-07-31T11:29:55.601362: step 4667, loss 0.578678.
Train: 2018-07-31T11:29:55.773166: step 4668, loss 0.595781.
Train: 2018-07-31T11:29:55.945002: step 4669, loss 0.579062.
Train: 2018-07-31T11:29:56.116866: step 4670, loss 0.562138.
Test: 2018-07-31T11:29:56.601128: step 4670, loss 0.590159.
Train: 2018-07-31T11:29:56.772932: step 4671, loss 0.5874.
Train: 2018-07-31T11:29:56.944766: step 4672, loss 0.595703.
Train: 2018-07-31T11:29:57.116632: step 4673, loss 0.56983.
Train: 2018-07-31T11:29:57.288438: step 4674, loss 0.57797.
Train: 2018-07-31T11:29:57.460302: step 4675, loss 0.568909.
Train: 2018-07-31T11:29:57.632136: step 4676, loss 0.618029.
Train: 2018-07-31T11:29:57.803941: step 4677, loss 0.548949.
Train: 2018-07-31T11:29:57.991410: step 4678, loss 0.593247.
Train: 2018-07-31T11:29:58.163232: step 4679, loss 0.61888.
Train: 2018-07-31T11:29:58.335096: step 4680, loss 0.565902.
Test: 2018-07-31T11:29:58.819328: step 4680, loss 0.585654.
Train: 2018-07-31T11:29:58.975572: step 4681, loss 0.506742.
Train: 2018-07-31T11:29:59.147408: step 4682, loss 0.617435.
Train: 2018-07-31T11:29:59.334857: step 4683, loss 0.68772.
Train: 2018-07-31T11:29:59.506698: step 4684, loss 0.666301.
Train: 2018-07-31T11:29:59.662881: step 4685, loss 0.606893.
Train: 2018-07-31T11:29:59.834716: step 4686, loss 0.600194.
Train: 2018-07-31T11:30:00.006550: step 4687, loss 0.588044.
Train: 2018-07-31T11:30:00.178418: step 4688, loss 0.534492.
Train: 2018-07-31T11:30:00.350251: step 4689, loss 0.551777.
Train: 2018-07-31T11:30:00.537700: step 4690, loss 0.569132.
Test: 2018-07-31T11:30:01.006346: step 4690, loss 0.580576.
Train: 2018-07-31T11:30:01.178185: step 4691, loss 0.67546.
Train: 2018-07-31T11:30:01.349986: step 4692, loss 0.595175.
Train: 2018-07-31T11:30:01.521851: step 4693, loss 0.59489.
Train: 2018-07-31T11:30:01.693654: step 4694, loss 0.630128.
Train: 2018-07-31T11:30:01.865489: step 4695, loss 0.594296.
Train: 2018-07-31T11:30:02.037325: step 4696, loss 0.58512.
Train: 2018-07-31T11:30:02.224781: step 4697, loss 0.609172.
Train: 2018-07-31T11:30:02.381025: step 4698, loss 0.608937.
Train: 2018-07-31T11:30:02.568475: step 4699, loss 0.57531.
Train: 2018-07-31T11:30:02.740284: step 4700, loss 0.539743.
Test: 2018-07-31T11:30:03.208955: step 4700, loss 0.577471.
Train: 2018-07-31T11:30:03.911916: step 4701, loss 0.645519.
Train: 2018-07-31T11:30:04.099342: step 4702, loss 0.583035.
Train: 2018-07-31T11:30:04.255554: step 4703, loss 0.520926.
Train: 2018-07-31T11:30:04.427390: step 4704, loss 0.55525.
Train: 2018-07-31T11:30:04.599249: step 4705, loss 0.546471.
Train: 2018-07-31T11:30:04.771090: step 4706, loss 0.590433.
Train: 2018-07-31T11:30:04.958517: step 4707, loss 0.685009.
Train: 2018-07-31T11:30:05.130350: step 4708, loss 0.748175.
Train: 2018-07-31T11:30:05.302184: step 4709, loss 0.551766.
Train: 2018-07-31T11:30:05.474021: step 4710, loss 0.598639.
Test: 2018-07-31T11:30:05.942662: step 4710, loss 0.574768.
Train: 2018-07-31T11:30:06.114528: step 4711, loss 0.598757.
Train: 2018-07-31T11:30:06.301985: step 4712, loss 0.607906.
Train: 2018-07-31T11:30:06.473816: step 4713, loss 0.608099.
Train: 2018-07-31T11:30:06.645620: step 4714, loss 0.635209.
Train: 2018-07-31T11:30:06.817455: step 4715, loss 0.581528.
Train: 2018-07-31T11:30:06.989320: step 4716, loss 0.61749.
Train: 2018-07-31T11:30:07.161157: step 4717, loss 0.61754.
Train: 2018-07-31T11:30:07.332989: step 4718, loss 0.590992.
Train: 2018-07-31T11:30:07.504795: step 4719, loss 0.555226.
Train: 2018-07-31T11:30:07.676663: step 4720, loss 0.590784.
Test: 2018-07-31T11:30:08.145299: step 4720, loss 0.575875.
Train: 2018-07-31T11:30:08.317103: step 4721, loss 0.670521.
Train: 2018-07-31T11:30:08.535834: step 4722, loss 0.55526.
Train: 2018-07-31T11:30:08.707638: step 4723, loss 0.678661.
Train: 2018-07-31T11:30:08.879472: step 4724, loss 0.563946.
Train: 2018-07-31T11:30:09.051308: step 4725, loss 0.686416.
Train: 2018-07-31T11:30:09.223172: step 4726, loss 0.563727.
Train: 2018-07-31T11:30:09.395007: step 4727, loss 0.537549.
Train: 2018-07-31T11:30:09.566845: step 4728, loss 0.641315.
Train: 2018-07-31T11:30:09.738679: step 4729, loss 0.580475.
Train: 2018-07-31T11:30:09.910515: step 4730, loss 0.597429.
Test: 2018-07-31T11:30:10.394743: step 4730, loss 0.574177.
Train: 2018-07-31T11:30:10.566578: step 4731, loss 0.55423.
Train: 2018-07-31T11:30:10.738442: step 4732, loss 0.60538.
Train: 2018-07-31T11:30:10.894650: step 4733, loss 0.579419.
Train: 2018-07-31T11:30:11.066486: step 4734, loss 0.545015.
Train: 2018-07-31T11:30:11.253917: step 4735, loss 0.63854.
Train: 2018-07-31T11:30:11.425775: step 4736, loss 0.629656.
Train: 2018-07-31T11:30:11.597617: step 4737, loss 0.654757.
Train: 2018-07-31T11:30:11.785068: step 4738, loss 0.527263.
Train: 2018-07-31T11:30:11.956877: step 4739, loss 0.645355.
Train: 2018-07-31T11:30:12.128745: step 4740, loss 0.592997.
Test: 2018-07-31T11:30:12.612973: step 4740, loss 0.571642.
Train: 2018-07-31T11:30:12.784808: step 4741, loss 0.74908.
Train: 2018-07-31T11:30:12.956673: step 4742, loss 0.577045.
Train: 2018-07-31T11:30:13.144098: step 4743, loss 0.518302.
Train: 2018-07-31T11:30:13.315934: step 4744, loss 0.559709.
Train: 2018-07-31T11:30:13.487768: step 4745, loss 0.534551.
Train: 2018-07-31T11:30:13.659627: step 4746, loss 0.534294.
Train: 2018-07-31T11:30:13.831464: step 4747, loss 0.583924.
Train: 2018-07-31T11:30:14.018894: step 4748, loss 0.592051.
Train: 2018-07-31T11:30:14.190728: step 4749, loss 0.591868.
Train: 2018-07-31T11:30:14.362590: step 4750, loss 0.574982.
Test: 2018-07-31T11:30:14.831204: step 4750, loss 0.569146.
Train: 2018-07-31T11:30:15.003038: step 4751, loss 0.624995.
Train: 2018-07-31T11:30:15.174872: step 4752, loss 0.507653.
Train: 2018-07-31T11:30:15.346738: step 4753, loss 0.633145.
Train: 2018-07-31T11:30:15.518573: step 4754, loss 0.607835.
Train: 2018-07-31T11:30:15.690401: step 4755, loss 0.565683.
Train: 2018-07-31T11:30:15.862213: step 4756, loss 0.573911.
Train: 2018-07-31T11:30:16.034079: step 4757, loss 0.598984.
Train: 2018-07-31T11:30:16.205882: step 4758, loss 0.590414.
Train: 2018-07-31T11:30:16.377717: step 4759, loss 0.632382.
Train: 2018-07-31T11:30:16.549552: step 4760, loss 0.590102.
Test: 2018-07-31T11:30:17.033813: step 4760, loss 0.567439.
Train: 2018-07-31T11:30:17.205647: step 4761, loss 0.606776.
Train: 2018-07-31T11:30:17.377512: step 4762, loss 0.581387.
Train: 2018-07-31T11:30:17.533695: step 4763, loss 0.530847.
Train: 2018-07-31T11:30:17.705561: step 4764, loss 0.614711.
Train: 2018-07-31T11:30:17.877366: step 4765, loss 0.564156.
Train: 2018-07-31T11:30:18.049200: step 4766, loss 0.589225.
Train: 2018-07-31T11:30:18.236686: step 4767, loss 0.538662.
Train: 2018-07-31T11:30:18.408490: step 4768, loss 0.555309.
Train: 2018-07-31T11:30:18.580325: step 4769, loss 0.563564.
Train: 2018-07-31T11:30:18.752161: step 4770, loss 0.563403.
Test: 2018-07-31T11:30:19.236421: step 4770, loss 0.565994.
Train: 2018-07-31T11:30:19.502010: step 4771, loss 0.546312.
Train: 2018-07-31T11:30:19.658198: step 4772, loss 0.588538.
Train: 2018-07-31T11:30:19.830033: step 4773, loss 0.537393.
Train: 2018-07-31T11:30:20.001867: step 4774, loss 0.613963.
Train: 2018-07-31T11:30:20.173738: step 4775, loss 0.631032.
Train: 2018-07-31T11:30:20.345537: step 4776, loss 0.579618.
Train: 2018-07-31T11:30:20.517373: step 4777, loss 0.57952.
Train: 2018-07-31T11:30:20.689238: step 4778, loss 0.48515.
Train: 2018-07-31T11:30:20.861072: step 4779, loss 0.527779.
Train: 2018-07-31T11:30:21.032909: step 4780, loss 0.54476.
Test: 2018-07-31T11:30:21.517168: step 4780, loss 0.564672.
Train: 2018-07-31T11:30:21.689003: step 4781, loss 0.613779.
Train: 2018-07-31T11:30:21.860832: step 4782, loss 0.579085.
Train: 2018-07-31T11:30:22.032672: step 4783, loss 0.622487.
Train: 2018-07-31T11:30:22.204501: step 4784, loss 0.526699.
Train: 2018-07-31T11:30:22.360720: step 4785, loss 0.622469.
Train: 2018-07-31T11:30:22.532524: step 4786, loss 0.552595.
Train: 2018-07-31T11:30:22.704390: step 4787, loss 0.631149.
Train: 2018-07-31T11:30:22.876225: step 4788, loss 0.482509.
Train: 2018-07-31T11:30:23.063681: step 4789, loss 0.534799.
Train: 2018-07-31T11:30:23.219895: step 4790, loss 0.59606.
Test: 2018-07-31T11:30:23.704127: step 4790, loss 0.563727.
Train: 2018-07-31T11:30:23.875991: step 4791, loss 0.604818.
Train: 2018-07-31T11:30:24.047800: step 4792, loss 0.595979.
Train: 2018-07-31T11:30:24.204038: step 4793, loss 0.525516.
Train: 2018-07-31T11:30:24.375873: step 4794, loss 0.648745.
Train: 2018-07-31T11:30:24.547678: step 4795, loss 0.551768.
Train: 2018-07-31T11:30:24.719515: step 4796, loss 0.578125.
Train: 2018-07-31T11:30:24.875727: step 4797, loss 0.534029.
Train: 2018-07-31T11:30:25.047560: step 4798, loss 0.578002.
Train: 2018-07-31T11:30:25.219426: step 4799, loss 0.613208.
Train: 2018-07-31T11:30:25.391261: step 4800, loss 0.542631.
Test: 2018-07-31T11:30:25.875491: step 4800, loss 0.563061.
Train: 2018-07-31T11:30:26.578483: step 4801, loss 0.533752.
Train: 2018-07-31T11:30:26.750288: step 4802, loss 0.542481.
Train: 2018-07-31T11:30:26.922156: step 4803, loss 0.560054.
Train: 2018-07-31T11:30:27.093982: step 4804, loss 0.630767.
Train: 2018-07-31T11:30:27.281446: step 4805, loss 0.524527.
Train: 2018-07-31T11:30:27.437657: step 4806, loss 0.586436.
Train: 2018-07-31T11:30:27.625083: step 4807, loss 0.586392.
Train: 2018-07-31T11:30:27.796951: step 4808, loss 0.604075.
Train: 2018-07-31T11:30:27.953130: step 4809, loss 0.577422.
Train: 2018-07-31T11:30:28.124996: step 4810, loss 0.550798.
Test: 2018-07-31T11:30:28.609227: step 4810, loss 0.562486.
Train: 2018-07-31T11:30:28.781062: step 4811, loss 0.550751.
Train: 2018-07-31T11:30:28.952926: step 4812, loss 0.60384.
Train: 2018-07-31T11:30:29.109140: step 4813, loss 0.630335.
Train: 2018-07-31T11:30:29.280945: step 4814, loss 0.55948.
Train: 2018-07-31T11:30:29.452779: step 4815, loss 0.56063.
Train: 2018-07-31T11:30:29.624645: step 4816, loss 0.532951.
Train: 2018-07-31T11:30:29.796451: step 4817, loss 0.585837.
Train: 2018-07-31T11:30:29.968284: step 4818, loss 0.638705.
Train: 2018-07-31T11:30:30.124497: step 4819, loss 0.559352.
Train: 2018-07-31T11:30:30.296331: step 4820, loss 0.506603.
Test: 2018-07-31T11:30:30.780592: step 4820, loss 0.562185.
Train: 2018-07-31T11:30:30.952428: step 4821, loss 0.57691.
Train: 2018-07-31T11:30:31.124263: step 4822, loss 0.594475.
Train: 2018-07-31T11:30:31.296132: step 4823, loss 0.559291.
Train: 2018-07-31T11:30:31.467963: step 4824, loss 0.559271.
Train: 2018-07-31T11:30:31.624176: step 4825, loss 0.568033.
Train: 2018-07-31T11:30:31.796015: step 4826, loss 0.550429.
Train: 2018-07-31T11:30:31.967816: step 4827, loss 0.620738.
Train: 2018-07-31T11:30:32.139680: step 4828, loss 0.576736.
Train: 2018-07-31T11:30:32.327137: step 4829, loss 0.664541.
Train: 2018-07-31T11:30:32.498942: step 4830, loss 0.524072.
Test: 2018-07-31T11:30:32.983233: step 4830, loss 0.561949.
Train: 2018-07-31T11:30:33.155068: step 4831, loss 0.559103.
Train: 2018-07-31T11:30:33.311281: step 4832, loss 0.688508.
Train: 2018-07-31T11:30:33.467465: step 4833, loss 0.550357.
Train: 2018-07-31T11:30:33.639329: step 4834, loss 0.515577.
Train: 2018-07-31T11:30:33.811133: step 4835, loss 0.585108.
Train: 2018-07-31T11:30:33.983000: step 4836, loss 0.480885.
Train: 2018-07-31T11:30:34.154829: step 4837, loss 0.671909.
Train: 2018-07-31T11:30:34.326639: step 4838, loss 0.524212.
Train: 2018-07-31T11:30:34.498503: step 4839, loss 0.584919.
Train: 2018-07-31T11:30:34.670342: step 4840, loss 0.515465.
Test: 2018-07-31T11:30:35.154599: step 4840, loss 0.561612.
Train: 2018-07-31T11:30:35.326405: step 4841, loss 0.619577.
Train: 2018-07-31T11:30:35.498272: step 4842, loss 0.532697.
Train: 2018-07-31T11:30:35.654482: step 4843, loss 0.576074.
Train: 2018-07-31T11:30:35.826311: step 4844, loss 0.541253.
Train: 2018-07-31T11:30:35.998152: step 4845, loss 0.584703.
Train: 2018-07-31T11:30:36.169957: step 4846, loss 0.602099.
Train: 2018-07-31T11:30:36.341791: step 4847, loss 0.575921.
Train: 2018-07-31T11:30:36.513656: step 4848, loss 0.575883.
Train: 2018-07-31T11:30:36.685460: step 4849, loss 0.610703.
Train: 2018-07-31T11:30:36.857296: step 4850, loss 0.558391.
Test: 2018-07-31T11:30:37.341557: step 4850, loss 0.561192.
Train: 2018-07-31T11:30:37.513392: step 4851, loss 0.654099.
Train: 2018-07-31T11:30:37.685226: step 4852, loss 0.540984.
Train: 2018-07-31T11:30:37.841470: step 4853, loss 0.619062.
Train: 2018-07-31T11:30:38.013274: step 4854, loss 0.54102.
Train: 2018-07-31T11:30:38.185140: step 4855, loss 0.558319.
Train: 2018-07-31T11:30:38.356945: step 4856, loss 0.575579.
Train: 2018-07-31T11:30:38.528779: step 4857, loss 0.523737.
Train: 2018-07-31T11:30:38.700614: step 4858, loss 0.635985.
Train: 2018-07-31T11:30:38.872449: step 4859, loss 0.592744.
Train: 2018-07-31T11:30:39.044308: step 4860, loss 0.54959.
Test: 2018-07-31T11:30:39.512955: step 4860, loss 0.560991.
Train: 2018-07-31T11:30:39.700410: step 4861, loss 0.644343.
Train: 2018-07-31T11:30:39.856593: step 4862, loss 0.583987.
Train: 2018-07-31T11:30:40.028458: step 4863, loss 0.532452.
Train: 2018-07-31T11:30:40.200262: step 4864, loss 0.541035.
Train: 2018-07-31T11:30:40.356506: step 4865, loss 0.583875.
Train: 2018-07-31T11:30:40.528344: step 4866, loss 0.515275.
Train: 2018-07-31T11:30:40.700147: step 4867, loss 0.523764.
Train: 2018-07-31T11:30:40.872011: step 4868, loss 0.601013.
Train: 2018-07-31T11:30:41.043849: step 4869, loss 0.583802.
Train: 2018-07-31T11:30:41.215651: step 4870, loss 0.583784.
Test: 2018-07-31T11:30:41.699910: step 4870, loss 0.5607.
Train: 2018-07-31T11:30:41.856126: step 4871, loss 0.592386.
Train: 2018-07-31T11:30:42.027992: step 4872, loss 0.58374.
Train: 2018-07-31T11:30:42.199794: step 4873, loss 0.531968.
Train: 2018-07-31T11:30:42.371630: step 4874, loss 0.557801.
Train: 2018-07-31T11:30:42.543488: step 4875, loss 0.609605.
Train: 2018-07-31T11:30:42.715328: step 4876, loss 0.592302.
Train: 2018-07-31T11:30:42.887134: step 4877, loss 0.670047.
Train: 2018-07-31T11:30:43.058969: step 4878, loss 0.583585.
Train: 2018-07-31T11:30:43.230803: step 4879, loss 0.566331.
Train: 2018-07-31T11:30:43.402638: step 4880, loss 0.531967.
Test: 2018-07-31T11:30:43.886929: step 4880, loss 0.560512.
Train: 2018-07-31T11:30:44.058768: step 4881, loss 0.540556.
Train: 2018-07-31T11:30:44.230570: step 4882, loss 0.566278.
Train: 2018-07-31T11:30:44.386783: step 4883, loss 0.523326.
Train: 2018-07-31T11:30:44.558617: step 4884, loss 0.574814.
Train: 2018-07-31T11:30:44.730452: step 4885, loss 0.523153.
Train: 2018-07-31T11:30:44.902287: step 4886, loss 0.514397.
Train: 2018-07-31T11:30:45.058501: step 4887, loss 0.574752.
Train: 2018-07-31T11:30:45.245956: step 4888, loss 0.592086.
Train: 2018-07-31T11:30:45.402170: step 4889, loss 0.548637.
Train: 2018-07-31T11:30:45.574004: step 4890, loss 0.583417.
Test: 2018-07-31T11:30:46.058266: step 4890, loss 0.560066.
Train: 2018-07-31T11:30:46.230131: step 4891, loss 0.592148.
Train: 2018-07-31T11:30:46.401936: step 4892, loss 0.644597.
Train: 2018-07-31T11:30:46.558173: step 4893, loss 0.688231.
Train: 2018-07-31T11:30:46.729984: step 4894, loss 0.496195.
Train: 2018-07-31T11:30:46.901848: step 4895, loss 0.539769.
Train: 2018-07-31T11:30:47.073654: step 4896, loss 0.59197.
Train: 2018-07-31T11:30:47.245488: step 4897, loss 0.565844.
Train: 2018-07-31T11:30:47.417322: step 4898, loss 0.635371.
Train: 2018-07-31T11:30:47.589188: step 4899, loss 0.58317.
Train: 2018-07-31T11:30:47.760992: step 4900, loss 0.600455.
Test: 2018-07-31T11:30:48.229662: step 4900, loss 0.559964.
Train: 2018-07-31T11:30:48.979456: step 4901, loss 0.565797.
Train: 2018-07-31T11:30:49.135670: step 4902, loss 0.522649.
Train: 2018-07-31T11:30:49.307505: step 4903, loss 0.608886.
Train: 2018-07-31T11:30:49.479339: step 4904, loss 0.617431.
Train: 2018-07-31T11:30:49.651199: step 4905, loss 0.574351.
Train: 2018-07-31T11:30:49.807418: step 4906, loss 0.57433.
Train: 2018-07-31T11:30:49.979223: step 4907, loss 0.617124.
Train: 2018-07-31T11:30:50.151088: step 4908, loss 0.60846.
Train: 2018-07-31T11:30:50.322923: step 4909, loss 0.591309.
Train: 2018-07-31T11:30:50.479132: step 4910, loss 0.557276.
Test: 2018-07-31T11:30:50.963397: step 4910, loss 0.560056.
Train: 2018-07-31T11:30:51.135202: step 4911, loss 0.565773.
Train: 2018-07-31T11:30:51.307061: step 4912, loss 0.591147.
Train: 2018-07-31T11:30:51.478871: step 4913, loss 0.557335.
Train: 2018-07-31T11:30:51.650738: step 4914, loss 0.565773.
Train: 2018-07-31T11:30:51.806950: step 4915, loss 0.473107.
Train: 2018-07-31T11:30:51.978754: step 4916, loss 0.523546.
Train: 2018-07-31T11:30:52.150614: step 4917, loss 0.540304.
Train: 2018-07-31T11:30:52.322424: step 4918, loss 0.58259.
Train: 2018-07-31T11:30:52.478637: step 4919, loss 0.599599.
Train: 2018-07-31T11:30:52.650473: step 4920, loss 0.48884.
Test: 2018-07-31T11:30:53.134764: step 4920, loss 0.559714.
Train: 2018-07-31T11:30:53.290947: step 4921, loss 0.574036.
Train: 2018-07-31T11:30:53.478433: step 4922, loss 0.574016.
Train: 2018-07-31T11:30:53.650238: step 4923, loss 0.548188.
Train: 2018-07-31T11:30:53.806481: step 4924, loss 0.625765.
Train: 2018-07-31T11:30:53.978316: step 4925, loss 0.565323.
Train: 2018-07-31T11:30:54.150121: step 4926, loss 0.582611.
Train: 2018-07-31T11:30:54.337576: step 4927, loss 0.599945.
Train: 2018-07-31T11:30:54.509412: step 4928, loss 0.547897.
Train: 2018-07-31T11:30:54.681247: step 4929, loss 0.582586.
Train: 2018-07-31T11:30:54.853107: step 4930, loss 0.556511.
Test: 2018-07-31T11:30:55.321721: step 4930, loss 0.559309.
Train: 2018-07-31T11:30:55.493556: step 4931, loss 0.591263.
Train: 2018-07-31T11:30:55.665391: step 4932, loss 0.60865.
Train: 2018-07-31T11:30:55.821605: step 4933, loss 0.573836.
Train: 2018-07-31T11:30:55.993439: step 4934, loss 0.573816.
Train: 2018-07-31T11:30:56.165275: step 4935, loss 0.547739.
Train: 2018-07-31T11:30:56.337110: step 4936, loss 0.539034.
Train: 2018-07-31T11:30:56.508968: step 4937, loss 0.547687.
Train: 2018-07-31T11:30:56.665157: step 4938, loss 0.556346.
Train: 2018-07-31T11:30:56.836992: step 4939, loss 0.538884.
Train: 2018-07-31T11:30:57.024480: step 4940, loss 0.61737.
Test: 2018-07-31T11:30:57.493118: step 4940, loss 0.559084.
Train: 2018-07-31T11:30:57.664923: step 4941, loss 0.617383.
Train: 2018-07-31T11:30:57.821166: step 4942, loss 0.564962.
Train: 2018-07-31T11:30:57.992970: step 4943, loss 0.530033.
Train: 2018-07-31T11:30:58.164805: step 4944, loss 0.57366.
Train: 2018-07-31T11:30:58.336671: step 4945, loss 0.573645.
Train: 2018-07-31T11:30:58.508509: step 4946, loss 0.564889.
Train: 2018-07-31T11:30:58.680340: step 4947, loss 0.66104.
Train: 2018-07-31T11:30:58.852175: step 4948, loss 0.599777.
Train: 2018-07-31T11:30:59.024012: step 4949, loss 0.608411.
Train: 2018-07-31T11:30:59.195844: step 4950, loss 0.564852.
Test: 2018-07-31T11:30:59.664455: step 4950, loss 0.559.
Train: 2018-07-31T11:30:59.836291: step 4951, loss 0.564849.
Train: 2018-07-31T11:31:00.008124: step 4952, loss 0.556192.
Train: 2018-07-31T11:31:00.179960: step 4953, loss 0.495713.
Train: 2018-07-31T11:31:00.351794: step 4954, loss 0.556176.
Train: 2018-07-31T11:31:00.508007: step 4955, loss 0.582097.
Train: 2018-07-31T11:31:00.679842: step 4956, loss 0.495563.
Train: 2018-07-31T11:31:00.851676: step 4957, loss 0.556083.
Train: 2018-07-31T11:31:01.023512: step 4958, loss 0.61684.
Train: 2018-07-31T11:31:01.179725: step 4959, loss 0.564702.
Train: 2018-07-31T11:31:01.367205: step 4960, loss 0.599486.
Test: 2018-07-31T11:31:01.835821: step 4960, loss 0.558792.
Train: 2018-07-31T11:31:01.992035: step 4961, loss 0.608179.
Train: 2018-07-31T11:31:02.163903: step 4962, loss 0.616837.
Train: 2018-07-31T11:31:02.335734: step 4963, loss 0.599382.
Train: 2018-07-31T11:31:02.507539: step 4964, loss 0.599309.
Train: 2018-07-31T11:31:02.679404: step 4965, loss 0.607873.
Train: 2018-07-31T11:31:02.851208: step 4966, loss 0.616374.
Train: 2018-07-31T11:31:03.023075: step 4967, loss 0.58184.
Train: 2018-07-31T11:31:03.194878: step 4968, loss 0.60748.
Train: 2018-07-31T11:31:03.366713: step 4969, loss 0.547634.
Train: 2018-07-31T11:31:03.522956: step 4970, loss 0.649753.
Test: 2018-07-31T11:31:04.007212: step 4970, loss 0.559021.
Train: 2018-07-31T11:31:04.163436: step 4971, loss 0.513921.
Train: 2018-07-31T11:31:04.335266: step 4972, loss 0.590095.
Train: 2018-07-31T11:31:04.507071: step 4973, loss 0.539481.
Train: 2018-07-31T11:31:04.678936: step 4974, loss 0.581436.
Train: 2018-07-31T11:31:04.866392: step 4975, loss 0.505942.
Train: 2018-07-31T11:31:05.038197: step 4976, loss 0.556352.
Train: 2018-07-31T11:31:05.194440: step 4977, loss 0.581573.
Train: 2018-07-31T11:31:05.366245: step 4978, loss 0.5305.
Train: 2018-07-31T11:31:05.538113: step 4979, loss 0.603351.
Train: 2018-07-31T11:31:05.709915: step 4980, loss 0.606009.
Test: 2018-07-31T11:31:06.178584: step 4980, loss 0.559117.
Train: 2018-07-31T11:31:06.350419: step 4981, loss 0.564381.
Train: 2018-07-31T11:31:06.522223: step 4982, loss 0.590256.
Train: 2018-07-31T11:31:06.678437: step 4983, loss 0.591516.
Train: 2018-07-31T11:31:06.850297: step 4984, loss 0.601599.
Train: 2018-07-31T11:31:07.022135: step 4985, loss 0.599199.
Train: 2018-07-31T11:31:07.178321: step 4986, loss 0.546755.
Train: 2018-07-31T11:31:07.350188: step 4987, loss 0.515471.
Train: 2018-07-31T11:31:07.521989: step 4988, loss 0.531954.
Train: 2018-07-31T11:31:07.693824: step 4989, loss 0.582667.
Train: 2018-07-31T11:31:07.865660: step 4990, loss 0.671407.
Test: 2018-07-31T11:31:08.345834: step 4990, loss 0.560101.
Train: 2018-07-31T11:31:08.502046: step 4991, loss 0.651384.
Train: 2018-07-31T11:31:08.673906: step 4992, loss 0.692489.
Train: 2018-07-31T11:31:08.845741: step 4993, loss 0.493551.
Train: 2018-07-31T11:31:09.033172: step 4994, loss 0.661568.
Train: 2018-07-31T11:31:09.189416: step 4995, loss 0.557275.
Train: 2018-07-31T11:31:09.361250: step 4996, loss 0.629407.
Train: 2018-07-31T11:31:09.533056: step 4997, loss 0.634738.
Train: 2018-07-31T11:31:09.704889: step 4998, loss 0.559416.
Train: 2018-07-31T11:31:09.876725: step 4999, loss 0.585384.
Train: 2018-07-31T11:31:10.048559: step 5000, loss 0.560829.
Test: 2018-07-31T11:31:10.517230: step 5000, loss 0.564469.
Train: 2018-07-31T11:31:11.220191: step 5001, loss 0.614089.
Train: 2018-07-31T11:31:11.392026: step 5002, loss 0.605406.
Train: 2018-07-31T11:31:11.548209: step 5003, loss 0.557347.
Train: 2018-07-31T11:31:11.720076: step 5004, loss 0.580331.
Train: 2018-07-31T11:31:11.891909: step 5005, loss 0.580887.
Train: 2018-07-31T11:31:12.063743: step 5006, loss 0.581287.
Train: 2018-07-31T11:31:12.219956: step 5007, loss 0.606776.
Train: 2018-07-31T11:31:12.391762: step 5008, loss 0.515521.
Train: 2018-07-31T11:31:12.563595: step 5009, loss 0.524178.
Train: 2018-07-31T11:31:12.735431: step 5010, loss 0.532731.
Test: 2018-07-31T11:31:13.219716: step 5010, loss 0.569146.
Train: 2018-07-31T11:31:13.391558: step 5011, loss 0.591604.
Train: 2018-07-31T11:31:13.563391: step 5012, loss 0.600273.
Train: 2018-07-31T11:31:13.735195: step 5013, loss 0.541379.
Train: 2018-07-31T11:31:13.891409: step 5014, loss 0.54982.
Train: 2018-07-31T11:31:14.063244: step 5015, loss 0.583757.
Train: 2018-07-31T11:31:14.235109: step 5016, loss 0.515517.
Train: 2018-07-31T11:31:14.406915: step 5017, loss 0.677987.
Train: 2018-07-31T11:31:14.578749: step 5018, loss 0.592235.
Train: 2018-07-31T11:31:14.750615: step 5019, loss 0.592137.
Train: 2018-07-31T11:31:14.922418: step 5020, loss 0.505887.
Test: 2018-07-31T11:31:15.391057: step 5020, loss 0.568765.
Train: 2018-07-31T11:31:15.562923: step 5021, loss 0.565957.
Train: 2018-07-31T11:31:15.734727: step 5022, loss 0.660985.
Train: 2018-07-31T11:31:15.906562: step 5023, loss 0.54817.
Train: 2018-07-31T11:31:16.078397: step 5024, loss 0.539226.
Train: 2018-07-31T11:31:16.250232: step 5025, loss 0.591105.
Train: 2018-07-31T11:31:16.406445: step 5026, loss 0.625748.
Train: 2018-07-31T11:31:16.578280: step 5027, loss 0.555798.
Train: 2018-07-31T11:31:16.750116: step 5028, loss 0.546819.
Train: 2018-07-31T11:31:16.921950: step 5029, loss 0.467893.
Train: 2018-07-31T11:31:17.093784: step 5030, loss 0.572522.
Test: 2018-07-31T11:31:17.562454: step 5030, loss 0.566354.
Train: 2018-07-31T11:31:17.749912: step 5031, loss 0.669079.
Train: 2018-07-31T11:31:17.906125: step 5032, loss 0.536843.
Train: 2018-07-31T11:31:18.093552: step 5033, loss 0.58951.
Train: 2018-07-31T11:31:18.265386: step 5034, loss 0.580496.
Train: 2018-07-31T11:31:18.437244: step 5035, loss 0.509573.
Train: 2018-07-31T11:31:18.609054: step 5036, loss 0.56242.
Train: 2018-07-31T11:31:18.780891: step 5037, loss 0.588861.
Train: 2018-07-31T11:31:18.952754: step 5038, loss 0.570931.
Train: 2018-07-31T11:31:19.124590: step 5039, loss 0.535144.
Train: 2018-07-31T11:31:19.280803: step 5040, loss 0.579536.
Test: 2018-07-31T11:31:19.749444: step 5040, loss 0.564433.
Train: 2018-07-31T11:31:19.921247: step 5041, loss 0.58834.
Train: 2018-07-31T11:31:20.093083: step 5042, loss 0.615058.
Train: 2018-07-31T11:31:20.249296: step 5043, loss 0.677557.
Train: 2018-07-31T11:31:20.436751: step 5044, loss 0.552188.
Train: 2018-07-31T11:31:20.608617: step 5045, loss 0.623413.
Train: 2018-07-31T11:31:20.764831: step 5046, loss 0.640947.
Train: 2018-07-31T11:31:20.936665: step 5047, loss 0.551952.
Train: 2018-07-31T11:31:21.108499: step 5048, loss 0.481196.
Train: 2018-07-31T11:31:21.264707: step 5049, loss 0.596403.
Train: 2018-07-31T11:31:21.436517: step 5050, loss 0.55175.
Test: 2018-07-31T11:31:21.905157: step 5050, loss 0.563453.
Train: 2018-07-31T11:31:22.076993: step 5051, loss 0.587027.
Train: 2018-07-31T11:31:22.248828: step 5052, loss 0.67513.
Train: 2018-07-31T11:31:22.420692: step 5053, loss 0.569568.
Train: 2018-07-31T11:31:22.592527: step 5054, loss 0.55217.
Train: 2018-07-31T11:31:22.764331: step 5055, loss 0.56106.
Train: 2018-07-31T11:31:22.936166: step 5056, loss 0.561176.
Train: 2018-07-31T11:31:23.108001: step 5057, loss 0.56127.
Train: 2018-07-31T11:31:23.279866: step 5058, loss 0.561335.
Train: 2018-07-31T11:31:23.451671: step 5059, loss 0.587508.
Train: 2018-07-31T11:31:23.623530: step 5060, loss 0.604922.
Test: 2018-07-31T11:31:24.092170: step 5060, loss 0.564201.
Train: 2018-07-31T11:31:24.263980: step 5061, loss 0.526576.
Train: 2018-07-31T11:31:24.435816: step 5062, loss 0.657019.
Train: 2018-07-31T11:31:24.607650: step 5063, loss 0.656814.
Train: 2018-07-31T11:31:24.779515: step 5064, loss 0.552568.
Train: 2018-07-31T11:31:24.951320: step 5065, loss 0.509294.
Train: 2018-07-31T11:31:25.123186: step 5066, loss 0.5783.
Train: 2018-07-31T11:31:25.295019: step 5067, loss 0.500528.
Train: 2018-07-31T11:31:25.466848: step 5068, loss 0.586657.
Train: 2018-07-31T11:31:25.623069: step 5069, loss 0.621078.
Train: 2018-07-31T11:31:25.794902: step 5070, loss 0.57772.
Test: 2018-07-31T11:31:26.279134: step 5070, loss 0.563103.
Train: 2018-07-31T11:31:26.450998: step 5071, loss 0.672555.
Train: 2018-07-31T11:31:26.622827: step 5072, loss 0.59464.
Train: 2018-07-31T11:31:26.794668: step 5073, loss 0.576787.
Train: 2018-07-31T11:31:26.950852: step 5074, loss 0.551396.
Train: 2018-07-31T11:31:27.122686: step 5075, loss 0.533563.
Train: 2018-07-31T11:31:27.294552: step 5076, loss 0.542351.
Train: 2018-07-31T11:31:27.466356: step 5077, loss 0.642612.
Train: 2018-07-31T11:31:27.638221: step 5078, loss 0.561033.
Train: 2018-07-31T11:31:27.810055: step 5079, loss 0.620884.
Train: 2018-07-31T11:31:27.981884: step 5080, loss 0.553109.
Test: 2018-07-31T11:31:28.450500: step 5080, loss 0.562157.
Train: 2018-07-31T11:31:28.622366: step 5081, loss 0.636153.
Train: 2018-07-31T11:31:28.794199: step 5082, loss 0.653139.
Train: 2018-07-31T11:31:28.966005: step 5083, loss 0.576613.
Train: 2018-07-31T11:31:29.137840: step 5084, loss 0.56827.
Train: 2018-07-31T11:31:29.294082: step 5085, loss 0.61906.
Train: 2018-07-31T11:31:29.465888: step 5086, loss 0.560138.
Train: 2018-07-31T11:31:29.637753: step 5087, loss 0.585889.
Train: 2018-07-31T11:31:29.809556: step 5088, loss 0.593922.
Train: 2018-07-31T11:31:29.965802: step 5089, loss 0.652498.
Train: 2018-07-31T11:31:30.137636: step 5090, loss 0.55232.
Test: 2018-07-31T11:31:30.621898: step 5090, loss 0.56345.
Train: 2018-07-31T11:31:30.778110: step 5091, loss 0.635465.
Train: 2018-07-31T11:31:30.949945: step 5092, loss 0.589834.
Train: 2018-07-31T11:31:31.121781: step 5093, loss 0.618474.
Train: 2018-07-31T11:31:31.293614: step 5094, loss 0.574511.
Train: 2018-07-31T11:31:31.465418: step 5095, loss 0.645424.
Train: 2018-07-31T11:31:31.637254: step 5096, loss 0.542997.
Train: 2018-07-31T11:31:31.809119: step 5097, loss 0.612704.
Train: 2018-07-31T11:31:31.980924: step 5098, loss 0.552536.
Train: 2018-07-31T11:31:32.152759: step 5099, loss 0.625549.
Train: 2018-07-31T11:31:32.324594: step 5100, loss 0.552894.
Test: 2018-07-31T11:31:32.808855: step 5100, loss 0.563938.
Train: 2018-07-31T11:31:33.589922: step 5101, loss 0.536158.
Train: 2018-07-31T11:31:33.761756: step 5102, loss 0.517402.
Train: 2018-07-31T11:31:33.933621: step 5103, loss 0.60013.
Train: 2018-07-31T11:31:34.105450: step 5104, loss 0.629028.
Train: 2018-07-31T11:31:34.277291: step 5105, loss 0.566871.
Train: 2018-07-31T11:31:34.449096: step 5106, loss 0.52769.
Train: 2018-07-31T11:31:34.620930: step 5107, loss 0.585774.
Train: 2018-07-31T11:31:34.792765: step 5108, loss 0.528698.
Train: 2018-07-31T11:31:34.964630: step 5109, loss 0.545005.
Train: 2018-07-31T11:31:35.136434: step 5110, loss 0.553069.
Test: 2018-07-31T11:31:35.620695: step 5110, loss 0.563437.
Train: 2018-07-31T11:31:35.792530: step 5111, loss 0.527492.
Train: 2018-07-31T11:31:35.964366: step 5112, loss 0.510541.
Train: 2018-07-31T11:31:36.136200: step 5113, loss 0.644577.
Train: 2018-07-31T11:31:36.308034: step 5114, loss 0.535174.
Train: 2018-07-31T11:31:36.479870: step 5115, loss 0.62821.
Train: 2018-07-31T11:31:36.651705: step 5116, loss 0.534834.
Train: 2018-07-31T11:31:36.823569: step 5117, loss 0.585862.
Train: 2018-07-31T11:31:36.995375: step 5118, loss 0.543055.
Train: 2018-07-31T11:31:37.167234: step 5119, loss 0.594451.
Train: 2018-07-31T11:31:37.339043: step 5120, loss 0.482435.
Test: 2018-07-31T11:31:37.823304: step 5120, loss 0.562678.
Train: 2018-07-31T11:31:37.979519: step 5121, loss 0.577175.
Train: 2018-07-31T11:31:38.151354: step 5122, loss 0.585812.
Train: 2018-07-31T11:31:38.323189: step 5123, loss 0.638103.
Train: 2018-07-31T11:31:38.510668: step 5124, loss 0.568256.
Train: 2018-07-31T11:31:38.682509: step 5125, loss 0.550667.
Train: 2018-07-31T11:31:38.854314: step 5126, loss 0.585601.
Train: 2018-07-31T11:31:39.026148: step 5127, loss 0.585529.
Train: 2018-07-31T11:31:39.198014: step 5128, loss 0.585449.
Train: 2018-07-31T11:31:39.369850: step 5129, loss 0.558985.
Train: 2018-07-31T11:31:39.557306: step 5130, loss 0.567677.
Test: 2018-07-31T11:31:40.041566: step 5130, loss 0.561635.
Train: 2018-07-31T11:31:40.213371: step 5131, loss 0.532355.
Train: 2018-07-31T11:31:40.385205: step 5132, loss 0.611565.
Train: 2018-07-31T11:31:40.557071: step 5133, loss 0.602664.
Train: 2018-07-31T11:31:40.713253: step 5134, loss 0.519649.
Train: 2018-07-31T11:31:40.900711: step 5135, loss 0.496564.
Train: 2018-07-31T11:31:41.056923: step 5136, loss 0.531697.
Train: 2018-07-31T11:31:41.228757: step 5137, loss 0.602463.
Train: 2018-07-31T11:31:41.400592: step 5138, loss 0.549121.
Train: 2018-07-31T11:31:41.572428: step 5139, loss 0.539896.
Train: 2018-07-31T11:31:41.728665: step 5140, loss 0.691667.
Test: 2018-07-31T11:31:42.212902: step 5140, loss 0.560785.
Train: 2018-07-31T11:31:42.384738: step 5141, loss 0.566803.
Train: 2018-07-31T11:31:42.556602: step 5142, loss 0.522326.
Train: 2018-07-31T11:31:42.728436: step 5143, loss 0.531349.
Train: 2018-07-31T11:31:42.900271: step 5144, loss 0.593953.
Train: 2018-07-31T11:31:43.072076: step 5145, loss 0.62091.
Train: 2018-07-31T11:31:43.243942: step 5146, loss 0.612081.
Train: 2018-07-31T11:31:43.431370: step 5147, loss 0.621048.
Train: 2018-07-31T11:31:43.603202: step 5148, loss 0.532074.
Train: 2018-07-31T11:31:43.759446: step 5149, loss 0.603249.
Train: 2018-07-31T11:31:43.946896: step 5150, loss 0.532311.
Test: 2018-07-31T11:31:44.415511: step 5150, loss 0.561815.
Train: 2018-07-31T11:31:44.587346: step 5151, loss 0.585501.
Train: 2018-07-31T11:31:44.759181: step 5152, loss 0.59432.
Train: 2018-07-31T11:31:44.931046: step 5153, loss 0.585424.
Train: 2018-07-31T11:31:45.102881: step 5154, loss 0.633512.
Train: 2018-07-31T11:31:45.274716: step 5155, loss 0.576524.
Train: 2018-07-31T11:31:45.446521: step 5156, loss 0.550438.
Train: 2018-07-31T11:31:45.618355: step 5157, loss 0.620868.
Train: 2018-07-31T11:31:45.774568: step 5158, loss 0.586308.
Train: 2018-07-31T11:31:45.946404: step 5159, loss 0.551986.
Train: 2018-07-31T11:31:46.118269: step 5160, loss 0.64813.
Test: 2018-07-31T11:31:46.602529: step 5160, loss 0.56465.
Train: 2018-07-31T11:31:46.774363: step 5161, loss 0.535857.
Train: 2018-07-31T11:31:46.946168: step 5162, loss 0.59692.
Train: 2018-07-31T11:31:47.118028: step 5163, loss 0.554214.
Train: 2018-07-31T11:31:47.274248: step 5164, loss 0.614889.
Train: 2018-07-31T11:31:47.446083: step 5165, loss 0.546416.
Train: 2018-07-31T11:31:47.617918: step 5166, loss 0.606763.
Train: 2018-07-31T11:31:47.789752: step 5167, loss 0.606906.
Train: 2018-07-31T11:31:47.945935: step 5168, loss 0.538591.
Train: 2018-07-31T11:31:48.117769: step 5169, loss 0.64118.
Train: 2018-07-31T11:31:48.289634: step 5170, loss 0.589926.
Test: 2018-07-31T11:31:48.773896: step 5170, loss 0.567116.
Train: 2018-07-31T11:31:48.945700: step 5171, loss 0.56435.
Train: 2018-07-31T11:31:49.117535: step 5172, loss 0.572783.
Train: 2018-07-31T11:31:49.289370: step 5173, loss 0.640554.
Train: 2018-07-31T11:31:49.461205: step 5174, loss 0.606405.
Train: 2018-07-31T11:31:49.633071: step 5175, loss 0.52229.
Train: 2018-07-31T11:31:49.804874: step 5176, loss 0.604169.
Train: 2018-07-31T11:31:49.961088: step 5177, loss 0.555212.
Train: 2018-07-31T11:31:50.132953: step 5178, loss 0.504515.
Train: 2018-07-31T11:31:50.304788: step 5179, loss 0.545679.
Train: 2018-07-31T11:31:50.492238: step 5180, loss 0.571406.
Test: 2018-07-31T11:31:50.960884: step 5180, loss 0.565627.
Train: 2018-07-31T11:31:51.132720: step 5181, loss 0.546119.
Train: 2018-07-31T11:31:51.304523: step 5182, loss 0.559973.
Train: 2018-07-31T11:31:51.476392: step 5183, loss 0.525806.
Train: 2018-07-31T11:31:51.632572: step 5184, loss 0.581504.
Train: 2018-07-31T11:31:51.804407: step 5185, loss 0.737261.
Train: 2018-07-31T11:31:51.976271: step 5186, loss 0.596449.
Train: 2018-07-31T11:31:52.148108: step 5187, loss 0.537002.
Train: 2018-07-31T11:31:52.319910: step 5188, loss 0.519471.
Train: 2018-07-31T11:31:52.476124: step 5189, loss 0.56279.
Train: 2018-07-31T11:31:52.647958: step 5190, loss 0.623774.
Test: 2018-07-31T11:31:53.132251: step 5190, loss 0.566551.
Train: 2018-07-31T11:31:53.304086: step 5191, loss 0.537737.
Train: 2018-07-31T11:31:53.475922: step 5192, loss 0.607677.
Train: 2018-07-31T11:31:53.663377: step 5193, loss 0.608254.
Train: 2018-07-31T11:31:53.835181: step 5194, loss 0.573939.
Train: 2018-07-31T11:31:54.007046: step 5195, loss 0.513364.
Train: 2018-07-31T11:31:54.178880: step 5196, loss 0.600954.
Train: 2018-07-31T11:31:54.350715: step 5197, loss 0.58381.
Train: 2018-07-31T11:31:54.506929: step 5198, loss 0.505242.
Train: 2018-07-31T11:31:54.678763: step 5199, loss 0.593054.
Train: 2018-07-31T11:31:54.850598: step 5200, loss 0.575612.
Test: 2018-07-31T11:31:55.334841: step 5200, loss 0.569729.
Train: 2018-07-31T11:31:56.037789: step 5201, loss 0.513962.
Train: 2018-07-31T11:31:56.209654: step 5202, loss 0.522631.
Train: 2018-07-31T11:31:56.381459: step 5203, loss 0.531271.
Train: 2018-07-31T11:31:56.553318: step 5204, loss 0.584492.
Train: 2018-07-31T11:31:56.709507: step 5205, loss 0.709677.
Train: 2018-07-31T11:31:56.896988: step 5206, loss 0.566373.
Train: 2018-07-31T11:31:57.068829: step 5207, loss 0.530358.
Train: 2018-07-31T11:31:57.225011: step 5208, loss 0.548046.
Train: 2018-07-31T11:31:57.412499: step 5209, loss 0.601725.
Train: 2018-07-31T11:31:57.584334: step 5210, loss 0.60153.
Test: 2018-07-31T11:31:58.052943: step 5210, loss 0.568221.
Train: 2018-07-31T11:31:58.224778: step 5211, loss 0.664325.
Train: 2018-07-31T11:31:58.396612: step 5212, loss 0.68193.
Train: 2018-07-31T11:31:58.568448: step 5213, loss 0.573726.
Train: 2018-07-31T11:31:58.724690: step 5214, loss 0.609263.
Train: 2018-07-31T11:31:58.896495: step 5215, loss 0.615648.
Train: 2018-07-31T11:31:59.083952: step 5216, loss 0.599631.
Train: 2018-07-31T11:31:59.240197: step 5217, loss 0.581626.
Train: 2018-07-31T11:31:59.412030: step 5218, loss 0.519805.
Train: 2018-07-31T11:31:59.599486: step 5219, loss 0.572338.
Train: 2018-07-31T11:31:59.771320: step 5220, loss 0.528711.
Test: 2018-07-31T11:32:00.239962: step 5220, loss 0.56599.
Train: 2018-07-31T11:32:00.411765: step 5221, loss 0.641859.
Train: 2018-07-31T11:32:00.583601: step 5222, loss 0.528045.
Train: 2018-07-31T11:32:00.755434: step 5223, loss 0.571471.
Train: 2018-07-31T11:32:00.927270: step 5224, loss 0.640962.
Train: 2018-07-31T11:32:01.099104: step 5225, loss 0.605853.
Train: 2018-07-31T11:32:01.286560: step 5226, loss 0.579568.
Train: 2018-07-31T11:32:01.442805: step 5227, loss 0.553411.
Train: 2018-07-31T11:32:01.614652: step 5228, loss 0.553265.
Train: 2018-07-31T11:32:01.786444: step 5229, loss 0.648079.
Train: 2018-07-31T11:32:01.958309: step 5230, loss 0.578686.
Test: 2018-07-31T11:32:02.442539: step 5230, loss 0.564276.
Train: 2018-07-31T11:32:02.614375: step 5231, loss 0.58727.
Train: 2018-07-31T11:32:02.786240: step 5232, loss 0.561357.
Train: 2018-07-31T11:32:02.958074: step 5233, loss 0.561231.
Train: 2018-07-31T11:32:03.129910: step 5234, loss 0.552544.
Train: 2018-07-31T11:32:03.286092: step 5235, loss 0.586635.
Train: 2018-07-31T11:32:03.457926: step 5236, loss 0.595047.
Train: 2018-07-31T11:32:03.629761: step 5237, loss 0.517996.
Train: 2018-07-31T11:32:03.801596: step 5238, loss 0.560587.
Train: 2018-07-31T11:32:03.973462: step 5239, loss 0.620359.
Train: 2018-07-31T11:32:04.145266: step 5240, loss 0.671605.
Test: 2018-07-31T11:32:04.613907: step 5240, loss 0.563036.
Train: 2018-07-31T11:32:04.785766: step 5241, loss 0.568799.
Train: 2018-07-31T11:32:04.957606: step 5242, loss 0.48344.
Train: 2018-07-31T11:32:05.129412: step 5243, loss 0.525941.
Train: 2018-07-31T11:32:05.301246: step 5244, loss 0.534302.
Train: 2018-07-31T11:32:05.473110: step 5245, loss 0.602628.
Train: 2018-07-31T11:32:05.644915: step 5246, loss 0.679789.
Train: 2018-07-31T11:32:05.816781: step 5247, loss 0.542455.
Train: 2018-07-31T11:32:05.988586: step 5248, loss 0.568086.
Train: 2018-07-31T11:32:06.160420: step 5249, loss 0.550848.
Train: 2018-07-31T11:32:06.332254: step 5250, loss 0.542163.
Test: 2018-07-31T11:32:06.816545: step 5250, loss 0.562019.
Train: 2018-07-31T11:32:06.988382: step 5251, loss 0.593601.
Train: 2018-07-31T11:32:07.160185: step 5252, loss 0.619345.
Train: 2018-07-31T11:32:07.316431: step 5253, loss 0.559056.
Train: 2018-07-31T11:32:07.488233: step 5254, loss 0.490152.
Train: 2018-07-31T11:32:07.660101: step 5255, loss 0.567493.
Train: 2018-07-31T11:32:07.831902: step 5256, loss 0.610595.
Train: 2018-07-31T11:32:08.003740: step 5257, loss 0.532747.
Train: 2018-07-31T11:32:08.175573: step 5258, loss 0.627883.
Train: 2018-07-31T11:32:08.347408: step 5259, loss 0.575849.
Train: 2018-07-31T11:32:08.550485: step 5260, loss 0.515103.
Test: 2018-07-31T11:32:09.034776: step 5260, loss 0.561185.
Train: 2018-07-31T11:32:09.206605: step 5261, loss 0.619136.
Train: 2018-07-31T11:32:09.378415: step 5262, loss 0.601722.
Train: 2018-07-31T11:32:09.534653: step 5263, loss 0.61034.
Train: 2018-07-31T11:32:09.706494: step 5264, loss 0.514809.
Train: 2018-07-31T11:32:09.878330: step 5265, loss 0.601519.
Train: 2018-07-31T11:32:10.050133: step 5266, loss 0.566748.
Train: 2018-07-31T11:32:10.221968: step 5267, loss 0.575368.
Train: 2018-07-31T11:32:10.393827: step 5268, loss 0.549291.
Train: 2018-07-31T11:32:10.550046: step 5269, loss 0.575259.
Train: 2018-07-31T11:32:10.721851: step 5270, loss 0.54917.
Test: 2018-07-31T11:32:11.206112: step 5270, loss 0.56061.
Train: 2018-07-31T11:32:11.377948: step 5271, loss 0.566471.
Train: 2018-07-31T11:32:11.534191: step 5272, loss 0.575106.
Train: 2018-07-31T11:32:11.705995: step 5273, loss 0.644629.
Train: 2018-07-31T11:32:11.877831: step 5274, loss 0.609757.
Train: 2018-07-31T11:32:12.049665: step 5275, loss 0.592299.
Train: 2018-07-31T11:32:12.221530: step 5276, loss 0.540266.
Train: 2018-07-31T11:32:12.393335: step 5277, loss 0.609444.
Train: 2018-07-31T11:32:12.565203: step 5278, loss 0.626605.
Train: 2018-07-31T11:32:12.737004: step 5279, loss 0.62642.
Train: 2018-07-31T11:32:12.908869: step 5280, loss 0.566119.
Test: 2018-07-31T11:32:13.377510: step 5280, loss 0.560326.
Train: 2018-07-31T11:32:13.549314: step 5281, loss 0.548986.
Train: 2018-07-31T11:32:13.721149: step 5282, loss 0.591691.
Train: 2018-07-31T11:32:13.893009: step 5283, loss 0.63421.
Train: 2018-07-31T11:32:14.064819: step 5284, loss 0.532076.
Train: 2018-07-31T11:32:14.221061: step 5285, loss 0.556418.
Train: 2018-07-31T11:32:14.392897: step 5286, loss 0.506751.
Train: 2018-07-31T11:32:14.564731: step 5287, loss 0.616748.
Train: 2018-07-31T11:32:14.720939: step 5288, loss 0.540531.
Train: 2018-07-31T11:32:14.892781: step 5289, loss 0.455839.
Train: 2018-07-31T11:32:15.064615: step 5290, loss 0.540334.
Test: 2018-07-31T11:32:15.548876: step 5290, loss 0.559977.
Train: 2018-07-31T11:32:15.720681: step 5291, loss 0.676448.
Train: 2018-07-31T11:32:15.876925: step 5292, loss 0.642398.
Train: 2018-07-31T11:32:16.048759: step 5293, loss 0.548605.
Train: 2018-07-31T11:32:16.220594: step 5294, loss 0.548567.
Train: 2018-07-31T11:32:16.392398: step 5295, loss 0.565562.
Train: 2018-07-31T11:32:16.548642: step 5296, loss 0.582578.
Train: 2018-07-31T11:32:16.720447: step 5297, loss 0.616685.
Train: 2018-07-31T11:32:16.892311: step 5298, loss 0.573974.
Train: 2018-07-31T11:32:17.064116: step 5299, loss 0.625108.
Train: 2018-07-31T11:32:17.235952: step 5300, loss 0.548355.
Test: 2018-07-31T11:32:17.720212: step 5300, loss 0.559617.
Train: 2018-07-31T11:32:18.454417: step 5301, loss 0.590892.
Train: 2018-07-31T11:32:18.626251: step 5302, loss 0.607849.
Train: 2018-07-31T11:32:18.782494: step 5303, loss 0.556825.
Train: 2018-07-31T11:32:18.954298: step 5304, loss 0.531364.
Train: 2018-07-31T11:32:19.126164: step 5305, loss 0.556774.
Train: 2018-07-31T11:32:19.297998: step 5306, loss 0.565217.
Train: 2018-07-31T11:32:19.469802: step 5307, loss 0.57367.
Train: 2018-07-31T11:32:19.626053: step 5308, loss 0.633127.
Train: 2018-07-31T11:32:19.797851: step 5309, loss 0.624565.
Train: 2018-07-31T11:32:19.969709: step 5310, loss 0.590535.
Test: 2018-07-31T11:32:20.453948: step 5310, loss 0.559372.
Train: 2018-07-31T11:32:20.672676: step 5311, loss 0.556619.
Train: 2018-07-31T11:32:20.844481: step 5312, loss 0.632706.
Train: 2018-07-31T11:32:21.016316: step 5313, loss 0.590369.
Train: 2018-07-31T11:32:21.188150: step 5314, loss 0.590308.
Train: 2018-07-31T11:32:21.360017: step 5315, loss 0.607041.
Train: 2018-07-31T11:32:21.516229: step 5316, loss 0.531573.
Train: 2018-07-31T11:32:21.688033: step 5317, loss 0.598499.
Train: 2018-07-31T11:32:21.859867: step 5318, loss 0.531666.
Train: 2018-07-31T11:32:22.031702: step 5319, loss 0.573371.
Train: 2018-07-31T11:32:22.187946: step 5320, loss 0.573343.
Test: 2018-07-31T11:32:22.672208: step 5320, loss 0.559351.
Train: 2018-07-31T11:32:22.844013: step 5321, loss 0.489936.
Train: 2018-07-31T11:32:23.000226: step 5322, loss 0.564922.
Train: 2018-07-31T11:32:23.172061: step 5323, loss 0.589976.
Train: 2018-07-31T11:32:23.343920: step 5324, loss 0.623497.
Train: 2018-07-31T11:32:23.515760: step 5325, loss 0.57317.
Train: 2018-07-31T11:32:23.687566: step 5326, loss 0.547971.
Train: 2018-07-31T11:32:23.859424: step 5327, loss 0.55631.
Train: 2018-07-31T11:32:24.031259: step 5328, loss 0.581484.
Train: 2018-07-31T11:32:24.203069: step 5329, loss 0.5562.
Train: 2018-07-31T11:32:24.359314: step 5330, loss 0.513972.
Test: 2018-07-31T11:32:24.827953: step 5330, loss 0.558809.
Train: 2018-07-31T11:32:24.999794: step 5331, loss 0.564516.
Train: 2018-07-31T11:32:25.171593: step 5332, loss 0.513578.
Train: 2018-07-31T11:32:25.327807: step 5333, loss 0.521831.
Train: 2018-07-31T11:32:25.499672: step 5334, loss 0.632729.
Train: 2018-07-31T11:32:25.671506: step 5335, loss 0.555697.
Train: 2018-07-31T11:32:25.827719: step 5336, loss 0.590021.
Train: 2018-07-31T11:32:26.015175: step 5337, loss 0.61589.
Train: 2018-07-31T11:32:26.186980: step 5338, loss 0.641795.
Train: 2018-07-31T11:32:26.358846: step 5339, loss 0.54688.
Train: 2018-07-31T11:32:26.546301: step 5340, loss 0.555478.
Test: 2018-07-31T11:32:27.014941: step 5340, loss 0.558253.
Train: 2018-07-31T11:32:27.186770: step 5341, loss 0.546821.
Train: 2018-07-31T11:32:27.358580: step 5342, loss 0.693556.
Train: 2018-07-31T11:32:27.530445: step 5343, loss 0.538177.
Train: 2018-07-31T11:32:27.702280: step 5344, loss 0.650126.
Train: 2018-07-31T11:32:27.874085: step 5345, loss 0.538243.
Train: 2018-07-31T11:32:28.030328: step 5346, loss 0.546848.
Train: 2018-07-31T11:32:28.202157: step 5347, loss 0.563986.
Train: 2018-07-31T11:32:28.373999: step 5348, loss 0.589664.
Train: 2018-07-31T11:32:28.545833: step 5349, loss 0.563952.
Train: 2018-07-31T11:32:28.717661: step 5350, loss 0.469847.
Test: 2018-07-31T11:32:29.186302: step 5350, loss 0.558116.
Train: 2018-07-31T11:32:29.358114: step 5351, loss 0.606736.
Train: 2018-07-31T11:32:29.529948: step 5352, loss 0.546719.
Train: 2018-07-31T11:32:29.701812: step 5353, loss 0.529494.
Train: 2018-07-31T11:32:29.857996: step 5354, loss 0.598207.
Train: 2018-07-31T11:32:30.029860: step 5355, loss 0.555151.
Train: 2018-07-31T11:32:30.201696: step 5356, loss 0.529227.
Train: 2018-07-31T11:32:30.373523: step 5357, loss 0.589631.
Train: 2018-07-31T11:32:30.545334: step 5358, loss 0.58964.
Train: 2018-07-31T11:32:30.717169: step 5359, loss 0.563636.
Train: 2018-07-31T11:32:30.904656: step 5360, loss 0.546254.
Test: 2018-07-31T11:32:31.373265: step 5360, loss 0.557716.
Train: 2018-07-31T11:32:31.545124: step 5361, loss 0.598336.
Train: 2018-07-31T11:32:31.701314: step 5362, loss 0.667897.
Train: 2018-07-31T11:32:31.888770: step 5363, loss 0.563542.
Train: 2018-07-31T11:32:32.045013: step 5364, loss 0.554856.
Train: 2018-07-31T11:32:32.216818: step 5365, loss 0.580846.
Train: 2018-07-31T11:32:32.388682: step 5366, loss 0.563499.
Train: 2018-07-31T11:32:32.560517: step 5367, loss 0.511564.
Train: 2018-07-31T11:32:32.732322: step 5368, loss 0.554801.
Train: 2018-07-31T11:32:32.904157: step 5369, loss 0.563434.
Train: 2018-07-31T11:32:33.060394: step 5370, loss 0.580757.
Test: 2018-07-31T11:32:33.544662: step 5370, loss 0.55753.
Train: 2018-07-31T11:32:33.716496: step 5371, loss 0.580744.
Train: 2018-07-31T11:32:33.872681: step 5372, loss 0.606773.
Train: 2018-07-31T11:32:34.044514: step 5373, loss 0.624089.
Train: 2018-07-31T11:32:34.216362: step 5374, loss 0.53735.
Train: 2018-07-31T11:32:34.372563: step 5375, loss 0.537355.
Train: 2018-07-31T11:32:34.544428: step 5376, loss 0.597935.
Train: 2018-07-31T11:32:34.716262: step 5377, loss 0.554641.
Train: 2018-07-31T11:32:34.888068: step 5378, loss 0.528676.
Train: 2018-07-31T11:32:35.059901: step 5379, loss 0.615187.
Train: 2018-07-31T11:32:35.231736: step 5380, loss 0.554584.
Test: 2018-07-31T11:32:35.700377: step 5380, loss 0.55738.
Train: 2018-07-31T11:32:35.856591: step 5381, loss 0.571872.
Train: 2018-07-31T11:32:36.044077: step 5382, loss 0.519938.
Train: 2018-07-31T11:32:36.215912: step 5383, loss 0.476563.
Train: 2018-07-31T11:32:36.387716: step 5384, loss 0.597885.
Train: 2018-07-31T11:32:36.559580: step 5385, loss 0.632736.
Train: 2018-07-31T11:32:36.715765: step 5386, loss 0.536973.
Train: 2018-07-31T11:32:36.887598: step 5387, loss 0.56307.
Train: 2018-07-31T11:32:37.059434: step 5388, loss 0.475801.
Train: 2018-07-31T11:32:37.215677: step 5389, loss 0.571767.
Train: 2018-07-31T11:32:37.387512: step 5390, loss 0.510369.
Test: 2018-07-31T11:32:37.871744: step 5390, loss 0.557026.
Train: 2018-07-31T11:32:38.043609: step 5391, loss 0.589362.
Train: 2018-07-31T11:32:38.199791: step 5392, loss 0.598226.
Train: 2018-07-31T11:32:38.371657: step 5393, loss 0.545249.
Train: 2018-07-31T11:32:38.543491: step 5394, loss 0.624862.
Train: 2018-07-31T11:32:38.715296: step 5395, loss 0.536317.
Train: 2018-07-31T11:32:38.887131: step 5396, loss 0.57173.
Train: 2018-07-31T11:32:39.058996: step 5397, loss 0.553979.
Train: 2018-07-31T11:32:39.230832: step 5398, loss 0.633849.
Train: 2018-07-31T11:32:39.402635: step 5399, loss 0.545073.
Train: 2018-07-31T11:32:39.574470: step 5400, loss 0.527322.
Test: 2018-07-31T11:32:40.043135: step 5400, loss 0.556796.
Train: 2018-07-31T11:32:40.761717: step 5401, loss 0.651526.
Train: 2018-07-31T11:32:40.933558: step 5402, loss 0.598221.
Train: 2018-07-31T11:32:41.105393: step 5403, loss 0.58045.
Train: 2018-07-31T11:32:41.277196: step 5404, loss 0.589233.
Train: 2018-07-31T11:32:41.449061: step 5405, loss 0.562733.
Train: 2018-07-31T11:32:41.605245: step 5406, loss 0.536346.
Train: 2018-07-31T11:32:41.777109: step 5407, loss 0.589058.
Train: 2018-07-31T11:32:41.948944: step 5408, loss 0.545165.
Train: 2018-07-31T11:32:42.120779: step 5409, loss 0.536411.
Train: 2018-07-31T11:32:42.292613: step 5410, loss 0.615223.
Test: 2018-07-31T11:32:42.761224: step 5410, loss 0.556764.
Train: 2018-07-31T11:32:42.933058: step 5411, loss 0.562666.
Train: 2018-07-31T11:32:43.104892: step 5412, loss 0.58887.
Train: 2018-07-31T11:32:43.276752: step 5413, loss 0.527741.
Train: 2018-07-31T11:32:43.432972: step 5414, loss 0.623694.
Train: 2018-07-31T11:32:43.604776: step 5415, loss 0.597466.
Train: 2018-07-31T11:32:43.776641: step 5416, loss 0.545238.
Train: 2018-07-31T11:32:43.948469: step 5417, loss 0.614702.
Train: 2018-07-31T11:32:44.104696: step 5418, loss 0.597265.
Train: 2018-07-31T11:32:44.276493: step 5419, loss 0.614452.
Train: 2018-07-31T11:32:44.448328: step 5420, loss 0.570288.
Test: 2018-07-31T11:32:44.932615: step 5420, loss 0.557291.
Train: 2018-07-31T11:32:45.088834: step 5421, loss 0.630944.
Train: 2018-07-31T11:32:45.260663: step 5422, loss 0.579876.
Train: 2018-07-31T11:32:45.432473: step 5423, loss 0.605634.
Train: 2018-07-31T11:32:45.604308: step 5424, loss 0.494624.
Train: 2018-07-31T11:32:45.776173: step 5425, loss 0.580241.
Train: 2018-07-31T11:32:45.932387: step 5426, loss 0.529194.
Train: 2018-07-31T11:32:46.104222: step 5427, loss 0.529367.
Train: 2018-07-31T11:32:46.276056: step 5428, loss 0.59796.
Train: 2018-07-31T11:32:46.447860: step 5429, loss 0.606734.
Train: 2018-07-31T11:32:46.619695: step 5430, loss 0.572653.
Test: 2018-07-31T11:32:47.088334: step 5430, loss 0.55847.
Train: 2018-07-31T11:32:47.260169: step 5431, loss 0.598506.
Train: 2018-07-31T11:32:47.432004: step 5432, loss 0.598634.
Train: 2018-07-31T11:32:47.603870: step 5433, loss 0.632934.
Train: 2018-07-31T11:32:47.760080: step 5434, loss 0.521989.
Train: 2018-07-31T11:32:47.931887: step 5435, loss 0.522116.
Train: 2018-07-31T11:32:48.088100: step 5436, loss 0.536943.
Train: 2018-07-31T11:32:48.259966: step 5437, loss 0.598989.
Train: 2018-07-31T11:32:48.431771: step 5438, loss 0.573405.
Train: 2018-07-31T11:32:48.603605: step 5439, loss 0.556327.
Train: 2018-07-31T11:32:48.775440: step 5440, loss 0.522101.
Test: 2018-07-31T11:32:49.259731: step 5440, loss 0.559057.
Train: 2018-07-31T11:32:49.431561: step 5441, loss 0.564839.
Train: 2018-07-31T11:32:49.587779: step 5442, loss 0.564802.
Train: 2018-07-31T11:32:49.759609: step 5443, loss 0.573356.
Train: 2018-07-31T11:32:49.931449: step 5444, loss 0.553931.
Train: 2018-07-31T11:32:50.103283: step 5445, loss 0.657562.
Train: 2018-07-31T11:32:50.275088: step 5446, loss 0.607734.
Train: 2018-07-31T11:32:50.446924: step 5447, loss 0.504223.
Train: 2018-07-31T11:32:50.618783: step 5448, loss 0.547244.
Train: 2018-07-31T11:32:50.790601: step 5449, loss 0.521266.
Train: 2018-07-31T11:32:50.946837: step 5450, loss 0.607177.
Test: 2018-07-31T11:32:51.415477: step 5450, loss 0.558463.
Train: 2018-07-31T11:32:51.587309: step 5451, loss 0.529674.
Train: 2018-07-31T11:32:51.759116: step 5452, loss 0.546996.
Train: 2018-07-31T11:32:51.930981: step 5453, loss 0.625289.
Train: 2018-07-31T11:32:52.118406: step 5454, loss 0.555881.
Train: 2018-07-31T11:32:52.274621: step 5455, loss 0.573445.
Train: 2018-07-31T11:32:52.446456: step 5456, loss 0.574542.
Train: 2018-07-31T11:32:52.618289: step 5457, loss 0.608686.
Train: 2018-07-31T11:32:52.790124: step 5458, loss 0.504228.
Train: 2018-07-31T11:32:52.946338: step 5459, loss 0.547997.
Train: 2018-07-31T11:32:53.133794: step 5460, loss 0.60061.
Test: 2018-07-31T11:32:53.602435: step 5460, loss 0.559886.
Train: 2018-07-31T11:32:53.774299: step 5461, loss 0.618297.
Train: 2018-07-31T11:32:53.946134: step 5462, loss 0.530936.
Train: 2018-07-31T11:32:54.117970: step 5463, loss 0.574792.
Train: 2018-07-31T11:32:54.289799: step 5464, loss 0.601145.
Train: 2018-07-31T11:32:54.461609: step 5465, loss 0.653714.
Train: 2018-07-31T11:32:54.633444: step 5466, loss 0.636106.
Train: 2018-07-31T11:32:54.805279: step 5467, loss 0.679453.
Train: 2018-07-31T11:32:54.977144: step 5468, loss 0.626903.
Train: 2018-07-31T11:32:55.148978: step 5469, loss 0.635175.
Train: 2018-07-31T11:32:55.305191: step 5470, loss 0.600486.
Test: 2018-07-31T11:32:55.789453: step 5470, loss 0.560493.
Train: 2018-07-31T11:32:55.961287: step 5471, loss 0.532094.
Train: 2018-07-31T11:32:56.117471: step 5472, loss 0.64239.
Train: 2018-07-31T11:32:56.289306: step 5473, loss 0.52921.
Train: 2018-07-31T11:32:56.461140: step 5474, loss 0.534482.
Train: 2018-07-31T11:32:56.633005: step 5475, loss 0.557757.
Train: 2018-07-31T11:32:56.804834: step 5476, loss 0.616262.
Train: 2018-07-31T11:32:56.976676: step 5477, loss 0.591277.
Train: 2018-07-31T11:32:57.148479: step 5478, loss 0.541522.
Train: 2018-07-31T11:32:57.320344: step 5479, loss 0.48362.
Train: 2018-07-31T11:32:57.476528: step 5480, loss 0.558399.
Test: 2018-07-31T11:32:57.960819: step 5480, loss 0.56119.
Train: 2018-07-31T11:32:58.117027: step 5481, loss 0.50857.
Train: 2018-07-31T11:32:58.288862: step 5482, loss 0.583552.
Train: 2018-07-31T11:32:58.460702: step 5483, loss 0.619684.
Train: 2018-07-31T11:32:58.632537: step 5484, loss 0.592097.
Train: 2018-07-31T11:32:58.804342: step 5485, loss 0.600574.
Train: 2018-07-31T11:32:58.960585: step 5486, loss 0.62584.
Train: 2018-07-31T11:32:59.132420: step 5487, loss 0.541966.
Train: 2018-07-31T11:32:59.304224: step 5488, loss 0.634331.
Train: 2018-07-31T11:32:59.476083: step 5489, loss 0.58398.
Train: 2018-07-31T11:32:59.647895: step 5490, loss 0.617502.
Test: 2018-07-31T11:33:00.116535: step 5490, loss 0.561564.
Train: 2018-07-31T11:33:00.288371: step 5491, loss 0.500261.
Train: 2018-07-31T11:33:00.460234: step 5492, loss 0.592277.
Train: 2018-07-31T11:33:00.647684: step 5493, loss 0.575467.
Train: 2018-07-31T11:33:00.819519: step 5494, loss 0.575383.
Train: 2018-07-31T11:33:00.975740: step 5495, loss 0.533392.
Train: 2018-07-31T11:33:01.147573: step 5496, loss 0.600328.
Train: 2018-07-31T11:33:01.319378: step 5497, loss 0.54145.
Train: 2018-07-31T11:33:01.491213: step 5498, loss 0.57489.
Train: 2018-07-31T11:33:01.663077: step 5499, loss 0.549479.
Train: 2018-07-31T11:33:01.819290: step 5500, loss 0.574585.
Test: 2018-07-31T11:33:02.287899: step 5500, loss 0.560271.
Train: 2018-07-31T11:33:03.006483: step 5501, loss 0.54907.
Train: 2018-07-31T11:33:03.178317: step 5502, loss 0.531908.
Train: 2018-07-31T11:33:03.350151: step 5503, loss 0.599601.
Train: 2018-07-31T11:33:03.521986: step 5504, loss 0.642084.
Train: 2018-07-31T11:33:03.693852: step 5505, loss 0.582324.
Train: 2018-07-31T11:33:03.865681: step 5506, loss 0.53956.
Train: 2018-07-31T11:33:04.021870: step 5507, loss 0.522315.
Train: 2018-07-31T11:33:04.193734: step 5508, loss 0.52207.
Train: 2018-07-31T11:33:04.365539: step 5509, loss 0.590401.
Train: 2018-07-31T11:33:04.537404: step 5510, loss 0.538736.
Test: 2018-07-31T11:33:05.021635: step 5510, loss 0.55857.
Train: 2018-07-31T11:33:05.193471: step 5511, loss 0.555766.
Train: 2018-07-31T11:33:05.365336: step 5512, loss 0.503737.
Train: 2018-07-31T11:33:05.537140: step 5513, loss 0.529405.
Train: 2018-07-31T11:33:05.693384: step 5514, loss 0.546565.
Train: 2018-07-31T11:33:05.865187: step 5515, loss 0.5814.
Train: 2018-07-31T11:33:06.052674: step 5516, loss 0.607728.
Train: 2018-07-31T11:33:06.224478: step 5517, loss 0.642988.
Train: 2018-07-31T11:33:06.380692: step 5518, loss 0.598883.
Train: 2018-07-31T11:33:06.552557: step 5519, loss 0.554732.
Train: 2018-07-31T11:33:06.724362: step 5520, loss 0.56348.
Test: 2018-07-31T11:33:07.193002: step 5520, loss 0.557466.
Train: 2018-07-31T11:33:07.364868: step 5521, loss 0.554598.
Train: 2018-07-31T11:33:07.536702: step 5522, loss 0.678049.
Train: 2018-07-31T11:33:07.708507: step 5523, loss 0.49286.
Train: 2018-07-31T11:33:07.864746: step 5524, loss 0.589667.
Train: 2018-07-31T11:33:08.036580: step 5525, loss 0.607198.
Train: 2018-07-31T11:33:08.208414: step 5526, loss 0.598309.
Train: 2018-07-31T11:33:08.380254: step 5527, loss 0.5719.
Train: 2018-07-31T11:33:08.552083: step 5528, loss 0.633113.
Train: 2018-07-31T11:33:08.708302: step 5529, loss 0.502002.
Train: 2018-07-31T11:33:08.880138: step 5530, loss 0.605711.
Test: 2018-07-31T11:33:09.364367: step 5530, loss 0.557157.
Train: 2018-07-31T11:33:09.536233: step 5531, loss 0.580416.
Train: 2018-07-31T11:33:09.692418: step 5532, loss 0.554332.
Train: 2018-07-31T11:33:09.864252: step 5533, loss 0.554332.
Train: 2018-07-31T11:33:10.036087: step 5534, loss 0.571635.
Train: 2018-07-31T11:33:10.207946: step 5535, loss 0.553878.
Train: 2018-07-31T11:33:10.379786: step 5536, loss 0.531706.
Train: 2018-07-31T11:33:10.551591: step 5537, loss 0.511155.
Train: 2018-07-31T11:33:10.707833: step 5538, loss 0.545864.
Train: 2018-07-31T11:33:10.879668: step 5539, loss 0.546052.
Train: 2018-07-31T11:33:11.051499: step 5540, loss 0.572357.
Test: 2018-07-31T11:33:11.535768: step 5540, loss 0.558068.
Train: 2018-07-31T11:33:11.707594: step 5541, loss 0.677187.
Train: 2018-07-31T11:33:11.879404: step 5542, loss 0.607799.
Train: 2018-07-31T11:33:12.051239: step 5543, loss 0.616714.
Train: 2018-07-31T11:33:12.223074: step 5544, loss 0.61689.
Train: 2018-07-31T11:33:12.379318: step 5545, loss 0.547782.
Train: 2018-07-31T11:33:12.551147: step 5546, loss 0.548052.
Train: 2018-07-31T11:33:12.722988: step 5547, loss 0.522404.
Train: 2018-07-31T11:33:12.879201: step 5548, loss 0.582933.
Train: 2018-07-31T11:33:13.066657: step 5549, loss 0.539959.
Train: 2018-07-31T11:33:13.238461: step 5550, loss 0.591787.
Test: 2018-07-31T11:33:13.707131: step 5550, loss 0.560169.
Train: 2018-07-31T11:33:13.878966: step 5551, loss 0.531494.
Train: 2018-07-31T11:33:14.050771: step 5552, loss 0.583286.
Train: 2018-07-31T11:33:14.222606: step 5553, loss 0.531497.
Train: 2018-07-31T11:33:14.394441: step 5554, loss 0.531434.
Train: 2018-07-31T11:33:14.566301: step 5555, loss 0.539988.
Train: 2018-07-31T11:33:14.722514: step 5556, loss 0.548549.
Train: 2018-07-31T11:33:14.894323: step 5557, loss 0.52229.
Train: 2018-07-31T11:33:15.066158: step 5558, loss 0.504562.
Train: 2018-07-31T11:33:15.253644: step 5559, loss 0.521747.
Train: 2018-07-31T11:33:15.409858: step 5560, loss 0.539068.
Test: 2018-07-31T11:33:15.894119: step 5560, loss 0.559445.
Train: 2018-07-31T11:33:16.065925: step 5561, loss 0.636384.
Train: 2018-07-31T11:33:16.222169: step 5562, loss 0.6098.
Train: 2018-07-31T11:33:16.393972: step 5563, loss 0.52063.
Train: 2018-07-31T11:33:16.565838: step 5564, loss 0.627638.
Train: 2018-07-31T11:33:16.737673: step 5565, loss 0.591797.
Train: 2018-07-31T11:33:16.909476: step 5566, loss 0.627486.
Train: 2018-07-31T11:33:17.065720: step 5567, loss 0.546821.
Train: 2018-07-31T11:33:17.237556: step 5568, loss 0.591398.
Train: 2018-07-31T11:33:17.409390: step 5569, loss 0.582317.
Train: 2018-07-31T11:33:17.565572: step 5570, loss 0.555413.
Test: 2018-07-31T11:33:18.049864: step 5570, loss 0.5582.
Train: 2018-07-31T11:33:18.221701: step 5571, loss 0.65333.
Train: 2018-07-31T11:33:18.377913: step 5572, loss 0.519652.
Train: 2018-07-31T11:33:18.549717: step 5573, loss 0.599486.
Train: 2018-07-31T11:33:18.721584: step 5574, loss 0.625882.
Train: 2018-07-31T11:33:18.893387: step 5575, loss 0.554939.
Train: 2018-07-31T11:33:19.065221: step 5576, loss 0.616531.
Train: 2018-07-31T11:33:19.237058: step 5577, loss 0.537238.
Train: 2018-07-31T11:33:19.408921: step 5578, loss 0.572254.
Train: 2018-07-31T11:33:19.580727: step 5579, loss 0.589634.
Train: 2018-07-31T11:33:19.752560: step 5580, loss 0.589492.
Test: 2018-07-31T11:33:20.221232: step 5580, loss 0.557393.
Train: 2018-07-31T11:33:20.393037: step 5581, loss 0.528473.
Train: 2018-07-31T11:33:20.549280: step 5582, loss 0.563189.
Train: 2018-07-31T11:33:20.736737: step 5583, loss 0.580464.
Train: 2018-07-31T11:33:20.908541: step 5584, loss 0.597696.
Train: 2018-07-31T11:33:21.064784: step 5585, loss 0.580285.
Train: 2018-07-31T11:33:21.236620: step 5586, loss 0.537056.
Train: 2018-07-31T11:33:21.392832: step 5587, loss 0.645061.
Train: 2018-07-31T11:33:21.549040: step 5588, loss 0.562843.
Train: 2018-07-31T11:33:21.720851: step 5589, loss 0.579964.
Train: 2018-07-31T11:33:21.892685: step 5590, loss 0.494257.
Test: 2018-07-31T11:33:22.376947: step 5590, loss 0.556937.
Train: 2018-07-31T11:33:22.533190: step 5591, loss 0.605524.
Train: 2018-07-31T11:33:22.705025: step 5592, loss 0.588336.
Train: 2018-07-31T11:33:22.876860: step 5593, loss 0.554091.
Train: 2018-07-31T11:33:23.048689: step 5594, loss 0.528441.
Train: 2018-07-31T11:33:23.220530: step 5595, loss 0.494215.
Train: 2018-07-31T11:33:23.407954: step 5596, loss 0.596728.
Train: 2018-07-31T11:33:23.579820: step 5597, loss 0.639571.
Train: 2018-07-31T11:33:23.736003: step 5598, loss 0.553847.
Train: 2018-07-31T11:33:23.907869: step 5599, loss 0.579519.
Train: 2018-07-31T11:33:24.079673: step 5600, loss 0.570925.
Test: 2018-07-31T11:33:24.548313: step 5600, loss 0.556555.
Train: 2018-07-31T11:33:25.282517: step 5601, loss 0.562335.
Train: 2018-07-31T11:33:25.454380: step 5602, loss 0.528044.
Train: 2018-07-31T11:33:25.610595: step 5603, loss 0.630879.
Train: 2018-07-31T11:33:25.782429: step 5604, loss 0.527971.
Train: 2018-07-31T11:33:25.954234: step 5605, loss 0.605116.
Train: 2018-07-31T11:33:26.126102: step 5606, loss 0.579361.
Train: 2018-07-31T11:33:26.297933: step 5607, loss 0.59648.
Train: 2018-07-31T11:33:26.454148: step 5608, loss 0.536482.
Train: 2018-07-31T11:33:26.625981: step 5609, loss 0.545026.
Train: 2018-07-31T11:33:26.797786: step 5610, loss 0.579265.
Test: 2018-07-31T11:33:27.266456: step 5610, loss 0.556319.
Train: 2018-07-31T11:33:27.438260: step 5611, loss 0.536389.
Train: 2018-07-31T11:33:27.610096: step 5612, loss 0.587811.
Train: 2018-07-31T11:33:27.781931: step 5613, loss 0.510539.
Train: 2018-07-31T11:33:27.953795: step 5614, loss 0.570609.
Train: 2018-07-31T11:33:28.125601: step 5615, loss 0.510299.
Train: 2018-07-31T11:33:28.281839: step 5616, loss 0.518757.
Train: 2018-07-31T11:33:28.453678: step 5617, loss 0.466573.
Train: 2018-07-31T11:33:28.625482: step 5618, loss 0.527001.
Train: 2018-07-31T11:33:28.797348: step 5619, loss 0.605589.
Train: 2018-07-31T11:33:28.969152: step 5620, loss 0.614529.
Test: 2018-07-31T11:33:29.437824: step 5620, loss 0.55579.
Train: 2018-07-31T11:33:29.609658: step 5621, loss 0.605827.
Train: 2018-07-31T11:33:29.781492: step 5622, loss 0.57055.
Train: 2018-07-31T11:33:29.953328: step 5623, loss 0.552858.
Train: 2018-07-31T11:33:30.125162: step 5624, loss 0.561681.
Train: 2018-07-31T11:33:30.296998: step 5625, loss 0.543934.
Train: 2018-07-31T11:33:30.468826: step 5626, loss 0.561646.
Train: 2018-07-31T11:33:30.640636: step 5627, loss 0.55274.
Train: 2018-07-31T11:33:30.812471: step 5628, loss 0.54381.
Train: 2018-07-31T11:33:30.968716: step 5629, loss 0.534849.
Train: 2018-07-31T11:33:31.140544: step 5630, loss 0.624128.
Test: 2018-07-31T11:33:31.624811: step 5630, loss 0.555541.
Train: 2018-07-31T11:33:31.796641: step 5631, loss 0.642026.
Train: 2018-07-31T11:33:31.968450: step 5632, loss 0.499053.
Train: 2018-07-31T11:33:32.124664: step 5633, loss 0.570476.
Train: 2018-07-31T11:33:32.296529: step 5634, loss 0.516876.
Train: 2018-07-31T11:33:32.468333: step 5635, loss 0.55258.
Train: 2018-07-31T11:33:32.640168: step 5636, loss 0.534665.
Train: 2018-07-31T11:33:32.812002: step 5637, loss 0.480862.
Train: 2018-07-31T11:33:32.983869: step 5638, loss 0.705229.
Train: 2018-07-31T11:33:33.155672: step 5639, loss 0.588416.
Train: 2018-07-31T11:33:33.327537: step 5640, loss 0.651187.
Test: 2018-07-31T11:33:33.827389: step 5640, loss 0.555404.
Train: 2018-07-31T11:33:33.983628: step 5641, loss 0.561443.
Train: 2018-07-31T11:33:34.155468: step 5642, loss 0.579287.
Train: 2018-07-31T11:33:34.327272: step 5643, loss 0.534695.
Train: 2018-07-31T11:33:34.514729: step 5644, loss 0.516945.
Train: 2018-07-31T11:33:34.670943: step 5645, loss 0.579165.
Train: 2018-07-31T11:33:34.842777: step 5646, loss 0.632404.
Train: 2018-07-31T11:33:35.014636: step 5647, loss 0.587946.
Train: 2018-07-31T11:33:35.186447: step 5648, loss 0.596708.
Train: 2018-07-31T11:33:35.358282: step 5649, loss 0.57017.
Train: 2018-07-31T11:33:35.530146: step 5650, loss 0.543785.
Test: 2018-07-31T11:33:36.014408: step 5650, loss 0.555437.
Train: 2018-07-31T11:33:36.186212: step 5651, loss 0.578887.
Train: 2018-07-31T11:33:36.342457: step 5652, loss 0.570098.
Train: 2018-07-31T11:33:36.514291: step 5653, loss 0.535156.
Train: 2018-07-31T11:33:36.686129: step 5654, loss 0.517743.
Train: 2018-07-31T11:33:36.857930: step 5655, loss 0.631085.
Train: 2018-07-31T11:33:37.029795: step 5656, loss 0.604862.
Train: 2018-07-31T11:33:37.201631: step 5657, loss 0.55264.
Train: 2018-07-31T11:33:37.357844: step 5658, loss 0.552652.
Train: 2018-07-31T11:33:37.529678: step 5659, loss 0.552658.
Train: 2018-07-31T11:33:37.717128: step 5660, loss 0.569977.
Test: 2018-07-31T11:33:38.201366: step 5660, loss 0.555472.
Train: 2018-07-31T11:33:38.373200: step 5661, loss 0.57862.
Train: 2018-07-31T11:33:38.545036: step 5662, loss 0.587247.
Train: 2018-07-31T11:33:38.701274: step 5663, loss 0.518117.
Train: 2018-07-31T11:33:38.888705: step 5664, loss 0.492193.
Train: 2018-07-31T11:33:39.044919: step 5665, loss 0.535317.
Train: 2018-07-31T11:33:39.232399: step 5666, loss 0.561247.
Train: 2018-07-31T11:33:39.388618: step 5667, loss 0.578601.
Train: 2018-07-31T11:33:39.560422: step 5668, loss 0.517696.
Train: 2018-07-31T11:33:39.732288: step 5669, loss 0.552459.
Train: 2018-07-31T11:33:39.904122: step 5670, loss 0.499947.
Test: 2018-07-31T11:33:40.388384: step 5670, loss 0.555216.
Train: 2018-07-31T11:33:40.560219: step 5671, loss 0.587466.
Train: 2018-07-31T11:33:40.747674: step 5672, loss 0.561119.
Train: 2018-07-31T11:33:40.903858: step 5673, loss 0.569925.
Train: 2018-07-31T11:33:41.075693: step 5674, loss 0.614134.
Train: 2018-07-31T11:33:41.247558: step 5675, loss 0.561077.
Train: 2018-07-31T11:33:41.403771: step 5676, loss 0.552209.
Train: 2018-07-31T11:33:41.575606: step 5677, loss 0.623106.
Train: 2018-07-31T11:33:41.747440: step 5678, loss 0.5256.
Train: 2018-07-31T11:33:41.919274: step 5679, loss 0.720603.
Train: 2018-07-31T11:33:42.091079: step 5680, loss 0.569869.
Test: 2018-07-31T11:33:42.559720: step 5680, loss 0.555076.
Train: 2018-07-31T11:33:42.731580: step 5681, loss 0.65801.
Train: 2018-07-31T11:33:42.903420: step 5682, loss 0.54347.
Train: 2018-07-31T11:33:43.075254: step 5683, loss 0.56978.
Train: 2018-07-31T11:33:43.247090: step 5684, loss 0.526154.
Train: 2018-07-31T11:33:43.403273: step 5685, loss 0.648062.
Train: 2018-07-31T11:33:43.575138: step 5686, loss 0.552377.
Train: 2018-07-31T11:33:43.746943: step 5687, loss 0.612936.
Train: 2018-07-31T11:33:43.918807: step 5688, loss 0.56969.
Train: 2018-07-31T11:33:44.074990: step 5689, loss 0.569678.
Train: 2018-07-31T11:33:44.246854: step 5690, loss 0.578232.
Test: 2018-07-31T11:33:44.731117: step 5690, loss 0.555362.
Train: 2018-07-31T11:33:44.902921: step 5691, loss 0.586739.
Train: 2018-07-31T11:33:45.074757: step 5692, loss 0.552629.
Train: 2018-07-31T11:33:45.246590: step 5693, loss 0.527177.
Train: 2018-07-31T11:33:45.418426: step 5694, loss 0.595103.
Train: 2018-07-31T11:33:45.590260: step 5695, loss 0.620484.
Train: 2018-07-31T11:33:45.762095: step 5696, loss 0.527364.
Train: 2018-07-31T11:33:45.933930: step 5697, loss 0.569631.
Train: 2018-07-31T11:33:46.105765: step 5698, loss 0.55275.
Train: 2018-07-31T11:33:46.277599: step 5699, loss 0.527447.
Train: 2018-07-31T11:33:46.433844: step 5700, loss 0.620245.
Test: 2018-07-31T11:33:46.933697: step 5700, loss 0.55547.
Train: 2018-07-31T11:33:47.652277: step 5701, loss 0.535854.
Train: 2018-07-31T11:33:47.824113: step 5702, loss 0.535828.
Train: 2018-07-31T11:33:47.995978: step 5703, loss 0.628725.
Train: 2018-07-31T11:33:48.167812: step 5704, loss 0.611811.
Train: 2018-07-31T11:33:48.339642: step 5705, loss 0.535797.
Train: 2018-07-31T11:33:48.511451: step 5706, loss 0.577992.
Train: 2018-07-31T11:33:48.683316: step 5707, loss 0.561104.
Train: 2018-07-31T11:33:48.855153: step 5708, loss 0.611734.
Train: 2018-07-31T11:33:49.026986: step 5709, loss 0.594829.
Train: 2018-07-31T11:33:49.183199: step 5710, loss 0.54425.
Test: 2018-07-31T11:33:49.667460: step 5710, loss 0.555413.
Train: 2018-07-31T11:33:49.839297: step 5711, loss 0.586358.
Train: 2018-07-31T11:33:50.011130: step 5712, loss 0.56951.
Train: 2018-07-31T11:33:50.182934: step 5713, loss 0.611562.
Train: 2018-07-31T11:33:50.354769: step 5714, loss 0.687118.
Train: 2018-07-31T11:33:50.526605: step 5715, loss 0.57788.
Train: 2018-07-31T11:33:50.698463: step 5716, loss 0.502777.
Train: 2018-07-31T11:33:50.870273: step 5717, loss 0.63617.
Train: 2018-07-31T11:33:51.042108: step 5718, loss 0.52798.
Train: 2018-07-31T11:33:51.198322: step 5719, loss 0.486537.
Train: 2018-07-31T11:33:51.370156: step 5720, loss 0.694115.
Test: 2018-07-31T11:33:51.854450: step 5720, loss 0.555632.
Train: 2018-07-31T11:33:52.010663: step 5721, loss 0.561226.
Train: 2018-07-31T11:33:52.182466: step 5722, loss 0.569519.
Train: 2018-07-31T11:33:52.354301: step 5723, loss 0.619154.
Train: 2018-07-31T11:33:52.526166: step 5724, loss 0.569525.
Train: 2018-07-31T11:33:52.697971: step 5725, loss 0.553038.
Train: 2018-07-31T11:33:52.869806: step 5726, loss 0.553049.
Train: 2018-07-31T11:33:53.026020: step 5727, loss 0.569519.
Train: 2018-07-31T11:33:53.197854: step 5728, loss 0.618943.
Train: 2018-07-31T11:33:53.385336: step 5729, loss 0.594205.
Train: 2018-07-31T11:33:53.541554: step 5730, loss 0.577733.
Test: 2018-07-31T11:33:54.025784: step 5730, loss 0.555754.
Train: 2018-07-31T11:33:54.181999: step 5731, loss 0.577726.
Train: 2018-07-31T11:33:54.353864: step 5732, loss 0.520266.
Train: 2018-07-31T11:33:54.541319: step 5733, loss 0.561285.
Train: 2018-07-31T11:33:54.713124: step 5734, loss 0.553041.
Train: 2018-07-31T11:33:54.884958: step 5735, loss 0.577691.
Train: 2018-07-31T11:33:55.056824: step 5736, loss 0.528229.
Train: 2018-07-31T11:33:55.228628: step 5737, loss 0.594195.
Train: 2018-07-31T11:33:55.384872: step 5738, loss 0.51643.
Train: 2018-07-31T11:33:55.556707: step 5739, loss 0.536165.
Train: 2018-07-31T11:33:55.728511: step 5740, loss 0.544344.
Test: 2018-07-31T11:33:56.197152: step 5740, loss 0.555295.
Train: 2018-07-31T11:33:56.368986: step 5741, loss 0.577657.
Train: 2018-07-31T11:33:56.540853: step 5742, loss 0.57766.
Train: 2018-07-31T11:33:56.712682: step 5743, loss 0.544008.
Train: 2018-07-31T11:33:56.884491: step 5744, loss 0.552345.
Train: 2018-07-31T11:33:57.056326: step 5745, loss 0.603092.
Train: 2018-07-31T11:33:57.228160: step 5746, loss 0.586177.
Train: 2018-07-31T11:33:57.384374: step 5747, loss 0.518144.
Train: 2018-07-31T11:33:57.556208: step 5748, loss 0.611822.
Train: 2018-07-31T11:33:57.743695: step 5749, loss 0.552073.
Train: 2018-07-31T11:33:57.915531: step 5750, loss 0.594831.
Test: 2018-07-31T11:33:58.384138: step 5750, loss 0.554792.
Train: 2018-07-31T11:33:58.555974: step 5751, loss 0.48345.
Train: 2018-07-31T11:33:58.727840: step 5752, loss 0.543355.
Train: 2018-07-31T11:33:58.899673: step 5753, loss 0.54327.
Train: 2018-07-31T11:33:59.071512: step 5754, loss 0.517238.
Train: 2018-07-31T11:33:59.243314: step 5755, loss 0.569128.
Train: 2018-07-31T11:33:59.415149: step 5756, loss 0.560418.
Train: 2018-07-31T11:33:59.587014: step 5757, loss 0.595362.
Train: 2018-07-31T11:33:59.758818: step 5758, loss 0.612948.
Train: 2018-07-31T11:33:59.915032: step 5759, loss 0.577908.
Train: 2018-07-31T11:34:00.086891: step 5760, loss 0.630573.
Test: 2018-07-31T11:34:00.586749: step 5760, loss 0.554432.
Train: 2018-07-31T11:34:00.758584: step 5761, loss 0.516497.
Train: 2018-07-31T11:34:00.914828: step 5762, loss 0.621755.
Train: 2018-07-31T11:34:01.086631: step 5763, loss 0.560337.
Train: 2018-07-31T11:34:01.258491: step 5764, loss 0.604127.
Train: 2018-07-31T11:34:01.430302: step 5765, loss 0.656535.
Train: 2018-07-31T11:34:01.602137: step 5766, loss 0.560338.
Train: 2018-07-31T11:34:01.774000: step 5767, loss 0.473411.
Train: 2018-07-31T11:34:01.945806: step 5768, loss 0.551653.
Train: 2018-07-31T11:34:02.102050: step 5769, loss 0.551649.
Train: 2018-07-31T11:34:02.273884: step 5770, loss 0.55164.
Test: 2018-07-31T11:34:02.758146: step 5770, loss 0.554453.
Train: 2018-07-31T11:34:02.929982: step 5771, loss 0.569007.
Train: 2018-07-31T11:34:03.101784: step 5772, loss 0.64723.
Train: 2018-07-31T11:34:03.273620: step 5773, loss 0.542949.
Train: 2018-07-31T11:34:03.429863: step 5774, loss 0.577651.
Train: 2018-07-31T11:34:03.601692: step 5775, loss 0.55164.
Train: 2018-07-31T11:34:03.773503: step 5776, loss 0.594939.
Train: 2018-07-31T11:34:03.945338: step 5777, loss 0.5084.
Train: 2018-07-31T11:34:04.117171: step 5778, loss 0.603552.
Train: 2018-07-31T11:34:04.289037: step 5779, loss 0.577582.
Train: 2018-07-31T11:34:04.445250: step 5780, loss 0.594851.
Test: 2018-07-31T11:34:04.929482: step 5780, loss 0.554462.
Train: 2018-07-31T11:34:05.101343: step 5781, loss 0.568917.
Train: 2018-07-31T11:34:05.273151: step 5782, loss 0.629257.
Train: 2018-07-31T11:34:05.444985: step 5783, loss 0.603304.
Train: 2018-07-31T11:34:05.616851: step 5784, loss 0.526005.
Train: 2018-07-31T11:34:05.788655: step 5785, loss 0.526068.
Train: 2018-07-31T11:34:05.944869: step 5786, loss 0.568876.
Train: 2018-07-31T11:34:06.116733: step 5787, loss 0.594532.
Train: 2018-07-31T11:34:06.288538: step 5788, loss 0.534679.
Train: 2018-07-31T11:34:06.460372: step 5789, loss 0.5774.
Train: 2018-07-31T11:34:06.632238: step 5790, loss 0.594476.
Test: 2018-07-31T11:34:07.116468: step 5790, loss 0.554546.
Train: 2018-07-31T11:34:07.288329: step 5791, loss 0.577378.
Train: 2018-07-31T11:34:07.444548: step 5792, loss 0.568836.
Train: 2018-07-31T11:34:07.616353: step 5793, loss 0.55179.
Train: 2018-07-31T11:34:07.788187: step 5794, loss 0.543263.
Train: 2018-07-31T11:34:07.975675: step 5795, loss 0.594383.
Train: 2018-07-31T11:34:08.147478: step 5796, loss 0.526204.
Train: 2018-07-31T11:34:08.319314: step 5797, loss 0.509109.
Train: 2018-07-31T11:34:08.506769: step 5798, loss 0.526069.
Train: 2018-07-31T11:34:08.689644: step 5799, loss 0.594485.
Train: 2018-07-31T11:34:08.861447: step 5800, loss 0.491519.
Test: 2018-07-31T11:34:09.345739: step 5800, loss 0.554345.
Train: 2018-07-31T11:34:10.048671: step 5801, loss 0.525706.
Train: 2018-07-31T11:34:10.220504: step 5802, loss 0.525536.
Train: 2018-07-31T11:34:10.392341: step 5803, loss 0.612191.
Train: 2018-07-31T11:34:10.548583: step 5804, loss 0.586193.
Train: 2018-07-31T11:34:10.720423: step 5805, loss 0.560046.
Train: 2018-07-31T11:34:10.892253: step 5806, loss 0.525041.
Train: 2018-07-31T11:34:11.064058: step 5807, loss 0.621406.
Train: 2018-07-31T11:34:11.220300: step 5808, loss 0.507313.
Train: 2018-07-31T11:34:11.392136: step 5809, loss 0.621593.
Train: 2018-07-31T11:34:11.563971: step 5810, loss 0.533553.
Test: 2018-07-31T11:34:12.048232: step 5810, loss 0.554016.
Train: 2018-07-31T11:34:12.220061: step 5811, loss 0.559967.
Train: 2018-07-31T11:34:12.391871: step 5812, loss 0.577619.
Train: 2018-07-31T11:34:12.563706: step 5813, loss 0.533435.
Train: 2018-07-31T11:34:12.719919: step 5814, loss 0.489146.
Train: 2018-07-31T11:34:12.891784: step 5815, loss 0.568802.
Train: 2018-07-31T11:34:13.063590: step 5816, loss 0.595487.
Train: 2018-07-31T11:34:13.235455: step 5817, loss 0.559914.
Train: 2018-07-31T11:34:13.407258: step 5818, loss 0.506429.
Train: 2018-07-31T11:34:13.563471: step 5819, loss 0.595627.
Train: 2018-07-31T11:34:13.735338: step 5820, loss 0.604603.
Test: 2018-07-31T11:34:14.219568: step 5820, loss 0.553854.
Train: 2018-07-31T11:34:14.391434: step 5821, loss 0.586717.
Train: 2018-07-31T11:34:14.547641: step 5822, loss 0.559881.
Train: 2018-07-31T11:34:14.719451: step 5823, loss 0.58668.
Train: 2018-07-31T11:34:14.891316: step 5824, loss 0.577721.
Train: 2018-07-31T11:34:15.063151: step 5825, loss 0.568776.
Train: 2018-07-31T11:34:15.234955: step 5826, loss 0.54204.
Train: 2018-07-31T11:34:15.391193: step 5827, loss 0.559845.
Train: 2018-07-31T11:34:15.578626: step 5828, loss 0.577625.
Train: 2018-07-31T11:34:15.750490: step 5829, loss 0.524294.
Train: 2018-07-31T11:34:15.922319: step 5830, loss 0.630892.
Test: 2018-07-31T11:34:16.390965: step 5830, loss 0.553835.
Train: 2018-07-31T11:34:16.562769: step 5831, loss 0.542081.
Train: 2018-07-31T11:34:16.734634: step 5832, loss 0.621839.
Train: 2018-07-31T11:34:16.890842: step 5833, loss 0.524444.
Train: 2018-07-31T11:34:17.062682: step 5834, loss 0.515649.
Train: 2018-07-31T11:34:17.234488: step 5835, loss 0.621612.
Train: 2018-07-31T11:34:17.406322: step 5836, loss 0.595071.
Train: 2018-07-31T11:34:17.562565: step 5837, loss 0.691829.
Train: 2018-07-31T11:34:17.734400: step 5838, loss 0.472155.
Train: 2018-07-31T11:34:17.906235: step 5839, loss 0.542309.
Train: 2018-07-31T11:34:18.078041: step 5840, loss 0.568538.
Test: 2018-07-31T11:34:18.562331: step 5840, loss 0.553915.
Train: 2018-07-31T11:34:18.734166: step 5841, loss 0.53363.
Train: 2018-07-31T11:34:18.905970: step 5842, loss 0.551079.
Train: 2018-07-31T11:34:19.077805: step 5843, loss 0.524934.
Train: 2018-07-31T11:34:19.234018: step 5844, loss 0.5598.
Train: 2018-07-31T11:34:19.421505: step 5845, loss 0.542412.
Train: 2018-07-31T11:34:19.593335: step 5846, loss 0.542515.
Train: 2018-07-31T11:34:19.765145: step 5847, loss 0.626974.
Train: 2018-07-31T11:34:19.937010: step 5848, loss 0.644264.
Train: 2018-07-31T11:34:20.108844: step 5849, loss 0.50787.
Train: 2018-07-31T11:34:20.280679: step 5850, loss 0.560269.
Test: 2018-07-31T11:34:20.749319: step 5850, loss 0.554446.
Train: 2018-07-31T11:34:20.967987: step 5851, loss 0.551598.
Train: 2018-07-31T11:34:21.155474: step 5852, loss 0.534142.
Train: 2018-07-31T11:34:21.327279: step 5853, loss 0.595668.
Train: 2018-07-31T11:34:21.499143: step 5854, loss 0.609861.
Train: 2018-07-31T11:34:21.655351: step 5855, loss 0.609056.
Train: 2018-07-31T11:34:21.827163: step 5856, loss 0.622283.
Train: 2018-07-31T11:34:21.998997: step 5857, loss 0.587088.
Train: 2018-07-31T11:34:22.170832: step 5858, loss 0.569938.
Train: 2018-07-31T11:34:22.342666: step 5859, loss 0.569413.
Train: 2018-07-31T11:34:22.514500: step 5860, loss 0.578102.
Test: 2018-07-31T11:34:22.998763: step 5860, loss 0.55589.
Train: 2018-07-31T11:34:23.170598: step 5861, loss 0.520087.
Train: 2018-07-31T11:34:23.342431: step 5862, loss 0.631444.
Train: 2018-07-31T11:34:23.514296: step 5863, loss 0.648564.
Train: 2018-07-31T11:34:23.686132: step 5864, loss 0.570561.
Train: 2018-07-31T11:34:23.857967: step 5865, loss 0.596984.
Train: 2018-07-31T11:34:24.029796: step 5866, loss 0.588037.
Train: 2018-07-31T11:34:24.201606: step 5867, loss 0.630714.
Train: 2018-07-31T11:34:24.373441: step 5868, loss 0.569297.
Train: 2018-07-31T11:34:24.545277: step 5869, loss 0.537128.
Train: 2018-07-31T11:34:24.732761: step 5870, loss 0.554111.
Test: 2018-07-31T11:34:25.201402: step 5870, loss 0.556909.
Train: 2018-07-31T11:34:25.388827: step 5871, loss 0.528888.
Train: 2018-07-31T11:34:25.560696: step 5872, loss 0.590834.
Train: 2018-07-31T11:34:25.732528: step 5873, loss 0.587847.
Train: 2018-07-31T11:34:25.904332: step 5874, loss 0.562648.
Train: 2018-07-31T11:34:26.060546: step 5875, loss 0.571634.
Train: 2018-07-31T11:34:26.248001: step 5876, loss 0.554784.
Train: 2018-07-31T11:34:26.404216: step 5877, loss 0.596374.
Train: 2018-07-31T11:34:26.591695: step 5878, loss 0.604801.
Train: 2018-07-31T11:34:26.763536: step 5879, loss 0.62995.
Train: 2018-07-31T11:34:26.935370: step 5880, loss 0.638652.
Test: 2018-07-31T11:34:27.403981: step 5880, loss 0.557451.
Train: 2018-07-31T11:34:27.591461: step 5881, loss 0.579744.
Train: 2018-07-31T11:34:27.747686: step 5882, loss 0.521668.
Train: 2018-07-31T11:34:27.919485: step 5883, loss 0.604905.
Train: 2018-07-31T11:34:28.091319: step 5884, loss 0.513894.
Train: 2018-07-31T11:34:28.263187: step 5885, loss 0.555515.
Train: 2018-07-31T11:34:28.419367: step 5886, loss 0.57228.
Train: 2018-07-31T11:34:28.606855: step 5887, loss 0.529146.
Train: 2018-07-31T11:34:28.794281: step 5888, loss 0.589263.
Train: 2018-07-31T11:34:28.950494: step 5889, loss 0.590567.
Train: 2018-07-31T11:34:29.122329: step 5890, loss 0.533421.
Test: 2018-07-31T11:34:29.590998: step 5890, loss 0.559126.
Train: 2018-07-31T11:34:29.778454: step 5891, loss 0.588885.
Train: 2018-07-31T11:34:29.934671: step 5892, loss 0.524295.
Train: 2018-07-31T11:34:30.106474: step 5893, loss 0.51314.
Train: 2018-07-31T11:34:30.278338: step 5894, loss 0.647017.
Train: 2018-07-31T11:34:30.450175: step 5895, loss 0.627981.
Train: 2018-07-31T11:34:30.621977: step 5896, loss 0.624342.
Train: 2018-07-31T11:34:30.793842: step 5897, loss 0.590885.
Train: 2018-07-31T11:34:30.965671: step 5898, loss 0.573382.
Train: 2018-07-31T11:34:31.121861: step 5899, loss 0.600101.
Train: 2018-07-31T11:34:31.293694: step 5900, loss 0.583808.
Test: 2018-07-31T11:34:31.777957: step 5900, loss 0.561939.
Train: 2018-07-31T11:34:32.527811: step 5901, loss 0.609636.
Train: 2018-07-31T11:34:32.699646: step 5902, loss 0.543093.
Train: 2018-07-31T11:34:32.871451: step 5903, loss 0.650213.
Train: 2018-07-31T11:34:33.043315: step 5904, loss 0.477391.
Train: 2018-07-31T11:34:33.215119: step 5905, loss 0.540935.
Train: 2018-07-31T11:34:33.386954: step 5906, loss 0.630005.
Train: 2018-07-31T11:34:33.543199: step 5907, loss 0.597254.
Train: 2018-07-31T11:34:33.715004: step 5908, loss 0.556008.
Train: 2018-07-31T11:34:33.886838: step 5909, loss 0.548335.
Train: 2018-07-31T11:34:34.058672: step 5910, loss 0.624972.
Test: 2018-07-31T11:34:34.542934: step 5910, loss 0.569215.
Train: 2018-07-31T11:34:34.699148: step 5911, loss 0.541116.
Train: 2018-07-31T11:34:34.886603: step 5912, loss 0.58388.
Train: 2018-07-31T11:34:35.042818: step 5913, loss 0.550463.
Train: 2018-07-31T11:34:35.230285: step 5914, loss 0.584597.
Train: 2018-07-31T11:34:35.386517: step 5915, loss 0.559263.
Train: 2018-07-31T11:34:35.558346: step 5916, loss 0.542247.
Train: 2018-07-31T11:34:35.730187: step 5917, loss 0.584953.
Train: 2018-07-31T11:34:35.901990: step 5918, loss 0.567735.
Train: 2018-07-31T11:34:36.073856: step 5919, loss 0.550348.
Train: 2018-07-31T11:34:36.245660: step 5920, loss 0.601895.
Test: 2018-07-31T11:34:36.729922: step 5920, loss 0.569881.
Train: 2018-07-31T11:34:36.901790: step 5921, loss 0.584387.
Train: 2018-07-31T11:34:37.073617: step 5922, loss 1.32345.
Train: 2018-07-31T11:34:37.245451: step 5923, loss 0.574327.
Train: 2018-07-31T11:34:37.432883: step 5924, loss 0.591256.
Train: 2018-07-31T11:34:37.604747: step 5925, loss 0.564389.
Train: 2018-07-31T11:34:37.776577: step 5926, loss 0.564172.
Train: 2018-07-31T11:34:37.948386: step 5927, loss 0.582024.
Train: 2018-07-31T11:34:38.120222: step 5928, loss 0.555153.
Train: 2018-07-31T11:34:38.307678: step 5929, loss 0.582319.
Train: 2018-07-31T11:34:38.463921: step 5930, loss 0.53718.
Test: 2018-07-31T11:34:38.948182: step 5930, loss 0.567478.
Train: 2018-07-31T11:34:39.135609: step 5931, loss 0.573619.
Train: 2018-07-31T11:34:39.307474: step 5932, loss 0.58291.
Train: 2018-07-31T11:34:39.479279: step 5933, loss 0.592225.
Train: 2018-07-31T11:34:39.651114: step 5934, loss 0.59236.
Train: 2018-07-31T11:34:39.822981: step 5935, loss 0.60163.
Train: 2018-07-31T11:34:40.010440: step 5936, loss 0.545992.
Train: 2018-07-31T11:34:40.182269: step 5937, loss 0.638459.
Train: 2018-07-31T11:34:40.354103: step 5938, loss 0.546441.
Train: 2018-07-31T11:34:40.510317: step 5939, loss 0.684213.
Train: 2018-07-31T11:34:40.697767: step 5940, loss 0.580505.
Test: 2018-07-31T11:34:41.166413: step 5940, loss 0.567452.
Train: 2018-07-31T11:34:41.338248: step 5941, loss 0.573685.
Train: 2018-07-31T11:34:41.510052: step 5942, loss 0.546631.
Train: 2018-07-31T11:34:41.681887: step 5943, loss 0.600807.
Train: 2018-07-31T11:34:41.853752: step 5944, loss 0.55469.
Train: 2018-07-31T11:34:42.025587: step 5945, loss 0.536463.
Train: 2018-07-31T11:34:42.197392: step 5946, loss 0.526997.
Train: 2018-07-31T11:34:42.384847: step 5947, loss 0.599589.
Train: 2018-07-31T11:34:42.556713: step 5948, loss 0.581212.
Train: 2018-07-31T11:34:42.728516: step 5949, loss 0.635582.
Train: 2018-07-31T11:34:42.900383: step 5950, loss 0.598984.
Test: 2018-07-31T11:34:43.369022: step 5950, loss 0.565456.
Train: 2018-07-31T11:34:43.540857: step 5951, loss 0.526288.
Train: 2018-07-31T11:34:43.712691: step 5952, loss 0.553284.
Train: 2018-07-31T11:34:43.884496: step 5953, loss 0.525986.
Train: 2018-07-31T11:34:44.056361: step 5954, loss 0.57097.
Train: 2018-07-31T11:34:44.228192: step 5955, loss 0.579795.
Train: 2018-07-31T11:34:44.400031: step 5956, loss 0.642826.
Train: 2018-07-31T11:34:44.571836: step 5957, loss 0.615413.
Train: 2018-07-31T11:34:44.743670: step 5958, loss 0.615068.
Train: 2018-07-31T11:34:44.931155: step 5959, loss 0.516007.
Train: 2018-07-31T11:34:45.087370: step 5960, loss 0.632289.
Test: 2018-07-31T11:34:45.571634: step 5960, loss 0.563342.
Train: 2018-07-31T11:34:45.743437: step 5961, loss 0.614016.
Train: 2018-07-31T11:34:45.915301: step 5962, loss 0.622531.
Train: 2018-07-31T11:34:46.087105: step 5963, loss 0.542328.
Train: 2018-07-31T11:34:46.258965: step 5964, loss 0.55986.
Train: 2018-07-31T11:34:46.430775: step 5965, loss 0.489194.
Train: 2018-07-31T11:34:46.602640: step 5966, loss 0.629884.
Train: 2018-07-31T11:34:46.774475: step 5967, loss 0.541702.
Train: 2018-07-31T11:34:46.946312: step 5968, loss 0.646844.
Train: 2018-07-31T11:34:47.118114: step 5969, loss 0.567666.
Train: 2018-07-31T11:34:47.289979: step 5970, loss 0.515071.
Test: 2018-07-31T11:34:47.758622: step 5970, loss 0.561406.
Train: 2018-07-31T11:34:47.930455: step 5971, loss 0.576024.
Train: 2018-07-31T11:34:48.102291: step 5972, loss 0.540954.
Train: 2018-07-31T11:34:48.274124: step 5973, loss 0.56694.
Train: 2018-07-31T11:34:48.445929: step 5974, loss 0.549329.
Train: 2018-07-31T11:34:48.602166: step 5975, loss 0.592773.
Train: 2018-07-31T11:34:48.773976: step 5976, loss 0.627508.
Train: 2018-07-31T11:34:48.945836: step 5977, loss 0.522726.
Train: 2018-07-31T11:34:49.117646: step 5978, loss 0.62714.
Train: 2018-07-31T11:34:49.289481: step 5979, loss 0.609526.
Train: 2018-07-31T11:34:49.461340: step 5980, loss 0.53114.
Test: 2018-07-31T11:34:49.929956: step 5980, loss 0.559906.
Train: 2018-07-31T11:34:50.101821: step 5981, loss 0.583121.
Train: 2018-07-31T11:34:50.273656: step 5982, loss 0.635011.
Train: 2018-07-31T11:34:50.461082: step 5983, loss 0.556887.
Train: 2018-07-31T11:34:50.632947: step 5984, loss 0.61726.
Train: 2018-07-31T11:34:50.804782: step 5985, loss 0.573959.
Train: 2018-07-31T11:34:50.976585: step 5986, loss 0.582449.
Train: 2018-07-31T11:34:51.148421: step 5987, loss 0.599484.
Train: 2018-07-31T11:34:51.320255: step 5988, loss 0.642106.
Train: 2018-07-31T11:34:51.476468: step 5989, loss 0.590611.
Train: 2018-07-31T11:34:51.663955: step 5990, loss 0.632931.
Test: 2018-07-31T11:34:52.148203: step 5990, loss 0.559241.
Train: 2018-07-31T11:34:52.304430: step 5991, loss 0.573401.
Train: 2018-07-31T11:34:52.491856: step 5992, loss 0.573331.
Train: 2018-07-31T11:34:52.663721: step 5993, loss 0.556486.
Train: 2018-07-31T11:34:52.835555: step 5994, loss 0.589935.
Train: 2018-07-31T11:34:53.007390: step 5995, loss 0.539765.
Train: 2018-07-31T11:34:53.179225: step 5996, loss 0.539755.
Train: 2018-07-31T11:34:53.351054: step 5997, loss 0.606315.
Train: 2018-07-31T11:34:53.522865: step 5998, loss 0.606217.
Train: 2018-07-31T11:34:53.694729: step 5999, loss 0.589515.
Train: 2018-07-31T11:34:53.850913: step 6000, loss 0.597711.
Test: 2018-07-31T11:34:54.335174: step 6000, loss 0.558934.
Train: 2018-07-31T11:34:55.038134: step 6001, loss 0.647264.
Train: 2018-07-31T11:34:55.209970: step 6002, loss 0.572758.
Train: 2018-07-31T11:34:55.381828: step 6003, loss 0.597401.
Train: 2018-07-31T11:34:55.538018: step 6004, loss 0.457856.
Train: 2018-07-31T11:34:55.725504: step 6005, loss 0.556224.
Train: 2018-07-31T11:34:55.897308: step 6006, loss 0.613639.
Train: 2018-07-31T11:34:56.069144: step 6007, loss 0.46574.
Train: 2018-07-31T11:34:56.240978: step 6008, loss 0.572446.
Train: 2018-07-31T11:34:56.428459: step 6009, loss 0.53934.
Train: 2018-07-31T11:34:56.600268: step 6010, loss 0.514298.
Test: 2018-07-31T11:34:57.068909: step 6010, loss 0.558284.
Train: 2018-07-31T11:34:57.240744: step 6011, loss 0.5639.
Train: 2018-07-31T11:34:57.412579: step 6012, loss 0.580512.
Train: 2018-07-31T11:34:57.584443: step 6013, loss 0.639219.
Train: 2018-07-31T11:34:57.756279: step 6014, loss 0.529994.
Train: 2018-07-31T11:34:57.928083: step 6015, loss 0.580416.
Train: 2018-07-31T11:34:58.099943: step 6016, loss 0.588842.
Train: 2018-07-31T11:34:58.287387: step 6017, loss 0.588829.
Train: 2018-07-31T11:34:58.443620: step 6018, loss 0.58033.
Train: 2018-07-31T11:34:58.615422: step 6019, loss 0.546332.
Train: 2018-07-31T11:34:58.787256: step 6020, loss 0.622802.
Test: 2018-07-31T11:34:59.255897: step 6020, loss 0.55748.
Train: 2018-07-31T11:34:59.443354: step 6021, loss 0.546203.
Train: 2018-07-31T11:34:59.599566: step 6022, loss 0.563177.
Train: 2018-07-31T11:34:59.771401: step 6023, loss 0.5205.
Train: 2018-07-31T11:34:59.958857: step 6024, loss 0.580168.
Train: 2018-07-31T11:35:00.130692: step 6025, loss 0.640064.
Train: 2018-07-31T11:35:00.302527: step 6026, loss 0.597239.
Train: 2018-07-31T11:35:00.458742: step 6027, loss 0.528747.
Train: 2018-07-31T11:35:00.630575: step 6028, loss 0.60574.
Train: 2018-07-31T11:35:00.802409: step 6029, loss 0.571468.
Train: 2018-07-31T11:35:00.974244: step 6030, loss 0.571439.
Test: 2018-07-31T11:35:01.442915: step 6030, loss 0.557082.
Train: 2018-07-31T11:35:01.614744: step 6031, loss 0.614186.
Train: 2018-07-31T11:35:01.786555: step 6032, loss 0.537198.
Train: 2018-07-31T11:35:01.958389: step 6033, loss 0.588445.
Train: 2018-07-31T11:35:02.130249: step 6034, loss 0.55425.
Train: 2018-07-31T11:35:02.302089: step 6035, loss 0.562764.
Train: 2018-07-31T11:35:02.473923: step 6036, loss 0.562737.
Train: 2018-07-31T11:35:02.645727: step 6037, loss 0.562709.
Train: 2018-07-31T11:35:02.817594: step 6038, loss 0.571227.
Train: 2018-07-31T11:35:02.989398: step 6039, loss 0.5541.
Train: 2018-07-31T11:35:03.145641: step 6040, loss 0.552921.
Test: 2018-07-31T11:35:03.614250: step 6040, loss 0.556806.
Train: 2018-07-31T11:35:03.801738: step 6041, loss 0.613989.
Train: 2018-07-31T11:35:03.973572: step 6042, loss 0.536859.
Train: 2018-07-31T11:35:04.129786: step 6043, loss 0.511083.
Train: 2018-07-31T11:35:04.301620: step 6044, loss 0.562493.
Train: 2018-07-31T11:35:04.473426: step 6045, loss 0.562455.
Train: 2018-07-31T11:35:04.645261: step 6046, loss 0.536541.
Train: 2018-07-31T11:35:04.817125: step 6047, loss 0.588321.
Train: 2018-07-31T11:35:04.988929: step 6048, loss 0.536359.
Train: 2018-07-31T11:35:05.145174: step 6049, loss 0.579677.
Train: 2018-07-31T11:35:05.316977: step 6050, loss 0.51879.
Test: 2018-07-31T11:35:05.801269: step 6050, loss 0.556363.
Train: 2018-07-31T11:35:05.973104: step 6051, loss 0.570969.
Train: 2018-07-31T11:35:06.144909: step 6052, loss 0.605925.
Train: 2018-07-31T11:35:06.301122: step 6053, loss 0.605955.
Train: 2018-07-31T11:35:06.472956: step 6054, loss 0.518395.
Train: 2018-07-31T11:35:06.644791: step 6055, loss 0.597214.
Train: 2018-07-31T11:35:06.816626: step 6056, loss 0.58844.
Train: 2018-07-31T11:35:06.988462: step 6057, loss 0.491944.
Train: 2018-07-31T11:35:07.160296: step 6058, loss 0.535734.
Train: 2018-07-31T11:35:07.332130: step 6059, loss 0.535659.
Train: 2018-07-31T11:35:07.503995: step 6060, loss 0.553217.
Test: 2018-07-31T11:35:07.972631: step 6060, loss 0.556049.
Train: 2018-07-31T11:35:08.160061: step 6061, loss 0.553196.
Train: 2018-07-31T11:35:08.331897: step 6062, loss 0.597444.
Train: 2018-07-31T11:35:08.503732: step 6063, loss 0.570853.
Train: 2018-07-31T11:35:08.675566: step 6064, loss 0.553079.
Train: 2018-07-31T11:35:08.831779: step 6065, loss 0.60642.
Train: 2018-07-31T11:35:09.003614: step 6066, loss 0.624206.
Train: 2018-07-31T11:35:09.175480: step 6067, loss 0.553092.
Train: 2018-07-31T11:35:09.347284: step 6068, loss 0.561978.
Train: 2018-07-31T11:35:09.519149: step 6069, loss 0.579891.
Train: 2018-07-31T11:35:09.690954: step 6070, loss 0.580116.
Test: 2018-07-31T11:35:10.159624: step 6070, loss 0.556699.
Train: 2018-07-31T11:35:10.331463: step 6071, loss 0.624651.
Train: 2018-07-31T11:35:10.503264: step 6072, loss 0.607152.
Train: 2018-07-31T11:35:10.675098: step 6073, loss 0.616146.
Train: 2018-07-31T11:35:10.831341: step 6074, loss 0.589935.
Train: 2018-07-31T11:35:11.003162: step 6075, loss 0.59887.
Train: 2018-07-31T11:35:11.190632: step 6076, loss 0.538046.
Train: 2018-07-31T11:35:11.362468: step 6077, loss 0.564403.
Train: 2018-07-31T11:35:11.534271: step 6078, loss 0.521308.
Train: 2018-07-31T11:35:11.706107: step 6079, loss 0.599353.
Train: 2018-07-31T11:35:11.877942: step 6080, loss 0.53036.
Test: 2018-07-31T11:35:12.346611: step 6080, loss 0.559161.
Train: 2018-07-31T11:35:12.518417: step 6081, loss 0.539102.
Train: 2018-07-31T11:35:12.690281: step 6082, loss 0.539156.
Train: 2018-07-31T11:35:12.862087: step 6083, loss 0.591935.
Train: 2018-07-31T11:35:13.033950: step 6084, loss 0.53052.
Train: 2018-07-31T11:35:13.190135: step 6085, loss 0.599636.
Train: 2018-07-31T11:35:13.361969: step 6086, loss 0.521761.
Train: 2018-07-31T11:35:13.533804: step 6087, loss 0.560725.
Train: 2018-07-31T11:35:13.705662: step 6088, loss 0.538834.
Train: 2018-07-31T11:35:13.877473: step 6089, loss 0.573482.
Train: 2018-07-31T11:35:14.049308: step 6090, loss 0.608283.
Test: 2018-07-31T11:35:14.533599: step 6090, loss 0.558767.
Train: 2018-07-31T11:35:14.705434: step 6091, loss 0.582097.
Train: 2018-07-31T11:35:14.877238: step 6092, loss 0.599505.
Train: 2018-07-31T11:35:15.049104: step 6093, loss 0.547095.
Train: 2018-07-31T11:35:15.220908: step 6094, loss 0.608139.
Train: 2018-07-31T11:35:15.392773: step 6095, loss 0.599343.
Train: 2018-07-31T11:35:15.548988: step 6096, loss 0.581824.
Train: 2018-07-31T11:35:15.736444: step 6097, loss 0.54693.
Train: 2018-07-31T11:35:15.908277: step 6098, loss 0.56428.
Train: 2018-07-31T11:35:16.080117: step 6099, loss 0.529433.
Train: 2018-07-31T11:35:16.251917: step 6100, loss 0.485847.
Test: 2018-07-31T11:35:16.736179: step 6100, loss 0.558172.
Train: 2018-07-31T11:35:17.470412: step 6101, loss 0.598927.
Train: 2018-07-31T11:35:17.642247: step 6102, loss 0.616347.
Train: 2018-07-31T11:35:17.814075: step 6103, loss 0.633736.
Train: 2018-07-31T11:35:17.970295: step 6104, loss 0.502765.
Train: 2018-07-31T11:35:18.157746: step 6105, loss 0.63351.
Train: 2018-07-31T11:35:18.329591: step 6106, loss 0.589784.
Train: 2018-07-31T11:35:18.485769: step 6107, loss 0.615772.
Train: 2018-07-31T11:35:18.657634: step 6108, loss 0.528758.
Train: 2018-07-31T11:35:18.829439: step 6109, loss 0.494031.
Train: 2018-07-31T11:35:19.001303: step 6110, loss 0.632731.
Test: 2018-07-31T11:35:19.485565: step 6110, loss 0.557389.
Train: 2018-07-31T11:35:19.641779: step 6111, loss 0.528562.
Train: 2018-07-31T11:35:19.813613: step 6112, loss 0.519805.
Train: 2018-07-31T11:35:19.985448: step 6113, loss 0.580407.
Train: 2018-07-31T11:35:20.157282: step 6114, loss 0.66447.
Train: 2018-07-31T11:35:20.313465: step 6115, loss 0.588979.
Train: 2018-07-31T11:35:20.485325: step 6116, loss 0.580264.
Train: 2018-07-31T11:35:20.657135: step 6117, loss 0.562937.
Train: 2018-07-31T11:35:20.829000: step 6118, loss 0.51972.
Train: 2018-07-31T11:35:20.985214: step 6119, loss 0.649464.
Train: 2018-07-31T11:35:21.157048: step 6120, loss 0.563011.
Test: 2018-07-31T11:35:21.641280: step 6120, loss 0.557218.
Train: 2018-07-31T11:35:21.813115: step 6121, loss 0.563038.
Train: 2018-07-31T11:35:21.984980: step 6122, loss 0.614747.
Train: 2018-07-31T11:35:22.156815: step 6123, loss 0.554474.
Train: 2018-07-31T11:35:22.328618: step 6124, loss 0.59743.
Train: 2018-07-31T11:35:22.500484: step 6125, loss 0.588797.
Train: 2018-07-31T11:35:22.672289: step 6126, loss 0.56307.
Train: 2018-07-31T11:35:22.844154: step 6127, loss 0.554508.
Train: 2018-07-31T11:35:23.015982: step 6128, loss 0.587494.
Train: 2018-07-31T11:35:23.187823: step 6129, loss 0.614279.
Train: 2018-07-31T11:35:23.359653: step 6130, loss 0.605966.
Test: 2018-07-31T11:35:23.828269: step 6130, loss 0.558128.
Train: 2018-07-31T11:35:24.000128: step 6131, loss 0.605636.
Train: 2018-07-31T11:35:24.171936: step 6132, loss 0.635799.
Train: 2018-07-31T11:35:24.343802: step 6133, loss 0.566305.
Train: 2018-07-31T11:35:24.515607: step 6134, loss 0.559604.
Train: 2018-07-31T11:35:24.687476: step 6135, loss 0.542463.
Train: 2018-07-31T11:35:24.874928: step 6136, loss 0.577628.
Train: 2018-07-31T11:35:25.046765: step 6137, loss 0.604429.
Train: 2018-07-31T11:35:25.218567: step 6138, loss 0.597595.
Train: 2018-07-31T11:35:25.390401: step 6139, loss 0.666218.
Train: 2018-07-31T11:35:25.546616: step 6140, loss 0.617218.
Test: 2018-07-31T11:35:26.030876: step 6140, loss 0.571075.
Train: 2018-07-31T11:35:26.187091: step 6141, loss 0.576705.
Train: 2018-07-31T11:35:26.358924: step 6142, loss 0.586206.
Train: 2018-07-31T11:35:26.530759: step 6143, loss 0.588021.
Train: 2018-07-31T11:35:26.702594: step 6144, loss 0.612942.
Train: 2018-07-31T11:35:26.874460: step 6145, loss 0.597046.
Train: 2018-07-31T11:35:27.046265: step 6146, loss 0.58107.
Train: 2018-07-31T11:35:27.218099: step 6147, loss 0.556744.
Train: 2018-07-31T11:35:27.389958: step 6148, loss 0.565295.
Train: 2018-07-31T11:35:27.561801: step 6149, loss 0.598491.
Train: 2018-07-31T11:35:27.733628: step 6150, loss 0.656422.
Test: 2018-07-31T11:35:28.202242: step 6150, loss 0.576451.
Train: 2018-07-31T11:35:28.374078: step 6151, loss 0.524211.
Train: 2018-07-31T11:35:28.530322: step 6152, loss 0.540564.
Train: 2018-07-31T11:35:28.702152: step 6153, loss 0.61483.
Train: 2018-07-31T11:35:28.873991: step 6154, loss 0.556518.
Train: 2018-07-31T11:35:29.045826: step 6155, loss 0.589401.
Train: 2018-07-31T11:35:29.202039: step 6156, loss 0.60572.
Train: 2018-07-31T11:35:29.373875: step 6157, loss 0.555233.
Train: 2018-07-31T11:35:29.545713: step 6158, loss 0.60497.
Train: 2018-07-31T11:35:29.717544: step 6159, loss 0.612951.
Train: 2018-07-31T11:35:29.889379: step 6160, loss 0.587307.
Test: 2018-07-31T11:35:30.357989: step 6160, loss 0.572739.
Train: 2018-07-31T11:35:30.529823: step 6161, loss 0.561588.
Train: 2018-07-31T11:35:30.701693: step 6162, loss 0.594774.
Train: 2018-07-31T11:35:30.873520: step 6163, loss 0.535196.
Train: 2018-07-31T11:35:31.045353: step 6164, loss 0.602291.
Train: 2018-07-31T11:35:31.217193: step 6165, loss 0.567908.
Train: 2018-07-31T11:35:31.388998: step 6166, loss 0.456868.
Train: 2018-07-31T11:35:31.560863: step 6167, loss 0.609539.
Train: 2018-07-31T11:35:31.732697: step 6168, loss 0.549145.
Train: 2018-07-31T11:35:31.904532: step 6169, loss 0.505498.
Train: 2018-07-31T11:35:32.060745: step 6170, loss 0.530622.
Test: 2018-07-31T11:35:32.544977: step 6170, loss 0.56757.
Train: 2018-07-31T11:35:32.716842: step 6171, loss 0.564738.
Train: 2018-07-31T11:35:32.888646: step 6172, loss 0.555494.
Train: 2018-07-31T11:35:33.044890: step 6173, loss 0.616605.
Train: 2018-07-31T11:35:33.216724: step 6174, loss 0.6429.
Train: 2018-07-31T11:35:33.388560: step 6175, loss 0.598392.
Train: 2018-07-31T11:35:33.560364: step 6176, loss 0.571461.
Train: 2018-07-31T11:35:33.716607: step 6177, loss 0.58.
Train: 2018-07-31T11:35:33.904058: step 6178, loss 0.659701.
Train: 2018-07-31T11:35:34.075898: step 6179, loss 0.588243.
Train: 2018-07-31T11:35:34.247702: step 6180, loss 0.561303.
Test: 2018-07-31T11:35:34.716342: step 6180, loss 0.563908.
Train: 2018-07-31T11:35:34.888178: step 6181, loss 0.56989.
Train: 2018-07-31T11:35:35.060043: step 6182, loss 0.543036.
Train: 2018-07-31T11:35:35.231877: step 6183, loss 0.516198.
Train: 2018-07-31T11:35:35.403682: step 6184, loss 0.480417.
Train: 2018-07-31T11:35:35.559928: step 6185, loss 0.577756.
Train: 2018-07-31T11:35:35.747377: step 6186, loss 0.577547.
Train: 2018-07-31T11:35:35.903565: step 6187, loss 0.630926.
Train: 2018-07-31T11:35:36.075400: step 6188, loss 0.586076.
Train: 2018-07-31T11:35:36.247234: step 6189, loss 0.621613.
Train: 2018-07-31T11:35:36.419100: step 6190, loss 0.683835.
Test: 2018-07-31T11:35:36.887739: step 6190, loss 0.561634.
Train: 2018-07-31T11:35:37.059544: step 6191, loss 0.652437.
Train: 2018-07-31T11:35:37.215782: step 6192, loss 0.61167.
Train: 2018-07-31T11:35:37.387623: step 6193, loss 0.549692.
Train: 2018-07-31T11:35:37.559460: step 6194, loss 0.575937.
Train: 2018-07-31T11:35:37.731263: step 6195, loss 0.636939.
Train: 2018-07-31T11:35:37.903122: step 6196, loss 0.584251.
Train: 2018-07-31T11:35:38.074962: step 6197, loss 0.584108.
Train: 2018-07-31T11:35:38.246791: step 6198, loss 0.583929.
Train: 2018-07-31T11:35:38.418626: step 6199, loss 0.5496.
Train: 2018-07-31T11:35:38.590461: step 6200, loss 0.545095.
Test: 2018-07-31T11:35:39.074696: step 6200, loss 0.560676.
Train: 2018-07-31T11:35:39.824557: step 6201, loss 0.598874.
Train: 2018-07-31T11:35:39.996357: step 6202, loss 0.553655.
Train: 2018-07-31T11:35:40.152570: step 6203, loss 0.666349.
Train: 2018-07-31T11:35:40.324404: step 6204, loss 0.503496.
Train: 2018-07-31T11:35:40.496270: step 6205, loss 0.546028.
Train: 2018-07-31T11:35:40.668075: step 6206, loss 0.658998.
Train: 2018-07-31T11:35:40.839909: step 6207, loss 0.614136.
Train: 2018-07-31T11:35:41.011743: step 6208, loss 0.583958.
Train: 2018-07-31T11:35:41.199200: step 6209, loss 0.60748.
Train: 2018-07-31T11:35:41.355445: step 6210, loss 0.633737.
Test: 2018-07-31T11:35:41.839676: step 6210, loss 0.56189.
Train: 2018-07-31T11:35:42.011510: step 6211, loss 0.592588.
Train: 2018-07-31T11:35:42.183375: step 6212, loss 0.625863.
Train: 2018-07-31T11:35:42.355179: step 6213, loss 0.626116.
Train: 2018-07-31T11:35:42.511392: step 6214, loss 0.581603.
Train: 2018-07-31T11:35:42.698879: step 6215, loss 0.52863.
Train: 2018-07-31T11:35:42.870683: step 6216, loss 0.569941.
Train: 2018-07-31T11:35:43.042520: step 6217, loss 0.578362.
Train: 2018-07-31T11:35:43.214384: step 6218, loss 0.546167.
Train: 2018-07-31T11:35:43.386219: step 6219, loss 0.473346.
Train: 2018-07-31T11:35:43.558054: step 6220, loss 0.619818.
Test: 2018-07-31T11:35:44.042283: step 6220, loss 0.565617.
Train: 2018-07-31T11:35:44.214149: step 6221, loss 0.620082.
Train: 2018-07-31T11:35:44.385984: step 6222, loss 0.571243.
Train: 2018-07-31T11:35:44.557821: step 6223, loss 0.685805.
Train: 2018-07-31T11:35:44.729623: step 6224, loss 0.547026.
Train: 2018-07-31T11:35:44.901458: step 6225, loss 0.563185.
Train: 2018-07-31T11:35:45.073293: step 6226, loss 0.62857.
Train: 2018-07-31T11:35:45.245158: step 6227, loss 0.538713.
Train: 2018-07-31T11:35:45.416992: step 6228, loss 0.554971.
Train: 2018-07-31T11:35:45.588796: step 6229, loss 0.60399.
Train: 2018-07-31T11:35:45.760656: step 6230, loss 0.628515.
Test: 2018-07-31T11:35:46.229302: step 6230, loss 0.565472.
Train: 2018-07-31T11:35:46.416727: step 6231, loss 0.529785.
Train: 2018-07-31T11:35:46.588593: step 6232, loss 0.496947.
Train: 2018-07-31T11:35:46.760428: step 6233, loss 0.611875.
Train: 2018-07-31T11:35:46.932233: step 6234, loss 0.702985.
Train: 2018-07-31T11:35:47.104091: step 6235, loss 0.583053.
Train: 2018-07-31T11:35:47.275902: step 6236, loss 0.635694.
Train: 2018-07-31T11:35:47.432145: step 6237, loss 0.545267.
Train: 2018-07-31T11:35:47.619602: step 6238, loss 0.584964.
Train: 2018-07-31T11:35:47.791407: step 6239, loss 0.561534.
Train: 2018-07-31T11:35:47.963241: step 6240, loss 0.631741.
Test: 2018-07-31T11:35:48.431882: step 6240, loss 0.564119.
Train: 2018-07-31T11:35:48.603746: step 6241, loss 0.519452.
Train: 2018-07-31T11:35:48.775550: step 6242, loss 0.578297.
Train: 2018-07-31T11:35:48.963033: step 6243, loss 0.536053.
Train: 2018-07-31T11:35:49.119251: step 6244, loss 0.544478.
Train: 2018-07-31T11:35:49.306707: step 6245, loss 0.578544.
Train: 2018-07-31T11:35:49.478512: step 6246, loss 0.535621.
Train: 2018-07-31T11:35:49.650346: step 6247, loss 0.535093.
Train: 2018-07-31T11:35:49.822183: step 6248, loss 0.624302.
Train: 2018-07-31T11:35:49.994016: step 6249, loss 0.587325.
Train: 2018-07-31T11:35:50.165881: step 6250, loss 0.648515.
Test: 2018-07-31T11:35:50.634489: step 6250, loss 0.564211.
Train: 2018-07-31T11:35:50.821981: step 6251, loss 0.552727.
Train: 2018-07-31T11:35:50.993782: step 6252, loss 0.570207.
Train: 2018-07-31T11:35:51.165617: step 6253, loss 0.579109.
Train: 2018-07-31T11:35:51.337451: step 6254, loss 0.526612.
Train: 2018-07-31T11:35:51.509286: step 6255, loss 0.582145.
Train: 2018-07-31T11:35:51.681147: step 6256, loss 0.57958.
Train: 2018-07-31T11:35:51.852954: step 6257, loss 0.491309.
Train: 2018-07-31T11:35:52.024790: step 6258, loss 0.535601.
Train: 2018-07-31T11:35:52.196625: step 6259, loss 0.624838.
Train: 2018-07-31T11:35:52.368492: step 6260, loss 0.580479.
Test: 2018-07-31T11:35:52.837100: step 6260, loss 0.56568.
Train: 2018-07-31T11:35:53.024557: step 6261, loss 0.625495.
Train: 2018-07-31T11:35:53.180770: step 6262, loss 0.634674.
Train: 2018-07-31T11:35:53.368226: step 6263, loss 0.643739.
Train: 2018-07-31T11:35:53.540061: step 6264, loss 0.607902.
Train: 2018-07-31T11:35:53.696309: step 6265, loss 0.563232.
Train: 2018-07-31T11:35:53.883730: step 6266, loss 0.509806.
Train: 2018-07-31T11:35:54.055564: step 6267, loss 0.616686.
Train: 2018-07-31T11:35:54.227429: step 6268, loss 0.581001.
Train: 2018-07-31T11:35:54.399264: step 6269, loss 0.518744.
Train: 2018-07-31T11:35:54.571069: step 6270, loss 0.563038.
Test: 2018-07-31T11:35:55.039739: step 6270, loss 0.565789.
Train: 2018-07-31T11:35:55.211545: step 6271, loss 0.642811.
Train: 2018-07-31T11:35:55.398999: step 6272, loss 0.562758.
Train: 2018-07-31T11:35:55.570864: step 6273, loss 0.642307.
Train: 2018-07-31T11:35:55.727047: step 6274, loss 0.562424.
Train: 2018-07-31T11:35:55.914534: step 6275, loss 0.474043.
Train: 2018-07-31T11:35:56.070717: step 6276, loss 0.544377.
Train: 2018-07-31T11:35:56.258198: step 6277, loss 0.57062.
Train: 2018-07-31T11:35:56.430009: step 6278, loss 0.596901.
Train: 2018-07-31T11:35:56.601842: step 6279, loss 0.552481.
Train: 2018-07-31T11:35:56.773709: step 6280, loss 0.51686.
Test: 2018-07-31T11:35:57.242318: step 6280, loss 0.563712.
Train: 2018-07-31T11:35:57.414153: step 6281, loss 0.548615.
Train: 2018-07-31T11:35:57.601609: step 6282, loss 0.454247.
Train: 2018-07-31T11:35:57.773444: step 6283, loss 0.632175.
Train: 2018-07-31T11:35:57.945279: step 6284, loss 0.710819.
Train: 2018-07-31T11:35:58.132764: step 6285, loss 0.630626.
Train: 2018-07-31T11:35:58.304603: step 6286, loss 0.552091.
Train: 2018-07-31T11:35:58.476405: step 6287, loss 0.668139.
Train: 2018-07-31T11:35:58.648263: step 6288, loss 0.832442.
Train: 2018-07-31T11:35:58.820109: step 6289, loss 0.830019.
Train: 2018-07-31T11:35:58.991909: step 6290, loss 0.835639.
Test: 2018-07-31T11:35:59.460549: step 6290, loss 0.571363.
Train: 2018-07-31T11:35:59.648035: step 6291, loss 0.602161.
Train: 2018-07-31T11:35:59.819865: step 6292, loss 0.597716.
Train: 2018-07-31T11:35:59.991704: step 6293, loss 0.555579.
Train: 2018-07-31T11:36:00.163508: step 6294, loss 0.585002.
Train: 2018-07-31T11:36:00.335374: step 6295, loss 0.641105.
Train: 2018-07-31T11:36:00.507211: step 6296, loss 0.599289.
Train: 2018-07-31T11:36:00.694670: step 6297, loss 0.556718.
Train: 2018-07-31T11:36:00.866499: step 6298, loss 0.647725.
Train: 2018-07-31T11:36:01.038305: step 6299, loss 0.551984.
Train: 2018-07-31T11:36:01.210169: step 6300, loss 0.615768.
Test: 2018-07-31T11:36:01.678809: step 6300, loss 0.593557.
Train: 2018-07-31T11:36:02.413012: step 6301, loss 0.519839.
Train: 2018-07-31T11:36:02.584817: step 6302, loss 0.662833.
Train: 2018-07-31T11:36:02.756652: step 6303, loss 0.531015.
Train: 2018-07-31T11:36:02.928516: step 6304, loss 0.664784.
Train: 2018-07-31T11:36:03.100322: step 6305, loss 0.567963.
Train: 2018-07-31T11:36:03.272186: step 6306, loss 0.621652.
Train: 2018-07-31T11:36:03.443992: step 6307, loss 0.604281.
Train: 2018-07-31T11:36:03.615826: step 6308, loss 0.613368.
Train: 2018-07-31T11:36:03.787662: step 6309, loss 0.702117.
Train: 2018-07-31T11:36:03.959496: step 6310, loss 0.666521.
Test: 2018-07-31T11:36:04.443757: step 6310, loss 0.598532.
Train: 2018-07-31T11:36:04.615621: step 6311, loss 0.586843.
Train: 2018-07-31T11:36:04.787426: step 6312, loss 0.621891.
Train: 2018-07-31T11:36:04.959291: step 6313, loss 0.595243.
Train: 2018-07-31T11:36:05.131126: step 6314, loss 0.58616.
Train: 2018-07-31T11:36:05.302961: step 6315, loss 0.787053.
Train: 2018-07-31T11:36:05.459174: step 6316, loss 0.585434.
Train: 2018-07-31T11:36:05.646631: step 6317, loss 0.585024.
Train: 2018-07-31T11:36:05.802813: step 6318, loss 0.653749.
Train: 2018-07-31T11:36:05.974673: step 6319, loss 0.592695.
Train: 2018-07-31T11:36:06.162104: step 6320, loss 0.592149.
Test: 2018-07-31T11:36:06.646397: step 6320, loss 0.594361.
Train: 2018-07-31T11:36:06.818200: step 6321, loss 0.67729.
Train: 2018-07-31T11:36:06.990035: step 6322, loss 0.599548.
Train: 2018-07-31T11:36:07.161903: step 6323, loss 0.624483.
Train: 2018-07-31T11:36:07.333735: step 6324, loss 0.63228.
Train: 2018-07-31T11:36:07.505570: step 6325, loss 0.690791.
Train: 2018-07-31T11:36:07.661784: step 6326, loss 0.588691.
Train: 2018-07-31T11:36:07.833588: step 6327, loss 0.646857.
Train: 2018-07-31T11:36:08.005423: step 6328, loss 0.620994.
Train: 2018-07-31T11:36:08.177290: step 6329, loss 0.57871.
Train: 2018-07-31T11:36:08.349093: step 6330, loss 0.536696.
Test: 2018-07-31T11:36:08.817733: step 6330, loss 0.588572.
Train: 2018-07-31T11:36:09.005189: step 6331, loss 0.569301.
Train: 2018-07-31T11:36:09.223918: step 6332, loss 0.568691.
Train: 2018-07-31T11:36:09.395722: step 6333, loss 0.642709.
Train: 2018-07-31T11:36:09.567557: step 6334, loss 0.625531.
Train: 2018-07-31T11:36:09.739392: step 6335, loss 0.624947.
Train: 2018-07-31T11:36:09.926848: step 6336, loss 0.674088.
Train: 2018-07-31T11:36:10.083062: step 6337, loss 0.557626.
Train: 2018-07-31T11:36:10.254896: step 6338, loss 0.548863.
Train: 2018-07-31T11:36:10.426730: step 6339, loss 0.531766.
Train: 2018-07-31T11:36:10.598566: step 6340, loss 0.564225.
Test: 2018-07-31T11:36:11.067206: step 6340, loss 0.582922.
Train: 2018-07-31T11:36:11.254692: step 6341, loss 0.630072.
Train: 2018-07-31T11:36:11.410876: step 6342, loss 0.578569.
Train: 2018-07-31T11:36:11.582711: step 6343, loss 0.579143.
Train: 2018-07-31T11:36:11.754545: step 6344, loss 0.603714.
Train: 2018-07-31T11:36:11.942001: step 6345, loss 0.569719.
Train: 2018-07-31T11:36:12.113860: step 6346, loss 0.611234.
Train: 2018-07-31T11:36:12.270050: step 6347, loss 0.610825.
Train: 2018-07-31T11:36:12.441914: step 6348, loss 0.644184.
Train: 2018-07-31T11:36:12.613749: step 6349, loss 0.525566.
Train: 2018-07-31T11:36:12.785553: step 6350, loss 0.541945.
Test: 2018-07-31T11:36:13.269845: step 6350, loss 0.578098.
Train: 2018-07-31T11:36:13.441649: step 6351, loss 0.600797.
Train: 2018-07-31T11:36:13.613514: step 6352, loss 0.600428.
Train: 2018-07-31T11:36:13.785349: step 6353, loss 0.506289.
Train: 2018-07-31T11:36:13.957185: step 6354, loss 0.651074.
Train: 2018-07-31T11:36:14.129020: step 6355, loss 0.633692.
Train: 2018-07-31T11:36:14.300853: step 6356, loss 0.530383.
Train: 2018-07-31T11:36:14.472659: step 6357, loss 0.572925.
Train: 2018-07-31T11:36:14.644493: step 6358, loss 0.546692.
Train: 2018-07-31T11:36:14.816327: step 6359, loss 0.61542.
Train: 2018-07-31T11:36:14.988192: step 6360, loss 0.502526.
Test: 2018-07-31T11:36:15.472423: step 6360, loss 0.574307.
Train: 2018-07-31T11:36:15.644289: step 6361, loss 0.580175.
Train: 2018-07-31T11:36:15.816093: step 6362, loss 0.579856.
Train: 2018-07-31T11:36:15.987928: step 6363, loss 0.640805.
Train: 2018-07-31T11:36:16.159764: step 6364, loss 0.58802.
Train: 2018-07-31T11:36:16.331631: step 6365, loss 0.508756.
Train: 2018-07-31T11:36:16.503463: step 6366, loss 0.701891.
Train: 2018-07-31T11:36:16.675267: step 6367, loss 0.613621.
Train: 2018-07-31T11:36:16.847132: step 6368, loss 0.560571.
Train: 2018-07-31T11:36:17.018967: step 6369, loss 0.604285.
Train: 2018-07-31T11:36:17.206423: step 6370, loss 0.639171.
Test: 2018-07-31T11:36:17.675033: step 6370, loss 0.5715.
Train: 2018-07-31T11:36:17.846867: step 6371, loss 0.603737.
Train: 2018-07-31T11:36:18.018702: step 6372, loss 0.620962.
Train: 2018-07-31T11:36:18.206184: step 6373, loss 0.620626.
Train: 2018-07-31T11:36:18.362372: step 6374, loss 0.507119.
Train: 2018-07-31T11:36:18.534207: step 6375, loss 0.602619.
Train: 2018-07-31T11:36:18.721663: step 6376, loss 0.558971.
Train: 2018-07-31T11:36:18.877912: step 6377, loss 0.584793.
Train: 2018-07-31T11:36:19.049712: step 6378, loss 0.523946.
Train: 2018-07-31T11:36:19.221576: step 6379, loss 0.610379.
Train: 2018-07-31T11:36:19.393381: step 6380, loss 0.610175.
Test: 2018-07-31T11:36:19.877642: step 6380, loss 0.569491.
Train: 2018-07-31T11:36:20.049477: step 6381, loss 0.549362.
Train: 2018-07-31T11:36:20.221336: step 6382, loss 0.566487.
Train: 2018-07-31T11:36:20.393177: step 6383, loss 0.540322.
Train: 2018-07-31T11:36:20.565011: step 6384, loss 0.505424.
Train: 2018-07-31T11:36:20.736817: step 6385, loss 0.704924.
Train: 2018-07-31T11:36:20.908650: step 6386, loss 0.557027.
Train: 2018-07-31T11:36:21.096106: step 6387, loss 0.582919.
Train: 2018-07-31T11:36:21.267972: step 6388, loss 0.617515.
Train: 2018-07-31T11:36:21.439807: step 6389, loss 0.599957.
Train: 2018-07-31T11:36:21.627232: step 6390, loss 0.573746.
Test: 2018-07-31T11:36:22.095873: step 6390, loss 0.567742.
Train: 2018-07-31T11:36:22.314601: step 6391, loss 0.616942.
Train: 2018-07-31T11:36:22.486437: step 6392, loss 0.582104.
Train: 2018-07-31T11:36:22.658271: step 6393, loss 0.607886.
Train: 2018-07-31T11:36:22.830077: step 6394, loss 0.599057.
Train: 2018-07-31T11:36:23.001911: step 6395, loss 0.52136.
Train: 2018-07-31T11:36:23.173771: step 6396, loss 0.572896.
Train: 2018-07-31T11:36:23.361201: step 6397, loss 0.581363.
Train: 2018-07-31T11:36:23.533066: step 6398, loss 0.572622.
Train: 2018-07-31T11:36:23.704896: step 6399, loss 0.538084.
Train: 2018-07-31T11:36:23.861110: step 6400, loss 0.56374.
Test: 2018-07-31T11:36:24.345346: step 6400, loss 0.566394.
Train: 2018-07-31T11:36:25.079579: step 6401, loss 0.554976.
Train: 2018-07-31T11:36:25.251414: step 6402, loss 0.537554.
Train: 2018-07-31T11:36:25.423220: step 6403, loss 0.615164.
Train: 2018-07-31T11:36:25.595053: step 6404, loss 0.563137.
Train: 2018-07-31T11:36:25.766889: step 6405, loss 0.562994.
Train: 2018-07-31T11:36:25.938758: step 6406, loss 0.545484.
Train: 2018-07-31T11:36:26.110559: step 6407, loss 0.60621.
Train: 2018-07-31T11:36:26.282423: step 6408, loss 0.571282.
Train: 2018-07-31T11:36:26.454227: step 6409, loss 0.579885.
Train: 2018-07-31T11:36:26.626063: step 6410, loss 0.58851.
Test: 2018-07-31T11:36:27.110323: step 6410, loss 0.565037.
Train: 2018-07-31T11:36:27.282158: step 6411, loss 0.623358.
Train: 2018-07-31T11:36:27.453993: step 6412, loss 0.579561.
Train: 2018-07-31T11:36:27.625828: step 6413, loss 0.640566.
Train: 2018-07-31T11:36:27.797695: step 6414, loss 0.579342.
Train: 2018-07-31T11:36:27.969527: step 6415, loss 0.55313.
Train: 2018-07-31T11:36:28.141363: step 6416, loss 0.561745.
Train: 2018-07-31T11:36:28.313167: step 6417, loss 0.587717.
Train: 2018-07-31T11:36:28.485032: step 6418, loss 0.544207.
Train: 2018-07-31T11:36:28.656870: step 6419, loss 0.544113.
Train: 2018-07-31T11:36:28.828672: step 6420, loss 0.604798.
Test: 2018-07-31T11:36:29.297312: step 6420, loss 0.564102.
Train: 2018-07-31T11:36:29.469146: step 6421, loss 0.604705.
Train: 2018-07-31T11:36:29.640982: step 6422, loss 0.621955.
Train: 2018-07-31T11:36:29.812846: step 6423, loss 0.613129.
Train: 2018-07-31T11:36:29.984681: step 6424, loss 0.552424.
Train: 2018-07-31T11:36:30.156485: step 6425, loss 0.509193.
Train: 2018-07-31T11:36:30.328321: step 6426, loss 0.552284.
Train: 2018-07-31T11:36:30.515806: step 6427, loss 0.586754.
Train: 2018-07-31T11:36:30.687646: step 6428, loss 0.638546.
Train: 2018-07-31T11:36:30.859476: step 6429, loss 0.603856.
Train: 2018-07-31T11:36:31.031280: step 6430, loss 0.577864.
Test: 2018-07-31T11:36:31.499951: step 6430, loss 0.563359.
Train: 2018-07-31T11:36:31.671756: step 6431, loss 0.534721.
Train: 2018-07-31T11:36:31.843590: step 6432, loss 0.560486.
Train: 2018-07-31T11:36:32.015455: step 6433, loss 0.551798.
Train: 2018-07-31T11:36:32.187295: step 6434, loss 0.629243.
Train: 2018-07-31T11:36:32.343504: step 6435, loss 0.525819.
Train: 2018-07-31T11:36:32.515308: step 6436, loss 0.611864.
Train: 2018-07-31T11:36:32.687172: step 6437, loss 0.568721.
Train: 2018-07-31T11:36:32.858977: step 6438, loss 0.62032.
Train: 2018-07-31T11:36:33.030813: step 6439, loss 0.620207.
Train: 2018-07-31T11:36:33.202647: step 6440, loss 0.594297.
Test: 2018-07-31T11:36:33.686910: step 6440, loss 0.562701.
Train: 2018-07-31T11:36:33.858773: step 6441, loss 0.577054.
Train: 2018-07-31T11:36:34.030579: step 6442, loss 0.568433.
Train: 2018-07-31T11:36:34.202445: step 6443, loss 0.56838.
Train: 2018-07-31T11:36:34.374278: step 6444, loss 0.551259.
Train: 2018-07-31T11:36:34.546115: step 6445, loss 0.559736.
Train: 2018-07-31T11:36:34.717918: step 6446, loss 0.619382.
Train: 2018-07-31T11:36:34.889753: step 6447, loss 0.585193.
Train: 2018-07-31T11:36:35.061586: step 6448, loss 0.585125.
Train: 2018-07-31T11:36:35.217825: step 6449, loss 0.627589.
Train: 2018-07-31T11:36:35.389634: step 6450, loss 0.4916.
Test: 2018-07-31T11:36:35.873895: step 6450, loss 0.562217.
Train: 2018-07-31T11:36:36.045731: step 6451, loss 0.533983.
Train: 2018-07-31T11:36:36.217567: step 6452, loss 0.593376.
Train: 2018-07-31T11:36:36.389401: step 6453, loss 0.576318.
Train: 2018-07-31T11:36:36.561260: step 6454, loss 0.550727.
Train: 2018-07-31T11:36:36.717450: step 6455, loss 0.499514.
Train: 2018-07-31T11:36:36.904905: step 6456, loss 0.499246.
Train: 2018-07-31T11:36:37.061119: step 6457, loss 0.627563.
Train: 2018-07-31T11:36:37.232979: step 6458, loss 0.670651.
Train: 2018-07-31T11:36:37.404813: step 6459, loss 0.619003.
Train: 2018-07-31T11:36:37.576622: step 6460, loss 0.550133.
Test: 2018-07-31T11:36:38.060919: step 6460, loss 0.561478.
Train: 2018-07-31T11:36:38.232750: step 6461, loss 0.679058.
Train: 2018-07-31T11:36:38.404553: step 6462, loss 0.524351.
Train: 2018-07-31T11:36:38.560767: step 6463, loss 0.618633.
Train: 2018-07-31T11:36:38.732602: step 6464, loss 0.490161.
Train: 2018-07-31T11:36:38.904437: step 6465, loss 0.550001.
Train: 2018-07-31T11:36:39.076271: step 6466, loss 0.567063.
Train: 2018-07-31T11:36:39.248137: step 6467, loss 0.567008.
Train: 2018-07-31T11:36:39.435563: step 6468, loss 0.592698.
Train: 2018-07-31T11:36:39.607396: step 6469, loss 0.566901.
Train: 2018-07-31T11:36:39.779232: step 6470, loss 0.584034.
Test: 2018-07-31T11:36:40.263493: step 6470, loss 0.561.
Train: 2018-07-31T11:36:40.435356: step 6471, loss 0.60978.
Train: 2018-07-31T11:36:40.607194: step 6472, loss 0.583944.
Train: 2018-07-31T11:36:40.778999: step 6473, loss 0.644012.
Train: 2018-07-31T11:36:40.950832: step 6474, loss 0.566691.
Train: 2018-07-31T11:36:41.122698: step 6475, loss 0.549545.
Train: 2018-07-31T11:36:41.294501: step 6476, loss 0.635035.
Train: 2018-07-31T11:36:41.466338: step 6477, loss 0.600739.
Train: 2018-07-31T11:36:41.638196: step 6478, loss 0.53253.
Train: 2018-07-31T11:36:41.810006: step 6479, loss 0.617579.
Train: 2018-07-31T11:36:41.981841: step 6480, loss 0.524098.
Test: 2018-07-31T11:36:42.466103: step 6480, loss 0.560778.
Train: 2018-07-31T11:36:42.637962: step 6481, loss 0.600432.
Train: 2018-07-31T11:36:42.809773: step 6482, loss 0.524103.
Train: 2018-07-31T11:36:42.981608: step 6483, loss 0.532526.
Train: 2018-07-31T11:36:43.153442: step 6484, loss 0.566378.
Train: 2018-07-31T11:36:43.325277: step 6485, loss 0.61731.
Train: 2018-07-31T11:36:43.497141: step 6486, loss 0.549284.
Train: 2018-07-31T11:36:43.668976: step 6487, loss 0.566237.
Train: 2018-07-31T11:36:43.840812: step 6488, loss 0.549158.
Train: 2018-07-31T11:36:44.012646: step 6489, loss 0.574667.
Train: 2018-07-31T11:36:44.184450: step 6490, loss 0.62587.
Test: 2018-07-31T11:36:44.653091: step 6490, loss 0.560287.
Train: 2018-07-31T11:36:44.824959: step 6491, loss 0.583134.
Train: 2018-07-31T11:36:45.012411: step 6492, loss 0.531849.
Train: 2018-07-31T11:36:45.184246: step 6493, loss 0.592759.
Train: 2018-07-31T11:36:45.356052: step 6494, loss 0.591591.
Train: 2018-07-31T11:36:45.527885: step 6495, loss 0.540236.
Train: 2018-07-31T11:36:45.699721: step 6496, loss 0.574416.
Train: 2018-07-31T11:36:45.871585: step 6497, loss 0.642913.
Train: 2018-07-31T11:36:46.059012: step 6498, loss 0.53155.
Train: 2018-07-31T11:36:46.215255: step 6499, loss 0.54007.
Train: 2018-07-31T11:36:46.387090: step 6500, loss 0.514297.
Test: 2018-07-31T11:36:46.855730: step 6500, loss 0.559868.
Train: 2018-07-31T11:36:47.574282: step 6501, loss 0.5399.
Train: 2018-07-31T11:36:47.746117: step 6502, loss 0.556999.
Train: 2018-07-31T11:36:47.917984: step 6503, loss 0.531025.
Train: 2018-07-31T11:36:48.089816: step 6504, loss 0.539518.
Train: 2018-07-31T11:36:48.261620: step 6505, loss 0.556757.
Train: 2018-07-31T11:36:48.433490: step 6506, loss 0.521781.
Train: 2018-07-31T11:36:48.605320: step 6507, loss 0.54783.
Train: 2018-07-31T11:36:48.777126: step 6508, loss 0.574103.
Train: 2018-07-31T11:36:48.948960: step 6509, loss 0.565267.
Train: 2018-07-31T11:36:49.120795: step 6510, loss 0.556374.
Test: 2018-07-31T11:36:49.605056: step 6510, loss 0.559203.
Train: 2018-07-31T11:36:49.776891: step 6511, loss 0.654061.
Train: 2018-07-31T11:36:49.948756: step 6512, loss 0.582961.
Train: 2018-07-31T11:36:50.120560: step 6513, loss 0.565141.
Train: 2018-07-31T11:36:50.292396: step 6514, loss 0.600738.
Train: 2018-07-31T11:36:50.464260: step 6515, loss 0.591798.
Train: 2018-07-31T11:36:50.636065: step 6516, loss 0.502764.
Train: 2018-07-31T11:36:50.807924: step 6517, loss 0.538315.
Train: 2018-07-31T11:36:50.995356: step 6518, loss 0.582828.
Train: 2018-07-31T11:36:51.151599: step 6519, loss 0.618488.
Train: 2018-07-31T11:36:51.323403: step 6520, loss 0.57386.
Test: 2018-07-31T11:36:51.807695: step 6520, loss 0.558907.
Train: 2018-07-31T11:36:51.995152: step 6521, loss 0.636191.
Train: 2018-07-31T11:36:52.151365: step 6522, loss 0.556001.
Train: 2018-07-31T11:36:52.323194: step 6523, loss 0.573744.
Train: 2018-07-31T11:36:52.495004: step 6524, loss 0.609156.
Train: 2018-07-31T11:36:52.666871: step 6525, loss 0.600189.
Train: 2018-07-31T11:36:52.838673: step 6526, loss 0.52071.
Train: 2018-07-31T11:36:53.010539: step 6527, loss 0.555974.
Train: 2018-07-31T11:36:53.182373: step 6528, loss 0.608726.
Train: 2018-07-31T11:36:53.354208: step 6529, loss 0.573515.
Train: 2018-07-31T11:36:53.526037: step 6530, loss 0.57348.
Test: 2018-07-31T11:36:54.010275: step 6530, loss 0.558799.
Train: 2018-07-31T11:36:54.182109: step 6531, loss 0.652166.
Train: 2018-07-31T11:36:54.353945: step 6532, loss 0.503667.
Train: 2018-07-31T11:36:54.525788: step 6533, loss 0.651717.
Train: 2018-07-31T11:36:54.697644: step 6534, loss 0.555993.
Train: 2018-07-31T11:36:54.869479: step 6535, loss 0.573317.
Train: 2018-07-31T11:36:55.041283: step 6536, loss 0.599195.
Train: 2018-07-31T11:36:55.213148: step 6537, loss 0.633549.
Train: 2018-07-31T11:36:55.384953: step 6538, loss 0.564657.
Train: 2018-07-31T11:36:55.556789: step 6539, loss 0.530448.
Train: 2018-07-31T11:36:55.713001: step 6540, loss 0.55612.
Test: 2018-07-31T11:36:56.197261: step 6540, loss 0.558894.
Train: 2018-07-31T11:36:56.369108: step 6541, loss 0.573173.
Train: 2018-07-31T11:36:56.540964: step 6542, loss 0.573152.
Train: 2018-07-31T11:36:56.712767: step 6543, loss 0.539104.
Train: 2018-07-31T11:36:56.884631: step 6544, loss 0.590121.
Train: 2018-07-31T11:36:57.056435: step 6545, loss 0.607096.
Train: 2018-07-31T11:36:57.228301: step 6546, loss 0.581561.
Train: 2018-07-31T11:36:57.400107: step 6547, loss 0.598499.
Train: 2018-07-31T11:36:57.571941: step 6548, loss 0.589972.
Train: 2018-07-31T11:36:57.743775: step 6549, loss 0.598383.
Train: 2018-07-31T11:36:57.915610: step 6550, loss 0.513931.
Test: 2018-07-31T11:36:58.399901: step 6550, loss 0.558856.
Train: 2018-07-31T11:36:58.556111: step 6551, loss 0.556112.
Train: 2018-07-31T11:36:58.727919: step 6552, loss 0.530783.
Train: 2018-07-31T11:36:58.899755: step 6553, loss 0.539157.
Train: 2018-07-31T11:36:59.071590: step 6554, loss 0.57291.
Train: 2018-07-31T11:36:59.243425: step 6555, loss 0.632204.
Train: 2018-07-31T11:36:59.415258: step 6556, loss 0.530481.
Train: 2018-07-31T11:36:59.587093: step 6557, loss 0.538893.
Train: 2018-07-31T11:36:59.758928: step 6558, loss 0.530301.
Train: 2018-07-31T11:36:59.915142: step 6559, loss 0.598369.
Train: 2018-07-31T11:37:00.102598: step 6560, loss 0.57277.
Test: 2018-07-31T11:37:00.571237: step 6560, loss 0.55842.
Train: 2018-07-31T11:37:00.743104: step 6561, loss 0.598418.
Train: 2018-07-31T11:37:00.914907: step 6562, loss 0.615553.
Train: 2018-07-31T11:37:01.071120: step 6563, loss 0.547016.
Train: 2018-07-31T11:37:01.242980: step 6564, loss 0.598399.
Train: 2018-07-31T11:37:01.414820: step 6565, loss 0.58124.
Train: 2018-07-31T11:37:01.586626: step 6566, loss 0.632621.
Train: 2018-07-31T11:37:01.758460: step 6567, loss 0.572635.
Train: 2018-07-31T11:37:01.945917: step 6568, loss 0.572617.
Train: 2018-07-31T11:37:02.117751: step 6569, loss 0.538469.
Train: 2018-07-31T11:37:02.289585: step 6570, loss 0.606702.
Test: 2018-07-31T11:37:02.773878: step 6570, loss 0.558293.
Train: 2018-07-31T11:37:02.945713: step 6571, loss 0.632217.
Train: 2018-07-31T11:37:03.101895: step 6572, loss 0.600397.
Train: 2018-07-31T11:37:03.273730: step 6573, loss 0.589514.
Train: 2018-07-31T11:37:03.445595: step 6574, loss 0.564104.
Train: 2018-07-31T11:37:03.617400: step 6575, loss 0.606374.
Train: 2018-07-31T11:37:03.789235: step 6576, loss 0.547361.
Train: 2018-07-31T11:37:03.961070: step 6577, loss 0.589496.
Train: 2018-07-31T11:37:04.132934: step 6578, loss 0.581114.
Train: 2018-07-31T11:37:04.304740: step 6579, loss 0.564378.
Train: 2018-07-31T11:37:04.460953: step 6580, loss 0.43883.
Test: 2018-07-31T11:37:04.945243: step 6580, loss 0.558765.
Train: 2018-07-31T11:37:05.117049: step 6581, loss 0.530865.
Train: 2018-07-31T11:37:05.288884: step 6582, loss 0.598074.
Train: 2018-07-31T11:37:05.460744: step 6583, loss 0.530675.
Train: 2018-07-31T11:37:05.616962: step 6584, loss 0.564388.
Train: 2018-07-31T11:37:05.788796: step 6585, loss 0.538918.
Train: 2018-07-31T11:37:05.960632: step 6586, loss 0.63243.
Train: 2018-07-31T11:37:06.132465: step 6587, loss 0.572846.
Train: 2018-07-31T11:37:06.304295: step 6588, loss 0.495972.
Train: 2018-07-31T11:37:06.476129: step 6589, loss 0.572832.
Train: 2018-07-31T11:37:06.647939: step 6590, loss 0.564229.
Test: 2018-07-31T11:37:07.116579: step 6590, loss 0.55838.
Train: 2018-07-31T11:37:07.288447: step 6591, loss 0.598656.
Train: 2018-07-31T11:37:07.475903: step 6592, loss 0.55553.
Train: 2018-07-31T11:37:07.632085: step 6593, loss 0.598725.
Train: 2018-07-31T11:37:07.803949: step 6594, loss 0.590077.
Train: 2018-07-31T11:37:07.975785: step 6595, loss 0.572734.
Train: 2018-07-31T11:37:08.147619: step 6596, loss 0.598722.
Train: 2018-07-31T11:37:08.319454: step 6597, loss 0.52933.
Train: 2018-07-31T11:37:08.491258: step 6598, loss 0.477206.
Train: 2018-07-31T11:37:08.663093: step 6599, loss 0.607433.
Train: 2018-07-31T11:37:08.834927: step 6600, loss 0.55518.
Test: 2018-07-31T11:37:09.315160: step 6600, loss 0.557965.
Train: 2018-07-31T11:37:10.096195: step 6601, loss 0.581312.
Train: 2018-07-31T11:37:10.268030: step 6602, loss 0.546337.
Train: 2018-07-31T11:37:10.439865: step 6603, loss 0.546272.
Train: 2018-07-31T11:37:10.611735: step 6604, loss 0.493577.
Train: 2018-07-31T11:37:10.783558: step 6605, loss 0.616491.
Train: 2018-07-31T11:37:10.955368: step 6606, loss 0.528404.
Train: 2018-07-31T11:37:11.127234: step 6607, loss 0.51061.
Train: 2018-07-31T11:37:11.299038: step 6608, loss 0.581323.
Train: 2018-07-31T11:37:11.470874: step 6609, loss 0.519117.
Train: 2018-07-31T11:37:11.642709: step 6610, loss 0.634871.
Test: 2018-07-31T11:37:12.111348: step 6610, loss 0.557476.
Train: 2018-07-31T11:37:12.283183: step 6611, loss 0.509917.
Train: 2018-07-31T11:37:12.455050: step 6612, loss 0.509763.
Train: 2018-07-31T11:37:12.626883: step 6613, loss 0.599376.
Train: 2018-07-31T11:37:12.798688: step 6614, loss 0.599429.
Train: 2018-07-31T11:37:12.970547: step 6615, loss 0.626477.
Train: 2018-07-31T11:37:13.142390: step 6616, loss 0.581396.
Train: 2018-07-31T11:37:13.298570: step 6617, loss 0.599362.
Train: 2018-07-31T11:37:13.486027: step 6618, loss 0.563326.
Train: 2018-07-31T11:37:13.657862: step 6619, loss 0.644111.
Train: 2018-07-31T11:37:13.814105: step 6620, loss 0.661772.
Test: 2018-07-31T11:37:14.298335: step 6620, loss 0.557229.
Train: 2018-07-31T11:37:14.470171: step 6621, loss 0.527584.
Train: 2018-07-31T11:37:14.642006: step 6622, loss 0.607652.
Train: 2018-07-31T11:37:14.813841: step 6623, loss 0.625167.
Train: 2018-07-31T11:37:14.985699: step 6624, loss 0.580813.
Train: 2018-07-31T11:37:15.141913: step 6625, loss 0.633348.
Train: 2018-07-31T11:37:15.313751: step 6626, loss 0.563187.
Train: 2018-07-31T11:37:15.485588: step 6627, loss 0.580554.
Train: 2018-07-31T11:37:15.657423: step 6628, loss 0.623685.
Train: 2018-07-31T11:37:15.829228: step 6629, loss 0.55463.
Train: 2018-07-31T11:37:16.001063: step 6630, loss 0.511906.
Test: 2018-07-31T11:37:16.469702: step 6630, loss 0.557492.
Train: 2018-07-31T11:37:16.641568: step 6631, loss 0.571783.
Train: 2018-07-31T11:37:16.813402: step 6632, loss 0.580281.
Train: 2018-07-31T11:37:16.985207: step 6633, loss 0.597232.
Train: 2018-07-31T11:37:17.157042: step 6634, loss 0.512452.
Train: 2018-07-31T11:37:17.328877: step 6635, loss 0.580191.
Train: 2018-07-31T11:37:17.500742: step 6636, loss 0.512542.
Train: 2018-07-31T11:37:17.656949: step 6637, loss 0.512497.
Train: 2018-07-31T11:37:17.828791: step 6638, loss 0.580146.
Train: 2018-07-31T11:37:18.016238: step 6639, loss 0.554684.
Train: 2018-07-31T11:37:18.188083: step 6640, loss 0.580133.
Test: 2018-07-31T11:37:18.656722: step 6640, loss 0.557359.
Train: 2018-07-31T11:37:18.828557: step 6641, loss 0.571615.
Train: 2018-07-31T11:37:19.000359: step 6642, loss 0.656827.
Train: 2018-07-31T11:37:19.172225: step 6643, loss 0.605653.
Train: 2018-07-31T11:37:19.328440: step 6644, loss 0.626013.
Train: 2018-07-31T11:37:19.500274: step 6645, loss 0.563071.
Train: 2018-07-31T11:37:19.672107: step 6646, loss 0.554604.
Train: 2018-07-31T11:37:19.843943: step 6647, loss 0.520765.
Train: 2018-07-31T11:37:20.031369: step 6648, loss 0.512283.
Train: 2018-07-31T11:37:20.203235: step 6649, loss 0.605402.
Train: 2018-07-31T11:37:20.359441: step 6650, loss 0.639328.
Test: 2018-07-31T11:37:20.843708: step 6650, loss 0.557284.
Train: 2018-07-31T11:37:21.015513: step 6651, loss 0.537583.
Train: 2018-07-31T11:37:21.187378: step 6652, loss 0.605352.
Train: 2018-07-31T11:37:21.359183: step 6653, loss 0.579915.
Train: 2018-07-31T11:37:21.531017: step 6654, loss 0.613742.
Train: 2018-07-31T11:37:21.718503: step 6655, loss 0.55453.
Train: 2018-07-31T11:37:21.890338: step 6656, loss 0.605179.
Train: 2018-07-31T11:37:22.062175: step 6657, loss 0.579837.
Train: 2018-07-31T11:37:22.233979: step 6658, loss 0.65556.
Train: 2018-07-31T11:37:22.405813: step 6659, loss 0.571402.
Train: 2018-07-31T11:37:22.577647: step 6660, loss 0.571402.
Test: 2018-07-31T11:37:23.046287: step 6660, loss 0.55742.
Train: 2018-07-31T11:37:23.233773: step 6661, loss 0.554705.
Train: 2018-07-31T11:37:23.389982: step 6662, loss 0.579734.
Train: 2018-07-31T11:37:23.577438: step 6663, loss 0.613012.
Train: 2018-07-31T11:37:23.749248: step 6664, loss 0.637845.
Train: 2018-07-31T11:37:23.921112: step 6665, loss 0.538292.
Train: 2018-07-31T11:37:24.092933: step 6666, loss 0.587936.
Train: 2018-07-31T11:37:24.264752: step 6667, loss 0.612645.
Train: 2018-07-31T11:37:24.436587: step 6668, loss 0.54675.
Train: 2018-07-31T11:37:24.592801: step 6669, loss 0.555002.
Train: 2018-07-31T11:37:24.764665: step 6670, loss 0.563215.
Test: 2018-07-31T11:37:25.248896: step 6670, loss 0.557672.
Train: 2018-07-31T11:37:25.420732: step 6671, loss 0.538597.
Train: 2018-07-31T11:37:25.592596: step 6672, loss 0.546763.
Train: 2018-07-31T11:37:25.748810: step 6673, loss 0.579589.
Train: 2018-07-31T11:37:25.936268: step 6674, loss 0.554877.
Train: 2018-07-31T11:37:26.108101: step 6675, loss 0.48884.
Train: 2018-07-31T11:37:26.279905: step 6676, loss 0.562995.
Train: 2018-07-31T11:37:26.451741: step 6677, loss 0.596161.
Train: 2018-07-31T11:37:26.623599: step 6678, loss 0.512873.
Train: 2018-07-31T11:37:26.795441: step 6679, loss 0.562801.
Train: 2018-07-31T11:37:26.967275: step 6680, loss 0.562736.
Test: 2018-07-31T11:37:27.435883: step 6680, loss 0.556986.
Train: 2018-07-31T11:37:27.623342: step 6681, loss 0.587973.
Train: 2018-07-31T11:37:27.779555: step 6682, loss 0.554168.
Train: 2018-07-31T11:37:27.951421: step 6683, loss 0.621986.
Train: 2018-07-31T11:37:28.123255: step 6684, loss 0.588055.
Train: 2018-07-31T11:37:28.295089: step 6685, loss 0.605093.
Train: 2018-07-31T11:37:28.466893: step 6686, loss 0.562506.
Train: 2018-07-31T11:37:28.638759: step 6687, loss 0.545439.
Train: 2018-07-31T11:37:28.810595: step 6688, loss 0.596604.
Train: 2018-07-31T11:37:28.982427: step 6689, loss 0.579527.
Train: 2018-07-31T11:37:29.154263: step 6690, loss 0.62223.
Test: 2018-07-31T11:37:29.622902: step 6690, loss 0.556671.
Train: 2018-07-31T11:37:29.794707: step 6691, loss 0.613644.
Train: 2018-07-31T11:37:29.966542: step 6692, loss 0.519827.
Train: 2018-07-31T11:37:30.138376: step 6693, loss 0.579466.
Train: 2018-07-31T11:37:30.310211: step 6694, loss 0.511329.
Train: 2018-07-31T11:37:30.482047: step 6695, loss 0.587972.
Train: 2018-07-31T11:37:30.653881: step 6696, loss 0.62208.
Train: 2018-07-31T11:37:30.825716: step 6697, loss 0.553858.
Train: 2018-07-31T11:37:30.997581: step 6698, loss 0.511245.
Train: 2018-07-31T11:37:31.169385: step 6699, loss 0.613538.
Train: 2018-07-31T11:37:31.341220: step 6700, loss 0.519673.
Test: 2018-07-31T11:37:31.809890: step 6700, loss 0.556549.
Train: 2018-07-31T11:37:32.544064: step 6701, loss 0.605033.
Train: 2018-07-31T11:37:32.715898: step 6702, loss 0.5623.
Train: 2018-07-31T11:37:32.887732: step 6703, loss 0.570837.
Train: 2018-07-31T11:37:33.059568: step 6704, loss 0.587941.
Train: 2018-07-31T11:37:33.231433: step 6705, loss 0.613608.
Train: 2018-07-31T11:37:33.403237: step 6706, loss 0.605013.
Train: 2018-07-31T11:37:33.575072: step 6707, loss 0.596415.
Train: 2018-07-31T11:37:33.746907: step 6708, loss 0.596361.
Train: 2018-07-31T11:37:33.918742: step 6709, loss 0.647331.
Train: 2018-07-31T11:37:34.090576: step 6710, loss 0.553831.
Test: 2018-07-31T11:37:34.559246: step 6710, loss 0.556625.
Train: 2018-07-31T11:37:34.731051: step 6711, loss 0.613034.
Train: 2018-07-31T11:37:34.918532: step 6712, loss 0.562359.
Train: 2018-07-31T11:37:35.090368: step 6713, loss 0.562385.
Train: 2018-07-31T11:37:35.262213: step 6714, loss 0.587545.
Train: 2018-07-31T11:37:35.434011: step 6715, loss 0.528982.
Train: 2018-07-31T11:37:35.605847: step 6716, loss 0.520666.
Train: 2018-07-31T11:37:35.777712: step 6717, loss 0.528991.
Train: 2018-07-31T11:37:35.949517: step 6718, loss 0.537281.
Train: 2018-07-31T11:37:36.121381: step 6719, loss 0.528801.
Train: 2018-07-31T11:37:36.293186: step 6720, loss 0.545474.
Test: 2018-07-31T11:37:36.777446: step 6720, loss 0.556551.
Train: 2018-07-31T11:37:36.949282: step 6721, loss 0.562245.
Train: 2018-07-31T11:37:37.105495: step 6722, loss 0.587599.
Train: 2018-07-31T11:37:37.277361: step 6723, loss 0.553669.
Train: 2018-07-31T11:37:37.449166: step 6724, loss 0.587659.
Train: 2018-07-31T11:37:37.621000: step 6725, loss 0.655937.
Train: 2018-07-31T11:37:37.792834: step 6726, loss 0.596208.
Train: 2018-07-31T11:37:37.964668: step 6727, loss 0.562072.
Train: 2018-07-31T11:37:38.136535: step 6728, loss 0.553537.
Train: 2018-07-31T11:37:38.308368: step 6729, loss 0.596171.
Train: 2018-07-31T11:37:38.480174: step 6730, loss 0.604681.
Test: 2018-07-31T11:37:38.964465: step 6730, loss 0.556299.
Train: 2018-07-31T11:37:39.136294: step 6731, loss 0.672788.
Train: 2018-07-31T11:37:39.308104: step 6732, loss 0.553577.
Train: 2018-07-31T11:37:39.464318: step 6733, loss 0.612919.
Train: 2018-07-31T11:37:39.651774: step 6734, loss 0.519879.
Train: 2018-07-31T11:37:39.823641: step 6735, loss 0.578988.
Train: 2018-07-31T11:37:39.995443: step 6736, loss 0.595815.
Train: 2018-07-31T11:37:40.182924: step 6737, loss 0.620987.
Train: 2018-07-31T11:37:40.354764: step 6738, loss 0.604089.
Train: 2018-07-31T11:37:40.526599: step 6739, loss 0.520409.
Train: 2018-07-31T11:37:40.698405: step 6740, loss 0.520482.
Test: 2018-07-31T11:37:41.167074: step 6740, loss 0.556574.
Train: 2018-07-31T11:37:41.338879: step 6741, loss 0.545513.
Train: 2018-07-31T11:37:41.510714: step 6742, loss 0.528782.
Train: 2018-07-31T11:37:41.682581: step 6743, loss 0.570519.
Train: 2018-07-31T11:37:41.838792: step 6744, loss 0.545365.
Train: 2018-07-31T11:37:42.010596: step 6745, loss 0.562085.
Train: 2018-07-31T11:37:42.182431: step 6746, loss 0.536801.
Train: 2018-07-31T11:37:42.369888: step 6747, loss 0.511366.
Train: 2018-07-31T11:37:42.541722: step 6748, loss 0.612786.
Train: 2018-07-31T11:37:42.713587: step 6749, loss 0.570406.
Train: 2018-07-31T11:37:42.869800: step 6750, loss 0.604457.
Test: 2018-07-31T11:37:43.354062: step 6750, loss 0.556098.
Train: 2018-07-31T11:37:43.525897: step 6751, loss 0.493621.
Train: 2018-07-31T11:37:43.713323: step 6752, loss 0.664472.
Train: 2018-07-31T11:37:43.885157: step 6753, loss 0.57036.
Train: 2018-07-31T11:37:44.056993: step 6754, loss 0.536087.
Train: 2018-07-31T11:37:44.228852: step 6755, loss 0.536033.
Train: 2018-07-31T11:37:44.385040: step 6756, loss 0.56174.
Train: 2018-07-31T11:37:44.572497: step 6757, loss 0.587547.
Train: 2018-07-31T11:37:44.744332: step 6758, loss 0.622049.
Train: 2018-07-31T11:37:44.900575: step 6759, loss 0.535822.
Train: 2018-07-31T11:37:45.072379: step 6760, loss 0.613447.
Test: 2018-07-31T11:37:45.556642: step 6760, loss 0.555846.
Train: 2018-07-31T11:37:45.728476: step 6761, loss 0.553041.
Train: 2018-07-31T11:37:45.884720: step 6762, loss 0.51852.
Train: 2018-07-31T11:37:46.072145: step 6763, loss 0.561641.
Train: 2018-07-31T11:37:46.228359: step 6764, loss 0.57892.
Train: 2018-07-31T11:37:46.400218: step 6765, loss 0.570265.
Train: 2018-07-31T11:37:46.572028: step 6766, loss 0.587582.
Train: 2018-07-31T11:37:46.743895: step 6767, loss 0.561586.
Train: 2018-07-31T11:37:46.931352: step 6768, loss 0.535574.
Train: 2018-07-31T11:37:47.103178: step 6769, loss 0.561561.
Train: 2018-07-31T11:37:47.259398: step 6770, loss 0.596285.
Test: 2018-07-31T11:37:47.743629: step 6770, loss 0.555674.
Train: 2018-07-31T11:37:47.915469: step 6771, loss 0.552849.
Train: 2018-07-31T11:37:48.087328: step 6772, loss 0.544139.
Train: 2018-07-31T11:37:48.243542: step 6773, loss 0.570212.
Train: 2018-07-31T11:37:48.415377: step 6774, loss 0.526662.
Train: 2018-07-31T11:37:48.587211: step 6775, loss 0.500422.
Train: 2018-07-31T11:37:48.759046: step 6776, loss 0.535218.
Train: 2018-07-31T11:37:48.930852: step 6777, loss 0.605314.
Train: 2018-07-31T11:37:49.102685: step 6778, loss 0.552632.
Train: 2018-07-31T11:37:49.274551: step 6779, loss 0.561411.
Train: 2018-07-31T11:37:49.461977: step 6780, loss 0.596707.
Test: 2018-07-31T11:37:49.930617: step 6780, loss 0.555426.
Train: 2018-07-31T11:37:50.102477: step 6781, loss 0.508371.
Train: 2018-07-31T11:37:50.274287: step 6782, loss 0.561376.
Train: 2018-07-31T11:37:50.446121: step 6783, loss 0.561366.
Train: 2018-07-31T11:37:50.617955: step 6784, loss 0.596899.
Train: 2018-07-31T11:37:50.774169: step 6785, loss 0.614705.
Train: 2018-07-31T11:37:50.946004: step 6786, loss 0.588009.
Train: 2018-07-31T11:37:51.117838: step 6787, loss 0.543568.
Train: 2018-07-31T11:37:51.289674: step 6788, loss 0.525809.
Train: 2018-07-31T11:37:51.461509: step 6789, loss 0.552434.
Train: 2018-07-31T11:37:51.633344: step 6790, loss 0.507984.
Test: 2018-07-31T11:37:52.102013: step 6790, loss 0.555293.
Train: 2018-07-31T11:37:52.273821: step 6791, loss 0.5613.
Train: 2018-07-31T11:37:52.445653: step 6792, loss 0.561291.
Train: 2018-07-31T11:37:52.601897: step 6793, loss 0.561284.
Train: 2018-07-31T11:37:52.773725: step 6794, loss 0.552341.
Train: 2018-07-31T11:37:52.929916: step 6795, loss 0.570216.
Train: 2018-07-31T11:37:53.117401: step 6796, loss 0.552309.
Train: 2018-07-31T11:37:53.273617: step 6797, loss 0.561256.
Train: 2018-07-31T11:37:53.445451: step 6798, loss 0.624037.
Train: 2018-07-31T11:37:53.617278: step 6799, loss 0.615029.
Train: 2018-07-31T11:37:53.789088: step 6800, loss 0.641784.
Test: 2018-07-31T11:37:54.273349: step 6800, loss 0.5552.
Train: 2018-07-31T11:37:54.991931: step 6801, loss 0.596912.
Train: 2018-07-31T11:37:55.163797: step 6802, loss 0.54343.
Train: 2018-07-31T11:37:55.335600: step 6803, loss 0.561207.
Train: 2018-07-31T11:37:55.507436: step 6804, loss 0.437378.
Train: 2018-07-31T11:37:55.679270: step 6805, loss 0.525805.
Train: 2018-07-31T11:37:55.851106: step 6806, loss 0.578898.
Train: 2018-07-31T11:37:56.022972: step 6807, loss 0.596624.
Train: 2018-07-31T11:37:56.194774: step 6808, loss 0.596611.
Train: 2018-07-31T11:37:56.366609: step 6809, loss 0.570015.
Train: 2018-07-31T11:37:56.538474: step 6810, loss 0.62307.
Test: 2018-07-31T11:37:57.007115: step 6810, loss 0.555197.
Train: 2018-07-31T11:37:57.194542: step 6811, loss 0.569979.
Train: 2018-07-31T11:37:57.366377: step 6812, loss 0.614006.
Train: 2018-07-31T11:37:57.538235: step 6813, loss 0.587504.
Train: 2018-07-31T11:37:57.710076: step 6814, loss 0.508606.
Train: 2018-07-31T11:37:57.881881: step 6815, loss 0.526175.
Train: 2018-07-31T11:37:58.053747: step 6816, loss 0.569887.
Train: 2018-07-31T11:37:58.225549: step 6817, loss 0.569876.
Train: 2018-07-31T11:37:58.397384: step 6818, loss 0.578594.
Train: 2018-07-31T11:37:58.569251: step 6819, loss 0.622181.
Train: 2018-07-31T11:37:58.741084: step 6820, loss 0.604657.
Test: 2018-07-31T11:37:59.209724: step 6820, loss 0.555286.
Train: 2018-07-31T11:37:59.381529: step 6821, loss 0.613234.
Train: 2018-07-31T11:37:59.553364: step 6822, loss 0.509237.
Train: 2018-07-31T11:37:59.725198: step 6823, loss 0.612992.
Train: 2018-07-31T11:37:59.897064: step 6824, loss 0.500854.
Train: 2018-07-31T11:38:00.053246: step 6825, loss 0.543951.
Train: 2018-07-31T11:38:00.225112: step 6826, loss 0.552556.
Train: 2018-07-31T11:38:00.396916: step 6827, loss 0.561156.
Train: 2018-07-31T11:38:00.568775: step 6828, loss 0.621437.
Train: 2018-07-31T11:38:00.756237: step 6829, loss 0.526721.
Train: 2018-07-31T11:38:00.928041: step 6830, loss 0.586958.
Test: 2018-07-31T11:38:01.396712: step 6830, loss 0.555324.
Train: 2018-07-31T11:38:01.568549: step 6831, loss 0.569736.
Train: 2018-07-31T11:38:01.740381: step 6832, loss 0.647159.
Train: 2018-07-31T11:38:01.912216: step 6833, loss 0.509611.
Train: 2018-07-31T11:38:02.084046: step 6834, loss 0.58688.
Train: 2018-07-31T11:38:02.240259: step 6835, loss 0.535397.
Train: 2018-07-31T11:38:02.427721: step 6836, loss 0.56112.
Train: 2018-07-31T11:38:02.599525: step 6837, loss 0.552531.
Train: 2018-07-31T11:38:02.771361: step 6838, loss 0.526757.
Train: 2018-07-31T11:38:02.943227: step 6839, loss 0.51809.
Train: 2018-07-31T11:38:03.115030: step 6840, loss 0.474866.
Test: 2018-07-31T11:38:03.599321: step 6840, loss 0.555176.
Train: 2018-07-31T11:38:03.771126: step 6841, loss 0.630254.
Train: 2018-07-31T11:38:03.942961: step 6842, loss 0.569669.
Train: 2018-07-31T11:38:04.114796: step 6843, loss 0.55228.
Train: 2018-07-31T11:38:04.286630: step 6844, loss 0.587094.
Train: 2018-07-31T11:38:04.458497: step 6845, loss 0.595846.
Train: 2018-07-31T11:38:04.630324: step 6846, loss 0.552197.
Train: 2018-07-31T11:38:04.802134: step 6847, loss 0.648332.
Train: 2018-07-31T11:38:04.973994: step 6848, loss 0.517246.
Train: 2018-07-31T11:38:05.130213: step 6849, loss 0.604585.
Train: 2018-07-31T11:38:05.302018: step 6850, loss 0.552172.
Test: 2018-07-31T11:38:05.770691: step 6850, loss 0.555008.
Train: 2018-07-31T11:38:05.942492: step 6851, loss 0.491075.
Train: 2018-07-31T11:38:06.114357: step 6852, loss 0.534666.
Train: 2018-07-31T11:38:06.286192: step 6853, loss 0.569624.
Train: 2018-07-31T11:38:06.473620: step 6854, loss 0.587161.
Train: 2018-07-31T11:38:06.645480: step 6855, loss 0.560846.
Train: 2018-07-31T11:38:06.801703: step 6856, loss 0.516914.
Train: 2018-07-31T11:38:06.973531: step 6857, loss 0.622422.
Train: 2018-07-31T11:38:07.145366: step 6858, loss 0.666454.
Train: 2018-07-31T11:38:07.317170: step 6859, loss 0.552026.
Train: 2018-07-31T11:38:07.489035: step 6860, loss 0.613466.
Test: 2018-07-31T11:38:07.957646: step 6860, loss 0.554904.
Train: 2018-07-31T11:38:08.129480: step 6861, loss 0.525787.
Train: 2018-07-31T11:38:08.301344: step 6862, loss 0.595793.
Train: 2018-07-31T11:38:08.473150: step 6863, loss 0.613195.
Train: 2018-07-31T11:38:08.644985: step 6864, loss 0.560817.
Train: 2018-07-31T11:38:08.816819: step 6865, loss 0.595579.
Train: 2018-07-31T11:38:08.988684: step 6866, loss 0.50883.
Train: 2018-07-31T11:38:09.160489: step 6867, loss 0.50023.
Train: 2018-07-31T11:38:09.363566: step 6868, loss 0.569482.
Train: 2018-07-31T11:38:09.535402: step 6869, loss 0.586804.
Train: 2018-07-31T11:38:09.707266: step 6870, loss 0.578132.
Test: 2018-07-31T11:38:10.175875: step 6870, loss 0.554957.
Train: 2018-07-31T11:38:10.347741: step 6871, loss 0.595442.
Train: 2018-07-31T11:38:10.519545: step 6872, loss 0.552148.
Train: 2018-07-31T11:38:10.691381: step 6873, loss 0.578093.
Train: 2018-07-31T11:38:10.863245: step 6874, loss 0.586723.
Train: 2018-07-31T11:38:11.035074: step 6875, loss 0.474452.
Train: 2018-07-31T11:38:11.206884: step 6876, loss 0.508918.
Train: 2018-07-31T11:38:11.363128: step 6877, loss 0.586748.
Train: 2018-07-31T11:38:11.534963: step 6878, loss 0.59545.
Train: 2018-07-31T11:38:11.706769: step 6879, loss 0.543361.
Train: 2018-07-31T11:38:11.878603: step 6880, loss 0.578106.
Test: 2018-07-31T11:38:12.347272: step 6880, loss 0.554832.
Train: 2018-07-31T11:38:12.519077: step 6881, loss 0.508489.
Train: 2018-07-31T11:38:12.706534: step 6882, loss 0.43861.
Train: 2018-07-31T11:38:12.878398: step 6883, loss 0.586937.
Train: 2018-07-31T11:38:13.050202: step 6884, loss 0.534274.
Train: 2018-07-31T11:38:13.222037: step 6885, loss 0.498869.
Train: 2018-07-31T11:38:13.378281: step 6886, loss 0.658109.
Train: 2018-07-31T11:38:13.550116: step 6887, loss 0.533952.
Train: 2018-07-31T11:38:13.721921: step 6888, loss 0.61403.
Train: 2018-07-31T11:38:13.893787: step 6889, loss 0.560588.
Train: 2018-07-31T11:38:14.065590: step 6890, loss 0.560582.
Test: 2018-07-31T11:38:14.549882: step 6890, loss 0.554546.
Train: 2018-07-31T11:38:14.721688: step 6891, loss 0.569511.
Train: 2018-07-31T11:38:14.893522: step 6892, loss 0.614219.
Train: 2018-07-31T11:38:15.065356: step 6893, loss 0.5695.
Train: 2018-07-31T11:38:15.237220: step 6894, loss 0.560556.
Train: 2018-07-31T11:38:15.393404: step 6895, loss 0.578402.
Train: 2018-07-31T11:38:15.565271: step 6896, loss 0.596218.
Train: 2018-07-31T11:38:15.737105: step 6897, loss 0.533819.
Train: 2018-07-31T11:38:15.908908: step 6898, loss 0.50714.
Train: 2018-07-31T11:38:16.080744: step 6899, loss 0.62283.
Train: 2018-07-31T11:38:16.252578: step 6900, loss 0.498267.
Test: 2018-07-31T11:38:16.736869: step 6900, loss 0.554506.
Train: 2018-07-31T11:38:17.471042: step 6901, loss 0.542716.
Train: 2018-07-31T11:38:17.642877: step 6902, loss 0.694045.
Train: 2018-07-31T11:38:17.814712: step 6903, loss 0.587151.
Train: 2018-07-31T11:38:17.986577: step 6904, loss 0.649134.
Train: 2018-07-31T11:38:18.142762: step 6905, loss 0.481023.
Train: 2018-07-31T11:38:18.330216: step 6906, loss 0.516421.
Train: 2018-07-31T11:38:18.486461: step 6907, loss 0.595715.
Train: 2018-07-31T11:38:18.658264: step 6908, loss 0.560482.
Train: 2018-07-31T11:38:18.830124: step 6909, loss 0.578052.
Train: 2018-07-31T11:38:19.001964: step 6910, loss 0.578027.
Test: 2018-07-31T11:38:19.486196: step 6910, loss 0.554564.
Train: 2018-07-31T11:38:19.658032: step 6911, loss 0.551714.
Train: 2018-07-31T11:38:19.829864: step 6912, loss 0.656753.
Train: 2018-07-31T11:38:20.001701: step 6913, loss 0.595391.
Train: 2018-07-31T11:38:20.173535: step 6914, loss 0.56919.
Train: 2018-07-31T11:38:20.345370: step 6915, loss 0.595197.
Train: 2018-07-31T11:38:20.501613: step 6916, loss 0.482704.
Train: 2018-07-31T11:38:20.673418: step 6917, loss 0.53461.
Train: 2018-07-31T11:38:20.845252: step 6918, loss 0.543248.
Train: 2018-07-31T11:38:21.032710: step 6919, loss 0.577775.
Train: 2018-07-31T11:38:21.204544: step 6920, loss 0.629574.
Test: 2018-07-31T11:38:21.673184: step 6920, loss 0.554685.
Train: 2018-07-31T11:38:21.845018: step 6921, loss 0.543258.
Train: 2018-07-31T11:38:22.016852: step 6922, loss 0.560502.
Train: 2018-07-31T11:38:22.188720: step 6923, loss 0.594951.
Train: 2018-07-31T11:38:22.360553: step 6924, loss 0.517478.
Train: 2018-07-31T11:38:22.532388: step 6925, loss 0.569099.
Train: 2018-07-31T11:38:22.704225: step 6926, loss 0.491623.
Train: 2018-07-31T11:38:22.891649: step 6927, loss 0.551842.
Train: 2018-07-31T11:38:23.047898: step 6928, loss 0.569085.
Train: 2018-07-31T11:38:23.235346: step 6929, loss 0.508498.
Train: 2018-07-31T11:38:23.407177: step 6930, loss 0.595118.
Test: 2018-07-31T11:38:23.875822: step 6930, loss 0.554519.
Train: 2018-07-31T11:38:24.094521: step 6931, loss 0.603862.
Train: 2018-07-31T11:38:24.266327: step 6932, loss 0.595187.
Train: 2018-07-31T11:38:24.438193: step 6933, loss 0.656121.
Train: 2018-07-31T11:38:24.594374: step 6934, loss 0.542991.
Train: 2018-07-31T11:38:24.781830: step 6935, loss 0.612456.
Train: 2018-07-31T11:38:24.953665: step 6936, loss 0.586369.
Train: 2018-07-31T11:38:25.125500: step 6937, loss 0.577676.
Train: 2018-07-31T11:38:25.297335: step 6938, loss 0.577648.
Train: 2018-07-31T11:38:25.469170: step 6939, loss 0.612055.
Train: 2018-07-31T11:38:25.641005: step 6940, loss 0.646259.
Test: 2018-07-31T11:38:26.125296: step 6940, loss 0.554686.
Train: 2018-07-31T11:38:26.297100: step 6941, loss 0.543359.
Train: 2018-07-31T11:38:26.468935: step 6942, loss 0.637152.
Train: 2018-07-31T11:38:26.640771: step 6943, loss 0.577483.
Train: 2018-07-31T11:38:26.812637: step 6944, loss 0.577454.
Train: 2018-07-31T11:38:26.984439: step 6945, loss 0.543772.
Train: 2018-07-31T11:38:27.140684: step 6946, loss 0.51532.
Train: 2018-07-31T11:38:27.312489: step 6947, loss 0.594165.
Train: 2018-07-31T11:38:27.468732: step 6948, loss 0.602496.
Train: 2018-07-31T11:38:27.656188: step 6949, loss 0.56903.
Train: 2018-07-31T11:38:27.812403: step 6950, loss 0.510676.
Test: 2018-07-31T11:38:28.296663: step 6950, loss 0.555067.
Train: 2018-07-31T11:38:28.468497: step 6951, loss 0.594038.
Train: 2018-07-31T11:38:28.640334: step 6952, loss 0.527364.
Train: 2018-07-31T11:38:28.812137: step 6953, loss 0.544002.
Train: 2018-07-31T11:38:28.983996: step 6954, loss 0.610745.
Train: 2018-07-31T11:38:29.155832: step 6955, loss 0.594049.
Train: 2018-07-31T11:38:29.327642: step 6956, loss 0.560637.
Train: 2018-07-31T11:38:29.499475: step 6957, loss 0.585686.
Train: 2018-07-31T11:38:29.671335: step 6958, loss 0.660851.
Train: 2018-07-31T11:38:29.843176: step 6959, loss 0.51063.
Train: 2018-07-31T11:38:30.015010: step 6960, loss 0.543978.
Test: 2018-07-31T11:38:30.483619: step 6960, loss 0.555007.
Train: 2018-07-31T11:38:30.655482: step 6961, loss 0.585642.
Train: 2018-07-31T11:38:30.842936: step 6962, loss 0.577299.
Train: 2018-07-31T11:38:31.014746: step 6963, loss 0.577294.
Train: 2018-07-31T11:38:31.186580: step 6964, loss 0.618981.
Train: 2018-07-31T11:38:31.342794: step 6965, loss 0.593941.
Train: 2018-07-31T11:38:31.514628: step 6966, loss 0.519034.
Train: 2018-07-31T11:38:31.686464: step 6967, loss 0.63551.
Train: 2018-07-31T11:38:31.858323: step 6968, loss 0.510777.
Train: 2018-07-31T11:38:32.030163: step 6969, loss 0.56894.
Train: 2018-07-31T11:38:32.201967: step 6970, loss 0.535663.
Test: 2018-07-31T11:38:32.670607: step 6970, loss 0.554968.
Train: 2018-07-31T11:38:32.858065: step 6971, loss 0.543932.
Train: 2018-07-31T11:38:33.029929: step 6972, loss 0.535522.
Train: 2018-07-31T11:38:33.201764: step 6973, loss 0.585609.
Train: 2018-07-31T11:38:33.389214: step 6974, loss 0.56886.
Train: 2018-07-31T11:38:33.545434: step 6975, loss 0.552044.
Train: 2018-07-31T11:38:33.717268: step 6976, loss 0.585668.
Train: 2018-07-31T11:38:33.889072: step 6977, loss 0.560382.
Train: 2018-07-31T11:38:34.060940: step 6978, loss 0.619503.
Train: 2018-07-31T11:38:34.232773: step 6979, loss 0.518065.
Train: 2018-07-31T11:38:34.404578: step 6980, loss 0.611135.
Test: 2018-07-31T11:38:34.873249: step 6980, loss 0.554582.
Train: 2018-07-31T11:38:35.060674: step 6981, loss 0.568777.
Train: 2018-07-31T11:38:35.216917: step 6982, loss 0.568769.
Train: 2018-07-31T11:38:35.404374: step 6983, loss 0.5433.
Train: 2018-07-31T11:38:35.560583: step 6984, loss 0.611243.
Train: 2018-07-31T11:38:35.732392: step 6985, loss 0.551748.
Train: 2018-07-31T11:38:35.904226: step 6986, loss 0.543226.
Train: 2018-07-31T11:38:36.076060: step 6987, loss 0.585762.
Train: 2018-07-31T11:38:36.247920: step 6988, loss 0.619849.
Train: 2018-07-31T11:38:36.419731: step 6989, loss 0.568721.
Train: 2018-07-31T11:38:36.591565: step 6990, loss 0.628309.
Test: 2018-07-31T11:38:37.060237: step 6990, loss 0.554478.
Train: 2018-07-31T11:38:37.232041: step 6991, loss 0.560213.
Train: 2018-07-31T11:38:37.419495: step 6992, loss 0.543242.
Train: 2018-07-31T11:38:37.575709: step 6993, loss 0.602646.
Train: 2018-07-31T11:38:37.747544: step 6994, loss 0.59413.
Train: 2018-07-31T11:38:37.919378: step 6995, loss 0.568701.
Train: 2018-07-31T11:38:38.091213: step 6996, loss 0.61941.
Train: 2018-07-31T11:38:38.263048: step 6997, loss 0.602433.
Train: 2018-07-31T11:38:38.434915: step 6998, loss 0.619168.
Train: 2018-07-31T11:38:38.606719: step 6999, loss 0.551956.
Train: 2018-07-31T11:38:38.778552: step 7000, loss 0.568727.
Test: 2018-07-31T11:38:39.247223: step 7000, loss 0.554766.
Train: 2018-07-31T11:38:39.965782: step 7001, loss 0.493672.
Train: 2018-07-31T11:38:40.137609: step 7002, loss 0.560391.
Train: 2018-07-31T11:38:40.309445: step 7003, loss 0.610425.
Train: 2018-07-31T11:38:40.481280: step 7004, loss 0.585392.
Train: 2018-07-31T11:38:40.653114: step 7005, loss 0.568721.
Train: 2018-07-31T11:38:40.824948: step 7006, loss 0.49381.
Train: 2018-07-31T11:38:40.996814: step 7007, loss 0.527033.
Train: 2018-07-31T11:38:41.184239: step 7008, loss 0.568686.
Train: 2018-07-31T11:38:41.356074: step 7009, loss 0.57704.
Train: 2018-07-31T11:38:41.527939: step 7010, loss 0.543489.
Test: 2018-07-31T11:38:42.012201: step 7010, loss 0.554554.
Train: 2018-07-31T11:38:42.184031: step 7011, loss 0.501372.
Train: 2018-07-31T11:38:42.355841: step 7012, loss 0.568615.
Train: 2018-07-31T11:38:42.527676: step 7013, loss 0.543192.
Train: 2018-07-31T11:38:42.683920: step 7014, loss 0.577084.
Train: 2018-07-31T11:38:42.855753: step 7015, loss 0.542991.
Train: 2018-07-31T11:38:43.027588: step 7016, loss 0.619905.
Train: 2018-07-31T11:38:43.215014: step 7017, loss 0.585706.
Train: 2018-07-31T11:38:43.386880: step 7018, loss 0.542794.
Train: 2018-07-31T11:38:43.558684: step 7019, loss 0.602955.
Train: 2018-07-31T11:38:43.730551: step 7020, loss 0.534104.
Test: 2018-07-31T11:38:44.214780: step 7020, loss 0.554098.
Train: 2018-07-31T11:38:44.386614: step 7021, loss 0.490934.
Train: 2018-07-31T11:38:44.558477: step 7022, loss 0.516646.
Train: 2018-07-31T11:38:44.730315: step 7023, loss 0.594585.
Train: 2018-07-31T11:38:44.902143: step 7024, loss 0.559839.
Train: 2018-07-31T11:38:45.073954: step 7025, loss 0.673282.
Train: 2018-07-31T11:38:45.230197: step 7026, loss 0.542358.
Train: 2018-07-31T11:38:45.417623: step 7027, loss 0.638402.
Train: 2018-07-31T11:38:45.573836: step 7028, loss 0.559806.
Train: 2018-07-31T11:38:45.745701: step 7029, loss 0.568521.
Train: 2018-07-31T11:38:45.917536: step 7030, loss 0.577221.
Test: 2018-07-31T11:38:46.401768: step 7030, loss 0.553934.
Train: 2018-07-31T11:38:46.573632: step 7031, loss 0.603301.
Train: 2018-07-31T11:38:46.745468: step 7032, loss 0.533756.
Train: 2018-07-31T11:38:46.917272: step 7033, loss 0.611868.
Train: 2018-07-31T11:38:47.089108: step 7034, loss 0.516508.
Train: 2018-07-31T11:38:47.245356: step 7035, loss 0.481903.
Train: 2018-07-31T11:38:47.417187: step 7036, loss 0.577134.
Train: 2018-07-31T11:38:47.604635: step 7037, loss 0.559787.
Train: 2018-07-31T11:38:47.760824: step 7038, loss 0.577145.
Train: 2018-07-31T11:38:47.932660: step 7039, loss 0.664047.
Train: 2018-07-31T11:38:48.104495: step 7040, loss 0.577125.
Test: 2018-07-31T11:38:48.588755: step 7040, loss 0.55393.
Train: 2018-07-31T11:38:48.760621: step 7041, loss 0.525123.
Train: 2018-07-31T11:38:48.932449: step 7042, loss 0.525141.
Train: 2018-07-31T11:38:49.088669: step 7043, loss 0.577089.
Train: 2018-07-31T11:38:49.260503: step 7044, loss 0.568422.
Train: 2018-07-31T11:38:49.432340: step 7045, loss 0.568421.
Train: 2018-07-31T11:38:49.604143: step 7046, loss 0.637725.
Train: 2018-07-31T11:38:49.791600: step 7047, loss 0.542487.
Train: 2018-07-31T11:38:49.947843: step 7048, loss 0.542526.
Train: 2018-07-31T11:38:50.119677: step 7049, loss 0.525276.
Train: 2018-07-31T11:38:50.291483: step 7050, loss 0.594433.
Test: 2018-07-31T11:38:50.775773: step 7050, loss 0.554045.
Train: 2018-07-31T11:38:50.947578: step 7051, loss 0.559878.
Train: 2018-07-31T11:38:51.103791: step 7052, loss 0.533962.
Train: 2018-07-31T11:38:51.275657: step 7053, loss 0.585877.
Train: 2018-07-31T11:38:51.447461: step 7054, loss 0.559934.
Train: 2018-07-31T11:38:51.619297: step 7055, loss 0.551287.
Train: 2018-07-31T11:38:51.791131: step 7056, loss 0.542623.
Train: 2018-07-31T11:38:51.962965: step 7057, loss 0.638043.
Train: 2018-07-31T11:38:52.134801: step 7058, loss 0.594654.
Train: 2018-07-31T11:38:52.306635: step 7059, loss 0.551314.
Train: 2018-07-31T11:38:52.462878: step 7060, loss 0.637897.
Test: 2018-07-31T11:38:52.947140: step 7060, loss 0.554162.
Train: 2018-07-31T11:38:53.103324: step 7061, loss 0.577271.
Train: 2018-07-31T11:38:53.275188: step 7062, loss 0.542759.
Train: 2018-07-31T11:38:53.447023: step 7063, loss 0.508347.
Train: 2018-07-31T11:38:53.618858: step 7064, loss 0.568604.
Train: 2018-07-31T11:38:53.790664: step 7065, loss 0.628864.
Train: 2018-07-31T11:38:53.962497: step 7066, loss 0.585777.
Train: 2018-07-31T11:38:54.134356: step 7067, loss 0.542805.
Train: 2018-07-31T11:38:54.306167: step 7068, loss 0.559969.
Train: 2018-07-31T11:38:54.478033: step 7069, loss 0.611401.
Train: 2018-07-31T11:38:54.649867: step 7070, loss 0.559953.
Test: 2018-07-31T11:38:55.118506: step 7070, loss 0.554175.
Train: 2018-07-31T11:38:55.290341: step 7071, loss 0.57705.
Train: 2018-07-31T11:38:55.462147: step 7072, loss 0.594108.
Train: 2018-07-31T11:38:55.634011: step 7073, loss 0.594052.
Train: 2018-07-31T11:38:55.805848: step 7074, loss 0.585475.
Train: 2018-07-31T11:38:55.977680: step 7075, loss 0.576932.
Train: 2018-07-31T11:38:56.149486: step 7076, loss 0.551473.
Train: 2018-07-31T11:38:56.321321: step 7077, loss 0.534555.
Train: 2018-07-31T11:38:56.493185: step 7078, loss 0.57686.
Train: 2018-07-31T11:38:56.665013: step 7079, loss 0.576841.
Train: 2018-07-31T11:38:56.836848: step 7080, loss 0.50076.
Test: 2018-07-31T11:38:57.305496: step 7080, loss 0.554184.
Train: 2018-07-31T11:38:57.477300: step 7081, loss 0.509131.
Train: 2018-07-31T11:38:57.649135: step 7082, loss 0.508971.
Train: 2018-07-31T11:38:57.820969: step 7083, loss 0.58533.
Train: 2018-07-31T11:38:57.992803: step 7084, loss 0.568294.
Train: 2018-07-31T11:38:58.164638: step 7085, loss 0.525505.
Train: 2018-07-31T11:38:58.336503: step 7086, loss 0.559684.
Train: 2018-07-31T11:38:58.508307: step 7087, loss 0.54243.
Train: 2018-07-31T11:38:58.680167: step 7088, loss 0.455968.
Train: 2018-07-31T11:38:58.851978: step 7089, loss 0.620346.
Train: 2018-07-31T11:38:59.023837: step 7090, loss 0.568251.
Test: 2018-07-31T11:38:59.492453: step 7090, loss 0.553613.
Train: 2018-07-31T11:38:59.664287: step 7091, loss 0.55077.
Train: 2018-07-31T11:38:59.820532: step 7092, loss 0.577027.
Train: 2018-07-31T11:38:59.992365: step 7093, loss 0.612216.
Train: 2018-07-31T11:39:00.164195: step 7094, loss 0.568262.
Train: 2018-07-31T11:39:00.336036: step 7095, loss 0.533021.
Train: 2018-07-31T11:39:00.507869: step 7096, loss 0.515325.
Train: 2018-07-31T11:39:00.664053: step 7097, loss 0.605997.
Train: 2018-07-31T11:39:00.835887: step 7098, loss 0.577122.
Train: 2018-07-31T11:39:01.007723: step 7099, loss 0.532833.
Train: 2018-07-31T11:39:01.195179: step 7100, loss 0.5594.
Test: 2018-07-31T11:39:01.663819: step 7100, loss 0.553401.
Train: 2018-07-31T11:39:02.382431: step 7101, loss 0.648178.
Train: 2018-07-31T11:39:02.554236: step 7102, loss 0.612615.
Train: 2018-07-31T11:39:02.710479: step 7103, loss 0.568238.
Train: 2018-07-31T11:39:02.882283: step 7104, loss 0.594738.
Train: 2018-07-31T11:39:03.054118: step 7105, loss 0.629934.
Train: 2018-07-31T11:39:03.241605: step 7106, loss 0.568175.
Train: 2018-07-31T11:39:03.413409: step 7107, loss 0.550626.
Train: 2018-07-31T11:39:03.585251: step 7108, loss 0.550655.
Train: 2018-07-31T11:39:03.741487: step 7109, loss 0.568117.
Train: 2018-07-31T11:39:03.913294: step 7110, loss 0.576807.
Test: 2018-07-31T11:39:04.397584: step 7110, loss 0.553552.
Train: 2018-07-31T11:39:04.569389: step 7111, loss 0.550728.
Train: 2018-07-31T11:39:04.741253: step 7112, loss 0.550746.
Train: 2018-07-31T11:39:04.913088: step 7113, loss 0.559418.
Train: 2018-07-31T11:39:05.084923: step 7114, loss 0.533459.
Train: 2018-07-31T11:39:05.256728: step 7115, loss 0.542109.
Train: 2018-07-31T11:39:05.428562: step 7116, loss 0.498808.
Train: 2018-07-31T11:39:05.600422: step 7117, loss 0.602757.
Train: 2018-07-31T11:39:05.772231: step 7118, loss 0.646188.
Train: 2018-07-31T11:39:05.944097: step 7119, loss 0.628763.
Train: 2018-07-31T11:39:06.100310: step 7120, loss 0.576698.
Test: 2018-07-31T11:39:06.584571: step 7120, loss 0.553574.
Train: 2018-07-31T11:39:06.772022: step 7121, loss 0.559401.
Train: 2018-07-31T11:39:06.943832: step 7122, loss 0.542172.
Train: 2018-07-31T11:39:07.115697: step 7123, loss 0.585241.
Train: 2018-07-31T11:39:07.287502: step 7124, loss 0.559419.
Train: 2018-07-31T11:39:07.459367: step 7125, loss 0.61954.
Train: 2018-07-31T11:39:07.631198: step 7126, loss 0.508018.
Train: 2018-07-31T11:39:07.803007: step 7127, loss 0.602264.
Train: 2018-07-31T11:39:07.974866: step 7128, loss 0.610771.
Train: 2018-07-31T11:39:08.146706: step 7129, loss 0.61068.
Train: 2018-07-31T11:39:08.318536: step 7130, loss 0.593536.
Test: 2018-07-31T11:39:08.802772: step 7130, loss 0.553785.
Train: 2018-07-31T11:39:08.974637: step 7131, loss 0.576486.
Train: 2018-07-31T11:39:09.146465: step 7132, loss 0.491837.
Train: 2018-07-31T11:39:09.318278: step 7133, loss 0.62719.
Train: 2018-07-31T11:39:09.490111: step 7134, loss 0.627075.
Train: 2018-07-31T11:39:09.661946: step 7135, loss 0.542769.
Train: 2018-07-31T11:39:09.833807: step 7136, loss 0.559617.
Train: 2018-07-31T11:39:10.005615: step 7137, loss 0.64346.
Train: 2018-07-31T11:39:10.177451: step 7138, loss 0.601456.
Train: 2018-07-31T11:39:10.349286: step 7139, loss 0.526395.
Train: 2018-07-31T11:39:10.521149: step 7140, loss 0.54311.
Test: 2018-07-31T11:39:11.005381: step 7140, loss 0.554142.
Train: 2018-07-31T11:39:11.177248: step 7141, loss 0.543136.
Train: 2018-07-31T11:39:11.349051: step 7142, loss 0.57635.
Train: 2018-07-31T11:39:11.520916: step 7143, loss 0.551438.
Train: 2018-07-31T11:39:11.692721: step 7144, loss 0.551424.
Train: 2018-07-31T11:39:11.848933: step 7145, loss 0.601274.
Train: 2018-07-31T11:39:12.036414: step 7146, loss 0.609589.
Train: 2018-07-31T11:39:12.192633: step 7147, loss 0.592943.
Train: 2018-07-31T11:39:12.364437: step 7148, loss 0.568017.
Train: 2018-07-31T11:39:12.536303: step 7149, loss 0.609493.
Train: 2018-07-31T11:39:12.708138: step 7150, loss 0.53489.
Test: 2018-07-31T11:39:13.192399: step 7150, loss 0.554152.
Train: 2018-07-31T11:39:13.364204: step 7151, loss 0.518338.
Train: 2018-07-31T11:39:13.551692: step 7152, loss 0.584585.
Train: 2018-07-31T11:39:13.707904: step 7153, loss 0.543113.
Train: 2018-07-31T11:39:13.895330: step 7154, loss 0.501532.
Train: 2018-07-31T11:39:14.051575: step 7155, loss 0.601281.
Train: 2018-07-31T11:39:14.223378: step 7156, loss 0.542897.
Train: 2018-07-31T11:39:14.395236: step 7157, loss 0.601396.
Train: 2018-07-31T11:39:14.567078: step 7158, loss 0.593054.
Train: 2018-07-31T11:39:14.738883: step 7159, loss 0.559506.
Train: 2018-07-31T11:39:14.910717: step 7160, loss 0.525881.
Test: 2018-07-31T11:39:15.379358: step 7160, loss 0.553772.
Train: 2018-07-31T11:39:15.566838: step 7161, loss 0.559453.
Train: 2018-07-31T11:39:15.738647: step 7162, loss 0.550983.
Train: 2018-07-31T11:39:15.910515: step 7163, loss 0.550929.
Train: 2018-07-31T11:39:16.082348: step 7164, loss 0.559352.
Train: 2018-07-31T11:39:16.254151: step 7165, loss 0.567823.
Train: 2018-07-31T11:39:16.425987: step 7166, loss 0.567814.
Train: 2018-07-31T11:39:16.597821: step 7167, loss 0.550722.
Train: 2018-07-31T11:39:16.769688: step 7168, loss 0.610613.
Train: 2018-07-31T11:39:16.925894: step 7169, loss 0.499213.
Train: 2018-07-31T11:39:17.097735: step 7170, loss 0.60217.
Test: 2018-07-31T11:39:17.581996: step 7170, loss 0.55337.
Train: 2018-07-31T11:39:17.753832: step 7171, loss 0.628044.
Train: 2018-07-31T11:39:17.925668: step 7172, loss 0.645261.
Train: 2018-07-31T11:39:18.097501: step 7173, loss 0.559182.
Train: 2018-07-31T11:39:18.269330: step 7174, loss 0.550604.
Train: 2018-07-31T11:39:18.456761: step 7175, loss 0.567767.
Train: 2018-07-31T11:39:18.628621: step 7176, loss 0.576333.
Train: 2018-07-31T11:39:18.800430: step 7177, loss 0.550631.
Train: 2018-07-31T11:39:18.972291: step 7178, loss 0.627677.
Train: 2018-07-31T11:39:19.144131: step 7179, loss 0.59339.
Train: 2018-07-31T11:39:19.315935: step 7180, loss 0.508042.
Test: 2018-07-31T11:39:19.784605: step 7180, loss 0.553465.
Train: 2018-07-31T11:39:19.956440: step 7181, loss 0.525116.
Train: 2018-07-31T11:39:20.128244: step 7182, loss 0.559208.
Train: 2018-07-31T11:39:20.300112: step 7183, loss 0.533588.
Train: 2018-07-31T11:39:20.471914: step 7184, loss 0.533534.
Train: 2018-07-31T11:39:20.643779: step 7185, loss 0.653378.
Train: 2018-07-31T11:39:20.815614: step 7186, loss 0.610543.
Train: 2018-07-31T11:39:20.987449: step 7187, loss 0.550598.
Train: 2018-07-31T11:39:21.159254: step 7188, loss 0.524942.
Train: 2018-07-31T11:39:21.331118: step 7189, loss 0.550588.
Train: 2018-07-31T11:39:21.502922: step 7190, loss 0.576263.
Test: 2018-07-31T11:39:21.971563: step 7190, loss 0.55334.
Train: 2018-07-31T11:39:22.159019: step 7191, loss 0.593404.
Train: 2018-07-31T11:39:22.330855: step 7192, loss 0.550549.
Train: 2018-07-31T11:39:22.502719: step 7193, loss 0.499101.
Train: 2018-07-31T11:39:22.658933: step 7194, loss 0.541914.
Train: 2018-07-31T11:39:22.830736: step 7195, loss 0.610713.
Train: 2018-07-31T11:39:23.002572: step 7196, loss 0.490127.
Train: 2018-07-31T11:39:23.174438: step 7197, loss 0.498565.
Train: 2018-07-31T11:39:23.346272: step 7198, loss 0.593687.
Train: 2018-07-31T11:39:23.518106: step 7199, loss 0.550288.
Train: 2018-07-31T11:39:23.689911: step 7200, loss 0.567683.
Test: 2018-07-31T11:39:24.174202: step 7200, loss 0.553051.
Train: 2018-07-31T11:39:25.017753: step 7201, loss 0.550208.
Train: 2018-07-31T11:39:25.205213: step 7202, loss 0.497603.
Train: 2018-07-31T11:39:25.377015: step 7203, loss 0.646845.
Train: 2018-07-31T11:39:25.548850: step 7204, loss 0.602935.
Train: 2018-07-31T11:39:25.720715: step 7205, loss 0.506048.
Train: 2018-07-31T11:39:25.892551: step 7206, loss 0.505958.
Train: 2018-07-31T11:39:26.064354: step 7207, loss 0.576575.
Train: 2018-07-31T11:39:26.236214: step 7208, loss 0.558878.
Train: 2018-07-31T11:39:26.408057: step 7209, loss 0.576627.
Train: 2018-07-31T11:39:26.579884: step 7210, loss 0.558866.
Test: 2018-07-31T11:39:27.048532: step 7210, loss 0.552857.
Train: 2018-07-31T11:39:27.235980: step 7211, loss 0.549963.
Train: 2018-07-31T11:39:27.407791: step 7212, loss 0.612311.
Train: 2018-07-31T11:39:27.579625: step 7213, loss 0.585579.
Train: 2018-07-31T11:39:27.751459: step 7214, loss 0.523232.
Train: 2018-07-31T11:39:27.923294: step 7215, loss 0.496502.
Train: 2018-07-31T11:39:28.095129: step 7216, loss 0.567759.
Train: 2018-07-31T11:39:28.266994: step 7217, loss 0.549907.
Train: 2018-07-31T11:39:28.438800: step 7218, loss 0.549892.
Train: 2018-07-31T11:39:28.610634: step 7219, loss 0.56778.
Train: 2018-07-31T11:39:28.782501: step 7220, loss 0.639456.
Test: 2018-07-31T11:39:29.251139: step 7220, loss 0.55278.
Train: 2018-07-31T11:39:29.422943: step 7221, loss 0.514063.
Train: 2018-07-31T11:39:29.610399: step 7222, loss 0.549864.
Train: 2018-07-31T11:39:29.782264: step 7223, loss 0.531948.
Train: 2018-07-31T11:39:29.954119: step 7224, loss 0.514001.
Train: 2018-07-31T11:39:30.125905: step 7225, loss 0.576758.
Train: 2018-07-31T11:39:30.282147: step 7226, loss 0.486929.
Train: 2018-07-31T11:39:30.453951: step 7227, loss 0.58582.
Train: 2018-07-31T11:39:30.625786: step 7228, loss 0.594878.
Train: 2018-07-31T11:39:30.797621: step 7229, loss 0.567826.
Train: 2018-07-31T11:39:30.985077: step 7230, loss 0.522698.
Test: 2018-07-31T11:39:31.453748: step 7230, loss 0.552703.
Train: 2018-07-31T11:39:31.625553: step 7231, loss 0.59493.
Train: 2018-07-31T11:39:31.797412: step 7232, loss 0.585893.
Train: 2018-07-31T11:39:31.953631: step 7233, loss 0.612953.
Train: 2018-07-31T11:39:32.141057: step 7234, loss 0.585815.
Train: 2018-07-31T11:39:32.312922: step 7235, loss 0.630717.
Train: 2018-07-31T11:39:32.484756: step 7236, loss 0.531867.
Train: 2018-07-31T11:39:32.640939: step 7237, loss 0.603453.
Train: 2018-07-31T11:39:32.828395: step 7238, loss 0.54092.
Train: 2018-07-31T11:39:33.000230: step 7239, loss 0.532072.
Train: 2018-07-31T11:39:33.156474: step 7240, loss 0.603106.
Test: 2018-07-31T11:39:33.640706: step 7240, loss 0.552762.
Train: 2018-07-31T11:39:33.812540: step 7241, loss 0.505623.
Train: 2018-07-31T11:39:33.984405: step 7242, loss 0.594103.
Train: 2018-07-31T11:39:34.140589: step 7243, loss 0.558732.
Train: 2018-07-31T11:39:34.312423: step 7244, loss 0.585175.
Train: 2018-07-31T11:39:34.484258: step 7245, loss 0.488343.
Train: 2018-07-31T11:39:34.656126: step 7246, loss 0.54993.
Train: 2018-07-31T11:39:34.827928: step 7247, loss 0.567522.
Train: 2018-07-31T11:39:34.999792: step 7248, loss 0.717683.
Train: 2018-07-31T11:39:35.171622: step 7249, loss 0.523644.
Train: 2018-07-31T11:39:35.343431: step 7250, loss 0.506218.
Test: 2018-07-31T11:39:35.812102: step 7250, loss 0.552827.
Train: 2018-07-31T11:39:35.983939: step 7251, loss 0.59371.
Train: 2018-07-31T11:39:36.155774: step 7252, loss 0.619869.
Train: 2018-07-31T11:39:36.327576: step 7253, loss 0.593591.
Train: 2018-07-31T11:39:36.499411: step 7254, loss 0.523988.
Train: 2018-07-31T11:39:36.655625: step 7255, loss 0.550077.
Train: 2018-07-31T11:39:36.843110: step 7256, loss 0.50676.
Train: 2018-07-31T11:39:37.014946: step 7257, loss 0.472074.
Train: 2018-07-31T11:39:37.186751: step 7258, loss 0.567419.
Train: 2018-07-31T11:39:37.358585: step 7259, loss 0.532609.
Train: 2018-07-31T11:39:37.530450: step 7260, loss 0.541255.
Test: 2018-07-31T11:39:37.999085: step 7260, loss 0.552784.
Train: 2018-07-31T11:39:38.170895: step 7261, loss 0.628652.
Train: 2018-07-31T11:39:38.342759: step 7262, loss 0.567431.
Train: 2018-07-31T11:39:38.514594: step 7263, loss 0.602474.
Train: 2018-07-31T11:39:38.686429: step 7264, loss 0.479832.
Train: 2018-07-31T11:39:38.842643: step 7265, loss 0.51479.
Train: 2018-07-31T11:39:39.014447: step 7266, loss 0.593822.
Train: 2018-07-31T11:39:39.201904: step 7267, loss 0.620289.
Train: 2018-07-31T11:39:39.373737: step 7268, loss 0.523396.
Train: 2018-07-31T11:39:39.545605: step 7269, loss 0.505725.
Train: 2018-07-31T11:39:39.717438: step 7270, loss 0.593952.
Test: 2018-07-31T11:39:40.186078: step 7270, loss 0.552644.
Train: 2018-07-31T11:39:40.373504: step 7271, loss 0.602826.
Train: 2018-07-31T11:39:40.545339: step 7272, loss 0.56745.
Train: 2018-07-31T11:39:40.701552: step 7273, loss 0.585132.
Train: 2018-07-31T11:39:40.889038: step 7274, loss 0.585114.
Train: 2018-07-31T11:39:41.060844: step 7275, loss 0.52328.
Train: 2018-07-31T11:39:41.232708: step 7276, loss 0.691012.
Train: 2018-07-31T11:39:41.404545: step 7277, loss 0.585006.
Train: 2018-07-31T11:39:41.576378: step 7278, loss 0.523494.
Train: 2018-07-31T11:39:41.732591: step 7279, loss 0.558604.
Train: 2018-07-31T11:39:41.920018: step 7280, loss 0.558607.
Test: 2018-07-31T11:39:42.388658: step 7280, loss 0.552716.
Train: 2018-07-31T11:39:42.576138: step 7281, loss 0.567342.
Train: 2018-07-31T11:39:42.747974: step 7282, loss 0.541169.
Train: 2018-07-31T11:39:42.904192: step 7283, loss 0.576041.
Train: 2018-07-31T11:39:43.076026: step 7284, loss 0.558612.
Train: 2018-07-31T11:39:43.247861: step 7285, loss 0.584712.
Train: 2018-07-31T11:39:43.435289: step 7286, loss 0.541232.
Train: 2018-07-31T11:39:43.607121: step 7287, loss 0.532553.
Train: 2018-07-31T11:39:43.778957: step 7288, loss 0.610738.
Train: 2018-07-31T11:39:43.950792: step 7289, loss 0.567289.
Train: 2018-07-31T11:39:44.122627: step 7290, loss 0.584633.
Test: 2018-07-31T11:39:44.606886: step 7290, loss 0.552763.
Train: 2018-07-31T11:39:44.778746: step 7291, loss 0.584607.
Train: 2018-07-31T11:39:44.950558: step 7292, loss 0.610535.
Train: 2018-07-31T11:39:45.106795: step 7293, loss 0.54136.
Train: 2018-07-31T11:39:45.294226: step 7294, loss 0.51553.
Train: 2018-07-31T11:39:45.466086: step 7295, loss 0.567251.
Train: 2018-07-31T11:39:45.637896: step 7296, loss 0.498316.
Train: 2018-07-31T11:39:45.809761: step 7297, loss 0.627632.
Train: 2018-07-31T11:39:45.981596: step 7298, loss 0.532739.
Train: 2018-07-31T11:39:46.153431: step 7299, loss 0.549977.
Train: 2018-07-31T11:39:46.325235: step 7300, loss 0.57587.
Test: 2018-07-31T11:39:46.809527: step 7300, loss 0.552759.
Train: 2018-07-31T11:39:47.528111: step 7301, loss 0.575871.
Train: 2018-07-31T11:39:47.715565: step 7302, loss 0.558584.
Train: 2018-07-31T11:39:47.887370: step 7303, loss 0.541285.
Train: 2018-07-31T11:39:48.059235: step 7304, loss 0.575874.
Train: 2018-07-31T11:39:48.215417: step 7305, loss 0.567218.
Train: 2018-07-31T11:39:48.402874: step 7306, loss 0.575876.
Train: 2018-07-31T11:39:48.574709: step 7307, loss 0.523896.
Train: 2018-07-31T11:39:48.746568: step 7308, loss 0.584553.
Train: 2018-07-31T11:39:48.918378: step 7309, loss 0.541178.
Train: 2018-07-31T11:39:49.090212: step 7310, loss 0.541157.
Test: 2018-07-31T11:39:49.558883: step 7310, loss 0.552657.
Train: 2018-07-31T11:39:49.730688: step 7311, loss 0.619397.
Train: 2018-07-31T11:39:49.902522: step 7312, loss 0.567233.
Train: 2018-07-31T11:39:50.074357: step 7313, loss 0.515072.
Train: 2018-07-31T11:39:50.246223: step 7314, loss 0.541152.
Train: 2018-07-31T11:39:50.418027: step 7315, loss 0.549854.
Train: 2018-07-31T11:39:50.589894: step 7316, loss 0.61096.
Train: 2018-07-31T11:39:50.777350: step 7317, loss 0.558591.
Train: 2018-07-31T11:39:50.949176: step 7318, loss 0.576076.
Train: 2018-07-31T11:39:51.105396: step 7319, loss 0.567358.
Train: 2018-07-31T11:39:51.277200: step 7320, loss 0.549882.
Test: 2018-07-31T11:39:51.761461: step 7320, loss 0.552728.
Train: 2018-07-31T11:39:51.933322: step 7321, loss 0.567367.
Train: 2018-07-31T11:39:52.105131: step 7322, loss 0.523661.
Train: 2018-07-31T11:39:52.276967: step 7323, loss 0.514876.
Train: 2018-07-31T11:39:52.448802: step 7324, loss 0.576163.
Train: 2018-07-31T11:39:52.620636: step 7325, loss 0.576183.
Train: 2018-07-31T11:39:52.792503: step 7326, loss 0.558619.
Train: 2018-07-31T11:39:52.964335: step 7327, loss 0.549816.
Train: 2018-07-31T11:39:53.136170: step 7328, loss 0.567413.
Train: 2018-07-31T11:39:53.308000: step 7329, loss 0.549784.
Train: 2018-07-31T11:39:53.479840: step 7330, loss 0.576234.
Test: 2018-07-31T11:39:53.964071: step 7330, loss 0.552622.
Train: 2018-07-31T11:39:54.135936: step 7331, loss 0.576235.
Train: 2018-07-31T11:39:54.307771: step 7332, loss 0.50559.
Train: 2018-07-31T11:39:54.479575: step 7333, loss 0.611604.
Train: 2018-07-31T11:39:54.667033: step 7334, loss 0.620462.
Train: 2018-07-31T11:39:54.838897: step 7335, loss 0.59387.
Train: 2018-07-31T11:39:55.010702: step 7336, loss 0.540898.
Train: 2018-07-31T11:39:55.166947: step 7337, loss 0.55853.
Train: 2018-07-31T11:39:55.338750: step 7338, loss 0.576126.
Train: 2018-07-31T11:39:55.526205: step 7339, loss 0.611258.
Train: 2018-07-31T11:39:55.698041: step 7340, loss 0.60238.
Test: 2018-07-31T11:39:56.166712: step 7340, loss 0.552632.
Train: 2018-07-31T11:39:56.338547: step 7341, loss 0.56728.
Train: 2018-07-31T11:39:56.510381: step 7342, loss 0.602159.
Train: 2018-07-31T11:39:56.682216: step 7343, loss 0.575944.
Train: 2018-07-31T11:39:56.854049: step 7344, loss 0.593241.
Train: 2018-07-31T11:39:57.025887: step 7345, loss 0.549939.
Train: 2018-07-31T11:39:57.182098: step 7346, loss 0.532743.
Train: 2018-07-31T11:39:57.353933: step 7347, loss 0.524194.
Train: 2018-07-31T11:39:57.525770: step 7348, loss 0.541405.
Train: 2018-07-31T11:39:57.713218: step 7349, loss 0.472632.
Train: 2018-07-31T11:39:57.869406: step 7350, loss 0.584405.
Test: 2018-07-31T11:39:58.353698: step 7350, loss 0.552726.
Train: 2018-07-31T11:39:58.525504: step 7351, loss 0.6103.
Train: 2018-07-31T11:39:58.697362: step 7352, loss 0.610307.
Train: 2018-07-31T11:39:58.869172: step 7353, loss 0.532664.
Train: 2018-07-31T11:39:59.041008: step 7354, loss 0.575894.
Train: 2018-07-31T11:39:59.212843: step 7355, loss 0.567144.
Train: 2018-07-31T11:39:59.384708: step 7356, loss 0.584395.
Train: 2018-07-31T11:39:59.556512: step 7357, loss 0.610252.
Train: 2018-07-31T11:39:59.728346: step 7358, loss 0.532759.
Train: 2018-07-31T11:39:59.900181: step 7359, loss 0.567199.
Train: 2018-07-31T11:40:00.072017: step 7360, loss 0.567218.
Test: 2018-07-31T11:40:00.556278: step 7360, loss 0.552846.
Train: 2018-07-31T11:40:00.728137: step 7361, loss 0.601605.
Train: 2018-07-31T11:40:00.899977: step 7362, loss 0.55867.
Train: 2018-07-31T11:40:01.071813: step 7363, loss 0.48152.
Train: 2018-07-31T11:40:01.243647: step 7364, loss 0.601613.
Train: 2018-07-31T11:40:01.415463: step 7365, loss 0.558699.
Train: 2018-07-31T11:40:01.587286: step 7366, loss 0.644596.
Train: 2018-07-31T11:40:01.759151: step 7367, loss 0.550136.
Train: 2018-07-31T11:40:01.946577: step 7368, loss 0.567291.
Train: 2018-07-31T11:40:02.102817: step 7369, loss 0.584417.
Train: 2018-07-31T11:40:02.274655: step 7370, loss 0.61006.
Test: 2018-07-31T11:40:02.758917: step 7370, loss 0.552975.
Train: 2018-07-31T11:40:02.930721: step 7371, loss 0.627051.
Train: 2018-07-31T11:40:03.102586: step 7372, loss 0.584298.
Train: 2018-07-31T11:40:03.274390: step 7373, loss 0.558778.
Train: 2018-07-31T11:40:03.446225: step 7374, loss 0.533395.
Train: 2018-07-31T11:40:03.618060: step 7375, loss 0.575704.
Train: 2018-07-31T11:40:03.774275: step 7376, loss 0.541913.
Train: 2018-07-31T11:40:03.946110: step 7377, loss 0.525042.
Train: 2018-07-31T11:40:04.117943: step 7378, loss 0.57565.
Train: 2018-07-31T11:40:04.289778: step 7379, loss 0.558748.
Train: 2018-07-31T11:40:04.477235: step 7380, loss 0.550276.
Test: 2018-07-31T11:40:04.945905: step 7380, loss 0.55299.
Train: 2018-07-31T11:40:05.117710: step 7381, loss 0.626354.
Train: 2018-07-31T11:40:05.289577: step 7382, loss 0.533324.
Train: 2018-07-31T11:40:05.461409: step 7383, loss 0.524833.
Train: 2018-07-31T11:40:05.633244: step 7384, loss 0.600984.
Train: 2018-07-31T11:40:05.805049: step 7385, loss 0.541659.
Train: 2018-07-31T11:40:05.976884: step 7386, loss 0.685868.
Train: 2018-07-31T11:40:06.133121: step 7387, loss 0.626366.
Train: 2018-07-31T11:40:06.304931: step 7388, loss 0.668452.
Train: 2018-07-31T11:40:06.476766: step 7389, loss 0.58388.
Train: 2018-07-31T11:40:06.648626: step 7390, loss 0.617307.
Test: 2018-07-31T11:40:07.132862: step 7390, loss 0.553134.
Train: 2018-07-31T11:40:07.304698: step 7391, loss 0.467118.
Train: 2018-07-31T11:40:07.476532: step 7392, loss 0.583715.
Train: 2018-07-31T11:40:07.648366: step 7393, loss 0.558798.
Train: 2018-07-31T11:40:07.820232: step 7394, loss 0.533965.
Train: 2018-07-31T11:40:07.992067: step 7395, loss 0.583646.
Train: 2018-07-31T11:40:08.148249: step 7396, loss 0.533989.
Train: 2018-07-31T11:40:08.320085: step 7397, loss 0.575349.
Train: 2018-07-31T11:40:08.491950: step 7398, loss 0.583621.
Train: 2018-07-31T11:40:08.648132: step 7399, loss 0.567052.
Train: 2018-07-31T11:40:08.819992: step 7400, loss 0.575326.
Test: 2018-07-31T11:40:09.288640: step 7400, loss 0.553157.
Train: 2018-07-31T11:40:10.081482: step 7401, loss 0.542177.
Train: 2018-07-31T11:40:10.268938: step 7402, loss 0.500671.
Train: 2018-07-31T11:40:10.440773: step 7403, loss 0.525401.
Train: 2018-07-31T11:40:10.612607: step 7404, loss 0.608693.
Train: 2018-07-31T11:40:10.784411: step 7405, loss 0.525156.
Train: 2018-07-31T11:40:10.940625: step 7406, loss 0.583754.
Train: 2018-07-31T11:40:11.112491: step 7407, loss 0.508111.
Train: 2018-07-31T11:40:11.284296: step 7408, loss 0.550114.
Train: 2018-07-31T11:40:11.456129: step 7409, loss 0.583961.
Train: 2018-07-31T11:40:11.627964: step 7410, loss 0.524525.
Test: 2018-07-31T11:40:12.096604: step 7410, loss 0.552754.
Train: 2018-07-31T11:40:12.268439: step 7411, loss 0.58411.
Train: 2018-07-31T11:40:12.440275: step 7412, loss 0.549943.
Train: 2018-07-31T11:40:12.612140: step 7413, loss 0.472637.
Train: 2018-07-31T11:40:12.783975: step 7414, loss 0.549836.
Train: 2018-07-31T11:40:12.955779: step 7415, loss 0.601777.
Train: 2018-07-31T11:40:13.127613: step 7416, loss 0.575817.
Train: 2018-07-31T11:40:13.299449: step 7417, loss 0.523523.
Train: 2018-07-31T11:40:13.471283: step 7418, loss 0.505887.
Train: 2018-07-31T11:40:13.643147: step 7419, loss 0.52322.
Train: 2018-07-31T11:40:13.814953: step 7420, loss 0.584834.
Test: 2018-07-31T11:40:14.283593: step 7420, loss 0.552355.
Train: 2018-07-31T11:40:14.455428: step 7421, loss 0.549474.
Train: 2018-07-31T11:40:14.627292: step 7422, loss 0.576098.
Train: 2018-07-31T11:40:14.799129: step 7423, loss 0.576131.
Train: 2018-07-31T11:40:14.970956: step 7424, loss 0.558288.
Train: 2018-07-31T11:40:15.142767: step 7425, loss 0.549324.
Train: 2018-07-31T11:40:15.314631: step 7426, loss 0.51343.
Train: 2018-07-31T11:40:15.486467: step 7427, loss 0.513304.
Train: 2018-07-31T11:40:15.658270: step 7428, loss 0.576268.
Train: 2018-07-31T11:40:15.830135: step 7429, loss 0.513038.
Train: 2018-07-31T11:40:16.017588: step 7430, loss 0.603542.
Test: 2018-07-31T11:40:16.486202: step 7430, loss 0.552084.
Train: 2018-07-31T11:40:16.658068: step 7431, loss 0.530972.
Train: 2018-07-31T11:40:16.829871: step 7432, loss 0.540009.
Train: 2018-07-31T11:40:17.001736: step 7433, loss 0.512626.
Train: 2018-07-31T11:40:17.173566: step 7434, loss 0.686146.
Train: 2018-07-31T11:40:17.345377: step 7435, loss 0.585592.
Train: 2018-07-31T11:40:17.517235: step 7436, loss 0.612933.
Train: 2018-07-31T11:40:17.689076: step 7437, loss 0.54903.
Train: 2018-07-31T11:40:17.876501: step 7438, loss 0.539931.
Train: 2018-07-31T11:40:18.048338: step 7439, loss 0.594431.
Train: 2018-07-31T11:40:18.220171: step 7440, loss 0.59434.
Test: 2018-07-31T11:40:18.704431: step 7440, loss 0.551963.
Train: 2018-07-31T11:40:18.876266: step 7441, loss 0.485757.
Train: 2018-07-31T11:40:19.048126: step 7442, loss 0.603213.
Train: 2018-07-31T11:40:19.219961: step 7443, loss 0.55804.
Train: 2018-07-31T11:40:19.391771: step 7444, loss 0.594028.
Train: 2018-07-31T11:40:19.563607: step 7445, loss 0.584835.
Train: 2018-07-31T11:40:19.735471: step 7446, loss 0.601249.
Train: 2018-07-31T11:40:19.907276: step 7447, loss 0.571991.
Train: 2018-07-31T11:40:20.079111: step 7448, loss 0.542329.
Train: 2018-07-31T11:40:20.266567: step 7449, loss 0.531339.
Train: 2018-07-31T11:40:20.438402: step 7450, loss 0.522438.
Test: 2018-07-31T11:40:20.922694: step 7450, loss 0.552339.
Train: 2018-07-31T11:40:21.110150: step 7451, loss 0.619893.
Train: 2018-07-31T11:40:21.281978: step 7452, loss 0.576492.
Train: 2018-07-31T11:40:21.469422: step 7453, loss 0.550021.
Train: 2018-07-31T11:40:21.641275: step 7454, loss 0.625661.
Train: 2018-07-31T11:40:21.813079: step 7455, loss 0.604514.
Train: 2018-07-31T11:40:21.984944: step 7456, loss 0.594498.
Train: 2018-07-31T11:40:22.156781: step 7457, loss 0.542276.
Train: 2018-07-31T11:40:22.344204: step 7458, loss 0.568871.
Train: 2018-07-31T11:40:22.516040: step 7459, loss 0.586624.
Train: 2018-07-31T11:40:22.703497: step 7460, loss 0.534913.
Test: 2018-07-31T11:40:23.172168: step 7460, loss 0.555423.
Train: 2018-07-31T11:40:23.359592: step 7461, loss 0.561261.
Train: 2018-07-31T11:40:23.531427: step 7462, loss 0.604755.
Train: 2018-07-31T11:40:23.718913: step 7463, loss 0.656643.
Train: 2018-07-31T11:40:23.890748: step 7464, loss 0.570706.
Train: 2018-07-31T11:40:24.062583: step 7465, loss 0.519522.
Train: 2018-07-31T11:40:24.250009: step 7466, loss 0.553917.
Train: 2018-07-31T11:40:24.421843: step 7467, loss 0.613722.
Train: 2018-07-31T11:40:24.609300: step 7468, loss 0.562668.
Train: 2018-07-31T11:40:24.781166: step 7469, loss 0.545747.
Train: 2018-07-31T11:40:24.952970: step 7470, loss 0.545788.
Test: 2018-07-31T11:40:25.437263: step 7470, loss 0.557023.
Train: 2018-07-31T11:40:25.640308: step 7471, loss 0.537304.
Train: 2018-07-31T11:40:25.827764: step 7472, loss 0.554221.
Train: 2018-07-31T11:40:25.999624: step 7473, loss 0.528669.
Train: 2018-07-31T11:40:26.171465: step 7474, loss 0.554046.
Train: 2018-07-31T11:40:26.358889: step 7475, loss 0.596566.
Train: 2018-07-31T11:40:26.530725: step 7476, loss 0.536727.
Train: 2018-07-31T11:40:26.702590: step 7477, loss 0.596449.
Train: 2018-07-31T11:40:26.874395: step 7478, loss 0.570667.
Train: 2018-07-31T11:40:27.061881: step 7479, loss 0.510511.
Train: 2018-07-31T11:40:27.233686: step 7480, loss 0.570428.
Test: 2018-07-31T11:40:27.702324: step 7480, loss 0.555877.
Train: 2018-07-31T11:40:27.874191: step 7481, loss 0.613379.
Train: 2018-07-31T11:40:28.046026: step 7482, loss 0.527066.
Train: 2018-07-31T11:40:28.233451: step 7483, loss 0.613417.
Train: 2018-07-31T11:40:28.405315: step 7484, loss 0.544.
Train: 2018-07-31T11:40:28.592766: step 7485, loss 0.587111.
Train: 2018-07-31T11:40:28.764607: step 7486, loss 0.56969.
Train: 2018-07-31T11:40:28.936437: step 7487, loss 0.517616.
Train: 2018-07-31T11:40:29.108277: step 7488, loss 0.543451.
Train: 2018-07-31T11:40:29.295733: step 7489, loss 0.595434.
Train: 2018-07-31T11:40:29.467567: step 7490, loss 0.612756.
Test: 2018-07-31T11:40:29.951797: step 7490, loss 0.554604.
Train: 2018-07-31T11:40:30.123634: step 7491, loss 0.603956.
Train: 2018-07-31T11:40:30.295468: step 7492, loss 0.603827.
Train: 2018-07-31T11:40:30.482924: step 7493, loss 0.508251.
Train: 2018-07-31T11:40:30.670380: step 7494, loss 0.542866.
Train: 2018-07-31T11:40:30.842214: step 7495, loss 0.586153.
Train: 2018-07-31T11:40:31.029671: step 7496, loss 0.525344.
Train: 2018-07-31T11:40:31.201505: step 7497, loss 0.646772.
Train: 2018-07-31T11:40:31.373371: step 7498, loss 0.525201.
Train: 2018-07-31T11:40:31.560797: step 7499, loss 0.533802.
Train: 2018-07-31T11:40:31.732632: step 7500, loss 0.568418.
Test: 2018-07-31T11:40:32.201302: step 7500, loss 0.553812.
Train: 2018-07-31T11:40:32.951097: step 7501, loss 0.55967.
Train: 2018-07-31T11:40:33.138588: step 7502, loss 0.542228.
Train: 2018-07-31T11:40:33.310418: step 7503, loss 0.65518.
Train: 2018-07-31T11:40:33.482246: step 7504, loss 0.628969.
Train: 2018-07-31T11:40:33.654056: step 7505, loss 0.490098.
Train: 2018-07-31T11:40:33.841513: step 7506, loss 0.611356.
Train: 2018-07-31T11:40:34.013378: step 7507, loss 0.559333.
Train: 2018-07-31T11:40:34.185182: step 7508, loss 0.576572.
Train: 2018-07-31T11:40:34.357018: step 7509, loss 0.57651.
Train: 2018-07-31T11:40:34.544504: step 7510, loss 0.567829.
Test: 2018-07-31T11:40:35.013144: step 7510, loss 0.553362.
Train: 2018-07-31T11:40:35.184949: step 7511, loss 0.61944.
Train: 2018-07-31T11:40:35.372403: step 7512, loss 0.62787.
Train: 2018-07-31T11:40:35.544270: step 7513, loss 0.576255.
Train: 2018-07-31T11:40:35.716104: step 7514, loss 0.507913.
Train: 2018-07-31T11:40:35.903559: step 7515, loss 0.593183.
Train: 2018-07-31T11:40:36.075401: step 7516, loss 0.644154.
Train: 2018-07-31T11:40:36.262851: step 7517, loss 0.576033.
Train: 2018-07-31T11:40:36.434655: step 7518, loss 0.567536.
Train: 2018-07-31T11:40:36.606489: step 7519, loss 0.508543.
Train: 2018-07-31T11:40:36.778326: step 7520, loss 0.567501.
Test: 2018-07-31T11:40:37.262586: step 7520, loss 0.5534.
Train: 2018-07-31T11:40:37.434452: step 7521, loss 0.668385.
Train: 2018-07-31T11:40:37.606286: step 7522, loss 0.609391.
Train: 2018-07-31T11:40:37.793744: step 7523, loss 0.53406.
Train: 2018-07-31T11:40:37.965547: step 7524, loss 0.542465.
Train: 2018-07-31T11:40:38.137382: step 7525, loss 0.542487.
Train: 2018-07-31T11:40:38.324868: step 7526, loss 0.567438.
Train: 2018-07-31T11:40:38.496703: step 7527, loss 0.609.
Train: 2018-07-31T11:40:38.668540: step 7528, loss 0.559098.
Train: 2018-07-31T11:40:38.840343: step 7529, loss 0.592295.
Train: 2018-07-31T11:40:39.027798: step 7530, loss 0.642023.
Test: 2018-07-31T11:40:39.496439: step 7530, loss 0.553514.
Train: 2018-07-31T11:40:39.683895: step 7531, loss 0.608741.
Train: 2018-07-31T11:40:39.855730: step 7532, loss 0.559126.
Train: 2018-07-31T11:40:40.027567: step 7533, loss 0.534454.
Train: 2018-07-31T11:40:40.215021: step 7534, loss 0.657793.
Train: 2018-07-31T11:40:40.386855: step 7535, loss 0.59196.
Train: 2018-07-31T11:40:40.574312: step 7536, loss 0.567379.
Train: 2018-07-31T11:40:40.746177: step 7537, loss 0.616298.
Train: 2018-07-31T11:40:40.918004: step 7538, loss 0.608032.
Train: 2018-07-31T11:40:41.089846: step 7539, loss 0.543129.
Train: 2018-07-31T11:40:41.261651: step 7540, loss 0.559353.
Test: 2018-07-31T11:40:41.730320: step 7540, loss 0.553925.
Train: 2018-07-31T11:40:41.917747: step 7541, loss 0.51097.
Train: 2018-07-31T11:40:42.089582: step 7542, loss 0.559343.
Train: 2018-07-31T11:40:42.261416: step 7543, loss 0.623909.
Train: 2018-07-31T11:40:42.433250: step 7544, loss 0.575443.
Train: 2018-07-31T11:40:42.620707: step 7545, loss 0.518936.
Train: 2018-07-31T11:40:42.792573: step 7546, loss 0.591568.
Train: 2018-07-31T11:40:42.979998: step 7547, loss 0.526857.
Train: 2018-07-31T11:40:43.151864: step 7548, loss 0.551047.
Train: 2018-07-31T11:40:43.339289: step 7549, loss 0.534707.
Train: 2018-07-31T11:40:43.495502: step 7550, loss 0.514974.
Test: 2018-07-31T11:40:43.979763: step 7550, loss 0.553381.
Train: 2018-07-31T11:40:44.151599: step 7551, loss 0.534334.
Train: 2018-07-31T11:40:44.323464: step 7552, loss 0.550572.
Train: 2018-07-31T11:40:44.510915: step 7553, loss 0.525601.
Train: 2018-07-31T11:40:44.682725: step 7554, loss 0.516982.
Train: 2018-07-31T11:40:44.854558: step 7555, loss 0.608776.
Train: 2018-07-31T11:40:45.026393: step 7556, loss 0.566733.
Train: 2018-07-31T11:40:45.213850: step 7557, loss 0.532982.
Train: 2018-07-31T11:40:45.385684: step 7558, loss 0.549794.
Train: 2018-07-31T11:40:45.557550: step 7559, loss 0.575303.
Train: 2018-07-31T11:40:45.729354: step 7560, loss 0.583924.
Test: 2018-07-31T11:40:46.197996: step 7560, loss 0.552348.
Train: 2018-07-31T11:40:46.369828: step 7561, loss 0.54916.
Train: 2018-07-31T11:40:46.557284: step 7562, loss 0.53251.
Train: 2018-07-31T11:40:46.729150: step 7563, loss 0.592772.
Train: 2018-07-31T11:40:46.900987: step 7564, loss 0.566778.
Train: 2018-07-31T11:40:47.072814: step 7565, loss 0.59293.
Train: 2018-07-31T11:40:47.260275: step 7566, loss 0.531898.
Train: 2018-07-31T11:40:47.432110: step 7567, loss 0.523097.
Train: 2018-07-31T11:40:47.603916: step 7568, loss 0.566835.
Train: 2018-07-31T11:40:47.775750: step 7569, loss 0.575639.
Train: 2018-07-31T11:40:47.947585: step 7570, loss 0.566865.
Test: 2018-07-31T11:40:48.416224: step 7570, loss 0.552111.
Train: 2018-07-31T11:40:48.588060: step 7571, loss 0.584506.
Train: 2018-07-31T11:40:48.775515: step 7572, loss 0.610929.
Train: 2018-07-31T11:40:48.947350: step 7573, loss 0.602155.
Train: 2018-07-31T11:40:49.119215: step 7574, loss 0.584487.
Train: 2018-07-31T11:40:49.291020: step 7575, loss 0.584449.
Train: 2018-07-31T11:40:49.462887: step 7576, loss 0.566845.
Train: 2018-07-31T11:40:49.650343: step 7577, loss 0.584356.
Train: 2018-07-31T11:40:49.822176: step 7578, loss 0.575559.
Train: 2018-07-31T11:40:49.993980: step 7579, loss 0.540626.
Train: 2018-07-31T11:40:50.165815: step 7580, loss 0.479673.
Test: 2018-07-31T11:40:50.650109: step 7580, loss 0.552182.
Train: 2018-07-31T11:40:50.837533: step 7581, loss 0.540633.
Train: 2018-07-31T11:40:51.009367: step 7582, loss 0.549326.
Train: 2018-07-31T11:40:51.181232: step 7583, loss 0.645355.
Train: 2018-07-31T11:40:51.353038: step 7584, loss 0.531842.
Train: 2018-07-31T11:40:51.524872: step 7585, loss 0.584191.
Train: 2018-07-31T11:40:51.696738: step 7586, loss 0.627772.
Train: 2018-07-31T11:40:51.884192: step 7587, loss 0.523597.
Train: 2018-07-31T11:40:52.055997: step 7588, loss 0.549306.
Train: 2018-07-31T11:40:52.243479: step 7589, loss 0.697093.
Train: 2018-07-31T11:40:52.415288: step 7590, loss 0.61.
Test: 2018-07-31T11:40:52.899550: step 7590, loss 0.552201.
Train: 2018-07-31T11:40:53.071415: step 7591, loss 0.575289.
Train: 2018-07-31T11:40:53.258866: step 7592, loss 0.523649.
Train: 2018-07-31T11:40:53.430675: step 7593, loss 0.64384.
Train: 2018-07-31T11:40:53.602541: step 7594, loss 0.506819.
Train: 2018-07-31T11:40:53.774346: step 7595, loss 0.583677.
Train: 2018-07-31T11:40:53.946210: step 7596, loss 0.515584.
Train: 2018-07-31T11:40:54.118014: step 7597, loss 0.575113.
Train: 2018-07-31T11:40:54.305471: step 7598, loss 0.566609.
Train: 2018-07-31T11:40:54.477336: step 7599, loss 0.524187.
Train: 2018-07-31T11:40:54.649165: step 7600, loss 0.634479.
Test: 2018-07-31T11:40:55.117779: step 7600, loss 0.552396.
Train: 2018-07-31T11:40:55.867604: step 7601, loss 0.464888.
Train: 2018-07-31T11:40:56.055086: step 7602, loss 0.532636.
Train: 2018-07-31T11:40:56.242547: step 7603, loss 0.566564.
Train: 2018-07-31T11:40:56.414383: step 7604, loss 0.609143.
Train: 2018-07-31T11:40:56.586216: step 7605, loss 0.540979.
Train: 2018-07-31T11:40:56.758021: step 7606, loss 0.600674.
Train: 2018-07-31T11:40:56.914234: step 7607, loss 0.532386.
Train: 2018-07-31T11:40:57.086099: step 7608, loss 0.592163.
Train: 2018-07-31T11:40:57.273526: step 7609, loss 0.523707.
Train: 2018-07-31T11:40:57.445391: step 7610, loss 0.575044.
Test: 2018-07-31T11:40:57.929622: step 7610, loss 0.552151.
Train: 2018-07-31T11:40:58.085860: step 7611, loss 0.56651.
Train: 2018-07-31T11:40:58.273323: step 7612, loss 0.583732.
Train: 2018-07-31T11:40:58.445156: step 7613, loss 0.514968.
Train: 2018-07-31T11:40:58.616961: step 7614, loss 0.583693.
Train: 2018-07-31T11:40:58.788795: step 7615, loss 0.57511.
Train: 2018-07-31T11:40:58.960661: step 7616, loss 0.506235.
Train: 2018-07-31T11:40:59.132465: step 7617, loss 0.618284.
Train: 2018-07-31T11:40:59.319952: step 7618, loss 0.575157.
Train: 2018-07-31T11:40:59.491780: step 7619, loss 0.626975.
Train: 2018-07-31T11:40:59.663590: step 7620, loss 0.532091.
Test: 2018-07-31T11:41:00.147852: step 7620, loss 0.55209.
Train: 2018-07-31T11:41:00.319688: step 7621, loss 0.532035.
Train: 2018-07-31T11:41:00.491556: step 7622, loss 0.601049.
Train: 2018-07-31T11:41:00.663388: step 7623, loss 0.583791.
Train: 2018-07-31T11:41:00.835191: step 7624, loss 0.557924.
Train: 2018-07-31T11:41:01.007027: step 7625, loss 0.592393.
Train: 2018-07-31T11:41:01.194483: step 7626, loss 0.575156.
Train: 2018-07-31T11:41:01.350727: step 7627, loss 0.549355.
Train: 2018-07-31T11:41:01.538153: step 7628, loss 0.540231.
Train: 2018-07-31T11:41:01.710017: step 7629, loss 0.532505.
Train: 2018-07-31T11:41:01.881851: step 7630, loss 0.618147.
Test: 2018-07-31T11:41:02.366113: step 7630, loss 0.552176.
Train: 2018-07-31T11:41:02.537917: step 7631, loss 0.60954.
Train: 2018-07-31T11:41:02.709783: step 7632, loss 0.609495.
Train: 2018-07-31T11:41:02.881588: step 7633, loss 0.61603.
Train: 2018-07-31T11:41:03.053452: step 7634, loss 0.523936.
Train: 2018-07-31T11:41:03.225288: step 7635, loss 0.609276.
Train: 2018-07-31T11:41:03.397091: step 7636, loss 0.592811.
Train: 2018-07-31T11:41:03.568957: step 7637, loss 0.532805.
Train: 2018-07-31T11:41:03.740762: step 7638, loss 0.524436.
Train: 2018-07-31T11:41:03.912596: step 7639, loss 0.545454.
Train: 2018-07-31T11:41:04.100051: step 7640, loss 0.551369.
Test: 2018-07-31T11:41:04.568722: step 7640, loss 0.552677.
Train: 2018-07-31T11:41:04.740557: step 7641, loss 0.543502.
Train: 2018-07-31T11:41:04.927982: step 7642, loss 0.570156.
Train: 2018-07-31T11:41:05.099849: step 7643, loss 0.583932.
Train: 2018-07-31T11:41:05.256032: step 7644, loss 0.550139.
Train: 2018-07-31T11:41:05.427896: step 7645, loss 0.533302.
Train: 2018-07-31T11:41:05.615352: step 7646, loss 0.541933.
Train: 2018-07-31T11:41:05.787157: step 7647, loss 0.627345.
Train: 2018-07-31T11:41:05.974637: step 7648, loss 0.49953.
Train: 2018-07-31T11:41:06.146447: step 7649, loss 0.585056.
Train: 2018-07-31T11:41:06.318313: step 7650, loss 0.550934.
Test: 2018-07-31T11:41:06.802543: step 7650, loss 0.553782.
Train: 2018-07-31T11:41:06.974378: step 7651, loss 0.568155.
Train: 2018-07-31T11:41:07.146213: step 7652, loss 0.516641.
Train: 2018-07-31T11:41:07.318079: step 7653, loss 0.576868.
Train: 2018-07-31T11:41:07.505535: step 7654, loss 0.628702.
Train: 2018-07-31T11:41:07.677340: step 7655, loss 0.600091.
Train: 2018-07-31T11:41:07.849174: step 7656, loss 0.506864.
Train: 2018-07-31T11:41:08.021009: step 7657, loss 0.577669.
Train: 2018-07-31T11:41:08.192843: step 7658, loss 0.628858.
Train: 2018-07-31T11:41:08.364678: step 7659, loss 0.569328.
Train: 2018-07-31T11:41:08.536544: step 7660, loss 0.533264.
Test: 2018-07-31T11:41:09.005183: step 7660, loss 0.553506.
Train: 2018-07-31T11:41:09.192639: step 7661, loss 0.637483.
Train: 2018-07-31T11:41:09.364475: step 7662, loss 0.549624.
Train: 2018-07-31T11:41:09.536310: step 7663, loss 0.546363.
Train: 2018-07-31T11:41:09.708145: step 7664, loss 0.590601.
Train: 2018-07-31T11:41:09.879949: step 7665, loss 0.733069.
Train: 2018-07-31T11:41:10.051784: step 7666, loss 0.541831.
Train: 2018-07-31T11:41:10.223648: step 7667, loss 0.561163.
Train: 2018-07-31T11:41:10.411106: step 7668, loss 0.596854.
Train: 2018-07-31T11:41:10.582908: step 7669, loss 0.546113.
Train: 2018-07-31T11:41:10.754744: step 7670, loss 0.582019.
Test: 2018-07-31T11:41:11.239035: step 7670, loss 0.560164.
Train: 2018-07-31T11:41:11.410864: step 7671, loss 0.548697.
Train: 2018-07-31T11:41:11.582705: step 7672, loss 0.611019.
Train: 2018-07-31T11:41:11.754547: step 7673, loss 0.523101.
Train: 2018-07-31T11:41:11.941967: step 7674, loss 0.61542.
Train: 2018-07-31T11:41:12.113801: step 7675, loss 0.563774.
Train: 2018-07-31T11:41:12.285666: step 7676, loss 0.653692.
Train: 2018-07-31T11:41:12.473090: step 7677, loss 0.52879.
Train: 2018-07-31T11:41:12.644925: step 7678, loss 0.57856.
Train: 2018-07-31T11:41:12.816760: step 7679, loss 0.633998.
Train: 2018-07-31T11:41:12.988625: step 7680, loss 0.600031.
Test: 2018-07-31T11:41:13.472857: step 7680, loss 0.568863.
Train: 2018-07-31T11:41:13.660337: step 7681, loss 0.57469.
Train: 2018-07-31T11:41:13.832178: step 7682, loss 0.609714.
Train: 2018-07-31T11:41:14.003983: step 7683, loss 0.653178.
Train: 2018-07-31T11:41:14.175817: step 7684, loss 0.644692.
Train: 2018-07-31T11:41:14.347653: step 7685, loss 0.593266.
Train: 2018-07-31T11:41:14.519519: step 7686, loss 0.636008.
Train: 2018-07-31T11:41:14.691321: step 7687, loss 0.550814.
Train: 2018-07-31T11:41:14.878777: step 7688, loss 0.576274.
Train: 2018-07-31T11:41:15.050643: step 7689, loss 0.618426.
Train: 2018-07-31T11:41:15.222477: step 7690, loss 0.576039.
Test: 2018-07-31T11:41:15.706734: step 7690, loss 0.570159.
Train: 2018-07-31T11:41:15.878545: step 7691, loss 0.575831.
Train: 2018-07-31T11:41:16.066000: step 7692, loss 0.575604.
Train: 2018-07-31T11:41:16.237859: step 7693, loss 0.575349.
Train: 2018-07-31T11:41:16.409701: step 7694, loss 0.533218.
Train: 2018-07-31T11:41:16.581504: step 7695, loss 0.532891.
Train: 2018-07-31T11:41:16.753369: step 7696, loss 0.532488.
Train: 2018-07-31T11:41:16.925175: step 7697, loss 0.540431.
Train: 2018-07-31T11:41:17.097010: step 7698, loss 0.523062.
Train: 2018-07-31T11:41:17.268869: step 7699, loss 0.573345.
Train: 2018-07-31T11:41:17.440711: step 7700, loss 0.607054.
Test: 2018-07-31T11:41:17.924939: step 7700, loss 0.56685.
Train: 2018-07-31T11:41:18.659174: step 7701, loss 0.690592.
Train: 2018-07-31T11:41:18.830978: step 7702, loss 0.546625.
Train: 2018-07-31T11:41:19.002811: step 7703, loss 0.580517.
Train: 2018-07-31T11:41:19.190299: step 7704, loss 0.511584.
Train: 2018-07-31T11:41:19.362134: step 7705, loss 0.588474.
Train: 2018-07-31T11:41:19.533937: step 7706, loss 0.605413.
Train: 2018-07-31T11:41:19.705773: step 7707, loss 0.59651.
Train: 2018-07-31T11:41:19.877607: step 7708, loss 0.604857.
Train: 2018-07-31T11:41:20.065064: step 7709, loss 0.66502.
Train: 2018-07-31T11:41:20.236899: step 7710, loss 0.604225.
Test: 2018-07-31T11:41:20.721190: step 7710, loss 0.563674.
Train: 2018-07-31T11:41:20.892995: step 7711, loss 0.526469.
Train: 2018-07-31T11:41:21.064859: step 7712, loss 0.552027.
Train: 2018-07-31T11:41:21.236665: step 7713, loss 0.620513.
Train: 2018-07-31T11:41:21.408500: step 7714, loss 0.585883.
Train: 2018-07-31T11:41:21.595986: step 7715, loss 0.585633.
Train: 2018-07-31T11:41:21.767821: step 7716, loss 0.593953.
Train: 2018-07-31T11:41:21.939648: step 7717, loss 0.508236.
Train: 2018-07-31T11:41:22.111484: step 7718, loss 0.610596.
Train: 2018-07-31T11:41:22.283327: step 7719, loss 0.644545.
Train: 2018-07-31T11:41:22.455131: step 7720, loss 0.593059.
Test: 2018-07-31T11:41:22.939391: step 7720, loss 0.561577.
Train: 2018-07-31T11:41:23.111255: step 7721, loss 0.592838.
Train: 2018-07-31T11:41:23.283061: step 7722, loss 0.575649.
Train: 2018-07-31T11:41:23.454895: step 7723, loss 0.550072.
Train: 2018-07-31T11:41:23.626729: step 7724, loss 0.524561.
Train: 2018-07-31T11:41:23.798563: step 7725, loss 0.608999.
Train: 2018-07-31T11:41:23.986045: step 7726, loss 0.532724.
Train: 2018-07-31T11:41:24.157856: step 7727, loss 0.617153.
Train: 2018-07-31T11:41:24.329720: step 7728, loss 0.532427.
Train: 2018-07-31T11:41:24.517145: step 7729, loss 0.549183.
Train: 2018-07-31T11:41:24.688981: step 7730, loss 0.481235.
Test: 2018-07-31T11:41:25.157653: step 7730, loss 0.56006.
Train: 2018-07-31T11:41:25.345078: step 7731, loss 0.633797.
Train: 2018-07-31T11:41:25.516943: step 7732, loss 0.574169.
Train: 2018-07-31T11:41:25.673155: step 7733, loss 0.539938.
Train: 2018-07-31T11:41:25.860611: step 7734, loss 0.51411.
Train: 2018-07-31T11:41:26.032416: step 7735, loss 0.496653.
Train: 2018-07-31T11:41:26.204251: step 7736, loss 0.616777.
Train: 2018-07-31T11:41:26.376086: step 7737, loss 0.608174.
Train: 2018-07-31T11:41:26.547951: step 7738, loss 0.547535.
Train: 2018-07-31T11:41:26.719785: step 7739, loss 0.599459.
Train: 2018-07-31T11:41:26.907211: step 7740, loss 0.512478.
Test: 2018-07-31T11:41:27.375881: step 7740, loss 0.558649.
Train: 2018-07-31T11:41:27.547716: step 7741, loss 0.591623.
Train: 2018-07-31T11:41:27.719551: step 7742, loss 0.59041.
Train: 2018-07-31T11:41:27.891356: step 7743, loss 0.555517.
Train: 2018-07-31T11:41:28.078836: step 7744, loss 0.608021.
Train: 2018-07-31T11:41:28.250677: step 7745, loss 0.616731.
Train: 2018-07-31T11:41:28.438105: step 7746, loss 0.50295.
Train: 2018-07-31T11:41:28.609968: step 7747, loss 0.544618.
Train: 2018-07-31T11:41:28.781797: step 7748, loss 0.545213.
Train: 2018-07-31T11:41:28.953607: step 7749, loss 0.615682.
Train: 2018-07-31T11:41:29.141088: step 7750, loss 0.61647.
Test: 2018-07-31T11:41:29.609733: step 7750, loss 0.558235.
Train: 2018-07-31T11:41:29.781538: step 7751, loss 0.520298.
Train: 2018-07-31T11:41:29.968993: step 7752, loss 0.573069.
Train: 2018-07-31T11:41:30.140860: step 7753, loss 0.611539.
Train: 2018-07-31T11:41:30.312694: step 7754, loss 0.556001.
Train: 2018-07-31T11:41:30.484529: step 7755, loss 0.590254.
Train: 2018-07-31T11:41:30.656332: step 7756, loss 0.575171.
Train: 2018-07-31T11:41:30.828198: step 7757, loss 0.55692.
Train: 2018-07-31T11:41:31.015625: step 7758, loss 0.636276.
Train: 2018-07-31T11:41:31.187489: step 7759, loss 0.583762.
Train: 2018-07-31T11:41:31.359295: step 7760, loss 0.580148.
Test: 2018-07-31T11:41:31.843585: step 7760, loss 0.560654.
Train: 2018-07-31T11:41:32.015422: step 7761, loss 0.531569.
Train: 2018-07-31T11:41:32.202846: step 7762, loss 0.601623.
Train: 2018-07-31T11:41:32.374713: step 7763, loss 0.538338.
Train: 2018-07-31T11:41:32.562137: step 7764, loss 0.528976.
Train: 2018-07-31T11:41:32.749624: step 7765, loss 0.553651.
Train: 2018-07-31T11:41:32.921427: step 7766, loss 0.518703.
Train: 2018-07-31T11:41:33.108884: step 7767, loss 0.834302.
Train: 2018-07-31T11:41:33.280744: step 7768, loss 0.575783.
Train: 2018-07-31T11:41:33.452554: step 7769, loss 0.611251.
Train: 2018-07-31T11:41:33.624389: step 7770, loss 0.515935.
Test: 2018-07-31T11:41:34.108680: step 7770, loss 0.563341.
Train: 2018-07-31T11:41:34.280484: step 7771, loss 0.610935.
Train: 2018-07-31T11:41:34.452319: step 7772, loss 0.559487.
Train: 2018-07-31T11:41:34.624154: step 7773, loss 0.71489.
Train: 2018-07-31T11:41:34.796019: step 7774, loss 0.580793.
Train: 2018-07-31T11:41:34.983475: step 7775, loss 0.546724.
Train: 2018-07-31T11:41:35.155279: step 7776, loss 0.538868.
Train: 2018-07-31T11:41:35.327139: step 7777, loss 0.714997.
Train: 2018-07-31T11:41:35.514571: step 7778, loss 0.558108.
Train: 2018-07-31T11:41:35.702027: step 7779, loss 0.628854.
Train: 2018-07-31T11:41:35.873861: step 7780, loss 0.516019.
Test: 2018-07-31T11:41:36.342500: step 7780, loss 0.571843.
Train: 2018-07-31T11:41:36.529989: step 7781, loss 0.569006.
Train: 2018-07-31T11:41:36.701817: step 7782, loss 0.534638.
Train: 2018-07-31T11:41:36.889248: step 7783, loss 0.559176.
Train: 2018-07-31T11:41:37.061084: step 7784, loss 0.500218.
Train: 2018-07-31T11:41:37.232949: step 7785, loss 0.623259.
Train: 2018-07-31T11:41:37.420405: step 7786, loss 0.623589.
Train: 2018-07-31T11:41:37.592208: step 7787, loss 0.61498.
Train: 2018-07-31T11:41:37.764043: step 7788, loss 0.570904.
Train: 2018-07-31T11:41:37.951500: step 7789, loss 0.624039.
Train: 2018-07-31T11:41:38.123334: step 7790, loss 0.606344.
Test: 2018-07-31T11:41:38.592005: step 7790, loss 0.573793.
Train: 2018-07-31T11:41:38.763839: step 7791, loss 0.597449.
Train: 2018-07-31T11:41:38.935644: step 7792, loss 0.562014.
Train: 2018-07-31T11:41:39.123100: step 7793, loss 0.667922.
Train: 2018-07-31T11:41:39.294936: step 7794, loss 0.579454.
Train: 2018-07-31T11:41:39.482425: step 7795, loss 0.588102.
Train: 2018-07-31T11:41:39.654251: step 7796, loss 0.596701.
Train: 2018-07-31T11:41:39.826091: step 7797, loss 0.631551.
Train: 2018-07-31T11:41:39.997926: step 7798, loss 0.604986.
Train: 2018-07-31T11:41:40.185382: step 7799, loss 0.587276.
Train: 2018-07-31T11:41:40.372809: step 7800, loss 0.65659.
Test: 2018-07-31T11:41:40.857099: step 7800, loss 0.572314.
Train: 2018-07-31T11:41:41.591305: step 7801, loss 0.569498.
Train: 2018-07-31T11:41:41.763107: step 7802, loss 0.59521.
Train: 2018-07-31T11:41:41.934942: step 7803, loss 0.603553.
Train: 2018-07-31T11:41:42.106810: step 7804, loss 0.577549.
Train: 2018-07-31T11:41:42.294263: step 7805, loss 0.577348.
Train: 2018-07-31T11:41:42.481713: step 7806, loss 0.577145.
Train: 2018-07-31T11:41:42.653523: step 7807, loss 0.67059.
Train: 2018-07-31T11:41:42.825389: step 7808, loss 0.551311.
Train: 2018-07-31T11:41:42.997223: step 7809, loss 0.593498.
Train: 2018-07-31T11:41:43.184649: step 7810, loss 0.643937.
Test: 2018-07-31T11:41:43.653289: step 7810, loss 0.570541.
Train: 2018-07-31T11:41:43.825124: step 7811, loss 0.57622.
Train: 2018-07-31T11:41:44.012581: step 7812, loss 0.576051.
Train: 2018-07-31T11:41:44.184445: step 7813, loss 0.584254.
Train: 2018-07-31T11:41:44.356251: step 7814, loss 0.542268.
Train: 2018-07-31T11:41:44.543706: step 7815, loss 0.592233.
Train: 2018-07-31T11:41:44.715576: step 7816, loss 0.608744.
Train: 2018-07-31T11:41:44.887375: step 7817, loss 0.566825.
Train: 2018-07-31T11:41:45.074832: step 7818, loss 0.600013.
Train: 2018-07-31T11:41:45.246696: step 7819, loss 0.658184.
Train: 2018-07-31T11:41:45.418502: step 7820, loss 0.632909.
Test: 2018-07-31T11:41:45.902762: step 7820, loss 0.568951.
Train: 2018-07-31T11:41:46.074628: step 7821, loss 0.491599.
Train: 2018-07-31T11:41:46.262055: step 7822, loss 0.582677.
Train: 2018-07-31T11:41:46.433889: step 7823, loss 0.549345.
Train: 2018-07-31T11:41:46.605722: step 7824, loss 0.730847.
Train: 2018-07-31T11:41:46.777589: step 7825, loss 0.515767.
Train: 2018-07-31T11:41:46.949392: step 7826, loss 0.62361.
Train: 2018-07-31T11:41:47.121258: step 7827, loss 0.581857.
Train: 2018-07-31T11:41:47.308708: step 7828, loss 0.598373.
Train: 2018-07-31T11:41:47.480549: step 7829, loss 0.581551.
Train: 2018-07-31T11:41:47.668005: step 7830, loss 0.556358.
Test: 2018-07-31T11:41:48.136644: step 7830, loss 0.567253.
Train: 2018-07-31T11:41:48.308480: step 7831, loss 0.514372.
Train: 2018-07-31T11:41:48.495906: step 7832, loss 0.581102.
Train: 2018-07-31T11:41:48.667740: step 7833, loss 0.564138.
Train: 2018-07-31T11:41:48.839575: step 7834, loss 0.53021.
Train: 2018-07-31T11:41:49.011441: step 7835, loss 0.55527.
Train: 2018-07-31T11:41:49.198896: step 7836, loss 0.623043.
Train: 2018-07-31T11:41:49.370733: step 7837, loss 0.597458.
Train: 2018-07-31T11:41:49.542535: step 7838, loss 0.648637.
Train: 2018-07-31T11:41:49.714400: step 7839, loss 0.55452.
Train: 2018-07-31T11:41:49.886206: step 7840, loss 0.597162.
Test: 2018-07-31T11:41:50.370467: step 7840, loss 0.565597.
Train: 2018-07-31T11:41:50.542302: step 7841, loss 0.545691.
Train: 2018-07-31T11:41:50.714136: step 7842, loss 0.51125.
Train: 2018-07-31T11:41:50.901617: step 7843, loss 0.553932.
Train: 2018-07-31T11:41:51.073426: step 7844, loss 0.579611.
Train: 2018-07-31T11:41:51.245292: step 7845, loss 0.57951.
Train: 2018-07-31T11:41:51.417127: step 7846, loss 0.562088.
Train: 2018-07-31T11:41:51.604553: step 7847, loss 0.605365.
Train: 2018-07-31T11:41:51.776388: step 7848, loss 0.631398.
Train: 2018-07-31T11:41:51.948252: step 7849, loss 0.535639.
Train: 2018-07-31T11:41:52.120056: step 7850, loss 0.605154.
Test: 2018-07-31T11:41:52.604348: step 7850, loss 0.564357.
Train: 2018-07-31T11:41:52.776185: step 7851, loss 0.587651.
Train: 2018-07-31T11:41:52.932392: step 7852, loss 0.57885.
Train: 2018-07-31T11:41:53.104232: step 7853, loss 0.570051.
Train: 2018-07-31T11:41:53.276036: step 7854, loss 0.561253.
Train: 2018-07-31T11:41:53.447901: step 7855, loss 0.508891.
Train: 2018-07-31T11:41:53.619736: step 7856, loss 0.569778.
Train: 2018-07-31T11:41:53.791541: step 7857, loss 0.52598.
Train: 2018-07-31T11:41:53.963376: step 7858, loss 0.630935.
Train: 2018-07-31T11:41:54.135210: step 7859, loss 0.560734.
Train: 2018-07-31T11:41:54.322667: step 7860, loss 0.59577.
Test: 2018-07-31T11:41:54.791305: step 7860, loss 0.56341.
Train: 2018-07-31T11:41:54.978796: step 7861, loss 0.5254.
Train: 2018-07-31T11:41:55.150629: step 7862, loss 0.560456.
Train: 2018-07-31T11:41:55.322457: step 7863, loss 0.551544.
Train: 2018-07-31T11:41:55.509920: step 7864, loss 0.542599.
Train: 2018-07-31T11:41:55.681756: step 7865, loss 0.560163.
Train: 2018-07-31T11:41:55.853589: step 7866, loss 0.595555.
Train: 2018-07-31T11:41:56.025423: step 7867, loss 0.568864.
Train: 2018-07-31T11:41:56.212879: step 7868, loss 0.542099.
Train: 2018-07-31T11:41:56.400305: step 7869, loss 0.684592.
Train: 2018-07-31T11:41:56.572139: step 7870, loss 0.586461.
Test: 2018-07-31T11:41:57.040810: step 7870, loss 0.562577.
Train: 2018-07-31T11:41:57.212615: step 7871, loss 0.64865.
Train: 2018-07-31T11:41:57.400071: step 7872, loss 0.604006.
Train: 2018-07-31T11:41:57.571905: step 7873, loss 0.586141.
Train: 2018-07-31T11:41:57.743770: step 7874, loss 0.55075.
Train: 2018-07-31T11:41:57.915574: step 7875, loss 0.541389.
Train: 2018-07-31T11:41:58.087409: step 7876, loss 0.621064.
Train: 2018-07-31T11:41:58.259273: step 7877, loss 0.515911.
Train: 2018-07-31T11:41:58.446725: step 7878, loss 0.586302.
Train: 2018-07-31T11:41:58.618534: step 7879, loss 0.56036.
Train: 2018-07-31T11:41:58.790400: step 7880, loss 0.57822.
Test: 2018-07-31T11:41:59.274630: step 7880, loss 0.563946.
Train: 2018-07-31T11:41:59.446467: step 7881, loss 0.561104.
Train: 2018-07-31T11:41:59.618301: step 7882, loss 0.666324.
Train: 2018-07-31T11:41:59.790166: step 7883, loss 0.52696.
Train: 2018-07-31T11:41:59.962000: step 7884, loss 0.623124.
Train: 2018-07-31T11:42:00.133837: step 7885, loss 0.553755.
Train: 2018-07-31T11:42:00.305641: step 7886, loss 0.55401.
Train: 2018-07-31T11:42:00.477474: step 7887, loss 0.675702.
Train: 2018-07-31T11:42:00.649341: step 7888, loss 0.632309.
Train: 2018-07-31T11:42:00.821175: step 7889, loss 0.649468.
Train: 2018-07-31T11:42:00.992980: step 7890, loss 0.55478.
Test: 2018-07-31T11:42:01.477241: step 7890, loss 0.566234.
Train: 2018-07-31T11:42:01.649107: step 7891, loss 0.631866.
Train: 2018-07-31T11:42:01.820911: step 7892, loss 0.537987.
Train: 2018-07-31T11:42:01.992744: step 7893, loss 0.58052.
Train: 2018-07-31T11:42:02.164579: step 7894, loss 0.538142.
Train: 2018-07-31T11:42:02.336445: step 7895, loss 0.622638.
Train: 2018-07-31T11:42:02.508278: step 7896, loss 0.639313.
Train: 2018-07-31T11:42:02.680084: step 7897, loss 0.546539.
Train: 2018-07-31T11:42:02.851949: step 7898, loss 0.529699.
Train: 2018-07-31T11:42:03.023754: step 7899, loss 0.605043.
Train: 2018-07-31T11:42:03.195589: step 7900, loss 0.588111.
Test: 2018-07-31T11:42:03.679881: step 7900, loss 0.565557.
Train: 2018-07-31T11:42:04.445325: step 7901, loss 0.60468.
Train: 2018-07-31T11:42:04.617131: step 7902, loss 0.696421.
Train: 2018-07-31T11:42:04.788966: step 7903, loss 0.579259.
Train: 2018-07-31T11:42:04.960830: step 7904, loss 0.51271.
Train: 2018-07-31T11:42:05.148287: step 7905, loss 0.545781.
Train: 2018-07-31T11:42:05.320091: step 7906, loss 0.578765.
Train: 2018-07-31T11:42:05.491956: step 7907, loss 0.59517.
Train: 2018-07-31T11:42:05.663761: step 7908, loss 0.586709.
Train: 2018-07-31T11:42:05.835595: step 7909, loss 0.603113.
Train: 2018-07-31T11:42:06.023076: step 7910, loss 0.578092.
Test: 2018-07-31T11:42:06.491721: step 7910, loss 0.564067.
Train: 2018-07-31T11:42:06.679178: step 7911, loss 0.511698.
Train: 2018-07-31T11:42:06.835360: step 7912, loss 0.602638.
Train: 2018-07-31T11:42:07.022818: step 7913, loss 0.544406.
Train: 2018-07-31T11:42:07.194652: step 7914, loss 0.560812.
Train: 2018-07-31T11:42:07.366517: step 7915, loss 0.577279.
Train: 2018-07-31T11:42:07.538352: step 7916, loss 0.602169.
Train: 2018-07-31T11:42:07.710157: step 7917, loss 0.485013.
Train: 2018-07-31T11:42:07.897643: step 7918, loss 0.618773.
Train: 2018-07-31T11:42:08.069478: step 7919, loss 0.551455.
Train: 2018-07-31T11:42:08.241283: step 7920, loss 0.551247.
Test: 2018-07-31T11:42:08.725544: step 7920, loss 0.562244.
Train: 2018-07-31T11:42:08.897377: step 7921, loss 0.610246.
Train: 2018-07-31T11:42:09.069212: step 7922, loss 0.559332.
Train: 2018-07-31T11:42:09.241078: step 7923, loss 0.559171.
Train: 2018-07-31T11:42:09.412914: step 7924, loss 0.635675.
Train: 2018-07-31T11:42:09.584748: step 7925, loss 0.524766.
Train: 2018-07-31T11:42:09.756552: step 7926, loss 0.575829.
Train: 2018-07-31T11:42:09.928386: step 7927, loss 0.584286.
Train: 2018-07-31T11:42:10.100222: step 7928, loss 0.601345.
Train: 2018-07-31T11:42:10.318920: step 7929, loss 0.515475.
Train: 2018-07-31T11:42:10.506401: step 7930, loss 0.618424.
Test: 2018-07-31T11:42:10.990668: step 7930, loss 0.560941.
Train: 2018-07-31T11:42:11.162508: step 7931, loss 0.540934.
Train: 2018-07-31T11:42:11.349929: step 7932, loss 0.566648.
Train: 2018-07-31T11:42:11.537384: step 7933, loss 0.583814.
Train: 2018-07-31T11:42:11.709220: step 7934, loss 0.592385.
Train: 2018-07-31T11:42:11.881054: step 7935, loss 0.670132.
Train: 2018-07-31T11:42:12.068511: step 7936, loss 0.609478.
Train: 2018-07-31T11:42:12.224756: step 7937, loss 0.626554.
Train: 2018-07-31T11:42:12.412210: step 7938, loss 0.531862.
Train: 2018-07-31T11:42:12.584015: step 7939, loss 0.600441.
Train: 2018-07-31T11:42:12.771471: step 7940, loss 0.557557.
Test: 2018-07-31T11:42:13.240140: step 7940, loss 0.560297.
Train: 2018-07-31T11:42:13.411945: step 7941, loss 0.566058.
Train: 2018-07-31T11:42:13.583781: step 7942, loss 0.583061.
Train: 2018-07-31T11:42:13.771261: step 7943, loss 0.540408.
Train: 2018-07-31T11:42:13.943101: step 7944, loss 0.574177.
Train: 2018-07-31T11:42:14.114936: step 7945, loss 0.557333.
Train: 2018-07-31T11:42:14.286742: step 7946, loss 0.59132.
Train: 2018-07-31T11:42:14.474197: step 7947, loss 0.574241.
Train: 2018-07-31T11:42:14.646031: step 7948, loss 0.506117.
Train: 2018-07-31T11:42:14.817867: step 7949, loss 0.565611.
Train: 2018-07-31T11:42:14.989701: step 7950, loss 0.522874.
Test: 2018-07-31T11:42:15.473962: step 7950, loss 0.559697.
Train: 2018-07-31T11:42:15.630208: step 7951, loss 0.642471.
Train: 2018-07-31T11:42:15.817662: step 7952, loss 0.556852.
Train: 2018-07-31T11:42:15.989502: step 7953, loss 0.608213.
Train: 2018-07-31T11:42:16.161332: step 7954, loss 0.539587.
Train: 2018-07-31T11:42:16.333137: step 7955, loss 0.573834.
Train: 2018-07-31T11:42:16.520624: step 7956, loss 0.651089.
Train: 2018-07-31T11:42:16.692457: step 7957, loss 0.496518.
Train: 2018-07-31T11:42:16.864263: step 7958, loss 0.659123.
Train: 2018-07-31T11:42:17.036097: step 7959, loss 0.625136.
Train: 2018-07-31T11:42:17.223553: step 7960, loss 0.599313.
Test: 2018-07-31T11:42:17.692194: step 7960, loss 0.559304.
Train: 2018-07-31T11:42:17.864058: step 7961, loss 0.556527.
Train: 2018-07-31T11:42:18.051485: step 7962, loss 0.559069.
Train: 2018-07-31T11:42:18.223319: step 7963, loss 0.582063.
Train: 2018-07-31T11:42:18.395185: step 7964, loss 0.53953.
Train: 2018-07-31T11:42:18.566988: step 7965, loss 0.548026.
Train: 2018-07-31T11:42:18.738854: step 7966, loss 0.633008.
Train: 2018-07-31T11:42:18.910688: step 7967, loss 0.54802.
Train: 2018-07-31T11:42:19.082523: step 7968, loss 0.518711.
Train: 2018-07-31T11:42:19.269949: step 7969, loss 0.522467.
Train: 2018-07-31T11:42:19.441784: step 7970, loss 0.607598.
Test: 2018-07-31T11:42:19.910424: step 7970, loss 0.55921.
Train: 2018-07-31T11:42:20.082259: step 7971, loss 0.547893.
Train: 2018-07-31T11:42:20.269714: step 7972, loss 0.487955.
Train: 2018-07-31T11:42:20.441549: step 7973, loss 0.573547.
Train: 2018-07-31T11:42:20.613385: step 7974, loss 0.521854.
Train: 2018-07-31T11:42:20.785250: step 7975, loss 0.547622.
Train: 2018-07-31T11:42:20.957054: step 7976, loss 0.582295.
Train: 2018-07-31T11:42:21.128913: step 7977, loss 0.530014.
Train: 2018-07-31T11:42:21.300748: step 7978, loss 0.582395.
Train: 2018-07-31T11:42:21.472558: step 7979, loss 0.591223.
Train: 2018-07-31T11:42:21.644424: step 7980, loss 0.547237.
Test: 2018-07-31T11:42:22.113064: step 7980, loss 0.558876.
Train: 2018-07-31T11:42:22.284868: step 7981, loss 0.582495.
Train: 2018-07-31T11:42:22.456704: step 7982, loss 0.67985.
Train: 2018-07-31T11:42:22.628538: step 7983, loss 0.555954.
Train: 2018-07-31T11:42:22.800372: step 7984, loss 0.564779.
Train: 2018-07-31T11:42:22.987859: step 7985, loss 0.573597.
Train: 2018-07-31T11:42:23.159664: step 7986, loss 0.582408.
Train: 2018-07-31T11:42:23.331497: step 7987, loss 0.573539.
Train: 2018-07-31T11:42:23.503364: step 7988, loss 0.56468.
Train: 2018-07-31T11:42:23.675167: step 7989, loss 0.591117.
Train: 2018-07-31T11:42:23.847002: step 7990, loss 0.529372.
Test: 2018-07-31T11:42:24.315673: step 7990, loss 0.558646.
Train: 2018-07-31T11:42:24.503098: step 7991, loss 0.573404.
Train: 2018-07-31T11:42:24.674934: step 7992, loss 0.57337.
Train: 2018-07-31T11:42:24.846798: step 7993, loss 0.546915.
Train: 2018-07-31T11:42:25.018634: step 7994, loss 0.520457.
Train: 2018-07-31T11:42:25.190438: step 7995, loss 0.643809.
Train: 2018-07-31T11:42:25.362297: step 7996, loss 0.568359.
Train: 2018-07-31T11:42:25.534137: step 7997, loss 0.511562.
Train: 2018-07-31T11:42:25.705942: step 7998, loss 0.590792.
Train: 2018-07-31T11:42:25.877809: step 7999, loss 0.555521.
Train: 2018-07-31T11:42:26.049642: step 8000, loss 0.6348.
Test: 2018-07-31T11:42:26.533903: step 8000, loss 0.558336.
Train: 2018-07-31T11:42:27.346207: step 8001, loss 0.670943.
Train: 2018-07-31T11:42:27.518042: step 8002, loss 0.546778.
Train: 2018-07-31T11:42:27.674231: step 8003, loss 0.554586.
Train: 2018-07-31T11:42:27.846066: step 8004, loss 0.547213.
Train: 2018-07-31T11:42:28.017932: step 8005, loss 0.477144.
Train: 2018-07-31T11:42:28.189736: step 8006, loss 0.530103.
Train: 2018-07-31T11:42:28.361600: step 8007, loss 0.521451.
Train: 2018-07-31T11:42:28.533405: step 8008, loss 0.583635.
Train: 2018-07-31T11:42:28.705270: step 8009, loss 0.57504.
Train: 2018-07-31T11:42:28.877104: step 8010, loss 0.602017.
Test: 2018-07-31T11:42:29.361366: step 8010, loss 0.560536.
Train: 2018-07-31T11:42:29.533202: step 8011, loss 0.6201.
Train: 2018-07-31T11:42:29.705036: step 8012, loss 0.531012.
Train: 2018-07-31T11:42:29.876840: step 8013, loss 0.459647.
Train: 2018-07-31T11:42:30.048675: step 8014, loss 0.522128.
Train: 2018-07-31T11:42:30.220540: step 8015, loss 0.575964.
Train: 2018-07-31T11:42:30.392376: step 8016, loss 0.557991.
Train: 2018-07-31T11:42:30.564209: step 8017, loss 0.521816.
Train: 2018-07-31T11:42:30.736013: step 8018, loss 0.594204.
Train: 2018-07-31T11:42:30.907879: step 8019, loss 0.594232.
Train: 2018-07-31T11:42:31.079714: step 8020, loss 0.557805.
Test: 2018-07-31T11:42:31.563944: step 8020, loss 0.560687.
Train: 2018-07-31T11:42:31.735810: step 8021, loss 0.575952.
Train: 2018-07-31T11:42:31.907650: step 8022, loss 0.566754.
Train: 2018-07-31T11:42:32.079480: step 8023, loss 0.639703.
Train: 2018-07-31T11:42:32.251284: step 8024, loss 0.548308.
Train: 2018-07-31T11:42:32.423149: step 8025, loss 0.56644.
Train: 2018-07-31T11:42:32.594954: step 8026, loss 0.675663.
Train: 2018-07-31T11:42:32.766819: step 8027, loss 0.575284.
Train: 2018-07-31T11:42:32.938624: step 8028, loss 0.584193.
Train: 2018-07-31T11:42:33.110458: step 8029, loss 0.593049.
Train: 2018-07-31T11:42:33.282323: step 8030, loss 0.547817.
Test: 2018-07-31T11:42:33.766586: step 8030, loss 0.55965.
Train: 2018-07-31T11:42:33.938389: step 8031, loss 0.556729.
Train: 2018-07-31T11:42:34.110248: step 8032, loss 0.601438.
Train: 2018-07-31T11:42:34.282059: step 8033, loss 0.574425.
Train: 2018-07-31T11:42:34.453893: step 8034, loss 0.636673.
Train: 2018-07-31T11:42:34.625728: step 8035, loss 0.618545.
Train: 2018-07-31T11:42:34.797587: step 8036, loss 0.715395.
Train: 2018-07-31T11:42:34.969422: step 8037, loss 0.556354.
Train: 2018-07-31T11:42:35.141232: step 8038, loss 0.608643.
Train: 2018-07-31T11:42:35.313098: step 8039, loss 0.60831.
Train: 2018-07-31T11:42:35.500535: step 8040, loss 0.521966.
Test: 2018-07-31T11:42:35.969193: step 8040, loss 0.55916.
Train: 2018-07-31T11:42:36.140999: step 8041, loss 0.607732.
Train: 2018-07-31T11:42:36.312857: step 8042, loss 0.641538.
Train: 2018-07-31T11:42:36.484698: step 8043, loss 0.573358.
Train: 2018-07-31T11:42:36.656533: step 8044, loss 0.606964.
Train: 2018-07-31T11:42:36.828367: step 8045, loss 0.523056.
Train: 2018-07-31T11:42:37.000172: step 8046, loss 0.539872.
Train: 2018-07-31T11:42:37.172032: step 8047, loss 0.585673.
Train: 2018-07-31T11:42:37.343872: step 8048, loss 0.523359.
Train: 2018-07-31T11:42:37.515676: step 8049, loss 0.598037.
Train: 2018-07-31T11:42:37.687541: step 8050, loss 0.631218.
Test: 2018-07-31T11:42:38.171773: step 8050, loss 0.559469.
Train: 2018-07-31T11:42:38.343638: step 8051, loss 0.639467.
Train: 2018-07-31T11:42:38.515468: step 8052, loss 0.598161.
Train: 2018-07-31T11:42:38.687301: step 8053, loss 0.524258.
Train: 2018-07-31T11:42:38.859111: step 8054, loss 0.549047.
Train: 2018-07-31T11:42:39.030946: step 8055, loss 0.573743.
Train: 2018-07-31T11:42:39.202811: step 8056, loss 0.573813.
Train: 2018-07-31T11:42:39.374648: step 8057, loss 0.582063.
Train: 2018-07-31T11:42:39.546452: step 8058, loss 0.541113.
Train: 2018-07-31T11:42:39.733937: step 8059, loss 0.565708.
Train: 2018-07-31T11:42:39.905772: step 8060, loss 0.623211.
Test: 2018-07-31T11:42:40.390003: step 8060, loss 0.560128.
Train: 2018-07-31T11:42:40.561837: step 8061, loss 0.598554.
Train: 2018-07-31T11:42:40.749295: step 8062, loss 0.557434.
Train: 2018-07-31T11:42:40.921159: step 8063, loss 0.508048.
Train: 2018-07-31T11:42:41.092994: step 8064, loss 0.540798.
Train: 2018-07-31T11:42:41.264798: step 8065, loss 0.581946.
Train: 2018-07-31T11:42:41.436663: step 8066, loss 0.54044.
Train: 2018-07-31T11:42:41.608471: step 8067, loss 0.540232.
Train: 2018-07-31T11:42:41.795925: step 8068, loss 0.565043.
Train: 2018-07-31T11:42:41.967760: step 8069, loss 0.598432.
Train: 2018-07-31T11:42:42.139625: step 8070, loss 0.615221.
Test: 2018-07-31T11:42:42.623857: step 8070, loss 0.558988.
Train: 2018-07-31T11:42:42.780099: step 8071, loss 0.573093.
Train: 2018-07-31T11:42:42.967555: step 8072, loss 0.547688.
Train: 2018-07-31T11:42:43.139385: step 8073, loss 0.547536.
Train: 2018-07-31T11:42:43.311221: step 8074, loss 0.56433.
Train: 2018-07-31T11:42:43.498687: step 8075, loss 0.598218.
Train: 2018-07-31T11:42:43.670485: step 8076, loss 0.606694.
Train: 2018-07-31T11:42:43.842320: step 8077, loss 0.564023.
Train: 2018-07-31T11:42:44.014155: step 8078, loss 0.512736.
Train: 2018-07-31T11:42:44.186014: step 8079, loss 0.589494.
Train: 2018-07-31T11:42:44.357825: step 8080, loss 0.538047.
Test: 2018-07-31T11:42:44.842085: step 8080, loss 0.557865.
Train: 2018-07-31T11:42:45.013920: step 8081, loss 0.537898.
Train: 2018-07-31T11:42:45.185755: step 8082, loss 0.546348.
Train: 2018-07-31T11:42:45.357620: step 8083, loss 0.615309.
Train: 2018-07-31T11:42:45.529450: step 8084, loss 0.598024.
Train: 2018-07-31T11:42:45.701260: step 8085, loss 0.563349.
Train: 2018-07-31T11:42:45.888716: step 8086, loss 0.519937.
Train: 2018-07-31T11:42:46.060581: step 8087, loss 0.606664.
Train: 2018-07-31T11:42:46.232386: step 8088, loss 0.650135.
Train: 2018-07-31T11:42:46.404220: step 8089, loss 0.528375.
Train: 2018-07-31T11:42:46.576054: step 8090, loss 0.632605.
Test: 2018-07-31T11:42:47.044725: step 8090, loss 0.557197.
Train: 2018-07-31T11:42:47.232181: step 8091, loss 0.51966.
Train: 2018-07-31T11:42:47.403987: step 8092, loss 0.528303.
Train: 2018-07-31T11:42:47.575851: step 8093, loss 0.536918.
Train: 2018-07-31T11:42:47.747687: step 8094, loss 0.519445.
Train: 2018-07-31T11:42:47.919522: step 8095, loss 0.528016.
Train: 2018-07-31T11:42:48.091326: step 8096, loss 0.571579.
Train: 2018-07-31T11:42:48.263191: step 8097, loss 0.615383.
Train: 2018-07-31T11:42:48.450616: step 8098, loss 0.624189.
Train: 2018-07-31T11:42:48.622482: step 8099, loss 0.571508.
Train: 2018-07-31T11:42:48.794315: step 8100, loss 0.492526.
Test: 2018-07-31T11:42:49.262926: step 8100, loss 0.556748.
Train: 2018-07-31T11:42:50.028401: step 8101, loss 0.562676.
Train: 2018-07-31T11:42:50.200207: step 8102, loss 0.615422.
Train: 2018-07-31T11:42:50.372040: step 8103, loss 0.59781.
Train: 2018-07-31T11:42:50.543905: step 8104, loss 0.553807.
Train: 2018-07-31T11:42:50.715710: step 8105, loss 0.536206.
Train: 2018-07-31T11:42:50.887546: step 8106, loss 0.588932.
Train: 2018-07-31T11:42:51.075031: step 8107, loss 0.615282.
Train: 2018-07-31T11:42:51.246865: step 8108, loss 0.623984.
Train: 2018-07-31T11:42:51.418695: step 8109, loss 0.46612.
Train: 2018-07-31T11:42:51.590529: step 8110, loss 0.527429.
Test: 2018-07-31T11:42:52.059146: step 8110, loss 0.556537.
Train: 2018-07-31T11:42:52.246601: step 8111, loss 0.597534.
Train: 2018-07-31T11:42:52.418467: step 8112, loss 0.536121.
Train: 2018-07-31T11:42:52.590302: step 8113, loss 0.615077.
Train: 2018-07-31T11:42:52.762137: step 8114, loss 0.562394.
Train: 2018-07-31T11:42:52.933942: step 8115, loss 0.544831.
Train: 2018-07-31T11:42:53.105805: step 8116, loss 0.579904.
Train: 2018-07-31T11:42:53.277635: step 8117, loss 0.553565.
Train: 2018-07-31T11:42:53.465097: step 8118, loss 0.579868.
Train: 2018-07-31T11:42:53.636932: step 8119, loss 0.562302.
Train: 2018-07-31T11:42:53.808766: step 8120, loss 0.527193.
Test: 2018-07-31T11:42:54.277406: step 8120, loss 0.55634.
Train: 2018-07-31T11:42:54.464835: step 8121, loss 0.518367.
Train: 2018-07-31T11:42:54.621045: step 8122, loss 0.597419.
Train: 2018-07-31T11:42:54.808502: step 8123, loss 0.553421.
Train: 2018-07-31T11:42:54.980337: step 8124, loss 0.526961.
Train: 2018-07-31T11:42:55.152172: step 8125, loss 0.668084.
Train: 2018-07-31T11:42:55.324006: step 8126, loss 0.650362.
Train: 2018-07-31T11:42:55.495840: step 8127, loss 0.535761.
Train: 2018-07-31T11:42:55.667675: step 8128, loss 0.570931.
Train: 2018-07-31T11:42:55.839510: step 8129, loss 0.588451.
Train: 2018-07-31T11:42:56.026968: step 8130, loss 0.649688.
Test: 2018-07-31T11:42:56.495636: step 8130, loss 0.556237.
Train: 2018-07-31T11:42:56.683064: step 8131, loss 0.579578.
Train: 2018-07-31T11:42:56.854925: step 8132, loss 0.605614.
Train: 2018-07-31T11:42:57.042385: step 8133, loss 0.562134.
Train: 2018-07-31T11:42:57.214218: step 8134, loss 0.518964.
Train: 2018-07-31T11:42:57.386023: step 8135, loss 0.596616.
Train: 2018-07-31T11:42:57.557888: step 8136, loss 0.544945.
Train: 2018-07-31T11:42:57.714102: step 8137, loss 0.579313.
Train: 2018-07-31T11:42:57.885906: step 8138, loss 0.536423.
Train: 2018-07-31T11:42:58.073363: step 8139, loss 0.510731.
Train: 2018-07-31T11:42:58.245198: step 8140, loss 0.570683.
Test: 2018-07-31T11:42:58.713867: step 8140, loss 0.556302.
Train: 2018-07-31T11:42:58.901324: step 8141, loss 0.536354.
Train: 2018-07-31T11:42:59.073128: step 8142, loss 0.579245.
Train: 2018-07-31T11:42:59.244995: step 8143, loss 0.562042.
Train: 2018-07-31T11:42:59.416831: step 8144, loss 0.536194.
Train: 2018-07-31T11:42:59.588632: step 8145, loss 0.596485.
Train: 2018-07-31T11:42:59.760468: step 8146, loss 0.656918.
Train: 2018-07-31T11:42:59.947954: step 8147, loss 0.6137.
Train: 2018-07-31T11:43:00.104167: step 8148, loss 0.561967.
Train: 2018-07-31T11:43:00.291594: step 8149, loss 0.596336.
Train: 2018-07-31T11:43:00.463452: step 8150, loss 0.570544.
Test: 2018-07-31T11:43:00.932099: step 8150, loss 0.556195.
Train: 2018-07-31T11:43:01.119556: step 8151, loss 0.561971.
Train: 2018-07-31T11:43:01.291389: step 8152, loss 0.519228.
Train: 2018-07-31T11:43:01.463193: step 8153, loss 0.587601.
Train: 2018-07-31T11:43:01.619439: step 8154, loss 0.6434.
Train: 2018-07-31T11:43:01.806862: step 8155, loss 0.613115.
Train: 2018-07-31T11:43:01.963076: step 8156, loss 0.553468.
Train: 2018-07-31T11:43:02.150558: step 8157, loss 0.545008.
Train: 2018-07-31T11:43:02.322367: step 8158, loss 0.587407.
Train: 2018-07-31T11:43:02.494234: step 8159, loss 0.55352.
Train: 2018-07-31T11:43:02.666067: step 8160, loss 0.494327.
Test: 2018-07-31T11:43:03.150328: step 8160, loss 0.556244.
Train: 2018-07-31T11:43:03.322163: step 8161, loss 0.502697.
Train: 2018-07-31T11:43:03.493969: step 8162, loss 0.570402.
Train: 2018-07-31T11:43:03.665838: step 8163, loss 0.561878.
Train: 2018-07-31T11:43:03.837667: step 8164, loss 0.578895.
Train: 2018-07-31T11:43:04.009504: step 8165, loss 0.484923.
Train: 2018-07-31T11:43:04.196959: step 8166, loss 0.57034.
Train: 2018-07-31T11:43:04.368764: step 8167, loss 0.596136.
Train: 2018-07-31T11:43:04.540628: step 8168, loss 0.47547.
Train: 2018-07-31T11:43:04.712462: step 8169, loss 0.578972.
Train: 2018-07-31T11:43:04.884298: step 8170, loss 0.544245.
Test: 2018-07-31T11:43:05.368560: step 8170, loss 0.555706.
Train: 2018-07-31T11:43:05.540394: step 8171, loss 0.579031.
Train: 2018-07-31T11:43:05.712223: step 8172, loss 0.517834.
Train: 2018-07-31T11:43:05.884033: step 8173, loss 0.570317.
Train: 2018-07-31T11:43:06.055867: step 8174, loss 0.596743.
Train: 2018-07-31T11:43:06.227702: step 8175, loss 0.482068.
Train: 2018-07-31T11:43:06.399538: step 8176, loss 0.543766.
Train: 2018-07-31T11:43:06.586993: step 8177, loss 0.552577.
Train: 2018-07-31T11:43:06.758829: step 8178, loss 0.632829.
Train: 2018-07-31T11:43:06.946284: step 8179, loss 0.632938.
Train: 2018-07-31T11:43:07.118144: step 8180, loss 0.588245.
Test: 2018-07-31T11:43:07.586759: step 8180, loss 0.555396.
Train: 2018-07-31T11:43:07.758594: step 8181, loss 0.588219.
Train: 2018-07-31T11:43:07.946052: step 8182, loss 0.525719.
Train: 2018-07-31T11:43:08.117886: step 8183, loss 0.543557.
Train: 2018-07-31T11:43:08.289744: step 8184, loss 0.650614.
Train: 2018-07-31T11:43:08.461585: step 8185, loss 0.499024.
Train: 2018-07-31T11:43:08.633390: step 8186, loss 0.605893.
Train: 2018-07-31T11:43:08.805255: step 8187, loss 0.534668.
Train: 2018-07-31T11:43:08.977059: step 8188, loss 0.543561.
Train: 2018-07-31T11:43:09.164516: step 8189, loss 0.543551.
Train: 2018-07-31T11:43:09.336349: step 8190, loss 0.579111.
Test: 2018-07-31T11:43:09.805020: step 8190, loss 0.55531.
Train: 2018-07-31T11:43:09.988360: step 8191, loss 0.570205.
Train: 2018-07-31T11:43:10.144604: step 8192, loss 0.561301.
Train: 2018-07-31T11:43:10.332031: step 8193, loss 0.56129.
Train: 2018-07-31T11:43:10.519520: step 8194, loss 0.534614.
Train: 2018-07-31T11:43:10.691320: step 8195, loss 0.552377.
Train: 2018-07-31T11:43:10.863181: step 8196, loss 0.623537.
Train: 2018-07-31T11:43:11.034991: step 8197, loss 0.516796.
Train: 2018-07-31T11:43:11.206826: step 8198, loss 0.676837.
Train: 2018-07-31T11:43:11.378660: step 8199, loss 0.561228.
Train: 2018-07-31T11:43:11.550495: step 8200, loss 0.543507.
Test: 2018-07-31T11:43:12.034786: step 8200, loss 0.555243.
Train: 2018-07-31T11:43:12.768990: step 8201, loss 0.6585.
Train: 2018-07-31T11:43:12.940795: step 8202, loss 0.490677.
Train: 2018-07-31T11:43:13.112629: step 8203, loss 0.640441.
Train: 2018-07-31T11:43:13.284463: step 8204, loss 0.50852.
Train: 2018-07-31T11:43:13.456299: step 8205, loss 0.578729.
Train: 2018-07-31T11:43:13.628164: step 8206, loss 0.604962.
Train: 2018-07-31T11:43:13.799998: step 8207, loss 0.613601.
Train: 2018-07-31T11:43:13.971834: step 8208, loss 0.59603.
Train: 2018-07-31T11:43:14.143637: step 8209, loss 0.595923.
Train: 2018-07-31T11:43:14.315502: step 8210, loss 0.509322.
Test: 2018-07-31T11:43:14.799764: step 8210, loss 0.555395.
Train: 2018-07-31T11:43:14.971568: step 8211, loss 0.526693.
Train: 2018-07-31T11:43:15.143434: step 8212, loss 0.587086.
Train: 2018-07-31T11:43:15.315269: step 8213, loss 0.707637.
Train: 2018-07-31T11:43:15.487097: step 8214, loss 0.561238.
Train: 2018-07-31T11:43:15.658937: step 8215, loss 0.552712.
Train: 2018-07-31T11:43:15.846395: step 8216, loss 0.62947.
Train: 2018-07-31T11:43:16.018228: step 8217, loss 0.57829.
Train: 2018-07-31T11:43:16.190057: step 8218, loss 0.552875.
Train: 2018-07-31T11:43:16.361868: step 8219, loss 0.569798.
Train: 2018-07-31T11:43:16.533734: step 8220, loss 0.578215.
Test: 2018-07-31T11:43:17.002373: step 8220, loss 0.555733.
Train: 2018-07-31T11:43:17.174208: step 8221, loss 0.544603.
Train: 2018-07-31T11:43:17.346044: step 8222, loss 0.586569.
Train: 2018-07-31T11:43:17.517878: step 8223, loss 0.569793.
Train: 2018-07-31T11:43:17.705336: step 8224, loss 0.536335.
Train: 2018-07-31T11:43:17.877168: step 8225, loss 0.544696.
Train: 2018-07-31T11:43:18.048973: step 8226, loss 0.53438.
Train: 2018-07-31T11:43:18.220839: step 8227, loss 0.578132.
Train: 2018-07-31T11:43:18.392672: step 8228, loss 0.527815.
Train: 2018-07-31T11:43:18.564507: step 8229, loss 0.510906.
Train: 2018-07-31T11:43:18.751963: step 8230, loss 0.510726.
Test: 2018-07-31T11:43:19.220572: step 8230, loss 0.555604.
Train: 2018-07-31T11:43:19.408029: step 8231, loss 0.518978.
Train: 2018-07-31T11:43:19.564273: step 8232, loss 0.561716.
Train: 2018-07-31T11:43:19.736110: step 8233, loss 0.587049.
Train: 2018-07-31T11:43:19.923564: step 8234, loss 0.604403.
Train: 2018-07-31T11:43:20.095400: step 8235, loss 0.561573.
Train: 2018-07-31T11:43:20.267204: step 8236, loss 0.561676.
Train: 2018-07-31T11:43:20.439038: step 8237, loss 0.654752.
Train: 2018-07-31T11:43:20.610873: step 8238, loss 0.579197.
Train: 2018-07-31T11:43:20.782709: step 8239, loss 0.561974.
Train: 2018-07-31T11:43:20.970163: step 8240, loss 0.570712.
Test: 2018-07-31T11:43:21.438803: step 8240, loss 0.556268.
Train: 2018-07-31T11:43:21.610639: step 8241, loss 0.648731.
Train: 2018-07-31T11:43:21.798095: step 8242, loss 0.499706.
Train: 2018-07-31T11:43:21.969961: step 8243, loss 0.586901.
Train: 2018-07-31T11:43:22.141765: step 8244, loss 0.570768.
Train: 2018-07-31T11:43:22.313599: step 8245, loss 0.504441.
Train: 2018-07-31T11:43:22.485459: step 8246, loss 0.577907.
Train: 2018-07-31T11:43:22.657298: step 8247, loss 0.537503.
Train: 2018-07-31T11:43:22.844725: step 8248, loss 0.496915.
Train: 2018-07-31T11:43:23.016564: step 8249, loss 0.699158.
Train: 2018-07-31T11:43:23.188395: step 8250, loss 0.537309.
Test: 2018-07-31T11:43:23.657035: step 8250, loss 0.560458.
Train: 2018-07-31T11:43:23.828902: step 8251, loss 0.539627.
Train: 2018-07-31T11:43:24.000730: step 8252, loss 0.554031.
Train: 2018-07-31T11:43:24.188160: step 8253, loss 0.576746.
Train: 2018-07-31T11:43:24.359996: step 8254, loss 0.566699.
Train: 2018-07-31T11:43:24.531854: step 8255, loss 0.600765.
Train: 2018-07-31T11:43:24.703694: step 8256, loss 0.631599.
Train: 2018-07-31T11:43:24.891151: step 8257, loss 0.536229.
Train: 2018-07-31T11:43:25.062955: step 8258, loss 0.624366.
Train: 2018-07-31T11:43:25.234820: step 8259, loss 0.581372.
Train: 2018-07-31T11:43:25.406625: step 8260, loss 0.573308.
Test: 2018-07-31T11:43:25.890899: step 8260, loss 0.568033.
Train: 2018-07-31T11:43:26.062752: step 8261, loss 0.6263.
Train: 2018-07-31T11:43:26.234557: step 8262, loss 0.583196.
Train: 2018-07-31T11:43:26.406422: step 8263, loss 0.627204.
Train: 2018-07-31T11:43:26.578255: step 8264, loss 0.584073.
Train: 2018-07-31T11:43:26.750085: step 8265, loss 0.601749.
Train: 2018-07-31T11:43:26.921925: step 8266, loss 0.584677.
Train: 2018-07-31T11:43:27.109381: step 8267, loss 0.619417.
Train: 2018-07-31T11:43:27.281216: step 8268, loss 0.619475.
Train: 2018-07-31T11:43:27.453050: step 8269, loss 0.619462.
Train: 2018-07-31T11:43:27.624856: step 8270, loss 0.56807.
Test: 2018-07-31T11:43:28.109147: step 8270, loss 0.570894.
Train: 2018-07-31T11:43:28.280952: step 8271, loss 0.593704.
Train: 2018-07-31T11:43:28.452786: step 8272, loss 0.644671.
Train: 2018-07-31T11:43:28.624654: step 8273, loss 0.618956.
Train: 2018-07-31T11:43:28.796455: step 8274, loss 0.54277.
Train: 2018-07-31T11:43:28.968291: step 8275, loss 0.643737.
Train: 2018-07-31T11:43:29.155747: step 8276, loss 0.618212.
Train: 2018-07-31T11:43:29.327582: step 8277, loss 0.584498.
Train: 2018-07-31T11:43:29.499417: step 8278, loss 0.559335.
Train: 2018-07-31T11:43:29.671281: step 8279, loss 0.600699.
Train: 2018-07-31T11:43:29.858708: step 8280, loss 0.567297.
Test: 2018-07-31T11:43:30.327348: step 8280, loss 0.569772.
Train: 2018-07-31T11:43:30.499182: step 8281, loss 0.509181.
Train: 2018-07-31T11:43:30.671042: step 8282, loss 0.59991.
Train: 2018-07-31T11:43:30.842884: step 8283, loss 0.539376.
Train: 2018-07-31T11:43:31.030338: step 8284, loss 0.607676.
Train: 2018-07-31T11:43:31.202175: step 8285, loss 0.549346.
Train: 2018-07-31T11:43:31.374011: step 8286, loss 0.632104.
Train: 2018-07-31T11:43:31.545811: step 8287, loss 0.615225.
Train: 2018-07-31T11:43:31.717679: step 8288, loss 0.598326.
Train: 2018-07-31T11:43:31.889507: step 8289, loss 0.622983.
Train: 2018-07-31T11:43:32.076970: step 8290, loss 0.630981.
Test: 2018-07-31T11:43:32.545608: step 8290, loss 0.567065.
Train: 2018-07-31T11:43:32.717444: step 8291, loss 0.50639.
Train: 2018-07-31T11:43:32.889248: step 8292, loss 0.597251.
Train: 2018-07-31T11:43:33.076733: step 8293, loss 0.588719.
Train: 2018-07-31T11:43:33.248538: step 8294, loss 0.522222.
Train: 2018-07-31T11:43:33.420405: step 8295, loss 0.571654.
Train: 2018-07-31T11:43:33.592207: step 8296, loss 0.621242.
Train: 2018-07-31T11:43:33.764073: step 8297, loss 0.554539.
Train: 2018-07-31T11:43:33.935902: step 8298, loss 0.645843.
Train: 2018-07-31T11:43:34.107713: step 8299, loss 0.579041.
Train: 2018-07-31T11:43:34.279577: step 8300, loss 0.620448.
Test: 2018-07-31T11:43:34.748186: step 8300, loss 0.564717.
Train: 2018-07-31T11:43:35.544876: step 8301, loss 0.586959.
Train: 2018-07-31T11:43:35.732332: step 8302, loss 0.54522.
Train: 2018-07-31T11:43:35.904167: step 8303, loss 0.611512.
Train: 2018-07-31T11:43:36.076001: step 8304, loss 0.486707.
Train: 2018-07-31T11:43:36.247837: step 8305, loss 0.560145.
Train: 2018-07-31T11:43:36.435316: step 8306, loss 0.627775.
Train: 2018-07-31T11:43:36.607157: step 8307, loss 0.527439.
Train: 2018-07-31T11:43:36.778992: step 8308, loss 0.577377.
Train: 2018-07-31T11:43:36.950796: step 8309, loss 0.54366.
Train: 2018-07-31T11:43:37.122631: step 8310, loss 0.585461.
Test: 2018-07-31T11:43:37.606923: step 8310, loss 0.562775.
Train: 2018-07-31T11:43:37.778757: step 8311, loss 0.585328.
Train: 2018-07-31T11:43:37.950563: step 8312, loss 0.542953.
Train: 2018-07-31T11:43:38.122427: step 8313, loss 0.661323.
Train: 2018-07-31T11:43:38.309855: step 8314, loss 0.593425.
Train: 2018-07-31T11:43:38.481717: step 8315, loss 0.550909.
Train: 2018-07-31T11:43:38.653548: step 8316, loss 0.559248.
Train: 2018-07-31T11:43:38.825387: step 8317, loss 0.525125.
Train: 2018-07-31T11:43:38.997222: step 8318, loss 0.601516.
Train: 2018-07-31T11:43:39.184679: step 8319, loss 0.609965.
Train: 2018-07-31T11:43:39.356516: step 8320, loss 0.516026.
Test: 2018-07-31T11:43:39.840776: step 8320, loss 0.561323.
Train: 2018-07-31T11:43:40.012580: step 8321, loss 0.567091.
Train: 2018-07-31T11:43:40.184444: step 8322, loss 0.584097.
Train: 2018-07-31T11:43:40.371871: step 8323, loss 0.584008.
Train: 2018-07-31T11:43:40.543707: step 8324, loss 0.575332.
Train: 2018-07-31T11:43:40.715572: step 8325, loss 0.56664.
Train: 2018-07-31T11:43:40.902998: step 8326, loss 0.575143.
Train: 2018-07-31T11:43:41.074831: step 8327, loss 0.531978.
Train: 2018-07-31T11:43:41.262322: step 8328, loss 0.531813.
Train: 2018-07-31T11:43:41.434152: step 8329, loss 0.495878.
Train: 2018-07-31T11:43:41.605986: step 8330, loss 0.583499.
Test: 2018-07-31T11:43:42.074629: step 8330, loss 0.560169.
Train: 2018-07-31T11:43:42.246432: step 8331, loss 0.513764.
Train: 2018-07-31T11:43:42.433888: step 8332, loss 0.736711.
Train: 2018-07-31T11:43:42.605752: step 8333, loss 0.530466.
Train: 2018-07-31T11:43:42.777557: step 8334, loss 0.539555.
Train: 2018-07-31T11:43:42.949392: step 8335, loss 0.574876.
Train: 2018-07-31T11:43:43.121226: step 8336, loss 0.59275.
Train: 2018-07-31T11:43:43.308707: step 8337, loss 0.557951.
Train: 2018-07-31T11:43:43.480542: step 8338, loss 0.599939.
Train: 2018-07-31T11:43:43.636760: step 8339, loss 0.637932.
Train: 2018-07-31T11:43:43.808596: step 8340, loss 0.549115.
Test: 2018-07-31T11:43:44.277205: step 8340, loss 0.561074.
Train: 2018-07-31T11:43:44.449041: step 8341, loss 0.513699.
Train: 2018-07-31T11:43:44.636496: step 8342, loss 0.602846.
Train: 2018-07-31T11:43:44.808361: step 8343, loss 0.620792.
Train: 2018-07-31T11:43:44.980166: step 8344, loss 0.65644.
Train: 2018-07-31T11:43:45.152031: step 8345, loss 0.549763.
Train: 2018-07-31T11:43:45.323835: step 8346, loss 0.593304.
Train: 2018-07-31T11:43:45.511322: step 8347, loss 0.488147.
Train: 2018-07-31T11:43:45.683162: step 8348, loss 0.595588.
Train: 2018-07-31T11:43:45.854991: step 8349, loss 0.541089.
Train: 2018-07-31T11:43:46.026826: step 8350, loss 0.494299.
Test: 2018-07-31T11:43:46.511089: step 8350, loss 0.561504.
Train: 2018-07-31T11:43:46.682917: step 8351, loss 0.48979.
Train: 2018-07-31T11:43:46.854758: step 8352, loss 0.638348.
Train: 2018-07-31T11:43:47.026592: step 8353, loss 0.65211.
Train: 2018-07-31T11:43:47.198427: step 8354, loss 0.602839.
Train: 2018-07-31T11:43:47.370231: step 8355, loss 0.612303.
Train: 2018-07-31T11:43:47.542097: step 8356, loss 0.594668.
Train: 2018-07-31T11:43:47.713933: step 8357, loss 0.613036.
Train: 2018-07-31T11:43:47.901357: step 8358, loss 0.578125.
Train: 2018-07-31T11:43:48.073191: step 8359, loss 0.649748.
Train: 2018-07-31T11:43:48.245059: step 8360, loss 0.606014.
Test: 2018-07-31T11:43:48.713698: step 8360, loss 0.565388.
Train: 2018-07-31T11:43:48.885502: step 8361, loss 0.659418.
Train: 2018-07-31T11:43:49.072958: step 8362, loss 0.589493.
Train: 2018-07-31T11:43:49.244823: step 8363, loss 0.659777.
Train: 2018-07-31T11:43:49.416658: step 8364, loss 0.651095.
Train: 2018-07-31T11:43:49.604084: step 8365, loss 0.607945.
Train: 2018-07-31T11:43:49.760297: step 8366, loss 0.608098.
Train: 2018-07-31T11:43:49.932162: step 8367, loss 0.506247.
Train: 2018-07-31T11:43:50.103996: step 8368, loss 0.523702.
Train: 2018-07-31T11:43:50.275832: step 8369, loss 0.625232.
Train: 2018-07-31T11:43:50.463289: step 8370, loss 0.583135.
Test: 2018-07-31T11:43:50.931898: step 8370, loss 0.569126.
Train: 2018-07-31T11:43:51.103762: step 8371, loss 0.608331.
Train: 2018-07-31T11:43:51.275598: step 8372, loss 0.641698.
Train: 2018-07-31T11:43:51.447401: step 8373, loss 0.599788.
Train: 2018-07-31T11:43:51.619236: step 8374, loss 0.541572.
Train: 2018-07-31T11:43:51.806693: step 8375, loss 0.582957.
Train: 2018-07-31T11:43:51.978528: step 8376, loss 0.549771.
Train: 2018-07-31T11:43:52.150386: step 8377, loss 0.62396.
Train: 2018-07-31T11:43:52.322222: step 8378, loss 0.615487.
Train: 2018-07-31T11:43:52.509683: step 8379, loss 0.598788.
Train: 2018-07-31T11:43:52.681520: step 8380, loss 0.590357.
Test: 2018-07-31T11:43:53.150127: step 8380, loss 0.568211.
Train: 2018-07-31T11:43:53.337616: step 8381, loss 0.565543.
Train: 2018-07-31T11:43:53.509419: step 8382, loss 0.62272.
Train: 2018-07-31T11:43:53.681254: step 8383, loss 0.622455.
Train: 2018-07-31T11:43:53.868710: step 8384, loss 0.630337.
Train: 2018-07-31T11:43:54.040545: step 8385, loss 0.621866.
Train: 2018-07-31T11:43:54.212380: step 8386, loss 0.556622.
Train: 2018-07-31T11:43:54.384215: step 8387, loss 0.507883.
Train: 2018-07-31T11:43:54.556049: step 8388, loss 0.588656.
Train: 2018-07-31T11:43:54.743535: step 8389, loss 0.612748.
Train: 2018-07-31T11:43:54.915370: step 8390, loss 0.547721.
Test: 2018-07-31T11:43:55.399601: step 8390, loss 0.566326.
Train: 2018-07-31T11:43:55.571435: step 8391, loss 0.547472.
Train: 2018-07-31T11:43:55.743301: step 8392, loss 0.571553.
Train: 2018-07-31T11:43:55.930757: step 8393, loss 0.555026.
Train: 2018-07-31T11:43:56.118213: step 8394, loss 0.620035.
Train: 2018-07-31T11:43:56.290042: step 8395, loss 0.521758.
Train: 2018-07-31T11:43:56.461878: step 8396, loss 0.570575.
Train: 2018-07-31T11:43:56.633687: step 8397, loss 0.545636.
Train: 2018-07-31T11:43:56.821168: step 8398, loss 0.570079.
Train: 2018-07-31T11:43:56.992977: step 8399, loss 0.586429.
Train: 2018-07-31T11:43:57.164843: step 8400, loss 0.511332.
Test: 2018-07-31T11:43:57.649077: step 8400, loss 0.563722.
Train: 2018-07-31T11:43:58.398898: step 8401, loss 0.561002.
Train: 2018-07-31T11:43:58.570734: step 8402, loss 0.543917.
Train: 2018-07-31T11:43:58.742599: step 8403, loss 0.619603.
Train: 2018-07-31T11:43:58.914404: step 8404, loss 0.543261.
Train: 2018-07-31T11:43:59.086270: step 8405, loss 0.577022.
Train: 2018-07-31T11:43:59.258103: step 8406, loss 0.534119.
Train: 2018-07-31T11:43:59.429938: step 8407, loss 0.593889.
Train: 2018-07-31T11:43:59.617364: step 8408, loss 0.67136.
Train: 2018-07-31T11:43:59.789198: step 8409, loss 0.602314.
Train: 2018-07-31T11:43:59.961058: step 8410, loss 0.567669.
Test: 2018-07-31T11:44:00.429673: step 8410, loss 0.561705.
Train: 2018-07-31T11:44:00.601508: step 8411, loss 0.489823.
Train: 2018-07-31T11:44:00.788964: step 8412, loss 0.584693.
Train: 2018-07-31T11:44:00.960830: step 8413, loss 0.575916.
Train: 2018-07-31T11:44:01.117043: step 8414, loss 0.549746.
Train: 2018-07-31T11:44:01.304492: step 8415, loss 0.636583.
Train: 2018-07-31T11:44:01.476333: step 8416, loss 0.566861.
Train: 2018-07-31T11:44:01.648168: step 8417, loss 0.566743.
Train: 2018-07-31T11:44:01.819973: step 8418, loss 0.592763.
Train: 2018-07-31T11:44:01.991807: step 8419, loss 0.671061.
Train: 2018-07-31T11:44:02.163643: step 8420, loss 0.575116.
Test: 2018-07-31T11:44:02.632313: step 8420, loss 0.560474.
Train: 2018-07-31T11:44:02.819739: step 8421, loss 0.566329.
Train: 2018-07-31T11:44:02.991605: step 8422, loss 0.540248.
Train: 2018-07-31T11:44:03.163439: step 8423, loss 0.505548.
Train: 2018-07-31T11:44:03.335244: step 8424, loss 0.600689.
Train: 2018-07-31T11:44:03.522726: step 8425, loss 0.557279.
Train: 2018-07-31T11:44:03.694564: step 8426, loss 0.591846.
Train: 2018-07-31T11:44:03.866369: step 8427, loss 0.565755.
Train: 2018-07-31T11:44:04.038203: step 8428, loss 0.565665.
Train: 2018-07-31T11:44:04.210070: step 8429, loss 0.608917.
Train: 2018-07-31T11:44:04.397493: step 8430, loss 0.530844.
Test: 2018-07-31T11:44:04.866134: step 8430, loss 0.559562.
Train: 2018-07-31T11:44:05.053590: step 8431, loss 0.626067.
Train: 2018-07-31T11:44:05.225450: step 8432, loss 0.582649.
Train: 2018-07-31T11:44:05.397261: step 8433, loss 0.539312.
Train: 2018-07-31T11:44:05.569119: step 8434, loss 0.547888.
Train: 2018-07-31T11:44:05.740930: step 8435, loss 0.591054.
Train: 2018-07-31T11:44:05.912765: step 8436, loss 0.642871.
Train: 2018-07-31T11:44:06.084631: step 8437, loss 0.573602.
Train: 2018-07-31T11:44:06.256459: step 8438, loss 0.547662.
Train: 2018-07-31T11:44:06.428307: step 8439, loss 0.590687.
Train: 2018-07-31T11:44:06.600103: step 8440, loss 0.607808.
Test: 2018-07-31T11:44:07.084397: step 8440, loss 0.558932.
Train: 2018-07-31T11:44:07.256199: step 8441, loss 0.48742.
Train: 2018-07-31T11:44:07.428039: step 8442, loss 0.633397.
Train: 2018-07-31T11:44:07.599869: step 8443, loss 0.504525.
Train: 2018-07-31T11:44:07.771728: step 8444, loss 0.555949.
Train: 2018-07-31T11:44:07.943539: step 8445, loss 0.478525.
Train: 2018-07-31T11:44:08.115373: step 8446, loss 0.564384.
Train: 2018-07-31T11:44:08.287208: step 8447, loss 0.564305.
Train: 2018-07-31T11:44:08.474665: step 8448, loss 0.590219.
Train: 2018-07-31T11:44:08.646530: step 8449, loss 0.572839.
Train: 2018-07-31T11:44:08.818366: step 8450, loss 0.598867.
Test: 2018-07-31T11:44:09.302594: step 8450, loss 0.558163.
Train: 2018-07-31T11:44:09.474461: step 8451, loss 0.537933.
Train: 2018-07-31T11:44:09.646265: step 8452, loss 0.459418.
Train: 2018-07-31T11:44:09.818130: step 8453, loss 0.572646.
Train: 2018-07-31T11:44:09.989935: step 8454, loss 0.598922.
Train: 2018-07-31T11:44:10.161770: step 8455, loss 0.581359.
Train: 2018-07-31T11:44:10.364877: step 8456, loss 0.628876.
Train: 2018-07-31T11:44:10.552302: step 8457, loss 0.493225.
Train: 2018-07-31T11:44:10.724138: step 8458, loss 0.554799.
Train: 2018-07-31T11:44:10.895973: step 8459, loss 0.607747.
Train: 2018-07-31T11:44:11.067807: step 8460, loss 0.501627.
Test: 2018-07-31T11:44:11.552068: step 8460, loss 0.557493.
Train: 2018-07-31T11:44:11.723934: step 8461, loss 0.572329.
Train: 2018-07-31T11:44:11.895768: step 8462, loss 0.510187.
Train: 2018-07-31T11:44:12.067573: step 8463, loss 0.598957.
Train: 2018-07-31T11:44:12.239407: step 8464, loss 0.545516.
Train: 2018-07-31T11:44:12.411242: step 8465, loss 0.51867.
Train: 2018-07-31T11:44:12.583109: step 8466, loss 0.590087.
Train: 2018-07-31T11:44:12.754913: step 8467, loss 0.554249.
Train: 2018-07-31T11:44:12.926777: step 8468, loss 0.625997.
Train: 2018-07-31T11:44:13.098612: step 8469, loss 0.590061.
Train: 2018-07-31T11:44:13.270416: step 8470, loss 0.527197.
Test: 2018-07-31T11:44:13.754677: step 8470, loss 0.556994.
Train: 2018-07-31T11:44:13.926513: step 8471, loss 0.554075.
Train: 2018-07-31T11:44:14.098378: step 8472, loss 0.563013.
Train: 2018-07-31T11:44:14.270212: step 8473, loss 0.589924.
Train: 2018-07-31T11:44:14.442018: step 8474, loss 0.616819.
Train: 2018-07-31T11:44:14.613852: step 8475, loss 0.634643.
Train: 2018-07-31T11:44:14.785717: step 8476, loss 0.527082.
Train: 2018-07-31T11:44:14.957553: step 8477, loss 0.57175.
Train: 2018-07-31T11:44:15.129357: step 8478, loss 0.51822.
Train: 2018-07-31T11:44:15.316813: step 8479, loss 0.589471.
Train: 2018-07-31T11:44:15.488646: step 8480, loss 0.633891.
Test: 2018-07-31T11:44:15.972907: step 8480, loss 0.556695.
Train: 2018-07-31T11:44:16.144773: step 8481, loss 0.527184.
Train: 2018-07-31T11:44:16.316608: step 8482, loss 0.562652.
Train: 2018-07-31T11:44:16.488443: step 8483, loss 0.67767.
Train: 2018-07-31T11:44:16.660278: step 8484, loss 0.571414.
Train: 2018-07-31T11:44:16.847734: step 8485, loss 0.641682.
Train: 2018-07-31T11:44:17.019569: step 8486, loss 0.588808.
Train: 2018-07-31T11:44:17.191373: step 8487, loss 0.501577.
Train: 2018-07-31T11:44:17.363208: step 8488, loss 0.501728.
Train: 2018-07-31T11:44:17.535067: step 8489, loss 0.649279.
Train: 2018-07-31T11:44:17.722532: step 8490, loss 0.579803.
Test: 2018-07-31T11:44:18.191139: step 8490, loss 0.55666.
Train: 2018-07-31T11:44:18.363004: step 8491, loss 0.562484.
Train: 2018-07-31T11:44:18.534839: step 8492, loss 0.622757.
Train: 2018-07-31T11:44:18.706644: step 8493, loss 0.493787.
Train: 2018-07-31T11:44:18.878477: step 8494, loss 0.596751.
Train: 2018-07-31T11:44:19.065935: step 8495, loss 0.63949.
Train: 2018-07-31T11:44:19.253392: step 8496, loss 0.605119.
Train: 2018-07-31T11:44:19.425249: step 8497, loss 0.596474.
Train: 2018-07-31T11:44:19.597059: step 8498, loss 0.562453.
Train: 2018-07-31T11:44:19.768919: step 8499, loss 0.520197.
Train: 2018-07-31T11:44:19.940762: step 8500, loss 0.554012.
Test: 2018-07-31T11:44:20.425024: step 8500, loss 0.556746.
Train: 2018-07-31T11:44:21.143603: step 8501, loss 0.503406.
Train: 2018-07-31T11:44:21.331029: step 8502, loss 0.57928.
Train: 2018-07-31T11:44:21.502863: step 8503, loss 0.545478.
Train: 2018-07-31T11:44:21.674728: step 8504, loss 0.520053.
Train: 2018-07-31T11:44:21.846564: step 8505, loss 0.570753.
Train: 2018-07-31T11:44:22.018368: step 8506, loss 0.553738.
Train: 2018-07-31T11:44:22.190235: step 8507, loss 0.502598.
Train: 2018-07-31T11:44:22.362037: step 8508, loss 0.55358.
Train: 2018-07-31T11:44:22.533895: step 8509, loss 0.604935.
Train: 2018-07-31T11:44:22.705739: step 8510, loss 0.501853.
Test: 2018-07-31T11:44:23.189969: step 8510, loss 0.556148.
Train: 2018-07-31T11:44:23.361834: step 8511, loss 0.648245.
Train: 2018-07-31T11:44:23.549290: step 8512, loss 0.536009.
Train: 2018-07-31T11:44:23.721125: step 8513, loss 0.518594.
Train: 2018-07-31T11:44:23.892929: step 8514, loss 0.613978.
Train: 2018-07-31T11:44:24.064764: step 8515, loss 0.579229.
Train: 2018-07-31T11:44:24.236629: step 8516, loss 0.535656.
Train: 2018-07-31T11:44:24.408433: step 8517, loss 0.587954.
Train: 2018-07-31T11:44:24.580267: step 8518, loss 0.605437.
Train: 2018-07-31T11:44:24.752103: step 8519, loss 0.500523.
Train: 2018-07-31T11:44:24.923938: step 8520, loss 0.482886.
Test: 2018-07-31T11:44:25.408198: step 8520, loss 0.55573.
Train: 2018-07-31T11:44:25.580035: step 8521, loss 0.605574.
Train: 2018-07-31T11:44:25.767490: step 8522, loss 0.605634.
Train: 2018-07-31T11:44:25.939357: step 8523, loss 0.53518.
Train: 2018-07-31T11:44:26.111160: step 8524, loss 0.54394.
Train: 2018-07-31T11:44:26.283018: step 8525, loss 0.570397.
Train: 2018-07-31T11:44:26.454829: step 8526, loss 0.605783.
Train: 2018-07-31T11:44:26.642285: step 8527, loss 0.588075.
Train: 2018-07-31T11:44:26.814152: step 8528, loss 0.596903.
Train: 2018-07-31T11:44:26.985985: step 8529, loss 0.579168.
Train: 2018-07-31T11:44:27.157789: step 8530, loss 0.55263.
Test: 2018-07-31T11:44:27.642050: step 8530, loss 0.55549.
Train: 2018-07-31T11:44:27.876396: step 8531, loss 0.605583.
Train: 2018-07-31T11:44:28.048236: step 8532, loss 0.517363.
Train: 2018-07-31T11:44:28.220071: step 8533, loss 0.640704.
Train: 2018-07-31T11:44:28.391905: step 8534, loss 0.543816.
Train: 2018-07-31T11:44:28.563742: step 8535, loss 0.561388.
Train: 2018-07-31T11:44:28.751191: step 8536, loss 0.62277.
Train: 2018-07-31T11:44:28.923031: step 8537, loss 0.508857.
Train: 2018-07-31T11:44:29.094860: step 8538, loss 0.526375.
Train: 2018-07-31T11:44:29.266701: step 8539, loss 0.587567.
Train: 2018-07-31T11:44:29.438535: step 8540, loss 0.5788.
Test: 2018-07-31T11:44:29.922796: step 8540, loss 0.555408.
Train: 2018-07-31T11:44:30.094601: step 8541, loss 0.52636.
Train: 2018-07-31T11:44:30.266436: step 8542, loss 0.508857.
Train: 2018-07-31T11:44:30.438272: step 8543, loss 0.52626.
Train: 2018-07-31T11:44:30.610107: step 8544, loss 0.60508.
Train: 2018-07-31T11:44:30.781940: step 8545, loss 0.596328.
Train: 2018-07-31T11:44:30.953775: step 8546, loss 0.526078.
Train: 2018-07-31T11:44:31.141231: step 8547, loss 0.596338.
Train: 2018-07-31T11:44:31.313098: step 8548, loss 0.517202.
Train: 2018-07-31T11:44:31.484931: step 8549, loss 0.569946.
Train: 2018-07-31T11:44:31.656766: step 8550, loss 0.569936.
Test: 2018-07-31T11:44:32.141027: step 8550, loss 0.555157.
Train: 2018-07-31T11:44:32.312832: step 8551, loss 0.666923.
Train: 2018-07-31T11:44:32.484666: step 8552, loss 0.561095.
Train: 2018-07-31T11:44:32.672153: step 8553, loss 0.675409.
Train: 2018-07-31T11:44:32.843989: step 8554, loss 0.648723.
Train: 2018-07-31T11:44:33.015817: step 8555, loss 0.587252.
Train: 2018-07-31T11:44:33.187627: step 8556, loss 0.578457.
Train: 2018-07-31T11:44:33.359492: step 8557, loss 0.48334.
Train: 2018-07-31T11:44:33.531327: step 8558, loss 0.518007.
Train: 2018-07-31T11:44:33.703132: step 8559, loss 0.474983.
Train: 2018-07-31T11:44:33.874996: step 8560, loss 0.578325.
Test: 2018-07-31T11:44:34.359258: step 8560, loss 0.55524.
Train: 2018-07-31T11:44:34.531088: step 8561, loss 0.509293.
Train: 2018-07-31T11:44:34.702929: step 8562, loss 0.621547.
Train: 2018-07-31T11:44:34.874732: step 8563, loss 0.595615.
Train: 2018-07-31T11:44:35.046567: step 8564, loss 0.58695.
Train: 2018-07-31T11:44:35.218432: step 8565, loss 0.673386.
Train: 2018-07-31T11:44:35.390266: step 8566, loss 0.65585.
Train: 2018-07-31T11:44:35.562071: step 8567, loss 0.595361.
Train: 2018-07-31T11:44:35.733906: step 8568, loss 0.612335.
Train: 2018-07-31T11:44:35.921363: step 8569, loss 0.552564.
Train: 2018-07-31T11:44:36.093198: step 8570, loss 0.654318.
Test: 2018-07-31T11:44:36.561869: step 8570, loss 0.555464.
Train: 2018-07-31T11:44:36.749317: step 8571, loss 0.552723.
Train: 2018-07-31T11:44:36.921128: step 8572, loss 0.544412.
Train: 2018-07-31T11:44:37.092963: step 8573, loss 0.653226.
Train: 2018-07-31T11:44:37.264797: step 8574, loss 0.577929.
Train: 2018-07-31T11:44:37.436632: step 8575, loss 0.544759.
Train: 2018-07-31T11:44:37.608497: step 8576, loss 0.635726.
Train: 2018-07-31T11:44:37.780332: step 8577, loss 0.602565.
Train: 2018-07-31T11:44:37.952168: step 8578, loss 0.495968.
Train: 2018-07-31T11:44:38.123971: step 8579, loss 0.561518.
Train: 2018-07-31T11:44:38.295836: step 8580, loss 0.504385.
Test: 2018-07-31T11:44:38.764447: step 8580, loss 0.555995.
Train: 2018-07-31T11:44:38.951902: step 8581, loss 0.586012.
Train: 2018-07-31T11:44:39.123738: step 8582, loss 0.618688.
Train: 2018-07-31T11:44:39.295572: step 8583, loss 0.585986.
Train: 2018-07-31T11:44:39.467407: step 8584, loss 0.561486.
Train: 2018-07-31T11:44:39.639242: step 8585, loss 0.545155.
Train: 2018-07-31T11:44:39.811106: step 8586, loss 0.585948.
Train: 2018-07-31T11:44:39.982910: step 8587, loss 0.53692.
Train: 2018-07-31T11:44:40.170396: step 8588, loss 0.545027.
Train: 2018-07-31T11:44:40.342202: step 8589, loss 0.544942.
Train: 2018-07-31T11:44:40.514038: step 8590, loss 0.594166.
Test: 2018-07-31T11:44:40.982677: step 8590, loss 0.555674.
Train: 2018-07-31T11:44:41.185755: step 8591, loss 0.602425.
Train: 2018-07-31T11:44:41.357613: step 8592, loss 0.511706.
Train: 2018-07-31T11:44:41.529455: step 8593, loss 0.577686.
Train: 2018-07-31T11:44:41.701283: step 8594, loss 0.561093.
Train: 2018-07-31T11:44:41.873123: step 8595, loss 0.644163.
Train: 2018-07-31T11:44:42.060582: step 8596, loss 0.56102.
Train: 2018-07-31T11:44:42.232385: step 8597, loss 0.577647.
Train: 2018-07-31T11:44:42.404220: step 8598, loss 0.61097.
Train: 2018-07-31T11:44:42.591675: step 8599, loss 0.544294.
Train: 2018-07-31T11:44:42.763510: step 8600, loss 0.5526.
Test: 2018-07-31T11:44:43.232180: step 8600, loss 0.555278.
Train: 2018-07-31T11:44:43.982006: step 8601, loss 0.552563.
Train: 2018-07-31T11:44:44.153839: step 8602, loss 0.560878.
Train: 2018-07-31T11:44:44.325675: step 8603, loss 0.552469.
Train: 2018-07-31T11:44:44.497480: step 8604, loss 0.569197.
Train: 2018-07-31T11:44:44.669314: step 8605, loss 0.443063.
Train: 2018-07-31T11:44:44.841178: step 8606, loss 0.552255.
Train: 2018-07-31T11:44:44.997392: step 8607, loss 0.623429.
Train: 2018-07-31T11:44:45.184817: step 8608, loss 0.594642.
Train: 2018-07-31T11:44:45.356653: step 8609, loss 0.552036.
Train: 2018-07-31T11:44:45.528518: step 8610, loss 0.586178.
Test: 2018-07-31T11:44:45.997127: step 8610, loss 0.554728.
Train: 2018-07-31T11:44:46.168993: step 8611, loss 0.526253.
Train: 2018-07-31T11:44:46.340816: step 8612, loss 0.611984.
Train: 2018-07-31T11:44:46.512662: step 8613, loss 0.577648.
Train: 2018-07-31T11:44:46.684467: step 8614, loss 0.63788.
Train: 2018-07-31T11:44:46.856302: step 8615, loss 0.64643.
Train: 2018-07-31T11:44:47.028166: step 8616, loss 0.543281.
Train: 2018-07-31T11:44:47.200003: step 8617, loss 0.543309.
Train: 2018-07-31T11:44:47.387428: step 8618, loss 0.551881.
Train: 2018-07-31T11:44:47.559292: step 8619, loss 0.611796.
Train: 2018-07-31T11:44:47.715505: step 8620, loss 0.577537.
Test: 2018-07-31T11:44:48.199736: step 8620, loss 0.554676.
Train: 2018-07-31T11:44:48.371571: step 8621, loss 0.594598.
Train: 2018-07-31T11:44:48.559028: step 8622, loss 0.586022.
Train: 2018-07-31T11:44:48.730862: step 8623, loss 0.603006.
Train: 2018-07-31T11:44:48.902729: step 8624, loss 0.58594.
Train: 2018-07-31T11:44:49.074564: step 8625, loss 0.535073.
Train: 2018-07-31T11:44:49.246397: step 8626, loss 0.552031.
Train: 2018-07-31T11:44:49.418203: step 8627, loss 0.552037.
Train: 2018-07-31T11:44:49.590066: step 8628, loss 0.552033.
Train: 2018-07-31T11:44:49.761901: step 8629, loss 0.577372.
Train: 2018-07-31T11:44:49.933738: step 8630, loss 0.543556.
Test: 2018-07-31T11:44:50.433600: step 8630, loss 0.554734.
Train: 2018-07-31T11:44:50.605453: step 8631, loss 0.551983.
Train: 2018-07-31T11:44:50.777288: step 8632, loss 0.585818.
Train: 2018-07-31T11:44:50.949092: step 8633, loss 0.551929.
Train: 2018-07-31T11:44:51.120959: step 8634, loss 0.602783.
Train: 2018-07-31T11:44:51.308383: step 8635, loss 0.543403.
Train: 2018-07-31T11:44:51.480244: step 8636, loss 0.560349.
Train: 2018-07-31T11:44:51.652054: step 8637, loss 0.568827.
Train: 2018-07-31T11:44:51.823920: step 8638, loss 0.585826.
Train: 2018-07-31T11:44:51.995753: step 8639, loss 0.534765.
Train: 2018-07-31T11:44:52.167587: step 8640, loss 0.568793.
Test: 2018-07-31T11:44:52.651849: step 8640, loss 0.554496.
Train: 2018-07-31T11:44:52.823654: step 8641, loss 0.534662.
Train: 2018-07-31T11:44:52.995521: step 8642, loss 0.543134.
Train: 2018-07-31T11:44:53.182975: step 8643, loss 0.585888.
Train: 2018-07-31T11:44:53.354804: step 8644, loss 0.517283.
Train: 2018-07-31T11:44:53.526615: step 8645, loss 0.61174.
Train: 2018-07-31T11:44:53.698449: step 8646, loss 0.551511.
Train: 2018-07-31T11:44:53.870314: step 8647, loss 0.603302.
Train: 2018-07-31T11:44:54.042119: step 8648, loss 0.551458.
Train: 2018-07-31T11:44:54.229605: step 8649, loss 0.603268.
Train: 2018-07-31T11:44:54.401409: step 8650, loss 0.620542.
Test: 2018-07-31T11:44:54.870049: step 8650, loss 0.554249.
Train: 2018-07-31T11:44:55.057506: step 8651, loss 0.542813.
Train: 2018-07-31T11:44:55.229340: step 8652, loss 0.577322.
Train: 2018-07-31T11:44:55.385554: step 8653, loss 0.577312.
Train: 2018-07-31T11:44:55.573010: step 8654, loss 0.551458.
Train: 2018-07-31T11:44:55.744844: step 8655, loss 0.499793.
Train: 2018-07-31T11:44:55.916679: step 8656, loss 0.577298.
Train: 2018-07-31T11:44:56.088545: step 8657, loss 0.516905.
Train: 2018-07-31T11:44:56.260379: step 8658, loss 0.577313.
Train: 2018-07-31T11:44:56.432216: step 8659, loss 0.447485.
Train: 2018-07-31T11:44:56.604050: step 8660, loss 0.586047.
Test: 2018-07-31T11:44:57.088279: step 8660, loss 0.554078.
Train: 2018-07-31T11:44:57.275736: step 8661, loss 0.516387.
Train: 2018-07-31T11:44:57.447603: step 8662, loss 0.647399.
Train: 2018-07-31T11:44:57.619436: step 8663, loss 0.551161.
Train: 2018-07-31T11:44:57.791266: step 8664, loss 0.568682.
Train: 2018-07-31T11:44:57.947486: step 8665, loss 0.595044.
Train: 2018-07-31T11:44:58.119288: step 8666, loss 0.603848.
Train: 2018-07-31T11:44:58.306746: step 8667, loss 0.51593.
Train: 2018-07-31T11:44:58.478580: step 8668, loss 0.489502.
Train: 2018-07-31T11:44:58.650445: step 8669, loss 0.595113.
Train: 2018-07-31T11:44:58.822249: step 8670, loss 0.586321.
Test: 2018-07-31T11:44:59.306510: step 8670, loss 0.553876.
Train: 2018-07-31T11:44:59.478376: step 8671, loss 0.595159.
Train: 2018-07-31T11:44:59.650181: step 8672, loss 0.542167.
Train: 2018-07-31T11:44:59.822015: step 8673, loss 0.559818.
Train: 2018-07-31T11:44:59.993850: step 8674, loss 0.524468.
Train: 2018-07-31T11:45:00.165714: step 8675, loss 0.568643.
Train: 2018-07-31T11:45:00.337519: step 8676, loss 0.444722.
Train: 2018-07-31T11:45:00.509384: step 8677, loss 0.586413.
Train: 2018-07-31T11:45:00.681218: step 8678, loss 0.639851.
Train: 2018-07-31T11:45:00.853024: step 8679, loss 0.515252.
Train: 2018-07-31T11:45:01.024889: step 8680, loss 0.622141.
Test: 2018-07-31T11:45:01.509150: step 8680, loss 0.55373.
Train: 2018-07-31T11:45:01.680987: step 8681, loss 0.639949.
Train: 2018-07-31T11:45:01.852790: step 8682, loss 0.541943.
Train: 2018-07-31T11:45:02.024624: step 8683, loss 0.613044.
Train: 2018-07-31T11:45:02.196491: step 8684, loss 0.621793.
Train: 2018-07-31T11:45:02.368324: step 8685, loss 0.52436.
Train: 2018-07-31T11:45:02.540129: step 8686, loss 0.515608.
Train: 2018-07-31T11:45:02.711993: step 8687, loss 0.674278.
Train: 2018-07-31T11:45:02.883799: step 8688, loss 0.577281.
Train: 2018-07-31T11:45:03.055665: step 8689, loss 0.656056.
Train: 2018-07-31T11:45:03.227498: step 8690, loss 0.516138.
Test: 2018-07-31T11:45:03.711729: step 8690, loss 0.553872.
Train: 2018-07-31T11:45:03.883595: step 8691, loss 0.594493.
Train: 2018-07-31T11:45:04.055398: step 8692, loss 0.559749.
Train: 2018-07-31T11:45:04.227263: step 8693, loss 0.559763.
Train: 2018-07-31T11:45:04.414691: step 8694, loss 0.568384.
Train: 2018-07-31T11:45:04.586557: step 8695, loss 0.594144.
Train: 2018-07-31T11:45:04.758358: step 8696, loss 0.602634.
Train: 2018-07-31T11:45:04.930226: step 8697, loss 0.517125.
Train: 2018-07-31T11:45:05.102053: step 8698, loss 0.500157.
Train: 2018-07-31T11:45:05.273893: step 8699, loss 0.559825.
Train: 2018-07-31T11:45:05.445730: step 8700, loss 0.542763.
Test: 2018-07-31T11:45:05.914338: step 8700, loss 0.554043.
Train: 2018-07-31T11:45:06.664163: step 8701, loss 0.534203.
Train: 2018-07-31T11:45:06.836031: step 8702, loss 0.559779.
Train: 2018-07-31T11:45:07.007863: step 8703, loss 0.602544.
Train: 2018-07-31T11:45:07.179697: step 8704, loss 0.54262.
Train: 2018-07-31T11:45:07.351503: step 8705, loss 0.551156.
Train: 2018-07-31T11:45:07.523367: step 8706, loss 0.576876.
Train: 2018-07-31T11:45:07.695203: step 8707, loss 0.619842.
Train: 2018-07-31T11:45:07.867037: step 8708, loss 0.568278.
Train: 2018-07-31T11:45:08.038871: step 8709, loss 0.568271.
Train: 2018-07-31T11:45:08.210705: step 8710, loss 0.59403.
Test: 2018-07-31T11:45:08.694936: step 8710, loss 0.553882.
Train: 2018-07-31T11:45:08.866773: step 8711, loss 0.585422.
Train: 2018-07-31T11:45:09.038637: step 8712, loss 0.533951.
Train: 2018-07-31T11:45:09.210442: step 8713, loss 0.585387.
Train: 2018-07-31T11:45:09.397898: step 8714, loss 0.628215.
Train: 2018-07-31T11:45:09.569733: step 8715, loss 0.559674.
Train: 2018-07-31T11:45:09.741598: step 8716, loss 0.508428.
Train: 2018-07-31T11:45:09.913432: step 8717, loss 0.551129.
Train: 2018-07-31T11:45:10.100857: step 8718, loss 0.556001.
Train: 2018-07-31T11:45:10.272694: step 8719, loss 0.482715.
Train: 2018-07-31T11:45:10.444527: step 8720, loss 0.636916.
Test: 2018-07-31T11:45:10.913168: step 8720, loss 0.554106.
Train: 2018-07-31T11:45:11.100623: step 8721, loss 0.550752.
Train: 2018-07-31T11:45:11.272458: step 8722, loss 0.56884.
Train: 2018-07-31T11:45:11.444293: step 8723, loss 0.548281.
Train: 2018-07-31T11:45:11.616159: step 8724, loss 0.56548.
Train: 2018-07-31T11:45:11.787963: step 8725, loss 0.836735.
Train: 2018-07-31T11:45:11.975419: step 8726, loss 0.545037.
Train: 2018-07-31T11:45:12.147285: step 8727, loss 0.528615.
Train: 2018-07-31T11:45:12.319120: step 8728, loss 0.590119.
Train: 2018-07-31T11:45:12.506569: step 8729, loss 0.547807.
Train: 2018-07-31T11:45:12.678409: step 8730, loss 0.557452.
Test: 2018-07-31T11:45:13.162640: step 8730, loss 0.561199.
Train: 2018-07-31T11:45:13.365719: step 8731, loss 0.54095.
Train: 2018-07-31T11:45:13.584449: step 8732, loss 0.550463.
Train: 2018-07-31T11:45:13.771906: step 8733, loss 0.594938.
Train: 2018-07-31T11:45:13.990604: step 8734, loss 0.621951.
Train: 2018-07-31T11:45:14.193677: step 8735, loss 0.525959.
Train: 2018-07-31T11:45:14.365485: step 8736, loss 0.552686.
Train: 2018-07-31T11:45:14.537319: step 8737, loss 0.570583.
Train: 2018-07-31T11:45:14.724776: step 8738, loss 0.614924.
Train: 2018-07-31T11:45:14.896640: step 8739, loss 0.650406.
Train: 2018-07-31T11:45:15.068445: step 8740, loss 0.63272.
Test: 2018-07-31T11:45:15.552707: step 8740, loss 0.564966.
Train: 2018-07-31T11:45:15.724542: step 8741, loss 0.597331.
Train: 2018-07-31T11:45:15.896376: step 8742, loss 0.526875.
Train: 2018-07-31T11:45:16.068243: step 8743, loss 0.561876.
Train: 2018-07-31T11:45:16.240070: step 8744, loss 0.561679.
Train: 2018-07-31T11:45:16.411880: step 8745, loss 0.596508.
Train: 2018-07-31T11:45:16.583716: step 8746, loss 0.5349.
Train: 2018-07-31T11:45:16.755579: step 8747, loss 0.622202.
Train: 2018-07-31T11:45:16.927417: step 8748, loss 0.551833.
Train: 2018-07-31T11:45:17.114871: step 8749, loss 0.560269.
Train: 2018-07-31T11:45:17.286705: step 8750, loss 0.577436.
Test: 2018-07-31T11:45:17.755314: step 8750, loss 0.562459.
Train: 2018-07-31T11:45:17.927150: step 8751, loss 0.594595.
Train: 2018-07-31T11:45:18.098985: step 8752, loss 0.602992.
Train: 2018-07-31T11:45:18.286441: step 8753, loss 0.541528.
Train: 2018-07-31T11:45:18.458275: step 8754, loss 0.628472.
Train: 2018-07-31T11:45:18.630141: step 8755, loss 0.628072.
Train: 2018-07-31T11:45:18.801978: step 8756, loss 0.566803.
Train: 2018-07-31T11:45:18.973811: step 8757, loss 0.557852.
Train: 2018-07-31T11:45:19.145615: step 8758, loss 0.556439.
Train: 2018-07-31T11:45:19.333071: step 8759, loss 0.574627.
Train: 2018-07-31T11:45:19.504939: step 8760, loss 0.557091.
Test: 2018-07-31T11:45:19.973545: step 8760, loss 0.559655.
Train: 2018-07-31T11:45:20.145381: step 8761, loss 0.582738.
Train: 2018-07-31T11:45:20.317247: step 8762, loss 0.556619.
Train: 2018-07-31T11:45:20.489050: step 8763, loss 0.556393.
Train: 2018-07-31T11:45:20.660915: step 8764, loss 0.607891.
Train: 2018-07-31T11:45:20.832721: step 8765, loss 0.573198.
Train: 2018-07-31T11:45:21.004585: step 8766, loss 0.564387.
Train: 2018-07-31T11:45:21.176390: step 8767, loss 0.615818.
Train: 2018-07-31T11:45:21.348225: step 8768, loss 0.564023.
Train: 2018-07-31T11:45:21.520090: step 8769, loss 0.572438.
Train: 2018-07-31T11:45:21.707545: step 8770, loss 0.738817.
Test: 2018-07-31T11:45:22.191809: step 8770, loss 0.557775.
Train: 2018-07-31T11:45:22.363642: step 8771, loss 0.597787.
Train: 2018-07-31T11:45:22.551068: step 8772, loss 0.648821.
Train: 2018-07-31T11:45:22.722902: step 8773, loss 0.673929.
Train: 2018-07-31T11:45:22.894737: step 8774, loss 0.622467.
Train: 2018-07-31T11:45:23.066572: step 8775, loss 0.546344.
Train: 2018-07-31T11:45:23.238406: step 8776, loss 0.521246.
Train: 2018-07-31T11:45:23.425895: step 8777, loss 0.504605.
Train: 2018-07-31T11:45:23.597727: step 8778, loss 0.663734.
Train: 2018-07-31T11:45:23.769562: step 8779, loss 0.537981.
Train: 2018-07-31T11:45:23.941399: step 8780, loss 0.638232.
Test: 2018-07-31T11:45:24.425629: step 8780, loss 0.557836.
Train: 2018-07-31T11:45:24.597494: step 8781, loss 0.6218.
Train: 2018-07-31T11:45:24.784950: step 8782, loss 0.547182.
Train: 2018-07-31T11:45:24.941165: step 8783, loss 0.547572.
Train: 2018-07-31T11:45:25.128615: step 8784, loss 0.656024.
Train: 2018-07-31T11:45:25.300423: step 8785, loss 0.556657.
Train: 2018-07-31T11:45:25.472258: step 8786, loss 0.598475.
Train: 2018-07-31T11:45:25.644093: step 8787, loss 0.57392.
Train: 2018-07-31T11:45:25.831579: step 8788, loss 0.640377.
Train: 2018-07-31T11:45:26.003414: step 8789, loss 0.590962.
Train: 2018-07-31T11:45:26.175251: step 8790, loss 0.599376.
Test: 2018-07-31T11:45:26.643889: step 8790, loss 0.561094.
Train: 2018-07-31T11:45:26.831314: step 8791, loss 0.574853.
Train: 2018-07-31T11:45:27.003150: step 8792, loss 0.550396.
Train: 2018-07-31T11:45:27.174984: step 8793, loss 0.591474.
Train: 2018-07-31T11:45:27.346819: step 8794, loss 0.526057.
Train: 2018-07-31T11:45:27.518684: step 8795, loss 0.616117.
Train: 2018-07-31T11:45:27.706141: step 8796, loss 0.59974.
Train: 2018-07-31T11:45:27.877946: step 8797, loss 0.517908.
Train: 2018-07-31T11:45:28.049779: step 8798, loss 0.558743.
Train: 2018-07-31T11:45:28.221614: step 8799, loss 0.566848.
Train: 2018-07-31T11:45:28.393449: step 8800, loss 0.533878.
Test: 2018-07-31T11:45:28.877710: step 8800, loss 0.561057.
Train: 2018-07-31T11:45:29.596317: step 8801, loss 0.517175.
Train: 2018-07-31T11:45:29.768157: step 8802, loss 0.525091.
Train: 2018-07-31T11:45:29.939962: step 8803, loss 0.624497.
Train: 2018-07-31T11:45:30.111797: step 8804, loss 0.524414.
Train: 2018-07-31T11:45:30.283662: step 8805, loss 0.607878.
Train: 2018-07-31T11:45:30.455467: step 8806, loss 0.540594.
Train: 2018-07-31T11:45:30.627302: step 8807, loss 0.523461.
Train: 2018-07-31T11:45:30.814782: step 8808, loss 0.557025.
Train: 2018-07-31T11:45:30.986622: step 8809, loss 0.573869.
Train: 2018-07-31T11:45:31.158457: step 8810, loss 0.522431.
Test: 2018-07-31T11:45:31.627097: step 8810, loss 0.559256.
Train: 2018-07-31T11:45:31.798931: step 8811, loss 0.504885.
Train: 2018-07-31T11:45:31.970737: step 8812, loss 0.616783.
Train: 2018-07-31T11:45:32.142602: step 8813, loss 0.56478.
Train: 2018-07-31T11:45:32.330027: step 8814, loss 0.582091.
Train: 2018-07-31T11:45:32.501887: step 8815, loss 0.555801.
Train: 2018-07-31T11:45:32.673730: step 8816, loss 0.590755.
Train: 2018-07-31T11:45:32.845562: step 8817, loss 0.555542.
Train: 2018-07-31T11:45:33.017397: step 8818, loss 0.581866.
Train: 2018-07-31T11:45:33.189233: step 8819, loss 0.53765.
Train: 2018-07-31T11:45:33.361066: step 8820, loss 0.528648.
Test: 2018-07-31T11:45:33.829677: step 8820, loss 0.557973.
Train: 2018-07-31T11:45:34.001511: step 8821, loss 0.626086.
Train: 2018-07-31T11:45:34.173346: step 8822, loss 0.55499.
Train: 2018-07-31T11:45:34.360803: step 8823, loss 0.617178.
Train: 2018-07-31T11:45:34.532637: step 8824, loss 0.519219.
Train: 2018-07-31T11:45:34.704472: step 8825, loss 0.697241.
Train: 2018-07-31T11:45:34.876306: step 8826, loss 0.590229.
Train: 2018-07-31T11:45:35.048142: step 8827, loss 0.448138.
Train: 2018-07-31T11:45:35.235628: step 8828, loss 0.616661.
Train: 2018-07-31T11:45:35.407462: step 8829, loss 0.473405.
Train: 2018-07-31T11:45:35.579266: step 8830, loss 0.589902.
Test: 2018-07-31T11:45:36.063562: step 8830, loss 0.557198.
Train: 2018-07-31T11:45:36.235393: step 8831, loss 0.568502.
Train: 2018-07-31T11:45:36.407231: step 8832, loss 0.707703.
Train: 2018-07-31T11:45:36.579033: step 8833, loss 0.545365.
Train: 2018-07-31T11:45:36.750897: step 8834, loss 0.54548.
Train: 2018-07-31T11:45:36.938355: step 8835, loss 0.554566.
Train: 2018-07-31T11:45:37.110188: step 8836, loss 0.590377.
Train: 2018-07-31T11:45:37.282023: step 8837, loss 0.510674.
Train: 2018-07-31T11:45:37.453852: step 8838, loss 0.662141.
Train: 2018-07-31T11:45:37.625692: step 8839, loss 0.529029.
Train: 2018-07-31T11:45:37.797498: step 8840, loss 0.618131.
Test: 2018-07-31T11:45:38.281789: step 8840, loss 0.559075.
Train: 2018-07-31T11:45:38.453624: step 8841, loss 0.529575.
Train: 2018-07-31T11:45:38.625429: step 8842, loss 0.653933.
Train: 2018-07-31T11:45:38.812917: step 8843, loss 0.565415.
Train: 2018-07-31T11:45:38.984719: step 8844, loss 0.5832.
Train: 2018-07-31T11:45:39.156553: step 8845, loss 0.58325.
Train: 2018-07-31T11:45:39.328390: step 8846, loss 0.618443.
Train: 2018-07-31T11:45:39.500224: step 8847, loss 0.495547.
Train: 2018-07-31T11:45:39.672060: step 8848, loss 0.53941.
Train: 2018-07-31T11:45:39.843923: step 8849, loss 0.626931.
Train: 2018-07-31T11:45:40.015727: step 8850, loss 0.521882.
Test: 2018-07-31T11:45:40.499990: step 8850, loss 0.559613.
Train: 2018-07-31T11:45:40.671824: step 8851, loss 0.591723.
Train: 2018-07-31T11:45:40.843688: step 8852, loss 0.547955.
Train: 2018-07-31T11:45:41.031145: step 8853, loss 0.512939.
Train: 2018-07-31T11:45:41.218595: step 8854, loss 0.582661.
Train: 2018-07-31T11:45:41.390405: step 8855, loss 0.556321.
Train: 2018-07-31T11:45:41.577888: step 8856, loss 0.599926.
Train: 2018-07-31T11:45:41.749727: step 8857, loss 0.582301.
Train: 2018-07-31T11:45:41.921561: step 8858, loss 0.529687.
Train: 2018-07-31T11:45:42.093366: step 8859, loss 0.555793.
Train: 2018-07-31T11:45:42.265226: step 8860, loss 0.643253.
Test: 2018-07-31T11:45:42.749487: step 8860, loss 0.55839.
Train: 2018-07-31T11:45:42.921325: step 8861, loss 0.573049.
Train: 2018-07-31T11:45:43.093133: step 8862, loss 0.686189.
Train: 2018-07-31T11:45:43.280589: step 8863, loss 0.61639.
Train: 2018-07-31T11:45:43.452453: step 8864, loss 0.520656.
Train: 2018-07-31T11:45:43.624258: step 8865, loss 0.537921.
Train: 2018-07-31T11:45:43.796122: step 8866, loss 0.60427.
Train: 2018-07-31T11:45:43.983548: step 8867, loss 0.520574.
Train: 2018-07-31T11:45:44.155384: step 8868, loss 0.520518.
Train: 2018-07-31T11:45:44.327249: step 8869, loss 0.577445.
Train: 2018-07-31T11:45:44.514705: step 8870, loss 0.492396.
Test: 2018-07-31T11:45:44.983315: step 8870, loss 0.557596.
Train: 2018-07-31T11:45:45.155149: step 8871, loss 0.536571.
Train: 2018-07-31T11:45:45.326984: step 8872, loss 0.660074.
Train: 2018-07-31T11:45:45.514471: step 8873, loss 0.554707.
Train: 2018-07-31T11:45:45.686275: step 8874, loss 0.615787.
Train: 2018-07-31T11:45:45.858109: step 8875, loss 0.59873.
Train: 2018-07-31T11:45:46.029945: step 8876, loss 0.546898.
Train: 2018-07-31T11:45:46.201779: step 8877, loss 0.599517.
Train: 2018-07-31T11:45:46.373644: step 8878, loss 0.573794.
Train: 2018-07-31T11:45:46.561070: step 8879, loss 0.61765.
Train: 2018-07-31T11:45:46.732937: step 8880, loss 0.539718.
Test: 2018-07-31T11:45:47.201575: step 8880, loss 0.560192.
Train: 2018-07-31T11:45:47.373410: step 8881, loss 0.592103.
Train: 2018-07-31T11:45:47.545215: step 8882, loss 0.55759.
Train: 2018-07-31T11:45:47.717079: step 8883, loss 0.56642.
Train: 2018-07-31T11:45:47.904530: step 8884, loss 0.514498.
Train: 2018-07-31T11:45:48.076341: step 8885, loss 0.540518.
Train: 2018-07-31T11:45:48.263821: step 8886, loss 0.618686.
Train: 2018-07-31T11:45:48.435631: step 8887, loss 0.592568.
Train: 2018-07-31T11:45:48.607490: step 8888, loss 0.540311.
Train: 2018-07-31T11:45:48.779337: step 8889, loss 0.601072.
Train: 2018-07-31T11:45:48.951135: step 8890, loss 0.52263.
Test: 2018-07-31T11:45:49.435396: step 8890, loss 0.560081.
Train: 2018-07-31T11:45:49.607231: step 8891, loss 0.496294.
Train: 2018-07-31T11:45:49.779096: step 8892, loss 0.600681.
Train: 2018-07-31T11:45:49.950901: step 8893, loss 0.556826.
Train: 2018-07-31T11:45:50.122766: step 8894, loss 0.626665.
Train: 2018-07-31T11:45:50.310193: step 8895, loss 0.600204.
Train: 2018-07-31T11:45:50.482057: step 8896, loss 0.661292.
Train: 2018-07-31T11:45:50.653892: step 8897, loss 0.556042.
Train: 2018-07-31T11:45:50.825696: step 8898, loss 0.564595.
Train: 2018-07-31T11:45:50.997561: step 8899, loss 0.590543.
Train: 2018-07-31T11:45:51.184987: step 8900, loss 0.555548.
Test: 2018-07-31T11:45:51.669249: step 8900, loss 0.558217.
Train: 2018-07-31T11:45:52.387861: step 8901, loss 0.650909.
Train: 2018-07-31T11:45:52.559697: step 8902, loss 0.581245.
Train: 2018-07-31T11:45:52.731530: step 8903, loss 0.54652.
Train: 2018-07-31T11:45:52.903366: step 8904, loss 0.563653.
Train: 2018-07-31T11:45:53.090790: step 8905, loss 0.683964.
Train: 2018-07-31T11:45:53.262650: step 8906, loss 0.520574.
Train: 2018-07-31T11:45:53.434494: step 8907, loss 0.554768.
Train: 2018-07-31T11:45:53.606295: step 8908, loss 0.546147.
Train: 2018-07-31T11:45:53.762509: step 8909, loss 0.535263.
Train: 2018-07-31T11:45:53.934343: step 8910, loss 0.588581.
Test: 2018-07-31T11:45:54.402982: step 8910, loss 0.557164.
Train: 2018-07-31T11:45:54.574818: step 8911, loss 0.579955.
Train: 2018-07-31T11:45:54.762275: step 8912, loss 0.613922.
Train: 2018-07-31T11:45:54.934134: step 8913, loss 0.571258.
Train: 2018-07-31T11:45:55.105974: step 8914, loss 0.537202.
Train: 2018-07-31T11:45:55.277780: step 8915, loss 0.60507.
Train: 2018-07-31T11:45:55.465235: step 8916, loss 0.554067.
Train: 2018-07-31T11:45:55.637070: step 8917, loss 0.537049.
Train: 2018-07-31T11:45:55.808905: step 8918, loss 0.604838.
Train: 2018-07-31T11:45:55.996391: step 8919, loss 0.579327.
Train: 2018-07-31T11:45:56.168225: step 8920, loss 0.638609.
Test: 2018-07-31T11:45:56.636835: step 8920, loss 0.556574.
Train: 2018-07-31T11:45:56.808702: step 8921, loss 0.562284.
Train: 2018-07-31T11:45:56.980506: step 8922, loss 0.596051.
Train: 2018-07-31T11:45:57.167986: step 8923, loss 0.621274.
Train: 2018-07-31T11:45:57.339797: step 8924, loss 0.587455.
Train: 2018-07-31T11:45:57.511662: step 8925, loss 0.595772.
Train: 2018-07-31T11:45:57.683497: step 8926, loss 0.520366.
Train: 2018-07-31T11:45:57.855330: step 8927, loss 0.587256.
Train: 2018-07-31T11:45:58.042781: step 8928, loss 0.61224.
Train: 2018-07-31T11:45:58.214621: step 8929, loss 0.595476.
Train: 2018-07-31T11:45:58.386456: step 8930, loss 0.520623.
Test: 2018-07-31T11:45:58.870688: step 8930, loss 0.556543.
Train: 2018-07-31T11:45:59.042521: step 8931, loss 0.636864.
Train: 2018-07-31T11:45:59.214386: step 8932, loss 0.562146.
Train: 2018-07-31T11:45:59.386222: step 8933, loss 0.520777.
Train: 2018-07-31T11:45:59.573678: step 8934, loss 0.562111.
Train: 2018-07-31T11:45:59.745512: step 8935, loss 0.586902.
Train: 2018-07-31T11:45:59.917342: step 8936, loss 0.562046.
Train: 2018-07-31T11:46:00.104803: step 8937, loss 0.603413.
Train: 2018-07-31T11:46:00.276638: step 8938, loss 0.545429.
Train: 2018-07-31T11:46:00.448443: step 8939, loss 0.545383.
Train: 2018-07-31T11:46:00.620277: step 8940, loss 0.636559.
Test: 2018-07-31T11:46:01.104570: step 8940, loss 0.556288.
Train: 2018-07-31T11:46:01.276404: step 8941, loss 0.537004.
Train: 2018-07-31T11:46:01.448239: step 8942, loss 0.595046.
Train: 2018-07-31T11:46:01.635689: step 8943, loss 0.586725.
Train: 2018-07-31T11:46:01.807526: step 8944, loss 0.545192.
Train: 2018-07-31T11:46:01.979334: step 8945, loss 0.603303.
Train: 2018-07-31T11:46:02.151199: step 8946, loss 0.6199.
Train: 2018-07-31T11:46:02.338649: step 8947, loss 0.628141.
Train: 2018-07-31T11:46:02.510492: step 8948, loss 0.586594.
Train: 2018-07-31T11:46:02.682295: step 8949, loss 0.570018.
Train: 2018-07-31T11:46:02.854154: step 8950, loss 0.58652.
Test: 2018-07-31T11:46:03.322770: step 8950, loss 0.556203.
Train: 2018-07-31T11:46:03.494605: step 8951, loss 0.627692.
Train: 2018-07-31T11:46:03.682092: step 8952, loss 0.586447.
Train: 2018-07-31T11:46:03.853896: step 8953, loss 0.586409.
Train: 2018-07-31T11:46:04.025730: step 8954, loss 0.594552.
Train: 2018-07-31T11:46:04.197595: step 8955, loss 0.618969.
Train: 2018-07-31T11:46:04.369424: step 8956, loss 0.537518.
Train: 2018-07-31T11:46:04.541262: step 8957, loss 0.570045.
Train: 2018-07-31T11:46:04.713100: step 8958, loss 0.52952.
Train: 2018-07-31T11:46:04.884905: step 8959, loss 0.586234.
Train: 2018-07-31T11:46:05.072361: step 8960, loss 0.505177.
Test: 2018-07-31T11:46:05.540999: step 8960, loss 0.556373.
Train: 2018-07-31T11:46:05.728487: step 8961, loss 0.505023.
Train: 2018-07-31T11:46:05.900291: step 8962, loss 0.578135.
Train: 2018-07-31T11:46:06.072156: step 8963, loss 0.553551.
Train: 2018-07-31T11:46:06.243960: step 8964, loss 0.586328.
Train: 2018-07-31T11:46:06.415825: step 8965, loss 0.602898.
Train: 2018-07-31T11:46:06.603252: step 8966, loss 0.528896.
Train: 2018-07-31T11:46:06.775087: step 8967, loss 0.537139.
Train: 2018-07-31T11:46:06.946920: step 8968, loss 0.553707.
Train: 2018-07-31T11:46:07.134378: step 8969, loss 0.595376.
Train: 2018-07-31T11:46:07.306244: step 8970, loss 0.612242.
Test: 2018-07-31T11:46:07.790473: step 8970, loss 0.556528.
Train: 2018-07-31T11:46:07.962309: step 8971, loss 0.629134.
Train: 2018-07-31T11:46:08.134144: step 8972, loss 0.562233.
Train: 2018-07-31T11:46:08.306002: step 8973, loss 0.553886.
Train: 2018-07-31T11:46:08.477843: step 8974, loss 0.604225.
Train: 2018-07-31T11:46:08.649678: step 8975, loss 0.621029.
Train: 2018-07-31T11:46:08.821483: step 8976, loss 0.56227.
Train: 2018-07-31T11:46:08.993318: step 8977, loss 0.562245.
Train: 2018-07-31T11:46:09.165182: step 8978, loss 0.486728.
Train: 2018-07-31T11:46:09.336986: step 8979, loss 0.553718.
Train: 2018-07-31T11:46:09.524442: step 8980, loss 0.604142.
Test: 2018-07-31T11:46:09.993113: step 8980, loss 0.55625.
Train: 2018-07-31T11:46:10.180571: step 8981, loss 0.570376.
Train: 2018-07-31T11:46:10.414859: step 8982, loss 0.56185.
Train: 2018-07-31T11:46:10.598235: step 8983, loss 0.553296.
Train: 2018-07-31T11:46:10.770071: step 8984, loss 0.587074.
Train: 2018-07-31T11:46:10.941877: step 8985, loss 0.544591.
Train: 2018-07-31T11:46:11.113710: step 8986, loss 0.569961.
Train: 2018-07-31T11:46:11.285576: step 8987, loss 0.527314.
Train: 2018-07-31T11:46:11.457410: step 8988, loss 0.586872.
Train: 2018-07-31T11:46:11.644866: step 8989, loss 0.561181.
Train: 2018-07-31T11:46:11.816670: step 8990, loss 0.646756.
Test: 2018-07-31T11:46:12.300962: step 8990, loss 0.555255.
Train: 2018-07-31T11:46:12.472797: step 8991, loss 0.518205.
Train: 2018-07-31T11:46:12.644626: step 8992, loss 0.518089.
Train: 2018-07-31T11:46:12.832056: step 8993, loss 0.552302.
Train: 2018-07-31T11:46:13.003893: step 8994, loss 0.595282.
Train: 2018-07-31T11:46:13.175757: step 8995, loss 0.552146.
Train: 2018-07-31T11:46:13.347561: step 8996, loss 0.655764.
Train: 2018-07-31T11:46:13.519396: step 8997, loss 0.629781.
Train: 2018-07-31T11:46:13.691231: step 8998, loss 0.560663.
Train: 2018-07-31T11:46:13.878712: step 8999, loss 0.586475.
Train: 2018-07-31T11:46:14.050522: step 9000, loss 0.552029.
Test: 2018-07-31T11:46:14.519193: step 9000, loss 0.554815.
Train: 2018-07-31T11:46:15.222153: step 9001, loss 0.569198.
Train: 2018-07-31T11:46:15.409578: step 9002, loss 0.509117.
Train: 2018-07-31T11:46:15.581414: step 9003, loss 0.586319.
Train: 2018-07-31T11:46:15.753249: step 9004, loss 0.594882.
Train: 2018-07-31T11:46:15.925083: step 9005, loss 0.612005.
Train: 2018-07-31T11:46:16.096919: step 9006, loss 0.543398.
Train: 2018-07-31T11:46:16.284399: step 9007, loss 0.57764.
Train: 2018-07-31T11:46:16.456244: step 9008, loss 0.58617.
Train: 2018-07-31T11:46:16.643695: step 9009, loss 0.509248.
Train: 2018-07-31T11:46:16.815501: step 9010, loss 0.611765.
Test: 2018-07-31T11:46:17.284140: step 9010, loss 0.554718.
Train: 2018-07-31T11:46:17.456009: step 9011, loss 0.569021.
Train: 2018-07-31T11:46:17.643461: step 9012, loss 0.603152.
Train: 2018-07-31T11:46:17.815266: step 9013, loss 0.551944.
Train: 2018-07-31T11:46:17.987100: step 9014, loss 0.577503.
Train: 2018-07-31T11:46:18.174587: step 9015, loss 0.594511.
Train: 2018-07-31T11:46:18.346421: step 9016, loss 0.560457.
Train: 2018-07-31T11:46:18.518256: step 9017, loss 0.551957.
Train: 2018-07-31T11:46:18.690061: step 9018, loss 0.594414.
Train: 2018-07-31T11:46:18.877547: step 9019, loss 0.585897.
Train: 2018-07-31T11:46:19.049382: step 9020, loss 0.543485.
Test: 2018-07-31T11:46:19.517992: step 9020, loss 0.554712.
Train: 2018-07-31T11:46:19.689827: step 9021, loss 0.628219.
Train: 2018-07-31T11:46:19.877314: step 9022, loss 0.594276.
Train: 2018-07-31T11:46:20.049117: step 9023, loss 0.526669.
Train: 2018-07-31T11:46:20.220983: step 9024, loss 0.526688.
Train: 2018-07-31T11:46:20.408408: step 9025, loss 0.509775.
Train: 2018-07-31T11:46:20.580244: step 9026, loss 0.526566.
Train: 2018-07-31T11:46:20.752108: step 9027, loss 0.50948.
Train: 2018-07-31T11:46:20.923913: step 9028, loss 0.551789.
Train: 2018-07-31T11:46:21.095777: step 9029, loss 0.551709.
Train: 2018-07-31T11:46:21.267582: step 9030, loss 0.474467.
Test: 2018-07-31T11:46:21.767495: step 9030, loss 0.554334.
Train: 2018-07-31T11:46:21.939335: step 9031, loss 0.568771.
Train: 2018-07-31T11:46:22.111135: step 9032, loss 0.482151.
Train: 2018-07-31T11:46:22.282970: step 9033, loss 0.56006.
Train: 2018-07-31T11:46:22.454840: step 9034, loss 0.542498.
Train: 2018-07-31T11:46:22.642261: step 9035, loss 0.551186.
Train: 2018-07-31T11:46:22.814095: step 9036, loss 0.542265.
Train: 2018-07-31T11:46:22.985961: step 9037, loss 0.568861.
Train: 2018-07-31T11:46:23.157796: step 9038, loss 0.56889.
Train: 2018-07-31T11:46:23.329599: step 9039, loss 0.48819.
Train: 2018-07-31T11:46:23.501433: step 9040, loss 0.514892.
Test: 2018-07-31T11:46:23.985725: step 9040, loss 0.553845.
Train: 2018-07-31T11:46:24.157555: step 9041, loss 0.52373.
Train: 2018-07-31T11:46:24.329365: step 9042, loss 0.578174.
Train: 2018-07-31T11:46:24.501230: step 9043, loss 0.496005.
Train: 2018-07-31T11:46:24.673065: step 9044, loss 0.688576.
Train: 2018-07-31T11:46:24.844869: step 9045, loss 0.587609.
Train: 2018-07-31T11:46:25.032326: step 9046, loss 0.550802.
Train: 2018-07-31T11:46:25.204160: step 9047, loss 0.624506.
Train: 2018-07-31T11:46:25.376025: step 9048, loss 0.624449.
Train: 2018-07-31T11:46:25.563484: step 9049, loss 0.559967.
Train: 2018-07-31T11:46:25.735286: step 9050, loss 0.541598.
Test: 2018-07-31T11:46:26.203956: step 9050, loss 0.553743.
Train: 2018-07-31T11:46:26.375791: step 9051, loss 0.559923.
Train: 2018-07-31T11:46:26.563217: step 9052, loss 0.559831.
Train: 2018-07-31T11:46:26.735051: step 9053, loss 0.532491.
Train: 2018-07-31T11:46:26.922508: step 9054, loss 0.605489.
Train: 2018-07-31T11:46:27.078751: step 9055, loss 0.559854.
Train: 2018-07-31T11:46:27.266178: step 9056, loss 0.487109.
Train: 2018-07-31T11:46:27.438013: step 9057, loss 0.550739.
Train: 2018-07-31T11:46:27.609877: step 9058, loss 0.550731.
Train: 2018-07-31T11:46:27.781711: step 9059, loss 0.514377.
Train: 2018-07-31T11:46:27.937895: step 9060, loss 0.511848.
Test: 2018-07-31T11:46:28.406534: step 9060, loss 0.553659.
Train: 2018-07-31T11:46:28.656478: step 9061, loss 0.54159.
Train: 2018-07-31T11:46:28.828341: step 9062, loss 0.587169.
Train: 2018-07-31T11:46:29.000177: step 9063, loss 0.541541.
Train: 2018-07-31T11:46:29.156390: step 9064, loss 0.532382.
Train: 2018-07-31T11:46:29.343816: step 9065, loss 0.550583.
Train: 2018-07-31T11:46:29.515651: step 9066, loss 0.550635.
Train: 2018-07-31T11:46:29.687515: step 9067, loss 0.523112.
Train: 2018-07-31T11:46:29.859350: step 9068, loss 0.495503.
Train: 2018-07-31T11:46:30.031186: step 9069, loss 0.532187.
Train: 2018-07-31T11:46:30.202990: step 9070, loss 0.605982.
Test: 2018-07-31T11:46:30.687252: step 9070, loss 0.55359.
Train: 2018-07-31T11:46:30.859130: step 9071, loss 0.559828.
Train: 2018-07-31T11:46:31.030951: step 9072, loss 0.59684.
Train: 2018-07-31T11:46:31.202785: step 9073, loss 0.578327.
Train: 2018-07-31T11:46:31.374590: step 9074, loss 0.615297.
Train: 2018-07-31T11:46:31.562046: step 9075, loss 0.504389.
Train: 2018-07-31T11:46:31.733881: step 9076, loss 0.522864.
Train: 2018-07-31T11:46:31.905716: step 9077, loss 0.513631.
Train: 2018-07-31T11:46:32.077551: step 9078, loss 0.615157.
Train: 2018-07-31T11:46:32.265006: step 9079, loss 0.568865.
Train: 2018-07-31T11:46:32.436866: step 9080, loss 0.522858.
Test: 2018-07-31T11:46:32.905512: step 9080, loss 0.553505.
Train: 2018-07-31T11:46:33.092938: step 9081, loss 0.559724.
Train: 2018-07-31T11:46:33.264802: step 9082, loss 0.532076.
Train: 2018-07-31T11:46:33.436640: step 9083, loss 0.642622.
Train: 2018-07-31T11:46:33.608472: step 9084, loss 0.651656.
Train: 2018-07-31T11:46:33.780307: step 9085, loss 0.660474.
Train: 2018-07-31T11:46:33.967733: step 9086, loss 0.568714.
Train: 2018-07-31T11:46:34.139598: step 9087, loss 0.505147.
Train: 2018-07-31T11:46:34.311435: step 9088, loss 0.532785.
Train: 2018-07-31T11:46:34.483238: step 9089, loss 0.55955.
Train: 2018-07-31T11:46:34.670694: step 9090, loss 0.568537.
Test: 2018-07-31T11:46:35.154955: step 9090, loss 0.553486.
Train: 2018-07-31T11:46:35.358032: step 9091, loss 0.622346.
Train: 2018-07-31T11:46:35.545490: step 9092, loss 0.624144.
Train: 2018-07-31T11:46:35.764215: step 9093, loss 0.568451.
Train: 2018-07-31T11:46:35.967264: step 9094, loss 0.541839.
Train: 2018-07-31T11:46:36.170342: step 9095, loss 0.612587.
Train: 2018-07-31T11:46:36.342176: step 9096, loss 0.550814.
Train: 2018-07-31T11:46:36.514011: step 9097, loss 0.594701.
Train: 2018-07-31T11:46:36.685846: step 9098, loss 0.568395.
Train: 2018-07-31T11:46:36.857712: step 9099, loss 0.550984.
Train: 2018-07-31T11:46:37.029515: step 9100, loss 0.533675.
Test: 2018-07-31T11:46:37.513807: step 9100, loss 0.553876.
Train: 2018-07-31T11:46:38.201141: step 9101, loss 0.611683.
Train: 2018-07-31T11:46:38.372981: step 9102, loss 0.568372.
Train: 2018-07-31T11:46:38.544787: step 9103, loss 0.585596.
Train: 2018-07-31T11:46:38.732243: step 9104, loss 0.645693.
Train: 2018-07-31T11:46:38.919698: step 9105, loss 0.578565.
Train: 2018-07-31T11:46:39.091534: step 9106, loss 0.568342.
Train: 2018-07-31T11:46:39.263368: step 9107, loss 0.559862.
Train: 2018-07-31T11:46:39.435233: step 9108, loss 0.60228.
Train: 2018-07-31T11:46:39.607070: step 9109, loss 0.559985.
Train: 2018-07-31T11:46:39.794524: step 9110, loss 0.551627.
Test: 2018-07-31T11:46:40.263164: step 9110, loss 0.554432.
Train: 2018-07-31T11:46:40.435001: step 9111, loss 0.551698.
Train: 2018-07-31T11:46:40.606804: step 9112, loss 0.610518.
Train: 2018-07-31T11:46:40.794260: step 9113, loss 0.57695.
Train: 2018-07-31T11:46:40.966097: step 9114, loss 0.585321.
Train: 2018-07-31T11:46:41.153575: step 9115, loss 0.535226.
Train: 2018-07-31T11:46:41.325420: step 9116, loss 0.601973.
Train: 2018-07-31T11:46:41.512874: step 9117, loss 0.593599.
Train: 2018-07-31T11:46:41.684676: step 9118, loss 0.568613.
Train: 2018-07-31T11:46:41.856510: step 9119, loss 0.576905.
Train: 2018-07-31T11:46:42.028345: step 9120, loss 0.61005.
Test: 2018-07-31T11:46:42.512638: step 9120, loss 0.554716.
Train: 2018-07-31T11:46:42.684472: step 9121, loss 0.518907.
Train: 2018-07-31T11:46:42.871922: step 9122, loss 0.526804.
Train: 2018-07-31T11:46:43.043732: step 9123, loss 0.553642.
Train: 2018-07-31T11:46:43.215567: step 9124, loss 0.585431.
Train: 2018-07-31T11:46:43.387403: step 9125, loss 0.585769.
Train: 2018-07-31T11:46:43.574859: step 9126, loss 0.577947.
Train: 2018-07-31T11:46:43.746694: step 9127, loss 0.54527.
Train: 2018-07-31T11:46:43.918527: step 9128, loss 0.545837.
Train: 2018-07-31T11:46:44.090362: step 9129, loss 0.538053.
Train: 2018-07-31T11:46:44.277849: step 9130, loss 0.569863.
Test: 2018-07-31T11:46:44.762087: step 9130, loss 0.558493.
Train: 2018-07-31T11:46:44.933915: step 9131, loss 0.547388.
Train: 2018-07-31T11:46:45.105751: step 9132, loss 0.619512.
Train: 2018-07-31T11:46:45.277585: step 9133, loss 0.573529.
Train: 2018-07-31T11:46:45.465040: step 9134, loss 0.566162.
Train: 2018-07-31T11:46:45.636900: step 9135, loss 0.606197.
Train: 2018-07-31T11:46:45.808743: step 9136, loss 0.532778.
Train: 2018-07-31T11:46:45.980544: step 9137, loss 0.533553.
Train: 2018-07-31T11:46:46.152379: step 9138, loss 0.49202.
Train: 2018-07-31T11:46:46.324214: step 9139, loss 0.594714.
Train: 2018-07-31T11:46:46.511695: step 9140, loss 0.572615.
Test: 2018-07-31T11:46:46.980341: step 9140, loss 0.565139.
Train: 2018-07-31T11:46:47.167767: step 9141, loss 0.536678.
Train: 2018-07-31T11:46:47.339602: step 9142, loss 0.674742.
Train: 2018-07-31T11:46:47.511437: step 9143, loss 0.598145.
Train: 2018-07-31T11:46:47.683270: step 9144, loss 0.547112.
Train: 2018-07-31T11:46:47.855136: step 9145, loss 0.573401.
Train: 2018-07-31T11:46:48.042563: step 9146, loss 0.591024.
Train: 2018-07-31T11:46:48.214398: step 9147, loss 0.582669.
Train: 2018-07-31T11:46:48.386232: step 9148, loss 0.557812.
Train: 2018-07-31T11:46:48.573722: step 9149, loss 0.554248.
Train: 2018-07-31T11:46:48.745553: step 9150, loss 0.68165.
Test: 2018-07-31T11:46:49.229784: step 9150, loss 0.568169.
Train: 2018-07-31T11:46:49.401619: step 9151, loss 0.57403.
Train: 2018-07-31T11:46:49.573453: step 9152, loss 0.547809.
Train: 2018-07-31T11:46:49.760910: step 9153, loss 0.56518.
Train: 2018-07-31T11:46:49.932745: step 9154, loss 0.600181.
Train: 2018-07-31T11:46:50.104579: step 9155, loss 0.617827.
Train: 2018-07-31T11:46:50.292035: step 9156, loss 0.635484.
Train: 2018-07-31T11:46:50.463901: step 9157, loss 0.495486.
Train: 2018-07-31T11:46:50.635705: step 9158, loss 0.60064.
Train: 2018-07-31T11:46:50.807540: step 9159, loss 0.61824.
Train: 2018-07-31T11:46:50.994996: step 9160, loss 0.548113.
Test: 2018-07-31T11:46:51.463666: step 9160, loss 0.568468.
Train: 2018-07-31T11:46:51.651122: step 9161, loss 0.635766.
Train: 2018-07-31T11:46:51.822927: step 9162, loss 0.679453.
Train: 2018-07-31T11:46:51.994761: step 9163, loss 0.591704.
Train: 2018-07-31T11:46:52.166626: step 9164, loss 0.57411.
Train: 2018-07-31T11:46:52.354054: step 9165, loss 0.564665.
Train: 2018-07-31T11:46:52.525912: step 9166, loss 0.617129.
Train: 2018-07-31T11:46:52.697752: step 9167, loss 0.539005.
Train: 2018-07-31T11:46:52.885179: step 9168, loss 0.616566.
Train: 2018-07-31T11:46:53.057013: step 9169, loss 0.555943.
Train: 2018-07-31T11:46:53.244469: step 9170, loss 0.607369.
Test: 2018-07-31T11:46:53.728760: step 9170, loss 0.566913.
Train: 2018-07-31T11:46:53.900565: step 9171, loss 0.510359.
Train: 2018-07-31T11:46:54.072400: step 9172, loss 0.581039.
Train: 2018-07-31T11:46:54.259886: step 9173, loss 0.572184.
Train: 2018-07-31T11:46:54.431690: step 9174, loss 0.535839.
Train: 2018-07-31T11:46:54.619177: step 9175, loss 0.681325.
Train: 2018-07-31T11:46:54.791015: step 9176, loss 0.562816.
Train: 2018-07-31T11:46:54.978438: step 9177, loss 0.622611.
Train: 2018-07-31T11:46:55.150303: step 9178, loss 0.635346.
Train: 2018-07-31T11:46:55.322108: step 9179, loss 0.489438.
Train: 2018-07-31T11:46:55.509564: step 9180, loss 0.592241.
Test: 2018-07-31T11:46:55.978234: step 9180, loss 0.565214.
Train: 2018-07-31T11:46:56.165690: step 9181, loss 0.560186.
Train: 2018-07-31T11:46:56.337495: step 9182, loss 0.554225.
Train: 2018-07-31T11:46:56.524951: step 9183, loss 0.663203.
Train: 2018-07-31T11:46:56.696787: step 9184, loss 0.542322.
Train: 2018-07-31T11:46:56.868644: step 9185, loss 0.596537.
Train: 2018-07-31T11:46:57.056109: step 9186, loss 0.589645.
Train: 2018-07-31T11:46:57.243533: step 9187, loss 0.530459.
Train: 2018-07-31T11:46:57.431014: step 9188, loss 0.624222.
Train: 2018-07-31T11:46:57.602853: step 9189, loss 0.530929.
Train: 2018-07-31T11:46:57.774658: step 9190, loss 0.599066.
Test: 2018-07-31T11:46:58.258949: step 9190, loss 0.567883.
Train: 2018-07-31T11:46:58.446377: step 9191, loss 0.616159.
Train: 2018-07-31T11:46:58.618210: step 9192, loss 0.556628.
Train: 2018-07-31T11:46:58.805667: step 9193, loss 0.582084.
Train: 2018-07-31T11:46:58.977501: step 9194, loss 0.571378.
Train: 2018-07-31T11:46:59.149335: step 9195, loss 0.658418.
Train: 2018-07-31T11:46:59.336792: step 9196, loss 0.530293.
Train: 2018-07-31T11:46:59.508657: step 9197, loss 0.564351.
Train: 2018-07-31T11:46:59.680462: step 9198, loss 0.555776.
Train: 2018-07-31T11:46:59.867918: step 9199, loss 0.590113.
Train: 2018-07-31T11:47:00.039783: step 9200, loss 0.521412.
Test: 2018-07-31T11:47:00.508422: step 9200, loss 0.567238.
Train: 2018-07-31T11:47:01.273839: step 9201, loss 0.616179.
Train: 2018-07-31T11:47:01.461326: step 9202, loss 0.581742.
Train: 2018-07-31T11:47:01.633160: step 9203, loss 0.581781.
Train: 2018-07-31T11:47:01.804994: step 9204, loss 0.538458.
Train: 2018-07-31T11:47:01.992422: step 9205, loss 0.625215.
Train: 2018-07-31T11:47:02.164254: step 9206, loss 0.590438.
Train: 2018-07-31T11:47:02.336090: step 9207, loss 0.572974.
Train: 2018-07-31T11:47:02.523576: step 9208, loss 0.633812.
Train: 2018-07-31T11:47:02.695380: step 9209, loss 0.529247.
Train: 2018-07-31T11:47:02.867246: step 9210, loss 0.598715.
Test: 2018-07-31T11:47:03.351507: step 9210, loss 0.566556.
Train: 2018-07-31T11:47:03.507721: step 9211, loss 0.655421.
Train: 2018-07-31T11:47:03.695146: step 9212, loss 0.485338.
Train: 2018-07-31T11:47:03.867011: step 9213, loss 0.537264.
Train: 2018-07-31T11:47:04.038846: step 9214, loss 0.615361.
Train: 2018-07-31T11:47:04.226303: step 9215, loss 0.54547.
Train: 2018-07-31T11:47:04.398132: step 9216, loss 0.707524.
Train: 2018-07-31T11:47:04.569966: step 9217, loss 0.597288.
Train: 2018-07-31T11:47:04.757397: step 9218, loss 0.605834.
Train: 2018-07-31T11:47:04.944854: step 9219, loss 0.614407.
Train: 2018-07-31T11:47:05.116719: step 9220, loss 0.553556.
Test: 2018-07-31T11:47:05.600951: step 9220, loss 0.565042.
Train: 2018-07-31T11:47:05.788406: step 9221, loss 0.536234.
Train: 2018-07-31T11:47:05.960271: step 9222, loss 0.562199.
Train: 2018-07-31T11:47:06.147698: step 9223, loss 0.553491.
Train: 2018-07-31T11:47:06.335154: step 9224, loss 0.536076.
Train: 2018-07-31T11:47:06.507018: step 9225, loss 0.570619.
Train: 2018-07-31T11:47:06.678853: step 9226, loss 0.570471.
Train: 2018-07-31T11:47:06.866280: step 9227, loss 0.509402.
Train: 2018-07-31T11:47:07.038114: step 9228, loss 0.648559.
Train: 2018-07-31T11:47:07.209979: step 9229, loss 0.613475.
Train: 2018-07-31T11:47:07.381784: step 9230, loss 0.499775.
Test: 2018-07-31T11:47:07.866045: step 9230, loss 0.563411.
Train: 2018-07-31T11:47:08.037909: step 9231, loss 0.63048.
Train: 2018-07-31T11:47:08.209745: step 9232, loss 0.551546.
Train: 2018-07-31T11:47:08.381549: step 9233, loss 0.568735.
Train: 2018-07-31T11:47:08.569035: step 9234, loss 0.577191.
Train: 2018-07-31T11:47:08.756492: step 9235, loss 0.568152.
Train: 2018-07-31T11:47:08.928321: step 9236, loss 0.707994.
Train: 2018-07-31T11:47:09.115782: step 9237, loss 0.532655.
Train: 2018-07-31T11:47:09.287586: step 9238, loss 0.567346.
Train: 2018-07-31T11:47:09.459452: step 9239, loss 0.602839.
Train: 2018-07-31T11:47:09.646908: step 9240, loss 0.57579.
Test: 2018-07-31T11:47:10.131140: step 9240, loss 0.561683.
Train: 2018-07-31T11:47:10.302973: step 9241, loss 0.576238.
Train: 2018-07-31T11:47:10.474839: step 9242, loss 0.559658.
Train: 2018-07-31T11:47:10.662266: step 9243, loss 0.56934.
Train: 2018-07-31T11:47:10.834130: step 9244, loss 0.613792.
Train: 2018-07-31T11:47:11.021586: step 9245, loss 0.528377.
Train: 2018-07-31T11:47:11.193425: step 9246, loss 0.529554.
Train: 2018-07-31T11:47:11.365256: step 9247, loss 0.591252.
Train: 2018-07-31T11:47:11.537090: step 9248, loss 0.626967.
Train: 2018-07-31T11:47:11.724516: step 9249, loss 0.549953.
Train: 2018-07-31T11:47:11.896351: step 9250, loss 0.568116.
Test: 2018-07-31T11:47:12.380612: step 9250, loss 0.571657.
Train: 2018-07-31T11:47:12.552447: step 9251, loss 0.542832.
Train: 2018-07-31T11:47:12.724307: step 9252, loss 0.630192.
Train: 2018-07-31T11:47:12.896148: step 9253, loss 0.587284.
Train: 2018-07-31T11:47:13.083603: step 9254, loss 0.587665.
Train: 2018-07-31T11:47:13.255407: step 9255, loss 0.640049.
Train: 2018-07-31T11:47:13.427242: step 9256, loss 0.527371.
Train: 2018-07-31T11:47:13.614699: step 9257, loss 0.53611.
Train: 2018-07-31T11:47:13.786534: step 9258, loss 0.562106.
Train: 2018-07-31T11:47:13.973990: step 9259, loss 0.553279.
Train: 2018-07-31T11:47:14.145824: step 9260, loss 0.596672.
Test: 2018-07-31T11:47:14.630116: step 9260, loss 0.573109.
Train: 2018-07-31T11:47:14.817576: step 9261, loss 0.500369.
Train: 2018-07-31T11:47:15.005028: step 9262, loss 0.63132.
Train: 2018-07-31T11:47:15.176832: step 9263, loss 0.560847.
Train: 2018-07-31T11:47:15.348667: step 9264, loss 0.586859.
Train: 2018-07-31T11:47:15.520502: step 9265, loss 0.604127.
Train: 2018-07-31T11:47:15.692372: step 9266, loss 0.533142.
Train: 2018-07-31T11:47:15.879794: step 9267, loss 0.532632.
Train: 2018-07-31T11:47:16.051627: step 9268, loss 0.603014.
Train: 2018-07-31T11:47:16.239085: step 9269, loss 0.584851.
Train: 2018-07-31T11:47:16.410949: step 9270, loss 0.628903.
Test: 2018-07-31T11:47:16.879589: step 9270, loss 0.569057.
Train: 2018-07-31T11:47:17.051394: step 9271, loss 0.53946.
Train: 2018-07-31T11:47:17.238880: step 9272, loss 0.690433.
Train: 2018-07-31T11:47:17.410715: step 9273, loss 0.556366.
Train: 2018-07-31T11:47:17.582519: step 9274, loss 0.662599.
Train: 2018-07-31T11:47:17.754354: step 9275, loss 0.546682.
Train: 2018-07-31T11:47:17.941811: step 9276, loss 0.670189.
Train: 2018-07-31T11:47:18.113675: step 9277, loss 0.563616.
Train: 2018-07-31T11:47:18.285504: step 9278, loss 0.642358.
Train: 2018-07-31T11:47:18.472937: step 9279, loss 0.597907.
Train: 2018-07-31T11:47:18.644770: step 9280, loss 0.527675.
Test: 2018-07-31T11:47:19.129063: step 9280, loss 0.565038.
Train: 2018-07-31T11:47:19.300867: step 9281, loss 0.640465.
Train: 2018-07-31T11:47:19.472703: step 9282, loss 0.605209.
Train: 2018-07-31T11:47:19.660159: step 9283, loss 0.570234.
Train: 2018-07-31T11:47:19.831993: step 9284, loss 0.587111.
Train: 2018-07-31T11:47:20.003857: step 9285, loss 0.492506.
Train: 2018-07-31T11:47:20.191308: step 9286, loss 0.517961.
Train: 2018-07-31T11:47:20.363119: step 9287, loss 0.551877.
Train: 2018-07-31T11:47:20.534983: step 9288, loss 0.594395.
Train: 2018-07-31T11:47:20.722409: step 9289, loss 0.611256.
Train: 2018-07-31T11:47:20.894268: step 9290, loss 0.585269.
Test: 2018-07-31T11:47:21.362883: step 9290, loss 0.562101.
Train: 2018-07-31T11:47:21.550340: step 9291, loss 0.52507.
Train: 2018-07-31T11:47:21.722176: step 9292, loss 0.541919.
Train: 2018-07-31T11:47:21.909632: step 9293, loss 0.541631.
Train: 2018-07-31T11:47:22.081466: step 9294, loss 0.618684.
Train: 2018-07-31T11:47:22.253331: step 9295, loss 0.609893.
Train: 2018-07-31T11:47:22.440757: step 9296, loss 0.626878.
Train: 2018-07-31T11:47:22.612592: step 9297, loss 0.669567.
Train: 2018-07-31T11:47:22.784426: step 9298, loss 0.566302.
Train: 2018-07-31T11:47:22.956261: step 9299, loss 0.549051.
Train: 2018-07-31T11:47:23.143726: step 9300, loss 0.506278.
Test: 2018-07-31T11:47:23.612388: step 9300, loss 0.560041.
Train: 2018-07-31T11:47:24.377834: step 9301, loss 0.514634.
Train: 2018-07-31T11:47:24.549671: step 9302, loss 0.548538.
Train: 2018-07-31T11:47:24.721472: step 9303, loss 0.616748.
Train: 2018-07-31T11:47:24.893332: step 9304, loss 0.556715.
Train: 2018-07-31T11:47:25.080764: step 9305, loss 0.556548.
Train: 2018-07-31T11:47:25.252629: step 9306, loss 0.522076.
Train: 2018-07-31T11:47:25.424466: step 9307, loss 0.54761.
Train: 2018-07-31T11:47:25.611920: step 9308, loss 0.598406.
Train: 2018-07-31T11:47:25.783723: step 9309, loss 0.57727.
Train: 2018-07-31T11:47:25.955584: step 9310, loss 0.624913.
Test: 2018-07-31T11:47:26.424198: step 9310, loss 0.558379.
Train: 2018-07-31T11:47:26.611680: step 9311, loss 0.538225.
Train: 2018-07-31T11:47:26.783515: step 9312, loss 0.651033.
Train: 2018-07-31T11:47:26.955326: step 9313, loss 0.618397.
Train: 2018-07-31T11:47:27.142781: step 9314, loss 0.59897.
Train: 2018-07-31T11:47:27.314646: step 9315, loss 0.63364.
Train: 2018-07-31T11:47:27.486480: step 9316, loss 0.633542.
Train: 2018-07-31T11:47:27.658286: step 9317, loss 0.495657.
Train: 2018-07-31T11:47:27.845740: step 9318, loss 0.547404.
Train: 2018-07-31T11:47:28.017600: step 9319, loss 0.547476.
Train: 2018-07-31T11:47:28.189436: step 9320, loss 0.633332.
Test: 2018-07-31T11:47:28.673673: step 9320, loss 0.558932.
Train: 2018-07-31T11:47:28.861129: step 9321, loss 0.573281.
Train: 2018-07-31T11:47:29.032988: step 9322, loss 0.633172.
Train: 2018-07-31T11:47:29.220449: step 9323, loss 0.607409.
Train: 2018-07-31T11:47:29.392254: step 9324, loss 0.581763.
Train: 2018-07-31T11:47:29.564089: step 9325, loss 0.581708.
Train: 2018-07-31T11:47:29.751546: step 9326, loss 0.590102.
Train: 2018-07-31T11:47:29.923380: step 9327, loss 0.530956.
Train: 2018-07-31T11:47:30.095245: step 9328, loss 0.573072.
Train: 2018-07-31T11:47:30.267079: step 9329, loss 0.59824.
Train: 2018-07-31T11:47:30.454505: step 9330, loss 0.530933.
Test: 2018-07-31T11:47:30.923176: step 9330, loss 0.558783.
Train: 2018-07-31T11:47:31.110601: step 9331, loss 0.640011.
Train: 2018-07-31T11:47:31.282436: step 9332, loss 0.581144.
Train: 2018-07-31T11:47:31.454272: step 9333, loss 0.589414.
Train: 2018-07-31T11:47:31.641728: step 9334, loss 0.514129.
Train: 2018-07-31T11:47:31.813592: step 9335, loss 0.539096.
Train: 2018-07-31T11:47:31.985426: step 9336, loss 0.589118.
Train: 2018-07-31T11:47:32.157256: step 9337, loss 0.563943.
Train: 2018-07-31T11:47:32.344718: step 9338, loss 0.547103.
Train: 2018-07-31T11:47:32.532144: step 9339, loss 0.53859.
Train: 2018-07-31T11:47:32.703978: step 9340, loss 0.530022.
Test: 2018-07-31T11:47:33.188258: step 9340, loss 0.5578.
Train: 2018-07-31T11:47:33.360074: step 9341, loss 0.622432.
Train: 2018-07-31T11:47:33.531909: step 9342, loss 0.588679.
Train: 2018-07-31T11:47:33.703744: step 9343, loss 0.571721.
Train: 2018-07-31T11:47:33.875579: step 9344, loss 0.571635.
Train: 2018-07-31T11:47:34.047414: step 9345, loss 0.580014.
Train: 2018-07-31T11:47:34.234900: step 9346, loss 0.512185.
Train: 2018-07-31T11:47:34.406735: step 9347, loss 0.571392.
Train: 2018-07-31T11:47:34.578570: step 9348, loss 0.664862.
Train: 2018-07-31T11:47:34.765997: step 9349, loss 0.605255.
Train: 2018-07-31T11:47:34.937830: step 9350, loss 0.58817.
Test: 2018-07-31T11:47:35.406469: step 9350, loss 0.556911.
Train: 2018-07-31T11:47:35.578306: step 9351, loss 0.562636.
Train: 2018-07-31T11:47:35.765762: step 9352, loss 0.588016.
Train: 2018-07-31T11:47:35.937626: step 9353, loss 0.53713.
Train: 2018-07-31T11:47:36.109431: step 9354, loss 0.554013.
Train: 2018-07-31T11:47:36.281296: step 9355, loss 0.537011.
Train: 2018-07-31T11:47:36.468752: step 9356, loss 0.553877.
Train: 2018-07-31T11:47:36.640557: step 9357, loss 0.579274.
Train: 2018-07-31T11:47:36.812392: step 9358, loss 0.587734.
Train: 2018-07-31T11:47:36.984226: step 9359, loss 0.57919.
Train: 2018-07-31T11:47:37.156091: step 9360, loss 0.528079.
Test: 2018-07-31T11:47:37.640321: step 9360, loss 0.556315.
Train: 2018-07-31T11:47:37.827808: step 9361, loss 0.54502.
Train: 2018-07-31T11:47:37.983993: step 9362, loss 0.661633.
Train: 2018-07-31T11:47:38.155828: step 9363, loss 0.527821.
Train: 2018-07-31T11:47:38.343314: step 9364, loss 0.570465.
Train: 2018-07-31T11:47:38.515148: step 9365, loss 0.613168.
Train: 2018-07-31T11:47:38.686953: step 9366, loss 0.58748.
Train: 2018-07-31T11:47:38.858787: step 9367, loss 0.613053.
Train: 2018-07-31T11:47:39.030621: step 9368, loss 0.638536.
Train: 2018-07-31T11:47:39.202456: step 9369, loss 0.621309.
Train: 2018-07-31T11:47:39.374291: step 9370, loss 0.553324.
Test: 2018-07-31T11:47:39.858601: step 9370, loss 0.556096.
Train: 2018-07-31T11:47:40.014800: step 9371, loss 0.536452.
Train: 2018-07-31T11:47:40.202223: step 9372, loss 0.64612.
Train: 2018-07-31T11:47:40.374057: step 9373, loss 0.528169.
Train: 2018-07-31T11:47:40.561539: step 9374, loss 0.586964.
Train: 2018-07-31T11:47:40.733347: step 9375, loss 0.553411.
Train: 2018-07-31T11:47:40.905213: step 9376, loss 0.561775.
Train: 2018-07-31T11:47:41.077042: step 9377, loss 0.570117.
Train: 2018-07-31T11:47:41.264475: step 9378, loss 0.545032.
Train: 2018-07-31T11:47:41.436309: step 9379, loss 0.595132.
Train: 2018-07-31T11:47:41.623765: step 9380, loss 0.570042.
Test: 2018-07-31T11:47:42.108056: step 9380, loss 0.556029.
Train: 2018-07-31T11:47:42.279892: step 9381, loss 0.503201.
Train: 2018-07-31T11:47:42.451695: step 9382, loss 0.578349.
Train: 2018-07-31T11:47:42.639182: step 9383, loss 0.5532.
Train: 2018-07-31T11:47:42.811021: step 9384, loss 0.57831.
Train: 2018-07-31T11:47:42.982854: step 9385, loss 0.553084.
Train: 2018-07-31T11:47:43.154687: step 9386, loss 0.527771.
Train: 2018-07-31T11:47:43.342143: step 9387, loss 0.578266.
Train: 2018-07-31T11:47:43.513977: step 9388, loss 0.612091.
Train: 2018-07-31T11:47:43.685782: step 9389, loss 0.586709.
Train: 2018-07-31T11:47:43.857647: step 9390, loss 0.544323.
Test: 2018-07-31T11:47:44.341908: step 9390, loss 0.555511.
Train: 2018-07-31T11:47:44.513743: step 9391, loss 0.544266.
Train: 2018-07-31T11:47:44.685578: step 9392, loss 0.637706.
Train: 2018-07-31T11:47:44.873034: step 9393, loss 0.518665.
Train: 2018-07-31T11:47:45.044871: step 9394, loss 0.569655.
Train: 2018-07-31T11:47:45.216674: step 9395, loss 0.620773.
Train: 2018-07-31T11:47:45.388538: step 9396, loss 0.58666.
Train: 2018-07-31T11:47:45.575989: step 9397, loss 0.586636.
Train: 2018-07-31T11:47:45.747800: step 9398, loss 0.586606.
Train: 2018-07-31T11:47:45.919658: step 9399, loss 0.544041.
Train: 2018-07-31T11:47:46.107090: step 9400, loss 0.612065.
Test: 2018-07-31T11:47:46.591351: step 9400, loss 0.555299.
Train: 2018-07-31T11:47:47.356827: step 9401, loss 0.561031.
Train: 2018-07-31T11:47:47.528632: step 9402, loss 0.544044.
Train: 2018-07-31T11:47:47.700468: step 9403, loss 0.535541.
Train: 2018-07-31T11:47:47.887923: step 9404, loss 0.527002.
Train: 2018-07-31T11:47:48.059757: step 9405, loss 0.611994.
Train: 2018-07-31T11:47:48.231623: step 9406, loss 0.526867.
Train: 2018-07-31T11:47:48.403428: step 9407, loss 0.518257.
Train: 2018-07-31T11:47:48.590917: step 9408, loss 0.577943.
Train: 2018-07-31T11:47:48.762742: step 9409, loss 0.560812.
Train: 2018-07-31T11:47:48.934554: step 9410, loss 0.577944.
Test: 2018-07-31T11:47:49.418844: step 9410, loss 0.554946.
Train: 2018-07-31T11:47:49.590680: step 9411, loss 0.560747.
Train: 2018-07-31T11:47:49.762517: step 9412, loss 0.629613.
Train: 2018-07-31T11:47:49.949941: step 9413, loss 0.50902.
Train: 2018-07-31T11:47:50.121800: step 9414, loss 0.552048.
Train: 2018-07-31T11:47:50.293639: step 9415, loss 0.560647.
Train: 2018-07-31T11:47:50.481097: step 9416, loss 0.638508.
Train: 2018-07-31T11:47:50.652900: step 9417, loss 0.543298.
Train: 2018-07-31T11:47:50.824765: step 9418, loss 0.534614.
Train: 2018-07-31T11:47:50.996595: step 9419, loss 0.54323.
Train: 2018-07-31T11:47:51.168435: step 9420, loss 0.551861.
Test: 2018-07-31T11:47:51.652665: step 9420, loss 0.55465.
Train: 2018-07-31T11:47:51.824531: step 9421, loss 0.577906.
Train: 2018-07-31T11:47:51.996335: step 9422, loss 0.482146.
Train: 2018-07-31T11:47:52.168200: step 9423, loss 0.586658.
Train: 2018-07-31T11:47:52.355657: step 9424, loss 0.525446.
Train: 2018-07-31T11:47:52.527492: step 9425, loss 0.577964.
Train: 2018-07-31T11:47:52.699326: step 9426, loss 0.498851.
Train: 2018-07-31T11:47:52.871161: step 9427, loss 0.463342.
Train: 2018-07-31T11:47:53.042966: step 9428, loss 0.657866.
Train: 2018-07-31T11:47:53.214830: step 9429, loss 0.551449.
Train: 2018-07-31T11:47:53.386665: step 9430, loss 0.524687.
Test: 2018-07-31T11:47:53.855274: step 9430, loss 0.554281.
Train: 2018-07-31T11:47:54.042764: step 9431, loss 0.524571.
Train: 2018-07-31T11:47:54.214597: step 9432, loss 0.60512.
Train: 2018-07-31T11:47:54.402053: step 9433, loss 0.596219.
Train: 2018-07-31T11:47:54.573857: step 9434, loss 0.623223.
Train: 2018-07-31T11:47:54.745692: step 9435, loss 0.569255.
Train: 2018-07-31T11:47:54.917527: step 9436, loss 0.551267.
Train: 2018-07-31T11:47:55.089391: step 9437, loss 0.47042.
Train: 2018-07-31T11:47:55.261197: step 9438, loss 0.5692.
Train: 2018-07-31T11:47:55.433061: step 9439, loss 0.524241.
Train: 2018-07-31T11:47:55.604866: step 9440, loss 0.5333.
Test: 2018-07-31T11:47:56.089156: step 9440, loss 0.554403.
Train: 2018-07-31T11:47:56.260987: step 9441, loss 0.578574.
Train: 2018-07-31T11:47:56.432827: step 9442, loss 0.560669.
Train: 2018-07-31T11:47:56.604662: step 9443, loss 0.551793.
Train: 2018-07-31T11:47:56.792088: step 9444, loss 0.551968.
Train: 2018-07-31T11:47:56.963922: step 9445, loss 0.533982.
Train: 2018-07-31T11:47:57.135757: step 9446, loss 0.552297.
Train: 2018-07-31T11:47:57.307592: step 9447, loss 0.588855.
Train: 2018-07-31T11:47:57.479427: step 9448, loss 0.634548.
Train: 2018-07-31T11:47:57.651262: step 9449, loss 0.579987.
Train: 2018-07-31T11:47:57.838717: step 9450, loss 0.480092.
Test: 2018-07-31T11:47:58.322979: step 9450, loss 0.555818.
Train: 2018-07-31T11:47:58.494844: step 9451, loss 0.552862.
Train: 2018-07-31T11:47:58.682300: step 9452, loss 0.489268.
Train: 2018-07-31T11:47:58.854135: step 9453, loss 0.67134.
Train: 2018-07-31T11:47:59.025939: step 9454, loss 0.625772.
Train: 2018-07-31T11:47:59.197775: step 9455, loss 0.607476.
Train: 2018-07-31T11:47:59.369640: step 9456, loss 0.516765.
Train: 2018-07-31T11:47:59.557094: step 9457, loss 0.55298.
Train: 2018-07-31T11:47:59.728901: step 9458, loss 0.58907.
Train: 2018-07-31T11:47:59.900765: step 9459, loss 0.561942.
Train: 2018-07-31T11:48:00.088221: step 9460, loss 0.552897.
Test: 2018-07-31T11:48:00.556831: step 9460, loss 0.555776.
Train: 2018-07-31T11:48:00.728667: step 9461, loss 0.642672.
Train: 2018-07-31T11:48:00.900500: step 9462, loss 0.543864.
Train: 2018-07-31T11:48:01.072336: step 9463, loss 0.633187.
Train: 2018-07-31T11:48:01.259792: step 9464, loss 0.534939.
Train: 2018-07-31T11:48:01.431627: step 9465, loss 0.614857.
Train: 2018-07-31T11:48:01.603460: step 9466, loss 0.526128.
Train: 2018-07-31T11:48:01.775296: step 9467, loss 0.579112.
Train: 2018-07-31T11:48:01.962753: step 9468, loss 0.53498.
Train: 2018-07-31T11:48:02.134617: step 9469, loss 0.596498.
Train: 2018-07-31T11:48:02.306421: step 9470, loss 0.650732.
Test: 2018-07-31T11:48:02.790702: step 9470, loss 0.555325.
Train: 2018-07-31T11:48:02.962518: step 9471, loss 0.535001.
Train: 2018-07-31T11:48:03.134353: step 9472, loss 0.569984.
Train: 2018-07-31T11:48:03.306217: step 9473, loss 0.54376.
Train: 2018-07-31T11:48:03.493674: step 9474, loss 0.552452.
Train: 2018-07-31T11:48:03.665478: step 9475, loss 0.543781.
Train: 2018-07-31T11:48:03.837338: step 9476, loss 0.639091.
Train: 2018-07-31T11:48:04.024768: step 9477, loss 0.587051.
Train: 2018-07-31T11:48:04.196604: step 9478, loss 0.638755.
Train: 2018-07-31T11:48:04.368439: step 9479, loss 0.526748.
Train: 2018-07-31T11:48:04.540274: step 9480, loss 0.586863.
Test: 2018-07-31T11:48:05.024565: step 9480, loss 0.555368.
Train: 2018-07-31T11:48:05.196399: step 9481, loss 0.484146.
Train: 2018-07-31T11:48:05.368234: step 9482, loss 0.603883.
Train: 2018-07-31T11:48:05.540039: step 9483, loss 0.655084.
Train: 2018-07-31T11:48:05.727495: step 9484, loss 0.509992.
Train: 2018-07-31T11:48:05.899360: step 9485, loss 0.569601.
Train: 2018-07-31T11:48:06.071178: step 9486, loss 0.595074.
Train: 2018-07-31T11:48:06.243030: step 9487, loss 0.578028.
Train: 2018-07-31T11:48:06.414867: step 9488, loss 0.527121.
Train: 2018-07-31T11:48:06.586699: step 9489, loss 0.637264.
Train: 2018-07-31T11:48:06.758528: step 9490, loss 0.603273.
Test: 2018-07-31T11:48:07.227174: step 9490, loss 0.555261.
Train: 2018-07-31T11:48:07.414601: step 9491, loss 0.57784.
Train: 2018-07-31T11:48:07.586435: step 9492, loss 0.611484.
Train: 2018-07-31T11:48:07.758300: step 9493, loss 0.63654.
Train: 2018-07-31T11:48:07.945727: step 9494, loss 0.552577.
Train: 2018-07-31T11:48:08.117561: step 9495, loss 0.585973.
Train: 2018-07-31T11:48:08.289395: step 9496, loss 0.577584.
Train: 2018-07-31T11:48:08.461230: step 9497, loss 0.577539.
Train: 2018-07-31T11:48:08.633065: step 9498, loss 0.627147.
Train: 2018-07-31T11:48:08.820552: step 9499, loss 0.569211.
Train: 2018-07-31T11:48:08.992387: step 9500, loss 0.511636.
Test: 2018-07-31T11:48:09.445373: step 9500, loss 0.555412.
Train: 2018-07-31T11:48:10.210850: step 9501, loss 0.585599.
Train: 2018-07-31T11:48:10.413928: step 9502, loss 0.544526.
Train: 2018-07-31T11:48:10.585763: step 9503, loss 0.552701.
Train: 2018-07-31T11:48:10.757567: step 9504, loss 0.618319.
Train: 2018-07-31T11:48:10.929401: step 9505, loss 0.569042.
Train: 2018-07-31T11:48:11.101237: step 9506, loss 0.544407.
Train: 2018-07-31T11:48:11.288694: step 9507, loss 0.593594.
Train: 2018-07-31T11:48:11.460553: step 9508, loss 0.65923.
Train: 2018-07-31T11:48:11.632393: step 9509, loss 0.462415.
Train: 2018-07-31T11:48:11.804227: step 9510, loss 0.601703.
Test: 2018-07-31T11:48:12.272868: step 9510, loss 0.555113.
Train: 2018-07-31T11:48:12.460318: step 9511, loss 0.59348.
Train: 2018-07-31T11:48:12.616537: step 9512, loss 0.462127.
Train: 2018-07-31T11:48:12.788372: step 9513, loss 0.53366.
Train: 2018-07-31T11:48:12.960177: step 9514, loss 0.510902.
Train: 2018-07-31T11:48:13.147663: step 9515, loss 0.585263.
Train: 2018-07-31T11:48:13.319497: step 9516, loss 0.618609.
Train: 2018-07-31T11:48:13.491301: step 9517, loss 0.568576.
Train: 2018-07-31T11:48:13.663136: step 9518, loss 0.58529.
Train: 2018-07-31T11:48:13.834972: step 9519, loss 0.49299.
Train: 2018-07-31T11:48:14.022427: step 9520, loss 0.517948.
Test: 2018-07-31T11:48:14.491099: step 9520, loss 0.554273.
Train: 2018-07-31T11:48:14.678554: step 9521, loss 0.60227.
Train: 2018-07-31T11:48:14.850360: step 9522, loss 0.551438.
Train: 2018-07-31T11:48:15.022193: step 9523, loss 0.517299.
Train: 2018-07-31T11:48:15.194028: step 9524, loss 0.576911.
Train: 2018-07-31T11:48:15.381484: step 9525, loss 0.585503.
Train: 2018-07-31T11:48:15.553319: step 9526, loss 0.576931.
Train: 2018-07-31T11:48:15.725184: step 9527, loss 0.559688.
Train: 2018-07-31T11:48:15.897019: step 9528, loss 0.490503.
Train: 2018-07-31T11:48:16.068825: step 9529, loss 0.53359.
Train: 2018-07-31T11:48:16.240689: step 9530, loss 0.585708.
Test: 2018-07-31T11:48:16.724952: step 9530, loss 0.553655.
Train: 2018-07-31T11:48:16.896785: step 9531, loss 0.577023.
Train: 2018-07-31T11:48:17.068619: step 9532, loss 0.673386.
Train: 2018-07-31T11:48:17.256045: step 9533, loss 0.524477.
Train: 2018-07-31T11:48:17.427880: step 9534, loss 0.559494.
Train: 2018-07-31T11:48:17.599745: step 9535, loss 0.533161.
Train: 2018-07-31T11:48:17.771580: step 9536, loss 0.594595.
Train: 2018-07-31T11:48:17.943384: step 9537, loss 0.5946.
Train: 2018-07-31T11:48:18.115219: step 9538, loss 0.559434.
Train: 2018-07-31T11:48:18.287084: step 9539, loss 0.550636.
Train: 2018-07-31T11:48:18.474540: step 9540, loss 0.612135.
Test: 2018-07-31T11:48:18.943151: step 9540, loss 0.553476.
Train: 2018-07-31T11:48:19.114984: step 9541, loss 0.58574.
Train: 2018-07-31T11:48:19.286852: step 9542, loss 0.612007.
Train: 2018-07-31T11:48:19.458654: step 9543, loss 0.57689.
Train: 2018-07-31T11:48:19.630520: step 9544, loss 0.585581.
Train: 2018-07-31T11:48:19.817953: step 9545, loss 0.498422.
Train: 2018-07-31T11:48:19.989780: step 9546, loss 0.559383.
Train: 2018-07-31T11:48:20.161614: step 9547, loss 0.568073.
Train: 2018-07-31T11:48:20.333481: step 9548, loss 0.533294.
Train: 2018-07-31T11:48:20.520907: step 9549, loss 0.585435.
Train: 2018-07-31T11:48:20.692741: step 9550, loss 0.594108.
Test: 2018-07-31T11:48:21.161413: step 9550, loss 0.553487.
Train: 2018-07-31T11:48:21.333246: step 9551, loss 0.515936.
Train: 2018-07-31T11:48:21.505081: step 9552, loss 0.576701.
Train: 2018-07-31T11:48:21.692531: step 9553, loss 0.57669.
Train: 2018-07-31T11:48:21.864365: step 9554, loss 0.515913.
Train: 2018-07-31T11:48:22.036177: step 9555, loss 0.524552.
Train: 2018-07-31T11:48:22.208011: step 9556, loss 0.585387.
Train: 2018-07-31T11:48:22.395497: step 9557, loss 0.628945.
Train: 2018-07-31T11:48:22.567332: step 9558, loss 0.498327.
Train: 2018-07-31T11:48:22.739136: step 9559, loss 0.620244.
Train: 2018-07-31T11:48:22.910971: step 9560, loss 0.524399.
Test: 2018-07-31T11:48:23.395233: step 9560, loss 0.553349.
Train: 2018-07-31T11:48:23.582689: step 9561, loss 0.646387.
Train: 2018-07-31T11:48:23.754554: step 9562, loss 0.533113.
Train: 2018-07-31T11:48:23.941979: step 9563, loss 0.61143.
Train: 2018-07-31T11:48:24.113846: step 9564, loss 0.507084.
Train: 2018-07-31T11:48:24.285679: step 9565, loss 0.602659.
Train: 2018-07-31T11:48:24.473136: step 9566, loss 0.541844.
Train: 2018-07-31T11:48:24.644970: step 9567, loss 0.611283.
Train: 2018-07-31T11:48:24.816775: step 9568, loss 0.59388.
Train: 2018-07-31T11:48:24.988609: step 9569, loss 0.524582.
Train: 2018-07-31T11:48:25.176065: step 9570, loss 0.49.
Test: 2018-07-31T11:48:25.644737: step 9570, loss 0.553342.
Train: 2018-07-31T11:48:25.816541: step 9571, loss 0.550526.
Train: 2018-07-31T11:48:26.003998: step 9572, loss 0.50715.
Train: 2018-07-31T11:48:26.175831: step 9573, loss 0.533076.
Train: 2018-07-31T11:48:26.347667: step 9574, loss 0.61141.
Train: 2018-07-31T11:48:26.519502: step 9575, loss 0.637656.
Train: 2018-07-31T11:48:26.691336: step 9576, loss 0.515472.
Train: 2018-07-31T11:48:26.863173: step 9577, loss 0.550357.
Train: 2018-07-31T11:48:27.035038: step 9578, loss 0.506623.
Train: 2018-07-31T11:48:27.206871: step 9579, loss 0.637909.
Train: 2018-07-31T11:48:27.441191: step 9580, loss 0.488941.
Test: 2018-07-31T11:48:27.925452: step 9580, loss 0.553114.
Train: 2018-07-31T11:48:28.097289: step 9581, loss 0.567819.
Train: 2018-07-31T11:48:28.269121: step 9582, loss 0.585407.
Train: 2018-07-31T11:48:28.440926: step 9583, loss 0.611825.
Train: 2018-07-31T11:48:28.612791: step 9584, loss 0.559009.
Train: 2018-07-31T11:48:28.784595: step 9585, loss 0.602995.
Train: 2018-07-31T11:48:28.956431: step 9586, loss 0.593867.
Train: 2018-07-31T11:48:29.128266: step 9587, loss 0.611659.
Train: 2018-07-31T11:48:29.315721: step 9588, loss 0.585276.
Train: 2018-07-31T11:48:29.487556: step 9589, loss 0.576488.
Train: 2018-07-31T11:48:29.659423: step 9590, loss 0.637431.
Test: 2018-07-31T11:48:30.143683: step 9590, loss 0.553212.
Train: 2018-07-31T11:48:30.362383: step 9591, loss 0.671858.
Train: 2018-07-31T11:48:30.534186: step 9592, loss 0.593616.
Train: 2018-07-31T11:48:30.721672: step 9593, loss 0.524854.
Train: 2018-07-31T11:48:30.893478: step 9594, loss 0.559204.
Train: 2018-07-31T11:48:31.065312: step 9595, loss 0.601785.
Train: 2018-07-31T11:48:31.252767: step 9596, loss 0.550813.
Train: 2018-07-31T11:48:31.424632: step 9597, loss 0.525536.
Train: 2018-07-31T11:48:31.596462: step 9598, loss 0.6015.
Train: 2018-07-31T11:48:31.768303: step 9599, loss 0.492083.
Train: 2018-07-31T11:48:31.940132: step 9600, loss 0.550964.
Test: 2018-07-31T11:48:32.424369: step 9600, loss 0.55369.
Train: 2018-07-31T11:48:33.189814: step 9601, loss 0.534136.
Train: 2018-07-31T11:48:33.361650: step 9602, loss 0.576181.
Train: 2018-07-31T11:48:33.533484: step 9603, loss 0.550903.
Train: 2018-07-31T11:48:33.720939: step 9604, loss 0.576174.
Train: 2018-07-31T11:48:33.892774: step 9605, loss 0.626825.
Train: 2018-07-31T11:48:34.064609: step 9606, loss 0.567719.
Train: 2018-07-31T11:48:34.252097: step 9607, loss 0.584585.
Train: 2018-07-31T11:48:34.423930: step 9608, loss 0.593001.
Train: 2018-07-31T11:48:34.595766: step 9609, loss 0.567695.
Train: 2018-07-31T11:48:34.767569: step 9610, loss 0.60978.
Test: 2018-07-31T11:48:35.251861: step 9610, loss 0.553607.
Train: 2018-07-31T11:48:35.439288: step 9611, loss 0.601266.
Train: 2018-07-31T11:48:35.611152: step 9612, loss 0.626415.
Train: 2018-07-31T11:48:35.782956: step 9613, loss 0.533379.
Train: 2018-07-31T11:48:35.970438: step 9614, loss 0.534369.
Train: 2018-07-31T11:48:36.142248: step 9615, loss 0.534471.
Train: 2018-07-31T11:48:36.314082: step 9616, loss 0.618023.
Train: 2018-07-31T11:48:36.485917: step 9617, loss 0.609778.
Train: 2018-07-31T11:48:36.673373: step 9618, loss 0.559896.
Train: 2018-07-31T11:48:36.845208: step 9619, loss 0.576684.
Train: 2018-07-31T11:48:37.017073: step 9620, loss 0.551876.
Test: 2018-07-31T11:48:37.501317: step 9620, loss 0.554705.
Train: 2018-07-31T11:48:37.673174: step 9621, loss 0.560311.
Train: 2018-07-31T11:48:37.844975: step 9622, loss 0.635207.
Train: 2018-07-31T11:48:38.016808: step 9623, loss 0.519032.
Train: 2018-07-31T11:48:38.188643: step 9624, loss 0.527402.
Train: 2018-07-31T11:48:38.360478: step 9625, loss 0.636322.
Train: 2018-07-31T11:48:38.547935: step 9626, loss 0.560735.
Train: 2018-07-31T11:48:38.719769: step 9627, loss 0.552555.
Train: 2018-07-31T11:48:38.891634: step 9628, loss 0.569331.
Train: 2018-07-31T11:48:39.063469: step 9629, loss 0.602785.
Train: 2018-07-31T11:48:39.235303: step 9630, loss 0.60296.
Test: 2018-07-31T11:48:39.719565: step 9630, loss 0.55591.
Train: 2018-07-31T11:48:39.891370: step 9631, loss 0.519935.
Train: 2018-07-31T11:48:40.063204: step 9632, loss 0.470094.
Train: 2018-07-31T11:48:40.250660: step 9633, loss 0.603494.
Train: 2018-07-31T11:48:40.422525: step 9634, loss 0.586925.
Train: 2018-07-31T11:48:40.609993: step 9635, loss 0.587029.
Train: 2018-07-31T11:48:40.781787: step 9636, loss 0.486381.
Train: 2018-07-31T11:48:40.953621: step 9637, loss 0.553503.
Train: 2018-07-31T11:48:41.125487: step 9638, loss 0.604139.
Train: 2018-07-31T11:48:41.297290: step 9639, loss 0.638076.
Train: 2018-07-31T11:48:41.484747: step 9640, loss 0.553379.
Test: 2018-07-31T11:48:41.969009: step 9640, loss 0.556088.
Train: 2018-07-31T11:48:42.140873: step 9641, loss 0.56181.
Train: 2018-07-31T11:48:42.328329: step 9642, loss 0.570245.
Train: 2018-07-31T11:48:42.500159: step 9643, loss 0.578688.
Train: 2018-07-31T11:48:42.671999: step 9644, loss 0.612656.
Train: 2018-07-31T11:48:42.843834: step 9645, loss 0.510528.
Train: 2018-07-31T11:48:43.015669: step 9646, loss 0.569985.
Train: 2018-07-31T11:48:43.187472: step 9647, loss 0.552858.
Train: 2018-07-31T11:48:43.359333: step 9648, loss 0.569826.
Train: 2018-07-31T11:48:43.531172: step 9649, loss 0.595385.
Train: 2018-07-31T11:48:43.703007: step 9650, loss 0.526896.
Test: 2018-07-31T11:48:44.171647: step 9650, loss 0.55523.
Train: 2018-07-31T11:48:44.343483: step 9651, loss 0.595271.
Train: 2018-07-31T11:48:44.530909: step 9652, loss 0.509484.
Train: 2018-07-31T11:48:44.702743: step 9653, loss 0.603765.
Train: 2018-07-31T11:48:44.874578: step 9654, loss 0.603719.
Train: 2018-07-31T11:48:45.046414: step 9655, loss 0.500435.
Train: 2018-07-31T11:48:45.218278: step 9656, loss 0.577774.
Train: 2018-07-31T11:48:45.405705: step 9657, loss 0.586332.
Train: 2018-07-31T11:48:45.577538: step 9658, loss 0.52584.
Train: 2018-07-31T11:48:45.749403: step 9659, loss 0.60351.
Train: 2018-07-31T11:48:45.921238: step 9660, loss 0.568852.
Test: 2018-07-31T11:48:46.389847: step 9660, loss 0.554283.
Train: 2018-07-31T11:48:46.577304: step 9661, loss 0.516814.
Train: 2018-07-31T11:48:46.749170: step 9662, loss 0.577381.
Train: 2018-07-31T11:48:46.921009: step 9663, loss 0.516571.
Train: 2018-07-31T11:48:47.092809: step 9664, loss 0.512924.
Train: 2018-07-31T11:48:47.264674: step 9665, loss 0.620867.
Train: 2018-07-31T11:48:47.452099: step 9666, loss 0.533534.
Train: 2018-07-31T11:48:47.623935: step 9667, loss 0.57718.
Train: 2018-07-31T11:48:47.795770: step 9668, loss 0.542072.
Train: 2018-07-31T11:48:47.967604: step 9669, loss 0.585717.
Train: 2018-07-31T11:48:48.139439: step 9670, loss 0.559503.
Test: 2018-07-31T11:48:48.623730: step 9670, loss 0.553516.
Train: 2018-07-31T11:48:48.795564: step 9671, loss 0.50665.
Train: 2018-07-31T11:48:48.967400: step 9672, loss 0.577044.
Train: 2018-07-31T11:48:49.139234: step 9673, loss 0.559046.
Train: 2018-07-31T11:48:49.311070: step 9674, loss 0.594648.
Train: 2018-07-31T11:48:49.482904: step 9675, loss 0.537363.
Train: 2018-07-31T11:48:49.670360: step 9676, loss 0.630714.
Train: 2018-07-31T11:48:49.842195: step 9677, loss 0.55012.
Train: 2018-07-31T11:48:50.013999: step 9678, loss 0.568343.
Train: 2018-07-31T11:48:50.185834: step 9679, loss 0.595232.
Train: 2018-07-31T11:48:50.373290: step 9680, loss 0.613302.
Test: 2018-07-31T11:48:50.841960: step 9680, loss 0.554688.
Train: 2018-07-31T11:48:51.029386: step 9681, loss 0.498821.
Train: 2018-07-31T11:48:51.201221: step 9682, loss 0.481611.
Train: 2018-07-31T11:48:51.373057: step 9683, loss 0.570724.
Train: 2018-07-31T11:48:51.560512: step 9684, loss 0.632673.
Train: 2018-07-31T11:48:51.732347: step 9685, loss 0.52675.
Train: 2018-07-31T11:48:51.919803: step 9686, loss 0.58896.
Train: 2018-07-31T11:48:52.091637: step 9687, loss 0.50953.
Train: 2018-07-31T11:48:52.279095: step 9688, loss 0.53623.
Train: 2018-07-31T11:48:52.450928: step 9689, loss 0.527459.
Train: 2018-07-31T11:48:52.622763: step 9690, loss 0.509714.
Test: 2018-07-31T11:48:53.107024: step 9690, loss 0.557105.
Train: 2018-07-31T11:48:53.278890: step 9691, loss 0.580948.
Train: 2018-07-31T11:48:53.450696: step 9692, loss 0.527435.
Train: 2018-07-31T11:48:53.638181: step 9693, loss 0.527369.
Train: 2018-07-31T11:48:53.809985: step 9694, loss 0.55423.
Train: 2018-07-31T11:48:53.981821: step 9695, loss 0.545189.
Train: 2018-07-31T11:48:54.153655: step 9696, loss 0.490889.
Train: 2018-07-31T11:48:54.341142: step 9697, loss 0.608541.
Train: 2018-07-31T11:48:54.512946: step 9698, loss 0.592946.
Train: 2018-07-31T11:48:54.684781: step 9699, loss 0.526629.
Train: 2018-07-31T11:48:54.872236: step 9700, loss 0.590541.
Test: 2018-07-31T11:48:55.356499: step 9700, loss 0.556985.
Train: 2018-07-31T11:48:56.121945: step 9701, loss 0.480764.
Train: 2018-07-31T11:48:56.309400: step 9702, loss 0.526498.
Train: 2018-07-31T11:48:56.481265: step 9703, loss 0.517237.
Train: 2018-07-31T11:48:56.653071: step 9704, loss 0.581839.
Train: 2018-07-31T11:48:56.840526: step 9705, loss 0.591194.
Train: 2018-07-31T11:48:57.012391: step 9706, loss 0.581967.
Train: 2018-07-31T11:48:57.184196: step 9707, loss 0.60055.
Train: 2018-07-31T11:48:57.371652: step 9708, loss 0.581912.
Train: 2018-07-31T11:48:57.543486: step 9709, loss 0.563247.
Train: 2018-07-31T11:48:57.715351: step 9710, loss 0.562687.
Test: 2018-07-31T11:48:58.199613: step 9710, loss 0.560602.
Train: 2018-07-31T11:48:58.371418: step 9711, loss 0.570073.
Train: 2018-07-31T11:48:58.543287: step 9712, loss 0.581058.
Train: 2018-07-31T11:48:58.730738: step 9713, loss 0.527446.
Train: 2018-07-31T11:48:58.918164: step 9714, loss 0.673536.
Train: 2018-07-31T11:48:59.089999: step 9715, loss 0.469826.
Train: 2018-07-31T11:48:59.277456: step 9716, loss 0.608443.
Train: 2018-07-31T11:48:59.449314: step 9717, loss 0.562363.
Train: 2018-07-31T11:48:59.621124: step 9718, loss 0.544386.
Train: 2018-07-31T11:48:59.808611: step 9719, loss 0.560139.
Train: 2018-07-31T11:48:59.980415: step 9720, loss 0.618309.
Test: 2018-07-31T11:49:00.464677: step 9720, loss 0.557026.
Train: 2018-07-31T11:49:00.652134: step 9721, loss 0.517283.
Train: 2018-07-31T11:49:00.839590: step 9722, loss 0.597201.
Train: 2018-07-31T11:49:01.027076: step 9723, loss 0.540079.
Train: 2018-07-31T11:49:01.198911: step 9724, loss 0.509843.
Train: 2018-07-31T11:49:01.370715: step 9725, loss 0.554957.
Train: 2018-07-31T11:49:01.558201: step 9726, loss 0.518652.
Train: 2018-07-31T11:49:01.730037: step 9727, loss 0.601518.
Train: 2018-07-31T11:49:01.901840: step 9728, loss 0.491848.
Train: 2018-07-31T11:49:02.089327: step 9729, loss 0.538672.
Train: 2018-07-31T11:49:02.276777: step 9730, loss 0.648623.
Test: 2018-07-31T11:49:02.745394: step 9730, loss 0.559952.
Train: 2018-07-31T11:49:02.932880: step 9731, loss 0.593714.
Train: 2018-07-31T11:49:03.135958: step 9732, loss 0.639772.
Train: 2018-07-31T11:49:03.307793: step 9733, loss 0.603102.
Train: 2018-07-31T11:49:03.479596: step 9734, loss 0.639568.
Train: 2018-07-31T11:49:03.651456: step 9735, loss 0.612005.
Train: 2018-07-31T11:49:03.838917: step 9736, loss 0.629836.
Train: 2018-07-31T11:49:04.026343: step 9737, loss 0.530478.
Train: 2018-07-31T11:49:04.198208: step 9738, loss 0.566335.
Train: 2018-07-31T11:49:04.385664: step 9739, loss 0.584025.
Train: 2018-07-31T11:49:04.573091: step 9740, loss 0.574905.
Test: 2018-07-31T11:49:05.057353: step 9740, loss 0.55987.
Train: 2018-07-31T11:49:05.229217: step 9741, loss 0.548146.
Train: 2018-07-31T11:49:05.416672: step 9742, loss 0.592098.
Train: 2018-07-31T11:49:05.588478: step 9743, loss 0.530264.
Train: 2018-07-31T11:49:05.775967: step 9744, loss 0.547667.
Train: 2018-07-31T11:49:05.963389: step 9745, loss 0.591266.
Train: 2018-07-31T11:49:06.135255: step 9746, loss 0.54728.
Train: 2018-07-31T11:49:06.322711: step 9747, loss 0.573277.
Train: 2018-07-31T11:49:06.510170: step 9748, loss 0.494543.
Train: 2018-07-31T11:49:06.682001: step 9749, loss 0.58156.
Train: 2018-07-31T11:49:06.869428: step 9750, loss 0.66864.
Test: 2018-07-31T11:49:07.338098: step 9750, loss 0.557816.
Train: 2018-07-31T11:49:07.525554: step 9751, loss 0.668244.
Train: 2018-07-31T11:49:07.697388: step 9752, loss 0.580876.
Train: 2018-07-31T11:49:07.884815: step 9753, loss 0.675773.
Train: 2018-07-31T11:49:08.072301: step 9754, loss 0.511602.
Train: 2018-07-31T11:49:08.244106: step 9755, loss 0.580196.
Train: 2018-07-31T11:49:08.431562: step 9756, loss 0.614152.
Train: 2018-07-31T11:49:08.619018: step 9757, loss 0.630829.
Train: 2018-07-31T11:49:08.790883: step 9758, loss 0.545778.
Train: 2018-07-31T11:49:08.978338: step 9759, loss 0.579457.
Train: 2018-07-31T11:49:09.150144: step 9760, loss 0.579302.
Test: 2018-07-31T11:49:09.634434: step 9760, loss 0.556767.
Train: 2018-07-31T11:49:09.821891: step 9761, loss 0.595899.
Train: 2018-07-31T11:49:09.993726: step 9762, loss 0.562337.
Train: 2018-07-31T11:49:10.181152: step 9763, loss 0.595534.
Train: 2018-07-31T11:49:10.352986: step 9764, loss 0.545601.
Train: 2018-07-31T11:49:10.540472: step 9765, loss 0.611786.
Train: 2018-07-31T11:49:10.712278: step 9766, loss 0.512503.
Train: 2018-07-31T11:49:10.895619: step 9767, loss 0.561969.
Train: 2018-07-31T11:49:11.083077: step 9768, loss 0.586634.
Train: 2018-07-31T11:49:11.254912: step 9769, loss 0.611285.
Train: 2018-07-31T11:49:11.442399: step 9770, loss 0.611168.
Test: 2018-07-31T11:49:11.911039: step 9770, loss 0.556153.
Train: 2018-07-31T11:49:12.098495: step 9771, loss 0.586369.
Train: 2018-07-31T11:49:12.285944: step 9772, loss 0.586285.
Train: 2018-07-31T11:49:12.457754: step 9773, loss 0.643558.
Train: 2018-07-31T11:49:12.645211: step 9774, loss 0.577955.
Train: 2018-07-31T11:49:12.817046: step 9775, loss 0.618629.
Train: 2018-07-31T11:49:13.004501: step 9776, loss 0.56162.
Train: 2018-07-31T11:49:13.176336: step 9777, loss 0.545422.
Train: 2018-07-31T11:49:13.363823: step 9778, loss 0.553509.
Train: 2018-07-31T11:49:13.551248: step 9779, loss 0.529235.
Train: 2018-07-31T11:49:13.723116: step 9780, loss 0.618099.
Test: 2018-07-31T11:49:14.191748: step 9780, loss 0.555998.
Train: 2018-07-31T11:49:14.379210: step 9781, loss 0.545284.
Train: 2018-07-31T11:49:14.551014: step 9782, loss 0.577575.
Train: 2018-07-31T11:49:14.738471: step 9783, loss 0.561338.
Train: 2018-07-31T11:49:14.925952: step 9784, loss 0.520748.
Train: 2018-07-31T11:49:15.097793: step 9785, loss 0.601812.
Train: 2018-07-31T11:49:15.285218: step 9786, loss 0.609951.
Train: 2018-07-31T11:49:15.457053: step 9787, loss 0.609939.
Train: 2018-07-31T11:49:15.628888: step 9788, loss 0.601758.
Train: 2018-07-31T11:49:15.816373: step 9789, loss 0.577285.
Train: 2018-07-31T11:49:15.988188: step 9790, loss 0.585393.
Test: 2018-07-31T11:49:16.472470: step 9790, loss 0.555459.
Train: 2018-07-31T11:49:16.644304: step 9791, loss 0.552813.
Train: 2018-07-31T11:49:16.831731: step 9792, loss 0.569052.
Train: 2018-07-31T11:49:17.003596: step 9793, loss 0.617872.
Train: 2018-07-31T11:49:17.191046: step 9794, loss 0.617825.
Train: 2018-07-31T11:49:17.362887: step 9795, loss 0.544593.
Train: 2018-07-31T11:49:17.550343: step 9796, loss 0.544577.
Train: 2018-07-31T11:49:17.722147: step 9797, loss 0.528269.
Train: 2018-07-31T11:49:17.909603: step 9798, loss 0.528152.
Train: 2018-07-31T11:49:18.081439: step 9799, loss 0.519814.
Train: 2018-07-31T11:49:18.253303: step 9800, loss 0.585154.
Test: 2018-07-31T11:49:18.753156: step 9800, loss 0.554921.
Train: 2018-07-31T11:49:19.518603: step 9801, loss 0.5687.
Train: 2018-07-31T11:49:19.706058: step 9802, loss 0.585156.
Train: 2018-07-31T11:49:19.877892: step 9803, loss 0.568603.
Train: 2018-07-31T11:49:20.049728: step 9804, loss 0.576859.
Train: 2018-07-31T11:49:20.237213: step 9805, loss 0.568523.
Train: 2018-07-31T11:49:20.409048: step 9806, loss 0.612716.
Train: 2018-07-31T11:49:20.596473: step 9807, loss 0.635233.
Train: 2018-07-31T11:49:20.768308: step 9808, loss 0.476683.
Train: 2018-07-31T11:49:20.955765: step 9809, loss 0.643709.
Train: 2018-07-31T11:49:21.143251: step 9810, loss 0.560118.
Test: 2018-07-31T11:49:21.611891: step 9810, loss 0.554485.
Train: 2018-07-31T11:49:21.799317: step 9811, loss 0.585229.
Train: 2018-07-31T11:49:21.986803: step 9812, loss 0.543411.
Train: 2018-07-31T11:49:22.158609: step 9813, loss 0.585281.
Train: 2018-07-31T11:49:22.346064: step 9814, loss 0.543405.
Train: 2018-07-31T11:49:22.517929: step 9815, loss 0.532757.
Train: 2018-07-31T11:49:22.689733: step 9816, loss 0.576958.
Train: 2018-07-31T11:49:22.877191: step 9817, loss 0.543292.
Train: 2018-07-31T11:49:23.049024: step 9818, loss 0.551668.
Train: 2018-07-31T11:49:23.236511: step 9819, loss 0.543158.
Train: 2018-07-31T11:49:23.408316: step 9820, loss 0.593962.
Test: 2018-07-31T11:49:23.892576: step 9820, loss 0.554269.
Train: 2018-07-31T11:49:24.064442: step 9821, loss 0.619492.
Train: 2018-07-31T11:49:24.251898: step 9822, loss 0.57699.
Train: 2018-07-31T11:49:24.423703: step 9823, loss 0.576975.
Train: 2018-07-31T11:49:24.611158: step 9824, loss 0.576957.
Train: 2018-07-31T11:49:24.783018: step 9825, loss 0.628009.
Train: 2018-07-31T11:49:24.954829: step 9826, loss 0.525887.
Train: 2018-07-31T11:49:25.142315: step 9827, loss 0.559871.
Train: 2018-07-31T11:49:25.314149: step 9828, loss 0.610862.
Train: 2018-07-31T11:49:25.501575: step 9829, loss 0.491844.
Train: 2018-07-31T11:49:25.673436: step 9830, loss 0.491731.
Test: 2018-07-31T11:49:26.157701: step 9830, loss 0.55397.
Train: 2018-07-31T11:49:26.329532: step 9831, loss 0.662073.
Train: 2018-07-31T11:49:26.516993: step 9832, loss 0.55116.
Train: 2018-07-31T11:49:26.688827: step 9833, loss 0.53398.
Train: 2018-07-31T11:49:26.876278: step 9834, loss 0.585281.
Train: 2018-07-31T11:49:27.048088: step 9835, loss 0.533953.
Train: 2018-07-31T11:49:27.235545: step 9836, loss 0.585357.
Train: 2018-07-31T11:49:27.407379: step 9837, loss 0.593988.
Train: 2018-07-31T11:49:27.594834: step 9838, loss 0.594032.
Train: 2018-07-31T11:49:27.766671: step 9839, loss 0.508226.
Train: 2018-07-31T11:49:27.938535: step 9840, loss 0.576925.
Test: 2018-07-31T11:49:28.422765: step 9840, loss 0.553946.
Train: 2018-07-31T11:49:28.594631: step 9841, loss 0.602737.
Train: 2018-07-31T11:49:28.782081: step 9842, loss 0.542536.
Train: 2018-07-31T11:49:28.953893: step 9843, loss 0.551056.
Train: 2018-07-31T11:49:29.141378: step 9844, loss 0.585585.
Train: 2018-07-31T11:49:29.313213: step 9845, loss 0.602179.
Train: 2018-07-31T11:49:29.500669: step 9846, loss 0.576667.
Train: 2018-07-31T11:49:29.688133: step 9847, loss 0.506482.
Train: 2018-07-31T11:49:29.859960: step 9848, loss 0.604927.
Train: 2018-07-31T11:49:30.031795: step 9849, loss 0.619722.
Train: 2018-07-31T11:49:30.219248: step 9850, loss 0.551904.
Test: 2018-07-31T11:49:30.703481: step 9850, loss 0.555353.
Train: 2018-07-31T11:49:30.875347: step 9851, loss 0.586944.
Train: 2018-07-31T11:49:31.062772: step 9852, loss 0.596351.
Train: 2018-07-31T11:49:31.234639: step 9853, loss 0.560331.
Train: 2018-07-31T11:49:31.422064: step 9854, loss 0.563813.
Train: 2018-07-31T11:49:31.609519: step 9855, loss 0.607468.
Train: 2018-07-31T11:49:31.781379: step 9856, loss 0.567853.
Train: 2018-07-31T11:49:31.968842: step 9857, loss 0.583136.
Train: 2018-07-31T11:49:32.140645: step 9858, loss 0.53244.
Train: 2018-07-31T11:49:32.328132: step 9859, loss 0.601201.
Train: 2018-07-31T11:49:32.499937: step 9860, loss 0.584468.
Test: 2018-07-31T11:49:32.984229: step 9860, loss 0.561897.
Train: 2018-07-31T11:49:33.171653: step 9861, loss 0.584712.
Train: 2018-07-31T11:49:33.343519: step 9862, loss 0.619916.
Train: 2018-07-31T11:49:33.530975: step 9863, loss 0.525307.
Train: 2018-07-31T11:49:33.702780: step 9864, loss 0.593365.
Train: 2018-07-31T11:49:33.890238: step 9865, loss 0.601791.
Train: 2018-07-31T11:49:34.062071: step 9866, loss 0.602081.
Train: 2018-07-31T11:49:34.249551: step 9867, loss 0.516744.
Train: 2018-07-31T11:49:34.421388: step 9868, loss 0.559163.
Train: 2018-07-31T11:49:34.593227: step 9869, loss 0.593135.
Train: 2018-07-31T11:49:34.780682: step 9870, loss 0.593531.
Test: 2018-07-31T11:49:35.249322: step 9870, loss 0.562029.
Train: 2018-07-31T11:49:35.421127: step 9871, loss 0.669619.
Train: 2018-07-31T11:49:35.608584: step 9872, loss 0.618672.
Train: 2018-07-31T11:49:35.780419: step 9873, loss 0.618624.
Train: 2018-07-31T11:49:35.967875: step 9874, loss 0.576446.
Train: 2018-07-31T11:49:36.155333: step 9875, loss 0.51772.
Train: 2018-07-31T11:49:36.327165: step 9876, loss 0.559659.
Train: 2018-07-31T11:49:36.499000: step 9877, loss 0.593081.
Train: 2018-07-31T11:49:36.686457: step 9878, loss 0.551096.
Train: 2018-07-31T11:49:36.858291: step 9879, loss 0.584389.
Train: 2018-07-31T11:49:37.045777: step 9880, loss 0.592544.
Test: 2018-07-31T11:49:37.514417: step 9880, loss 0.56157.
Train: 2018-07-31T11:49:37.701868: step 9881, loss 0.583939.
Train: 2018-07-31T11:49:37.873679: step 9882, loss 0.642197.
Train: 2018-07-31T11:49:38.061134: step 9883, loss 0.541679.
Train: 2018-07-31T11:49:38.232969: step 9884, loss 0.608138.
Train: 2018-07-31T11:49:38.404834: step 9885, loss 0.549519.
Train: 2018-07-31T11:49:38.576669: step 9886, loss 0.507619.
Train: 2018-07-31T11:49:38.764125: step 9887, loss 0.557261.
Train: 2018-07-31T11:49:38.935930: step 9888, loss 0.523543.
Train: 2018-07-31T11:49:39.107764: step 9889, loss 0.58174.
Train: 2018-07-31T11:49:39.295222: step 9890, loss 0.51433.
Test: 2018-07-31T11:49:39.763890: step 9890, loss 0.558722.
Train: 2018-07-31T11:49:39.951341: step 9891, loss 0.513853.
Train: 2018-07-31T11:49:40.138772: step 9892, loss 0.572583.
Train: 2018-07-31T11:49:40.310633: step 9893, loss 0.614857.
Train: 2018-07-31T11:49:40.498094: step 9894, loss 0.538031.
Train: 2018-07-31T11:49:40.669898: step 9895, loss 0.469272.
Train: 2018-07-31T11:49:40.841732: step 9896, loss 0.545949.
Train: 2018-07-31T11:49:41.029189: step 9897, loss 0.528353.
Train: 2018-07-31T11:49:41.201054: step 9898, loss 0.597516.
Train: 2018-07-31T11:49:41.372889: step 9899, loss 0.527631.
Train: 2018-07-31T11:49:41.560348: step 9900, loss 0.500977.
Test: 2018-07-31T11:49:42.028955: step 9900, loss 0.556292.
Train: 2018-07-31T11:49:42.778779: step 9901, loss 0.474009.
Train: 2018-07-31T11:49:42.966236: step 9902, loss 0.668719.
Train: 2018-07-31T11:49:43.138071: step 9903, loss 0.597666.
Train: 2018-07-31T11:49:43.309906: step 9904, loss 0.517149.
Train: 2018-07-31T11:49:43.481766: step 9905, loss 0.588701.
Train: 2018-07-31T11:49:43.669220: step 9906, loss 0.543682.
Train: 2018-07-31T11:49:43.856676: step 9907, loss 0.570613.
Train: 2018-07-31T11:49:44.028518: step 9908, loss 0.642872.
Train: 2018-07-31T11:49:44.200352: step 9909, loss 0.525261.
Train: 2018-07-31T11:49:44.387777: step 9910, loss 0.678998.
Test: 2018-07-31T11:49:44.872040: step 9910, loss 0.555181.
Train: 2018-07-31T11:49:45.043904: step 9911, loss 0.561277.
Train: 2018-07-31T11:49:45.231360: step 9912, loss 0.43491.
Train: 2018-07-31T11:49:45.403165: step 9913, loss 0.597266.
Train: 2018-07-31T11:49:45.574999: step 9914, loss 0.633339.
Train: 2018-07-31T11:49:45.762485: step 9915, loss 0.570045.
Train: 2018-07-31T11:49:45.949912: step 9916, loss 0.542954.
Train: 2018-07-31T11:49:46.137399: step 9917, loss 0.587908.
Train: 2018-07-31T11:49:46.309236: step 9918, loss 0.479999.
Train: 2018-07-31T11:49:46.481068: step 9919, loss 0.596762.
Train: 2018-07-31T11:49:46.668494: step 9920, loss 0.578733.
Test: 2018-07-31T11:49:47.137163: step 9920, loss 0.554668.
Train: 2018-07-31T11:49:47.324591: step 9921, loss 0.596619.
Train: 2018-07-31T11:49:47.512046: step 9922, loss 0.542759.
Train: 2018-07-31T11:49:47.683881: step 9923, loss 0.542735.
Train: 2018-07-31T11:49:47.855716: step 9924, loss 0.551651.
Train: 2018-07-31T11:49:48.043173: step 9925, loss 0.587382.
Train: 2018-07-31T11:49:48.215037: step 9926, loss 0.515852.
Train: 2018-07-31T11:49:48.402493: step 9927, loss 0.57835.
Train: 2018-07-31T11:49:48.574328: step 9928, loss 0.560447.
Train: 2018-07-31T11:49:48.746162: step 9929, loss 0.622909.
Train: 2018-07-31T11:49:48.933588: step 9930, loss 0.640608.
Test: 2018-07-31T11:49:49.402258: step 9930, loss 0.554348.
Train: 2018-07-31T11:49:49.589715: step 9931, loss 0.569234.
Train: 2018-07-31T11:49:49.761519: step 9932, loss 0.542591.
Train: 2018-07-31T11:49:49.949000: step 9933, loss 0.542601.
Train: 2018-07-31T11:49:50.120840: step 9934, loss 0.586748.
Train: 2018-07-31T11:49:50.292669: step 9935, loss 0.57786.
Train: 2018-07-31T11:49:50.480101: step 9936, loss 0.648154.
Train: 2018-07-31T11:49:50.651936: step 9937, loss 0.525144.
Train: 2018-07-31T11:49:50.839393: step 9938, loss 0.490245.
Train: 2018-07-31T11:49:50.995635: step 9939, loss 0.577625.
Train: 2018-07-31T11:49:51.183062: step 9940, loss 0.516488.
Test: 2018-07-31T11:49:51.651701: step 9940, loss 0.554212.
Train: 2018-07-31T11:49:51.839158: step 9941, loss 0.551373.
Train: 2018-07-31T11:49:52.011026: step 9942, loss 0.560075.
Train: 2018-07-31T11:49:52.198479: step 9943, loss 0.664917.
Train: 2018-07-31T11:49:52.370308: step 9944, loss 0.551305.
Train: 2018-07-31T11:49:52.557764: step 9945, loss 0.647162.
Train: 2018-07-31T11:49:52.729605: step 9946, loss 0.577384.
Train: 2018-07-31T11:49:52.901410: step 9947, loss 0.559995.
Train: 2018-07-31T11:49:53.088890: step 9948, loss 0.559987.
Train: 2018-07-31T11:49:53.260729: step 9949, loss 0.5945.
Train: 2018-07-31T11:49:53.432565: step 9950, loss 0.594414.
Test: 2018-07-31T11:49:53.916826: step 9950, loss 0.554179.
Train: 2018-07-31T11:49:54.088661: step 9951, loss 0.53279.
Train: 2018-07-31T11:49:54.276087: step 9952, loss 0.538344.
Train: 2018-07-31T11:49:54.447952: step 9953, loss 0.620213.
Train: 2018-07-31T11:49:54.619781: step 9954, loss 0.517831.
Train: 2018-07-31T11:49:54.807214: step 9955, loss 0.595191.
Train: 2018-07-31T11:49:54.979078: step 9956, loss 0.587066.
Train: 2018-07-31T11:49:55.166504: step 9957, loss 0.544834.
Train: 2018-07-31T11:49:55.338362: step 9958, loss 0.536726.
Train: 2018-07-31T11:49:55.525825: step 9959, loss 0.554128.
Train: 2018-07-31T11:49:55.697629: step 9960, loss 0.537349.
Test: 2018-07-31T11:49:56.181891: step 9960, loss 0.557413.
Train: 2018-07-31T11:49:56.353726: step 9961, loss 0.605908.
Train: 2018-07-31T11:49:56.525585: step 9962, loss 0.597561.
Train: 2018-07-31T11:49:56.697426: step 9963, loss 0.572043.
Train: 2018-07-31T11:49:56.869260: step 9964, loss 0.572129.
Train: 2018-07-31T11:49:57.056686: step 9965, loss 0.614919.
Train: 2018-07-31T11:49:57.212899: step 9966, loss 0.64507.
Train: 2018-07-31T11:49:57.384735: step 9967, loss 0.580692.
Train: 2018-07-31T11:49:57.572190: step 9968, loss 0.572134.
Train: 2018-07-31T11:49:57.744024: step 9969, loss 0.631462.
Train: 2018-07-31T11:49:57.915860: step 9970, loss 0.52126.
Test: 2018-07-31T11:49:58.400151: step 9970, loss 0.557772.
Train: 2018-07-31T11:49:58.571987: step 9971, loss 0.614133.
Train: 2018-07-31T11:49:58.743821: step 9972, loss 0.630784.
Train: 2018-07-31T11:49:58.915626: step 9973, loss 0.52972.
Train: 2018-07-31T11:49:59.087460: step 9974, loss 0.579961.
Train: 2018-07-31T11:49:59.259326: step 9975, loss 0.571448.
Train: 2018-07-31T11:49:59.431160: step 9976, loss 0.562955.
Train: 2018-07-31T11:49:59.618616: step 9977, loss 0.604536.
Train: 2018-07-31T11:49:59.790451: step 9978, loss 0.52941.
Train: 2018-07-31T11:49:59.977877: step 9979, loss 0.537548.
Train: 2018-07-31T11:50:00.165334: step 9980, loss 0.629041.
Test: 2018-07-31T11:50:00.649624: step 9980, loss 0.556613.
Train: 2018-07-31T11:50:00.837050: step 9981, loss 0.620532.
Train: 2018-07-31T11:50:01.024506: step 9982, loss 0.537166.
Train: 2018-07-31T11:50:01.196341: step 9983, loss 0.545359.
Train: 2018-07-31T11:50:01.383797: step 9984, loss 0.536904.
Train: 2018-07-31T11:50:01.571254: step 9985, loss 0.536731.
Train: 2018-07-31T11:50:01.743088: step 9986, loss 0.628276.
Train: 2018-07-31T11:50:01.930544: step 9987, loss 0.544742.
Train: 2018-07-31T11:50:02.102380: step 9988, loss 0.569661.
Train: 2018-07-31T11:50:02.274240: step 9989, loss 0.544452.
Train: 2018-07-31T11:50:02.446080: step 9990, loss 0.594582.
Test: 2018-07-31T11:50:02.930311: step 9990, loss 0.555286.
Train: 2018-07-31T11:50:03.102146: step 9991, loss 0.560948.
Train: 2018-07-31T11:50:03.289631: step 9992, loss 0.52723.
Train: 2018-07-31T11:50:03.461461: step 9993, loss 0.518616.
Train: 2018-07-31T11:50:03.633302: step 9994, loss 0.552148.
Train: 2018-07-31T11:50:03.820757: step 9995, loss 0.636766.
Train: 2018-07-31T11:50:03.992586: step 9996, loss 0.52642.
Train: 2018-07-31T11:50:04.180018: step 9997, loss 0.492197.
Train: 2018-07-31T11:50:04.351852: step 9998, loss 0.551629.
Train: 2018-07-31T11:50:04.523720: step 9999, loss 0.577223.
Train: 2018-07-31T11:50:04.695553: step 10000, loss 0.491144.
Test: 2018-07-31T11:50:05.179784: step 10000, loss 0.554053.
Train: 2018-07-31T11:50:05.960876: step 10001, loss 0.646324.
Train: 2018-07-31T11:50:06.132686: step 10002, loss 0.551149.
Train: 2018-07-31T11:50:06.304520: step 10003, loss 0.568431.
Train: 2018-07-31T11:50:06.476356: step 10004, loss 0.638027.
Train: 2018-07-31T11:50:06.663845: step 10005, loss 0.638011.
Train: 2018-07-31T11:50:06.835677: step 10006, loss 0.5074.
Train: 2018-07-31T11:50:07.007511: step 10007, loss 0.559549.
Train: 2018-07-31T11:50:07.179315: step 10008, loss 0.576913.
Train: 2018-07-31T11:50:07.351180: step 10009, loss 0.576875.
Train: 2018-07-31T11:50:07.538631: step 10010, loss 0.568134.
Test: 2018-07-31T11:50:08.007277: step 10010, loss 0.553532.
Train: 2018-07-31T11:50:08.194733: step 10011, loss 0.550703.
Train: 2018-07-31T11:50:08.366567: step 10012, loss 0.533273.
Train: 2018-07-31T11:50:08.538373: step 10013, loss 0.611557.
Train: 2018-07-31T11:50:08.710238: step 10014, loss 0.5245.
Train: 2018-07-31T11:50:08.882072: step 10015, loss 0.52445.
Train: 2018-07-31T11:50:09.053877: step 10016, loss 0.515654.
Train: 2018-07-31T11:50:09.241333: step 10017, loss 0.567937.
Train: 2018-07-31T11:50:09.413192: step 10018, loss 0.602929.
Train: 2018-07-31T11:50:09.600657: step 10019, loss 0.58542.
Train: 2018-07-31T11:50:09.772459: step 10020, loss 0.524066.
Test: 2018-07-31T11:50:10.241099: step 10020, loss 0.553171.
Train: 2018-07-31T11:50:10.428555: step 10021, loss 0.594177.
Train: 2018-07-31T11:50:10.647283: step 10022, loss 0.532743.
Train: 2018-07-31T11:50:10.819119: step 10023, loss 0.532695.
Train: 2018-07-31T11:50:10.990953: step 10024, loss 0.523837.
Train: 2018-07-31T11:50:11.178409: step 10025, loss 0.585433.
Train: 2018-07-31T11:50:11.350214: step 10026, loss 0.576622.
Train: 2018-07-31T11:50:11.522079: step 10027, loss 0.647285.
Train: 2018-07-31T11:50:11.709505: step 10028, loss 0.532458.
Train: 2018-07-31T11:50:11.881339: step 10029, loss 0.523625.
Train: 2018-07-31T11:50:12.053174: step 10030, loss 0.523588.
Test: 2018-07-31T11:50:12.537466: step 10030, loss 0.552916.
Train: 2018-07-31T11:50:12.709295: step 10031, loss 0.550041.
Train: 2018-07-31T11:50:12.896752: step 10032, loss 0.576562.
Train: 2018-07-31T11:50:13.068562: step 10033, loss 0.567702.
Train: 2018-07-31T11:50:13.240420: step 10034, loss 0.505646.
Train: 2018-07-31T11:50:13.412230: step 10035, loss 0.620959.
Train: 2018-07-31T11:50:13.599687: step 10036, loss 0.647601.
Train: 2018-07-31T11:50:13.771553: step 10037, loss 0.620851.
Train: 2018-07-31T11:50:13.943392: step 10038, loss 0.479178.
Train: 2018-07-31T11:50:14.115191: step 10039, loss 0.647134.
Train: 2018-07-31T11:50:14.302647: step 10040, loss 0.594016.
Test: 2018-07-31T11:50:14.771318: step 10040, loss 0.552812.
Train: 2018-07-31T11:50:14.943122: step 10041, loss 0.585119.
Train: 2018-07-31T11:50:15.130606: step 10042, loss 0.558742.
Train: 2018-07-31T11:50:15.302443: step 10043, loss 0.549998.
Train: 2018-07-31T11:50:15.489894: step 10044, loss 0.550013.
Train: 2018-07-31T11:50:15.661703: step 10045, loss 0.506478.
Train: 2018-07-31T11:50:15.833538: step 10046, loss 0.619667.
Train: 2018-07-31T11:50:16.005374: step 10047, loss 0.567411.
Train: 2018-07-31T11:50:16.192861: step 10048, loss 0.628174.
Train: 2018-07-31T11:50:16.364665: step 10049, loss 0.584698.
Train: 2018-07-31T11:50:16.536524: step 10050, loss 0.63646.
Test: 2018-07-31T11:50:17.020760: step 10050, loss 0.552935.
Train: 2018-07-31T11:50:17.192626: step 10051, loss 0.446895.
Train: 2018-07-31T11:50:17.380082: step 10052, loss 0.601712.
Train: 2018-07-31T11:50:17.551916: step 10053, loss 0.627388.
Train: 2018-07-31T11:50:17.723751: step 10054, loss 0.481737.
Train: 2018-07-31T11:50:17.895586: step 10055, loss 0.661371.
Train: 2018-07-31T11:50:18.083011: step 10056, loss 0.601411.
Train: 2018-07-31T11:50:18.254846: step 10057, loss 0.53327.
Train: 2018-07-31T11:50:18.442304: step 10058, loss 0.575767.
Train: 2018-07-31T11:50:18.614137: step 10059, loss 0.491032.
Train: 2018-07-31T11:50:18.785973: step 10060, loss 0.575735.
Test: 2018-07-31T11:50:19.270234: step 10060, loss 0.553068.
Train: 2018-07-31T11:50:19.442101: step 10061, loss 0.651959.
Train: 2018-07-31T11:50:19.613904: step 10062, loss 0.575704.
Train: 2018-07-31T11:50:19.801360: step 10063, loss 0.533489.
Train: 2018-07-31T11:50:19.988840: step 10064, loss 0.508212.
Train: 2018-07-31T11:50:20.160676: step 10065, loss 0.617854.
Train: 2018-07-31T11:50:20.332516: step 10066, loss 0.558784.
Train: 2018-07-31T11:50:20.504350: step 10067, loss 0.651539.
Train: 2018-07-31T11:50:20.691776: step 10068, loss 0.550373.
Train: 2018-07-31T11:50:20.863641: step 10069, loss 0.533577.
Train: 2018-07-31T11:50:21.051097: step 10070, loss 0.61762.
Test: 2018-07-31T11:50:21.519737: step 10070, loss 0.553135.
Train: 2018-07-31T11:50:21.707165: step 10071, loss 0.508437.
Train: 2018-07-31T11:50:21.879029: step 10072, loss 0.500014.
Train: 2018-07-31T11:50:22.066455: step 10073, loss 0.567164.
Train: 2018-07-31T11:50:22.238319: step 10074, loss 0.617703.
Train: 2018-07-31T11:50:22.410123: step 10075, loss 0.524984.
Train: 2018-07-31T11:50:22.597580: step 10076, loss 0.617781.
Train: 2018-07-31T11:50:22.769445: step 10077, loss 0.617787.
Train: 2018-07-31T11:50:22.941249: step 10078, loss 0.617742.
Train: 2018-07-31T11:50:23.128735: step 10079, loss 0.583954.
Train: 2018-07-31T11:50:23.300540: step 10080, loss 0.516644.
Test: 2018-07-31T11:50:23.769211: step 10080, loss 0.553017.
Train: 2018-07-31T11:50:23.956637: step 10081, loss 0.60072.
Train: 2018-07-31T11:50:24.128472: step 10082, loss 0.533502.
Train: 2018-07-31T11:50:24.300306: step 10083, loss 0.567084.
Train: 2018-07-31T11:50:24.472172: step 10084, loss 0.558678.
Train: 2018-07-31T11:50:24.659628: step 10085, loss 0.525067.
Train: 2018-07-31T11:50:24.847054: step 10086, loss 0.524997.
Train: 2018-07-31T11:50:25.018918: step 10087, loss 0.524887.
Train: 2018-07-31T11:50:25.190753: step 10088, loss 0.583924.
Train: 2018-07-31T11:50:25.362558: step 10089, loss 0.456827.
Train: 2018-07-31T11:50:25.534391: step 10090, loss 0.566981.
Test: 2018-07-31T11:50:26.034305: step 10090, loss 0.55265.
Train: 2018-07-31T11:50:26.206140: step 10091, loss 0.575517.
Train: 2018-07-31T11:50:26.377974: step 10092, loss 0.592697.
Train: 2018-07-31T11:50:26.565401: step 10093, loss 0.678778.
Train: 2018-07-31T11:50:26.737235: step 10094, loss 0.523933.
Train: 2018-07-31T11:50:26.909070: step 10095, loss 0.523887.
Train: 2018-07-31T11:50:27.080930: step 10096, loss 0.532433.
Train: 2018-07-31T11:50:27.268362: step 10097, loss 0.523708.
Train: 2018-07-31T11:50:27.440227: step 10098, loss 0.575592.
Train: 2018-07-31T11:50:27.612055: step 10099, loss 0.610363.
Train: 2018-07-31T11:50:27.783865: step 10100, loss 0.55822.
Test: 2018-07-31T11:50:28.268159: step 10100, loss 0.55233.
Train: 2018-07-31T11:50:29.064839: step 10101, loss 0.523374.
Train: 2018-07-31T11:50:29.236650: step 10102, loss 0.523293.
Train: 2018-07-31T11:50:29.408515: step 10103, loss 0.55817.
Train: 2018-07-31T11:50:29.595942: step 10104, loss 0.531856.
Train: 2018-07-31T11:50:29.767800: step 10105, loss 0.63724.
Train: 2018-07-31T11:50:29.939610: step 10106, loss 0.531735.
Train: 2018-07-31T11:50:30.111445: step 10107, loss 0.566924.
Train: 2018-07-31T11:50:30.298902: step 10108, loss 0.575743.
Train: 2018-07-31T11:50:30.470766: step 10109, loss 0.602221.
Train: 2018-07-31T11:50:30.642595: step 10110, loss 0.566912.
Test: 2018-07-31T11:50:31.111241: step 10110, loss 0.552131.
Train: 2018-07-31T11:50:31.329941: step 10111, loss 0.549262.
Train: 2018-07-31T11:50:31.501775: step 10112, loss 0.584534.
Train: 2018-07-31T11:50:31.689231: step 10113, loss 0.549255.
Train: 2018-07-31T11:50:31.861036: step 10114, loss 0.610937.
Train: 2018-07-31T11:50:32.032901: step 10115, loss 0.62847.
Train: 2018-07-31T11:50:32.220351: step 10116, loss 0.531722.
Train: 2018-07-31T11:50:32.376539: step 10117, loss 0.679015.
Train: 2018-07-31T11:50:32.564027: step 10118, loss 0.654111.
Train: 2018-07-31T11:50:32.751452: step 10119, loss 0.575458.
Train: 2018-07-31T11:50:32.923317: step 10120, loss 0.635879.
Test: 2018-07-31T11:50:33.407547: step 10120, loss 0.552359.
Train: 2018-07-31T11:50:33.579384: step 10121, loss 0.532389.
Train: 2018-07-31T11:50:33.751248: step 10122, loss 0.481264.
Train: 2018-07-31T11:50:33.938698: step 10123, loss 0.583785.
Train: 2018-07-31T11:50:34.110539: step 10124, loss 0.541217.
Train: 2018-07-31T11:50:34.282344: step 10125, loss 0.566726.
Train: 2018-07-31T11:50:34.454178: step 10126, loss 0.583677.
Train: 2018-07-31T11:50:34.641665: step 10127, loss 0.617497.
Train: 2018-07-31T11:50:34.813500: step 10128, loss 0.541399.
Train: 2018-07-31T11:50:34.985305: step 10129, loss 0.566722.
Train: 2018-07-31T11:50:35.172761: step 10130, loss 0.608798.
Test: 2018-07-31T11:50:35.641431: step 10130, loss 0.55266.
Train: 2018-07-31T11:50:35.813271: step 10131, loss 0.541531.
Train: 2018-07-31T11:50:36.000692: step 10132, loss 0.558336.
Train: 2018-07-31T11:50:36.172556: step 10133, loss 0.575101.
Train: 2018-07-31T11:50:36.344391: step 10134, loss 0.533225.
Train: 2018-07-31T11:50:36.531817: step 10135, loss 0.600209.
Train: 2018-07-31T11:50:36.703682: step 10136, loss 0.541596.
Train: 2018-07-31T11:50:36.875517: step 10137, loss 0.533211.
Train: 2018-07-31T11:50:37.047351: step 10138, loss 0.583449.
Train: 2018-07-31T11:50:37.234779: step 10139, loss 0.558291.
Train: 2018-07-31T11:50:37.406612: step 10140, loss 0.533088.
Test: 2018-07-31T11:50:37.890904: step 10140, loss 0.55257.
Train: 2018-07-31T11:50:38.078360: step 10141, loss 0.558244.
Train: 2018-07-31T11:50:38.250196: step 10142, loss 0.516096.
Train: 2018-07-31T11:50:38.437621: step 10143, loss 0.591963.
Train: 2018-07-31T11:50:38.609456: step 10144, loss 0.651254.
Train: 2018-07-31T11:50:38.781315: step 10145, loss 0.507347.
Train: 2018-07-31T11:50:38.953155: step 10146, loss 0.592022.
Train: 2018-07-31T11:50:39.140580: step 10147, loss 0.58355.
Train: 2018-07-31T11:50:39.312416: step 10148, loss 0.541116.
Train: 2018-07-31T11:50:39.484250: step 10149, loss 0.558073.
Train: 2018-07-31T11:50:39.671707: step 10150, loss 0.59207.
Test: 2018-07-31T11:50:40.140347: step 10150, loss 0.552304.
Train: 2018-07-31T11:50:40.312212: step 10151, loss 0.600583.
Train: 2018-07-31T11:50:40.499668: step 10152, loss 0.694125.
Train: 2018-07-31T11:50:40.671502: step 10153, loss 0.549587.
Train: 2018-07-31T11:50:40.858947: step 10154, loss 0.600392.
Train: 2018-07-31T11:50:41.030794: step 10155, loss 0.574988.
Train: 2018-07-31T11:50:41.202629: step 10156, loss 0.566554.
Train: 2018-07-31T11:50:41.374463: step 10157, loss 0.600151.
Train: 2018-07-31T11:50:41.561920: step 10158, loss 0.558188.
Train: 2018-07-31T11:50:41.733724: step 10159, loss 0.616725.
Train: 2018-07-31T11:50:41.905559: step 10160, loss 0.508233.
Test: 2018-07-31T11:50:42.389846: step 10160, loss 0.552638.
Train: 2018-07-31T11:50:42.561655: step 10161, loss 0.574908.
Train: 2018-07-31T11:50:42.749111: step 10162, loss 0.5749.
Train: 2018-07-31T11:50:42.936567: step 10163, loss 0.59151.
Train: 2018-07-31T11:50:43.108402: step 10164, loss 0.566587.
Train: 2018-07-31T11:50:43.280236: step 10165, loss 0.550008.
Train: 2018-07-31T11:50:43.452102: step 10166, loss 0.558298.
Train: 2018-07-31T11:50:43.639558: step 10167, loss 0.59144.
Train: 2018-07-31T11:50:43.811362: step 10168, loss 0.492018.
Train: 2018-07-31T11:50:43.983198: step 10169, loss 0.599746.
Train: 2018-07-31T11:50:44.170653: step 10170, loss 0.516726.
Test: 2018-07-31T11:50:44.639323: step 10170, loss 0.552593.
Train: 2018-07-31T11:50:44.826749: step 10171, loss 0.541567.
Train: 2018-07-31T11:50:44.998585: step 10172, loss 0.624893.
Train: 2018-07-31T11:50:45.170418: step 10173, loss 0.591543.
Train: 2018-07-31T11:50:45.357905: step 10174, loss 0.591548.
Train: 2018-07-31T11:50:45.529740: step 10175, loss 0.482927.
Train: 2018-07-31T11:50:45.701544: step 10176, loss 0.616694.
Train: 2018-07-31T11:50:45.889001: step 10177, loss 0.524555.
Train: 2018-07-31T11:50:46.060835: step 10178, loss 0.57483.
Train: 2018-07-31T11:50:46.248292: step 10179, loss 0.558016.
Train: 2018-07-31T11:50:46.420157: step 10180, loss 0.532726.
Test: 2018-07-31T11:50:46.888766: step 10180, loss 0.55226.
Train: 2018-07-31T11:50:47.076253: step 10181, loss 0.490429.
Train: 2018-07-31T11:50:47.248087: step 10182, loss 0.600271.
Train: 2018-07-31T11:50:47.419916: step 10183, loss 0.506891.
Train: 2018-07-31T11:50:47.591726: step 10184, loss 0.489595.
Train: 2018-07-31T11:50:47.779207: step 10185, loss 0.592061.
Train: 2018-07-31T11:50:47.951018: step 10186, loss 0.592159.
Train: 2018-07-31T11:50:48.122882: step 10187, loss 0.549073.
Train: 2018-07-31T11:50:48.310308: step 10188, loss 0.635607.
Train: 2018-07-31T11:50:48.482144: step 10189, loss 0.609683.
Train: 2018-07-31T11:50:48.654008: step 10190, loss 0.661713.
Test: 2018-07-31T11:50:49.138238: step 10190, loss 0.551825.
Train: 2018-07-31T11:50:49.310105: step 10191, loss 0.574975.
Train: 2018-07-31T11:50:49.497530: step 10192, loss 0.57495.
Train: 2018-07-31T11:50:49.669365: step 10193, loss 0.600791.
Train: 2018-07-31T11:50:49.841225: step 10194, loss 0.549095.
Train: 2018-07-31T11:50:50.013065: step 10195, loss 0.62638.
Train: 2018-07-31T11:50:50.184869: step 10196, loss 0.634755.
Train: 2018-07-31T11:50:50.356705: step 10197, loss 0.523666.
Train: 2018-07-31T11:50:50.544161: step 10198, loss 0.574783.
Train: 2018-07-31T11:50:50.731616: step 10199, loss 0.523898.
Train: 2018-07-31T11:50:50.903451: step 10200, loss 0.498561.
Test: 2018-07-31T11:50:51.372092: step 10200, loss 0.552096.
Train: 2018-07-31T11:50:52.153183: step 10201, loss 0.600146.
Train: 2018-07-31T11:50:52.324994: step 10202, loss 0.591663.
Train: 2018-07-31T11:50:52.512449: step 10203, loss 0.56627.
Train: 2018-07-31T11:50:52.684315: step 10204, loss 0.574717.
Train: 2018-07-31T11:50:52.856149: step 10205, loss 0.608482.
Train: 2018-07-31T11:50:53.027953: step 10206, loss 0.566265.
Train: 2018-07-31T11:50:53.215409: step 10207, loss 0.507328.
Train: 2018-07-31T11:50:53.387275: step 10208, loss 0.549416.
Train: 2018-07-31T11:50:53.574731: step 10209, loss 0.498837.
Train: 2018-07-31T11:50:53.746536: step 10210, loss 0.532458.
Test: 2018-07-31T11:50:54.215175: step 10210, loss 0.552042.
Train: 2018-07-31T11:50:54.387010: step 10211, loss 0.532352.
Train: 2018-07-31T11:50:54.558871: step 10212, loss 0.506742.
Train: 2018-07-31T11:50:54.730710: step 10213, loss 0.57473.
Train: 2018-07-31T11:50:54.918169: step 10214, loss 0.540502.
Train: 2018-07-31T11:50:55.089970: step 10215, loss 0.488826.
Train: 2018-07-31T11:50:55.261805: step 10216, loss 0.514352.
Train: 2018-07-31T11:50:55.449263: step 10217, loss 0.635687.
Train: 2018-07-31T11:50:55.621097: step 10218, loss 0.644653.
Train: 2018-07-31T11:50:55.792965: step 10219, loss 0.531281.
Train: 2018-07-31T11:50:55.980388: step 10220, loss 0.531216.
Test: 2018-07-31T11:50:56.449028: step 10220, loss 0.551526.
Train: 2018-07-31T11:50:56.636514: step 10221, loss 0.636345.
Train: 2018-07-31T11:50:56.808349: step 10222, loss 0.557434.
Train: 2018-07-31T11:50:56.980184: step 10223, loss 0.531094.
Train: 2018-07-31T11:50:57.167640: step 10224, loss 0.531056.
Train: 2018-07-31T11:50:57.339475: step 10225, loss 0.566211.
Train: 2018-07-31T11:50:57.511309: step 10226, loss 0.5574.
Train: 2018-07-31T11:50:57.698736: step 10227, loss 0.575042.
Train: 2018-07-31T11:50:57.870570: step 10228, loss 0.636873.
Train: 2018-07-31T11:50:58.042435: step 10229, loss 0.601506.
Train: 2018-07-31T11:50:58.229860: step 10230, loss 0.486893.
Test: 2018-07-31T11:50:58.698532: step 10230, loss 0.55143.
Train: 2018-07-31T11:50:58.870336: step 10231, loss 0.539753.
Train: 2018-07-31T11:50:59.057823: step 10232, loss 0.55737.
Train: 2018-07-31T11:50:59.229626: step 10233, loss 0.522086.
Train: 2018-07-31T11:50:59.417083: step 10234, loss 0.610335.
Train: 2018-07-31T11:50:59.588918: step 10235, loss 0.557352.
Train: 2018-07-31T11:50:59.760753: step 10236, loss 0.566177.
Train: 2018-07-31T11:50:59.932588: step 10237, loss 0.557342.
Train: 2018-07-31T11:51:00.120044: step 10238, loss 0.557337.
Train: 2018-07-31T11:51:00.291914: step 10239, loss 0.495525.
Train: 2018-07-31T11:51:00.463743: step 10240, loss 0.521961.
Test: 2018-07-31T11:51:00.932383: step 10240, loss 0.551341.
Train: 2018-07-31T11:51:01.119809: step 10241, loss 0.672469.
Train: 2018-07-31T11:51:01.291674: step 10242, loss 0.521902.
Train: 2018-07-31T11:51:01.479100: step 10243, loss 0.486468.
Train: 2018-07-31T11:51:01.650965: step 10244, loss 0.575041.
Train: 2018-07-31T11:51:01.822769: step 10245, loss 0.539537.
Train: 2018-07-31T11:51:01.994634: step 10246, loss 0.592858.
Train: 2018-07-31T11:51:02.182091: step 10247, loss 0.512801.
Train: 2018-07-31T11:51:02.353895: step 10248, loss 0.56619.
Train: 2018-07-31T11:51:02.525754: step 10249, loss 0.566195.
Train: 2018-07-31T11:51:02.713217: step 10250, loss 0.548346.
Test: 2018-07-31T11:51:03.197477: step 10250, loss 0.55124.
Train: 2018-07-31T11:51:03.369313: step 10251, loss 0.584068.
Train: 2018-07-31T11:51:03.541117: step 10252, loss 0.530457.
Train: 2018-07-31T11:51:03.728603: step 10253, loss 0.557259.
Train: 2018-07-31T11:51:03.900408: step 10254, loss 0.521465.
Train: 2018-07-31T11:51:04.072243: step 10255, loss 0.548293.
Train: 2018-07-31T11:51:04.244108: step 10256, loss 0.629011.
Train: 2018-07-31T11:51:04.431534: step 10257, loss 0.61105.
Train: 2018-07-31T11:51:04.603398: step 10258, loss 0.584105.
Train: 2018-07-31T11:51:04.775234: step 10259, loss 0.566172.
Train: 2018-07-31T11:51:04.962660: step 10260, loss 0.512604.
Test: 2018-07-31T11:51:05.431330: step 10260, loss 0.551203.
Train: 2018-07-31T11:51:05.603135: step 10261, loss 0.566139.
Train: 2018-07-31T11:51:05.790620: step 10262, loss 0.601769.
Train: 2018-07-31T11:51:05.962461: step 10263, loss 0.575004.
Train: 2018-07-31T11:51:06.134291: step 10264, loss 0.601608.
Train: 2018-07-31T11:51:06.306125: step 10265, loss 0.574922.
Train: 2018-07-31T11:51:06.477962: step 10266, loss 0.530701.
Train: 2018-07-31T11:51:06.665415: step 10267, loss 0.627761.
Train: 2018-07-31T11:51:06.821630: step 10268, loss 0.490965.
Train: 2018-07-31T11:51:06.993464: step 10269, loss 0.56599.
Train: 2018-07-31T11:51:07.165268: step 10270, loss 0.513351.
Test: 2018-07-31T11:51:07.633940: step 10270, loss 0.551286.
Train: 2018-07-31T11:51:07.821365: step 10271, loss 0.583513.
Train: 2018-07-31T11:51:07.993229: step 10272, loss 0.539673.
Train: 2018-07-31T11:51:08.165034: step 10273, loss 0.601015.
Train: 2018-07-31T11:51:08.336869: step 10274, loss 0.539686.
Train: 2018-07-31T11:51:08.524355: step 10275, loss 0.592202.
Train: 2018-07-31T11:51:08.696161: step 10276, loss 0.583424.
Train: 2018-07-31T11:51:08.883615: step 10277, loss 0.583391.
Train: 2018-07-31T11:51:09.055481: step 10278, loss 0.539764.
Train: 2018-07-31T11:51:09.227315: step 10279, loss 0.539783.
Train: 2018-07-31T11:51:09.414772: step 10280, loss 0.600721.
Test: 2018-07-31T11:51:09.883412: step 10280, loss 0.551333.
Train: 2018-07-31T11:51:10.070868: step 10281, loss 0.531117.
Train: 2018-07-31T11:51:10.242703: step 10282, loss 0.652795.
Train: 2018-07-31T11:51:10.430130: step 10283, loss 0.643919.
Train: 2018-07-31T11:51:10.601964: step 10284, loss 0.539948.
Train: 2018-07-31T11:51:10.773824: step 10285, loss 0.496938.
Train: 2018-07-31T11:51:10.961285: step 10286, loss 0.531432.
Train: 2018-07-31T11:51:11.133119: step 10287, loss 0.54004.
Train: 2018-07-31T11:51:11.320575: step 10288, loss 0.488399.
Train: 2018-07-31T11:51:11.492380: step 10289, loss 0.5486.
Train: 2018-07-31T11:51:11.664245: step 10290, loss 0.52265.
Test: 2018-07-31T11:51:12.132885: step 10290, loss 0.551336.
Train: 2018-07-31T11:51:12.320341: step 10291, loss 0.583163.
Train: 2018-07-31T11:51:12.492177: step 10292, loss 0.583198.
Train: 2018-07-31T11:51:12.663981: step 10293, loss 0.583222.
Train: 2018-07-31T11:51:12.835816: step 10294, loss 0.56584.
Train: 2018-07-31T11:51:13.023302: step 10295, loss 0.539728.
Train: 2018-07-31T11:51:13.195131: step 10296, loss 0.583262.
Train: 2018-07-31T11:51:13.366972: step 10297, loss 0.609419.
Train: 2018-07-31T11:51:13.554398: step 10298, loss 0.565829.
Train: 2018-07-31T11:51:13.726231: step 10299, loss 0.565823.
Train: 2018-07-31T11:51:13.898066: step 10300, loss 0.513596.
Test: 2018-07-31T11:51:14.382358: step 10300, loss 0.551231.
Train: 2018-07-31T11:51:15.116561: step 10301, loss 0.565814.
Train: 2018-07-31T11:51:15.288396: step 10302, loss 0.583234.
Train: 2018-07-31T11:51:15.475822: step 10303, loss 0.609361.
Train: 2018-07-31T11:51:15.647687: step 10304, loss 0.530993.
Train: 2018-07-31T11:51:15.819517: step 10305, loss 0.530997.
Train: 2018-07-31T11:51:16.006949: step 10306, loss 0.5919.
Train: 2018-07-31T11:51:16.178783: step 10307, loss 0.539685.
Train: 2018-07-31T11:51:16.350618: step 10308, loss 0.609297.
Train: 2018-07-31T11:51:16.538103: step 10309, loss 0.530992.
Train: 2018-07-31T11:51:16.709909: step 10310, loss 0.487508.
Test: 2018-07-31T11:51:17.194200: step 10310, loss 0.551187.
Train: 2018-07-31T11:51:17.366034: step 10311, loss 0.522226.
Train: 2018-07-31T11:51:17.553461: step 10312, loss 0.565778.
Train: 2018-07-31T11:51:17.725296: step 10313, loss 0.530801.
Train: 2018-07-31T11:51:17.897161: step 10314, loss 0.548258.
Train: 2018-07-31T11:51:18.068964: step 10315, loss 0.592156.
Train: 2018-07-31T11:51:18.240800: step 10316, loss 0.539404.
Train: 2018-07-31T11:51:18.412664: step 10317, loss 0.539365.
Train: 2018-07-31T11:51:18.600120: step 10318, loss 0.592311.
Train: 2018-07-31T11:51:18.771926: step 10319, loss 0.512774.
Train: 2018-07-31T11:51:18.943761: step 10320, loss 0.56582.
Test: 2018-07-31T11:51:19.428052: step 10320, loss 0.551221.
Train: 2018-07-31T11:51:19.599886: step 10321, loss 0.521721.
Train: 2018-07-31T11:51:19.787346: step 10322, loss 0.531156.
Train: 2018-07-31T11:51:19.959147: step 10323, loss 0.549794.
Train: 2018-07-31T11:51:20.131012: step 10324, loss 0.470405.
Train: 2018-07-31T11:51:20.318438: step 10325, loss 0.623629.
Train: 2018-07-31T11:51:20.490303: step 10326, loss 0.526023.
Train: 2018-07-31T11:51:20.662132: step 10327, loss 0.752307.
Train: 2018-07-31T11:51:20.849563: step 10328, loss 0.501076.
Train: 2018-07-31T11:51:21.021429: step 10329, loss 0.448006.
Train: 2018-07-31T11:51:21.208885: step 10330, loss 0.610919.
Test: 2018-07-31T11:51:21.677494: step 10330, loss 0.560447.
Train: 2018-07-31T11:51:21.849330: step 10331, loss 0.557511.
Train: 2018-07-31T11:51:22.036816: step 10332, loss 0.530993.
Train: 2018-07-31T11:51:22.208651: step 10333, loss 0.513334.
Train: 2018-07-31T11:51:22.380455: step 10334, loss 0.577117.
Train: 2018-07-31T11:51:22.567911: step 10335, loss 0.550174.
Train: 2018-07-31T11:51:22.739777: step 10336, loss 0.650534.
Train: 2018-07-31T11:51:22.927201: step 10337, loss 0.596571.
Train: 2018-07-31T11:51:23.099037: step 10338, loss 0.668905.
Train: 2018-07-31T11:51:23.286517: step 10339, loss 0.550623.
Train: 2018-07-31T11:51:23.458328: step 10340, loss 0.57781.
Test: 2018-07-31T11:51:23.942589: step 10340, loss 0.562567.
Train: 2018-07-31T11:51:24.114424: step 10341, loss 0.604848.
Train: 2018-07-31T11:51:24.286297: step 10342, loss 0.568555.
Train: 2018-07-31T11:51:24.458094: step 10343, loss 0.56841.
Train: 2018-07-31T11:51:24.629959: step 10344, loss 0.55926.
Train: 2018-07-31T11:51:24.817384: step 10345, loss 0.594939.
Train: 2018-07-31T11:51:24.989243: step 10346, loss 0.469461.
Train: 2018-07-31T11:51:25.161084: step 10347, loss 0.674933.
Train: 2018-07-31T11:51:25.348510: step 10348, loss 0.522775.
Train: 2018-07-31T11:51:25.520346: step 10349, loss 0.620634.
Train: 2018-07-31T11:51:25.692179: step 10350, loss 0.584706.
Test: 2018-07-31T11:51:26.176442: step 10350, loss 0.560689.
Train: 2018-07-31T11:51:26.348275: step 10351, loss 0.61993.
Train: 2018-07-31T11:51:26.520111: step 10352, loss 0.513349.
Train: 2018-07-31T11:51:26.691981: step 10353, loss 0.583878.
Train: 2018-07-31T11:51:26.879432: step 10354, loss 0.610073.
Train: 2018-07-31T11:51:27.051236: step 10355, loss 0.565758.
Train: 2018-07-31T11:51:27.238692: step 10356, loss 0.565542.
Train: 2018-07-31T11:51:27.410527: step 10357, loss 0.617904.
Train: 2018-07-31T11:51:27.582362: step 10358, loss 0.722428.
Train: 2018-07-31T11:51:27.754196: step 10359, loss 0.530186.
Train: 2018-07-31T11:51:27.926032: step 10360, loss 0.590741.
Test: 2018-07-31T11:51:28.410292: step 10360, loss 0.558791.
Train: 2018-07-31T11:51:28.582129: step 10361, loss 0.590467.
Train: 2018-07-31T11:51:28.753993: step 10362, loss 0.633124.
Train: 2018-07-31T11:51:28.925833: step 10363, loss 0.649725.
Train: 2018-07-31T11:51:29.113253: step 10364, loss 0.56419.
Train: 2018-07-31T11:51:29.285118: step 10365, loss 0.555629.
Train: 2018-07-31T11:51:29.456954: step 10366, loss 0.639632.
Train: 2018-07-31T11:51:29.628791: step 10367, loss 0.588954.
Train: 2018-07-31T11:51:29.816214: step 10368, loss 0.4972.
Train: 2018-07-31T11:51:29.988078: step 10369, loss 0.588556.
Train: 2018-07-31T11:51:30.159883: step 10370, loss 0.61321.
Test: 2018-07-31T11:51:30.628554: step 10370, loss 0.557888.
Train: 2018-07-31T11:51:30.815980: step 10371, loss 0.513952.
Train: 2018-07-31T11:51:30.987814: step 10372, loss 0.5139.
Train: 2018-07-31T11:51:31.175270: step 10373, loss 0.620881.
Train: 2018-07-31T11:51:31.347136: step 10374, loss 0.620719.
Train: 2018-07-31T11:51:31.518971: step 10375, loss 0.546519.
Train: 2018-07-31T11:51:31.706396: step 10376, loss 0.579285.
Train: 2018-07-31T11:51:31.878261: step 10377, loss 0.628443.
Train: 2018-07-31T11:51:32.050066: step 10378, loss 0.570844.
Train: 2018-07-31T11:51:32.237523: step 10379, loss 0.587121.
Train: 2018-07-31T11:51:32.409358: step 10380, loss 0.505194.
Test: 2018-07-31T11:51:32.878029: step 10380, loss 0.556814.
Train: 2018-07-31T11:51:33.049862: step 10381, loss 0.554152.
Train: 2018-07-31T11:51:33.237288: step 10382, loss 0.5786.
Train: 2018-07-31T11:51:33.424744: step 10383, loss 0.553883.
Train: 2018-07-31T11:51:33.596579: step 10384, loss 0.627694.
Train: 2018-07-31T11:51:33.768414: step 10385, loss 0.586513.
Train: 2018-07-31T11:51:33.940248: step 10386, loss 0.569978.
Train: 2018-07-31T11:51:34.127704: step 10387, loss 0.545214.
Train: 2018-07-31T11:51:34.299570: step 10388, loss 0.578014.
Train: 2018-07-31T11:51:34.486996: step 10389, loss 0.577926.
Train: 2018-07-31T11:51:34.658829: step 10390, loss 0.586087.
Test: 2018-07-31T11:51:35.143090: step 10390, loss 0.555686.
Train: 2018-07-31T11:51:35.314957: step 10391, loss 0.553002.
Train: 2018-07-31T11:51:35.502382: step 10392, loss 0.668542.
Train: 2018-07-31T11:51:35.674217: step 10393, loss 0.602352.
Train: 2018-07-31T11:51:35.861704: step 10394, loss 0.569281.
Train: 2018-07-31T11:51:36.033533: step 10395, loss 0.610365.
Train: 2018-07-31T11:51:36.205367: step 10396, loss 0.593804.
Train: 2018-07-31T11:51:36.377178: step 10397, loss 0.577305.
Train: 2018-07-31T11:51:36.564633: step 10398, loss 0.47902.
Train: 2018-07-31T11:51:36.736498: step 10399, loss 0.58536.
Train: 2018-07-31T11:51:36.908303: step 10400, loss 0.519737.
Test: 2018-07-31T11:51:37.392565: step 10400, loss 0.555075.
Train: 2018-07-31T11:51:38.142390: step 10401, loss 0.503146.
Train: 2018-07-31T11:51:38.329845: step 10402, loss 0.609918.
Train: 2018-07-31T11:51:38.501681: step 10403, loss 0.593419.
Train: 2018-07-31T11:51:38.689167: step 10404, loss 0.5355.
Train: 2018-07-31T11:51:38.861001: step 10405, loss 0.585079.
Train: 2018-07-31T11:51:39.032836: step 10406, loss 0.551818.
Train: 2018-07-31T11:51:39.220261: step 10407, loss 0.510085.
Train: 2018-07-31T11:51:39.392097: step 10408, loss 0.551574.
Train: 2018-07-31T11:51:39.579554: step 10409, loss 0.517904.
Train: 2018-07-31T11:51:39.751417: step 10410, loss 0.652348.
Test: 2018-07-31T11:51:40.220058: step 10410, loss 0.55394.
Train: 2018-07-31T11:51:40.391863: step 10411, loss 0.618717.
Train: 2018-07-31T11:51:40.579318: step 10412, loss 0.542675.
Train: 2018-07-31T11:51:40.751183: step 10413, loss 0.517197.
Train: 2018-07-31T11:51:40.923019: step 10414, loss 0.644262.
Train: 2018-07-31T11:51:41.094823: step 10415, loss 0.593332.
Train: 2018-07-31T11:51:41.282280: step 10416, loss 0.584756.
Train: 2018-07-31T11:51:41.454143: step 10417, loss 0.508357.
Train: 2018-07-31T11:51:41.625949: step 10418, loss 0.567781.
Train: 2018-07-31T11:51:41.797814: step 10419, loss 0.449479.
Train: 2018-07-31T11:51:41.985239: step 10420, loss 0.567739.
Test: 2018-07-31T11:51:42.453910: step 10420, loss 0.55355.
Train: 2018-07-31T11:51:42.641366: step 10421, loss 0.525025.
Train: 2018-07-31T11:51:42.813201: step 10422, loss 0.507885.
Train: 2018-07-31T11:51:43.000626: step 10423, loss 0.594527.
Train: 2018-07-31T11:51:43.188083: step 10424, loss 0.673273.
Train: 2018-07-31T11:51:43.359917: step 10425, loss 0.482208.
Train: 2018-07-31T11:51:43.547384: step 10426, loss 0.578396.
Train: 2018-07-31T11:51:43.719210: step 10427, loss 0.63129.
Train: 2018-07-31T11:51:43.906694: step 10428, loss 0.622874.
Train: 2018-07-31T11:51:44.078529: step 10429, loss 0.561778.
Train: 2018-07-31T11:51:44.265973: step 10430, loss 0.623295.
Test: 2018-07-31T11:51:44.734625: step 10430, loss 0.556251.
Train: 2018-07-31T11:51:44.906461: step 10431, loss 0.59714.
Train: 2018-07-31T11:51:45.093917: step 10432, loss 0.536091.
Train: 2018-07-31T11:51:45.265721: step 10433, loss 0.588538.
Train: 2018-07-31T11:51:45.453177: step 10434, loss 0.588559.
Train: 2018-07-31T11:51:45.625036: step 10435, loss 0.527674.
Train: 2018-07-31T11:51:45.812499: step 10436, loss 0.579832.
Train: 2018-07-31T11:51:45.999954: step 10437, loss 0.571108.
Train: 2018-07-31T11:51:46.171789: step 10438, loss 0.683836.
Train: 2018-07-31T11:51:46.343594: step 10439, loss 0.596926.
Train: 2018-07-31T11:51:46.531051: step 10440, loss 0.553652.
Test: 2018-07-31T11:51:46.999690: step 10440, loss 0.5564.
Train: 2018-07-31T11:51:47.187177: step 10441, loss 0.553604.
Train: 2018-07-31T11:51:47.358981: step 10442, loss 0.53638.
Train: 2018-07-31T11:51:47.530846: step 10443, loss 0.536313.
Train: 2018-07-31T11:51:47.702651: step 10444, loss 0.570481.
Train: 2018-07-31T11:51:47.874485: step 10445, loss 0.587496.
Train: 2018-07-31T11:51:48.061971: step 10446, loss 0.638724.
Train: 2018-07-31T11:51:48.233776: step 10447, loss 0.578679.
Train: 2018-07-31T11:51:48.405610: step 10448, loss 0.595601.
Train: 2018-07-31T11:51:48.593091: step 10449, loss 0.620952.
Train: 2018-07-31T11:51:48.764901: step 10450, loss 0.688536.
Test: 2018-07-31T11:51:49.233541: step 10450, loss 0.555581.
Train: 2018-07-31T11:51:49.421023: step 10451, loss 0.5444.
Train: 2018-07-31T11:51:49.592863: step 10452, loss 0.485597.
Train: 2018-07-31T11:51:49.780289: step 10453, loss 0.510811.
Train: 2018-07-31T11:51:49.952124: step 10454, loss 0.569419.
Train: 2018-07-31T11:51:50.123959: step 10455, loss 0.560936.
Train: 2018-07-31T11:51:50.311415: step 10456, loss 0.585994.
Train: 2018-07-31T11:51:50.483250: step 10457, loss 0.56913.
Train: 2018-07-31T11:51:50.655083: step 10458, loss 0.560655.
Train: 2018-07-31T11:51:50.826918: step 10459, loss 0.527012.
Train: 2018-07-31T11:51:51.014406: step 10460, loss 0.636054.
Test: 2018-07-31T11:51:51.498635: step 10460, loss 0.55472.
Train: 2018-07-31T11:51:51.670471: step 10461, loss 0.526796.
Train: 2018-07-31T11:51:51.857927: step 10462, loss 0.627535.
Train: 2018-07-31T11:51:52.029762: step 10463, loss 0.56863.
Train: 2018-07-31T11:51:52.201596: step 10464, loss 0.602156.
Train: 2018-07-31T11:51:52.373462: step 10465, loss 0.518147.
Train: 2018-07-31T11:51:52.560887: step 10466, loss 0.551638.
Train: 2018-07-31T11:51:52.748343: step 10467, loss 0.492742.
Train: 2018-07-31T11:51:52.920178: step 10468, loss 0.551444.
Train: 2018-07-31T11:51:53.107635: step 10469, loss 0.627337.
Train: 2018-07-31T11:51:53.279504: step 10470, loss 0.576615.
Test: 2018-07-31T11:51:53.763762: step 10470, loss 0.553926.
Train: 2018-07-31T11:51:53.935596: step 10471, loss 0.559637.
Train: 2018-07-31T11:51:54.123052: step 10472, loss 0.58499.
Train: 2018-07-31T11:51:54.294892: step 10473, loss 0.584949.
Train: 2018-07-31T11:51:54.466722: step 10474, loss 0.610349.
Train: 2018-07-31T11:51:54.638556: step 10475, loss 0.483117.
Train: 2018-07-31T11:51:54.825984: step 10476, loss 0.610285.
Train: 2018-07-31T11:51:54.997818: step 10477, loss 0.550809.
Train: 2018-07-31T11:51:55.169652: step 10478, loss 0.567747.
Train: 2018-07-31T11:51:55.357107: step 10479, loss 0.542188.
Train: 2018-07-31T11:51:55.528942: step 10480, loss 0.593201.
Test: 2018-07-31T11:51:56.013254: step 10480, loss 0.553345.
Train: 2018-07-31T11:51:56.185070: step 10481, loss 0.533535.
Train: 2018-07-31T11:51:56.356875: step 10482, loss 0.524918.
Train: 2018-07-31T11:51:56.528733: step 10483, loss 0.550433.
Train: 2018-07-31T11:51:56.716165: step 10484, loss 0.56749.
Train: 2018-07-31T11:51:56.887999: step 10485, loss 0.567452.
Train: 2018-07-31T11:51:57.059834: step 10486, loss 0.533016.
Train: 2018-07-31T11:51:57.247290: step 10487, loss 0.541522.
Train: 2018-07-31T11:51:57.419156: step 10488, loss 0.558708.
Train: 2018-07-31T11:51:57.590990: step 10489, loss 0.575984.
Train: 2018-07-31T11:51:57.778417: step 10490, loss 0.489165.
Test: 2018-07-31T11:51:58.262711: step 10490, loss 0.552683.
Train: 2018-07-31T11:51:58.434513: step 10491, loss 0.488883.
Train: 2018-07-31T11:51:58.606377: step 10492, loss 0.54101.
Train: 2018-07-31T11:51:58.778212: step 10493, loss 0.58482.
Train: 2018-07-31T11:51:58.950016: step 10494, loss 0.611327.
Train: 2018-07-31T11:51:59.137473: step 10495, loss 0.629081.
Train: 2018-07-31T11:51:59.309307: step 10496, loss 0.567195.
Train: 2018-07-31T11:51:59.481173: step 10497, loss 0.593707.
Train: 2018-07-31T11:51:59.653007: step 10498, loss 0.6202.
Train: 2018-07-31T11:51:59.824836: step 10499, loss 0.549439.
Train: 2018-07-31T11:51:59.996677: step 10500, loss 0.522959.
Test: 2018-07-31T11:52:00.480908: step 10500, loss 0.552267.
Train: 2018-07-31T11:52:01.230757: step 10501, loss 0.593495.
Train: 2018-07-31T11:52:01.402568: step 10502, loss 0.505324.
Train: 2018-07-31T11:52:01.590023: step 10503, loss 0.628683.
Train: 2018-07-31T11:52:01.761888: step 10504, loss 0.496509.
Train: 2018-07-31T11:52:01.933723: step 10505, loss 0.593351.
Train: 2018-07-31T11:52:02.121148: step 10506, loss 0.584511.
Train: 2018-07-31T11:52:02.293014: step 10507, loss 0.566873.
Train: 2018-07-31T11:52:02.464849: step 10508, loss 0.628379.
Train: 2018-07-31T11:52:02.636653: step 10509, loss 0.610673.
Train: 2018-07-31T11:52:02.808489: step 10510, loss 0.479298.
Test: 2018-07-31T11:52:03.292780: step 10510, loss 0.552111.
Train: 2018-07-31T11:52:03.480237: step 10511, loss 0.566748.
Train: 2018-07-31T11:52:03.652071: step 10512, loss 0.592918.
Train: 2018-07-31T11:52:03.823900: step 10513, loss 0.601574.
Train: 2018-07-31T11:52:03.995709: step 10514, loss 0.57537.
Train: 2018-07-31T11:52:04.167575: step 10515, loss 0.505857.
Train: 2018-07-31T11:52:04.355028: step 10516, loss 0.557942.
Train: 2018-07-31T11:52:04.526866: step 10517, loss 0.609961.
Train: 2018-07-31T11:52:04.698670: step 10518, loss 0.549254.
Train: 2018-07-31T11:52:04.870505: step 10519, loss 0.592511.
Train: 2018-07-31T11:52:05.042339: step 10520, loss 0.549253.
Test: 2018-07-31T11:52:05.526602: step 10520, loss 0.552057.
Train: 2018-07-31T11:52:05.698466: step 10521, loss 0.54925.
Train: 2018-07-31T11:52:05.885892: step 10522, loss 0.52336.
Train: 2018-07-31T11:52:06.057728: step 10523, loss 0.480177.
Train: 2018-07-31T11:52:06.229592: step 10524, loss 0.61835.
Train: 2018-07-31T11:52:06.401427: step 10525, loss 0.601065.
Train: 2018-07-31T11:52:06.573256: step 10526, loss 0.575084.
Train: 2018-07-31T11:52:06.760720: step 10527, loss 0.60967.
Train: 2018-07-31T11:52:06.932553: step 10528, loss 0.488622.
Train: 2018-07-31T11:52:07.119979: step 10529, loss 0.557733.
Train: 2018-07-31T11:52:07.291812: step 10530, loss 0.557714.
Test: 2018-07-31T11:52:07.776075: step 10530, loss 0.551852.
Train: 2018-07-31T11:52:07.947910: step 10531, loss 0.583663.
Train: 2018-07-31T11:52:08.119744: step 10532, loss 0.583652.
Train: 2018-07-31T11:52:08.291610: step 10533, loss 0.479754.
Train: 2018-07-31T11:52:08.479035: step 10534, loss 0.522959.
Train: 2018-07-31T11:52:08.650870: step 10535, loss 0.531544.
Train: 2018-07-31T11:52:08.822735: step 10536, loss 0.566293.
Train: 2018-07-31T11:52:08.994564: step 10537, loss 0.548825.
Train: 2018-07-31T11:52:09.181995: step 10538, loss 0.592539.
Train: 2018-07-31T11:52:09.353830: step 10539, loss 0.531219.
Train: 2018-07-31T11:52:09.525666: step 10540, loss 0.601399.
Test: 2018-07-31T11:52:10.009927: step 10540, loss 0.551545.
Train: 2018-07-31T11:52:10.181786: step 10541, loss 0.548687.
Train: 2018-07-31T11:52:10.353596: step 10542, loss 0.610252.
Train: 2018-07-31T11:52:10.541083: step 10543, loss 0.531051.
Train: 2018-07-31T11:52:10.759782: step 10544, loss 0.610252.
Train: 2018-07-31T11:52:10.947237: step 10545, loss 0.601416.
Train: 2018-07-31T11:52:11.119042: step 10546, loss 0.5662.
Train: 2018-07-31T11:52:11.290877: step 10547, loss 0.531073.
Train: 2018-07-31T11:52:11.473751: step 10548, loss 0.548621.
Train: 2018-07-31T11:52:11.645586: step 10549, loss 0.49599.
Train: 2018-07-31T11:52:11.817421: step 10550, loss 0.539807.
Test: 2018-07-31T11:52:12.301686: step 10550, loss 0.551418.
Train: 2018-07-31T11:52:12.473517: step 10551, loss 0.557351.
Train: 2018-07-31T11:52:12.660968: step 10552, loss 0.574943.
Train: 2018-07-31T11:52:12.832808: step 10553, loss 0.478019.
Train: 2018-07-31T11:52:13.020258: step 10554, loss 0.53964.
Train: 2018-07-31T11:52:13.192098: step 10555, loss 0.59271.
Train: 2018-07-31T11:52:13.363903: step 10556, loss 0.557277.
Train: 2018-07-31T11:52:13.535739: step 10557, loss 0.566147.
Train: 2018-07-31T11:52:13.723225: step 10558, loss 0.539468.
Train: 2018-07-31T11:52:13.910650: step 10559, loss 0.548338.
Train: 2018-07-31T11:52:14.082486: step 10560, loss 0.628577.
Test: 2018-07-31T11:52:14.551126: step 10560, loss 0.551205.
Train: 2018-07-31T11:52:14.738612: step 10561, loss 0.62856.
Train: 2018-07-31T11:52:14.910446: step 10562, loss 0.566117.
Train: 2018-07-31T11:52:15.082281: step 10563, loss 0.601658.
Train: 2018-07-31T11:52:15.254086: step 10564, loss 0.637039.
Train: 2018-07-31T11:52:15.425921: step 10565, loss 0.601392.
Train: 2018-07-31T11:52:15.597786: step 10566, loss 0.601215.
Train: 2018-07-31T11:52:15.785212: step 10567, loss 0.513362.
Train: 2018-07-31T11:52:15.957047: step 10568, loss 0.557197.
Train: 2018-07-31T11:52:16.128912: step 10569, loss 0.504903.
Train: 2018-07-31T11:52:16.300750: step 10570, loss 0.584474.
Test: 2018-07-31T11:52:16.784978: step 10570, loss 0.551332.
Train: 2018-07-31T11:52:16.972434: step 10571, loss 0.583272.
Train: 2018-07-31T11:52:17.144292: step 10572, loss 0.626604.
Train: 2018-07-31T11:52:17.316127: step 10573, loss 0.583159.
Train: 2018-07-31T11:52:17.503559: step 10574, loss 0.496854.
Train: 2018-07-31T11:52:17.675428: step 10575, loss 0.557223.
Train: 2018-07-31T11:52:17.847259: step 10576, loss 0.488398.
Train: 2018-07-31T11:52:18.019063: step 10577, loss 0.565818.
Train: 2018-07-31T11:52:18.206520: step 10578, loss 0.583036.
Train: 2018-07-31T11:52:18.378355: step 10579, loss 0.471045.
Train: 2018-07-31T11:52:18.550223: step 10580, loss 0.539901.
Test: 2018-07-31T11:52:19.034449: step 10580, loss 0.551302.
Train: 2018-07-31T11:52:19.206336: step 10581, loss 0.55714.
Train: 2018-07-31T11:52:19.378150: step 10582, loss 0.513762.
Train: 2018-07-31T11:52:19.549954: step 10583, loss 0.496211.
Train: 2018-07-31T11:52:19.737412: step 10584, loss 0.530867.
Train: 2018-07-31T11:52:19.909246: step 10585, loss 0.548269.
Train: 2018-07-31T11:52:20.081080: step 10586, loss 0.592237.
Train: 2018-07-31T11:52:20.252916: step 10587, loss 0.574665.
Train: 2018-07-31T11:52:20.424749: step 10588, loss 0.574695.
Train: 2018-07-31T11:52:20.596586: step 10589, loss 0.583586.
Train: 2018-07-31T11:52:20.768420: step 10590, loss 0.556973.
Test: 2018-07-31T11:52:21.252681: step 10590, loss 0.550968.
Train: 2018-07-31T11:52:21.424549: step 10591, loss 0.512528.
Train: 2018-07-31T11:52:21.596350: step 10592, loss 0.521343.
Train: 2018-07-31T11:52:21.768210: step 10593, loss 0.592646.
Train: 2018-07-31T11:52:21.955641: step 10594, loss 0.556944.
Train: 2018-07-31T11:52:22.111885: step 10595, loss 0.637463.
Train: 2018-07-31T11:52:22.299310: step 10596, loss 0.530101.
Train: 2018-07-31T11:52:22.471176: step 10597, loss 0.530092.
Train: 2018-07-31T11:52:22.643011: step 10598, loss 0.646407.
Train: 2018-07-31T11:52:22.830437: step 10599, loss 0.673104.
Train: 2018-07-31T11:52:23.017893: step 10600, loss 0.556901.
Test: 2018-07-31T11:52:23.486564: step 10600, loss 0.550903.
Train: 2018-07-31T11:52:24.236388: step 10601, loss 0.503607.
Train: 2018-07-31T11:52:24.408222: step 10602, loss 0.512557.
Train: 2018-07-31T11:52:24.580027: step 10603, loss 0.583468.
Train: 2018-07-31T11:52:24.767483: step 10604, loss 0.636545.
Train: 2018-07-31T11:52:24.939318: step 10605, loss 0.486238.
Train: 2018-07-31T11:52:25.111153: step 10606, loss 0.583341.
Train: 2018-07-31T11:52:25.283017: step 10607, loss 0.609742.
Train: 2018-07-31T11:52:25.470473: step 10608, loss 0.574456.
Train: 2018-07-31T11:52:25.642279: step 10609, loss 0.556869.
Train: 2018-07-31T11:52:25.814143: step 10610, loss 0.55687.
Test: 2018-07-31T11:52:26.298409: step 10610, loss 0.55097.
Train: 2018-07-31T11:52:26.470209: step 10611, loss 0.521893.
Train: 2018-07-31T11:52:26.642043: step 10612, loss 0.521913.
Train: 2018-07-31T11:52:26.813878: step 10613, loss 0.626781.
Train: 2018-07-31T11:52:26.985739: step 10614, loss 0.61797.
Train: 2018-07-31T11:52:27.173170: step 10615, loss 0.565576.
Train: 2018-07-31T11:52:27.345035: step 10616, loss 0.565563.
Train: 2018-07-31T11:52:27.516869: step 10617, loss 0.617616.
Train: 2018-07-31T11:52:27.688676: step 10618, loss 0.556886.
Train: 2018-07-31T11:52:27.876129: step 10619, loss 0.617323.
Train: 2018-07-31T11:52:28.047965: step 10620, loss 0.651562.
Test: 2018-07-31T11:52:28.516604: step 10620, loss 0.551172.
Train: 2018-07-31T11:52:28.688470: step 10621, loss 0.548387.
Train: 2018-07-31T11:52:28.860306: step 10622, loss 0.591102.
Train: 2018-07-31T11:52:29.047764: step 10623, loss 0.582505.
Train: 2018-07-31T11:52:29.219565: step 10624, loss 0.514745.
Train: 2018-07-31T11:52:29.391400: step 10625, loss 0.607728.
Train: 2018-07-31T11:52:29.563235: step 10626, loss 0.506614.
Train: 2018-07-31T11:52:29.750722: step 10627, loss 0.548718.
Train: 2018-07-31T11:52:29.922525: step 10628, loss 0.607515.
Train: 2018-07-31T11:52:30.094391: step 10629, loss 0.5236.
Train: 2018-07-31T11:52:30.266227: step 10630, loss 0.557141.
Test: 2018-07-31T11:52:30.734865: step 10630, loss 0.55148.
Train: 2018-07-31T11:52:30.922291: step 10631, loss 0.548753.
Train: 2018-07-31T11:52:31.094128: step 10632, loss 0.65776.
Train: 2018-07-31T11:52:31.265986: step 10633, loss 0.607383.
Train: 2018-07-31T11:52:31.453448: step 10634, loss 0.590588.
Train: 2018-07-31T11:52:31.625283: step 10635, loss 0.482162.
Train: 2018-07-31T11:52:31.812743: step 10636, loss 0.615537.
Train: 2018-07-31T11:52:31.984573: step 10637, loss 0.62381.
Train: 2018-07-31T11:52:32.156408: step 10638, loss 0.590461.
Train: 2018-07-31T11:52:32.328212: step 10639, loss 0.557257.
Train: 2018-07-31T11:52:32.500047: step 10640, loss 0.549004.
Test: 2018-07-31T11:52:32.984308: step 10640, loss 0.551713.
Train: 2018-07-31T11:52:33.203038: step 10641, loss 0.557291.
Train: 2018-07-31T11:52:33.374867: step 10642, loss 0.548596.
Train: 2018-07-31T11:52:33.546711: step 10643, loss 0.606866.
Train: 2018-07-31T11:52:33.734133: step 10644, loss 0.573815.
Train: 2018-07-31T11:52:33.905969: step 10645, loss 0.524305.
Train: 2018-07-31T11:52:34.077803: step 10646, loss 0.557302.
Train: 2018-07-31T11:52:34.265259: step 10647, loss 0.606866.
Train: 2018-07-31T11:52:34.437094: step 10648, loss 0.540761.
Train: 2018-07-31T11:52:34.608928: step 10649, loss 0.491119.
Train: 2018-07-31T11:52:34.796384: step 10650, loss 0.532355.
Test: 2018-07-31T11:52:35.265054: step 10650, loss 0.551575.
Train: 2018-07-31T11:52:35.436859: step 10651, loss 0.540554.
Train: 2018-07-31T11:52:35.624340: step 10652, loss 0.507065.
Train: 2018-07-31T11:52:35.796150: step 10653, loss 0.5487.
Train: 2018-07-31T11:52:35.968016: step 10654, loss 0.531766.
Train: 2018-07-31T11:52:36.139851: step 10655, loss 0.556983.
Train: 2018-07-31T11:52:36.311686: step 10656, loss 0.607958.
Train: 2018-07-31T11:52:36.499110: step 10657, loss 0.548382.
Train: 2018-07-31T11:52:36.670979: step 10658, loss 0.608246.
Train: 2018-07-31T11:52:36.842780: step 10659, loss 0.513984.
Train: 2018-07-31T11:52:37.014645: step 10660, loss 0.634281.
Test: 2018-07-31T11:52:37.498876: step 10660, loss 0.55103.
Train: 2018-07-31T11:52:37.670745: step 10661, loss 0.65158.
Train: 2018-07-31T11:52:37.858168: step 10662, loss 0.599904.
Train: 2018-07-31T11:52:38.030003: step 10663, loss 0.541287.
Train: 2018-07-31T11:52:38.217489: step 10664, loss 0.565537.
Train: 2018-07-31T11:52:38.389324: step 10665, loss 0.617172.
Train: 2018-07-31T11:52:38.576779: step 10666, loss 0.565826.
Train: 2018-07-31T11:52:38.764205: step 10667, loss 0.523222.
Train: 2018-07-31T11:52:38.936070: step 10668, loss 0.566215.
Train: 2018-07-31T11:52:39.107899: step 10669, loss 0.574966.
Train: 2018-07-31T11:52:39.295374: step 10670, loss 0.532414.
Test: 2018-07-31T11:52:39.764002: step 10670, loss 0.552459.
Train: 2018-07-31T11:52:39.935805: step 10671, loss 0.592429.
Train: 2018-07-31T11:52:40.107671: step 10672, loss 0.609674.
Train: 2018-07-31T11:52:40.295097: step 10673, loss 0.532916.
Train: 2018-07-31T11:52:40.466962: step 10674, loss 0.498892.
Train: 2018-07-31T11:52:40.638796: step 10675, loss 0.575814.
Train: 2018-07-31T11:52:40.810631: step 10676, loss 0.524561.
Train: 2018-07-31T11:52:40.998057: step 10677, loss 0.541668.
Train: 2018-07-31T11:52:41.169917: step 10678, loss 0.601748.
Train: 2018-07-31T11:52:41.341757: step 10679, loss 0.567404.
Train: 2018-07-31T11:52:41.513561: step 10680, loss 0.610418.
Test: 2018-07-31T11:52:41.982232: step 10680, loss 0.569301.
Train: 2018-07-31T11:52:42.185279: step 10681, loss 0.64524.
Train: 2018-07-31T11:52:42.357141: step 10682, loss 0.566666.
Train: 2018-07-31T11:52:42.544569: step 10683, loss 0.618478.
Train: 2018-07-31T11:52:42.716430: step 10684, loss 0.609885.
Train: 2018-07-31T11:52:42.888239: step 10685, loss 0.593163.
Train: 2018-07-31T11:52:43.075696: step 10686, loss 0.471974.
Train: 2018-07-31T11:52:43.247530: step 10687, loss 0.531578.
Train: 2018-07-31T11:52:43.434987: step 10688, loss 0.532084.
Train: 2018-07-31T11:52:43.622443: step 10689, loss 0.523265.
Train: 2018-07-31T11:52:43.794278: step 10690, loss 0.540556.
Test: 2018-07-31T11:52:44.278538: step 10690, loss 0.552319.
Train: 2018-07-31T11:52:44.466025: step 10691, loss 0.584659.
Train: 2018-07-31T11:52:44.637829: step 10692, loss 0.599689.
Train: 2018-07-31T11:52:44.825287: step 10693, loss 0.531215.
Train: 2018-07-31T11:52:44.997121: step 10694, loss 0.561493.
Train: 2018-07-31T11:52:45.184577: step 10695, loss 0.589034.
Train: 2018-07-31T11:52:45.356441: step 10696, loss 0.631801.
Train: 2018-07-31T11:52:45.543868: step 10697, loss 0.572049.
Train: 2018-07-31T11:52:45.715702: step 10698, loss 0.57647.
Train: 2018-07-31T11:52:45.903188: step 10699, loss 0.560134.
Train: 2018-07-31T11:52:46.090645: step 10700, loss 0.532201.
Test: 2018-07-31T11:52:46.574877: step 10700, loss 0.556451.
Train: 2018-07-31T11:52:47.371594: step 10701, loss 1.17409.
Train: 2018-07-31T11:52:47.559051: step 10702, loss 0.693507.
Train: 2018-07-31T11:52:47.746478: step 10703, loss 0.54295.
Train: 2018-07-31T11:52:47.918311: step 10704, loss 0.499161.
Train: 2018-07-31T11:52:48.090146: step 10705, loss 0.59507.
Train: 2018-07-31T11:52:48.277634: step 10706, loss 0.551485.
Train: 2018-07-31T11:52:48.465058: step 10707, loss 0.489066.
Train: 2018-07-31T11:52:48.636923: step 10708, loss 0.562457.
Train: 2018-07-31T11:52:48.824350: step 10709, loss 0.602251.
Train: 2018-07-31T11:52:48.996215: step 10710, loss 0.591668.
Test: 2018-07-31T11:52:49.480446: step 10710, loss 0.569533.
Train: 2018-07-31T11:52:49.667902: step 10711, loss 0.663499.
Train: 2018-07-31T11:52:49.839738: step 10712, loss 0.682325.
Train: 2018-07-31T11:52:50.027223: step 10713, loss 0.57805.
Train: 2018-07-31T11:52:50.214680: step 10714, loss 0.570414.
Train: 2018-07-31T11:52:50.402106: step 10715, loss 0.615135.
Train: 2018-07-31T11:52:50.573970: step 10716, loss 0.589845.
Train: 2018-07-31T11:52:50.761397: step 10717, loss 0.599312.
Train: 2018-07-31T11:52:50.948882: step 10718, loss 0.617272.
Train: 2018-07-31T11:52:51.136332: step 10719, loss 0.574436.
Train: 2018-07-31T11:52:51.308143: step 10720, loss 0.592072.
Test: 2018-07-31T11:52:51.776784: step 10720, loss 0.577836.
Train: 2018-07-31T11:52:51.948619: step 10721, loss 0.629032.
Train: 2018-07-31T11:52:52.136075: step 10722, loss 0.566547.
Train: 2018-07-31T11:52:52.307909: step 10723, loss 0.583702.
Train: 2018-07-31T11:52:52.495365: step 10724, loss 0.626405.
Train: 2018-07-31T11:52:52.682851: step 10725, loss 0.660305.
Train: 2018-07-31T11:52:52.870279: step 10726, loss 0.676819.
Train: 2018-07-31T11:52:53.042145: step 10727, loss 0.616633.
Train: 2018-07-31T11:52:53.229598: step 10728, loss 0.590736.
Train: 2018-07-31T11:52:53.401433: step 10729, loss 0.514547.
Train: 2018-07-31T11:52:53.588860: step 10730, loss 0.556159.
Test: 2018-07-31T11:52:54.057499: step 10730, loss 0.57515.
Train: 2018-07-31T11:52:54.244955: step 10731, loss 0.555648.
Train: 2018-07-31T11:52:54.432442: step 10732, loss 0.496374.
Train: 2018-07-31T11:52:54.604246: step 10733, loss 0.624247.
Train: 2018-07-31T11:52:54.791736: step 10734, loss 0.621126.
Train: 2018-07-31T11:52:54.979189: step 10735, loss 0.513961.
Train: 2018-07-31T11:52:55.151024: step 10736, loss 0.569482.
Train: 2018-07-31T11:52:55.338450: step 10737, loss 0.552003.
Train: 2018-07-31T11:52:55.510285: step 10738, loss 0.525955.
Train: 2018-07-31T11:52:55.697741: step 10739, loss 0.567796.
Train: 2018-07-31T11:52:55.885229: step 10740, loss 0.592894.
Test: 2018-07-31T11:52:56.353837: step 10740, loss 0.569529.
Train: 2018-07-31T11:52:56.525703: step 10741, loss 0.58389.
Train: 2018-07-31T11:52:56.713157: step 10742, loss 0.583448.
Train: 2018-07-31T11:52:56.884995: step 10743, loss 0.608893.
Train: 2018-07-31T11:52:57.072419: step 10744, loss 0.608528.
Train: 2018-07-31T11:52:57.244253: step 10745, loss 0.573556.
Train: 2018-07-31T11:52:57.431734: step 10746, loss 0.590475.
Train: 2018-07-31T11:52:57.603574: step 10747, loss 0.555439.
Train: 2018-07-31T11:52:57.791033: step 10748, loss 0.555042.
Train: 2018-07-31T11:52:57.978480: step 10749, loss 0.502507.
Train: 2018-07-31T11:52:58.165939: step 10750, loss 0.58038.
Test: 2018-07-31T11:52:58.634583: step 10750, loss 0.56542.
Train: 2018-07-31T11:52:58.822008: step 10751, loss 0.606266.
Train: 2018-07-31T11:52:59.009465: step 10752, loss 0.640999.
Train: 2018-07-31T11:52:59.181330: step 10753, loss 0.640695.
Train: 2018-07-31T11:52:59.368786: step 10754, loss 0.570363.
Train: 2018-07-31T11:52:59.556213: step 10755, loss 0.552598.
Train: 2018-07-31T11:52:59.728046: step 10756, loss 0.561051.
Train: 2018-07-31T11:52:59.899882: step 10757, loss 0.578235.
Train: 2018-07-31T11:53:00.102989: step 10758, loss 0.508128.
Train: 2018-07-31T11:53:00.274793: step 10759, loss 0.551488.
Train: 2018-07-31T11:53:00.462250: step 10760, loss 0.629977.
Test: 2018-07-31T11:53:00.946541: step 10760, loss 0.562555.
Train: 2018-07-31T11:53:01.118381: step 10761, loss 0.52469.
Train: 2018-07-31T11:53:01.305801: step 10762, loss 0.524399.
Train: 2018-07-31T11:53:01.493257: step 10763, loss 0.594328.
Train: 2018-07-31T11:53:01.680714: step 10764, loss 0.567758.
Train: 2018-07-31T11:53:01.852549: step 10765, loss 0.611557.
Train: 2018-07-31T11:53:02.040039: step 10766, loss 0.558524.
Train: 2018-07-31T11:53:02.211870: step 10767, loss 0.575938.
Train: 2018-07-31T11:53:02.399328: step 10768, loss 0.584558.
Train: 2018-07-31T11:53:02.586751: step 10769, loss 0.522675.
Train: 2018-07-31T11:53:02.774207: step 10770, loss 0.531271.
Test: 2018-07-31T11:53:03.242848: step 10770, loss 0.560413.
Train: 2018-07-31T11:53:03.414717: step 10771, loss 0.53987.
Train: 2018-07-31T11:53:03.602139: step 10772, loss 0.592757.
Train: 2018-07-31T11:53:03.773974: step 10773, loss 0.583759.
Train: 2018-07-31T11:53:03.945808: step 10774, loss 0.627966.
Train: 2018-07-31T11:53:04.148887: step 10775, loss 0.627781.
Train: 2018-07-31T11:53:04.336341: step 10776, loss 0.547859.
Train: 2018-07-31T11:53:04.508176: step 10777, loss 0.53889.
Train: 2018-07-31T11:53:04.695663: step 10778, loss 0.538759.
Train: 2018-07-31T11:53:04.883088: step 10779, loss 0.582821.
Train: 2018-07-31T11:53:05.054923: step 10780, loss 0.538485.
Test: 2018-07-31T11:53:05.539219: step 10780, loss 0.558907.
Train: 2018-07-31T11:53:05.711050: step 10781, loss 0.547187.
Train: 2018-07-31T11:53:05.882854: step 10782, loss 0.529345.
Train: 2018-07-31T11:53:06.070341: step 10783, loss 0.733018.
Train: 2018-07-31T11:53:06.257766: step 10784, loss 0.617577.
Train: 2018-07-31T11:53:06.445248: step 10785, loss 0.617319.
Train: 2018-07-31T11:53:06.617088: step 10786, loss 0.537923.
Train: 2018-07-31T11:53:06.804544: step 10787, loss 0.572958.
Train: 2018-07-31T11:53:06.991970: step 10788, loss 0.616566.
Train: 2018-07-31T11:53:07.179456: step 10789, loss 0.54657.
Train: 2018-07-31T11:53:07.366883: step 10790, loss 0.598703.
Test: 2018-07-31T11:53:07.835523: step 10790, loss 0.557983.
Train: 2018-07-31T11:53:08.022980: step 10791, loss 0.494455.
Train: 2018-07-31T11:53:08.194844: step 10792, loss 0.520426.
Train: 2018-07-31T11:53:08.382270: step 10793, loss 0.580987.
Train: 2018-07-31T11:53:08.554105: step 10794, loss 0.485586.
Train: 2018-07-31T11:53:08.741560: step 10795, loss 0.563473.
Train: 2018-07-31T11:53:08.913396: step 10796, loss 0.554689.
Train: 2018-07-31T11:53:09.100881: step 10797, loss 0.554584.
Train: 2018-07-31T11:53:09.288341: step 10798, loss 0.589374.
Train: 2018-07-31T11:53:09.460169: step 10799, loss 0.615525.
Train: 2018-07-31T11:53:09.647603: step 10800, loss 0.55431.
Test: 2018-07-31T11:53:10.131860: step 10800, loss 0.557073.
Train: 2018-07-31T11:53:10.944203: step 10801, loss 0.589185.
Train: 2018-07-31T11:53:11.162869: step 10802, loss 0.597849.
Train: 2018-07-31T11:53:11.350325: step 10803, loss 0.580294.
Train: 2018-07-31T11:53:11.522189: step 10804, loss 0.562765.
Train: 2018-07-31T11:53:11.693994: step 10805, loss 0.527829.
Train: 2018-07-31T11:53:11.881480: step 10806, loss 0.501597.
Train: 2018-07-31T11:53:12.068936: step 10807, loss 0.667352.
Train: 2018-07-31T11:53:12.240741: step 10808, loss 0.545047.
Train: 2018-07-31T11:53:12.428227: step 10809, loss 0.579889.
Train: 2018-07-31T11:53:12.600031: step 10810, loss 0.588544.
Test: 2018-07-31T11:53:13.084323: step 10810, loss 0.556442.
Train: 2018-07-31T11:53:13.271749: step 10811, loss 0.544893.
Train: 2018-07-31T11:53:13.459238: step 10812, loss 0.614541.
Train: 2018-07-31T11:53:13.631042: step 10813, loss 0.562212.
Train: 2018-07-31T11:53:13.818527: step 10814, loss 0.649101.
Train: 2018-07-31T11:53:13.990362: step 10815, loss 0.570791.
Train: 2018-07-31T11:53:14.177788: step 10816, loss 0.544774.
Train: 2018-07-31T11:53:14.349623: step 10817, loss 0.527483.
Train: 2018-07-31T11:53:14.537112: step 10818, loss 0.622422.
Train: 2018-07-31T11:53:14.708944: step 10819, loss 0.526991.
Train: 2018-07-31T11:53:14.896400: step 10820, loss 0.564412.
Test: 2018-07-31T11:53:15.365009: step 10820, loss 0.556156.
Train: 2018-07-31T11:53:15.552466: step 10821, loss 0.613633.
Train: 2018-07-31T11:53:15.724300: step 10822, loss 0.654249.
Train: 2018-07-31T11:53:15.911756: step 10823, loss 0.553689.
Train: 2018-07-31T11:53:16.099212: step 10824, loss 0.605739.
Train: 2018-07-31T11:53:16.286702: step 10825, loss 0.511483.
Train: 2018-07-31T11:53:16.458503: step 10826, loss 0.537809.
Train: 2018-07-31T11:53:16.645990: step 10827, loss 0.51243.
Train: 2018-07-31T11:53:16.817824: step 10828, loss 0.633932.
Train: 2018-07-31T11:53:17.005251: step 10829, loss 0.582535.
Train: 2018-07-31T11:53:17.177115: step 10830, loss 0.548313.
Test: 2018-07-31T11:53:17.645756: step 10830, loss 0.56016.
Train: 2018-07-31T11:53:17.833207: step 10831, loss 0.539982.
Train: 2018-07-31T11:53:18.020637: step 10832, loss 0.514154.
Train: 2018-07-31T11:53:18.208118: step 10833, loss 0.557818.
Train: 2018-07-31T11:53:18.379958: step 10834, loss 0.627877.
Train: 2018-07-31T11:53:18.567386: step 10835, loss 0.540555.
Train: 2018-07-31T11:53:18.754870: step 10836, loss 0.619469.
Train: 2018-07-31T11:53:18.926676: step 10837, loss 0.58443.
Train: 2018-07-31T11:53:19.114132: step 10838, loss 0.654611.
Train: 2018-07-31T11:53:19.301618: step 10839, loss 0.566814.
Train: 2018-07-31T11:53:19.473452: step 10840, loss 0.60174.
Test: 2018-07-31T11:53:19.957714: step 10840, loss 0.560735.
Train: 2018-07-31T11:53:20.145140: step 10841, loss 0.51421.
Train: 2018-07-31T11:53:20.317005: step 10842, loss 0.514103.
Train: 2018-07-31T11:53:20.504461: step 10843, loss 0.618796.
Train: 2018-07-31T11:53:20.676265: step 10844, loss 0.592416.
Train: 2018-07-31T11:53:20.863722: step 10845, loss 0.609705.
Train: 2018-07-31T11:53:21.035590: step 10846, loss 0.68797.
Train: 2018-07-31T11:53:21.223037: step 10847, loss 0.60919.
Train: 2018-07-31T11:53:21.394847: step 10848, loss 0.53965.
Train: 2018-07-31T11:53:21.582336: step 10849, loss 0.556848.
Train: 2018-07-31T11:53:21.769759: step 10850, loss 0.565343.
Test: 2018-07-31T11:53:22.238430: step 10850, loss 0.559415.
Train: 2018-07-31T11:53:22.425856: step 10851, loss 0.573803.
Train: 2018-07-31T11:53:22.597721: step 10852, loss 0.607965.
Train: 2018-07-31T11:53:22.785181: step 10853, loss 0.539298.
Train: 2018-07-31T11:53:22.957012: step 10854, loss 0.57339.
Train: 2018-07-31T11:53:23.144468: step 10855, loss 0.547658.
Train: 2018-07-31T11:53:23.316272: step 10856, loss 0.598724.
Train: 2018-07-31T11:53:23.503728: step 10857, loss 0.598576.
Train: 2018-07-31T11:53:23.675565: step 10858, loss 0.606931.
Train: 2018-07-31T11:53:23.863050: step 10859, loss 0.547298.
Train: 2018-07-31T11:53:24.034879: step 10860, loss 0.53873.
Test: 2018-07-31T11:53:24.534737: step 10860, loss 0.558347.
Train: 2018-07-31T11:53:24.706602: step 10861, loss 0.581032.
Train: 2018-07-31T11:53:24.878436: step 10862, loss 0.623308.
Train: 2018-07-31T11:53:25.065893: step 10863, loss 0.530003.
Train: 2018-07-31T11:53:25.237727: step 10864, loss 0.555301.
Train: 2018-07-31T11:53:25.425153: step 10865, loss 0.605988.
Train: 2018-07-31T11:53:25.612610: step 10866, loss 0.605871.
Train: 2018-07-31T11:53:25.784478: step 10867, loss 0.647999.
Train: 2018-07-31T11:53:25.971900: step 10868, loss 0.512823.
Train: 2018-07-31T11:53:26.143736: step 10869, loss 0.605438.
Train: 2018-07-31T11:53:26.315600: step 10870, loss 0.605301.
Test: 2018-07-31T11:53:26.784209: step 10870, loss 0.55752.
Train: 2018-07-31T11:53:26.971691: step 10871, loss 0.546397.
Train: 2018-07-31T11:53:27.143502: step 10872, loss 0.625153.
Train: 2018-07-31T11:53:27.315360: step 10873, loss 0.554686.
Train: 2018-07-31T11:53:27.502793: step 10874, loss 0.604763.
Train: 2018-07-31T11:53:27.674657: step 10875, loss 0.571273.
Train: 2018-07-31T11:53:27.862084: step 10876, loss 0.637816.
Train: 2018-07-31T11:53:28.033951: step 10877, loss 0.562842.
Train: 2018-07-31T11:53:28.221404: step 10878, loss 0.595946.
Train: 2018-07-31T11:53:28.393208: step 10879, loss 0.595835.
Train: 2018-07-31T11:53:28.580691: step 10880, loss 0.628714.
Test: 2018-07-31T11:53:29.064926: step 10880, loss 0.55718.
Train: 2018-07-31T11:53:29.252382: step 10881, loss 0.612045.
Train: 2018-07-31T11:53:29.424248: step 10882, loss 0.570917.
Train: 2018-07-31T11:53:29.611704: step 10883, loss 0.521906.
Train: 2018-07-31T11:53:29.799129: step 10884, loss 0.562689.
Train: 2018-07-31T11:53:29.986616: step 10885, loss 0.546358.
Train: 2018-07-31T11:53:30.158421: step 10886, loss 0.570733.
Train: 2018-07-31T11:53:30.330286: step 10887, loss 0.562528.
Train: 2018-07-31T11:53:30.517711: step 10888, loss 0.554308.
Train: 2018-07-31T11:53:30.689546: step 10889, loss 0.570547.
Train: 2018-07-31T11:53:30.861381: step 10890, loss 0.603177.
Test: 2018-07-31T11:53:31.330020: step 10890, loss 0.556725.
Train: 2018-07-31T11:53:31.517511: step 10891, loss 0.529526.
Train: 2018-07-31T11:53:31.689336: step 10892, loss 0.53758.
Train: 2018-07-31T11:53:31.876798: step 10893, loss 0.611367.
Train: 2018-07-31T11:53:32.064225: step 10894, loss 0.512621.
Train: 2018-07-31T11:53:32.236060: step 10895, loss 0.570156.
Train: 2018-07-31T11:53:32.423545: step 10896, loss 0.594922.
Train: 2018-07-31T11:53:32.595349: step 10897, loss 0.594917.
Train: 2018-07-31T11:53:32.767185: step 10898, loss 0.553362.
Train: 2018-07-31T11:53:32.954641: step 10899, loss 0.536628.
Train: 2018-07-31T11:53:33.142097: step 10900, loss 0.644983.
Test: 2018-07-31T11:53:33.610737: step 10900, loss 0.555836.
Train: 2018-07-31T11:53:34.360586: step 10901, loss 0.55312.
Train: 2018-07-31T11:53:34.532396: step 10902, loss 0.59487.
Train: 2018-07-31T11:53:34.719852: step 10903, loss 0.64506.
Train: 2018-07-31T11:53:34.907342: step 10904, loss 0.552987.
Train: 2018-07-31T11:53:35.079144: step 10905, loss 0.611467.
Train: 2018-07-31T11:53:35.266600: step 10906, loss 0.636439.
Train: 2018-07-31T11:53:35.454055: step 10907, loss 0.627934.
Train: 2018-07-31T11:53:35.625914: step 10908, loss 0.594523.
Train: 2018-07-31T11:53:35.797725: step 10909, loss 0.486862.
Train: 2018-07-31T11:53:35.969589: step 10910, loss 0.57785.
Test: 2018-07-31T11:53:36.453851: step 10910, loss 0.555727.
Train: 2018-07-31T11:53:36.625657: step 10911, loss 0.544782.
Train: 2018-07-31T11:53:36.813137: step 10912, loss 0.511718.
Train: 2018-07-31T11:53:36.984972: step 10913, loss 0.536405.
Train: 2018-07-31T11:53:37.172403: step 10914, loss 0.536285.
Train: 2018-07-31T11:53:37.359858: step 10915, loss 0.610955.
Train: 2018-07-31T11:53:37.531693: step 10916, loss 0.561022.
Train: 2018-07-31T11:53:37.703528: step 10917, loss 0.569309.
Train: 2018-07-31T11:53:37.891012: step 10918, loss 0.519108.
Train: 2018-07-31T11:53:38.062850: step 10919, loss 0.527303.
Train: 2018-07-31T11:53:38.250309: step 10920, loss 0.468197.
Test: 2018-07-31T11:53:38.718945: step 10920, loss 0.554964.
Train: 2018-07-31T11:53:38.906405: step 10921, loss 0.54375.
Train: 2018-07-31T11:53:39.078237: step 10922, loss 0.577607.
Train: 2018-07-31T11:53:39.250065: step 10923, loss 0.526311.
Train: 2018-07-31T11:53:39.421875: step 10924, loss 0.491677.
Train: 2018-07-31T11:53:39.609362: step 10925, loss 0.525772.
Train: 2018-07-31T11:53:39.781200: step 10926, loss 0.508072.
Train: 2018-07-31T11:53:39.953031: step 10927, loss 0.525186.
Train: 2018-07-31T11:53:40.140458: step 10928, loss 0.639662.
Train: 2018-07-31T11:53:40.327913: step 10929, loss 0.577918.
Train: 2018-07-31T11:53:40.499749: step 10930, loss 0.551249.
Test: 2018-07-31T11:53:40.984010: step 10930, loss 0.554102.
Train: 2018-07-31T11:53:41.155878: step 10931, loss 0.595881.
Train: 2018-07-31T11:53:41.343301: step 10932, loss 0.586994.
Train: 2018-07-31T11:53:41.515135: step 10933, loss 0.533164.
Train: 2018-07-31T11:53:41.686970: step 10934, loss 0.560076.
Train: 2018-07-31T11:53:41.858835: step 10935, loss 0.650176.
Train: 2018-07-31T11:53:42.030674: step 10936, loss 0.523995.
Train: 2018-07-31T11:53:42.218127: step 10937, loss 0.578049.
Train: 2018-07-31T11:53:42.389930: step 10938, loss 0.559998.
Train: 2018-07-31T11:53:42.577387: step 10939, loss 0.541945.
Train: 2018-07-31T11:53:42.764874: step 10940, loss 0.532901.
Test: 2018-07-31T11:53:43.233482: step 10940, loss 0.55385.
Train: 2018-07-31T11:53:43.420939: step 10941, loss 0.550915.
Train: 2018-07-31T11:53:43.592774: step 10942, loss 0.550891.
Train: 2018-07-31T11:53:43.780230: step 10943, loss 0.577985.
Train: 2018-07-31T11:53:43.952095: step 10944, loss 0.532762.
Train: 2018-07-31T11:53:44.139551: step 10945, loss 0.478434.
Train: 2018-07-31T11:53:44.311386: step 10946, loss 0.577997.
Train: 2018-07-31T11:53:44.483221: step 10947, loss 0.605254.
Train: 2018-07-31T11:53:44.655059: step 10948, loss 0.596171.
Train: 2018-07-31T11:53:44.842506: step 10949, loss 0.686941.
Train: 2018-07-31T11:53:45.014346: step 10950, loss 0.623167.
Test: 2018-07-31T11:53:45.482957: step 10950, loss 0.553665.
Train: 2018-07-31T11:53:45.670442: step 10951, loss 0.559752.
Train: 2018-07-31T11:53:45.842278: step 10952, loss 0.568714.
Train: 2018-07-31T11:53:46.029728: step 10953, loss 0.523868.
Train: 2018-07-31T11:53:46.201568: step 10954, loss 0.514997.
Train: 2018-07-31T11:53:46.388994: step 10955, loss 0.62214.
Train: 2018-07-31T11:53:46.560859: step 10956, loss 0.595262.
Train: 2018-07-31T11:53:46.732695: step 10957, loss 0.595145.
Train: 2018-07-31T11:53:46.920121: step 10958, loss 0.665813.
Train: 2018-07-31T11:53:47.091985: step 10959, loss 0.612446.
Train: 2018-07-31T11:53:47.263790: step 10960, loss 0.612159.
Test: 2018-07-31T11:53:47.732454: step 10960, loss 0.553753.
Train: 2018-07-31T11:53:47.919887: step 10961, loss 0.49869.
Train: 2018-07-31T11:53:48.091721: step 10962, loss 0.490273.
Train: 2018-07-31T11:53:48.263555: step 10963, loss 0.568284.
Train: 2018-07-31T11:53:48.451012: step 10964, loss 0.559631.
Train: 2018-07-31T11:53:48.622870: step 10965, loss 0.559627.
Train: 2018-07-31T11:53:48.794711: step 10966, loss 0.619893.
Train: 2018-07-31T11:53:48.982137: step 10967, loss 0.499488.
Train: 2018-07-31T11:53:49.154006: step 10968, loss 0.533858.
Train: 2018-07-31T11:53:49.325832: step 10969, loss 0.628295.
Train: 2018-07-31T11:53:49.497671: step 10970, loss 0.611058.
Test: 2018-07-31T11:53:49.981933: step 10970, loss 0.553818.
Train: 2018-07-31T11:53:50.153772: step 10971, loss 0.602402.
Train: 2018-07-31T11:53:50.325603: step 10972, loss 0.576685.
Train: 2018-07-31T11:53:50.497431: step 10973, loss 0.53405.
Train: 2018-07-31T11:53:50.669243: step 10974, loss 0.542593.
Train: 2018-07-31T11:53:50.841077: step 10975, loss 0.474566.
Train: 2018-07-31T11:53:51.012942: step 10976, loss 0.499963.
Train: 2018-07-31T11:53:51.184777: step 10977, loss 0.550995.
Train: 2018-07-31T11:53:51.372232: step 10978, loss 0.619443.
Train: 2018-07-31T11:53:51.544068: step 10979, loss 0.550891.
Train: 2018-07-31T11:53:51.715896: step 10980, loss 0.636764.
Test: 2018-07-31T11:53:52.184513: step 10980, loss 0.553628.
Train: 2018-07-31T11:53:52.356347: step 10981, loss 0.619573.
Train: 2018-07-31T11:53:52.543804: step 10982, loss 0.610925.
Train: 2018-07-31T11:53:52.715668: step 10983, loss 0.576561.
Train: 2018-07-31T11:53:52.887472: step 10984, loss 0.585087.
Train: 2018-07-31T11:53:53.074929: step 10985, loss 0.550895.
Train: 2018-07-31T11:53:53.246764: step 10986, loss 0.542382.
Train: 2018-07-31T11:53:53.418628: step 10987, loss 0.610544.
Train: 2018-07-31T11:53:53.590433: step 10988, loss 0.567936.
Train: 2018-07-31T11:53:53.762298: step 10989, loss 0.457476.
Train: 2018-07-31T11:53:53.934133: step 10990, loss 0.618951.
Test: 2018-07-31T11:53:54.418363: step 10990, loss 0.553652.
Train: 2018-07-31T11:53:54.590200: step 10991, loss 0.550885.
Train: 2018-07-31T11:53:54.762063: step 10992, loss 0.559375.
Train: 2018-07-31T11:53:54.933899: step 10993, loss 0.533808.
Train: 2018-07-31T11:53:55.105703: step 10994, loss 0.610497.
Train: 2018-07-31T11:53:55.293161: step 10995, loss 0.601968.
Train: 2018-07-31T11:53:55.465024: step 10996, loss 0.559309.
Train: 2018-07-31T11:53:55.636860: step 10997, loss 0.695705.
Train: 2018-07-31T11:53:55.824316: step 10998, loss 0.533811.
Train: 2018-07-31T11:53:55.996151: step 10999, loss 0.610243.
Train: 2018-07-31T11:53:56.167955: step 11000, loss 0.576271.
Test: 2018-07-31T11:53:56.652215: step 11000, loss 0.553655.
Train: 2018-07-31T11:53:57.448935: step 11001, loss 0.525568.
Train: 2018-07-31T11:53:57.620770: step 11002, loss 0.567793.
Train: 2018-07-31T11:53:57.808195: step 11003, loss 0.601503.
Train: 2018-07-31T11:53:57.980030: step 11004, loss 0.567781.
Train: 2018-07-31T11:53:58.151865: step 11005, loss 0.517336.
Train: 2018-07-31T11:53:58.323699: step 11006, loss 0.601392.
Train: 2018-07-31T11:53:58.495535: step 11007, loss 0.58456.
Train: 2018-07-31T11:53:58.683015: step 11008, loss 0.626519.
Train: 2018-07-31T11:53:58.854824: step 11009, loss 0.576127.
Train: 2018-07-31T11:53:59.026660: step 11010, loss 0.55938.
Test: 2018-07-31T11:53:59.510951: step 11010, loss 0.55375.
Train: 2018-07-31T11:53:59.682757: step 11011, loss 0.576097.
Train: 2018-07-31T11:53:59.870250: step 11012, loss 0.467602.
Train: 2018-07-31T11:54:00.042078: step 11013, loss 0.592786.
Train: 2018-07-31T11:54:00.213883: step 11014, loss 0.550985.
Train: 2018-07-31T11:54:00.401339: step 11015, loss 0.60954.
Train: 2018-07-31T11:54:00.573174: step 11016, loss 0.567679.
Train: 2018-07-31T11:54:00.745008: step 11017, loss 0.467177.
Train: 2018-07-31T11:54:00.916842: step 11018, loss 0.6516.
Train: 2018-07-31T11:54:01.088677: step 11019, loss 0.542429.
Train: 2018-07-31T11:54:01.260511: step 11020, loss 0.483511.
Test: 2018-07-31T11:54:01.744803: step 11020, loss 0.553467.
Train: 2018-07-31T11:54:01.916639: step 11021, loss 0.516983.
Train: 2018-07-31T11:54:02.088473: step 11022, loss 0.533702.
Train: 2018-07-31T11:54:02.260279: step 11023, loss 0.585691.
Train: 2018-07-31T11:54:02.432143: step 11024, loss 0.567536.
Train: 2018-07-31T11:54:02.619599: step 11025, loss 0.567524.
Train: 2018-07-31T11:54:02.791433: step 11026, loss 0.679081.
Train: 2018-07-31T11:54:02.963265: step 11027, loss 0.558921.
Train: 2018-07-31T11:54:03.135072: step 11028, loss 0.47304.
Train: 2018-07-31T11:54:03.322530: step 11029, loss 0.507244.
Train: 2018-07-31T11:54:03.494364: step 11030, loss 0.593383.
Test: 2018-07-31T11:54:03.963003: step 11030, loss 0.552982.
Train: 2018-07-31T11:54:04.150459: step 11031, loss 0.550167.
Train: 2018-07-31T11:54:04.306704: step 11032, loss 0.593497.
Train: 2018-07-31T11:54:04.494130: step 11033, loss 0.524022.
Train: 2018-07-31T11:54:04.665989: step 11034, loss 0.602299.
Train: 2018-07-31T11:54:04.822177: step 11035, loss 0.602338.
Train: 2018-07-31T11:54:05.009634: step 11036, loss 0.584898.
Train: 2018-07-31T11:54:05.181500: step 11037, loss 0.445341.
Train: 2018-07-31T11:54:05.353333: step 11038, loss 0.549961.
Train: 2018-07-31T11:54:05.525172: step 11039, loss 0.646296.
Train: 2018-07-31T11:54:05.697004: step 11040, loss 0.655086.
Test: 2018-07-31T11:54:06.181265: step 11040, loss 0.552765.
Train: 2018-07-31T11:54:06.353069: step 11041, loss 0.549918.
Train: 2018-07-31T11:54:06.540555: step 11042, loss 0.654836.
Train: 2018-07-31T11:54:06.712390: step 11043, loss 0.549952.
Train: 2018-07-31T11:54:06.884219: step 11044, loss 0.532575.
Train: 2018-07-31T11:54:07.071681: step 11045, loss 0.532611.
Train: 2018-07-31T11:54:07.243486: step 11046, loss 0.541294.
Train: 2018-07-31T11:54:07.430943: step 11047, loss 0.523931.
Train: 2018-07-31T11:54:07.602777: step 11048, loss 0.445757.
Train: 2018-07-31T11:54:07.774645: step 11049, loss 0.567464.
Train: 2018-07-31T11:54:07.946477: step 11050, loss 0.611307.
Test: 2018-07-31T11:54:08.430738: step 11050, loss 0.552888.
Train: 2018-07-31T11:54:08.602573: step 11051, loss 0.602538.
Train: 2018-07-31T11:54:08.774411: step 11052, loss 0.576369.
Train: 2018-07-31T11:54:08.961861: step 11053, loss 0.585201.
Train: 2018-07-31T11:54:09.133667: step 11054, loss 0.550254.
Train: 2018-07-31T11:54:09.305502: step 11055, loss 0.524025.
Train: 2018-07-31T11:54:09.492958: step 11056, loss 0.567753.
Train: 2018-07-31T11:54:09.664793: step 11057, loss 0.637505.
Train: 2018-07-31T11:54:09.836629: step 11058, loss 0.575369.
Train: 2018-07-31T11:54:10.024118: step 11059, loss 0.689374.
Train: 2018-07-31T11:54:10.195919: step 11060, loss 0.516698.
Test: 2018-07-31T11:54:10.680181: step 11060, loss 0.55354.
Train: 2018-07-31T11:54:10.852047: step 11061, loss 0.591882.
Train: 2018-07-31T11:54:11.086366: step 11062, loss 0.606578.
Train: 2018-07-31T11:54:11.258171: step 11063, loss 0.786303.
Train: 2018-07-31T11:54:11.445627: step 11064, loss 0.466449.
Train: 2018-07-31T11:54:11.617462: step 11065, loss 0.637505.
Train: 2018-07-31T11:54:11.804949: step 11066, loss 0.637667.
Train: 2018-07-31T11:54:11.976786: step 11067, loss 0.586334.
Train: 2018-07-31T11:54:12.164209: step 11068, loss 0.569777.
Train: 2018-07-31T11:54:12.336073: step 11069, loss 0.553278.
Train: 2018-07-31T11:54:12.507908: step 11070, loss 0.502886.
Test: 2018-07-31T11:54:12.992166: step 11070, loss 0.55675.
Train: 2018-07-31T11:54:13.163975: step 11071, loss 0.596134.
Train: 2018-07-31T11:54:13.351461: step 11072, loss 0.528398.
Train: 2018-07-31T11:54:13.523299: step 11073, loss 0.548912.
Train: 2018-07-31T11:54:13.710722: step 11074, loss 0.593873.
Train: 2018-07-31T11:54:13.882587: step 11075, loss 0.534241.
Train: 2018-07-31T11:54:14.070043: step 11076, loss 0.573774.
Train: 2018-07-31T11:54:14.257469: step 11077, loss 0.612746.
Train: 2018-07-31T11:54:14.429333: step 11078, loss 0.543106.
Train: 2018-07-31T11:54:14.616789: step 11079, loss 0.584788.
Train: 2018-07-31T11:54:14.804216: step 11080, loss 0.548259.
Test: 2018-07-31T11:54:15.272855: step 11080, loss 0.562741.
Train: 2018-07-31T11:54:15.460342: step 11081, loss 0.644031.
Train: 2018-07-31T11:54:15.647801: step 11082, loss 0.517494.
Train: 2018-07-31T11:54:15.819603: step 11083, loss 0.56047.
Train: 2018-07-31T11:54:16.007059: step 11084, loss 0.52654.
Train: 2018-07-31T11:54:16.194546: step 11085, loss 0.560504.
Train: 2018-07-31T11:54:16.366379: step 11086, loss 0.568176.
Train: 2018-07-31T11:54:16.553835: step 11087, loss 0.616603.
Train: 2018-07-31T11:54:16.741261: step 11088, loss 0.587887.
Train: 2018-07-31T11:54:16.913096: step 11089, loss 0.635396.
Train: 2018-07-31T11:54:17.100583: step 11090, loss 0.579744.
Test: 2018-07-31T11:54:17.584844: step 11090, loss 0.565272.
Train: 2018-07-31T11:54:17.772271: step 11091, loss 0.579956.
Train: 2018-07-31T11:54:17.959726: step 11092, loss 0.559993.
Train: 2018-07-31T11:54:18.131562: step 11093, loss 0.555242.
Train: 2018-07-31T11:54:18.319017: step 11094, loss 0.667768.
Train: 2018-07-31T11:54:18.490876: step 11095, loss 0.625004.
Train: 2018-07-31T11:54:18.678338: step 11096, loss 0.533992.
Train: 2018-07-31T11:54:18.865764: step 11097, loss 0.596012.
Train: 2018-07-31T11:54:19.053220: step 11098, loss 0.657649.
Train: 2018-07-31T11:54:19.225085: step 11099, loss 0.649543.
Train: 2018-07-31T11:54:19.412537: step 11100, loss 0.595742.
Test: 2018-07-31T11:54:19.896773: step 11100, loss 0.575102.
Train: 2018-07-31T11:54:20.693462: step 11101, loss 0.538054.
Train: 2018-07-31T11:54:20.865320: step 11102, loss 0.581235.
Train: 2018-07-31T11:54:21.052782: step 11103, loss 0.565584.
Train: 2018-07-31T11:54:21.224617: step 11104, loss 0.630254.
Train: 2018-07-31T11:54:21.412073: step 11105, loss 0.608666.
Train: 2018-07-31T11:54:21.599499: step 11106, loss 0.590596.
Train: 2018-07-31T11:54:21.771333: step 11107, loss 0.591945.
Train: 2018-07-31T11:54:21.958790: step 11108, loss 0.662744.
Train: 2018-07-31T11:54:22.146280: step 11109, loss 0.570323.
Train: 2018-07-31T11:54:22.318081: step 11110, loss 0.614484.
Test: 2018-07-31T11:54:22.802343: step 11110, loss 0.574496.
Train: 2018-07-31T11:54:22.989823: step 11111, loss 0.665062.
Train: 2018-07-31T11:54:23.177285: step 11112, loss 0.580962.
Train: 2018-07-31T11:54:23.364711: step 11113, loss 0.661018.
Train: 2018-07-31T11:54:23.552166: step 11114, loss 0.512489.
Train: 2018-07-31T11:54:23.724002: step 11115, loss 0.652524.
Train: 2018-07-31T11:54:23.911488: step 11116, loss 0.560095.
Train: 2018-07-31T11:54:24.098914: step 11117, loss 0.646749.
Train: 2018-07-31T11:54:24.270782: step 11118, loss 0.562595.
Train: 2018-07-31T11:54:24.458235: step 11119, loss 0.580208.
Train: 2018-07-31T11:54:24.645660: step 11120, loss 0.589834.
Test: 2018-07-31T11:54:25.114301: step 11120, loss 0.584608.
Train: 2018-07-31T11:54:25.301788: step 11121, loss 0.573326.
Train: 2018-07-31T11:54:25.473623: step 11122, loss 0.590846.
Train: 2018-07-31T11:54:25.661048: step 11123, loss 0.599688.
Train: 2018-07-31T11:54:25.848534: step 11124, loss 0.6163.
Train: 2018-07-31T11:54:26.020373: step 11125, loss 0.527277.
Train: 2018-07-31T11:54:26.207825: step 11126, loss 0.589833.
Train: 2018-07-31T11:54:26.379660: step 11127, loss 0.657541.
Train: 2018-07-31T11:54:26.567116: step 11128, loss 0.529956.
Train: 2018-07-31T11:54:26.754568: step 11129, loss 0.65007.
Train: 2018-07-31T11:54:26.926407: step 11130, loss 0.618277.
Test: 2018-07-31T11:54:27.410638: step 11130, loss 0.582031.
Train: 2018-07-31T11:54:27.582504: step 11131, loss 0.667939.
Train: 2018-07-31T11:54:27.769953: step 11132, loss 0.546482.
Train: 2018-07-31T11:54:27.957409: step 11133, loss 0.648346.
Train: 2018-07-31T11:54:28.144841: step 11134, loss 0.578306.
Train: 2018-07-31T11:54:28.332327: step 11135, loss 0.633345.
Train: 2018-07-31T11:54:28.519753: step 11136, loss 0.582336.
Train: 2018-07-31T11:54:28.691588: step 11137, loss 0.626984.
Train: 2018-07-31T11:54:28.879078: step 11138, loss 0.662841.
Train: 2018-07-31T11:54:29.066500: step 11139, loss 0.533207.
Train: 2018-07-31T11:54:29.238366: step 11140, loss 0.629539.
Test: 2018-07-31T11:54:29.722597: step 11140, loss 0.589349.
Train: 2018-07-31T11:54:29.910077: step 11141, loss 0.59172.
Train: 2018-07-31T11:54:30.097535: step 11142, loss 0.604131.
Train: 2018-07-31T11:54:30.284995: step 11143, loss 0.578073.
Train: 2018-07-31T11:54:30.472448: step 11144, loss 0.61271.
Train: 2018-07-31T11:54:30.644289: step 11145, loss 0.612411.
Train: 2018-07-31T11:54:30.831713: step 11146, loss 0.594579.
Train: 2018-07-31T11:54:31.019202: step 11147, loss 0.585335.
Train: 2018-07-31T11:54:31.206655: step 11148, loss 0.641456.
Train: 2018-07-31T11:54:31.378460: step 11149, loss 0.566777.
Train: 2018-07-31T11:54:31.581537: step 11150, loss 0.583783.
Test: 2018-07-31T11:54:32.050176: step 11150, loss 0.586286.
Train: 2018-07-31T11:54:32.237663: step 11151, loss 0.557322.
Train: 2018-07-31T11:54:32.425090: step 11152, loss 0.600584.
Train: 2018-07-31T11:54:32.612546: step 11153, loss 0.652683.
Train: 2018-07-31T11:54:32.800031: step 11154, loss 0.652335.
Train: 2018-07-31T11:54:32.987487: step 11155, loss 0.529742.
Train: 2018-07-31T11:54:33.174913: step 11156, loss 0.59906.
Train: 2018-07-31T11:54:33.362399: step 11157, loss 0.59852.
Train: 2018-07-31T11:54:33.549825: step 11158, loss 0.685186.
Train: 2018-07-31T11:54:33.721691: step 11159, loss 0.649501.
Train: 2018-07-31T11:54:33.909141: step 11160, loss 0.570419.
Test: 2018-07-31T11:54:34.377757: step 11160, loss 0.581177.
Train: 2018-07-31T11:54:34.627729: step 11161, loss 0.5957.
Train: 2018-07-31T11:54:34.815185: step 11162, loss 0.542944.
Train: 2018-07-31T11:54:34.987019: step 11163, loss 0.663235.
Train: 2018-07-31T11:54:35.174476: step 11164, loss 0.575905.
Train: 2018-07-31T11:54:35.361902: step 11165, loss 0.669882.
Train: 2018-07-31T11:54:35.549357: step 11166, loss 0.634412.
Train: 2018-07-31T11:54:35.736813: step 11167, loss 0.556335.
Train: 2018-07-31T11:54:35.908678: step 11168, loss 0.658097.
Train: 2018-07-31T11:54:36.096134: step 11169, loss 1.6077.
Train: 2018-07-31T11:54:36.283590: step 11170, loss 0.629376.
Test: 2018-07-31T11:54:36.767821: step 11170, loss 0.571185.
Train: 2018-07-31T11:54:36.939657: step 11171, loss 0.525674.
Train: 2018-07-31T11:54:37.127112: step 11172, loss 0.636438.
Train: 2018-07-31T11:54:37.298978: step 11173, loss 0.567924.
Train: 2018-07-31T11:54:37.470813: step 11174, loss 0.586444.
Train: 2018-07-31T11:54:37.658268: step 11175, loss 0.611925.
Train: 2018-07-31T11:54:37.830074: step 11176, loss 0.604065.
Train: 2018-07-31T11:54:38.017529: step 11177, loss 0.587609.
Train: 2018-07-31T11:54:38.205010: step 11178, loss 0.5625.
Train: 2018-07-31T11:54:38.376850: step 11179, loss 0.57176.
Train: 2018-07-31T11:54:38.564307: step 11180, loss 0.710352.
Test: 2018-07-31T11:54:39.032947: step 11180, loss 0.575572.
Train: 2018-07-31T11:54:39.220403: step 11181, loss 0.546927.
Train: 2018-07-31T11:54:39.407859: step 11182, loss 0.598937.
Train: 2018-07-31T11:54:39.579694: step 11183, loss 0.590543.
Train: 2018-07-31T11:54:39.751523: step 11184, loss 0.633638.
Train: 2018-07-31T11:54:39.938985: step 11185, loss 0.599244.
Train: 2018-07-31T11:54:40.110790: step 11186, loss 0.667727.
Train: 2018-07-31T11:54:40.298276: step 11187, loss 0.556233.
Train: 2018-07-31T11:54:40.470111: step 11188, loss 0.598725.
Train: 2018-07-31T11:54:40.657567: step 11189, loss 0.538824.
Train: 2018-07-31T11:54:40.844993: step 11190, loss 0.572563.
Test: 2018-07-31T11:54:41.329284: step 11190, loss 0.574965.
Train: 2018-07-31T11:54:41.516709: step 11191, loss 0.691311.
Train: 2018-07-31T11:54:41.688575: step 11192, loss 0.569754.
Train: 2018-07-31T11:54:41.876001: step 11193, loss 0.579951.
Train: 2018-07-31T11:54:42.063458: step 11194, loss 0.638812.
Train: 2018-07-31T11:54:42.250938: step 11195, loss 0.573875.
Train: 2018-07-31T11:54:42.438400: step 11196, loss 0.604063.
Train: 2018-07-31T11:54:42.625855: step 11197, loss 0.55319.
Train: 2018-07-31T11:54:42.828904: step 11198, loss 0.611878.
Train: 2018-07-31T11:54:43.000738: step 11199, loss 0.611656.
Train: 2018-07-31T11:54:43.188218: step 11200, loss 0.603057.
Test: 2018-07-31T11:54:43.672454: step 11200, loss 0.572067.
Train: 2018-07-31T11:54:44.437925: step 11201, loss 0.572049.
Train: 2018-07-31T11:54:44.609736: step 11202, loss 0.53597.
Train: 2018-07-31T11:54:44.797216: step 11203, loss 0.586809.
Train: 2018-07-31T11:54:44.969057: step 11204, loss 0.596016.
Train: 2018-07-31T11:54:45.156484: step 11205, loss 0.563356.
Train: 2018-07-31T11:54:45.343938: step 11206, loss 0.572743.
Train: 2018-07-31T11:54:45.515808: step 11207, loss 0.573707.
Train: 2018-07-31T11:54:45.703230: step 11208, loss 0.532377.
Train: 2018-07-31T11:54:45.890719: step 11209, loss 0.592349.
Train: 2018-07-31T11:54:46.062545: step 11210, loss 0.542118.
Test: 2018-07-31T11:54:46.546781: step 11210, loss 0.579446.
Train: 2018-07-31T11:54:46.734239: step 11211, loss 0.674185.
Train: 2018-07-31T11:54:46.921724: step 11212, loss 0.576925.
Train: 2018-07-31T11:54:47.109150: step 11213, loss 0.628682.
Train: 2018-07-31T11:54:47.296607: step 11214, loss 0.620388.
Train: 2018-07-31T11:54:47.468442: step 11215, loss 0.637855.
Train: 2018-07-31T11:54:47.655927: step 11216, loss 0.491191.
Train: 2018-07-31T11:54:47.843384: step 11217, loss 0.638116.
Train: 2018-07-31T11:54:48.030842: step 11218, loss 0.560012.
Train: 2018-07-31T11:54:48.218295: step 11219, loss 0.872899.
Train: 2018-07-31T11:54:48.405723: step 11220, loss 0.593681.
Test: 2018-07-31T11:54:48.874393: step 11220, loss 0.578625.
Train: 2018-07-31T11:54:49.061848: step 11221, loss 0.523012.
Train: 2018-07-31T11:54:49.233677: step 11222, loss 0.57577.
Train: 2018-07-31T11:54:49.421108: step 11223, loss 0.584959.
Train: 2018-07-31T11:54:49.592943: step 11224, loss 0.576512.
Train: 2018-07-31T11:54:49.780424: step 11225, loss 0.675864.
Train: 2018-07-31T11:54:49.967856: step 11226, loss 0.541464.
Train: 2018-07-31T11:54:50.139692: step 11227, loss 0.52368.
Train: 2018-07-31T11:54:50.327177: step 11228, loss 0.541906.
Train: 2018-07-31T11:54:50.514603: step 11229, loss 0.596581.
Train: 2018-07-31T11:54:50.686437: step 11230, loss 0.624447.
Test: 2018-07-31T11:54:51.170698: step 11230, loss 0.581316.
Train: 2018-07-31T11:54:51.358156: step 11231, loss 0.633317.
Train: 2018-07-31T11:54:51.545636: step 11232, loss 0.58734.
Train: 2018-07-31T11:54:51.717447: step 11233, loss 0.587092.
Train: 2018-07-31T11:54:51.904903: step 11234, loss 0.550009.
Train: 2018-07-31T11:54:52.092358: step 11235, loss 0.586394.
Train: 2018-07-31T11:54:52.279815: step 11236, loss 0.678066.
Train: 2018-07-31T11:54:52.467270: step 11237, loss 0.613104.
Train: 2018-07-31T11:54:52.639130: step 11238, loss 0.612564.
Train: 2018-07-31T11:54:52.826591: step 11239, loss 0.556973.
Train: 2018-07-31T11:54:52.998426: step 11240, loss 0.638894.
Test: 2018-07-31T11:54:53.482689: step 11240, loss 0.577247.
Train: 2018-07-31T11:54:53.654523: step 11241, loss 0.610818.
Train: 2018-07-31T11:54:53.841978: step 11242, loss 0.564644.
Train: 2018-07-31T11:54:54.013784: step 11243, loss 0.659003.
Train: 2018-07-31T11:54:54.201270: step 11244, loss 0.591051.
Train: 2018-07-31T11:54:54.388696: step 11245, loss 0.654324.
Train: 2018-07-31T11:54:54.576151: step 11246, loss 0.510466.
Train: 2018-07-31T11:54:54.747987: step 11247, loss 0.672665.
Train: 2018-07-31T11:54:54.935443: step 11248, loss 0.628255.
Train: 2018-07-31T11:54:55.122932: step 11249, loss 0.54865.
Train: 2018-07-31T11:54:55.310356: step 11250, loss 0.584874.
Test: 2018-07-31T11:54:55.779025: step 11250, loss 0.5795.
Train: 2018-07-31T11:54:55.966452: step 11251, loss 0.594305.
Train: 2018-07-31T11:54:56.153937: step 11252, loss 0.585983.
Train: 2018-07-31T11:54:56.341394: step 11253, loss 0.560034.
Train: 2018-07-31T11:54:56.513232: step 11254, loss 0.586741.
Train: 2018-07-31T11:54:56.700655: step 11255, loss 0.622028.
Train: 2018-07-31T11:54:56.888110: step 11256, loss 0.604596.
Train: 2018-07-31T11:54:57.059975: step 11257, loss 0.70063.
Train: 2018-07-31T11:54:57.247434: step 11258, loss 0.647974.
Train: 2018-07-31T11:54:57.434888: step 11259, loss 0.656244.
Train: 2018-07-31T11:54:57.622313: step 11260, loss 0.604052.
Test: 2018-07-31T11:54:58.090952: step 11260, loss 0.580839.
Train: 2018-07-31T11:54:58.278410: step 11261, loss 0.603751.
Train: 2018-07-31T11:54:58.465867: step 11262, loss 0.594885.
Train: 2018-07-31T11:54:58.637735: step 11263, loss 0.577574.
Train: 2018-07-31T11:54:58.809535: step 11264, loss 0.59419.
Train: 2018-07-31T11:54:58.997018: step 11265, loss 0.610672.
Train: 2018-07-31T11:54:59.184472: step 11266, loss 0.593406.
Train: 2018-07-31T11:54:59.356282: step 11267, loss 0.626537.
Train: 2018-07-31T11:54:59.543738: step 11268, loss 0.600941.
Train: 2018-07-31T11:54:59.715599: step 11269, loss 0.617175.
Train: 2018-07-31T11:54:59.903030: step 11270, loss 0.541872.
Test: 2018-07-31T11:55:00.387291: step 11270, loss 0.577428.
Train: 2018-07-31T11:55:00.574777: step 11271, loss 0.566426.
Train: 2018-07-31T11:55:00.762203: step 11272, loss 0.624098.
Train: 2018-07-31T11:55:00.934071: step 11273, loss 0.590501.
Train: 2018-07-31T11:55:01.121493: step 11274, loss 0.60666.
Train: 2018-07-31T11:55:01.293353: step 11275, loss 0.589703.
Train: 2018-07-31T11:55:01.480784: step 11276, loss 0.638924.
Train: 2018-07-31T11:55:01.668271: step 11277, loss 0.621967.
Train: 2018-07-31T11:55:01.840106: step 11278, loss 0.506186.
Train: 2018-07-31T11:55:02.027532: step 11279, loss 0.612948.
Train: 2018-07-31T11:55:02.214987: step 11280, loss 0.604351.
Test: 2018-07-31T11:55:02.683658: step 11280, loss 0.573727.
Train: 2018-07-31T11:55:02.871084: step 11281, loss 0.554568.
Train: 2018-07-31T11:55:03.042949: step 11282, loss 0.562452.
Train: 2018-07-31T11:55:03.214778: step 11283, loss 0.628133.
Train: 2018-07-31T11:55:03.402210: step 11284, loss 0.619575.
Train: 2018-07-31T11:55:03.574074: step 11285, loss 0.577972.
Train: 2018-07-31T11:55:03.777152: step 11286, loss 0.495059.
Train: 2018-07-31T11:55:03.948957: step 11287, loss 0.569067.
Train: 2018-07-31T11:55:04.136412: step 11288, loss 0.518908.
Train: 2018-07-31T11:55:04.323900: step 11289, loss 0.485007.
Train: 2018-07-31T11:55:04.495703: step 11290, loss 0.593129.
Test: 2018-07-31T11:55:04.979997: step 11290, loss 0.570356.
Train: 2018-07-31T11:55:05.151800: step 11291, loss 0.601325.
Train: 2018-07-31T11:55:05.339286: step 11292, loss 0.685774.
Train: 2018-07-31T11:55:05.526713: step 11293, loss 0.550053.
Train: 2018-07-31T11:55:05.698548: step 11294, loss 0.609233.
Train: 2018-07-31T11:55:05.886037: step 11295, loss 0.489858.
Train: 2018-07-31T11:55:06.073459: step 11296, loss 0.591832.
Train: 2018-07-31T11:55:06.245295: step 11297, loss 0.57451.
Train: 2018-07-31T11:55:06.432782: step 11298, loss 0.514139.
Train: 2018-07-31T11:55:06.604609: step 11299, loss 0.582682.
Train: 2018-07-31T11:55:06.792042: step 11300, loss 0.487289.
Test: 2018-07-31T11:55:07.276333: step 11300, loss 0.567758.
Train: 2018-07-31T11:55:08.010540: step 11301, loss 0.59972.
Train: 2018-07-31T11:55:08.182365: step 11302, loss 0.538495.
Train: 2018-07-31T11:55:08.369797: step 11303, loss 0.608317.
Train: 2018-07-31T11:55:08.541656: step 11304, loss 0.555454.
Train: 2018-07-31T11:55:08.713496: step 11305, loss 0.564051.
Train: 2018-07-31T11:55:08.900924: step 11306, loss 0.546141.
Train: 2018-07-31T11:55:09.088378: step 11307, loss 0.545891.
Train: 2018-07-31T11:55:09.260247: step 11308, loss 0.608082.
Train: 2018-07-31T11:55:09.447699: step 11309, loss 0.599083.
Train: 2018-07-31T11:55:09.619537: step 11310, loss 0.554198.
Test: 2018-07-31T11:55:10.103765: step 11310, loss 0.56592.
Train: 2018-07-31T11:55:10.291251: step 11311, loss 0.527101.
Train: 2018-07-31T11:55:10.463057: step 11312, loss 0.616831.
Train: 2018-07-31T11:55:10.650537: step 11313, loss 0.535676.
Train: 2018-07-31T11:55:10.822348: step 11314, loss 0.535485.
Train: 2018-07-31T11:55:11.009837: step 11315, loss 0.634794.
Train: 2018-07-31T11:55:11.181662: step 11316, loss 0.652831.
Train: 2018-07-31T11:55:11.369124: step 11317, loss 0.643594.
Train: 2018-07-31T11:55:11.540960: step 11318, loss 0.571098.
Train: 2018-07-31T11:55:11.712794: step 11319, loss 0.588996.
Train: 2018-07-31T11:55:11.896112: step 11320, loss 0.561868.
Test: 2018-07-31T11:55:12.380373: step 11320, loss 0.564688.
Train: 2018-07-31T11:55:12.552207: step 11321, loss 0.642535.
Train: 2018-07-31T11:55:12.739663: step 11322, loss 0.543789.
Train: 2018-07-31T11:55:12.911530: step 11323, loss 0.516959.
Train: 2018-07-31T11:55:13.098954: step 11324, loss 0.570419.
Train: 2018-07-31T11:55:13.255168: step 11325, loss 0.712285.
Train: 2018-07-31T11:55:13.442625: step 11326, loss 0.579099.
Train: 2018-07-31T11:55:13.614490: step 11327, loss 0.499324.
Train: 2018-07-31T11:55:13.801940: step 11328, loss 0.57887.
Train: 2018-07-31T11:55:13.973781: step 11329, loss 0.508188.
Train: 2018-07-31T11:55:14.161205: step 11330, loss 0.587493.
Test: 2018-07-31T11:55:14.629877: step 11330, loss 0.563816.
Train: 2018-07-31T11:55:14.817332: step 11331, loss 0.605029.
Train: 2018-07-31T11:55:14.989137: step 11332, loss 0.631328.
Train: 2018-07-31T11:55:15.176618: step 11333, loss 0.508079.
Train: 2018-07-31T11:55:15.348427: step 11334, loss 0.551951.
Train: 2018-07-31T11:55:15.535884: step 11335, loss 0.569435.
Train: 2018-07-31T11:55:15.707753: step 11336, loss 0.604478.
Train: 2018-07-31T11:55:15.879584: step 11337, loss 0.569277.
Train: 2018-07-31T11:55:16.067012: step 11338, loss 0.463962.
Train: 2018-07-31T11:55:16.238845: step 11339, loss 0.577903.
Train: 2018-07-31T11:55:16.426301: step 11340, loss 0.639403.
Test: 2018-07-31T11:55:16.894941: step 11340, loss 0.563032.
Train: 2018-07-31T11:55:17.082427: step 11341, loss 0.507405.
Train: 2018-07-31T11:55:17.254232: step 11342, loss 0.577695.
Train: 2018-07-31T11:55:17.426067: step 11343, loss 0.61288.
Train: 2018-07-31T11:55:17.613523: step 11344, loss 0.568747.
Train: 2018-07-31T11:55:17.785357: step 11345, loss 0.577492.
Train: 2018-07-31T11:55:17.957191: step 11346, loss 0.68318.
Train: 2018-07-31T11:55:18.144682: step 11347, loss 0.61252.
Train: 2018-07-31T11:55:18.332105: step 11348, loss 0.568494.
Train: 2018-07-31T11:55:18.503939: step 11349, loss 0.629683.
Train: 2018-07-31T11:55:18.675773: step 11350, loss 0.542229.
Test: 2018-07-31T11:55:19.160061: step 11350, loss 0.56247.
Train: 2018-07-31T11:55:19.347492: step 11351, loss 0.594442.
Train: 2018-07-31T11:55:19.519356: step 11352, loss 0.550937.
Train: 2018-07-31T11:55:19.691191: step 11353, loss 0.55092.
Train: 2018-07-31T11:55:19.878617: step 11354, loss 0.533584.
Train: 2018-07-31T11:55:20.050453: step 11355, loss 0.60589.
Train: 2018-07-31T11:55:20.237938: step 11356, loss 0.559543.
Train: 2018-07-31T11:55:20.409744: step 11357, loss 0.594304.
Train: 2018-07-31T11:55:20.581577: step 11358, loss 0.551513.
Train: 2018-07-31T11:55:20.769033: step 11359, loss 0.64671.
Train: 2018-07-31T11:55:20.940898: step 11360, loss 0.517062.
Test: 2018-07-31T11:55:21.409539: step 11360, loss 0.56407.
Train: 2018-07-31T11:55:21.581373: step 11361, loss 0.569881.
Train: 2018-07-31T11:55:21.768830: step 11362, loss 0.566638.
Train: 2018-07-31T11:55:21.940634: step 11363, loss 0.719118.
Train: 2018-07-31T11:55:22.128114: step 11364, loss 0.562619.
Train: 2018-07-31T11:55:22.299956: step 11365, loss 0.545981.
Train: 2018-07-31T11:55:22.471761: step 11366, loss 0.598412.
Train: 2018-07-31T11:55:22.659217: step 11367, loss 0.668206.
Train: 2018-07-31T11:55:22.831081: step 11368, loss 0.582497.
Train: 2018-07-31T11:55:23.002916: step 11369, loss 0.583111.
Train: 2018-07-31T11:55:23.190343: step 11370, loss 0.549228.
Test: 2018-07-31T11:55:23.674603: step 11370, loss 0.569724.
Train: 2018-07-31T11:55:23.846438: step 11371, loss 0.566925.
Train: 2018-07-31T11:55:24.018273: step 11372, loss 0.679208.
Train: 2018-07-31T11:55:24.205765: step 11373, loss 0.602033.
Train: 2018-07-31T11:55:24.377594: step 11374, loss 0.602239.
Train: 2018-07-31T11:55:24.565050: step 11375, loss 0.568147.
Train: 2018-07-31T11:55:24.736888: step 11376, loss 0.48285.
Train: 2018-07-31T11:55:24.924341: step 11377, loss 0.576851.
Train: 2018-07-31T11:55:25.096175: step 11378, loss 0.561229.
Train: 2018-07-31T11:55:25.283602: step 11379, loss 0.533927.
Train: 2018-07-31T11:55:25.455473: step 11380, loss 0.56806.
Test: 2018-07-31T11:55:25.924075: step 11380, loss 0.570698.
Train: 2018-07-31T11:55:26.111562: step 11381, loss 0.585108.
Train: 2018-07-31T11:55:26.283398: step 11382, loss 0.567718.
Train: 2018-07-31T11:55:26.455232: step 11383, loss 0.558874.
Train: 2018-07-31T11:55:26.627038: step 11384, loss 0.523983.
Train: 2018-07-31T11:55:26.798902: step 11385, loss 0.549657.
Train: 2018-07-31T11:55:26.986328: step 11386, loss 0.55805.
Train: 2018-07-31T11:55:27.158163: step 11387, loss 0.627769.
Train: 2018-07-31T11:55:27.330022: step 11388, loss 0.539909.
Train: 2018-07-31T11:55:27.501862: step 11389, loss 0.53076.
Train: 2018-07-31T11:55:27.673698: step 11390, loss 0.556841.
Test: 2018-07-31T11:55:28.157959: step 11390, loss 0.568265.
Train: 2018-07-31T11:55:28.329793: step 11391, loss 0.529945.
Train: 2018-07-31T11:55:28.517243: step 11392, loss 0.565094.
Train: 2018-07-31T11:55:28.689084: step 11393, loss 0.56481.
Train: 2018-07-31T11:55:28.860889: step 11394, loss 0.664097.
Train: 2018-07-31T11:55:29.032723: step 11395, loss 0.573247.
Train: 2018-07-31T11:55:29.220179: step 11396, loss 0.546063.
Train: 2018-07-31T11:55:29.392045: step 11397, loss 0.572796.
Train: 2018-07-31T11:55:29.563879: step 11398, loss 0.545548.
Train: 2018-07-31T11:55:29.751317: step 11399, loss 0.518214.
Train: 2018-07-31T11:55:29.923164: step 11400, loss 0.554079.
Test: 2018-07-31T11:55:30.391811: step 11400, loss 0.565862.
Train: 2018-07-31T11:55:31.141635: step 11401, loss 0.581054.
Train: 2018-07-31T11:55:31.329093: step 11402, loss 0.635416.
Train: 2018-07-31T11:55:31.500926: step 11403, loss 0.662538.
Train: 2018-07-31T11:55:31.672761: step 11404, loss 0.61681.
Train: 2018-07-31T11:55:31.844595: step 11405, loss 0.589324.
Train: 2018-07-31T11:55:32.032052: step 11406, loss 0.598131.
Train: 2018-07-31T11:55:32.219508: step 11407, loss 0.525692.
Train: 2018-07-31T11:55:32.391312: step 11408, loss 0.660695.
Train: 2018-07-31T11:55:32.578793: step 11409, loss 0.525527.
Train: 2018-07-31T11:55:32.750633: step 11410, loss 0.579214.
Test: 2018-07-31T11:55:33.234894: step 11410, loss 0.564042.
Train: 2018-07-31T11:55:33.406730: step 11411, loss 0.632692.
Train: 2018-07-31T11:55:33.578534: step 11412, loss 0.587746.
Train: 2018-07-31T11:55:33.765991: step 11413, loss 0.614205.
Train: 2018-07-31T11:55:33.937825: step 11414, loss 0.587312.
Train: 2018-07-31T11:55:34.109660: step 11415, loss 0.5253.
Train: 2018-07-31T11:55:34.297115: step 11416, loss 0.595727.
Train: 2018-07-31T11:55:34.468951: step 11417, loss 0.55159.
Train: 2018-07-31T11:55:34.656437: step 11418, loss 0.525179.
Train: 2018-07-31T11:55:34.828241: step 11419, loss 0.630238.
Train: 2018-07-31T11:55:35.000106: step 11420, loss 0.699981.
Test: 2018-07-31T11:55:35.484369: step 11420, loss 0.562785.
Train: 2018-07-31T11:55:35.656173: step 11421, loss 0.629645.
Train: 2018-07-31T11:55:35.828037: step 11422, loss 0.603248.
Train: 2018-07-31T11:55:35.999872: step 11423, loss 0.585737.
Train: 2018-07-31T11:55:36.187298: step 11424, loss 0.594167.
Train: 2018-07-31T11:55:36.359133: step 11425, loss 0.58543.
Train: 2018-07-31T11:55:36.546619: step 11426, loss 0.602318.
Train: 2018-07-31T11:55:36.718425: step 11427, loss 0.61059.
Train: 2018-07-31T11:55:36.905910: step 11428, loss 0.568157.
Train: 2018-07-31T11:55:37.077715: step 11429, loss 0.635356.
Train: 2018-07-31T11:55:37.249550: step 11430, loss 0.58481.
Test: 2018-07-31T11:55:37.733810: step 11430, loss 0.562422.
Train: 2018-07-31T11:55:37.905646: step 11431, loss 0.609703.
Train: 2018-07-31T11:55:38.077511: step 11432, loss 0.534842.
Train: 2018-07-31T11:55:38.264967: step 11433, loss 0.576254.
Train: 2018-07-31T11:55:38.436801: step 11434, loss 0.584442.
Train: 2018-07-31T11:55:38.608606: step 11435, loss 0.592594.
Train: 2018-07-31T11:55:38.796062: step 11436, loss 0.600713.
Train: 2018-07-31T11:55:38.967898: step 11437, loss 0.551434.
Train: 2018-07-31T11:55:39.139731: step 11438, loss 0.64143.
Train: 2018-07-31T11:55:39.311597: step 11439, loss 0.608568.
Train: 2018-07-31T11:55:39.483401: step 11440, loss 0.543301.
Test: 2018-07-31T11:55:39.967662: step 11440, loss 0.562202.
Train: 2018-07-31T11:55:40.139528: step 11441, loss 0.527035.
Train: 2018-07-31T11:55:40.311363: step 11442, loss 0.632672.
Train: 2018-07-31T11:55:40.483191: step 11443, loss 0.551331.
Train: 2018-07-31T11:55:40.655027: step 11444, loss 0.624376.
Train: 2018-07-31T11:55:40.826838: step 11445, loss 0.583706.
Train: 2018-07-31T11:55:40.998702: step 11446, loss 0.559321.
Train: 2018-07-31T11:55:41.186158: step 11447, loss 0.526828.
Train: 2018-07-31T11:55:41.357992: step 11448, loss 0.616013.
Train: 2018-07-31T11:55:41.529833: step 11449, loss 0.567227.
Train: 2018-07-31T11:55:41.701632: step 11450, loss 0.550892.
Test: 2018-07-31T11:55:42.170272: step 11450, loss 0.561577.
Train: 2018-07-31T11:55:42.389001: step 11451, loss 0.6078.
Train: 2018-07-31T11:55:42.592048: step 11452, loss 0.599618.
Train: 2018-07-31T11:55:42.810746: step 11453, loss 0.517993.
Train: 2018-07-31T11:55:43.013825: step 11454, loss 0.534158.
Train: 2018-07-31T11:55:43.232554: step 11455, loss 0.574957.
Train: 2018-07-31T11:55:43.388736: step 11456, loss 0.599553.
Train: 2018-07-31T11:55:43.576193: step 11457, loss 0.533614.
Train: 2018-07-31T11:55:43.748053: step 11458, loss 0.591284.
Train: 2018-07-31T11:55:43.919862: step 11459, loss 0.533248.
Train: 2018-07-31T11:55:44.091728: step 11460, loss 0.483145.
Test: 2018-07-31T11:55:44.575990: step 11460, loss 0.560551.
Train: 2018-07-31T11:55:44.763445: step 11461, loss 0.549474.
Train: 2018-07-31T11:55:44.935250: step 11462, loss 0.566081.
Train: 2018-07-31T11:55:45.107114: step 11463, loss 0.616646.
Train: 2018-07-31T11:55:45.294564: step 11464, loss 0.591325.
Train: 2018-07-31T11:55:45.466406: step 11465, loss 0.497827.
Train: 2018-07-31T11:55:45.638210: step 11466, loss 0.574286.
Train: 2018-07-31T11:55:45.810075: step 11467, loss 0.608526.
Train: 2018-07-31T11:55:45.981910: step 11468, loss 0.591399.
Train: 2018-07-31T11:55:46.153745: step 11469, loss 0.591402.
Train: 2018-07-31T11:55:46.341201: step 11470, loss 0.548271.
Test: 2018-07-31T11:55:46.825462: step 11470, loss 0.559643.
Train: 2018-07-31T11:55:46.997268: step 11471, loss 0.591399.
Train: 2018-07-31T11:55:47.184723: step 11472, loss 0.61735.
Train: 2018-07-31T11:55:47.356558: step 11473, loss 0.530776.
Train: 2018-07-31T11:55:47.528417: step 11474, loss 0.600018.
Train: 2018-07-31T11:55:47.715850: step 11475, loss 0.599998.
Train: 2018-07-31T11:55:47.872063: step 11476, loss 0.592451.
Train: 2018-07-31T11:55:48.043927: step 11477, loss 0.530622.
Train: 2018-07-31T11:55:48.231353: step 11478, loss 0.582565.
Train: 2018-07-31T11:55:48.403218: step 11479, loss 0.617195.
Train: 2018-07-31T11:55:48.575052: step 11480, loss 0.556527.
Test: 2018-07-31T11:55:49.059285: step 11480, loss 0.55932.
Train: 2018-07-31T11:55:49.231143: step 11481, loss 0.5392.
Train: 2018-07-31T11:55:49.402984: step 11482, loss 0.565129.
Train: 2018-07-31T11:55:49.574788: step 11483, loss 0.565099.
Train: 2018-07-31T11:55:49.762275: step 11484, loss 0.599714.
Train: 2018-07-31T11:55:49.934103: step 11485, loss 0.634322.
Train: 2018-07-31T11:55:50.105938: step 11486, loss 0.573673.
Train: 2018-07-31T11:55:50.277779: step 11487, loss 0.530463.
Train: 2018-07-31T11:55:50.465229: step 11488, loss 0.504551.
Train: 2018-07-31T11:55:50.637070: step 11489, loss 0.590879.
Train: 2018-07-31T11:55:50.824495: step 11490, loss 0.608159.
Test: 2018-07-31T11:55:51.293136: step 11490, loss 0.559066.
Train: 2018-07-31T11:55:51.480592: step 11491, loss 0.573545.
Train: 2018-07-31T11:55:51.652426: step 11492, loss 0.57352.
Train: 2018-07-31T11:55:51.824262: step 11493, loss 0.590777.
Train: 2018-07-31T11:55:51.996126: step 11494, loss 0.504382.
Train: 2018-07-31T11:55:52.183552: step 11495, loss 0.58209.
Train: 2018-07-31T11:55:52.355386: step 11496, loss 0.547482.
Train: 2018-07-31T11:55:52.527221: step 11497, loss 0.590714.
Train: 2018-07-31T11:55:52.699057: step 11498, loss 0.60802.
Train: 2018-07-31T11:55:52.870915: step 11499, loss 0.547386.
Train: 2018-07-31T11:55:53.058378: step 11500, loss 0.478089.
Test: 2018-07-31T11:55:53.527018: step 11500, loss 0.558787.
Train: 2018-07-31T11:55:54.308055: step 11501, loss 0.555965.
Train: 2018-07-31T11:55:54.479889: step 11502, loss 0.521134.
Train: 2018-07-31T11:55:54.651755: step 11503, loss 0.520973.
Train: 2018-07-31T11:55:54.839214: step 11504, loss 0.555782.
Train: 2018-07-31T11:55:55.011045: step 11505, loss 0.538158.
Train: 2018-07-31T11:55:55.182849: step 11506, loss 0.555654.
Train: 2018-07-31T11:55:55.354685: step 11507, loss 0.617493.
Train: 2018-07-31T11:55:55.526519: step 11508, loss 0.617582.
Train: 2018-07-31T11:55:55.698355: step 11509, loss 0.537782.
Train: 2018-07-31T11:55:55.870219: step 11510, loss 0.608779.
Test: 2018-07-31T11:55:56.354451: step 11510, loss 0.558353.
Train: 2018-07-31T11:55:56.526286: step 11511, loss 0.573235.
Train: 2018-07-31T11:55:56.698145: step 11512, loss 0.573216.
Train: 2018-07-31T11:55:56.869955: step 11513, loss 0.537646.
Train: 2018-07-31T11:55:57.057441: step 11514, loss 0.590965.
Train: 2018-07-31T11:55:57.229276: step 11515, loss 0.564269.
Train: 2018-07-31T11:55:57.401111: step 11516, loss 0.590925.
Train: 2018-07-31T11:55:57.572939: step 11517, loss 0.653105.
Train: 2018-07-31T11:55:57.744749: step 11518, loss 0.661768.
Train: 2018-07-31T11:55:57.916584: step 11519, loss 0.520021.
Train: 2018-07-31T11:55:58.088450: step 11520, loss 0.599439.
Test: 2018-07-31T11:55:58.557089: step 11520, loss 0.558244.
Train: 2018-07-31T11:55:58.744546: step 11521, loss 0.529022.
Train: 2018-07-31T11:55:58.916352: step 11522, loss 0.572933.
Train: 2018-07-31T11:55:59.088210: step 11523, loss 0.739271.
Train: 2018-07-31T11:55:59.275641: step 11524, loss 0.599006.
Train: 2018-07-31T11:55:59.447476: step 11525, loss 0.62485.
Train: 2018-07-31T11:55:59.619311: step 11526, loss 0.564184.
Train: 2018-07-31T11:55:59.806767: step 11527, loss 0.512736.
Train: 2018-07-31T11:55:59.978632: step 11528, loss 0.589868.
Train: 2018-07-31T11:56:00.150461: step 11529, loss 0.530141.
Train: 2018-07-31T11:56:00.322301: step 11530, loss 0.572739.
Test: 2018-07-31T11:56:00.806563: step 11530, loss 0.558507.
Train: 2018-07-31T11:56:00.978369: step 11531, loss 0.589704.
Train: 2018-07-31T11:56:01.165825: step 11532, loss 0.606605.
Train: 2018-07-31T11:56:01.337688: step 11533, loss 0.623419.
Train: 2018-07-31T11:56:01.525139: step 11534, loss 0.63168.
Train: 2018-07-31T11:56:01.696949: step 11535, loss 0.522339.
Train: 2018-07-31T11:56:01.868814: step 11536, loss 0.581063.
Train: 2018-07-31T11:56:02.056242: step 11537, loss 0.55598.
Train: 2018-07-31T11:56:02.228075: step 11538, loss 0.5977.
Train: 2018-07-31T11:56:02.399910: step 11539, loss 0.605974.
Train: 2018-07-31T11:56:02.571774: step 11540, loss 0.564365.
Test: 2018-07-31T11:56:03.056006: step 11540, loss 0.558778.
Train: 2018-07-31T11:56:03.227841: step 11541, loss 0.564374.
Train: 2018-07-31T11:56:03.399676: step 11542, loss 0.547809.
Train: 2018-07-31T11:56:03.571541: step 11543, loss 0.514682.
Train: 2018-07-31T11:56:03.758967: step 11544, loss 0.556041.
Train: 2018-07-31T11:56:03.930831: step 11545, loss 0.56429.
Train: 2018-07-31T11:56:04.102666: step 11546, loss 0.489401.
Train: 2018-07-31T11:56:04.274502: step 11547, loss 0.572526.
Train: 2018-07-31T11:56:04.446336: step 11548, loss 0.522259.
Train: 2018-07-31T11:56:04.618141: step 11549, loss 0.60608.
Train: 2018-07-31T11:56:04.805597: step 11550, loss 0.5387.
Test: 2018-07-31T11:56:05.274267: step 11550, loss 0.558228.
Train: 2018-07-31T11:56:05.446071: step 11551, loss 0.597783.
Train: 2018-07-31T11:56:05.633558: step 11552, loss 0.58086.
Train: 2018-07-31T11:56:05.805363: step 11553, loss 0.529837.
Train: 2018-07-31T11:56:05.977227: step 11554, loss 0.563805.
Train: 2018-07-31T11:56:06.149031: step 11555, loss 0.520996.
Train: 2018-07-31T11:56:06.336488: step 11556, loss 0.615221.
Train: 2018-07-31T11:56:06.508323: step 11557, loss 0.623914.
Train: 2018-07-31T11:56:06.680157: step 11558, loss 0.520584.
Train: 2018-07-31T11:56:06.852022: step 11559, loss 0.598152.
Train: 2018-07-31T11:56:07.023826: step 11560, loss 0.580886.
Test: 2018-07-31T11:56:07.508087: step 11560, loss 0.557743.
Train: 2018-07-31T11:56:07.679957: step 11561, loss 0.56358.
Train: 2018-07-31T11:56:07.851788: step 11562, loss 0.537583.
Train: 2018-07-31T11:56:08.023623: step 11563, loss 0.580877.
Train: 2018-07-31T11:56:08.195427: step 11564, loss 0.580874.
Train: 2018-07-31T11:56:08.367262: step 11565, loss 0.563492.
Train: 2018-07-31T11:56:08.554743: step 11566, loss 0.589559.
Train: 2018-07-31T11:56:08.726554: step 11567, loss 0.572154.
Train: 2018-07-31T11:56:08.898419: step 11568, loss 0.659146.
Train: 2018-07-31T11:56:09.070223: step 11569, loss 0.624243.
Train: 2018-07-31T11:56:09.257709: step 11570, loss 0.520107.
Test: 2018-07-31T11:56:09.726319: step 11570, loss 0.557587.
Train: 2018-07-31T11:56:09.898156: step 11571, loss 0.563427.
Train: 2018-07-31T11:56:10.070018: step 11572, loss 0.554774.
Train: 2018-07-31T11:56:10.257475: step 11573, loss 0.502925.
Train: 2018-07-31T11:56:10.429280: step 11574, loss 0.580684.
Train: 2018-07-31T11:56:10.601145: step 11575, loss 0.572023.
Train: 2018-07-31T11:56:10.772980: step 11576, loss 0.580665.
Train: 2018-07-31T11:56:10.944815: step 11577, loss 0.520052.
Train: 2018-07-31T11:56:11.179134: step 11578, loss 0.580651.
Train: 2018-07-31T11:56:11.350972: step 11579, loss 0.511251.
Train: 2018-07-31T11:56:11.522773: step 11580, loss 0.615416.
Test: 2018-07-31T11:56:12.007035: step 11580, loss 0.557385.
Train: 2018-07-31T11:56:12.178900: step 11581, loss 0.571951.
Train: 2018-07-31T11:56:12.366326: step 11582, loss 0.589345.
Train: 2018-07-31T11:56:12.538162: step 11583, loss 0.563221.
Train: 2018-07-31T11:56:12.709995: step 11584, loss 0.571912.
Train: 2018-07-31T11:56:12.881860: step 11585, loss 0.606731.
Train: 2018-07-31T11:56:13.069316: step 11586, loss 0.650206.
Train: 2018-07-31T11:56:13.241146: step 11587, loss 0.545811.
Train: 2018-07-31T11:56:13.428607: step 11588, loss 0.632541.
Train: 2018-07-31T11:56:13.600442: step 11589, loss 0.54588.
Train: 2018-07-31T11:56:13.787898: step 11590, loss 0.649498.
Test: 2018-07-31T11:56:14.256507: step 11590, loss 0.557382.
Train: 2018-07-31T11:56:14.428373: step 11591, loss 0.597603.
Train: 2018-07-31T11:56:14.600208: step 11592, loss 0.580351.
Train: 2018-07-31T11:56:14.787664: step 11593, loss 0.529037.
Train: 2018-07-31T11:56:14.959469: step 11594, loss 0.520588.
Train: 2018-07-31T11:56:15.131303: step 11595, loss 0.486536.
Train: 2018-07-31T11:56:15.318790: step 11596, loss 0.597315.
Train: 2018-07-31T11:56:15.490625: step 11597, loss 0.580246.
Train: 2018-07-31T11:56:15.678051: step 11598, loss 0.563164.
Train: 2018-07-31T11:56:15.849885: step 11599, loss 0.61438.
Train: 2018-07-31T11:56:16.021720: step 11600, loss 0.588742.
Test: 2018-07-31T11:56:16.506011: step 11600, loss 0.557377.
Train: 2018-07-31T11:56:17.271451: step 11601, loss 0.546075.
Train: 2018-07-31T11:56:17.443287: step 11602, loss 0.605757.
Train: 2018-07-31T11:56:17.615096: step 11603, loss 0.588676.
Train: 2018-07-31T11:56:17.786962: step 11604, loss 0.622695.
Train: 2018-07-31T11:56:17.974387: step 11605, loss 0.588605.
Train: 2018-07-31T11:56:18.130631: step 11606, loss 0.512262.
Train: 2018-07-31T11:56:18.318088: step 11607, loss 0.580068.
Train: 2018-07-31T11:56:18.489892: step 11608, loss 0.588519.
Train: 2018-07-31T11:56:18.661752: step 11609, loss 0.537733.
Train: 2018-07-31T11:56:18.833561: step 11610, loss 0.54618.
Test: 2018-07-31T11:56:19.317823: step 11610, loss 0.557369.
Train: 2018-07-31T11:56:19.489689: step 11611, loss 0.588476.
Train: 2018-07-31T11:56:19.677113: step 11612, loss 0.58.
Train: 2018-07-31T11:56:19.848983: step 11613, loss 0.554583.
Train: 2018-07-31T11:56:20.020783: step 11614, loss 0.579978.
Train: 2018-07-31T11:56:20.192619: step 11615, loss 0.537591.
Train: 2018-07-31T11:56:20.380074: step 11616, loss 0.546024.
Train: 2018-07-31T11:56:20.551908: step 11617, loss 0.57146.
Train: 2018-07-31T11:56:20.739396: step 11618, loss 0.571445.
Train: 2018-07-31T11:56:20.911201: step 11619, loss 0.537347.
Train: 2018-07-31T11:56:21.098655: step 11620, loss 0.545804.
Test: 2018-07-31T11:56:21.567326: step 11620, loss 0.557071.
Train: 2018-07-31T11:56:21.754777: step 11621, loss 0.597071.
Train: 2018-07-31T11:56:21.926619: step 11622, loss 0.631376.
Train: 2018-07-31T11:56:22.098421: step 11623, loss 0.579946.
Train: 2018-07-31T11:56:22.270281: step 11624, loss 0.545651.
Train: 2018-07-31T11:56:22.457743: step 11625, loss 0.571351.
Train: 2018-07-31T11:56:22.629572: step 11626, loss 0.502707.
Train: 2018-07-31T11:56:22.785792: step 11627, loss 0.571326.
Train: 2018-07-31T11:56:22.957596: step 11628, loss 0.562707.
Train: 2018-07-31T11:56:23.145082: step 11629, loss 0.528193.
Train: 2018-07-31T11:56:23.316887: step 11630, loss 0.554011.
Test: 2018-07-31T11:56:23.785526: step 11630, loss 0.55678.
Train: 2018-07-31T11:56:23.957361: step 11631, loss 0.519312.
Train: 2018-07-31T11:56:24.129226: step 11632, loss 0.579974.
Train: 2018-07-31T11:56:24.316683: step 11633, loss 0.545144.
Train: 2018-07-31T11:56:24.488511: step 11634, loss 0.553807.
Train: 2018-07-31T11:56:24.660323: step 11635, loss 0.518717.
Train: 2018-07-31T11:56:24.847809: step 11636, loss 0.597657.
Train: 2018-07-31T11:56:25.019613: step 11637, loss 0.606535.
Train: 2018-07-31T11:56:25.191481: step 11638, loss 0.659523.
Train: 2018-07-31T11:56:25.363313: step 11639, loss 0.518359.
Train: 2018-07-31T11:56:25.535148: step 11640, loss 0.562439.
Test: 2018-07-31T11:56:26.003757: step 11640, loss 0.556471.
Train: 2018-07-31T11:56:26.191213: step 11641, loss 0.535952.
Train: 2018-07-31T11:56:26.363078: step 11642, loss 0.641905.
Train: 2018-07-31T11:56:26.550530: step 11643, loss 0.527097.
Train: 2018-07-31T11:56:26.722363: step 11644, loss 0.535909.
Train: 2018-07-31T11:56:26.894204: step 11645, loss 0.544712.
Train: 2018-07-31T11:56:27.066008: step 11646, loss 0.553523.
Train: 2018-07-31T11:56:27.253509: step 11647, loss 0.535799.
Train: 2018-07-31T11:56:27.425324: step 11648, loss 0.500291.
Train: 2018-07-31T11:56:27.597133: step 11649, loss 0.588982.
Train: 2018-07-31T11:56:27.769000: step 11650, loss 0.59792.
Test: 2018-07-31T11:56:28.253230: step 11650, loss 0.556287.
Train: 2018-07-31T11:56:28.425066: step 11651, loss 0.562299.
Train: 2018-07-31T11:56:28.596901: step 11652, loss 0.651471.
Train: 2018-07-31T11:56:28.768765: step 11653, loss 0.553367.
Train: 2018-07-31T11:56:28.940602: step 11654, loss 0.562265.
Train: 2018-07-31T11:56:29.128025: step 11655, loss 0.553355.
Train: 2018-07-31T11:56:29.299860: step 11656, loss 0.562243.
Train: 2018-07-31T11:56:29.471725: step 11657, loss 0.588907.
Train: 2018-07-31T11:56:29.643561: step 11658, loss 0.59776.
Train: 2018-07-31T11:56:29.815395: step 11659, loss 0.588828.
Train: 2018-07-31T11:56:29.987229: step 11660, loss 0.526773.
Test: 2018-07-31T11:56:30.471460: step 11660, loss 0.55622.
Train: 2018-07-31T11:56:30.643296: step 11661, loss 0.632986.
Train: 2018-07-31T11:56:30.830777: step 11662, loss 0.579845.
Train: 2018-07-31T11:56:31.002617: step 11663, loss 0.615044.
Train: 2018-07-31T11:56:31.174451: step 11664, loss 0.606099.
Train: 2018-07-31T11:56:31.346256: step 11665, loss 0.553423.
Train: 2018-07-31T11:56:31.533712: step 11666, loss 0.55345.
Train: 2018-07-31T11:56:31.705548: step 11667, loss 0.579596.
Train: 2018-07-31T11:56:31.877407: step 11668, loss 0.570869.
Train: 2018-07-31T11:56:32.049246: step 11669, loss 0.596858.
Train: 2018-07-31T11:56:32.221082: step 11670, loss 0.605422.
Test: 2018-07-31T11:56:32.689690: step 11670, loss 0.556383.
Train: 2018-07-31T11:56:32.877172: step 11671, loss 0.57944.
Train: 2018-07-31T11:56:33.048983: step 11672, loss 0.596592.
Train: 2018-07-31T11:56:33.220816: step 11673, loss 0.527959.
Train: 2018-07-31T11:56:33.392655: step 11674, loss 0.604987.
Train: 2018-07-31T11:56:33.580138: step 11675, loss 0.579307.
Train: 2018-07-31T11:56:33.751943: step 11676, loss 0.545245.
Train: 2018-07-31T11:56:33.923777: step 11677, loss 0.570763.
Train: 2018-07-31T11:56:34.095613: step 11678, loss 0.570756.
Train: 2018-07-31T11:56:34.267478: step 11679, loss 0.545329.
Train: 2018-07-31T11:56:34.454933: step 11680, loss 0.56227.
Test: 2018-07-31T11:56:34.939195: step 11680, loss 0.556548.
Train: 2018-07-31T11:56:35.157888: step 11681, loss 0.579196.
Train: 2018-07-31T11:56:35.329728: step 11682, loss 0.596113.
Train: 2018-07-31T11:56:35.517154: step 11683, loss 0.579169.
Train: 2018-07-31T11:56:35.689020: step 11684, loss 0.536896.
Train: 2018-07-31T11:56:35.860824: step 11685, loss 0.579144.
Train: 2018-07-31T11:56:36.032659: step 11686, loss 0.553777.
Train: 2018-07-31T11:56:36.204524: step 11687, loss 0.528391.
Train: 2018-07-31T11:56:36.376329: step 11688, loss 0.587588.
Train: 2018-07-31T11:56:36.563784: step 11689, loss 0.52827.
Train: 2018-07-31T11:56:36.735649: step 11690, loss 0.545159.
Test: 2018-07-31T11:56:37.204290: step 11690, loss 0.556363.
Train: 2018-07-31T11:56:37.376125: step 11691, loss 0.494041.
Train: 2018-07-31T11:56:37.547959: step 11692, loss 0.562058.
Train: 2018-07-31T11:56:37.735385: step 11693, loss 0.570583.
Train: 2018-07-31T11:56:37.907250: step 11694, loss 0.596356.
Train: 2018-07-31T11:56:38.079054: step 11695, loss 0.579178.
Train: 2018-07-31T11:56:38.266511: step 11696, loss 0.613701.
Train: 2018-07-31T11:56:38.438346: step 11697, loss 0.630993.
Train: 2018-07-31T11:56:38.610181: step 11698, loss 0.561906.
Train: 2018-07-31T11:56:38.797636: step 11699, loss 0.579151.
Train: 2018-07-31T11:56:38.969496: step 11700, loss 0.579135.
Test: 2018-07-31T11:56:39.453763: step 11700, loss 0.556075.
Train: 2018-07-31T11:56:40.203587: step 11701, loss 0.596347.
Train: 2018-07-31T11:56:40.375422: step 11702, loss 0.596305.
Train: 2018-07-31T11:56:40.562855: step 11703, loss 0.587662.
Train: 2018-07-31T11:56:40.734713: step 11704, loss 0.527597.
Train: 2018-07-31T11:56:40.906517: step 11705, loss 0.553324.
Train: 2018-07-31T11:56:41.078383: step 11706, loss 0.66467.
Train: 2018-07-31T11:56:41.250217: step 11707, loss 0.519165.
Train: 2018-07-31T11:56:41.422022: step 11708, loss 0.519204.
Train: 2018-07-31T11:56:41.609508: step 11709, loss 0.587503.
Train: 2018-07-31T11:56:41.781343: step 11710, loss 0.553334.
Test: 2018-07-31T11:56:42.249983: step 11710, loss 0.556097.
Train: 2018-07-31T11:56:42.421788: step 11711, loss 0.570402.
Train: 2018-07-31T11:56:42.609277: step 11712, loss 0.570392.
Train: 2018-07-31T11:56:42.781104: step 11713, loss 0.536196.
Train: 2018-07-31T11:56:42.968536: step 11714, loss 0.510488.
Train: 2018-07-31T11:56:43.140400: step 11715, loss 0.527495.
Train: 2018-07-31T11:56:43.327851: step 11716, loss 0.553157.
Train: 2018-07-31T11:56:43.499691: step 11717, loss 0.604831.
Train: 2018-07-31T11:56:43.671528: step 11718, loss 0.527147.
Train: 2018-07-31T11:56:43.843361: step 11719, loss 0.553012.
Train: 2018-07-31T11:56:44.030786: step 11720, loss 0.622418.
Test: 2018-07-31T11:56:44.499456: step 11720, loss 0.555764.
Train: 2018-07-31T11:56:44.671291: step 11721, loss 0.613782.
Train: 2018-07-31T11:56:44.858729: step 11722, loss 0.561619.
Train: 2018-07-31T11:56:45.030551: step 11723, loss 0.535522.
Train: 2018-07-31T11:56:45.202418: step 11724, loss 0.596403.
Train: 2018-07-31T11:56:45.374222: step 11725, loss 0.605103.
Train: 2018-07-31T11:56:45.546087: step 11726, loss 0.605069.
Train: 2018-07-31T11:56:45.717890: step 11727, loss 0.605009.
Train: 2018-07-31T11:56:45.905378: step 11728, loss 0.561577.
Train: 2018-07-31T11:56:46.077181: step 11729, loss 0.622167.
Train: 2018-07-31T11:56:46.249016: step 11730, loss 0.587485.
Test: 2018-07-31T11:56:46.717655: step 11730, loss 0.555787.
Train: 2018-07-31T11:56:46.889491: step 11731, loss 0.570206.
Train: 2018-07-31T11:56:47.061356: step 11732, loss 0.54443.
Train: 2018-07-31T11:56:47.233194: step 11733, loss 0.587333.
Train: 2018-07-31T11:56:47.404995: step 11734, loss 0.527389.
Train: 2018-07-31T11:56:47.592452: step 11735, loss 0.518863.
Train: 2018-07-31T11:56:47.764287: step 11736, loss 0.604374.
Train: 2018-07-31T11:56:47.936152: step 11737, loss 0.5787.
Train: 2018-07-31T11:56:48.107956: step 11738, loss 0.553047.
Train: 2018-07-31T11:56:48.279815: step 11739, loss 0.578677.
Train: 2018-07-31T11:56:48.451661: step 11740, loss 0.578666.
Test: 2018-07-31T11:56:48.935917: step 11740, loss 0.555808.
Train: 2018-07-31T11:56:49.123381: step 11741, loss 0.595736.
Train: 2018-07-31T11:56:49.295202: step 11742, loss 0.493299.
Train: 2018-07-31T11:56:49.467043: step 11743, loss 0.467597.
Train: 2018-07-31T11:56:49.638847: step 11744, loss 0.544382.
Train: 2018-07-31T11:56:49.810683: step 11745, loss 0.595856.
Train: 2018-07-31T11:56:49.982547: step 11746, loss 0.595909.
Train: 2018-07-31T11:56:50.154352: step 11747, loss 0.544175.
Train: 2018-07-31T11:56:50.326187: step 11748, loss 0.621916.
Train: 2018-07-31T11:56:50.513643: step 11749, loss 0.526798.
Train: 2018-07-31T11:56:50.685478: step 11750, loss 0.466103.
Test: 2018-07-31T11:56:51.169769: step 11750, loss 0.555483.
Train: 2018-07-31T11:56:51.341604: step 11751, loss 0.570037.
Train: 2018-07-31T11:56:51.513439: step 11752, loss 0.604899.
Train: 2018-07-31T11:56:51.700895: step 11753, loss 0.570035.
Train: 2018-07-31T11:56:51.872730: step 11754, loss 0.631249.
Train: 2018-07-31T11:56:52.044535: step 11755, loss 0.552532.
Train: 2018-07-31T11:56:52.232020: step 11756, loss 0.578766.
Train: 2018-07-31T11:56:52.403826: step 11757, loss 0.543759.
Train: 2018-07-31T11:56:52.591312: step 11758, loss 0.54374.
Train: 2018-07-31T11:56:52.763147: step 11759, loss 0.569996.
Train: 2018-07-31T11:56:52.934981: step 11760, loss 0.499853.
Test: 2018-07-31T11:56:53.403591: step 11760, loss 0.55528.
Train: 2018-07-31T11:56:53.575456: step 11761, loss 0.605127.
Train: 2018-07-31T11:56:53.762907: step 11762, loss 0.561196.
Train: 2018-07-31T11:56:53.934742: step 11763, loss 0.552384.
Train: 2018-07-31T11:56:54.106552: step 11764, loss 0.631641.
Train: 2018-07-31T11:56:54.278417: step 11765, loss 0.587579.
Train: 2018-07-31T11:56:54.450251: step 11766, loss 0.525969.
Train: 2018-07-31T11:56:54.637708: step 11767, loss 0.569946.
Train: 2018-07-31T11:56:54.809511: step 11768, loss 0.552345.
Train: 2018-07-31T11:56:54.981378: step 11769, loss 0.578726.
Train: 2018-07-31T11:56:55.168827: step 11770, loss 0.552328.
Test: 2018-07-31T11:56:55.637474: step 11770, loss 0.555179.
Train: 2018-07-31T11:56:55.809308: step 11771, loss 0.649075.
Train: 2018-07-31T11:56:55.996734: step 11772, loss 0.648927.
Train: 2018-07-31T11:56:56.168598: step 11773, loss 0.639899.
Train: 2018-07-31T11:56:56.340404: step 11774, loss 0.473964.
Train: 2018-07-31T11:56:56.527890: step 11775, loss 0.552428.
Train: 2018-07-31T11:56:56.699724: step 11776, loss 0.613233.
Train: 2018-07-31T11:56:56.871559: step 11777, loss 0.57846.
Train: 2018-07-31T11:56:57.027773: step 11778, loss 0.680422.
Train: 2018-07-31T11:56:57.215198: step 11779, loss 0.630008.
Train: 2018-07-31T11:56:57.387032: step 11780, loss 0.56976.
Test: 2018-07-31T11:56:57.855673: step 11780, loss 0.555487.
Train: 2018-07-31T11:56:58.043129: step 11781, loss 0.578276.
Train: 2018-07-31T11:56:58.214964: step 11782, loss 0.58672.
Train: 2018-07-31T11:56:58.386830: step 11783, loss 0.544424.
Train: 2018-07-31T11:56:58.574285: step 11784, loss 0.595017.
Train: 2018-07-31T11:56:58.746120: step 11785, loss 0.544605.
Train: 2018-07-31T11:56:58.917955: step 11786, loss 0.519565.
Train: 2018-07-31T11:56:59.089790: step 11787, loss 0.511251.
Train: 2018-07-31T11:56:59.277241: step 11788, loss 0.519573.
Train: 2018-07-31T11:56:59.449080: step 11789, loss 0.519471.
Train: 2018-07-31T11:56:59.620918: step 11790, loss 0.620131.
Test: 2018-07-31T11:57:00.089524: step 11790, loss 0.555615.
Train: 2018-07-31T11:57:00.277012: step 11791, loss 0.569706.
Train: 2018-07-31T11:57:00.448846: step 11792, loss 0.594969.
Train: 2018-07-31T11:57:00.620681: step 11793, loss 0.60341.
Train: 2018-07-31T11:57:00.792485: step 11794, loss 0.662434.
Train: 2018-07-31T11:57:00.964320: step 11795, loss 0.536002.
Train: 2018-07-31T11:57:01.136186: step 11796, loss 0.578076.
Train: 2018-07-31T11:57:01.308020: step 11797, loss 0.653692.
Train: 2018-07-31T11:57:01.479825: step 11798, loss 0.586429.
Train: 2018-07-31T11:57:01.651690: step 11799, loss 0.561309.
Train: 2018-07-31T11:57:01.839146: step 11800, loss 0.536294.
Test: 2018-07-31T11:57:02.307782: step 11800, loss 0.555707.
Train: 2018-07-31T11:57:03.057610: step 11801, loss 0.536321.
Train: 2018-07-31T11:57:03.229415: step 11802, loss 0.603014.
Train: 2018-07-31T11:57:03.416872: step 11803, loss 0.527989.
Train: 2018-07-31T11:57:03.588706: step 11804, loss 0.661366.
Train: 2018-07-31T11:57:03.760565: step 11805, loss 0.619607.
Train: 2018-07-31T11:57:03.932406: step 11806, loss 0.511491.
Train: 2018-07-31T11:57:04.119862: step 11807, loss 0.53643.
Train: 2018-07-31T11:57:04.307287: step 11808, loss 0.586251.
Train: 2018-07-31T11:57:04.479153: step 11809, loss 0.594554.
Train: 2018-07-31T11:57:04.650989: step 11810, loss 0.644382.
Test: 2018-07-31T11:57:05.119627: step 11810, loss 0.555736.
Train: 2018-07-31T11:57:05.291463: step 11811, loss 0.611087.
Train: 2018-07-31T11:57:05.478918: step 11812, loss 0.586181.
Train: 2018-07-31T11:57:05.650723: step 11813, loss 0.511863.
Train: 2018-07-31T11:57:05.806967: step 11814, loss 0.569641.
Train: 2018-07-31T11:57:05.978801: step 11815, loss 0.561389.
Train: 2018-07-31T11:57:06.150636: step 11816, loss 0.602617.
Train: 2018-07-31T11:57:06.338092: step 11817, loss 0.602597.
Train: 2018-07-31T11:57:06.509898: step 11818, loss 0.561388.
Train: 2018-07-31T11:57:06.681761: step 11819, loss 0.619008.
Train: 2018-07-31T11:57:06.869188: step 11820, loss 0.536744.
Test: 2018-07-31T11:57:07.337858: step 11820, loss 0.555856.
Train: 2018-07-31T11:57:07.525284: step 11821, loss 0.503877.
Train: 2018-07-31T11:57:07.697149: step 11822, loss 0.503761.
Train: 2018-07-31T11:57:07.868978: step 11823, loss 0.553056.
Train: 2018-07-31T11:57:08.040788: step 11824, loss 0.577811.
Train: 2018-07-31T11:57:08.228244: step 11825, loss 0.552891.
Train: 2018-07-31T11:57:08.400080: step 11826, loss 0.519488.
Train: 2018-07-31T11:57:08.571945: step 11827, loss 0.56944.
Train: 2018-07-31T11:57:08.743749: step 11828, loss 0.477062.
Train: 2018-07-31T11:57:08.931205: step 11829, loss 0.544059.
Train: 2018-07-31T11:57:09.103071: step 11830, loss 0.577846.
Test: 2018-07-31T11:57:09.587331: step 11830, loss 0.555058.
Train: 2018-07-31T11:57:09.759137: step 11831, loss 0.62904.
Train: 2018-07-31T11:57:09.930971: step 11832, loss 0.603549.
Train: 2018-07-31T11:57:10.102805: step 11833, loss 0.543606.
Train: 2018-07-31T11:57:10.274640: step 11834, loss 0.577907.
Train: 2018-07-31T11:57:10.462095: step 11835, loss 0.595135.
Train: 2018-07-31T11:57:10.633932: step 11836, loss 0.534822.
Train: 2018-07-31T11:57:10.821388: step 11837, loss 0.57793.
Train: 2018-07-31T11:57:10.993222: step 11838, loss 0.456881.
Train: 2018-07-31T11:57:11.180678: step 11839, loss 0.525904.
Train: 2018-07-31T11:57:11.352543: step 11840, loss 0.578006.
Test: 2018-07-31T11:57:11.821183: step 11840, loss 0.554662.
Train: 2018-07-31T11:57:11.992988: step 11841, loss 0.525598.
Train: 2018-07-31T11:57:12.180468: step 11842, loss 0.595629.
Train: 2018-07-31T11:57:12.352278: step 11843, loss 0.604506.
Train: 2018-07-31T11:57:12.524143: step 11844, loss 0.534073.
Train: 2018-07-31T11:57:12.695978: step 11845, loss 0.534006.
Train: 2018-07-31T11:57:12.867813: step 11846, loss 0.516229.
Train: 2018-07-31T11:57:13.039618: step 11847, loss 0.551594.
Train: 2018-07-31T11:57:13.211462: step 11848, loss 0.55156.
Train: 2018-07-31T11:57:13.383288: step 11849, loss 0.578314.
Train: 2018-07-31T11:57:13.570743: step 11850, loss 0.596243.
Test: 2018-07-31T11:57:14.039384: step 11850, loss 0.554399.
Train: 2018-07-31T11:57:14.226870: step 11851, loss 0.488771.
Train: 2018-07-31T11:57:14.398705: step 11852, loss 0.605347.
Train: 2018-07-31T11:57:14.570510: step 11853, loss 0.569429.
Train: 2018-07-31T11:57:14.742344: step 11854, loss 0.587436.
Train: 2018-07-31T11:57:14.914179: step 11855, loss 0.61445.
Train: 2018-07-31T11:57:15.101635: step 11856, loss 0.506432.
Train: 2018-07-31T11:57:15.273469: step 11857, loss 0.578407.
Train: 2018-07-31T11:57:15.445304: step 11858, loss 0.578396.
Train: 2018-07-31T11:57:15.617140: step 11859, loss 0.524418.
Train: 2018-07-31T11:57:15.789005: step 11860, loss 0.524405.
Test: 2018-07-31T11:57:16.273266: step 11860, loss 0.554304.
Train: 2018-07-31T11:57:16.460694: step 11861, loss 0.470359.
Train: 2018-07-31T11:57:16.632557: step 11862, loss 0.560378.
Train: 2018-07-31T11:57:16.804362: step 11863, loss 0.560377.
Train: 2018-07-31T11:57:16.976226: step 11864, loss 0.632839.
Train: 2018-07-31T11:57:17.148061: step 11865, loss 0.569429.
Train: 2018-07-31T11:57:17.319896: step 11866, loss 0.51507.
Train: 2018-07-31T11:57:17.507347: step 11867, loss 0.524099.
Train: 2018-07-31T11:57:17.679186: step 11868, loss 0.587583.
Train: 2018-07-31T11:57:17.850991: step 11869, loss 0.587591.
Train: 2018-07-31T11:57:18.022856: step 11870, loss 0.569421.
Test: 2018-07-31T11:57:18.491466: step 11870, loss 0.55421.
Train: 2018-07-31T11:57:18.663331: step 11871, loss 0.642012.
Train: 2018-07-31T11:57:18.850757: step 11872, loss 0.515029.
Train: 2018-07-31T11:57:19.022616: step 11873, loss 0.596501.
Train: 2018-07-31T11:57:19.194427: step 11874, loss 0.551259.
Train: 2018-07-31T11:57:19.366291: step 11875, loss 0.533217.
Train: 2018-07-31T11:57:19.538095: step 11876, loss 0.5152.
Train: 2018-07-31T11:57:19.709961: step 11877, loss 0.614355.
Train: 2018-07-31T11:57:19.897387: step 11878, loss 0.677334.
Train: 2018-07-31T11:57:20.069252: step 11879, loss 0.587173.
Train: 2018-07-31T11:57:20.241082: step 11880, loss 0.578122.
Test: 2018-07-31T11:57:20.709726: step 11880, loss 0.554201.
Train: 2018-07-31T11:57:20.897152: step 11881, loss 0.613718.
Train: 2018-07-31T11:57:21.069012: step 11882, loss 0.551332.
Train: 2018-07-31T11:57:21.240852: step 11883, loss 0.577905.
Train: 2018-07-31T11:57:21.412657: step 11884, loss 0.595477.
Train: 2018-07-31T11:57:21.584491: step 11885, loss 0.621697.
Train: 2018-07-31T11:57:21.771978: step 11886, loss 0.525251.
Train: 2018-07-31T11:57:21.943807: step 11887, loss 0.595085.
Train: 2018-07-31T11:57:22.115643: step 11888, loss 0.594971.
Train: 2018-07-31T11:57:22.287451: step 11889, loss 0.551614.
Train: 2018-07-31T11:57:22.459316: step 11890, loss 0.551658.
Test: 2018-07-31T11:57:22.943549: step 11890, loss 0.554489.
Train: 2018-07-31T11:57:23.115408: step 11891, loss 0.603282.
Train: 2018-07-31T11:57:23.287243: step 11892, loss 0.491732.
Train: 2018-07-31T11:57:23.459083: step 11893, loss 0.526062.
Train: 2018-07-31T11:57:23.630918: step 11894, loss 0.654483.
Train: 2018-07-31T11:57:23.802723: step 11895, loss 0.568862.
Train: 2018-07-31T11:57:23.974557: step 11896, loss 0.568856.
Train: 2018-07-31T11:57:24.146393: step 11897, loss 0.560333.
Train: 2018-07-31T11:57:24.318227: step 11898, loss 0.509283.
Train: 2018-07-31T11:57:24.490091: step 11899, loss 0.645442.
Train: 2018-07-31T11:57:24.661897: step 11900, loss 0.517833.
Test: 2018-07-31T11:57:25.146158: step 11900, loss 0.554592.
Train: 2018-07-31T11:57:25.942876: step 11901, loss 0.577327.
Train: 2018-07-31T11:57:26.130333: step 11902, loss 0.526331.
Train: 2018-07-31T11:57:26.302167: step 11903, loss 0.60283.
Train: 2018-07-31T11:57:26.473972: step 11904, loss 0.543291.
Train: 2018-07-31T11:57:26.645836: step 11905, loss 0.585816.
Train: 2018-07-31T11:57:26.817641: step 11906, loss 0.585812.
Train: 2018-07-31T11:57:26.989477: step 11907, loss 0.534742.
Train: 2018-07-31T11:57:27.161340: step 11908, loss 0.6369.
Train: 2018-07-31T11:57:27.333176: step 11909, loss 0.534736.
Train: 2018-07-31T11:57:27.504980: step 11910, loss 0.628323.
Test: 2018-07-31T11:57:27.989272: step 11910, loss 0.554525.
Train: 2018-07-31T11:57:28.161076: step 11911, loss 0.50927.
Train: 2018-07-31T11:57:28.348532: step 11912, loss 0.53475.
Train: 2018-07-31T11:57:28.535989: step 11913, loss 0.594267.
Train: 2018-07-31T11:57:28.707823: step 11914, loss 0.585757.
Train: 2018-07-31T11:57:28.879659: step 11915, loss 0.619795.
Train: 2018-07-31T11:57:29.051524: step 11916, loss 0.526211.
Train: 2018-07-31T11:57:29.238949: step 11917, loss 0.602726.
Train: 2018-07-31T11:57:29.410814: step 11918, loss 0.534724.
Train: 2018-07-31T11:57:29.582650: step 11919, loss 0.543209.
Train: 2018-07-31T11:57:29.754454: step 11920, loss 0.577199.
Test: 2018-07-31T11:57:30.223123: step 11920, loss 0.554438.
Train: 2018-07-31T11:57:30.410574: step 11921, loss 0.585704.
Train: 2018-07-31T11:57:30.582385: step 11922, loss 0.60272.
Train: 2018-07-31T11:57:30.754220: step 11923, loss 0.551664.
Train: 2018-07-31T11:57:30.926084: step 11924, loss 0.628198.
Train: 2018-07-31T11:57:31.097890: step 11925, loss 0.568664.
Train: 2018-07-31T11:57:31.285346: step 11926, loss 0.560176.
Train: 2018-07-31T11:57:31.457180: step 11927, loss 0.644947.
Train: 2018-07-31T11:57:31.629014: step 11928, loss 0.500995.
Train: 2018-07-31T11:57:31.785227: step 11929, loss 0.604716.
Train: 2018-07-31T11:57:31.972683: step 11930, loss 0.610862.
Test: 2018-07-31T11:57:32.441325: step 11930, loss 0.554535.
Train: 2018-07-31T11:57:32.613189: step 11931, loss 0.568648.
Train: 2018-07-31T11:57:32.800616: step 11932, loss 0.518169.
Train: 2018-07-31T11:57:32.972480: step 11933, loss 0.602286.
Train: 2018-07-31T11:57:33.144309: step 11934, loss 0.602253.
Train: 2018-07-31T11:57:33.331771: step 11935, loss 0.610596.
Train: 2018-07-31T11:57:33.503575: step 11936, loss 0.56027.
Train: 2018-07-31T11:57:33.675440: step 11937, loss 0.551926.
Train: 2018-07-31T11:57:33.847245: step 11938, loss 0.643816.
Train: 2018-07-31T11:57:34.034726: step 11939, loss 0.585318.
Train: 2018-07-31T11:57:34.206566: step 11940, loss 0.5936.
Test: 2018-07-31T11:57:34.690828: step 11940, loss 0.554787.
Train: 2018-07-31T11:57:34.862657: step 11941, loss 0.568674.
Train: 2018-07-31T11:57:35.050118: step 11942, loss 0.535588.
Train: 2018-07-31T11:57:35.221924: step 11943, loss 0.486029.
Train: 2018-07-31T11:57:35.393758: step 11944, loss 0.593494.
Train: 2018-07-31T11:57:35.565593: step 11945, loss 0.576938.
Train: 2018-07-31T11:57:35.737428: step 11946, loss 0.552077.
Train: 2018-07-31T11:57:35.909262: step 11947, loss 0.543752.
Train: 2018-07-31T11:57:36.081127: step 11948, loss 0.552.
Train: 2018-07-31T11:57:36.252963: step 11949, loss 0.568594.
Train: 2018-07-31T11:57:36.424766: step 11950, loss 0.526875.
Test: 2018-07-31T11:57:36.909030: step 11950, loss 0.554545.
Train: 2018-07-31T11:57:37.080887: step 11951, loss 0.551825.
Train: 2018-07-31T11:57:37.252729: step 11952, loss 0.493031.
Train: 2018-07-31T11:57:37.424557: step 11953, loss 0.602209.
Train: 2018-07-31T11:57:37.596367: step 11954, loss 0.627666.
Train: 2018-07-31T11:57:37.768202: step 11955, loss 0.602352.
Train: 2018-07-31T11:57:37.940036: step 11956, loss 0.543031.
Train: 2018-07-31T11:57:38.111871: step 11957, loss 0.542984.
Train: 2018-07-31T11:57:38.283706: step 11958, loss 0.5089.
Train: 2018-07-31T11:57:38.471162: step 11959, loss 0.50871.
Train: 2018-07-31T11:57:38.642997: step 11960, loss 0.542726.
Test: 2018-07-31T11:57:39.111636: step 11960, loss 0.554013.
Train: 2018-07-31T11:57:39.283502: step 11961, loss 0.611424.
Train: 2018-07-31T11:57:39.455307: step 11962, loss 0.516663.
Train: 2018-07-31T11:57:39.627172: step 11963, loss 0.559758.
Train: 2018-07-31T11:57:39.798976: step 11964, loss 0.568415.
Train: 2018-07-31T11:57:39.986433: step 11965, loss 0.611967.
Train: 2018-07-31T11:57:40.158267: step 11966, loss 0.481176.
Train: 2018-07-31T11:57:40.330132: step 11967, loss 0.612185.
Train: 2018-07-31T11:57:40.501937: step 11968, loss 0.550891.
Train: 2018-07-31T11:57:40.673802: step 11969, loss 0.542076.
Train: 2018-07-31T11:57:40.845637: step 11970, loss 0.647676.
Test: 2018-07-31T11:57:41.329901: step 11970, loss 0.553685.
Train: 2018-07-31T11:57:41.517324: step 11971, loss 0.550821.
Train: 2018-07-31T11:57:41.689158: step 11972, loss 0.594861.
Train: 2018-07-31T11:57:41.861023: step 11973, loss 0.559614.
Train: 2018-07-31T11:57:42.032828: step 11974, loss 0.603641.
Train: 2018-07-31T11:57:42.204662: step 11975, loss 0.550806.
Train: 2018-07-31T11:57:42.376528: step 11976, loss 0.585978.
Train: 2018-07-31T11:57:42.563984: step 11977, loss 0.621081.
Train: 2018-07-31T11:57:42.735813: step 11978, loss 0.559598.
Train: 2018-07-31T11:57:42.907654: step 11979, loss 0.577097.
Train: 2018-07-31T11:57:43.079459: step 11980, loss 0.559602.
Test: 2018-07-31T11:57:43.563749: step 11980, loss 0.55372.
Train: 2018-07-31T11:57:43.735554: step 11981, loss 0.550885.
Train: 2018-07-31T11:57:43.923010: step 11982, loss 0.53348.
Train: 2018-07-31T11:57:44.094846: step 11983, loss 0.559599.
Train: 2018-07-31T11:57:44.282301: step 11984, loss 0.550893.
Train: 2018-07-31T11:57:44.469757: step 11985, loss 0.559588.
Train: 2018-07-31T11:57:44.641616: step 11986, loss 0.620502.
Train: 2018-07-31T11:57:44.813428: step 11987, loss 0.533498.
Train: 2018-07-31T11:57:44.985296: step 11988, loss 0.594344.
Train: 2018-07-31T11:57:45.157127: step 11989, loss 0.56826.
Train: 2018-07-31T11:57:45.328962: step 11990, loss 0.524862.
Test: 2018-07-31T11:57:45.813192: step 11990, loss 0.553711.
Train: 2018-07-31T11:57:45.985028: step 11991, loss 0.602961.
Train: 2018-07-31T11:57:46.156893: step 11992, loss 0.5162.
Train: 2018-07-31T11:57:46.344319: step 11993, loss 0.602939.
Train: 2018-07-31T11:57:46.516178: step 11994, loss 0.542206.
Train: 2018-07-31T11:57:46.687987: step 11995, loss 0.62894.
Train: 2018-07-31T11:57:46.875445: step 11996, loss 0.585539.
Train: 2018-07-31T11:57:47.047309: step 11997, loss 0.602808.
Train: 2018-07-31T11:57:47.219114: step 11998, loss 0.516392.
Train: 2018-07-31T11:57:47.390949: step 11999, loss 0.60269.
Train: 2018-07-31T11:57:47.562783: step 12000, loss 0.602631.
Test: 2018-07-31T11:57:48.047044: step 12000, loss 0.553775.
Train: 2018-07-31T11:57:48.781247: step 12001, loss 0.56817.
Train: 2018-07-31T11:57:48.953083: step 12002, loss 0.525259.
Train: 2018-07-31T11:57:49.124917: step 12003, loss 0.551009.
Train: 2018-07-31T11:57:49.296752: step 12004, loss 0.559581.
Train: 2018-07-31T11:57:49.484208: step 12005, loss 0.49101.
Train: 2018-07-31T11:57:49.656043: step 12006, loss 0.619641.
Train: 2018-07-31T11:57:49.827903: step 12007, loss 0.55956.
Train: 2018-07-31T11:57:49.999744: step 12008, loss 0.525199.
Train: 2018-07-31T11:57:50.171572: step 12009, loss 0.576748.
Train: 2018-07-31T11:57:50.343412: step 12010, loss 0.654213.
Test: 2018-07-31T11:57:50.827673: step 12010, loss 0.553783.
Train: 2018-07-31T11:57:50.999478: step 12011, loss 0.57678.
Train: 2018-07-31T11:57:51.186935: step 12012, loss 0.542063.
Train: 2018-07-31T11:57:51.358799: step 12013, loss 0.55965.
Train: 2018-07-31T11:57:51.530635: step 12014, loss 0.559875.
Train: 2018-07-31T11:57:51.702471: step 12015, loss 0.509807.
Train: 2018-07-31T11:57:51.874304: step 12016, loss 0.628535.
Train: 2018-07-31T11:57:52.046108: step 12017, loss 0.602808.
Train: 2018-07-31T11:57:52.217974: step 12018, loss 0.491211.
Train: 2018-07-31T11:57:52.389777: step 12019, loss 0.559963.
Train: 2018-07-31T11:57:52.561613: step 12020, loss 0.551412.
Test: 2018-07-31T11:57:53.045875: step 12020, loss 0.554246.
Train: 2018-07-31T11:57:53.217709: step 12021, loss 0.603108.
Train: 2018-07-31T11:57:53.405165: step 12022, loss 0.534257.
Train: 2018-07-31T11:57:53.577030: step 12023, loss 0.542883.
Train: 2018-07-31T11:57:53.748835: step 12024, loss 0.568774.
Train: 2018-07-31T11:57:53.920670: step 12025, loss 0.594719.
Train: 2018-07-31T11:57:54.092504: step 12026, loss 0.525576.
Train: 2018-07-31T11:57:54.279990: step 12027, loss 0.568812.
Train: 2018-07-31T11:57:54.451825: step 12028, loss 0.638136.
Train: 2018-07-31T11:57:54.623630: step 12029, loss 0.534153.
Train: 2018-07-31T11:57:54.795465: step 12030, loss 0.490809.
Test: 2018-07-31T11:57:55.279727: step 12030, loss 0.554242.
Train: 2018-07-31T11:57:55.451586: step 12031, loss 0.568777.
Train: 2018-07-31T11:57:55.623396: step 12032, loss 0.542688.
Train: 2018-07-31T11:57:55.795254: step 12033, loss 0.586155.
Train: 2018-07-31T11:57:55.967095: step 12034, loss 0.568722.
Train: 2018-07-31T11:57:56.138930: step 12035, loss 0.664667.
Train: 2018-07-31T11:57:56.310765: step 12036, loss 0.54252.
Train: 2018-07-31T11:57:56.482607: step 12037, loss 0.542502.
Train: 2018-07-31T11:57:56.654405: step 12038, loss 0.594745.
Train: 2018-07-31T11:57:56.826239: step 12039, loss 0.568578.
Train: 2018-07-31T11:57:56.998074: step 12040, loss 0.61206.
Test: 2018-07-31T11:57:57.482365: step 12040, loss 0.553959.
Train: 2018-07-31T11:57:57.654171: step 12041, loss 0.646731.
Train: 2018-07-31T11:57:57.826005: step 12042, loss 0.542476.
Train: 2018-07-31T11:57:58.013461: step 12043, loss 0.525197.
Train: 2018-07-31T11:57:58.185296: step 12044, loss 0.542495.
Train: 2018-07-31T11:57:58.357131: step 12045, loss 0.611573.
Train: 2018-07-31T11:57:58.528965: step 12046, loss 0.507984.
Train: 2018-07-31T11:57:58.700830: step 12047, loss 0.542463.
Train: 2018-07-31T11:57:58.872634: step 12048, loss 0.585572.
Train: 2018-07-31T11:57:59.044470: step 12049, loss 0.611437.
Train: 2018-07-31T11:57:59.216304: step 12050, loss 0.551025.
Test: 2018-07-31T11:57:59.684945: step 12050, loss 0.553813.
Train: 2018-07-31T11:57:59.872401: step 12051, loss 0.525155.
Train: 2018-07-31T11:58:00.044261: step 12052, loss 0.663069.
Train: 2018-07-31T11:58:00.200448: step 12053, loss 0.559597.
Train: 2018-07-31T11:58:00.387929: step 12054, loss 0.585387.
Train: 2018-07-31T11:58:00.559739: step 12055, loss 0.550993.
Train: 2018-07-31T11:58:00.731605: step 12056, loss 0.568152.
Train: 2018-07-31T11:58:00.903410: step 12057, loss 0.516697.
Train: 2018-07-31T11:58:01.090896: step 12058, loss 0.508094.
Train: 2018-07-31T11:58:01.262730: step 12059, loss 0.516585.
Train: 2018-07-31T11:58:01.418944: step 12060, loss 0.559488.
Test: 2018-07-31T11:58:01.903205: step 12060, loss 0.553639.
Train: 2018-07-31T11:58:02.075009: step 12061, loss 0.576705.
Train: 2018-07-31T11:58:02.246844: step 12062, loss 0.62854.
Train: 2018-07-31T11:58:02.418709: step 12063, loss 0.611268.
Train: 2018-07-31T11:58:02.606135: step 12064, loss 0.507585.
Train: 2018-07-31T11:58:02.777970: step 12065, loss 0.542107.
Train: 2018-07-31T11:58:02.934214: step 12066, loss 0.559388.
Train: 2018-07-31T11:58:03.121640: step 12067, loss 0.585342.
Train: 2018-07-31T11:58:03.293474: step 12068, loss 0.620009.
Train: 2018-07-31T11:58:03.465340: step 12069, loss 0.472719.
Train: 2018-07-31T11:58:03.637174: step 12070, loss 0.602691.
Test: 2018-07-31T11:58:04.121405: step 12070, loss 0.553459.
Train: 2018-07-31T11:58:04.293271: step 12071, loss 0.567991.
Train: 2018-07-31T11:58:04.465106: step 12072, loss 0.611389.
Train: 2018-07-31T11:58:04.621289: step 12073, loss 0.5593.
Train: 2018-07-31T11:58:04.808744: step 12074, loss 0.585315.
Train: 2018-07-31T11:58:04.980610: step 12075, loss 0.619964.
Train: 2018-07-31T11:58:05.168061: step 12076, loss 0.559295.
Train: 2018-07-31T11:58:05.339900: step 12077, loss 0.550655.
Train: 2018-07-31T11:58:05.511741: step 12078, loss 0.550662.
Train: 2018-07-31T11:58:05.683541: step 12079, loss 0.59381.
Train: 2018-07-31T11:58:05.839754: step 12080, loss 0.641478.
Test: 2018-07-31T11:58:06.324048: step 12080, loss 0.553499.
Train: 2018-07-31T11:58:06.495849: step 12081, loss 0.567904.
Train: 2018-07-31T11:58:06.667685: step 12082, loss 0.636562.
Train: 2018-07-31T11:58:06.839520: step 12083, loss 0.559333.
Train: 2018-07-31T11:58:07.011355: step 12084, loss 0.508153.
Train: 2018-07-31T11:58:07.167568: step 12085, loss 0.584923.
Train: 2018-07-31T11:58:07.339432: step 12086, loss 0.533833.
Train: 2018-07-31T11:58:07.511267: step 12087, loss 0.559629.
Train: 2018-07-31T11:58:07.683102: step 12088, loss 0.559381.
Train: 2018-07-31T11:58:07.854906: step 12089, loss 0.610441.
Train: 2018-07-31T11:58:08.026742: step 12090, loss 0.542533.
Test: 2018-07-31T11:58:08.511033: step 12090, loss 0.553885.
Train: 2018-07-31T11:58:08.682868: step 12091, loss 0.551123.
Train: 2018-07-31T11:58:08.854672: step 12092, loss 0.576695.
Train: 2018-07-31T11:58:09.026506: step 12093, loss 0.525838.
Train: 2018-07-31T11:58:09.198366: step 12094, loss 0.559894.
Train: 2018-07-31T11:58:09.370178: step 12095, loss 0.483395.
Train: 2018-07-31T11:58:09.542012: step 12096, loss 0.551492.
Train: 2018-07-31T11:58:09.713877: step 12097, loss 0.525853.
Train: 2018-07-31T11:58:09.885681: step 12098, loss 0.637312.
Train: 2018-07-31T11:58:10.057516: step 12099, loss 0.525744.
Train: 2018-07-31T11:58:10.213760: step 12100, loss 0.611809.
Test: 2018-07-31T11:58:10.698020: step 12100, loss 0.554323.
Train: 2018-07-31T11:58:11.447816: step 12101, loss 0.611879.
Train: 2018-07-31T11:58:11.650923: step 12102, loss 0.560148.
Train: 2018-07-31T11:58:11.822761: step 12103, loss 0.611909.
Train: 2018-07-31T11:58:11.994563: step 12104, loss 0.491152.
Train: 2018-07-31T11:58:12.166396: step 12105, loss 0.516954.
Train: 2018-07-31T11:58:12.334122: step 12106, loss 0.637893.
Train: 2018-07-31T11:58:12.505955: step 12107, loss 0.681129.
Train: 2018-07-31T11:58:12.677790: step 12108, loss 0.560014.
Train: 2018-07-31T11:58:12.849626: step 12109, loss 0.534125.
Train: 2018-07-31T11:58:13.005840: step 12110, loss 0.611628.
Test: 2018-07-31T11:58:13.490130: step 12110, loss 0.55411.
Train: 2018-07-31T11:58:13.661935: step 12111, loss 0.688919.
Train: 2018-07-31T11:58:13.833771: step 12112, loss 0.611312.
Train: 2018-07-31T11:58:14.005635: step 12113, loss 0.508681.
Train: 2018-07-31T11:58:14.177439: step 12114, loss 0.602428.
Train: 2018-07-31T11:58:14.349304: step 12115, loss 0.610775.
Train: 2018-07-31T11:58:14.521142: step 12116, loss 0.568297.
Train: 2018-07-31T11:58:14.692943: step 12117, loss 0.559828.
Train: 2018-07-31T11:58:14.864778: step 12118, loss 0.610293.
Train: 2018-07-31T11:58:15.021017: step 12119, loss 0.568198.
Train: 2018-07-31T11:58:15.192826: step 12120, loss 0.559803.
Test: 2018-07-31T11:58:15.677089: step 12120, loss 0.554159.
Train: 2018-07-31T11:58:15.848924: step 12121, loss 0.559793.
Train: 2018-07-31T11:58:16.005167: step 12122, loss 0.601459.
Train: 2018-07-31T11:58:16.176971: step 12123, loss 0.543132.
Train: 2018-07-31T11:58:16.348806: step 12124, loss 0.576376.
Train: 2018-07-31T11:58:16.520671: step 12125, loss 0.518231.
Train: 2018-07-31T11:58:16.692505: step 12126, loss 0.576314.
Train: 2018-07-31T11:58:16.864310: step 12127, loss 0.551372.
Train: 2018-07-31T11:58:17.036145: step 12128, loss 0.509782.
Train: 2018-07-31T11:58:17.208007: step 12129, loss 0.509637.
Train: 2018-07-31T11:58:17.364218: step 12130, loss 0.567873.
Test: 2018-07-31T11:58:17.848485: step 12130, loss 0.553813.
Train: 2018-07-31T11:58:18.020290: step 12131, loss 0.559463.
Train: 2018-07-31T11:58:18.192154: step 12132, loss 0.567803.
Train: 2018-07-31T11:58:18.363958: step 12133, loss 0.576191.
Train: 2018-07-31T11:58:18.535793: step 12134, loss 0.593058.
Train: 2018-07-31T11:58:18.707628: step 12135, loss 0.567726.
Train: 2018-07-31T11:58:18.863872: step 12136, loss 0.525391.
Train: 2018-07-31T11:58:19.035701: step 12137, loss 0.550722.
Train: 2018-07-31T11:58:19.191920: step 12138, loss 0.567667.
Train: 2018-07-31T11:58:19.363755: step 12139, loss 0.593212.
Train: 2018-07-31T11:58:19.535559: step 12140, loss 0.507907.
Test: 2018-07-31T11:58:20.019852: step 12140, loss 0.553298.
Train: 2018-07-31T11:58:20.191691: step 12141, loss 0.584733.
Train: 2018-07-31T11:58:20.347870: step 12142, loss 0.559042.
Train: 2018-07-31T11:58:20.519703: step 12143, loss 0.567603.
Train: 2018-07-31T11:58:20.691538: step 12144, loss 0.524585.
Train: 2018-07-31T11:58:20.863400: step 12145, loss 0.645184.
Train: 2018-07-31T11:58:21.035242: step 12146, loss 0.515817.
Train: 2018-07-31T11:58:21.207044: step 12147, loss 0.584853.
Train: 2018-07-31T11:58:21.378909: step 12148, loss 0.541621.
Train: 2018-07-31T11:58:21.535092: step 12149, loss 0.584883.
Train: 2018-07-31T11:58:21.706927: step 12150, loss 0.524219.
Test: 2018-07-31T11:58:22.191218: step 12150, loss 0.553015.
Train: 2018-07-31T11:58:22.363022: step 12151, loss 0.498107.
Train: 2018-07-31T11:58:22.519267: step 12152, loss 0.558843.
Train: 2018-07-31T11:58:22.691101: step 12153, loss 0.541402.
Train: 2018-07-31T11:58:22.862906: step 12154, loss 0.532643.
Train: 2018-07-31T11:58:23.034741: step 12155, loss 0.558939.
Train: 2018-07-31T11:58:23.206605: step 12156, loss 0.506235.
Train: 2018-07-31T11:58:23.378410: step 12157, loss 0.559083.
Train: 2018-07-31T11:58:23.534657: step 12158, loss 0.568014.
Train: 2018-07-31T11:58:23.706458: step 12159, loss 0.550361.
Train: 2018-07-31T11:58:23.878323: step 12160, loss 0.470331.
Test: 2018-07-31T11:58:24.362554: step 12160, loss 0.553361.
Train: 2018-07-31T11:58:24.534425: step 12161, loss 0.577253.
Train: 2018-07-31T11:58:24.706224: step 12162, loss 0.514676.
Train: 2018-07-31T11:58:24.878058: step 12163, loss 0.604479.
Train: 2018-07-31T11:58:25.049918: step 12164, loss 0.550571.
Train: 2018-07-31T11:58:25.206137: step 12165, loss 0.577705.
Train: 2018-07-31T11:58:25.377971: step 12166, loss 0.523486.
Train: 2018-07-31T11:58:25.534194: step 12167, loss 0.614115.
Train: 2018-07-31T11:58:25.706019: step 12168, loss 0.577865.
Train: 2018-07-31T11:58:25.877851: step 12169, loss 0.568813.
Train: 2018-07-31T11:58:26.034072: step 12170, loss 0.614199.
Test: 2018-07-31T11:58:26.518329: step 12170, loss 0.553588.
Train: 2018-07-31T11:58:26.690134: step 12171, loss 0.550638.
Train: 2018-07-31T11:58:26.862003: step 12172, loss 0.53249.
Train: 2018-07-31T11:58:27.033838: step 12173, loss 0.523403.
Train: 2018-07-31T11:58:27.190017: step 12174, loss 0.487103.
Train: 2018-07-31T11:58:27.361851: step 12175, loss 0.550495.
Train: 2018-07-31T11:58:27.533721: step 12176, loss 0.523133.
Train: 2018-07-31T11:58:27.689900: step 12177, loss 0.577698.
Train: 2018-07-31T11:58:27.861765: step 12178, loss 0.541273.
Train: 2018-07-31T11:58:28.033570: step 12179, loss 0.595936.
Train: 2018-07-31T11:58:28.189783: step 12180, loss 0.495543.
Test: 2018-07-31T11:58:28.674075: step 12180, loss 0.553348.
Train: 2018-07-31T11:58:28.845879: step 12181, loss 0.550375.
Train: 2018-07-31T11:58:29.017713: step 12182, loss 0.605371.
Train: 2018-07-31T11:58:29.189579: step 12183, loss 0.523062.
Train: 2018-07-31T11:58:29.345793: step 12184, loss 0.596437.
Train: 2018-07-31T11:58:29.517630: step 12185, loss 0.624012.
Train: 2018-07-31T11:58:29.689432: step 12186, loss 0.550801.
Train: 2018-07-31T11:58:29.861267: step 12187, loss 0.560025.
Train: 2018-07-31T11:58:30.017480: step 12188, loss 0.587482.
Train: 2018-07-31T11:58:30.189345: step 12189, loss 0.605725.
Train: 2018-07-31T11:58:30.345528: step 12190, loss 0.614757.
Test: 2018-07-31T11:58:30.829791: step 12190, loss 0.554082.
Train: 2018-07-31T11:58:31.001625: step 12191, loss 0.551133.
Train: 2018-07-31T11:58:31.157839: step 12192, loss 0.569264.
Train: 2018-07-31T11:58:31.329674: step 12193, loss 0.57827.
Train: 2018-07-31T11:58:31.501507: step 12194, loss 0.578221.
Train: 2018-07-31T11:58:31.657721: step 12195, loss 0.587134.
Train: 2018-07-31T11:58:31.829555: step 12196, loss 0.613869.
Train: 2018-07-31T11:58:32.001390: step 12197, loss 0.622562.
Train: 2018-07-31T11:58:32.173226: step 12198, loss 0.640004.
Train: 2018-07-31T11:58:32.329439: step 12199, loss 0.613084.
Train: 2018-07-31T11:58:32.501303: step 12200, loss 0.61276.
Test: 2018-07-31T11:58:32.969914: step 12200, loss 0.554242.
Train: 2018-07-31T11:58:33.751011: step 12201, loss 0.621192.
Train: 2018-07-31T11:58:33.907228: step 12202, loss 0.560121.
Train: 2018-07-31T11:58:34.079030: step 12203, loss 0.560121.
Train: 2018-07-31T11:58:34.250889: step 12204, loss 0.551434.
Train: 2018-07-31T11:58:34.422698: step 12205, loss 0.611281.
Train: 2018-07-31T11:58:34.594534: step 12206, loss 0.543164.
Train: 2018-07-31T11:58:34.750778: step 12207, loss 0.517877.
Train: 2018-07-31T11:58:34.922606: step 12208, loss 0.534847.
Train: 2018-07-31T11:58:35.078795: step 12209, loss 0.610688.
Train: 2018-07-31T11:58:35.250629: step 12210, loss 0.518109.
Test: 2018-07-31T11:58:35.734890: step 12210, loss 0.55446.
Train: 2018-07-31T11:58:35.953620: step 12211, loss 0.585333.
Train: 2018-07-31T11:58:36.125456: step 12212, loss 0.593691.
Train: 2018-07-31T11:58:36.297260: step 12213, loss 0.559779.
Train: 2018-07-31T11:58:36.469119: step 12214, loss 0.610365.
Train: 2018-07-31T11:58:36.625309: step 12215, loss 0.558122.
Train: 2018-07-31T11:58:36.797142: step 12216, loss 0.507821.
Train: 2018-07-31T11:58:36.969007: step 12217, loss 0.565776.
Train: 2018-07-31T11:58:37.140813: step 12218, loss 0.618241.
Train: 2018-07-31T11:58:37.312648: step 12219, loss 0.610335.
Train: 2018-07-31T11:58:37.468890: step 12220, loss 0.597065.
Test: 2018-07-31T11:58:37.953122: step 12220, loss 0.554786.
Train: 2018-07-31T11:58:38.140578: step 12221, loss 0.677223.
Train: 2018-07-31T11:58:38.296817: step 12222, loss 0.568931.
Train: 2018-07-31T11:58:38.468657: step 12223, loss 0.61887.
Train: 2018-07-31T11:58:38.640461: step 12224, loss 0.552783.
Train: 2018-07-31T11:58:38.812296: step 12225, loss 0.643676.
Train: 2018-07-31T11:58:38.968539: step 12226, loss 0.487548.
Train: 2018-07-31T11:58:39.140378: step 12227, loss 0.586174.
Train: 2018-07-31T11:58:39.312209: step 12228, loss 0.52404.
Train: 2018-07-31T11:58:39.468392: step 12229, loss 0.555375.
Train: 2018-07-31T11:58:39.640227: step 12230, loss 0.603943.
Test: 2018-07-31T11:58:40.124489: step 12230, loss 0.556384.
Train: 2018-07-31T11:58:40.280701: step 12231, loss 0.531366.
Train: 2018-07-31T11:58:40.436945: step 12232, loss 0.566014.
Train: 2018-07-31T11:58:40.608749: step 12233, loss 0.586107.
Train: 2018-07-31T11:58:40.780585: step 12234, loss 0.517017.
Train: 2018-07-31T11:58:40.936798: step 12235, loss 0.605129.
Train: 2018-07-31T11:58:41.108666: step 12236, loss 0.570417.
Train: 2018-07-31T11:58:41.280492: step 12237, loss 0.587955.
Train: 2018-07-31T11:58:41.452332: step 12238, loss 0.521599.
Train: 2018-07-31T11:58:41.624138: step 12239, loss 0.546745.
Train: 2018-07-31T11:58:41.780350: step 12240, loss 0.524088.
Test: 2018-07-31T11:58:42.264612: step 12240, loss 0.557559.
Train: 2018-07-31T11:58:42.420856: step 12241, loss 0.560193.
Train: 2018-07-31T11:58:42.592691: step 12242, loss 0.505332.
Train: 2018-07-31T11:58:42.764525: step 12243, loss 0.563428.
Train: 2018-07-31T11:58:42.936360: step 12244, loss 0.580263.
Train: 2018-07-31T11:58:43.108195: step 12245, loss 0.563671.
Train: 2018-07-31T11:58:43.280033: step 12246, loss 0.580594.
Train: 2018-07-31T11:58:43.451858: step 12247, loss 0.639821.
Train: 2018-07-31T11:58:43.639323: step 12248, loss 0.572342.
Train: 2018-07-31T11:58:43.795504: step 12249, loss 0.606265.
Train: 2018-07-31T11:58:43.967369: step 12250, loss 0.555442.
Test: 2018-07-31T11:58:44.451630: step 12250, loss 0.55817.
Train: 2018-07-31T11:58:44.623465: step 12251, loss 0.521464.
Train: 2018-07-31T11:58:44.795300: step 12252, loss 0.623366.
Train: 2018-07-31T11:58:44.967104: step 12253, loss 0.538243.
Train: 2018-07-31T11:58:45.138971: step 12254, loss 0.563688.
Train: 2018-07-31T11:58:45.310804: step 12255, loss 0.589194.
Train: 2018-07-31T11:58:45.482608: step 12256, loss 0.546385.
Train: 2018-07-31T11:58:45.654477: step 12257, loss 0.597584.
Train: 2018-07-31T11:58:45.826309: step 12258, loss 0.623184.
Train: 2018-07-31T11:58:45.982492: step 12259, loss 0.614489.
Train: 2018-07-31T11:58:46.154360: step 12260, loss 0.537337.
Test: 2018-07-31T11:58:46.638617: step 12260, loss 0.557106.
Train: 2018-07-31T11:58:46.810453: step 12261, loss 0.537221.
Train: 2018-07-31T11:58:46.982282: step 12262, loss 0.596957.
Train: 2018-07-31T11:58:47.138505: step 12263, loss 0.596908.
Train: 2018-07-31T11:58:47.310306: step 12264, loss 0.631117.
Train: 2018-07-31T11:58:47.482140: step 12265, loss 0.656706.
Train: 2018-07-31T11:58:47.638387: step 12266, loss 0.528911.
Train: 2018-07-31T11:58:47.810219: step 12267, loss 0.546109.
Train: 2018-07-31T11:58:47.982024: step 12268, loss 0.571718.
Train: 2018-07-31T11:58:48.138261: step 12269, loss 0.520993.
Train: 2018-07-31T11:58:48.341344: step 12270, loss 0.538037.
Test: 2018-07-31T11:58:48.809953: step 12270, loss 0.557809.
Train: 2018-07-31T11:58:48.981789: step 12271, loss 0.529604.
Train: 2018-07-31T11:58:49.153623: step 12272, loss 0.546578.
Train: 2018-07-31T11:58:49.325459: step 12273, loss 0.572091.
Train: 2018-07-31T11:58:49.497323: step 12274, loss 0.589149.
Train: 2018-07-31T11:58:49.653508: step 12275, loss 0.572074.
Train: 2018-07-31T11:58:49.825342: step 12276, loss 0.572033.
Train: 2018-07-31T11:58:49.997176: step 12277, loss 0.597652.
Train: 2018-07-31T11:58:50.169041: step 12278, loss 0.494829.
Train: 2018-07-31T11:58:50.325255: step 12279, loss 0.571814.
Train: 2018-07-31T11:58:50.497061: step 12280, loss 0.62329.
Test: 2018-07-31T11:58:50.981320: step 12280, loss 0.557211.
Train: 2018-07-31T11:58:51.137566: step 12281, loss 0.588815.
Train: 2018-07-31T11:58:51.309399: step 12282, loss 0.562904.
Train: 2018-07-31T11:58:51.481234: step 12283, loss 0.614417.
Train: 2018-07-31T11:58:51.637418: step 12284, loss 0.588111.
Train: 2018-07-31T11:58:51.809282: step 12285, loss 0.579932.
Train: 2018-07-31T11:58:51.981086: step 12286, loss 0.614459.
Train: 2018-07-31T11:58:52.152922: step 12287, loss 0.589045.
Train: 2018-07-31T11:58:52.309135: step 12288, loss 0.597779.
Train: 2018-07-31T11:58:52.480969: step 12289, loss 0.598237.
Train: 2018-07-31T11:58:52.652804: step 12290, loss 0.547698.
Test: 2018-07-31T11:58:53.137065: step 12290, loss 0.559743.
Train: 2018-07-31T11:58:53.308901: step 12291, loss 0.588573.
Train: 2018-07-31T11:58:53.480766: step 12292, loss 0.598083.
Train: 2018-07-31T11:58:53.636975: step 12293, loss 1.04657.
Train: 2018-07-31T11:58:53.808815: step 12294, loss 0.525606.
Train: 2018-07-31T11:58:53.980618: step 12295, loss 0.54303.
Train: 2018-07-31T11:58:54.136832: step 12296, loss 0.54463.
Train: 2018-07-31T11:58:54.308666: step 12297, loss 0.520547.
Train: 2018-07-31T11:58:54.480532: step 12298, loss 0.564127.
Train: 2018-07-31T11:58:54.652336: step 12299, loss 0.540012.
Train: 2018-07-31T11:58:54.824201: step 12300, loss 0.55821.
Test: 2018-07-31T11:58:55.308463: step 12300, loss 0.570642.
Train: 2018-07-31T11:58:56.042666: step 12301, loss 0.584925.
Train: 2018-07-31T11:58:56.214472: step 12302, loss 0.620207.
Train: 2018-07-31T11:58:56.386329: step 12303, loss 0.621201.
Train: 2018-07-31T11:58:56.542520: step 12304, loss 0.562003.
Train: 2018-07-31T11:58:56.714353: step 12305, loss 0.648446.
Train: 2018-07-31T11:58:56.886218: step 12306, loss 0.57167.
Train: 2018-07-31T11:58:57.058023: step 12307, loss 0.580596.
Train: 2018-07-31T11:58:57.229888: step 12308, loss 0.589398.
Train: 2018-07-31T11:58:57.386095: step 12309, loss 0.666852.
Train: 2018-07-31T11:58:57.557905: step 12310, loss 0.546565.
Test: 2018-07-31T11:58:58.042199: step 12310, loss 0.57502.
Train: 2018-07-31T11:58:58.214004: step 12311, loss 0.632314.
Train: 2018-07-31T11:58:58.370246: step 12312, loss 0.623523.
Train: 2018-07-31T11:58:58.542081: step 12313, loss 0.657479.
Train: 2018-07-31T11:58:58.713915: step 12314, loss 0.580154.
Train: 2018-07-31T11:58:58.870099: step 12315, loss 0.596877.
Train: 2018-07-31T11:58:59.041934: step 12316, loss 0.545486.
Train: 2018-07-31T11:58:59.213798: step 12317, loss 0.630049.
Train: 2018-07-31T11:58:59.385633: step 12318, loss 0.561748.
Train: 2018-07-31T11:58:59.557468: step 12319, loss 0.561343.
Train: 2018-07-31T11:58:59.729302: step 12320, loss 0.662481.
Test: 2018-07-31T11:59:00.197942: step 12320, loss 0.571711.
Train: 2018-07-31T11:59:00.369747: step 12321, loss 0.59431.
Train: 2018-07-31T11:59:00.541582: step 12322, loss 0.576983.
Train: 2018-07-31T11:59:00.713448: step 12323, loss 0.593398.
Train: 2018-07-31T11:59:00.869660: step 12324, loss 0.635013.
Train: 2018-07-31T11:59:01.041465: step 12325, loss 0.533733.
Train: 2018-07-31T11:59:01.213330: step 12326, loss 0.524954.
Train: 2018-07-31T11:59:01.385165: step 12327, loss 0.583254.
Train: 2018-07-31T11:59:01.556999: step 12328, loss 0.591238.
Train: 2018-07-31T11:59:01.728834: step 12329, loss 0.557222.
Train: 2018-07-31T11:59:01.900673: step 12330, loss 0.556799.
Test: 2018-07-31T11:59:02.384901: step 12330, loss 0.56754.
Train: 2018-07-31T11:59:02.541114: step 12331, loss 0.556374.
Train: 2018-07-31T11:59:02.712948: step 12332, loss 0.615042.
Train: 2018-07-31T11:59:02.884813: step 12333, loss 0.606268.
Train: 2018-07-31T11:59:03.056649: step 12334, loss 0.614392.
Train: 2018-07-31T11:59:03.228484: step 12335, loss 0.630968.
Train: 2018-07-31T11:59:03.400288: step 12336, loss 0.529222.
Train: 2018-07-31T11:59:03.556501: step 12337, loss 0.588046.
Train: 2018-07-31T11:59:03.728337: step 12338, loss 0.528596.
Train: 2018-07-31T11:59:03.900171: step 12339, loss 0.655134.
Train: 2018-07-31T11:59:04.072030: step 12340, loss 0.688644.
Test: 2018-07-31T11:59:04.540644: step 12340, loss 0.564333.
Train: 2018-07-31T11:59:04.712480: step 12341, loss 0.553155.
Train: 2018-07-31T11:59:04.868724: step 12342, loss 0.536103.
Train: 2018-07-31T11:59:05.040559: step 12343, loss 0.594786.
Train: 2018-07-31T11:59:05.212388: step 12344, loss 0.661803.
Train: 2018-07-31T11:59:05.368577: step 12345, loss 0.611049.
Train: 2018-07-31T11:59:05.540412: step 12346, loss 0.627482.
Train: 2018-07-31T11:59:05.712247: step 12347, loss 0.57708.
Train: 2018-07-31T11:59:05.884081: step 12348, loss 0.576887.
Train: 2018-07-31T11:59:06.055915: step 12349, loss 0.584996.
Train: 2018-07-31T11:59:06.212159: step 12350, loss 0.51858.
Test: 2018-07-31T11:59:06.696390: step 12350, loss 0.562482.
Train: 2018-07-31T11:59:06.868226: step 12351, loss 0.501888.
Train: 2018-07-31T11:59:07.040091: step 12352, loss 0.567861.
Train: 2018-07-31T11:59:07.196274: step 12353, loss 0.625707.
Train: 2018-07-31T11:59:07.383731: step 12354, loss 0.600664.
Train: 2018-07-31T11:59:07.539943: step 12355, loss 0.517552.
Train: 2018-07-31T11:59:07.711814: step 12356, loss 0.600357.
Train: 2018-07-31T11:59:07.883646: step 12357, loss 0.609547.
Train: 2018-07-31T11:59:08.039852: step 12358, loss 0.533557.
Train: 2018-07-31T11:59:08.211692: step 12359, loss 0.525045.
Train: 2018-07-31T11:59:08.383520: step 12360, loss 0.624894.
Test: 2018-07-31T11:59:08.867756: step 12360, loss 0.560752.
Train: 2018-07-31T11:59:09.039592: step 12361, loss 0.616485.
Train: 2018-07-31T11:59:09.211452: step 12362, loss 0.532877.
Train: 2018-07-31T11:59:09.383294: step 12363, loss 0.516005.
Train: 2018-07-31T11:59:09.555097: step 12364, loss 0.599578.
Train: 2018-07-31T11:59:09.726932: step 12365, loss 0.557547.
Train: 2018-07-31T11:59:09.898800: step 12366, loss 0.506944.
Train: 2018-07-31T11:59:10.054980: step 12367, loss 0.523508.
Train: 2018-07-31T11:59:10.226839: step 12368, loss 0.582514.
Train: 2018-07-31T11:59:10.398683: step 12369, loss 0.480432.
Train: 2018-07-31T11:59:10.554892: step 12370, loss 0.633651.
Test: 2018-07-31T11:59:11.039154: step 12370, loss 0.559403.
Train: 2018-07-31T11:59:11.210989: step 12371, loss 0.565186.
Train: 2018-07-31T11:59:11.367172: step 12372, loss 0.590862.
Train: 2018-07-31T11:59:11.539007: step 12373, loss 0.539103.
Train: 2018-07-31T11:59:11.695251: step 12374, loss 0.582133.
Train: 2018-07-31T11:59:11.882709: step 12375, loss 0.547412.
Train: 2018-07-31T11:59:12.038920: step 12376, loss 0.573316.
Train: 2018-07-31T11:59:12.210725: step 12377, loss 0.555821.
Train: 2018-07-31T11:59:12.382560: step 12378, loss 0.529525.
Train: 2018-07-31T11:59:12.538773: step 12379, loss 0.511827.
Train: 2018-07-31T11:59:12.710638: step 12380, loss 0.546658.
Test: 2018-07-31T11:59:13.194868: step 12380, loss 0.558173.
Train: 2018-07-31T11:59:13.351112: step 12381, loss 0.528874.
Train: 2018-07-31T11:59:13.507328: step 12382, loss 0.535128.
Train: 2018-07-31T11:59:13.679155: step 12383, loss 0.555051.
Train: 2018-07-31T11:59:13.850965: step 12384, loss 0.546018.
Train: 2018-07-31T11:59:14.007180: step 12385, loss 0.527976.
Train: 2018-07-31T11:59:14.179014: step 12386, loss 0.554709.
Train: 2018-07-31T11:59:14.350878: step 12387, loss 0.536575.
Train: 2018-07-31T11:59:14.522717: step 12388, loss 0.53641.
Train: 2018-07-31T11:59:14.678921: step 12389, loss 0.572579.
Train: 2018-07-31T11:59:14.850755: step 12390, loss 0.627213.
Test: 2018-07-31T11:59:15.334992: step 12390, loss 0.557219.
Train: 2018-07-31T11:59:15.491237: step 12391, loss 0.51775.
Train: 2018-07-31T11:59:15.663071: step 12392, loss 0.62733.
Train: 2018-07-31T11:59:15.834906: step 12393, loss 0.581557.
Train: 2018-07-31T11:59:16.006735: step 12394, loss 0.599801.
Train: 2018-07-31T11:59:16.162955: step 12395, loss 0.599713.
Train: 2018-07-31T11:59:16.334760: step 12396, loss 0.64528.
Train: 2018-07-31T11:59:16.506593: step 12397, loss 0.553861.
Train: 2018-07-31T11:59:16.678429: step 12398, loss 0.535626.
Train: 2018-07-31T11:59:16.850288: step 12399, loss 0.617297.
Train: 2018-07-31T11:59:17.006477: step 12400, loss 0.571822.
Test: 2018-07-31T11:59:17.490738: step 12400, loss 0.556609.
Train: 2018-07-31T11:59:18.224940: step 12401, loss 0.517547.
Train: 2018-07-31T11:59:18.396810: step 12402, loss 0.598714.
Train: 2018-07-31T11:59:18.568611: step 12403, loss 0.544588.
Train: 2018-07-31T11:59:18.740446: step 12404, loss 0.526592.
Train: 2018-07-31T11:59:18.912305: step 12405, loss 0.544524.
Train: 2018-07-31T11:59:19.068524: step 12406, loss 0.571401.
Train: 2018-07-31T11:59:19.240329: step 12407, loss 0.535477.
Train: 2018-07-31T11:59:19.412163: step 12408, loss 0.625095.
Train: 2018-07-31T11:59:19.584028: step 12409, loss 0.517508.
Train: 2018-07-31T11:59:19.740241: step 12410, loss 0.535382.
Test: 2018-07-31T11:59:20.224503: step 12410, loss 0.556153.
Train: 2018-07-31T11:59:20.427550: step 12411, loss 0.580101.
Train: 2018-07-31T11:59:20.615041: step 12412, loss 0.526345.
Train: 2018-07-31T11:59:20.818083: step 12413, loss 0.59793.
Train: 2018-07-31T11:59:21.005569: step 12414, loss 0.562067.
Train: 2018-07-31T11:59:21.208617: step 12415, loss 0.606778.
Train: 2018-07-31T11:59:21.380452: step 12416, loss 0.660335.
Train: 2018-07-31T11:59:21.552288: step 12417, loss 0.544111.
Train: 2018-07-31T11:59:21.724152: step 12418, loss 0.633088.
Train: 2018-07-31T11:59:21.895956: step 12419, loss 0.561871.
Train: 2018-07-31T11:59:22.052170: step 12420, loss 0.544149.
Test: 2018-07-31T11:59:22.536461: step 12420, loss 0.555851.
Train: 2018-07-31T11:59:22.708266: step 12421, loss 0.544157.
Train: 2018-07-31T11:59:22.864479: step 12422, loss 0.588203.
Train: 2018-07-31T11:59:23.036344: step 12423, loss 0.535366.
Train: 2018-07-31T11:59:23.192556: step 12424, loss 0.561714.
Train: 2018-07-31T11:59:23.364362: step 12425, loss 0.544137.
Train: 2018-07-31T11:59:23.536198: step 12426, loss 0.544115.
Train: 2018-07-31T11:59:23.708062: step 12427, loss 0.552852.
Train: 2018-07-31T11:59:23.879892: step 12428, loss 0.693126.
Train: 2018-07-31T11:59:24.051702: step 12429, loss 0.579064.
Train: 2018-07-31T11:59:24.239190: step 12430, loss 0.640115.
Test: 2018-07-31T11:59:24.707828: step 12430, loss 0.555659.
Train: 2018-07-31T11:59:24.879663: step 12431, loss 0.622434.
Train: 2018-07-31T11:59:25.051467: step 12432, loss 0.561525.
Train: 2018-07-31T11:59:25.223303: step 12433, loss 0.561519.
Train: 2018-07-31T11:59:25.395171: step 12434, loss 0.587324.
Train: 2018-07-31T11:59:25.566972: step 12435, loss 0.604411.
Train: 2018-07-31T11:59:25.723185: step 12436, loss 0.578744.
Train: 2018-07-31T11:59:25.895021: step 12437, loss 0.535966.
Train: 2018-07-31T11:59:26.066856: step 12438, loss 0.638072.
Train: 2018-07-31T11:59:26.238690: step 12439, loss 0.612443.
Train: 2018-07-31T11:59:26.410525: step 12440, loss 0.536341.
Test: 2018-07-31T11:59:26.879163: step 12440, loss 0.556038.
Train: 2018-07-31T11:59:27.051029: step 12441, loss 0.578555.
Train: 2018-07-31T11:59:27.222864: step 12442, loss 0.578565.
Train: 2018-07-31T11:59:27.379048: step 12443, loss 0.519937.
Train: 2018-07-31T11:59:27.550883: step 12444, loss 0.586957.
Train: 2018-07-31T11:59:27.722718: step 12445, loss 0.628751.
Train: 2018-07-31T11:59:27.894552: step 12446, loss 0.570257.
Train: 2018-07-31T11:59:28.066386: step 12447, loss 0.503656.
Train: 2018-07-31T11:59:28.238252: step 12448, loss 0.586921.
Train: 2018-07-31T11:59:28.394435: step 12449, loss 0.462057.
Train: 2018-07-31T11:59:28.566300: step 12450, loss 0.603601.
Test: 2018-07-31T11:59:29.050562: step 12450, loss 0.556227.
Train: 2018-07-31T11:59:29.222398: step 12451, loss 0.586918.
Train: 2018-07-31T11:59:29.394201: step 12452, loss 0.620347.
Train: 2018-07-31T11:59:29.566036: step 12453, loss 0.6454.
Train: 2018-07-31T11:59:29.737900: step 12454, loss 0.503368.
Train: 2018-07-31T11:59:29.909735: step 12455, loss 0.603513.
Train: 2018-07-31T11:59:30.081569: step 12456, loss 0.578435.
Train: 2018-07-31T11:59:30.237784: step 12457, loss 0.511665.
Train: 2018-07-31T11:59:30.409618: step 12458, loss 0.519921.
Train: 2018-07-31T11:59:30.581453: step 12459, loss 0.628541.
Train: 2018-07-31T11:59:30.753258: step 12460, loss 0.569937.
Test: 2018-07-31T11:59:31.221897: step 12460, loss 0.555865.
Train: 2018-07-31T11:59:31.393766: step 12461, loss 0.586655.
Train: 2018-07-31T11:59:31.565567: step 12462, loss 0.553096.
Train: 2018-07-31T11:59:31.737432: step 12463, loss 0.611769.
Train: 2018-07-31T11:59:31.909237: step 12464, loss 0.586566.
Train: 2018-07-31T11:59:32.065480: step 12465, loss 0.594917.
Train: 2018-07-31T11:59:32.237315: step 12466, loss 0.55296.
Train: 2018-07-31T11:59:32.409149: step 12467, loss 0.561311.
Train: 2018-07-31T11:59:32.565333: step 12468, loss 0.6134.
Train: 2018-07-31T11:59:32.737169: step 12469, loss 0.578023.
Train: 2018-07-31T11:59:32.909003: step 12470, loss 0.594784.
Test: 2018-07-31T11:59:33.393263: step 12470, loss 0.555699.
Train: 2018-07-31T11:59:33.565098: step 12471, loss 0.569709.
Train: 2018-07-31T11:59:33.721313: step 12472, loss 0.477812.
Train: 2018-07-31T11:59:33.893146: step 12473, loss 0.594934.
Train: 2018-07-31T11:59:34.065012: step 12474, loss 0.611768.
Train: 2018-07-31T11:59:34.236846: step 12475, loss 0.511301.
Train: 2018-07-31T11:59:34.408681: step 12476, loss 0.528051.
Train: 2018-07-31T11:59:34.580522: step 12477, loss 0.586839.
Train: 2018-07-31T11:59:34.752351: step 12478, loss 0.586899.
Train: 2018-07-31T11:59:34.924186: step 12479, loss 0.511082.
Train: 2018-07-31T11:59:35.080399: step 12480, loss 0.595443.
Test: 2018-07-31T11:59:35.564629: step 12480, loss 0.555928.
Train: 2018-07-31T11:59:35.736492: step 12481, loss 0.578564.
Train: 2018-07-31T11:59:35.908300: step 12482, loss 0.603995.
Train: 2018-07-31T11:59:36.080135: step 12483, loss 0.561607.
Train: 2018-07-31T11:59:36.252000: step 12484, loss 0.51915.
Train: 2018-07-31T11:59:36.423835: step 12485, loss 0.570042.
Train: 2018-07-31T11:59:36.595669: step 12486, loss 0.552979.
Train: 2018-07-31T11:59:36.767508: step 12487, loss 0.561442.
Train: 2018-07-31T11:59:36.939339: step 12488, loss 0.527201.
Train: 2018-07-31T11:59:37.111174: step 12489, loss 0.587023.
Train: 2018-07-31T11:59:37.282978: step 12490, loss 0.578428.
Test: 2018-07-31T11:59:37.767240: step 12490, loss 0.5554.
Train: 2018-07-31T11:59:37.939075: step 12491, loss 0.561201.
Train: 2018-07-31T11:59:38.110910: step 12492, loss 0.586972.
Train: 2018-07-31T11:59:38.267122: step 12493, loss 0.561086.
Train: 2018-07-31T11:59:38.438988: step 12494, loss 0.526516.
Train: 2018-07-31T11:59:38.610793: step 12495, loss 0.578257.
Train: 2018-07-31T11:59:38.798279: step 12496, loss 0.612848.
Train: 2018-07-31T11:59:38.970113: step 12497, loss 0.586831.
Train: 2018-07-31T11:59:39.126331: step 12498, loss 0.552162.
Train: 2018-07-31T11:59:39.298131: step 12499, loss 0.516373.
Train: 2018-07-31T11:59:39.469967: step 12500, loss 0.548132.
Test: 2018-07-31T11:59:39.954228: step 12500, loss 0.957698.
Train: 2018-07-31T11:59:40.688462: step 12501, loss 1.2532.
Train: 2018-07-31T11:59:40.860290: step 12502, loss 0.540524.
Train: 2018-07-31T11:59:41.032101: step 12503, loss 0.526288.
Train: 2018-07-31T11:59:41.203935: step 12504, loss 0.639811.
Train: 2018-07-31T11:59:41.375771: step 12505, loss 0.65811.
Train: 2018-07-31T11:59:41.547605: step 12506, loss 0.520767.
Train: 2018-07-31T11:59:41.719439: step 12507, loss 0.496203.
Train: 2018-07-31T11:59:41.891274: step 12508, loss 0.652907.
Train: 2018-07-31T11:59:42.063108: step 12509, loss 0.567825.
Train: 2018-07-31T11:59:42.234973: step 12510, loss 0.534573.
Test: 2018-07-31T11:59:42.703615: step 12510, loss 0.564348.
Train: 2018-07-31T11:59:42.891040: step 12511, loss 0.596006.
Train: 2018-07-31T11:59:43.062905: step 12512, loss 0.579805.
Train: 2018-07-31T11:59:43.234740: step 12513, loss 0.554908.
Train: 2018-07-31T11:59:43.422196: step 12514, loss 0.572943.
Train: 2018-07-31T11:59:43.594002: step 12515, loss 0.616707.
Train: 2018-07-31T11:59:43.781457: step 12516, loss 0.634513.
Train: 2018-07-31T11:59:43.953291: step 12517, loss 0.549001.
Train: 2018-07-31T11:59:44.140771: step 12518, loss 0.540846.
Train: 2018-07-31T11:59:44.312581: step 12519, loss 0.523884.
Train: 2018-07-31T11:59:44.484416: step 12520, loss 0.584369.
Test: 2018-07-31T11:59:44.953057: step 12520, loss 0.570121.
Train: 2018-07-31T11:59:45.124893: step 12521, loss 0.575933.
Train: 2018-07-31T11:59:45.312373: step 12522, loss 0.584661.
Train: 2018-07-31T11:59:45.484182: step 12523, loss 0.584717.
Train: 2018-07-31T11:59:45.640396: step 12524, loss 0.558793.
Train: 2018-07-31T11:59:45.827852: step 12525, loss 0.619297.
Train: 2018-07-31T11:59:45.999711: step 12526, loss 0.636513.
Train: 2018-07-31T11:59:46.171521: step 12527, loss 0.551297.
Train: 2018-07-31T11:59:46.343356: step 12528, loss 0.549715.
Train: 2018-07-31T11:59:46.515192: step 12529, loss 0.5322.
Train: 2018-07-31T11:59:46.702648: step 12530, loss 0.609998.
Test: 2018-07-31T11:59:47.186939: step 12530, loss 0.569233.
Train: 2018-07-31T11:59:47.358773: step 12531, loss 0.574832.
Train: 2018-07-31T11:59:47.530603: step 12532, loss 0.574869.
Train: 2018-07-31T11:59:47.686822: step 12533, loss 0.509116.
Train: 2018-07-31T11:59:47.858657: step 12534, loss 0.65283.
Train: 2018-07-31T11:59:48.030491: step 12535, loss 0.574142.
Train: 2018-07-31T11:59:48.202297: step 12536, loss 0.504085.
Train: 2018-07-31T11:59:48.389753: step 12537, loss 0.538666.
Train: 2018-07-31T11:59:48.561617: step 12538, loss 0.547088.
Train: 2018-07-31T11:59:48.733421: step 12539, loss 0.581914.
Train: 2018-07-31T11:59:48.905257: step 12540, loss 0.713745.
Test: 2018-07-31T11:59:49.373926: step 12540, loss 0.566694.
Train: 2018-07-31T11:59:49.545732: step 12541, loss 0.62622.
Train: 2018-07-31T11:59:49.733187: step 12542, loss 0.519871.
Train: 2018-07-31T11:59:49.905053: step 12543, loss 0.519438.
Train: 2018-07-31T11:59:50.076890: step 12544, loss 0.571995.
Train: 2018-07-31T11:59:50.264313: step 12545, loss 0.501338.
Train: 2018-07-31T11:59:50.436148: step 12546, loss 0.580447.
Train: 2018-07-31T11:59:50.608013: step 12547, loss 0.562599.
Train: 2018-07-31T11:59:50.779817: step 12548, loss 0.597813.
Train: 2018-07-31T11:59:50.951653: step 12549, loss 0.579913.
Train: 2018-07-31T11:59:51.123512: step 12550, loss 0.535344.
Test: 2018-07-31T11:59:51.607778: step 12550, loss 0.564666.
Train: 2018-07-31T11:59:51.779584: step 12551, loss 0.615114.
Train: 2018-07-31T11:59:51.967040: step 12552, loss 0.686109.
Train: 2018-07-31T11:59:52.138874: step 12553, loss 0.561765.
Train: 2018-07-31T11:59:52.310734: step 12554, loss 0.60561.
Train: 2018-07-31T11:59:52.482568: step 12555, loss 0.596548.
Train: 2018-07-31T11:59:52.654409: step 12556, loss 0.596363.
Train: 2018-07-31T11:59:52.826244: step 12557, loss 0.569735.
Train: 2018-07-31T11:59:52.998049: step 12558, loss 0.596391.
Train: 2018-07-31T11:59:53.169884: step 12559, loss 0.60463.
Train: 2018-07-31T11:59:53.357340: step 12560, loss 0.490605.
Test: 2018-07-31T11:59:53.825979: step 12560, loss 0.563389.
Train: 2018-07-31T11:59:54.013435: step 12561, loss 0.543047.
Train: 2018-07-31T11:59:54.185271: step 12562, loss 0.604164.
Train: 2018-07-31T11:59:54.357135: step 12563, loss 0.604023.
Train: 2018-07-31T11:59:54.528939: step 12564, loss 0.577676.
Train: 2018-07-31T11:59:54.716396: step 12565, loss 0.542775.
Train: 2018-07-31T11:59:54.888265: step 12566, loss 0.525126.
Train: 2018-07-31T11:59:55.060065: step 12567, loss 0.60345.
Train: 2018-07-31T11:59:55.247554: step 12568, loss 0.551009.
Train: 2018-07-31T11:59:55.435008: step 12569, loss 0.559593.
Train: 2018-07-31T11:59:55.606842: step 12570, loss 0.56818.
Test: 2018-07-31T11:59:56.075452: step 12570, loss 0.562152.
Train: 2018-07-31T11:59:56.262909: step 12571, loss 0.533122.
Train: 2018-07-31T11:59:56.434745: step 12572, loss 0.62909.
Train: 2018-07-31T11:59:56.606578: step 12573, loss 0.506573.
Train: 2018-07-31T11:59:56.794059: step 12574, loss 0.58513.
Train: 2018-07-31T11:59:56.965899: step 12575, loss 0.523683.
Train: 2018-07-31T11:59:57.137704: step 12576, loss 0.593674.
Train: 2018-07-31T11:59:57.309539: step 12577, loss 0.532076.
Train: 2018-07-31T11:59:57.496995: step 12578, loss 0.628683.
Train: 2018-07-31T11:59:57.668830: step 12579, loss 0.549352.
Train: 2018-07-31T11:59:57.856316: step 12580, loss 0.63732.
Test: 2018-07-31T11:59:58.324956: step 12580, loss 0.560778.
Train: 2018-07-31T11:59:58.512381: step 12581, loss 0.601942.
Train: 2018-07-31T11:59:58.684246: step 12582, loss 0.557811.
Train: 2018-07-31T11:59:58.856051: step 12583, loss 0.645599.
Train: 2018-07-31T11:59:59.027912: step 12584, loss 0.662872.
Train: 2018-07-31T11:59:59.199746: step 12585, loss 0.575034.
Train: 2018-07-31T11:59:59.371555: step 12586, loss 0.670733.
Train: 2018-07-31T11:59:59.559012: step 12587, loss 0.54012.
Train: 2018-07-31T11:59:59.730846: step 12588, loss 0.583322.
Train: 2018-07-31T11:59:59.902682: step 12589, loss 0.540161.
Train: 2018-07-31T12:00:00.090137: step 12590, loss 0.548736.
Test: 2018-07-31T12:00:00.574398: step 12590, loss 0.560043.
Train: 2018-07-31T12:00:00.746264: step 12591, loss 0.57439.
Train: 2018-07-31T12:00:00.918069: step 12592, loss 0.540088.
Train: 2018-07-31T12:00:01.089904: step 12593, loss 0.5913.
Train: 2018-07-31T12:00:01.261737: step 12594, loss 0.616803.
Train: 2018-07-31T12:00:01.433572: step 12595, loss 0.488824.
Train: 2018-07-31T12:00:01.621028: step 12596, loss 0.556907.
Train: 2018-07-31T12:00:01.792864: step 12597, loss 0.556816.
Train: 2018-07-31T12:00:01.964729: step 12598, loss 0.624979.
Train: 2018-07-31T12:00:02.136567: step 12599, loss 0.582235.
Train: 2018-07-31T12:00:02.308392: step 12600, loss 0.599207.
Test: 2018-07-31T12:00:02.792630: step 12600, loss 0.559284.
Train: 2018-07-31T12:00:03.511245: step 12601, loss 0.522436.
Train: 2018-07-31T12:00:03.683076: step 12602, loss 0.599042.
Train: 2018-07-31T12:00:03.870532: step 12603, loss 0.564884.
Train: 2018-07-31T12:00:04.042368: step 12604, loss 0.581851.
Train: 2018-07-31T12:00:04.214201: step 12605, loss 0.632892.
Train: 2018-07-31T12:00:04.386031: step 12606, loss 0.598717.
Train: 2018-07-31T12:00:04.557841: step 12607, loss 0.52216.
Train: 2018-07-31T12:00:04.729675: step 12608, loss 0.539097.
Train: 2018-07-31T12:00:04.901541: step 12609, loss 0.640948.
Train: 2018-07-31T12:00:05.073376: step 12610, loss 0.564446.
Test: 2018-07-31T12:00:05.542015: step 12610, loss 0.558667.
Train: 2018-07-31T12:00:05.729468: step 12611, loss 0.521987.
Train: 2018-07-31T12:00:05.901307: step 12612, loss 0.564321.
Train: 2018-07-31T12:00:06.073111: step 12613, loss 0.538784.
Train: 2018-07-31T12:00:06.244946: step 12614, loss 0.538677.
Train: 2018-07-31T12:00:06.432403: step 12615, loss 0.60668.
Train: 2018-07-31T12:00:06.604238: step 12616, loss 0.54698.
Train: 2018-07-31T12:00:06.791694: step 12617, loss 0.538346.
Train: 2018-07-31T12:00:06.963552: step 12618, loss 0.555328.
Train: 2018-07-31T12:00:07.150983: step 12619, loss 0.53809.
Train: 2018-07-31T12:00:07.322818: step 12620, loss 0.580919.
Test: 2018-07-31T12:00:07.791489: step 12620, loss 0.557847.
Train: 2018-07-31T12:00:07.963293: step 12621, loss 0.615336.
Train: 2018-07-31T12:00:08.135129: step 12622, loss 0.580841.
Train: 2018-07-31T12:00:08.306993: step 12623, loss 0.503135.
Train: 2018-07-31T12:00:08.494452: step 12624, loss 0.520241.
Train: 2018-07-31T12:00:08.666255: step 12625, loss 0.546063.
Train: 2018-07-31T12:00:08.838114: step 12626, loss 0.632896.
Train: 2018-07-31T12:00:09.025576: step 12627, loss 0.598107.
Train: 2018-07-31T12:00:09.197405: step 12628, loss 0.615506.
Train: 2018-07-31T12:00:09.369214: step 12629, loss 0.537044.
Train: 2018-07-31T12:00:09.541049: step 12630, loss 0.597987.
Test: 2018-07-31T12:00:10.025311: step 12630, loss 0.557204.
Train: 2018-07-31T12:00:10.197171: step 12631, loss 0.597934.
Train: 2018-07-31T12:00:10.368981: step 12632, loss 0.615278.
Train: 2018-07-31T12:00:10.540816: step 12633, loss 0.519541.
Train: 2018-07-31T12:00:10.712650: step 12634, loss 0.632469.
Train: 2018-07-31T12:00:10.884485: step 12635, loss 0.519555.
Train: 2018-07-31T12:00:11.056349: step 12636, loss 0.571556.
Train: 2018-07-31T12:00:11.228185: step 12637, loss 0.588841.
Train: 2018-07-31T12:00:11.415610: step 12638, loss 0.588782.
Train: 2018-07-31T12:00:11.571854: step 12639, loss 0.50224.
Train: 2018-07-31T12:00:11.821795: step 12640, loss 0.562732.
Test: 2018-07-31T12:00:12.290404: step 12640, loss 0.55685.
Train: 2018-07-31T12:00:12.462270: step 12641, loss 0.528076.
Train: 2018-07-31T12:00:12.634074: step 12642, loss 0.588629.
Train: 2018-07-31T12:00:12.805940: step 12643, loss 0.64061.
Train: 2018-07-31T12:00:12.993366: step 12644, loss 0.493273.
Train: 2018-07-31T12:00:13.165231: step 12645, loss 0.605857.
Train: 2018-07-31T12:00:13.337066: step 12646, loss 0.59715.
Train: 2018-07-31T12:00:13.508904: step 12647, loss 0.5278.
Train: 2018-07-31T12:00:13.680704: step 12648, loss 0.597066.
Train: 2018-07-31T12:00:13.852539: step 12649, loss 0.59702.
Train: 2018-07-31T12:00:14.024375: step 12650, loss 0.570998.
Test: 2018-07-31T12:00:14.508670: step 12650, loss 0.556477.
Train: 2018-07-31T12:00:14.680501: step 12651, loss 0.596904.
Train: 2018-07-31T12:00:14.852306: step 12652, loss 0.536374.
Train: 2018-07-31T12:00:15.039792: step 12653, loss 0.596787.
Train: 2018-07-31T12:00:15.211622: step 12654, loss 0.596727.
Train: 2018-07-31T12:00:15.383456: step 12655, loss 0.631109.
Train: 2018-07-31T12:00:15.555296: step 12656, loss 0.510633.
Train: 2018-07-31T12:00:15.727131: step 12657, loss 0.570747.
Train: 2018-07-31T12:00:15.898936: step 12658, loss 0.596442.
Train: 2018-07-31T12:00:16.070801: step 12659, loss 0.587811.
Train: 2018-07-31T12:00:16.242637: step 12660, loss 0.587754.
Test: 2018-07-31T12:00:16.726896: step 12660, loss 0.556323.
Train: 2018-07-31T12:00:16.898731: step 12661, loss 0.527935.
Train: 2018-07-31T12:00:17.086188: step 12662, loss 0.502385.
Train: 2018-07-31T12:00:17.258017: step 12663, loss 0.596173.
Train: 2018-07-31T12:00:17.429857: step 12664, loss 0.561993.
Train: 2018-07-31T12:00:17.601694: step 12665, loss 0.579049.
Train: 2018-07-31T12:00:17.773496: step 12666, loss 0.579027.
Train: 2018-07-31T12:00:17.945361: step 12667, loss 0.579004.
Train: 2018-07-31T12:00:18.117191: step 12668, loss 0.536264.
Train: 2018-07-31T12:00:18.304647: step 12669, loss 0.578961.
Train: 2018-07-31T12:00:18.476481: step 12670, loss 0.527636.
Test: 2018-07-31T12:00:18.960718: step 12670, loss 0.556029.
Train: 2018-07-31T12:00:19.132554: step 12671, loss 0.578928.
Train: 2018-07-31T12:00:19.304389: step 12672, loss 0.561775.
Train: 2018-07-31T12:00:19.491845: step 12673, loss 0.561743.
Train: 2018-07-31T12:00:19.663679: step 12674, loss 0.613214.
Train: 2018-07-31T12:00:19.835544: step 12675, loss 0.501578.
Train: 2018-07-31T12:00:20.007373: step 12676, loss 0.604642.
Train: 2018-07-31T12:00:20.194805: step 12677, loss 0.621846.
Train: 2018-07-31T12:00:20.366669: step 12678, loss 0.578797.
Train: 2018-07-31T12:00:20.538474: step 12679, loss 0.544387.
Train: 2018-07-31T12:00:20.710308: step 12680, loss 0.552957.
Test: 2018-07-31T12:00:21.194600: step 12680, loss 0.555722.
Train: 2018-07-31T12:00:21.366435: step 12681, loss 0.570119.
Train: 2018-07-31T12:00:21.538265: step 12682, loss 0.535698.
Train: 2018-07-31T12:00:21.710075: step 12683, loss 0.613099.
Train: 2018-07-31T12:00:21.881910: step 12684, loss 0.533324.
Train: 2018-07-31T12:00:22.053745: step 12685, loss 0.613072.
Train: 2018-07-31T12:00:22.225603: step 12686, loss 0.518337.
Train: 2018-07-31T12:00:22.397413: step 12687, loss 0.535504.
Train: 2018-07-31T12:00:22.584900: step 12688, loss 0.587194.
Train: 2018-07-31T12:00:22.756738: step 12689, loss 0.587183.
Train: 2018-07-31T12:00:22.928540: step 12690, loss 0.595802.
Test: 2018-07-31T12:00:23.397211: step 12690, loss 0.555405.
Train: 2018-07-31T12:00:23.584666: step 12691, loss 0.561231.
Train: 2018-07-31T12:00:23.756471: step 12692, loss 0.587111.
Train: 2018-07-31T12:00:23.928306: step 12693, loss 0.561185.
Train: 2018-07-31T12:00:24.100139: step 12694, loss 0.543903.
Train: 2018-07-31T12:00:24.271999: step 12695, loss 0.604297.
Train: 2018-07-31T12:00:24.443841: step 12696, loss 0.673285.
Train: 2018-07-31T12:00:24.615645: step 12697, loss 0.52668.
Train: 2018-07-31T12:00:24.803136: step 12698, loss 0.655655.
Train: 2018-07-31T12:00:24.974965: step 12699, loss 0.638242.
Train: 2018-07-31T12:00:25.146769: step 12700, loss 0.535501.
Test: 2018-07-31T12:00:25.615409: step 12700, loss 0.555375.
Train: 2018-07-31T12:00:26.365258: step 12701, loss 0.578159.
Train: 2018-07-31T12:00:26.552691: step 12702, loss 0.646029.
Train: 2018-07-31T12:00:26.724525: step 12703, loss 0.578063.
Train: 2018-07-31T12:00:26.896391: step 12704, loss 0.485335.
Train: 2018-07-31T12:00:27.083815: step 12705, loss 0.535929.
Train: 2018-07-31T12:00:27.255651: step 12706, loss 0.561156.
Train: 2018-07-31T12:00:27.427517: step 12707, loss 0.586357.
Train: 2018-07-31T12:00:27.614974: step 12708, loss 0.577929.
Train: 2018-07-31T12:00:27.786807: step 12709, loss 0.56951.
Train: 2018-07-31T12:00:27.958643: step 12710, loss 0.561098.
Test: 2018-07-31T12:00:28.427251: step 12710, loss 0.555416.
Train: 2018-07-31T12:00:28.599085: step 12711, loss 0.594653.
Train: 2018-07-31T12:00:28.770921: step 12712, loss 0.561067.
Train: 2018-07-31T12:00:28.942756: step 12713, loss 0.552662.
Train: 2018-07-31T12:00:29.130236: step 12714, loss 0.586198.
Train: 2018-07-31T12:00:29.302077: step 12715, loss 0.510666.
Train: 2018-07-31T12:00:29.473912: step 12716, loss 0.569374.
Train: 2018-07-31T12:00:29.661367: step 12717, loss 0.628234.
Train: 2018-07-31T12:00:29.833172: step 12718, loss 0.56092.
Train: 2018-07-31T12:00:30.005041: step 12719, loss 0.611387.
Train: 2018-07-31T12:00:30.176872: step 12720, loss 0.535655.
Test: 2018-07-31T12:00:30.661133: step 12720, loss 0.555188.
Train: 2018-07-31T12:00:30.832970: step 12721, loss 0.569278.
Train: 2018-07-31T12:00:31.004773: step 12722, loss 0.535594.
Train: 2018-07-31T12:00:31.176608: step 12723, loss 0.60294.
Train: 2018-07-31T12:00:31.364094: step 12724, loss 0.628221.
Train: 2018-07-31T12:00:31.535929: step 12725, loss 0.628162.
Train: 2018-07-31T12:00:31.707733: step 12726, loss 0.543974.
Train: 2018-07-31T12:00:31.879602: step 12727, loss 0.611179.
Train: 2018-07-31T12:00:32.067023: step 12728, loss 0.619486.
Train: 2018-07-31T12:00:32.238889: step 12729, loss 0.527349.
Train: 2018-07-31T12:00:32.410694: step 12730, loss 0.5441.
Test: 2018-07-31T12:00:32.894985: step 12730, loss 0.555165.
Train: 2018-07-31T12:00:33.066791: step 12731, loss 0.602552.
Train: 2018-07-31T12:00:33.238626: step 12732, loss 0.535771.
Train: 2018-07-31T12:00:33.410485: step 12733, loss 0.5441.
Train: 2018-07-31T12:00:33.582324: step 12734, loss 0.527374.
Train: 2018-07-31T12:00:33.769750: step 12735, loss 0.594158.
Train: 2018-07-31T12:00:33.941586: step 12736, loss 0.602528.
Train: 2018-07-31T12:00:34.113419: step 12737, loss 0.535568.
Train: 2018-07-31T12:00:34.285285: step 12738, loss 0.560646.
Train: 2018-07-31T12:00:34.472711: step 12739, loss 0.602555.
Train: 2018-07-31T12:00:34.644546: step 12740, loss 0.51024.
Test: 2018-07-31T12:00:35.113186: step 12740, loss 0.554884.
Train: 2018-07-31T12:00:35.300672: step 12741, loss 0.594184.
Train: 2018-07-31T12:00:35.472507: step 12742, loss 0.543694.
Train: 2018-07-31T12:00:35.644312: step 12743, loss 0.619504.
Train: 2018-07-31T12:00:35.831768: step 12744, loss 0.535167.
Train: 2018-07-31T12:00:36.003636: step 12745, loss 0.585778.
Train: 2018-07-31T12:00:36.175437: step 12746, loss 0.535062.
Train: 2018-07-31T12:00:36.347272: step 12747, loss 0.560387.
Train: 2018-07-31T12:00:36.534729: step 12748, loss 0.509488.
Train: 2018-07-31T12:00:36.706593: step 12749, loss 0.509312.
Train: 2018-07-31T12:00:36.878422: step 12750, loss 0.602917.
Test: 2018-07-31T12:00:37.362659: step 12750, loss 0.554454.
Train: 2018-07-31T12:00:37.565737: step 12751, loss 0.526015.
Train: 2018-07-31T12:00:37.737604: step 12752, loss 0.577343.
Train: 2018-07-31T12:00:37.909407: step 12753, loss 0.508533.
Train: 2018-07-31T12:00:38.081271: step 12754, loss 0.499678.
Train: 2018-07-31T12:00:38.268726: step 12755, loss 0.603423.
Train: 2018-07-31T12:00:38.440532: step 12756, loss 0.533933.
Train: 2018-07-31T12:00:38.612366: step 12757, loss 0.551271.
Train: 2018-07-31T12:00:38.784202: step 12758, loss 0.60378.
Train: 2018-07-31T12:00:38.956035: step 12759, loss 0.489701.
Train: 2018-07-31T12:00:39.127871: step 12760, loss 0.577555.
Test: 2018-07-31T12:00:39.612132: step 12760, loss 0.553949.
Train: 2018-07-31T12:00:39.783997: step 12761, loss 0.648281.
Train: 2018-07-31T12:00:39.955833: step 12762, loss 0.542207.
Train: 2018-07-31T12:00:40.143260: step 12763, loss 0.61302.
Train: 2018-07-31T12:00:40.315093: step 12764, loss 0.542156.
Train: 2018-07-31T12:00:40.486960: step 12765, loss 0.586443.
Train: 2018-07-31T12:00:40.658792: step 12766, loss 0.58643.
Train: 2018-07-31T12:00:40.830627: step 12767, loss 0.568691.
Train: 2018-07-31T12:00:41.018053: step 12768, loss 0.542119.
Train: 2018-07-31T12:00:41.189888: step 12769, loss 0.506708.
Train: 2018-07-31T12:00:41.361753: step 12770, loss 0.533224.
Test: 2018-07-31T12:00:41.830363: step 12770, loss 0.5538.
Train: 2018-07-31T12:00:42.017820: step 12771, loss 0.550916.
Train: 2018-07-31T12:00:42.189684: step 12772, loss 0.568653.
Train: 2018-07-31T12:00:42.361488: step 12773, loss 0.488639.
Train: 2018-07-31T12:00:42.533354: step 12774, loss 0.541928.
Train: 2018-07-31T12:00:42.705188: step 12775, loss 0.568668.
Train: 2018-07-31T12:00:42.877017: step 12776, loss 0.523937.
Train: 2018-07-31T12:00:43.048827: step 12777, loss 0.487971.
Train: 2018-07-31T12:00:43.220663: step 12778, loss 0.523721.
Train: 2018-07-31T12:00:43.392528: step 12779, loss 0.613899.
Train: 2018-07-31T12:00:43.579953: step 12780, loss 0.54161.
Test: 2018-07-31T12:00:44.064245: step 12780, loss 0.553589.
Train: 2018-07-31T12:00:44.251705: step 12781, loss 0.550639.
Train: 2018-07-31T12:00:44.423536: step 12782, loss 0.586978.
Train: 2018-07-31T12:00:44.595370: step 12783, loss 0.58701.
Train: 2018-07-31T12:00:44.782815: step 12784, loss 0.532374.
Train: 2018-07-31T12:00:44.954656: step 12785, loss 0.614388.
Train: 2018-07-31T12:00:45.126465: step 12786, loss 0.550565.
Train: 2018-07-31T12:00:45.298301: step 12787, loss 0.559668.
Train: 2018-07-31T12:00:45.470166: step 12788, loss 0.559656.
Train: 2018-07-31T12:00:45.642004: step 12789, loss 0.632515.
Train: 2018-07-31T12:00:45.813835: step 12790, loss 0.650567.
Test: 2018-07-31T12:00:46.298066: step 12790, loss 0.553483.
Train: 2018-07-31T12:00:46.469901: step 12791, loss 0.487073.
Train: 2018-07-31T12:00:46.641735: step 12792, loss 0.559584.
Train: 2018-07-31T12:00:46.813596: step 12793, loss 0.559567.
Train: 2018-07-31T12:00:47.001061: step 12794, loss 0.559554.
Train: 2018-07-31T12:00:47.188483: step 12795, loss 0.595588.
Train: 2018-07-31T12:00:47.360317: step 12796, loss 0.559524.
Train: 2018-07-31T12:00:47.532183: step 12797, loss 0.57747.
Train: 2018-07-31T12:00:47.703988: step 12798, loss 0.586384.
Train: 2018-07-31T12:00:47.875856: step 12799, loss 0.523718.
Train: 2018-07-31T12:00:48.063279: step 12800, loss 0.559475.
Test: 2018-07-31T12:00:48.531949: step 12800, loss 0.553448.
Train: 2018-07-31T12:00:49.344229: step 12801, loss 0.550547.
Train: 2018-07-31T12:00:49.531684: step 12802, loss 0.550547.
Train: 2018-07-31T12:00:49.687928: step 12803, loss 0.568348.
Train: 2018-07-31T12:00:49.875354: step 12804, loss 0.514963.
Train: 2018-07-31T12:00:50.047188: step 12805, loss 0.6128.
Train: 2018-07-31T12:00:50.219054: step 12806, loss 0.541644.
Train: 2018-07-31T12:00:50.390858: step 12807, loss 0.621591.
Train: 2018-07-31T12:00:50.578344: step 12808, loss 0.603748.
Train: 2018-07-31T12:00:50.750183: step 12809, loss 0.532845.
Train: 2018-07-31T12:00:50.921983: step 12810, loss 0.577062.
Test: 2018-07-31T12:00:51.406246: step 12810, loss 0.553428.
Train: 2018-07-31T12:00:51.578081: step 12811, loss 0.585861.
Train: 2018-07-31T12:00:51.749949: step 12812, loss 0.57699.
Train: 2018-07-31T12:00:51.937372: step 12813, loss 0.550587.
Train: 2018-07-31T12:00:52.109240: step 12814, loss 0.620797.
Train: 2018-07-31T12:00:52.281065: step 12815, loss 0.550621.
Train: 2018-07-31T12:00:52.452905: step 12816, loss 0.576847.
Train: 2018-07-31T12:00:52.624711: step 12817, loss 0.594248.
Train: 2018-07-31T12:00:52.812196: step 12818, loss 0.611579.
Train: 2018-07-31T12:00:52.984031: step 12819, loss 0.52472.
Train: 2018-07-31T12:00:53.155866: step 12820, loss 0.542112.
Test: 2018-07-31T12:00:53.640127: step 12820, loss 0.553606.
Train: 2018-07-31T12:00:53.811962: step 12821, loss 0.559436.
Train: 2018-07-31T12:00:53.983792: step 12822, loss 0.568089.
Train: 2018-07-31T12:00:54.171222: step 12823, loss 0.593764.
Train: 2018-07-31T12:00:54.343087: step 12824, loss 0.576732.
Train: 2018-07-31T12:00:54.514892: step 12825, loss 0.576741.
Train: 2018-07-31T12:00:54.686757: step 12826, loss 0.569699.
Train: 2018-07-31T12:00:54.858593: step 12827, loss 0.550986.
Train: 2018-07-31T12:00:55.046042: step 12828, loss 0.610954.
Train: 2018-07-31T12:00:55.217853: step 12829, loss 0.576892.
Train: 2018-07-31T12:00:55.405310: step 12830, loss 0.645464.
Test: 2018-07-31T12:00:55.873983: step 12830, loss 0.554746.
Train: 2018-07-31T12:00:56.061405: step 12831, loss 0.603052.
Train: 2018-07-31T12:00:56.233270: step 12832, loss 0.543983.
Train: 2018-07-31T12:00:56.405105: step 12833, loss 0.552953.
Train: 2018-07-31T12:00:56.576910: step 12834, loss 0.570306.
Train: 2018-07-31T12:00:56.733154: step 12835, loss 0.552701.
Train: 2018-07-31T12:00:56.920580: step 12836, loss 0.537327.
Train: 2018-07-31T12:00:57.092414: step 12837, loss 0.537613.
Train: 2018-07-31T12:00:57.279900: step 12838, loss 0.563112.
Train: 2018-07-31T12:00:57.451735: step 12839, loss 0.546405.
Train: 2018-07-31T12:00:57.623539: step 12840, loss 0.538051.
Test: 2018-07-31T12:00:58.107801: step 12840, loss 0.557759.
Train: 2018-07-31T12:00:58.279636: step 12841, loss 0.605806.
Train: 2018-07-31T12:00:58.451495: step 12842, loss 0.605899.
Train: 2018-07-31T12:00:58.638961: step 12843, loss 0.495701.
Train: 2018-07-31T12:00:58.810762: step 12844, loss 0.673965.
Train: 2018-07-31T12:00:58.982596: step 12845, loss 0.563488.
Train: 2018-07-31T12:00:59.170053: step 12846, loss 0.520966.
Train: 2018-07-31T12:00:59.341888: step 12847, loss 0.529262.
Train: 2018-07-31T12:00:59.513751: step 12848, loss 0.580006.
Train: 2018-07-31T12:00:59.685556: step 12849, loss 0.563124.
Train: 2018-07-31T12:00:59.857392: step 12850, loss 0.587056.
Test: 2018-07-31T12:01:00.341653: step 12850, loss 0.557096.
Train: 2018-07-31T12:01:00.513487: step 12851, loss 0.535811.
Train: 2018-07-31T12:01:00.685352: step 12852, loss 0.578357.
Train: 2018-07-31T12:01:00.872813: step 12853, loss 0.544.
Train: 2018-07-31T12:01:01.044638: step 12854, loss 0.587663.
Train: 2018-07-31T12:01:01.216482: step 12855, loss 0.562387.
Train: 2018-07-31T12:01:01.403935: step 12856, loss 0.64838.
Train: 2018-07-31T12:01:01.575769: step 12857, loss 0.596592.
Train: 2018-07-31T12:01:01.747608: step 12858, loss 0.665261.
Train: 2018-07-31T12:01:01.919408: step 12859, loss 0.587738.
Train: 2018-07-31T12:01:02.091243: step 12860, loss 0.553409.
Test: 2018-07-31T12:01:02.575535: step 12860, loss 0.556152.
Train: 2018-07-31T12:01:02.747373: step 12861, loss 0.53631.
Train: 2018-07-31T12:01:02.934796: step 12862, loss 0.595937.
Train: 2018-07-31T12:01:03.122287: step 12863, loss 0.570285.
Train: 2018-07-31T12:01:03.294087: step 12864, loss 0.544699.
Train: 2018-07-31T12:01:03.465922: step 12865, loss 0.468196.
Train: 2018-07-31T12:01:03.653386: step 12866, loss 0.612495.
Train: 2018-07-31T12:01:03.825213: step 12867, loss 0.518834.
Train: 2018-07-31T12:01:03.997047: step 12868, loss 0.578304.
Train: 2018-07-31T12:01:04.168908: step 12869, loss 0.595294.
Train: 2018-07-31T12:01:04.340747: step 12870, loss 0.5867.
Test: 2018-07-31T12:01:04.824978: step 12870, loss 0.555271.
Train: 2018-07-31T12:01:04.996847: step 12871, loss 0.518346.
Train: 2018-07-31T12:01:05.168678: step 12872, loss 0.595153.
Train: 2018-07-31T12:01:05.356137: step 12873, loss 0.586555.
Train: 2018-07-31T12:01:05.543559: step 12874, loss 0.569388.
Train: 2018-07-31T12:01:05.715395: step 12875, loss 0.586443.
Train: 2018-07-31T12:01:05.887230: step 12876, loss 0.671946.
Train: 2018-07-31T12:01:06.059095: step 12877, loss 0.535083.
Train: 2018-07-31T12:01:06.230900: step 12878, loss 0.560667.
Train: 2018-07-31T12:01:06.402734: step 12879, loss 0.577677.
Train: 2018-07-31T12:01:06.590189: step 12880, loss 0.594653.
Test: 2018-07-31T12:01:07.058829: step 12880, loss 0.554845.
Train: 2018-07-31T12:01:07.246287: step 12881, loss 0.654081.
Train: 2018-07-31T12:01:07.418151: step 12882, loss 0.543618.
Train: 2018-07-31T12:01:07.605607: step 12883, loss 0.585927.
Train: 2018-07-31T12:01:07.777412: step 12884, loss 0.535214.
Train: 2018-07-31T12:01:07.949276: step 12885, loss 0.543651.
Train: 2018-07-31T12:01:08.136727: step 12886, loss 0.509917.
Train: 2018-07-31T12:01:08.324159: step 12887, loss 0.560439.
Train: 2018-07-31T12:01:08.496027: step 12888, loss 0.577282.
Train: 2018-07-31T12:01:08.683449: step 12889, loss 0.619515.
Train: 2018-07-31T12:01:08.855314: step 12890, loss 0.63637.
Test: 2018-07-31T12:01:09.339576: step 12890, loss 0.55462.
Train: 2018-07-31T12:01:09.511411: step 12891, loss 0.543438.
Train: 2018-07-31T12:01:09.683249: step 12892, loss 0.467556.
Train: 2018-07-31T12:01:09.855080: step 12893, loss 0.50115.
Train: 2018-07-31T12:01:10.042536: step 12894, loss 0.627925.
Train: 2018-07-31T12:01:10.214372: step 12895, loss 0.526246.
Train: 2018-07-31T12:01:10.386206: step 12896, loss 0.560109.
Train: 2018-07-31T12:01:10.558012: step 12897, loss 0.543038.
Train: 2018-07-31T12:01:10.745492: step 12898, loss 0.551484.
Train: 2018-07-31T12:01:10.932923: step 12899, loss 0.645524.
Train: 2018-07-31T12:01:11.104787: step 12900, loss 0.568504.
Test: 2018-07-31T12:01:11.573397: step 12900, loss 0.554133.
Train: 2018-07-31T12:01:12.338868: step 12901, loss 0.508506.
Train: 2018-07-31T12:01:12.526330: step 12902, loss 0.508375.
Train: 2018-07-31T12:01:12.698164: step 12903, loss 0.602869.
Train: 2018-07-31T12:01:12.881511: step 12904, loss 0.525303.
Train: 2018-07-31T12:01:13.053315: step 12905, loss 0.620267.
Train: 2018-07-31T12:01:13.225180: step 12906, loss 0.533766.
Train: 2018-07-31T12:01:13.396984: step 12907, loss 0.533703.
Train: 2018-07-31T12:01:13.568849: step 12908, loss 0.594416.
Train: 2018-07-31T12:01:13.756305: step 12909, loss 0.61184.
Train: 2018-07-31T12:01:13.928110: step 12910, loss 0.507419.
Test: 2018-07-31T12:01:14.396780: step 12910, loss 0.55373.
Train: 2018-07-31T12:01:14.584236: step 12911, loss 0.611894.
Train: 2018-07-31T12:01:14.756065: step 12912, loss 0.542156.
Train: 2018-07-31T12:01:14.927906: step 12913, loss 0.603186.
Train: 2018-07-31T12:01:15.115362: step 12914, loss 0.56827.
Train: 2018-07-31T12:01:15.287199: step 12915, loss 0.629304.
Train: 2018-07-31T12:01:15.459031: step 12916, loss 0.533384.
Train: 2018-07-31T12:01:15.646457: step 12917, loss 0.585617.
Train: 2018-07-31T12:01:15.818291: step 12918, loss 0.507311.
Train: 2018-07-31T12:01:15.990157: step 12919, loss 0.515989.
Train: 2018-07-31T12:01:16.161961: step 12920, loss 0.550745.
Test: 2018-07-31T12:01:16.646223: step 12920, loss 0.553547.
Train: 2018-07-31T12:01:16.818058: step 12921, loss 0.524551.
Train: 2018-07-31T12:01:16.989917: step 12922, loss 0.61183.
Train: 2018-07-31T12:01:17.161760: step 12923, loss 0.58567.
Train: 2018-07-31T12:01:17.349208: step 12924, loss 0.550703.
Train: 2018-07-31T12:01:17.521051: step 12925, loss 0.576868.
Train: 2018-07-31T12:01:17.692852: step 12926, loss 0.594369.
Train: 2018-07-31T12:01:17.864718: step 12927, loss 0.594347.
Train: 2018-07-31T12:01:18.036555: step 12928, loss 0.550595.
Train: 2018-07-31T12:01:18.223979: step 12929, loss 0.533116.
Train: 2018-07-31T12:01:18.395837: step 12930, loss 0.506889.
Test: 2018-07-31T12:01:18.864483: step 12930, loss 0.553398.
Train: 2018-07-31T12:01:19.051939: step 12931, loss 0.533057.
Train: 2018-07-31T12:01:19.223775: step 12932, loss 0.541759.
Train: 2018-07-31T12:01:19.395580: step 12933, loss 0.638263.
Train: 2018-07-31T12:01:19.583065: step 12934, loss 0.462675.
Train: 2018-07-31T12:01:19.754902: step 12935, loss 0.664806.
Train: 2018-07-31T12:01:19.926704: step 12936, loss 0.55922.
Train: 2018-07-31T12:01:20.098540: step 12937, loss 0.568005.
Train: 2018-07-31T12:01:20.286029: step 12938, loss 0.576787.
Train: 2018-07-31T12:01:20.457831: step 12939, loss 0.603149.
Train: 2018-07-31T12:01:20.629696: step 12940, loss 0.515253.
Test: 2018-07-31T12:01:21.113957: step 12940, loss 0.553233.
Train: 2018-07-31T12:01:21.285794: step 12941, loss 0.550376.
Train: 2018-07-31T12:01:21.457626: step 12942, loss 0.541578.
Train: 2018-07-31T12:01:21.629431: step 12943, loss 0.594287.
Train: 2018-07-31T12:01:21.816887: step 12944, loss 0.576695.
Train: 2018-07-31T12:01:21.988722: step 12945, loss 0.532757.
Train: 2018-07-31T12:01:22.160589: step 12946, loss 0.532739.
Train: 2018-07-31T12:01:22.332422: step 12947, loss 0.576667.
Train: 2018-07-31T12:01:22.519878: step 12948, loss 0.638229.
Train: 2018-07-31T12:01:22.691706: step 12949, loss 0.620564.
Train: 2018-07-31T12:01:22.863547: step 12950, loss 0.559057.
Test: 2018-07-31T12:01:23.347795: step 12950, loss 0.553146.
Train: 2018-07-31T12:01:23.519614: step 12951, loss 0.559052.
Train: 2018-07-31T12:01:23.691448: step 12952, loss 0.559048.
Train: 2018-07-31T12:01:23.863283: step 12953, loss 0.637595.
Train: 2018-07-31T12:01:24.050740: step 12954, loss 0.646102.
Train: 2018-07-31T12:01:24.222600: step 12955, loss 0.524369.
Train: 2018-07-31T12:01:24.394409: step 12956, loss 0.507172.
Train: 2018-07-31T12:01:24.566243: step 12957, loss 0.567701.
Train: 2018-07-31T12:01:24.738108: step 12958, loss 0.498673.
Train: 2018-07-31T12:01:24.909912: step 12959, loss 0.619452.
Train: 2018-07-31T12:01:25.097369: step 12960, loss 0.671128.
Test: 2018-07-31T12:01:25.566010: step 12960, loss 0.553257.
Train: 2018-07-31T12:01:25.753498: step 12961, loss 0.636439.
Train: 2018-07-31T12:01:25.925300: step 12962, loss 0.541951.
Train: 2018-07-31T12:01:26.112756: step 12963, loss 0.542019.
Train: 2018-07-31T12:01:26.284624: step 12964, loss 0.618761.
Train: 2018-07-31T12:01:26.456450: step 12965, loss 0.525144.
Train: 2018-07-31T12:01:26.628260: step 12966, loss 0.567623.
Train: 2018-07-31T12:01:26.800095: step 12967, loss 0.576087.
Train: 2018-07-31T12:01:26.971960: step 12968, loss 0.525331.
Train: 2018-07-31T12:01:27.159419: step 12969, loss 0.533794.
Train: 2018-07-31T12:01:27.331251: step 12970, loss 0.542228.
Test: 2018-07-31T12:01:27.799860: step 12970, loss 0.55341.
Train: 2018-07-31T12:01:27.971725: step 12971, loss 0.567584.
Train: 2018-07-31T12:01:28.159185: step 12972, loss 0.609927.
Train: 2018-07-31T12:01:28.330986: step 12973, loss 0.601447.
Train: 2018-07-31T12:01:28.502852: step 12974, loss 0.516766.
Train: 2018-07-31T12:01:28.674656: step 12975, loss 0.53367.
Train: 2018-07-31T12:01:28.846522: step 12976, loss 0.626894.
Train: 2018-07-31T12:01:29.018325: step 12977, loss 0.542091.
Train: 2018-07-31T12:01:29.205783: step 12978, loss 0.635386.
Train: 2018-07-31T12:01:29.377617: step 12979, loss 0.550561.
Train: 2018-07-31T12:01:29.549452: step 12980, loss 0.533614.
Test: 2018-07-31T12:01:30.018122: step 12980, loss 0.553301.
Train: 2018-07-31T12:01:30.189960: step 12981, loss 0.618355.
Train: 2018-07-31T12:01:30.361761: step 12982, loss 0.575962.
Train: 2018-07-31T12:01:30.533596: step 12983, loss 0.533623.
Train: 2018-07-31T12:01:30.705461: step 12984, loss 0.55901.
Train: 2018-07-31T12:01:30.877296: step 12985, loss 0.533589.
Train: 2018-07-31T12:01:31.049130: step 12986, loss 0.603634.
Train: 2018-07-31T12:01:31.220965: step 12987, loss 0.601374.
Train: 2018-07-31T12:01:31.392769: step 12988, loss 0.575921.
Train: 2018-07-31T12:01:31.564604: step 12989, loss 0.575911.
Train: 2018-07-31T12:01:31.752091: step 12990, loss 0.58437.
Test: 2018-07-31T12:01:32.236322: step 12990, loss 0.553252.
Train: 2018-07-31T12:01:32.408190: step 12991, loss 0.567426.
Train: 2018-07-31T12:01:32.579993: step 12992, loss 0.56742.
Train: 2018-07-31T12:01:32.767480: step 12993, loss 0.60123.
Train: 2018-07-31T12:01:32.939319: step 12994, loss 0.61808.
Train: 2018-07-31T12:01:33.111148: step 12995, loss 0.626412.
Train: 2018-07-31T12:01:33.282985: step 12996, loss 0.533796.
Train: 2018-07-31T12:01:33.454787: step 12997, loss 0.676486.
Train: 2018-07-31T12:01:33.626622: step 12998, loss 0.43369.
Train: 2018-07-31T12:01:33.798457: step 12999, loss 0.525653.
Train: 2018-07-31T12:01:33.970291: step 13000, loss 0.575774.
Test: 2018-07-31T12:01:34.454553: step 13000, loss 0.553401.
Train: 2018-07-31T12:01:35.204378: step 13001, loss 0.483783.
Train: 2018-07-31T12:01:35.376242: step 13002, loss 0.567388.
Train: 2018-07-31T12:01:35.548047: step 13003, loss 0.558973.
Train: 2018-07-31T12:01:35.735503: step 13004, loss 0.617849.
Train: 2018-07-31T12:01:35.907337: step 13005, loss 0.47468.
Train: 2018-07-31T12:01:36.079203: step 13006, loss 0.668698.
Train: 2018-07-31T12:01:36.251037: step 13007, loss 0.626485.
Train: 2018-07-31T12:01:36.422875: step 13008, loss 0.634892.
Train: 2018-07-31T12:01:36.594708: step 13009, loss 0.542017.
Train: 2018-07-31T12:01:36.766511: step 13010, loss 0.499924.
Test: 2018-07-31T12:01:37.250805: step 13010, loss 0.553188.
Train: 2018-07-31T12:01:37.422609: step 13011, loss 0.617866.
Train: 2018-07-31T12:01:37.594467: step 13012, loss 0.634682.
Train: 2018-07-31T12:01:37.766278: step 13013, loss 0.609342.
Train: 2018-07-31T12:01:37.938142: step 13014, loss 0.584081.
Train: 2018-07-31T12:01:38.109947: step 13015, loss 0.533828.
Train: 2018-07-31T12:01:38.297434: step 13016, loss 0.492076.
Train: 2018-07-31T12:01:38.469262: step 13017, loss 0.525485.
Train: 2018-07-31T12:01:38.641073: step 13018, loss 0.567286.
Train: 2018-07-31T12:01:38.812907: step 13019, loss 0.584045.
Train: 2018-07-31T12:01:39.000388: step 13020, loss 0.600838.
Test: 2018-07-31T12:01:39.469033: step 13020, loss 0.553192.
Train: 2018-07-31T12:01:39.656493: step 13021, loss 0.592444.
Train: 2018-07-31T12:01:39.828325: step 13022, loss 0.567249.
Train: 2018-07-31T12:01:40.000130: step 13023, loss 0.550453.
Train: 2018-07-31T12:01:40.171963: step 13024, loss 0.483251.
Train: 2018-07-31T12:01:40.343799: step 13025, loss 0.525138.
Train: 2018-07-31T12:01:40.515634: step 13026, loss 0.584082.
Train: 2018-07-31T12:01:40.687467: step 13027, loss 0.584107.
Train: 2018-07-31T12:01:40.859304: step 13028, loss 0.567177.
Train: 2018-07-31T12:01:41.046789: step 13029, loss 0.490769.
Train: 2018-07-31T12:01:41.218624: step 13030, loss 0.61824.
Test: 2018-07-31T12:01:41.687233: step 13030, loss 0.552861.
Train: 2018-07-31T12:01:41.859099: step 13031, loss 0.464794.
Train: 2018-07-31T12:01:42.030928: step 13032, loss 0.567137.
Train: 2018-07-31T12:01:42.202769: step 13033, loss 0.56713.
Train: 2018-07-31T12:01:42.374603: step 13034, loss 0.575738.
Train: 2018-07-31T12:01:42.546433: step 13035, loss 0.55849.
Train: 2018-07-31T12:01:42.733894: step 13036, loss 0.644997.
Train: 2018-07-31T12:01:42.905699: step 13037, loss 0.541143.
Train: 2018-07-31T12:01:43.077564: step 13038, loss 0.532449.
Train: 2018-07-31T12:01:43.249368: step 13039, loss 0.584468.
Train: 2018-07-31T12:01:43.421204: step 13040, loss 0.645289.
Test: 2018-07-31T12:01:43.889842: step 13040, loss 0.552561.
Train: 2018-07-31T12:01:44.077300: step 13041, loss 0.532375.
Train: 2018-07-31T12:01:44.249135: step 13042, loss 0.541051.
Train: 2018-07-31T12:01:44.420970: step 13043, loss 0.575774.
Train: 2018-07-31T12:01:44.608426: step 13044, loss 0.514966.
Train: 2018-07-31T12:01:44.780293: step 13045, loss 0.532299.
Train: 2018-07-31T12:01:44.952125: step 13046, loss 0.601918.
Train: 2018-07-31T12:01:45.123960: step 13047, loss 0.567078.
Train: 2018-07-31T12:01:45.295795: step 13048, loss 0.636834.
Train: 2018-07-31T12:01:45.467599: step 13049, loss 0.593203.
Train: 2018-07-31T12:01:45.639433: step 13050, loss 0.549654.
Test: 2018-07-31T12:01:46.123726: step 13050, loss 0.552489.
Train: 2018-07-31T12:01:46.295530: step 13051, loss 0.584432.
Train: 2018-07-31T12:01:46.467364: step 13052, loss 0.610447.
Train: 2018-07-31T12:01:46.639223: step 13053, loss 0.627681.
Train: 2018-07-31T12:01:46.811035: step 13054, loss 0.558377.
Train: 2018-07-31T12:01:46.982870: step 13055, loss 0.532538.
Train: 2018-07-31T12:01:47.154705: step 13056, loss 0.610015.
Train: 2018-07-31T12:01:47.326539: step 13057, loss 0.601325.
Train: 2018-07-31T12:01:47.498373: step 13058, loss 0.601226.
Train: 2018-07-31T12:01:47.670238: step 13059, loss 0.515792.
Train: 2018-07-31T12:01:47.826421: step 13060, loss 0.507366.
Test: 2018-07-31T12:01:48.310713: step 13060, loss 0.552721.
Train: 2018-07-31T12:01:48.482549: step 13061, loss 0.498875.
Train: 2018-07-31T12:01:48.654385: step 13062, loss 0.601056.
Train: 2018-07-31T12:01:48.826218: step 13063, loss 0.584012.
Train: 2018-07-31T12:01:48.998022: step 13064, loss 0.549916.
Train: 2018-07-31T12:01:49.169887: step 13065, loss 0.532857.
Train: 2018-07-31T12:01:49.341716: step 13066, loss 0.549884.
Train: 2018-07-31T12:01:49.513557: step 13067, loss 0.455887.
Train: 2018-07-31T12:01:49.685361: step 13068, loss 0.532654.
Train: 2018-07-31T12:01:49.841576: step 13069, loss 0.584132.
Train: 2018-07-31T12:01:50.029064: step 13070, loss 0.592804.
Test: 2018-07-31T12:01:50.497701: step 13070, loss 0.55245.
Train: 2018-07-31T12:01:50.669535: step 13071, loss 0.592855.
Train: 2018-07-31T12:01:50.841371: step 13072, loss 0.584232.
Train: 2018-07-31T12:01:51.013208: step 13073, loss 0.506288.
Train: 2018-07-31T12:01:51.185034: step 13074, loss 0.454111.
Train: 2018-07-31T12:01:51.356877: step 13075, loss 0.558215.
Train: 2018-07-31T12:01:51.528712: step 13076, loss 0.663067.
Train: 2018-07-31T12:01:51.684923: step 13077, loss 0.610683.
Train: 2018-07-31T12:01:51.856759: step 13078, loss 0.505666.
Train: 2018-07-31T12:01:52.044184: step 13079, loss 0.540642.
Train: 2018-07-31T12:01:52.200397: step 13080, loss 0.523054.
Test: 2018-07-31T12:01:52.684689: step 13080, loss 0.552211.
Train: 2018-07-31T12:01:52.872115: step 13081, loss 0.522971.
Train: 2018-07-31T12:01:53.028362: step 13082, loss 0.558133.
Train: 2018-07-31T12:01:53.200193: step 13083, loss 0.575795.
Train: 2018-07-31T12:01:53.372028: step 13084, loss 0.637783.
Train: 2018-07-31T12:01:53.543834: step 13085, loss 0.584671.
Train: 2018-07-31T12:01:53.715699: step 13086, loss 0.611214.
Train: 2018-07-31T12:01:53.887502: step 13087, loss 0.513895.
Train: 2018-07-31T12:01:54.059336: step 13088, loss 0.566936.
Train: 2018-07-31T12:01:54.231172: step 13089, loss 0.549256.
Train: 2018-07-31T12:01:54.403031: step 13090, loss 0.59343.
Test: 2018-07-31T12:01:54.871678: step 13090, loss 0.552126.
Train: 2018-07-31T12:01:55.043512: step 13091, loss 0.549255.
Train: 2018-07-31T12:01:55.215317: step 13092, loss 0.575732.
Train: 2018-07-31T12:01:55.387181: step 13093, loss 0.549257.
Train: 2018-07-31T12:01:55.558986: step 13094, loss 0.63742.
Train: 2018-07-31T12:01:55.730851: step 13095, loss 0.549272.
Train: 2018-07-31T12:01:55.902680: step 13096, loss 0.549283.
Train: 2018-07-31T12:01:56.058868: step 13097, loss 0.50539.
Train: 2018-07-31T12:01:56.230703: step 13098, loss 0.540503.
Train: 2018-07-31T12:01:56.402571: step 13099, loss 0.549273.
Train: 2018-07-31T12:01:56.574404: step 13100, loss 0.566842.
Test: 2018-07-31T12:01:57.043045: step 13100, loss 0.552111.
Train: 2018-07-31T12:01:57.792862: step 13101, loss 0.54925.
Train: 2018-07-31T12:01:57.964703: step 13102, loss 0.663648.
Train: 2018-07-31T12:01:58.136540: step 13103, loss 0.558038.
Train: 2018-07-31T12:01:58.308373: step 13104, loss 0.584376.
Train: 2018-07-31T12:01:58.480208: step 13105, loss 0.549269.
Train: 2018-07-31T12:01:58.652011: step 13106, loss 0.549278.
Train: 2018-07-31T12:01:58.823846: step 13107, loss 0.593042.
Train: 2018-07-31T12:01:58.980090: step 13108, loss 0.566776.
Train: 2018-07-31T12:01:59.167515: step 13109, loss 0.584229.
Train: 2018-07-31T12:01:59.339381: step 13110, loss 0.5406.
Test: 2018-07-31T12:01:59.823612: step 13110, loss 0.552159.
Train: 2018-07-31T12:01:59.979826: step 13111, loss 0.59288.
Train: 2018-07-31T12:02:00.167282: step 13112, loss 0.488448.
Train: 2018-07-31T12:02:00.323525: step 13113, loss 0.540633.
Train: 2018-07-31T12:02:00.495331: step 13114, loss 0.601554.
Train: 2018-07-31T12:02:00.667195: step 13115, loss 0.48839.
Train: 2018-07-31T12:02:00.839030: step 13116, loss 0.575441.
Train: 2018-07-31T12:02:01.010865: step 13117, loss 0.505668.
Train: 2018-07-31T12:02:01.182669: step 13118, loss 0.531774.
Train: 2018-07-31T12:02:01.354528: step 13119, loss 0.522943.
Train: 2018-07-31T12:02:01.526339: step 13120, loss 0.549179.
Test: 2018-07-31T12:02:01.995008: step 13120, loss 0.552007.
Train: 2018-07-31T12:02:02.166843: step 13121, loss 0.522728.
Train: 2018-07-31T12:02:02.338680: step 13122, loss 0.681587.
Train: 2018-07-31T12:02:02.510483: step 13123, loss 0.549093.
Train: 2018-07-31T12:02:02.682317: step 13124, loss 0.566768.
Train: 2018-07-31T12:02:02.854152: step 13125, loss 0.611006.
Train: 2018-07-31T12:02:03.010396: step 13126, loss 0.584446.
Train: 2018-07-31T12:02:03.197857: step 13127, loss 0.531405.
Train: 2018-07-31T12:02:03.354036: step 13128, loss 0.619741.
Train: 2018-07-31T12:02:03.525871: step 13129, loss 0.628479.
Train: 2018-07-31T12:02:03.697706: step 13130, loss 0.557909.
Test: 2018-07-31T12:02:04.181992: step 13130, loss 0.551986.
Train: 2018-07-31T12:02:04.353802: step 13131, loss 0.557911.
Train: 2018-07-31T12:02:04.525637: step 13132, loss 0.549148.
Train: 2018-07-31T12:02:04.697472: step 13133, loss 0.619178.
Train: 2018-07-31T12:02:04.869306: step 13134, loss 0.566651.
Train: 2018-07-31T12:02:05.041141: step 13135, loss 0.61891.
Train: 2018-07-31T12:02:05.228620: step 13136, loss 0.57531.
Train: 2018-07-31T12:02:05.369218: step 13137, loss 0.548138.
Train: 2018-07-31T12:02:05.541053: step 13138, loss 0.514767.
Train: 2018-07-31T12:02:05.712857: step 13139, loss 0.583856.
Train: 2018-07-31T12:02:05.884692: step 13140, loss 0.497655.
Test: 2018-07-31T12:02:06.353333: step 13140, loss 0.552155.
Train: 2018-07-31T12:02:06.540789: step 13141, loss 0.583821.
Train: 2018-07-31T12:02:06.712623: step 13142, loss 0.575196.
Train: 2018-07-31T12:02:06.884459: step 13143, loss 0.618238.
Train: 2018-07-31T12:02:07.040702: step 13144, loss 0.549376.
Train: 2018-07-31T12:02:07.212537: step 13145, loss 0.618103.
Train: 2018-07-31T12:02:07.384341: step 13146, loss 0.618003.
Train: 2018-07-31T12:02:07.556208: step 13147, loss 0.592211.
Train: 2018-07-31T12:02:07.728012: step 13148, loss 0.634764.
Train: 2018-07-31T12:02:07.899879: step 13149, loss 0.592038.
Train: 2018-07-31T12:02:08.056089: step 13150, loss 0.498898.
Test: 2018-07-31T12:02:08.540352: step 13150, loss 0.552435.
Train: 2018-07-31T12:02:08.712185: step 13151, loss 0.47373.
Train: 2018-07-31T12:02:08.884015: step 13152, loss 0.625642.
Train: 2018-07-31T12:02:09.055855: step 13153, loss 0.49913.
Train: 2018-07-31T12:02:09.212068: step 13154, loss 0.541263.
Train: 2018-07-31T12:02:09.383874: step 13155, loss 0.608754.
Train: 2018-07-31T12:02:09.555740: step 13156, loss 0.634074.
Train: 2018-07-31T12:02:09.727542: step 13157, loss 0.58341.
Train: 2018-07-31T12:02:09.899412: step 13158, loss 0.56655.
Train: 2018-07-31T12:02:10.055621: step 13159, loss 0.58337.
Train: 2018-07-31T12:02:10.227427: step 13160, loss 0.566553.
Test: 2018-07-31T12:02:10.711717: step 13160, loss 0.552502.
Train: 2018-07-31T12:02:10.883548: step 13161, loss 0.549774.
Train: 2018-07-31T12:02:11.055387: step 13162, loss 0.608479.
Train: 2018-07-31T12:02:11.227193: step 13163, loss 0.625181.
Train: 2018-07-31T12:02:11.399027: step 13164, loss 0.583276.
Train: 2018-07-31T12:02:11.570861: step 13165, loss 0.583248.
Train: 2018-07-31T12:02:11.742726: step 13166, loss 0.508332.
Train: 2018-07-31T12:02:11.961425: step 13167, loss 0.583212.
Train: 2018-07-31T12:02:12.133259: step 13168, loss 0.599819.
Train: 2018-07-31T12:02:12.289443: step 13169, loss 0.583184.
Train: 2018-07-31T12:02:12.461308: step 13170, loss 0.591456.
Test: 2018-07-31T12:02:12.945570: step 13170, loss 0.552739.
Train: 2018-07-31T12:02:13.117374: step 13171, loss 0.516944.
Train: 2018-07-31T12:02:13.289238: step 13172, loss 0.550048.
Train: 2018-07-31T12:02:13.461044: step 13173, loss 0.574866.
Train: 2018-07-31T12:02:13.617256: step 13174, loss 0.541746.
Train: 2018-07-31T12:02:13.789093: step 13175, loss 0.500279.
Train: 2018-07-31T12:02:13.960926: step 13176, loss 0.558245.
Train: 2018-07-31T12:02:14.132794: step 13177, loss 0.549878.
Train: 2018-07-31T12:02:14.304626: step 13178, loss 0.574858.
Train: 2018-07-31T12:02:14.476461: step 13179, loss 0.54139.
Train: 2018-07-31T12:02:14.648298: step 13180, loss 0.549695.
Test: 2018-07-31T12:02:15.116938: step 13180, loss 0.552366.
Train: 2018-07-31T12:02:15.288740: step 13181, loss 0.524388.
Train: 2018-07-31T12:02:15.460574: step 13182, loss 0.54111.
Train: 2018-07-31T12:02:15.632441: step 13183, loss 0.600323.
Train: 2018-07-31T12:02:15.788623: step 13184, loss 0.566414.
Train: 2018-07-31T12:02:15.960458: step 13185, loss 0.498257.
Train: 2018-07-31T12:02:16.132323: step 13186, loss 0.617691.
Train: 2018-07-31T12:02:16.304128: step 13187, loss 0.592096.
Train: 2018-07-31T12:02:16.475993: step 13188, loss 0.549226.
Train: 2018-07-31T12:02:16.632209: step 13189, loss 0.600766.
Train: 2018-07-31T12:02:16.804042: step 13190, loss 0.56638.
Test: 2018-07-31T12:02:17.288302: step 13190, loss 0.551958.
Train: 2018-07-31T12:02:17.460140: step 13191, loss 0.557768.
Train: 2018-07-31T12:02:17.631941: step 13192, loss 0.506059.
Train: 2018-07-31T12:02:17.803776: step 13193, loss 0.531842.
Train: 2018-07-31T12:02:17.975612: step 13194, loss 0.505805.
Train: 2018-07-31T12:02:18.147479: step 13195, loss 0.514296.
Train: 2018-07-31T12:02:18.303659: step 13196, loss 0.627372.
Train: 2018-07-31T12:02:18.475495: step 13197, loss 0.575119.
Train: 2018-07-31T12:02:18.647330: step 13198, loss 0.548898.
Train: 2018-07-31T12:02:18.819165: step 13199, loss 0.566396.
Train: 2018-07-31T12:02:18.991029: step 13200, loss 0.575175.
Test: 2018-07-31T12:02:19.459669: step 13200, loss 0.551688.
Train: 2018-07-31T12:02:20.193873: step 13201, loss 0.583973.
Train: 2018-07-31T12:02:20.365706: step 13202, loss 0.54882.
Train: 2018-07-31T12:02:20.537511: step 13203, loss 0.557604.
Train: 2018-07-31T12:02:20.709347: step 13204, loss 0.636827.
Train: 2018-07-31T12:02:20.881211: step 13205, loss 0.522411.
Train: 2018-07-31T12:02:21.053015: step 13206, loss 0.610373.
Train: 2018-07-31T12:02:21.224881: step 13207, loss 0.557592.
Train: 2018-07-31T12:02:21.396718: step 13208, loss 0.55759.
Train: 2018-07-31T12:02:21.568550: step 13209, loss 0.504928.
Train: 2018-07-31T12:02:21.756001: step 13210, loss 0.583926.
Test: 2018-07-31T12:02:22.224615: step 13210, loss 0.551654.
Train: 2018-07-31T12:02:22.396450: step 13211, loss 0.513674.
Train: 2018-07-31T12:02:22.568287: step 13212, loss 0.601516.
Train: 2018-07-31T12:02:22.740145: step 13213, loss 0.592725.
Train: 2018-07-31T12:02:22.911985: step 13214, loss 0.548782.
Train: 2018-07-31T12:02:23.083790: step 13215, loss 0.59269.
Train: 2018-07-31T12:02:23.255655: step 13216, loss 0.583886.
Train: 2018-07-31T12:02:23.427459: step 13217, loss 0.592619.
Train: 2018-07-31T12:02:23.583674: step 13218, loss 0.601315.
Train: 2018-07-31T12:02:23.771162: step 13219, loss 0.53138.
Train: 2018-07-31T12:02:23.942963: step 13220, loss 0.53142.
Test: 2018-07-31T12:02:24.427225: step 13220, loss 0.551696.
Train: 2018-07-31T12:02:24.599061: step 13221, loss 0.574999.
Train: 2018-07-31T12:02:24.755274: step 13222, loss 0.592394.
Train: 2018-07-31T12:02:24.927138: step 13223, loss 0.453261.
Train: 2018-07-31T12:02:25.098973: step 13224, loss 0.548869.
Train: 2018-07-31T12:02:25.270777: step 13225, loss 0.548851.
Train: 2018-07-31T12:02:25.442614: step 13226, loss 0.540111.
Train: 2018-07-31T12:02:25.598826: step 13227, loss 0.522607.
Train: 2018-07-31T12:02:25.770693: step 13228, loss 0.645042.
Train: 2018-07-31T12:02:25.942526: step 13229, loss 0.548768.
Train: 2018-07-31T12:02:26.114331: step 13230, loss 0.618829.
Test: 2018-07-31T12:02:26.598622: step 13230, loss 0.551608.
Train: 2018-07-31T12:02:26.770427: step 13231, loss 0.5225.
Train: 2018-07-31T12:02:26.942262: step 13232, loss 0.548755.
Train: 2018-07-31T12:02:27.114096: step 13233, loss 0.566265.
Train: 2018-07-31T12:02:27.285961: step 13234, loss 0.548739.
Train: 2018-07-31T12:02:27.442144: step 13235, loss 0.557495.
Train: 2018-07-31T12:02:27.629601: step 13236, loss 0.531178.
Train: 2018-07-31T12:02:27.785813: step 13237, loss 0.601379.
Train: 2018-07-31T12:02:27.957648: step 13238, loss 0.548697.
Train: 2018-07-31T12:02:28.129483: step 13239, loss 0.548689.
Train: 2018-07-31T12:02:28.301349: step 13240, loss 0.618989.
Test: 2018-07-31T12:02:28.785610: step 13240, loss 0.551539.
Train: 2018-07-31T12:02:28.957445: step 13241, loss 0.583817.
Train: 2018-07-31T12:02:29.129280: step 13242, loss 0.54869.
Train: 2018-07-31T12:02:29.301115: step 13243, loss 0.583774.
Train: 2018-07-31T12:02:29.472918: step 13244, loss 0.574987.
Train: 2018-07-31T12:02:29.644787: step 13245, loss 0.557465.
Train: 2018-07-31T12:02:29.800998: step 13246, loss 0.592438.
Train: 2018-07-31T12:02:29.972832: step 13247, loss 0.522546.
Train: 2018-07-31T12:02:30.144669: step 13248, loss 0.609822.
Train: 2018-07-31T12:02:30.316501: step 13249, loss 0.566184.
Train: 2018-07-31T12:02:30.488306: step 13250, loss 0.566176.
Test: 2018-07-31T12:02:30.956945: step 13250, loss 0.551612.
Train: 2018-07-31T12:02:31.128814: step 13251, loss 0.635701.
Train: 2018-07-31T12:02:31.300640: step 13252, loss 0.600838.
Train: 2018-07-31T12:02:31.472480: step 13253, loss 0.583437.
Train: 2018-07-31T12:02:31.628696: step 13254, loss 0.514431.
Train: 2018-07-31T12:02:31.800500: step 13255, loss 0.52312.
Train: 2018-07-31T12:02:31.972360: step 13256, loss 0.574728.
Train: 2018-07-31T12:02:32.144168: step 13257, loss 0.643428.
Train: 2018-07-31T12:02:32.316033: step 13258, loss 0.506138.
Train: 2018-07-31T12:02:32.487868: step 13259, loss 0.591806.
Train: 2018-07-31T12:02:32.659672: step 13260, loss 0.557567.
Test: 2018-07-31T12:02:33.143935: step 13260, loss 0.551809.
Train: 2018-07-31T12:02:33.315769: step 13261, loss 0.617369.
Train: 2018-07-31T12:02:33.487604: step 13262, loss 0.57464.
Train: 2018-07-31T12:02:33.659466: step 13263, loss 0.64271.
Train: 2018-07-31T12:02:33.831304: step 13264, loss 0.489757.
Train: 2018-07-31T12:02:34.003109: step 13265, loss 0.57459.
Train: 2018-07-31T12:02:34.174973: step 13266, loss 0.557653.
Train: 2018-07-31T12:02:34.346808: step 13267, loss 0.523837.
Train: 2018-07-31T12:02:34.518613: step 13268, loss 0.591481.
Train: 2018-07-31T12:02:34.690477: step 13269, loss 0.566109.
Train: 2018-07-31T12:02:34.862281: step 13270, loss 0.523849.
Test: 2018-07-31T12:02:35.346544: step 13270, loss 0.551937.
Train: 2018-07-31T12:02:35.518378: step 13271, loss 0.625301.
Train: 2018-07-31T12:02:35.674616: step 13272, loss 0.532287.
Train: 2018-07-31T12:02:35.846456: step 13273, loss 0.515362.
Train: 2018-07-31T12:02:36.018294: step 13274, loss 0.625353.
Train: 2018-07-31T12:02:36.190126: step 13275, loss 0.523747.
Train: 2018-07-31T12:02:36.361961: step 13276, loss 0.61693.
Train: 2018-07-31T12:02:36.533766: step 13277, loss 0.591498.
Train: 2018-07-31T12:02:36.705630: step 13278, loss 0.549131.
Train: 2018-07-31T12:02:36.877465: step 13279, loss 0.566069.
Train: 2018-07-31T12:02:37.049305: step 13280, loss 0.574535.
Test: 2018-07-31T12:02:37.533561: step 13280, loss 0.551879.
Train: 2018-07-31T12:02:37.752230: step 13281, loss 0.574531.
Train: 2018-07-31T12:02:37.924064: step 13282, loss 0.582992.
Train: 2018-07-31T12:02:38.095930: step 13283, loss 0.50682.
Train: 2018-07-31T12:02:38.267767: step 13284, loss 0.532174.
Train: 2018-07-31T12:02:38.439600: step 13285, loss 0.574526.
Train: 2018-07-31T12:02:38.611429: step 13286, loss 0.540563.
Train: 2018-07-31T12:02:38.783240: step 13287, loss 0.497988.
Train: 2018-07-31T12:02:38.923861: step 13288, loss 0.620612.
Train: 2018-07-31T12:02:39.111286: step 13289, loss 0.57456.
Train: 2018-07-31T12:02:39.283151: step 13290, loss 0.574566.
Test: 2018-07-31T12:02:39.751793: step 13290, loss 0.551673.
Train: 2018-07-31T12:02:39.923626: step 13291, loss 0.677294.
Train: 2018-07-31T12:02:40.095456: step 13292, loss 0.566007.
Train: 2018-07-31T12:02:40.267299: step 13293, loss 0.540388.
Train: 2018-07-31T12:02:40.454752: step 13294, loss 0.583071.
Train: 2018-07-31T12:02:40.626587: step 13295, loss 0.506307.
Train: 2018-07-31T12:02:40.798422: step 13296, loss 0.591591.
Train: 2018-07-31T12:02:40.970258: step 13297, loss 0.557462.
Train: 2018-07-31T12:02:41.142094: step 13298, loss 0.634249.
Train: 2018-07-31T12:02:41.313897: step 13299, loss 0.591555.
Train: 2018-07-31T12:02:41.485762: step 13300, loss 0.497912.
Test: 2018-07-31T12:02:41.970022: step 13300, loss 0.551732.
Train: 2018-07-31T12:02:42.735469: step 13301, loss 0.514928.
Train: 2018-07-31T12:02:42.907273: step 13302, loss 0.668194.
Train: 2018-07-31T12:02:43.063488: step 13303, loss 0.608522.
Train: 2018-07-31T12:02:43.235322: step 13304, loss 0.548988.
Train: 2018-07-31T12:02:43.407155: step 13305, loss 0.481134.
Train: 2018-07-31T12:02:43.594612: step 13306, loss 0.472585.
Train: 2018-07-31T12:02:43.766481: step 13307, loss 0.600005.
Train: 2018-07-31T12:02:43.969524: step 13308, loss 0.608577.
Train: 2018-07-31T12:02:44.125768: step 13309, loss 0.497719.
Train: 2018-07-31T12:02:44.313193: step 13310, loss 0.583039.
Test: 2018-07-31T12:02:44.781835: step 13310, loss 0.551612.
Train: 2018-07-31T12:02:44.953670: step 13311, loss 0.574501.
Train: 2018-07-31T12:02:45.125503: step 13312, loss 0.497405.
Train: 2018-07-31T12:02:45.297368: step 13313, loss 0.591696.
Train: 2018-07-31T12:02:45.469203: step 13314, loss 0.497132.
Train: 2018-07-31T12:02:45.641007: step 13315, loss 0.540062.
Train: 2018-07-31T12:02:45.812842: step 13316, loss 0.548637.
Train: 2018-07-31T12:02:45.984677: step 13317, loss 0.54859.
Train: 2018-07-31T12:02:46.156512: step 13318, loss 0.592039.
Train: 2018-07-31T12:02:46.328377: step 13319, loss 0.539796.
Train: 2018-07-31T12:02:46.500182: step 13320, loss 0.574688.
Test: 2018-07-31T12:02:46.984443: step 13320, loss 0.551299.
Train: 2018-07-31T12:02:47.156278: step 13321, loss 0.539701.
Train: 2018-07-31T12:02:47.328113: step 13322, loss 0.60104.
Train: 2018-07-31T12:02:47.499947: step 13323, loss 0.574744.
Train: 2018-07-31T12:02:47.656191: step 13324, loss 0.583535.
Train: 2018-07-31T12:02:47.828026: step 13325, loss 0.57475.
Train: 2018-07-31T12:02:48.015485: step 13326, loss 0.539603.
Train: 2018-07-31T12:02:48.171695: step 13327, loss 0.530805.
Train: 2018-07-31T12:02:48.359121: step 13328, loss 0.601142.
Train: 2018-07-31T12:02:48.530957: step 13329, loss 0.565957.
Train: 2018-07-31T12:02:48.702817: step 13330, loss 0.583542.
Test: 2018-07-31T12:02:49.187082: step 13330, loss 0.551226.
Train: 2018-07-31T12:02:49.358887: step 13331, loss 0.539578.
Train: 2018-07-31T12:02:49.530753: step 13332, loss 0.601098.
Train: 2018-07-31T12:02:49.702556: step 13333, loss 0.513247.
Train: 2018-07-31T12:02:49.874391: step 13334, loss 0.495674.
Train: 2018-07-31T12:02:50.046251: step 13335, loss 0.565936.
Train: 2018-07-31T12:02:50.218098: step 13336, loss 0.557137.
Train: 2018-07-31T12:02:50.389921: step 13337, loss 0.548319.
Train: 2018-07-31T12:02:50.561755: step 13338, loss 0.548304.
Train: 2018-07-31T12:02:50.733596: step 13339, loss 0.51296.
Train: 2018-07-31T12:02:50.905401: step 13340, loss 0.601357.
Test: 2018-07-31T12:02:51.389661: step 13340, loss 0.551132.
Train: 2018-07-31T12:02:51.561527: step 13341, loss 0.610252.
Train: 2018-07-31T12:02:51.748952: step 13342, loss 0.521679.
Train: 2018-07-31T12:02:51.920818: step 13343, loss 0.557101.
Train: 2018-07-31T12:02:52.092622: step 13344, loss 0.565964.
Train: 2018-07-31T12:02:52.264457: step 13345, loss 0.530485.
Train: 2018-07-31T12:02:52.436323: step 13346, loss 0.548213.
Train: 2018-07-31T12:02:52.608128: step 13347, loss 0.565972.
Train: 2018-07-31T12:02:52.779991: step 13348, loss 0.53041.
Train: 2018-07-31T12:02:52.951828: step 13349, loss 0.548179.
Train: 2018-07-31T12:02:53.123630: step 13350, loss 0.628364.
Test: 2018-07-31T12:02:53.607923: step 13350, loss 0.551062.
Train: 2018-07-31T12:02:53.779726: step 13351, loss 0.592708.
Train: 2018-07-31T12:02:53.951586: step 13352, loss 0.610476.
Train: 2018-07-31T12:02:54.123429: step 13353, loss 0.628152.
Train: 2018-07-31T12:02:54.295230: step 13354, loss 0.619089.
Train: 2018-07-31T12:02:54.467096: step 13355, loss 0.565898.
Train: 2018-07-31T12:02:54.638933: step 13356, loss 0.539481.
Train: 2018-07-31T12:02:54.810735: step 13357, loss 0.583402.
Train: 2018-07-31T12:02:54.982570: step 13358, loss 0.574586.
Train: 2018-07-31T12:02:55.154404: step 13359, loss 0.504759.
Train: 2018-07-31T12:02:55.326271: step 13360, loss 0.618077.
Test: 2018-07-31T12:02:55.810500: step 13360, loss 0.551251.
Train: 2018-07-31T12:02:55.982335: step 13361, loss 0.63532.
Train: 2018-07-31T12:02:56.154201: step 13362, loss 0.531156.
Train: 2018-07-31T12:02:56.326036: step 13363, loss 0.583057.
Train: 2018-07-31T12:02:56.497841: step 13364, loss 0.600231.
Train: 2018-07-31T12:02:56.654085: step 13365, loss 0.60012.
Train: 2018-07-31T12:02:56.825888: step 13366, loss 0.591441.
Train: 2018-07-31T12:02:57.013378: step 13367, loss 0.565771.
Train: 2018-07-31T12:02:57.185179: step 13368, loss 0.574272.
Train: 2018-07-31T12:02:57.357044: step 13369, loss 0.557305.
Train: 2018-07-31T12:02:57.528849: step 13370, loss 0.616489.
Test: 2018-07-31T12:02:57.997522: step 13370, loss 0.551683.
Train: 2018-07-31T12:02:58.169324: step 13371, loss 0.548944.
Train: 2018-07-31T12:02:58.356789: step 13372, loss 0.565799.
Train: 2018-07-31T12:02:58.528639: step 13373, loss 0.616113.
Train: 2018-07-31T12:02:58.700450: step 13374, loss 0.582539.
Train: 2018-07-31T12:02:58.887906: step 13375, loss 0.524148.
Train: 2018-07-31T12:02:59.059771: step 13376, loss 0.532544.
Train: 2018-07-31T12:02:59.231605: step 13377, loss 0.499289.
Train: 2018-07-31T12:02:59.403411: step 13378, loss 0.615793.
Train: 2018-07-31T12:02:59.575245: step 13379, loss 0.61579.
Train: 2018-07-31T12:02:59.747079: step 13380, loss 0.549192.
Test: 2018-07-31T12:03:00.231341: step 13380, loss 0.551901.
Train: 2018-07-31T12:03:00.403206: step 13381, loss 0.557513.
Train: 2018-07-31T12:03:00.575040: step 13382, loss 0.574144.
Train: 2018-07-31T12:03:00.746870: step 13383, loss 0.582457.
Train: 2018-07-31T12:03:00.918680: step 13384, loss 0.615711.
Train: 2018-07-31T12:03:01.090516: step 13385, loss 0.632272.
Train: 2018-07-31T12:03:01.278001: step 13386, loss 0.491255.
Train: 2018-07-31T12:03:01.449837: step 13387, loss 0.524415.
Train: 2018-07-31T12:03:01.621671: step 13388, loss 0.598995.
Train: 2018-07-31T12:03:01.793502: step 13389, loss 0.499489.
Train: 2018-07-31T12:03:01.965310: step 13390, loss 0.540894.
Test: 2018-07-31T12:03:02.433950: step 13390, loss 0.551854.
Train: 2018-07-31T12:03:02.621431: step 13391, loss 0.59909.
Train: 2018-07-31T12:03:02.793271: step 13392, loss 0.62413.
Train: 2018-07-31T12:03:02.965111: step 13393, loss 0.649147.
Train: 2018-07-31T12:03:03.136911: step 13394, loss 0.549136.
Train: 2018-07-31T12:03:03.324367: step 13395, loss 0.540834.
Train: 2018-07-31T12:03:03.496201: step 13396, loss 0.499247.
Train: 2018-07-31T12:03:03.668036: step 13397, loss 0.565777.
Train: 2018-07-31T12:03:03.839895: step 13398, loss 0.540743.
Train: 2018-07-31T12:03:04.011739: step 13399, loss 0.557397.
Train: 2018-07-31T12:03:04.183540: step 13400, loss 0.615987.
Test: 2018-07-31T12:03:04.652181: step 13400, loss 0.551699.
Train: 2018-07-31T12:03:05.433248: step 13401, loss 0.590881.
Train: 2018-07-31T12:03:05.605113: step 13402, loss 0.582502.
Train: 2018-07-31T12:03:05.776917: step 13403, loss 0.548958.
Train: 2018-07-31T12:03:05.948752: step 13404, loss 0.649631.
Train: 2018-07-31T12:03:06.120587: step 13405, loss 0.616016.
Train: 2018-07-31T12:03:06.292452: step 13406, loss 0.507179.
Train: 2018-07-31T12:03:06.464287: step 13407, loss 0.523927.
Train: 2018-07-31T12:03:06.636124: step 13408, loss 0.565729.
Train: 2018-07-31T12:03:06.807925: step 13409, loss 0.574095.
Train: 2018-07-31T12:03:06.979760: step 13410, loss 0.532211.
Test: 2018-07-31T12:03:07.464052: step 13410, loss 0.551661.
Train: 2018-07-31T12:03:07.635890: step 13411, loss 0.523773.
Train: 2018-07-31T12:03:07.807722: step 13412, loss 0.590906.
Train: 2018-07-31T12:03:07.979557: step 13413, loss 0.523603.
Train: 2018-07-31T12:03:08.151387: step 13414, loss 0.607846.
Train: 2018-07-31T12:03:08.323196: step 13415, loss 0.48121.
Train: 2018-07-31T12:03:08.495030: step 13416, loss 0.57412.
Train: 2018-07-31T12:03:08.682487: step 13417, loss 0.506209.
Train: 2018-07-31T12:03:08.854322: step 13418, loss 0.565631.
Train: 2018-07-31T12:03:09.026157: step 13419, loss 0.497258.
Train: 2018-07-31T12:03:09.213643: step 13420, loss 0.574199.
Test: 2018-07-31T12:03:09.682283: step 13420, loss 0.551194.
Train: 2018-07-31T12:03:09.869733: step 13421, loss 0.548394.
Train: 2018-07-31T12:03:10.041568: step 13422, loss 0.548335.
Train: 2018-07-31T12:03:10.213378: step 13423, loss 0.634986.
Train: 2018-07-31T12:03:10.400834: step 13424, loss 0.574307.
Train: 2018-07-31T12:03:10.557078: step 13425, loss 0.609107.
Train: 2018-07-31T12:03:10.744504: step 13426, loss 0.54822.
Train: 2018-07-31T12:03:10.916369: step 13427, loss 0.583027.
Train: 2018-07-31T12:03:11.072582: step 13428, loss 0.55691.
Train: 2018-07-31T12:03:11.260032: step 13429, loss 0.57432.
Train: 2018-07-31T12:03:11.431844: step 13430, loss 0.652674.
Test: 2018-07-31T12:03:11.900514: step 13430, loss 0.551045.
Train: 2018-07-31T12:03:12.072349: step 13431, loss 0.548219.
Train: 2018-07-31T12:03:12.244184: step 13432, loss 0.574274.
Train: 2018-07-31T12:03:12.415987: step 13433, loss 0.600259.
Train: 2018-07-31T12:03:12.587823: step 13434, loss 0.453121.
Train: 2018-07-31T12:03:12.759690: step 13435, loss 0.487695.
Train: 2018-07-31T12:03:12.931491: step 13436, loss 0.539573.
Train: 2018-07-31T12:03:13.103358: step 13437, loss 0.574268.
Train: 2018-07-31T12:03:13.290807: step 13438, loss 0.504684.
Train: 2018-07-31T12:03:13.447026: step 13439, loss 0.56559.
Train: 2018-07-31T12:03:13.618863: step 13440, loss 0.565596.
Test: 2018-07-31T12:03:14.103093: step 13440, loss 0.550935.
Train: 2018-07-31T12:03:14.274958: step 13441, loss 0.60063.
Train: 2018-07-31T12:03:14.446793: step 13442, loss 0.60943.
Train: 2018-07-31T12:03:14.618627: step 13443, loss 0.65325.
Train: 2018-07-31T12:03:14.806083: step 13444, loss 0.583088.
Train: 2018-07-31T12:03:14.977919: step 13445, loss 0.609239.
Train: 2018-07-31T12:03:15.149753: step 13446, loss 0.617817.
Train: 2018-07-31T12:03:15.337179: step 13447, loss 0.57423.
Train: 2018-07-31T12:03:15.509047: step 13448, loss 0.539592.
Train: 2018-07-31T12:03:15.680849: step 13449, loss 0.556906.
Train: 2018-07-31T12:03:15.852714: step 13450, loss 0.574138.
Test: 2018-07-31T12:03:16.336977: step 13450, loss 0.551138.
Train: 2018-07-31T12:03:16.508812: step 13451, loss 0.599886.
Train: 2018-07-31T12:03:16.680615: step 13452, loss 0.565522.
Train: 2018-07-31T12:03:16.852449: step 13453, loss 0.531329.
Train: 2018-07-31T12:03:17.039906: step 13454, loss 0.599662.
Train: 2018-07-31T12:03:17.211740: step 13455, loss 0.531442.
Train: 2018-07-31T12:03:17.383574: step 13456, loss 0.599559.
Train: 2018-07-31T12:03:17.555439: step 13457, loss 0.540028.
Train: 2018-07-31T12:03:17.727275: step 13458, loss 0.540049.
Train: 2018-07-31T12:03:17.899109: step 13459, loss 0.607952.
Train: 2018-07-31T12:03:18.086535: step 13460, loss 0.540079.
Test: 2018-07-31T12:03:18.555205: step 13460, loss 0.551319.
Train: 2018-07-31T12:03:18.727040: step 13461, loss 0.582465.
Train: 2018-07-31T12:03:18.898874: step 13462, loss 0.565513.
Train: 2018-07-31T12:03:19.070680: step 13463, loss 0.573978.
Train: 2018-07-31T12:03:19.258168: step 13464, loss 0.624744.
Train: 2018-07-31T12:03:19.429995: step 13465, loss 0.540168.
Train: 2018-07-31T12:03:19.601804: step 13466, loss 0.590836.
Train: 2018-07-31T12:03:19.789261: step 13467, loss 0.48964.
Train: 2018-07-31T12:03:19.961097: step 13468, loss 0.540206.
Train: 2018-07-31T12:03:20.132955: step 13469, loss 0.60772.
Train: 2018-07-31T12:03:20.304766: step 13470, loss 0.565501.
Test: 2018-07-31T12:03:20.773406: step 13470, loss 0.551352.
Train: 2018-07-31T12:03:20.960886: step 13471, loss 0.565497.
Train: 2018-07-31T12:03:21.132729: step 13472, loss 0.506355.
Train: 2018-07-31T12:03:21.304561: step 13473, loss 0.548565.
Train: 2018-07-31T12:03:21.476367: step 13474, loss 0.557003.
Train: 2018-07-31T12:03:21.648201: step 13475, loss 0.590943.
Train: 2018-07-31T12:03:21.820035: step 13476, loss 0.565467.
Train: 2018-07-31T12:03:21.991901: step 13477, loss 0.556953.
Train: 2018-07-31T12:03:22.163707: step 13478, loss 0.582495.
Train: 2018-07-31T12:03:22.351191: step 13479, loss 0.565454.
Train: 2018-07-31T12:03:22.523026: step 13480, loss 0.608105.
Test: 2018-07-31T12:03:23.007290: step 13480, loss 0.551163.
Train: 2018-07-31T12:03:23.179092: step 13481, loss 0.573977.
Train: 2018-07-31T12:03:23.350957: step 13482, loss 0.565445.
Train: 2018-07-31T12:03:23.538383: step 13483, loss 0.505772.
Train: 2018-07-31T12:03:23.710217: step 13484, loss 0.548374.
Train: 2018-07-31T12:03:23.882052: step 13485, loss 0.514176.
Train: 2018-07-31T12:03:24.053918: step 13486, loss 0.565431.
Train: 2018-07-31T12:03:24.225722: step 13487, loss 0.659784.
Train: 2018-07-31T12:03:24.397556: step 13488, loss 0.565426.
Train: 2018-07-31T12:03:24.585037: step 13489, loss 0.46251.
Train: 2018-07-31T12:03:24.756873: step 13490, loss 0.591198.
Test: 2018-07-31T12:03:25.225488: step 13490, loss 0.551011.
Train: 2018-07-31T12:03:25.412945: step 13491, loss 0.53961.
Train: 2018-07-31T12:03:25.584779: step 13492, loss 0.513719.
Train: 2018-07-31T12:03:25.756638: step 13493, loss 0.539507.
Train: 2018-07-31T12:03:25.944094: step 13494, loss 0.600058.
Train: 2018-07-31T12:03:26.115904: step 13495, loss 0.539401.
Train: 2018-07-31T12:03:26.287769: step 13496, loss 0.574115.
Train: 2018-07-31T12:03:26.475195: step 13497, loss 0.530613.
Train: 2018-07-31T12:03:26.647061: step 13498, loss 0.521829.
Train: 2018-07-31T12:03:26.818895: step 13499, loss 0.609149.
Train: 2018-07-31T12:03:26.990730: step 13500, loss 0.600457.
Test: 2018-07-31T12:03:27.459370: step 13500, loss 0.550774.
Train: 2018-07-31T12:03:28.302922: step 13501, loss 0.5742.
Train: 2018-07-31T12:03:28.474728: step 13502, loss 0.539162.
Train: 2018-07-31T12:03:28.646562: step 13503, loss 0.56544.
Train: 2018-07-31T12:03:28.818421: step 13504, loss 0.539136.
Train: 2018-07-31T12:03:28.990232: step 13505, loss 0.521569.
Train: 2018-07-31T12:03:29.162066: step 13506, loss 0.547873.
Train: 2018-07-31T12:03:29.333901: step 13507, loss 0.583051.
Train: 2018-07-31T12:03:29.521358: step 13508, loss 0.539032.
Train: 2018-07-31T12:03:29.693222: step 13509, loss 0.636002.
Train: 2018-07-31T12:03:29.865026: step 13510, loss 0.503752.
Test: 2018-07-31T12:03:30.349288: step 13510, loss 0.550684.
Train: 2018-07-31T12:03:30.536744: step 13511, loss 0.618382.
Train: 2018-07-31T12:03:30.708610: step 13512, loss 0.539001.
Train: 2018-07-31T12:03:30.880447: step 13513, loss 0.503734.
Train: 2018-07-31T12:03:31.052248: step 13514, loss 0.556627.
Train: 2018-07-31T12:03:31.224084: step 13515, loss 0.54779.
Train: 2018-07-31T12:03:31.395918: step 13516, loss 0.52125.
Train: 2018-07-31T12:03:31.583374: step 13517, loss 0.556613.
Train: 2018-07-31T12:03:31.755234: step 13518, loss 0.54774.
Train: 2018-07-31T12:03:31.927046: step 13519, loss 0.645427.
Train: 2018-07-31T12:03:32.098903: step 13520, loss 0.512203.
Test: 2018-07-31T12:03:32.567549: step 13520, loss 0.550604.
Train: 2018-07-31T12:03:32.755000: step 13521, loss 0.5566.
Train: 2018-07-31T12:03:32.926809: step 13522, loss 0.485478.
Train: 2018-07-31T12:03:33.098669: step 13523, loss 0.520971.
Train: 2018-07-31T12:03:33.270478: step 13524, loss 0.503038.
Train: 2018-07-31T12:03:33.442313: step 13525, loss 0.565544.
Train: 2018-07-31T12:03:33.629770: step 13526, loss 0.502748.
Train: 2018-07-31T12:03:33.801604: step 13527, loss 0.5656.
Train: 2018-07-31T12:03:33.973440: step 13528, loss 0.53855.
Train: 2018-07-31T12:03:34.145304: step 13529, loss 0.619957.
Train: 2018-07-31T12:03:34.317139: step 13530, loss 0.683456.
Test: 2018-07-31T12:03:34.785750: step 13530, loss 0.550498.
Train: 2018-07-31T12:03:34.957615: step 13531, loss 0.565653.
Train: 2018-07-31T12:03:35.145072: step 13532, loss 0.565634.
Train: 2018-07-31T12:03:35.316909: step 13533, loss 0.529516.
Train: 2018-07-31T12:03:35.488709: step 13534, loss 0.502484.
Train: 2018-07-31T12:03:35.660569: step 13535, loss 0.574618.
Train: 2018-07-31T12:03:35.832410: step 13536, loss 0.511502.
Train: 2018-07-31T12:03:36.004247: step 13537, loss 0.574618.
Train: 2018-07-31T12:03:36.176079: step 13538, loss 0.60168.
Train: 2018-07-31T12:03:36.347884: step 13539, loss 0.538543.
Train: 2018-07-31T12:03:36.519748: step 13540, loss 0.556567.
Test: 2018-07-31T12:03:37.004012: step 13540, loss 0.550487.
Train: 2018-07-31T12:03:37.175847: step 13541, loss 0.646626.
Train: 2018-07-31T12:03:37.347650: step 13542, loss 0.574533.
Train: 2018-07-31T12:03:37.535105: step 13543, loss 0.664171.
Train: 2018-07-31T12:03:37.706971: step 13544, loss 0.556543.
Train: 2018-07-31T12:03:37.878774: step 13545, loss 0.592147.
Train: 2018-07-31T12:03:38.066232: step 13546, loss 0.592011.
Train: 2018-07-31T12:03:38.238099: step 13547, loss 0.530048.
Train: 2018-07-31T12:03:38.409902: step 13548, loss 0.512529.
Train: 2018-07-31T12:03:38.581736: step 13549, loss 0.591696.
Train: 2018-07-31T12:03:38.769222: step 13550, loss 0.539027.
Test: 2018-07-31T12:03:39.253455: step 13550, loss 0.550658.
Train: 2018-07-31T12:03:39.409667: step 13551, loss 0.617809.
Train: 2018-07-31T12:03:39.597153: step 13552, loss 0.565298.
Train: 2018-07-31T12:03:39.768982: step 13553, loss 0.530467.
Train: 2018-07-31T12:03:39.956413: step 13554, loss 0.573969.
Train: 2018-07-31T12:03:40.128281: step 13555, loss 0.530568.
Train: 2018-07-31T12:03:40.300114: step 13556, loss 0.573935.
Train: 2018-07-31T12:03:40.471917: step 13557, loss 0.495991.
Train: 2018-07-31T12:03:40.643752: step 13558, loss 0.539275.
Train: 2018-07-31T12:03:40.831208: step 13559, loss 0.530586.
Train: 2018-07-31T12:03:41.003076: step 13560, loss 0.556581.
Test: 2018-07-31T12:03:41.471717: step 13560, loss 0.550707.
Train: 2018-07-31T12:03:41.643519: step 13561, loss 0.600025.
Train: 2018-07-31T12:03:41.815353: step 13562, loss 0.530483.
Train: 2018-07-31T12:03:41.987212: step 13563, loss 0.600074.
Train: 2018-07-31T12:03:42.174674: step 13564, loss 0.513032.
Train: 2018-07-31T12:03:42.346479: step 13565, loss 0.608831.
Train: 2018-07-31T12:03:42.518313: step 13566, loss 0.582688.
Train: 2018-07-31T12:03:42.705794: step 13567, loss 0.53912.
Train: 2018-07-31T12:03:42.877605: step 13568, loss 0.547828.
Train: 2018-07-31T12:03:43.049439: step 13569, loss 0.634982.
Train: 2018-07-31T12:03:43.236896: step 13570, loss 0.539125.
Test: 2018-07-31T12:03:43.721187: step 13570, loss 0.550667.
Train: 2018-07-31T12:03:43.893022: step 13571, loss 0.591351.
Train: 2018-07-31T12:03:44.080450: step 13572, loss 0.547849.
Train: 2018-07-31T12:03:44.252282: step 13573, loss 0.556544.
Train: 2018-07-31T12:03:44.455390: step 13574, loss 0.513123.
Train: 2018-07-31T12:03:44.627195: step 13575, loss 0.539162.
Train: 2018-07-31T12:03:44.799061: step 13576, loss 0.547835.
Train: 2018-07-31T12:03:44.970864: step 13577, loss 0.634876.
Train: 2018-07-31T12:03:45.158344: step 13578, loss 0.573928.
Train: 2018-07-31T12:03:45.330154: step 13579, loss 0.582615.
Train: 2018-07-31T12:03:45.517612: step 13580, loss 0.591283.
Test: 2018-07-31T12:03:45.986282: step 13580, loss 0.550678.
Train: 2018-07-31T12:03:46.158086: step 13581, loss 0.634626.
Train: 2018-07-31T12:03:46.329922: step 13582, loss 0.513275.
Train: 2018-07-31T12:03:46.517408: step 13583, loss 0.556554.
Train: 2018-07-31T12:03:46.689236: step 13584, loss 0.591089.
Train: 2018-07-31T12:03:46.861047: step 13585, loss 0.573807.
Train: 2018-07-31T12:03:47.032881: step 13586, loss 0.539368.
Train: 2018-07-31T12:03:47.204746: step 13587, loss 0.616765.
Train: 2018-07-31T12:03:47.376550: step 13588, loss 0.599504.
Train: 2018-07-31T12:03:47.564008: step 13589, loss 0.59086.
Train: 2018-07-31T12:03:47.720250: step 13590, loss 0.565176.
Test: 2018-07-31T12:03:48.188886: step 13590, loss 0.55091.
Train: 2018-07-31T12:03:48.360728: step 13591, loss 0.599252.
Train: 2018-07-31T12:03:48.532529: step 13592, loss 0.56518.
Train: 2018-07-31T12:03:48.719986: step 13593, loss 0.565183.
Train: 2018-07-31T12:03:48.891821: step 13594, loss 0.599014.
Train: 2018-07-31T12:03:49.063687: step 13595, loss 0.573629.
Train: 2018-07-31T12:03:49.235490: step 13596, loss 0.674593.
Train: 2018-07-31T12:03:49.407326: step 13597, loss 0.514947.
Train: 2018-07-31T12:03:49.579160: step 13598, loss 0.581943.
Train: 2018-07-31T12:03:49.751028: step 13599, loss 0.640244.
Train: 2018-07-31T12:03:49.922831: step 13600, loss 0.631678.
Test: 2018-07-31T12:03:50.407090: step 13600, loss 0.551466.
Train: 2018-07-31T12:03:51.172538: step 13601, loss 0.548779.
Train: 2018-07-31T12:03:51.344372: step 13602, loss 0.573564.
Train: 2018-07-31T12:03:51.516243: step 13603, loss 0.507938.
Train: 2018-07-31T12:03:51.688071: step 13604, loss 0.499854.
Train: 2018-07-31T12:03:51.859907: step 13605, loss 0.573563.
Train: 2018-07-31T12:03:52.031710: step 13606, loss 0.51621.
Train: 2018-07-31T12:03:52.219191: step 13607, loss 0.598169.
Train: 2018-07-31T12:03:52.391001: step 13608, loss 0.573552.
Train: 2018-07-31T12:03:52.562861: step 13609, loss 0.614633.
Train: 2018-07-31T12:03:52.734671: step 13610, loss 0.622838.
Test: 2018-07-31T12:03:53.218962: step 13610, loss 0.551599.
Train: 2018-07-31T12:03:53.390767: step 13611, loss 0.51611.
Train: 2018-07-31T12:03:53.562603: step 13612, loss 0.483272.
Train: 2018-07-31T12:03:53.734437: step 13613, loss 0.59821.
Train: 2018-07-31T12:03:53.921893: step 13614, loss 0.548831.
Train: 2018-07-31T12:03:54.093758: step 13615, loss 0.62303.
Train: 2018-07-31T12:03:54.281228: step 13616, loss 0.606547.
Train: 2018-07-31T12:03:54.453045: step 13617, loss 0.532266.
Train: 2018-07-31T12:03:54.624886: step 13618, loss 0.523977.
Train: 2018-07-31T12:03:54.796688: step 13619, loss 0.548709.
Train: 2018-07-31T12:03:54.968524: step 13620, loss 0.5901.
Test: 2018-07-31T12:03:55.452785: step 13620, loss 0.551321.
Train: 2018-07-31T12:03:55.624652: step 13621, loss 0.49882.
Train: 2018-07-31T12:03:55.796484: step 13622, loss 0.606819.
Train: 2018-07-31T12:03:55.983910: step 13623, loss 0.581864.
Train: 2018-07-31T12:03:56.171397: step 13624, loss 0.565176.
Train: 2018-07-31T12:03:56.343200: step 13625, loss 0.623699.
Train: 2018-07-31T12:03:56.515036: step 13626, loss 0.590253.
Train: 2018-07-31T12:03:56.686895: step 13627, loss 0.598606.
Train: 2018-07-31T12:03:56.858736: step 13628, loss 0.606937.
Train: 2018-07-31T12:03:57.030541: step 13629, loss 0.573513.
Train: 2018-07-31T12:03:57.202375: step 13630, loss 0.540184.
Test: 2018-07-31T12:03:57.686666: step 13630, loss 0.551234.
Train: 2018-07-31T12:03:57.874125: step 13631, loss 0.506895.
Train: 2018-07-31T12:03:58.045927: step 13632, loss 0.573504.
Train: 2018-07-31T12:03:58.217792: step 13633, loss 0.573503.
Train: 2018-07-31T12:03:58.389597: step 13634, loss 0.515097.
Train: 2018-07-31T12:03:58.577054: step 13635, loss 0.598577.
Train: 2018-07-31T12:03:58.748887: step 13636, loss 0.531678.
Train: 2018-07-31T12:03:58.920722: step 13637, loss 0.514854.
Train: 2018-07-31T12:03:59.092588: step 13638, loss 0.573514.
Train: 2018-07-31T12:03:59.280014: step 13639, loss 0.573521.
Train: 2018-07-31T12:03:59.436226: step 13640, loss 0.522911.
Test: 2018-07-31T12:03:59.920518: step 13640, loss 0.550914.
Train: 2018-07-31T12:04:00.092353: step 13641, loss 0.590456.
Train: 2018-07-31T12:04:00.264159: step 13642, loss 0.471846.
Train: 2018-07-31T12:04:00.435992: step 13643, loss 0.539544.
Train: 2018-07-31T12:04:00.623481: step 13644, loss 0.599208.
Train: 2018-07-31T12:04:00.795284: step 13645, loss 0.573612.
Train: 2018-07-31T12:04:00.982740: step 13646, loss 0.607958.
Train: 2018-07-31T12:04:01.154574: step 13647, loss 0.573638.
Train: 2018-07-31T12:04:01.326410: step 13648, loss 0.556442.
Train: 2018-07-31T12:04:01.498269: step 13649, loss 0.616702.
Train: 2018-07-31T12:04:01.670080: step 13650, loss 0.599476.
Test: 2018-07-31T12:04:02.154341: step 13650, loss 0.55063.
Train: 2018-07-31T12:04:02.326175: step 13651, loss 0.659665.
Train: 2018-07-31T12:04:02.498011: step 13652, loss 0.599357.
Train: 2018-07-31T12:04:02.685496: step 13653, loss 0.496588.
Train: 2018-07-31T12:04:02.857301: step 13654, loss 0.52231.
Train: 2018-07-31T12:04:03.029166: step 13655, loss 0.590654.
Train: 2018-07-31T12:04:03.201000: step 13656, loss 0.607702.
Train: 2018-07-31T12:04:03.372806: step 13657, loss 0.462765.
Train: 2018-07-31T12:04:03.560261: step 13658, loss 0.54797.
Train: 2018-07-31T12:04:03.732098: step 13659, loss 0.590629.
Train: 2018-07-31T12:04:03.903931: step 13660, loss 0.55648.
Test: 2018-07-31T12:04:04.388222: step 13660, loss 0.550708.
Train: 2018-07-31T12:04:04.560026: step 13661, loss 0.57356.
Train: 2018-07-31T12:04:04.731892: step 13662, loss 0.539374.
Train: 2018-07-31T12:04:04.903721: step 13663, loss 0.607778.
Train: 2018-07-31T12:04:05.091152: step 13664, loss 0.488034.
Train: 2018-07-31T12:04:05.262986: step 13665, loss 0.547876.
Train: 2018-07-31T12:04:05.434822: step 13666, loss 0.607898.
Train: 2018-07-31T12:04:05.622277: step 13667, loss 0.556417.
Train: 2018-07-31T12:04:05.794126: step 13668, loss 0.650906.
Train: 2018-07-31T12:04:05.965978: step 13669, loss 0.633653.
Train: 2018-07-31T12:04:06.137782: step 13670, loss 0.599247.
Test: 2018-07-31T12:04:06.622043: step 13670, loss 0.550693.
Train: 2018-07-31T12:04:06.793879: step 13671, loss 0.496675.
Train: 2018-07-31T12:04:06.981366: step 13672, loss 0.556463.
Train: 2018-07-31T12:04:07.153170: step 13673, loss 0.488273.
Train: 2018-07-31T12:04:07.325004: step 13674, loss 0.547926.
Train: 2018-07-31T12:04:07.496840: step 13675, loss 0.539363.
Train: 2018-07-31T12:04:07.684295: step 13676, loss 0.607754.
Train: 2018-07-31T12:04:07.856130: step 13677, loss 0.556422.
Train: 2018-07-31T12:04:08.027965: step 13678, loss 0.556414.
Train: 2018-07-31T12:04:08.199799: step 13679, loss 0.599261.
Train: 2018-07-31T12:04:08.387256: step 13680, loss 0.547831.
Test: 2018-07-31T12:04:08.855926: step 13680, loss 0.550611.
Train: 2018-07-31T12:04:09.027761: step 13681, loss 0.496374.
Train: 2018-07-31T12:04:09.215188: step 13682, loss 0.616503.
Train: 2018-07-31T12:04:09.387052: step 13683, loss 0.496223.
Train: 2018-07-31T12:04:09.558887: step 13684, loss 0.590791.
Train: 2018-07-31T12:04:09.730691: step 13685, loss 0.625287.
Train: 2018-07-31T12:04:09.918148: step 13686, loss 0.616657.
Train: 2018-07-31T12:04:10.090012: step 13687, loss 0.504723.
Train: 2018-07-31T12:04:10.261818: step 13688, loss 0.58217.
Train: 2018-07-31T12:04:10.433652: step 13689, loss 0.616576.
Train: 2018-07-31T12:04:10.605520: step 13690, loss 0.547768.
Test: 2018-07-31T12:04:11.089748: step 13690, loss 0.550572.
Train: 2018-07-31T12:04:11.261582: step 13691, loss 0.539195.
Train: 2018-07-31T12:04:11.433418: step 13692, loss 0.530615.
Train: 2018-07-31T12:04:11.620906: step 13693, loss 0.55636.
Train: 2018-07-31T12:04:11.792742: step 13694, loss 0.582129.
Train: 2018-07-31T12:04:12.027030: step 13695, loss 0.55635.
Train: 2018-07-31T12:04:12.198900: step 13696, loss 0.539153.
Train: 2018-07-31T12:04:12.370730: step 13697, loss 0.539132.
Train: 2018-07-31T12:04:12.558153: step 13698, loss 0.547713.
Train: 2018-07-31T12:04:12.730019: step 13699, loss 0.573562.
Train: 2018-07-31T12:04:12.901823: step 13700, loss 0.54767.
Test: 2018-07-31T12:04:13.385003: step 13700, loss 0.55046.
Train: 2018-07-31T12:04:14.150415: step 13701, loss 0.539005.
Train: 2018-07-31T12:04:14.322280: step 13702, loss 0.521649.
Train: 2018-07-31T12:04:14.509707: step 13703, loss 0.573617.
Train: 2018-07-31T12:04:14.681542: step 13704, loss 0.564944.
Train: 2018-07-31T12:04:14.853401: step 13705, loss 0.651999.
Train: 2018-07-31T12:04:15.040833: step 13706, loss 0.521432.
Train: 2018-07-31T12:04:15.212698: step 13707, loss 0.538824.
Train: 2018-07-31T12:04:15.384535: step 13708, loss 0.477807.
Train: 2018-07-31T12:04:15.556337: step 13709, loss 0.591152.
Train: 2018-07-31T12:04:15.743793: step 13710, loss 0.538714.
Test: 2018-07-31T12:04:16.212433: step 13710, loss 0.550286.
Train: 2018-07-31T12:04:16.384293: step 13711, loss 0.591249.
Train: 2018-07-31T12:04:16.556133: step 13712, loss 0.591278.
Train: 2018-07-31T12:04:16.743591: step 13713, loss 0.564964.
Train: 2018-07-31T12:04:16.915423: step 13714, loss 0.494755.
Train: 2018-07-31T12:04:17.087228: step 13715, loss 0.547392.
Train: 2018-07-31T12:04:17.259096: step 13716, loss 0.564975.
Train: 2018-07-31T12:04:17.446520: step 13717, loss 0.547357.
Train: 2018-07-31T12:04:17.618384: step 13718, loss 0.556163.
Train: 2018-07-31T12:04:17.790213: step 13719, loss 0.670992.
Train: 2018-07-31T12:04:17.962049: step 13720, loss 0.547334.
Test: 2018-07-31T12:04:18.430662: step 13720, loss 0.550208.
Train: 2018-07-31T12:04:18.618150: step 13721, loss 0.54734.
Train: 2018-07-31T12:04:18.789954: step 13722, loss 0.520897.
Train: 2018-07-31T12:04:18.961789: step 13723, loss 0.547337.
Train: 2018-07-31T12:04:19.133624: step 13724, loss 0.59144.
Train: 2018-07-31T12:04:19.305489: step 13725, loss 0.485584.
Train: 2018-07-31T12:04:19.477293: step 13726, loss 0.609132.
Train: 2018-07-31T12:04:19.649129: step 13727, loss 0.644467.
Train: 2018-07-31T12:04:19.836584: step 13728, loss 0.564963.
Train: 2018-07-31T12:04:20.008452: step 13729, loss 0.520917.
Train: 2018-07-31T12:04:20.180284: step 13730, loss 0.547343.
Test: 2018-07-31T12:04:20.664546: step 13730, loss 0.550206.
Train: 2018-07-31T12:04:20.836380: step 13731, loss 0.538546.
Train: 2018-07-31T12:04:21.008218: step 13732, loss 0.556144.
Train: 2018-07-31T12:04:21.195641: step 13733, loss 0.564941.
Train: 2018-07-31T12:04:21.367505: step 13734, loss 0.556142.
Train: 2018-07-31T12:04:21.539343: step 13735, loss 0.538542.
Train: 2018-07-31T12:04:21.711170: step 13736, loss 0.573759.
Train: 2018-07-31T12:04:21.882979: step 13737, loss 0.617797.
Train: 2018-07-31T12:04:22.054844: step 13738, loss 0.556159.
Train: 2018-07-31T12:04:22.242272: step 13739, loss 0.556166.
Train: 2018-07-31T12:04:22.414136: step 13740, loss 0.51225.
Test: 2018-07-31T12:04:22.898397: step 13740, loss 0.550245.
Train: 2018-07-31T12:04:23.054611: step 13741, loss 0.489983.
Train: 2018-07-31T12:04:23.226416: step 13742, loss 0.582571.
Train: 2018-07-31T12:04:23.398250: step 13743, loss 0.617826.
Train: 2018-07-31T12:04:23.585707: step 13744, loss 0.512145.
Train: 2018-07-31T12:04:23.757574: step 13745, loss 0.635471.
Train: 2018-07-31T12:04:23.929406: step 13746, loss 0.600191.
Train: 2018-07-31T12:04:24.116832: step 13747, loss 0.564967.
Train: 2018-07-31T12:04:24.288697: step 13748, loss 0.573734.
Train: 2018-07-31T12:04:24.460502: step 13749, loss 0.512371.
Train: 2018-07-31T12:04:24.632370: step 13750, loss 0.573706.
Test: 2018-07-31T12:04:25.116628: step 13750, loss 0.550285.
Train: 2018-07-31T12:04:25.288463: step 13751, loss 0.591198.
Train: 2018-07-31T12:04:25.475919: step 13752, loss 0.652344.
Train: 2018-07-31T12:04:25.647754: step 13753, loss 0.538771.
Train: 2018-07-31T12:04:25.819559: step 13754, loss 0.538815.
Train: 2018-07-31T12:04:25.991392: step 13755, loss 0.547529.
Train: 2018-07-31T12:04:26.163227: step 13756, loss 0.504164.
Train: 2018-07-31T12:04:26.350684: step 13757, loss 0.564885.
Train: 2018-07-31T12:04:26.522549: step 13758, loss 0.547526.
Train: 2018-07-31T12:04:26.678762: step 13759, loss 0.564875.
Train: 2018-07-31T12:04:26.850597: step 13760, loss 0.582235.
Test: 2018-07-31T12:04:27.334828: step 13760, loss 0.550327.
Train: 2018-07-31T12:04:27.506662: step 13761, loss 0.538823.
Train: 2018-07-31T12:04:27.678498: step 13762, loss 0.53881.
Train: 2018-07-31T12:04:27.850357: step 13763, loss 0.616997.
Train: 2018-07-31T12:04:28.022167: step 13764, loss 0.521421.
Train: 2018-07-31T12:04:28.209624: step 13765, loss 0.556159.
Train: 2018-07-31T12:04:28.381491: step 13766, loss 0.521376.
Train: 2018-07-31T12:04:28.553294: step 13767, loss 0.521324.
Train: 2018-07-31T12:04:28.725129: step 13768, loss 0.625891.
Train: 2018-07-31T12:04:28.896992: step 13769, loss 0.608466.
Train: 2018-07-31T12:04:29.068798: step 13770, loss 0.521243.
Test: 2018-07-31T12:04:29.568680: step 13770, loss 0.550231.
Train: 2018-07-31T12:04:29.740516: step 13771, loss 0.564838.
Train: 2018-07-31T12:04:29.912349: step 13772, loss 0.547388.
Train: 2018-07-31T12:04:30.084218: step 13773, loss 0.556106.
Train: 2018-07-31T12:04:30.256053: step 13774, loss 0.617222.
Train: 2018-07-31T12:04:30.427884: step 13775, loss 0.547374.
Train: 2018-07-31T12:04:30.615310: step 13776, loss 0.52993.
Train: 2018-07-31T12:04:30.787175: step 13777, loss 0.460111.
Train: 2018-07-31T12:04:30.959009: step 13778, loss 0.477381.
Train: 2018-07-31T12:04:31.130848: step 13779, loss 0.582392.
Train: 2018-07-31T12:04:31.302679: step 13780, loss 0.538458.
Test: 2018-07-31T12:04:31.786941: step 13780, loss 0.550092.
Train: 2018-07-31T12:04:31.958775: step 13781, loss 0.511928.
Train: 2018-07-31T12:04:32.130581: step 13782, loss 0.529477.
Train: 2018-07-31T12:04:32.302445: step 13783, loss 0.538262.
Train: 2018-07-31T12:04:32.474250: step 13784, loss 0.520366.
Train: 2018-07-31T12:04:32.646115: step 13785, loss 0.520231.
Train: 2018-07-31T12:04:32.833571: step 13786, loss 0.538062.
Train: 2018-07-31T12:04:33.005409: step 13787, loss 0.519961.
Train: 2018-07-31T12:04:33.177235: step 13788, loss 0.565108.
Train: 2018-07-31T12:04:33.364667: step 13789, loss 0.583327.
Train: 2018-07-31T12:04:33.536502: step 13790, loss 0.574296.
Test: 2018-07-31T12:04:34.020763: step 13790, loss 0.549926.
Train: 2018-07-31T12:04:34.192628: step 13791, loss 0.546958.
Train: 2018-07-31T12:04:34.364433: step 13792, loss 0.546951.
Train: 2018-07-31T12:04:34.551888: step 13793, loss 0.528631.
Train: 2018-07-31T12:04:34.723724: step 13794, loss 0.601975.
Train: 2018-07-31T12:04:34.895557: step 13795, loss 0.60201.
Train: 2018-07-31T12:04:35.067423: step 13796, loss 0.537757.
Train: 2018-07-31T12:04:35.254848: step 13797, loss 0.556108.
Train: 2018-07-31T12:04:35.426683: step 13798, loss 0.528582.
Train: 2018-07-31T12:04:35.598518: step 13799, loss 0.592806.
Train: 2018-07-31T12:04:35.770354: step 13800, loss 0.583607.
Test: 2018-07-31T12:04:36.254615: step 13800, loss 0.549906.
Train: 2018-07-31T12:04:37.020060: step 13801, loss 0.537768.
Train: 2018-07-31T12:04:37.191926: step 13802, loss 0.519471.
Train: 2018-07-31T12:04:37.363760: step 13803, loss 0.537776.
Train: 2018-07-31T12:04:37.535596: step 13804, loss 0.601824.
Train: 2018-07-31T12:04:37.707430: step 13805, loss 0.583491.
Train: 2018-07-31T12:04:37.879234: step 13806, loss 0.546927.
Train: 2018-07-31T12:04:38.051099: step 13807, loss 0.501339.
Train: 2018-07-31T12:04:38.222935: step 13808, loss 0.528696.
Train: 2018-07-31T12:04:38.394769: step 13809, loss 0.592511.
Train: 2018-07-31T12:04:38.582225: step 13810, loss 0.565149.
Test: 2018-07-31T12:04:39.050834: step 13810, loss 0.549887.
Train: 2018-07-31T12:04:39.253945: step 13811, loss 0.565135.
Train: 2018-07-31T12:04:39.425748: step 13812, loss 0.601504.
Train: 2018-07-31T12:04:39.597583: step 13813, loss 0.583254.
Train: 2018-07-31T12:04:39.769446: step 13814, loss 0.510698.
Train: 2018-07-31T12:04:39.941251: step 13815, loss 0.519795.
Train: 2018-07-31T12:04:40.128725: step 13816, loss 0.619308.
Train: 2018-07-31T12:04:40.300572: step 13817, loss 0.610166.
Train: 2018-07-31T12:04:40.472376: step 13818, loss 0.646054.
Train: 2018-07-31T12:04:40.644211: step 13819, loss 0.564936.
Train: 2018-07-31T12:04:40.816047: step 13820, loss 0.618534.
Test: 2018-07-31T12:04:41.300308: step 13820, loss 0.549948.
Train: 2018-07-31T12:04:41.472143: step 13821, loss 0.520355.
Train: 2018-07-31T12:04:41.644008: step 13822, loss 0.564823.
Train: 2018-07-31T12:04:41.831464: step 13823, loss 0.600159.
Train: 2018-07-31T12:04:42.003302: step 13824, loss 0.547154.
Train: 2018-07-31T12:04:42.175133: step 13825, loss 0.652556.
Train: 2018-07-31T12:04:42.346964: step 13826, loss 0.573467.
Train: 2018-07-31T12:04:42.518802: step 13827, loss 0.529898.
Train: 2018-07-31T12:04:42.706229: step 13828, loss 0.521329.
Train: 2018-07-31T12:04:42.878096: step 13829, loss 0.547381.
Train: 2018-07-31T12:04:43.049928: step 13830, loss 0.573323.
Test: 2018-07-31T12:04:43.518569: step 13830, loss 0.550237.
Train: 2018-07-31T12:04:43.690403: step 13831, loss 0.625047.
Train: 2018-07-31T12:04:43.862209: step 13832, loss 0.607675.
Train: 2018-07-31T12:04:44.034042: step 13833, loss 0.487533.
Train: 2018-07-31T12:04:44.205879: step 13834, loss 0.564672.
Train: 2018-07-31T12:04:44.377713: step 13835, loss 0.513391.
Train: 2018-07-31T12:04:44.565168: step 13836, loss 0.564669.
Train: 2018-07-31T12:04:44.737036: step 13837, loss 0.556124.
Train: 2018-07-31T12:04:44.908868: step 13838, loss 0.581754.
Train: 2018-07-31T12:04:45.080703: step 13839, loss 0.556123.
Train: 2018-07-31T12:04:45.252508: step 13840, loss 0.539039.
Test: 2018-07-31T12:04:45.736799: step 13840, loss 0.550349.
Train: 2018-07-31T12:04:45.908634: step 13841, loss 0.624476.
Train: 2018-07-31T12:04:46.080469: step 13842, loss 0.55612.
Train: 2018-07-31T12:04:46.252307: step 13843, loss 0.539054.
Train: 2018-07-31T12:04:46.439755: step 13844, loss 0.581727.
Train: 2018-07-31T12:04:46.611594: step 13845, loss 0.581721.
Train: 2018-07-31T12:04:46.799021: step 13846, loss 0.547595.
Train: 2018-07-31T12:04:46.970882: step 13847, loss 0.607293.
Train: 2018-07-31T12:04:47.142689: step 13848, loss 0.607253.
Train: 2018-07-31T12:04:47.314524: step 13849, loss 0.547639.
Train: 2018-07-31T12:04:47.486359: step 13850, loss 0.522166.
Test: 2018-07-31T12:04:47.970621: step 13850, loss 0.550421.
Train: 2018-07-31T12:04:48.158105: step 13851, loss 0.573147.
Train: 2018-07-31T12:04:48.329942: step 13852, loss 0.573144.
Train: 2018-07-31T12:04:48.501780: step 13853, loss 0.57314.
Train: 2018-07-31T12:04:48.673581: step 13854, loss 0.564647.
Train: 2018-07-31T12:04:48.845416: step 13855, loss 0.564646.
Train: 2018-07-31T12:04:49.017252: step 13856, loss 0.573129.
Train: 2018-07-31T12:04:49.204706: step 13857, loss 0.598566.
Train: 2018-07-31T12:04:49.360920: step 13858, loss 0.530755.
Train: 2018-07-31T12:04:49.548406: step 13859, loss 0.5477.
Train: 2018-07-31T12:04:49.720241: step 13860, loss 0.581589.
Test: 2018-07-31T12:04:50.188884: step 13860, loss 0.550447.
Train: 2018-07-31T12:04:50.360716: step 13861, loss 0.598535.
Train: 2018-07-31T12:04:50.532553: step 13862, loss 0.615451.
Train: 2018-07-31T12:04:50.704386: step 13863, loss 0.539275.
Train: 2018-07-31T12:04:50.876224: step 13864, loss 0.564643.
Train: 2018-07-31T12:04:51.048050: step 13865, loss 0.573087.
Train: 2018-07-31T12:04:51.235481: step 13866, loss 0.556207.
Train: 2018-07-31T12:04:51.407316: step 13867, loss 0.480305.
Train: 2018-07-31T12:04:51.579151: step 13868, loss 0.564637.
Train: 2018-07-31T12:04:51.751019: step 13869, loss 0.539258.
Train: 2018-07-31T12:04:51.922821: step 13870, loss 0.49684.
Test: 2018-07-31T12:04:52.407112: step 13870, loss 0.550381.
Train: 2018-07-31T12:04:52.578947: step 13871, loss 0.590111.
Train: 2018-07-31T12:04:52.750752: step 13872, loss 0.632753.
Train: 2018-07-31T12:04:52.922586: step 13873, loss 0.61575.
Train: 2018-07-31T12:04:53.078799: step 13874, loss 0.53052.
Train: 2018-07-31T12:04:53.266263: step 13875, loss 0.692468.
Train: 2018-07-31T12:04:53.438120: step 13876, loss 0.556098.
Train: 2018-07-31T12:04:53.609926: step 13877, loss 0.539133.
Train: 2018-07-31T12:04:53.781760: step 13878, loss 0.505229.
Train: 2018-07-31T12:04:53.953595: step 13879, loss 0.640972.
Train: 2018-07-31T12:04:54.125429: step 13880, loss 0.581556.
Test: 2018-07-31T12:04:54.594101: step 13880, loss 0.550433.
Train: 2018-07-31T12:04:54.781525: step 13881, loss 0.54768.
Train: 2018-07-31T12:04:54.953361: step 13882, loss 0.606893.
Train: 2018-07-31T12:04:55.125220: step 13883, loss 0.564612.
Train: 2018-07-31T12:04:55.297055: step 13884, loss 0.632079.
Train: 2018-07-31T12:04:55.468895: step 13885, loss 0.606685.
Train: 2018-07-31T12:04:55.640701: step 13886, loss 0.539475.
Train: 2018-07-31T12:04:55.812564: step 13887, loss 0.564645.
Train: 2018-07-31T12:04:55.984370: step 13888, loss 0.547944.
Train: 2018-07-31T12:04:56.156234: step 13889, loss 0.59804.
Train: 2018-07-31T12:04:56.328063: step 13890, loss 0.539671.
Test: 2018-07-31T12:04:56.812330: step 13890, loss 0.550727.
Train: 2018-07-31T12:04:56.984160: step 13891, loss 0.572996.
Train: 2018-07-31T12:04:57.140373: step 13892, loss 0.60017.
Train: 2018-07-31T12:04:57.312183: step 13893, loss 0.572989.
Train: 2018-07-31T12:04:57.484018: step 13894, loss 0.523191.
Train: 2018-07-31T12:04:57.655853: step 13895, loss 0.572985.
Train: 2018-07-31T12:04:57.827687: step 13896, loss 0.572983.
Train: 2018-07-31T12:04:57.999522: step 13897, loss 0.514896.
Train: 2018-07-31T12:04:58.171357: step 13898, loss 0.614517.
Train: 2018-07-31T12:04:58.343222: step 13899, loss 0.539746.
Train: 2018-07-31T12:04:58.515027: step 13900, loss 0.614551.
Test: 2018-07-31T12:04:58.999287: step 13900, loss 0.550741.
Train: 2018-07-31T12:04:59.717870: step 13901, loss 0.564661.
Train: 2018-07-31T12:04:59.905326: step 13902, loss 0.51478.
Train: 2018-07-31T12:05:00.077161: step 13903, loss 0.589647.
Train: 2018-07-31T12:05:00.248995: step 13904, loss 0.548004.
Train: 2018-07-31T12:05:00.420831: step 13905, loss 0.56465.
Train: 2018-07-31T12:05:00.592665: step 13906, loss 0.581334.
Train: 2018-07-31T12:05:00.764500: step 13907, loss 0.581349.
Train: 2018-07-31T12:05:00.936336: step 13908, loss 0.573.
Train: 2018-07-31T12:05:01.108170: step 13909, loss 0.547929.
Train: 2018-07-31T12:05:01.280034: step 13910, loss 0.497722.
Test: 2018-07-31T12:05:01.748677: step 13910, loss 0.550595.
Train: 2018-07-31T12:05:01.920480: step 13911, loss 0.64009.
Train: 2018-07-31T12:05:02.092338: step 13912, loss 0.531083.
Train: 2018-07-31T12:05:02.264149: step 13913, loss 0.564632.
Train: 2018-07-31T12:05:02.436015: step 13914, loss 0.598255.
Train: 2018-07-31T12:05:02.607819: step 13915, loss 0.539383.
Train: 2018-07-31T12:05:02.779683: step 13916, loss 0.505661.
Train: 2018-07-31T12:05:02.951518: step 13917, loss 0.564608.
Train: 2018-07-31T12:05:03.123322: step 13918, loss 0.606894.
Train: 2018-07-31T12:05:03.295157: step 13919, loss 0.53072.
Train: 2018-07-31T12:05:03.466993: step 13920, loss 0.573068.
Test: 2018-07-31T12:05:03.935663: step 13920, loss 0.550352.
Train: 2018-07-31T12:05:04.107467: step 13921, loss 0.581568.
Train: 2018-07-31T12:05:04.279332: step 13922, loss 0.598585.
Train: 2018-07-31T12:05:04.466782: step 13923, loss 0.581582.
Train: 2018-07-31T12:05:04.638624: step 13924, loss 0.590078.
Train: 2018-07-31T12:05:04.810461: step 13925, loss 0.598561.
Train: 2018-07-31T12:05:04.982287: step 13926, loss 0.65795.
Train: 2018-07-31T12:05:05.154127: step 13927, loss 0.530701.
Train: 2018-07-31T12:05:05.325931: step 13928, loss 0.606817.
Train: 2018-07-31T12:05:05.497799: step 13929, loss 0.615148.
Train: 2018-07-31T12:05:05.669601: step 13930, loss 0.497355.
Test: 2018-07-31T12:05:06.138242: step 13930, loss 0.550529.
Train: 2018-07-31T12:05:06.325720: step 13931, loss 0.522625.
Train: 2018-07-31T12:05:06.497566: step 13932, loss 0.531023.
Train: 2018-07-31T12:05:06.669367: step 13933, loss 0.530999.
Train: 2018-07-31T12:05:06.841202: step 13934, loss 0.530946.
Train: 2018-07-31T12:05:07.013037: step 13935, loss 0.547711.
Train: 2018-07-31T12:05:07.169251: step 13936, loss 0.556101.
Train: 2018-07-31T12:05:07.356708: step 13937, loss 0.572988.
Train: 2018-07-31T12:05:07.512919: step 13938, loss 0.522158.
Train: 2018-07-31T12:05:07.700375: step 13939, loss 0.573009.
Train: 2018-07-31T12:05:07.872212: step 13940, loss 0.513428.
Test: 2018-07-31T12:05:08.356472: step 13940, loss 0.550201.
Train: 2018-07-31T12:05:08.528308: step 13941, loss 0.624275.
Train: 2018-07-31T12:05:08.700141: step 13942, loss 0.598708.
Train: 2018-07-31T12:05:08.871976: step 13943, loss 0.530263.
Train: 2018-07-31T12:05:09.043842: step 13944, loss 0.598755.
Train: 2018-07-31T12:05:09.215676: step 13945, loss 0.547344.
Train: 2018-07-31T12:05:09.387480: step 13946, loss 0.530167.
Train: 2018-07-31T12:05:09.559346: step 13947, loss 0.564492.
Train: 2018-07-31T12:05:09.731151: step 13948, loss 0.573091.
Train: 2018-07-31T12:05:09.902985: step 13949, loss 0.573103.
Train: 2018-07-31T12:05:10.074851: step 13950, loss 0.642057.
Test: 2018-07-31T12:05:10.559082: step 13950, loss 0.55007.
Train: 2018-07-31T12:05:10.746539: step 13951, loss 0.53866.
Train: 2018-07-31T12:05:10.918372: step 13952, loss 0.581688.
Train: 2018-07-31T12:05:11.090206: step 13953, loss 0.469888.
Train: 2018-07-31T12:05:11.262067: step 13954, loss 0.564498.
Train: 2018-07-31T12:05:11.433877: step 13955, loss 0.538639.
Train: 2018-07-31T12:05:11.605712: step 13956, loss 0.547243.
Train: 2018-07-31T12:05:11.777547: step 13957, loss 0.642343.
Train: 2018-07-31T12:05:11.949380: step 13958, loss 0.625041.
Train: 2018-07-31T12:05:12.121249: step 13959, loss 0.607688.
Train: 2018-07-31T12:05:12.293051: step 13960, loss 0.590359.
Test: 2018-07-31T12:05:12.777342: step 13960, loss 0.550098.
Train: 2018-07-31T12:05:12.964769: step 13961, loss 0.512886.
Train: 2018-07-31T12:05:13.136603: step 13962, loss 0.54733.
Train: 2018-07-31T12:05:13.308468: step 13963, loss 0.667499.
Train: 2018-07-31T12:05:13.480273: step 13964, loss 0.530259.
Train: 2018-07-31T12:05:13.652137: step 13965, loss 0.684093.
Train: 2018-07-31T12:05:13.823942: step 13966, loss 0.538993.
Train: 2018-07-31T12:05:13.995810: step 13967, loss 0.555988.
Train: 2018-07-31T12:05:14.167642: step 13968, loss 0.598479.
Train: 2018-07-31T12:05:14.339476: step 13969, loss 0.547673.
Train: 2018-07-31T12:05:14.511282: step 13970, loss 0.530888.
Test: 2018-07-31T12:05:14.995575: step 13970, loss 0.550495.
Train: 2018-07-31T12:05:15.167407: step 13971, loss 0.547766.
Train: 2018-07-31T12:05:15.339211: step 13972, loss 0.589847.
Train: 2018-07-31T12:05:15.511047: step 13973, loss 0.648653.
Train: 2018-07-31T12:05:15.682912: step 13974, loss 0.589803.
Train: 2018-07-31T12:05:15.854717: step 13975, loss 0.522889.
Train: 2018-07-31T12:05:16.026582: step 13976, loss 0.656447.
Train: 2018-07-31T12:05:16.198385: step 13977, loss 0.572956.
Train: 2018-07-31T12:05:16.370220: step 13978, loss 0.61449.
Train: 2018-07-31T12:05:16.542086: step 13979, loss 0.589541.
Train: 2018-07-31T12:05:16.713891: step 13980, loss 0.548271.
Test: 2018-07-31T12:05:17.198151: step 13980, loss 0.550997.
Train: 2018-07-31T12:05:17.369987: step 13981, loss 0.572994.
Train: 2018-07-31T12:05:17.541852: step 13982, loss 0.630386.
Train: 2018-07-31T12:05:17.713681: step 13983, loss 0.581144.
Train: 2018-07-31T12:05:17.885490: step 13984, loss 0.548521.
Train: 2018-07-31T12:05:18.072976: step 13985, loss 0.605493.
Train: 2018-07-31T12:05:18.244814: step 13986, loss 0.540524.
Train: 2018-07-31T12:05:18.416617: step 13987, loss 0.524359.
Train: 2018-07-31T12:05:18.588451: step 13988, loss 0.556763.
Train: 2018-07-31T12:05:18.760287: step 13989, loss 0.580993.
Train: 2018-07-31T12:05:18.932121: step 13990, loss 0.573258.
Test: 2018-07-31T12:05:19.400791: step 13990, loss 0.551371.
Train: 2018-07-31T12:05:19.588216: step 13991, loss 0.451754.
Train: 2018-07-31T12:05:19.760076: step 13992, loss 0.556542.
Train: 2018-07-31T12:05:19.931917: step 13993, loss 0.573248.
Train: 2018-07-31T12:05:20.103722: step 13994, loss 0.581466.
Train: 2018-07-31T12:05:20.275556: step 13995, loss 0.524274.
Train: 2018-07-31T12:05:20.447421: step 13996, loss 0.614696.
Train: 2018-07-31T12:05:20.634847: step 13997, loss 0.54912.
Train: 2018-07-31T12:05:20.806707: step 13998, loss 0.491306.
Train: 2018-07-31T12:05:20.978517: step 13999, loss 0.565802.
Train: 2018-07-31T12:05:21.150381: step 14000, loss 0.56555.
Test: 2018-07-31T12:05:21.618991: step 14000, loss 0.551513.
Train: 2018-07-31T12:05:22.384438: step 14001, loss 0.582178.
Train: 2018-07-31T12:05:22.556302: step 14002, loss 0.490053.
Train: 2018-07-31T12:05:22.728137: step 14003, loss 0.506464.
Train: 2018-07-31T12:05:22.899942: step 14004, loss 0.530836.
Train: 2018-07-31T12:05:23.071777: step 14005, loss 0.55688.
Train: 2018-07-31T12:05:23.243612: step 14006, loss 0.590705.
Train: 2018-07-31T12:05:23.431097: step 14007, loss 0.555351.
Train: 2018-07-31T12:05:23.602901: step 14008, loss 0.58483.
Train: 2018-07-31T12:05:23.774767: step 14009, loss 0.581103.
Train: 2018-07-31T12:05:23.946571: step 14010, loss 0.7354.
Test: 2018-07-31T12:05:24.430863: step 14010, loss 0.550987.
Train: 2018-07-31T12:05:24.602667: step 14011, loss 0.54903.
Train: 2018-07-31T12:05:24.790123: step 14012, loss 0.487062.
Train: 2018-07-31T12:05:24.961989: step 14013, loss 0.55703.
Train: 2018-07-31T12:05:25.133818: step 14014, loss 0.513546.
Train: 2018-07-31T12:05:25.321279: step 14015, loss 0.568767.
Train: 2018-07-31T12:05:25.493084: step 14016, loss 0.576085.
Train: 2018-07-31T12:05:25.664918: step 14017, loss 0.505481.
Train: 2018-07-31T12:05:25.852375: step 14018, loss 0.541854.
Train: 2018-07-31T12:05:26.024210: step 14019, loss 0.598708.
Train: 2018-07-31T12:05:26.196045: step 14020, loss 0.54261.
Test: 2018-07-31T12:05:26.664684: step 14020, loss 0.555137.
Train: 2018-07-31T12:05:26.852140: step 14021, loss 0.552176.
Train: 2018-07-31T12:05:27.024006: step 14022, loss 0.561706.
Train: 2018-07-31T12:05:27.195841: step 14023, loss 0.607789.
Train: 2018-07-31T12:05:27.383267: step 14024, loss 0.544137.
Train: 2018-07-31T12:05:27.555125: step 14025, loss 0.645013.
Train: 2018-07-31T12:05:27.742557: step 14026, loss 0.59933.
Train: 2018-07-31T12:05:27.914392: step 14027, loss 0.60878.
Train: 2018-07-31T12:05:28.086227: step 14028, loss 0.590216.
Train: 2018-07-31T12:05:28.273683: step 14029, loss 0.599494.
Train: 2018-07-31T12:05:28.445519: step 14030, loss 0.507009.
Test: 2018-07-31T12:05:28.929791: step 14030, loss 0.557183.
Train: 2018-07-31T12:05:29.101614: step 14031, loss 0.538517.
Train: 2018-07-31T12:05:29.273450: step 14032, loss 0.535048.
Train: 2018-07-31T12:05:29.460905: step 14033, loss 0.563248.
Train: 2018-07-31T12:05:29.632740: step 14034, loss 0.618552.
Train: 2018-07-31T12:05:29.804575: step 14035, loss 0.63608.
Train: 2018-07-31T12:05:29.976439: step 14036, loss 0.516593.
Train: 2018-07-31T12:05:30.148275: step 14037, loss 0.579921.
Train: 2018-07-31T12:05:30.335701: step 14038, loss 0.547608.
Train: 2018-07-31T12:05:30.523186: step 14039, loss 0.56587.
Train: 2018-07-31T12:05:30.695022: step 14040, loss 0.503631.
Test: 2018-07-31T12:05:31.179283: step 14040, loss 0.560673.
Train: 2018-07-31T12:05:31.351088: step 14041, loss 0.521892.
Train: 2018-07-31T12:05:31.522952: step 14042, loss 0.549032.
Train: 2018-07-31T12:05:31.694757: step 14043, loss 0.614455.
Train: 2018-07-31T12:05:31.866591: step 14044, loss 0.567898.
Train: 2018-07-31T12:05:32.054048: step 14045, loss 0.573324.
Train: 2018-07-31T12:05:32.225912: step 14046, loss 0.59023.
Train: 2018-07-31T12:05:32.413364: step 14047, loss 0.510907.
Train: 2018-07-31T12:05:32.585174: step 14048, loss 0.585199.
Train: 2018-07-31T12:05:32.757009: step 14049, loss 0.566815.
Train: 2018-07-31T12:05:32.944464: step 14050, loss 0.568878.
Test: 2018-07-31T12:05:33.413134: step 14050, loss 0.560626.
Train: 2018-07-31T12:05:33.600590: step 14051, loss 0.584945.
Train: 2018-07-31T12:05:33.772420: step 14052, loss 0.606934.
Train: 2018-07-31T12:05:33.959882: step 14053, loss 0.559297.
Train: 2018-07-31T12:05:34.131710: step 14054, loss 0.599187.
Train: 2018-07-31T12:05:34.303551: step 14055, loss 0.549561.
Train: 2018-07-31T12:05:34.490977: step 14056, loss 0.532018.
Train: 2018-07-31T12:05:34.662812: step 14057, loss 0.63989.
Train: 2018-07-31T12:05:34.850300: step 14058, loss 0.622251.
Train: 2018-07-31T12:05:35.022133: step 14059, loss 0.56897.
Train: 2018-07-31T12:05:35.209583: step 14060, loss 0.515262.
Test: 2018-07-31T12:05:35.678229: step 14060, loss 0.570506.
Train: 2018-07-31T12:05:35.865654: step 14061, loss 0.619786.
Train: 2018-07-31T12:05:36.053112: step 14062, loss 0.649212.
Train: 2018-07-31T12:05:36.224946: step 14063, loss 0.56816.
Train: 2018-07-31T12:05:36.412402: step 14064, loss 0.573646.
Train: 2018-07-31T12:05:36.584237: step 14065, loss 0.56967.
Train: 2018-07-31T12:05:36.771693: step 14066, loss 0.587668.
Train: 2018-07-31T12:05:36.943528: step 14067, loss 0.614547.
Train: 2018-07-31T12:05:37.130984: step 14068, loss 0.482479.
Train: 2018-07-31T12:05:37.302843: step 14069, loss 0.650541.
Train: 2018-07-31T12:05:37.490276: step 14070, loss 0.650763.
Test: 2018-07-31T12:05:37.974537: step 14070, loss 0.565781.
Train: 2018-07-31T12:05:38.146402: step 14071, loss 0.668401.
Train: 2018-07-31T12:05:38.333858: step 14072, loss 0.5369.
Train: 2018-07-31T12:05:38.505689: step 14073, loss 0.572036.
Train: 2018-07-31T12:05:38.693119: step 14074, loss 0.598197.
Train: 2018-07-31T12:05:38.864986: step 14075, loss 0.580748.
Train: 2018-07-31T12:05:39.052439: step 14076, loss 0.597968.
Train: 2018-07-31T12:05:39.224274: step 14077, loss 0.597779.
Train: 2018-07-31T12:05:39.411700: step 14078, loss 0.502828.
Train: 2018-07-31T12:05:39.583564: step 14079, loss 0.623126.
Train: 2018-07-31T12:05:39.771015: step 14080, loss 0.519026.
Test: 2018-07-31T12:05:40.239664: step 14080, loss 0.565253.
Train: 2018-07-31T12:05:40.427112: step 14081, loss 0.579628.
Train: 2018-07-31T12:05:40.598951: step 14082, loss 0.61356.
Train: 2018-07-31T12:05:40.786379: step 14083, loss 0.620025.
Train: 2018-07-31T12:05:40.958213: step 14084, loss 0.516152.
Train: 2018-07-31T12:05:41.145670: step 14085, loss 0.583843.
Train: 2018-07-31T12:05:41.317533: step 14086, loss 0.597721.
Train: 2018-07-31T12:05:41.504959: step 14087, loss 0.543387.
Train: 2018-07-31T12:05:41.692446: step 14088, loss 0.564265.
Train: 2018-07-31T12:05:41.864284: step 14089, loss 0.522937.
Train: 2018-07-31T12:05:42.051707: step 14090, loss 0.577386.
Test: 2018-07-31T12:05:42.520377: step 14090, loss 0.564529.
Train: 2018-07-31T12:05:42.707804: step 14091, loss 0.592276.
Train: 2018-07-31T12:05:42.895289: step 14092, loss 0.531481.
Train: 2018-07-31T12:05:43.067124: step 14093, loss 0.761101.
Train: 2018-07-31T12:05:43.254550: step 14094, loss 0.792257.
Train: 2018-07-31T12:05:43.442036: step 14095, loss 0.560715.
Train: 2018-07-31T12:05:43.613874: step 14096, loss 0.592617.
Train: 2018-07-31T12:05:43.801327: step 14097, loss 0.607437.
Train: 2018-07-31T12:05:43.988786: step 14098, loss 0.568757.
Train: 2018-07-31T12:05:44.176240: step 14099, loss 0.598248.
Train: 2018-07-31T12:05:44.363666: step 14100, loss 0.557866.
Test: 2018-07-31T12:05:44.847957: step 14100, loss 0.565716.
Train: 2018-07-31T12:05:45.597781: step 14101, loss 0.598915.
Train: 2018-07-31T12:05:45.769617: step 14102, loss 0.601782.
Train: 2018-07-31T12:05:45.957072: step 14103, loss 0.630249.
Train: 2018-07-31T12:05:46.128877: step 14104, loss 0.564268.
Train: 2018-07-31T12:05:46.316334: step 14105, loss 0.68921.
Train: 2018-07-31T12:05:46.488167: step 14106, loss 0.583875.
Train: 2018-07-31T12:05:46.675654: step 14107, loss 0.583991.
Train: 2018-07-31T12:05:46.863080: step 14108, loss 0.534867.
Train: 2018-07-31T12:05:47.034915: step 14109, loss 0.535945.
Train: 2018-07-31T12:05:47.222401: step 14110, loss 0.620311.
Test: 2018-07-31T12:05:47.706638: step 14110, loss 0.572937.
Train: 2018-07-31T12:05:47.878467: step 14111, loss 0.621257.
Train: 2018-07-31T12:05:48.065924: step 14112, loss 0.647153.
Train: 2018-07-31T12:05:48.237791: step 14113, loss 0.553841.
Train: 2018-07-31T12:05:48.409623: step 14114, loss 0.605219.
Train: 2018-07-31T12:05:48.597079: step 14115, loss 0.528876.
Train: 2018-07-31T12:05:48.768914: step 14116, loss 0.622657.
Train: 2018-07-31T12:05:48.956341: step 14117, loss 0.546078.
Train: 2018-07-31T12:05:49.128175: step 14118, loss 0.656868.
Train: 2018-07-31T12:05:49.315661: step 14119, loss 0.639698.
Train: 2018-07-31T12:05:49.503087: step 14120, loss 0.588381.
Test: 2018-07-31T12:05:49.971727: step 14120, loss 0.573893.
Train: 2018-07-31T12:05:50.143562: step 14121, loss 0.593509.
Train: 2018-07-31T12:05:50.331048: step 14122, loss 0.611598.
Train: 2018-07-31T12:05:50.502883: step 14123, loss 0.629966.
Train: 2018-07-31T12:05:50.690339: step 14124, loss 0.604292.
Train: 2018-07-31T12:05:50.862177: step 14125, loss 0.570291.
Train: 2018-07-31T12:05:51.049600: step 14126, loss 0.494208.
Train: 2018-07-31T12:05:51.221468: step 14127, loss 0.55318.
Train: 2018-07-31T12:05:51.408891: step 14128, loss 0.561461.
Train: 2018-07-31T12:05:51.596379: step 14129, loss 0.620595.
Train: 2018-07-31T12:05:51.783803: step 14130, loss 0.594958.
Test: 2018-07-31T12:05:52.268063: step 14130, loss 0.571991.
Train: 2018-07-31T12:05:52.439923: step 14131, loss 0.577722.
Train: 2018-07-31T12:05:52.627381: step 14132, loss 0.662446.
Train: 2018-07-31T12:05:52.814812: step 14133, loss 0.568589.
Train: 2018-07-31T12:05:53.002267: step 14134, loss 0.559717.
Train: 2018-07-31T12:05:53.174103: step 14135, loss 0.576293.
Train: 2018-07-31T12:05:53.361591: step 14136, loss 0.567345.
Train: 2018-07-31T12:05:53.533417: step 14137, loss 0.558361.
Train: 2018-07-31T12:05:53.720850: step 14138, loss 0.549334.
Train: 2018-07-31T12:05:53.908336: step 14139, loss 0.59149.
Train: 2018-07-31T12:05:54.080164: step 14140, loss 0.573923.
Test: 2018-07-31T12:05:54.564400: step 14140, loss 0.567661.
Train: 2018-07-31T12:05:54.751857: step 14141, loss 0.607706.
Train: 2018-07-31T12:05:54.923693: step 14142, loss 0.598696.
Train: 2018-07-31T12:05:55.111148: step 14143, loss 0.546787.
Train: 2018-07-31T12:05:55.282983: step 14144, loss 0.589256.
Train: 2018-07-31T12:05:55.470472: step 14145, loss 0.502896.
Train: 2018-07-31T12:05:55.657896: step 14146, loss 0.528169.
Train: 2018-07-31T12:05:55.829730: step 14147, loss 0.544919.
Train: 2018-07-31T12:05:56.017216: step 14148, loss 0.613802.
Train: 2018-07-31T12:05:56.189052: step 14149, loss 0.578789.
Train: 2018-07-31T12:05:56.376508: step 14150, loss 0.543646.
Test: 2018-07-31T12:05:56.845147: step 14150, loss 0.563555.
Train: 2018-07-31T12:05:57.032598: step 14151, loss 0.55199.
Train: 2018-07-31T12:05:57.204438: step 14152, loss 0.525806.
Train: 2018-07-31T12:05:57.391888: step 14153, loss 0.524935.
Train: 2018-07-31T12:05:57.563698: step 14154, loss 0.568602.
Train: 2018-07-31T12:05:57.751185: step 14155, loss 0.61266.
Train: 2018-07-31T12:05:57.923020: step 14156, loss 0.55947.
Train: 2018-07-31T12:05:58.110480: step 14157, loss 0.559366.
Train: 2018-07-31T12:05:58.282311: step 14158, loss 0.541469.
Train: 2018-07-31T12:05:58.469736: step 14159, loss 0.612756.
Train: 2018-07-31T12:05:58.657218: step 14160, loss 0.568053.
Test: 2018-07-31T12:05:59.125863: step 14160, loss 0.561935.
Train: 2018-07-31T12:05:59.313314: step 14161, loss 0.639586.
Train: 2018-07-31T12:05:59.485123: step 14162, loss 0.621582.
Train: 2018-07-31T12:05:59.672580: step 14163, loss 0.666111.
Train: 2018-07-31T12:05:59.860036: step 14164, loss 0.576564.
Train: 2018-07-31T12:06:00.047493: step 14165, loss 0.674201.
Train: 2018-07-31T12:06:00.234949: step 14166, loss 0.523106.
Train: 2018-07-31T12:06:00.406783: step 14167, loss 0.584861.
Train: 2018-07-31T12:06:00.594264: step 14168, loss 0.558253.
Train: 2018-07-31T12:06:00.766104: step 14169, loss 0.575658.
Train: 2018-07-31T12:06:00.953530: step 14170, loss 0.566714.
Test: 2018-07-31T12:06:01.437802: step 14170, loss 0.560649.
Train: 2018-07-31T12:06:01.609660: step 14171, loss 0.470456.
Train: 2018-07-31T12:06:01.797113: step 14172, loss 0.531424.
Train: 2018-07-31T12:06:01.984539: step 14173, loss 0.618635.
Train: 2018-07-31T12:06:02.156374: step 14174, loss 0.574753.
Train: 2018-07-31T12:06:02.343830: step 14175, loss 0.574584.
Train: 2018-07-31T12:06:02.531285: step 14176, loss 0.653027.
Train: 2018-07-31T12:06:02.703151: step 14177, loss 0.565541.
Train: 2018-07-31T12:06:02.890607: step 14178, loss 0.556696.
Train: 2018-07-31T12:06:03.078063: step 14179, loss 0.60004.
Train: 2018-07-31T12:06:03.249868: step 14180, loss 0.582499.
Test: 2018-07-31T12:06:03.734129: step 14180, loss 0.559174.
Train: 2018-07-31T12:06:03.921586: step 14181, loss 0.651675.
Train: 2018-07-31T12:06:04.093419: step 14182, loss 0.521723.
Train: 2018-07-31T12:06:04.280906: step 14183, loss 0.616558.
Train: 2018-07-31T12:06:04.452741: step 14184, loss 0.538915.
Train: 2018-07-31T12:06:04.640200: step 14185, loss 0.556041.
Train: 2018-07-31T12:06:04.827625: step 14186, loss 0.581698.
Train: 2018-07-31T12:06:04.999457: step 14187, loss 0.54731.
Train: 2018-07-31T12:06:05.186915: step 14188, loss 0.564363.
Train: 2018-07-31T12:06:05.374371: step 14189, loss 0.581402.
Train: 2018-07-31T12:06:05.561856: step 14190, loss 0.701148.
Test: 2018-07-31T12:06:06.030465: step 14190, loss 0.558377.
Train: 2018-07-31T12:06:06.217953: step 14191, loss 0.529996.
Train: 2018-07-31T12:06:06.405379: step 14192, loss 0.589634.
Train: 2018-07-31T12:06:06.577243: step 14193, loss 0.564014.
Train: 2018-07-31T12:06:06.749049: step 14194, loss 0.572445.
Train: 2018-07-31T12:06:06.920882: step 14195, loss 0.580855.
Train: 2018-07-31T12:06:07.108372: step 14196, loss 0.631594.
Train: 2018-07-31T12:06:07.280174: step 14197, loss 0.648297.
Train: 2018-07-31T12:06:07.467660: step 14198, loss 0.538504.
Train: 2018-07-31T12:06:07.655116: step 14199, loss 0.572131.
Train: 2018-07-31T12:06:07.826951: step 14200, loss 0.630758.
Test: 2018-07-31T12:06:08.311182: step 14200, loss 0.558032.
Train: 2018-07-31T12:06:09.061006: step 14201, loss 0.521881.
Train: 2018-07-31T12:06:09.232842: step 14202, loss 0.538596.
Train: 2018-07-31T12:06:09.420297: step 14203, loss 0.521878.
Train: 2018-07-31T12:06:09.607786: step 14204, loss 0.663663.
Train: 2018-07-31T12:06:09.795211: step 14205, loss 0.620848.
Train: 2018-07-31T12:06:09.982699: step 14206, loss 0.571803.
Train: 2018-07-31T12:06:10.154500: step 14207, loss 0.480556.
Train: 2018-07-31T12:06:10.341987: step 14208, loss 0.605141.
Train: 2018-07-31T12:06:10.513792: step 14209, loss 0.621851.
Train: 2018-07-31T12:06:10.701278: step 14210, loss 0.572103.
Test: 2018-07-31T12:06:11.169887: step 14210, loss 0.558316.
Train: 2018-07-31T12:06:11.341722: step 14211, loss 0.580506.
Train: 2018-07-31T12:06:11.529209: step 14212, loss 0.57858.
Train: 2018-07-31T12:06:11.701013: step 14213, loss 0.588973.
Train: 2018-07-31T12:06:11.888499: step 14214, loss 0.597327.
Train: 2018-07-31T12:06:12.138444: step 14215, loss 0.511031.
Train: 2018-07-31T12:06:12.325897: step 14216, loss 0.652452.
Train: 2018-07-31T12:06:12.513324: step 14217, loss 0.539705.
Train: 2018-07-31T12:06:12.700804: step 14218, loss 0.51538.
Train: 2018-07-31T12:06:12.872644: step 14219, loss 0.541059.
Train: 2018-07-31T12:06:13.044473: step 14220, loss 0.617094.
Test: 2018-07-31T12:06:13.528741: step 14220, loss 0.562513.
Train: 2018-07-31T12:06:13.700575: step 14221, loss 0.54307.
Train: 2018-07-31T12:06:13.888026: step 14222, loss 0.560882.
Train: 2018-07-31T12:06:14.075488: step 14223, loss 0.620784.
Train: 2018-07-31T12:06:14.247322: step 14224, loss 0.65557.
Train: 2018-07-31T12:06:14.434750: step 14225, loss 0.639639.
Train: 2018-07-31T12:06:14.622205: step 14226, loss 0.547759.
Train: 2018-07-31T12:06:14.794069: step 14227, loss 0.548432.
Train: 2018-07-31T12:06:14.965874: step 14228, loss 0.582664.
Train: 2018-07-31T12:06:15.153330: step 14229, loss 0.66743.
Train: 2018-07-31T12:06:15.340785: step 14230, loss 0.600303.
Test: 2018-07-31T12:06:15.825047: step 14230, loss 0.569616.
Train: 2018-07-31T12:06:15.996882: step 14231, loss 0.575294.
Train: 2018-07-31T12:06:16.168718: step 14232, loss 0.575467.
Train: 2018-07-31T12:06:16.356174: step 14233, loss 0.558748.
Train: 2018-07-31T12:06:16.543661: step 14234, loss 0.60079.
Train: 2018-07-31T12:06:16.715465: step 14235, loss 0.651178.
Train: 2018-07-31T12:06:16.902945: step 14236, loss 0.558638.
Train: 2018-07-31T12:06:17.074785: step 14237, loss 0.62562.
Train: 2018-07-31T12:06:17.262236: step 14238, loss 0.658887.
Train: 2018-07-31T12:06:17.434045: step 14239, loss 0.599954.
Train: 2018-07-31T12:06:17.605880: step 14240, loss 0.5663.
Test: 2018-07-31T12:06:18.090167: step 14240, loss 0.568744.
Train: 2018-07-31T12:06:18.262008: step 14241, loss 0.657578.
Train: 2018-07-31T12:06:18.433811: step 14242, loss 0.623859.
Train: 2018-07-31T12:06:18.621267: step 14243, loss 0.598596.
Train: 2018-07-31T12:06:18.808725: step 14244, loss 0.573495.
Train: 2018-07-31T12:06:18.980560: step 14245, loss 0.606056.
Train: 2018-07-31T12:06:19.168015: step 14246, loss 0.564671.
Train: 2018-07-31T12:06:19.355471: step 14247, loss 0.539817.
Train: 2018-07-31T12:06:19.527337: step 14248, loss 0.580371.
Train: 2018-07-31T12:06:19.699171: step 14249, loss 0.571849.
Train: 2018-07-31T12:06:19.886596: step 14250, loss 0.579675.
Test: 2018-07-31T12:06:20.370888: step 14250, loss 0.56564.
Train: 2018-07-31T12:06:20.558315: step 14251, loss 0.68563.
Train: 2018-07-31T12:06:20.730182: step 14252, loss 0.611671.
Train: 2018-07-31T12:06:20.902015: step 14253, loss 0.604239.
Train: 2018-07-31T12:06:21.089440: step 14254, loss 0.602819.
Train: 2018-07-31T12:06:21.261305: step 14255, loss 0.626862.
Train: 2018-07-31T12:06:21.448761: step 14256, loss 0.561786.
Train: 2018-07-31T12:06:21.620565: step 14257, loss 0.626247.
Train: 2018-07-31T12:06:21.792425: step 14258, loss 0.601783.
Train: 2018-07-31T12:06:21.979856: step 14259, loss 0.54528.
Train: 2018-07-31T12:06:22.151716: step 14260, loss 0.504988.
Test: 2018-07-31T12:06:22.620362: step 14260, loss 0.563616.
Train: 2018-07-31T12:06:22.807817: step 14261, loss 0.585119.
Train: 2018-07-31T12:06:22.979656: step 14262, loss 0.59298.
Train: 2018-07-31T12:06:23.167109: step 14263, loss 0.480063.
Train: 2018-07-31T12:06:23.338946: step 14264, loss 0.544164.
Train: 2018-07-31T12:06:23.510778: step 14265, loss 0.543813.
Train: 2018-07-31T12:06:23.698203: step 14266, loss 0.527152.
Train: 2018-07-31T12:06:23.870069: step 14267, loss 0.60033.
Train: 2018-07-31T12:06:24.041904: step 14268, loss 0.501558.
Train: 2018-07-31T12:06:24.229363: step 14269, loss 0.50091.
Train: 2018-07-31T12:06:24.401189: step 14270, loss 0.550112.
Test: 2018-07-31T12:06:24.885425: step 14270, loss 0.560827.
Train: 2018-07-31T12:06:25.057285: step 14271, loss 0.52459.
Train: 2018-07-31T12:06:25.244747: step 14272, loss 0.566209.
Train: 2018-07-31T12:06:25.416551: step 14273, loss 0.557464.
Train: 2018-07-31T12:06:25.604041: step 14274, loss 0.574266.
Train: 2018-07-31T12:06:25.775843: step 14275, loss 0.625658.
Train: 2018-07-31T12:06:25.963298: step 14276, loss 0.565306.
Train: 2018-07-31T12:06:26.135164: step 14277, loss 0.634405.
Train: 2018-07-31T12:06:26.322620: step 14278, loss 0.556273.
Train: 2018-07-31T12:06:26.494454: step 14279, loss 0.529995.
Train: 2018-07-31T12:06:26.666284: step 14280, loss 0.459961.
Test: 2018-07-31T12:06:27.150550: step 14280, loss 0.558562.
Train: 2018-07-31T12:06:27.338001: step 14281, loss 0.573236.
Train: 2018-07-31T12:06:27.509812: step 14282, loss 0.573125.
Train: 2018-07-31T12:06:27.681677: step 14283, loss 0.528886.
Train: 2018-07-31T12:06:27.853481: step 14284, loss 0.608364.
Train: 2018-07-31T12:06:28.040967: step 14285, loss 0.581711.
Train: 2018-07-31T12:06:28.212773: step 14286, loss 0.581634.
Train: 2018-07-31T12:06:28.384607: step 14287, loss 0.536983.
Train: 2018-07-31T12:06:28.572093: step 14288, loss 0.581484.
Train: 2018-07-31T12:06:28.743897: step 14289, loss 0.545642.
Train: 2018-07-31T12:06:28.915732: step 14290, loss 0.599255.
Test: 2018-07-31T12:06:29.400025: step 14290, loss 0.557297.
Train: 2018-07-31T12:06:29.571859: step 14291, loss 0.590227.
Train: 2018-07-31T12:06:29.759309: step 14292, loss 0.53637.
Train: 2018-07-31T12:06:29.931150: step 14293, loss 0.652844.
Train: 2018-07-31T12:06:30.102955: step 14294, loss 0.598919.
Train: 2018-07-31T12:06:30.290435: step 14295, loss 0.589837.
Train: 2018-07-31T12:06:30.462294: step 14296, loss 0.580782.
Train: 2018-07-31T12:06:30.634080: step 14297, loss 0.553955.
Train: 2018-07-31T12:06:30.821537: step 14298, loss 0.598349.
Train: 2018-07-31T12:06:30.993370: step 14299, loss 0.553855.
Train: 2018-07-31T12:06:31.165230: step 14300, loss 0.56266.
Test: 2018-07-31T12:06:31.633875: step 14300, loss 0.556635.
Train: 2018-07-31T12:06:32.368079: step 14301, loss 0.456529.
Train: 2018-07-31T12:06:32.539917: step 14302, loss 0.544849.
Train: 2018-07-31T12:06:32.711748: step 14303, loss 0.597877.
Train: 2018-07-31T12:06:32.899205: step 14304, loss 0.562415.
Train: 2018-07-31T12:06:33.071039: step 14305, loss 0.544653.
Train: 2018-07-31T12:06:33.258496: step 14306, loss 0.526875.
Train: 2018-07-31T12:06:33.430331: step 14307, loss 0.482449.
Train: 2018-07-31T12:06:33.602135: step 14308, loss 0.553304.
Train: 2018-07-31T12:06:33.789591: step 14309, loss 0.597776.
Train: 2018-07-31T12:06:33.961426: step 14310, loss 0.553169.
Test: 2018-07-31T12:06:34.445718: step 14310, loss 0.556013.
Train: 2018-07-31T12:06:34.617552: step 14311, loss 0.597783.
Train: 2018-07-31T12:06:34.789357: step 14312, loss 0.553054.
Train: 2018-07-31T12:06:34.976813: step 14313, loss 0.535106.
Train: 2018-07-31T12:06:35.148672: step 14314, loss 0.508157.
Train: 2018-07-31T12:06:35.336104: step 14315, loss 0.525962.
Train: 2018-07-31T12:06:35.507938: step 14316, loss 0.561826.
Train: 2018-07-31T12:06:35.695425: step 14317, loss 0.570804.
Train: 2018-07-31T12:06:35.867259: step 14318, loss 0.624964.
Train: 2018-07-31T12:06:36.054686: step 14319, loss 0.624945.
Train: 2018-07-31T12:06:36.226555: step 14320, loss 0.480444.
Test: 2018-07-31T12:06:36.695160: step 14320, loss 0.55554.
Train: 2018-07-31T12:06:36.866996: step 14321, loss 0.570664.
Train: 2018-07-31T12:06:37.054482: step 14322, loss 0.633861.
Train: 2018-07-31T12:06:37.226320: step 14323, loss 0.543508.
Train: 2018-07-31T12:06:37.398120: step 14324, loss 0.543481.
Train: 2018-07-31T12:06:37.585607: step 14325, loss 0.579508.
Train: 2018-07-31T12:06:37.773063: step 14326, loss 0.66954.
Train: 2018-07-31T12:06:37.944898: step 14327, loss 0.678209.
Train: 2018-07-31T12:06:38.116702: step 14328, loss 0.543469.
Train: 2018-07-31T12:06:38.304188: step 14329, loss 0.507846.
Train: 2018-07-31T12:06:38.476026: step 14330, loss 0.543508.
Test: 2018-07-31T12:06:38.960288: step 14330, loss 0.555272.
Train: 2018-07-31T12:06:39.132115: step 14331, loss 0.570137.
Train: 2018-07-31T12:06:39.303955: step 14332, loss 0.552371.
Train: 2018-07-31T12:06:39.475783: step 14333, loss 0.534659.
Train: 2018-07-31T12:06:39.647595: step 14334, loss 0.561173.
Train: 2018-07-31T12:06:39.835050: step 14335, loss 0.578815.
Train: 2018-07-31T12:06:40.022507: step 14336, loss 0.614081.
Train: 2018-07-31T12:06:40.194341: step 14337, loss 0.51703.
Train: 2018-07-31T12:06:40.366200: step 14338, loss 0.596289.
Train: 2018-07-31T12:06:40.553632: step 14339, loss 0.525858.
Train: 2018-07-31T12:06:40.725467: step 14340, loss 0.569805.
Test: 2018-07-31T12:06:41.209731: step 14340, loss 0.555058.
Train: 2018-07-31T12:06:41.412806: step 14341, loss 0.543412.
Train: 2018-07-31T12:06:41.584670: step 14342, loss 0.569748.
Train: 2018-07-31T12:06:41.756506: step 14343, loss 0.543363.
Train: 2018-07-31T12:06:41.943962: step 14344, loss 0.60485.
Train: 2018-07-31T12:06:42.100144: step 14345, loss 0.588407.
Train: 2018-07-31T12:06:42.287632: step 14346, loss 0.525759.
Train: 2018-07-31T12:06:42.459460: step 14347, loss 0.578387.
Train: 2018-07-31T12:06:42.631270: step 14348, loss 0.516956.
Train: 2018-07-31T12:06:42.803130: step 14349, loss 0.578342.
Train: 2018-07-31T12:06:42.974973: step 14350, loss 0.551986.
Test: 2018-07-31T12:06:43.443610: step 14350, loss 0.554813.
Train: 2018-07-31T12:06:43.631067: step 14351, loss 0.630999.
Train: 2018-07-31T12:06:43.802871: step 14352, loss 0.525619.
Train: 2018-07-31T12:06:43.974706: step 14353, loss 0.613344.
Train: 2018-07-31T12:06:44.162192: step 14354, loss 0.525612.
Train: 2018-07-31T12:06:44.334028: step 14355, loss 0.613246.
Train: 2018-07-31T12:06:44.505831: step 14356, loss 0.473076.
Train: 2018-07-31T12:06:44.693326: step 14357, loss 0.55185.
Train: 2018-07-31T12:06:44.865152: step 14358, loss 0.57813.
Train: 2018-07-31T12:06:45.052603: step 14359, loss 0.516685.
Train: 2018-07-31T12:06:45.224412: step 14360, loss 0.578117.
Test: 2018-07-31T12:06:45.693054: step 14360, loss 0.554581.
Train: 2018-07-31T12:06:45.880540: step 14361, loss 0.595705.
Train: 2018-07-31T12:06:46.052374: step 14362, loss 0.507698.
Train: 2018-07-31T12:06:46.224212: step 14363, loss 0.534039.
Train: 2018-07-31T12:06:46.411636: step 14364, loss 0.604577.
Train: 2018-07-31T12:06:46.583469: step 14365, loss 0.525097.
Train: 2018-07-31T12:06:46.755305: step 14366, loss 0.622316.
Train: 2018-07-31T12:06:46.927140: step 14367, loss 0.613459.
Train: 2018-07-31T12:06:47.098973: step 14368, loss 0.533854.
Train: 2018-07-31T12:06:47.286430: step 14369, loss 0.569187.
Train: 2018-07-31T12:06:47.458265: step 14370, loss 0.613333.
Test: 2018-07-31T12:06:47.942527: step 14370, loss 0.554364.
Train: 2018-07-31T12:06:48.114391: step 14371, loss 0.586785.
Train: 2018-07-31T12:06:48.286227: step 14372, loss 0.533877.
Train: 2018-07-31T12:06:48.473652: step 14373, loss 0.533884.
Train: 2018-07-31T12:06:48.645517: step 14374, loss 0.533873.
Train: 2018-07-31T12:06:48.817352: step 14375, loss 0.516243.
Train: 2018-07-31T12:06:48.989189: step 14376, loss 0.621916.
Train: 2018-07-31T12:06:49.176648: step 14377, loss 0.569027.
Train: 2018-07-31T12:06:49.348472: step 14378, loss 0.595439.
Train: 2018-07-31T12:06:49.520282: step 14379, loss 0.560184.
Train: 2018-07-31T12:06:49.707739: step 14380, loss 0.595362.
Test: 2018-07-31T12:06:50.176379: step 14380, loss 0.554226.
Train: 2018-07-31T12:06:50.363865: step 14381, loss 0.58652.
Train: 2018-07-31T12:06:50.551290: step 14382, loss 0.51627.
Train: 2018-07-31T12:06:50.723158: step 14383, loss 0.621529.
Train: 2018-07-31T12:06:50.910583: step 14384, loss 0.577637.
Train: 2018-07-31T12:06:51.082416: step 14385, loss 0.560111.
Train: 2018-07-31T12:06:51.269873: step 14386, loss 0.498973.
Train: 2018-07-31T12:06:51.441708: step 14387, loss 0.621216.
Train: 2018-07-31T12:06:51.613572: step 14388, loss 0.525181.
Train: 2018-07-31T12:06:51.785378: step 14389, loss 0.499008.
Train: 2018-07-31T12:06:51.957242: step 14390, loss 0.603698.
Test: 2018-07-31T12:06:52.457125: step 14390, loss 0.554133.
Train: 2018-07-31T12:06:52.628929: step 14391, loss 0.525093.
Train: 2018-07-31T12:06:52.800795: step 14392, loss 0.568748.
Train: 2018-07-31T12:06:52.988248: step 14393, loss 0.524996.
Train: 2018-07-31T12:06:53.160086: step 14394, loss 0.533687.
Train: 2018-07-31T12:06:53.347544: step 14395, loss 0.568724.
Train: 2018-07-31T12:06:53.519376: step 14396, loss 0.489607.
Train: 2018-07-31T12:06:53.691211: step 14397, loss 0.639236.
Train: 2018-07-31T12:06:53.863046: step 14398, loss 0.586361.
Train: 2018-07-31T12:06:54.034851: step 14399, loss 0.542219.
Train: 2018-07-31T12:06:54.206685: step 14400, loss 0.542209.
Test: 2018-07-31T12:06:54.690977: step 14400, loss 0.553876.
Train: 2018-07-31T12:06:55.487635: step 14401, loss 0.630604.
Train: 2018-07-31T12:06:55.675121: step 14402, loss 0.559832.
Train: 2018-07-31T12:06:55.846959: step 14403, loss 0.559819.
Train: 2018-07-31T12:06:56.034381: step 14404, loss 0.577484.
Train: 2018-07-31T12:06:56.206216: step 14405, loss 0.577464.
Train: 2018-07-31T12:06:56.378052: step 14406, loss 0.497978.
Train: 2018-07-31T12:06:56.549916: step 14407, loss 0.577436.
Train: 2018-07-31T12:06:56.721754: step 14408, loss 0.533247.
Train: 2018-07-31T12:06:56.893585: step 14409, loss 0.542056.
Train: 2018-07-31T12:06:57.081045: step 14410, loss 0.559726.
Test: 2018-07-31T12:06:57.549681: step 14410, loss 0.553735.
Train: 2018-07-31T12:06:57.737138: step 14411, loss 0.541994.
Train: 2018-07-31T12:06:57.908973: step 14412, loss 0.54196.
Train: 2018-07-31T12:06:58.080803: step 14413, loss 0.595213.
Train: 2018-07-31T12:06:58.252642: step 14414, loss 0.506348.
Train: 2018-07-31T12:06:58.424477: step 14415, loss 0.550759.
Train: 2018-07-31T12:06:58.611934: step 14416, loss 0.604222.
Train: 2018-07-31T12:06:58.783737: step 14417, loss 0.515036.
Train: 2018-07-31T12:06:58.955602: step 14418, loss 0.514966.
Train: 2018-07-31T12:06:59.127438: step 14419, loss 0.541716.
Train: 2018-07-31T12:06:59.299242: step 14420, loss 0.676066.
Test: 2018-07-31T12:06:59.783535: step 14420, loss 0.553544.
Train: 2018-07-31T12:06:59.970993: step 14421, loss 0.523731.
Train: 2018-07-31T12:07:00.142825: step 14422, loss 0.613399.
Train: 2018-07-31T12:07:00.314659: step 14423, loss 0.595426.
Train: 2018-07-31T12:07:00.486489: step 14424, loss 0.604323.
Train: 2018-07-31T12:07:00.673920: step 14425, loss 0.577399.
Train: 2018-07-31T12:07:00.845779: step 14426, loss 0.6309.
Train: 2018-07-31T12:07:01.017591: step 14427, loss 0.603975.
Train: 2018-07-31T12:07:01.189452: step 14428, loss 0.522612.
Train: 2018-07-31T12:07:01.376905: step 14429, loss 0.620961.
Train: 2018-07-31T12:07:01.548747: step 14430, loss 0.531585.
Test: 2018-07-31T12:07:02.032977: step 14430, loss 0.553661.
Train: 2018-07-31T12:07:02.220466: step 14431, loss 0.611537.
Train: 2018-07-31T12:07:02.392298: step 14432, loss 0.612165.
Train: 2018-07-31T12:07:02.564103: step 14433, loss 0.586014.
Train: 2018-07-31T12:07:02.751591: step 14434, loss 0.54276.
Train: 2018-07-31T12:07:02.923395: step 14435, loss 0.577781.
Train: 2018-07-31T12:07:03.110849: step 14436, loss 0.552157.
Train: 2018-07-31T12:07:03.282683: step 14437, loss 0.578365.
Train: 2018-07-31T12:07:03.454519: step 14438, loss 0.53577.
Train: 2018-07-31T12:07:03.641999: step 14439, loss 0.553129.
Train: 2018-07-31T12:07:03.813835: step 14440, loss 0.561954.
Test: 2018-07-31T12:07:04.298104: step 14440, loss 0.556363.
Train: 2018-07-31T12:07:04.469907: step 14441, loss 0.570736.
Train: 2018-07-31T12:07:04.657362: step 14442, loss 0.570873.
Train: 2018-07-31T12:07:04.829222: step 14443, loss 0.613813.
Train: 2018-07-31T12:07:05.001034: step 14444, loss 0.519728.
Train: 2018-07-31T12:07:05.188489: step 14445, loss 0.494115.
Train: 2018-07-31T12:07:05.360322: step 14446, loss 0.545414.
Train: 2018-07-31T12:07:05.532188: step 14447, loss 0.622584.
Train: 2018-07-31T12:07:05.719647: step 14448, loss 0.622581.
Train: 2018-07-31T12:07:05.907070: step 14449, loss 0.46804.
Train: 2018-07-31T12:07:06.078935: step 14450, loss 0.648312.
Test: 2018-07-31T12:07:06.563196: step 14450, loss 0.556494.
Train: 2018-07-31T12:07:06.735000: step 14451, loss 0.579481.
Train: 2018-07-31T12:07:06.922458: step 14452, loss 0.596586.
Train: 2018-07-31T12:07:07.094317: step 14453, loss 0.579315.
Train: 2018-07-31T12:07:07.281778: step 14454, loss 0.622116.
Train: 2018-07-31T12:07:07.453613: step 14455, loss 0.519186.
Train: 2018-07-31T12:07:07.625418: step 14456, loss 0.502017.
Train: 2018-07-31T12:07:07.812874: step 14457, loss 0.536142.
Train: 2018-07-31T12:07:07.984708: step 14458, loss 0.613416.
Train: 2018-07-31T12:07:08.172165: step 14459, loss 0.553088.
Train: 2018-07-31T12:07:08.344000: step 14460, loss 0.578767.
Test: 2018-07-31T12:07:08.828261: step 14460, loss 0.555723.
Train: 2018-07-31T12:07:09.000096: step 14461, loss 0.587295.
Train: 2018-07-31T12:07:09.187553: step 14462, loss 0.527083.
Train: 2018-07-31T12:07:09.375032: step 14463, loss 0.587184.
Train: 2018-07-31T12:07:09.546873: step 14464, loss 0.638757.
Train: 2018-07-31T12:07:09.734329: step 14465, loss 0.59565.
Train: 2018-07-31T12:07:09.921777: step 14466, loss 0.561215.
Train: 2018-07-31T12:07:10.093589: step 14467, loss 0.664081.
Train: 2018-07-31T12:07:10.265454: step 14468, loss 0.535473.
Train: 2018-07-31T12:07:10.452879: step 14469, loss 0.535479.
Train: 2018-07-31T12:07:10.624745: step 14470, loss 0.612218.
Test: 2018-07-31T12:07:11.093388: step 14470, loss 0.555257.
Train: 2018-07-31T12:07:11.280843: step 14471, loss 0.569518.
Train: 2018-07-31T12:07:11.452677: step 14472, loss 0.577968.
Train: 2018-07-31T12:07:11.624482: step 14473, loss 0.569413.
Train: 2018-07-31T12:07:11.811936: step 14474, loss 0.560883.
Train: 2018-07-31T12:07:11.983802: step 14475, loss 0.594727.
Train: 2018-07-31T12:07:12.155637: step 14476, loss 0.535419.
Train: 2018-07-31T12:07:12.343063: step 14477, loss 0.628416.
Train: 2018-07-31T12:07:12.514922: step 14478, loss 0.518498.
Train: 2018-07-31T12:07:12.686732: step 14479, loss 0.526907.
Train: 2018-07-31T12:07:12.874218: step 14480, loss 0.628211.
Test: 2018-07-31T12:07:13.358449: step 14480, loss 0.554885.
Train: 2018-07-31T12:07:13.530285: step 14481, loss 0.501472.
Train: 2018-07-31T12:07:13.713629: step 14482, loss 0.577431.
Train: 2018-07-31T12:07:13.901110: step 14483, loss 0.58659.
Train: 2018-07-31T12:07:14.072920: step 14484, loss 0.636599.
Train: 2018-07-31T12:07:14.260377: step 14485, loss 0.585807.
Train: 2018-07-31T12:07:14.432241: step 14486, loss 0.585809.
Train: 2018-07-31T12:07:14.619700: step 14487, loss 0.526775.
Train: 2018-07-31T12:07:14.791533: step 14488, loss 0.585854.
Train: 2018-07-31T12:07:14.963367: step 14489, loss 0.585885.
Train: 2018-07-31T12:07:15.135204: step 14490, loss 0.577486.
Test: 2018-07-31T12:07:15.619464: step 14490, loss 0.554995.
Train: 2018-07-31T12:07:15.806919: step 14491, loss 0.628023.
Train: 2018-07-31T12:07:15.978754: step 14492, loss 0.585931.
Train: 2018-07-31T12:07:16.150559: step 14493, loss 0.543979.
Train: 2018-07-31T12:07:16.338045: step 14494, loss 0.686478.
Train: 2018-07-31T12:07:16.509882: step 14495, loss 0.544106.
Train: 2018-07-31T12:07:16.681685: step 14496, loss 0.586947.
Train: 2018-07-31T12:07:16.853519: step 14497, loss 0.527579.
Train: 2018-07-31T12:07:17.025385: step 14498, loss 0.594059.
Train: 2018-07-31T12:07:17.197188: step 14499, loss 0.585703.
Train: 2018-07-31T12:07:17.384644: step 14500, loss 0.593934.
Test: 2018-07-31T12:07:17.853315: step 14500, loss 0.555192.
Train: 2018-07-31T12:07:18.618756: step 14501, loss 0.535958.
Train: 2018-07-31T12:07:18.790596: step 14502, loss 0.519399.
Train: 2018-07-31T12:07:18.962400: step 14503, loss 0.577215.
Train: 2018-07-31T12:07:19.149886: step 14504, loss 0.55233.
Train: 2018-07-31T12:07:19.321722: step 14505, loss 0.585394.
Train: 2018-07-31T12:07:19.509148: step 14506, loss 0.618515.
Train: 2018-07-31T12:07:19.681012: step 14507, loss 0.576999.
Train: 2018-07-31T12:07:19.852830: step 14508, loss 0.610101.
Train: 2018-07-31T12:07:20.040273: step 14509, loss 0.560331.
Train: 2018-07-31T12:07:20.212138: step 14510, loss 0.485791.
Test: 2018-07-31T12:07:20.696370: step 14510, loss 0.554624.
Train: 2018-07-31T12:07:20.868204: step 14511, loss 0.593376.
Train: 2018-07-31T12:07:21.055661: step 14512, loss 0.585049.
Train: 2018-07-31T12:07:21.227495: step 14513, loss 0.5518.
Train: 2018-07-31T12:07:21.399354: step 14514, loss 0.526798.
Train: 2018-07-31T12:07:21.586810: step 14515, loss 0.576634.
Train: 2018-07-31T12:07:21.758622: step 14516, loss 0.643344.
Train: 2018-07-31T12:07:21.930456: step 14517, loss 0.534847.
Train: 2018-07-31T12:07:22.117942: step 14518, loss 0.618297.
Train: 2018-07-31T12:07:22.289777: step 14519, loss 0.543102.
Train: 2018-07-31T12:07:22.461611: step 14520, loss 0.517987.
Test: 2018-07-31T12:07:22.945843: step 14520, loss 0.554071.
Train: 2018-07-31T12:07:23.117678: step 14521, loss 0.609939.
Train: 2018-07-31T12:07:23.289543: step 14522, loss 0.584812.
Train: 2018-07-31T12:07:23.461377: step 14523, loss 0.601554.
Train: 2018-07-31T12:07:23.648803: step 14524, loss 0.517742.
Train: 2018-07-31T12:07:23.836259: step 14525, loss 0.584731.
Train: 2018-07-31T12:07:24.008095: step 14526, loss 0.551183.
Train: 2018-07-31T12:07:24.179929: step 14527, loss 0.57635.
Train: 2018-07-31T12:07:24.367385: step 14528, loss 0.584749.
Train: 2018-07-31T12:07:24.539220: step 14529, loss 0.500632.
Train: 2018-07-31T12:07:24.726701: step 14530, loss 0.610043.
Test: 2018-07-31T12:07:25.195346: step 14530, loss 0.553767.
Train: 2018-07-31T12:07:25.382797: step 14531, loss 0.603222.
Train: 2018-07-31T12:07:25.554637: step 14532, loss 0.534167.
Train: 2018-07-31T12:07:25.726441: step 14533, loss 0.576434.
Train: 2018-07-31T12:07:25.913900: step 14534, loss 0.508939.
Train: 2018-07-31T12:07:26.085757: step 14535, loss 0.542822.
Train: 2018-07-31T12:07:26.257599: step 14536, loss 0.542899.
Train: 2018-07-31T12:07:26.429401: step 14537, loss 0.542965.
Train: 2018-07-31T12:07:26.616857: step 14538, loss 0.536583.
Train: 2018-07-31T12:07:26.788723: step 14539, loss 0.560177.
Train: 2018-07-31T12:07:26.976150: step 14540, loss 0.517341.
Test: 2018-07-31T12:07:27.444789: step 14540, loss 0.554571.
Train: 2018-07-31T12:07:27.632277: step 14541, loss 0.543146.
Train: 2018-07-31T12:07:27.804081: step 14542, loss 0.56914.
Train: 2018-07-31T12:07:27.975944: step 14543, loss 0.517151.
Train: 2018-07-31T12:07:28.163370: step 14544, loss 0.560643.
Train: 2018-07-31T12:07:28.335236: step 14545, loss 0.613205.
Train: 2018-07-31T12:07:28.507065: step 14546, loss 0.569508.
Train: 2018-07-31T12:07:28.694527: step 14547, loss 0.551965.
Train: 2018-07-31T12:07:28.866364: step 14548, loss 0.560757.
Train: 2018-07-31T12:07:29.038196: step 14549, loss 0.640174.
Train: 2018-07-31T12:07:29.210026: step 14550, loss 0.596027.
Test: 2018-07-31T12:07:29.694262: step 14550, loss 0.554716.
Train: 2018-07-31T12:07:29.866098: step 14551, loss 0.551846.
Train: 2018-07-31T12:07:30.037962: step 14552, loss 0.613565.
Train: 2018-07-31T12:07:30.225413: step 14553, loss 0.61345.
Train: 2018-07-31T12:07:30.397254: step 14554, loss 0.516497.
Train: 2018-07-31T12:07:30.584678: step 14555, loss 0.542835.
Train: 2018-07-31T12:07:30.756546: step 14556, loss 0.560342.
Train: 2018-07-31T12:07:30.928379: step 14557, loss 0.612983.
Train: 2018-07-31T12:07:31.100213: step 14558, loss 0.568963.
Train: 2018-07-31T12:07:31.272017: step 14559, loss 0.516278.
Train: 2018-07-31T12:07:31.459473: step 14560, loss 0.551269.
Test: 2018-07-31T12:07:31.943766: step 14560, loss 0.554044.
Train: 2018-07-31T12:07:32.115601: step 14561, loss 0.472294.
Train: 2018-07-31T12:07:32.303057: step 14562, loss 0.603793.
Train: 2018-07-31T12:07:32.474891: step 14563, loss 0.63892.
Train: 2018-07-31T12:07:32.646727: step 14564, loss 0.603669.
Train: 2018-07-31T12:07:32.818561: step 14565, loss 0.550916.
Train: 2018-07-31T12:07:32.990395: step 14566, loss 0.577163.
Train: 2018-07-31T12:07:33.162224: step 14567, loss 0.542074.
Train: 2018-07-31T12:07:33.334034: step 14568, loss 0.568283.
Train: 2018-07-31T12:07:33.521490: step 14569, loss 0.585719.
Train: 2018-07-31T12:07:33.693326: step 14570, loss 0.559446.
Test: 2018-07-31T12:07:34.161996: step 14570, loss 0.553516.
Train: 2018-07-31T12:07:34.349452: step 14571, loss 0.611775.
Train: 2018-07-31T12:07:34.521291: step 14572, loss 0.559372.
Train: 2018-07-31T12:07:34.693122: step 14573, loss 0.602852.
Train: 2018-07-31T12:07:34.880548: step 14574, loss 0.594056.
Train: 2018-07-31T12:07:35.052382: step 14575, loss 0.541964.
Train: 2018-07-31T12:07:35.224247: step 14576, loss 0.567923.
Train: 2018-07-31T12:07:35.411704: step 14577, loss 0.541975.
Train: 2018-07-31T12:07:35.583539: step 14578, loss 0.593749.
Train: 2018-07-31T12:07:35.755375: step 14579, loss 0.654016.
Train: 2018-07-31T12:07:35.942824: step 14580, loss 0.610769.
Test: 2018-07-31T12:07:36.427091: step 14580, loss 0.55343.
Train: 2018-07-31T12:07:36.598895: step 14581, loss 0.507823.
Train: 2018-07-31T12:07:36.770760: step 14582, loss 0.473724.
Train: 2018-07-31T12:07:36.958217: step 14583, loss 0.550629.
Train: 2018-07-31T12:07:37.145644: step 14584, loss 0.567704.
Train: 2018-07-31T12:07:37.317477: step 14585, loss 0.64469.
Train: 2018-07-31T12:07:37.489342: step 14586, loss 0.507834.
Train: 2018-07-31T12:07:37.661180: step 14587, loss 0.550546.
Train: 2018-07-31T12:07:37.848602: step 14588, loss 0.524859.
Train: 2018-07-31T12:07:38.020467: step 14589, loss 0.559041.
Train: 2018-07-31T12:07:38.207918: step 14590, loss 0.516133.
Test: 2018-07-31T12:07:38.676564: step 14590, loss 0.553178.
Train: 2018-07-31T12:07:38.848368: step 14591, loss 0.550382.
Train: 2018-07-31T12:07:39.035855: step 14592, loss 0.533099.
Train: 2018-07-31T12:07:39.207661: step 14593, loss 0.541631.
Train: 2018-07-31T12:07:39.379526: step 14594, loss 0.567538.
Train: 2018-07-31T12:07:39.551360: step 14595, loss 0.619647.
Train: 2018-07-31T12:07:39.738786: step 14596, loss 0.593612.
Train: 2018-07-31T12:07:39.910621: step 14597, loss 0.576211.
Train: 2018-07-31T12:07:40.082455: step 14598, loss 0.515268.
Train: 2018-07-31T12:07:40.254317: step 14599, loss 0.567489.
Train: 2018-07-31T12:07:40.441746: step 14600, loss 0.56748.
Test: 2018-07-31T12:07:40.910386: step 14600, loss 0.552849.
Train: 2018-07-31T12:07:41.675832: step 14601, loss 0.611128.
Train: 2018-07-31T12:07:41.847667: step 14602, loss 0.55.
Train: 2018-07-31T12:07:42.019501: step 14603, loss 0.567448.
Train: 2018-07-31T12:07:42.206958: step 14604, loss 0.558706.
Train: 2018-07-31T12:07:42.378824: step 14605, loss 0.515034.
Train: 2018-07-31T12:07:42.550626: step 14606, loss 0.628607.
Train: 2018-07-31T12:07:42.722491: step 14607, loss 0.611098.
Train: 2018-07-31T12:07:42.894327: step 14608, loss 0.602303.
Train: 2018-07-31T12:07:43.081751: step 14609, loss 0.549951.
Train: 2018-07-31T12:07:43.253587: step 14610, loss 0.69786.
Test: 2018-07-31T12:07:43.722260: step 14610, loss 0.552823.
Train: 2018-07-31T12:07:43.894092: step 14611, loss 0.558671.
Train: 2018-07-31T12:07:44.065897: step 14612, loss 0.550044.
Train: 2018-07-31T12:07:44.253385: step 14613, loss 0.644853.
Train: 2018-07-31T12:07:44.425218: step 14614, loss 0.575879.
Train: 2018-07-31T12:07:44.597023: step 14615, loss 0.567288.
Train: 2018-07-31T12:07:44.784479: step 14616, loss 0.609892.
Train: 2018-07-31T12:07:44.956344: step 14617, loss 0.635196.
Train: 2018-07-31T12:07:45.159415: step 14618, loss 0.592631.
Train: 2018-07-31T12:07:45.331256: step 14619, loss 0.542064.
Train: 2018-07-31T12:07:45.518715: step 14620, loss 0.491896.
Test: 2018-07-31T12:07:46.002975: step 14620, loss 0.553292.
Train: 2018-07-31T12:07:46.174778: step 14621, loss 0.592392.
Train: 2018-07-31T12:07:46.346643: step 14622, loss 0.533909.
Train: 2018-07-31T12:07:46.518447: step 14623, loss 0.575638.
Train: 2018-07-31T12:07:46.690282: step 14624, loss 0.542295.
Train: 2018-07-31T12:07:46.862150: step 14625, loss 0.550626.
Train: 2018-07-31T12:07:47.033951: step 14626, loss 0.600615.
Train: 2018-07-31T12:07:47.205817: step 14627, loss 0.575602.
Train: 2018-07-31T12:07:47.377621: step 14628, loss 0.550603.
Train: 2018-07-31T12:07:47.549488: step 14629, loss 0.525593.
Train: 2018-07-31T12:07:47.721316: step 14630, loss 0.508843.
Test: 2018-07-31T12:07:48.205553: step 14630, loss 0.553214.
Train: 2018-07-31T12:07:48.377420: step 14631, loss 0.592308.
Train: 2018-07-31T12:07:48.564844: step 14632, loss 0.525316.
Train: 2018-07-31T12:07:48.736678: step 14633, loss 0.583981.
Train: 2018-07-31T12:07:48.908543: step 14634, loss 0.525088.
Train: 2018-07-31T12:07:49.095969: step 14635, loss 0.524953.
Train: 2018-07-31T12:07:49.267838: step 14636, loss 0.516323.
Train: 2018-07-31T12:07:49.439639: step 14637, loss 0.558607.
Train: 2018-07-31T12:07:49.611473: step 14638, loss 0.558563.
Train: 2018-07-31T12:07:49.783338: step 14639, loss 0.609928.
Train: 2018-07-31T12:07:49.970764: step 14640, loss 0.567083.
Test: 2018-07-31T12:07:50.439435: step 14640, loss 0.552661.
Train: 2018-07-31T12:07:50.611269: step 14641, loss 0.541254.
Train: 2018-07-31T12:07:50.798720: step 14642, loss 0.523927.
Train: 2018-07-31T12:07:50.970560: step 14643, loss 0.523795.
Train: 2018-07-31T12:07:51.142396: step 14644, loss 0.567074.
Train: 2018-07-31T12:07:51.314229: step 14645, loss 0.671632.
Train: 2018-07-31T12:07:51.501690: step 14646, loss 0.628107.
Train: 2018-07-31T12:07:51.657900: step 14647, loss 0.660016.
Train: 2018-07-31T12:07:51.829734: step 14648, loss 0.523577.
Train: 2018-07-31T12:07:52.001538: step 14649, loss 0.567035.
Train: 2018-07-31T12:07:52.189020: step 14650, loss 0.497584.
Test: 2018-07-31T12:07:52.673286: step 14650, loss 0.552491.
Train: 2018-07-31T12:07:52.845121: step 14651, loss 0.558344.
Train: 2018-07-31T12:07:53.016925: step 14652, loss 0.627736.
Train: 2018-07-31T12:07:53.188760: step 14653, loss 0.506335.
Train: 2018-07-31T12:07:53.376241: step 14654, loss 0.54099.
Train: 2018-07-31T12:07:53.548052: step 14655, loss 0.514945.
Train: 2018-07-31T12:07:53.735508: step 14656, loss 0.601592.
Train: 2018-07-31T12:07:53.907373: step 14657, loss 0.566983.
Train: 2018-07-31T12:07:54.079177: step 14658, loss 0.523935.
Train: 2018-07-31T12:07:54.266659: step 14659, loss 0.567506.
Train: 2018-07-31T12:07:54.438498: step 14660, loss 0.567162.
Test: 2018-07-31T12:07:54.922729: step 14660, loss 0.552385.
Train: 2018-07-31T12:07:55.094565: step 14661, loss 0.619347.
Train: 2018-07-31T12:07:55.266429: step 14662, loss 0.558295.
Train: 2018-07-31T12:07:55.453885: step 14663, loss 0.601928.
Train: 2018-07-31T12:07:55.625722: step 14664, loss 0.628064.
Train: 2018-07-31T12:07:55.797526: step 14665, loss 0.506223.
Train: 2018-07-31T12:07:55.969359: step 14666, loss 0.567116.
Train: 2018-07-31T12:07:56.156846: step 14667, loss 0.567138.
Train: 2018-07-31T12:07:56.328678: step 14668, loss 0.584513.
Train: 2018-07-31T12:07:56.500517: step 14669, loss 0.627856.
Train: 2018-07-31T12:07:56.672350: step 14670, loss 0.558526.
Test: 2018-07-31T12:07:57.156580: step 14670, loss 0.552718.
Train: 2018-07-31T12:07:57.328446: step 14671, loss 0.54991.
Train: 2018-07-31T12:07:57.500276: step 14672, loss 0.601682.
Train: 2018-07-31T12:07:57.672086: step 14673, loss 0.4897.
Train: 2018-07-31T12:07:57.843921: step 14674, loss 0.524167.
Train: 2018-07-31T12:07:58.015785: step 14675, loss 0.593006.
Train: 2018-07-31T12:07:58.203236: step 14676, loss 0.627451.
Train: 2018-07-31T12:07:58.375078: step 14677, loss 0.58435.
Train: 2018-07-31T12:07:58.546881: step 14678, loss 0.567127.
Train: 2018-07-31T12:07:58.718715: step 14679, loss 0.481293.
Train: 2018-07-31T12:07:58.906172: step 14680, loss 0.532746.
Test: 2018-07-31T12:07:59.390450: step 14680, loss 0.552676.
Train: 2018-07-31T12:07:59.562268: step 14681, loss 0.498301.
Train: 2018-07-31T12:07:59.734104: step 14682, loss 0.558437.
Train: 2018-07-31T12:07:59.905938: step 14683, loss 0.472037.
Train: 2018-07-31T12:08:00.093394: step 14684, loss 0.601701.
Train: 2018-07-31T12:08:00.265229: step 14685, loss 0.53224.
Train: 2018-07-31T12:08:00.437093: step 14686, loss 0.628038.
Train: 2018-07-31T12:08:00.624551: step 14687, loss 0.63685.
Train: 2018-07-31T12:08:00.812005: step 14688, loss 0.549507.
Train: 2018-07-31T12:08:00.983843: step 14689, loss 0.540752.
Train: 2018-07-31T12:08:01.155645: step 14690, loss 0.619379.
Test: 2018-07-31T12:08:01.624317: step 14690, loss 0.552292.
Train: 2018-07-31T12:08:01.796144: step 14691, loss 0.549451.
Train: 2018-07-31T12:08:01.983577: step 14692, loss 0.584371.
Train: 2018-07-31T12:08:02.155441: step 14693, loss 0.584343.
Train: 2018-07-31T12:08:02.327276: step 14694, loss 0.514545.
Train: 2018-07-31T12:08:02.499080: step 14695, loss 0.497085.
Train: 2018-07-31T12:08:02.686561: step 14696, loss 0.627969.
Train: 2018-07-31T12:08:02.858404: step 14697, loss 0.610488.
Train: 2018-07-31T12:08:03.030205: step 14698, loss 0.636601.
Train: 2018-07-31T12:08:03.202072: step 14699, loss 0.592904.
Train: 2018-07-31T12:08:03.373876: step 14700, loss 0.514679.
Test: 2018-07-31T12:08:03.858167: step 14700, loss 0.552235.
Train: 2018-07-31T12:08:04.607962: step 14701, loss 0.566756.
Train: 2018-07-31T12:08:04.779827: step 14702, loss 0.532105.
Train: 2018-07-31T12:08:04.951632: step 14703, loss 0.618654.
Train: 2018-07-31T12:08:05.123466: step 14704, loss 0.506228.
Train: 2018-07-31T12:08:05.295301: step 14705, loss 0.601261.
Train: 2018-07-31T12:08:05.482757: step 14706, loss 0.575324.
Train: 2018-07-31T12:08:05.654622: step 14707, loss 0.55806.
Train: 2018-07-31T12:08:05.842072: step 14708, loss 0.575288.
Train: 2018-07-31T12:08:06.013883: step 14709, loss 0.566662.
Train: 2018-07-31T12:08:06.185718: step 14710, loss 0.661268.
Test: 2018-07-31T12:08:06.654388: step 14710, loss 0.552275.
Train: 2018-07-31T12:08:06.841863: step 14711, loss 0.600962.
Train: 2018-07-31T12:08:07.029269: step 14712, loss 0.498205.
Train: 2018-07-31T12:08:07.201134: step 14713, loss 0.6008.
Train: 2018-07-31T12:08:07.372971: step 14714, loss 0.541044.
Train: 2018-07-31T12:08:07.544774: step 14715, loss 0.609203.
Train: 2018-07-31T12:08:07.716633: step 14716, loss 0.566615.
Train: 2018-07-31T12:08:07.888444: step 14717, loss 0.532659.
Train: 2018-07-31T12:08:08.060278: step 14718, loss 0.558123.
Train: 2018-07-31T12:08:08.232146: step 14719, loss 0.566599.
Train: 2018-07-31T12:08:08.419594: step 14720, loss 0.558118.
Test: 2018-07-31T12:08:08.903830: step 14720, loss 0.552394.
Train: 2018-07-31T12:08:09.075665: step 14721, loss 0.583534.
Train: 2018-07-31T12:08:09.247499: step 14722, loss 0.617404.
Train: 2018-07-31T12:08:09.419365: step 14723, loss 0.591956.
Train: 2018-07-31T12:08:09.591169: step 14724, loss 0.549683.
Train: 2018-07-31T12:08:09.778655: step 14725, loss 0.549697.
Train: 2018-07-31T12:08:09.950461: step 14726, loss 0.490669.
Train: 2018-07-31T12:08:10.137917: step 14727, loss 0.541229.
Train: 2018-07-31T12:08:10.309781: step 14728, loss 0.591913.
Train: 2018-07-31T12:08:10.481620: step 14729, loss 0.549607.
Train: 2018-07-31T12:08:10.669072: step 14730, loss 0.608902.
Test: 2018-07-31T12:08:11.137712: step 14730, loss 0.552319.
Train: 2018-07-31T12:08:11.309547: step 14731, loss 0.566518.
Train: 2018-07-31T12:08:11.481382: step 14732, loss 0.490176.
Train: 2018-07-31T12:08:11.653187: step 14733, loss 0.600491.
Train: 2018-07-31T12:08:11.825022: step 14734, loss 0.481422.
Train: 2018-07-31T12:08:11.996857: step 14735, loss 0.532362.
Train: 2018-07-31T12:08:12.231200: step 14736, loss 0.566472.
Train: 2018-07-31T12:08:12.418663: step 14737, loss 0.575046.
Train: 2018-07-31T12:08:12.590468: step 14738, loss 0.58366.
Train: 2018-07-31T12:08:12.762334: step 14739, loss 0.600917.
Train: 2018-07-31T12:08:12.934162: step 14740, loss 0.549162.
Test: 2018-07-31T12:08:13.418397: step 14740, loss 0.551993.
Train: 2018-07-31T12:08:13.590263: step 14741, loss 0.53203.
Train: 2018-07-31T12:08:13.777689: step 14742, loss 0.56673.
Train: 2018-07-31T12:08:13.965147: step 14743, loss 0.549152.
Train: 2018-07-31T12:08:14.137010: step 14744, loss 0.497046.
Train: 2018-07-31T12:08:14.324436: step 14745, loss 0.522355.
Train: 2018-07-31T12:08:14.496303: step 14746, loss 0.522308.
Train: 2018-07-31T12:08:14.668136: step 14747, loss 0.581979.
Train: 2018-07-31T12:08:14.855592: step 14748, loss 0.54316.
Train: 2018-07-31T12:08:15.043049: step 14749, loss 0.566468.
Train: 2018-07-31T12:08:15.214882: step 14750, loss 0.584236.
Test: 2018-07-31T12:08:15.683493: step 14750, loss 0.552016.
Train: 2018-07-31T12:08:15.870982: step 14751, loss 0.575611.
Train: 2018-07-31T12:08:16.042814: step 14752, loss 0.602064.
Train: 2018-07-31T12:08:16.214619: step 14753, loss 0.557722.
Train: 2018-07-31T12:08:16.402075: step 14754, loss 0.602946.
Train: 2018-07-31T12:08:16.573940: step 14755, loss 0.51559.
Train: 2018-07-31T12:08:16.745769: step 14756, loss 0.699932.
Train: 2018-07-31T12:08:16.933231: step 14757, loss 0.594064.
Train: 2018-07-31T12:08:17.105068: step 14758, loss 0.532657.
Train: 2018-07-31T12:08:17.292521: step 14759, loss 0.611772.
Train: 2018-07-31T12:08:17.464326: step 14760, loss 0.541783.
Test: 2018-07-31T12:08:17.948618: step 14760, loss 0.553479.
Train: 2018-07-31T12:08:18.120422: step 14761, loss 0.524456.
Train: 2018-07-31T12:08:18.307879: step 14762, loss 0.568062.
Train: 2018-07-31T12:08:18.479713: step 14763, loss 0.651552.
Train: 2018-07-31T12:08:18.667199: step 14764, loss 0.529445.
Train: 2018-07-31T12:08:18.854655: step 14765, loss 0.568214.
Train: 2018-07-31T12:08:19.026490: step 14766, loss 0.559413.
Train: 2018-07-31T12:08:19.213917: step 14767, loss 0.542121.
Train: 2018-07-31T12:08:19.385776: step 14768, loss 0.568259.
Train: 2018-07-31T12:08:19.573208: step 14769, loss 0.544014.
Train: 2018-07-31T12:08:19.745074: step 14770, loss 0.541674.
Test: 2018-07-31T12:08:20.213682: step 14770, loss 0.554449.
Train: 2018-07-31T12:08:20.416786: step 14771, loss 0.570042.
Train: 2018-07-31T12:08:20.588625: step 14772, loss 0.578107.
Train: 2018-07-31T12:08:20.776051: step 14773, loss 0.588435.
Train: 2018-07-31T12:08:20.947915: step 14774, loss 0.551696.
Train: 2018-07-31T12:08:21.135341: step 14775, loss 0.51794.
Train: 2018-07-31T12:08:21.322797: step 14776, loss 0.535504.
Train: 2018-07-31T12:08:21.510278: step 14777, loss 0.693803.
Train: 2018-07-31T12:08:21.697709: step 14778, loss 0.616562.
Train: 2018-07-31T12:08:21.869544: step 14779, loss 0.590665.
Train: 2018-07-31T12:08:22.072621: step 14780, loss 0.659542.
Test: 2018-07-31T12:08:22.541291: step 14780, loss 0.557996.
Train: 2018-07-31T12:08:22.728748: step 14781, loss 0.537939.
Train: 2018-07-31T12:08:22.916175: step 14782, loss 0.493061.
Train: 2018-07-31T12:08:23.103663: step 14783, loss 0.606015.
Train: 2018-07-31T12:08:23.275465: step 14784, loss 0.614399.
Train: 2018-07-31T12:08:23.478572: step 14785, loss 0.552118.
Train: 2018-07-31T12:08:23.666029: step 14786, loss 0.574416.
Train: 2018-07-31T12:08:23.853487: step 14787, loss 0.636951.
Train: 2018-07-31T12:08:24.040927: step 14788, loss 0.601125.
Train: 2018-07-31T12:08:24.212776: step 14789, loss 0.596906.
Train: 2018-07-31T12:08:24.400232: step 14790, loss 0.631994.
Test: 2018-07-31T12:08:24.868841: step 14790, loss 0.564277.
Train: 2018-07-31T12:08:25.056298: step 14791, loss 0.625555.
Train: 2018-07-31T12:08:25.243755: step 14792, loss 0.660701.
Train: 2018-07-31T12:08:25.431238: step 14793, loss 0.593806.
Train: 2018-07-31T12:08:25.603046: step 14794, loss 0.585341.
Train: 2018-07-31T12:08:25.790531: step 14795, loss 0.648223.
Train: 2018-07-31T12:08:25.977958: step 14796, loss 0.529988.
Train: 2018-07-31T12:08:26.165444: step 14797, loss 0.547885.
Train: 2018-07-31T12:08:26.337278: step 14798, loss 0.702586.
Train: 2018-07-31T12:08:26.524735: step 14799, loss 0.567291.
Train: 2018-07-31T12:08:26.712190: step 14800, loss 0.60217.
Test: 2018-07-31T12:08:27.180831: step 14800, loss 0.572344.
Train: 2018-07-31T12:08:27.946276: step 14801, loss 0.620004.
Train: 2018-07-31T12:08:28.133735: step 14802, loss 0.570468.
Train: 2018-07-31T12:08:28.321188: step 14803, loss 0.562802.
Train: 2018-07-31T12:08:28.508644: step 14804, loss 0.528917.
Train: 2018-07-31T12:08:28.696096: step 14805, loss 0.563758.
Train: 2018-07-31T12:08:28.883551: step 14806, loss 0.630714.
Train: 2018-07-31T12:08:29.070983: step 14807, loss 0.522207.
Train: 2018-07-31T12:08:29.258469: step 14808, loss 0.622396.
Train: 2018-07-31T12:08:29.430304: step 14809, loss 0.623427.
Train: 2018-07-31T12:08:29.633376: step 14810, loss 0.573858.
Test: 2018-07-31T12:08:30.102021: step 14810, loss 0.577351.
Train: 2018-07-31T12:08:30.289449: step 14811, loss 0.591381.
Train: 2018-07-31T12:08:30.476904: step 14812, loss 0.525164.
Train: 2018-07-31T12:08:30.664391: step 14813, loss 0.576358.
Train: 2018-07-31T12:08:30.851848: step 14814, loss 0.54338.
Train: 2018-07-31T12:08:31.039302: step 14815, loss 0.569931.
Train: 2018-07-31T12:08:31.211106: step 14816, loss 0.636832.
Train: 2018-07-31T12:08:31.398593: step 14817, loss 0.604285.
Train: 2018-07-31T12:08:31.586019: step 14818, loss 0.579003.
Train: 2018-07-31T12:08:31.773475: step 14819, loss 0.519297.
Train: 2018-07-31T12:08:31.960930: step 14820, loss 0.639146.
Test: 2018-07-31T12:08:32.445193: step 14820, loss 0.581777.
Train: 2018-07-31T12:08:32.632648: step 14821, loss 0.553198.
Train: 2018-07-31T12:08:32.820105: step 14822, loss 0.587347.
Train: 2018-07-31T12:08:33.007591: step 14823, loss 0.587017.
Train: 2018-07-31T12:08:33.195016: step 14824, loss 0.482658.
Train: 2018-07-31T12:08:33.382474: step 14825, loss 0.655691.
Train: 2018-07-31T12:08:33.569960: step 14826, loss 0.576832.
Train: 2018-07-31T12:08:33.741796: step 14827, loss 0.549991.
Train: 2018-07-31T12:08:33.944855: step 14828, loss 0.58436.
Train: 2018-07-31T12:08:34.132328: step 14829, loss 0.599136.
Train: 2018-07-31T12:08:34.304132: step 14830, loss 0.565476.
Test: 2018-07-31T12:08:34.788424: step 14830, loss 0.576515.
Train: 2018-07-31T12:08:34.975849: step 14831, loss 0.591315.
Train: 2018-07-31T12:08:35.163336: step 14832, loss 0.55529.
Train: 2018-07-31T12:08:35.350795: step 14833, loss 0.577604.
Train: 2018-07-31T12:08:35.522598: step 14834, loss 0.60739.
Train: 2018-07-31T12:08:35.710079: step 14835, loss 0.518152.
Train: 2018-07-31T12:08:35.897509: step 14836, loss 0.518101.
Train: 2018-07-31T12:08:36.084995: step 14837, loss 0.518176.
Train: 2018-07-31T12:08:36.288043: step 14838, loss 0.635399.
Train: 2018-07-31T12:08:36.459878: step 14839, loss 0.590827.
Train: 2018-07-31T12:08:36.647335: step 14840, loss 0.672756.
Test: 2018-07-31T12:08:37.115999: step 14840, loss 0.576501.
Train: 2018-07-31T12:08:37.303460: step 14841, loss 0.673182.
Train: 2018-07-31T12:08:37.490918: step 14842, loss 0.679586.
Train: 2018-07-31T12:08:37.678373: step 14843, loss 0.547034.
Train: 2018-07-31T12:08:37.865798: step 14844, loss 0.574334.
Train: 2018-07-31T12:08:38.053286: step 14845, loss 0.583704.
Train: 2018-07-31T12:08:38.240710: step 14846, loss 0.646829.
Train: 2018-07-31T12:08:38.428191: step 14847, loss 0.574746.
Train: 2018-07-31T12:08:38.615624: step 14848, loss 0.611095.
Train: 2018-07-31T12:08:38.803108: step 14849, loss 0.647363.
Train: 2018-07-31T12:08:38.990559: step 14850, loss 0.629508.
Test: 2018-07-31T12:08:39.459175: step 14850, loss 0.578766.
Train: 2018-07-31T12:08:39.646631: step 14851, loss 0.665416.
Train: 2018-07-31T12:08:39.834087: step 14852, loss 0.504691.
Train: 2018-07-31T12:08:40.005923: step 14853, loss 0.611741.
Train: 2018-07-31T12:08:40.193408: step 14854, loss 0.620551.
Train: 2018-07-31T12:08:40.380834: step 14855, loss 0.684231.
Train: 2018-07-31T12:08:40.568321: step 14856, loss 0.637698.
Train: 2018-07-31T12:08:40.755747: step 14857, loss 0.619708.
Train: 2018-07-31T12:08:40.943203: step 14858, loss 0.558209.
Train: 2018-07-31T12:08:41.130658: step 14859, loss 0.618846.
Train: 2018-07-31T12:08:41.302524: step 14860, loss 0.627015.
Test: 2018-07-31T12:08:41.786785: step 14860, loss 0.577542.
Train: 2018-07-31T12:08:42.021106: step 14861, loss 0.695392.
Train: 2018-07-31T12:08:42.208562: step 14862, loss 0.59153.
Train: 2018-07-31T12:08:42.395987: step 14863, loss 0.60811.
Train: 2018-07-31T12:08:42.583473: step 14864, loss 0.616024.
Train: 2018-07-31T12:08:42.755308: step 14865, loss 0.649151.
Train: 2018-07-31T12:08:42.942736: step 14866, loss 0.564473.
Train: 2018-07-31T12:08:43.130191: step 14867, loss 0.589155.
Train: 2018-07-31T12:08:43.317646: step 14868, loss 0.505537.
Train: 2018-07-31T12:08:43.505103: step 14869, loss 0.57991.
Train: 2018-07-31T12:08:43.692590: step 14870, loss 0.612613.
Test: 2018-07-31T12:08:44.161209: step 14870, loss 0.573438.
Train: 2018-07-31T12:08:44.348685: step 14871, loss 0.595575.
Train: 2018-07-31T12:08:44.536111: step 14872, loss 0.557757.
Train: 2018-07-31T12:08:44.723567: step 14873, loss 0.660911.
Train: 2018-07-31T12:08:44.911025: step 14874, loss 0.652376.
Train: 2018-07-31T12:08:45.098504: step 14875, loss 0.627469.
Train: 2018-07-31T12:08:45.285936: step 14876, loss 0.692939.
Train: 2018-07-31T12:08:45.473425: step 14877, loss 0.504834.
Train: 2018-07-31T12:08:45.660882: step 14878, loss 0.594605.
Train: 2018-07-31T12:08:45.848304: step 14879, loss 0.659589.
Train: 2018-07-31T12:08:46.035760: step 14880, loss 0.53795.
Test: 2018-07-31T12:08:46.520046: step 14880, loss 0.572956.
Train: 2018-07-31T12:08:46.707507: step 14881, loss 0.66731.
Train: 2018-07-31T12:08:46.894964: step 14882, loss 0.667027.
Train: 2018-07-31T12:08:47.066799: step 14883, loss 0.634531.
Train: 2018-07-31T12:08:47.269871: step 14884, loss 0.602265.
Train: 2018-07-31T12:08:47.457332: step 14885, loss 0.665732.
Train: 2018-07-31T12:08:47.644758: step 14886, loss 0.586018.
Train: 2018-07-31T12:08:47.816593: step 14887, loss 0.609496.
Train: 2018-07-31T12:08:48.019701: step 14888, loss 0.601355.
Train: 2018-07-31T12:08:48.191539: step 14889, loss 0.554132.
Train: 2018-07-31T12:08:48.378962: step 14890, loss 0.553943.
Test: 2018-07-31T12:08:48.847602: step 14890, loss 0.571825.
Train: 2018-07-31T12:08:49.035057: step 14891, loss 0.608307.
Train: 2018-07-31T12:08:49.222513: step 14892, loss 0.6158.
Train: 2018-07-31T12:08:49.410000: step 14893, loss 0.568741.
Train: 2018-07-31T12:08:49.597425: step 14894, loss 0.552861.
Train: 2018-07-31T12:08:49.784912: step 14895, loss 0.505711.
Train: 2018-07-31T12:08:49.972339: step 14896, loss 0.64595.
Train: 2018-07-31T12:08:50.159824: step 14897, loss 0.575194.
Train: 2018-07-31T12:08:50.347252: step 14898, loss 0.590572.
Train: 2018-07-31T12:08:50.534736: step 14899, loss 0.582403.
Train: 2018-07-31T12:08:50.722188: step 14900, loss 0.589993.
Test: 2018-07-31T12:08:51.190803: step 14900, loss 0.568548.
Train: 2018-07-31T12:08:51.940658: step 14901, loss 0.558061.
Train: 2018-07-31T12:08:52.128083: step 14902, loss 0.621192.
Train: 2018-07-31T12:08:52.299918: step 14903, loss 0.525539.
Train: 2018-07-31T12:08:52.487406: step 14904, loss 0.549003.
Train: 2018-07-31T12:08:52.674860: step 14905, loss 0.604699.
Train: 2018-07-31T12:08:52.862319: step 14906, loss 0.580377.
Train: 2018-07-31T12:08:53.049773: step 14907, loss 0.531673.
Train: 2018-07-31T12:08:53.237199: step 14908, loss 0.514976.
Train: 2018-07-31T12:08:53.440278: step 14909, loss 0.612203.
Train: 2018-07-31T12:08:53.627733: step 14910, loss 0.595723.
Test: 2018-07-31T12:08:54.096372: step 14910, loss 0.565336.
Train: 2018-07-31T12:08:54.283829: step 14911, loss 0.620229.
Train: 2018-07-31T12:08:54.471286: step 14912, loss 0.653126.
Train: 2018-07-31T12:08:54.658772: step 14913, loss 0.520903.
Train: 2018-07-31T12:08:54.830576: step 14914, loss 0.619901.
Train: 2018-07-31T12:08:55.018056: step 14915, loss 0.619783.
Train: 2018-07-31T12:08:55.205487: step 14916, loss 0.586457.
Train: 2018-07-31T12:08:55.392944: step 14917, loss 0.627803.
Train: 2018-07-31T12:08:55.580430: step 14918, loss 0.561251.
Train: 2018-07-31T12:08:55.767886: step 14919, loss 0.5113.
Train: 2018-07-31T12:08:55.939690: step 14920, loss 0.560915.
Test: 2018-07-31T12:08:56.439604: step 14920, loss 0.56344.
Train: 2018-07-31T12:08:56.611441: step 14921, loss 0.544072.
Train: 2018-07-31T12:08:56.814510: step 14922, loss 0.560538.
Train: 2018-07-31T12:08:57.001973: step 14923, loss 0.585468.
Train: 2018-07-31T12:08:57.173776: step 14924, loss 0.526594.
Train: 2018-07-31T12:08:57.361234: step 14925, loss 0.57682.
Train: 2018-07-31T12:08:57.548719: step 14926, loss 0.627368.
Train: 2018-07-31T12:08:57.736146: step 14927, loss 0.585032.
Train: 2018-07-31T12:08:57.908010: step 14928, loss 0.551036.
Train: 2018-07-31T12:08:58.095436: step 14929, loss 0.508429.
Train: 2018-07-31T12:08:58.282892: step 14930, loss 0.593264.
Test: 2018-07-31T12:08:58.767153: step 14930, loss 0.561833.
Train: 2018-07-31T12:08:58.939020: step 14931, loss 0.533449.
Train: 2018-07-31T12:08:59.126469: step 14932, loss 0.550343.
Train: 2018-07-31T12:08:59.313901: step 14933, loss 0.541575.
Train: 2018-07-31T12:08:59.501356: step 14934, loss 0.549983.
Train: 2018-07-31T12:08:59.673192: step 14935, loss 0.532502.
Train: 2018-07-31T12:08:59.860648: step 14936, loss 0.593048.
Train: 2018-07-31T12:09:00.032507: step 14937, loss 0.52332.
Train: 2018-07-31T12:09:00.219963: step 14938, loss 0.557079.
Train: 2018-07-31T12:09:00.407425: step 14939, loss 0.601811.
Train: 2018-07-31T12:09:00.594881: step 14940, loss 0.628243.
Test: 2018-07-31T12:09:01.063521: step 14940, loss 0.56063.
Train: 2018-07-31T12:09:01.250948: step 14941, loss 0.54853.
Train: 2018-07-31T12:09:01.438433: step 14942, loss 0.540059.
Train: 2018-07-31T12:09:01.625859: step 14943, loss 0.469241.
Train: 2018-07-31T12:09:01.813346: step 14944, loss 0.509562.
Train: 2018-07-31T12:09:02.000772: step 14945, loss 0.635829.
Train: 2018-07-31T12:09:02.188258: step 14946, loss 0.600809.
Train: 2018-07-31T12:09:02.375714: step 14947, loss 0.494215.
Train: 2018-07-31T12:09:02.547519: step 14948, loss 0.63135.
Train: 2018-07-31T12:09:02.719354: step 14949, loss 0.521636.
Train: 2018-07-31T12:09:02.906809: step 14950, loss 0.643589.
Test: 2018-07-31T12:09:03.375480: step 14950, loss 0.566741.
Train: 2018-07-31T12:09:03.562935: step 14951, loss 0.553635.
Train: 2018-07-31T12:09:03.750392: step 14952, loss 0.536428.
Train: 2018-07-31T12:09:03.937817: step 14953, loss 0.611609.
Train: 2018-07-31T12:09:04.125275: step 14954, loss 0.558544.
Train: 2018-07-31T12:09:04.297139: step 14955, loss 0.65902.
Train: 2018-07-31T12:09:04.484565: step 14956, loss 0.569403.
Train: 2018-07-31T12:09:04.672020: step 14957, loss 0.633283.
Train: 2018-07-31T12:09:04.859502: step 14958, loss 0.579603.
Train: 2018-07-31T12:09:05.046933: step 14959, loss 0.597974.
Train: 2018-07-31T12:09:05.234390: step 14960, loss 0.589199.
Test: 2018-07-31T12:09:05.703028: step 14960, loss 0.574295.
Train: 2018-07-31T12:09:05.890487: step 14961, loss 0.580349.
Train: 2018-07-31T12:09:06.077942: step 14962, loss 0.643097.
Train: 2018-07-31T12:09:06.249776: step 14963, loss 0.491012.
Train: 2018-07-31T12:09:06.437232: step 14964, loss 0.615968.
Train: 2018-07-31T12:09:06.624719: step 14965, loss 0.580092.
Train: 2018-07-31T12:09:06.812175: step 14966, loss 0.597686.
Train: 2018-07-31T12:09:06.983979: step 14967, loss 0.606289.
Train: 2018-07-31T12:09:07.171466: step 14968, loss 0.597057.
Train: 2018-07-31T12:09:07.358892: step 14969, loss 0.658715.
Train: 2018-07-31T12:09:07.530726: step 14970, loss 0.684605.
Test: 2018-07-31T12:09:08.014987: step 14970, loss 0.572258.
Train: 2018-07-31T12:09:08.186853: step 14971, loss 0.622165.
Train: 2018-07-31T12:09:08.374309: step 14972, loss 0.525279.
Train: 2018-07-31T12:09:08.561735: step 14973, loss 0.525035.
Train: 2018-07-31T12:09:08.733571: step 14974, loss 0.585632.
Train: 2018-07-31T12:09:08.921056: step 14975, loss 0.595136.
Train: 2018-07-31T12:09:09.108514: step 14976, loss 0.576057.
Train: 2018-07-31T12:09:09.280317: step 14977, loss 0.549673.
Train: 2018-07-31T12:09:09.467805: step 14978, loss 0.540644.
Train: 2018-07-31T12:09:09.655230: step 14979, loss 0.566201.
Train: 2018-07-31T12:09:09.842685: step 14980, loss 0.548509.
Test: 2018-07-31T12:09:10.326946: step 14980, loss 0.568245.
Train: 2018-07-31T12:09:10.498807: step 14981, loss 0.513466.
Train: 2018-07-31T12:09:10.686238: step 14982, loss 0.551025.
Train: 2018-07-31T12:09:10.858073: step 14983, loss 0.634249.
Train: 2018-07-31T12:09:11.045558: step 14984, loss 0.642903.
Train: 2018-07-31T12:09:11.217364: step 14985, loss 0.581861.
Train: 2018-07-31T12:09:11.404851: step 14986, loss 0.590663.
Train: 2018-07-31T12:09:11.592305: step 14987, loss 0.512275.
Train: 2018-07-31T12:09:11.764141: step 14988, loss 0.590934.
Train: 2018-07-31T12:09:11.967187: step 14989, loss 0.564838.
Train: 2018-07-31T12:09:12.139023: step 14990, loss 0.704899.
Test: 2018-07-31T12:09:12.623284: step 14990, loss 0.5678.
Train: 2018-07-31T12:09:12.810770: step 14991, loss 0.556221.
Train: 2018-07-31T12:09:12.982575: step 14992, loss 0.599867.
Train: 2018-07-31T12:09:13.170030: step 14993, loss 0.556171.
Train: 2018-07-31T12:09:13.357511: step 14994, loss 0.582196.
Train: 2018-07-31T12:09:13.544944: step 14995, loss 0.573306.
Train: 2018-07-31T12:09:13.716777: step 14996, loss 0.564383.
Train: 2018-07-31T12:09:13.904265: step 14997, loss 0.633739.
Train: 2018-07-31T12:09:14.076069: step 14998, loss 0.53778.
Train: 2018-07-31T12:09:14.263525: step 14999, loss 0.49404.
Train: 2018-07-31T12:09:14.435391: step 15000, loss 0.554505.
Test: 2018-07-31T12:09:14.903999: step 15000, loss 0.565683.
Train: 2018-07-31T12:09:15.685097: step 15001, loss 0.5977.
Train: 2018-07-31T12:09:15.872523: step 15002, loss 0.588663.
Train: 2018-07-31T12:09:16.044388: step 15003, loss 0.562156.
Train: 2018-07-31T12:09:16.231838: step 15004, loss 0.561816.
Train: 2018-07-31T12:09:16.403680: step 15005, loss 0.596457.
Train: 2018-07-31T12:09:16.591137: step 15006, loss 0.639899.
Train: 2018-07-31T12:09:16.778585: step 15007, loss 0.569623.
Train: 2018-07-31T12:09:16.950396: step 15008, loss 0.578079.
Train: 2018-07-31T12:09:17.122230: step 15009, loss 0.577807.
Train: 2018-07-31T12:09:17.309716: step 15010, loss 0.525195.
Test: 2018-07-31T12:09:17.778327: step 15010, loss 0.562688.
Train: 2018-07-31T12:09:17.965783: step 15011, loss 0.568578.
Train: 2018-07-31T12:09:18.137619: step 15012, loss 0.603272.
Train: 2018-07-31T12:09:18.325104: step 15013, loss 0.568126.
Train: 2018-07-31T12:09:18.496939: step 15014, loss 0.602833.
Train: 2018-07-31T12:09:18.684395: step 15015, loss 0.524102.
Train: 2018-07-31T12:09:18.856224: step 15016, loss 0.51517.
Train: 2018-07-31T12:09:19.043656: step 15017, loss 0.541124.
Train: 2018-07-31T12:09:19.215516: step 15018, loss 0.549648.
Train: 2018-07-31T12:09:19.402971: step 15019, loss 0.549437.
Train: 2018-07-31T12:09:19.574782: step 15020, loss 0.47013.
Test: 2018-07-31T12:09:20.059042: step 15020, loss 0.560689.
Train: 2018-07-31T12:09:20.230878: step 15021, loss 0.637214.
Train: 2018-07-31T12:09:20.402742: step 15022, loss 0.593007.
Train: 2018-07-31T12:09:20.590169: step 15023, loss 0.584045.
Train: 2018-07-31T12:09:20.777655: step 15024, loss 0.583922.
Train: 2018-07-31T12:09:20.949459: step 15025, loss 0.592666.
Train: 2018-07-31T12:09:21.136915: step 15026, loss 0.574802.
Train: 2018-07-31T12:09:21.308780: step 15027, loss 0.503729.
Train: 2018-07-31T12:09:21.496207: step 15028, loss 0.627834.
Train: 2018-07-31T12:09:21.668071: step 15029, loss 0.592204.
Train: 2018-07-31T12:09:21.855521: step 15030, loss 0.618696.
Test: 2018-07-31T12:09:22.339789: step 15030, loss 0.559381.
Train: 2018-07-31T12:09:22.527215: step 15031, loss 0.600797.
Train: 2018-07-31T12:09:22.699050: step 15032, loss 0.591788.
Train: 2018-07-31T12:09:22.870885: step 15033, loss 0.591639.
Train: 2018-07-31T12:09:23.058340: step 15034, loss 0.547474.
Train: 2018-07-31T12:09:23.230207: step 15035, loss 0.644081.
Train: 2018-07-31T12:09:23.417632: step 15036, loss 0.626249.
Train: 2018-07-31T12:09:23.589466: step 15037, loss 0.556125.
Train: 2018-07-31T12:09:23.761332: step 15038, loss 0.582193.
Train: 2018-07-31T12:09:23.933135: step 15039, loss 0.651482.
Train: 2018-07-31T12:09:24.120592: step 15040, loss 0.607878.
Test: 2018-07-31T12:09:24.589232: step 15040, loss 0.558847.
Train: 2018-07-31T12:09:24.776719: step 15041, loss 0.538851.
Train: 2018-07-31T12:09:24.948553: step 15042, loss 0.57318.
Train: 2018-07-31T12:09:25.120388: step 15043, loss 0.564573.
Train: 2018-07-31T12:09:25.292193: step 15044, loss 0.59862.
Train: 2018-07-31T12:09:25.479680: step 15045, loss 0.530513.
Train: 2018-07-31T12:09:25.651513: step 15046, loss 0.700187.
Train: 2018-07-31T12:09:25.823348: step 15047, loss 0.5053.
Train: 2018-07-31T12:09:26.010800: step 15048, loss 0.623437.
Train: 2018-07-31T12:09:26.182638: step 15049, loss 0.539177.
Train: 2018-07-31T12:09:26.370066: step 15050, loss 0.539195.
Test: 2018-07-31T12:09:26.838705: step 15050, loss 0.558673.
Train: 2018-07-31T12:09:27.026191: step 15051, loss 0.555946.
Train: 2018-07-31T12:09:27.198034: step 15052, loss 0.606192.
Train: 2018-07-31T12:09:27.385453: step 15053, loss 0.505634.
Train: 2018-07-31T12:09:27.557287: step 15054, loss 0.580951.
Train: 2018-07-31T12:09:27.729122: step 15055, loss 0.606055.
Train: 2018-07-31T12:09:27.900957: step 15056, loss 0.580861.
Train: 2018-07-31T12:09:28.072816: step 15057, loss 0.555676.
Train: 2018-07-31T12:09:28.260272: step 15058, loss 0.555627.
Train: 2018-07-31T12:09:28.432112: step 15059, loss 0.505239.
Train: 2018-07-31T12:09:28.619563: step 15060, loss 0.572568.
Test: 2018-07-31T12:09:29.088179: step 15060, loss 0.558164.
Train: 2018-07-31T12:09:29.275635: step 15061, loss 0.513316.
Train: 2018-07-31T12:09:29.447499: step 15062, loss 0.504692.
Train: 2018-07-31T12:09:29.619305: step 15063, loss 0.572279.
Train: 2018-07-31T12:09:29.806761: step 15064, loss 0.555284.
Train: 2018-07-31T12:09:29.978596: step 15065, loss 0.495457.
Train: 2018-07-31T12:09:30.150460: step 15066, loss 0.641044.
Train: 2018-07-31T12:09:30.322265: step 15067, loss 0.477714.
Train: 2018-07-31T12:09:30.494100: step 15068, loss 0.615671.
Train: 2018-07-31T12:09:30.681556: step 15069, loss 0.546437.
Train: 2018-07-31T12:09:30.853422: step 15070, loss 0.624676.
Test: 2018-07-31T12:09:31.322060: step 15070, loss 0.557866.
Train: 2018-07-31T12:09:31.493895: step 15071, loss 0.650925.
Train: 2018-07-31T12:09:31.681352: step 15072, loss 0.607324.
Train: 2018-07-31T12:09:31.853188: step 15073, loss 0.528843.
Train: 2018-07-31T12:09:32.024991: step 15074, loss 0.546229.
Train: 2018-07-31T12:09:32.196856: step 15075, loss 0.520004.
Train: 2018-07-31T12:09:32.368660: step 15076, loss 0.58103.
Train: 2018-07-31T12:09:32.540496: step 15077, loss 0.624712.
Train: 2018-07-31T12:09:32.727983: step 15078, loss 0.554702.
Train: 2018-07-31T12:09:32.899785: step 15079, loss 0.598368.
Train: 2018-07-31T12:09:33.071650: step 15080, loss 0.502138.
Test: 2018-07-31T12:09:33.555882: step 15080, loss 0.557365.
Train: 2018-07-31T12:09:33.727717: step 15081, loss 0.598264.
Train: 2018-07-31T12:09:33.899552: step 15082, loss 0.563211.
Train: 2018-07-31T12:09:34.071386: step 15083, loss 0.510637.
Train: 2018-07-31T12:09:34.243252: step 15084, loss 0.519278.
Train: 2018-07-31T12:09:34.430678: step 15085, loss 0.589381.
Train: 2018-07-31T12:09:34.602544: step 15086, loss 0.492642.
Train: 2018-07-31T12:09:34.774348: step 15087, loss 0.633454.
Train: 2018-07-31T12:09:34.946212: step 15088, loss 0.615834.
Train: 2018-07-31T12:09:35.118047: step 15089, loss 0.589321.
Train: 2018-07-31T12:09:35.305503: step 15090, loss 0.545165.
Test: 2018-07-31T12:09:35.789765: step 15090, loss 0.556819.
Train: 2018-07-31T12:09:35.961598: step 15091, loss 0.580415.
Train: 2018-07-31T12:09:36.133434: step 15092, loss 0.545099.
Train: 2018-07-31T12:09:36.320889: step 15093, loss 0.606808.
Train: 2018-07-31T12:09:36.492695: step 15094, loss 0.456898.
Train: 2018-07-31T12:09:36.664528: step 15095, loss 0.615603.
Train: 2018-07-31T12:09:36.851985: step 15096, loss 0.589102.
Train: 2018-07-31T12:09:37.023821: step 15097, loss 0.624384.
Train: 2018-07-31T12:09:37.195655: step 15098, loss 0.615468.
Train: 2018-07-31T12:09:37.383145: step 15099, loss 0.632943.
Train: 2018-07-31T12:09:37.539324: step 15100, loss 0.646171.
Test: 2018-07-31T12:09:38.007963: step 15100, loss 0.556621.
