Train: 2018-08-02T10:40:27.236016: step 1, loss 0.62481.
Train: 2018-08-02T10:40:27.501573: step 2, loss 2.908.
Train: 2018-08-02T10:40:27.720272: step 3, loss 0.959565.
Train: 2018-08-02T10:40:27.970220: step 4, loss 0.711848.
Train: 2018-08-02T10:40:28.220161: step 5, loss 0.61323.
Train: 2018-08-02T10:40:28.470071: step 6, loss 0.569846.
Train: 2018-08-02T10:40:28.720014: step 7, loss 0.574326.
Train: 2018-08-02T10:40:28.969955: step 8, loss 0.622851.
Train: 2018-08-02T10:40:29.219933: step 9, loss 0.697829.
Train: 2018-08-02T10:40:29.454217: step 10, loss 0.475863.
Test: 2018-08-02T10:40:30.922647: step 10, loss 0.595443.
Train: 2018-08-02T10:40:31.188215: step 11, loss 0.670854.
Train: 2018-08-02T10:40:31.438157: step 12, loss 0.563154.
Train: 2018-08-02T10:40:31.688099: step 13, loss 0.617864.
Train: 2018-08-02T10:40:31.938040: step 14, loss 0.558975.
Train: 2018-08-02T10:40:32.187981: step 15, loss 0.554433.
Train: 2018-08-02T10:40:32.422274: step 16, loss 0.569463.
Train: 2018-08-02T10:40:32.672245: step 17, loss 0.597372.
Train: 2018-08-02T10:40:32.906534: step 18, loss 0.610006.
Train: 2018-08-02T10:40:33.156505: step 19, loss 0.508503.
Train: 2018-08-02T10:40:33.406416: step 20, loss 0.612226.
Test: 2018-08-02T10:40:34.562395: step 20, loss 0.661315.
Train: 2018-08-02T10:40:34.796746: step 21, loss 0.598881.
Train: 2018-08-02T10:40:35.031067: step 22, loss 0.583748.
Train: 2018-08-02T10:40:35.265388: step 23, loss 0.628903.
Train: 2018-08-02T10:40:35.515322: step 24, loss 0.585448.
Train: 2018-08-02T10:40:35.749649: step 25, loss 0.592544.
Train: 2018-08-02T10:40:35.999590: step 26, loss 0.565043.
Train: 2018-08-02T10:40:36.233916: step 27, loss 0.580811.
Train: 2018-08-02T10:40:36.468229: step 28, loss 0.60672.
Train: 2018-08-02T10:40:36.718170: step 29, loss 0.553534.
Train: 2018-08-02T10:40:36.952461: step 30, loss 0.634008.
Test: 2018-08-02T10:40:38.108440: step 30, loss 0.641193.
Train: 2018-08-02T10:40:38.342785: step 31, loss 0.590232.
Train: 2018-08-02T10:40:38.577081: step 32, loss 0.60014.
Train: 2018-08-02T10:40:38.827023: step 33, loss 0.547107.
Train: 2018-08-02T10:40:39.061352: step 34, loss 0.69212.
Train: 2018-08-02T10:40:39.295693: step 35, loss 0.601591.
Train: 2018-08-02T10:40:39.530013: step 36, loss 0.567633.
Train: 2018-08-02T10:40:39.764334: step 37, loss 0.61806.
Train: 2018-08-02T10:40:40.014243: step 38, loss 0.537469.
Train: 2018-08-02T10:40:40.232943: step 39, loss 0.578305.
Train: 2018-08-02T10:40:40.482884: step 40, loss 0.561275.
Test: 2018-08-02T10:40:41.623242: step 40, loss 0.611157.
Train: 2018-08-02T10:40:41.857593: step 41, loss 0.5348.
Train: 2018-08-02T10:40:42.091932: step 42, loss 0.569984.
Train: 2018-08-02T10:40:42.326204: step 43, loss 0.5759.
Train: 2018-08-02T10:40:42.560526: step 44, loss 0.550432.
Train: 2018-08-02T10:40:42.810494: step 45, loss 0.499307.
Train: 2018-08-02T10:40:43.060436: step 46, loss 0.553833.
Train: 2018-08-02T10:40:43.310347: step 47, loss 0.529545.
Train: 2018-08-02T10:40:43.560288: step 48, loss 0.489053.
Train: 2018-08-02T10:40:43.794640: step 49, loss 0.629622.
Train: 2018-08-02T10:40:44.044550: step 50, loss 0.568932.
Test: 2018-08-02T10:40:45.184908: step 50, loss 0.61052.
Train: 2018-08-02T10:40:45.419229: step 51, loss 0.53698.
Train: 2018-08-02T10:40:45.669210: step 52, loss 0.549204.
Train: 2018-08-02T10:40:45.903521: step 53, loss 0.543529.
Train: 2018-08-02T10:40:46.122218: step 54, loss 0.588527.
Train: 2018-08-02T10:40:46.387787: step 55, loss 0.633585.
Train: 2018-08-02T10:40:46.637724: step 56, loss 0.498931.
Train: 2018-08-02T10:40:46.887666: step 57, loss 0.609207.
Train: 2018-08-02T10:40:47.121985: step 58, loss 0.531729.
Train: 2018-08-02T10:40:47.387519: step 59, loss 0.593553.
Train: 2018-08-02T10:40:47.637490: step 60, loss 0.513929.
Test: 2018-08-02T10:40:48.793438: step 60, loss 0.588602.
Train: 2018-08-02T10:40:49.027759: step 61, loss 0.591117.
Train: 2018-08-02T10:40:49.277700: step 62, loss 0.60046.
Train: 2018-08-02T10:40:49.527673: step 63, loss 0.552621.
Train: 2018-08-02T10:40:49.793212: step 64, loss 0.575893.
Train: 2018-08-02T10:40:50.043176: step 65, loss 0.565848.
Train: 2018-08-02T10:40:50.293117: step 66, loss 0.56544.
Train: 2018-08-02T10:40:50.543059: step 67, loss 0.571448.
Train: 2018-08-02T10:40:50.792995: step 68, loss 0.616124.
Train: 2018-08-02T10:40:51.042936: step 69, loss 0.587258.
Train: 2018-08-02T10:40:51.292884: step 70, loss 0.543096.
Test: 2018-08-02T10:40:52.480075: step 70, loss 0.581818.
Train: 2018-08-02T10:40:52.714396: step 71, loss 0.62595.
Train: 2018-08-02T10:40:52.964360: step 72, loss 0.554557.
Train: 2018-08-02T10:40:53.183065: step 73, loss 0.55352.
Train: 2018-08-02T10:40:53.433001: step 74, loss 0.602427.
Train: 2018-08-02T10:40:53.682948: step 75, loss 0.634986.
Train: 2018-08-02T10:40:53.932889: step 76, loss 0.620067.
Train: 2018-08-02T10:40:54.167186: step 77, loss 0.601826.
Train: 2018-08-02T10:40:54.417151: step 78, loss 0.516841.
Train: 2018-08-02T10:40:54.667092: step 79, loss 0.556683.
Train: 2018-08-02T10:40:54.917035: step 80, loss 0.541868.
Test: 2018-08-02T10:40:56.104227: step 80, loss 0.568205.
Train: 2018-08-02T10:40:56.354198: step 81, loss 0.611812.
Train: 2018-08-02T10:40:56.604109: step 82, loss 0.597286.
Train: 2018-08-02T10:40:56.838460: step 83, loss 0.604719.
Train: 2018-08-02T10:40:57.088404: step 84, loss 0.569157.
Train: 2018-08-02T10:40:57.338321: step 85, loss 0.567318.
Train: 2018-08-02T10:40:57.588284: step 86, loss 0.540408.
Train: 2018-08-02T10:40:57.838225: step 87, loss 0.594033.
Train: 2018-08-02T10:40:58.088162: step 88, loss 0.578317.
Train: 2018-08-02T10:40:58.338079: step 89, loss 0.542477.
Train: 2018-08-02T10:40:58.572424: step 90, loss 0.558145.
Test: 2018-08-02T10:40:59.728377: step 90, loss 0.556308.
Train: 2018-08-02T10:40:59.962722: step 91, loss 0.620239.
Train: 2018-08-02T10:41:00.197044: step 92, loss 0.535636.
Train: 2018-08-02T10:41:00.446992: step 93, loss 0.607834.
Train: 2018-08-02T10:41:00.696927: step 94, loss 0.673132.
Train: 2018-08-02T10:41:00.946842: step 95, loss 0.529399.
Train: 2018-08-02T10:41:01.196817: step 96, loss 0.565546.
Train: 2018-08-02T10:41:01.462373: step 97, loss 0.580364.
Train: 2018-08-02T10:41:01.727910: step 98, loss 0.566594.
Train: 2018-08-02T10:41:01.977851: step 99, loss 0.60201.
Train: 2018-08-02T10:41:02.227801: step 100, loss 0.564867.
Test: 2018-08-02T10:41:03.399393: step 100, loss 0.552846.
Train: 2018-08-02T10:41:04.305456: step 101, loss 0.582165.
Train: 2018-08-02T10:41:04.539781: step 102, loss 0.499635.
Train: 2018-08-02T10:41:04.789722: step 103, loss 0.577368.
Train: 2018-08-02T10:41:05.039665: step 104, loss 0.56363.
Train: 2018-08-02T10:41:05.289608: step 105, loss 0.552126.
Train: 2018-08-02T10:41:05.523925: step 106, loss 0.562249.
Train: 2018-08-02T10:41:05.758247: step 107, loss 0.557775.
Train: 2018-08-02T10:41:06.008181: step 108, loss 0.553527.
Train: 2018-08-02T10:41:06.258131: step 109, loss 0.62391.
Train: 2018-08-02T10:41:06.508071: step 110, loss 0.575164.
Test: 2018-08-02T10:41:07.664019: step 110, loss 0.549534.
Train: 2018-08-02T10:41:07.898339: step 111, loss 0.579092.
Train: 2018-08-02T10:41:08.148281: step 112, loss 0.602783.
Train: 2018-08-02T10:41:08.382602: step 113, loss 0.556642.
Train: 2018-08-02T10:41:08.632566: step 114, loss 0.660895.
Train: 2018-08-02T10:41:08.882508: step 115, loss 0.594824.
Train: 2018-08-02T10:41:09.116834: step 116, loss 0.563041.
Train: 2018-08-02T10:41:09.351150: step 117, loss 0.608098.
Train: 2018-08-02T10:41:09.585446: step 118, loss 0.586717.
Train: 2018-08-02T10:41:09.835416: step 119, loss 0.634745.
Train: 2018-08-02T10:41:10.069705: step 120, loss 0.546354.
Test: 2018-08-02T10:41:11.241307: step 120, loss 0.5537.
Train: 2018-08-02T10:41:11.491272: step 121, loss 0.575358.
Train: 2018-08-02T10:41:11.725598: step 122, loss 0.619187.
Train: 2018-08-02T10:41:11.975535: step 123, loss 0.514449.
Train: 2018-08-02T10:41:12.209829: step 124, loss 0.579139.
Train: 2018-08-02T10:41:12.459797: step 125, loss 0.564887.
Train: 2018-08-02T10:41:12.709715: step 126, loss 0.518852.
Train: 2018-08-02T10:41:12.975276: step 127, loss 0.626617.
Train: 2018-08-02T10:41:13.225248: step 128, loss 0.516216.
Train: 2018-08-02T10:41:13.443945: step 129, loss 0.507221.
Train: 2018-08-02T10:41:13.689749: step 130, loss 0.548444.
Test: 2018-08-02T10:41:14.861348: step 130, loss 0.550658.
Train: 2018-08-02T10:41:15.080077: step 131, loss 0.512805.
Train: 2018-08-02T10:41:15.329988: step 132, loss 0.620317.
Train: 2018-08-02T10:41:15.564338: step 133, loss 0.571466.
Train: 2018-08-02T10:41:15.814280: step 134, loss 0.55996.
Train: 2018-08-02T10:41:16.048604: step 135, loss 0.595856.
Train: 2018-08-02T10:41:16.298545: step 136, loss 0.555416.
Train: 2018-08-02T10:41:16.548454: step 137, loss 0.55133.
Train: 2018-08-02T10:41:16.782775: step 138, loss 0.610196.
Train: 2018-08-02T10:41:17.017125: step 139, loss 0.543773.
Train: 2018-08-02T10:41:17.267035: step 140, loss 0.605202.
Test: 2018-08-02T10:41:18.423015: step 140, loss 0.548284.
Train: 2018-08-02T10:41:18.657365: step 141, loss 0.601223.
Train: 2018-08-02T10:41:18.891655: step 142, loss 0.55444.
Train: 2018-08-02T10:41:19.110385: step 143, loss 0.534936.
Train: 2018-08-02T10:41:19.360296: step 144, loss 0.539689.
Train: 2018-08-02T10:41:19.594646: step 145, loss 0.521427.
Train: 2018-08-02T10:41:19.828966: step 146, loss 0.559294.
Train: 2018-08-02T10:41:20.063255: step 147, loss 0.536992.
Train: 2018-08-02T10:41:20.313227: step 148, loss 0.513279.
Train: 2018-08-02T10:41:20.563168: step 149, loss 0.583734.
Train: 2018-08-02T10:41:20.797460: step 150, loss 0.582454.
Test: 2018-08-02T10:41:21.969059: step 150, loss 0.548376.
Train: 2018-08-02T10:41:22.375244: step 151, loss 0.465428.
Train: 2018-08-02T10:41:22.625185: step 152, loss 0.565078.
Train: 2018-08-02T10:41:22.859506: step 153, loss 0.632669.
Train: 2018-08-02T10:41:23.109447: step 154, loss 0.551583.
Train: 2018-08-02T10:41:23.343737: step 155, loss 0.556769.
Train: 2018-08-02T10:41:23.578088: step 156, loss 0.589721.
Train: 2018-08-02T10:41:23.828024: step 157, loss 0.561023.
Train: 2018-08-02T10:41:24.062349: step 158, loss 0.538018.
Train: 2018-08-02T10:41:24.312291: step 159, loss 0.528458.
Train: 2018-08-02T10:41:24.562232: step 160, loss 0.584298.
Test: 2018-08-02T10:41:25.718181: step 160, loss 0.549116.
Train: 2018-08-02T10:41:25.952527: step 161, loss 0.601203.
Train: 2018-08-02T10:41:26.186822: step 162, loss 0.545492.
Train: 2018-08-02T10:41:26.436764: step 163, loss 0.585379.
Train: 2018-08-02T10:41:26.686735: step 164, loss 0.566817.
Train: 2018-08-02T10:41:26.936676: step 165, loss 0.605429.
Train: 2018-08-02T10:41:27.170969: step 166, loss 0.630121.
Train: 2018-08-02T10:41:27.420908: step 167, loss 0.603191.
Train: 2018-08-02T10:41:27.670882: step 168, loss 0.612308.
Train: 2018-08-02T10:41:27.920823: step 169, loss 0.619286.
Train: 2018-08-02T10:41:28.155159: step 170, loss 0.629828.
Test: 2018-08-02T10:41:29.311089: step 170, loss 0.554194.
Train: 2018-08-02T10:41:29.545411: step 171, loss 0.567458.
Train: 2018-08-02T10:41:29.779730: step 172, loss 0.59167.
Train: 2018-08-02T10:41:30.029672: step 173, loss 0.584471.
Train: 2018-08-02T10:41:30.279643: step 174, loss 0.597231.
Train: 2018-08-02T10:41:30.529555: step 175, loss 0.555565.
Train: 2018-08-02T10:41:30.779497: step 176, loss 0.584813.
Train: 2018-08-02T10:41:31.029439: step 177, loss 0.557052.
Train: 2018-08-02T10:41:31.279410: step 178, loss 0.552984.
Train: 2018-08-02T10:41:31.513711: step 179, loss 0.572126.
Train: 2018-08-02T10:41:31.763672: step 180, loss 0.516998.
Test: 2018-08-02T10:41:32.997727: step 180, loss 0.556076.
Train: 2018-08-02T10:41:33.247669: step 181, loss 0.580679.
Train: 2018-08-02T10:41:33.497640: step 182, loss 0.55529.
Train: 2018-08-02T10:41:33.747582: step 183, loss 0.54839.
Train: 2018-08-02T10:41:33.997524: step 184, loss 0.624274.
Train: 2018-08-02T10:41:34.231815: step 185, loss 0.486693.
Train: 2018-08-02T10:41:34.481760: step 186, loss 0.516419.
Train: 2018-08-02T10:41:34.716106: step 187, loss 0.527656.
Train: 2018-08-02T10:41:34.950395: step 188, loss 0.481264.
Train: 2018-08-02T10:41:35.184746: step 189, loss 0.624855.
Train: 2018-08-02T10:41:35.434657: step 190, loss 0.545322.
Test: 2018-08-02T10:41:36.590635: step 190, loss 0.548756.
Train: 2018-08-02T10:41:36.809364: step 191, loss 0.585103.
Train: 2018-08-02T10:41:37.043688: step 192, loss 0.520749.
Train: 2018-08-02T10:41:37.309247: step 193, loss 0.583274.
Train: 2018-08-02T10:41:37.543539: step 194, loss 0.512894.
Train: 2018-08-02T10:41:37.793511: step 195, loss 0.542122.
Train: 2018-08-02T10:41:38.043453: step 196, loss 0.568438.
Train: 2018-08-02T10:41:38.293386: step 197, loss 0.631009.
Train: 2018-08-02T10:41:38.527682: step 198, loss 0.642004.
Train: 2018-08-02T10:41:38.777653: step 199, loss 0.579253.
Train: 2018-08-02T10:41:39.011977: step 200, loss 0.544889.
Test: 2018-08-02T10:41:40.167922: step 200, loss 0.547013.
Train: 2018-08-02T10:41:40.995854: step 201, loss 0.571471.
Train: 2018-08-02T10:41:41.230174: step 202, loss 0.56598.
Train: 2018-08-02T10:41:41.480117: step 203, loss 0.626165.
Train: 2018-08-02T10:41:41.730087: step 204, loss 0.626348.
Train: 2018-08-02T10:41:41.980024: step 205, loss 0.595141.
Train: 2018-08-02T10:41:42.214348: step 206, loss 0.533313.
Train: 2018-08-02T10:41:42.464261: step 207, loss 0.584684.
Train: 2018-08-02T10:41:42.682989: step 208, loss 0.520267.
Train: 2018-08-02T10:41:42.932901: step 209, loss 0.573992.
Train: 2018-08-02T10:41:43.151598: step 210, loss 0.56346.
Test: 2018-08-02T10:41:44.307578: step 210, loss 0.550067.
Train: 2018-08-02T10:41:44.541931: step 211, loss 0.557614.
Train: 2018-08-02T10:41:44.791841: step 212, loss 0.542031.
Train: 2018-08-02T10:41:45.026194: step 213, loss 0.545779.
Train: 2018-08-02T10:41:45.276131: step 214, loss 0.514649.
Train: 2018-08-02T10:41:45.510422: step 215, loss 0.541127.
Train: 2018-08-02T10:41:45.760393: step 216, loss 0.548483.
Train: 2018-08-02T10:41:46.010335: step 217, loss 0.610642.
Train: 2018-08-02T10:41:46.244654: step 218, loss 0.573477.
Train: 2018-08-02T10:41:46.494565: step 219, loss 0.534971.
Train: 2018-08-02T10:41:46.744508: step 220, loss 0.500977.
Test: 2018-08-02T10:41:47.884866: step 220, loss 0.548758.
Train: 2018-08-02T10:41:48.134807: step 221, loss 0.557763.
Train: 2018-08-02T10:41:48.384773: step 222, loss 0.543754.
Train: 2018-08-02T10:41:48.619071: step 223, loss 0.506024.
Train: 2018-08-02T10:41:48.853420: step 224, loss 0.55903.
Train: 2018-08-02T10:41:49.103331: step 225, loss 0.642207.
Train: 2018-08-02T10:41:49.353302: step 226, loss 0.53363.
Train: 2018-08-02T10:41:49.603243: step 227, loss 0.570963.
Train: 2018-08-02T10:41:49.853185: step 228, loss 0.5424.
Train: 2018-08-02T10:41:50.087506: step 229, loss 0.602448.
Train: 2018-08-02T10:41:50.337446: step 230, loss 0.594402.
Test: 2018-08-02T10:41:51.477774: step 230, loss 0.548282.
Train: 2018-08-02T10:41:51.696472: step 231, loss 0.60746.
Train: 2018-08-02T10:41:51.930823: step 232, loss 0.603523.
Train: 2018-08-02T10:41:52.180764: step 233, loss 0.540057.
Train: 2018-08-02T10:41:52.415088: step 234, loss 0.637891.
Train: 2018-08-02T10:41:52.633753: step 235, loss 0.566864.
Train: 2018-08-02T10:41:52.868074: step 236, loss 0.574515.
Train: 2018-08-02T10:41:53.118046: step 237, loss 0.599856.
Train: 2018-08-02T10:41:53.367987: step 238, loss 0.513098.
Train: 2018-08-02T10:41:53.617898: step 239, loss 0.558826.
Train: 2018-08-02T10:41:53.867839: step 240, loss 0.544746.
Test: 2018-08-02T10:41:55.023818: step 240, loss 0.549825.
Train: 2018-08-02T10:41:55.273761: step 241, loss 0.568851.
Train: 2018-08-02T10:41:55.523701: step 242, loss 0.61833.
Train: 2018-08-02T10:41:55.773673: step 243, loss 0.61814.
Train: 2018-08-02T10:41:56.007988: step 244, loss 0.538934.
Train: 2018-08-02T10:41:56.242296: step 245, loss 0.541913.
Train: 2018-08-02T10:41:56.476636: step 246, loss 0.488169.
Train: 2018-08-02T10:41:56.726546: step 247, loss 0.521327.
Train: 2018-08-02T10:41:56.976511: step 248, loss 0.563015.
Train: 2018-08-02T10:41:57.226454: step 249, loss 0.578068.
Train: 2018-08-02T10:41:57.476399: step 250, loss 0.59782.
Test: 2018-08-02T10:41:58.632348: step 250, loss 0.548367.
Train: 2018-08-02T10:41:58.866673: step 251, loss 0.599466.
Train: 2018-08-02T10:41:59.101014: step 252, loss 0.503683.
Train: 2018-08-02T10:41:59.335341: step 253, loss 0.528886.
Train: 2018-08-02T10:41:59.585251: step 254, loss 0.527106.
Train: 2018-08-02T10:41:59.835222: step 255, loss 0.545202.
Train: 2018-08-02T10:42:00.085158: step 256, loss 0.591454.
Train: 2018-08-02T10:42:00.335106: step 257, loss 0.522925.
Train: 2018-08-02T10:42:00.569396: step 258, loss 0.524572.
Train: 2018-08-02T10:42:00.803747: step 259, loss 0.591399.
Train: 2018-08-02T10:42:01.053657: step 260, loss 0.51328.
Test: 2018-08-02T10:42:02.209636: step 260, loss 0.546796.
Train: 2018-08-02T10:42:02.428334: step 261, loss 0.568271.
Train: 2018-08-02T10:42:02.662689: step 262, loss 0.632231.
Train: 2018-08-02T10:42:02.912596: step 263, loss 0.539013.
Train: 2018-08-02T10:42:03.162571: step 264, loss 0.582576.
Train: 2018-08-02T10:42:03.412512: step 265, loss 0.575199.
Train: 2018-08-02T10:42:03.646799: step 266, loss 0.466779.
Train: 2018-08-02T10:42:03.896772: step 267, loss 0.601103.
Train: 2018-08-02T10:42:04.146682: step 268, loss 0.578686.
Train: 2018-08-02T10:42:04.381003: step 269, loss 0.538986.
Train: 2018-08-02T10:42:04.630974: step 270, loss 0.503537.
Test: 2018-08-02T10:42:05.786922: step 270, loss 0.549111.
Train: 2018-08-02T10:42:06.005621: step 271, loss 0.563067.
Train: 2018-08-02T10:42:06.255593: step 272, loss 0.623888.
Train: 2018-08-02T10:42:06.505535: step 273, loss 0.607348.
Train: 2018-08-02T10:42:06.739855: step 274, loss 0.615463.
Train: 2018-08-02T10:42:06.989796: step 275, loss 0.505105.
Train: 2018-08-02T10:42:07.239709: step 276, loss 0.540292.
Train: 2018-08-02T10:42:07.474034: step 277, loss 0.571111.
Train: 2018-08-02T10:42:07.708352: step 278, loss 0.555466.
Train: 2018-08-02T10:42:07.958319: step 279, loss 0.532092.
Train: 2018-08-02T10:42:08.192611: step 280, loss 0.552583.
Test: 2018-08-02T10:42:09.348589: step 280, loss 0.548575.
Train: 2018-08-02T10:42:09.567287: step 281, loss 0.557049.
Train: 2018-08-02T10:42:09.817230: step 282, loss 0.559235.
Train: 2018-08-02T10:42:10.067172: step 283, loss 0.542803.
Train: 2018-08-02T10:42:10.317113: step 284, loss 0.586976.
Train: 2018-08-02T10:42:10.551463: step 285, loss 0.519894.
Train: 2018-08-02T10:42:10.801410: step 286, loss 0.582204.
Train: 2018-08-02T10:42:11.051346: step 287, loss 0.523821.
Train: 2018-08-02T10:42:11.301287: step 288, loss 0.511449.
Train: 2018-08-02T10:42:11.535610: step 289, loss 0.586193.
Train: 2018-08-02T10:42:11.785520: step 290, loss 0.601694.
Test: 2018-08-02T10:42:12.941498: step 290, loss 0.549058.
Train: 2018-08-02T10:42:13.160229: step 291, loss 0.563239.
Train: 2018-08-02T10:42:13.410139: step 292, loss 0.562687.
Train: 2018-08-02T10:42:13.660107: step 293, loss 0.56293.
Train: 2018-08-02T10:42:13.894432: step 294, loss 0.518622.
Train: 2018-08-02T10:42:14.128720: step 295, loss 0.546937.
Train: 2018-08-02T10:42:14.363041: step 296, loss 0.522598.
Train: 2018-08-02T10:42:14.597376: step 297, loss 0.589647.
Train: 2018-08-02T10:42:14.831710: step 298, loss 0.438862.
Train: 2018-08-02T10:42:15.066001: step 299, loss 0.560611.
Train: 2018-08-02T10:42:15.315941: step 300, loss 0.571953.
Test: 2018-08-02T10:42:16.456299: step 300, loss 0.549474.
Train: 2018-08-02T10:42:17.362338: step 301, loss 0.482326.
Train: 2018-08-02T10:42:17.565439: step 302, loss 0.580387.
Train: 2018-08-02T10:42:17.815388: step 303, loss 0.501067.
Train: 2018-08-02T10:42:18.049711: step 304, loss 0.578902.
Train: 2018-08-02T10:42:18.284030: step 305, loss 0.574181.
Train: 2018-08-02T10:42:18.533971: step 306, loss 0.54537.
Train: 2018-08-02T10:42:18.830774: step 307, loss 0.659014.
Train: 2018-08-02T10:42:19.080710: step 308, loss 0.578082.
Train: 2018-08-02T10:42:19.330626: step 309, loss 0.544023.
Train: 2018-08-02T10:42:19.549355: step 310, loss 0.653813.
Test: 2018-08-02T10:42:20.705304: step 310, loss 0.547634.
Train: 2018-08-02T10:42:21.002140: step 311, loss 0.522807.
Train: 2018-08-02T10:42:21.252054: step 312, loss 0.620931.
Train: 2018-08-02T10:42:21.486402: step 313, loss 0.574877.
Train: 2018-08-02T10:42:21.720693: step 314, loss 0.575154.
Train: 2018-08-02T10:42:21.955014: step 315, loss 0.658046.
Train: 2018-08-02T10:42:22.204978: step 316, loss 0.495246.
Train: 2018-08-02T10:42:22.439275: step 317, loss 0.527598.
Train: 2018-08-02T10:42:22.689215: step 318, loss 0.567358.
Train: 2018-08-02T10:42:22.923569: step 319, loss 0.555269.
Train: 2018-08-02T10:42:23.157856: step 320, loss 0.523873.
Test: 2018-08-02T10:42:24.313834: step 320, loss 0.55012.
Train: 2018-08-02T10:42:24.563776: step 321, loss 0.549487.
Train: 2018-08-02T10:42:24.813719: step 322, loss 0.557065.
Train: 2018-08-02T10:42:25.063689: step 323, loss 0.536023.
Train: 2018-08-02T10:42:25.282382: step 324, loss 0.613818.
Train: 2018-08-02T10:42:25.516679: step 325, loss 0.597242.
Train: 2018-08-02T10:42:25.766650: step 326, loss 0.50878.
Train: 2018-08-02T10:42:26.000970: step 327, loss 0.531651.
Train: 2018-08-02T10:42:26.219663: step 328, loss 0.603327.
Train: 2018-08-02T10:42:26.469616: step 329, loss 0.576189.
Train: 2018-08-02T10:42:26.719531: step 330, loss 0.583854.
Test: 2018-08-02T10:42:27.875501: step 330, loss 0.549919.
Train: 2018-08-02T10:42:28.109852: step 331, loss 0.614207.
Train: 2018-08-02T10:42:28.359800: step 332, loss 0.552121.
Train: 2018-08-02T10:42:28.594083: step 333, loss 0.54574.
Train: 2018-08-02T10:42:28.812781: step 334, loss 0.540362.
Train: 2018-08-02T10:42:29.062752: step 335, loss 0.525719.
Train: 2018-08-02T10:42:29.281421: step 336, loss 0.53097.
Train: 2018-08-02T10:42:29.531395: step 337, loss 0.605735.
Train: 2018-08-02T10:42:29.781344: step 338, loss 0.617537.
Train: 2018-08-02T10:42:30.015625: step 339, loss 0.587996.
Train: 2018-08-02T10:42:30.249975: step 340, loss 0.579927.
Test: 2018-08-02T10:42:31.405923: step 340, loss 0.549535.
Train: 2018-08-02T10:42:31.640273: step 341, loss 0.620763.
Train: 2018-08-02T10:42:31.874595: step 342, loss 0.607121.
Train: 2018-08-02T10:42:32.124506: step 343, loss 0.546538.
Train: 2018-08-02T10:42:32.343234: step 344, loss 0.569953.
Train: 2018-08-02T10:42:32.577555: step 345, loss 0.553545.
Train: 2018-08-02T10:42:32.811857: step 346, loss 0.485725.
Train: 2018-08-02T10:42:33.046197: step 347, loss 0.542549.
Train: 2018-08-02T10:42:33.296105: step 348, loss 0.533914.
Train: 2018-08-02T10:42:33.530460: step 349, loss 0.524081.
Train: 2018-08-02T10:42:33.764777: step 350, loss 0.626242.
Test: 2018-08-02T10:42:34.920725: step 350, loss 0.549785.
Train: 2018-08-02T10:42:35.155047: step 351, loss 0.504383.
Train: 2018-08-02T10:42:35.389397: step 352, loss 0.561408.
Train: 2018-08-02T10:42:35.623717: step 353, loss 0.491828.
Train: 2018-08-02T10:42:35.873627: step 354, loss 0.579501.
Train: 2018-08-02T10:42:36.107978: step 355, loss 0.568799.
Train: 2018-08-02T10:42:36.357921: step 356, loss 0.505403.
Train: 2018-08-02T10:42:36.592209: step 357, loss 0.541036.
Train: 2018-08-02T10:42:36.826562: step 358, loss 0.520338.
Train: 2018-08-02T10:42:37.060852: step 359, loss 0.485021.
Train: 2018-08-02T10:42:37.295170: step 360, loss 0.625965.
Test: 2018-08-02T10:42:38.451149: step 360, loss 0.54845.
Train: 2018-08-02T10:42:38.685505: step 361, loss 0.574319.
Train: 2018-08-02T10:42:38.919790: step 362, loss 0.542827.
Train: 2018-08-02T10:42:39.154112: step 363, loss 0.495528.
Train: 2018-08-02T10:42:39.388429: step 364, loss 0.573014.
Train: 2018-08-02T10:42:39.622749: step 365, loss 0.497522.
Train: 2018-08-02T10:42:39.857072: step 366, loss 0.516754.
Train: 2018-08-02T10:42:40.091422: step 367, loss 0.471778.
Train: 2018-08-02T10:42:40.325710: step 368, loss 0.471038.
Train: 2018-08-02T10:42:40.560061: step 369, loss 0.525803.
Train: 2018-08-02T10:42:40.794351: step 370, loss 0.553331.
Test: 2018-08-02T10:42:41.950329: step 370, loss 0.548264.
Train: 2018-08-02T10:42:42.169052: step 371, loss 0.637488.
Train: 2018-08-02T10:42:42.403386: step 372, loss 0.539718.
Train: 2018-08-02T10:42:42.637700: step 373, loss 0.519414.
Train: 2018-08-02T10:42:42.872020: step 374, loss 0.592413.
Train: 2018-08-02T10:42:43.106333: step 375, loss 0.626537.
Train: 2018-08-02T10:42:43.325037: step 376, loss 0.499869.
Train: 2018-08-02T10:42:43.543737: step 377, loss 0.498314.
Train: 2018-08-02T10:42:43.793678: step 378, loss 0.583011.
Train: 2018-08-02T10:42:44.028001: step 379, loss 0.527949.
Train: 2018-08-02T10:42:44.246697: step 380, loss 0.640735.
Test: 2018-08-02T10:42:45.402646: step 380, loss 0.547549.
Train: 2018-08-02T10:42:45.605748: step 381, loss 0.527649.
Train: 2018-08-02T10:42:45.824421: step 382, loss 0.549917.
Train: 2018-08-02T10:42:46.043120: step 383, loss 0.540088.
Train: 2018-08-02T10:42:46.261849: step 384, loss 0.579346.
Train: 2018-08-02T10:42:46.480549: step 385, loss 0.553749.
Train: 2018-08-02T10:42:46.714871: step 386, loss 0.501323.
Train: 2018-08-02T10:42:46.933567: step 387, loss 0.559646.
Train: 2018-08-02T10:42:47.152237: step 388, loss 0.472111.
Train: 2018-08-02T10:42:47.370936: step 389, loss 0.578898.
Train: 2018-08-02T10:42:47.589635: step 390, loss 0.615949.
Test: 2018-08-02T10:42:48.745613: step 390, loss 0.547197.
Train: 2018-08-02T10:42:48.979934: step 391, loss 0.572948.
Train: 2018-08-02T10:42:49.183040: step 392, loss 0.539784.
Train: 2018-08-02T10:42:49.401735: step 393, loss 0.694524.
Train: 2018-08-02T10:42:49.636057: step 394, loss 0.509524.
Train: 2018-08-02T10:42:49.870350: step 395, loss 0.520821.
Train: 2018-08-02T10:42:50.089079: step 396, loss 0.52925.
Train: 2018-08-02T10:42:50.307777: step 397, loss 0.560348.
Train: 2018-08-02T10:42:50.542098: step 398, loss 0.551034.
Train: 2018-08-02T10:42:50.760797: step 399, loss 0.630899.
Train: 2018-08-02T10:42:50.979465: step 400, loss 0.56736.
Test: 2018-08-02T10:42:52.119823: step 400, loss 0.549446.
Train: 2018-08-02T10:42:52.900923: step 401, loss 0.578469.
Train: 2018-08-02T10:42:53.119618: step 402, loss 0.540567.
Train: 2018-08-02T10:42:53.338318: step 403, loss 0.579272.
Train: 2018-08-02T10:42:53.556987: step 404, loss 0.613421.
Train: 2018-08-02T10:42:53.775718: step 405, loss 0.532385.
Train: 2018-08-02T10:42:53.994414: step 406, loss 0.595526.
Train: 2018-08-02T10:42:54.213082: step 407, loss 0.545679.
Train: 2018-08-02T10:42:54.431812: step 408, loss 0.610134.
Train: 2018-08-02T10:42:54.650481: step 409, loss 0.505839.
Train: 2018-08-02T10:42:54.869209: step 410, loss 0.551507.
Test: 2018-08-02T10:42:56.025158: step 410, loss 0.551211.
Train: 2018-08-02T10:42:56.228266: step 411, loss 0.574474.
Train: 2018-08-02T10:42:56.446965: step 412, loss 0.559697.
Train: 2018-08-02T10:42:56.665664: step 413, loss 0.628719.
Train: 2018-08-02T10:42:56.884333: step 414, loss 0.580026.
Train: 2018-08-02T10:42:57.103064: step 415, loss 0.55837.
Train: 2018-08-02T10:42:57.321760: step 416, loss 0.594454.
Train: 2018-08-02T10:42:57.540459: step 417, loss 0.641749.
Train: 2018-08-02T10:42:57.759160: step 418, loss 0.543361.
Train: 2018-08-02T10:42:57.977860: step 419, loss 0.564443.
Train: 2018-08-02T10:42:58.196556: step 420, loss 0.540831.
Test: 2018-08-02T10:42:59.352504: step 420, loss 0.550743.
Train: 2018-08-02T10:42:59.555582: step 421, loss 0.588369.
Train: 2018-08-02T10:42:59.774314: step 422, loss 0.526904.
Train: 2018-08-02T10:42:59.992979: step 423, loss 0.552571.
Train: 2018-08-02T10:43:00.211679: step 424, loss 0.579104.
Train: 2018-08-02T10:43:00.430407: step 425, loss 0.55623.
Train: 2018-08-02T10:43:00.649104: step 426, loss 0.586933.
Train: 2018-08-02T10:43:00.867805: step 427, loss 0.555902.
Train: 2018-08-02T10:43:01.086473: step 428, loss 0.523606.
Train: 2018-08-02T10:43:01.305202: step 429, loss 0.556163.
Train: 2018-08-02T10:43:01.523904: step 430, loss 0.55645.
Test: 2018-08-02T10:43:02.742336: step 430, loss 0.54881.
Train: 2018-08-02T10:43:02.961064: step 431, loss 0.531594.
Train: 2018-08-02T10:43:03.179763: step 432, loss 0.561809.
Train: 2018-08-02T10:43:03.398457: step 433, loss 0.598749.
Train: 2018-08-02T10:43:03.632752: step 434, loss 0.600308.
Train: 2018-08-02T10:43:03.882694: step 435, loss 0.554874.
Train: 2018-08-02T10:43:04.085802: step 436, loss 0.57816.
Train: 2018-08-02T10:43:04.304471: step 437, loss 0.586642.
Train: 2018-08-02T10:43:04.523202: step 438, loss 0.572875.
Train: 2018-08-02T10:43:04.741893: step 439, loss 0.564015.
Train: 2018-08-02T10:43:04.960597: step 440, loss 0.68358.
Test: 2018-08-02T10:43:06.163410: step 440, loss 0.55059.
Train: 2018-08-02T10:43:06.366488: step 441, loss 0.52977.
Train: 2018-08-02T10:43:06.585186: step 442, loss 0.594812.
Train: 2018-08-02T10:43:06.803914: step 443, loss 0.565021.
Train: 2018-08-02T10:43:07.022583: step 444, loss 0.536911.
Train: 2018-08-02T10:43:07.241312: step 445, loss 0.593632.
Train: 2018-08-02T10:43:07.460006: step 446, loss 0.579431.
Train: 2018-08-02T10:43:07.694301: step 447, loss 0.53193.
Train: 2018-08-02T10:43:07.912999: step 448, loss 0.639596.
Train: 2018-08-02T10:43:08.131698: step 449, loss 0.530814.
Train: 2018-08-02T10:43:08.350428: step 450, loss 0.623157.
Test: 2018-08-02T10:43:09.553241: step 450, loss 0.548917.
Train: 2018-08-02T10:43:09.787561: step 451, loss 0.561958.
Train: 2018-08-02T10:43:10.037503: step 452, loss 0.635087.
Train: 2018-08-02T10:43:10.224960: step 453, loss 0.496058.
Train: 2018-08-02T10:43:10.459281: step 454, loss 0.547317.
Train: 2018-08-02T10:43:10.709221: step 455, loss 0.574203.
Train: 2018-08-02T10:43:11.052890: step 456, loss 0.605247.
Train: 2018-08-02T10:43:11.560321: step 457, loss 0.54006.
Train: 2018-08-02T10:43:11.872380: step 458, loss 0.576193.
Train: 2018-08-02T10:43:12.190713: step 459, loss 0.533724.
Train: 2018-08-02T10:43:12.441092: step 460, loss 0.504631.
Test: 2018-08-02T10:43:13.627030: step 460, loss 0.549314.
Train: 2018-08-02T10:43:13.830801: step 461, loss 0.533598.
Train: 2018-08-02T10:43:14.043894: step 462, loss 0.51639.
Train: 2018-08-02T10:43:14.268419: step 463, loss 0.616072.
Train: 2018-08-02T10:43:14.489618: step 464, loss 0.474971.
Train: 2018-08-02T10:43:14.710076: step 465, loss 0.596523.
Train: 2018-08-02T10:43:14.930135: step 466, loss 0.600564.
Train: 2018-08-02T10:43:15.148960: step 467, loss 0.5623.
Train: 2018-08-02T10:43:15.365766: step 468, loss 0.544853.
Train: 2018-08-02T10:43:15.596551: step 469, loss 0.587354.
Train: 2018-08-02T10:43:15.817271: step 470, loss 0.599242.
Test: 2018-08-02T10:43:17.005859: step 470, loss 0.54792.
Train: 2018-08-02T10:43:17.230830: step 471, loss 0.58891.
Train: 2018-08-02T10:43:17.465690: step 472, loss 0.622675.
Train: 2018-08-02T10:43:17.689850: step 473, loss 0.594232.
Train: 2018-08-02T10:43:17.899082: step 474, loss 0.478752.
Train: 2018-08-02T10:43:18.124075: step 475, loss 0.528643.
Train: 2018-08-02T10:43:18.333039: step 476, loss 0.585608.
Train: 2018-08-02T10:43:18.545676: step 477, loss 0.489355.
Train: 2018-08-02T10:43:18.770901: step 478, loss 0.488044.
Train: 2018-08-02T10:43:18.996977: step 479, loss 0.560633.
Train: 2018-08-02T10:43:19.231605: step 480, loss 0.53416.
Test: 2018-08-02T10:43:20.417954: step 480, loss 0.547618.
Train: 2018-08-02T10:43:20.634717: step 481, loss 0.542484.
Train: 2018-08-02T10:43:20.854321: step 482, loss 0.563622.
Train: 2018-08-02T10:43:21.090082: step 483, loss 0.589427.
Train: 2018-08-02T10:43:21.313381: step 484, loss 0.577959.
Train: 2018-08-02T10:43:21.535031: step 485, loss 0.556251.
Train: 2018-08-02T10:43:21.755581: step 486, loss 0.562997.
Train: 2018-08-02T10:43:21.977162: step 487, loss 0.562993.
Train: 2018-08-02T10:43:22.202121: step 488, loss 0.552135.
Train: 2018-08-02T10:43:22.429080: step 489, loss 0.656049.
Train: 2018-08-02T10:43:22.649980: step 490, loss 0.609389.
Test: 2018-08-02T10:43:24.108843: step 490, loss 0.547841.
Train: 2018-08-02T10:43:24.374407: step 491, loss 0.551455.
Train: 2018-08-02T10:43:24.608726: step 492, loss 0.535402.
Train: 2018-08-02T10:43:24.827454: step 493, loss 0.562889.
Train: 2018-08-02T10:43:25.046154: step 494, loss 0.55382.
Train: 2018-08-02T10:43:25.264852: step 495, loss 0.549982.
Train: 2018-08-02T10:43:25.483554: step 496, loss 0.571444.
Train: 2018-08-02T10:43:25.702250: step 497, loss 0.52681.
Train: 2018-08-02T10:43:25.920948: step 498, loss 0.525384.
Train: 2018-08-02T10:43:26.139647: step 499, loss 0.551742.
Train: 2018-08-02T10:43:26.373963: step 500, loss 0.509394.
Test: 2018-08-02T10:43:27.576780: step 500, loss 0.547829.
Train: 2018-08-02T10:43:28.404742: step 501, loss 0.524058.
Train: 2018-08-02T10:43:28.623410: step 502, loss 0.517605.
Train: 2018-08-02T10:43:28.842109: step 503, loss 0.556377.
Train: 2018-08-02T10:43:29.060844: step 504, loss 0.544259.
Train: 2018-08-02T10:43:29.279537: step 505, loss 0.59474.
Train: 2018-08-02T10:43:29.498224: step 506, loss 0.634962.
Train: 2018-08-02T10:43:29.716929: step 507, loss 0.566157.
Train: 2018-08-02T10:43:29.951251: step 508, loss 0.568174.
Train: 2018-08-02T10:43:30.216788: step 509, loss 0.572255.
Train: 2018-08-02T10:43:30.435486: step 510, loss 0.53582.
Test: 2018-08-02T10:43:31.591465: step 510, loss 0.549215.
Train: 2018-08-02T10:43:31.810189: step 511, loss 0.560898.
Train: 2018-08-02T10:43:32.028894: step 512, loss 0.526013.
Train: 2018-08-02T10:43:32.247586: step 513, loss 0.570689.
Train: 2018-08-02T10:43:32.466291: step 514, loss 0.530175.
Train: 2018-08-02T10:43:32.684989: step 515, loss 0.607849.
Train: 2018-08-02T10:43:32.903659: step 516, loss 0.633551.
Train: 2018-08-02T10:43:33.122356: step 517, loss 0.601668.
Train: 2018-08-02T10:43:33.341085: step 518, loss 0.545018.
Train: 2018-08-02T10:43:33.559784: step 519, loss 0.5388.
Train: 2018-08-02T10:43:33.778483: step 520, loss 0.572301.
Test: 2018-08-02T10:43:34.934433: step 520, loss 0.548241.
Train: 2018-08-02T10:43:35.153131: step 521, loss 0.561816.
Train: 2018-08-02T10:43:35.371860: step 522, loss 0.553119.
Train: 2018-08-02T10:43:35.590559: step 523, loss 0.546717.
Train: 2018-08-02T10:43:35.809257: step 524, loss 0.591543.
Train: 2018-08-02T10:43:36.043549: step 525, loss 0.563907.
Train: 2018-08-02T10:43:36.277868: step 526, loss 0.622571.
Train: 2018-08-02T10:43:36.496567: step 527, loss 0.516433.
Train: 2018-08-02T10:43:36.730899: step 528, loss 0.544948.
Train: 2018-08-02T10:43:36.933964: step 529, loss 0.658649.
Train: 2018-08-02T10:43:37.152663: step 530, loss 0.588188.
Test: 2018-08-02T10:43:38.371129: step 530, loss 0.550168.
Train: 2018-08-02T10:43:38.589826: step 531, loss 0.602988.
Train: 2018-08-02T10:43:38.808556: step 532, loss 0.506359.
Train: 2018-08-02T10:43:39.027254: step 533, loss 0.564032.
Train: 2018-08-02T10:43:39.245955: step 534, loss 0.527821.
Train: 2018-08-02T10:43:39.464652: step 535, loss 0.592921.
Train: 2018-08-02T10:43:39.683353: step 536, loss 0.50538.
Train: 2018-08-02T10:43:39.902044: step 537, loss 0.570047.
Train: 2018-08-02T10:43:40.120749: step 538, loss 0.587948.
Train: 2018-08-02T10:43:40.339442: step 539, loss 0.58882.
Train: 2018-08-02T10:43:40.558140: step 540, loss 0.564345.
Test: 2018-08-02T10:43:41.745337: step 540, loss 0.550838.
Train: 2018-08-02T10:43:41.948440: step 541, loss 0.567458.
Train: 2018-08-02T10:43:42.167138: step 542, loss 0.556966.
Train: 2018-08-02T10:43:42.385837: step 543, loss 0.577462.
Train: 2018-08-02T10:43:42.604535: step 544, loss 0.624913.
Train: 2018-08-02T10:43:42.823238: step 545, loss 0.507078.
Train: 2018-08-02T10:43:43.041939: step 546, loss 0.638105.
Train: 2018-08-02T10:43:43.260638: step 547, loss 0.599787.
Train: 2018-08-02T10:43:43.479341: step 548, loss 0.564841.
Train: 2018-08-02T10:43:43.698035: step 549, loss 0.493065.
Train: 2018-08-02T10:43:43.916736: step 550, loss 0.559726.
Test: 2018-08-02T10:43:45.072684: step 550, loss 0.550725.
Train: 2018-08-02T10:43:45.275792: step 551, loss 0.518704.
Train: 2018-08-02T10:43:45.494490: step 552, loss 0.563475.
Train: 2018-08-02T10:43:45.728804: step 553, loss 0.561554.
Train: 2018-08-02T10:43:45.947480: step 554, loss 0.499807.
Train: 2018-08-02T10:43:46.166207: step 555, loss 0.557419.
Train: 2018-08-02T10:43:46.384907: step 556, loss 0.562775.
Train: 2018-08-02T10:43:46.603609: step 557, loss 0.515572.
Train: 2018-08-02T10:43:46.822304: step 558, loss 0.555833.
Train: 2018-08-02T10:43:47.040972: step 559, loss 0.570491.
Train: 2018-08-02T10:43:47.259671: step 560, loss 0.516826.
Test: 2018-08-02T10:43:48.446893: step 560, loss 0.54813.
Train: 2018-08-02T10:43:48.649995: step 561, loss 0.564926.
Train: 2018-08-02T10:43:48.868669: step 562, loss 0.522456.
Train: 2018-08-02T10:43:49.087397: step 563, loss 0.62419.
Train: 2018-08-02T10:43:49.306067: step 564, loss 0.571365.
Train: 2018-08-02T10:43:49.524790: step 565, loss 0.563425.
Train: 2018-08-02T10:43:49.759117: step 566, loss 0.612386.
Train: 2018-08-02T10:43:49.977810: step 567, loss 0.588877.
Train: 2018-08-02T10:43:50.196514: step 568, loss 0.616763.
Train: 2018-08-02T10:43:50.415213: step 569, loss 0.556298.
Train: 2018-08-02T10:43:50.633883: step 570, loss 0.612583.
Test: 2018-08-02T10:43:51.805481: step 570, loss 0.549941.
Train: 2018-08-02T10:43:52.008584: step 571, loss 0.535817.
Train: 2018-08-02T10:43:52.227292: step 572, loss 0.619443.
Train: 2018-08-02T10:43:52.445987: step 573, loss 0.547018.
Train: 2018-08-02T10:43:52.695922: step 574, loss 0.550891.
Train: 2018-08-02T10:43:52.899005: step 575, loss 0.530771.
Train: 2018-08-02T10:43:53.117674: step 576, loss 0.573177.
Train: 2018-08-02T10:43:53.336375: step 577, loss 0.627895.
Train: 2018-08-02T10:43:53.570724: step 578, loss 0.514495.
Train: 2018-08-02T10:43:53.773800: step 579, loss 0.538736.
Train: 2018-08-02T10:43:53.992500: step 580, loss 0.530414.
Test: 2018-08-02T10:43:55.179691: step 580, loss 0.549571.
Train: 2018-08-02T10:43:55.382769: step 581, loss 0.563125.
Train: 2018-08-02T10:43:55.601501: step 582, loss 0.545019.
Train: 2018-08-02T10:43:55.835789: step 583, loss 0.534697.
Train: 2018-08-02T10:43:56.054486: step 584, loss 0.578864.
Train: 2018-08-02T10:43:56.273216: step 585, loss 0.540701.
Train: 2018-08-02T10:43:56.491883: step 586, loss 0.611833.
Train: 2018-08-02T10:43:56.710613: step 587, loss 0.519788.
Train: 2018-08-02T10:43:56.929316: step 588, loss 0.547691.
Train: 2018-08-02T10:43:57.179247: step 589, loss 0.511173.
Train: 2018-08-02T10:43:57.397953: step 590, loss 0.491223.
Test: 2018-08-02T10:43:58.585144: step 590, loss 0.548906.
Train: 2018-08-02T10:43:58.788251: step 591, loss 0.559977.
Train: 2018-08-02T10:43:59.006951: step 592, loss 0.540157.
Train: 2018-08-02T10:43:59.209997: step 593, loss 0.522192.
Train: 2018-08-02T10:43:59.428705: step 594, loss 0.61897.
Train: 2018-08-02T10:43:59.647395: step 595, loss 0.538253.
Train: 2018-08-02T10:43:59.866124: step 596, loss 0.510797.
Train: 2018-08-02T10:44:00.100415: step 597, loss 0.604725.
Train: 2018-08-02T10:44:00.303492: step 598, loss 0.570412.
Train: 2018-08-02T10:44:00.522221: step 599, loss 0.638191.
Train: 2018-08-02T10:44:00.740889: step 600, loss 0.574534.
Test: 2018-08-02T10:44:01.896868: step 600, loss 0.54786.
Train: 2018-08-02T10:44:02.724833: step 601, loss 0.562142.
Train: 2018-08-02T10:44:02.943528: step 602, loss 0.698781.
Train: 2018-08-02T10:44:03.162227: step 603, loss 0.558339.
Train: 2018-08-02T10:44:03.349656: step 604, loss 0.557974.
Train: 2018-08-02T10:44:03.552762: step 605, loss 0.550721.
Train: 2018-08-02T10:44:03.771455: step 606, loss 0.551736.
Train: 2018-08-02T10:44:03.990159: step 607, loss 0.564137.
Train: 2018-08-02T10:44:04.224482: step 608, loss 0.531504.
Train: 2018-08-02T10:44:04.443177: step 609, loss 0.587775.
Train: 2018-08-02T10:44:04.661877: step 610, loss 0.645155.
Test: 2018-08-02T10:44:05.849068: step 610, loss 0.548826.
Train: 2018-08-02T10:44:06.052145: step 611, loss 0.585718.
Train: 2018-08-02T10:44:06.255247: step 612, loss 0.575705.
Train: 2018-08-02T10:44:06.473923: step 613, loss 0.56481.
Train: 2018-08-02T10:44:06.692652: step 614, loss 0.548846.
Train: 2018-08-02T10:44:06.911320: step 615, loss 0.564089.
Train: 2018-08-02T10:44:07.130043: step 616, loss 0.587751.
Train: 2018-08-02T10:44:07.348725: step 617, loss 0.56615.
Train: 2018-08-02T10:44:07.567446: step 618, loss 0.557876.
Train: 2018-08-02T10:44:07.786114: step 619, loss 0.605366.
Train: 2018-08-02T10:44:08.004843: step 620, loss 0.59494.
Test: 2018-08-02T10:44:09.160792: step 620, loss 0.550661.
Train: 2018-08-02T10:44:09.379516: step 621, loss 0.571143.
Train: 2018-08-02T10:44:09.598189: step 622, loss 0.593392.
Train: 2018-08-02T10:44:09.816919: step 623, loss 0.609371.
Train: 2018-08-02T10:44:10.035589: step 624, loss 0.548126.
Train: 2018-08-02T10:44:10.269909: step 625, loss 0.567282.
Train: 2018-08-02T10:44:10.488607: step 626, loss 0.526186.
Train: 2018-08-02T10:44:10.707336: step 627, loss 0.559082.
Train: 2018-08-02T10:44:10.926035: step 628, loss 0.601426.
Train: 2018-08-02T10:44:11.144734: step 629, loss 0.54258.
Train: 2018-08-02T10:44:11.379023: step 630, loss 0.524493.
Test: 2018-08-02T10:44:12.566245: step 630, loss 0.551029.
Train: 2018-08-02T10:44:12.769322: step 631, loss 0.60987.
Train: 2018-08-02T10:44:12.988051: step 632, loss 0.610141.
Train: 2018-08-02T10:44:13.206751: step 633, loss 0.556834.
Train: 2018-08-02T10:44:13.409822: step 634, loss 0.579275.
Train: 2018-08-02T10:44:13.628496: step 635, loss 0.534079.
Train: 2018-08-02T10:44:13.831573: step 636, loss 0.539972.
Train: 2018-08-02T10:44:14.050272: step 637, loss 0.54932.
Train: 2018-08-02T10:44:14.280537: step 638, loss 0.579299.
Train: 2018-08-02T10:44:14.499234: step 639, loss 0.631959.
Train: 2018-08-02T10:44:14.717933: step 640, loss 0.587144.
Test: 2018-08-02T10:44:15.889533: step 640, loss 0.55117.
Train: 2018-08-02T10:44:16.092641: step 641, loss 0.608393.
Train: 2018-08-02T10:44:16.311340: step 642, loss 0.542439.
Train: 2018-08-02T10:44:16.530039: step 643, loss 0.594196.
Train: 2018-08-02T10:44:16.748708: step 644, loss 0.540962.
Train: 2018-08-02T10:44:16.967437: step 645, loss 0.588239.
Train: 2018-08-02T10:44:17.186135: step 646, loss 0.477763.
Train: 2018-08-02T10:44:17.404829: step 647, loss 0.548517.
Train: 2018-08-02T10:44:17.623533: step 648, loss 0.532881.
Train: 2018-08-02T10:44:17.842231: step 649, loss 0.491563.
Train: 2018-08-02T10:44:18.060934: step 650, loss 0.504157.
Test: 2018-08-02T10:44:19.216880: step 650, loss 0.549061.
Train: 2018-08-02T10:44:19.451200: step 651, loss 0.498212.
Train: 2018-08-02T10:44:19.685551: step 652, loss 0.546413.
Train: 2018-08-02T10:44:19.904249: step 653, loss 0.581448.
Train: 2018-08-02T10:44:20.122941: step 654, loss 0.611505.
Train: 2018-08-02T10:44:20.341616: step 655, loss 0.58588.
Train: 2018-08-02T10:44:20.560316: step 656, loss 0.602526.
Train: 2018-08-02T10:44:20.779044: step 657, loss 0.585586.
Train: 2018-08-02T10:44:20.982123: step 658, loss 0.600977.
Train: 2018-08-02T10:44:21.185200: step 659, loss 0.538329.
Train: 2018-08-02T10:44:21.388247: step 660, loss 0.530492.
Test: 2018-08-02T10:44:22.544226: step 660, loss 0.547896.
Train: 2018-08-02T10:44:22.825409: step 661, loss 0.568495.
Train: 2018-08-02T10:44:23.044139: step 662, loss 0.622951.
Train: 2018-08-02T10:44:23.262837: step 663, loss 0.569643.
Train: 2018-08-02T10:44:23.481538: step 664, loss 0.528691.
Train: 2018-08-02T10:44:23.700230: step 665, loss 0.52889.
Train: 2018-08-02T10:44:23.918935: step 666, loss 0.537248.
Train: 2018-08-02T10:44:24.137632: step 667, loss 0.530815.
Train: 2018-08-02T10:44:24.340710: step 668, loss 0.589843.
Train: 2018-08-02T10:44:24.559408: step 669, loss 0.530975.
Train: 2018-08-02T10:44:24.778107: step 670, loss 0.522016.
Test: 2018-08-02T10:44:25.934057: step 670, loss 0.547476.
Train: 2018-08-02T10:44:26.137164: step 671, loss 0.494103.
Train: 2018-08-02T10:44:26.355865: step 672, loss 0.60345.
Train: 2018-08-02T10:44:26.574531: step 673, loss 0.560963.
Train: 2018-08-02T10:44:26.793261: step 674, loss 0.505588.
Train: 2018-08-02T10:44:27.011929: step 675, loss 0.56186.
Train: 2018-08-02T10:44:27.230627: step 676, loss 0.545609.
Train: 2018-08-02T10:44:27.449356: step 677, loss 0.604109.
Train: 2018-08-02T10:44:27.668056: step 678, loss 0.608573.
Train: 2018-08-02T10:44:27.886754: step 679, loss 0.483833.
Train: 2018-08-02T10:44:28.105456: step 680, loss 0.538409.
Test: 2018-08-02T10:44:29.261402: step 680, loss 0.549406.
Train: 2018-08-02T10:44:29.464479: step 681, loss 0.580935.
Train: 2018-08-02T10:44:29.667582: step 682, loss 0.544875.
Train: 2018-08-02T10:44:29.886256: step 683, loss 0.590773.
Train: 2018-08-02T10:44:30.104955: step 684, loss 0.551215.
Train: 2018-08-02T10:44:30.323684: step 685, loss 0.509199.
Train: 2018-08-02T10:44:30.526762: step 686, loss 0.588147.
Train: 2018-08-02T10:44:30.745460: step 687, loss 0.597644.
Train: 2018-08-02T10:44:30.964153: step 688, loss 0.545795.
Train: 2018-08-02T10:44:31.182858: step 689, loss 0.569887.
Train: 2018-08-02T10:44:31.401556: step 690, loss 0.45493.
Test: 2018-08-02T10:44:32.557505: step 690, loss 0.547781.
Train: 2018-08-02T10:44:32.760607: step 691, loss 0.502847.
Train: 2018-08-02T10:44:32.979314: step 692, loss 0.596205.
Train: 2018-08-02T10:44:33.197983: step 693, loss 0.631299.
Train: 2018-08-02T10:44:33.416679: step 694, loss 0.597718.
Train: 2018-08-02T10:44:33.635408: step 695, loss 0.569249.
Train: 2018-08-02T10:44:33.854109: step 696, loss 0.629606.
Train: 2018-08-02T10:44:34.072775: step 697, loss 0.57975.
Train: 2018-08-02T10:44:34.291507: step 698, loss 0.539204.
Train: 2018-08-02T10:44:34.522212: step 699, loss 0.591977.
Train: 2018-08-02T10:44:34.769946: step 700, loss 0.589768.
Test: 2018-08-02T10:44:36.008025: step 700, loss 0.547695.
Train: 2018-08-02T10:44:36.845626: step 701, loss 0.527786.
Train: 2018-08-02T10:44:37.065526: step 702, loss 0.572359.
Train: 2018-08-02T10:44:37.285408: step 703, loss 0.544996.
Train: 2018-08-02T10:44:37.510666: step 704, loss 0.521883.
Train: 2018-08-02T10:44:37.729365: step 705, loss 0.553824.
Train: 2018-08-02T10:44:37.948064: step 706, loss 0.650123.
Train: 2018-08-02T10:44:38.166762: step 707, loss 0.547149.
Train: 2018-08-02T10:44:38.385431: step 708, loss 0.538473.
Train: 2018-08-02T10:44:38.635403: step 709, loss 0.588688.
Train: 2018-08-02T10:44:38.854075: step 710, loss 0.61228.
Test: 2018-08-02T10:44:40.181886: step 710, loss 0.549692.
Train: 2018-08-02T10:44:40.400619: step 711, loss 0.570314.
Train: 2018-08-02T10:44:40.619315: step 712, loss 0.633484.
Train: 2018-08-02T10:44:40.838012: step 713, loss 0.539055.
Train: 2018-08-02T10:44:41.056711: step 714, loss 0.577029.
Train: 2018-08-02T10:44:41.275409: step 715, loss 0.499701.
Train: 2018-08-02T10:44:41.494108: step 716, loss 0.61033.
Train: 2018-08-02T10:44:41.712807: step 717, loss 0.563906.
Train: 2018-08-02T10:44:41.931506: step 718, loss 0.508334.
Train: 2018-08-02T10:44:42.150204: step 719, loss 0.572022.
Train: 2018-08-02T10:44:42.368903: step 720, loss 0.529579.
Test: 2018-08-02T10:44:43.524853: step 720, loss 0.549909.
Train: 2018-08-02T10:44:43.743581: step 721, loss 0.571046.
Train: 2018-08-02T10:44:43.962275: step 722, loss 0.577171.
Train: 2018-08-02T10:44:44.180979: step 723, loss 0.574485.
Train: 2018-08-02T10:44:44.399677: step 724, loss 0.491568.
Train: 2018-08-02T10:44:44.633967: step 725, loss 0.507913.
Train: 2018-08-02T10:44:44.852696: step 726, loss 0.618992.
Train: 2018-08-02T10:44:45.071390: step 727, loss 0.585553.
Train: 2018-08-02T10:44:45.290097: step 728, loss 0.491621.
Train: 2018-08-02T10:44:45.508789: step 729, loss 0.565627.
Train: 2018-08-02T10:44:45.727491: step 730, loss 0.530622.
Test: 2018-08-02T10:44:46.930306: step 730, loss 0.54843.
Train: 2018-08-02T10:44:47.133413: step 731, loss 0.511702.
Train: 2018-08-02T10:44:47.336489: step 732, loss 0.488905.
Train: 2018-08-02T10:44:47.555183: step 733, loss 0.503935.
Train: 2018-08-02T10:44:47.773857: step 734, loss 0.555937.
Train: 2018-08-02T10:44:47.992586: step 735, loss 0.637716.
Train: 2018-08-02T10:44:48.242498: step 736, loss 0.623374.
Train: 2018-08-02T10:44:48.461221: step 737, loss 0.51081.
Train: 2018-08-02T10:44:48.679895: step 738, loss 0.529005.
Train: 2018-08-02T10:44:48.898621: step 739, loss 0.571916.
Train: 2018-08-02T10:44:49.117319: step 740, loss 0.53317.
Test: 2018-08-02T10:44:50.304514: step 740, loss 0.549487.
Train: 2018-08-02T10:44:50.523238: step 741, loss 0.579398.
Train: 2018-08-02T10:44:50.741942: step 742, loss 0.562353.
Train: 2018-08-02T10:44:50.960642: step 743, loss 0.564515.
Train: 2018-08-02T10:44:51.179340: step 744, loss 0.527804.
Train: 2018-08-02T10:44:51.398009: step 745, loss 0.552911.
Train: 2018-08-02T10:44:51.616709: step 746, loss 0.597463.
Train: 2018-08-02T10:44:51.835431: step 747, loss 0.528605.
Train: 2018-08-02T10:44:52.054136: step 748, loss 0.571521.
Train: 2018-08-02T10:44:52.272834: step 749, loss 0.573914.
Train: 2018-08-02T10:44:52.491532: step 750, loss 0.536574.
Test: 2018-08-02T10:44:53.663103: step 750, loss 0.547804.
Train: 2018-08-02T10:44:53.866205: step 751, loss 0.582579.
Train: 2018-08-02T10:44:54.084904: step 752, loss 0.492362.
Train: 2018-08-02T10:44:54.319231: step 753, loss 0.642426.
Train: 2018-08-02T10:44:54.537927: step 754, loss 0.594365.
Train: 2018-08-02T10:44:54.725355: step 755, loss 0.599411.
Train: 2018-08-02T10:44:54.944053: step 756, loss 0.612738.
Train: 2018-08-02T10:44:55.162753: step 757, loss 0.504153.
Train: 2018-08-02T10:44:55.381481: step 758, loss 0.561296.
Train: 2018-08-02T10:44:55.600175: step 759, loss 0.563765.
Train: 2018-08-02T10:44:55.818878: step 760, loss 0.561466.
Test: 2018-08-02T10:44:57.006070: step 760, loss 0.548095.
Train: 2018-08-02T10:44:57.209147: step 761, loss 0.594496.
Train: 2018-08-02T10:44:57.412255: step 762, loss 0.540187.
Train: 2018-08-02T10:44:57.630954: step 763, loss 0.596521.
Train: 2018-08-02T10:44:57.849636: step 764, loss 0.603111.
Train: 2018-08-02T10:44:58.068347: step 765, loss 0.576928.
Train: 2018-08-02T10:44:58.287020: step 766, loss 0.540023.
Train: 2018-08-02T10:44:58.505719: step 767, loss 0.547603.
Train: 2018-08-02T10:44:58.724418: step 768, loss 0.550066.
Train: 2018-08-02T10:44:58.943146: step 769, loss 0.561843.
Train: 2018-08-02T10:44:59.161816: step 770, loss 0.60193.
Test: 2018-08-02T10:45:00.349037: step 770, loss 0.549644.
Train: 2018-08-02T10:45:00.552139: step 771, loss 0.564371.
Train: 2018-08-02T10:45:00.770813: step 772, loss 0.490517.
Train: 2018-08-02T10:45:00.989513: step 773, loss 0.523852.
Train: 2018-08-02T10:45:01.208241: step 774, loss 0.537127.
Train: 2018-08-02T10:45:01.426911: step 775, loss 0.579752.
Train: 2018-08-02T10:45:01.645613: step 776, loss 0.562412.
Train: 2018-08-02T10:45:01.864310: step 777, loss 0.562499.
Train: 2018-08-02T10:45:02.083006: step 778, loss 0.602175.
Train: 2018-08-02T10:45:02.301736: step 779, loss 0.567343.
Train: 2018-08-02T10:45:02.520404: step 780, loss 0.576311.
Test: 2018-08-02T10:45:03.692004: step 780, loss 0.549278.
Train: 2018-08-02T10:45:03.895106: step 781, loss 0.564812.
Train: 2018-08-02T10:45:04.113805: step 782, loss 0.613065.
Train: 2018-08-02T10:45:04.332481: step 783, loss 0.55202.
Train: 2018-08-02T10:45:04.551503: step 784, loss 0.562074.
Train: 2018-08-02T10:45:04.765828: step 785, loss 0.56882.
Train: 2018-08-02T10:45:04.984556: step 786, loss 0.579275.
Train: 2018-08-02T10:45:05.203259: step 787, loss 0.576789.
Train: 2018-08-02T10:45:05.421955: step 788, loss 0.588798.
Train: 2018-08-02T10:45:05.671868: step 789, loss 0.608699.
Train: 2018-08-02T10:45:05.890565: step 790, loss 0.583865.
Test: 2018-08-02T10:45:07.046543: step 790, loss 0.548646.
Train: 2018-08-02T10:45:07.249645: step 791, loss 0.52827.
Train: 2018-08-02T10:45:07.468322: step 792, loss 0.555487.
Train: 2018-08-02T10:45:07.687043: step 793, loss 0.515605.
Train: 2018-08-02T10:45:07.905718: step 794, loss 0.549905.
Train: 2018-08-02T10:45:08.124449: step 795, loss 0.584203.
Train: 2018-08-02T10:45:08.343146: step 796, loss 0.545514.
Train: 2018-08-02T10:45:08.561844: step 797, loss 0.534093.
Train: 2018-08-02T10:45:08.780542: step 798, loss 0.546962.
Train: 2018-08-02T10:45:08.999241: step 799, loss 0.53186.
Train: 2018-08-02T10:45:09.217910: step 800, loss 0.570717.
Test: 2018-08-02T10:45:10.373889: step 800, loss 0.55014.
Train: 2018-08-02T10:45:11.170609: step 801, loss 0.569861.
Train: 2018-08-02T10:45:11.435254: step 802, loss 0.506482.
Train: 2018-08-02T10:45:11.648277: step 803, loss 0.637148.
Train: 2018-08-02T10:45:11.868135: step 804, loss 0.54737.
Train: 2018-08-02T10:45:12.088323: step 805, loss 0.597932.
Train: 2018-08-02T10:45:12.308317: step 806, loss 0.569069.
Train: 2018-08-02T10:45:12.529790: step 807, loss 0.570194.
Train: 2018-08-02T10:45:12.748415: step 808, loss 0.613036.
Train: 2018-08-02T10:45:12.955034: step 809, loss 0.475404.
Train: 2018-08-02T10:45:13.173733: step 810, loss 0.465189.
Test: 2018-08-02T10:45:14.329683: step 810, loss 0.549099.
Train: 2018-08-02T10:45:14.548405: step 811, loss 0.594258.
Train: 2018-08-02T10:45:14.767113: step 812, loss 0.55148.
Train: 2018-08-02T10:45:14.985778: step 813, loss 0.594471.
Train: 2018-08-02T10:45:15.204507: step 814, loss 0.612588.
Train: 2018-08-02T10:45:15.423176: step 815, loss 0.569587.
Train: 2018-08-02T10:45:15.641874: step 816, loss 0.590719.
Train: 2018-08-02T10:45:15.860574: step 817, loss 0.496539.
Train: 2018-08-02T10:45:16.079272: step 818, loss 0.574391.
Train: 2018-08-02T10:45:16.297972: step 819, loss 0.586872.
Train: 2018-08-02T10:45:16.516703: step 820, loss 0.578295.
Test: 2018-08-02T10:45:17.672649: step 820, loss 0.550154.
Train: 2018-08-02T10:45:17.875728: step 821, loss 0.628218.
Train: 2018-08-02T10:45:18.094426: step 822, loss 0.494662.
Train: 2018-08-02T10:45:18.313155: step 823, loss 0.586588.
Train: 2018-08-02T10:45:18.531856: step 824, loss 0.572097.
Train: 2018-08-02T10:45:18.750521: step 825, loss 0.504385.
Train: 2018-08-02T10:45:18.969251: step 826, loss 0.544928.
Train: 2018-08-02T10:45:19.187949: step 827, loss 0.502521.
Train: 2018-08-02T10:45:19.406649: step 828, loss 0.515539.
Train: 2018-08-02T10:45:19.625347: step 829, loss 0.564275.
Train: 2018-08-02T10:45:19.844016: step 830, loss 0.580953.
Test: 2018-08-02T10:45:20.999995: step 830, loss 0.547662.
Train: 2018-08-02T10:45:21.218694: step 831, loss 0.472556.
Train: 2018-08-02T10:45:21.437423: step 832, loss 0.501691.
Train: 2018-08-02T10:45:21.656091: step 833, loss 0.580948.
Train: 2018-08-02T10:45:21.874821: step 834, loss 0.61383.
Train: 2018-08-02T10:45:22.093519: step 835, loss 0.540986.
Train: 2018-08-02T10:45:22.312218: step 836, loss 0.510698.
Train: 2018-08-02T10:45:22.530938: step 837, loss 0.579429.
Train: 2018-08-02T10:45:22.749585: step 838, loss 0.605244.
Train: 2018-08-02T10:45:22.968316: step 839, loss 0.517978.
Train: 2018-08-02T10:45:23.187007: step 840, loss 0.536272.
Test: 2018-08-02T10:45:24.342962: step 840, loss 0.547167.
Train: 2018-08-02T10:45:24.546064: step 841, loss 0.640011.
Train: 2018-08-02T10:45:24.764771: step 842, loss 0.531021.
Train: 2018-08-02T10:45:24.983468: step 843, loss 0.649347.
Train: 2018-08-02T10:45:25.202146: step 844, loss 0.518667.
Train: 2018-08-02T10:45:25.420865: step 845, loss 0.561246.
Train: 2018-08-02T10:45:25.639533: step 846, loss 0.504501.
Train: 2018-08-02T10:45:25.858259: step 847, loss 0.616174.
Train: 2018-08-02T10:45:26.076961: step 848, loss 0.562558.
Train: 2018-08-02T10:45:26.295654: step 849, loss 0.552709.
Train: 2018-08-02T10:45:26.514353: step 850, loss 0.561266.
Test: 2018-08-02T10:45:27.670308: step 850, loss 0.548511.
Train: 2018-08-02T10:45:27.889041: step 851, loss 0.639712.
Train: 2018-08-02T10:45:28.107705: step 852, loss 0.487312.
Train: 2018-08-02T10:45:28.326436: step 853, loss 0.571267.
Train: 2018-08-02T10:45:28.545134: step 854, loss 0.554172.
Train: 2018-08-02T10:45:28.763835: step 855, loss 0.596201.
Train: 2018-08-02T10:45:28.982501: step 856, loss 0.551031.
Train: 2018-08-02T10:45:29.201230: step 857, loss 0.562704.
Train: 2018-08-02T10:45:29.419929: step 858, loss 0.496416.
Train: 2018-08-02T10:45:29.638622: step 859, loss 0.571849.
Train: 2018-08-02T10:45:29.857321: step 860, loss 0.595976.
Test: 2018-08-02T10:45:31.013275: step 860, loss 0.548989.
Train: 2018-08-02T10:45:31.216377: step 861, loss 0.530504.
Train: 2018-08-02T10:45:31.435082: step 862, loss 0.547129.
Train: 2018-08-02T10:45:31.653781: step 863, loss 0.670907.
Train: 2018-08-02T10:45:31.872449: step 864, loss 0.580296.
Train: 2018-08-02T10:45:32.091178: step 865, loss 0.569693.
Train: 2018-08-02T10:45:32.309846: step 866, loss 0.69341.
Train: 2018-08-02T10:45:32.528576: step 867, loss 0.506219.
Train: 2018-08-02T10:45:32.747245: step 868, loss 0.538681.
Train: 2018-08-02T10:45:32.965975: step 869, loss 0.627031.
Train: 2018-08-02T10:45:33.184672: step 870, loss 0.659552.
Test: 2018-08-02T10:45:34.340621: step 870, loss 0.549788.
Train: 2018-08-02T10:45:34.543723: step 871, loss 0.539889.
Train: 2018-08-02T10:45:34.746807: step 872, loss 0.485385.
Train: 2018-08-02T10:45:34.965505: step 873, loss 0.610061.
Train: 2018-08-02T10:45:35.184198: step 874, loss 0.515153.
Train: 2018-08-02T10:45:35.402872: step 875, loss 0.524318.
Train: 2018-08-02T10:45:35.621604: step 876, loss 0.586219.
Train: 2018-08-02T10:45:35.840294: step 877, loss 0.524662.
Train: 2018-08-02T10:45:36.058999: step 878, loss 0.532583.
Train: 2018-08-02T10:45:36.277698: step 879, loss 0.518103.
Train: 2018-08-02T10:45:36.496396: step 880, loss 0.516043.
Test: 2018-08-02T10:45:37.652345: step 880, loss 0.550978.
Train: 2018-08-02T10:45:37.855424: step 881, loss 0.547069.
Train: 2018-08-02T10:45:38.058530: step 882, loss 0.515686.
Train: 2018-08-02T10:45:38.277230: step 883, loss 0.580343.
Train: 2018-08-02T10:45:38.495928: step 884, loss 0.529137.
Train: 2018-08-02T10:45:38.714627: step 885, loss 0.595369.
Train: 2018-08-02T10:45:38.933326: step 886, loss 0.669725.
Train: 2018-08-02T10:45:39.152024: step 887, loss 0.536449.
Train: 2018-08-02T10:45:39.370723: step 888, loss 0.570133.
Train: 2018-08-02T10:45:39.589422: step 889, loss 0.611874.
Train: 2018-08-02T10:45:39.808117: step 890, loss 0.578496.
Test: 2018-08-02T10:45:40.964069: step 890, loss 0.549169.
Train: 2018-08-02T10:45:41.182800: step 891, loss 0.520498.
Train: 2018-08-02T10:45:41.401498: step 892, loss 0.580402.
Train: 2018-08-02T10:45:41.620194: step 893, loss 0.56313.
Train: 2018-08-02T10:45:41.838896: step 894, loss 0.591032.
Train: 2018-08-02T10:45:42.057594: step 895, loss 0.563738.
Train: 2018-08-02T10:45:42.276288: step 896, loss 0.481569.
Train: 2018-08-02T10:45:42.494994: step 897, loss 0.531346.
Train: 2018-08-02T10:45:42.713690: step 898, loss 0.587979.
Train: 2018-08-02T10:45:42.932389: step 899, loss 0.521091.
Train: 2018-08-02T10:45:43.151059: step 900, loss 0.572666.
Test: 2018-08-02T10:45:44.307037: step 900, loss 0.549207.
Train: 2018-08-02T10:45:45.119390: step 901, loss 0.594731.
Train: 2018-08-02T10:45:45.338046: step 902, loss 0.529883.
Train: 2018-08-02T10:45:45.556774: step 903, loss 0.580992.
Train: 2018-08-02T10:45:45.775474: step 904, loss 0.61166.
Train: 2018-08-02T10:45:45.994142: step 905, loss 0.530375.
Train: 2018-08-02T10:45:46.165977: step 906, loss 0.545534.
Train: 2018-08-02T10:45:46.384676: step 907, loss 0.645055.
Train: 2018-08-02T10:45:46.603374: step 908, loss 0.563019.
Train: 2018-08-02T10:45:46.822074: step 909, loss 0.602666.
Train: 2018-08-02T10:45:47.040771: step 910, loss 0.522205.
Test: 2018-08-02T10:45:48.196751: step 910, loss 0.549612.
Train: 2018-08-02T10:45:48.415475: step 911, loss 0.579704.
Train: 2018-08-02T10:45:48.634182: step 912, loss 0.578041.
Train: 2018-08-02T10:45:48.852878: step 913, loss 0.594812.
Train: 2018-08-02T10:45:49.071576: step 914, loss 0.50583.
Train: 2018-08-02T10:45:49.290275: step 915, loss 0.611899.
Train: 2018-08-02T10:45:49.508977: step 916, loss 0.554631.
Train: 2018-08-02T10:45:49.727673: step 917, loss 0.595148.
Train: 2018-08-02T10:45:49.946388: step 918, loss 0.538472.
Train: 2018-08-02T10:45:50.165071: step 919, loss 0.483726.
Train: 2018-08-02T10:45:50.383739: step 920, loss 0.522806.
Test: 2018-08-02T10:45:51.539719: step 920, loss 0.549325.
Train: 2018-08-02T10:45:51.742796: step 921, loss 0.483676.
Train: 2018-08-02T10:45:51.961524: step 922, loss 0.619228.
Train: 2018-08-02T10:45:52.164571: step 923, loss 0.569489.
Train: 2018-08-02T10:45:52.383296: step 924, loss 0.653007.
Train: 2018-08-02T10:45:52.602000: step 925, loss 0.530523.
Train: 2018-08-02T10:45:52.820698: step 926, loss 0.578165.
Train: 2018-08-02T10:45:53.039368: step 927, loss 0.578774.
Train: 2018-08-02T10:45:53.258090: step 928, loss 0.620242.
Train: 2018-08-02T10:45:53.476766: step 929, loss 0.633969.
Train: 2018-08-02T10:45:53.695464: step 930, loss 0.586642.
Test: 2018-08-02T10:45:54.851443: step 930, loss 0.548644.
Train: 2018-08-02T10:45:55.054545: step 931, loss 0.569752.
Train: 2018-08-02T10:45:55.257623: step 932, loss 0.585689.
Train: 2018-08-02T10:45:55.476305: step 933, loss 0.507525.
Train: 2018-08-02T10:45:55.694995: step 934, loss 0.532023.
Train: 2018-08-02T10:45:55.913694: step 935, loss 0.579191.
Train: 2018-08-02T10:45:56.132393: step 936, loss 0.483059.
Train: 2018-08-02T10:45:56.351122: step 937, loss 0.580126.
Train: 2018-08-02T10:45:56.569790: step 938, loss 0.586457.
Train: 2018-08-02T10:45:56.788519: step 939, loss 0.515074.
Train: 2018-08-02T10:45:57.007221: step 940, loss 0.579348.
Test: 2018-08-02T10:45:58.210032: step 940, loss 0.548896.
Train: 2018-08-02T10:45:58.413110: step 941, loss 0.595766.
Train: 2018-08-02T10:45:58.631807: step 942, loss 0.609418.
Train: 2018-08-02T10:45:58.866167: step 943, loss 0.563013.
Train: 2018-08-02T10:45:59.069207: step 944, loss 0.457868.
Train: 2018-08-02T10:45:59.287929: step 945, loss 0.554987.
Train: 2018-08-02T10:45:59.506636: step 946, loss 0.570069.
Train: 2018-08-02T10:45:59.725302: step 947, loss 0.539527.
Train: 2018-08-02T10:45:59.944030: step 948, loss 0.563633.
Train: 2018-08-02T10:46:00.162730: step 949, loss 0.60497.
Train: 2018-08-02T10:46:00.381431: step 950, loss 0.4983.
Test: 2018-08-02T10:46:01.537377: step 950, loss 0.548997.
Train: 2018-08-02T10:46:01.740487: step 951, loss 0.620907.
Train: 2018-08-02T10:46:01.959184: step 952, loss 0.521819.
Train: 2018-08-02T10:46:02.177882: step 953, loss 0.521848.
Train: 2018-08-02T10:46:02.396581: step 954, loss 0.619578.
Train: 2018-08-02T10:46:02.615280: step 955, loss 0.562773.
Train: 2018-08-02T10:46:02.833974: step 956, loss 0.529702.
Train: 2018-08-02T10:46:03.052680: step 957, loss 0.610924.
Train: 2018-08-02T10:46:03.271377: step 958, loss 0.594206.
Train: 2018-08-02T10:46:03.490075: step 959, loss 0.636788.
Train: 2018-08-02T10:46:03.708743: step 960, loss 0.496956.
Test: 2018-08-02T10:46:04.864723: step 960, loss 0.549456.
Train: 2018-08-02T10:46:05.067826: step 961, loss 0.586959.
Train: 2018-08-02T10:46:05.286530: step 962, loss 0.635529.
Train: 2018-08-02T10:46:05.505198: step 963, loss 0.555944.
Train: 2018-08-02T10:46:05.723898: step 964, loss 0.529466.
Train: 2018-08-02T10:46:05.942626: step 965, loss 0.514406.
Train: 2018-08-02T10:46:06.161325: step 966, loss 0.554322.
Train: 2018-08-02T10:46:06.364403: step 967, loss 0.610512.
Train: 2018-08-02T10:46:06.583101: step 968, loss 0.555213.
Train: 2018-08-02T10:46:06.801803: step 969, loss 0.545856.
Train: 2018-08-02T10:46:07.020499: step 970, loss 0.473179.
Test: 2018-08-02T10:46:08.176447: step 970, loss 0.548523.
Train: 2018-08-02T10:46:08.379549: step 971, loss 0.611288.
Train: 2018-08-02T10:46:08.598254: step 972, loss 0.521994.
Train: 2018-08-02T10:46:08.816953: step 973, loss 0.561772.
Train: 2018-08-02T10:46:09.035621: step 974, loss 0.626645.
Train: 2018-08-02T10:46:09.254353: step 975, loss 0.538525.
Train: 2018-08-02T10:46:09.473049: step 976, loss 0.570129.
Train: 2018-08-02T10:46:09.691747: step 977, loss 0.538014.
Train: 2018-08-02T10:46:09.910447: step 978, loss 0.636326.
Train: 2018-08-02T10:46:10.129116: step 979, loss 0.56946.
Train: 2018-08-02T10:46:10.347844: step 980, loss 0.58687.
Test: 2018-08-02T10:46:11.503793: step 980, loss 0.548751.
Train: 2018-08-02T10:46:11.722523: step 981, loss 0.547013.
Train: 2018-08-02T10:46:11.925599: step 982, loss 0.578346.
Train: 2018-08-02T10:46:12.144298: step 983, loss 0.538857.
Train: 2018-08-02T10:46:12.362968: step 984, loss 0.555246.
Train: 2018-08-02T10:46:12.581696: step 985, loss 0.570957.
Train: 2018-08-02T10:46:12.800418: step 986, loss 0.522137.
Train: 2018-08-02T10:46:13.019093: step 987, loss 0.578683.
Train: 2018-08-02T10:46:13.237787: step 988, loss 0.521559.
Train: 2018-08-02T10:46:13.456492: step 989, loss 0.580914.
Train: 2018-08-02T10:46:13.675189: step 990, loss 0.555407.
Test: 2018-08-02T10:46:14.831139: step 990, loss 0.549703.
Train: 2018-08-02T10:46:15.049838: step 991, loss 0.53156.
Train: 2018-08-02T10:46:15.268561: step 992, loss 0.589002.
Train: 2018-08-02T10:46:15.487266: step 993, loss 0.562774.
Train: 2018-08-02T10:46:15.705966: step 994, loss 0.479978.
Train: 2018-08-02T10:46:15.924634: step 995, loss 0.465371.
Train: 2018-08-02T10:46:16.143333: step 996, loss 0.595348.
Train: 2018-08-02T10:46:16.362061: step 997, loss 0.511639.
Train: 2018-08-02T10:46:16.580729: step 998, loss 0.538522.
Train: 2018-08-02T10:46:16.783837: step 999, loss 0.513556.
Train: 2018-08-02T10:46:17.002538: step 1000, loss 0.595768.
Test: 2018-08-02T10:46:18.174106: step 1000, loss 0.548259.
Train: 2018-08-02T10:46:19.033280: step 1001, loss 0.543731.
Train: 2018-08-02T10:46:19.252012: step 1002, loss 0.6045.
Train: 2018-08-02T10:46:19.470708: step 1003, loss 0.476627.
Train: 2018-08-02T10:46:19.689407: step 1004, loss 0.605352.
Train: 2018-08-02T10:46:19.908107: step 1005, loss 0.553724.
Train: 2018-08-02T10:46:20.126775: step 1006, loss 0.529556.
Train: 2018-08-02T10:46:20.345474: step 1007, loss 0.587493.
Train: 2018-08-02T10:46:20.564172: step 1008, loss 0.486071.
Train: 2018-08-02T10:46:20.782901: step 1009, loss 0.554268.
Train: 2018-08-02T10:46:21.001602: step 1010, loss 0.631054.
Test: 2018-08-02T10:46:22.157548: step 1010, loss 0.547714.
Train: 2018-08-02T10:46:22.376247: step 1011, loss 0.555822.
Train: 2018-08-02T10:46:22.594977: step 1012, loss 0.562335.
Train: 2018-08-02T10:46:22.813675: step 1013, loss 0.553236.
Train: 2018-08-02T10:46:23.032344: step 1014, loss 0.56288.
Train: 2018-08-02T10:46:23.251076: step 1015, loss 0.519027.
Train: 2018-08-02T10:46:23.469772: step 1016, loss 0.553626.
Train: 2018-08-02T10:46:23.688473: step 1017, loss 0.543827.
Train: 2018-08-02T10:46:23.907165: step 1018, loss 0.56213.
Train: 2018-08-02T10:46:24.125838: step 1019, loss 0.527593.
Train: 2018-08-02T10:46:24.344567: step 1020, loss 0.630844.
Test: 2018-08-02T10:46:25.500516: step 1020, loss 0.547409.
Train: 2018-08-02T10:46:25.766078: step 1021, loss 0.503953.
Train: 2018-08-02T10:46:25.984807: step 1022, loss 0.647271.
Train: 2018-08-02T10:46:26.203506: step 1023, loss 0.562704.
Train: 2018-08-02T10:46:26.422174: step 1024, loss 0.528268.
Train: 2018-08-02T10:46:26.640898: step 1025, loss 0.57953.
Train: 2018-08-02T10:46:26.859602: step 1026, loss 0.595435.
Train: 2018-08-02T10:46:27.078271: step 1027, loss 0.587559.
Train: 2018-08-02T10:46:27.296970: step 1028, loss 0.594636.
Train: 2018-08-02T10:46:27.515698: step 1029, loss 0.545015.
Train: 2018-08-02T10:46:27.734401: step 1030, loss 0.52892.
Test: 2018-08-02T10:46:28.890347: step 1030, loss 0.549136.
Train: 2018-08-02T10:46:29.093448: step 1031, loss 0.536621.
Train: 2018-08-02T10:46:29.312153: step 1032, loss 0.562736.
Train: 2018-08-02T10:46:29.530847: step 1033, loss 0.528303.
Train: 2018-08-02T10:46:29.749522: step 1034, loss 0.529656.
Train: 2018-08-02T10:46:29.968249: step 1035, loss 0.579152.
Train: 2018-08-02T10:46:30.171327: step 1036, loss 0.578638.
Train: 2018-08-02T10:46:30.390026: step 1037, loss 0.554148.
Train: 2018-08-02T10:46:30.608724: step 1038, loss 0.605332.
Train: 2018-08-02T10:46:30.827424: step 1039, loss 0.578779.
Train: 2018-08-02T10:46:31.046122: step 1040, loss 0.59689.
Test: 2018-08-02T10:46:32.202071: step 1040, loss 0.548801.
Train: 2018-08-02T10:46:32.420795: step 1041, loss 0.602612.
Train: 2018-08-02T10:46:32.639469: step 1042, loss 0.603457.
Train: 2018-08-02T10:46:32.858168: step 1043, loss 0.54628.
Train: 2018-08-02T10:46:33.076897: step 1044, loss 0.522854.
Train: 2018-08-02T10:46:33.295596: step 1045, loss 0.561654.
Train: 2018-08-02T10:46:33.514295: step 1046, loss 0.579845.
Train: 2018-08-02T10:46:33.732988: step 1047, loss 0.505779.
Train: 2018-08-02T10:46:33.951692: step 1048, loss 0.497425.
Train: 2018-08-02T10:46:34.170385: step 1049, loss 0.579153.
Train: 2018-08-02T10:46:34.389089: step 1050, loss 0.571418.
Test: 2018-08-02T10:46:35.545039: step 1050, loss 0.549731.
Train: 2018-08-02T10:46:35.748116: step 1051, loss 0.570839.
Train: 2018-08-02T10:46:35.966815: step 1052, loss 0.602208.
Train: 2018-08-02T10:46:36.185544: step 1053, loss 0.620174.
Train: 2018-08-02T10:46:36.404246: step 1054, loss 0.496723.
Train: 2018-08-02T10:46:36.622944: step 1055, loss 0.595138.
Train: 2018-08-02T10:46:36.841611: step 1056, loss 0.545996.
Train: 2018-08-02T10:46:37.029067: step 1057, loss 0.51106.
Train: 2018-08-02T10:46:37.247798: step 1058, loss 0.570941.
Train: 2018-08-02T10:46:37.466493: step 1059, loss 0.571019.
Train: 2018-08-02T10:46:37.685192: step 1060, loss 0.545389.
Test: 2018-08-02T10:46:38.841141: step 1060, loss 0.547354.
Train: 2018-08-02T10:46:39.044249: step 1061, loss 0.595701.
Train: 2018-08-02T10:46:39.262917: step 1062, loss 0.537471.
Train: 2018-08-02T10:46:39.481646: step 1063, loss 0.578196.
Train: 2018-08-02T10:46:39.700341: step 1064, loss 0.531487.
Train: 2018-08-02T10:46:39.919042: step 1065, loss 0.57109.
Train: 2018-08-02T10:46:40.137714: step 1066, loss 0.580623.
Train: 2018-08-02T10:46:40.356440: step 1067, loss 0.571366.
Train: 2018-08-02T10:46:40.575141: step 1068, loss 0.537296.
Train: 2018-08-02T10:46:40.793810: step 1069, loss 0.498266.
Train: 2018-08-02T10:46:41.012538: step 1070, loss 0.595978.
Test: 2018-08-02T10:46:42.168487: step 1070, loss 0.549576.
Train: 2018-08-02T10:46:42.387187: step 1071, loss 0.578035.
Train: 2018-08-02T10:46:42.605915: step 1072, loss 0.596726.
Train: 2018-08-02T10:46:42.824614: step 1073, loss 0.578492.
Train: 2018-08-02T10:46:43.043313: step 1074, loss 0.472191.
Train: 2018-08-02T10:46:43.262011: step 1075, loss 0.571357.
Train: 2018-08-02T10:46:43.480710: step 1076, loss 0.572888.
Train: 2018-08-02T10:46:43.699378: step 1077, loss 0.504313.
Train: 2018-08-02T10:46:43.918110: step 1078, loss 0.572941.
Train: 2018-08-02T10:46:44.136807: step 1079, loss 0.62903.
Train: 2018-08-02T10:46:44.355500: step 1080, loss 0.547242.
Test: 2018-08-02T10:46:45.511454: step 1080, loss 0.548168.
Train: 2018-08-02T10:46:45.714531: step 1081, loss 0.538862.
Train: 2018-08-02T10:46:45.917640: step 1082, loss 0.563063.
Train: 2018-08-02T10:46:46.120720: step 1083, loss 0.586909.
Train: 2018-08-02T10:46:46.339416: step 1084, loss 0.611871.
Train: 2018-08-02T10:46:46.558086: step 1085, loss 0.513777.
Train: 2018-08-02T10:46:46.776783: step 1086, loss 0.553917.
Train: 2018-08-02T10:46:46.995515: step 1087, loss 0.553567.
Train: 2018-08-02T10:46:47.214212: step 1088, loss 0.513536.
Train: 2018-08-02T10:46:47.432910: step 1089, loss 0.536804.
Train: 2018-08-02T10:46:47.635981: step 1090, loss 0.553101.
Test: 2018-08-02T10:46:48.791936: step 1090, loss 0.547577.
Train: 2018-08-02T10:46:48.995047: step 1091, loss 0.587494.
Train: 2018-08-02T10:46:49.213743: step 1092, loss 0.505236.
Train: 2018-08-02T10:46:49.432443: step 1093, loss 0.521462.
Train: 2018-08-02T10:46:49.651141: step 1094, loss 0.586045.
Train: 2018-08-02T10:46:49.869842: step 1095, loss 0.547105.
Train: 2018-08-02T10:46:50.088507: step 1096, loss 0.605757.
Train: 2018-08-02T10:46:50.307237: step 1097, loss 0.529233.
Train: 2018-08-02T10:46:50.510315: step 1098, loss 0.570172.
Train: 2018-08-02T10:46:50.729013: step 1099, loss 0.562394.
Train: 2018-08-02T10:46:50.947682: step 1100, loss 0.680616.
Test: 2018-08-02T10:46:52.103661: step 1100, loss 0.548639.
Train: 2018-08-02T10:46:52.884728: step 1101, loss 0.521334.
Train: 2018-08-02T10:46:53.103457: step 1102, loss 0.545585.
Train: 2018-08-02T10:46:53.322156: step 1103, loss 0.554632.
Train: 2018-08-02T10:46:53.525233: step 1104, loss 0.628789.
Train: 2018-08-02T10:46:53.743935: step 1105, loss 0.495535.
Train: 2018-08-02T10:46:53.962601: step 1106, loss 0.570624.
Train: 2018-08-02T10:46:54.181329: step 1107, loss 0.578846.
Train: 2018-08-02T10:46:54.400028: step 1108, loss 0.611089.
Train: 2018-08-02T10:46:54.618730: step 1109, loss 0.586855.
Train: 2018-08-02T10:46:54.837426: step 1110, loss 0.604147.
Test: 2018-08-02T10:46:56.008996: step 1110, loss 0.548945.
Train: 2018-08-02T10:46:56.212104: step 1111, loss 0.587359.
Train: 2018-08-02T10:46:56.430773: step 1112, loss 0.545381.
Train: 2018-08-02T10:46:56.649502: step 1113, loss 0.57159.
Train: 2018-08-02T10:46:56.868171: step 1114, loss 0.586552.
Train: 2018-08-02T10:46:57.086895: step 1115, loss 0.571236.
Train: 2018-08-02T10:46:57.305599: step 1116, loss 0.56271.
Train: 2018-08-02T10:46:57.524304: step 1117, loss 0.642172.
Train: 2018-08-02T10:46:57.742998: step 1118, loss 0.588125.
Train: 2018-08-02T10:46:57.961694: step 1119, loss 0.563498.
Train: 2018-08-02T10:46:58.180393: step 1120, loss 0.594567.
Test: 2018-08-02T10:46:59.336342: step 1120, loss 0.550842.
Train: 2018-08-02T10:46:59.555041: step 1121, loss 0.499595.
Train: 2018-08-02T10:46:59.773740: step 1122, loss 0.571063.
Train: 2018-08-02T10:46:59.992472: step 1123, loss 0.539653.
Train: 2018-08-02T10:47:00.211161: step 1124, loss 0.532685.
Train: 2018-08-02T10:47:00.429867: step 1125, loss 0.602959.
Train: 2018-08-02T10:47:00.648534: step 1126, loss 0.546669.
Train: 2018-08-02T10:47:00.867267: step 1127, loss 0.602831.
Train: 2018-08-02T10:47:01.085932: step 1128, loss 0.508886.
Train: 2018-08-02T10:47:01.304661: step 1129, loss 0.547665.
Train: 2018-08-02T10:47:01.523361: step 1130, loss 0.507985.
Test: 2018-08-02T10:47:02.679309: step 1130, loss 0.549253.
Train: 2018-08-02T10:47:02.882387: step 1131, loss 0.493108.
Train: 2018-08-02T10:47:03.101118: step 1132, loss 0.498885.
Train: 2018-08-02T10:47:03.319815: step 1133, loss 0.515607.
Train: 2018-08-02T10:47:03.538483: step 1134, loss 0.513578.
Train: 2018-08-02T10:47:03.757212: step 1135, loss 0.562749.
Train: 2018-08-02T10:47:03.975909: step 1136, loss 0.530439.
Train: 2018-08-02T10:47:04.194604: step 1137, loss 0.603915.
Train: 2018-08-02T10:47:04.413309: step 1138, loss 0.579766.
Train: 2018-08-02T10:47:04.632007: step 1139, loss 0.53005.
Train: 2018-08-02T10:47:04.850707: step 1140, loss 0.537217.
Test: 2018-08-02T10:47:06.006655: step 1140, loss 0.549059.
Train: 2018-08-02T10:47:06.209763: step 1141, loss 0.469483.
Train: 2018-08-02T10:47:06.428462: step 1142, loss 0.596667.
Train: 2018-08-02T10:47:06.647131: step 1143, loss 0.544469.
Train: 2018-08-02T10:47:06.865862: step 1144, loss 0.562434.
Train: 2018-08-02T10:47:07.084557: step 1145, loss 0.49358.
Train: 2018-08-02T10:47:07.303257: step 1146, loss 0.605041.
Train: 2018-08-02T10:47:07.521957: step 1147, loss 0.554053.
Train: 2018-08-02T10:47:07.740657: step 1148, loss 0.570271.
Train: 2018-08-02T10:47:07.959352: step 1149, loss 0.599086.
Train: 2018-08-02T10:47:08.178052: step 1150, loss 0.580998.
Test: 2018-08-02T10:47:09.334001: step 1150, loss 0.550219.
Train: 2018-08-02T10:47:09.537108: step 1151, loss 0.597613.
Train: 2018-08-02T10:47:09.740189: step 1152, loss 0.537114.
Train: 2018-08-02T10:47:09.974522: step 1153, loss 0.685049.
Train: 2018-08-02T10:47:10.193199: step 1154, loss 0.626036.
Train: 2018-08-02T10:47:10.411873: step 1155, loss 0.536812.
Train: 2018-08-02T10:47:10.630596: step 1156, loss 0.630568.
Train: 2018-08-02T10:47:10.849302: step 1157, loss 0.562452.
Train: 2018-08-02T10:47:11.067969: step 1158, loss 0.504041.
Train: 2018-08-02T10:47:11.286699: step 1159, loss 0.512241.
Train: 2018-08-02T10:47:11.505367: step 1160, loss 0.604093.
Test: 2018-08-02T10:47:12.676968: step 1160, loss 0.548584.
Train: 2018-08-02T10:47:12.880076: step 1161, loss 0.587472.
Train: 2018-08-02T10:47:13.098777: step 1162, loss 0.554652.
Train: 2018-08-02T10:47:13.317444: step 1163, loss 0.579046.
Train: 2018-08-02T10:47:13.536142: step 1164, loss 0.587028.
Train: 2018-08-02T10:47:13.754871: step 1165, loss 0.530431.
Train: 2018-08-02T10:47:13.973539: step 1166, loss 0.49726.
Train: 2018-08-02T10:47:14.192270: step 1167, loss 0.522267.
Train: 2018-08-02T10:47:14.410968: step 1168, loss 0.571383.
Train: 2018-08-02T10:47:14.625586: step 1169, loss 0.506261.
Train: 2018-08-02T10:47:14.844320: step 1170, loss 0.603226.
Test: 2018-08-02T10:47:16.000264: step 1170, loss 0.54917.
Train: 2018-08-02T10:47:16.203342: step 1171, loss 0.6115.
Train: 2018-08-02T10:47:16.422071: step 1172, loss 0.563059.
Train: 2018-08-02T10:47:16.640770: step 1173, loss 0.64351.
Train: 2018-08-02T10:47:16.859439: step 1174, loss 0.571362.
Train: 2018-08-02T10:47:17.078162: step 1175, loss 0.531104.
Train: 2018-08-02T10:47:17.296867: step 1176, loss 0.627087.
Train: 2018-08-02T10:47:17.515566: step 1177, loss 0.610899.
Train: 2018-08-02T10:47:17.734264: step 1178, loss 0.515325.
Train: 2018-08-02T10:47:17.952965: step 1179, loss 0.578343.
Train: 2018-08-02T10:47:18.171632: step 1180, loss 0.523229.
Test: 2018-08-02T10:47:19.327610: step 1180, loss 0.550094.
Train: 2018-08-02T10:47:19.546340: step 1181, loss 0.547079.
Train: 2018-08-02T10:47:19.765038: step 1182, loss 0.554649.
Train: 2018-08-02T10:47:19.983737: step 1183, loss 0.586634.
Train: 2018-08-02T10:47:20.202436: step 1184, loss 0.539602.
Train: 2018-08-02T10:47:20.421134: step 1185, loss 0.539395.
Train: 2018-08-02T10:47:20.639833: step 1186, loss 0.602913.
Train: 2018-08-02T10:47:20.858503: step 1187, loss 0.538841.
Train: 2018-08-02T10:47:21.077229: step 1188, loss 0.49073.
Train: 2018-08-02T10:47:21.295923: step 1189, loss 0.54689.
Train: 2018-08-02T10:47:21.514628: step 1190, loss 0.498115.
Test: 2018-08-02T10:47:22.670577: step 1190, loss 0.548524.
Train: 2018-08-02T10:47:22.873656: step 1191, loss 0.514108.
Train: 2018-08-02T10:47:23.092383: step 1192, loss 0.627901.
Train: 2018-08-02T10:47:23.311052: step 1193, loss 0.54658.
Train: 2018-08-02T10:47:23.529782: step 1194, loss 0.553712.
Train: 2018-08-02T10:47:23.748481: step 1195, loss 0.594843.
Train: 2018-08-02T10:47:23.967148: step 1196, loss 0.562469.
Train: 2018-08-02T10:47:24.185848: step 1197, loss 0.570871.
Train: 2018-08-02T10:47:24.404547: step 1198, loss 0.63688.
Train: 2018-08-02T10:47:24.623275: step 1199, loss 0.569641.
Train: 2018-08-02T10:47:24.841973: step 1200, loss 0.562176.
Test: 2018-08-02T10:47:25.997924: step 1200, loss 0.548736.
Train: 2018-08-02T10:47:26.825879: step 1201, loss 0.595548.
Train: 2018-08-02T10:47:27.044583: step 1202, loss 0.561826.
Train: 2018-08-02T10:47:27.263252: step 1203, loss 0.603159.
Train: 2018-08-02T10:47:27.497573: step 1204, loss 0.472512.
Train: 2018-08-02T10:47:27.700680: step 1205, loss 0.505093.
Train: 2018-08-02T10:47:27.919379: step 1206, loss 0.627787.
Train: 2018-08-02T10:47:28.138048: step 1207, loss 0.594748.
Train: 2018-08-02T10:47:28.309883: step 1208, loss 0.545853.
Train: 2018-08-02T10:47:28.528611: step 1209, loss 0.5782.
Train: 2018-08-02T10:47:28.747310: step 1210, loss 0.553965.
Test: 2018-08-02T10:47:29.903259: step 1210, loss 0.547907.
Train: 2018-08-02T10:47:30.106367: step 1211, loss 0.603639.
Train: 2018-08-02T10:47:30.325065: step 1212, loss 0.594312.
Train: 2018-08-02T10:47:30.543764: step 1213, loss 0.538127.
Train: 2018-08-02T10:47:30.762462: step 1214, loss 0.595169.
Train: 2018-08-02T10:47:30.981132: step 1215, loss 0.521846.
Train: 2018-08-02T10:47:31.199860: step 1216, loss 0.55502.
Train: 2018-08-02T10:47:31.418528: step 1217, loss 0.489612.
Train: 2018-08-02T10:47:31.637227: step 1218, loss 0.506237.
Train: 2018-08-02T10:47:31.855951: step 1219, loss 0.514582.
Train: 2018-08-02T10:47:32.074655: step 1220, loss 0.562527.
Test: 2018-08-02T10:47:33.230604: step 1220, loss 0.548012.
Train: 2018-08-02T10:47:33.433712: step 1221, loss 0.602049.
Train: 2018-08-02T10:47:33.652381: step 1222, loss 0.579927.
Train: 2018-08-02T10:47:33.871110: step 1223, loss 0.538722.
Train: 2018-08-02T10:47:34.089808: step 1224, loss 0.587131.
Train: 2018-08-02T10:47:34.308507: step 1225, loss 0.595527.
Train: 2018-08-02T10:47:34.527176: step 1226, loss 0.537185.
Train: 2018-08-02T10:47:34.745875: step 1227, loss 0.595244.
Train: 2018-08-02T10:47:34.964603: step 1228, loss 0.554703.
Train: 2018-08-02T10:47:35.198925: step 1229, loss 0.529144.
Train: 2018-08-02T10:47:35.402001: step 1230, loss 0.547123.
Test: 2018-08-02T10:47:36.557950: step 1230, loss 0.54828.
Train: 2018-08-02T10:47:36.761029: step 1231, loss 0.562435.
Train: 2018-08-02T10:47:36.979727: step 1232, loss 0.562851.
Train: 2018-08-02T10:47:37.182834: step 1233, loss 0.537357.
Train: 2018-08-02T10:47:37.401533: step 1234, loss 0.554411.
Train: 2018-08-02T10:47:37.620232: step 1235, loss 0.603938.
Train: 2018-08-02T10:47:37.838900: step 1236, loss 0.487655.
Train: 2018-08-02T10:47:38.057630: step 1237, loss 0.571026.
Train: 2018-08-02T10:47:38.276298: step 1238, loss 0.505188.
Train: 2018-08-02T10:47:38.495027: step 1239, loss 0.522246.
Train: 2018-08-02T10:47:38.713695: step 1240, loss 0.519924.
Test: 2018-08-02T10:47:39.869675: step 1240, loss 0.550132.
Train: 2018-08-02T10:47:40.088374: step 1241, loss 0.54609.
Train: 2018-08-02T10:47:40.291482: step 1242, loss 0.538026.
Train: 2018-08-02T10:47:40.510179: step 1243, loss 0.578788.
Train: 2018-08-02T10:47:40.728873: step 1244, loss 0.521424.
Train: 2018-08-02T10:47:40.947578: step 1245, loss 0.578898.
Train: 2018-08-02T10:47:41.166279: step 1246, loss 0.569945.
Train: 2018-08-02T10:47:41.384976: step 1247, loss 0.571695.
Train: 2018-08-02T10:47:41.603674: step 1248, loss 0.603742.
Train: 2018-08-02T10:47:41.822367: step 1249, loss 0.528914.
Train: 2018-08-02T10:47:42.041071: step 1250, loss 0.553789.
Test: 2018-08-02T10:47:43.197021: step 1250, loss 0.5481.
Train: 2018-08-02T10:47:43.415749: step 1251, loss 0.57114.
Train: 2018-08-02T10:47:43.634449: step 1252, loss 0.494155.
Train: 2018-08-02T10:47:43.853118: step 1253, loss 0.519345.
Train: 2018-08-02T10:47:44.071846: step 1254, loss 0.537832.
Train: 2018-08-02T10:47:44.290539: step 1255, loss 0.620478.
Train: 2018-08-02T10:47:44.509243: step 1256, loss 0.561216.
Train: 2018-08-02T10:47:44.727938: step 1257, loss 0.578887.
Train: 2018-08-02T10:47:44.946641: step 1258, loss 0.502405.
Train: 2018-08-02T10:47:45.165340: step 1259, loss 0.54454.
Train: 2018-08-02T10:47:45.384033: step 1260, loss 0.5876.
Test: 2018-08-02T10:47:46.539988: step 1260, loss 0.548409.
Train: 2018-08-02T10:47:46.743096: step 1261, loss 0.588636.
Train: 2018-08-02T10:47:46.946172: step 1262, loss 0.53671.
Train: 2018-08-02T10:47:47.164867: step 1263, loss 0.528502.
Train: 2018-08-02T10:47:47.383540: step 1264, loss 0.578585.
Train: 2018-08-02T10:47:47.602269: step 1265, loss 0.519981.
Train: 2018-08-02T10:47:47.820968: step 1266, loss 0.570832.
Train: 2018-08-02T10:47:48.039668: step 1267, loss 0.545489.
Train: 2018-08-02T10:47:48.258360: step 1268, loss 0.578361.
Train: 2018-08-02T10:47:48.477034: step 1269, loss 0.544878.
Train: 2018-08-02T10:47:48.695763: step 1270, loss 0.595923.
Test: 2018-08-02T10:47:49.851712: step 1270, loss 0.5481.
Train: 2018-08-02T10:47:50.070411: step 1271, loss 0.54438.
Train: 2018-08-02T10:47:50.289141: step 1272, loss 0.605954.
Train: 2018-08-02T10:47:50.492218: step 1273, loss 0.588156.
Train: 2018-08-02T10:47:50.710921: step 1274, loss 0.587477.
Train: 2018-08-02T10:47:50.929615: step 1275, loss 0.620896.
Train: 2018-08-02T10:47:51.148284: step 1276, loss 0.595986.
Train: 2018-08-02T10:47:51.367013: step 1277, loss 0.561805.
Train: 2018-08-02T10:47:51.585716: step 1278, loss 0.529287.
Train: 2018-08-02T10:47:51.804410: step 1279, loss 0.586891.
Train: 2018-08-02T10:47:52.023109: step 1280, loss 0.58686.
Test: 2018-08-02T10:47:53.179058: step 1280, loss 0.54963.
Train: 2018-08-02T10:47:53.382166: step 1281, loss 0.587295.
Train: 2018-08-02T10:47:53.600836: step 1282, loss 0.611477.
Train: 2018-08-02T10:47:53.819564: step 1283, loss 0.514158.
Train: 2018-08-02T10:47:54.038232: step 1284, loss 0.539299.
Train: 2018-08-02T10:47:54.256961: step 1285, loss 0.507082.
Train: 2018-08-02T10:47:54.475654: step 1286, loss 0.522028.
Train: 2018-08-02T10:47:54.694358: step 1287, loss 0.569869.
Train: 2018-08-02T10:47:54.913057: step 1288, loss 0.586981.
Train: 2018-08-02T10:47:55.131756: step 1289, loss 0.522755.
Train: 2018-08-02T10:47:55.350424: step 1290, loss 0.570809.
Test: 2018-08-02T10:47:56.506404: step 1290, loss 0.549645.
Train: 2018-08-02T10:47:56.709481: step 1291, loss 0.563677.
Train: 2018-08-02T10:47:56.912589: step 1292, loss 0.522977.
Train: 2018-08-02T10:47:57.131288: step 1293, loss 0.546628.
Train: 2018-08-02T10:47:57.349986: step 1294, loss 0.522151.
Train: 2018-08-02T10:47:57.568655: step 1295, loss 0.52319.
Train: 2018-08-02T10:47:57.787384: step 1296, loss 0.547295.
Train: 2018-08-02T10:47:58.006082: step 1297, loss 0.53873.
Train: 2018-08-02T10:47:58.224751: step 1298, loss 0.587178.
Train: 2018-08-02T10:47:58.443480: step 1299, loss 0.56333.
Train: 2018-08-02T10:47:58.662179: step 1300, loss 0.552865.
Test: 2018-08-02T10:47:59.818129: step 1300, loss 0.550365.
Train: 2018-08-02T10:48:00.646090: step 1301, loss 0.562048.
Train: 2018-08-02T10:48:00.864788: step 1302, loss 0.585993.
Train: 2018-08-02T10:48:01.083486: step 1303, loss 0.521977.
Train: 2018-08-02T10:48:01.302157: step 1304, loss 0.629265.
Train: 2018-08-02T10:48:01.520880: step 1305, loss 0.512643.
Train: 2018-08-02T10:48:01.739553: step 1306, loss 0.59687.
Train: 2018-08-02T10:48:01.958287: step 1307, loss 0.588004.
Train: 2018-08-02T10:48:02.176981: step 1308, loss 0.562914.
Train: 2018-08-02T10:48:02.395680: step 1309, loss 0.579814.
Train: 2018-08-02T10:48:02.614379: step 1310, loss 0.505263.
Test: 2018-08-02T10:48:03.770328: step 1310, loss 0.5482.
Train: 2018-08-02T10:48:03.973432: step 1311, loss 0.587547.
Train: 2018-08-02T10:48:04.192138: step 1312, loss 0.570786.
Train: 2018-08-02T10:48:04.410833: step 1313, loss 0.587605.
Train: 2018-08-02T10:48:04.629532: step 1314, loss 0.604229.
Train: 2018-08-02T10:48:04.848231: step 1315, loss 0.628151.
Train: 2018-08-02T10:48:05.066933: step 1316, loss 0.554122.
Train: 2018-08-02T10:48:05.285629: step 1317, loss 0.571461.
Train: 2018-08-02T10:48:05.504297: step 1318, loss 0.521353.
Train: 2018-08-02T10:48:05.723026: step 1319, loss 0.529699.
Train: 2018-08-02T10:48:05.941728: step 1320, loss 0.578528.
Test: 2018-08-02T10:48:07.097674: step 1320, loss 0.548934.
Train: 2018-08-02T10:48:07.300776: step 1321, loss 0.554473.
Train: 2018-08-02T10:48:07.519452: step 1322, loss 0.594926.
Train: 2018-08-02T10:48:07.738148: step 1323, loss 0.522733.
Train: 2018-08-02T10:48:07.956883: step 1324, loss 0.570622.
Train: 2018-08-02T10:48:08.175579: step 1325, loss 0.587494.
Train: 2018-08-02T10:48:08.394276: step 1326, loss 0.497924.
Train: 2018-08-02T10:48:08.612968: step 1327, loss 0.586234.
Train: 2018-08-02T10:48:08.831678: step 1328, loss 0.554389.
Train: 2018-08-02T10:48:09.050371: step 1329, loss 0.522993.
Train: 2018-08-02T10:48:09.269040: step 1330, loss 0.635882.
Test: 2018-08-02T10:48:10.440640: step 1330, loss 0.549154.
Train: 2018-08-02T10:48:10.643752: step 1331, loss 0.610942.
Train: 2018-08-02T10:48:10.862448: step 1332, loss 0.603389.
Train: 2018-08-02T10:48:11.081151: step 1333, loss 0.56289.
Train: 2018-08-02T10:48:11.299815: step 1334, loss 0.61084.
Train: 2018-08-02T10:48:11.518544: step 1335, loss 0.61867.
Train: 2018-08-02T10:48:11.721621: step 1336, loss 0.61025.
Train: 2018-08-02T10:48:11.940291: step 1337, loss 0.571114.
Train: 2018-08-02T10:48:12.159019: step 1338, loss 0.5633.
Train: 2018-08-02T10:48:12.377717: step 1339, loss 0.555825.
Train: 2018-08-02T10:48:12.596416: step 1340, loss 0.492649.
Test: 2018-08-02T10:48:13.752365: step 1340, loss 0.54918.
Train: 2018-08-02T10:48:13.955444: step 1341, loss 0.594879.
Train: 2018-08-02T10:48:14.158520: step 1342, loss 0.523844.
Train: 2018-08-02T10:48:14.377250: step 1343, loss 0.562619.
Train: 2018-08-02T10:48:14.595942: step 1344, loss 0.602249.
Train: 2018-08-02T10:48:14.814647: step 1345, loss 0.570538.
Train: 2018-08-02T10:48:15.033345: step 1346, loss 0.570789.
Train: 2018-08-02T10:48:15.252044: step 1347, loss 0.586679.
Train: 2018-08-02T10:48:15.470713: step 1348, loss 0.547438.
Train: 2018-08-02T10:48:15.689442: step 1349, loss 0.548135.
Train: 2018-08-02T10:48:15.908135: step 1350, loss 0.618672.
Test: 2018-08-02T10:48:17.064090: step 1350, loss 0.550319.
Train: 2018-08-02T10:48:17.282819: step 1351, loss 0.563491.
Train: 2018-08-02T10:48:17.501487: step 1352, loss 0.516341.
Train: 2018-08-02T10:48:17.720211: step 1353, loss 0.602208.
Train: 2018-08-02T10:48:17.938916: step 1354, loss 0.562972.
Train: 2018-08-02T10:48:18.157614: step 1355, loss 0.531797.
Train: 2018-08-02T10:48:18.391904: step 1356, loss 0.55565.
Train: 2018-08-02T10:48:18.594981: step 1357, loss 0.554858.
Train: 2018-08-02T10:48:18.813711: step 1358, loss 0.578595.
Train: 2018-08-02T10:48:19.001161: step 1359, loss 0.631026.
Train: 2018-08-02T10:48:19.219836: step 1360, loss 0.531834.
Test: 2018-08-02T10:48:20.391436: step 1360, loss 0.549842.
Train: 2018-08-02T10:48:20.610160: step 1361, loss 0.594169.
Train: 2018-08-02T10:48:20.844480: step 1362, loss 0.555937.
Train: 2018-08-02T10:48:21.063183: step 1363, loss 0.554846.
Train: 2018-08-02T10:48:21.281852: step 1364, loss 0.523443.
Train: 2018-08-02T10:48:21.500552: step 1365, loss 0.570947.
Train: 2018-08-02T10:48:21.719249: step 1366, loss 0.563296.
Train: 2018-08-02T10:48:21.937978: step 1367, loss 0.586953.
Train: 2018-08-02T10:48:22.156678: step 1368, loss 0.61844.
Train: 2018-08-02T10:48:22.390970: step 1369, loss 0.626349.
Train: 2018-08-02T10:48:22.609666: step 1370, loss 0.619054.
Test: 2018-08-02T10:48:23.781267: step 1370, loss 0.550154.
Train: 2018-08-02T10:48:23.999997: step 1371, loss 0.603208.
Train: 2018-08-02T10:48:24.218665: step 1372, loss 0.555036.
Train: 2018-08-02T10:48:24.437388: step 1373, loss 0.53946.
Train: 2018-08-02T10:48:24.656093: step 1374, loss 0.500356.
Train: 2018-08-02T10:48:24.874791: step 1375, loss 0.484634.
Train: 2018-08-02T10:48:25.093490: step 1376, loss 0.547144.
Train: 2018-08-02T10:48:25.312188: step 1377, loss 0.617387.
Train: 2018-08-02T10:48:25.530888: step 1378, loss 0.626439.
Train: 2018-08-02T10:48:25.749581: step 1379, loss 0.547574.
Train: 2018-08-02T10:48:25.968279: step 1380, loss 0.546849.
Test: 2018-08-02T10:48:27.139856: step 1380, loss 0.550526.
Train: 2018-08-02T10:48:27.405444: step 1381, loss 0.547426.
Train: 2018-08-02T10:48:27.624116: step 1382, loss 0.602539.
Train: 2018-08-02T10:48:27.842815: step 1383, loss 0.579437.
Train: 2018-08-02T10:48:28.061545: step 1384, loss 0.634347.
Train: 2018-08-02T10:48:28.280244: step 1385, loss 0.507248.
Train: 2018-08-02T10:48:28.498946: step 1386, loss 0.578828.
Train: 2018-08-02T10:48:28.717641: step 1387, loss 0.539424.
Train: 2018-08-02T10:48:28.936340: step 1388, loss 0.562902.
Train: 2018-08-02T10:48:29.155038: step 1389, loss 0.579204.
Train: 2018-08-02T10:48:29.373707: step 1390, loss 0.594817.
Test: 2018-08-02T10:48:30.529686: step 1390, loss 0.551.
Train: 2018-08-02T10:48:30.732788: step 1391, loss 0.56292.
Train: 2018-08-02T10:48:30.951494: step 1392, loss 0.626217.
Train: 2018-08-02T10:48:31.170161: step 1393, loss 0.538836.
Train: 2018-08-02T10:48:31.388861: step 1394, loss 0.539821.
Train: 2018-08-02T10:48:31.607560: step 1395, loss 0.55519.
Train: 2018-08-02T10:48:31.841906: step 1396, loss 0.547174.
Train: 2018-08-02T10:48:32.060578: step 1397, loss 0.516127.
Train: 2018-08-02T10:48:32.279277: step 1398, loss 0.594578.
Train: 2018-08-02T10:48:32.498001: step 1399, loss 0.546063.
Train: 2018-08-02T10:48:32.716675: step 1400, loss 0.555205.
Test: 2018-08-02T10:48:33.872654: step 1400, loss 0.548757.
Train: 2018-08-02T10:48:34.716236: step 1401, loss 0.530943.
Train: 2018-08-02T10:48:34.934936: step 1402, loss 0.515173.
Train: 2018-08-02T10:48:35.153634: step 1403, loss 0.5464.
Train: 2018-08-02T10:48:35.372302: step 1404, loss 0.554892.
Train: 2018-08-02T10:48:35.591031: step 1405, loss 0.538844.
Train: 2018-08-02T10:48:35.809725: step 1406, loss 0.546713.
Train: 2018-08-02T10:48:36.028431: step 1407, loss 0.489396.
Train: 2018-08-02T10:48:36.247128: step 1408, loss 0.537534.
Train: 2018-08-02T10:48:36.465826: step 1409, loss 0.56995.
Train: 2018-08-02T10:48:36.684525: step 1410, loss 0.578973.
Test: 2018-08-02T10:48:37.840474: step 1410, loss 0.54864.
Train: 2018-08-02T10:48:38.059174: step 1411, loss 0.520026.
Train: 2018-08-02T10:48:38.277902: step 1412, loss 0.478529.
Train: 2018-08-02T10:48:38.496601: step 1413, loss 0.536869.
Train: 2018-08-02T10:48:38.715300: step 1414, loss 0.579359.
Train: 2018-08-02T10:48:38.934001: step 1415, loss 0.520145.
Train: 2018-08-02T10:48:39.152697: step 1416, loss 0.518632.
Train: 2018-08-02T10:48:39.371397: step 1417, loss 0.578966.
Train: 2018-08-02T10:48:39.590095: step 1418, loss 0.535894.
Train: 2018-08-02T10:48:39.808789: step 1419, loss 0.624487.
Train: 2018-08-02T10:48:40.027493: step 1420, loss 0.570755.
Test: 2018-08-02T10:48:41.199063: step 1420, loss 0.549194.
Train: 2018-08-02T10:48:41.402171: step 1421, loss 0.519593.
Train: 2018-08-02T10:48:41.620864: step 1422, loss 0.605847.
Train: 2018-08-02T10:48:41.839569: step 1423, loss 0.563107.
Train: 2018-08-02T10:48:42.058267: step 1424, loss 0.571063.
Train: 2018-08-02T10:48:42.276936: step 1425, loss 0.570954.
Train: 2018-08-02T10:48:42.495664: step 1426, loss 0.536002.
Train: 2018-08-02T10:48:42.714357: step 1427, loss 0.545241.
Train: 2018-08-02T10:48:42.933062: step 1428, loss 0.569872.
Train: 2018-08-02T10:48:43.151761: step 1429, loss 0.571283.
Train: 2018-08-02T10:48:43.370460: step 1430, loss 0.52722.
Test: 2018-08-02T10:48:44.542030: step 1430, loss 0.548303.
Train: 2018-08-02T10:48:44.745107: step 1431, loss 0.458846.
Train: 2018-08-02T10:48:44.963836: step 1432, loss 0.544542.
Train: 2018-08-02T10:48:45.182538: step 1433, loss 0.500381.
Train: 2018-08-02T10:48:45.401205: step 1434, loss 0.54543.
Train: 2018-08-02T10:48:45.619941: step 1435, loss 0.555223.
Train: 2018-08-02T10:48:45.838632: step 1436, loss 0.501207.
Train: 2018-08-02T10:48:46.057330: step 1437, loss 0.606172.
Train: 2018-08-02T10:48:46.276032: step 1438, loss 0.501009.
Train: 2018-08-02T10:48:46.494730: step 1439, loss 0.640969.
Train: 2018-08-02T10:48:46.713397: step 1440, loss 0.474008.
Test: 2018-08-02T10:48:47.869376: step 1440, loss 0.548936.
Train: 2018-08-02T10:48:48.088105: step 1441, loss 0.571959.
Train: 2018-08-02T10:48:48.306799: step 1442, loss 0.615287.
Train: 2018-08-02T10:48:48.525503: step 1443, loss 0.597607.
Train: 2018-08-02T10:48:48.744171: step 1444, loss 0.579253.
Train: 2018-08-02T10:48:48.962900: step 1445, loss 0.571651.
Train: 2018-08-02T10:48:49.181599: step 1446, loss 0.519014.
Train: 2018-08-02T10:48:49.400292: step 1447, loss 0.537336.
Train: 2018-08-02T10:48:49.618967: step 1448, loss 0.588733.
Train: 2018-08-02T10:48:49.837666: step 1449, loss 0.562021.
Train: 2018-08-02T10:48:50.056397: step 1450, loss 0.62307.
Test: 2018-08-02T10:48:51.212343: step 1450, loss 0.546888.
Train: 2018-08-02T10:48:51.415421: step 1451, loss 0.588007.
Train: 2018-08-02T10:48:51.634120: step 1452, loss 0.493686.
Train: 2018-08-02T10:48:51.852843: step 1453, loss 0.613462.
Train: 2018-08-02T10:48:52.071547: step 1454, loss 0.562311.
Train: 2018-08-02T10:48:52.290246: step 1455, loss 0.629237.
Train: 2018-08-02T10:48:52.508914: step 1456, loss 0.55475.
Train: 2018-08-02T10:48:52.727645: step 1457, loss 0.462651.
Train: 2018-08-02T10:48:52.946341: step 1458, loss 0.561592.
Train: 2018-08-02T10:48:53.165041: step 1459, loss 0.612125.
Train: 2018-08-02T10:48:53.383710: step 1460, loss 0.553683.
Test: 2018-08-02T10:48:54.539689: step 1460, loss 0.548982.
Train: 2018-08-02T10:48:54.758387: step 1461, loss 0.562905.
Train: 2018-08-02T10:48:54.977086: step 1462, loss 0.513396.
Train: 2018-08-02T10:48:55.195820: step 1463, loss 0.570548.
Train: 2018-08-02T10:48:55.414485: step 1464, loss 0.545779.
Train: 2018-08-02T10:48:55.633213: step 1465, loss 0.594924.
Train: 2018-08-02T10:48:55.851912: step 1466, loss 0.562509.
Train: 2018-08-02T10:48:56.070610: step 1467, loss 0.546736.
Train: 2018-08-02T10:48:56.289279: step 1468, loss 0.586691.
Train: 2018-08-02T10:48:56.508003: step 1469, loss 0.603077.
Train: 2018-08-02T10:48:56.726677: step 1470, loss 0.538617.
Test: 2018-08-02T10:48:57.898277: step 1470, loss 0.549616.
Train: 2018-08-02T10:48:58.101389: step 1471, loss 0.553686.
Train: 2018-08-02T10:48:58.320078: step 1472, loss 0.546668.
Train: 2018-08-02T10:48:58.538783: step 1473, loss 0.59548.
Train: 2018-08-02T10:48:58.757482: step 1474, loss 0.578614.
Train: 2018-08-02T10:48:58.976151: step 1475, loss 0.651589.
Train: 2018-08-02T10:48:59.194879: step 1476, loss 0.611433.
Train: 2018-08-02T10:48:59.413549: step 1477, loss 0.586908.
Train: 2018-08-02T10:48:59.632247: step 1478, loss 0.562754.
Train: 2018-08-02T10:48:59.850970: step 1479, loss 0.523535.
Train: 2018-08-02T10:49:00.069645: step 1480, loss 0.483277.
Test: 2018-08-02T10:49:01.241245: step 1480, loss 0.548939.
Train: 2018-08-02T10:49:01.444352: step 1481, loss 0.563252.
Train: 2018-08-02T10:49:01.647436: step 1482, loss 0.578842.
Train: 2018-08-02T10:49:01.866129: step 1483, loss 0.650362.
Train: 2018-08-02T10:49:02.084822: step 1484, loss 0.539443.
Train: 2018-08-02T10:49:02.303526: step 1485, loss 0.634376.
Train: 2018-08-02T10:49:02.522225: step 1486, loss 0.523528.
Train: 2018-08-02T10:49:02.740924: step 1487, loss 0.570811.
Train: 2018-08-02T10:49:02.959593: step 1488, loss 0.547127.
Train: 2018-08-02T10:49:03.178321: step 1489, loss 0.656829.
Train: 2018-08-02T10:49:03.396989: step 1490, loss 0.555613.
Test: 2018-08-02T10:49:04.552969: step 1490, loss 0.548835.
Train: 2018-08-02T10:49:04.756046: step 1491, loss 0.563014.
Train: 2018-08-02T10:49:04.974776: step 1492, loss 0.571362.
Train: 2018-08-02T10:49:05.193444: step 1493, loss 0.571357.
Train: 2018-08-02T10:49:05.412173: step 1494, loss 0.633224.
Train: 2018-08-02T10:49:05.630872: step 1495, loss 0.516456.
Train: 2018-08-02T10:49:05.849571: step 1496, loss 0.564164.
Train: 2018-08-02T10:49:06.068274: step 1497, loss 0.508877.
Train: 2018-08-02T10:49:06.286968: step 1498, loss 0.516859.
Train: 2018-08-02T10:49:06.505667: step 1499, loss 0.555496.
Train: 2018-08-02T10:49:06.724365: step 1500, loss 0.578902.
Test: 2018-08-02T10:49:07.880315: step 1500, loss 0.549545.
Train: 2018-08-02T10:49:08.739519: step 1501, loss 0.609514.
Train: 2018-08-02T10:49:08.958187: step 1502, loss 0.538958.
Train: 2018-08-02T10:49:09.176916: step 1503, loss 0.563773.
Train: 2018-08-02T10:49:09.395586: step 1504, loss 0.626249.
Train: 2018-08-02T10:49:09.614314: step 1505, loss 0.492557.
Train: 2018-08-02T10:49:09.832986: step 1506, loss 0.538725.
Train: 2018-08-02T10:49:10.051711: step 1507, loss 0.594903.
Train: 2018-08-02T10:49:10.270379: step 1508, loss 0.539623.
Train: 2018-08-02T10:49:10.489109: step 1509, loss 0.578868.
Train: 2018-08-02T10:49:10.676565: step 1510, loss 0.648903.
Test: 2018-08-02T10:49:11.832515: step 1510, loss 0.549407.
Train: 2018-08-02T10:49:12.051237: step 1511, loss 0.53134.
Train: 2018-08-02T10:49:12.269911: step 1512, loss 0.626392.
Train: 2018-08-02T10:49:12.488641: step 1513, loss 0.483203.
Train: 2018-08-02T10:49:12.707340: step 1514, loss 0.587118.
Train: 2018-08-02T10:49:12.926039: step 1515, loss 0.562507.
Train: 2018-08-02T10:49:13.144738: step 1516, loss 0.59514.
Train: 2018-08-02T10:49:13.363436: step 1517, loss 0.490963.
Train: 2018-08-02T10:49:13.582134: step 1518, loss 0.586417.
Train: 2018-08-02T10:49:13.800803: step 1519, loss 0.547286.
Train: 2018-08-02T10:49:14.019532: step 1520, loss 0.602281.
Test: 2018-08-02T10:49:15.175482: step 1520, loss 0.548089.
Train: 2018-08-02T10:49:15.394211: step 1521, loss 0.594172.
Train: 2018-08-02T10:49:15.612904: step 1522, loss 0.507262.
Train: 2018-08-02T10:49:15.847200: step 1523, loss 0.562269.
Train: 2018-08-02T10:49:16.073410: step 1524, loss 0.691841.
Train: 2018-08-02T10:49:16.292138: step 1525, loss 0.5713.
Train: 2018-08-02T10:49:16.510838: step 1526, loss 0.562217.
Train: 2018-08-02T10:49:16.729506: step 1527, loss 0.578591.
Train: 2018-08-02T10:49:16.948238: step 1528, loss 0.570535.
Train: 2018-08-02T10:49:17.166904: step 1529, loss 0.539301.
Train: 2018-08-02T10:49:17.385602: step 1530, loss 0.571224.
Test: 2018-08-02T10:49:18.557204: step 1530, loss 0.549976.
Train: 2018-08-02T10:49:18.760281: step 1531, loss 0.594372.
Train: 2018-08-02T10:49:18.978979: step 1532, loss 0.539437.
Train: 2018-08-02T10:49:19.197710: step 1533, loss 0.538833.
Train: 2018-08-02T10:49:19.416407: step 1534, loss 0.586939.
Train: 2018-08-02T10:49:19.635106: step 1535, loss 0.618316.
Train: 2018-08-02T10:49:19.853775: step 1536, loss 0.562437.
Train: 2018-08-02T10:49:20.072474: step 1537, loss 0.563012.
Train: 2018-08-02T10:49:20.291202: step 1538, loss 0.492056.
Train: 2018-08-02T10:49:20.509901: step 1539, loss 0.586711.
Train: 2018-08-02T10:49:20.728601: step 1540, loss 0.507384.
Test: 2018-08-02T10:49:21.884549: step 1540, loss 0.549818.
Train: 2018-08-02T10:49:22.087627: step 1541, loss 0.53969.
Train: 2018-08-02T10:49:22.290734: step 1542, loss 0.466952.
Train: 2018-08-02T10:49:22.509434: step 1543, loss 0.554742.
Train: 2018-08-02T10:49:22.728133: step 1544, loss 0.578777.
Train: 2018-08-02T10:49:22.946800: step 1545, loss 0.619241.
Train: 2018-08-02T10:49:23.165532: step 1546, loss 0.586842.
Train: 2018-08-02T10:49:23.384231: step 1547, loss 0.571872.
Train: 2018-08-02T10:49:23.602927: step 1548, loss 0.58816.
Train: 2018-08-02T10:49:23.821596: step 1549, loss 0.530021.
Train: 2018-08-02T10:49:24.040324: step 1550, loss 0.619383.
Test: 2018-08-02T10:49:25.196273: step 1550, loss 0.548658.
Train: 2018-08-02T10:49:25.399351: step 1551, loss 0.578016.
Train: 2018-08-02T10:49:25.602458: step 1552, loss 0.55437.
Train: 2018-08-02T10:49:25.821129: step 1553, loss 0.546381.
Train: 2018-08-02T10:49:26.039826: step 1554, loss 0.619927.
Train: 2018-08-02T10:49:26.258550: step 1555, loss 0.55493.
Train: 2018-08-02T10:49:26.477224: step 1556, loss 0.603087.
Train: 2018-08-02T10:49:26.695947: step 1557, loss 0.547435.
Train: 2018-08-02T10:49:26.914651: step 1558, loss 0.579038.
Train: 2018-08-02T10:49:27.133349: step 1559, loss 0.635648.
Train: 2018-08-02T10:49:27.352049: step 1560, loss 0.554685.
Test: 2018-08-02T10:49:28.523619: step 1560, loss 0.548919.
Train: 2018-08-02T10:49:28.726698: step 1561, loss 0.490577.
Train: 2018-08-02T10:49:28.945395: step 1562, loss 0.546788.
Train: 2018-08-02T10:49:29.164094: step 1563, loss 0.507476.
Train: 2018-08-02T10:49:29.382793: step 1564, loss 0.498581.
Train: 2018-08-02T10:49:29.601522: step 1565, loss 0.514731.
Train: 2018-08-02T10:49:29.820223: step 1566, loss 0.555014.
Train: 2018-08-02T10:49:30.038920: step 1567, loss 0.547014.
Train: 2018-08-02T10:49:30.257619: step 1568, loss 0.554769.
Train: 2018-08-02T10:49:30.476286: step 1569, loss 0.546414.
Train: 2018-08-02T10:49:30.695018: step 1570, loss 0.497399.
Test: 2018-08-02T10:49:31.866586: step 1570, loss 0.547207.
Train: 2018-08-02T10:49:32.069696: step 1571, loss 0.520994.
Train: 2018-08-02T10:49:32.288363: step 1572, loss 0.554063.
Train: 2018-08-02T10:49:32.507093: step 1573, loss 0.520753.
Train: 2018-08-02T10:49:32.725792: step 1574, loss 0.53761.
Train: 2018-08-02T10:49:32.944492: step 1575, loss 0.554.
Train: 2018-08-02T10:49:33.163160: step 1576, loss 0.586845.
Train: 2018-08-02T10:49:33.381881: step 1577, loss 0.554027.
Train: 2018-08-02T10:49:33.600555: step 1578, loss 0.595871.
Train: 2018-08-02T10:49:33.819255: step 1579, loss 0.587599.
Train: 2018-08-02T10:49:34.037983: step 1580, loss 0.57015.
Test: 2018-08-02T10:49:35.209554: step 1580, loss 0.548436.
Train: 2018-08-02T10:49:35.412662: step 1581, loss 0.536963.
Train: 2018-08-02T10:49:35.631355: step 1582, loss 0.639653.
Train: 2018-08-02T10:49:35.850029: step 1583, loss 0.529615.
Train: 2018-08-02T10:49:36.068727: step 1584, loss 0.54504.
Train: 2018-08-02T10:49:36.287457: step 1585, loss 0.57125.
Train: 2018-08-02T10:49:36.506155: step 1586, loss 0.528065.
Train: 2018-08-02T10:49:36.724854: step 1587, loss 0.604789.
Train: 2018-08-02T10:49:36.943555: step 1588, loss 0.511381.
Train: 2018-08-02T10:49:37.162222: step 1589, loss 0.511233.
Train: 2018-08-02T10:49:37.380951: step 1590, loss 0.553556.
Test: 2018-08-02T10:49:38.536900: step 1590, loss 0.549122.
Train: 2018-08-02T10:49:38.755598: step 1591, loss 0.554102.
Train: 2018-08-02T10:49:38.974327: step 1592, loss 0.56292.
Train: 2018-08-02T10:49:39.193028: step 1593, loss 0.605131.
Train: 2018-08-02T10:49:39.411721: step 1594, loss 0.477532.
Train: 2018-08-02T10:49:39.630423: step 1595, loss 0.612913.
Train: 2018-08-02T10:49:39.849123: step 1596, loss 0.485758.
Train: 2018-08-02T10:49:40.067791: step 1597, loss 0.511641.
Train: 2018-08-02T10:49:40.286490: step 1598, loss 0.571142.
Train: 2018-08-02T10:49:40.505213: step 1599, loss 0.5961.
Train: 2018-08-02T10:49:40.723918: step 1600, loss 0.596215.
Test: 2018-08-02T10:49:41.895488: step 1600, loss 0.548505.
Train: 2018-08-02T10:49:42.707797: step 1601, loss 0.578709.
Train: 2018-08-02T10:49:42.926529: step 1602, loss 0.613753.
Train: 2018-08-02T10:49:43.145225: step 1603, loss 0.527855.
Train: 2018-08-02T10:49:43.363924: step 1604, loss 0.579072.
Train: 2018-08-02T10:49:43.582593: step 1605, loss 0.528212.
Train: 2018-08-02T10:49:43.801291: step 1606, loss 0.511647.
Train: 2018-08-02T10:49:44.020023: step 1607, loss 0.511034.
Train: 2018-08-02T10:49:44.238719: step 1608, loss 0.621323.
Train: 2018-08-02T10:49:44.457418: step 1609, loss 0.612261.
Train: 2018-08-02T10:49:44.676117: step 1610, loss 0.545368.
Test: 2018-08-02T10:49:45.832066: step 1610, loss 0.547953.
Train: 2018-08-02T10:49:46.050797: step 1611, loss 0.563344.
Train: 2018-08-02T10:49:46.269463: step 1612, loss 0.570804.
Train: 2018-08-02T10:49:46.488193: step 1613, loss 0.537084.
Train: 2018-08-02T10:49:46.706892: step 1614, loss 0.570883.
Train: 2018-08-02T10:49:46.925590: step 1615, loss 0.520248.
Train: 2018-08-02T10:49:47.144289: step 1616, loss 0.663759.
Train: 2018-08-02T10:49:47.362988: step 1617, loss 0.596409.
Train: 2018-08-02T10:49:47.581687: step 1618, loss 0.603384.
Train: 2018-08-02T10:49:47.800388: step 1619, loss 0.55442.
Train: 2018-08-02T10:49:48.019084: step 1620, loss 0.644675.
Test: 2018-08-02T10:49:49.190654: step 1620, loss 0.548748.
Train: 2018-08-02T10:49:49.393756: step 1621, loss 0.562384.
Train: 2018-08-02T10:49:49.612455: step 1622, loss 0.554231.
Train: 2018-08-02T10:49:49.831159: step 1623, loss 0.603071.
Train: 2018-08-02T10:49:50.049830: step 1624, loss 0.643821.
Train: 2018-08-02T10:49:50.268558: step 1625, loss 0.498626.
Train: 2018-08-02T10:49:50.487256: step 1626, loss 0.562978.
Train: 2018-08-02T10:49:50.705924: step 1627, loss 0.571534.
Train: 2018-08-02T10:49:50.924654: step 1628, loss 0.56293.
Train: 2018-08-02T10:49:51.143323: step 1629, loss 0.563687.
Train: 2018-08-02T10:49:51.362051: step 1630, loss 0.500154.
Test: 2018-08-02T10:49:52.518000: step 1630, loss 0.550186.
Train: 2018-08-02T10:49:52.736729: step 1631, loss 0.579145.
Train: 2018-08-02T10:49:52.955428: step 1632, loss 0.633669.
Train: 2018-08-02T10:49:53.174129: step 1633, loss 0.531921.
Train: 2018-08-02T10:49:53.392826: step 1634, loss 0.578509.
Train: 2018-08-02T10:49:53.611495: step 1635, loss 0.602281.
Train: 2018-08-02T10:49:53.830223: step 1636, loss 0.602482.
Train: 2018-08-02T10:49:54.048893: step 1637, loss 0.602468.
Train: 2018-08-02T10:49:54.267590: step 1638, loss 0.555915.
Train: 2018-08-02T10:49:54.486314: step 1639, loss 0.547882.
Train: 2018-08-02T10:49:54.705013: step 1640, loss 0.548113.
Test: 2018-08-02T10:49:55.876590: step 1640, loss 0.549086.
Train: 2018-08-02T10:49:56.079691: step 1641, loss 0.548136.
Train: 2018-08-02T10:49:56.298398: step 1642, loss 0.555862.
Train: 2018-08-02T10:49:56.517088: step 1643, loss 0.602056.
Train: 2018-08-02T10:49:56.735763: step 1644, loss 0.478096.
Train: 2018-08-02T10:49:56.954491: step 1645, loss 0.579003.
Train: 2018-08-02T10:49:57.173190: step 1646, loss 0.540166.
Train: 2018-08-02T10:49:57.391858: step 1647, loss 0.594409.
Train: 2018-08-02T10:49:57.610588: step 1648, loss 0.602348.
Train: 2018-08-02T10:49:57.829291: step 1649, loss 0.609918.
Train: 2018-08-02T10:49:58.047985: step 1650, loss 0.500168.
Test: 2018-08-02T10:49:59.203935: step 1650, loss 0.551172.
Train: 2018-08-02T10:49:59.422634: step 1651, loss 0.547498.
Train: 2018-08-02T10:49:59.641362: step 1652, loss 0.539501.
Train: 2018-08-02T10:49:59.860062: step 1653, loss 0.562783.
Train: 2018-08-02T10:50:00.078729: step 1654, loss 0.571289.
Train: 2018-08-02T10:50:00.297459: step 1655, loss 0.538943.
Train: 2018-08-02T10:50:00.516157: step 1656, loss 0.538859.
Train: 2018-08-02T10:50:00.734857: step 1657, loss 0.531024.
Train: 2018-08-02T10:50:00.953557: step 1658, loss 0.56264.
Train: 2018-08-02T10:50:01.172257: step 1659, loss 0.506629.
Train: 2018-08-02T10:50:01.390953: step 1660, loss 0.57878.
Test: 2018-08-02T10:50:02.546902: step 1660, loss 0.549474.
Train: 2018-08-02T10:50:02.718767: step 1661, loss 0.545422.
Train: 2018-08-02T10:50:02.937435: step 1662, loss 0.521265.
Train: 2018-08-02T10:50:03.156164: step 1663, loss 0.55428.
Train: 2018-08-02T10:50:03.374834: step 1664, loss 0.521475.
Train: 2018-08-02T10:50:03.593561: step 1665, loss 0.587943.
Train: 2018-08-02T10:50:03.812263: step 1666, loss 0.587448.
Train: 2018-08-02T10:50:04.030960: step 1667, loss 0.529225.
Train: 2018-08-02T10:50:04.249628: step 1668, loss 0.529099.
Train: 2018-08-02T10:50:04.468358: step 1669, loss 0.537982.
Train: 2018-08-02T10:50:04.687026: step 1670, loss 0.587107.
Test: 2018-08-02T10:50:05.843005: step 1670, loss 0.548476.
Train: 2018-08-02T10:50:06.061736: step 1671, loss 0.528535.
Train: 2018-08-02T10:50:06.280433: step 1672, loss 0.588041.
Train: 2018-08-02T10:50:06.499132: step 1673, loss 0.519496.
Train: 2018-08-02T10:50:06.717801: step 1674, loss 0.56266.
Train: 2018-08-02T10:50:06.936531: step 1675, loss 0.528528.
Train: 2018-08-02T10:50:07.155228: step 1676, loss 0.613527.
Train: 2018-08-02T10:50:07.373929: step 1677, loss 0.570861.
Train: 2018-08-02T10:50:07.592595: step 1678, loss 0.58007.
Train: 2018-08-02T10:50:07.811295: step 1679, loss 0.613449.
Train: 2018-08-02T10:50:08.030023: step 1680, loss 0.587977.
Test: 2018-08-02T10:50:09.201593: step 1680, loss 0.548386.
Train: 2018-08-02T10:50:09.404702: step 1681, loss 0.511274.
Train: 2018-08-02T10:50:09.623427: step 1682, loss 0.553572.
Train: 2018-08-02T10:50:09.842093: step 1683, loss 0.638722.
Train: 2018-08-02T10:50:10.060767: step 1684, loss 0.613596.
Train: 2018-08-02T10:50:10.279491: step 1685, loss 0.536543.
Train: 2018-08-02T10:50:10.498195: step 1686, loss 0.595081.
Train: 2018-08-02T10:50:10.716893: step 1687, loss 0.545461.
Train: 2018-08-02T10:50:10.935594: step 1688, loss 0.595943.
Train: 2018-08-02T10:50:11.154291: step 1689, loss 0.620118.
Train: 2018-08-02T10:50:11.372990: step 1690, loss 0.513363.
Test: 2018-08-02T10:50:12.560181: step 1690, loss 0.549254.
Train: 2018-08-02T10:50:12.763260: step 1691, loss 0.505475.
Train: 2018-08-02T10:50:12.981984: step 1692, loss 0.586937.
Train: 2018-08-02T10:50:13.200688: step 1693, loss 0.522388.
Train: 2018-08-02T10:50:13.419388: step 1694, loss 0.57938.
Train: 2018-08-02T10:50:13.638085: step 1695, loss 0.538557.
Train: 2018-08-02T10:50:13.856783: step 1696, loss 0.546114.
Train: 2018-08-02T10:50:14.075476: step 1697, loss 0.497557.
Train: 2018-08-02T10:50:14.294181: step 1698, loss 0.530091.
Train: 2018-08-02T10:50:14.512880: step 1699, loss 0.554553.
Train: 2018-08-02T10:50:14.731579: step 1700, loss 0.578966.
Test: 2018-08-02T10:50:15.903148: step 1700, loss 0.548109.
Train: 2018-08-02T10:50:16.731111: step 1701, loss 0.537327.
Train: 2018-08-02T10:50:16.949810: step 1702, loss 0.586943.
Train: 2018-08-02T10:50:17.168508: step 1703, loss 0.554707.
Train: 2018-08-02T10:50:17.387207: step 1704, loss 0.55498.
Train: 2018-08-02T10:50:17.605876: step 1705, loss 0.463608.
Train: 2018-08-02T10:50:17.824604: step 1706, loss 0.604322.
Train: 2018-08-02T10:50:18.043274: step 1707, loss 0.520612.
Train: 2018-08-02T10:50:18.261972: step 1708, loss 0.537392.
Train: 2018-08-02T10:50:18.480701: step 1709, loss 0.595265.
Train: 2018-08-02T10:50:18.699370: step 1710, loss 0.595588.
Test: 2018-08-02T10:50:19.870970: step 1710, loss 0.548447.
Train: 2018-08-02T10:50:20.074047: step 1711, loss 0.595751.
Train: 2018-08-02T10:50:20.292776: step 1712, loss 0.570504.
Train: 2018-08-02T10:50:20.511444: step 1713, loss 0.588152.
Train: 2018-08-02T10:50:20.730174: step 1714, loss 0.604352.
Train: 2018-08-02T10:50:20.964473: step 1715, loss 0.579842.
Train: 2018-08-02T10:50:21.183164: step 1716, loss 0.562788.
Train: 2018-08-02T10:50:21.401861: step 1717, loss 0.595454.
Train: 2018-08-02T10:50:21.620592: step 1718, loss 0.488265.
Train: 2018-08-02T10:50:21.839289: step 1719, loss 0.52983.
Train: 2018-08-02T10:50:22.057990: step 1720, loss 0.570899.
Test: 2018-08-02T10:50:23.213937: step 1720, loss 0.548023.
Train: 2018-08-02T10:50:23.432661: step 1721, loss 0.50501.
Train: 2018-08-02T10:50:23.651365: step 1722, loss 0.521456.
Train: 2018-08-02T10:50:23.870064: step 1723, loss 0.521375.
Train: 2018-08-02T10:50:24.088763: step 1724, loss 0.571086.
Train: 2018-08-02T10:50:24.307464: step 1725, loss 0.562313.
Train: 2018-08-02T10:50:24.526160: step 1726, loss 0.570967.
Train: 2018-08-02T10:50:24.744859: step 1727, loss 0.637538.
Train: 2018-08-02T10:50:24.963557: step 1728, loss 0.512434.
Train: 2018-08-02T10:50:25.182226: step 1729, loss 0.628981.
Train: 2018-08-02T10:50:25.400955: step 1730, loss 0.61154.
Test: 2018-08-02T10:50:26.572525: step 1730, loss 0.548669.
Train: 2018-08-02T10:50:26.775603: step 1731, loss 0.603738.
Train: 2018-08-02T10:50:26.994302: step 1732, loss 0.595117.
Train: 2018-08-02T10:50:27.213001: step 1733, loss 0.58675.
Train: 2018-08-02T10:50:27.431700: step 1734, loss 0.587077.
Train: 2018-08-02T10:50:27.650428: step 1735, loss 0.546447.
Train: 2018-08-02T10:50:27.869121: step 1736, loss 0.522456.
Train: 2018-08-02T10:50:28.087796: step 1737, loss 0.546711.
Train: 2018-08-02T10:50:28.306495: step 1738, loss 0.619445.
Train: 2018-08-02T10:50:28.525224: step 1739, loss 0.538317.
Train: 2018-08-02T10:50:28.743892: step 1740, loss 0.595034.
Test: 2018-08-02T10:50:29.915493: step 1740, loss 0.549386.
Train: 2018-08-02T10:50:30.165435: step 1741, loss 0.554969.
Train: 2018-08-02T10:50:30.384163: step 1742, loss 0.538671.
Train: 2018-08-02T10:50:30.602865: step 1743, loss 0.530701.
Train: 2018-08-02T10:50:30.821561: step 1744, loss 0.555071.
Train: 2018-08-02T10:50:31.040259: step 1745, loss 0.578409.
Train: 2018-08-02T10:50:31.258959: step 1746, loss 0.522749.
Train: 2018-08-02T10:50:31.477661: step 1747, loss 0.562908.
Train: 2018-08-02T10:50:31.696356: step 1748, loss 0.450154.
Train: 2018-08-02T10:50:31.915057: step 1749, loss 0.603114.
Train: 2018-08-02T10:50:32.133754: step 1750, loss 0.571098.
Test: 2018-08-02T10:50:33.289703: step 1750, loss 0.548036.
Train: 2018-08-02T10:50:33.508432: step 1751, loss 0.546494.
Train: 2018-08-02T10:50:33.727131: step 1752, loss 0.579081.
Train: 2018-08-02T10:50:33.945829: step 1753, loss 0.562534.
Train: 2018-08-02T10:50:34.164528: step 1754, loss 0.521805.
Train: 2018-08-02T10:50:34.383227: step 1755, loss 0.578883.
Train: 2018-08-02T10:50:34.601926: step 1756, loss 0.562487.
Train: 2018-08-02T10:50:34.820624: step 1757, loss 0.554318.
Train: 2018-08-02T10:50:35.039326: step 1758, loss 0.595503.
Train: 2018-08-02T10:50:35.258022: step 1759, loss 0.505108.
Train: 2018-08-02T10:50:35.476721: step 1760, loss 0.504746.
Test: 2018-08-02T10:50:36.632670: step 1760, loss 0.548702.
Train: 2018-08-02T10:50:36.851370: step 1761, loss 0.562847.
Train: 2018-08-02T10:50:37.070098: step 1762, loss 0.546236.
Train: 2018-08-02T10:50:37.288767: step 1763, loss 0.58744.
Train: 2018-08-02T10:50:37.507466: step 1764, loss 0.621139.
Train: 2018-08-02T10:50:37.726165: step 1765, loss 0.545803.
Train: 2018-08-02T10:50:37.944896: step 1766, loss 0.520954.
Train: 2018-08-02T10:50:38.163563: step 1767, loss 0.620423.
Train: 2018-08-02T10:50:38.382291: step 1768, loss 0.562155.
Train: 2018-08-02T10:50:38.600958: step 1769, loss 0.545418.
Train: 2018-08-02T10:50:38.819658: step 1770, loss 0.579046.
Test: 2018-08-02T10:50:39.975637: step 1770, loss 0.547221.
Train: 2018-08-02T10:50:40.194361: step 1771, loss 0.587389.
Train: 2018-08-02T10:50:40.413034: step 1772, loss 0.554288.
Train: 2018-08-02T10:50:40.631767: step 1773, loss 0.537479.
Train: 2018-08-02T10:50:40.850462: step 1774, loss 0.587243.
Train: 2018-08-02T10:50:41.069161: step 1775, loss 0.604358.
Train: 2018-08-02T10:50:41.287830: step 1776, loss 0.562857.
Train: 2018-08-02T10:50:41.506558: step 1777, loss 0.587175.
Train: 2018-08-02T10:50:41.725257: step 1778, loss 0.554377.
Train: 2018-08-02T10:50:41.943951: step 1779, loss 0.521566.
Train: 2018-08-02T10:50:42.162649: step 1780, loss 0.554.
Test: 2018-08-02T10:50:43.318605: step 1780, loss 0.548863.
Train: 2018-08-02T10:50:43.521711: step 1781, loss 0.538019.
Train: 2018-08-02T10:50:43.724760: step 1782, loss 0.628512.
Train: 2018-08-02T10:50:43.943488: step 1783, loss 0.636233.
Train: 2018-08-02T10:50:44.162200: step 1784, loss 0.56248.
Train: 2018-08-02T10:50:44.380856: step 1785, loss 0.611497.
Train: 2018-08-02T10:50:44.599585: step 1786, loss 0.54619.
Train: 2018-08-02T10:50:44.818283: step 1787, loss 0.554539.
Train: 2018-08-02T10:50:45.036951: step 1788, loss 0.554703.
Train: 2018-08-02T10:50:45.255651: step 1789, loss 0.562633.
Train: 2018-08-02T10:50:45.474380: step 1790, loss 0.506913.
Test: 2018-08-02T10:50:46.630329: step 1790, loss 0.549396.
Train: 2018-08-02T10:50:46.849058: step 1791, loss 0.634627.
Train: 2018-08-02T10:50:47.067751: step 1792, loss 0.546798.
Train: 2018-08-02T10:50:47.286455: step 1793, loss 0.594449.
Train: 2018-08-02T10:50:47.505155: step 1794, loss 0.594478.
Train: 2018-08-02T10:50:47.723853: step 1795, loss 0.522687.
Train: 2018-08-02T10:50:47.942521: step 1796, loss 0.578932.
Train: 2018-08-02T10:50:48.161250: step 1797, loss 0.466981.
Train: 2018-08-02T10:50:48.379944: step 1798, loss 0.52285.
Train: 2018-08-02T10:50:48.598617: step 1799, loss 0.619046.
Train: 2018-08-02T10:50:48.817350: step 1800, loss 0.514609.
Test: 2018-08-02T10:50:49.973296: step 1800, loss 0.548622.
Train: 2018-08-02T10:50:50.832501: step 1801, loss 0.57881.
Train: 2018-08-02T10:50:51.051169: step 1802, loss 0.546577.
Train: 2018-08-02T10:50:51.269868: step 1803, loss 0.611495.
Train: 2018-08-02T10:50:51.488592: step 1804, loss 0.602916.
Train: 2018-08-02T10:50:51.707265: step 1805, loss 0.546273.
Train: 2018-08-02T10:50:51.925963: step 1806, loss 0.570692.
Train: 2018-08-02T10:50:52.144687: step 1807, loss 0.53028.
Train: 2018-08-02T10:50:52.363360: step 1808, loss 0.594717.
Train: 2018-08-02T10:50:52.582060: step 1809, loss 0.554501.
Train: 2018-08-02T10:50:52.800791: step 1810, loss 0.497583.
Test: 2018-08-02T10:50:53.956738: step 1810, loss 0.548924.
Train: 2018-08-02T10:50:54.175465: step 1811, loss 0.578851.
Train: 2018-08-02T10:50:54.347302: step 1812, loss 0.527503.
Train: 2018-08-02T10:50:54.565997: step 1813, loss 0.595019.
Train: 2018-08-02T10:50:54.784699: step 1814, loss 0.587217.
Train: 2018-08-02T10:50:55.003398: step 1815, loss 0.546075.
Train: 2018-08-02T10:50:55.222097: step 1816, loss 0.554707.
Train: 2018-08-02T10:50:55.440765: step 1817, loss 0.546122.
Train: 2018-08-02T10:50:55.659497: step 1818, loss 0.562233.
Train: 2018-08-02T10:50:55.878193: step 1819, loss 0.587168.
Train: 2018-08-02T10:50:56.096862: step 1820, loss 0.489018.
Test: 2018-08-02T10:50:57.268462: step 1820, loss 0.54858.
Train: 2018-08-02T10:50:57.471570: step 1821, loss 0.6533.
Train: 2018-08-02T10:50:57.690238: step 1822, loss 0.587485.
Train: 2018-08-02T10:50:57.908970: step 1823, loss 0.545956.
Train: 2018-08-02T10:50:58.127667: step 1824, loss 0.562095.
Train: 2018-08-02T10:50:58.346336: step 1825, loss 0.521626.
Train: 2018-08-02T10:50:58.565064: step 1826, loss 0.603316.
Train: 2018-08-02T10:50:58.783758: step 1827, loss 0.480369.
Train: 2018-08-02T10:50:59.002462: step 1828, loss 0.602973.
Train: 2018-08-02T10:50:59.221130: step 1829, loss 0.612304.
Train: 2018-08-02T10:50:59.439854: step 1830, loss 0.488488.
Test: 2018-08-02T10:51:00.611429: step 1830, loss 0.548586.
Train: 2018-08-02T10:51:00.814507: step 1831, loss 0.595695.
Train: 2018-08-02T10:51:01.033206: step 1832, loss 0.578424.
Train: 2018-08-02T10:51:01.251904: step 1833, loss 0.545811.
Train: 2018-08-02T10:51:01.455013: step 1834, loss 0.57093.
Train: 2018-08-02T10:51:01.673713: step 1835, loss 0.5946.
Train: 2018-08-02T10:51:01.892410: step 1836, loss 0.58696.
Train: 2018-08-02T10:51:02.111109: step 1837, loss 0.546919.
Train: 2018-08-02T10:51:02.329802: step 1838, loss 0.587035.
Train: 2018-08-02T10:51:02.548477: step 1839, loss 0.546126.
Train: 2018-08-02T10:51:02.767201: step 1840, loss 0.554614.
Test: 2018-08-02T10:51:03.923154: step 1840, loss 0.54915.
Train: 2018-08-02T10:51:04.141883: step 1841, loss 0.521833.
Train: 2018-08-02T10:51:04.360576: step 1842, loss 0.546327.
Train: 2018-08-02T10:51:04.579250: step 1843, loss 0.620072.
Train: 2018-08-02T10:51:04.797982: step 1844, loss 0.538559.
Train: 2018-08-02T10:51:05.016679: step 1845, loss 0.562404.
Train: 2018-08-02T10:51:05.235377: step 1846, loss 0.537939.
Train: 2018-08-02T10:51:05.454076: step 1847, loss 0.571529.
Train: 2018-08-02T10:51:05.672776: step 1848, loss 0.587635.
Train: 2018-08-02T10:51:05.891468: step 1849, loss 0.595164.
Train: 2018-08-02T10:51:06.110168: step 1850, loss 0.521548.
Test: 2018-08-02T10:51:07.266121: step 1850, loss 0.548645.
Train: 2018-08-02T10:51:07.484821: step 1851, loss 0.546572.
Train: 2018-08-02T10:51:07.703543: step 1852, loss 0.611238.
Train: 2018-08-02T10:51:07.922217: step 1853, loss 0.554638.
Train: 2018-08-02T10:51:08.140916: step 1854, loss 0.554245.
Train: 2018-08-02T10:51:08.359646: step 1855, loss 0.587464.
Train: 2018-08-02T10:51:08.578314: step 1856, loss 0.562364.
Train: 2018-08-02T10:51:08.797014: step 1857, loss 0.611375.
Train: 2018-08-02T10:51:09.015742: step 1858, loss 0.498368.
Train: 2018-08-02T10:51:09.234441: step 1859, loss 0.603012.
Train: 2018-08-02T10:51:09.453110: step 1860, loss 0.547137.
Test: 2018-08-02T10:51:10.609088: step 1860, loss 0.54941.
Train: 2018-08-02T10:51:10.827818: step 1861, loss 0.538507.
Train: 2018-08-02T10:51:11.046518: step 1862, loss 0.570462.
Train: 2018-08-02T10:51:11.265215: step 1863, loss 0.594843.
Train: 2018-08-02T10:51:11.483914: step 1864, loss 0.498263.
Train: 2018-08-02T10:51:11.702582: step 1865, loss 0.586723.
Train: 2018-08-02T10:51:11.921313: step 1866, loss 0.521586.
Train: 2018-08-02T10:51:12.140010: step 1867, loss 0.546272.
Train: 2018-08-02T10:51:12.358709: step 1868, loss 0.498127.
Train: 2018-08-02T10:51:12.577377: step 1869, loss 0.61165.
Train: 2018-08-02T10:51:12.796109: step 1870, loss 0.537887.
Test: 2018-08-02T10:51:13.967676: step 1870, loss 0.548903.
Train: 2018-08-02T10:51:14.170786: step 1871, loss 0.562433.
Train: 2018-08-02T10:51:14.389484: step 1872, loss 0.546148.
Train: 2018-08-02T10:51:14.608183: step 1873, loss 0.570543.
Train: 2018-08-02T10:51:14.826859: step 1874, loss 0.546487.
Train: 2018-08-02T10:51:15.045582: step 1875, loss 0.563193.
Train: 2018-08-02T10:51:15.264278: step 1876, loss 0.586744.
Train: 2018-08-02T10:51:15.482977: step 1877, loss 0.488559.
Train: 2018-08-02T10:51:15.701677: step 1878, loss 0.57849.
Train: 2018-08-02T10:51:15.920377: step 1879, loss 0.48753.
Train: 2018-08-02T10:51:16.139077: step 1880, loss 0.604592.
Test: 2018-08-02T10:51:17.310644: step 1880, loss 0.548447.
Train: 2018-08-02T10:51:17.529344: step 1881, loss 0.605333.
Train: 2018-08-02T10:51:17.748072: step 1882, loss 0.562961.
Train: 2018-08-02T10:51:17.966740: step 1883, loss 0.55408.
Train: 2018-08-02T10:51:18.185470: step 1884, loss 0.612559.
Train: 2018-08-02T10:51:18.404168: step 1885, loss 0.604415.
Train: 2018-08-02T10:51:18.622837: step 1886, loss 0.5948.
Train: 2018-08-02T10:51:18.841535: step 1887, loss 0.587696.
Train: 2018-08-02T10:51:19.060234: step 1888, loss 0.545435.
Train: 2018-08-02T10:51:19.278963: step 1889, loss 0.620066.
Train: 2018-08-02T10:51:19.497663: step 1890, loss 0.522291.
Test: 2018-08-02T10:51:20.653611: step 1890, loss 0.549092.
Train: 2018-08-02T10:51:20.872340: step 1891, loss 0.652889.
Train: 2018-08-02T10:51:21.091039: step 1892, loss 0.521949.
Train: 2018-08-02T10:51:21.309738: step 1893, loss 0.587304.
Train: 2018-08-02T10:51:21.528406: step 1894, loss 0.522022.
Train: 2018-08-02T10:51:21.747135: step 1895, loss 0.546246.
Train: 2018-08-02T10:51:21.965804: step 1896, loss 0.554456.
Train: 2018-08-02T10:51:22.184527: step 1897, loss 0.562869.
Train: 2018-08-02T10:51:22.403232: step 1898, loss 0.547142.
Train: 2018-08-02T10:51:22.621931: step 1899, loss 0.546425.
Train: 2018-08-02T10:51:22.840629: step 1900, loss 0.546227.
Test: 2018-08-02T10:51:23.996578: step 1900, loss 0.547648.
Train: 2018-08-02T10:51:24.824509: step 1901, loss 0.530231.
Train: 2018-08-02T10:51:25.043241: step 1902, loss 0.522793.
Train: 2018-08-02T10:51:25.261908: step 1903, loss 0.562682.
Train: 2018-08-02T10:51:25.480636: step 1904, loss 0.571173.
Train: 2018-08-02T10:51:25.699304: step 1905, loss 0.521312.
Train: 2018-08-02T10:51:25.918036: step 1906, loss 0.578686.
Train: 2018-08-02T10:51:26.136733: step 1907, loss 0.530312.
Train: 2018-08-02T10:51:26.355434: step 1908, loss 0.554246.
Train: 2018-08-02T10:51:26.574131: step 1909, loss 0.578864.
Train: 2018-08-02T10:51:26.792831: step 1910, loss 0.546103.
Test: 2018-08-02T10:51:27.964400: step 1910, loss 0.548513.
Train: 2018-08-02T10:51:28.167505: step 1911, loss 0.562894.
Train: 2018-08-02T10:51:28.386205: step 1912, loss 0.594906.
Train: 2018-08-02T10:51:28.604874: step 1913, loss 0.562357.
Train: 2018-08-02T10:51:28.823604: step 1914, loss 0.554378.
Train: 2018-08-02T10:51:29.042272: step 1915, loss 0.571285.
Train: 2018-08-02T10:51:29.260970: step 1916, loss 0.537451.
Train: 2018-08-02T10:51:29.479701: step 1917, loss 0.587514.
Train: 2018-08-02T10:51:29.698369: step 1918, loss 0.512835.
Train: 2018-08-02T10:51:29.917099: step 1919, loss 0.619672.
Train: 2018-08-02T10:51:30.135768: step 1920, loss 0.646023.
Test: 2018-08-02T10:51:31.307366: step 1920, loss 0.550542.
Train: 2018-08-02T10:51:31.510469: step 1921, loss 0.520737.
Train: 2018-08-02T10:51:31.729167: step 1922, loss 0.578115.
Train: 2018-08-02T10:51:31.947841: step 1923, loss 0.620553.
Train: 2018-08-02T10:51:32.166572: step 1924, loss 0.504147.
Train: 2018-08-02T10:51:32.385239: step 1925, loss 0.537999.
Train: 2018-08-02T10:51:32.603968: step 1926, loss 0.570242.
Train: 2018-08-02T10:51:32.822661: step 1927, loss 0.587446.
Train: 2018-08-02T10:51:33.041368: step 1928, loss 0.529836.
Train: 2018-08-02T10:51:33.260035: step 1929, loss 0.595662.
Train: 2018-08-02T10:51:33.478734: step 1930, loss 0.553974.
Test: 2018-08-02T10:51:34.634713: step 1930, loss 0.548909.
Train: 2018-08-02T10:51:34.853439: step 1931, loss 0.537018.
Train: 2018-08-02T10:51:35.072141: step 1932, loss 0.613974.
Train: 2018-08-02T10:51:35.290841: step 1933, loss 0.5872.
Train: 2018-08-02T10:51:35.509541: step 1934, loss 0.489054.
Train: 2018-08-02T10:51:35.728230: step 1935, loss 0.554877.
Train: 2018-08-02T10:51:35.946935: step 1936, loss 0.513482.
Train: 2018-08-02T10:51:36.165636: step 1937, loss 0.521528.
Train: 2018-08-02T10:51:36.384303: step 1938, loss 0.563068.
Train: 2018-08-02T10:51:36.603033: step 1939, loss 0.635787.
Train: 2018-08-02T10:51:36.821725: step 1940, loss 0.570817.
Test: 2018-08-02T10:51:37.977679: step 1940, loss 0.54924.
Train: 2018-08-02T10:51:38.196378: step 1941, loss 0.481375.
Train: 2018-08-02T10:51:38.415109: step 1942, loss 0.554446.
Train: 2018-08-02T10:51:38.633801: step 1943, loss 0.55336.
Train: 2018-08-02T10:51:38.852505: step 1944, loss 0.545988.
Train: 2018-08-02T10:51:39.071203: step 1945, loss 0.579502.
Train: 2018-08-02T10:51:39.289904: step 1946, loss 0.621069.
Train: 2018-08-02T10:51:39.508595: step 1947, loss 0.529513.
Train: 2018-08-02T10:51:39.727301: step 1948, loss 0.520573.
Train: 2018-08-02T10:51:39.946000: step 1949, loss 0.586724.
Train: 2018-08-02T10:51:40.164699: step 1950, loss 0.588415.
Test: 2018-08-02T10:51:41.336268: step 1950, loss 0.548636.
Train: 2018-08-02T10:51:41.539378: step 1951, loss 0.578345.
Train: 2018-08-02T10:51:41.758044: step 1952, loss 0.578526.
Train: 2018-08-02T10:51:41.976773: step 1953, loss 0.554285.
Train: 2018-08-02T10:51:42.195466: step 1954, loss 0.644879.
Train: 2018-08-02T10:51:42.414142: step 1955, loss 0.562627.
Train: 2018-08-02T10:51:42.648461: step 1956, loss 0.505805.
Train: 2018-08-02T10:51:42.851538: step 1957, loss 0.529939.
Train: 2018-08-02T10:51:43.070268: step 1958, loss 0.571488.
Train: 2018-08-02T10:51:43.288935: step 1959, loss 0.504839.
Train: 2018-08-02T10:51:43.507665: step 1960, loss 0.5136.
Test: 2018-08-02T10:51:44.679235: step 1960, loss 0.548809.
Train: 2018-08-02T10:51:44.882337: step 1961, loss 0.595403.
Train: 2018-08-02T10:51:45.101011: step 1962, loss 0.538092.
Train: 2018-08-02T10:51:45.288467: step 1963, loss 0.615232.
Train: 2018-08-02T10:51:45.491577: step 1964, loss 0.578191.
Train: 2018-08-02T10:51:45.710274: step 1965, loss 0.562554.
Train: 2018-08-02T10:51:45.928973: step 1966, loss 0.520731.
Train: 2018-08-02T10:51:46.147666: step 1967, loss 0.553919.
Train: 2018-08-02T10:51:46.366370: step 1968, loss 0.56312.
Train: 2018-08-02T10:51:46.585070: step 1969, loss 0.488066.
Train: 2018-08-02T10:51:46.803738: step 1970, loss 0.529097.
Test: 2018-08-02T10:51:47.975338: step 1970, loss 0.547647.
Train: 2018-08-02T10:51:48.178446: step 1971, loss 0.586525.
Train: 2018-08-02T10:51:48.397144: step 1972, loss 0.545414.
Train: 2018-08-02T10:51:48.615843: step 1973, loss 0.578784.
Train: 2018-08-02T10:51:48.834543: step 1974, loss 0.529426.
Train: 2018-08-02T10:51:49.053235: step 1975, loss 0.504623.
Train: 2018-08-02T10:51:49.271940: step 1976, loss 0.503448.
Train: 2018-08-02T10:51:49.490638: step 1977, loss 0.605177.
Train: 2018-08-02T10:51:49.709338: step 1978, loss 0.486632.
Train: 2018-08-02T10:51:49.928006: step 1979, loss 0.570679.
Train: 2018-08-02T10:51:50.146704: step 1980, loss 0.588331.
Test: 2018-08-02T10:51:51.302684: step 1980, loss 0.548291.
Train: 2018-08-02T10:51:51.521415: step 1981, loss 0.537549.
Train: 2018-08-02T10:51:51.740112: step 1982, loss 0.544614.
Train: 2018-08-02T10:51:51.958811: step 1983, loss 0.545642.
Train: 2018-08-02T10:51:52.177504: step 1984, loss 0.623323.
Train: 2018-08-02T10:51:52.396208: step 1985, loss 0.484953.
Train: 2018-08-02T10:51:52.614909: step 1986, loss 0.562482.
Train: 2018-08-02T10:51:52.833605: step 1987, loss 0.563236.
Train: 2018-08-02T10:51:53.052300: step 1988, loss 0.621593.
Train: 2018-08-02T10:51:53.271003: step 1989, loss 0.570582.
Train: 2018-08-02T10:51:53.489705: step 1990, loss 0.664177.
Test: 2018-08-02T10:51:54.661272: step 1990, loss 0.548888.
Train: 2018-08-02T10:51:54.864374: step 1991, loss 0.579509.
Train: 2018-08-02T10:51:55.083073: step 1992, loss 0.561959.
Train: 2018-08-02T10:51:55.301748: step 1993, loss 0.620925.
Train: 2018-08-02T10:51:55.520479: step 1994, loss 0.562452.
Train: 2018-08-02T10:51:55.739146: step 1995, loss 0.545771.
Train: 2018-08-02T10:51:55.957869: step 1996, loss 0.54548.
Train: 2018-08-02T10:51:56.176573: step 1997, loss 0.562006.
Train: 2018-08-02T10:51:56.395271: step 1998, loss 0.554296.
Train: 2018-08-02T10:51:56.613971: step 1999, loss 0.595233.
Train: 2018-08-02T10:51:56.832673: step 2000, loss 0.620349.
Test: 2018-08-02T10:51:57.988618: step 2000, loss 0.54914.
Train: 2018-08-02T10:51:58.800959: step 2001, loss 0.496768.
Train: 2018-08-02T10:51:59.019627: step 2002, loss 0.579184.
Train: 2018-08-02T10:51:59.238357: step 2003, loss 0.513829.
Train: 2018-08-02T10:51:59.457055: step 2004, loss 0.594598.
Train: 2018-08-02T10:51:59.675724: step 2005, loss 0.586899.
Train: 2018-08-02T10:51:59.894423: step 2006, loss 0.554269.
Train: 2018-08-02T10:52:00.113154: step 2007, loss 0.55395.
Train: 2018-08-02T10:52:00.331850: step 2008, loss 0.497851.
Train: 2018-08-02T10:52:00.550549: step 2009, loss 0.555702.
Train: 2018-08-02T10:52:00.769248: step 2010, loss 0.57882.
Test: 2018-08-02T10:52:01.925196: step 2010, loss 0.548818.
Train: 2018-08-02T10:52:02.128274: step 2011, loss 0.554732.
Train: 2018-08-02T10:52:02.347003: step 2012, loss 0.497473.
Train: 2018-08-02T10:52:02.565702: step 2013, loss 0.595349.
Train: 2018-08-02T10:52:02.784369: step 2014, loss 0.530343.
Train: 2018-08-02T10:52:03.003100: step 2015, loss 0.579223.
Train: 2018-08-02T10:52:03.221798: step 2016, loss 0.612144.
Train: 2018-08-02T10:52:03.440497: step 2017, loss 0.572194.
Train: 2018-08-02T10:52:03.659198: step 2018, loss 0.505639.
Train: 2018-08-02T10:52:03.877865: step 2019, loss 0.579228.
Train: 2018-08-02T10:52:04.096563: step 2020, loss 0.481778.
Test: 2018-08-02T10:52:05.268164: step 2020, loss 0.549482.
Train: 2018-08-02T10:52:05.471240: step 2021, loss 0.538636.
Train: 2018-08-02T10:52:05.689970: step 2022, loss 0.529283.
Train: 2018-08-02T10:52:05.908671: step 2023, loss 0.563093.
Train: 2018-08-02T10:52:06.127368: step 2024, loss 0.480146.
Train: 2018-08-02T10:52:06.346037: step 2025, loss 0.513494.
Train: 2018-08-02T10:52:06.564765: step 2026, loss 0.528875.
Train: 2018-08-02T10:52:06.783435: step 2027, loss 0.580149.
Train: 2018-08-02T10:52:07.002162: step 2028, loss 0.588475.
Train: 2018-08-02T10:52:07.220832: step 2029, loss 0.545465.
Train: 2018-08-02T10:52:07.439562: step 2030, loss 0.620676.
Test: 2018-08-02T10:52:08.595510: step 2030, loss 0.546664.
Train: 2018-08-02T10:52:08.814233: step 2031, loss 0.537255.
Train: 2018-08-02T10:52:09.032939: step 2032, loss 0.519395.
Train: 2018-08-02T10:52:09.251607: step 2033, loss 0.553414.
Train: 2018-08-02T10:52:09.470336: step 2034, loss 0.485866.
Train: 2018-08-02T10:52:09.689004: step 2035, loss 0.528286.
Train: 2018-08-02T10:52:09.907734: step 2036, loss 0.579793.
Train: 2018-08-02T10:52:10.126435: step 2037, loss 0.596518.
Train: 2018-08-02T10:52:10.345130: step 2038, loss 0.511492.
Train: 2018-08-02T10:52:10.563827: step 2039, loss 0.587599.
Train: 2018-08-02T10:52:10.782530: step 2040, loss 0.613884.
Test: 2018-08-02T10:52:11.938477: step 2040, loss 0.547963.
Train: 2018-08-02T10:52:12.157200: step 2041, loss 0.571135.
Train: 2018-08-02T10:52:12.375904: step 2042, loss 0.571246.
Train: 2018-08-02T10:52:12.594601: step 2043, loss 0.562354.
Train: 2018-08-02T10:52:12.813303: step 2044, loss 0.528154.
Train: 2018-08-02T10:52:13.032003: step 2045, loss 0.604498.
Train: 2018-08-02T10:52:13.250700: step 2046, loss 0.587464.
Train: 2018-08-02T10:52:13.469369: step 2047, loss 0.579319.
Train: 2018-08-02T10:52:13.688097: step 2048, loss 0.579503.
Train: 2018-08-02T10:52:13.906766: step 2049, loss 0.536668.
Train: 2018-08-02T10:52:14.125489: step 2050, loss 0.478118.
Test: 2018-08-02T10:52:15.281444: step 2050, loss 0.546757.
Train: 2018-08-02T10:52:15.500167: step 2051, loss 0.545247.
Train: 2018-08-02T10:52:15.718872: step 2052, loss 0.706134.
Train: 2018-08-02T10:52:15.937573: step 2053, loss 0.654573.
Train: 2018-08-02T10:52:16.156264: step 2054, loss 0.621119.
Train: 2018-08-02T10:52:16.374962: step 2055, loss 0.554669.
Train: 2018-08-02T10:52:16.589555: step 2056, loss 0.562835.
Train: 2018-08-02T10:52:16.808284: step 2057, loss 0.636208.
Train: 2018-08-02T10:52:17.011363: step 2058, loss 0.586811.
Train: 2018-08-02T10:52:17.230064: step 2059, loss 0.554196.
Train: 2018-08-02T10:52:17.448730: step 2060, loss 0.546435.
Test: 2018-08-02T10:52:18.620330: step 2060, loss 0.551256.
Train: 2018-08-02T10:52:18.823407: step 2061, loss 0.578889.
Train: 2018-08-02T10:52:19.042106: step 2062, loss 0.547171.
Train: 2018-08-02T10:52:19.260835: step 2063, loss 0.578682.
Train: 2018-08-02T10:52:19.479536: step 2064, loss 0.563131.
Train: 2018-08-02T10:52:19.698233: step 2065, loss 0.594688.
Train: 2018-08-02T10:52:19.916931: step 2066, loss 0.532234.
Train: 2018-08-02T10:52:20.135624: step 2067, loss 0.609915.
Train: 2018-08-02T10:52:20.354332: step 2068, loss 0.571029.
Train: 2018-08-02T10:52:20.572999: step 2069, loss 0.563189.
Train: 2018-08-02T10:52:20.791727: step 2070, loss 0.586449.
Test: 2018-08-02T10:52:21.978918: step 2070, loss 0.549426.
Train: 2018-08-02T10:52:22.182026: step 2071, loss 0.524826.
Train: 2018-08-02T10:52:22.400725: step 2072, loss 0.570867.
Train: 2018-08-02T10:52:22.619393: step 2073, loss 0.54802.
Train: 2018-08-02T10:52:22.838118: step 2074, loss 0.586486.
Train: 2018-08-02T10:52:23.056821: step 2075, loss 0.563398.
Train: 2018-08-02T10:52:23.275489: step 2076, loss 0.587068.
Train: 2018-08-02T10:52:23.494219: step 2077, loss 0.594539.
Train: 2018-08-02T10:52:23.712917: step 2078, loss 0.594726.
Train: 2018-08-02T10:52:23.931586: step 2079, loss 0.54797.
Train: 2018-08-02T10:52:24.150316: step 2080, loss 0.478361.
Test: 2018-08-02T10:52:25.321885: step 2080, loss 0.551008.
Train: 2018-08-02T10:52:25.524963: step 2081, loss 0.60929.
Train: 2018-08-02T10:52:25.743661: step 2082, loss 0.571048.
Train: 2018-08-02T10:52:25.962391: step 2083, loss 0.586737.
Train: 2018-08-02T10:52:26.181060: step 2084, loss 0.547914.
Train: 2018-08-02T10:52:26.399783: step 2085, loss 0.555423.
Train: 2018-08-02T10:52:26.618488: step 2086, loss 0.50077.
Train: 2018-08-02T10:52:26.837185: step 2087, loss 0.657293.
Train: 2018-08-02T10:52:27.055885: step 2088, loss 0.515808.
Train: 2018-08-02T10:52:27.274583: step 2089, loss 0.507757.
Train: 2018-08-02T10:52:27.493282: step 2090, loss 0.507802.
Test: 2018-08-02T10:52:28.664852: step 2090, loss 0.550317.
Train: 2018-08-02T10:52:28.867955: step 2091, loss 0.507709.
Train: 2018-08-02T10:52:29.071038: step 2092, loss 0.594519.
Train: 2018-08-02T10:52:29.289732: step 2093, loss 0.587161.
Train: 2018-08-02T10:52:29.508406: step 2094, loss 0.578973.
Train: 2018-08-02T10:52:29.727103: step 2095, loss 0.506089.
Train: 2018-08-02T10:52:29.945833: step 2096, loss 0.635362.
Train: 2018-08-02T10:52:30.164531: step 2097, loss 0.571713.
Train: 2018-08-02T10:52:30.383231: step 2098, loss 0.570797.
Train: 2018-08-02T10:52:30.601932: step 2099, loss 0.529714.
Train: 2018-08-02T10:52:30.820627: step 2100, loss 0.595525.
Test: 2018-08-02T10:52:31.992198: step 2100, loss 0.549892.
Train: 2018-08-02T10:52:32.804538: step 2101, loss 0.612022.
Train: 2018-08-02T10:52:33.023239: step 2102, loss 0.619905.
Train: 2018-08-02T10:52:33.241936: step 2103, loss 0.586387.
Train: 2018-08-02T10:52:33.460605: step 2104, loss 0.571004.
Train: 2018-08-02T10:52:33.679333: step 2105, loss 0.481683.
Train: 2018-08-02T10:52:33.898033: step 2106, loss 0.562545.
Train: 2018-08-02T10:52:34.116702: step 2107, loss 0.587608.
Train: 2018-08-02T10:52:34.335424: step 2108, loss 0.489995.
Train: 2018-08-02T10:52:34.554129: step 2109, loss 0.579183.
Train: 2018-08-02T10:52:34.772827: step 2110, loss 0.554552.
Test: 2018-08-02T10:52:35.928776: step 2110, loss 0.548892.
Train: 2018-08-02T10:52:36.147501: step 2111, loss 0.513731.
Train: 2018-08-02T10:52:36.366200: step 2112, loss 0.546932.
Train: 2018-08-02T10:52:36.584897: step 2113, loss 0.636052.
Train: 2018-08-02T10:52:36.756708: step 2114, loss 0.580275.
Train: 2018-08-02T10:52:36.975440: step 2115, loss 0.562202.
Train: 2018-08-02T10:52:37.194135: step 2116, loss 0.554398.
Train: 2018-08-02T10:52:37.412828: step 2117, loss 0.603231.
Train: 2018-08-02T10:52:37.631538: step 2118, loss 0.538498.
Train: 2018-08-02T10:52:37.850235: step 2119, loss 0.530338.
Train: 2018-08-02T10:52:38.068931: step 2120, loss 0.563111.
Test: 2018-08-02T10:52:39.224880: step 2120, loss 0.54845.
Train: 2018-08-02T10:52:39.427957: step 2121, loss 0.538253.
Train: 2018-08-02T10:52:39.646657: step 2122, loss 0.513478.
Train: 2018-08-02T10:52:39.865385: step 2123, loss 0.619532.
Train: 2018-08-02T10:52:40.084085: step 2124, loss 0.562605.
Train: 2018-08-02T10:52:40.302753: step 2125, loss 0.513734.
Train: 2018-08-02T10:52:40.521481: step 2126, loss 0.579351.
Train: 2018-08-02T10:52:40.740180: step 2127, loss 0.570966.
Train: 2018-08-02T10:52:40.958879: step 2128, loss 0.562394.
Train: 2018-08-02T10:52:41.177578: step 2129, loss 0.595516.
Train: 2018-08-02T10:52:41.396247: step 2130, loss 0.571151.
Test: 2018-08-02T10:52:42.567847: step 2130, loss 0.548121.
Train: 2018-08-02T10:52:42.786545: step 2131, loss 0.521906.
Train: 2018-08-02T10:52:43.005269: step 2132, loss 0.578519.
Train: 2018-08-02T10:52:43.223977: step 2133, loss 0.521805.
Train: 2018-08-02T10:52:43.442672: step 2134, loss 0.554392.
Train: 2018-08-02T10:52:43.661365: step 2135, loss 0.538065.
Train: 2018-08-02T10:52:43.880069: step 2136, loss 0.521418.
Train: 2018-08-02T10:52:44.098772: step 2137, loss 0.661089.
Train: 2018-08-02T10:52:44.317467: step 2138, loss 0.545818.
Train: 2018-08-02T10:52:44.536137: step 2139, loss 0.603571.
Train: 2018-08-02T10:52:44.754865: step 2140, loss 0.546151.
Test: 2018-08-02T10:52:45.926435: step 2140, loss 0.548799.
Train: 2018-08-02T10:52:46.129543: step 2141, loss 0.570693.
Train: 2018-08-02T10:52:46.348245: step 2142, loss 0.52988.
Train: 2018-08-02T10:52:46.566940: step 2143, loss 0.513402.
Train: 2018-08-02T10:52:46.785610: step 2144, loss 0.644897.
Train: 2018-08-02T10:52:47.004338: step 2145, loss 0.661265.
Train: 2018-08-02T10:52:47.223040: step 2146, loss 0.57912.
Train: 2018-08-02T10:52:47.441731: step 2147, loss 0.586893.
Train: 2018-08-02T10:52:47.660435: step 2148, loss 0.595142.
Train: 2018-08-02T10:52:47.879128: step 2149, loss 0.635677.
Train: 2018-08-02T10:52:48.097827: step 2150, loss 0.54656.
Test: 2018-08-02T10:52:49.253781: step 2150, loss 0.548704.
Train: 2018-08-02T10:52:49.456884: step 2151, loss 0.506931.
Train: 2018-08-02T10:52:49.675587: step 2152, loss 0.650897.
Train: 2018-08-02T10:52:49.894286: step 2153, loss 0.586871.
Train: 2018-08-02T10:52:50.112955: step 2154, loss 0.539196.
Train: 2018-08-02T10:52:50.331684: step 2155, loss 0.649968.
Train: 2018-08-02T10:52:50.550377: step 2156, loss 0.618262.
Train: 2018-08-02T10:52:50.769082: step 2157, loss 0.571131.
Train: 2018-08-02T10:52:50.987751: step 2158, loss 0.602272.
Train: 2018-08-02T10:52:51.206449: step 2159, loss 0.58661.
Train: 2018-08-02T10:52:51.425148: step 2160, loss 0.509785.
Test: 2018-08-02T10:52:52.596748: step 2160, loss 0.551553.
Train: 2018-08-02T10:52:52.799826: step 2161, loss 0.556017.
Train: 2018-08-02T10:52:53.018556: step 2162, loss 0.601926.
Train: 2018-08-02T10:52:53.237254: step 2163, loss 0.525617.
Train: 2018-08-02T10:52:53.455922: step 2164, loss 0.563755.
Train: 2018-08-02T10:52:53.674651: step 2165, loss 0.632337.
Train: 2018-08-02T10:52:53.893320: step 2166, loss 0.563859.
Train: 2018-08-02T10:52:54.112019: step 2167, loss 0.510702.
Train: 2018-08-02T10:52:54.330747: step 2168, loss 0.540968.
Train: 2018-08-02T10:52:54.549446: step 2169, loss 0.662729.
Train: 2018-08-02T10:52:54.768140: step 2170, loss 0.609569.
Test: 2018-08-02T10:52:55.924094: step 2170, loss 0.550322.
Train: 2018-08-02T10:52:56.127171: step 2171, loss 0.548703.
Train: 2018-08-02T10:52:56.345904: step 2172, loss 0.563869.
Train: 2018-08-02T10:52:56.564600: step 2173, loss 0.526061.
Train: 2018-08-02T10:52:56.783300: step 2174, loss 0.563893.
Train: 2018-08-02T10:52:57.001997: step 2175, loss 0.609504.
Train: 2018-08-02T10:52:57.220690: step 2176, loss 0.579045.
Train: 2018-08-02T10:52:57.439394: step 2177, loss 0.518185.
Train: 2018-08-02T10:52:57.658064: step 2178, loss 0.525632.
Train: 2018-08-02T10:52:57.876792: step 2179, loss 0.571314.
Train: 2018-08-02T10:52:58.095461: step 2180, loss 0.532878.
Test: 2018-08-02T10:52:59.267061: step 2180, loss 0.549444.
Train: 2018-08-02T10:52:59.470173: step 2181, loss 0.571177.
Train: 2018-08-02T10:52:59.688868: step 2182, loss 0.509379.
Train: 2018-08-02T10:52:59.907566: step 2183, loss 0.540092.
Train: 2018-08-02T10:53:00.126265: step 2184, loss 0.508691.
Train: 2018-08-02T10:53:00.344934: step 2185, loss 0.571002.
Train: 2018-08-02T10:53:00.563663: step 2186, loss 0.594644.
Train: 2018-08-02T10:53:00.782359: step 2187, loss 0.539197.
Train: 2018-08-02T10:53:01.001030: step 2188, loss 0.586784.
Train: 2018-08-02T10:53:01.219763: step 2189, loss 0.642853.
Train: 2018-08-02T10:53:01.438454: step 2190, loss 0.562935.
Test: 2018-08-02T10:53:02.594407: step 2190, loss 0.548677.
Train: 2018-08-02T10:53:02.813106: step 2191, loss 0.506743.
Train: 2018-08-02T10:53:03.031835: step 2192, loss 0.562807.
Train: 2018-08-02T10:53:03.250533: step 2193, loss 0.562713.
Train: 2018-08-02T10:53:03.469227: step 2194, loss 0.570827.
Train: 2018-08-02T10:53:03.687931: step 2195, loss 0.554586.
Train: 2018-08-02T10:53:03.906630: step 2196, loss 0.546399.
Train: 2018-08-02T10:53:04.125328: step 2197, loss 0.570875.
Train: 2018-08-02T10:53:04.344031: step 2198, loss 0.627822.
Train: 2018-08-02T10:53:04.562722: step 2199, loss 0.570798.
Train: 2018-08-02T10:53:04.781426: step 2200, loss 0.554444.
Test: 2018-08-02T10:53:05.952995: step 2200, loss 0.549396.
Train: 2018-08-02T10:53:06.780957: step 2201, loss 0.530036.
Train: 2018-08-02T10:53:06.999655: step 2202, loss 0.538147.
Train: 2018-08-02T10:53:07.218357: step 2203, loss 0.497322.
Train: 2018-08-02T10:53:07.437024: step 2204, loss 0.570773.
Train: 2018-08-02T10:53:07.655752: step 2205, loss 0.54618.
Train: 2018-08-02T10:53:07.874420: step 2206, loss 0.603637.
Train: 2018-08-02T10:53:08.093153: step 2207, loss 0.529514.
Train: 2018-08-02T10:53:08.311848: step 2208, loss 0.6037.
Train: 2018-08-02T10:53:08.514896: step 2209, loss 0.554261.
Train: 2018-08-02T10:53:08.733625: step 2210, loss 0.570767.
Test: 2018-08-02T10:53:09.889573: step 2210, loss 0.549082.
Train: 2018-08-02T10:53:10.108303: step 2211, loss 0.53779.
Train: 2018-08-02T10:53:10.327000: step 2212, loss 0.587337.
Train: 2018-08-02T10:53:10.545702: step 2213, loss 0.603777.
Train: 2018-08-02T10:53:10.764369: step 2214, loss 0.612124.
Train: 2018-08-02T10:53:10.983067: step 2215, loss 0.595468.
Train: 2018-08-02T10:53:11.201799: step 2216, loss 0.579045.
Train: 2018-08-02T10:53:11.420466: step 2217, loss 0.603566.
Train: 2018-08-02T10:53:11.639195: step 2218, loss 0.587064.
Train: 2018-08-02T10:53:11.857888: step 2219, loss 0.570774.
Train: 2018-08-02T10:53:12.076592: step 2220, loss 0.562663.
Test: 2018-08-02T10:53:13.248162: step 2220, loss 0.549794.
Train: 2018-08-02T10:53:13.451240: step 2221, loss 0.506157.
Train: 2018-08-02T10:53:13.654348: step 2222, loss 0.594967.
Train: 2018-08-02T10:53:13.873046: step 2223, loss 0.562771.
Train: 2018-08-02T10:53:14.091714: step 2224, loss 0.538712.
Train: 2018-08-02T10:53:14.310444: step 2225, loss 0.578807.
Train: 2018-08-02T10:53:14.529142: step 2226, loss 0.57079.
Train: 2018-08-02T10:53:14.747841: step 2227, loss 0.530823.
Train: 2018-08-02T10:53:14.966543: step 2228, loss 0.530835.
Train: 2018-08-02T10:53:15.185239: step 2229, loss 0.514724.
Train: 2018-08-02T10:53:15.403938: step 2230, loss 0.602996.
Test: 2018-08-02T10:53:16.559887: step 2230, loss 0.549192.
Train: 2018-08-02T10:53:16.762994: step 2231, loss 0.570828.
Train: 2018-08-02T10:53:16.981694: step 2232, loss 0.594983.
Train: 2018-08-02T10:53:17.200390: step 2233, loss 0.530602.
Train: 2018-08-02T10:53:17.419092: step 2234, loss 0.57081.
Train: 2018-08-02T10:53:17.637790: step 2235, loss 0.63527.
Train: 2018-08-02T10:53:17.856488: step 2236, loss 0.578827.
Train: 2018-08-02T10:53:18.075187: step 2237, loss 0.538715.
Train: 2018-08-02T10:53:18.293887: step 2238, loss 0.466504.
Train: 2018-08-02T10:53:18.512579: step 2239, loss 0.53063.
Train: 2018-08-02T10:53:18.731277: step 2240, loss 0.546631.
Test: 2018-08-02T10:53:19.902854: step 2240, loss 0.549783.
Train: 2018-08-02T10:53:20.121553: step 2241, loss 0.481894.
Train: 2018-08-02T10:53:20.340257: step 2242, loss 0.546464.
Train: 2018-08-02T10:53:20.558981: step 2243, loss 0.546356.
Train: 2018-08-02T10:53:20.777679: step 2244, loss 0.603593.
Train: 2018-08-02T10:53:20.996372: step 2245, loss 0.587075.
Train: 2018-08-02T10:53:21.215080: step 2246, loss 0.471972.
Train: 2018-08-02T10:53:21.433775: step 2247, loss 0.504684.
Train: 2018-08-02T10:53:21.652474: step 2248, loss 0.479456.
Train: 2018-08-02T10:53:21.871173: step 2249, loss 0.562328.
Train: 2018-08-02T10:53:22.089875: step 2250, loss 0.612768.
Test: 2018-08-02T10:53:23.245821: step 2250, loss 0.548504.
Train: 2018-08-02T10:53:23.464553: step 2251, loss 0.579135.
Train: 2018-08-02T10:53:23.683249: step 2252, loss 0.477923.
Train: 2018-08-02T10:53:23.901942: step 2253, loss 0.460682.
Train: 2018-08-02T10:53:24.120646: step 2254, loss 0.536812.
Train: 2018-08-02T10:53:24.339344: step 2255, loss 0.596575.
Train: 2018-08-02T10:53:24.558044: step 2256, loss 0.579544.
Train: 2018-08-02T10:53:24.776731: step 2257, loss 0.562442.
Train: 2018-08-02T10:53:24.995442: step 2258, loss 0.527741.
Train: 2018-08-02T10:53:25.214143: step 2259, loss 0.562396.
Train: 2018-08-02T10:53:25.417188: step 2260, loss 0.562298.
Test: 2018-08-02T10:53:26.588789: step 2260, loss 0.547636.
Train: 2018-08-02T10:53:26.791890: step 2261, loss 0.55364.
Train: 2018-08-02T10:53:27.010596: step 2262, loss 0.536189.
Train: 2018-08-02T10:53:27.229293: step 2263, loss 0.483869.
Train: 2018-08-02T10:53:27.447963: step 2264, loss 0.553657.
Train: 2018-08-02T10:53:27.635419: step 2265, loss 0.543895.
Train: 2018-08-02T10:53:27.854116: step 2266, loss 0.553525.
Train: 2018-08-02T10:53:28.072817: step 2267, loss 0.518521.
Train: 2018-08-02T10:53:28.291545: step 2268, loss 0.597545.
Train: 2018-08-02T10:53:28.510238: step 2269, loss 0.527149.
Train: 2018-08-02T10:53:28.728913: step 2270, loss 0.615192.
Test: 2018-08-02T10:53:29.900513: step 2270, loss 0.547303.
Train: 2018-08-02T10:53:30.103590: step 2271, loss 0.527163.
Train: 2018-08-02T10:53:30.322319: step 2272, loss 0.509647.
Train: 2018-08-02T10:53:30.541022: step 2273, loss 0.509444.
Train: 2018-08-02T10:53:30.759717: step 2274, loss 0.606503.
Train: 2018-08-02T10:53:30.978415: step 2275, loss 0.579871.
Train: 2018-08-02T10:53:31.197109: step 2276, loss 0.606503.
Train: 2018-08-02T10:53:31.415811: step 2277, loss 0.597477.
Train: 2018-08-02T10:53:31.634513: step 2278, loss 0.57134.
Train: 2018-08-02T10:53:31.853211: step 2279, loss 0.5186.
Train: 2018-08-02T10:53:32.071909: step 2280, loss 0.544826.
Test: 2018-08-02T10:53:33.227858: step 2280, loss 0.549441.
Train: 2018-08-02T10:53:33.446582: step 2281, loss 0.492767.
Train: 2018-08-02T10:53:33.665286: step 2282, loss 0.46669.
Train: 2018-08-02T10:53:33.883985: step 2283, loss 0.518779.
Train: 2018-08-02T10:53:34.102683: step 2284, loss 0.501466.
Train: 2018-08-02T10:53:34.321383: step 2285, loss 0.562375.
Train: 2018-08-02T10:53:34.540082: step 2286, loss 0.536086.
Train: 2018-08-02T10:53:34.758780: step 2287, loss 0.544861.
Train: 2018-08-02T10:53:34.977479: step 2288, loss 0.518388.
Train: 2018-08-02T10:53:35.196148: step 2289, loss 0.588882.
Train: 2018-08-02T10:53:35.414847: step 2290, loss 0.544672.
Test: 2018-08-02T10:53:36.570826: step 2290, loss 0.547909.
Train: 2018-08-02T10:53:36.789554: step 2291, loss 0.606377.
Train: 2018-08-02T10:53:37.008248: step 2292, loss 0.553631.
Train: 2018-08-02T10:53:37.226953: step 2293, loss 0.553535.
Train: 2018-08-02T10:53:37.445651: step 2294, loss 0.527102.
Train: 2018-08-02T10:53:37.664350: step 2295, loss 0.536197.
Train: 2018-08-02T10:53:37.883049: step 2296, loss 0.562309.
Train: 2018-08-02T10:53:38.101747: step 2297, loss 0.614865.
Train: 2018-08-02T10:53:38.320415: step 2298, loss 0.580003.
Train: 2018-08-02T10:53:38.539145: step 2299, loss 0.53598.
Train: 2018-08-02T10:53:38.757844: step 2300, loss 0.562462.
Test: 2018-08-02T10:53:39.913793: step 2300, loss 0.547021.
Train: 2018-08-02T10:53:40.741724: step 2301, loss 0.579896.
Train: 2018-08-02T10:53:40.960447: step 2302, loss 0.605927.
Train: 2018-08-02T10:53:41.179152: step 2303, loss 0.484468.
Train: 2018-08-02T10:53:41.397821: step 2304, loss 0.562261.
Train: 2018-08-02T10:53:41.616520: step 2305, loss 0.570983.
Train: 2018-08-02T10:53:41.835218: step 2306, loss 0.544832.
Train: 2018-08-02T10:53:42.053946: step 2307, loss 0.605151.
Train: 2018-08-02T10:53:42.272646: step 2308, loss 0.630633.
Train: 2018-08-02T10:53:42.491344: step 2309, loss 0.511328.
Train: 2018-08-02T10:53:42.710014: step 2310, loss 0.63861.
Test: 2018-08-02T10:53:43.881614: step 2310, loss 0.548444.
Train: 2018-08-02T10:53:44.084725: step 2311, loss 0.638486.
Train: 2018-08-02T10:53:44.303423: step 2312, loss 0.613002.
Train: 2018-08-02T10:53:44.522088: step 2313, loss 0.562281.
Train: 2018-08-02T10:53:44.740818: step 2314, loss 0.554328.
Train: 2018-08-02T10:53:44.959520: step 2315, loss 0.562272.
Train: 2018-08-02T10:53:45.178215: step 2316, loss 0.513588.
Train: 2018-08-02T10:53:45.396914: step 2317, loss 0.546284.
Train: 2018-08-02T10:53:45.615613: step 2318, loss 0.489241.
Train: 2018-08-02T10:53:45.834281: step 2319, loss 0.554394.
Train: 2018-08-02T10:53:46.053011: step 2320, loss 0.497379.
Test: 2018-08-02T10:53:47.224581: step 2320, loss 0.548263.
Train: 2018-08-02T10:53:47.427689: step 2321, loss 0.562594.
Train: 2018-08-02T10:53:47.646383: step 2322, loss 0.513508.
Train: 2018-08-02T10:53:47.865086: step 2323, loss 0.562432.
Train: 2018-08-02T10:53:48.083788: step 2324, loss 0.505149.
Train: 2018-08-02T10:53:48.302483: step 2325, loss 0.603558.
Train: 2018-08-02T10:53:48.521152: step 2326, loss 0.513249.
Train: 2018-08-02T10:53:48.739882: step 2327, loss 0.512904.
Train: 2018-08-02T10:53:48.958574: step 2328, loss 0.612053.
Train: 2018-08-02T10:53:49.177249: step 2329, loss 0.587275.
Train: 2018-08-02T10:53:49.395977: step 2330, loss 0.554055.
Test: 2018-08-02T10:53:50.567548: step 2330, loss 0.549506.
Train: 2018-08-02T10:53:50.770651: step 2331, loss 0.587684.
Train: 2018-08-02T10:53:50.989324: step 2332, loss 0.595643.
Train: 2018-08-02T10:53:51.208024: step 2333, loss 0.579009.
Train: 2018-08-02T10:53:51.426752: step 2334, loss 0.604245.
Train: 2018-08-02T10:53:51.645420: step 2335, loss 0.521069.
Train: 2018-08-02T10:53:51.864149: step 2336, loss 0.66185.
Train: 2018-08-02T10:53:52.082843: step 2337, loss 0.529686.
Train: 2018-08-02T10:53:52.301547: step 2338, loss 0.603659.
Train: 2018-08-02T10:53:52.520246: step 2339, loss 0.562491.
Train: 2018-08-02T10:53:52.738939: step 2340, loss 0.537902.
Test: 2018-08-02T10:53:53.894894: step 2340, loss 0.548762.
Train: 2018-08-02T10:53:54.113622: step 2341, loss 0.562581.
Train: 2018-08-02T10:53:54.332321: step 2342, loss 0.546082.
Train: 2018-08-02T10:53:54.551020: step 2343, loss 0.578928.
Train: 2018-08-02T10:53:54.769719: step 2344, loss 0.465095.
Train: 2018-08-02T10:53:54.988418: step 2345, loss 0.489097.
Train: 2018-08-02T10:53:55.207119: step 2346, loss 0.546042.
Train: 2018-08-02T10:53:55.425816: step 2347, loss 0.562625.
Train: 2018-08-02T10:53:55.644514: step 2348, loss 0.562332.
Train: 2018-08-02T10:53:55.863213: step 2349, loss 0.587432.
Train: 2018-08-02T10:53:56.081915: step 2350, loss 0.620604.
Test: 2018-08-02T10:53:57.253482: step 2350, loss 0.54894.
Train: 2018-08-02T10:53:57.456585: step 2351, loss 0.463336.
Train: 2018-08-02T10:53:57.675290: step 2352, loss 0.595394.
Train: 2018-08-02T10:53:57.893988: step 2353, loss 0.57931.
Train: 2018-08-02T10:53:58.112686: step 2354, loss 0.570644.
Train: 2018-08-02T10:53:58.331356: step 2355, loss 0.52134.
Train: 2018-08-02T10:53:58.550084: step 2356, loss 0.554026.
Train: 2018-08-02T10:53:58.768753: step 2357, loss 0.570773.
Train: 2018-08-02T10:53:58.987451: step 2358, loss 0.56277.
Train: 2018-08-02T10:53:59.206184: step 2359, loss 0.520958.
Train: 2018-08-02T10:53:59.424879: step 2360, loss 0.59584.
Test: 2018-08-02T10:54:00.596449: step 2360, loss 0.549506.
Train: 2018-08-02T10:54:00.799551: step 2361, loss 0.56242.
Train: 2018-08-02T10:54:01.018227: step 2362, loss 0.529219.
Train: 2018-08-02T10:54:01.236925: step 2363, loss 0.537717.
Train: 2018-08-02T10:54:01.455659: step 2364, loss 0.545677.
Train: 2018-08-02T10:54:01.674352: step 2365, loss 0.578837.
Train: 2018-08-02T10:54:01.893046: step 2366, loss 0.604327.
Train: 2018-08-02T10:54:02.111751: step 2367, loss 0.579223.
Train: 2018-08-02T10:54:02.330420: step 2368, loss 0.645675.
Train: 2018-08-02T10:54:02.549148: step 2369, loss 0.687014.
Train: 2018-08-02T10:54:02.767846: step 2370, loss 0.587374.
Test: 2018-08-02T10:54:03.923796: step 2370, loss 0.549014.
Train: 2018-08-02T10:54:04.142494: step 2371, loss 0.562548.
Train: 2018-08-02T10:54:04.361218: step 2372, loss 0.529723.
Train: 2018-08-02T10:54:04.579916: step 2373, loss 0.530011.
Train: 2018-08-02T10:54:04.798620: step 2374, loss 0.619256.
Train: 2018-08-02T10:54:05.017320: step 2375, loss 0.611241.
Train: 2018-08-02T10:54:05.236018: step 2376, loss 0.594805.
Train: 2018-08-02T10:54:05.454720: step 2377, loss 0.554759.
Train: 2018-08-02T10:54:05.673385: step 2378, loss 0.554996.
Train: 2018-08-02T10:54:05.892088: step 2379, loss 0.499464.
Train: 2018-08-02T10:54:06.110813: step 2380, loss 0.515408.
Test: 2018-08-02T10:54:07.282384: step 2380, loss 0.54915.
Train: 2018-08-02T10:54:07.485492: step 2381, loss 0.555016.
Train: 2018-08-02T10:54:07.704193: step 2382, loss 0.578862.
Train: 2018-08-02T10:54:07.922890: step 2383, loss 0.539131.
Train: 2018-08-02T10:54:08.141591: step 2384, loss 0.531161.
Train: 2018-08-02T10:54:08.360281: step 2385, loss 0.642514.
Train: 2018-08-02T10:54:08.578956: step 2386, loss 0.586848.
Train: 2018-08-02T10:54:08.797684: step 2387, loss 0.626745.
Train: 2018-08-02T10:54:09.016382: step 2388, loss 0.547117.
Train: 2018-08-02T10:54:09.235082: step 2389, loss 0.602592.
Train: 2018-08-02T10:54:09.453784: step 2390, loss 0.579226.
Test: 2018-08-02T10:54:10.625351: step 2390, loss 0.549481.
Train: 2018-08-02T10:54:10.828453: step 2391, loss 0.594708.
Train: 2018-08-02T10:54:11.047128: step 2392, loss 0.5245.
Train: 2018-08-02T10:54:11.265856: step 2393, loss 0.492696.
Train: 2018-08-02T10:54:11.484555: step 2394, loss 0.555191.
Train: 2018-08-02T10:54:11.703257: step 2395, loss 0.626246.
Train: 2018-08-02T10:54:11.921922: step 2396, loss 0.507961.
Train: 2018-08-02T10:54:12.140655: step 2397, loss 0.531563.
Train: 2018-08-02T10:54:12.359321: step 2398, loss 0.515649.
Train: 2018-08-02T10:54:12.578019: step 2399, loss 0.586749.
Train: 2018-08-02T10:54:12.796718: step 2400, loss 0.507251.
Test: 2018-08-02T10:54:13.952697: step 2400, loss 0.549177.
Train: 2018-08-02T10:54:14.811901: step 2401, loss 0.594704.
Train: 2018-08-02T10:54:15.030595: step 2402, loss 0.57078.
Train: 2018-08-02T10:54:15.249299: step 2403, loss 0.619005.
Train: 2018-08-02T10:54:15.467967: step 2404, loss 0.570848.
Train: 2018-08-02T10:54:15.686704: step 2405, loss 0.618916.
Train: 2018-08-02T10:54:15.905394: step 2406, loss 0.554811.
Train: 2018-08-02T10:54:16.124094: step 2407, loss 0.59498.
Train: 2018-08-02T10:54:16.342793: step 2408, loss 0.562904.
Train: 2018-08-02T10:54:16.561491: step 2409, loss 0.587026.
Train: 2018-08-02T10:54:16.780160: step 2410, loss 0.554695.
Test: 2018-08-02T10:54:17.936139: step 2410, loss 0.549876.
Train: 2018-08-02T10:54:18.139241: step 2411, loss 0.586782.
Train: 2018-08-02T10:54:18.357945: step 2412, loss 0.547014.
Train: 2018-08-02T10:54:18.576648: step 2413, loss 0.578883.
Train: 2018-08-02T10:54:18.795344: step 2414, loss 0.586878.
Train: 2018-08-02T10:54:19.014036: step 2415, loss 0.554913.
Train: 2018-08-02T10:54:19.201467: step 2416, loss 0.477975.
Train: 2018-08-02T10:54:19.435788: step 2417, loss 0.594831.
Train: 2018-08-02T10:54:19.654487: step 2418, loss 0.642568.
Train: 2018-08-02T10:54:19.873210: step 2419, loss 0.562888.
Train: 2018-08-02T10:54:20.091915: step 2420, loss 0.523181.
Test: 2018-08-02T10:54:21.247864: step 2420, loss 0.54889.
Train: 2018-08-02T10:54:21.466562: step 2421, loss 0.64277.
Train: 2018-08-02T10:54:21.732156: step 2422, loss 0.570846.
Train: 2018-08-02T10:54:21.950857: step 2423, loss 0.539098.
Train: 2018-08-02T10:54:22.169554: step 2424, loss 0.52346.
Train: 2018-08-02T10:54:22.388246: step 2425, loss 0.618362.
Train: 2018-08-02T10:54:22.606951: step 2426, loss 0.570983.
Train: 2018-08-02T10:54:22.825653: step 2427, loss 0.523344.
Train: 2018-08-02T10:54:23.044319: step 2428, loss 0.642295.
Train: 2018-08-02T10:54:23.263047: step 2429, loss 0.594688.
Train: 2018-08-02T10:54:23.481715: step 2430, loss 0.547281.
Test: 2018-08-02T10:54:24.653315: step 2430, loss 0.548434.
Train: 2018-08-02T10:54:24.856395: step 2431, loss 0.58686.
Train: 2018-08-02T10:54:25.075117: step 2432, loss 0.633937.
Train: 2018-08-02T10:54:25.293791: step 2433, loss 0.523941.
Train: 2018-08-02T10:54:25.512520: step 2434, loss 0.539596.
Train: 2018-08-02T10:54:25.731218: step 2435, loss 0.555276.
Train: 2018-08-02T10:54:25.949921: step 2436, loss 0.555322.
Train: 2018-08-02T10:54:26.168617: step 2437, loss 0.602392.
Train: 2018-08-02T10:54:26.387310: step 2438, loss 0.578944.
Train: 2018-08-02T10:54:26.606014: step 2439, loss 0.555204.
Train: 2018-08-02T10:54:26.824682: step 2440, loss 0.571145.
Test: 2018-08-02T10:54:27.996283: step 2440, loss 0.549927.
Train: 2018-08-02T10:54:28.199394: step 2441, loss 0.46126.
Train: 2018-08-02T10:54:28.418089: step 2442, loss 0.618113.
Train: 2018-08-02T10:54:28.636788: step 2443, loss 0.547302.
Train: 2018-08-02T10:54:28.855488: step 2444, loss 0.468333.
Train: 2018-08-02T10:54:29.074189: step 2445, loss 0.562858.
Train: 2018-08-02T10:54:29.292885: step 2446, loss 0.58657.
Train: 2018-08-02T10:54:29.511554: step 2447, loss 0.523012.
Train: 2018-08-02T10:54:29.730252: step 2448, loss 0.546914.
Train: 2018-08-02T10:54:29.948984: step 2449, loss 0.63491.
Train: 2018-08-02T10:54:30.167680: step 2450, loss 0.594739.
Test: 2018-08-02T10:54:31.339249: step 2450, loss 0.548922.
Train: 2018-08-02T10:54:31.542327: step 2451, loss 0.490182.
Train: 2018-08-02T10:54:31.761056: step 2452, loss 0.571067.
Train: 2018-08-02T10:54:31.979750: step 2453, loss 0.522281.
Train: 2018-08-02T10:54:32.198425: step 2454, loss 0.603575.
Train: 2018-08-02T10:54:32.417154: step 2455, loss 0.554408.
Train: 2018-08-02T10:54:32.635853: step 2456, loss 0.521753.
Train: 2018-08-02T10:54:32.854551: step 2457, loss 0.513379.
Train: 2018-08-02T10:54:33.073219: step 2458, loss 0.56241.
Train: 2018-08-02T10:54:33.291917: step 2459, loss 0.537856.
Train: 2018-08-02T10:54:33.510616: step 2460, loss 0.521405.
Test: 2018-08-02T10:54:34.666596: step 2460, loss 0.549374.
Train: 2018-08-02T10:54:34.947813: step 2461, loss 0.463031.
Train: 2018-08-02T10:54:35.166509: step 2462, loss 0.587311.
Train: 2018-08-02T10:54:35.385178: step 2463, loss 0.587427.
Train: 2018-08-02T10:54:35.603906: step 2464, loss 0.545701.
Train: 2018-08-02T10:54:35.822575: step 2465, loss 0.570975.
Train: 2018-08-02T10:54:36.041305: step 2466, loss 0.629636.
Train: 2018-08-02T10:54:36.259973: step 2467, loss 0.579663.
Train: 2018-08-02T10:54:36.478697: step 2468, loss 0.562442.
Train: 2018-08-02T10:54:36.697405: step 2469, loss 0.604468.
Train: 2018-08-02T10:54:36.916100: step 2470, loss 0.587406.
Test: 2018-08-02T10:54:38.087670: step 2470, loss 0.548697.
Train: 2018-08-02T10:54:38.290772: step 2471, loss 0.554159.
Train: 2018-08-02T10:54:38.509476: step 2472, loss 0.55385.
Train: 2018-08-02T10:54:38.728145: step 2473, loss 0.554046.
Train: 2018-08-02T10:54:38.946877: step 2474, loss 0.587565.
Train: 2018-08-02T10:54:39.165572: step 2475, loss 0.57106.
Train: 2018-08-02T10:54:39.384272: step 2476, loss 0.520569.
Train: 2018-08-02T10:54:39.602965: step 2477, loss 0.58738.
Train: 2018-08-02T10:54:39.821673: step 2478, loss 0.5956.
Train: 2018-08-02T10:54:40.040368: step 2479, loss 0.570822.
Train: 2018-08-02T10:54:40.259063: step 2480, loss 0.587376.
Test: 2018-08-02T10:54:41.430637: step 2480, loss 0.549274.
Train: 2018-08-02T10:54:41.633745: step 2481, loss 0.603384.
Train: 2018-08-02T10:54:41.852443: step 2482, loss 0.587103.
Train: 2018-08-02T10:54:42.071145: step 2483, loss 0.546504.
Train: 2018-08-02T10:54:42.289844: step 2484, loss 0.562471.
Train: 2018-08-02T10:54:42.508510: step 2485, loss 0.513461.
Train: 2018-08-02T10:54:42.742831: step 2486, loss 0.611662.
Train: 2018-08-02T10:54:42.945907: step 2487, loss 0.562538.
Train: 2018-08-02T10:54:43.164639: step 2488, loss 0.619386.
Train: 2018-08-02T10:54:43.383304: step 2489, loss 0.514087.
Train: 2018-08-02T10:54:43.602034: step 2490, loss 0.546821.
Test: 2018-08-02T10:54:44.757983: step 2490, loss 0.548765.
Train: 2018-08-02T10:54:44.976712: step 2491, loss 0.530599.
Train: 2018-08-02T10:54:45.195382: step 2492, loss 0.538738.
Train: 2018-08-02T10:54:45.414110: step 2493, loss 0.579012.
Train: 2018-08-02T10:54:45.632808: step 2494, loss 0.554898.
Train: 2018-08-02T10:54:45.851508: step 2495, loss 0.554563.
Train: 2018-08-02T10:54:46.070209: step 2496, loss 0.562892.
Train: 2018-08-02T10:54:46.288904: step 2497, loss 0.602954.
Train: 2018-08-02T10:54:46.507603: step 2498, loss 0.530534.
Train: 2018-08-02T10:54:46.726303: step 2499, loss 0.522143.
Train: 2018-08-02T10:54:46.944971: step 2500, loss 0.578692.
Test: 2018-08-02T10:54:48.100950: step 2500, loss 0.547521.
Train: 2018-08-02T10:54:48.928911: step 2501, loss 0.497834.
Train: 2018-08-02T10:54:49.147580: step 2502, loss 0.603331.
Train: 2018-08-02T10:54:49.366309: step 2503, loss 0.627897.
Train: 2018-08-02T10:54:49.585008: step 2504, loss 0.54626.
Train: 2018-08-02T10:54:49.803706: step 2505, loss 0.554442.
Train: 2018-08-02T10:54:50.022376: step 2506, loss 0.530437.
Train: 2018-08-02T10:54:50.241104: step 2507, loss 0.587102.
Train: 2018-08-02T10:54:50.459773: step 2508, loss 0.587167.
Train: 2018-08-02T10:54:50.678502: step 2509, loss 0.570713.
Train: 2018-08-02T10:54:50.897203: step 2510, loss 0.465224.
Test: 2018-08-02T10:54:52.068771: step 2510, loss 0.550149.
Train: 2018-08-02T10:54:52.271849: step 2511, loss 0.5708.
Train: 2018-08-02T10:54:52.490547: step 2512, loss 0.587311.
Train: 2018-08-02T10:54:52.709276: step 2513, loss 0.620079.
Train: 2018-08-02T10:54:52.927975: step 2514, loss 0.579113.
Train: 2018-08-02T10:54:53.146668: step 2515, loss 0.603544.
Train: 2018-08-02T10:54:53.365366: step 2516, loss 0.554236.
Train: 2018-08-02T10:54:53.584042: step 2517, loss 0.50554.
Train: 2018-08-02T10:54:53.802768: step 2518, loss 0.49784.
Train: 2018-08-02T10:54:54.021472: step 2519, loss 0.562778.
Train: 2018-08-02T10:54:54.240168: step 2520, loss 0.521821.
Test: 2018-08-02T10:54:55.411738: step 2520, loss 0.548777.
Train: 2018-08-02T10:54:55.614816: step 2521, loss 0.586881.
Train: 2018-08-02T10:54:55.833544: step 2522, loss 0.636122.
Train: 2018-08-02T10:54:56.052243: step 2523, loss 0.54636.
Train: 2018-08-02T10:54:56.270943: step 2524, loss 0.578962.
Train: 2018-08-02T10:54:56.489610: step 2525, loss 0.538041.
Train: 2018-08-02T10:54:56.708311: step 2526, loss 0.595311.
Train: 2018-08-02T10:54:56.927040: step 2527, loss 0.586958.
Train: 2018-08-02T10:54:57.145737: step 2528, loss 0.570662.
Train: 2018-08-02T10:54:57.364437: step 2529, loss 0.570561.
Train: 2018-08-02T10:54:57.583104: step 2530, loss 0.627916.
Test: 2018-08-02T10:54:58.739084: step 2530, loss 0.547613.
Train: 2018-08-02T10:54:58.942195: step 2531, loss 0.521956.
Train: 2018-08-02T10:54:59.160890: step 2532, loss 0.554738.
Train: 2018-08-02T10:54:59.379583: step 2533, loss 0.63525.
Train: 2018-08-02T10:54:59.598288: step 2534, loss 0.546357.
Train: 2018-08-02T10:54:59.816990: step 2535, loss 0.538451.
Train: 2018-08-02T10:55:00.035686: step 2536, loss 0.489878.
Train: 2018-08-02T10:55:00.254386: step 2537, loss 0.578953.
Train: 2018-08-02T10:55:00.473083: step 2538, loss 0.643448.
Train: 2018-08-02T10:55:00.691785: step 2539, loss 0.603022.
Train: 2018-08-02T10:55:00.926097: step 2540, loss 0.562439.
Test: 2018-08-02T10:55:02.082051: step 2540, loss 0.547934.
Train: 2018-08-02T10:55:02.285128: step 2541, loss 0.595221.
Train: 2018-08-02T10:55:02.503857: step 2542, loss 0.52301.
Train: 2018-08-02T10:55:02.722556: step 2543, loss 0.514601.
Train: 2018-08-02T10:55:02.941258: step 2544, loss 0.531056.
Train: 2018-08-02T10:55:03.159948: step 2545, loss 0.530808.
Train: 2018-08-02T10:55:03.378653: step 2546, loss 0.538968.
Train: 2018-08-02T10:55:03.597351: step 2547, loss 0.594973.
Train: 2018-08-02T10:55:03.816053: step 2548, loss 0.538556.
Train: 2018-08-02T10:55:04.034749: step 2549, loss 0.571109.
Train: 2018-08-02T10:55:04.253448: step 2550, loss 0.586651.
Test: 2018-08-02T10:55:05.425018: step 2550, loss 0.549375.
Train: 2018-08-02T10:55:05.628095: step 2551, loss 0.56273.
Train: 2018-08-02T10:55:05.846820: step 2552, loss 0.579255.
Train: 2018-08-02T10:55:06.065494: step 2553, loss 0.554906.
Train: 2018-08-02T10:55:06.284222: step 2554, loss 0.497881.
Train: 2018-08-02T10:55:06.502892: step 2555, loss 0.514092.
Train: 2018-08-02T10:55:06.721620: step 2556, loss 0.562564.
Train: 2018-08-02T10:55:06.940313: step 2557, loss 0.570779.
Train: 2018-08-02T10:55:07.159018: step 2558, loss 0.497339.
Train: 2018-08-02T10:55:07.377716: step 2559, loss 0.57072.
Train: 2018-08-02T10:55:07.596384: step 2560, loss 0.521319.
Test: 2018-08-02T10:55:08.767985: step 2560, loss 0.548477.
Train: 2018-08-02T10:55:08.971063: step 2561, loss 0.53732.
Train: 2018-08-02T10:55:09.189761: step 2562, loss 0.504199.
Train: 2018-08-02T10:55:09.408491: step 2563, loss 0.537255.
Train: 2018-08-02T10:55:09.627189: step 2564, loss 0.537204.
Train: 2018-08-02T10:55:09.845889: step 2565, loss 0.562185.
Train: 2018-08-02T10:55:10.064582: step 2566, loss 0.562636.
Train: 2018-08-02T10:55:10.252037: step 2567, loss 0.579913.
Train: 2018-08-02T10:55:10.486366: step 2568, loss 0.630762.
Train: 2018-08-02T10:55:10.705062: step 2569, loss 0.622207.
Train: 2018-08-02T10:55:10.939386: step 2570, loss 0.613337.
Test: 2018-08-02T10:55:12.095332: step 2570, loss 0.547953.
Train: 2018-08-02T10:55:12.314030: step 2571, loss 0.528735.
Train: 2018-08-02T10:55:12.532759: step 2572, loss 0.571322.
Train: 2018-08-02T10:55:12.751452: step 2573, loss 0.596192.
Train: 2018-08-02T10:55:12.970157: step 2574, loss 0.570827.
Train: 2018-08-02T10:55:13.188858: step 2575, loss 0.528791.
Train: 2018-08-02T10:55:13.407554: step 2576, loss 0.587763.
Train: 2018-08-02T10:55:13.626253: step 2577, loss 0.645668.
Train: 2018-08-02T10:55:13.844952: step 2578, loss 0.504138.
Train: 2018-08-02T10:55:14.063654: step 2579, loss 0.521023.
Train: 2018-08-02T10:55:14.282349: step 2580, loss 0.529514.
Test: 2018-08-02T10:55:15.438298: step 2580, loss 0.549429.
Train: 2018-08-02T10:55:15.656997: step 2581, loss 0.554261.
Train: 2018-08-02T10:55:15.875729: step 2582, loss 0.529377.
Train: 2018-08-02T10:55:16.094395: step 2583, loss 0.645089.
Train: 2018-08-02T10:55:16.313127: step 2584, loss 0.58713.
Train: 2018-08-02T10:55:16.531822: step 2585, loss 0.529685.
Train: 2018-08-02T10:55:16.750522: step 2586, loss 0.529657.
Train: 2018-08-02T10:55:16.980278: step 2587, loss 0.595191.
Train: 2018-08-02T10:55:17.198982: step 2588, loss 0.529501.
Train: 2018-08-02T10:55:17.417651: step 2589, loss 0.463869.
Train: 2018-08-02T10:55:17.636375: step 2590, loss 0.496498.
Test: 2018-08-02T10:55:18.792330: step 2590, loss 0.548329.
Train: 2018-08-02T10:55:19.011053: step 2591, loss 0.537747.
Train: 2018-08-02T10:55:19.245348: step 2592, loss 0.553835.
Train: 2018-08-02T10:55:19.448425: step 2593, loss 0.587656.
Train: 2018-08-02T10:55:19.667125: step 2594, loss 0.587688.
Train: 2018-08-02T10:55:19.885854: step 2595, loss 0.545797.
Train: 2018-08-02T10:55:20.104523: step 2596, loss 0.595549.
Train: 2018-08-02T10:55:20.323251: step 2597, loss 0.545328.
Train: 2018-08-02T10:55:20.541952: step 2598, loss 0.596026.
Train: 2018-08-02T10:55:20.760648: step 2599, loss 0.545649.
Train: 2018-08-02T10:55:20.979348: step 2600, loss 0.537446.
Test: 2018-08-02T10:55:22.150918: step 2600, loss 0.548998.
Train: 2018-08-02T10:55:22.978851: step 2601, loss 0.545591.
Train: 2018-08-02T10:55:23.197548: step 2602, loss 0.545672.
Train: 2018-08-02T10:55:23.416277: step 2603, loss 0.603878.
Train: 2018-08-02T10:55:23.634945: step 2604, loss 0.612872.
Train: 2018-08-02T10:55:23.853669: step 2605, loss 0.646487.
Train: 2018-08-02T10:55:24.072343: step 2606, loss 0.504268.
Train: 2018-08-02T10:55:24.291073: step 2607, loss 0.612421.
Train: 2018-08-02T10:55:24.509771: step 2608, loss 0.570622.
Train: 2018-08-02T10:55:24.728439: step 2609, loss 0.521315.
Train: 2018-08-02T10:55:24.947138: step 2610, loss 0.521152.
Test: 2018-08-02T10:55:26.118739: step 2610, loss 0.549875.
Train: 2018-08-02T10:55:26.321843: step 2611, loss 0.579078.
Train: 2018-08-02T10:55:26.540546: step 2612, loss 0.546026.
Train: 2018-08-02T10:55:26.759247: step 2613, loss 0.488309.
Train: 2018-08-02T10:55:26.977943: step 2614, loss 0.570846.
Train: 2018-08-02T10:55:27.196644: step 2615, loss 0.529201.
Train: 2018-08-02T10:55:27.415341: step 2616, loss 0.513137.
Train: 2018-08-02T10:55:27.634034: step 2617, loss 0.529393.
Train: 2018-08-02T10:55:27.852714: step 2618, loss 0.570542.
Train: 2018-08-02T10:55:28.071436: step 2619, loss 0.645566.
Train: 2018-08-02T10:55:28.290106: step 2620, loss 0.562444.
Test: 2018-08-02T10:55:29.461706: step 2620, loss 0.549695.
Train: 2018-08-02T10:55:29.664785: step 2621, loss 0.496071.
Train: 2018-08-02T10:55:29.883512: step 2622, loss 0.545582.
Train: 2018-08-02T10:55:30.102213: step 2623, loss 0.512524.
Train: 2018-08-02T10:55:30.320910: step 2624, loss 0.587288.
Train: 2018-08-02T10:55:30.539609: step 2625, loss 0.5539.
Train: 2018-08-02T10:55:30.758277: step 2626, loss 0.595954.
Train: 2018-08-02T10:55:30.977006: step 2627, loss 0.604041.
Train: 2018-08-02T10:55:31.195705: step 2628, loss 0.621398.
Train: 2018-08-02T10:55:31.414375: step 2629, loss 0.596437.
Train: 2018-08-02T10:55:31.633103: step 2630, loss 0.537315.
Test: 2018-08-02T10:55:32.789052: step 2630, loss 0.548355.
Train: 2018-08-02T10:55:33.007750: step 2631, loss 0.57077.
Train: 2018-08-02T10:55:33.226479: step 2632, loss 0.487709.
Train: 2018-08-02T10:55:33.445179: step 2633, loss 0.554276.
Train: 2018-08-02T10:55:33.663878: step 2634, loss 0.537167.
Train: 2018-08-02T10:55:33.882576: step 2635, loss 0.595598.
Train: 2018-08-02T10:55:34.101275: step 2636, loss 0.604307.
Train: 2018-08-02T10:55:34.319968: step 2637, loss 0.587134.
Train: 2018-08-02T10:55:34.538642: step 2638, loss 0.586862.
Train: 2018-08-02T10:55:34.757371: step 2639, loss 0.562504.
Train: 2018-08-02T10:55:34.976070: step 2640, loss 0.521321.
Test: 2018-08-02T10:55:36.147640: step 2640, loss 0.54865.
Train: 2018-08-02T10:55:36.350718: step 2641, loss 0.603561.
Train: 2018-08-02T10:55:36.553820: step 2642, loss 0.505243.
Train: 2018-08-02T10:55:36.772525: step 2643, loss 0.529497.
Train: 2018-08-02T10:55:36.991223: step 2644, loss 0.620163.
Train: 2018-08-02T10:55:37.209892: step 2645, loss 0.603438.
Train: 2018-08-02T10:55:37.428616: step 2646, loss 0.620046.
Train: 2018-08-02T10:55:37.647290: step 2647, loss 0.513698.
Train: 2018-08-02T10:55:37.865989: step 2648, loss 0.554744.
Train: 2018-08-02T10:55:38.084717: step 2649, loss 0.611643.
Train: 2018-08-02T10:55:38.303415: step 2650, loss 0.554507.
Test: 2018-08-02T10:55:39.474986: step 2650, loss 0.548674.
Train: 2018-08-02T10:55:39.678099: step 2651, loss 0.56274.
Train: 2018-08-02T10:55:39.896788: step 2652, loss 0.61088.
Train: 2018-08-02T10:55:40.115494: step 2653, loss 0.530458.
Train: 2018-08-02T10:55:40.334190: step 2654, loss 0.602999.
Train: 2018-08-02T10:55:40.552860: step 2655, loss 0.562844.
Train: 2018-08-02T10:55:40.771589: step 2656, loss 0.530734.
Train: 2018-08-02T10:55:40.990287: step 2657, loss 0.498863.
Train: 2018-08-02T10:55:41.208985: step 2658, loss 0.562677.
Train: 2018-08-02T10:55:41.427684: step 2659, loss 0.546691.
Train: 2018-08-02T10:55:41.646384: step 2660, loss 0.514639.
Test: 2018-08-02T10:55:42.817953: step 2660, loss 0.54861.
Train: 2018-08-02T10:55:43.021030: step 2661, loss 0.610709.
Train: 2018-08-02T10:55:43.239755: step 2662, loss 0.619086.
Train: 2018-08-02T10:55:43.458428: step 2663, loss 0.610943.
Train: 2018-08-02T10:55:43.677159: step 2664, loss 0.587082.
Train: 2018-08-02T10:55:43.895826: step 2665, loss 0.555088.
Train: 2018-08-02T10:55:44.114555: step 2666, loss 0.635147.
Train: 2018-08-02T10:55:44.333253: step 2667, loss 0.530792.
Train: 2018-08-02T10:55:44.551947: step 2668, loss 0.578563.
Train: 2018-08-02T10:55:44.770653: step 2669, loss 0.586522.
Train: 2018-08-02T10:55:44.989321: step 2670, loss 0.54716.
Test: 2018-08-02T10:55:46.160920: step 2670, loss 0.549158.
Train: 2018-08-02T10:55:46.364028: step 2671, loss 0.56303.
Train: 2018-08-02T10:55:46.582727: step 2672, loss 0.595029.
Train: 2018-08-02T10:55:46.801428: step 2673, loss 0.523513.
Train: 2018-08-02T10:55:47.020119: step 2674, loss 0.53959.
Train: 2018-08-02T10:55:47.238823: step 2675, loss 0.515326.
Train: 2018-08-02T10:55:47.457493: step 2676, loss 0.59444.
Train: 2018-08-02T10:55:47.676191: step 2677, loss 0.602476.
Train: 2018-08-02T10:55:47.894921: step 2678, loss 0.55518.
Train: 2018-08-02T10:55:48.113589: step 2679, loss 0.578482.
Train: 2018-08-02T10:55:48.332288: step 2680, loss 0.586685.
Test: 2018-08-02T10:55:49.488266: step 2680, loss 0.549692.
Train: 2018-08-02T10:55:49.691345: step 2681, loss 0.602478.
Train: 2018-08-02T10:55:49.910074: step 2682, loss 0.523171.
Train: 2018-08-02T10:55:50.128741: step 2683, loss 0.515072.
Train: 2018-08-02T10:55:50.347470: step 2684, loss 0.555026.
Train: 2018-08-02T10:55:50.566169: step 2685, loss 0.491284.
Train: 2018-08-02T10:55:50.784841: step 2686, loss 0.611195.
Train: 2018-08-02T10:55:51.003561: step 2687, loss 0.626646.
Train: 2018-08-02T10:55:51.222265: step 2688, loss 0.570698.
Train: 2018-08-02T10:55:51.440964: step 2689, loss 0.547652.
Train: 2018-08-02T10:55:51.659664: step 2690, loss 0.450543.
Test: 2018-08-02T10:55:52.831233: step 2690, loss 0.548031.
Train: 2018-08-02T10:55:53.034341: step 2691, loss 0.603385.
Train: 2018-08-02T10:55:53.253040: step 2692, loss 0.571048.
Train: 2018-08-02T10:55:53.471739: step 2693, loss 0.602575.
Train: 2018-08-02T10:55:53.690437: step 2694, loss 0.570959.
Train: 2018-08-02T10:55:53.909137: step 2695, loss 0.586779.
Train: 2018-08-02T10:55:54.127835: step 2696, loss 0.522214.
Train: 2018-08-02T10:55:54.346505: step 2697, loss 0.505988.
Train: 2018-08-02T10:55:54.565233: step 2698, loss 0.513481.
Train: 2018-08-02T10:55:54.783935: step 2699, loss 0.530347.
Train: 2018-08-02T10:55:55.002630: step 2700, loss 0.562642.
Test: 2018-08-02T10:55:56.158579: step 2700, loss 0.548484.
Train: 2018-08-02T10:55:57.017785: step 2701, loss 0.627938.
Train: 2018-08-02T10:55:57.236482: step 2702, loss 0.513386.
Train: 2018-08-02T10:55:57.455181: step 2703, loss 0.587659.
Train: 2018-08-02T10:55:57.673850: step 2704, loss 0.521405.
Train: 2018-08-02T10:55:57.892548: step 2705, loss 0.587365.
Train: 2018-08-02T10:55:58.111247: step 2706, loss 0.612151.
Train: 2018-08-02T10:55:58.329976: step 2707, loss 0.529561.
Train: 2018-08-02T10:55:58.548670: step 2708, loss 0.496399.
Train: 2018-08-02T10:55:58.767375: step 2709, loss 0.538114.
Train: 2018-08-02T10:55:58.986073: step 2710, loss 0.471252.
Test: 2018-08-02T10:56:00.157642: step 2710, loss 0.548666.
Train: 2018-08-02T10:56:00.360745: step 2711, loss 0.504126.
Train: 2018-08-02T10:56:00.579449: step 2712, loss 0.604768.
Train: 2018-08-02T10:56:00.798118: step 2713, loss 0.604626.
Train: 2018-08-02T10:56:01.016818: step 2714, loss 0.537098.
Train: 2018-08-02T10:56:01.235546: step 2715, loss 0.511662.
Train: 2018-08-02T10:56:01.454215: step 2716, loss 0.604524.
Train: 2018-08-02T10:56:01.672913: step 2717, loss 0.605423.
Train: 2018-08-02T10:56:01.860400: step 2718, loss 0.453561.
Train: 2018-08-02T10:56:02.079068: step 2719, loss 0.553642.
Train: 2018-08-02T10:56:02.297797: step 2720, loss 0.579129.
Test: 2018-08-02T10:56:03.469367: step 2720, loss 0.548111.
Train: 2018-08-02T10:56:03.672469: step 2721, loss 0.579834.
Train: 2018-08-02T10:56:03.891144: step 2722, loss 0.519642.
Train: 2018-08-02T10:56:04.109843: step 2723, loss 0.55404.
Train: 2018-08-02T10:56:04.328543: step 2724, loss 0.562121.
Train: 2018-08-02T10:56:04.547240: step 2725, loss 0.622382.
Train: 2018-08-02T10:56:04.765939: step 2726, loss 0.596828.
Train: 2018-08-02T10:56:04.984662: step 2727, loss 0.579298.
Train: 2018-08-02T10:56:05.203367: step 2728, loss 0.571085.
Train: 2018-08-02T10:56:05.422065: step 2729, loss 0.562456.
Train: 2018-08-02T10:56:05.640765: step 2730, loss 0.57907.
Test: 2018-08-02T10:56:06.812334: step 2730, loss 0.548695.
Train: 2018-08-02T10:56:07.015411: step 2731, loss 0.595712.
Train: 2018-08-02T10:56:07.234141: step 2732, loss 0.528662.
Train: 2018-08-02T10:56:07.452840: step 2733, loss 0.612657.
Train: 2018-08-02T10:56:07.671532: step 2734, loss 0.620546.
Train: 2018-08-02T10:56:07.890244: step 2735, loss 0.579123.
Train: 2018-08-02T10:56:08.093311: step 2736, loss 0.669938.
Train: 2018-08-02T10:56:08.312013: step 2737, loss 0.529615.
Train: 2018-08-02T10:56:08.530713: step 2738, loss 0.611552.
Train: 2018-08-02T10:56:08.749411: step 2739, loss 0.554518.
Train: 2018-08-02T10:56:08.968109: step 2740, loss 0.546855.
Test: 2018-08-02T10:56:10.170923: step 2740, loss 0.549201.
Train: 2018-08-02T10:56:10.389623: step 2741, loss 0.522703.
Train: 2018-08-02T10:56:10.608350: step 2742, loss 0.466667.
Train: 2018-08-02T10:56:10.827020: step 2743, loss 0.555051.
Train: 2018-08-02T10:56:11.050753: step 2744, loss 0.522718.
Train: 2018-08-02T10:56:11.269476: step 2745, loss 0.546795.
Train: 2018-08-02T10:56:11.488151: step 2746, loss 0.61089.
Train: 2018-08-02T10:56:11.706879: step 2747, loss 0.570698.
Train: 2018-08-02T10:56:11.925550: step 2748, loss 0.570882.
Train: 2018-08-02T10:56:12.144248: step 2749, loss 0.594735.
Train: 2018-08-02T10:56:12.362976: step 2750, loss 0.603116.
Test: 2018-08-02T10:56:13.518925: step 2750, loss 0.548496.
Train: 2018-08-02T10:56:13.737648: step 2751, loss 0.54665.
Train: 2018-08-02T10:56:13.956348: step 2752, loss 0.651043.
Train: 2018-08-02T10:56:14.175046: step 2753, loss 0.562712.
Train: 2018-08-02T10:56:14.393745: step 2754, loss 0.555031.
Train: 2018-08-02T10:56:14.612449: step 2755, loss 0.555187.
Train: 2018-08-02T10:56:14.831118: step 2756, loss 0.515361.
Train: 2018-08-02T10:56:15.049847: step 2757, loss 0.570796.
Train: 2018-08-02T10:56:15.268515: step 2758, loss 0.586871.
Train: 2018-08-02T10:56:15.487244: step 2759, loss 0.507258.
Train: 2018-08-02T10:56:15.705943: step 2760, loss 0.483382.
Test: 2018-08-02T10:56:16.877513: step 2760, loss 0.549569.
Train: 2018-08-02T10:56:17.080616: step 2761, loss 0.578899.
Train: 2018-08-02T10:56:17.299291: step 2762, loss 0.515145.
Train: 2018-08-02T10:56:17.518019: step 2763, loss 0.643229.
Train: 2018-08-02T10:56:17.736711: step 2764, loss 0.522521.
Train: 2018-08-02T10:56:17.955416: step 2765, loss 0.562541.
Train: 2018-08-02T10:56:18.174117: step 2766, loss 0.514313.
Train: 2018-08-02T10:56:18.392814: step 2767, loss 0.554274.
Train: 2018-08-02T10:56:18.611512: step 2768, loss 0.611285.
Train: 2018-08-02T10:56:18.830211: step 2769, loss 0.530177.
Train: 2018-08-02T10:56:19.048910: step 2770, loss 0.562742.
Test: 2018-08-02T10:56:20.220481: step 2770, loss 0.548568.
Train: 2018-08-02T10:56:20.439211: step 2771, loss 0.513734.
Train: 2018-08-02T10:56:20.657878: step 2772, loss 0.611533.
Train: 2018-08-02T10:56:20.876577: step 2773, loss 0.521339.
Train: 2018-08-02T10:56:21.095276: step 2774, loss 0.579089.
Train: 2018-08-02T10:56:21.314005: step 2775, loss 0.587063.
Train: 2018-08-02T10:56:21.532707: step 2776, loss 0.52139.
Train: 2018-08-02T10:56:21.798267: step 2777, loss 0.520972.
Train: 2018-08-02T10:56:22.016966: step 2778, loss 0.587724.
Train: 2018-08-02T10:56:22.235664: step 2779, loss 0.504245.
Train: 2018-08-02T10:56:22.454333: step 2780, loss 0.487533.
Test: 2018-08-02T10:56:23.625933: step 2780, loss 0.548088.
Train: 2018-08-02T10:56:23.829041: step 2781, loss 0.55429.
Train: 2018-08-02T10:56:24.047740: step 2782, loss 0.570753.
Train: 2018-08-02T10:56:24.266408: step 2783, loss 0.553978.
Train: 2018-08-02T10:56:24.485137: step 2784, loss 0.571246.
Train: 2018-08-02T10:56:24.703836: step 2785, loss 0.545765.
Train: 2018-08-02T10:56:24.922504: step 2786, loss 0.570801.
Train: 2018-08-02T10:56:25.141234: step 2787, loss 0.562054.
Train: 2018-08-02T10:56:25.359932: step 2788, loss 0.612957.
Train: 2018-08-02T10:56:25.578631: step 2789, loss 0.494799.
Train: 2018-08-02T10:56:25.797330: step 2790, loss 0.519706.
Test: 2018-08-02T10:56:26.968900: step 2790, loss 0.547957.
Train: 2018-08-02T10:56:27.172002: step 2791, loss 0.545536.
Train: 2018-08-02T10:56:27.390677: step 2792, loss 0.545193.
Train: 2018-08-02T10:56:27.624997: step 2793, loss 0.528076.
Train: 2018-08-02T10:56:27.828111: step 2794, loss 0.536279.
Train: 2018-08-02T10:56:28.046803: step 2795, loss 0.476865.
Train: 2018-08-02T10:56:28.265503: step 2796, loss 0.493718.
Train: 2018-08-02T10:56:28.484201: step 2797, loss 0.544926.
Train: 2018-08-02T10:56:28.702899: step 2798, loss 0.579966.
Train: 2018-08-02T10:56:28.921598: step 2799, loss 0.562369.
Train: 2018-08-02T10:56:29.140298: step 2800, loss 0.544553.
Test: 2018-08-02T10:56:30.311867: step 2800, loss 0.545759.
Train: 2018-08-02T10:56:31.139830: step 2801, loss 0.60621.
Train: 2018-08-02T10:56:31.358527: step 2802, loss 0.500822.
Train: 2018-08-02T10:56:31.577226: step 2803, loss 0.606393.
Train: 2018-08-02T10:56:31.795926: step 2804, loss 0.588769.
Train: 2018-08-02T10:56:32.014625: step 2805, loss 0.536543.
Train: 2018-08-02T10:56:32.233323: step 2806, loss 0.641053.
Train: 2018-08-02T10:56:32.452021: step 2807, loss 0.597309.
Train: 2018-08-02T10:56:32.670689: step 2808, loss 0.597172.
Train: 2018-08-02T10:56:32.889420: step 2809, loss 0.553721.
Train: 2018-08-02T10:56:33.108118: step 2810, loss 0.502128.
Test: 2018-08-02T10:56:34.279688: step 2810, loss 0.547987.
Train: 2018-08-02T10:56:34.482765: step 2811, loss 0.536805.
Train: 2018-08-02T10:56:34.701465: step 2812, loss 0.614406.
Train: 2018-08-02T10:56:34.920190: step 2813, loss 0.54557.
Train: 2018-08-02T10:56:35.138862: step 2814, loss 0.545413.
Train: 2018-08-02T10:56:35.357591: step 2815, loss 0.613373.
Train: 2018-08-02T10:56:35.576259: step 2816, loss 0.553455.
Train: 2018-08-02T10:56:35.794958: step 2817, loss 0.604799.
Train: 2018-08-02T10:56:36.013688: step 2818, loss 0.469925.
Train: 2018-08-02T10:56:36.232386: step 2819, loss 0.596016.
Train: 2018-08-02T10:56:36.451080: step 2820, loss 0.579321.
Test: 2018-08-02T10:56:37.622655: step 2820, loss 0.548874.
Train: 2018-08-02T10:56:37.872598: step 2821, loss 0.554166.
Train: 2018-08-02T10:56:38.091321: step 2822, loss 0.53745.
Train: 2018-08-02T10:56:38.309995: step 2823, loss 0.603957.
Train: 2018-08-02T10:56:38.528724: step 2824, loss 0.57907.
Train: 2018-08-02T10:56:38.747422: step 2825, loss 0.488103.
Train: 2018-08-02T10:56:38.966091: step 2826, loss 0.612213.
Train: 2018-08-02T10:56:39.184790: step 2827, loss 0.545928.
Train: 2018-08-02T10:56:39.403519: step 2828, loss 0.529692.
Train: 2018-08-02T10:56:39.622212: step 2829, loss 0.628419.
Train: 2018-08-02T10:56:39.840917: step 2830, loss 0.5298.
Test: 2018-08-02T10:56:41.012486: step 2830, loss 0.548132.
Train: 2018-08-02T10:56:41.215564: step 2831, loss 0.570718.
Train: 2018-08-02T10:56:41.434296: step 2832, loss 0.521663.
Train: 2018-08-02T10:56:41.652992: step 2833, loss 0.521635.
Train: 2018-08-02T10:56:41.871691: step 2834, loss 0.578957.
Train: 2018-08-02T10:56:42.090390: step 2835, loss 0.562622.
Train: 2018-08-02T10:56:42.309088: step 2836, loss 0.587132.
Train: 2018-08-02T10:56:42.527781: step 2837, loss 0.595347.
Train: 2018-08-02T10:56:42.746456: step 2838, loss 0.529877.
Train: 2018-08-02T10:56:42.965185: step 2839, loss 0.578943.
Train: 2018-08-02T10:56:43.199504: step 2840, loss 0.587099.
Test: 2018-08-02T10:56:44.355454: step 2840, loss 0.548417.
Train: 2018-08-02T10:56:44.558562: step 2841, loss 0.58706.
Train: 2018-08-02T10:56:44.777261: step 2842, loss 0.546371.
Train: 2018-08-02T10:56:44.995958: step 2843, loss 0.513815.
Train: 2018-08-02T10:56:45.214654: step 2844, loss 0.546352.
Train: 2018-08-02T10:56:45.433356: step 2845, loss 0.619638.
Train: 2018-08-02T10:56:45.652056: step 2846, loss 0.570816.
Train: 2018-08-02T10:56:45.870754: step 2847, loss 0.522003.
Train: 2018-08-02T10:56:46.089452: step 2848, loss 0.546412.
Train: 2018-08-02T10:56:46.308152: step 2849, loss 0.562633.
Train: 2018-08-02T10:56:46.526851: step 2850, loss 0.603391.
Test: 2018-08-02T10:56:47.682799: step 2850, loss 0.548117.
Train: 2018-08-02T10:56:47.901529: step 2851, loss 0.595202.
Train: 2018-08-02T10:56:48.120228: step 2852, loss 0.538303.
Train: 2018-08-02T10:56:48.338896: step 2853, loss 0.530212.
Train: 2018-08-02T10:56:48.557625: step 2854, loss 0.619539.
Train: 2018-08-02T10:56:48.776324: step 2855, loss 0.57888.
Train: 2018-08-02T10:56:48.994993: step 2856, loss 0.554582.
Train: 2018-08-02T10:56:49.213722: step 2857, loss 0.562686.
Train: 2018-08-02T10:56:49.432420: step 2858, loss 0.506099.
Train: 2018-08-02T10:56:49.651120: step 2859, loss 0.595078.
Train: 2018-08-02T10:56:49.869818: step 2860, loss 0.481763.
Test: 2018-08-02T10:56:51.025767: step 2860, loss 0.549515.
Train: 2018-08-02T10:56:51.244495: step 2861, loss 0.611338.
Train: 2018-08-02T10:56:51.463194: step 2862, loss 0.627583.
Train: 2018-08-02T10:56:51.681894: step 2863, loss 0.546469.
Train: 2018-08-02T10:56:51.900586: step 2864, loss 0.570794.
Train: 2018-08-02T10:56:52.119286: step 2865, loss 0.562668.
Train: 2018-08-02T10:56:52.337960: step 2866, loss 0.562701.
Train: 2018-08-02T10:56:52.556683: step 2867, loss 0.611263.
Train: 2018-08-02T10:56:52.775389: step 2868, loss 0.546591.
Train: 2018-08-02T10:56:52.962843: step 2869, loss 0.631599.
Train: 2018-08-02T10:56:53.181513: step 2870, loss 0.514475.
Test: 2018-08-02T10:56:54.353112: step 2870, loss 0.550655.
Train: 2018-08-02T10:56:54.556189: step 2871, loss 0.562783.
Train: 2018-08-02T10:56:54.774920: step 2872, loss 0.474412.
Train: 2018-08-02T10:56:54.993614: step 2873, loss 0.562774.
Train: 2018-08-02T10:56:55.212316: step 2874, loss 0.562743.
Train: 2018-08-02T10:56:55.431010: step 2875, loss 0.530447.
Train: 2018-08-02T10:56:55.649715: step 2876, loss 0.603139.
Train: 2018-08-02T10:56:55.868408: step 2877, loss 0.546526.
Train: 2018-08-02T10:56:56.087112: step 2878, loss 0.52214.
Train: 2018-08-02T10:56:56.305810: step 2879, loss 0.595175.
Train: 2018-08-02T10:56:56.524479: step 2880, loss 0.595202.
Test: 2018-08-02T10:56:57.696080: step 2880, loss 0.548878.
Train: 2018-08-02T10:56:57.914808: step 2881, loss 0.570779.
Train: 2018-08-02T10:56:58.133508: step 2882, loss 0.513837.
Train: 2018-08-02T10:56:58.352206: step 2883, loss 0.578944.
Train: 2018-08-02T10:56:58.570876: step 2884, loss 0.578921.
Train: 2018-08-02T10:56:58.789573: step 2885, loss 0.530049.
Train: 2018-08-02T10:56:59.008304: step 2886, loss 0.529973.
Train: 2018-08-02T10:56:59.227002: step 2887, loss 0.685235.
Train: 2018-08-02T10:56:59.445701: step 2888, loss 0.595274.
Train: 2018-08-02T10:56:59.664399: step 2889, loss 0.603408.
Train: 2018-08-02T10:56:59.883099: step 2890, loss 0.587035.
Test: 2018-08-02T10:57:01.054668: step 2890, loss 0.549363.
Train: 2018-08-02T10:57:01.257776: step 2891, loss 0.578902.
Train: 2018-08-02T10:57:01.476475: step 2892, loss 0.619232.
Train: 2018-08-02T10:57:01.695173: step 2893, loss 0.594959.
Train: 2018-08-02T10:57:01.913872: step 2894, loss 0.530848.
Train: 2018-08-02T10:57:02.132566: step 2895, loss 0.554917.
Train: 2018-08-02T10:57:02.351270: step 2896, loss 0.491325.
Train: 2018-08-02T10:57:02.569963: step 2897, loss 0.547038.
Train: 2018-08-02T10:57:02.788668: step 2898, loss 0.570904.
Train: 2018-08-02T10:57:03.007360: step 2899, loss 0.475411.
Train: 2018-08-02T10:57:03.226059: step 2900, loss 0.554931.
Test: 2018-08-02T10:57:04.397635: step 2900, loss 0.549518.
Train: 2018-08-02T10:57:05.241187: step 2901, loss 0.60285.
Train: 2018-08-02T10:57:05.459917: step 2902, loss 0.530826.
Train: 2018-08-02T10:57:05.678587: step 2903, loss 0.530735.
Train: 2018-08-02T10:57:05.897315: step 2904, loss 0.578877.
Train: 2018-08-02T10:57:06.116013: step 2905, loss 0.562757.
Train: 2018-08-02T10:57:06.334706: step 2906, loss 0.554657.
Train: 2018-08-02T10:57:06.553405: step 2907, loss 0.554614.
Train: 2018-08-02T10:57:06.772111: step 2908, loss 0.530251.
Train: 2018-08-02T10:57:06.990803: step 2909, loss 0.562649.
Train: 2018-08-02T10:57:07.209507: step 2910, loss 0.53818.
Test: 2018-08-02T10:57:08.365456: step 2910, loss 0.549139.
Train: 2018-08-02T10:57:08.584154: step 2911, loss 0.57077.
Train: 2018-08-02T10:57:08.802884: step 2912, loss 0.562571.
Train: 2018-08-02T10:57:09.021583: step 2913, loss 0.570766.
Train: 2018-08-02T10:57:09.240277: step 2914, loss 0.546102.
Train: 2018-08-02T10:57:09.458980: step 2915, loss 0.628388.
Train: 2018-08-02T10:57:09.677679: step 2916, loss 0.611918.
Train: 2018-08-02T10:57:09.896379: step 2917, loss 0.480301.
Train: 2018-08-02T10:57:10.115046: step 2918, loss 0.554304.
Train: 2018-08-02T10:57:10.333745: step 2919, loss 0.529568.
Train: 2018-08-02T10:57:10.552445: step 2920, loss 0.53777.
Test: 2018-08-02T10:57:11.708423: step 2920, loss 0.548309.
Train: 2018-08-02T10:57:11.927123: step 2921, loss 0.537684.
Train: 2018-08-02T10:57:12.145848: step 2922, loss 0.545896.
Train: 2018-08-02T10:57:12.364550: step 2923, loss 0.537531.
Train: 2018-08-02T10:57:12.583248: step 2924, loss 0.570724.
Train: 2018-08-02T10:57:12.801949: step 2925, loss 0.5456.
Train: 2018-08-02T10:57:13.020647: step 2926, loss 0.570981.
Train: 2018-08-02T10:57:13.239316: step 2927, loss 0.554009.
Train: 2018-08-02T10:57:13.458040: step 2928, loss 0.512056.
Train: 2018-08-02T10:57:13.676713: step 2929, loss 0.553998.
Train: 2018-08-02T10:57:13.895437: step 2930, loss 0.579483.
Test: 2018-08-02T10:57:15.051390: step 2930, loss 0.54806.
Train: 2018-08-02T10:57:15.270090: step 2931, loss 0.570783.
Train: 2018-08-02T10:57:15.473167: step 2932, loss 0.621601.
Train: 2018-08-02T10:57:15.691896: step 2933, loss 0.545491.
Train: 2018-08-02T10:57:15.910564: step 2934, loss 0.56236.
Train: 2018-08-02T10:57:16.129294: step 2935, loss 0.66333.
Train: 2018-08-02T10:57:16.347993: step 2936, loss 0.528841.
Train: 2018-08-02T10:57:16.566691: step 2937, loss 0.612613.
Train: 2018-08-02T10:57:16.785359: step 2938, loss 0.5374.
Train: 2018-08-02T10:57:17.004058: step 2939, loss 0.56244.
Train: 2018-08-02T10:57:17.222783: step 2940, loss 0.529242.
Test: 2018-08-02T10:57:18.378736: step 2940, loss 0.547664.
Train: 2018-08-02T10:57:18.581844: step 2941, loss 0.504423.
Train: 2018-08-02T10:57:18.800543: step 2942, loss 0.637092.
Train: 2018-08-02T10:57:19.019239: step 2943, loss 0.512816.
Train: 2018-08-02T10:57:19.237911: step 2944, loss 0.587302.
Train: 2018-08-02T10:57:19.456609: step 2945, loss 0.496386.
Train: 2018-08-02T10:57:19.675332: step 2946, loss 0.603825.
Train: 2018-08-02T10:57:19.894032: step 2947, loss 0.537709.
Train: 2018-08-02T10:57:20.112707: step 2948, loss 0.537712.
Train: 2018-08-02T10:57:20.331404: step 2949, loss 0.562492.
Train: 2018-08-02T10:57:20.550136: step 2950, loss 0.661696.
Test: 2018-08-02T10:57:21.721703: step 2950, loss 0.550436.
Train: 2018-08-02T10:57:21.924781: step 2951, loss 0.537754.
Train: 2018-08-02T10:57:22.143481: step 2952, loss 0.570755.
Train: 2018-08-02T10:57:22.362179: step 2953, loss 0.562531.
Train: 2018-08-02T10:57:22.580877: step 2954, loss 0.587193.
Train: 2018-08-02T10:57:22.799576: step 2955, loss 0.644578.
Train: 2018-08-02T10:57:23.018305: step 2956, loss 0.578941.
Train: 2018-08-02T10:57:23.237004: step 2957, loss 0.55449.
Train: 2018-08-02T10:57:23.455703: step 2958, loss 0.587026.
Train: 2018-08-02T10:57:23.674401: step 2959, loss 0.578891.
Train: 2018-08-02T10:57:23.893101: step 2960, loss 0.49017.
Test: 2018-08-02T10:57:25.049049: step 2960, loss 0.54876.
Train: 2018-08-02T10:57:25.267748: step 2961, loss 0.627206.
Train: 2018-08-02T10:57:25.486477: step 2962, loss 0.522632.
Train: 2018-08-02T10:57:25.705145: step 2963, loss 0.60294.
Train: 2018-08-02T10:57:25.923875: step 2964, loss 0.530819.
Train: 2018-08-02T10:57:26.158165: step 2965, loss 0.554863.
Train: 2018-08-02T10:57:26.361273: step 2966, loss 0.562873.
Train: 2018-08-02T10:57:26.579971: step 2967, loss 0.56288.
Train: 2018-08-02T10:57:26.798641: step 2968, loss 0.570871.
Train: 2018-08-02T10:57:27.017338: step 2969, loss 0.60282.
Train: 2018-08-02T10:57:27.236067: step 2970, loss 0.48313.
Test: 2018-08-02T10:57:28.407638: step 2970, loss 0.549001.
Train: 2018-08-02T10:57:28.610746: step 2971, loss 0.570875.
Train: 2018-08-02T10:57:28.829444: step 2972, loss 0.530909.
Train: 2018-08-02T10:57:29.048143: step 2973, loss 0.538837.
Train: 2018-08-02T10:57:29.266843: step 2974, loss 0.562822.
Train: 2018-08-02T10:57:29.485535: step 2975, loss 0.522599.
Train: 2018-08-02T10:57:29.704239: step 2976, loss 0.522449.
Train: 2018-08-02T10:57:29.922938: step 2977, loss 0.578888.
Train: 2018-08-02T10:57:30.141608: step 2978, loss 0.497772.
Train: 2018-08-02T10:57:30.360307: step 2979, loss 0.595225.
Train: 2018-08-02T10:57:30.579034: step 2980, loss 0.57894.
Test: 2018-08-02T10:57:31.750605: step 2980, loss 0.55059.
Train: 2018-08-02T10:57:31.953682: step 2981, loss 0.562592.
Train: 2018-08-02T10:57:32.172407: step 2982, loss 0.595387.
Train: 2018-08-02T10:57:32.391104: step 2983, loss 0.529682.
Train: 2018-08-02T10:57:32.609778: step 2984, loss 0.521395.
Train: 2018-08-02T10:57:32.828502: step 2985, loss 0.587245.
Train: 2018-08-02T10:57:33.047176: step 2986, loss 0.496425.
Train: 2018-08-02T10:57:33.265906: step 2987, loss 0.479663.
Train: 2018-08-02T10:57:33.484604: step 2988, loss 0.620664.
Train: 2018-08-02T10:57:33.703302: step 2989, loss 0.604115.
Train: 2018-08-02T10:57:33.921971: step 2990, loss 0.620869.
Test: 2018-08-02T10:57:35.093572: step 2990, loss 0.549641.
Train: 2018-08-02T10:57:35.296650: step 2991, loss 0.562421.
Train: 2018-08-02T10:57:35.515379: step 2992, loss 0.512386.
Train: 2018-08-02T10:57:35.734047: step 2993, loss 0.654209.
Train: 2018-08-02T10:57:35.952748: step 2994, loss 0.604081.
Train: 2018-08-02T10:57:36.171447: step 2995, loss 0.512599.
Train: 2018-08-02T10:57:36.390143: step 2996, loss 0.529264.
Train: 2018-08-02T10:57:36.608873: step 2997, loss 0.562465.
Train: 2018-08-02T10:57:36.827573: step 2998, loss 0.545889.
Train: 2018-08-02T10:57:37.046240: step 2999, loss 0.537602.
Train: 2018-08-02T10:57:37.264969: step 3000, loss 0.587342.
Test: 2018-08-02T10:57:38.420918: step 3000, loss 0.549007.
Train: 2018-08-02T10:57:39.217636: step 3001, loss 0.595631.
Train: 2018-08-02T10:57:39.436305: step 3002, loss 0.587316.
Train: 2018-08-02T10:57:39.655003: step 3003, loss 0.512896.
Train: 2018-08-02T10:57:39.873704: step 3004, loss 0.554237.
Train: 2018-08-02T10:57:40.092427: step 3005, loss 0.612054.
Train: 2018-08-02T10:57:40.311125: step 3006, loss 0.529524.
Train: 2018-08-02T10:57:40.529830: step 3007, loss 0.603728.
Train: 2018-08-02T10:57:40.748498: step 3008, loss 0.54607.
Train: 2018-08-02T10:57:40.967227: step 3009, loss 0.5872.
Train: 2018-08-02T10:57:41.185896: step 3010, loss 0.595383.
Test: 2018-08-02T10:57:42.357496: step 3010, loss 0.549085.
Train: 2018-08-02T10:57:42.560598: step 3011, loss 0.513436.
Train: 2018-08-02T10:57:42.779302: step 3012, loss 0.578947.
Train: 2018-08-02T10:57:42.998002: step 3013, loss 0.587114.
Train: 2018-08-02T10:57:43.216700: step 3014, loss 0.619725.
Train: 2018-08-02T10:57:43.435399: step 3015, loss 0.538235.
Train: 2018-08-02T10:57:43.654093: step 3016, loss 0.611389.
Train: 2018-08-02T10:57:43.872797: step 3017, loss 0.53841.
Train: 2018-08-02T10:57:44.091495: step 3018, loss 0.562726.
Train: 2018-08-02T10:57:44.310189: step 3019, loss 0.554682.
Train: 2018-08-02T10:57:44.497651: step 3020, loss 0.562768.
Test: 2018-08-02T10:57:45.653600: step 3020, loss 0.55064.
Train: 2018-08-02T10:57:45.872298: step 3021, loss 0.514502.
Train: 2018-08-02T10:57:46.091021: step 3022, loss 0.506441.
Train: 2018-08-02T10:57:46.309726: step 3023, loss 0.49827.
Train: 2018-08-02T10:57:46.528394: step 3024, loss 0.538464.
Train: 2018-08-02T10:57:46.747129: step 3025, loss 0.562679.
Train: 2018-08-02T10:57:46.965824: step 3026, loss 0.513823.
Train: 2018-08-02T10:57:47.184521: step 3027, loss 0.497255.
Train: 2018-08-02T10:57:47.403189: step 3028, loss 0.562549.
Train: 2018-08-02T10:57:47.621889: step 3029, loss 0.513042.
Train: 2018-08-02T10:57:47.840617: step 3030, loss 0.562471.
Test: 2018-08-02T10:57:49.012188: step 3030, loss 0.547006.
Train: 2018-08-02T10:57:49.215290: step 3031, loss 0.595733.
Train: 2018-08-02T10:57:49.433965: step 3032, loss 0.604168.
Train: 2018-08-02T10:57:49.652693: step 3033, loss 0.629319.
Train: 2018-08-02T10:57:49.871387: step 3034, loss 0.545681.
Train: 2018-08-02T10:57:50.090090: step 3035, loss 0.554045.
Train: 2018-08-02T10:57:50.308759: step 3036, loss 0.579141.
Train: 2018-08-02T10:57:50.527458: step 3037, loss 0.562406.
Train: 2018-08-02T10:57:50.746182: step 3038, loss 0.595857.
Train: 2018-08-02T10:57:50.964855: step 3039, loss 0.537363.
Train: 2018-08-02T10:57:51.183584: step 3040, loss 0.529038.
Test: 2018-08-02T10:57:52.355155: step 3040, loss 0.548483.
Train: 2018-08-02T10:57:52.558232: step 3041, loss 0.612497.
Train: 2018-08-02T10:57:52.776961: step 3042, loss 0.587437.
Train: 2018-08-02T10:57:52.995661: step 3043, loss 0.662298.
Train: 2018-08-02T10:57:53.214330: step 3044, loss 0.545894.
Train: 2018-08-02T10:57:53.433058: step 3045, loss 0.554233.
Train: 2018-08-02T10:57:53.651726: step 3046, loss 0.603712.
Train: 2018-08-02T10:57:53.870456: step 3047, loss 0.488668.
Train: 2018-08-02T10:57:54.089123: step 3048, loss 0.546172.
Train: 2018-08-02T10:57:54.307853: step 3049, loss 0.603519.
Train: 2018-08-02T10:57:54.526521: step 3050, loss 0.562597.
Test: 2018-08-02T10:57:55.682501: step 3050, loss 0.549175.
Train: 2018-08-02T10:57:55.901199: step 3051, loss 0.513661.
Train: 2018-08-02T10:57:56.119898: step 3052, loss 0.562618.
Train: 2018-08-02T10:57:56.338622: step 3053, loss 0.497403.
Train: 2018-08-02T10:57:56.557326: step 3054, loss 0.546285.
Train: 2018-08-02T10:57:56.775994: step 3055, loss 0.55442.
Train: 2018-08-02T10:57:56.994731: step 3056, loss 0.603509.
Train: 2018-08-02T10:57:57.213424: step 3057, loss 0.497065.
Train: 2018-08-02T10:57:57.432092: step 3058, loss 0.562557.
Train: 2018-08-02T10:57:57.650790: step 3059, loss 0.636493.
Train: 2018-08-02T10:57:57.869520: step 3060, loss 0.562545.
Test: 2018-08-02T10:57:59.041090: step 3060, loss 0.549401.
Train: 2018-08-02T10:57:59.244191: step 3061, loss 0.587184.
Train: 2018-08-02T10:57:59.462897: step 3062, loss 0.496921.
Train: 2018-08-02T10:57:59.681595: step 3063, loss 0.55434.
Train: 2018-08-02T10:57:59.900293: step 3064, loss 0.620058.
Train: 2018-08-02T10:58:00.118993: step 3065, loss 0.546127.
Train: 2018-08-02T10:58:00.337691: step 3066, loss 0.595387.
Train: 2018-08-02T10:58:00.556393: step 3067, loss 0.611764.
Train: 2018-08-02T10:58:00.775059: step 3068, loss 0.529849.
Train: 2018-08-02T10:58:00.993788: step 3069, loss 0.497202.
Train: 2018-08-02T10:58:01.212487: step 3070, loss 0.513521.
Test: 2018-08-02T10:58:02.368436: step 3070, loss 0.548329.
Train: 2018-08-02T10:58:02.587164: step 3071, loss 0.578954.
Train: 2018-08-02T10:58:02.805832: step 3072, loss 0.546166.
Train: 2018-08-02T10:58:03.024532: step 3073, loss 0.628222.
Train: 2018-08-02T10:58:03.243230: step 3074, loss 0.562556.
Train: 2018-08-02T10:58:03.461961: step 3075, loss 0.677386.
Train: 2018-08-02T10:58:03.680658: step 3076, loss 0.587121.
Train: 2018-08-02T10:58:03.899358: step 3077, loss 0.538176.
Train: 2018-08-02T10:58:04.118057: step 3078, loss 0.538263.
Train: 2018-08-02T10:58:04.336755: step 3079, loss 0.570789.
Train: 2018-08-02T10:58:04.555454: step 3080, loss 0.587003.
Test: 2018-08-02T10:58:05.711402: step 3080, loss 0.549953.
Train: 2018-08-02T10:58:05.930125: step 3081, loss 0.554628.
Train: 2018-08-02T10:58:06.148824: step 3082, loss 0.554661.
Train: 2018-08-02T10:58:06.367530: step 3083, loss 0.514358.
Train: 2018-08-02T10:58:06.586228: step 3084, loss 0.538549.
Train: 2018-08-02T10:58:06.804921: step 3085, loss 0.522375.
Train: 2018-08-02T10:58:07.023625: step 3086, loss 0.514199.
Train: 2018-08-02T10:58:07.242294: step 3087, loss 0.570793.
Train: 2018-08-02T10:58:07.460993: step 3088, loss 0.53015.
Train: 2018-08-02T10:58:07.679722: step 3089, loss 0.546326.
Train: 2018-08-02T10:58:07.898420: step 3090, loss 0.513553.
Test: 2018-08-02T10:58:09.054370: step 3090, loss 0.547722.
Train: 2018-08-02T10:58:09.273069: step 3091, loss 0.480515.
Train: 2018-08-02T10:58:09.491793: step 3092, loss 0.58725.
Train: 2018-08-02T10:58:09.710496: step 3093, loss 0.554197.
Train: 2018-08-02T10:58:09.929195: step 3094, loss 0.562449.
Train: 2018-08-02T10:58:10.147893: step 3095, loss 0.63747.
Train: 2018-08-02T10:58:10.366562: step 3096, loss 0.520683.
Train: 2018-08-02T10:58:10.585260: step 3097, loss 0.612586.
Train: 2018-08-02T10:58:10.803989: step 3098, loss 0.604238.
Train: 2018-08-02T10:58:11.038281: step 3099, loss 0.51225.
Train: 2018-08-02T10:58:11.241389: step 3100, loss 0.51223.
Test: 2018-08-02T10:58:12.397337: step 3100, loss 0.548417.
Train: 2018-08-02T10:58:13.209647: step 3101, loss 0.512157.
Train: 2018-08-02T10:58:13.428376: step 3102, loss 0.545607.
Train: 2018-08-02T10:58:13.647074: step 3103, loss 0.579201.
Train: 2018-08-02T10:58:13.865744: step 3104, loss 0.528686.
Train: 2018-08-02T10:58:14.084471: step 3105, loss 0.596118.
Train: 2018-08-02T10:58:14.303171: step 3106, loss 0.646804.
Train: 2018-08-02T10:58:14.521838: step 3107, loss 0.579235.
Train: 2018-08-02T10:58:14.740569: step 3108, loss 0.570795.
Train: 2018-08-02T10:58:14.959266: step 3109, loss 0.511973.
Train: 2018-08-02T10:58:15.177935: step 3110, loss 0.520411.
Test: 2018-08-02T10:58:16.333915: step 3110, loss 0.547013.
Train: 2018-08-02T10:58:16.552614: step 3111, loss 0.587577.
Train: 2018-08-02T10:58:16.771342: step 3112, loss 0.587565.
Train: 2018-08-02T10:58:16.990043: step 3113, loss 0.512115.
Train: 2018-08-02T10:58:17.208710: step 3114, loss 0.621052.
Train: 2018-08-02T10:58:17.427440: step 3115, loss 0.453651.
Train: 2018-08-02T10:58:17.646138: step 3116, loss 0.570776.
Train: 2018-08-02T10:58:17.864838: step 3117, loss 0.503764.
Train: 2018-08-02T10:58:18.083506: step 3118, loss 0.595951.
Train: 2018-08-02T10:58:18.302203: step 3119, loss 0.553996.
Train: 2018-08-02T10:58:18.520933: step 3120, loss 0.579184.
Test: 2018-08-02T10:58:19.676881: step 3120, loss 0.547976.
Train: 2018-08-02T10:58:19.895605: step 3121, loss 0.688354.
Train: 2018-08-02T10:58:20.129901: step 3122, loss 0.637761.
Train: 2018-08-02T10:58:20.332978: step 3123, loss 0.587437.
Train: 2018-08-02T10:58:20.551708: step 3124, loss 0.595649.
Train: 2018-08-02T10:58:20.770406: step 3125, loss 0.554246.
Train: 2018-08-02T10:58:20.989106: step 3126, loss 0.529667.
Train: 2018-08-02T10:58:21.207804: step 3127, loss 0.554382.
Train: 2018-08-02T10:58:21.426502: step 3128, loss 0.578938.
Train: 2018-08-02T10:58:21.645201: step 3129, loss 0.521914.
Train: 2018-08-02T10:58:21.879518: step 3130, loss 0.619563.
Test: 2018-08-02T10:58:23.051092: step 3130, loss 0.548401.
Train: 2018-08-02T10:58:23.269815: step 3131, loss 0.587007.
Train: 2018-08-02T10:58:23.488520: step 3132, loss 0.570806.
Train: 2018-08-02T10:58:23.707218: step 3133, loss 0.586937.
Train: 2018-08-02T10:58:23.925917: step 3134, loss 0.530677.
Train: 2018-08-02T10:58:24.144616: step 3135, loss 0.562834.
Train: 2018-08-02T10:58:24.363316: step 3136, loss 0.546855.
Train: 2018-08-02T10:58:24.581984: step 3137, loss 0.554882.
Train: 2018-08-02T10:58:24.800712: step 3138, loss 0.538921.
Train: 2018-08-02T10:58:25.019410: step 3139, loss 0.610816.
Train: 2018-08-02T10:58:25.238080: step 3140, loss 0.538957.
Test: 2018-08-02T10:58:26.394060: step 3140, loss 0.550128.
Train: 2018-08-02T10:58:26.612757: step 3141, loss 0.562902.
Train: 2018-08-02T10:58:26.831486: step 3142, loss 0.554925.
Train: 2018-08-02T10:58:27.050185: step 3143, loss 0.594821.
Train: 2018-08-02T10:58:27.268879: step 3144, loss 0.57886.
Train: 2018-08-02T10:58:27.487584: step 3145, loss 0.523058.
Train: 2018-08-02T10:58:27.706278: step 3146, loss 0.523032.
Train: 2018-08-02T10:58:27.924981: step 3147, loss 0.514961.
Train: 2018-08-02T10:58:28.143679: step 3148, loss 0.562848.
Train: 2018-08-02T10:58:28.362379: step 3149, loss 0.482531.
Train: 2018-08-02T10:58:28.581096: step 3150, loss 0.554695.
Test: 2018-08-02T10:58:29.752648: step 3150, loss 0.549374.
Train: 2018-08-02T10:58:29.955750: step 3151, loss 0.595083.
Train: 2018-08-02T10:58:30.174455: step 3152, loss 0.578906.
Train: 2018-08-02T10:58:30.393153: step 3153, loss 0.49756.
Train: 2018-08-02T10:58:30.611857: step 3154, loss 0.619756.
Train: 2018-08-02T10:58:30.830550: step 3155, loss 0.611663.
Train: 2018-08-02T10:58:31.049249: step 3156, loss 0.546216.
Train: 2018-08-02T10:58:31.267948: step 3157, loss 0.554384.
Train: 2018-08-02T10:58:31.486616: step 3158, loss 0.570763.
Train: 2018-08-02T10:58:31.705346: step 3159, loss 0.578964.
Train: 2018-08-02T10:58:31.924044: step 3160, loss 0.603573.
Test: 2018-08-02T10:58:33.095614: step 3160, loss 0.54869.
Train: 2018-08-02T10:58:33.298692: step 3161, loss 0.595351.
Train: 2018-08-02T10:58:33.517421: step 3162, loss 0.48075.
Train: 2018-08-02T10:58:33.736120: step 3163, loss 0.587139.
Train: 2018-08-02T10:58:33.954819: step 3164, loss 0.587136.
Train: 2018-08-02T10:58:34.173517: step 3165, loss 0.587126.
Train: 2018-08-02T10:58:34.392216: step 3166, loss 0.595278.
Train: 2018-08-02T10:58:34.610916: step 3167, loss 0.587082.
Train: 2018-08-02T10:58:34.829613: step 3168, loss 0.578916.
Train: 2018-08-02T10:58:35.048313: step 3169, loss 0.570789.
Train: 2018-08-02T10:58:35.266981: step 3170, loss 0.578895.
Test: 2018-08-02T10:58:36.438581: step 3170, loss 0.549057.
Train: 2018-08-02T10:58:36.610441: step 3171, loss 0.614425.
Train: 2018-08-02T10:58:36.813525: step 3172, loss 0.619123.
Train: 2018-08-02T10:58:37.032223: step 3173, loss 0.546808.
Train: 2018-08-02T10:58:37.250922: step 3174, loss 0.546914.
Train: 2018-08-02T10:58:37.469620: step 3175, loss 0.491235.
Train: 2018-08-02T10:58:37.688319: step 3176, loss 0.531088.
Train: 2018-08-02T10:58:37.907017: step 3177, loss 0.570894.
Train: 2018-08-02T10:58:38.125718: step 3178, loss 0.658541.
Train: 2018-08-02T10:58:38.344416: step 3179, loss 0.562953.
Train: 2018-08-02T10:58:38.563085: step 3180, loss 0.594739.
Test: 2018-08-02T10:58:39.734684: step 3180, loss 0.549048.
Train: 2018-08-02T10:58:39.953383: step 3181, loss 0.515468.
Train: 2018-08-02T10:58:40.172113: step 3182, loss 0.539261.
Train: 2018-08-02T10:58:40.390812: step 3183, loss 0.570938.
Train: 2018-08-02T10:58:40.609511: step 3184, loss 0.523399.
Train: 2018-08-02T10:58:40.828179: step 3185, loss 0.547126.
Train: 2018-08-02T10:58:41.046909: step 3186, loss 0.499394.
Train: 2018-08-02T10:58:41.265607: step 3187, loss 0.539.
Train: 2018-08-02T10:58:41.484306: step 3188, loss 0.594864.
Train: 2018-08-02T10:58:41.703004: step 3189, loss 0.586888.
Train: 2018-08-02T10:58:41.921703: step 3190, loss 0.514593.
Test: 2018-08-02T10:58:43.093274: step 3190, loss 0.55042.
Train: 2018-08-02T10:58:43.296351: step 3191, loss 0.546649.
Train: 2018-08-02T10:58:43.515049: step 3192, loss 0.627377.
Train: 2018-08-02T10:58:43.733748: step 3193, loss 0.570801.
Train: 2018-08-02T10:58:43.952477: step 3194, loss 0.538403.
Train: 2018-08-02T10:58:44.171176: step 3195, loss 0.619453.
Train: 2018-08-02T10:58:44.389874: step 3196, loss 0.603232.
Train: 2018-08-02T10:58:44.608568: step 3197, loss 0.578898.
Train: 2018-08-02T10:58:44.827272: step 3198, loss 0.570801.
Train: 2018-08-02T10:58:45.045967: step 3199, loss 0.578889.
Train: 2018-08-02T10:58:45.264670: step 3200, loss 0.603098.
Test: 2018-08-02T10:58:46.436240: step 3200, loss 0.549135.
Train: 2018-08-02T10:58:47.217309: step 3201, loss 0.578877.
Train: 2018-08-02T10:58:47.436035: step 3202, loss 0.570837.
Train: 2018-08-02T10:58:47.654735: step 3203, loss 0.562832.
Train: 2018-08-02T10:58:47.873404: step 3204, loss 0.570862.
Train: 2018-08-02T10:58:48.092102: step 3205, loss 0.554899.
Train: 2018-08-02T10:58:48.310833: step 3206, loss 0.554929.
Train: 2018-08-02T10:58:48.529525: step 3207, loss 0.55495.
Train: 2018-08-02T10:58:48.748199: step 3208, loss 0.53903.
Train: 2018-08-02T10:58:48.966929: step 3209, loss 0.570891.
Train: 2018-08-02T10:58:49.185596: step 3210, loss 0.562921.
Test: 2018-08-02T10:58:50.357198: step 3210, loss 0.549971.
Train: 2018-08-02T10:58:50.560299: step 3211, loss 0.515087.
Train: 2018-08-02T10:58:50.779004: step 3212, loss 0.562893.
Train: 2018-08-02T10:58:50.997703: step 3213, loss 0.594854.
Train: 2018-08-02T10:58:51.216402: step 3214, loss 0.522857.
Train: 2018-08-02T10:58:51.435100: step 3215, loss 0.530781.
Train: 2018-08-02T10:58:51.653799: step 3216, loss 0.570838.
Train: 2018-08-02T10:58:51.872467: step 3217, loss 0.562777.
Train: 2018-08-02T10:58:52.091197: step 3218, loss 0.59501.
Train: 2018-08-02T10:58:52.309895: step 3219, loss 0.554668.
Train: 2018-08-02T10:58:52.528595: step 3220, loss 0.546564.
Test: 2018-08-02T10:58:53.700164: step 3220, loss 0.548448.
Train: 2018-08-02T10:58:53.903276: step 3221, loss 0.578893.
Train: 2018-08-02T10:58:54.106344: step 3222, loss 0.522195.
Train: 2018-08-02T10:58:54.325049: step 3223, loss 0.627596.
Train: 2018-08-02T10:58:54.543747: step 3224, loss 0.578905.
Train: 2018-08-02T10:58:54.762446: step 3225, loss 0.546446.
Train: 2018-08-02T10:58:54.981166: step 3226, loss 0.489628.
Train: 2018-08-02T10:58:55.199843: step 3227, loss 0.521991.
Train: 2018-08-02T10:58:55.418542: step 3228, loss 0.554467.
Train: 2018-08-02T10:58:55.637242: step 3229, loss 0.570768.
Train: 2018-08-02T10:58:55.855910: step 3230, loss 0.537992.
Test: 2018-08-02T10:58:57.027513: step 3230, loss 0.548641.
Train: 2018-08-02T10:58:57.230588: step 3231, loss 0.496843.
Train: 2018-08-02T10:58:57.449287: step 3232, loss 0.513053.
Train: 2018-08-02T10:58:57.668015: step 3233, loss 0.579036.
Train: 2018-08-02T10:58:57.886714: step 3234, loss 0.562451.
Train: 2018-08-02T10:58:58.105414: step 3235, loss 0.562431.
Train: 2018-08-02T10:58:58.324112: step 3236, loss 0.671035.
Train: 2018-08-02T10:58:58.542815: step 3237, loss 0.57077.
Train: 2018-08-02T10:58:58.761510: step 3238, loss 0.562419.
Train: 2018-08-02T10:58:58.980209: step 3239, loss 0.637527.
Train: 2018-08-02T10:58:59.198902: step 3240, loss 0.570763.
Test: 2018-08-02T10:59:00.354856: step 3240, loss 0.549729.
Train: 2018-08-02T10:59:00.573554: step 3241, loss 0.529229.
Train: 2018-08-02T10:59:00.792284: step 3242, loss 0.587345.
Train: 2018-08-02T10:59:01.010956: step 3243, loss 0.545925.
Train: 2018-08-02T10:59:01.229652: step 3244, loss 0.554226.
Train: 2018-08-02T10:59:01.448350: step 3245, loss 0.579011.
Train: 2018-08-02T10:59:01.667079: step 3246, loss 0.669661.
Train: 2018-08-02T10:59:01.885748: step 3247, loss 0.57897.
Train: 2018-08-02T10:59:02.104447: step 3248, loss 0.529879.
Train: 2018-08-02T10:59:02.323176: step 3249, loss 0.54631.
Train: 2018-08-02T10:59:02.541874: step 3250, loss 0.603329.
Test: 2018-08-02T10:59:03.713444: step 3250, loss 0.5495.
Train: 2018-08-02T10:59:03.916522: step 3251, loss 0.660041.
Train: 2018-08-02T10:59:04.135221: step 3252, loss 0.53044.
Train: 2018-08-02T10:59:04.369542: step 3253, loss 0.578875.
Train: 2018-08-02T10:59:04.572649: step 3254, loss 0.5468.
Train: 2018-08-02T10:59:04.806969: step 3255, loss 0.538889.
Train: 2018-08-02T10:59:05.010041: step 3256, loss 0.610783.
Train: 2018-08-02T10:59:05.228746: step 3257, loss 0.547022.
Train: 2018-08-02T10:59:05.447444: step 3258, loss 0.578858.
Train: 2018-08-02T10:59:05.666144: step 3259, loss 0.563001.
Train: 2018-08-02T10:59:05.884812: step 3260, loss 0.555112.
Test: 2018-08-02T10:59:07.056412: step 3260, loss 0.550205.
Train: 2018-08-02T10:59:07.259515: step 3261, loss 0.539325.
Train: 2018-08-02T10:59:07.478212: step 3262, loss 0.53143.
Train: 2018-08-02T10:59:07.696918: step 3263, loss 0.586771.
Train: 2018-08-02T10:59:07.915587: step 3264, loss 0.586773.
Train: 2018-08-02T10:59:08.134284: step 3265, loss 0.57886.
Train: 2018-08-02T10:59:08.353014: step 3266, loss 0.563046.
Train: 2018-08-02T10:59:08.571707: step 3267, loss 0.499804.
Train: 2018-08-02T10:59:08.790381: step 3268, loss 0.586777.
Train: 2018-08-02T10:59:09.009105: step 3269, loss 0.547158.
Train: 2018-08-02T10:59:09.227809: step 3270, loss 0.45188.
Test: 2018-08-02T10:59:10.399379: step 3270, loss 0.549613.
Train: 2018-08-02T10:59:10.602487: step 3271, loss 0.634641.
Train: 2018-08-02T10:59:10.821180: step 3272, loss 0.530949.
Train: 2018-08-02T10:59:11.039853: step 3273, loss 0.514808.
Train: 2018-08-02T10:59:11.258583: step 3274, loss 0.578872.
Train: 2018-08-02T10:59:11.472692: step 3275, loss 0.57888.
Train: 2018-08-02T10:59:11.691424: step 3276, loss 0.570807.
Train: 2018-08-02T10:59:11.910122: step 3277, loss 0.562702.
Train: 2018-08-02T10:59:12.128821: step 3278, loss 0.522124.
Train: 2018-08-02T10:59:12.347521: step 3279, loss 0.505722.
Train: 2018-08-02T10:59:12.566219: step 3280, loss 0.538121.
Test: 2018-08-02T10:59:13.737789: step 3280, loss 0.548129.
Train: 2018-08-02T10:59:13.940866: step 3281, loss 0.521596.
Train: 2018-08-02T10:59:14.159596: step 3282, loss 0.529605.
Train: 2018-08-02T10:59:14.378288: step 3283, loss 0.595563.
Train: 2018-08-02T10:59:14.596963: step 3284, loss 0.587351.
Train: 2018-08-02T10:59:14.815661: step 3285, loss 0.570761.
Train: 2018-08-02T10:59:15.034391: step 3286, loss 0.54577.
Train: 2018-08-02T10:59:15.253089: step 3287, loss 0.512337.
Train: 2018-08-02T10:59:15.471789: step 3288, loss 0.562405.
Train: 2018-08-02T10:59:15.690457: step 3289, loss 0.570782.
Train: 2018-08-02T10:59:15.909156: step 3290, loss 0.587596.
Test: 2018-08-02T10:59:17.065135: step 3290, loss 0.549495.
Train: 2018-08-02T10:59:17.283864: step 3291, loss 0.604435.
Train: 2018-08-02T10:59:17.502563: step 3292, loss 0.612832.
Train: 2018-08-02T10:59:17.721262: step 3293, loss 0.528808.
Train: 2018-08-02T10:59:17.939954: step 3294, loss 0.528839.
Train: 2018-08-02T10:59:18.158629: step 3295, loss 0.562394.
Train: 2018-08-02T10:59:18.377359: step 3296, loss 0.64625.
Train: 2018-08-02T10:59:18.596056: step 3297, loss 0.537306.
Train: 2018-08-02T10:59:18.814725: step 3298, loss 0.562416.
Train: 2018-08-02T10:59:19.033423: step 3299, loss 0.545744.
Train: 2018-08-02T10:59:19.252151: step 3300, loss 0.529108.
Test: 2018-08-02T10:59:20.423724: step 3300, loss 0.548527.
Train: 2018-08-02T10:59:21.236058: step 3301, loss 0.562435.
Train: 2018-08-02T10:59:21.454765: step 3302, loss 0.495844.
Train: 2018-08-02T10:59:21.673461: step 3303, loss 0.604094.
Train: 2018-08-02T10:59:21.892160: step 3304, loss 0.595757.
Train: 2018-08-02T10:59:22.110828: step 3305, loss 0.604053.
Train: 2018-08-02T10:59:22.329526: step 3306, loss 0.529227.
Train: 2018-08-02T10:59:22.548256: step 3307, loss 0.579056.
Train: 2018-08-02T10:59:22.766954: step 3308, loss 0.612184.
Train: 2018-08-02T10:59:22.985653: step 3309, loss 0.579021.
Train: 2018-08-02T10:59:23.204356: step 3310, loss 0.562515.
Test: 2018-08-02T10:59:24.375923: step 3310, loss 0.548239.
Train: 2018-08-02T10:59:24.594651: step 3311, loss 0.595423.
Train: 2018-08-02T10:59:24.813350: step 3312, loss 0.537978.
Train: 2018-08-02T10:59:25.032050: step 3313, loss 0.587123.
Train: 2018-08-02T10:59:25.250718: step 3314, loss 0.529989.
Train: 2018-08-02T10:59:25.469447: step 3315, loss 0.570778.
Train: 2018-08-02T10:59:25.688146: step 3316, loss 0.521998.
Train: 2018-08-02T10:59:25.906818: step 3317, loss 0.538277.
Train: 2018-08-02T10:59:26.125544: step 3318, loss 0.465117.
Train: 2018-08-02T10:59:26.344242: step 3319, loss 0.562628.
Train: 2018-08-02T10:59:26.578566: step 3320, loss 0.538105.
Test: 2018-08-02T10:59:27.734511: step 3320, loss 0.549847.
Train: 2018-08-02T10:59:27.953235: step 3321, loss 0.570765.
Train: 2018-08-02T10:59:28.140690: step 3322, loss 0.597562.
Train: 2018-08-02T10:59:28.359395: step 3323, loss 0.562547.
Train: 2018-08-02T10:59:28.578063: step 3324, loss 0.570759.
Train: 2018-08-02T10:59:28.796792: step 3325, loss 0.496737.
Train: 2018-08-02T10:59:29.015491: step 3326, loss 0.537793.
Train: 2018-08-02T10:59:29.234192: step 3327, loss 0.537717.
Train: 2018-08-02T10:59:29.452892: step 3328, loss 0.645286.
Train: 2018-08-02T10:59:29.655966: step 3329, loss 0.537622.
Train: 2018-08-02T10:59:29.874667: step 3330, loss 0.620499.
Test: 2018-08-02T10:59:31.046236: step 3330, loss 0.548827.
Train: 2018-08-02T10:59:31.264959: step 3331, loss 0.537619.
Train: 2018-08-02T10:59:31.483663: step 3332, loss 0.570757.
Train: 2018-08-02T10:59:31.702365: step 3333, loss 0.504516.
Train: 2018-08-02T10:59:31.921061: step 3334, loss 0.637056.
Train: 2018-08-02T10:59:32.139760: step 3335, loss 0.628711.
Train: 2018-08-02T10:59:32.358456: step 3336, loss 0.554238.
Train: 2018-08-02T10:59:32.577160: step 3337, loss 0.521308.
Train: 2018-08-02T10:59:32.795864: step 3338, loss 0.620157.
Train: 2018-08-02T10:59:33.014524: step 3339, loss 0.620046.
Train: 2018-08-02T10:59:33.233254: step 3340, loss 0.62807.
Test: 2018-08-02T10:59:34.404824: step 3340, loss 0.548268.
Train: 2018-08-02T10:59:34.607931: step 3341, loss 0.554479.
Train: 2018-08-02T10:59:34.826634: step 3342, loss 0.497744.
Train: 2018-08-02T10:59:35.045300: step 3343, loss 0.538399.
Train: 2018-08-02T10:59:35.264028: step 3344, loss 0.586981.
Train: 2018-08-02T10:59:35.482727: step 3345, loss 0.586962.
Train: 2018-08-02T10:59:35.701429: step 3346, loss 0.50634.
Train: 2018-08-02T10:59:35.920118: step 3347, loss 0.538595.
Train: 2018-08-02T10:59:36.138823: step 3348, loss 0.562761.
Train: 2018-08-02T10:59:36.357525: step 3349, loss 0.595003.
Train: 2018-08-02T10:59:36.576225: step 3350, loss 0.586937.
Test: 2018-08-02T10:59:37.732170: step 3350, loss 0.549515.
Train: 2018-08-02T10:59:37.935272: step 3351, loss 0.546674.
Train: 2018-08-02T10:59:38.153977: step 3352, loss 0.490351.
Train: 2018-08-02T10:59:38.372676: step 3353, loss 0.554701.
Train: 2018-08-02T10:59:38.591374: step 3354, loss 0.546595.
Train: 2018-08-02T10:59:38.841311: step 3355, loss 0.546542.
Train: 2018-08-02T10:59:39.044362: step 3356, loss 0.538378.
Train: 2018-08-02T10:59:39.263092: step 3357, loss 0.538287.
Train: 2018-08-02T10:59:39.481760: step 3358, loss 0.538186.
Train: 2018-08-02T10:59:39.700460: step 3359, loss 0.554422.
Train: 2018-08-02T10:59:39.919158: step 3360, loss 0.57896.
Test: 2018-08-02T10:59:41.090759: step 3360, loss 0.548255.
Train: 2018-08-02T10:59:41.293836: step 3361, loss 0.554328.
Train: 2018-08-02T10:59:41.512560: step 3362, loss 0.603693.
Train: 2018-08-02T10:59:41.731233: step 3363, loss 0.570757.
Train: 2018-08-02T10:59:41.949966: step 3364, loss 0.554263.
Train: 2018-08-02T10:59:42.168662: step 3365, loss 0.562504.
Train: 2018-08-02T10:59:42.387360: step 3366, loss 0.579013.
Train: 2018-08-02T10:59:42.606029: step 3367, loss 0.562498.
Train: 2018-08-02T10:59:42.824728: step 3368, loss 0.587272.
Train: 2018-08-02T10:59:43.043456: step 3369, loss 0.669786.
Train: 2018-08-02T10:59:43.262149: step 3370, loss 0.587212.
Test: 2018-08-02T10:59:44.433726: step 3370, loss 0.548682.
Train: 2018-08-02T10:59:44.652425: step 3371, loss 0.570762.
Train: 2018-08-02T10:59:44.871124: step 3372, loss 0.546253.
Train: 2018-08-02T10:59:45.089822: step 3373, loss 0.554474.
Train: 2018-08-02T10:59:45.308549: step 3374, loss 0.587048.
Train: 2018-08-02T10:59:45.527250: step 3375, loss 0.587016.
Train: 2018-08-02T10:59:45.745918: step 3376, loss 0.586982.
Train: 2018-08-02T10:59:45.964647: step 3377, loss 0.578881.
Train: 2018-08-02T10:59:46.183347: step 3378, loss 0.578873.
Train: 2018-08-02T10:59:46.402045: step 3379, loss 0.626953.
Train: 2018-08-02T10:59:46.620745: step 3380, loss 0.50704.
Test: 2018-08-02T10:59:47.792314: step 3380, loss 0.549092.
Train: 2018-08-02T10:59:47.995392: step 3381, loss 0.626625.
Train: 2018-08-02T10:59:48.214091: step 3382, loss 0.555062.
Train: 2018-08-02T10:59:48.432820: step 3383, loss 0.507678.
Train: 2018-08-02T10:59:48.651519: step 3384, loss 0.507756.
Train: 2018-08-02T10:59:48.870218: step 3385, loss 0.53143.
Train: 2018-08-02T10:59:49.088916: step 3386, loss 0.491775.
Train: 2018-08-02T10:59:49.307615: step 3387, loss 0.547088.
Train: 2018-08-02T10:59:49.541905: step 3388, loss 0.57089.
Train: 2018-08-02T10:59:49.760628: step 3389, loss 0.586856.
Train: 2018-08-02T10:59:49.979304: step 3390, loss 0.522784.
Test: 2018-08-02T10:59:51.135282: step 3390, loss 0.549563.
Train: 2018-08-02T10:59:51.354010: step 3391, loss 0.554763.
Train: 2018-08-02T10:59:51.572712: step 3392, loss 0.554698.
Train: 2018-08-02T10:59:51.791409: step 3393, loss 0.627396.
Train: 2018-08-02T10:59:52.010107: step 3394, loss 0.489863.
Train: 2018-08-02T10:59:52.228805: step 3395, loss 0.595138.
Train: 2018-08-02T10:59:52.447507: step 3396, loss 0.578914.
Train: 2018-08-02T10:59:52.666203: step 3397, loss 0.595202.
Train: 2018-08-02T10:59:52.884901: step 3398, loss 0.578921.
Train: 2018-08-02T10:59:53.103570: step 3399, loss 0.562635.
Train: 2018-08-02T10:59:53.322270: step 3400, loss 0.521921.
Test: 2018-08-02T10:59:54.493869: step 3400, loss 0.548638.
Train: 2018-08-02T10:59:55.290589: step 3401, loss 0.587076.
Train: 2018-08-02T10:59:55.509287: step 3402, loss 0.603384.
Train: 2018-08-02T10:59:55.727986: step 3403, loss 0.546337.
Train: 2018-08-02T10:59:55.946679: step 3404, loss 0.603355.
Train: 2018-08-02T10:59:56.165384: step 3405, loss 0.538242.
Train: 2018-08-02T10:59:56.384082: step 3406, loss 0.595175.
Train: 2018-08-02T10:59:56.602781: step 3407, loss 0.530182.
Train: 2018-08-02T10:59:56.821474: step 3408, loss 0.522076.
Train: 2018-08-02T10:59:57.040179: step 3409, loss 0.57891.
Train: 2018-08-02T10:59:57.258877: step 3410, loss 0.546403.
Test: 2018-08-02T10:59:58.430448: step 3410, loss 0.54963.
Train: 2018-08-02T10:59:58.633550: step 3411, loss 0.538252.
Train: 2018-08-02T10:59:58.852257: step 3412, loss 0.587062.
Train: 2018-08-02T10:59:59.070947: step 3413, loss 0.603361.
Train: 2018-08-02T10:59:59.289646: step 3414, loss 0.57892.
Train: 2018-08-02T10:59:59.508352: step 3415, loss 0.619592.
Train: 2018-08-02T10:59:59.727049: step 3416, loss 0.587025.
Train: 2018-08-02T10:59:59.945751: step 3417, loss 0.603191.
Train: 2018-08-02T11:00:00.164447: step 3418, loss 0.595031.
Train: 2018-08-02T11:00:00.383147: step 3419, loss 0.594963.
Train: 2018-08-02T11:00:00.601844: step 3420, loss 0.586879.
Test: 2018-08-02T11:00:01.773415: step 3420, loss 0.548836.
Train: 2018-08-02T11:00:01.976522: step 3421, loss 0.546938.
Train: 2018-08-02T11:00:02.195225: step 3422, loss 0.547036.
Train: 2018-08-02T11:00:02.413921: step 3423, loss 0.54711.
Train: 2018-08-02T11:00:02.632588: step 3424, loss 0.594709.
Train: 2018-08-02T11:00:02.851318: step 3425, loss 0.555134.
Train: 2018-08-02T11:00:03.070015: step 3426, loss 0.570965.
Train: 2018-08-02T11:00:03.288716: step 3427, loss 0.539432.
Train: 2018-08-02T11:00:03.507414: step 3428, loss 0.634044.
Train: 2018-08-02T11:00:03.726113: step 3429, loss 0.5474.
Train: 2018-08-02T11:00:03.944815: step 3430, loss 0.52387.
Test: 2018-08-02T11:00:05.116382: step 3430, loss 0.549488.
Train: 2018-08-02T11:00:05.335107: step 3431, loss 0.555296.
Train: 2018-08-02T11:00:05.569428: step 3432, loss 0.602455.
Train: 2018-08-02T11:00:05.788131: step 3433, loss 0.586729.
Train: 2018-08-02T11:00:06.006830: step 3434, loss 0.578872.
Train: 2018-08-02T11:00:06.225528: step 3435, loss 0.531796.
Train: 2018-08-02T11:00:06.444197: step 3436, loss 0.59457.
Train: 2018-08-02T11:00:06.662925: step 3437, loss 0.571029.
Train: 2018-08-02T11:00:06.881624: step 3438, loss 0.571033.
Train: 2018-08-02T11:00:07.100293: step 3439, loss 0.523996.
Train: 2018-08-02T11:00:07.319025: step 3440, loss 0.547484.
Test: 2018-08-02T11:00:08.490592: step 3440, loss 0.550932.
Train: 2018-08-02T11:00:08.693703: step 3441, loss 0.547435.
Train: 2018-08-02T11:00:08.912398: step 3442, loss 0.523749.
Train: 2018-08-02T11:00:09.131098: step 3443, loss 0.610449.
Train: 2018-08-02T11:00:09.349796: step 3444, loss 0.515587.
Train: 2018-08-02T11:00:09.568498: step 3445, loss 0.531272.
Train: 2018-08-02T11:00:09.787194: step 3446, loss 0.578859.
Train: 2018-08-02T11:00:10.005892: step 3447, loss 0.554918.
Train: 2018-08-02T11:00:10.224561: step 3448, loss 0.514837.
Train: 2018-08-02T11:00:10.443259: step 3449, loss 0.530667.
Train: 2018-08-02T11:00:10.661990: step 3450, loss 0.546609.
Test: 2018-08-02T11:00:11.833559: step 3450, loss 0.549532.
Train: 2018-08-02T11:00:12.036662: step 3451, loss 0.546486.
Train: 2018-08-02T11:00:12.255366: step 3452, loss 0.546367.
Train: 2018-08-02T11:00:12.474064: step 3453, loss 0.529908.
Train: 2018-08-02T11:00:12.692767: step 3454, loss 0.587179.
Train: 2018-08-02T11:00:12.911462: step 3455, loss 0.504855.
Train: 2018-08-02T11:00:13.130161: step 3456, loss 0.612126.
Train: 2018-08-02T11:00:13.348860: step 3457, loss 0.554165.
Train: 2018-08-02T11:00:13.567561: step 3458, loss 0.554126.
Train: 2018-08-02T11:00:13.786243: step 3459, loss 0.537416.
Train: 2018-08-02T11:00:14.004956: step 3460, loss 0.570771.
Test: 2018-08-02T11:00:15.160905: step 3460, loss 0.547256.
Train: 2018-08-02T11:00:15.379604: step 3461, loss 0.537273.
Train: 2018-08-02T11:00:15.598303: step 3462, loss 0.503623.
Train: 2018-08-02T11:00:15.817002: step 3463, loss 0.587647.
Train: 2018-08-02T11:00:16.051352: step 3464, loss 0.486378.
Train: 2018-08-02T11:00:16.270045: step 3465, loss 0.562308.
Train: 2018-08-02T11:00:16.488744: step 3466, loss 0.536782.
Train: 2018-08-02T11:00:16.707448: step 3467, loss 0.519463.
Train: 2018-08-02T11:00:16.926146: step 3468, loss 0.588034.
Train: 2018-08-02T11:00:17.144817: step 3469, loss 0.518513.
Train: 2018-08-02T11:00:17.363539: step 3470, loss 0.519339.
Test: 2018-08-02T11:00:18.535115: step 3470, loss 0.546206.
Train: 2018-08-02T11:00:18.738216: step 3471, loss 0.552877.
Train: 2018-08-02T11:00:18.956922: step 3472, loss 0.517958.
Train: 2018-08-02T11:00:19.144377: step 3473, loss 0.62186.
Train: 2018-08-02T11:00:19.363077: step 3474, loss 0.607183.
Train: 2018-08-02T11:00:19.581769: step 3475, loss 0.580071.
Train: 2018-08-02T11:00:19.800445: step 3476, loss 0.571783.
Train: 2018-08-02T11:00:20.019142: step 3477, loss 0.492169.
Train: 2018-08-02T11:00:20.237874: step 3478, loss 0.580272.
Train: 2018-08-02T11:00:20.456540: step 3479, loss 0.475809.
Train: 2018-08-02T11:00:20.675263: step 3480, loss 0.562419.
Test: 2018-08-02T11:00:21.846839: step 3480, loss 0.54934.
Train: 2018-08-02T11:00:22.049941: step 3481, loss 0.519356.
Train: 2018-08-02T11:00:22.284267: step 3482, loss 0.502105.
Train: 2018-08-02T11:00:22.502966: step 3483, loss 0.536483.
Train: 2018-08-02T11:00:22.737257: step 3484, loss 0.4933.
Train: 2018-08-02T11:00:22.955985: step 3485, loss 0.501771.
Train: 2018-08-02T11:00:23.174653: step 3486, loss 0.56235.
Train: 2018-08-02T11:00:23.393382: step 3487, loss 0.536239.
Train: 2018-08-02T11:00:23.612082: step 3488, loss 0.51872.
Train: 2018-08-02T11:00:23.830780: step 3489, loss 0.641181.
Train: 2018-08-02T11:00:24.049448: step 3490, loss 0.588668.
Test: 2018-08-02T11:00:25.205428: step 3490, loss 0.548561.
Train: 2018-08-02T11:00:25.424151: step 3491, loss 0.536105.
Train: 2018-08-02T11:00:25.642862: step 3492, loss 0.579899.
Train: 2018-08-02T11:00:25.861555: step 3493, loss 0.544875.
Train: 2018-08-02T11:00:26.080248: step 3494, loss 0.562375.
Train: 2018-08-02T11:00:26.298946: step 3495, loss 0.579845.
Train: 2018-08-02T11:00:26.517651: step 3496, loss 0.553642.
Train: 2018-08-02T11:00:26.736349: step 3497, loss 0.579778.
Train: 2018-08-02T11:00:26.955049: step 3498, loss 0.510207.
Train: 2018-08-02T11:00:27.173747: step 3499, loss 0.527626.
Train: 2018-08-02T11:00:27.392446: step 3500, loss 0.484267.
Test: 2018-08-02T11:00:28.564017: step 3500, loss 0.54745.
Train: 2018-08-02T11:00:29.376359: step 3501, loss 0.623127.
Train: 2018-08-02T11:00:29.595055: step 3502, loss 0.579697.
Train: 2018-08-02T11:00:29.813757: step 3503, loss 0.545021.
Train: 2018-08-02T11:00:30.032453: step 3504, loss 0.614248.
Train: 2018-08-02T11:00:30.251151: step 3505, loss 0.510564.
Train: 2018-08-02T11:00:30.469850: step 3506, loss 0.614034.
Train: 2018-08-02T11:00:30.688519: step 3507, loss 0.562335.
Train: 2018-08-02T11:00:30.907248: step 3508, loss 0.493777.
Train: 2018-08-02T11:00:31.125947: step 3509, loss 0.553777.
Train: 2018-08-02T11:00:31.344645: step 3510, loss 0.502491.
Test: 2018-08-02T11:00:32.531838: step 3510, loss 0.547862.
Train: 2018-08-02T11:00:32.734916: step 3511, loss 0.536688.
Train: 2018-08-02T11:00:32.953615: step 3512, loss 0.622202.
Train: 2018-08-02T11:00:33.172316: step 3513, loss 0.587962.
Train: 2018-08-02T11:00:33.391042: step 3514, loss 0.664643.
Train: 2018-08-02T11:00:33.625332: step 3515, loss 0.553859.
Train: 2018-08-02T11:00:33.859678: step 3516, loss 0.579274.
Train: 2018-08-02T11:00:34.078376: step 3517, loss 0.545526.
Train: 2018-08-02T11:00:34.297051: step 3518, loss 0.553994.
Train: 2018-08-02T11:00:34.515749: step 3519, loss 0.612626.
Train: 2018-08-02T11:00:34.765690: step 3520, loss 0.554092.
Test: 2018-08-02T11:00:36.218475: step 3520, loss 0.550108.
Train: 2018-08-02T11:00:36.468418: step 3521, loss 0.570759.
Train: 2018-08-02T11:00:36.687148: step 3522, loss 0.579035.
Train: 2018-08-02T11:00:36.905813: step 3523, loss 0.529512.
Train: 2018-08-02T11:00:37.124544: step 3524, loss 0.554303.
Train: 2018-08-02T11:00:37.343241: step 3525, loss 0.505073.
Train: 2018-08-02T11:00:37.593153: step 3526, loss 0.513315.
Train: 2018-08-02T11:00:37.811853: step 3527, loss 0.513294.
Train: 2018-08-02T11:00:38.030551: step 3528, loss 0.504966.
Train: 2018-08-02T11:00:38.249273: step 3529, loss 0.546.
Train: 2018-08-02T11:00:38.467981: step 3530, loss 0.5542.
Test: 2018-08-02T11:00:39.639547: step 3530, loss 0.54708.
Train: 2018-08-02T11:00:39.858278: step 3531, loss 0.537572.
Train: 2018-08-02T11:00:40.076946: step 3532, loss 0.504171.
Train: 2018-08-02T11:00:40.295674: step 3533, loss 0.579174.
Train: 2018-08-02T11:00:40.514344: step 3534, loss 0.579184.
Train: 2018-08-02T11:00:40.733072: step 3535, loss 0.570658.
Train: 2018-08-02T11:00:40.951772: step 3536, loss 0.60447.
Train: 2018-08-02T11:00:41.170469: step 3537, loss 0.59612.
Train: 2018-08-02T11:00:41.389163: step 3538, loss 0.545548.
Train: 2018-08-02T11:00:41.607863: step 3539, loss 0.570862.
Train: 2018-08-02T11:00:41.826537: step 3540, loss 0.537133.
Test: 2018-08-02T11:00:42.998137: step 3540, loss 0.547978.
Train: 2018-08-02T11:00:43.279322: step 3541, loss 0.595955.
Train: 2018-08-02T11:00:43.513645: step 3542, loss 0.545706.
Train: 2018-08-02T11:00:43.716748: step 3543, loss 0.54563.
Train: 2018-08-02T11:00:43.935441: step 3544, loss 0.470333.
Train: 2018-08-02T11:00:44.154146: step 3545, loss 0.621155.
Train: 2018-08-02T11:00:44.372845: step 3546, loss 0.59594.
Train: 2018-08-02T11:00:44.591538: step 3547, loss 0.570763.
Train: 2018-08-02T11:00:44.825835: step 3548, loss 0.587512.
Train: 2018-08-02T11:00:45.028938: step 3549, loss 0.57077.
Train: 2018-08-02T11:00:45.247635: step 3550, loss 0.504142.
Test: 2018-08-02T11:00:46.434832: step 3550, loss 0.547389.
Train: 2018-08-02T11:00:46.653561: step 3551, loss 0.545793.
Train: 2018-08-02T11:00:46.872254: step 3552, loss 0.604048.
Train: 2018-08-02T11:00:47.106550: step 3553, loss 0.579072.
Train: 2018-08-02T11:00:47.325281: step 3554, loss 0.537565.
Train: 2018-08-02T11:00:47.559572: step 3555, loss 0.545886.
Train: 2018-08-02T11:00:47.778640: step 3556, loss 0.496184.
Train: 2018-08-02T11:00:48.012974: step 3557, loss 0.603933.
Train: 2018-08-02T11:00:48.216062: step 3558, loss 0.554173.
Train: 2018-08-02T11:00:48.434761: step 3559, loss 0.537591.
Train: 2018-08-02T11:00:48.653433: step 3560, loss 0.620529.
Test: 2018-08-02T11:00:49.840656: step 3560, loss 0.546724.
Train: 2018-08-02T11:00:50.059355: step 3561, loss 0.554184.
Train: 2018-08-02T11:00:50.324918: step 3562, loss 0.545918.
Train: 2018-08-02T11:00:50.559263: step 3563, loss 0.595584.
Train: 2018-08-02T11:00:50.793561: step 3564, loss 0.504632.
Train: 2018-08-02T11:00:51.027878: step 3565, loss 0.603823.
Train: 2018-08-02T11:00:51.246578: step 3566, loss 0.587275.
Train: 2018-08-02T11:00:51.527761: step 3567, loss 0.65323.
Train: 2018-08-02T11:00:51.777704: step 3568, loss 0.595415.
Train: 2018-08-02T11:00:52.074509: step 3569, loss 0.554392.
Train: 2018-08-02T11:00:52.277587: step 3570, loss 0.58709.
Test: 2018-08-02T11:00:53.464807: step 3570, loss 0.547957.
Train: 2018-08-02T11:00:53.699129: step 3571, loss 0.611427.
Train: 2018-08-02T11:00:53.917858: step 3572, loss 0.497969.
Train: 2018-08-02T11:00:54.136550: step 3573, loss 0.514302.
Train: 2018-08-02T11:00:54.355248: step 3574, loss 0.51436.
Train: 2018-08-02T11:00:54.573922: step 3575, loss 0.611156.
Train: 2018-08-02T11:00:54.792651: step 3576, loss 0.586942.
Train: 2018-08-02T11:00:55.011350: step 3577, loss 0.578877.
Train: 2018-08-02T11:00:55.245648: step 3578, loss 0.586913.
Train: 2018-08-02T11:00:55.448749: step 3579, loss 0.578869.
Train: 2018-08-02T11:00:55.667417: step 3580, loss 0.554835.
Test: 2018-08-02T11:00:56.854639: step 3580, loss 0.54914.
Train: 2018-08-02T11:00:57.073339: step 3581, loss 0.522874.
Train: 2018-08-02T11:00:57.292036: step 3582, loss 0.538879.
Train: 2018-08-02T11:00:57.510765: step 3583, loss 0.546861.
Train: 2018-08-02T11:00:57.729465: step 3584, loss 0.642931.
Train: 2018-08-02T11:00:57.948133: step 3585, loss 0.586863.
Train: 2018-08-02T11:00:58.182484: step 3586, loss 0.58685.
Train: 2018-08-02T11:00:58.401182: step 3587, loss 0.554937.
Train: 2018-08-02T11:00:58.619858: step 3588, loss 0.531074.
Train: 2018-08-02T11:00:58.838573: step 3589, loss 0.570897.
Train: 2018-08-02T11:00:59.057278: step 3590, loss 0.586819.
Test: 2018-08-02T11:01:00.275713: step 3590, loss 0.549116.
Train: 2018-08-02T11:01:00.478820: step 3591, loss 0.54704.
Train: 2018-08-02T11:01:00.697519: step 3592, loss 0.586812.
Train: 2018-08-02T11:01:00.916214: step 3593, loss 0.56296.
Train: 2018-08-02T11:01:01.134885: step 3594, loss 0.650373.
Train: 2018-08-02T11:01:01.416070: step 3595, loss 0.570933.
Train: 2018-08-02T11:01:01.634771: step 3596, loss 0.594679.
Train: 2018-08-02T11:01:01.869092: step 3597, loss 0.578864.
Train: 2018-08-02T11:01:02.087789: step 3598, loss 0.602472.
Train: 2018-08-02T11:01:02.306488: step 3599, loss 0.500441.
Train: 2018-08-02T11:01:02.525216: step 3600, loss 0.578878.
Test: 2018-08-02T11:01:03.712407: step 3600, loss 0.550697.
Train: 2018-08-02T11:01:04.837146: step 3601, loss 0.563227.
Train: 2018-08-02T11:01:05.071465: step 3602, loss 0.563242.
Train: 2018-08-02T11:01:05.290163: step 3603, loss 0.586701.
Train: 2018-08-02T11:01:05.508887: step 3604, loss 0.555456.
Train: 2018-08-02T11:01:05.743207: step 3605, loss 0.547657.
Train: 2018-08-02T11:01:05.961882: step 3606, loss 0.563263.
Train: 2018-08-02T11:01:06.180604: step 3607, loss 0.563252.
Train: 2018-08-02T11:01:06.399305: step 3608, loss 0.547598.
Train: 2018-08-02T11:01:06.618009: step 3609, loss 0.563213.
Train: 2018-08-02T11:01:06.836706: step 3610, loss 0.508286.
Test: 2018-08-02T11:01:08.055141: step 3610, loss 0.549819.
Train: 2018-08-02T11:01:08.273870: step 3611, loss 0.555274.
Train: 2018-08-02T11:01:08.492539: step 3612, loss 0.578855.
Train: 2018-08-02T11:01:08.711267: step 3613, loss 0.57885.
Train: 2018-08-02T11:01:08.929937: step 3614, loss 0.57886.
Train: 2018-08-02T11:01:09.148635: step 3615, loss 0.491541.
Train: 2018-08-02T11:01:09.367363: step 3616, loss 0.53904.
Train: 2018-08-02T11:01:09.586062: step 3617, loss 0.634818.
Train: 2018-08-02T11:01:09.804762: step 3618, loss 0.586876.
Train: 2018-08-02T11:01:10.023460: step 3619, loss 0.554859.
Train: 2018-08-02T11:01:10.242159: step 3620, loss 0.570811.
Test: 2018-08-02T11:01:11.444972: step 3620, loss 0.549386.
Train: 2018-08-02T11:01:11.648074: step 3621, loss 0.635133.
Train: 2018-08-02T11:01:11.866778: step 3622, loss 0.514637.
Train: 2018-08-02T11:01:12.085446: step 3623, loss 0.651144.
Train: 2018-08-02T11:01:12.272927: step 3624, loss 0.477318.
Train: 2018-08-02T11:01:12.491601: step 3625, loss 0.506701.
Train: 2018-08-02T11:01:12.710330: step 3626, loss 0.578855.
Train: 2018-08-02T11:01:12.929024: step 3627, loss 0.603034.
Train: 2018-08-02T11:01:13.147698: step 3628, loss 0.578865.
Train: 2018-08-02T11:01:13.366427: step 3629, loss 0.578875.
Train: 2018-08-02T11:01:13.585120: step 3630, loss 0.570833.
Test: 2018-08-02T11:01:14.741075: step 3630, loss 0.550065.
Train: 2018-08-02T11:01:14.959773: step 3631, loss 0.578869.
Train: 2018-08-02T11:01:15.178496: step 3632, loss 0.554729.
Train: 2018-08-02T11:01:15.397202: step 3633, loss 0.602967.
Train: 2018-08-02T11:01:15.615901: step 3634, loss 0.562777.
Train: 2018-08-02T11:01:15.834599: step 3635, loss 0.562859.
Train: 2018-08-02T11:01:16.068922: step 3636, loss 0.49865.
Train: 2018-08-02T11:01:16.303210: step 3637, loss 0.57083.
Train: 2018-08-02T11:01:16.521938: step 3638, loss 0.546781.
Train: 2018-08-02T11:01:16.740637: step 3639, loss 0.546729.
Train: 2018-08-02T11:01:16.959331: step 3640, loss 0.602981.
Test: 2018-08-02T11:01:18.130906: step 3640, loss 0.549293.
Train: 2018-08-02T11:01:18.349605: step 3641, loss 0.578871.
Train: 2018-08-02T11:01:18.568334: step 3642, loss 0.538622.
Train: 2018-08-02T11:01:18.771413: step 3643, loss 0.522349.
Train: 2018-08-02T11:01:18.990105: step 3644, loss 0.514308.
Train: 2018-08-02T11:01:19.208779: step 3645, loss 0.562639.
Train: 2018-08-02T11:01:19.427508: step 3646, loss 0.587044.
Train: 2018-08-02T11:01:19.646207: step 3647, loss 0.570862.
Train: 2018-08-02T11:01:19.864905: step 3648, loss 0.52181.
Train: 2018-08-02T11:01:20.083609: step 3649, loss 0.570841.
Train: 2018-08-02T11:01:20.317930: step 3650, loss 0.50537.
Test: 2018-08-02T11:01:21.489494: step 3650, loss 0.548275.
Train: 2018-08-02T11:01:21.692573: step 3651, loss 0.579008.
Train: 2018-08-02T11:01:21.911301: step 3652, loss 0.480364.
Train: 2018-08-02T11:01:22.129969: step 3653, loss 0.504758.
Train: 2018-08-02T11:01:22.348698: step 3654, loss 0.612199.
Train: 2018-08-02T11:01:22.567397: step 3655, loss 0.612231.
Train: 2018-08-02T11:01:22.786096: step 3656, loss 0.587496.
Train: 2018-08-02T11:01:23.004794: step 3657, loss 0.537346.
Train: 2018-08-02T11:01:23.223494: step 3658, loss 0.595866.
Train: 2018-08-02T11:01:23.442162: step 3659, loss 0.570784.
Train: 2018-08-02T11:01:23.660861: step 3660, loss 0.579096.
Test: 2018-08-02T11:01:24.816841: step 3660, loss 0.549285.
Train: 2018-08-02T11:01:25.035564: step 3661, loss 0.579097.
Train: 2018-08-02T11:01:25.254271: step 3662, loss 0.579143.
Train: 2018-08-02T11:01:25.472967: step 3663, loss 0.429557.
Train: 2018-08-02T11:01:25.691636: step 3664, loss 0.604047.
Train: 2018-08-02T11:01:25.910365: step 3665, loss 0.51253.
Train: 2018-08-02T11:01:26.129033: step 3666, loss 0.55411.
Train: 2018-08-02T11:01:26.347732: step 3667, loss 0.52907.
Train: 2018-08-02T11:01:26.566461: step 3668, loss 0.579118.
Train: 2018-08-02T11:01:26.785160: step 3669, loss 0.554045.
Train: 2018-08-02T11:01:27.003859: step 3670, loss 0.621011.
Test: 2018-08-02T11:01:28.159807: step 3670, loss 0.548815.
Train: 2018-08-02T11:01:28.378507: step 3671, loss 0.620984.
Train: 2018-08-02T11:01:28.597235: step 3672, loss 0.612531.
Train: 2018-08-02T11:01:28.815937: step 3673, loss 0.570763.
Train: 2018-08-02T11:01:29.034602: step 3674, loss 0.570759.
Train: 2018-08-02T11:01:29.253331: step 3675, loss 0.603867.
Train: 2018-08-02T11:01:29.472025: step 3676, loss 0.570756.
Train: 2018-08-02T11:01:29.690699: step 3677, loss 0.537892.
Train: 2018-08-02T11:01:29.909428: step 3678, loss 0.513404.
Train: 2018-08-02T11:01:30.128098: step 3679, loss 0.644415.
Train: 2018-08-02T11:01:30.362417: step 3680, loss 0.570773.
Test: 2018-08-02T11:01:31.518397: step 3680, loss 0.548506.
Train: 2018-08-02T11:01:31.721474: step 3681, loss 0.513854.
Train: 2018-08-02T11:01:31.940202: step 3682, loss 0.595147.
Train: 2018-08-02T11:01:32.158907: step 3683, loss 0.587.
Train: 2018-08-02T11:01:32.377601: step 3684, loss 0.627379.
Train: 2018-08-02T11:01:32.596299: step 3685, loss 0.530569.
Train: 2018-08-02T11:01:32.814998: step 3686, loss 0.546748.
Train: 2018-08-02T11:01:33.033697: step 3687, loss 0.602914.
Train: 2018-08-02T11:01:33.252395: step 3688, loss 0.506906.
Train: 2018-08-02T11:01:33.471088: step 3689, loss 0.570873.
Train: 2018-08-02T11:01:33.689787: step 3690, loss 0.570879.
Test: 2018-08-02T11:01:34.861363: step 3690, loss 0.549222.
Train: 2018-08-02T11:01:35.064469: step 3691, loss 0.554933.
Train: 2018-08-02T11:01:35.283169: step 3692, loss 0.554943.
Train: 2018-08-02T11:01:35.501869: step 3693, loss 0.507115.
Train: 2018-08-02T11:01:35.720567: step 3694, loss 0.60281.
Train: 2018-08-02T11:01:35.939266: step 3695, loss 0.554901.
Train: 2018-08-02T11:01:36.157935: step 3696, loss 0.514925.
Train: 2018-08-02T11:01:36.376658: step 3697, loss 0.602887.
Train: 2018-08-02T11:01:36.595356: step 3698, loss 0.490711.
Train: 2018-08-02T11:01:36.814061: step 3699, loss 0.546731.
Train: 2018-08-02T11:01:37.032760: step 3700, loss 0.562763.
Test: 2018-08-02T11:01:38.204330: step 3700, loss 0.548308.
Train: 2018-08-02T11:01:39.001049: step 3701, loss 0.570808.
Train: 2018-08-02T11:01:39.219748: step 3702, loss 0.570799.
Train: 2018-08-02T11:01:39.438418: step 3703, loss 0.619458.
Train: 2018-08-02T11:01:39.657140: step 3704, loss 0.54645.
Train: 2018-08-02T11:01:39.875815: step 3705, loss 0.546432.
Train: 2018-08-02T11:01:40.094543: step 3706, loss 0.570784.
Train: 2018-08-02T11:01:40.313242: step 3707, loss 0.587048.
Train: 2018-08-02T11:01:40.531946: step 3708, loss 0.578915.
Train: 2018-08-02T11:01:40.750640: step 3709, loss 0.562651.
Train: 2018-08-02T11:01:40.969338: step 3710, loss 0.627688.
Test: 2018-08-02T11:01:42.125287: step 3710, loss 0.55043.
Train: 2018-08-02T11:01:42.344016: step 3711, loss 0.55456.
Train: 2018-08-02T11:01:42.578342: step 3712, loss 0.554588.
Train: 2018-08-02T11:01:42.797036: step 3713, loss 0.554608.
Train: 2018-08-02T11:01:43.015729: step 3714, loss 0.619341.
Train: 2018-08-02T11:01:43.234433: step 3715, loss 0.595033.
Train: 2018-08-02T11:01:43.453132: step 3716, loss 0.498337.
Train: 2018-08-02T11:01:43.671800: step 3717, loss 0.627171.
Train: 2018-08-02T11:01:43.890529: step 3718, loss 0.602969.
Train: 2018-08-02T11:01:44.109224: step 3719, loss 0.450697.
Train: 2018-08-02T11:01:44.327904: step 3720, loss 0.642975.
Test: 2018-08-02T11:01:45.483876: step 3720, loss 0.548946.
Train: 2018-08-02T11:01:45.702575: step 3721, loss 0.55486.
Train: 2018-08-02T11:01:45.921280: step 3722, loss 0.522914.
Train: 2018-08-02T11:01:46.139998: step 3723, loss 0.562875.
Train: 2018-08-02T11:01:46.358672: step 3724, loss 0.562872.
Train: 2018-08-02T11:01:46.577369: step 3725, loss 0.522881.
Train: 2018-08-02T11:01:46.796099: step 3726, loss 0.514798.
Train: 2018-08-02T11:01:47.014791: step 3727, loss 0.57887.
Train: 2018-08-02T11:01:47.233497: step 3728, loss 0.603005.
Train: 2018-08-02T11:01:47.452200: step 3729, loss 0.498371.
Train: 2018-08-02T11:01:47.670894: step 3730, loss 0.603092.
Test: 2018-08-02T11:01:48.826842: step 3730, loss 0.549049.
Train: 2018-08-02T11:01:49.045572: step 3731, loss 0.554649.
Train: 2018-08-02T11:01:49.264270: step 3732, loss 0.554623.
Train: 2018-08-02T11:01:49.482969: step 3733, loss 0.457369.
Train: 2018-08-02T11:01:49.701662: step 3734, loss 0.56265.
Train: 2018-08-02T11:01:49.920336: step 3735, loss 0.595255.
Train: 2018-08-02T11:01:50.139066: step 3736, loss 0.521684.
Train: 2018-08-02T11:01:50.357765: step 3737, loss 0.603583.
Train: 2018-08-02T11:01:50.576435: step 3738, loss 0.603636.
Train: 2018-08-02T11:01:50.795162: step 3739, loss 0.620102.
Train: 2018-08-02T11:01:51.013856: step 3740, loss 0.496807.
Test: 2018-08-02T11:01:52.169810: step 3740, loss 0.548425.
Train: 2018-08-02T11:01:52.388541: step 3741, loss 0.578981.
Train: 2018-08-02T11:01:52.607237: step 3742, loss 0.537863.
Train: 2018-08-02T11:01:52.841530: step 3743, loss 0.529609.
Train: 2018-08-02T11:01:53.044605: step 3744, loss 0.570758.
Train: 2018-08-02T11:01:53.263337: step 3745, loss 0.529511.
Train: 2018-08-02T11:01:53.482033: step 3746, loss 0.562493.
Train: 2018-08-02T11:01:53.700731: step 3747, loss 0.628669.
Train: 2018-08-02T11:01:53.919431: step 3748, loss 0.537676.
Train: 2018-08-02T11:01:54.138131: step 3749, loss 0.521128.
Train: 2018-08-02T11:01:54.356828: step 3750, loss 0.612155.
Test: 2018-08-02T11:01:55.512778: step 3750, loss 0.548846.
Train: 2018-08-02T11:01:55.715855: step 3751, loss 0.570757.
Train: 2018-08-02T11:01:55.934556: step 3752, loss 0.570756.
Train: 2018-08-02T11:01:56.153253: step 3753, loss 0.56249.
Train: 2018-08-02T11:01:56.371981: step 3754, loss 0.603796.
Train: 2018-08-02T11:01:56.590680: step 3755, loss 0.554265.
Train: 2018-08-02T11:01:56.809373: step 3756, loss 0.513116.
Train: 2018-08-02T11:01:57.028078: step 3757, loss 0.562524.
Train: 2018-08-02T11:01:57.246776: step 3758, loss 0.5296.
Train: 2018-08-02T11:01:57.465446: step 3759, loss 0.603701.
Train: 2018-08-02T11:01:57.684143: step 3760, loss 0.554294.
Test: 2018-08-02T11:01:58.855744: step 3760, loss 0.547839.
Train: 2018-08-02T11:01:59.058846: step 3761, loss 0.513156.
Train: 2018-08-02T11:01:59.277551: step 3762, loss 0.562522.
Train: 2018-08-02T11:01:59.496253: step 3763, loss 0.496587.
Train: 2018-08-02T11:01:59.714948: step 3764, loss 0.562498.
Train: 2018-08-02T11:01:59.933647: step 3765, loss 0.570756.
Train: 2018-08-02T11:02:00.152324: step 3766, loss 0.661886.
Train: 2018-08-02T11:02:00.371015: step 3767, loss 0.554206.
Train: 2018-08-02T11:02:00.589713: step 3768, loss 0.537687.
Train: 2018-08-02T11:02:00.808412: step 3769, loss 0.488111.
Train: 2018-08-02T11:02:01.027112: step 3770, loss 0.537654.
Test: 2018-08-02T11:02:02.198711: step 3770, loss 0.54786.
Train: 2018-08-02T11:02:02.401790: step 3771, loss 0.562468.
Train: 2018-08-02T11:02:02.620487: step 3772, loss 0.562457.
Train: 2018-08-02T11:02:02.839217: step 3773, loss 0.562448.
Train: 2018-08-02T11:02:03.057917: step 3774, loss 0.554121.
Train: 2018-08-02T11:02:03.229750: step 3775, loss 0.633504.
Train: 2018-08-02T11:02:03.448444: step 3776, loss 0.637332.
Train: 2018-08-02T11:02:03.667148: step 3777, loss 0.562459.
Train: 2018-08-02T11:02:03.885817: step 3778, loss 0.545918.
Train: 2018-08-02T11:02:04.104545: step 3779, loss 0.562492.
Train: 2018-08-02T11:02:04.323244: step 3780, loss 0.529509.
Test: 2018-08-02T11:02:05.494814: step 3780, loss 0.54856.
Train: 2018-08-02T11:02:05.697923: step 3781, loss 0.562515.
Train: 2018-08-02T11:02:05.916621: step 3782, loss 0.595463.
Train: 2018-08-02T11:02:06.135320: step 3783, loss 0.562536.
Train: 2018-08-02T11:02:06.354019: step 3784, loss 0.505081.
Train: 2018-08-02T11:02:06.572687: step 3785, loss 0.611809.
Train: 2018-08-02T11:02:06.791416: step 3786, loss 0.505163.
Train: 2018-08-02T11:02:07.010085: step 3787, loss 0.570762.
Train: 2018-08-02T11:02:07.228785: step 3788, loss 0.529754.
Train: 2018-08-02T11:02:07.447512: step 3789, loss 0.521515.
Train: 2018-08-02T11:02:07.666212: step 3790, loss 0.529656.
Test: 2018-08-02T11:02:08.837782: step 3790, loss 0.548191.
Train: 2018-08-02T11:02:09.056481: step 3791, loss 0.562519.
Train: 2018-08-02T11:02:09.275204: step 3792, loss 0.545995.
Train: 2018-08-02T11:02:09.493879: step 3793, loss 0.554216.
Train: 2018-08-02T11:02:09.712577: step 3794, loss 0.562471.
Train: 2018-08-02T11:02:09.931275: step 3795, loss 0.562459.
Train: 2018-08-02T11:02:10.150000: step 3796, loss 0.545826.
Train: 2018-08-02T11:02:10.368704: step 3797, loss 0.595735.
Train: 2018-08-02T11:02:10.587402: step 3798, loss 0.487481.
Train: 2018-08-02T11:02:10.806071: step 3799, loss 0.595804.
Train: 2018-08-02T11:02:11.024769: step 3800, loss 0.59583.
Test: 2018-08-02T11:02:12.207377: step 3800, loss 0.549619.
Train: 2018-08-02T11:02:13.004096: step 3801, loss 0.545709.
Train: 2018-08-02T11:02:13.222797: step 3802, loss 0.595836.
Train: 2018-08-02T11:02:13.441462: step 3803, loss 0.545719.
Train: 2018-08-02T11:02:13.660192: step 3804, loss 0.595809.
Train: 2018-08-02T11:02:13.878860: step 3805, loss 0.645798.
Train: 2018-08-02T11:02:14.097590: step 3806, loss 0.57076.
Train: 2018-08-02T11:02:14.316288: step 3807, loss 0.579043.
Train: 2018-08-02T11:02:14.534988: step 3808, loss 0.603791.
Train: 2018-08-02T11:02:14.753680: step 3809, loss 0.46382.
Train: 2018-08-02T11:02:14.972385: step 3810, loss 0.636482.
Test: 2018-08-02T11:02:16.143955: step 3810, loss 0.548704.
Train: 2018-08-02T11:02:16.347064: step 3811, loss 0.554382.
Train: 2018-08-02T11:02:16.565762: step 3812, loss 0.546257.
Train: 2018-08-02T11:02:16.784460: step 3813, loss 0.48922.
Train: 2018-08-02T11:02:17.003158: step 3814, loss 0.53815.
Train: 2018-08-02T11:02:17.221828: step 3815, loss 0.627899.
Train: 2018-08-02T11:02:17.440552: step 3816, loss 0.456633.
Train: 2018-08-02T11:02:17.659225: step 3817, loss 0.562604.
Train: 2018-08-02T11:02:17.877954: step 3818, loss 0.562589.
Train: 2018-08-02T11:02:18.096624: step 3819, loss 0.497067.
Train: 2018-08-02T11:02:18.315352: step 3820, loss 0.529707.
Test: 2018-08-02T11:02:19.486922: step 3820, loss 0.549334.
Train: 2018-08-02T11:02:19.690030: step 3821, loss 0.677829.
Train: 2018-08-02T11:02:19.908699: step 3822, loss 0.554283.
Train: 2018-08-02T11:02:20.127427: step 3823, loss 0.562518.
Train: 2018-08-02T11:02:20.346097: step 3824, loss 0.537798.
Train: 2018-08-02T11:02:20.564825: step 3825, loss 0.570757.
Train: 2018-08-02T11:02:20.783500: step 3826, loss 0.52127.
Train: 2018-08-02T11:02:21.002222: step 3827, loss 0.52121.
Train: 2018-08-02T11:02:21.220923: step 3828, loss 0.570756.
Train: 2018-08-02T11:02:21.439590: step 3829, loss 0.496182.
Train: 2018-08-02T11:02:21.658319: step 3830, loss 0.504287.
Test: 2018-08-02T11:02:22.829889: step 3830, loss 0.548498.
Train: 2018-08-02T11:02:23.032967: step 3831, loss 0.587446.
Train: 2018-08-02T11:02:23.298560: step 3832, loss 0.528962.
Train: 2018-08-02T11:02:23.517258: step 3833, loss 0.54562.
Train: 2018-08-02T11:02:23.735928: step 3834, loss 0.638086.
Train: 2018-08-02T11:02:23.954657: step 3835, loss 0.562377.
Train: 2018-08-02T11:02:24.173356: step 3836, loss 0.57922.
Train: 2018-08-02T11:02:24.392054: step 3837, loss 0.604487.
Train: 2018-08-02T11:02:24.610752: step 3838, loss 0.545554.
Train: 2018-08-02T11:02:24.829452: step 3839, loss 0.629632.
Train: 2018-08-02T11:02:25.048150: step 3840, loss 0.629483.
Test: 2018-08-02T11:02:26.219720: step 3840, loss 0.548656.
Train: 2018-08-02T11:02:26.422822: step 3841, loss 0.57077.
Train: 2018-08-02T11:02:26.641528: step 3842, loss 0.595731.
Train: 2018-08-02T11:02:26.860226: step 3843, loss 0.521031.
Train: 2018-08-02T11:02:27.078925: step 3844, loss 0.537703.
Train: 2018-08-02T11:02:27.297594: step 3845, loss 0.570757.
Train: 2018-08-02T11:02:27.516292: step 3846, loss 0.554304.
Train: 2018-08-02T11:02:27.735017: step 3847, loss 0.488642.
Train: 2018-08-02T11:02:27.953691: step 3848, loss 0.644667.
Train: 2018-08-02T11:02:28.172420: step 3849, loss 0.505194.
Train: 2018-08-02T11:02:28.391117: step 3850, loss 0.578956.
Test: 2018-08-02T11:02:29.547067: step 3850, loss 0.548342.
Train: 2018-08-02T11:02:29.765796: step 3851, loss 0.554393.
Train: 2018-08-02T11:02:29.984489: step 3852, loss 0.578948.
Train: 2018-08-02T11:02:30.203194: step 3853, loss 0.627988.
Train: 2018-08-02T11:02:30.421862: step 3854, loss 0.603394.
Train: 2018-08-02T11:02:30.640590: step 3855, loss 0.619563.
Train: 2018-08-02T11:02:30.859290: step 3856, loss 0.578895.
Train: 2018-08-02T11:02:31.077989: step 3857, loss 0.546627.
Train: 2018-08-02T11:02:31.296689: step 3858, loss 0.58691.
Train: 2018-08-02T11:02:31.515387: step 3859, loss 0.594885.
Train: 2018-08-02T11:02:31.734085: step 3860, loss 0.58684.
Test: 2018-08-02T11:02:32.890033: step 3860, loss 0.550968.
Train: 2018-08-02T11:02:33.108757: step 3861, loss 0.539116.
Train: 2018-08-02T11:02:33.327432: step 3862, loss 0.459965.
Train: 2018-08-02T11:02:33.546129: step 3863, loss 0.634354.
Train: 2018-08-02T11:02:33.764859: step 3864, loss 0.499695.
Train: 2018-08-02T11:02:33.983557: step 3865, loss 0.586779.
Train: 2018-08-02T11:02:34.202258: step 3866, loss 0.547181.
Train: 2018-08-02T11:02:34.420956: step 3867, loss 0.547165.
Train: 2018-08-02T11:02:34.639653: step 3868, loss 0.610585.
Train: 2018-08-02T11:02:34.858353: step 3869, loss 0.570928.
Train: 2018-08-02T11:02:35.077047: step 3870, loss 0.507497.
Test: 2018-08-02T11:02:36.248622: step 3870, loss 0.550266.
Train: 2018-08-02T11:02:36.451731: step 3871, loss 0.523279.
Train: 2018-08-02T11:02:36.670428: step 3872, loss 0.515189.
Train: 2018-08-02T11:02:36.889098: step 3873, loss 0.602819.
Train: 2018-08-02T11:02:37.107826: step 3874, loss 0.514835.
Train: 2018-08-02T11:02:37.326496: step 3875, loss 0.57084.
Train: 2018-08-02T11:02:37.545223: step 3876, loss 0.627189.
Train: 2018-08-02T11:02:37.763923: step 3877, loss 0.570819.
Train: 2018-08-02T11:02:37.982623: step 3878, loss 0.595015.
Train: 2018-08-02T11:02:38.201321: step 3879, loss 0.619215.
Train: 2018-08-02T11:02:38.420020: step 3880, loss 0.562766.
Test: 2018-08-02T11:02:39.591589: step 3880, loss 0.548972.
Train: 2018-08-02T11:02:39.794699: step 3881, loss 0.586922.
Train: 2018-08-02T11:02:40.013366: step 3882, loss 0.578871.
Train: 2018-08-02T11:02:40.232094: step 3883, loss 0.538761.
Train: 2018-08-02T11:02:40.450794: step 3884, loss 0.554823.
Train: 2018-08-02T11:02:40.669486: step 3885, loss 0.562844.
Train: 2018-08-02T11:02:40.888191: step 3886, loss 0.570857.
Train: 2018-08-02T11:02:41.106889: step 3887, loss 0.458793.
Train: 2018-08-02T11:02:41.341181: step 3888, loss 0.554799.
Train: 2018-08-02T11:02:41.559908: step 3889, loss 0.570832.
Train: 2018-08-02T11:02:41.778607: step 3890, loss 0.578879.
Test: 2018-08-02T11:02:42.934557: step 3890, loss 0.549084.
Train: 2018-08-02T11:02:43.153286: step 3891, loss 0.586952.
Train: 2018-08-02T11:02:43.371985: step 3892, loss 0.55466.
Train: 2018-08-02T11:02:43.590683: step 3893, loss 0.546558.
Train: 2018-08-02T11:02:43.809353: step 3894, loss 0.546522.
Train: 2018-08-02T11:02:44.028050: step 3895, loss 0.514054.
Train: 2018-08-02T11:02:44.246779: step 3896, loss 0.643925.
Train: 2018-08-02T11:02:44.465479: step 3897, loss 0.595174.
Train: 2018-08-02T11:02:44.684148: step 3898, loss 0.546403.
Train: 2018-08-02T11:02:44.902875: step 3899, loss 0.465123.
Train: 2018-08-02T11:02:45.121543: step 3900, loss 0.570776.
Test: 2018-08-02T11:02:46.293145: step 3900, loss 0.548975.
Train: 2018-08-02T11:02:47.089864: step 3901, loss 0.570771.
Train: 2018-08-02T11:02:47.308562: step 3902, loss 0.513537.
Train: 2018-08-02T11:02:47.527262: step 3903, loss 0.603549.
Train: 2018-08-02T11:02:47.745929: step 3904, loss 0.578968.
Train: 2018-08-02T11:02:47.964660: step 3905, loss 0.578973.
Train: 2018-08-02T11:02:48.183358: step 3906, loss 0.578975.
Train: 2018-08-02T11:02:48.402057: step 3907, loss 0.603615.
Train: 2018-08-02T11:02:48.620755: step 3908, loss 0.546149.
Train: 2018-08-02T11:02:48.839424: step 3909, loss 0.611755.
Train: 2018-08-02T11:02:49.058153: step 3910, loss 0.578949.
Test: 2018-08-02T11:02:50.229723: step 3910, loss 0.549339.
Train: 2018-08-02T11:02:50.432825: step 3911, loss 0.521767.
Train: 2018-08-02T11:02:50.651529: step 3912, loss 0.521812.
Train: 2018-08-02T11:02:50.870223: step 3913, loss 0.546288.
Train: 2018-08-02T11:02:51.088898: step 3914, loss 0.521777.
Train: 2018-08-02T11:02:51.307595: step 3915, loss 0.488994.
Train: 2018-08-02T11:02:51.526325: step 3916, loss 0.570762.
Train: 2018-08-02T11:02:51.745023: step 3917, loss 0.628312.
Train: 2018-08-02T11:02:51.963723: step 3918, loss 0.529622.
Train: 2018-08-02T11:02:52.182390: step 3919, loss 0.603708.
Train: 2018-08-02T11:02:52.401120: step 3920, loss 0.644909.
Test: 2018-08-02T11:02:53.572690: step 3920, loss 0.549367.
Train: 2018-08-02T11:02:53.775792: step 3921, loss 0.620104.
Train: 2018-08-02T11:02:53.994496: step 3922, loss 0.554363.
Train: 2018-08-02T11:02:54.213196: step 3923, loss 0.497158.
Train: 2018-08-02T11:02:54.431864: step 3924, loss 0.50539.
Train: 2018-08-02T11:02:54.650563: step 3925, loss 0.472639.
Train: 2018-08-02T11:02:54.838019: step 3926, loss 0.615036.
Train: 2018-08-02T11:02:55.056719: step 3927, loss 0.505096.
Train: 2018-08-02T11:02:55.275446: step 3928, loss 0.546077.
Train: 2018-08-02T11:02:55.494147: step 3929, loss 0.504784.
Train: 2018-08-02T11:02:55.712845: step 3930, loss 0.595581.
Test: 2018-08-02T11:02:56.868794: step 3930, loss 0.548423.
Train: 2018-08-02T11:02:57.087523: step 3931, loss 0.595638.
Train: 2018-08-02T11:02:57.306221: step 3932, loss 0.562456.
Train: 2018-08-02T11:02:57.524889: step 3933, loss 0.562449.
Train: 2018-08-02T11:02:57.743618: step 3934, loss 0.587396.
Train: 2018-08-02T11:02:57.962318: step 3935, loss 0.653936.
Train: 2018-08-02T11:02:58.181017: step 3936, loss 0.52096.
Train: 2018-08-02T11:02:58.399715: step 3937, loss 0.562467.
Train: 2018-08-02T11:02:58.618414: step 3938, loss 0.562475.
Train: 2018-08-02T11:02:58.837113: step 3939, loss 0.670039.
Train: 2018-08-02T11:02:59.055811: step 3940, loss 0.661453.
Test: 2018-08-02T11:03:00.227381: step 3940, loss 0.548111.
Train: 2018-08-02T11:03:00.430490: step 3941, loss 0.53796.
Train: 2018-08-02T11:03:00.649158: step 3942, loss 0.60343.
Train: 2018-08-02T11:03:00.867887: step 3943, loss 0.546411.
Train: 2018-08-02T11:03:01.086556: step 3944, loss 0.611262.
Train: 2018-08-02T11:03:01.305255: step 3945, loss 0.586931.
Train: 2018-08-02T11:03:01.523983: step 3946, loss 0.554821.
Train: 2018-08-02T11:03:01.742653: step 3947, loss 0.538948.
Train: 2018-08-02T11:03:01.961381: step 3948, loss 0.594776.
Train: 2018-08-02T11:03:02.180082: step 3949, loss 0.562995.
Train: 2018-08-02T11:03:02.398779: step 3950, loss 0.539314.
Test: 2018-08-02T11:03:03.570349: step 3950, loss 0.551161.
Train: 2018-08-02T11:03:03.773451: step 3951, loss 0.570968.
Train: 2018-08-02T11:03:03.992155: step 3952, loss 0.578865.
Train: 2018-08-02T11:03:04.210824: step 3953, loss 0.594603.
Train: 2018-08-02T11:03:04.429554: step 3954, loss 0.492518.
Train: 2018-08-02T11:03:04.648221: step 3955, loss 0.555321.
Train: 2018-08-02T11:03:04.866950: step 3956, loss 0.618141.
Train: 2018-08-02T11:03:05.085650: step 3957, loss 0.618111.
Train: 2018-08-02T11:03:05.304348: step 3958, loss 0.594544.
Train: 2018-08-02T11:03:05.523047: step 3959, loss 0.492925.
Train: 2018-08-02T11:03:05.741746: step 3960, loss 0.633585.
Test: 2018-08-02T11:03:06.913316: step 3960, loss 0.551337.
Train: 2018-08-02T11:03:07.116394: step 3961, loss 0.539884.
Train: 2018-08-02T11:03:07.335117: step 3962, loss 0.571097.
Train: 2018-08-02T11:03:07.553815: step 3963, loss 0.672406.
Train: 2018-08-02T11:03:07.772489: step 3964, loss 0.571139.
Train: 2018-08-02T11:03:07.991188: step 3965, loss 0.509183.
Train: 2018-08-02T11:03:08.209887: step 3966, loss 0.462781.
Train: 2018-08-02T11:03:08.428617: step 3967, loss 0.594433.
Train: 2018-08-02T11:03:08.647315: step 3968, loss 0.547822.
Train: 2018-08-02T11:03:08.866015: step 3969, loss 0.501035.
Train: 2018-08-02T11:03:09.084714: step 3970, loss 0.500744.
Test: 2018-08-02T11:03:10.256283: step 3970, loss 0.549327.
Train: 2018-08-02T11:03:10.459387: step 3971, loss 0.571019.
Train: 2018-08-02T11:03:10.678090: step 3972, loss 0.555198.
Train: 2018-08-02T11:03:10.896758: step 3973, loss 0.539244.
Train: 2018-08-02T11:03:11.115487: step 3974, loss 0.578859.
Train: 2018-08-02T11:03:11.334187: step 3975, loss 0.546897.
Train: 2018-08-02T11:03:11.552885: step 3976, loss 0.586891.
Train: 2018-08-02T11:03:11.771583: step 3977, loss 0.514492.
Train: 2018-08-02T11:03:11.990278: step 3978, loss 0.538487.
Train: 2018-08-02T11:03:12.208976: step 3979, loss 0.538334.
Train: 2018-08-02T11:03:12.427680: step 3980, loss 0.521879.
Test: 2018-08-02T11:03:13.583629: step 3980, loss 0.547014.
Train: 2018-08-02T11:03:13.802358: step 3981, loss 0.513442.
Train: 2018-08-02T11:03:14.021027: step 3982, loss 0.587225.
Train: 2018-08-02T11:03:14.239751: step 3983, loss 0.529412.
Train: 2018-08-02T11:03:14.458455: step 3984, loss 0.587371.
Train: 2018-08-02T11:03:14.677153: step 3985, loss 0.545762.
Train: 2018-08-02T11:03:14.895847: step 3986, loss 0.528964.
Train: 2018-08-02T11:03:15.114551: step 3987, loss 0.554.
Train: 2018-08-02T11:03:15.333250: step 3988, loss 0.528702.
Train: 2018-08-02T11:03:15.551948: step 3989, loss 0.570811.
Train: 2018-08-02T11:03:15.770647: step 3990, loss 0.579297.
Test: 2018-08-02T11:03:16.942218: step 3990, loss 0.54622.
Train: 2018-08-02T11:03:17.145326: step 3991, loss 0.5963.
Train: 2018-08-02T11:03:17.364024: step 3992, loss 0.553852.
Train: 2018-08-02T11:03:17.582723: step 3993, loss 0.536844.
Train: 2018-08-02T11:03:17.801392: step 3994, loss 0.536817.
Train: 2018-08-02T11:03:18.020091: step 3995, loss 0.57086.
Train: 2018-08-02T11:03:18.238790: step 3996, loss 0.61349.
Train: 2018-08-02T11:03:18.457488: step 3997, loss 0.613455.
Train: 2018-08-02T11:03:18.676187: step 3998, loss 0.494324.
Train: 2018-08-02T11:03:18.894915: step 3999, loss 0.570844.
Train: 2018-08-02T11:03:19.113614: step 4000, loss 0.562348.
Test: 2018-08-02T11:03:20.285184: step 4000, loss 0.547987.
Train: 2018-08-02T11:03:21.128768: step 4001, loss 0.511451.
Train: 2018-08-02T11:03:21.347466: step 4002, loss 0.485999.
Train: 2018-08-02T11:03:21.566166: step 4003, loss 0.46889.
Train: 2018-08-02T11:03:21.784864: step 4004, loss 0.579386.
Train: 2018-08-02T11:03:22.003533: step 4005, loss 0.476923.
Train: 2018-08-02T11:03:22.222231: step 4006, loss 0.510904.
Train: 2018-08-02T11:03:22.440929: step 4007, loss 0.579549.
Train: 2018-08-02T11:03:22.659659: step 4008, loss 0.553706.
Train: 2018-08-02T11:03:22.878327: step 4009, loss 0.648888.
Train: 2018-08-02T11:03:23.097027: step 4010, loss 0.588313.
Test: 2018-08-02T11:03:24.268627: step 4010, loss 0.548292.
Train: 2018-08-02T11:03:24.471728: step 4011, loss 0.536388.
Train: 2018-08-02T11:03:24.690404: step 4012, loss 0.527747.
Train: 2018-08-02T11:03:24.909103: step 4013, loss 0.527742.
Train: 2018-08-02T11:03:25.127802: step 4014, loss 0.579652.
Train: 2018-08-02T11:03:25.346529: step 4015, loss 0.57965.
Train: 2018-08-02T11:03:25.565223: step 4016, loss 0.536401.
Train: 2018-08-02T11:03:25.783928: step 4017, loss 0.536412.
Train: 2018-08-02T11:03:26.002620: step 4018, loss 0.570982.
Train: 2018-08-02T11:03:26.221295: step 4019, loss 0.519158.
Train: 2018-08-02T11:03:26.440025: step 4020, loss 0.570976.
Test: 2018-08-02T11:03:27.595973: step 4020, loss 0.54792.
Train: 2018-08-02T11:03:27.814696: step 4021, loss 0.579606.
Train: 2018-08-02T11:03:28.033400: step 4022, loss 0.588212.
Train: 2018-08-02T11:03:28.252100: step 4023, loss 0.519286.
Train: 2018-08-02T11:03:28.470792: step 4024, loss 0.588142.
Train: 2018-08-02T11:03:28.689466: step 4025, loss 0.527983.
Train: 2018-08-02T11:03:28.908195: step 4026, loss 0.613811.
Train: 2018-08-02T11:03:29.126864: step 4027, loss 0.536655.
Train: 2018-08-02T11:03:29.345564: step 4028, loss 0.536701.
Train: 2018-08-02T11:03:29.564293: step 4029, loss 0.639152.
Train: 2018-08-02T11:03:29.782991: step 4030, loss 0.587872.
Test: 2018-08-02T11:03:30.954561: step 4030, loss 0.548579.
Train: 2018-08-02T11:03:31.157638: step 4031, loss 0.562351.
Train: 2018-08-02T11:03:31.376337: step 4032, loss 0.528553.
Train: 2018-08-02T11:03:31.595061: step 4033, loss 0.511781.
Train: 2018-08-02T11:03:31.813766: step 4034, loss 0.562375.
Train: 2018-08-02T11:03:32.032464: step 4035, loss 0.553971.
Train: 2018-08-02T11:03:32.251164: step 4036, loss 0.570787.
Train: 2018-08-02T11:03:32.469857: step 4037, loss 0.528837.
Train: 2018-08-02T11:03:32.688531: step 4038, loss 0.57078.
Train: 2018-08-02T11:03:32.907259: step 4039, loss 0.612658.
Train: 2018-08-02T11:03:33.125959: step 4040, loss 0.570771.
Test: 2018-08-02T11:03:34.297528: step 4040, loss 0.54792.
Train: 2018-08-02T11:03:34.500631: step 4041, loss 0.554085.
Train: 2018-08-02T11:03:34.719335: step 4042, loss 0.554112.
Train: 2018-08-02T11:03:34.938035: step 4043, loss 0.612319.
Train: 2018-08-02T11:03:35.156733: step 4044, loss 0.521021.
Train: 2018-08-02T11:03:35.375431: step 4045, loss 0.53765.
Train: 2018-08-02T11:03:35.594133: step 4046, loss 0.62037.
Train: 2018-08-02T11:03:35.812799: step 4047, loss 0.570756.
Train: 2018-08-02T11:03:36.031528: step 4048, loss 0.57899.
Train: 2018-08-02T11:03:36.250226: step 4049, loss 0.5297.
Train: 2018-08-02T11:03:36.468926: step 4050, loss 0.636359.
Test: 2018-08-02T11:03:37.640496: step 4050, loss 0.548942.
Train: 2018-08-02T11:03:37.843573: step 4051, loss 0.521721.
Train: 2018-08-02T11:03:38.062273: step 4052, loss 0.513657.
Train: 2018-08-02T11:03:38.280976: step 4053, loss 0.554464.
Train: 2018-08-02T11:03:38.499700: step 4054, loss 0.627844.
Train: 2018-08-02T11:03:38.718393: step 4055, loss 0.554503.
Train: 2018-08-02T11:03:38.937092: step 4056, loss 0.570784.
Train: 2018-08-02T11:03:39.155796: step 4057, loss 0.554559.
Train: 2018-08-02T11:03:39.390105: step 4058, loss 0.530261.
Train: 2018-08-02T11:03:39.593194: step 4059, loss 0.651845.
Train: 2018-08-02T11:03:39.811893: step 4060, loss 0.530369.
Test: 2018-08-02T11:03:40.967842: step 4060, loss 0.548499.
Train: 2018-08-02T11:03:41.186566: step 4061, loss 0.554655.
Train: 2018-08-02T11:03:41.405263: step 4062, loss 0.643447.
Train: 2018-08-02T11:03:41.623971: step 4063, loss 0.603024.
Train: 2018-08-02T11:03:41.842662: step 4064, loss 0.546776.
Train: 2018-08-02T11:03:42.061367: step 4065, loss 0.570861.
Train: 2018-08-02T11:03:42.295682: step 4066, loss 0.59483.
Train: 2018-08-02T11:03:42.514385: step 4067, loss 0.54701.
Train: 2018-08-02T11:03:42.733083: step 4068, loss 0.650377.
Train: 2018-08-02T11:03:42.951776: step 4069, loss 0.57886.
Train: 2018-08-02T11:03:43.170479: step 4070, loss 0.610413.
Test: 2018-08-02T11:03:44.342051: step 4070, loss 0.549689.
Train: 2018-08-02T11:03:44.545159: step 4071, loss 0.578872.
Train: 2018-08-02T11:03:44.763857: step 4072, loss 0.461569.
Train: 2018-08-02T11:03:44.982558: step 4073, loss 0.532003.
Train: 2018-08-02T11:03:45.201256: step 4074, loss 0.563254.
Train: 2018-08-02T11:03:45.419954: step 4075, loss 0.524154.
Train: 2018-08-02T11:03:45.638648: step 4076, loss 0.477078.
Train: 2018-08-02T11:03:45.826078: step 4077, loss 0.663764.
Train: 2018-08-02T11:03:46.044811: step 4078, loss 0.563129.
Train: 2018-08-02T11:03:46.263477: step 4079, loss 0.578866.
Train: 2018-08-02T11:03:46.482175: step 4080, loss 0.55521.
Test: 2018-08-02T11:03:47.653775: step 4080, loss 0.551346.
Train: 2018-08-02T11:03:47.856887: step 4081, loss 0.555182.
Train: 2018-08-02T11:03:48.075553: step 4082, loss 0.586765.
Train: 2018-08-02T11:03:48.294281: step 4083, loss 0.539307.
Train: 2018-08-02T11:03:48.512979: step 4084, loss 0.555092.
Train: 2018-08-02T11:03:48.731678: step 4085, loss 0.618536.
Train: 2018-08-02T11:03:48.950378: step 4086, loss 0.562983.
Train: 2018-08-02T11:03:49.169076: step 4087, loss 0.523272.
Train: 2018-08-02T11:03:49.387746: step 4088, loss 0.539096.
Train: 2018-08-02T11:03:49.606473: step 4089, loss 0.539016.
Train: 2018-08-02T11:03:49.825142: step 4090, loss 0.498974.
Test: 2018-08-02T11:03:50.981121: step 4090, loss 0.548141.
Train: 2018-08-02T11:03:51.199851: step 4091, loss 0.490647.
Train: 2018-08-02T11:03:51.418549: step 4092, loss 0.54663.
Train: 2018-08-02T11:03:51.637219: step 4093, loss 0.603213.
Train: 2018-08-02T11:03:51.855947: step 4094, loss 0.554511.
Train: 2018-08-02T11:03:52.074616: step 4095, loss 0.554443.
Train: 2018-08-02T11:03:52.293344: step 4096, loss 0.611721.
Train: 2018-08-02T11:03:52.512043: step 4097, loss 0.554347.
Train: 2018-08-02T11:03:52.730738: step 4098, loss 0.652977.
Train: 2018-08-02T11:03:52.949441: step 4099, loss 0.578977.
Train: 2018-08-02T11:03:53.168139: step 4100, loss 0.620028.
Test: 2018-08-02T11:03:54.324089: step 4100, loss 0.548507.
Train: 2018-08-02T11:03:55.152021: step 4101, loss 0.488822.
Train: 2018-08-02T11:03:55.370748: step 4102, loss 0.554379.
Train: 2018-08-02T11:03:55.589448: step 4103, loss 0.603532.
Train: 2018-08-02T11:03:55.808148: step 4104, loss 0.570765.
Train: 2018-08-02T11:03:56.026861: step 4105, loss 0.538068.
Train: 2018-08-02T11:03:56.245544: step 4106, loss 0.546255.
Train: 2018-08-02T11:03:56.464247: step 4107, loss 0.529914.
Train: 2018-08-02T11:03:56.682943: step 4108, loss 0.529883.
Train: 2018-08-02T11:03:56.901635: step 4109, loss 0.570764.
Train: 2018-08-02T11:03:57.120339: step 4110, loss 0.546173.
Test: 2018-08-02T11:03:58.291909: step 4110, loss 0.548658.
Train: 2018-08-02T11:03:58.495017: step 4111, loss 0.529725.
Train: 2018-08-02T11:03:58.713716: step 4112, loss 0.562536.
Train: 2018-08-02T11:03:58.932416: step 4113, loss 0.488397.
Train: 2018-08-02T11:03:59.151116: step 4114, loss 0.612064.
Train: 2018-08-02T11:03:59.369812: step 4115, loss 0.562482.
Train: 2018-08-02T11:03:59.588511: step 4116, loss 0.570757.
Train: 2018-08-02T11:03:59.807211: step 4117, loss 0.570758.
Train: 2018-08-02T11:04:00.025904: step 4118, loss 0.56246.
Train: 2018-08-02T11:04:00.244608: step 4119, loss 0.612272.
Train: 2018-08-02T11:04:00.463307: step 4120, loss 0.512688.
Test: 2018-08-02T11:04:01.634877: step 4120, loss 0.548601.
Train: 2018-08-02T11:04:01.837984: step 4121, loss 0.587355.
Train: 2018-08-02T11:04:02.056684: step 4122, loss 0.512692.
Train: 2018-08-02T11:04:02.275378: step 4123, loss 0.58736.
Train: 2018-08-02T11:04:02.494081: step 4124, loss 0.545857.
Train: 2018-08-02T11:04:02.712780: step 4125, loss 0.529245.
Train: 2018-08-02T11:04:02.931479: step 4126, loss 0.604.
Train: 2018-08-02T11:04:03.150177: step 4127, loss 0.57076.
Train: 2018-08-02T11:04:03.368845: step 4128, loss 0.579063.
Train: 2018-08-02T11:04:03.587570: step 4129, loss 0.529275.
Train: 2018-08-02T11:04:03.806268: step 4130, loss 0.595644.
Test: 2018-08-02T11:04:04.977843: step 4130, loss 0.548439.
Train: 2018-08-02T11:04:05.180922: step 4131, loss 0.587332.
Train: 2018-08-02T11:04:05.399650: step 4132, loss 0.537658.
Train: 2018-08-02T11:04:05.618352: step 4133, loss 0.504619.
Train: 2018-08-02T11:04:05.837043: step 4134, loss 0.504589.
Train: 2018-08-02T11:04:06.055748: step 4135, loss 0.703308.
Train: 2018-08-02T11:04:06.274446: step 4136, loss 0.521138.
Train: 2018-08-02T11:04:06.493144: step 4137, loss 0.521175.
Train: 2018-08-02T11:04:06.711843: step 4138, loss 0.488105.
Train: 2018-08-02T11:04:06.930543: step 4139, loss 0.620436.
Train: 2018-08-02T11:04:07.149236: step 4140, loss 0.504503.
Test: 2018-08-02T11:04:08.305190: step 4140, loss 0.548232.
Train: 2018-08-02T11:04:08.523919: step 4141, loss 0.562464.
Train: 2018-08-02T11:04:08.742618: step 4142, loss 0.537546.
Train: 2018-08-02T11:04:08.961286: step 4143, loss 0.587393.
Train: 2018-08-02T11:04:09.180021: step 4144, loss 0.570762.
Train: 2018-08-02T11:04:09.398714: step 4145, loss 0.604064.
Train: 2018-08-02T11:04:09.617407: step 4146, loss 0.5874.
Train: 2018-08-02T11:04:09.836111: step 4147, loss 0.512599.
Train: 2018-08-02T11:04:10.070434: step 4148, loss 0.562452.
Train: 2018-08-02T11:04:10.289100: step 4149, loss 0.545842.
Train: 2018-08-02T11:04:10.507824: step 4150, loss 0.512615.
Test: 2018-08-02T11:04:11.679400: step 4150, loss 0.546836.
Train: 2018-08-02T11:04:11.898129: step 4151, loss 0.545815.
Train: 2018-08-02T11:04:12.116828: step 4152, loss 0.579089.
Train: 2018-08-02T11:04:12.335526: step 4153, loss 0.579095.
Train: 2018-08-02T11:04:12.554226: step 4154, loss 0.612424.
Train: 2018-08-02T11:04:12.772923: step 4155, loss 0.545794.
Train: 2018-08-02T11:04:12.991592: step 4156, loss 0.537495.
Train: 2018-08-02T11:04:13.210325: step 4157, loss 0.537502.
Train: 2018-08-02T11:04:13.429021: step 4158, loss 0.604029.
Train: 2018-08-02T11:04:13.647688: step 4159, loss 0.57907.
Train: 2018-08-02T11:04:13.866418: step 4160, loss 0.545857.
Test: 2018-08-02T11:04:15.037988: step 4160, loss 0.54823.
Train: 2018-08-02T11:04:15.241096: step 4161, loss 0.645404.
Train: 2018-08-02T11:04:15.459794: step 4162, loss 0.562484.
Train: 2018-08-02T11:04:15.678494: step 4163, loss 0.554253.
Train: 2018-08-02T11:04:15.897193: step 4164, loss 0.554289.
Train: 2018-08-02T11:04:16.115891: step 4165, loss 0.652958.
Train: 2018-08-02T11:04:16.334590: step 4166, loss 0.521626.
Train: 2018-08-02T11:04:16.553290: step 4167, loss 0.562599.
Train: 2018-08-02T11:04:16.771987: step 4168, loss 0.51371.
Train: 2018-08-02T11:04:16.990686: step 4169, loss 0.587068.
Train: 2018-08-02T11:04:17.209379: step 4170, loss 0.635858.
Test: 2018-08-02T11:04:18.365334: step 4170, loss 0.549881.
Train: 2018-08-02T11:04:18.584064: step 4171, loss 0.635681.
Train: 2018-08-02T11:04:18.802763: step 4172, loss 0.578886.
Train: 2018-08-02T11:04:19.021460: step 4173, loss 0.570831.
Train: 2018-08-02T11:04:19.240160: step 4174, loss 0.586877.
Train: 2018-08-02T11:04:19.458858: step 4175, loss 0.538966.
Train: 2018-08-02T11:04:19.677527: step 4176, loss 0.539082.
Train: 2018-08-02T11:04:19.896225: step 4177, loss 0.562979.
Train: 2018-08-02T11:04:20.114925: step 4178, loss 0.610567.
Train: 2018-08-02T11:04:20.333653: step 4179, loss 0.570953.
Train: 2018-08-02T11:04:20.552354: step 4180, loss 0.515748.
Test: 2018-08-02T11:04:21.739544: step 4180, loss 0.550293.
Train: 2018-08-02T11:04:21.958243: step 4181, loss 0.594632.
Train: 2018-08-02T11:04:22.176973: step 4182, loss 0.602489.
Train: 2018-08-02T11:04:22.395673: step 4183, loss 0.571011.
Train: 2018-08-02T11:04:22.614370: step 4184, loss 0.523956.
Train: 2018-08-02T11:04:22.833067: step 4185, loss 0.649458.
Train: 2018-08-02T11:04:23.051736: step 4186, loss 0.594531.
Train: 2018-08-02T11:04:23.301715: step 4187, loss 0.57889.
Train: 2018-08-02T11:04:23.520407: step 4188, loss 0.50106.
Train: 2018-08-02T11:04:23.739107: step 4189, loss 0.555561.
Train: 2018-08-02T11:04:23.957775: step 4190, loss 0.578902.
Test: 2018-08-02T11:04:25.113754: step 4190, loss 0.550171.
Train: 2018-08-02T11:04:25.332484: step 4191, loss 0.547784.
Train: 2018-08-02T11:04:25.551182: step 4192, loss 0.5789.
Train: 2018-08-02T11:04:25.769880: step 4193, loss 0.516608.
Train: 2018-08-02T11:04:25.988580: step 4194, loss 0.508693.
Train: 2018-08-02T11:04:26.207247: step 4195, loss 0.618005.
Train: 2018-08-02T11:04:26.425978: step 4196, loss 0.555363.
Train: 2018-08-02T11:04:26.644678: step 4197, loss 0.555313.
Train: 2018-08-02T11:04:26.863374: step 4198, loss 0.563128.
Train: 2018-08-02T11:04:27.082073: step 4199, loss 0.578864.
Train: 2018-08-02T11:04:27.300773: step 4200, loss 0.626256.
Test: 2018-08-02T11:04:28.456721: step 4200, loss 0.549323.
Train: 2018-08-02T11:04:29.284682: step 4201, loss 0.523563.
Train: 2018-08-02T11:04:29.503380: step 4202, loss 0.626318.
Train: 2018-08-02T11:04:29.722080: step 4203, loss 0.52351.
Train: 2018-08-02T11:04:29.940779: step 4204, loss 0.570946.
Train: 2018-08-02T11:04:30.159447: step 4205, loss 0.5551.
Train: 2018-08-02T11:04:30.378146: step 4206, loss 0.586787.
Train: 2018-08-02T11:04:30.596844: step 4207, loss 0.562995.
Train: 2018-08-02T11:04:30.815569: step 4208, loss 0.531241.
Train: 2018-08-02T11:04:31.034272: step 4209, loss 0.523224.
Train: 2018-08-02T11:04:31.252971: step 4210, loss 0.594793.
Test: 2018-08-02T11:04:32.424541: step 4210, loss 0.550129.
Train: 2018-08-02T11:04:32.627649: step 4211, loss 0.546945.
Train: 2018-08-02T11:04:32.846348: step 4212, loss 0.530901.
Train: 2018-08-02T11:04:33.065043: step 4213, loss 0.578866.
Train: 2018-08-02T11:04:33.283745: step 4214, loss 0.546751.
Train: 2018-08-02T11:04:33.502445: step 4215, loss 0.546685.
Train: 2018-08-02T11:04:33.721113: step 4216, loss 0.578882.
Train: 2018-08-02T11:04:33.939812: step 4217, loss 0.489978.
Train: 2018-08-02T11:04:34.158540: step 4218, loss 0.570792.
Train: 2018-08-02T11:04:34.377239: step 4219, loss 0.578916.
Train: 2018-08-02T11:04:34.595942: step 4220, loss 0.595234.
Test: 2018-08-02T11:04:35.767508: step 4220, loss 0.549725.
Train: 2018-08-02T11:04:35.970617: step 4221, loss 0.538116.
Train: 2018-08-02T11:04:36.189316: step 4222, loss 0.497167.
Train: 2018-08-02T11:04:36.408014: step 4223, loss 0.554356.
Train: 2018-08-02T11:04:36.626683: step 4224, loss 0.537853.
Train: 2018-08-02T11:04:36.845405: step 4225, loss 0.479985.
Train: 2018-08-02T11:04:37.064104: step 4226, loss 0.479568.
Train: 2018-08-02T11:04:37.282809: step 4227, loss 0.637476.
Train: 2018-08-02T11:04:37.470235: step 4228, loss 0.651664.
Train: 2018-08-02T11:04:37.688965: step 4229, loss 0.570777.
Train: 2018-08-02T11:04:37.907663: step 4230, loss 0.562397.
Test: 2018-08-02T11:04:39.079234: step 4230, loss 0.54955.
Train: 2018-08-02T11:04:39.282336: step 4231, loss 0.562395.
Train: 2018-08-02T11:04:39.501010: step 4232, loss 0.612716.
Train: 2018-08-02T11:04:39.719739: step 4233, loss 0.570778.
Train: 2018-08-02T11:04:39.938438: step 4234, loss 0.570774.
Train: 2018-08-02T11:04:40.157105: step 4235, loss 0.503924.
Train: 2018-08-02T11:04:40.375835: step 4236, loss 0.545706.
Train: 2018-08-02T11:04:40.594528: step 4237, loss 0.537348.
Train: 2018-08-02T11:04:40.813233: step 4238, loss 0.629293.
Train: 2018-08-02T11:04:41.031902: step 4239, loss 0.562418.
Train: 2018-08-02T11:04:41.250630: step 4240, loss 0.512376.
Test: 2018-08-02T11:04:42.422202: step 4240, loss 0.550222.
Train: 2018-08-02T11:04:42.625302: step 4241, loss 0.537401.
Train: 2018-08-02T11:04:42.844007: step 4242, loss 0.595802.
Train: 2018-08-02T11:04:43.062700: step 4243, loss 0.554084.
Train: 2018-08-02T11:04:43.281375: step 4244, loss 0.554089.
Train: 2018-08-02T11:04:43.500105: step 4245, loss 0.579101.
Train: 2018-08-02T11:04:43.718798: step 4246, loss 0.587424.
Train: 2018-08-02T11:04:43.937501: step 4247, loss 0.537484.
Train: 2018-08-02T11:04:44.156199: step 4248, loss 0.628954.
Train: 2018-08-02T11:04:44.390492: step 4249, loss 0.520989.
Train: 2018-08-02T11:04:44.609218: step 4250, loss 0.587327.
Test: 2018-08-02T11:04:45.780788: step 4250, loss 0.549243.
Train: 2018-08-02T11:04:45.983867: step 4251, loss 0.587299.
Train: 2018-08-02T11:04:46.202595: step 4252, loss 0.612022.
Train: 2018-08-02T11:04:46.421295: step 4253, loss 0.504939.
Train: 2018-08-02T11:04:46.639993: step 4254, loss 0.529687.
Train: 2018-08-02T11:04:46.858662: step 4255, loss 0.521507.
Train: 2018-08-02T11:04:47.077385: step 4256, loss 0.620027.
Train: 2018-08-02T11:04:47.311685: step 4257, loss 0.546155.
Train: 2018-08-02T11:04:47.530404: step 4258, loss 0.521581.
Train: 2018-08-02T11:04:47.749109: step 4259, loss 0.505164.
Train: 2018-08-02T11:04:47.967807: step 4260, loss 0.537908.
Test: 2018-08-02T11:04:49.139377: step 4260, loss 0.549544.
Train: 2018-08-02T11:04:49.373728: step 4261, loss 0.595445.
Train: 2018-08-02T11:04:49.592427: step 4262, loss 0.554283.
Train: 2018-08-02T11:04:49.811095: step 4263, loss 0.570757.
Train: 2018-08-02T11:04:50.029824: step 4264, loss 0.521253.
Train: 2018-08-02T11:04:50.248522: step 4265, loss 0.570756.
Train: 2018-08-02T11:04:50.467222: step 4266, loss 0.570756.
Train: 2018-08-02T11:04:50.685921: step 4267, loss 0.537647.
Train: 2018-08-02T11:04:50.904589: step 4268, loss 0.570757.
Train: 2018-08-02T11:04:51.123318: step 4269, loss 0.595636.
Train: 2018-08-02T11:04:51.342017: step 4270, loss 0.579049.
Test: 2018-08-02T11:04:52.513587: step 4270, loss 0.54787.
Train: 2018-08-02T11:04:52.716695: step 4271, loss 0.579043.
Train: 2018-08-02T11:04:52.935364: step 4272, loss 0.512813.
Train: 2018-08-02T11:04:53.154093: step 4273, loss 0.54592.
Train: 2018-08-02T11:04:53.372785: step 4274, loss 0.554192.
Train: 2018-08-02T11:04:53.591484: step 4275, loss 0.479608.
Train: 2018-08-02T11:04:53.810190: step 4276, loss 0.579064.
Train: 2018-08-02T11:04:54.028858: step 4277, loss 0.520856.
Train: 2018-08-02T11:04:54.247586: step 4278, loss 0.554093.
Train: 2018-08-02T11:04:54.466279: step 4279, loss 0.57077.
Train: 2018-08-02T11:04:54.684985: step 4280, loss 0.587505.
Test: 2018-08-02T11:04:55.840933: step 4280, loss 0.547459.
Train: 2018-08-02T11:04:56.059663: step 4281, loss 0.562404.
Train: 2018-08-02T11:04:56.278358: step 4282, loss 0.579153.
Train: 2018-08-02T11:04:56.497029: step 4283, loss 0.520524.
Train: 2018-08-02T11:04:56.715758: step 4284, loss 0.562397.
Train: 2018-08-02T11:04:56.934458: step 4285, loss 0.545623.
Train: 2018-08-02T11:04:57.153125: step 4286, loss 0.579175.
Train: 2018-08-02T11:04:57.371855: step 4287, loss 0.587569.
Train: 2018-08-02T11:04:57.590554: step 4288, loss 0.595945.
Train: 2018-08-02T11:04:57.809221: step 4289, loss 0.50377.
Train: 2018-08-02T11:04:58.027951: step 4290, loss 0.495402.
Test: 2018-08-02T11:04:59.199521: step 4290, loss 0.548777.
Train: 2018-08-02T11:04:59.402599: step 4291, loss 0.595936.
Train: 2018-08-02T11:04:59.621328: step 4292, loss 0.520458.
Train: 2018-08-02T11:04:59.840022: step 4293, loss 0.478433.
Train: 2018-08-02T11:05:00.058728: step 4294, loss 0.55396.
Train: 2018-08-02T11:05:00.277425: step 4295, loss 0.629866.
Train: 2018-08-02T11:05:00.496123: step 4296, loss 0.537042.
Train: 2018-08-02T11:05:00.714793: step 4297, loss 0.570811.
Train: 2018-08-02T11:05:00.933515: step 4298, loss 0.494753.
Train: 2018-08-02T11:05:01.152214: step 4299, loss 0.553892.
Train: 2018-08-02T11:05:01.370918: step 4300, loss 0.503014.
Test: 2018-08-02T11:05:02.542489: step 4300, loss 0.547177.
Train: 2018-08-02T11:05:03.354831: step 4301, loss 0.579341.
Train: 2018-08-02T11:05:03.573522: step 4302, loss 0.528301.
Train: 2018-08-02T11:05:03.807818: step 4303, loss 0.56234.
Train: 2018-08-02T11:05:04.034521: step 4304, loss 0.587957.
Train: 2018-08-02T11:05:04.237598: step 4305, loss 0.587972.
Train: 2018-08-02T11:05:04.456328: step 4306, loss 0.605051.
Train: 2018-08-02T11:05:04.674996: step 4307, loss 0.519685.
Train: 2018-08-02T11:05:04.893698: step 4308, loss 0.570866.
Train: 2018-08-02T11:05:05.112425: step 4309, loss 0.468638.
Train: 2018-08-02T11:05:05.331123: step 4310, loss 0.58792.
Test: 2018-08-02T11:05:06.502693: step 4310, loss 0.54849.
Train: 2018-08-02T11:05:06.705801: step 4311, loss 0.545284.
Train: 2018-08-02T11:05:06.924502: step 4312, loss 0.622047.
Train: 2018-08-02T11:05:07.143194: step 4313, loss 0.587898.
Train: 2018-08-02T11:05:07.361868: step 4314, loss 0.57935.
Train: 2018-08-02T11:05:07.580598: step 4315, loss 0.596281.
Train: 2018-08-02T11:05:07.799290: step 4316, loss 0.621559.
Train: 2018-08-02T11:05:08.017995: step 4317, loss 0.520267.
Train: 2018-08-02T11:05:08.236687: step 4318, loss 0.553993.
Train: 2018-08-02T11:05:08.455392: step 4319, loss 0.60427.
Train: 2018-08-02T11:05:08.674060: step 4320, loss 0.504015.
Test: 2018-08-02T11:05:09.845661: step 4320, loss 0.548336.
Train: 2018-08-02T11:05:10.048763: step 4321, loss 0.562436.
Train: 2018-08-02T11:05:10.267437: step 4322, loss 0.545825.
Train: 2018-08-02T11:05:10.486162: step 4323, loss 0.496058.
Train: 2018-08-02T11:05:10.704878: step 4324, loss 0.562458.
Train: 2018-08-02T11:05:10.923564: step 4325, loss 0.587361.
Train: 2018-08-02T11:05:11.142262: step 4326, loss 0.637126.
Train: 2018-08-02T11:05:11.360962: step 4327, loss 0.554203.
Train: 2018-08-02T11:05:11.579655: step 4328, loss 0.529456.
Train: 2018-08-02T11:05:11.798353: step 4329, loss 0.554255.
Train: 2018-08-02T11:05:12.017060: step 4330, loss 0.521296.
Test: 2018-08-02T11:05:13.188628: step 4330, loss 0.546467.
Train: 2018-08-02T11:05:13.407351: step 4331, loss 0.579001.
Train: 2018-08-02T11:05:13.626026: step 4332, loss 0.554274.
Train: 2018-08-02T11:05:13.844749: step 4333, loss 0.578997.
Train: 2018-08-02T11:05:14.063422: step 4334, loss 0.603697.
Train: 2018-08-02T11:05:14.282147: step 4335, loss 0.620092.
Train: 2018-08-02T11:05:14.500820: step 4336, loss 0.578962.
Train: 2018-08-02T11:05:14.719545: step 4337, loss 0.529888.
Train: 2018-08-02T11:05:14.938248: step 4338, loss 0.554451.
Train: 2018-08-02T11:05:15.156948: step 4339, loss 0.562627.
Train: 2018-08-02T11:05:15.375646: step 4340, loss 0.57078.
Test: 2018-08-02T11:05:16.547217: step 4340, loss 0.550583.
Train: 2018-08-02T11:05:16.750296: step 4341, loss 0.53828.
Train: 2018-08-02T11:05:16.969020: step 4342, loss 0.562666.
Train: 2018-08-02T11:05:17.187722: step 4343, loss 0.587021.
Train: 2018-08-02T11:05:17.406420: step 4344, loss 0.587008.
Train: 2018-08-02T11:05:17.625113: step 4345, loss 0.562704.
Train: 2018-08-02T11:05:17.843819: step 4346, loss 0.562721.
Train: 2018-08-02T11:05:18.062517: step 4347, loss 0.546585.
Train: 2018-08-02T11:05:18.281211: step 4348, loss 0.619235.
Train: 2018-08-02T11:05:18.499909: step 4349, loss 0.482213.
Train: 2018-08-02T11:05:18.718609: step 4350, loss 0.554704.
Test: 2018-08-02T11:05:19.874563: step 4350, loss 0.548362.
Train: 2018-08-02T11:05:20.093262: step 4351, loss 0.562755.
Train: 2018-08-02T11:05:20.311961: step 4352, loss 0.546613.
Train: 2018-08-02T11:05:20.530689: step 4353, loss 0.60311.
Train: 2018-08-02T11:05:20.749358: step 4354, loss 0.562736.
Train: 2018-08-02T11:05:20.983678: step 4355, loss 0.635407.
Train: 2018-08-02T11:05:21.202401: step 4356, loss 0.546637.
Train: 2018-08-02T11:05:21.421099: step 4357, loss 0.538616.
Train: 2018-08-02T11:05:21.639800: step 4358, loss 0.562777.
Train: 2018-08-02T11:05:21.858507: step 4359, loss 0.522542.
Train: 2018-08-02T11:05:22.077173: step 4360, loss 0.635259.
Test: 2018-08-02T11:05:23.248772: step 4360, loss 0.549527.
Train: 2018-08-02T11:05:23.451850: step 4361, loss 0.546687.
Train: 2018-08-02T11:05:23.670549: step 4362, loss 0.562786.
Train: 2018-08-02T11:05:23.889248: step 4363, loss 0.594957.
Train: 2018-08-02T11:05:24.107977: step 4364, loss 0.530667.
Train: 2018-08-02T11:05:24.326675: step 4365, loss 0.611008.
Train: 2018-08-02T11:05:24.545374: step 4366, loss 0.554793.
Train: 2018-08-02T11:05:24.764042: step 4367, loss 0.594907.
Train: 2018-08-02T11:05:24.982772: step 4368, loss 0.642937.
Train: 2018-08-02T11:05:25.201465: step 4369, loss 0.530953.
Train: 2018-08-02T11:05:25.420139: step 4370, loss 0.523071.
Test: 2018-08-02T11:05:26.591740: step 4370, loss 0.550177.
Train: 2018-08-02T11:05:26.794848: step 4371, loss 0.570894.
Train: 2018-08-02T11:05:27.013546: step 4372, loss 0.626621.
Train: 2018-08-02T11:05:27.232245: step 4373, loss 0.578858.
Train: 2018-08-02T11:05:27.450944: step 4374, loss 0.515432.
Train: 2018-08-02T11:05:27.669612: step 4375, loss 0.523388.
Train: 2018-08-02T11:05:27.888311: step 4376, loss 0.666086.
Train: 2018-08-02T11:05:28.107009: step 4377, loss 0.547198.
Train: 2018-08-02T11:05:28.325738: step 4378, loss 0.49979.
Train: 2018-08-02T11:05:28.513164: step 4379, loss 0.630559.
Train: 2018-08-02T11:05:28.731894: step 4380, loss 0.539328.
Test: 2018-08-02T11:05:29.903463: step 4380, loss 0.54966.
Train: 2018-08-02T11:05:30.106566: step 4381, loss 0.570954.
Train: 2018-08-02T11:05:30.325266: step 4382, loss 0.507702.
Train: 2018-08-02T11:05:30.543970: step 4383, loss 0.570941.
Train: 2018-08-02T11:05:30.762638: step 4384, loss 0.539214.
Train: 2018-08-02T11:05:30.981338: step 4385, loss 0.570914.
Train: 2018-08-02T11:05:31.200036: step 4386, loss 0.523155.
Train: 2018-08-02T11:05:31.418759: step 4387, loss 0.586839.
Train: 2018-08-02T11:05:31.637464: step 4388, loss 0.538895.
Train: 2018-08-02T11:05:31.856131: step 4389, loss 0.62694.
Train: 2018-08-02T11:05:32.074861: step 4390, loss 0.554814.
Test: 2018-08-02T11:05:33.246431: step 4390, loss 0.549048.
Train: 2018-08-02T11:05:33.449533: step 4391, loss 0.586893.
Train: 2018-08-02T11:05:33.668207: step 4392, loss 0.586896.
Train: 2018-08-02T11:05:33.886937: step 4393, loss 0.522692.
Train: 2018-08-02T11:05:34.105605: step 4394, loss 0.554775.
Train: 2018-08-02T11:05:34.324328: step 4395, loss 0.619074.
Train: 2018-08-02T11:05:34.543032: step 4396, loss 0.570835.
Train: 2018-08-02T11:05:34.761727: step 4397, loss 0.61904.
Train: 2018-08-02T11:05:34.980431: step 4398, loss 0.522725.
Train: 2018-08-02T11:05:35.199129: step 4399, loss 0.562833.
Train: 2018-08-02T11:05:35.417828: step 4400, loss 0.490711.
Test: 2018-08-02T11:05:36.589398: step 4400, loss 0.549411.
Train: 2018-08-02T11:05:37.401739: step 4401, loss 0.522683.
Train: 2018-08-02T11:05:37.620437: step 4402, loss 0.570829.
Train: 2018-08-02T11:05:37.839139: step 4403, loss 0.578881.
Train: 2018-08-02T11:05:38.057834: step 4404, loss 0.562735.
Train: 2018-08-02T11:05:38.276533: step 4405, loss 0.603151.
Train: 2018-08-02T11:05:38.495232: step 4406, loss 0.554622.
Train: 2018-08-02T11:05:38.713932: step 4407, loss 0.578894.
Train: 2018-08-02T11:05:38.932625: step 4408, loss 0.522225.
Train: 2018-08-02T11:05:39.151328: step 4409, loss 0.554584.
Train: 2018-08-02T11:05:39.369998: step 4410, loss 0.595135.
Test: 2018-08-02T11:05:40.541598: step 4410, loss 0.548738.
Train: 2018-08-02T11:05:40.744700: step 4411, loss 0.635734.
Train: 2018-08-02T11:05:40.963373: step 4412, loss 0.611329.
Train: 2018-08-02T11:05:41.182106: step 4413, loss 0.506099.
Train: 2018-08-02T11:05:41.400802: step 4414, loss 0.570806.
Train: 2018-08-02T11:05:41.635124: step 4415, loss 0.554659.
Train: 2018-08-02T11:05:41.853814: step 4416, loss 0.530454.
Train: 2018-08-02T11:05:42.072519: step 4417, loss 0.60311.
Train: 2018-08-02T11:05:42.291218: step 4418, loss 0.619235.
Train: 2018-08-02T11:05:42.509918: step 4419, loss 0.603045.
Train: 2018-08-02T11:05:42.728616: step 4420, loss 0.562801.
Test: 2018-08-02T11:05:43.884565: step 4420, loss 0.548151.
Train: 2018-08-02T11:05:44.103263: step 4421, loss 0.530762.
Train: 2018-08-02T11:05:44.337611: step 4422, loss 0.562848.
Train: 2018-08-02T11:05:44.556312: step 4423, loss 0.530858.
Train: 2018-08-02T11:05:44.774982: step 4424, loss 0.514853.
Train: 2018-08-02T11:05:44.993710: step 4425, loss 0.570854.
Train: 2018-08-02T11:05:45.212410: step 4426, loss 0.578868.
Train: 2018-08-02T11:05:45.431108: step 4427, loss 0.635051.
Train: 2018-08-02T11:05:45.649807: step 4428, loss 0.546797.
Train: 2018-08-02T11:05:45.884099: step 4429, loss 0.659001.
Train: 2018-08-02T11:05:46.087206: step 4430, loss 0.498942.
Test: 2018-08-02T11:05:47.258775: step 4430, loss 0.549184.
Train: 2018-08-02T11:05:47.461883: step 4431, loss 0.634763.
Train: 2018-08-02T11:05:47.680581: step 4432, loss 0.610729.
Train: 2018-08-02T11:05:47.899249: step 4433, loss 0.539147.
Train: 2018-08-02T11:05:48.117979: step 4434, loss 0.610561.
Train: 2018-08-02T11:05:48.336678: step 4435, loss 0.586764.
Train: 2018-08-02T11:05:48.555376: step 4436, loss 0.56311.
Train: 2018-08-02T11:05:48.774044: step 4437, loss 0.571013.
Train: 2018-08-02T11:05:48.992774: step 4438, loss 0.563198.
Train: 2018-08-02T11:05:49.211443: step 4439, loss 0.563234.
Train: 2018-08-02T11:05:49.430171: step 4440, loss 0.586699.
Test: 2018-08-02T11:05:50.601742: step 4440, loss 0.549911.
Train: 2018-08-02T11:05:50.820441: step 4441, loss 0.563295.
Train: 2018-08-02T11:05:51.039164: step 4442, loss 0.610052.
Train: 2018-08-02T11:05:51.257868: step 4443, loss 0.532275.
Train: 2018-08-02T11:05:51.476537: step 4444, loss 0.602205.
Train: 2018-08-02T11:05:51.695266: step 4445, loss 0.555658.
Train: 2018-08-02T11:05:51.913934: step 4446, loss 0.547937.
Train: 2018-08-02T11:05:52.132634: step 4447, loss 0.633138.
Train: 2018-08-02T11:05:52.351363: step 4448, loss 0.509341.
Train: 2018-08-02T11:05:52.570055: step 4449, loss 0.532527.
Train: 2018-08-02T11:05:52.788760: step 4450, loss 0.594411.
Test: 2018-08-02T11:05:53.960330: step 4450, loss 0.551731.
Train: 2018-08-02T11:05:54.163408: step 4451, loss 0.555675.
Train: 2018-08-02T11:05:54.382106: step 4452, loss 0.563402.
Train: 2018-08-02T11:05:54.600831: step 4453, loss 0.555612.
Train: 2018-08-02T11:05:54.819535: step 4454, loss 0.547792.
Train: 2018-08-02T11:05:55.053855: step 4455, loss 0.555514.
Train: 2018-08-02T11:05:55.272553: step 4456, loss 0.500772.
Train: 2018-08-02T11:05:55.491251: step 4457, loss 0.578875.
Train: 2018-08-02T11:05:55.709952: step 4458, loss 0.618206.
Train: 2018-08-02T11:05:55.928650: step 4459, loss 0.523696.
Train: 2018-08-02T11:05:56.147348: step 4460, loss 0.578861.
Test: 2018-08-02T11:05:57.318919: step 4460, loss 0.549426.
Train: 2018-08-02T11:05:57.522027: step 4461, loss 0.594701.
Train: 2018-08-02T11:05:57.740725: step 4462, loss 0.507476.
Train: 2018-08-02T11:05:57.959447: step 4463, loss 0.539092.
Train: 2018-08-02T11:05:58.178093: step 4464, loss 0.570882.
Train: 2018-08-02T11:05:58.396822: step 4465, loss 0.602866.
Train: 2018-08-02T11:05:58.615490: step 4466, loss 0.602908.
Train: 2018-08-02T11:05:58.834220: step 4467, loss 0.506701.
Train: 2018-08-02T11:05:59.052888: step 4468, loss 0.554768.
Train: 2018-08-02T11:05:59.271616: step 4469, loss 0.482268.
Train: 2018-08-02T11:05:59.490315: step 4470, loss 0.465745.
Test: 2018-08-02T11:06:00.661886: step 4470, loss 0.548706.
Train: 2018-08-02T11:06:00.864989: step 4471, loss 0.562655.
Train: 2018-08-02T11:06:01.083686: step 4472, loss 0.546256.
Train: 2018-08-02T11:06:01.302392: step 4473, loss 0.521488.
Train: 2018-08-02T11:06:01.521090: step 4474, loss 0.5625.
Train: 2018-08-02T11:06:01.739758: step 4475, loss 0.487798.
Train: 2018-08-02T11:06:01.958489: step 4476, loss 0.562422.
Train: 2018-08-02T11:06:02.177187: step 4477, loss 0.545618.
Train: 2018-08-02T11:06:02.395885: step 4478, loss 0.587658.
Train: 2018-08-02T11:06:02.614579: step 4479, loss 0.587736.
Train: 2018-08-02T11:06:02.833253: step 4480, loss 0.587792.
Test: 2018-08-02T11:06:04.004853: step 4480, loss 0.548945.
Train: 2018-08-02T11:06:04.207956: step 4481, loss 0.638785.
Train: 2018-08-02T11:06:04.426654: step 4482, loss 0.502927.
Train: 2018-08-02T11:06:04.645353: step 4483, loss 0.587829.
Train: 2018-08-02T11:06:04.864027: step 4484, loss 0.502901.
Train: 2018-08-02T11:06:05.082750: step 4485, loss 0.579346.
Train: 2018-08-02T11:06:05.301455: step 4486, loss 0.647366.
Train: 2018-08-02T11:06:05.520148: step 4487, loss 0.553863.
Train: 2018-08-02T11:06:05.738847: step 4488, loss 0.528464.
Train: 2018-08-02T11:06:05.957552: step 4489, loss 0.503107.
Train: 2018-08-02T11:06:06.176220: step 4490, loss 0.477693.
Test: 2018-08-02T11:06:07.347821: step 4490, loss 0.548966.
Train: 2018-08-02T11:06:07.550922: step 4491, loss 0.570833.
Train: 2018-08-02T11:06:07.769596: step 4492, loss 0.638785.
Train: 2018-08-02T11:06:07.988326: step 4493, loss 0.570836.
Train: 2018-08-02T11:06:08.207030: step 4494, loss 0.57083.
Train: 2018-08-02T11:06:08.425723: step 4495, loss 0.553888.
Train: 2018-08-02T11:06:08.644422: step 4496, loss 0.579275.
Train: 2018-08-02T11:06:08.863121: step 4497, loss 0.646807.
Train: 2018-08-02T11:06:09.081820: step 4498, loss 0.612868.
Train: 2018-08-02T11:06:09.300519: step 4499, loss 0.5624.
Train: 2018-08-02T11:06:09.519186: step 4500, loss 0.545736.
Test: 2018-08-02T11:06:10.690787: step 4500, loss 0.549325.
Train: 2018-08-02T11:06:11.534370: step 4501, loss 0.512557.
Train: 2018-08-02T11:06:11.753069: step 4502, loss 0.496082.
Train: 2018-08-02T11:06:11.971737: step 4503, loss 0.545878.
Train: 2018-08-02T11:06:12.190462: step 4504, loss 0.554174.
Train: 2018-08-02T11:06:12.424759: step 4505, loss 0.554174.
Train: 2018-08-02T11:06:12.643485: step 4506, loss 0.595635.
Train: 2018-08-02T11:06:12.862184: step 4507, loss 0.579043.
Train: 2018-08-02T11:06:13.080884: step 4508, loss 0.53765.
Train: 2018-08-02T11:06:13.299581: step 4509, loss 0.570756.
Train: 2018-08-02T11:06:13.518281: step 4510, loss 0.587288.
Test: 2018-08-02T11:06:14.689851: step 4510, loss 0.549475.
Train: 2018-08-02T11:06:14.892928: step 4511, loss 0.479953.
Train: 2018-08-02T11:06:15.111627: step 4512, loss 0.603797.
Train: 2018-08-02T11:06:15.330350: step 4513, loss 0.579013.
Train: 2018-08-02T11:06:15.549058: step 4514, loss 0.521259.
Train: 2018-08-02T11:06:15.767753: step 4515, loss 0.554255.
Train: 2018-08-02T11:06:15.986453: step 4516, loss 0.545998.
Train: 2018-08-02T11:06:16.205152: step 4517, loss 0.554242.
Train: 2018-08-02T11:06:16.423844: step 4518, loss 0.570756.
Train: 2018-08-02T11:06:16.642518: step 4519, loss 0.59555.
Train: 2018-08-02T11:06:16.861218: step 4520, loss 0.545976.
Test: 2018-08-02T11:06:18.032819: step 4520, loss 0.548706.
Train: 2018-08-02T11:06:18.235925: step 4521, loss 0.60379.
Train: 2018-08-02T11:06:18.454626: step 4522, loss 0.521265.
Train: 2018-08-02T11:06:18.673324: step 4523, loss 0.554262.
Train: 2018-08-02T11:06:18.892022: step 4524, loss 0.504781.
Train: 2018-08-02T11:06:19.110721: step 4525, loss 0.579013.
Train: 2018-08-02T11:06:19.329390: step 4526, loss 0.48813.
Train: 2018-08-02T11:06:19.548113: step 4527, loss 0.562476.
Train: 2018-08-02T11:06:19.766812: step 4528, loss 0.520976.
Train: 2018-08-02T11:06:19.985516: step 4529, loss 0.52085.
Train: 2018-08-02T11:06:20.172972: step 4530, loss 0.651433.
Test: 2018-08-02T11:06:21.344543: step 4530, loss 0.548856.
Train: 2018-08-02T11:06:21.563241: step 4531, loss 0.537366.
Train: 2018-08-02T11:06:21.781940: step 4532, loss 0.63765.
Train: 2018-08-02T11:06:22.000638: step 4533, loss 0.529005.
Train: 2018-08-02T11:06:22.234960: step 4534, loss 0.554066.
Train: 2018-08-02T11:06:22.438067: step 4535, loss 0.503958.
Train: 2018-08-02T11:06:22.656765: step 4536, loss 0.537328.
Train: 2018-08-02T11:06:22.875464: step 4537, loss 0.562403.
Train: 2018-08-02T11:06:23.094134: step 4538, loss 0.579163.
Train: 2018-08-02T11:06:23.328485: step 4539, loss 0.545619.
Train: 2018-08-02T11:06:23.547151: step 4540, loss 0.503635.
Test: 2018-08-02T11:06:24.703131: step 4540, loss 0.547951.
Train: 2018-08-02T11:06:24.921855: step 4541, loss 0.553973.
Train: 2018-08-02T11:06:25.140529: step 4542, loss 0.553952.
Train: 2018-08-02T11:06:25.359258: step 4543, loss 0.528628.
Train: 2018-08-02T11:06:25.577958: step 4544, loss 0.596167.
Train: 2018-08-02T11:06:25.796656: step 4545, loss 0.528526.
Train: 2018-08-02T11:06:26.015354: step 4546, loss 0.528479.
Train: 2018-08-02T11:06:26.234052: step 4547, loss 0.545383.
Train: 2018-08-02T11:06:26.452753: step 4548, loss 0.562346.
Train: 2018-08-02T11:06:26.671450: step 4549, loss 0.604888.
Train: 2018-08-02T11:06:26.890149: step 4550, loss 0.528306.
Test: 2018-08-02T11:06:28.061719: step 4550, loss 0.548319.
Train: 2018-08-02T11:06:28.264798: step 4551, loss 0.502743.
Train: 2018-08-02T11:06:28.483527: step 4552, loss 0.56234.
Train: 2018-08-02T11:06:28.702194: step 4553, loss 0.536725.
Train: 2018-08-02T11:06:28.920923: step 4554, loss 0.545237.
Train: 2018-08-02T11:06:29.139623: step 4555, loss 0.605142.
Train: 2018-08-02T11:06:29.358325: step 4556, loss 0.62227.
Train: 2018-08-02T11:06:29.577021: step 4557, loss 0.570886.
Train: 2018-08-02T11:06:29.795713: step 4558, loss 0.553804.
Train: 2018-08-02T11:06:30.014418: step 4559, loss 0.587904.
Train: 2018-08-02T11:06:30.233086: step 4560, loss 0.587852.
Test: 2018-08-02T11:06:31.389065: step 4560, loss 0.549363.
Train: 2018-08-02T11:06:31.607794: step 4561, loss 0.57083.
Train: 2018-08-02T11:06:31.826494: step 4562, loss 0.62154.
Train: 2018-08-02T11:06:32.045192: step 4563, loss 0.587635.
Train: 2018-08-02T11:06:32.263861: step 4564, loss 0.554014.
Train: 2018-08-02T11:06:32.482589: step 4565, loss 0.587468.
Train: 2018-08-02T11:06:32.701288: step 4566, loss 0.512561.
Train: 2018-08-02T11:06:32.919993: step 4567, loss 0.562467.
Train: 2018-08-02T11:06:33.138657: step 4568, loss 0.529411.
Train: 2018-08-02T11:06:33.357385: step 4569, loss 0.479948.
Train: 2018-08-02T11:06:33.576085: step 4570, loss 0.512951.
Test: 2018-08-02T11:06:34.747654: step 4570, loss 0.548867.
Train: 2018-08-02T11:06:34.950757: step 4571, loss 0.488064.
Train: 2018-08-02T11:06:35.169460: step 4572, loss 0.570758.
Train: 2018-08-02T11:06:35.388159: step 4573, loss 0.628943.
Train: 2018-08-02T11:06:35.606852: step 4574, loss 0.562445.
Train: 2018-08-02T11:06:35.825558: step 4575, loss 0.570761.
Train: 2018-08-02T11:06:36.044226: step 4576, loss 0.65394.
Train: 2018-08-02T11:06:36.262924: step 4577, loss 0.570759.
Train: 2018-08-02T11:06:36.481653: step 4578, loss 0.562476.
Train: 2018-08-02T11:06:36.700322: step 4579, loss 0.545965.
Train: 2018-08-02T11:06:36.919051: step 4580, loss 0.570756.
Test: 2018-08-02T11:06:38.090621: step 4580, loss 0.548954.
Train: 2018-08-02T11:06:38.293729: step 4581, loss 0.529574.
Train: 2018-08-02T11:06:38.512428: step 4582, loss 0.562528.
Train: 2018-08-02T11:06:38.731127: step 4583, loss 0.562535.
Train: 2018-08-02T11:06:38.949826: step 4584, loss 0.636498.
Train: 2018-08-02T11:06:39.168495: step 4585, loss 0.488789.
Train: 2018-08-02T11:06:39.387224: step 4586, loss 0.578957.
Train: 2018-08-02T11:06:39.605922: step 4587, loss 0.578951.
Train: 2018-08-02T11:06:39.824591: step 4588, loss 0.578944.
Train: 2018-08-02T11:06:40.043290: step 4589, loss 0.546276.
Train: 2018-08-02T11:06:40.262013: step 4590, loss 0.513673.
Test: 2018-08-02T11:06:41.433588: step 4590, loss 0.548234.
Train: 2018-08-02T11:06:41.636666: step 4591, loss 0.538134.
Train: 2018-08-02T11:06:41.855395: step 4592, loss 0.54627.
Train: 2018-08-02T11:06:42.074095: step 4593, loss 0.595295.
Train: 2018-08-02T11:06:42.308385: step 4594, loss 0.546233.
Train: 2018-08-02T11:06:42.511491: step 4595, loss 0.546218.
Train: 2018-08-02T11:06:42.730160: step 4596, loss 0.546194.
Train: 2018-08-02T11:06:42.948889: step 4597, loss 0.562563.
Train: 2018-08-02T11:06:43.167557: step 4598, loss 0.595384.
Train: 2018-08-02T11:06:43.386280: step 4599, loss 0.628222.
Train: 2018-08-02T11:06:43.604986: step 4600, loss 0.578959.
Test: 2018-08-02T11:06:44.760934: step 4600, loss 0.549294.
Train: 2018-08-02T11:06:45.588896: step 4601, loss 0.497122.
Train: 2018-08-02T11:06:45.807594: step 4602, loss 0.587131.
Train: 2018-08-02T11:06:46.026293: step 4603, loss 0.578944.
Train: 2018-08-02T11:06:46.244992: step 4604, loss 0.570769.
Train: 2018-08-02T11:06:46.463691: step 4605, loss 0.619735.
Train: 2018-08-02T11:06:46.682359: step 4606, loss 0.587062.
Train: 2018-08-02T11:06:46.901058: step 4607, loss 0.619507.
Train: 2018-08-02T11:06:47.119786: step 4608, loss 0.570802.
Train: 2018-08-02T11:06:47.338480: step 4609, loss 0.603064.
Train: 2018-08-02T11:06:47.557185: step 4610, loss 0.619011.
Test: 2018-08-02T11:06:48.713134: step 4610, loss 0.549544.
Train: 2018-08-02T11:06:48.931862: step 4611, loss 0.49898.
Train: 2018-08-02T11:06:49.150561: step 4612, loss 0.5231.
Train: 2018-08-02T11:06:49.369254: step 4613, loss 0.610675.
Train: 2018-08-02T11:06:49.587960: step 4614, loss 0.6106.
Train: 2018-08-02T11:06:49.806628: step 4615, loss 0.594681.
Train: 2018-08-02T11:06:50.025357: step 4616, loss 0.468507.
Train: 2018-08-02T11:06:50.244056: step 4617, loss 0.515841.
Train: 2018-08-02T11:06:50.462754: step 4618, loss 0.55521.
Train: 2018-08-02T11:06:50.681452: step 4619, loss 0.5315.
Train: 2018-08-02T11:06:50.900152: step 4620, loss 0.539313.
Test: 2018-08-02T11:06:52.056102: step 4620, loss 0.551217.
Train: 2018-08-02T11:06:52.337310: step 4621, loss 0.570929.
Train: 2018-08-02T11:06:52.555985: step 4622, loss 0.539122.
Train: 2018-08-02T11:06:52.774713: step 4623, loss 0.475262.
Train: 2018-08-02T11:06:52.993411: step 4624, loss 0.634911.
Train: 2018-08-02T11:06:53.212111: step 4625, loss 0.627033.
Train: 2018-08-02T11:06:53.430809: step 4626, loss 0.530667.
Train: 2018-08-02T11:06:53.649503: step 4627, loss 0.546688.
Train: 2018-08-02T11:06:53.868177: step 4628, loss 0.53857.
Train: 2018-08-02T11:06:54.086875: step 4629, loss 0.546564.
Train: 2018-08-02T11:06:54.305604: step 4630, loss 0.603202.
Test: 2018-08-02T11:06:55.477175: step 4630, loss 0.548943.
Train: 2018-08-02T11:06:55.680282: step 4631, loss 0.611353.
Train: 2018-08-02T11:06:55.898984: step 4632, loss 0.611356.
Train: 2018-08-02T11:06:56.117649: step 4633, loss 0.554585.
Train: 2018-08-02T11:06:56.336379: step 4634, loss 0.570797.
Train: 2018-08-02T11:06:56.555072: step 4635, loss 0.595077.
Train: 2018-08-02T11:06:56.773776: step 4636, loss 0.562727.
Train: 2018-08-02T11:06:56.992445: step 4637, loss 0.506252.
Train: 2018-08-02T11:06:57.211175: step 4638, loss 0.63539.
Train: 2018-08-02T11:06:57.429867: step 4639, loss 0.595001.
Train: 2018-08-02T11:06:57.648571: step 4640, loss 0.554741.
Test: 2018-08-02T11:06:58.804521: step 4640, loss 0.548839.
Train: 2018-08-02T11:06:59.023250: step 4641, loss 0.522648.
Train: 2018-08-02T11:06:59.241948: step 4642, loss 0.619014.
Train: 2018-08-02T11:06:59.460648: step 4643, loss 0.49069.
Train: 2018-08-02T11:06:59.679316: step 4644, loss 0.554809.
Train: 2018-08-02T11:06:59.898044: step 4645, loss 0.4906.
Train: 2018-08-02T11:07:00.116714: step 4646, loss 0.578874.
Train: 2018-08-02T11:07:00.335413: step 4647, loss 0.611114.
Train: 2018-08-02T11:07:00.554141: step 4648, loss 0.530502.
Train: 2018-08-02T11:07:00.772840: step 4649, loss 0.538516.
Train: 2018-08-02T11:07:00.991508: step 4650, loss 0.562714.
Test: 2018-08-02T11:07:02.163109: step 4650, loss 0.548229.
Train: 2018-08-02T11:07:02.366186: step 4651, loss 0.578898.
Train: 2018-08-02T11:07:02.584916: step 4652, loss 0.61135.
Train: 2018-08-02T11:07:02.803617: step 4653, loss 0.522125.
Train: 2018-08-02T11:07:03.022314: step 4654, loss 0.530195.
Train: 2018-08-02T11:07:03.241007: step 4655, loss 0.635835.
Train: 2018-08-02T11:07:03.459680: step 4656, loss 0.595172.
Train: 2018-08-02T11:07:03.678409: step 4657, loss 0.538301.
Train: 2018-08-02T11:07:03.897108: step 4658, loss 0.54643.
Train: 2018-08-02T11:07:04.115808: step 4659, loss 0.65199.
Train: 2018-08-02T11:07:04.334475: step 4660, loss 0.643734.
Test: 2018-08-02T11:07:05.506076: step 4660, loss 0.548507.
Train: 2018-08-02T11:07:05.709155: step 4661, loss 0.514287.
Train: 2018-08-02T11:07:05.927883: step 4662, loss 0.594995.
Train: 2018-08-02T11:07:06.146583: step 4663, loss 0.490465.
Train: 2018-08-02T11:07:06.365282: step 4664, loss 0.586904.
Train: 2018-08-02T11:07:06.583949: step 4665, loss 0.562819.
Train: 2018-08-02T11:07:06.802677: step 4666, loss 0.578867.
Train: 2018-08-02T11:07:07.021376: step 4667, loss 0.538813.
Train: 2018-08-02T11:07:07.240046: step 4668, loss 0.586873.
Train: 2018-08-02T11:07:07.458745: step 4669, loss 0.594869.
Train: 2018-08-02T11:07:07.677444: step 4670, loss 0.546893.
Test: 2018-08-02T11:07:08.849043: step 4670, loss 0.550836.
Train: 2018-08-02T11:07:09.052151: step 4671, loss 0.578861.
Train: 2018-08-02T11:07:09.270850: step 4672, loss 0.515024.
Train: 2018-08-02T11:07:09.489544: step 4673, loss 0.491045.
Train: 2018-08-02T11:07:09.708241: step 4674, loss 0.538856.
Train: 2018-08-02T11:07:09.926941: step 4675, loss 0.522705.
Train: 2018-08-02T11:07:10.145615: step 4676, loss 0.49031.
Train: 2018-08-02T11:07:10.364346: step 4677, loss 0.603167.
Train: 2018-08-02T11:07:10.583043: step 4678, loss 0.587027.
Train: 2018-08-02T11:07:10.801737: step 4679, loss 0.505653.
Train: 2018-08-02T11:07:11.020435: step 4680, loss 0.58711.
Test: 2018-08-02T11:07:12.192011: step 4680, loss 0.547945.
Train: 2018-08-02T11:07:12.379466: step 4681, loss 0.492659.
Train: 2018-08-02T11:07:12.598165: step 4682, loss 0.537855.
Train: 2018-08-02T11:07:12.816895: step 4683, loss 0.570756.
Train: 2018-08-02T11:07:13.035564: step 4684, loss 0.554181.
Train: 2018-08-02T11:07:13.254262: step 4685, loss 0.529187.
Train: 2018-08-02T11:07:13.472960: step 4686, loss 0.478983.
Train: 2018-08-02T11:07:13.691661: step 4687, loss 0.537241.
Train: 2018-08-02T11:07:13.910394: step 4688, loss 0.553948.
Train: 2018-08-02T11:07:14.129056: step 4689, loss 0.469281.
Train: 2018-08-02T11:07:14.347786: step 4690, loss 0.562343.
Test: 2018-08-02T11:07:15.519357: step 4690, loss 0.548055.
Train: 2018-08-02T11:07:15.722460: step 4691, loss 0.579439.
Train: 2018-08-02T11:07:15.941133: step 4692, loss 0.528.
Train: 2018-08-02T11:07:16.159831: step 4693, loss 0.570953.
Train: 2018-08-02T11:07:16.378560: step 4694, loss 0.545055.
Train: 2018-08-02T11:07:16.597259: step 4695, loss 0.510346.
Train: 2018-08-02T11:07:16.815960: step 4696, loss 0.597134.
Train: 2018-08-02T11:07:17.034656: step 4697, loss 0.536226.
Train: 2018-08-02T11:07:17.253350: step 4698, loss 0.59728.
Train: 2018-08-02T11:07:17.472049: step 4699, loss 0.544902.
Train: 2018-08-02T11:07:17.690754: step 4700, loss 0.536154.
Test: 2018-08-02T11:07:18.862324: step 4700, loss 0.548574.
Train: 2018-08-02T11:07:19.659042: step 4701, loss 0.544884.
Train: 2018-08-02T11:07:19.877742: step 4702, loss 0.606139.
Train: 2018-08-02T11:07:20.096440: step 4703, loss 0.553629.
Train: 2018-08-02T11:07:20.315139: step 4704, loss 0.606075.
Train: 2018-08-02T11:07:20.533838: step 4705, loss 0.553641.
Train: 2018-08-02T11:07:20.752537: step 4706, loss 0.571067.
Train: 2018-08-02T11:07:20.971205: step 4707, loss 0.518907.
Train: 2018-08-02T11:07:21.189903: step 4708, loss 0.640442.
Train: 2018-08-02T11:07:21.408632: step 4709, loss 0.562342.
Train: 2018-08-02T11:07:21.627310: step 4710, loss 0.57096.
Test: 2018-08-02T11:07:22.798902: step 4710, loss 0.549367.
Train: 2018-08-02T11:07:23.002009: step 4711, loss 0.588117.
Train: 2018-08-02T11:07:23.220703: step 4712, loss 0.647952.
Train: 2018-08-02T11:07:23.439377: step 4713, loss 0.536798.
Train: 2018-08-02T11:07:23.658106: step 4714, loss 0.562353.
Train: 2018-08-02T11:07:23.876804: step 4715, loss 0.511736.
Train: 2018-08-02T11:07:24.095475: step 4716, loss 0.495076.
Train: 2018-08-02T11:07:24.314173: step 4717, loss 0.545584.
Train: 2018-08-02T11:07:24.532901: step 4718, loss 0.579176.
Train: 2018-08-02T11:07:24.751600: step 4719, loss 0.66296.
Train: 2018-08-02T11:07:24.970298: step 4720, loss 0.512323.
Test: 2018-08-02T11:07:26.141869: step 4720, loss 0.548714.
Train: 2018-08-02T11:07:26.344947: step 4721, loss 0.495794.
Train: 2018-08-02T11:07:26.563676: step 4722, loss 0.54579.
Train: 2018-08-02T11:07:26.782344: step 4723, loss 0.604046.
Train: 2018-08-02T11:07:27.001073: step 4724, loss 0.587379.
Train: 2018-08-02T11:07:27.219772: step 4725, loss 0.55417.
Train: 2018-08-02T11:07:27.438471: step 4726, loss 0.579037.
Train: 2018-08-02T11:07:27.657169: step 4727, loss 0.570756.
Train: 2018-08-02T11:07:27.875868: step 4728, loss 0.579005.
Train: 2018-08-02T11:07:28.094568: step 4729, loss 0.578988.
Train: 2018-08-02T11:07:28.313260: step 4730, loss 0.537919.
Test: 2018-08-02T11:07:29.484836: step 4730, loss 0.549066.
Train: 2018-08-02T11:07:29.703560: step 4731, loss 0.496993.
Train: 2018-08-02T11:07:29.922264: step 4732, loss 0.50519.
Train: 2018-08-02T11:07:30.140963: step 4733, loss 0.505102.
Train: 2018-08-02T11:07:30.359662: step 4734, loss 0.587213.
Train: 2018-08-02T11:07:30.578361: step 4735, loss 0.570757.
Train: 2018-08-02T11:07:30.797059: step 4736, loss 0.587257.
Train: 2018-08-02T11:07:31.015757: step 4737, loss 0.587264.
Train: 2018-08-02T11:07:31.234457: step 4738, loss 0.587261.
Train: 2018-08-02T11:07:31.453156: step 4739, loss 0.52953.
Train: 2018-08-02T11:07:31.671825: step 4740, loss 0.562512.
Test: 2018-08-02T11:07:32.843424: step 4740, loss 0.548365.
Train: 2018-08-02T11:07:33.046532: step 4741, loss 0.587245.
Train: 2018-08-02T11:07:33.265200: step 4742, loss 0.554281.
Train: 2018-08-02T11:07:33.483930: step 4743, loss 0.578991.
Train: 2018-08-02T11:07:33.702629: step 4744, loss 0.587211.
Train: 2018-08-02T11:07:33.921297: step 4745, loss 0.546114.
Train: 2018-08-02T11:07:34.140026: step 4746, loss 0.595383.
Train: 2018-08-02T11:07:34.358725: step 4747, loss 0.546181.
Train: 2018-08-02T11:07:34.577394: step 4748, loss 0.546211.
Train: 2018-08-02T11:07:34.796123: step 4749, loss 0.488971.
Train: 2018-08-02T11:07:35.014821: step 4750, loss 0.578953.
Test: 2018-08-02T11:07:36.186391: step 4750, loss 0.547754.
Train: 2018-08-02T11:07:36.389470: step 4751, loss 0.669086.
Train: 2018-08-02T11:07:36.608199: step 4752, loss 0.5953.
Train: 2018-08-02T11:07:36.826898: step 4753, loss 0.595244.
Train: 2018-08-02T11:07:37.045595: step 4754, loss 0.570782.
Train: 2018-08-02T11:07:37.264265: step 4755, loss 0.473496.
Train: 2018-08-02T11:07:37.482993: step 4756, loss 0.595107.
Train: 2018-08-02T11:07:37.701686: step 4757, loss 0.578894.
Train: 2018-08-02T11:07:37.920361: step 4758, loss 0.538475.
Train: 2018-08-02T11:07:38.139060: step 4759, loss 0.522344.
Train: 2018-08-02T11:07:38.357783: step 4760, loss 0.59505.
Test: 2018-08-02T11:07:39.529359: step 4760, loss 0.549054.
Train: 2018-08-02T11:07:39.732467: step 4761, loss 0.546575.
Train: 2018-08-02T11:07:39.951162: step 4762, loss 0.498097.
Train: 2018-08-02T11:07:40.169864: step 4763, loss 0.595079.
Train: 2018-08-02T11:07:40.388563: step 4764, loss 0.530301.
Train: 2018-08-02T11:07:40.607231: step 4765, loss 0.570791.
Train: 2018-08-02T11:07:40.825961: step 4766, loss 0.530181.
Train: 2018-08-02T11:07:41.044659: step 4767, loss 0.521962.
Train: 2018-08-02T11:07:41.263358: step 4768, loss 0.562615.
Train: 2018-08-02T11:07:41.482057: step 4769, loss 0.603476.
Train: 2018-08-02T11:07:41.700725: step 4770, loss 0.529831.
Test: 2018-08-02T11:07:42.856705: step 4770, loss 0.549052.
Train: 2018-08-02T11:07:43.075405: step 4771, loss 0.513352.
Train: 2018-08-02T11:07:43.294132: step 4772, loss 0.578982.
Train: 2018-08-02T11:07:43.512801: step 4773, loss 0.570757.
Train: 2018-08-02T11:07:43.731500: step 4774, loss 0.545997.
Train: 2018-08-02T11:07:43.950198: step 4775, loss 0.488083.
Train: 2018-08-02T11:07:44.168898: step 4776, loss 0.52929.
Train: 2018-08-02T11:07:44.387626: step 4777, loss 0.545794.
Train: 2018-08-02T11:07:44.606325: step 4778, loss 0.604173.
Train: 2018-08-02T11:07:44.825018: step 4779, loss 0.604246.
Train: 2018-08-02T11:07:45.043717: step 4780, loss 0.629395.
Test: 2018-08-02T11:07:46.199672: step 4780, loss 0.548435.
Train: 2018-08-02T11:07:46.418401: step 4781, loss 0.537309.
Train: 2018-08-02T11:07:46.637069: step 4782, loss 0.478784.
Train: 2018-08-02T11:07:46.855798: step 4783, loss 0.654515.
Train: 2018-08-02T11:07:47.074468: step 4784, loss 0.545676.
Train: 2018-08-02T11:07:47.293200: step 4785, loss 0.612574.
Train: 2018-08-02T11:07:47.511895: step 4786, loss 0.504004.
Train: 2018-08-02T11:07:47.730564: step 4787, loss 0.570767.
Train: 2018-08-02T11:07:47.949293: step 4788, loss 0.562429.
Train: 2018-08-02T11:07:48.167991: step 4789, loss 0.595752.
Train: 2018-08-02T11:07:48.386660: step 4790, loss 0.579078.
Test: 2018-08-02T11:07:49.558260: step 4790, loss 0.548593.
Train: 2018-08-02T11:07:49.761363: step 4791, loss 0.562457.
Train: 2018-08-02T11:07:49.980068: step 4792, loss 0.595618.
Train: 2018-08-02T11:07:50.198735: step 4793, loss 0.529422.
Train: 2018-08-02T11:07:50.417464: step 4794, loss 0.52123.
Train: 2018-08-02T11:07:50.636164: step 4795, loss 0.653262.
Train: 2018-08-02T11:07:50.854862: step 4796, loss 0.504922.
Train: 2018-08-02T11:07:51.073530: step 4797, loss 0.620084.
Train: 2018-08-02T11:07:51.292260: step 4798, loss 0.554357.
Train: 2018-08-02T11:07:51.510958: step 4799, loss 0.619884.
Train: 2018-08-02T11:07:51.729652: step 4800, loss 0.56261.
Test: 2018-08-02T11:07:52.885606: step 4800, loss 0.548861.
Train: 2018-08-02T11:07:53.729189: step 4801, loss 0.562641.
Train: 2018-08-02T11:07:53.947888: step 4802, loss 0.595143.
Train: 2018-08-02T11:07:54.166586: step 4803, loss 0.554612.
Train: 2018-08-02T11:07:54.385255: step 4804, loss 0.578885.
Train: 2018-08-02T11:07:54.603985: step 4805, loss 0.538611.
Train: 2018-08-02T11:07:54.822683: step 4806, loss 0.546713.
Train: 2018-08-02T11:07:55.041382: step 4807, loss 0.57887.
Train: 2018-08-02T11:07:55.260080: step 4808, loss 0.594911.
Train: 2018-08-02T11:07:55.478779: step 4809, loss 0.602885.
Train: 2018-08-02T11:07:55.713086: step 4810, loss 0.483022.
Test: 2018-08-02T11:07:56.869049: step 4810, loss 0.549372.
Train: 2018-08-02T11:07:57.087777: step 4811, loss 0.64274.
Train: 2018-08-02T11:07:57.306470: step 4812, loss 0.594797.
Train: 2018-08-02T11:07:57.525169: step 4813, loss 0.57091.
Train: 2018-08-02T11:07:57.743869: step 4814, loss 0.499553.
Train: 2018-08-02T11:07:57.962567: step 4815, loss 0.563004.
Train: 2018-08-02T11:07:58.181266: step 4816, loss 0.602638.
Train: 2018-08-02T11:07:58.399970: step 4817, loss 0.515513.
Train: 2018-08-02T11:07:58.618669: step 4818, loss 0.563016.
Train: 2018-08-02T11:07:58.837362: step 4819, loss 0.586785.
Train: 2018-08-02T11:07:59.056068: step 4820, loss 0.499595.
Test: 2018-08-02T11:08:00.227637: step 4820, loss 0.550631.
Train: 2018-08-02T11:08:00.430745: step 4821, loss 0.570918.
Train: 2018-08-02T11:08:00.649443: step 4822, loss 0.547049.
Train: 2018-08-02T11:08:00.868142: step 4823, loss 0.578859.
Train: 2018-08-02T11:08:01.086810: step 4824, loss 0.554924.
Train: 2018-08-02T11:08:01.305510: step 4825, loss 0.530912.
Train: 2018-08-02T11:08:01.524238: step 4826, loss 0.578865.
Train: 2018-08-02T11:08:01.742937: step 4827, loss 0.610967.
Train: 2018-08-02T11:08:01.961636: step 4828, loss 0.554783.
Train: 2018-08-02T11:08:02.180336: step 4829, loss 0.554767.
Train: 2018-08-02T11:08:02.399029: step 4830, loss 0.619085.
Test: 2018-08-02T11:08:03.570604: step 4830, loss 0.548629.
Train: 2018-08-02T11:08:03.773705: step 4831, loss 0.546717.
Train: 2018-08-02T11:08:03.961162: step 4832, loss 0.648547.
Train: 2018-08-02T11:08:04.179867: step 4833, loss 0.506653.
Train: 2018-08-02T11:08:04.398536: step 4834, loss 0.514696.
Train: 2018-08-02T11:08:04.615124: step 4835, loss 0.562811.
Train: 2018-08-02T11:08:04.833823: step 4836, loss 0.554761.
Train: 2018-08-02T11:08:05.052552: step 4837, loss 0.562782.
Train: 2018-08-02T11:08:05.271250: step 4838, loss 0.546658.
Train: 2018-08-02T11:08:05.489953: step 4839, loss 0.482084.
Train: 2018-08-02T11:08:05.708643: step 4840, loss 0.619358.
Test: 2018-08-02T11:08:06.880218: step 4840, loss 0.548777.
Train: 2018-08-02T11:08:07.083296: step 4841, loss 0.595112.
Train: 2018-08-02T11:08:07.302026: step 4842, loss 0.603238.
Train: 2018-08-02T11:08:07.520694: step 4843, loss 0.522135.
Train: 2018-08-02T11:08:07.739392: step 4844, loss 0.643829.
Train: 2018-08-02T11:08:07.958121: step 4845, loss 0.619427.
Train: 2018-08-02T11:08:08.176820: step 4846, loss 0.586976.
Train: 2018-08-02T11:08:08.395519: step 4847, loss 0.538561.
Train: 2018-08-02T11:08:08.614222: step 4848, loss 0.570826.
Train: 2018-08-02T11:08:08.832887: step 4849, loss 0.562801.
Train: 2018-08-02T11:08:09.051585: step 4850, loss 0.578868.
Test: 2018-08-02T11:08:10.207565: step 4850, loss 0.548731.
Train: 2018-08-02T11:08:10.426293: step 4851, loss 0.506777.
Train: 2018-08-02T11:08:10.644992: step 4852, loss 0.458724.
Train: 2018-08-02T11:08:10.863694: step 4853, loss 0.506601.
Train: 2018-08-02T11:08:11.082360: step 4854, loss 0.506341.
Train: 2018-08-02T11:08:11.301084: step 4855, loss 0.530308.
Train: 2018-08-02T11:08:11.519787: step 4856, loss 0.48939.
Train: 2018-08-02T11:08:11.738456: step 4857, loss 0.636281.
Train: 2018-08-02T11:08:11.957186: step 4858, loss 0.595424.
Train: 2018-08-02T11:08:12.175878: step 4859, loss 0.554269.
Train: 2018-08-02T11:08:12.394583: step 4860, loss 0.562494.
Test: 2018-08-02T11:08:13.566153: step 4860, loss 0.549597.
Train: 2018-08-02T11:08:13.784876: step 4861, loss 0.620449.
Train: 2018-08-02T11:08:14.019177: step 4862, loss 0.529325.
Train: 2018-08-02T11:08:14.222249: step 4863, loss 0.645426.
Train: 2018-08-02T11:08:14.440978: step 4864, loss 0.628781.
Train: 2018-08-02T11:08:14.659677: step 4865, loss 0.587296.
Train: 2018-08-02T11:08:14.878376: step 4866, loss 0.570757.
Train: 2018-08-02T11:08:15.097074: step 4867, loss 0.587207.
Train: 2018-08-02T11:08:15.315774: step 4868, loss 0.611755.
Train: 2018-08-02T11:08:15.534472: step 4869, loss 0.529933.
Train: 2018-08-02T11:08:15.753141: step 4870, loss 0.546327.
Test: 2018-08-02T11:08:16.924741: step 4870, loss 0.549823.
Train: 2018-08-02T11:08:17.127843: step 4871, loss 0.538249.
Train: 2018-08-02T11:08:17.346542: step 4872, loss 0.627779.
Train: 2018-08-02T11:08:17.565246: step 4873, loss 0.578881.
Train: 2018-08-02T11:08:17.783945: step 4874, loss 0.522387.
Train: 2018-08-02T11:08:18.002644: step 4875, loss 0.554704.
Train: 2018-08-02T11:08:18.221343: step 4876, loss 0.514481.
Train: 2018-08-02T11:08:18.440041: step 4877, loss 0.586927.
Train: 2018-08-02T11:08:18.658711: step 4878, loss 0.538634.
Train: 2018-08-02T11:08:18.877434: step 4879, loss 0.522518.
Train: 2018-08-02T11:08:19.096138: step 4880, loss 0.546632.
Test: 2018-08-02T11:08:20.267708: step 4880, loss 0.548505.
Train: 2018-08-02T11:08:20.470786: step 4881, loss 0.538509.
Train: 2018-08-02T11:08:20.689514: step 4882, loss 0.514155.
Train: 2018-08-02T11:08:20.908183: step 4883, loss 0.635726.
Train: 2018-08-02T11:08:21.126912: step 4884, loss 0.578911.
Train: 2018-08-02T11:08:21.345580: step 4885, loss 0.53826.
Train: 2018-08-02T11:08:21.564305: step 4886, loss 0.530085.
Train: 2018-08-02T11:08:21.782978: step 4887, loss 0.56262.
Train: 2018-08-02T11:08:22.001702: step 4888, loss 0.619774.
Train: 2018-08-02T11:08:22.220408: step 4889, loss 0.546265.
Train: 2018-08-02T11:08:22.439075: step 4890, loss 0.603456.
Test: 2018-08-02T11:08:23.610676: step 4890, loss 0.549714.
Train: 2018-08-02T11:08:23.813783: step 4891, loss 0.497263.
Train: 2018-08-02T11:08:24.079346: step 4892, loss 0.538064.
Train: 2018-08-02T11:08:24.298014: step 4893, loss 0.538013.
Train: 2018-08-02T11:08:24.516744: step 4894, loss 0.628183.
Train: 2018-08-02T11:08:24.735446: step 4895, loss 0.529741.
Train: 2018-08-02T11:08:24.954112: step 4896, loss 0.521494.
Train: 2018-08-02T11:08:25.172810: step 4897, loss 0.578983.
Train: 2018-08-02T11:08:25.391508: step 4898, loss 0.562524.
Train: 2018-08-02T11:08:25.610242: step 4899, loss 0.546034.
Train: 2018-08-02T11:08:25.828936: step 4900, loss 0.587258.
Test: 2018-08-02T11:08:27.000506: step 4900, loss 0.548908.
Train: 2018-08-02T11:08:27.828438: step 4901, loss 0.587264.
Train: 2018-08-02T11:08:28.047137: step 4902, loss 0.529497.
Train: 2018-08-02T11:08:28.265835: step 4903, loss 0.570756.
Train: 2018-08-02T11:08:28.484568: step 4904, loss 0.545985.
Train: 2018-08-02T11:08:28.703234: step 4905, loss 0.595538.
Train: 2018-08-02T11:08:28.921956: step 4906, loss 0.620301.
Train: 2018-08-02T11:08:29.140661: step 4907, loss 0.570757.
Train: 2018-08-02T11:08:29.359364: step 4908, loss 0.537845.
Train: 2018-08-02T11:08:29.578053: step 4909, loss 0.56254.
Train: 2018-08-02T11:08:29.796753: step 4910, loss 0.56255.
Test: 2018-08-02T11:08:30.968327: step 4910, loss 0.547917.
Train: 2018-08-02T11:08:31.171406: step 4911, loss 0.554357.
Train: 2018-08-02T11:08:31.390135: step 4912, loss 0.488799.
Train: 2018-08-02T11:08:31.608836: step 4913, loss 0.546147.
Train: 2018-08-02T11:08:31.827531: step 4914, loss 0.570759.
Train: 2018-08-02T11:08:32.046231: step 4915, loss 0.578981.
Train: 2018-08-02T11:08:32.264929: step 4916, loss 0.603662.
Train: 2018-08-02T11:08:32.483622: step 4917, loss 0.620083.
Train: 2018-08-02T11:08:32.702321: step 4918, loss 0.529737.
Train: 2018-08-02T11:08:32.921025: step 4919, loss 0.537976.
Train: 2018-08-02T11:08:33.139695: step 4920, loss 0.603538.
Test: 2018-08-02T11:08:34.311294: step 4920, loss 0.549105.
Train: 2018-08-02T11:08:34.514396: step 4921, loss 0.521669.
Train: 2018-08-02T11:08:34.733071: step 4922, loss 0.619851.
Train: 2018-08-02T11:08:34.951794: step 4923, loss 0.644281.
Train: 2018-08-02T11:08:35.170469: step 4924, loss 0.538215.
Train: 2018-08-02T11:08:35.389197: step 4925, loss 0.513938.
Train: 2018-08-02T11:08:35.607900: step 4926, loss 0.505887.
Train: 2018-08-02T11:08:35.826565: step 4927, loss 0.627605.
Train: 2018-08-02T11:08:36.045293: step 4928, loss 0.595116.
Train: 2018-08-02T11:08:36.263993: step 4929, loss 0.546519.
Train: 2018-08-02T11:08:36.482686: step 4930, loss 0.570805.
Test: 2018-08-02T11:08:37.638640: step 4930, loss 0.548506.
Train: 2018-08-02T11:08:37.857364: step 4931, loss 0.498134.
Train: 2018-08-02T11:08:38.076068: step 4932, loss 0.562729.
Train: 2018-08-02T11:08:38.294783: step 4933, loss 0.546555.
Train: 2018-08-02T11:08:38.513436: step 4934, loss 0.530347.
Train: 2018-08-02T11:08:38.747756: step 4935, loss 0.522171.
Train: 2018-08-02T11:08:38.966484: step 4936, loss 0.546414.
Train: 2018-08-02T11:08:39.185183: step 4937, loss 0.538198.
Train: 2018-08-02T11:08:39.403882: step 4938, loss 0.652453.
Train: 2018-08-02T11:08:39.622576: step 4939, loss 0.554423.
Train: 2018-08-02T11:08:39.856871: step 4940, loss 0.497168.
Test: 2018-08-02T11:08:41.012851: step 4940, loss 0.549826.
Train: 2018-08-02T11:08:41.215956: step 4941, loss 0.578957.
Train: 2018-08-02T11:08:41.434627: step 4942, loss 0.603585.
Train: 2018-08-02T11:08:41.668947: step 4943, loss 0.505089.
Train: 2018-08-02T11:08:41.887645: step 4944, loss 0.554311.
Train: 2018-08-02T11:08:42.106370: step 4945, loss 0.611932.
Train: 2018-08-02T11:08:42.325042: step 4946, loss 0.653116.
Train: 2018-08-02T11:08:42.543772: step 4947, loss 0.455681.
Train: 2018-08-02T11:08:42.762471: step 4948, loss 0.587209.
Train: 2018-08-02T11:08:42.981174: step 4949, loss 0.587206.
Train: 2018-08-02T11:08:43.199868: step 4950, loss 0.61186.
Test: 2018-08-02T11:08:44.371438: step 4950, loss 0.549226.
Train: 2018-08-02T11:08:44.574547: step 4951, loss 0.652801.
Train: 2018-08-02T11:08:44.793246: step 4952, loss 0.513552.
Train: 2018-08-02T11:08:45.011915: step 4953, loss 0.530059.
Train: 2018-08-02T11:08:45.230647: step 4954, loss 0.521917.
Train: 2018-08-02T11:08:45.449342: step 4955, loss 0.595223.
Train: 2018-08-02T11:08:45.668035: step 4956, loss 0.48121.
Train: 2018-08-02T11:08:45.902331: step 4957, loss 0.562623.
Train: 2018-08-02T11:08:46.121059: step 4958, loss 0.587092.
Train: 2018-08-02T11:08:46.339752: step 4959, loss 0.50547.
Train: 2018-08-02T11:08:46.558457: step 4960, loss 0.538064.
Test: 2018-08-02T11:08:47.730027: step 4960, loss 0.548701.
Train: 2018-08-02T11:08:47.933135: step 4961, loss 0.628109.
Train: 2018-08-02T11:08:48.151828: step 4962, loss 0.537986.
Train: 2018-08-02T11:08:48.370502: step 4963, loss 0.562561.
Train: 2018-08-02T11:08:48.589232: step 4964, loss 0.59538.
Train: 2018-08-02T11:08:48.807924: step 4965, loss 0.570761.
Train: 2018-08-02T11:08:49.026631: step 4966, loss 0.546156.
Train: 2018-08-02T11:08:49.245328: step 4967, loss 0.619972.
Train: 2018-08-02T11:08:49.464026: step 4968, loss 0.63629.
Train: 2018-08-02T11:08:49.682725: step 4969, loss 0.521768.
Train: 2018-08-02T11:08:49.901394: step 4970, loss 0.546315.
Test: 2018-08-02T11:08:51.072994: step 4970, loss 0.549217.
Train: 2018-08-02T11:08:51.276096: step 4971, loss 0.546352.
Train: 2018-08-02T11:08:51.494801: step 4972, loss 0.521945.
Train: 2018-08-02T11:08:51.713470: step 4973, loss 0.635901.
Train: 2018-08-02T11:08:51.932199: step 4974, loss 0.546385.
Train: 2018-08-02T11:08:52.150892: step 4975, loss 0.619547.
Train: 2018-08-02T11:08:52.369565: step 4976, loss 0.546474.
Train: 2018-08-02T11:08:52.588295: step 4977, loss 0.522203.
Train: 2018-08-02T11:08:52.806993: step 4978, loss 0.586997.
Train: 2018-08-02T11:08:53.041314: step 4979, loss 0.570798.
Train: 2018-08-02T11:08:53.244392: step 4980, loss 0.530397.
Test: 2018-08-02T11:08:54.400341: step 4980, loss 0.548855.
Train: 2018-08-02T11:08:54.634692: step 4981, loss 0.55466.
Train: 2018-08-02T11:08:54.853394: step 4982, loss 0.562729.
Train: 2018-08-02T11:08:55.040848: step 4983, loss 0.476484.
Train: 2018-08-02T11:08:55.259514: step 4984, loss 0.514038.
Train: 2018-08-02T11:08:55.493866: step 4985, loss 0.546367.
Train: 2018-08-02T11:08:55.712564: step 4986, loss 0.603419.
Train: 2018-08-02T11:08:55.931257: step 4987, loss 0.57899.
Train: 2018-08-02T11:08:56.149931: step 4988, loss 0.587194.
Train: 2018-08-02T11:08:56.368659: step 4989, loss 0.587135.
Train: 2018-08-02T11:08:56.587386: step 4990, loss 0.587153.
Test: 2018-08-02T11:08:57.758929: step 4990, loss 0.549283.
Train: 2018-08-02T11:08:57.962006: step 4991, loss 0.603544.
Train: 2018-08-02T11:08:58.180735: step 4992, loss 0.562599.
Train: 2018-08-02T11:08:58.399434: step 4993, loss 0.521815.
Train: 2018-08-02T11:08:58.618133: step 4994, loss 0.554458.
Train: 2018-08-02T11:08:58.836831: step 4995, loss 0.497362.
Train: 2018-08-02T11:08:59.055530: step 4996, loss 0.644288.
Train: 2018-08-02T11:08:59.274199: step 4997, loss 0.570771.
Train: 2018-08-02T11:08:59.492927: step 4998, loss 0.652353.
Train: 2018-08-02T11:08:59.711621: step 4999, loss 0.587052.
Train: 2018-08-02T11:08:59.930326: step 5000, loss 0.554568.
Test: 2018-08-02T11:09:01.101895: step 5000, loss 0.548452.
Train: 2018-08-02T11:09:01.898585: step 5001, loss 0.595075.
Train: 2018-08-02T11:09:02.117314: step 5002, loss 0.554681.
Train: 2018-08-02T11:09:02.336012: step 5003, loss 0.586922.
Train: 2018-08-02T11:09:02.554705: step 5004, loss 0.506639.
Train: 2018-08-02T11:09:02.773410: step 5005, loss 0.562832.
Train: 2018-08-02T11:09:02.992109: step 5006, loss 0.554833.
Train: 2018-08-02T11:09:03.210808: step 5007, loss 0.610892.
Train: 2018-08-02T11:09:03.429506: step 5008, loss 0.578861.
Train: 2018-08-02T11:09:03.648205: step 5009, loss 0.522996.
Train: 2018-08-02T11:09:03.866904: step 5010, loss 0.515031.
Test: 2018-08-02T11:09:05.038474: step 5010, loss 0.549559.
Train: 2018-08-02T11:09:05.241576: step 5011, loss 0.514999.
Train: 2018-08-02T11:09:05.460281: step 5012, loss 0.522848.
Train: 2018-08-02T11:09:05.678979: step 5013, loss 0.538739.
Train: 2018-08-02T11:09:05.897678: step 5014, loss 0.586931.
Train: 2018-08-02T11:09:06.116372: step 5015, loss 0.522371.
Train: 2018-08-02T11:09:06.335075: step 5016, loss 0.554601.
Train: 2018-08-02T11:09:06.553774: step 5017, loss 0.497666.
Train: 2018-08-02T11:09:06.772444: step 5018, loss 0.603409.
Train: 2018-08-02T11:09:06.991143: step 5019, loss 0.595321.
Train: 2018-08-02T11:09:07.209870: step 5020, loss 0.570767.
Test: 2018-08-02T11:09:08.381442: step 5020, loss 0.549029.
Train: 2018-08-02T11:09:08.600165: step 5021, loss 0.587179.
Train: 2018-08-02T11:09:08.818869: step 5022, loss 0.537898.
Train: 2018-08-02T11:09:09.037538: step 5023, loss 0.562533.
Train: 2018-08-02T11:09:09.256237: step 5024, loss 0.570757.
Train: 2018-08-02T11:09:09.474935: step 5025, loss 0.50485.
Train: 2018-08-02T11:09:09.693664: step 5026, loss 0.537742.
Train: 2018-08-02T11:09:09.912363: step 5027, loss 0.636928.
Train: 2018-08-02T11:09:10.131031: step 5028, loss 0.562483.
Train: 2018-08-02T11:09:10.349764: step 5029, loss 0.504566.
Train: 2018-08-02T11:09:10.568429: step 5030, loss 0.595611.
Test: 2018-08-02T11:09:11.740030: step 5030, loss 0.547484.
Train: 2018-08-02T11:09:11.943107: step 5031, loss 0.554182.
Train: 2018-08-02T11:09:12.161805: step 5032, loss 0.529302.
Train: 2018-08-02T11:09:12.380535: step 5033, loss 0.587358.
Train: 2018-08-02T11:09:12.599238: step 5034, loss 0.545852.
Train: 2018-08-02T11:09:12.817929: step 5035, loss 0.587373.
Train: 2018-08-02T11:09:13.036632: step 5036, loss 0.653814.
Train: 2018-08-02T11:09:13.255330: step 5037, loss 0.529324.
Train: 2018-08-02T11:09:13.474033: step 5038, loss 0.636961.
Train: 2018-08-02T11:09:13.692728: step 5039, loss 0.595511.
Train: 2018-08-02T11:09:13.911397: step 5040, loss 0.57898.
Test: 2018-08-02T11:09:15.082997: step 5040, loss 0.548325.
Train: 2018-08-02T11:09:15.301720: step 5041, loss 0.546188.
Train: 2018-08-02T11:09:15.520425: step 5042, loss 0.546266.
Train: 2018-08-02T11:09:15.739124: step 5043, loss 0.554476.
Train: 2018-08-02T11:09:15.957818: step 5044, loss 0.595186.
Train: 2018-08-02T11:09:16.176522: step 5045, loss 0.554559.
Train: 2018-08-02T11:09:16.395214: step 5046, loss 0.570798.
Train: 2018-08-02T11:09:16.613919: step 5047, loss 0.53847.
Train: 2018-08-02T11:09:16.832587: step 5048, loss 0.530436.
Train: 2018-08-02T11:09:17.051316: step 5049, loss 0.570811.
Train: 2018-08-02T11:09:17.270015: step 5050, loss 0.506246.
Test: 2018-08-02T11:09:18.441586: step 5050, loss 0.549978.
Train: 2018-08-02T11:09:18.644687: step 5051, loss 0.562728.
Train: 2018-08-02T11:09:18.863392: step 5052, loss 0.562715.
Train: 2018-08-02T11:09:19.082091: step 5053, loss 0.538414.
Train: 2018-08-02T11:09:19.300792: step 5054, loss 0.595118.
Train: 2018-08-02T11:09:19.519458: step 5055, loss 0.473431.
Train: 2018-08-02T11:09:19.738157: step 5056, loss 0.562646.
Train: 2018-08-02T11:09:19.956886: step 5057, loss 0.54631.
Train: 2018-08-02T11:09:20.175585: step 5058, loss 0.505366.
Train: 2018-08-02T11:09:20.394284: step 5059, loss 0.546148.
Train: 2018-08-02T11:09:20.612982: step 5060, loss 0.55429.
Test: 2018-08-02T11:09:21.800174: step 5060, loss 0.549459.
Train: 2018-08-02T11:09:22.003252: step 5061, loss 0.537712.
Train: 2018-08-02T11:09:22.237572: step 5062, loss 0.570757.
Train: 2018-08-02T11:09:22.456295: step 5063, loss 0.54582.
Train: 2018-08-02T11:09:22.674999: step 5064, loss 0.545754.
Train: 2018-08-02T11:09:22.893699: step 5065, loss 0.595854.
Train: 2018-08-02T11:09:23.112392: step 5066, loss 0.528904.
Train: 2018-08-02T11:09:23.331096: step 5067, loss 0.528823.
Train: 2018-08-02T11:09:23.549789: step 5068, loss 0.469839.
Train: 2018-08-02T11:09:23.768462: step 5069, loss 0.537022.
Train: 2018-08-02T11:09:23.987192: step 5070, loss 0.579313.
Test: 2018-08-02T11:09:25.143141: step 5070, loss 0.54794.
Train: 2018-08-02T11:09:25.361865: step 5071, loss 0.528314.
Train: 2018-08-02T11:09:25.580567: step 5072, loss 0.579408.
Train: 2018-08-02T11:09:25.799268: step 5073, loss 0.579446.
Train: 2018-08-02T11:09:26.017961: step 5074, loss 0.596605.
Train: 2018-08-02T11:09:26.236636: step 5075, loss 0.519482.
Train: 2018-08-02T11:09:26.455364: step 5076, loss 0.579493.
Train: 2018-08-02T11:09:26.674035: step 5077, loss 0.536591.
Train: 2018-08-02T11:09:26.892761: step 5078, loss 0.467886.
Train: 2018-08-02T11:09:27.111460: step 5079, loss 0.536519.
Train: 2018-08-02T11:09:27.330160: step 5080, loss 0.57959.
Test: 2018-08-02T11:09:28.486108: step 5080, loss 0.547915.
Train: 2018-08-02T11:09:28.704832: step 5081, loss 0.64007.
Train: 2018-08-02T11:09:28.923537: step 5082, loss 0.527814.
Train: 2018-08-02T11:09:29.142235: step 5083, loss 0.605488.
Train: 2018-08-02T11:09:29.360933: step 5084, loss 0.570957.
Train: 2018-08-02T11:09:29.579602: step 5085, loss 0.570941.
Train: 2018-08-02T11:09:29.798331: step 5086, loss 0.613864.
Train: 2018-08-02T11:09:30.017000: step 5087, loss 0.596584.
Train: 2018-08-02T11:09:30.235698: step 5088, loss 0.536749.
Train: 2018-08-02T11:09:30.454422: step 5089, loss 0.562344.
Train: 2018-08-02T11:09:30.673126: step 5090, loss 0.64714.
Test: 2018-08-02T11:09:31.844697: step 5090, loss 0.54769.
Train: 2018-08-02T11:09:32.047799: step 5091, loss 0.528611.
Train: 2018-08-02T11:09:32.266498: step 5092, loss 0.562382.
Train: 2018-08-02T11:09:32.485202: step 5093, loss 0.629428.
Train: 2018-08-02T11:09:32.703871: step 5094, loss 0.620799.
Train: 2018-08-02T11:09:32.922600: step 5095, loss 0.579044.
Train: 2018-08-02T11:09:33.141298: step 5096, loss 0.611981.
Train: 2018-08-02T11:09:33.359998: step 5097, loss 0.603522.
Train: 2018-08-02T11:09:33.578692: step 5098, loss 0.587045.
Train: 2018-08-02T11:09:33.797395: step 5099, loss 0.522315.
Train: 2018-08-02T11:09:34.016094: step 5100, loss 0.538653.
Test: 2018-08-02T11:09:35.203285: step 5100, loss 0.55002.
Train: 2018-08-02T11:09:36.000004: step 5101, loss 0.546826.
Train: 2018-08-02T11:09:36.218706: step 5102, loss 0.538908.
Train: 2018-08-02T11:09:36.437402: step 5103, loss 0.56291.
Train: 2018-08-02T11:09:36.656069: step 5104, loss 0.554975.
Train: 2018-08-02T11:09:36.874768: step 5105, loss 0.547053.
Train: 2018-08-02T11:09:37.093502: step 5106, loss 0.539124.
Train: 2018-08-02T11:09:37.312167: step 5107, loss 0.618601.
Train: 2018-08-02T11:09:37.530895: step 5108, loss 0.586799.
Train: 2018-08-02T11:09:37.749595: step 5109, loss 0.563001.
Train: 2018-08-02T11:09:37.968297: step 5110, loss 0.539257.
Test: 2018-08-02T11:09:39.139864: step 5110, loss 0.549436.
Train: 2018-08-02T11:09:39.342941: step 5111, loss 0.531349.
Train: 2018-08-02T11:09:39.561671: step 5112, loss 0.499629.
Train: 2018-08-02T11:09:39.780364: step 5113, loss 0.578861.
Train: 2018-08-02T11:09:39.999068: step 5114, loss 0.554991.
Train: 2018-08-02T11:09:40.217736: step 5115, loss 0.562916.
Train: 2018-08-02T11:09:40.436436: step 5116, loss 0.594839.
Train: 2018-08-02T11:09:40.655164: step 5117, loss 0.594856.
Train: 2018-08-02T11:09:40.873859: step 5118, loss 0.554866.
Train: 2018-08-02T11:09:41.092531: step 5119, loss 0.59487.
Train: 2018-08-02T11:09:41.311231: step 5120, loss 0.530854.
Test: 2018-08-02T11:09:42.482831: step 5120, loss 0.548372.
Train: 2018-08-02T11:09:42.685938: step 5121, loss 0.586872.
Train: 2018-08-02T11:09:42.904608: step 5122, loss 0.562848.
Train: 2018-08-02T11:09:43.123330: step 5123, loss 0.482746.
Train: 2018-08-02T11:09:43.342038: step 5124, loss 0.610981.
Train: 2018-08-02T11:09:43.560703: step 5125, loss 0.554766.
Train: 2018-08-02T11:09:43.779427: step 5126, loss 0.522564.
Train: 2018-08-02T11:09:43.998126: step 5127, loss 0.554698.
Train: 2018-08-02T11:09:44.216799: step 5128, loss 0.586966.
Train: 2018-08-02T11:09:44.435529: step 5129, loss 0.538458.
Train: 2018-08-02T11:09:44.654227: step 5130, loss 0.59511.
Test: 2018-08-02T11:09:45.825798: step 5130, loss 0.548205.
Train: 2018-08-02T11:09:46.028875: step 5131, loss 0.578902.
Train: 2018-08-02T11:09:46.247574: step 5132, loss 0.570789.
Train: 2018-08-02T11:09:46.466307: step 5133, loss 0.554559.
Train: 2018-08-02T11:09:46.653755: step 5134, loss 0.45875.
Train: 2018-08-02T11:09:46.872459: step 5135, loss 0.513786.
Train: 2018-08-02T11:09:47.091160: step 5136, loss 0.521736.
Train: 2018-08-02T11:09:47.309825: step 5137, loss 0.562554.
Train: 2018-08-02T11:09:47.528554: step 5138, loss 0.595471.
Train: 2018-08-02T11:09:47.747253: step 5139, loss 0.51294.
Train: 2018-08-02T11:09:47.965956: step 5140, loss 0.521032.
Test: 2018-08-02T11:09:49.137522: step 5140, loss 0.548164.
Train: 2018-08-02T11:09:49.340600: step 5141, loss 0.5874.
Train: 2018-08-02T11:09:49.559329: step 5142, loss 0.562424.
Train: 2018-08-02T11:09:49.778028: step 5143, loss 0.545685.
Train: 2018-08-02T11:09:49.996727: step 5144, loss 0.545632.
Train: 2018-08-02T11:09:50.262285: step 5145, loss 0.663236.
Train: 2018-08-02T11:09:50.480959: step 5146, loss 0.553981.
Train: 2018-08-02T11:09:50.699688: step 5147, loss 0.537182.
Train: 2018-08-02T11:09:50.918386: step 5148, loss 0.587591.
Train: 2018-08-02T11:09:51.137079: step 5149, loss 0.621184.
Train: 2018-08-02T11:09:51.355777: step 5150, loss 0.612682.
Test: 2018-08-02T11:09:52.511732: step 5150, loss 0.548459.
Train: 2018-08-02T11:09:52.730430: step 5151, loss 0.59584.
Train: 2018-08-02T11:09:52.949161: step 5152, loss 0.545796.
Train: 2018-08-02T11:09:53.167859: step 5153, loss 0.554139.
Train: 2018-08-02T11:09:53.386527: step 5154, loss 0.537631.
Train: 2018-08-02T11:09:53.605256: step 5155, loss 0.570751.
Train: 2018-08-02T11:09:53.823925: step 5156, loss 0.628533.
Train: 2018-08-02T11:09:54.042648: step 5157, loss 0.587211.
Train: 2018-08-02T11:09:54.261322: step 5158, loss 0.562563.
Train: 2018-08-02T11:09:54.480051: step 5159, loss 0.505405.
Train: 2018-08-02T11:09:54.698729: step 5160, loss 0.513669.
Test: 2018-08-02T11:09:55.870321: step 5160, loss 0.548998.
Train: 2018-08-02T11:09:56.089054: step 5161, loss 0.570774.
Train: 2018-08-02T11:09:56.323365: step 5162, loss 0.530016.
Train: 2018-08-02T11:09:56.526447: step 5163, loss 0.65232.
Train: 2018-08-02T11:09:56.745140: step 5164, loss 0.619619.
Train: 2018-08-02T11:09:56.963849: step 5165, loss 0.50586.
Train: 2018-08-02T11:09:57.182544: step 5166, loss 0.53837.
Train: 2018-08-02T11:09:57.401243: step 5167, loss 0.554593.
Train: 2018-08-02T11:09:57.619941: step 5168, loss 0.514099.
Train: 2018-08-02T11:09:57.838644: step 5169, loss 0.530252.
Train: 2018-08-02T11:09:58.057339: step 5170, loss 0.546431.
Test: 2018-08-02T11:09:59.244531: step 5170, loss 0.548484.
Train: 2018-08-02T11:09:59.447639: step 5171, loss 0.473085.
Train: 2018-08-02T11:09:59.666308: step 5172, loss 0.619853.
Train: 2018-08-02T11:09:59.885035: step 5173, loss 0.570728.
Train: 2018-08-02T11:10:00.103706: step 5174, loss 0.529669.
Train: 2018-08-02T11:10:00.322438: step 5175, loss 0.579155.
Train: 2018-08-02T11:10:00.541132: step 5176, loss 0.620269.
Train: 2018-08-02T11:10:00.759831: step 5177, loss 0.537994.
Train: 2018-08-02T11:10:00.978529: step 5178, loss 0.603698.
Train: 2018-08-02T11:10:01.197233: step 5179, loss 0.570758.
Train: 2018-08-02T11:10:01.415898: step 5180, loss 0.48856.
Test: 2018-08-02T11:10:02.571876: step 5180, loss 0.548409.
Train: 2018-08-02T11:10:02.790605: step 5181, loss 0.636583.
Train: 2018-08-02T11:10:03.009304: step 5182, loss 0.554316.
Train: 2018-08-02T11:10:03.228002: step 5183, loss 0.570759.
Train: 2018-08-02T11:10:03.446671: step 5184, loss 0.595391.
Train: 2018-08-02T11:10:03.665403: step 5185, loss 0.603555.
Train: 2018-08-02T11:10:03.884099: step 5186, loss 0.554409.
Train: 2018-08-02T11:10:04.102798: step 5187, loss 0.587098.
Train: 2018-08-02T11:10:04.321497: step 5188, loss 0.570777.
Train: 2018-08-02T11:10:04.540166: step 5189, loss 0.587037.
Train: 2018-08-02T11:10:04.758865: step 5190, loss 0.538373.
Test: 2018-08-02T11:10:05.930465: step 5190, loss 0.54957.
Train: 2018-08-02T11:10:06.133543: step 5191, loss 0.538436.
Train: 2018-08-02T11:10:06.352241: step 5192, loss 0.530388.
Train: 2018-08-02T11:10:06.570970: step 5193, loss 0.546556.
Train: 2018-08-02T11:10:06.789669: step 5194, loss 0.562718.
Train: 2018-08-02T11:10:07.023990: step 5195, loss 0.473727.
Train: 2018-08-02T11:10:07.242688: step 5196, loss 0.603231.
Train: 2018-08-02T11:10:07.461357: step 5197, loss 0.570787.
Train: 2018-08-02T11:10:07.680085: step 5198, loss 0.554526.
Train: 2018-08-02T11:10:07.898755: step 5199, loss 0.562632.
Train: 2018-08-02T11:10:08.117479: step 5200, loss 0.570788.
Test: 2018-08-02T11:10:09.289054: step 5200, loss 0.548989.
Train: 2018-08-02T11:10:10.101393: step 5201, loss 0.570818.
Train: 2018-08-02T11:10:10.320093: step 5202, loss 0.570779.
Train: 2018-08-02T11:10:10.538791: step 5203, loss 0.595299.
Train: 2018-08-02T11:10:10.757489: step 5204, loss 0.529952.
Train: 2018-08-02T11:10:10.976184: step 5205, loss 0.5055.
Train: 2018-08-02T11:10:11.194888: step 5206, loss 0.562593.
Train: 2018-08-02T11:10:11.413586: step 5207, loss 0.554379.
Train: 2018-08-02T11:10:11.632285: step 5208, loss 0.5953.
Train: 2018-08-02T11:10:11.850984: step 5209, loss 0.504959.
Train: 2018-08-02T11:10:12.069680: step 5210, loss 0.546044.
Test: 2018-08-02T11:10:13.241253: step 5210, loss 0.549046.
Train: 2018-08-02T11:10:13.459952: step 5211, loss 0.546119.
Train: 2018-08-02T11:10:13.678651: step 5212, loss 0.546521.
Train: 2018-08-02T11:10:13.897350: step 5213, loss 0.529222.
Train: 2018-08-02T11:10:14.116079: step 5214, loss 0.579087.
Train: 2018-08-02T11:10:14.334747: step 5215, loss 0.520151.
Train: 2018-08-02T11:10:14.553475: step 5216, loss 0.587128.
Train: 2018-08-02T11:10:14.772174: step 5217, loss 0.59667.
Train: 2018-08-02T11:10:14.990873: step 5218, loss 0.588598.
Train: 2018-08-02T11:10:15.209566: step 5219, loss 0.520375.
Train: 2018-08-02T11:10:15.428274: step 5220, loss 0.536899.
Test: 2018-08-02T11:10:16.599841: step 5220, loss 0.548196.
Train: 2018-08-02T11:10:16.818541: step 5221, loss 0.553759.
Train: 2018-08-02T11:10:17.037269: step 5222, loss 0.596105.
Train: 2018-08-02T11:10:17.255968: step 5223, loss 0.553987.
Train: 2018-08-02T11:10:17.474641: step 5224, loss 0.579251.
Train: 2018-08-02T11:10:17.693336: step 5225, loss 0.553662.
Train: 2018-08-02T11:10:17.912064: step 5226, loss 0.621181.
Train: 2018-08-02T11:10:18.130764: step 5227, loss 0.55414.
Train: 2018-08-02T11:10:18.349431: step 5228, loss 0.537698.
Train: 2018-08-02T11:10:18.568162: step 5229, loss 0.520962.
Train: 2018-08-02T11:10:18.802476: step 5230, loss 0.579061.
Test: 2018-08-02T11:10:19.974052: step 5230, loss 0.549765.
Train: 2018-08-02T11:10:20.177153: step 5231, loss 0.579034.
Train: 2018-08-02T11:10:20.395858: step 5232, loss 0.545915.
Train: 2018-08-02T11:10:20.614526: step 5233, loss 0.562481.
Train: 2018-08-02T11:10:20.833259: step 5234, loss 0.529408.
Train: 2018-08-02T11:10:21.051950: step 5235, loss 0.570756.
Train: 2018-08-02T11:10:21.270648: step 5236, loss 0.521135.
Train: 2018-08-02T11:10:21.489345: step 5237, loss 0.579033.
Train: 2018-08-02T11:10:21.708054: step 5238, loss 0.545921.
Train: 2018-08-02T11:10:21.926750: step 5239, loss 0.545909.
Train: 2018-08-02T11:10:22.145419: step 5240, loss 0.54589.
Test: 2018-08-02T11:10:23.317018: step 5240, loss 0.548604.
Train: 2018-08-02T11:10:23.535742: step 5241, loss 0.579056.
Train: 2018-08-02T11:10:23.754440: step 5242, loss 0.479451.
Train: 2018-08-02T11:10:23.973144: step 5243, loss 0.604037.
Train: 2018-08-02T11:10:24.238677: step 5244, loss 0.595742.
Train: 2018-08-02T11:10:24.473029: step 5245, loss 0.63737.
Train: 2018-08-02T11:10:24.691729: step 5246, loss 0.57076.
Train: 2018-08-02T11:10:24.910396: step 5247, loss 0.562464.
Train: 2018-08-02T11:10:25.129094: step 5248, loss 0.587315.
Train: 2018-08-02T11:10:25.347817: step 5249, loss 0.554236.
Train: 2018-08-02T11:10:25.566516: step 5250, loss 0.562513.
Test: 2018-08-02T11:10:26.722471: step 5250, loss 0.548976.
Train: 2018-08-02T11:10:26.941171: step 5251, loss 0.488471.
Train: 2018-08-02T11:10:27.159899: step 5252, loss 0.48024.
Train: 2018-08-02T11:10:27.378592: step 5253, loss 0.521291.
Train: 2018-08-02T11:10:27.597296: step 5254, loss 0.595553.
Train: 2018-08-02T11:10:27.815995: step 5255, loss 0.537646.
Train: 2018-08-02T11:10:28.034694: step 5256, loss 0.529293.
Train: 2018-08-02T11:10:28.253393: step 5257, loss 0.579073.
Train: 2018-08-02T11:10:28.472094: step 5258, loss 0.554113.
Train: 2018-08-02T11:10:28.690761: step 5259, loss 0.537413.
Train: 2018-08-02T11:10:28.909459: step 5260, loss 0.620889.
Test: 2018-08-02T11:10:30.081059: step 5260, loss 0.549231.
Train: 2018-08-02T11:10:30.284167: step 5261, loss 0.637608.
Train: 2018-08-02T11:10:30.502866: step 5262, loss 0.629154.
Train: 2018-08-02T11:10:30.721534: step 5263, loss 0.58739.
Train: 2018-08-02T11:10:30.940263: step 5264, loss 0.637049.
Train: 2018-08-02T11:10:31.158933: step 5265, loss 0.562515.
Train: 2018-08-02T11:10:31.377661: step 5266, loss 0.603596.
Train: 2018-08-02T11:10:31.596360: step 5267, loss 0.538099.
Train: 2018-08-02T11:10:31.815063: step 5268, loss 0.595186.
Train: 2018-08-02T11:10:32.033757: step 5269, loss 0.586996.
Train: 2018-08-02T11:10:32.252456: step 5270, loss 0.530503.
Test: 2018-08-02T11:10:33.424027: step 5270, loss 0.549005.
Train: 2018-08-02T11:10:33.627135: step 5271, loss 0.546725.
Train: 2018-08-02T11:10:33.845833: step 5272, loss 0.514728.
Train: 2018-08-02T11:10:34.064531: step 5273, loss 0.594884.
Train: 2018-08-02T11:10:34.283231: step 5274, loss 0.578863.
Train: 2018-08-02T11:10:34.501899: step 5275, loss 0.507005.
Train: 2018-08-02T11:10:34.720628: step 5276, loss 0.570875.
Train: 2018-08-02T11:10:34.939322: step 5277, loss 0.562895.
Train: 2018-08-02T11:10:35.158026: step 5278, loss 0.610792.
Train: 2018-08-02T11:10:35.376724: step 5279, loss 0.55494.
Train: 2018-08-02T11:10:35.595425: step 5280, loss 0.570886.
Test: 2018-08-02T11:10:36.766994: step 5280, loss 0.549814.
Train: 2018-08-02T11:10:36.970096: step 5281, loss 0.547007.
Train: 2018-08-02T11:10:37.188800: step 5282, loss 0.554962.
Train: 2018-08-02T11:10:37.407468: step 5283, loss 0.570891.
Train: 2018-08-02T11:10:37.626199: step 5284, loss 0.55495.
Train: 2018-08-02T11:10:37.813624: step 5285, loss 0.443865.
Train: 2018-08-02T11:10:38.032354: step 5286, loss 0.57886.
Train: 2018-08-02T11:10:38.251046: step 5287, loss 0.578858.
Train: 2018-08-02T11:10:38.469750: step 5288, loss 0.506451.
Train: 2018-08-02T11:10:38.688449: step 5289, loss 0.578913.
Train: 2018-08-02T11:10:38.907119: step 5290, loss 0.611378.
Test: 2018-08-02T11:10:40.063097: step 5290, loss 0.548023.
Train: 2018-08-02T11:10:40.281795: step 5291, loss 0.546452.
Train: 2018-08-02T11:10:40.500494: step 5292, loss 0.546426.
Train: 2018-08-02T11:10:40.719223: step 5293, loss 0.619649.
Train: 2018-08-02T11:10:40.937922: step 5294, loss 0.546354.
Train: 2018-08-02T11:10:41.156621: step 5295, loss 0.546407.
Train: 2018-08-02T11:10:41.375289: step 5296, loss 0.603392.
Train: 2018-08-02T11:10:41.594018: step 5297, loss 0.530018.
Train: 2018-08-02T11:10:41.812717: step 5298, loss 0.529995.
Train: 2018-08-02T11:10:42.031416: step 5299, loss 0.562615.
Train: 2018-08-02T11:10:42.250115: step 5300, loss 0.578938.
Test: 2018-08-02T11:10:43.421685: step 5300, loss 0.547982.
Train: 2018-08-02T11:10:44.265238: step 5301, loss 0.59531.
Train: 2018-08-02T11:10:44.483967: step 5302, loss 0.538061.
Train: 2018-08-02T11:10:44.702670: step 5303, loss 0.636231.
Train: 2018-08-02T11:10:44.921364: step 5304, loss 0.554431.
Train: 2018-08-02T11:10:45.140064: step 5305, loss 0.578936.
Train: 2018-08-02T11:10:45.358756: step 5306, loss 0.61968.
Train: 2018-08-02T11:10:45.577430: step 5307, loss 0.546392.
Train: 2018-08-02T11:10:45.796133: step 5308, loss 0.619479.
Train: 2018-08-02T11:10:46.014859: step 5309, loss 0.522259.
Train: 2018-08-02T11:10:46.233551: step 5310, loss 0.554656.
Test: 2018-08-02T11:10:47.405128: step 5310, loss 0.54835.
Train: 2018-08-02T11:10:47.608204: step 5311, loss 0.570816.
Train: 2018-08-02T11:10:47.826938: step 5312, loss 0.554711.
Train: 2018-08-02T11:10:48.045633: step 5313, loss 0.586924.
Train: 2018-08-02T11:10:48.264302: step 5314, loss 0.562797.
Train: 2018-08-02T11:10:48.483030: step 5315, loss 0.61901.
Train: 2018-08-02T11:10:48.701725: step 5316, loss 0.602894.
Train: 2018-08-02T11:10:48.920423: step 5317, loss 0.538932.
Train: 2018-08-02T11:10:49.139102: step 5318, loss 0.578859.
Train: 2018-08-02T11:10:49.357825: step 5319, loss 0.523184.
Train: 2018-08-02T11:10:49.576494: step 5320, loss 0.547068.
Test: 2018-08-02T11:10:50.748095: step 5320, loss 0.550061.
Train: 2018-08-02T11:10:50.951197: step 5321, loss 0.562968.
Train: 2018-08-02T11:10:51.169872: step 5322, loss 0.555018.
Train: 2018-08-02T11:10:51.388600: step 5323, loss 0.618605.
Train: 2018-08-02T11:10:51.607293: step 5324, loss 0.555032.
Train: 2018-08-02T11:10:51.825968: step 5325, loss 0.578858.
Train: 2018-08-02T11:10:52.044696: step 5326, loss 0.634385.
Train: 2018-08-02T11:10:52.263396: step 5327, loss 0.52346.
Train: 2018-08-02T11:10:52.482093: step 5328, loss 0.523511.
Train: 2018-08-02T11:10:52.700792: step 5329, loss 0.547221.
Train: 2018-08-02T11:10:52.919493: step 5330, loss 0.563026.
Test: 2018-08-02T11:10:54.091062: step 5330, loss 0.549412.
Train: 2018-08-02T11:10:54.294164: step 5331, loss 0.642253.
Train: 2018-08-02T11:10:54.528486: step 5332, loss 0.610526.
Train: 2018-08-02T11:10:54.731567: step 5333, loss 0.515651.
Train: 2018-08-02T11:10:54.950260: step 5334, loss 0.507777.
Train: 2018-08-02T11:10:55.168960: step 5335, loss 0.618401.
Train: 2018-08-02T11:10:55.387634: step 5336, loss 0.570954.
Train: 2018-08-02T11:10:55.606333: step 5337, loss 0.507707.
Train: 2018-08-02T11:10:55.825065: step 5338, loss 0.49177.
Train: 2018-08-02T11:10:56.043759: step 5339, loss 0.602687.
Train: 2018-08-02T11:10:56.262458: step 5340, loss 0.523147.
Test: 2018-08-02T11:10:57.434030: step 5340, loss 0.54883.
Train: 2018-08-02T11:10:57.699626: step 5341, loss 0.570879.
Train: 2018-08-02T11:10:57.918321: step 5342, loss 0.546852.
Train: 2018-08-02T11:10:58.137019: step 5343, loss 0.530719.
Train: 2018-08-02T11:10:58.355718: step 5344, loss 0.554721.
Train: 2018-08-02T11:10:58.574421: step 5345, loss 0.554652.
Train: 2018-08-02T11:10:58.793116: step 5346, loss 0.530274.
Train: 2018-08-02T11:10:59.011815: step 5347, loss 0.587049.
Train: 2018-08-02T11:10:59.230513: step 5348, loss 0.636021.
Train: 2018-08-02T11:10:59.449216: step 5349, loss 0.570772.
Train: 2018-08-02T11:10:59.667911: step 5350, loss 0.554439.
Test: 2018-08-02T11:11:00.823861: step 5350, loss 0.549142.
Train: 2018-08-02T11:11:01.042584: step 5351, loss 0.587109.
Train: 2018-08-02T11:11:01.276904: step 5352, loss 0.578938.
Train: 2018-08-02T11:11:01.479957: step 5353, loss 0.529939.
Train: 2018-08-02T11:11:01.698655: step 5354, loss 0.570771.
Train: 2018-08-02T11:11:01.917380: step 5355, loss 0.52992.
Train: 2018-08-02T11:11:02.136083: step 5356, loss 0.56259.
Train: 2018-08-02T11:11:02.354752: step 5357, loss 0.48893.
Train: 2018-08-02T11:11:02.573451: step 5358, loss 0.570758.
Train: 2018-08-02T11:11:02.792150: step 5359, loss 0.505011.
Train: 2018-08-02T11:11:03.010878: step 5360, loss 0.61198.
Test: 2018-08-02T11:11:04.166828: step 5360, loss 0.550041.
Train: 2018-08-02T11:11:04.385550: step 5361, loss 0.545988.
Train: 2018-08-02T11:11:04.604226: step 5362, loss 0.562487.
Train: 2018-08-02T11:11:04.822925: step 5363, loss 0.562474.
Train: 2018-08-02T11:11:05.053162: step 5364, loss 0.529295.
Train: 2018-08-02T11:11:05.271861: step 5365, loss 0.61229.
Train: 2018-08-02T11:11:05.490530: step 5366, loss 0.554141.
Train: 2018-08-02T11:11:05.709259: step 5367, loss 0.595698.
Train: 2018-08-02T11:11:05.927958: step 5368, loss 0.52922.
Train: 2018-08-02T11:11:06.146626: step 5369, loss 0.628935.
Train: 2018-08-02T11:11:06.365325: step 5370, loss 0.587355.
Test: 2018-08-02T11:11:07.536925: step 5370, loss 0.549594.
Train: 2018-08-02T11:11:07.740037: step 5371, loss 0.587323.
Train: 2018-08-02T11:11:07.958734: step 5372, loss 0.545968.
Train: 2018-08-02T11:11:08.177401: step 5373, loss 0.587251.
Train: 2018-08-02T11:11:08.396123: step 5374, loss 0.56253.
Train: 2018-08-02T11:11:08.614823: step 5375, loss 0.455807.
Train: 2018-08-02T11:11:08.833527: step 5376, loss 0.603621.
Train: 2018-08-02T11:11:09.052196: step 5377, loss 0.521493.
Train: 2018-08-02T11:11:09.270894: step 5378, loss 0.554329.
Train: 2018-08-02T11:11:09.489627: step 5379, loss 0.636516.
Train: 2018-08-02T11:11:09.708292: step 5380, loss 0.546132.
Test: 2018-08-02T11:11:10.879892: step 5380, loss 0.547537.
Train: 2018-08-02T11:11:11.082971: step 5381, loss 0.611776.
Train: 2018-08-02T11:11:11.301703: step 5382, loss 0.570765.
Train: 2018-08-02T11:11:11.520398: step 5383, loss 0.546253.
Train: 2018-08-02T11:11:11.754688: step 5384, loss 0.54629.
Train: 2018-08-02T11:11:11.957790: step 5385, loss 0.578928.
Train: 2018-08-02T11:11:12.176495: step 5386, loss 0.587067.
Train: 2018-08-02T11:11:12.395193: step 5387, loss 0.505729.
Train: 2018-08-02T11:11:12.613896: step 5388, loss 0.54639.
Train: 2018-08-02T11:11:12.832591: step 5389, loss 0.619584.
Train: 2018-08-02T11:11:13.051289: step 5390, loss 0.505782.
Test: 2018-08-02T11:11:14.222859: step 5390, loss 0.548705.
Train: 2018-08-02T11:11:14.425962: step 5391, loss 0.570784.
Train: 2018-08-02T11:11:14.644668: step 5392, loss 0.53013.
Train: 2018-08-02T11:11:14.863334: step 5393, loss 0.554502.
Train: 2018-08-02T11:11:15.082058: step 5394, loss 0.603367.
Train: 2018-08-02T11:11:15.300763: step 5395, loss 0.521891.
Train: 2018-08-02T11:11:15.519461: step 5396, loss 0.538151.
Train: 2018-08-02T11:11:15.738160: step 5397, loss 0.578937.
Train: 2018-08-02T11:11:15.956858: step 5398, loss 0.529893.
Train: 2018-08-02T11:11:16.175558: step 5399, loss 0.619892.
Train: 2018-08-02T11:11:16.394227: step 5400, loss 0.554388.
Test: 2018-08-02T11:11:17.565827: step 5400, loss 0.548518.
Train: 2018-08-02T11:11:18.362549: step 5401, loss 0.587144.
Train: 2018-08-02T11:11:18.581245: step 5402, loss 0.603513.
Train: 2018-08-02T11:11:18.799946: step 5403, loss 0.578943.
Train: 2018-08-02T11:11:19.018623: step 5404, loss 0.546283.
Train: 2018-08-02T11:11:19.237345: step 5405, loss 0.464772.
Train: 2018-08-02T11:11:19.456039: step 5406, loss 0.578936.
Train: 2018-08-02T11:11:19.674709: step 5407, loss 0.538082.
Train: 2018-08-02T11:11:19.893439: step 5408, loss 0.538036.
Train: 2018-08-02T11:11:20.112140: step 5409, loss 0.4806.
Train: 2018-08-02T11:11:20.330830: step 5410, loss 0.529634.
Test: 2018-08-02T11:11:21.502406: step 5410, loss 0.549092.
Train: 2018-08-02T11:11:21.705507: step 5411, loss 0.529475.
Train: 2018-08-02T11:11:21.924182: step 5412, loss 0.554177.
Train: 2018-08-02T11:11:22.142896: step 5413, loss 0.579084.
Train: 2018-08-02T11:11:22.377219: step 5414, loss 0.529038.
Train: 2018-08-02T11:11:22.595923: step 5415, loss 0.545657.
Train: 2018-08-02T11:11:22.814628: step 5416, loss 0.570786.
Train: 2018-08-02T11:11:23.033327: step 5417, loss 0.537114.
Train: 2018-08-02T11:11:23.252026: step 5418, loss 0.553922.
Train: 2018-08-02T11:11:23.470694: step 5419, loss 0.545431.
Train: 2018-08-02T11:11:23.689423: step 5420, loss 0.56235.
Test: 2018-08-02T11:11:24.860994: step 5420, loss 0.548348.
Train: 2018-08-02T11:11:25.079692: step 5421, loss 0.502856.
Train: 2018-08-02T11:11:25.298392: step 5422, loss 0.570863.
Train: 2018-08-02T11:11:25.517090: step 5423, loss 0.562338.
Train: 2018-08-02T11:11:25.735813: step 5424, loss 0.562336.
Train: 2018-08-02T11:11:25.954517: step 5425, loss 0.528077.
Train: 2018-08-02T11:11:26.173211: step 5426, loss 0.5366.
Train: 2018-08-02T11:11:26.391915: step 5427, loss 0.527962.
Train: 2018-08-02T11:11:26.610608: step 5428, loss 0.536504.
Train: 2018-08-02T11:11:26.829313: step 5429, loss 0.631369.
Train: 2018-08-02T11:11:27.048013: step 5430, loss 0.570969.
Test: 2018-08-02T11:11:28.219582: step 5430, loss 0.547334.
Train: 2018-08-02T11:11:28.422689: step 5431, loss 0.570965.
Train: 2018-08-02T11:11:28.641389: step 5432, loss 0.596818.
Train: 2018-08-02T11:11:28.860091: step 5433, loss 0.570941.
Train: 2018-08-02T11:11:29.078756: step 5434, loss 0.553747.
Train: 2018-08-02T11:11:29.297485: step 5435, loss 0.562335.
Train: 2018-08-02T11:11:29.484936: step 5436, loss 0.63534.
Train: 2018-08-02T11:11:29.703634: step 5437, loss 0.48562.
Train: 2018-08-02T11:11:29.922338: step 5438, loss 0.59638.
Train: 2018-08-02T11:11:30.141037: step 5439, loss 0.519912.
Train: 2018-08-02T11:11:30.359736: step 5440, loss 0.503037.
Test: 2018-08-02T11:11:31.531306: step 5440, loss 0.548207.
Train: 2018-08-02T11:11:31.734418: step 5441, loss 0.553883.
Train: 2018-08-02T11:11:31.953082: step 5442, loss 0.553887.
Train: 2018-08-02T11:11:32.171811: step 5443, loss 0.579288.
Train: 2018-08-02T11:11:32.390481: step 5444, loss 0.621572.
Train: 2018-08-02T11:11:32.609213: step 5445, loss 0.520165.
Train: 2018-08-02T11:11:32.827908: step 5446, loss 0.545512.
Train: 2018-08-02T11:11:33.046607: step 5447, loss 0.553953.
Train: 2018-08-02T11:11:33.265275: step 5448, loss 0.562378.
Train: 2018-08-02T11:11:33.499629: step 5449, loss 0.511938.
Train: 2018-08-02T11:11:33.718327: step 5450, loss 0.596017.
Test: 2018-08-02T11:11:34.889894: step 5450, loss 0.548933.
Train: 2018-08-02T11:11:35.092972: step 5451, loss 0.595995.
Train: 2018-08-02T11:11:35.311705: step 5452, loss 0.621117.
Train: 2018-08-02T11:11:35.530371: step 5453, loss 0.545679.
Train: 2018-08-02T11:11:35.749099: step 5454, loss 0.595802.
Train: 2018-08-02T11:11:35.967801: step 5455, loss 0.587401.
Train: 2018-08-02T11:11:36.186497: step 5456, loss 0.579049.
Train: 2018-08-02T11:11:36.405191: step 5457, loss 0.570756.
Train: 2018-08-02T11:11:36.639487: step 5458, loss 0.554291.
Train: 2018-08-02T11:11:36.858218: step 5459, loss 0.537925.
Train: 2018-08-02T11:11:37.076913: step 5460, loss 0.505235.
Test: 2018-08-02T11:11:38.248483: step 5460, loss 0.548342.
Train: 2018-08-02T11:11:38.451586: step 5461, loss 0.578951.
Train: 2018-08-02T11:11:38.670261: step 5462, loss 0.570767.
Train: 2018-08-02T11:11:38.888959: step 5463, loss 0.562599.
Train: 2018-08-02T11:11:39.107657: step 5464, loss 0.61158.
Train: 2018-08-02T11:11:39.326387: step 5465, loss 0.513763.
Train: 2018-08-02T11:11:39.545086: step 5466, loss 0.513804.
Train: 2018-08-02T11:11:39.779376: step 5467, loss 0.63593.
Train: 2018-08-02T11:11:39.982487: step 5468, loss 0.570782.
Train: 2018-08-02T11:11:40.201182: step 5469, loss 0.538292.
Train: 2018-08-02T11:11:40.419880: step 5470, loss 0.538312.
Test: 2018-08-02T11:11:41.591450: step 5470, loss 0.54892.
Train: 2018-08-02T11:11:41.794553: step 5471, loss 0.538308.
Train: 2018-08-02T11:11:42.013227: step 5472, loss 0.627662.
Train: 2018-08-02T11:11:42.231926: step 5473, loss 0.578903.
Train: 2018-08-02T11:11:42.450655: step 5474, loss 0.578898.
Train: 2018-08-02T11:11:42.669355: step 5475, loss 0.457442.
Train: 2018-08-02T11:11:42.888052: step 5476, loss 0.55459.
Train: 2018-08-02T11:11:43.106721: step 5477, loss 0.522083.
Train: 2018-08-02T11:11:43.325450: step 5478, loss 0.570785.
Train: 2018-08-02T11:11:43.544119: step 5479, loss 0.59523.
Train: 2018-08-02T11:11:43.762848: step 5480, loss 0.58709.
Test: 2018-08-02T11:11:44.934418: step 5480, loss 0.549735.
Train: 2018-08-02T11:11:45.137496: step 5481, loss 0.578932.
Train: 2018-08-02T11:11:45.356228: step 5482, loss 0.546298.
Train: 2018-08-02T11:11:45.574924: step 5483, loss 0.570772.
Train: 2018-08-02T11:11:45.793592: step 5484, loss 0.587092.
Train: 2018-08-02T11:11:46.012290: step 5485, loss 0.595238.
Train: 2018-08-02T11:11:46.231023: step 5486, loss 0.481196.
Train: 2018-08-02T11:11:46.449718: step 5487, loss 0.562626.
Train: 2018-08-02T11:11:46.668386: step 5488, loss 0.59524.
Train: 2018-08-02T11:11:46.887116: step 5489, loss 0.56262.
Train: 2018-08-02T11:11:47.105818: step 5490, loss 0.578927.
Test: 2018-08-02T11:11:48.277384: step 5490, loss 0.548644.
Train: 2018-08-02T11:11:48.480463: step 5491, loss 0.538182.
Train: 2018-08-02T11:11:48.699191: step 5492, loss 0.538178.
Train: 2018-08-02T11:11:48.917860: step 5493, loss 0.619704.
Train: 2018-08-02T11:11:49.136559: step 5494, loss 0.52188.
Train: 2018-08-02T11:11:49.355292: step 5495, loss 0.505563.
Train: 2018-08-02T11:11:49.573987: step 5496, loss 0.55444.
Train: 2018-08-02T11:11:49.792686: step 5497, loss 0.636196.
Train: 2018-08-02T11:11:50.011378: step 5498, loss 0.66071.
Train: 2018-08-02T11:11:50.230086: step 5499, loss 0.529996.
Train: 2018-08-02T11:11:50.448752: step 5500, loss 0.546349.
Test: 2018-08-02T11:11:51.620352: step 5500, loss 0.548312.
Train: 2018-08-02T11:11:52.417041: step 5501, loss 0.635859.
Train: 2018-08-02T11:11:52.635741: step 5502, loss 0.578904.
Train: 2018-08-02T11:11:52.870085: step 5503, loss 0.603165.
Train: 2018-08-02T11:11:53.088789: step 5504, loss 0.554691.
Train: 2018-08-02T11:11:53.307487: step 5505, loss 0.53063.
Train: 2018-08-02T11:11:53.526186: step 5506, loss 0.586896.
Train: 2018-08-02T11:11:53.744854: step 5507, loss 0.546823.
Train: 2018-08-02T11:11:53.963583: step 5508, loss 0.562864.
Train: 2018-08-02T11:11:54.182282: step 5509, loss 0.602835.
Train: 2018-08-02T11:11:54.400982: step 5510, loss 0.586836.
Test: 2018-08-02T11:11:55.572552: step 5510, loss 0.550749.
Train: 2018-08-02T11:11:55.775663: step 5511, loss 0.523149.
Train: 2018-08-02T11:11:55.994328: step 5512, loss 0.531147.
Train: 2018-08-02T11:11:56.213026: step 5513, loss 0.507279.
Train: 2018-08-02T11:11:56.431755: step 5514, loss 0.475291.
Train: 2018-08-02T11:11:56.650455: step 5515, loss 0.546874.
Train: 2018-08-02T11:11:56.869148: step 5516, loss 0.530693.
Train: 2018-08-02T11:11:57.087852: step 5517, loss 0.554679.
Train: 2018-08-02T11:11:57.306521: step 5518, loss 0.522179.
Train: 2018-08-02T11:11:57.525244: step 5519, loss 0.587051.
Train: 2018-08-02T11:11:57.743948: step 5520, loss 0.505261.
Test: 2018-08-02T11:11:58.899897: step 5520, loss 0.549908.
Train: 2018-08-02T11:11:59.103000: step 5521, loss 0.578916.
Train: 2018-08-02T11:11:59.321704: step 5522, loss 0.496284.
Train: 2018-08-02T11:11:59.540397: step 5523, loss 0.570421.
Train: 2018-08-02T11:11:59.759102: step 5524, loss 0.58755.
Train: 2018-08-02T11:11:59.977804: step 5525, loss 0.57131.
Train: 2018-08-02T11:12:00.196500: step 5526, loss 0.519853.
Train: 2018-08-02T11:12:00.415168: step 5527, loss 0.612883.
Train: 2018-08-02T11:12:00.633896: step 5528, loss 0.561855.
Train: 2018-08-02T11:12:00.852591: step 5529, loss 0.595301.
Train: 2018-08-02T11:12:01.071265: step 5530, loss 0.553916.
Test: 2018-08-02T11:12:02.242865: step 5530, loss 0.548217.
Train: 2018-08-02T11:12:02.445973: step 5531, loss 0.552547.
Train: 2018-08-02T11:12:02.664671: step 5532, loss 0.517819.
Train: 2018-08-02T11:12:02.883371: step 5533, loss 0.588475.
Train: 2018-08-02T11:12:03.102072: step 5534, loss 0.606665.
Train: 2018-08-02T11:12:03.320770: step 5535, loss 0.5469.
Train: 2018-08-02T11:12:03.539466: step 5536, loss 0.639636.
Train: 2018-08-02T11:12:03.758135: step 5537, loss 0.59655.
Train: 2018-08-02T11:12:03.976868: step 5538, loss 0.528558.
Train: 2018-08-02T11:12:04.195534: step 5539, loss 0.562277.
Train: 2018-08-02T11:12:04.414231: step 5540, loss 0.504566.
Test: 2018-08-02T11:12:05.585832: step 5540, loss 0.548131.
Train: 2018-08-02T11:12:05.804555: step 5541, loss 0.570713.
Train: 2018-08-02T11:12:06.023229: step 5542, loss 0.496661.
Train: 2018-08-02T11:12:06.241958: step 5543, loss 0.513175.
Train: 2018-08-02T11:12:06.460657: step 5544, loss 0.595466.
Train: 2018-08-02T11:12:06.679356: step 5545, loss 0.603638.
Train: 2018-08-02T11:12:06.898024: step 5546, loss 0.628277.
Train: 2018-08-02T11:12:07.116749: step 5547, loss 0.546181.
Train: 2018-08-02T11:12:07.335452: step 5548, loss 0.587116.
Train: 2018-08-02T11:12:07.554151: step 5549, loss 0.627922.
Train: 2018-08-02T11:12:07.772850: step 5550, loss 0.546392.
Test: 2018-08-02T11:12:08.944421: step 5550, loss 0.550265.
Train: 2018-08-02T11:12:09.147522: step 5551, loss 0.587008.
Train: 2018-08-02T11:12:09.366227: step 5552, loss 0.578889.
Train: 2018-08-02T11:12:09.584926: step 5553, loss 0.53053.
Train: 2018-08-02T11:12:09.803624: step 5554, loss 0.594959.
Train: 2018-08-02T11:12:10.022318: step 5555, loss 0.546776.
Train: 2018-08-02T11:12:10.240991: step 5556, loss 0.522801.
Train: 2018-08-02T11:12:10.459724: step 5557, loss 0.46679.
Train: 2018-08-02T11:12:10.678419: step 5558, loss 0.602933.
Train: 2018-08-02T11:12:10.897118: step 5559, loss 0.651132.
Train: 2018-08-02T11:12:11.115787: step 5560, loss 0.570848.
Test: 2018-08-02T11:12:12.271767: step 5560, loss 0.54947.
Train: 2018-08-02T11:12:12.506087: step 5561, loss 0.570856.
Train: 2018-08-02T11:12:12.709194: step 5562, loss 0.466868.
Train: 2018-08-02T11:12:12.927893: step 5563, loss 0.554833.
Train: 2018-08-02T11:12:13.146594: step 5564, loss 0.546777.
Train: 2018-08-02T11:12:13.365291: step 5565, loss 0.578872.
Train: 2018-08-02T11:12:13.583991: step 5566, loss 0.55473.
Train: 2018-08-02T11:12:13.802689: step 5567, loss 0.554698.
Train: 2018-08-02T11:12:14.021386: step 5568, loss 0.49815.
Train: 2018-08-02T11:12:14.240080: step 5569, loss 0.586989.
Train: 2018-08-02T11:12:14.458785: step 5570, loss 0.530196.
Test: 2018-08-02T11:12:15.630355: step 5570, loss 0.549224.
Train: 2018-08-02T11:12:15.833462: step 5571, loss 0.595229.
Train: 2018-08-02T11:12:16.052161: step 5572, loss 0.562636.
Train: 2018-08-02T11:12:16.270860: step 5573, loss 0.636147.
Train: 2018-08-02T11:12:16.489559: step 5574, loss 0.611581.
Train: 2018-08-02T11:12:16.708227: step 5575, loss 0.644116.
Train: 2018-08-02T11:12:16.926956: step 5576, loss 0.603275.
Train: 2018-08-02T11:12:17.161253: step 5577, loss 0.473716.
Train: 2018-08-02T11:12:17.364354: step 5578, loss 0.53849.
Train: 2018-08-02T11:12:17.583055: step 5579, loss 0.595035.
Train: 2018-08-02T11:12:17.801751: step 5580, loss 0.570817.
Test: 2018-08-02T11:12:18.957700: step 5580, loss 0.549687.
Train: 2018-08-02T11:12:19.176424: step 5581, loss 0.538605.
Train: 2018-08-02T11:12:19.395128: step 5582, loss 0.554724.
Train: 2018-08-02T11:12:19.613822: step 5583, loss 0.611073.
Train: 2018-08-02T11:12:19.832526: step 5584, loss 0.51456.
Train: 2018-08-02T11:12:20.051225: step 5585, loss 0.562793.
Train: 2018-08-02T11:12:20.269894: step 5586, loss 0.514543.
Train: 2018-08-02T11:12:20.441759: step 5587, loss 0.545593.
Train: 2018-08-02T11:12:20.660457: step 5588, loss 0.603082.
Train: 2018-08-02T11:12:20.879157: step 5589, loss 0.586956.
Train: 2018-08-02T11:12:21.097858: step 5590, loss 0.578884.
Test: 2018-08-02T11:12:22.269425: step 5590, loss 0.549637.
Train: 2018-08-02T11:12:22.472527: step 5591, loss 0.546601.
Train: 2018-08-02T11:12:22.691201: step 5592, loss 0.586957.
Train: 2018-08-02T11:12:22.909930: step 5593, loss 0.514313.
Train: 2018-08-02T11:12:23.128630: step 5594, loss 0.530406.
Train: 2018-08-02T11:12:23.347332: step 5595, loss 0.514196.
Train: 2018-08-02T11:12:23.566004: step 5596, loss 0.505806.
Train: 2018-08-02T11:12:23.784695: step 5597, loss 0.513659.
Train: 2018-08-02T11:12:24.003424: step 5598, loss 0.546593.
Train: 2018-08-02T11:12:24.284608: step 5599, loss 0.595158.
Train: 2018-08-02T11:12:24.503307: step 5600, loss 0.587802.
Test: 2018-08-02T11:12:25.674878: step 5600, loss 0.548828.
Train: 2018-08-02T11:12:26.455974: step 5601, loss 0.521093.
Train: 2018-08-02T11:12:26.674644: step 5602, loss 0.554243.
Train: 2018-08-02T11:12:26.893372: step 5603, loss 0.60401.
Train: 2018-08-02T11:12:27.112066: step 5604, loss 0.604002.
Train: 2018-08-02T11:12:27.330740: step 5605, loss 0.537505.
Train: 2018-08-02T11:12:27.549469: step 5606, loss 0.520853.
Train: 2018-08-02T11:12:27.768138: step 5607, loss 0.512455.
Train: 2018-08-02T11:12:27.986861: step 5608, loss 0.629208.
Train: 2018-08-02T11:12:28.205566: step 5609, loss 0.545717.
Train: 2018-08-02T11:12:28.424261: step 5610, loss 0.570771.
Test: 2018-08-02T11:12:29.580213: step 5610, loss 0.549031.
Train: 2018-08-02T11:12:29.798943: step 5611, loss 0.587487.
Train: 2018-08-02T11:12:30.017610: step 5612, loss 0.579124.
Train: 2018-08-02T11:12:30.236339: step 5613, loss 0.537382.
Train: 2018-08-02T11:12:30.455038: step 5614, loss 0.520708.
Train: 2018-08-02T11:12:30.673733: step 5615, loss 0.562421.
Train: 2018-08-02T11:12:30.892436: step 5616, loss 0.512323.
Train: 2018-08-02T11:12:31.111134: step 5617, loss 0.579133.
Train: 2018-08-02T11:12:31.329833: step 5618, loss 0.646063.
Train: 2018-08-02T11:12:31.548502: step 5619, loss 0.595832.
Train: 2018-08-02T11:12:31.767231: step 5620, loss 0.537423.
Test: 2018-08-02T11:12:32.938801: step 5620, loss 0.548348.
Train: 2018-08-02T11:12:33.141903: step 5621, loss 0.604052.
Train: 2018-08-02T11:12:33.360578: step 5622, loss 0.579062.
Train: 2018-08-02T11:12:33.579301: step 5623, loss 0.587318.
Train: 2018-08-02T11:12:33.798006: step 5624, loss 0.521223.
Train: 2018-08-02T11:12:34.016704: step 5625, loss 0.562517.
Train: 2018-08-02T11:12:34.235403: step 5626, loss 0.54608.
Train: 2018-08-02T11:12:34.454102: step 5627, loss 0.521464.
Train: 2018-08-02T11:12:34.672797: step 5628, loss 0.562547.
Train: 2018-08-02T11:12:34.891499: step 5629, loss 0.488622.
Train: 2018-08-02T11:12:35.110198: step 5630, loss 0.611891.
Test: 2018-08-02T11:12:36.281768: step 5630, loss 0.548595.
Train: 2018-08-02T11:12:36.484847: step 5631, loss 0.521372.
Train: 2018-08-02T11:12:36.703544: step 5632, loss 0.60371.
Train: 2018-08-02T11:12:36.922274: step 5633, loss 0.579072.
Train: 2018-08-02T11:12:37.140976: step 5634, loss 0.537782.
Train: 2018-08-02T11:12:37.359671: step 5635, loss 0.579024.
Train: 2018-08-02T11:12:37.578370: step 5636, loss 0.603694.
Train: 2018-08-02T11:12:37.797069: step 5637, loss 0.546074.
Train: 2018-08-02T11:12:38.015769: step 5638, loss 0.6036.
Train: 2018-08-02T11:12:38.234466: step 5639, loss 0.537964.
Train: 2018-08-02T11:12:38.453166: step 5640, loss 0.55437.
Test: 2018-08-02T11:12:39.624735: step 5640, loss 0.547593.
Train: 2018-08-02T11:12:39.827848: step 5641, loss 0.521646.
Train: 2018-08-02T11:12:40.046537: step 5642, loss 0.546183.
Train: 2018-08-02T11:12:40.265235: step 5643, loss 0.529751.
Train: 2018-08-02T11:12:40.483934: step 5644, loss 0.488527.
Train: 2018-08-02T11:12:40.702609: step 5645, loss 0.488048.
Train: 2018-08-02T11:12:40.921333: step 5646, loss 0.58836.
Train: 2018-08-02T11:12:41.140039: step 5647, loss 0.520238.
Train: 2018-08-02T11:12:41.358731: step 5648, loss 0.605145.
Train: 2018-08-02T11:12:41.577433: step 5649, loss 0.614807.
Train: 2018-08-02T11:12:41.796133: step 5650, loss 0.62066.
Test: 2018-08-02T11:12:42.967703: step 5650, loss 0.548264.
Train: 2018-08-02T11:12:43.170805: step 5651, loss 0.512417.
Train: 2018-08-02T11:12:43.389510: step 5652, loss 0.545836.
Train: 2018-08-02T11:12:43.608208: step 5653, loss 0.520827.
Train: 2018-08-02T11:12:43.826876: step 5654, loss 0.58743.
Train: 2018-08-02T11:12:44.045606: step 5655, loss 0.579101.
Train: 2018-08-02T11:12:44.264304: step 5656, loss 0.579098.
Train: 2018-08-02T11:12:44.482973: step 5657, loss 0.604076.
Train: 2018-08-02T11:12:44.701703: step 5658, loss 0.529189.
Train: 2018-08-02T11:12:44.920401: step 5659, loss 0.595682.
Train: 2018-08-02T11:12:45.139069: step 5660, loss 0.579052.
Test: 2018-08-02T11:12:46.310670: step 5660, loss 0.547891.
Train: 2018-08-02T11:12:46.513778: step 5661, loss 0.620425.
Train: 2018-08-02T11:12:46.732474: step 5662, loss 0.529496.
Train: 2018-08-02T11:12:46.951176: step 5663, loss 0.513117.
Train: 2018-08-02T11:12:47.169874: step 5664, loss 0.504938.
Train: 2018-08-02T11:12:47.388574: step 5665, loss 0.57899.
Train: 2018-08-02T11:12:47.607271: step 5666, loss 0.570757.
Train: 2018-08-02T11:12:47.825972: step 5667, loss 0.496678.
Train: 2018-08-02T11:12:48.044638: step 5668, loss 0.603724.
Train: 2018-08-02T11:12:48.263367: step 5669, loss 0.595486.
Train: 2018-08-02T11:12:48.482063: step 5670, loss 0.537806.
Test: 2018-08-02T11:12:49.638016: step 5670, loss 0.548763.
Train: 2018-08-02T11:12:49.856739: step 5671, loss 0.537809.
Train: 2018-08-02T11:12:50.075438: step 5672, loss 0.562516.
Train: 2018-08-02T11:12:50.294112: step 5673, loss 0.537782.
Train: 2018-08-02T11:12:50.512836: step 5674, loss 0.587257.
Train: 2018-08-02T11:12:50.731540: step 5675, loss 0.521248.
Train: 2018-08-02T11:12:50.950239: step 5676, loss 0.537724.
Train: 2018-08-02T11:12:51.168938: step 5677, loss 0.562483.
Train: 2018-08-02T11:12:51.387608: step 5678, loss 0.603881.
Train: 2018-08-02T11:12:51.606335: step 5679, loss 0.570761.
Train: 2018-08-02T11:12:51.825028: step 5680, loss 0.562477.
Test: 2018-08-02T11:12:52.996604: step 5680, loss 0.549225.
Train: 2018-08-02T11:12:53.199683: step 5681, loss 0.487981.
Train: 2018-08-02T11:12:53.418381: step 5682, loss 0.537599.
Train: 2018-08-02T11:12:53.637079: step 5683, loss 0.579076.
Train: 2018-08-02T11:12:53.855779: step 5684, loss 0.554135.
Train: 2018-08-02T11:12:54.074478: step 5685, loss 0.570772.
Train: 2018-08-02T11:12:54.293205: step 5686, loss 0.579085.
Train: 2018-08-02T11:12:54.511904: step 5687, loss 0.562427.
Train: 2018-08-02T11:12:54.730603: step 5688, loss 0.570759.
Train: 2018-08-02T11:12:54.949273: step 5689, loss 0.570774.
Train: 2018-08-02T11:12:55.167972: step 5690, loss 0.587387.
Test: 2018-08-02T11:12:56.323950: step 5690, loss 0.548375.
Train: 2018-08-02T11:12:56.542673: step 5691, loss 0.562463.
Train: 2018-08-02T11:12:56.761378: step 5692, loss 0.537573.
Train: 2018-08-02T11:12:56.980046: step 5693, loss 0.520979.
Train: 2018-08-02T11:12:57.198776: step 5694, loss 0.587349.
Train: 2018-08-02T11:12:57.417475: step 5695, loss 0.52099.
Train: 2018-08-02T11:12:57.636168: step 5696, loss 0.545876.
Train: 2018-08-02T11:12:57.854872: step 5697, loss 0.603988.
Train: 2018-08-02T11:12:58.073540: step 5698, loss 0.512635.
Train: 2018-08-02T11:12:58.292269: step 5699, loss 0.554124.
Train: 2018-08-02T11:12:58.510940: step 5700, loss 0.54582.
Test: 2018-08-02T11:12:59.681927: step 5700, loss 0.548151.
Train: 2018-08-02T11:13:00.558560: step 5701, loss 0.545789.
Train: 2018-08-02T11:13:00.808702: step 5702, loss 0.570776.
Train: 2018-08-02T11:13:01.048538: step 5703, loss 0.56243.
Train: 2018-08-02T11:13:01.272837: step 5704, loss 0.545727.
Train: 2018-08-02T11:13:01.509166: step 5705, loss 0.587461.
Train: 2018-08-02T11:13:01.722105: step 5706, loss 0.570756.
Train: 2018-08-02T11:13:01.958164: step 5707, loss 0.570756.
Train: 2018-08-02T11:13:02.209918: step 5708, loss 0.587468.
Train: 2018-08-02T11:13:02.444240: step 5709, loss 0.529125.
Train: 2018-08-02T11:13:02.678562: step 5710, loss 0.595744.
Test: 2018-08-02T11:13:03.850161: step 5710, loss 0.550279.
Train: 2018-08-02T11:13:04.053237: step 5711, loss 0.520875.
Train: 2018-08-02T11:13:04.271962: step 5712, loss 0.604006.
Train: 2018-08-02T11:13:04.506281: step 5713, loss 0.545853.
Train: 2018-08-02T11:13:04.724955: step 5714, loss 0.57905.
Train: 2018-08-02T11:13:04.943688: step 5715, loss 0.562471.
Train: 2018-08-02T11:13:05.162383: step 5716, loss 0.512866.
Train: 2018-08-02T11:13:05.381077: step 5717, loss 0.587298.
Train: 2018-08-02T11:13:05.599775: step 5718, loss 0.554224.
Train: 2018-08-02T11:13:05.818448: step 5719, loss 0.529452.
Train: 2018-08-02T11:13:06.037178: step 5720, loss 0.562498.
Test: 2018-08-02T11:13:07.271234: step 5720, loss 0.549451.
Train: 2018-08-02T11:13:07.505585: step 5721, loss 0.636867.
Train: 2018-08-02T11:13:07.724252: step 5722, loss 0.603756.
Train: 2018-08-02T11:13:07.942952: step 5723, loss 0.620131.
Train: 2018-08-02T11:13:08.161651: step 5724, loss 0.562564.
Train: 2018-08-02T11:13:08.380351: step 5725, loss 0.57894.
Train: 2018-08-02T11:13:08.599048: step 5726, loss 0.627775.
Train: 2018-08-02T11:13:08.817746: step 5727, loss 0.570795.
Train: 2018-08-02T11:13:09.036448: step 5728, loss 0.570814.
Train: 2018-08-02T11:13:09.255175: step 5729, loss 0.554766.
Train: 2018-08-02T11:13:09.473868: step 5730, loss 0.506794.
Test: 2018-08-02T11:13:10.645444: step 5730, loss 0.550992.
Train: 2018-08-02T11:13:10.848552: step 5731, loss 0.578861.
Train: 2018-08-02T11:13:11.067251: step 5732, loss 0.594822.
Train: 2018-08-02T11:13:11.301542: step 5733, loss 0.578859.
Train: 2018-08-02T11:13:11.535860: step 5734, loss 0.634469.
Train: 2018-08-02T11:13:11.754584: step 5735, loss 0.563031.
Train: 2018-08-02T11:13:11.988879: step 5736, loss 0.547308.
Train: 2018-08-02T11:13:12.191981: step 5737, loss 0.610349.
Train: 2018-08-02T11:13:12.379437: step 5738, loss 0.479496.
Train: 2018-08-02T11:13:12.598112: step 5739, loss 0.500453.
Train: 2018-08-02T11:13:12.816811: step 5740, loss 0.571018.
Test: 2018-08-02T11:13:13.988411: step 5740, loss 0.550731.
Train: 2018-08-02T11:13:14.191490: step 5741, loss 0.53955.
Train: 2018-08-02T11:13:14.410187: step 5742, loss 0.539469.
Train: 2018-08-02T11:13:14.644509: step 5743, loss 0.55516.
Train: 2018-08-02T11:13:14.863206: step 5744, loss 0.547171.
Train: 2018-08-02T11:13:15.081905: step 5745, loss 0.515304.
Train: 2018-08-02T11:13:15.300603: step 5746, loss 0.515037.
Train: 2018-08-02T11:13:15.519304: step 5747, loss 0.546785.
Train: 2018-08-02T11:13:15.738002: step 5748, loss 0.603048.
Train: 2018-08-02T11:13:15.956700: step 5749, loss 0.498001.
Train: 2018-08-02T11:13:16.175426: step 5750, loss 0.53005.
Test: 2018-08-02T11:13:17.347000: step 5750, loss 0.548322.
Train: 2018-08-02T11:13:17.565699: step 5751, loss 0.578977.
Train: 2018-08-02T11:13:17.800022: step 5752, loss 0.562428.
Train: 2018-08-02T11:13:18.018717: step 5753, loss 0.612642.
Train: 2018-08-02T11:13:18.253064: step 5754, loss 0.587305.
Train: 2018-08-02T11:13:18.471767: step 5755, loss 0.628608.
Train: 2018-08-02T11:13:18.690437: step 5756, loss 0.562469.
Train: 2018-08-02T11:13:18.909159: step 5757, loss 0.603722.
Train: 2018-08-02T11:13:19.127863: step 5758, loss 0.578988.
Train: 2018-08-02T11:13:19.362179: step 5759, loss 0.652881.
Train: 2018-08-02T11:13:19.596476: step 5760, loss 0.562586.
Test: 2018-08-02T11:13:20.768074: step 5760, loss 0.548635.
Train: 2018-08-02T11:13:20.986802: step 5761, loss 0.562624.
Train: 2018-08-02T11:13:21.205501: step 5762, loss 0.595161.
Train: 2018-08-02T11:13:21.424206: step 5763, loss 0.570799.
Train: 2018-08-02T11:13:21.642899: step 5764, loss 0.554677.
Train: 2018-08-02T11:13:21.861598: step 5765, loss 0.570829.
Train: 2018-08-02T11:13:22.080296: step 5766, loss 0.594928.
Train: 2018-08-02T11:13:22.298990: step 5767, loss 0.562864.
Train: 2018-08-02T11:13:22.533316: step 5768, loss 0.60279.
Train: 2018-08-02T11:13:22.767605: step 5769, loss 0.578857.
Train: 2018-08-02T11:13:22.986305: step 5770, loss 0.570935.
Test: 2018-08-02T11:13:24.157905: step 5770, loss 0.549508.
Train: 2018-08-02T11:13:24.376604: step 5771, loss 0.563061.
Train: 2018-08-02T11:13:24.595303: step 5772, loss 0.500079.
Train: 2018-08-02T11:13:24.814002: step 5773, loss 0.476478.
Train: 2018-08-02T11:13:25.032730: step 5774, loss 0.555192.
Train: 2018-08-02T11:13:25.251399: step 5775, loss 0.586758.
Train: 2018-08-02T11:13:25.470124: step 5776, loss 0.626351.
Train: 2018-08-02T11:13:25.688797: step 5777, loss 0.491795.
Train: 2018-08-02T11:13:25.907526: step 5778, loss 0.539247.
Train: 2018-08-02T11:13:26.126194: step 5779, loss 0.539119.
Train: 2018-08-02T11:13:26.344893: step 5780, loss 0.554921.
Test: 2018-08-02T11:13:27.516493: step 5780, loss 0.54877.
Train: 2018-08-02T11:13:27.719595: step 5781, loss 0.490885.
Train: 2018-08-02T11:13:27.938300: step 5782, loss 0.57887.
Train: 2018-08-02T11:13:28.156999: step 5783, loss 0.562746.
Train: 2018-08-02T11:13:28.375692: step 5784, loss 0.635559.
Train: 2018-08-02T11:13:28.594394: step 5785, loss 0.570786.
Train: 2018-08-02T11:13:28.813095: step 5786, loss 0.497747.
Train: 2018-08-02T11:13:29.031764: step 5787, loss 0.587033.
Train: 2018-08-02T11:13:29.250462: step 5788, loss 0.570833.
Train: 2018-08-02T11:13:29.500403: step 5789, loss 0.513528.
Train: 2018-08-02T11:13:29.719103: step 5790, loss 0.554262.
Test: 2018-08-02T11:13:30.937567: step 5790, loss 0.548385.
Train: 2018-08-02T11:13:31.156266: step 5791, loss 0.57063.
Train: 2018-08-02T11:13:31.374965: step 5792, loss 0.521002.
Train: 2018-08-02T11:13:31.593664: step 5793, loss 0.513372.
Train: 2018-08-02T11:13:31.796741: step 5794, loss 0.638868.
Train: 2018-08-02T11:13:32.015465: step 5795, loss 0.586962.
Train: 2018-08-02T11:13:32.234139: step 5796, loss 0.571476.
Train: 2018-08-02T11:13:32.452839: step 5797, loss 0.654021.
Train: 2018-08-02T11:13:32.671564: step 5798, loss 0.537597.
Train: 2018-08-02T11:13:32.890235: step 5799, loss 0.529555.
Train: 2018-08-02T11:13:33.108933: step 5800, loss 0.471973.
Test: 2018-08-02T11:13:34.311777: step 5800, loss 0.547606.
Train: 2018-08-02T11:13:35.358426: step 5801, loss 0.521303.
Train: 2018-08-02T11:13:35.577130: step 5802, loss 0.537692.
Train: 2018-08-02T11:13:35.795805: step 5803, loss 0.628745.
Train: 2018-08-02T11:13:36.014503: step 5804, loss 0.504406.
Train: 2018-08-02T11:13:36.233203: step 5805, loss 0.570783.
Train: 2018-08-02T11:13:36.451931: step 5806, loss 0.512447.
Train: 2018-08-02T11:13:36.670599: step 5807, loss 0.554066.
Train: 2018-08-02T11:13:36.904920: step 5808, loss 0.50376.
Train: 2018-08-02T11:13:37.107998: step 5809, loss 0.503505.
Train: 2018-08-02T11:13:37.326728: step 5810, loss 0.562452.
Test: 2018-08-02T11:13:38.498297: step 5810, loss 0.54793.
Train: 2018-08-02T11:13:38.701375: step 5811, loss 0.511387.
Train: 2018-08-02T11:13:38.920105: step 5812, loss 0.570268.
Train: 2018-08-02T11:13:39.138802: step 5813, loss 0.570803.
Train: 2018-08-02T11:13:39.357470: step 5814, loss 0.56073.
Train: 2018-08-02T11:13:39.576199: step 5815, loss 0.579225.
Train: 2018-08-02T11:13:39.794893: step 5816, loss 0.57268.
Train: 2018-08-02T11:13:40.013594: step 5817, loss 0.528717.
Train: 2018-08-02T11:13:40.232296: step 5818, loss 0.517466.
Train: 2018-08-02T11:13:40.450995: step 5819, loss 0.5362.
Train: 2018-08-02T11:13:40.669694: step 5820, loss 0.527699.
Test: 2018-08-02T11:13:41.856885: step 5820, loss 0.548442.
Train: 2018-08-02T11:13:42.059993: step 5821, loss 0.670911.
Train: 2018-08-02T11:13:42.278694: step 5822, loss 0.52696.
Train: 2018-08-02T11:13:42.497394: step 5823, loss 0.580448.
Train: 2018-08-02T11:13:42.716059: step 5824, loss 0.613616.
Train: 2018-08-02T11:13:42.934788: step 5825, loss 0.579249.
Train: 2018-08-02T11:13:43.153489: step 5826, loss 0.544919.
Train: 2018-08-02T11:13:43.372186: step 5827, loss 0.520294.
Train: 2018-08-02T11:13:43.590880: step 5828, loss 0.553897.
Train: 2018-08-02T11:13:43.809584: step 5829, loss 0.621512.
Train: 2018-08-02T11:13:44.028283: step 5830, loss 0.528798.
Test: 2018-08-02T11:13:45.199852: step 5830, loss 0.547644.
Train: 2018-08-02T11:13:45.402930: step 5831, loss 0.604264.
Train: 2018-08-02T11:13:45.621629: step 5832, loss 0.562417.
Train: 2018-08-02T11:13:45.840327: step 5833, loss 0.604073.
Train: 2018-08-02T11:13:46.074682: step 5834, loss 0.595637.
Train: 2018-08-02T11:13:46.293377: step 5835, loss 0.504667.
Train: 2018-08-02T11:13:46.512075: step 5836, loss 0.537784.
Train: 2018-08-02T11:13:46.730774: step 5837, loss 0.64484.
Train: 2018-08-02T11:13:46.949473: step 5838, loss 0.570783.
Train: 2018-08-02T11:13:47.168167: step 5839, loss 0.562604.
Train: 2018-08-02T11:13:47.386865: step 5840, loss 0.505608.
Test: 2018-08-02T11:13:48.558441: step 5840, loss 0.549438.
Train: 2018-08-02T11:13:48.777140: step 5841, loss 0.562651.
Train: 2018-08-02T11:13:48.995864: step 5842, loss 0.530166.
Train: 2018-08-02T11:13:49.214577: step 5843, loss 0.538308.
Train: 2018-08-02T11:13:49.448857: step 5844, loss 0.619525.
Train: 2018-08-02T11:13:49.651979: step 5845, loss 0.578903.
Train: 2018-08-02T11:13:49.870634: step 5846, loss 0.522169.
Train: 2018-08-02T11:13:50.089333: step 5847, loss 0.514077.
Train: 2018-08-02T11:13:50.308062: step 5848, loss 0.522126.
Train: 2018-08-02T11:13:50.526730: step 5849, loss 0.595167.
Train: 2018-08-02T11:13:50.745458: step 5850, loss 0.603321.
Test: 2018-08-02T11:13:51.917029: step 5850, loss 0.548313.
Train: 2018-08-02T11:13:52.135758: step 5851, loss 0.570781.
Train: 2018-08-02T11:13:52.370074: step 5852, loss 0.56265.
Train: 2018-08-02T11:13:52.604400: step 5853, loss 0.587044.
Train: 2018-08-02T11:13:52.807470: step 5854, loss 0.538289.
Train: 2018-08-02T11:13:53.026146: step 5855, loss 0.6114.
Train: 2018-08-02T11:13:53.244874: step 5856, loss 0.570797.
Train: 2018-08-02T11:13:53.463543: step 5857, loss 0.546482.
Train: 2018-08-02T11:13:53.682266: step 5858, loss 0.562709.
Train: 2018-08-02T11:13:53.900965: step 5859, loss 0.659777.
Train: 2018-08-02T11:13:54.119670: step 5860, loss 0.627263.
Test: 2018-08-02T11:13:55.306861: step 5860, loss 0.549207.
Train: 2018-08-02T11:13:55.509968: step 5861, loss 0.619073.
Train: 2018-08-02T11:13:55.728662: step 5862, loss 0.578891.
Train: 2018-08-02T11:13:55.947360: step 5863, loss 0.539084.
Train: 2018-08-02T11:13:56.166067: step 5864, loss 0.570924.
Train: 2018-08-02T11:13:56.384734: step 5865, loss 0.531453.
Train: 2018-08-02T11:13:56.603462: step 5866, loss 0.673478.
Train: 2018-08-02T11:13:56.822131: step 5867, loss 0.523922.
Train: 2018-08-02T11:13:57.040854: step 5868, loss 0.547562.
Train: 2018-08-02T11:13:57.259528: step 5869, loss 0.524209.
Train: 2018-08-02T11:13:57.478259: step 5870, loss 0.516417.
Test: 2018-08-02T11:13:58.665449: step 5870, loss 0.549474.
Train: 2018-08-02T11:13:58.868554: step 5871, loss 0.571068.
Train: 2018-08-02T11:13:59.087225: step 5872, loss 0.555404.
Train: 2018-08-02T11:13:59.305924: step 5873, loss 0.618061.
Train: 2018-08-02T11:13:59.524656: step 5874, loss 0.594541.
Train: 2018-08-02T11:13:59.743351: step 5875, loss 0.555376.
Train: 2018-08-02T11:13:59.962050: step 5876, loss 0.586705.
Train: 2018-08-02T11:14:00.180745: step 5877, loss 0.516266.
Train: 2018-08-02T11:14:00.399443: step 5878, loss 0.539699.
Train: 2018-08-02T11:14:00.618118: step 5879, loss 0.625955.
Train: 2018-08-02T11:14:00.836846: step 5880, loss 0.610282.
Test: 2018-08-02T11:14:02.008416: step 5880, loss 0.549365.
Train: 2018-08-02T11:14:02.227147: step 5881, loss 0.508261.
Train: 2018-08-02T11:14:02.445844: step 5882, loss 0.586733.
Train: 2018-08-02T11:14:02.664513: step 5883, loss 0.531728.
Train: 2018-08-02T11:14:02.883212: step 5884, loss 0.51589.
Train: 2018-08-02T11:14:03.101934: step 5885, loss 0.602546.
Train: 2018-08-02T11:14:03.320610: step 5886, loss 0.53929.
Train: 2018-08-02T11:14:03.554954: step 5887, loss 0.578884.
Train: 2018-08-02T11:14:03.789281: step 5888, loss 0.531065.
Train: 2018-08-02T11:14:03.961115: step 5889, loss 0.562796.
Train: 2018-08-02T11:14:04.179807: step 5890, loss 0.579066.
Test: 2018-08-02T11:14:05.367005: step 5890, loss 0.549236.
Train: 2018-08-02T11:14:05.581119: step 5891, loss 0.643222.
Train: 2018-08-02T11:14:05.799793: step 5892, loss 0.530245.
Train: 2018-08-02T11:14:06.018524: step 5893, loss 0.498787.
Train: 2018-08-02T11:14:06.237208: step 5894, loss 0.49819.
Train: 2018-08-02T11:14:06.455889: step 5895, loss 0.652719.
Train: 2018-08-02T11:14:06.690210: step 5896, loss 0.553606.
Train: 2018-08-02T11:14:06.908932: step 5897, loss 0.594945.
Train: 2018-08-02T11:14:07.127632: step 5898, loss 0.578991.
Train: 2018-08-02T11:14:07.346336: step 5899, loss 0.562288.
Train: 2018-08-02T11:14:07.580651: step 5900, loss 0.56286.
Test: 2018-08-02T11:14:08.752226: step 5900, loss 0.547893.
Train: 2018-08-02T11:14:09.658264: step 5901, loss 0.65283.
Train: 2018-08-02T11:14:09.876989: step 5902, loss 0.579004.
Train: 2018-08-02T11:14:10.095691: step 5903, loss 0.514205.
Train: 2018-08-02T11:14:10.330006: step 5904, loss 0.627798.
Train: 2018-08-02T11:14:10.548713: step 5905, loss 0.579178.
Train: 2018-08-02T11:14:10.767410: step 5906, loss 0.539162.
Train: 2018-08-02T11:14:10.986108: step 5907, loss 0.586704.
Train: 2018-08-02T11:14:11.220423: step 5908, loss 0.491561.
Train: 2018-08-02T11:14:11.439127: step 5909, loss 0.555009.
Train: 2018-08-02T11:14:11.657826: step 5910, loss 0.539147.
Test: 2018-08-02T11:14:12.829396: step 5910, loss 0.549129.
Train: 2018-08-02T11:14:13.048128: step 5911, loss 0.539103.
Train: 2018-08-02T11:14:13.266820: step 5912, loss 0.523099.
Train: 2018-08-02T11:14:13.485522: step 5913, loss 0.586851.
Train: 2018-08-02T11:14:13.704222: step 5914, loss 0.530837.
Train: 2018-08-02T11:14:13.922892: step 5915, loss 0.562817.
Train: 2018-08-02T11:14:14.141619: step 5916, loss 0.586923.
Train: 2018-08-02T11:14:14.375935: step 5917, loss 0.667549.
Train: 2018-08-02T11:14:14.594608: step 5918, loss 0.578876.
Train: 2018-08-02T11:14:14.813337: step 5919, loss 0.578874.
Train: 2018-08-02T11:14:15.032030: step 5920, loss 0.586907.
Test: 2018-08-02T11:14:16.187985: step 5920, loss 0.549989.
Train: 2018-08-02T11:14:16.406684: step 5921, loss 0.530749.
Train: 2018-08-02T11:14:16.641004: step 5922, loss 0.594893.
Train: 2018-08-02T11:14:16.859702: step 5923, loss 0.514836.
Train: 2018-08-02T11:14:17.078432: step 5924, loss 0.562856.
Train: 2018-08-02T11:14:17.297133: step 5925, loss 0.562853.
Train: 2018-08-02T11:14:17.515798: step 5926, loss 0.602889.
Train: 2018-08-02T11:14:17.734528: step 5927, loss 0.602872.
Train: 2018-08-02T11:14:17.953227: step 5928, loss 0.546899.
Train: 2018-08-02T11:14:18.171928: step 5929, loss 0.538939.
Train: 2018-08-02T11:14:18.406240: step 5930, loss 0.546925.
Test: 2018-08-02T11:14:19.593437: step 5930, loss 0.549729.
Train: 2018-08-02T11:14:19.796547: step 5931, loss 0.530935.
Train: 2018-08-02T11:14:20.015247: step 5932, loss 0.570865.
Train: 2018-08-02T11:14:20.249564: step 5933, loss 0.498793.
Train: 2018-08-02T11:14:20.483884: step 5934, loss 0.554785.
Train: 2018-08-02T11:14:20.702582: step 5935, loss 0.554726.
Train: 2018-08-02T11:14:20.921284: step 5936, loss 0.457814.
Train: 2018-08-02T11:14:21.139981: step 5937, loss 0.570791.
Train: 2018-08-02T11:14:21.358679: step 5938, loss 0.554484.
Train: 2018-08-02T11:14:21.577374: step 5939, loss 0.644379.
Train: 2018-08-02T11:14:21.796071: step 5940, loss 0.529799.
Test: 2018-08-02T11:14:22.983268: step 5940, loss 0.549024.
Train: 2018-08-02T11:14:23.201991: step 5941, loss 0.587183.
Train: 2018-08-02T11:14:23.420692: step 5942, loss 0.570758.
Train: 2018-08-02T11:14:23.639389: step 5943, loss 0.513137.
Train: 2018-08-02T11:14:23.858064: step 5944, loss 0.587253.
Train: 2018-08-02T11:14:24.076762: step 5945, loss 0.570756.
Train: 2018-08-02T11:14:24.373598: step 5946, loss 0.554227.
Train: 2018-08-02T11:14:24.607915: step 5947, loss 0.603844.
Train: 2018-08-02T11:14:24.826605: step 5948, loss 0.570756.
Train: 2018-08-02T11:14:25.045318: step 5949, loss 0.54596.
Train: 2018-08-02T11:14:25.279631: step 5950, loss 0.587285.
Test: 2018-08-02T11:14:26.451206: step 5950, loss 0.550608.
Train: 2018-08-02T11:14:26.669905: step 5951, loss 0.562498.
Train: 2018-08-02T11:14:26.888605: step 5952, loss 0.48824.
Train: 2018-08-02T11:14:27.122949: step 5953, loss 0.620314.
Train: 2018-08-02T11:14:27.341648: step 5954, loss 0.562501.
Train: 2018-08-02T11:14:27.560348: step 5955, loss 0.562505.
Train: 2018-08-02T11:14:27.794676: step 5956, loss 0.562512.
Train: 2018-08-02T11:14:28.028968: step 5957, loss 0.562515.
Train: 2018-08-02T11:14:28.232069: step 5958, loss 0.504858.
Train: 2018-08-02T11:14:28.450769: step 5959, loss 0.653194.
Train: 2018-08-02T11:14:28.669469: step 5960, loss 0.521366.
Test: 2018-08-02T11:14:29.856659: step 5960, loss 0.549166.
Train: 2018-08-02T11:14:30.059769: step 5961, loss 0.587214.
Train: 2018-08-02T11:14:30.278465: step 5962, loss 0.620082.
Train: 2018-08-02T11:14:30.512755: step 5963, loss 0.496945.
Train: 2018-08-02T11:14:30.731484: step 5964, loss 0.570763.
Train: 2018-08-02T11:14:30.950153: step 5965, loss 0.488846.
Train: 2018-08-02T11:14:31.168884: step 5966, loss 0.619964.
Train: 2018-08-02T11:14:31.387550: step 5967, loss 0.546172.
Train: 2018-08-02T11:14:31.606279: step 5968, loss 0.636339.
Train: 2018-08-02T11:14:31.824978: step 5969, loss 0.562583.
Train: 2018-08-02T11:14:32.043680: step 5970, loss 0.578939.
Test: 2018-08-02T11:14:33.230869: step 5970, loss 0.5475.
Train: 2018-08-02T11:14:33.465214: step 5971, loss 0.554466.
Train: 2018-08-02T11:14:33.668299: step 5972, loss 0.587063.
Train: 2018-08-02T11:14:33.886998: step 5973, loss 0.55453.
Train: 2018-08-02T11:14:34.105665: step 5974, loss 0.554561.
Train: 2018-08-02T11:14:34.324393: step 5975, loss 0.56269.
Train: 2018-08-02T11:14:34.543090: step 5976, loss 0.554604.
Train: 2018-08-02T11:14:34.761793: step 5977, loss 0.554618.
Train: 2018-08-02T11:14:34.980460: step 5978, loss 0.619336.
Train: 2018-08-02T11:14:35.199188: step 5979, loss 0.578886.
Train: 2018-08-02T11:14:35.417889: step 5980, loss 0.546636.
Test: 2018-08-02T11:14:36.605079: step 5980, loss 0.549328.
Train: 2018-08-02T11:14:36.823778: step 5981, loss 0.60303.
Train: 2018-08-02T11:14:37.073743: step 5982, loss 0.474414.
Train: 2018-08-02T11:14:37.292450: step 5983, loss 0.522601.
Train: 2018-08-02T11:14:37.511143: step 5984, loss 0.611078.
Train: 2018-08-02T11:14:37.729841: step 5985, loss 0.586929.
Train: 2018-08-02T11:14:37.964137: step 5986, loss 0.570827.
Train: 2018-08-02T11:14:38.182834: step 5987, loss 0.554737.
Train: 2018-08-02T11:14:38.417169: step 5988, loss 0.53865.
Train: 2018-08-02T11:14:38.635854: step 5989, loss 0.562777.
Train: 2018-08-02T11:14:38.854553: step 5990, loss 0.6272.
Test: 2018-08-02T11:14:40.041774: step 5990, loss 0.550087.
Train: 2018-08-02T11:14:40.244863: step 5991, loss 0.554738.
Train: 2018-08-02T11:14:40.463576: step 5992, loss 0.554752.
Train: 2018-08-02T11:14:40.697896: step 5993, loss 0.611025.
Train: 2018-08-02T11:14:40.916569: step 5994, loss 0.578869.
Train: 2018-08-02T11:14:41.135297: step 5995, loss 0.522763.
Train: 2018-08-02T11:14:41.369619: step 5996, loss 0.546817.
Train: 2018-08-02T11:14:41.588317: step 5997, loss 0.578866.
Train: 2018-08-02T11:14:41.806985: step 5998, loss 0.610917.
Train: 2018-08-02T11:14:42.025717: step 5999, loss 0.59487.
Train: 2018-08-02T11:14:42.260038: step 6000, loss 0.570873.
Test: 2018-08-02T11:14:43.431605: step 6000, loss 0.549773.
Train: 2018-08-02T11:14:44.306431: step 6001, loss 0.610762.
Train: 2018-08-02T11:14:44.525127: step 6002, loss 0.578859.
Train: 2018-08-02T11:14:44.743822: step 6003, loss 0.547123.
Train: 2018-08-02T11:14:44.962527: step 6004, loss 0.5551.
Train: 2018-08-02T11:14:45.181222: step 6005, loss 0.499765.
Train: 2018-08-02T11:14:45.399947: step 6006, loss 0.563031.
Train: 2018-08-02T11:14:45.618592: step 6007, loss 0.61846.
Train: 2018-08-02T11:14:45.837293: step 6008, loss 0.539283.
Train: 2018-08-02T11:14:46.055990: step 6009, loss 0.52344.
Train: 2018-08-02T11:14:46.274714: step 6010, loss 0.523364.
Test: 2018-08-02T11:14:47.461912: step 6010, loss 0.549877.
Train: 2018-08-02T11:14:47.665000: step 6011, loss 0.562965.
Train: 2018-08-02T11:14:47.883712: step 6012, loss 0.523107.
Train: 2018-08-02T11:14:48.102416: step 6013, loss 0.634789.
Train: 2018-08-02T11:14:48.321111: step 6014, loss 0.618858.
Train: 2018-08-02T11:14:48.555408: step 6015, loss 0.514887.
Train: 2018-08-02T11:14:48.774135: step 6016, loss 0.58687.
Train: 2018-08-02T11:14:48.992833: step 6017, loss 0.538819.
Train: 2018-08-02T11:14:49.211503: step 6018, loss 0.514723.
Train: 2018-08-02T11:14:49.430201: step 6019, loss 0.554763.
Train: 2018-08-02T11:14:49.648932: step 6020, loss 0.530549.
Test: 2018-08-02T11:14:50.820500: step 6020, loss 0.548867.
Train: 2018-08-02T11:14:51.039199: step 6021, loss 0.546573.
Train: 2018-08-02T11:14:51.257923: step 6022, loss 0.595105.
Train: 2018-08-02T11:14:51.507864: step 6023, loss 0.513952.
Train: 2018-08-02T11:14:51.726537: step 6024, loss 0.521916.
Train: 2018-08-02T11:14:51.960884: step 6025, loss 0.562595.
Train: 2018-08-02T11:14:52.179558: step 6026, loss 0.570762.
Train: 2018-08-02T11:14:52.413905: step 6027, loss 0.521423.
Train: 2018-08-02T11:14:52.632605: step 6028, loss 0.546006.
Train: 2018-08-02T11:14:52.851304: step 6029, loss 0.487981.
Train: 2018-08-02T11:14:53.070004: step 6030, loss 0.495918.
Test: 2018-08-02T11:14:54.257194: step 6030, loss 0.548829.
Train: 2018-08-02T11:14:54.475894: step 6031, loss 0.587497.
Train: 2018-08-02T11:14:54.694623: step 6032, loss 0.537193.
Train: 2018-08-02T11:14:54.928948: step 6033, loss 0.570803.
Train: 2018-08-02T11:14:55.147611: step 6034, loss 0.604672.
Train: 2018-08-02T11:14:55.366341: step 6035, loss 0.460596.
Train: 2018-08-02T11:14:55.585010: step 6036, loss 0.579367.
Train: 2018-08-02T11:14:55.803709: step 6037, loss 0.57941.
Train: 2018-08-02T11:14:56.022436: step 6038, loss 0.553784.
Train: 2018-08-02T11:14:56.256758: step 6039, loss 0.579469.
Train: 2018-08-02T11:14:56.444213: step 6040, loss 0.397707.
Test: 2018-08-02T11:14:57.631405: step 6040, loss 0.547165.
Train: 2018-08-02T11:14:57.834483: step 6041, loss 0.613991.
Train: 2018-08-02T11:14:58.053182: step 6042, loss 0.570968.
Train: 2018-08-02T11:14:58.287534: step 6043, loss 0.588268.
Train: 2018-08-02T11:14:58.506231: step 6044, loss 0.570988.
Train: 2018-08-02T11:14:58.724929: step 6045, loss 0.622876.
Train: 2018-08-02T11:14:58.959250: step 6046, loss 0.64005.
Train: 2018-08-02T11:14:59.193540: step 6047, loss 0.648397.
Train: 2018-08-02T11:14:59.427860: step 6048, loss 0.536648.
Train: 2018-08-02T11:14:59.646588: step 6049, loss 0.528236.
Train: 2018-08-02T11:14:59.880879: step 6050, loss 0.579341.
Test: 2018-08-02T11:15:01.068100: step 6050, loss 0.54841.
Train: 2018-08-02T11:15:01.302451: step 6051, loss 0.596223.
Train: 2018-08-02T11:15:01.521153: step 6052, loss 0.511782.
Train: 2018-08-02T11:15:01.739843: step 6053, loss 0.528753.
Train: 2018-08-02T11:15:01.958547: step 6054, loss 0.512047.
Train: 2018-08-02T11:15:02.177241: step 6055, loss 0.528859.
Train: 2018-08-02T11:15:02.411560: step 6056, loss 0.512098.
Train: 2018-08-02T11:15:02.630234: step 6057, loss 0.570782.
Train: 2018-08-02T11:15:02.848933: step 6058, loss 0.570784.
Train: 2018-08-02T11:15:03.067662: step 6059, loss 0.570784.
Train: 2018-08-02T11:15:03.286363: step 6060, loss 0.49525.
Test: 2018-08-02T11:15:04.473552: step 6060, loss 0.547967.
Train: 2018-08-02T11:15:04.676663: step 6061, loss 0.579189.
Train: 2018-08-02T11:15:04.895361: step 6062, loss 0.545572.
Train: 2018-08-02T11:15:05.114057: step 6063, loss 0.486679.
Train: 2018-08-02T11:15:05.332755: step 6064, loss 0.553943.
Train: 2018-08-02T11:15:05.551426: step 6065, loss 0.596144.
Train: 2018-08-02T11:15:05.770153: step 6066, loss 0.55391.
Train: 2018-08-02T11:15:05.988853: step 6067, loss 0.553902.
Train: 2018-08-02T11:15:06.207546: step 6068, loss 0.621598.
Train: 2018-08-02T11:15:06.426251: step 6069, loss 0.57927.
Train: 2018-08-02T11:15:06.644950: step 6070, loss 0.570808.
Test: 2018-08-02T11:15:07.832141: step 6070, loss 0.547128.
Train: 2018-08-02T11:15:08.050871: step 6071, loss 0.520221.
Train: 2018-08-02T11:15:08.253917: step 6072, loss 0.587646.
Train: 2018-08-02T11:15:08.472647: step 6073, loss 0.545554.
Train: 2018-08-02T11:15:08.691314: step 6074, loss 0.612811.
Train: 2018-08-02T11:15:08.925665: step 6075, loss 0.579167.
Train: 2018-08-02T11:15:09.144367: step 6076, loss 0.637697.
Train: 2018-08-02T11:15:09.363058: step 6077, loss 0.545769.
Train: 2018-08-02T11:15:09.581732: step 6078, loss 0.603972.
Train: 2018-08-02T11:15:09.800441: step 6079, loss 0.496334.
Train: 2018-08-02T11:15:10.019157: step 6080, loss 0.537755.
Test: 2018-08-02T11:15:11.206351: step 6080, loss 0.54952.
Train: 2018-08-02T11:15:11.409429: step 6081, loss 0.587233.
Train: 2018-08-02T11:15:11.643748: step 6082, loss 0.529653.
Train: 2018-08-02T11:15:11.862453: step 6083, loss 0.611821.
Train: 2018-08-02T11:15:12.096794: step 6084, loss 0.587152.
Train: 2018-08-02T11:15:12.315491: step 6085, loss 0.554421.
Train: 2018-08-02T11:15:12.534189: step 6086, loss 0.578929.
Train: 2018-08-02T11:15:12.752864: step 6087, loss 0.578917.
Train: 2018-08-02T11:15:12.971588: step 6088, loss 0.522084.
Train: 2018-08-02T11:15:13.190294: step 6089, loss 0.489719.
Train: 2018-08-02T11:15:13.408992: step 6090, loss 0.611355.
Test: 2018-08-02T11:15:14.580561: step 6090, loss 0.548395.
Train: 2018-08-02T11:15:14.799292: step 6091, loss 0.497815.
Train: 2018-08-02T11:15:15.017989: step 6092, loss 0.595141.
Train: 2018-08-02T11:15:15.236688: step 6093, loss 0.562668.
Train: 2018-08-02T11:15:15.470977: step 6094, loss 0.635758.
Train: 2018-08-02T11:15:15.689676: step 6095, loss 0.562683.
Train: 2018-08-02T11:15:15.908374: step 6096, loss 0.50601.
Train: 2018-08-02T11:15:16.127106: step 6097, loss 0.611297.
Train: 2018-08-02T11:15:16.345772: step 6098, loss 0.627447.
Train: 2018-08-02T11:15:16.564501: step 6099, loss 0.562738.
Train: 2018-08-02T11:15:16.783194: step 6100, loss 0.546653.
Test: 2018-08-02T11:15:17.970392: step 6100, loss 0.54972.
Train: 2018-08-02T11:15:18.907672: step 6101, loss 0.57083.
Train: 2018-08-02T11:15:19.141993: step 6102, loss 0.594938.
Train: 2018-08-02T11:15:19.345100: step 6103, loss 0.578867.
Train: 2018-08-02T11:15:19.563799: step 6104, loss 0.570863.
Train: 2018-08-02T11:15:19.782493: step 6105, loss 0.570876.
Train: 2018-08-02T11:15:20.001196: step 6106, loss 0.570889.
Train: 2018-08-02T11:15:20.219896: step 6107, loss 0.55499.
Train: 2018-08-02T11:15:20.454218: step 6108, loss 0.586804.
Train: 2018-08-02T11:15:20.672914: step 6109, loss 0.555056.
Train: 2018-08-02T11:15:20.891613: step 6110, loss 0.586787.
Test: 2018-08-02T11:15:22.063183: step 6110, loss 0.549448.
Train: 2018-08-02T11:15:22.266260: step 6111, loss 0.578852.
Train: 2018-08-02T11:15:22.484959: step 6112, loss 0.523534.
Train: 2018-08-02T11:15:22.719280: step 6113, loss 0.491807.
Train: 2018-08-02T11:15:22.938003: step 6114, loss 0.523144.
Train: 2018-08-02T11:15:23.156705: step 6115, loss 0.554367.
Train: 2018-08-02T11:15:23.375401: step 6116, loss 0.545454.
Train: 2018-08-02T11:15:23.594100: step 6117, loss 0.552225.
Train: 2018-08-02T11:15:23.812799: step 6118, loss 0.537693.
Train: 2018-08-02T11:15:24.047119: step 6119, loss 0.611009.
Train: 2018-08-02T11:15:24.281448: step 6120, loss 0.595855.
Test: 2018-08-02T11:15:25.468636: step 6120, loss 0.546813.
Train: 2018-08-02T11:15:25.671738: step 6121, loss 0.559868.
Train: 2018-08-02T11:15:25.890413: step 6122, loss 0.538409.
Train: 2018-08-02T11:15:26.124734: step 6123, loss 0.619963.
Train: 2018-08-02T11:15:26.343461: step 6124, loss 0.555526.
Train: 2018-08-02T11:15:26.577752: step 6125, loss 0.505055.
Train: 2018-08-02T11:15:26.796451: step 6126, loss 0.578425.
Train: 2018-08-02T11:15:27.015182: step 6127, loss 0.569896.
Train: 2018-08-02T11:15:27.249470: step 6128, loss 0.562688.
Train: 2018-08-02T11:15:27.468192: step 6129, loss 0.474442.
Train: 2018-08-02T11:15:27.686897: step 6130, loss 0.587021.
Test: 2018-08-02T11:15:28.874089: step 6130, loss 0.548484.
Train: 2018-08-02T11:15:29.077166: step 6131, loss 0.506626.
Train: 2018-08-02T11:15:29.295870: step 6132, loss 0.578864.
Train: 2018-08-02T11:15:29.514592: step 6133, loss 0.554787.
Train: 2018-08-02T11:15:29.764535: step 6134, loss 0.586963.
Train: 2018-08-02T11:15:29.983235: step 6135, loss 0.595014.
Train: 2018-08-02T11:15:30.201932: step 6136, loss 0.498234.
Train: 2018-08-02T11:15:30.451875: step 6137, loss 0.627369.
Train: 2018-08-02T11:15:30.670573: step 6138, loss 0.595053.
Train: 2018-08-02T11:15:30.889241: step 6139, loss 0.586965.
Train: 2018-08-02T11:15:31.123592: step 6140, loss 0.570813.
Test: 2018-08-02T11:15:32.310783: step 6140, loss 0.549293.
Train: 2018-08-02T11:15:32.529482: step 6141, loss 0.570818.
Train: 2018-08-02T11:15:32.748206: step 6142, loss 0.514451.
Train: 2018-08-02T11:15:32.966911: step 6143, loss 0.514434.
Train: 2018-08-02T11:15:33.185604: step 6144, loss 0.55468.
Train: 2018-08-02T11:15:33.419930: step 6145, loss 0.65161.
Train: 2018-08-02T11:15:33.638623: step 6146, loss 0.498137.
Train: 2018-08-02T11:15:33.857296: step 6147, loss 0.489976.
Train: 2018-08-02T11:15:34.076050: step 6148, loss 0.587004.
Train: 2018-08-02T11:15:34.294725: step 6149, loss 0.473348.
Train: 2018-08-02T11:15:34.513423: step 6150, loss 0.570775.
Test: 2018-08-02T11:15:35.684993: step 6150, loss 0.548556.
Train: 2018-08-02T11:15:35.903692: step 6151, loss 0.56259.
Train: 2018-08-02T11:15:36.138013: step 6152, loss 0.570762.
Train: 2018-08-02T11:15:36.356742: step 6153, loss 0.529656.
Train: 2018-08-02T11:15:36.575423: step 6154, loss 0.562513.
Train: 2018-08-02T11:15:36.794139: step 6155, loss 0.554227.
Train: 2018-08-02T11:15:37.028462: step 6156, loss 0.579041.
Train: 2018-08-02T11:15:37.247129: step 6157, loss 0.579056.
Train: 2018-08-02T11:15:37.481449: step 6158, loss 0.54584.
Train: 2018-08-02T11:15:37.715799: step 6159, loss 0.554128.
Train: 2018-08-02T11:15:37.934466: step 6160, loss 0.687333.
Test: 2018-08-02T11:15:39.121689: step 6160, loss 0.549526.
Train: 2018-08-02T11:15:39.340412: step 6161, loss 0.520895.
Train: 2018-08-02T11:15:39.559117: step 6162, loss 0.554152.
Train: 2018-08-02T11:15:39.777817: step 6163, loss 0.554162.
Train: 2018-08-02T11:15:39.996509: step 6164, loss 0.59564.
Train: 2018-08-02T11:15:40.215183: step 6165, loss 0.512774.
Train: 2018-08-02T11:15:40.433906: step 6166, loss 0.562475.
Train: 2018-08-02T11:15:40.652613: step 6167, loss 0.537634.
Train: 2018-08-02T11:15:40.871279: step 6168, loss 0.496206.
Train: 2018-08-02T11:15:41.090011: step 6169, loss 0.529269.
Train: 2018-08-02T11:15:41.308703: step 6170, loss 0.537496.
Test: 2018-08-02T11:15:42.495898: step 6170, loss 0.549657.
Train: 2018-08-02T11:15:42.714598: step 6171, loss 0.520746.
Train: 2018-08-02T11:15:42.933327: step 6172, loss 0.512239.
Train: 2018-08-02T11:15:43.152028: step 6173, loss 0.579177.
Train: 2018-08-02T11:15:43.386341: step 6174, loss 0.570794.
Train: 2018-08-02T11:15:43.605045: step 6175, loss 0.528629.
Train: 2018-08-02T11:15:43.839389: step 6176, loss 0.604641.
Train: 2018-08-02T11:15:44.058034: step 6177, loss 0.486161.
Train: 2018-08-02T11:15:44.276765: step 6178, loss 0.562349.
Train: 2018-08-02T11:15:44.495430: step 6179, loss 0.596364.
Train: 2018-08-02T11:15:44.714129: step 6180, loss 0.638955.
Test: 2018-08-02T11:15:45.901351: step 6180, loss 0.54736.
Train: 2018-08-02T11:15:46.120075: step 6181, loss 0.536834.
Train: 2018-08-02T11:15:46.338779: step 6182, loss 0.494361.
Train: 2018-08-02T11:15:46.557447: step 6183, loss 0.545336.
Train: 2018-08-02T11:15:46.776175: step 6184, loss 0.553832.
Train: 2018-08-02T11:15:46.994845: step 6185, loss 0.553825.
Train: 2018-08-02T11:15:47.213574: step 6186, loss 0.647557.
Train: 2018-08-02T11:15:47.432243: step 6187, loss 0.502784.
Train: 2018-08-02T11:15:47.650941: step 6188, loss 0.579356.
Train: 2018-08-02T11:15:47.869671: step 6189, loss 0.604838.
Train: 2018-08-02T11:15:48.103961: step 6190, loss 0.604761.
Test: 2018-08-02T11:15:49.291182: step 6190, loss 0.54726.
Train: 2018-08-02T11:15:49.463019: step 6191, loss 0.580403.
Train: 2018-08-02T11:15:49.681743: step 6192, loss 0.545507.
Train: 2018-08-02T11:15:49.900431: step 6193, loss 0.503518.
Train: 2018-08-02T11:15:50.119113: step 6194, loss 0.537191.
Train: 2018-08-02T11:15:50.337841: step 6195, loss 0.595962.
Train: 2018-08-02T11:15:50.556511: step 6196, loss 0.595918.
Train: 2018-08-02T11:15:50.775243: step 6197, loss 0.570772.
Train: 2018-08-02T11:15:50.993934: step 6198, loss 0.604131.
Train: 2018-08-02T11:15:51.212637: step 6199, loss 0.44604.
Train: 2018-08-02T11:15:51.446953: step 6200, loss 0.579072.
Test: 2018-08-02T11:15:52.618528: step 6200, loss 0.548392.
Train: 2018-08-02T11:15:53.462082: step 6201, loss 0.562454.
Train: 2018-08-02T11:15:53.680779: step 6202, loss 0.579058.
Train: 2018-08-02T11:15:53.899512: step 6203, loss 0.529312.
Train: 2018-08-02T11:15:54.133827: step 6204, loss 0.5459.
Train: 2018-08-02T11:15:54.368145: step 6205, loss 0.587328.
Train: 2018-08-02T11:15:54.586848: step 6206, loss 0.603874.
Train: 2018-08-02T11:15:54.805547: step 6207, loss 0.579022.
Train: 2018-08-02T11:15:55.024248: step 6208, loss 0.529507.
Train: 2018-08-02T11:15:55.242952: step 6209, loss 0.546034.
Train: 2018-08-02T11:15:55.461643: step 6210, loss 0.529577.
Test: 2018-08-02T11:15:56.648841: step 6210, loss 0.547057.
Train: 2018-08-02T11:15:56.851913: step 6211, loss 0.513096.
Train: 2018-08-02T11:15:57.070636: step 6212, loss 0.653227.
Train: 2018-08-02T11:15:57.289325: step 6213, loss 0.587234.
Train: 2018-08-02T11:15:57.508038: step 6214, loss 0.58721.
Train: 2018-08-02T11:15:57.726708: step 6215, loss 0.554343.
Train: 2018-08-02T11:15:57.945437: step 6216, loss 0.652717.
Train: 2018-08-02T11:15:58.164129: step 6217, loss 0.570771.
Train: 2018-08-02T11:15:58.382834: step 6218, loss 0.530095.
Train: 2018-08-02T11:15:58.601527: step 6219, loss 0.603262.
Train: 2018-08-02T11:15:58.820228: step 6220, loss 0.554616.
Test: 2018-08-02T11:16:00.023045: step 6220, loss 0.548888.
Train: 2018-08-02T11:16:00.226123: step 6221, loss 0.570812.
Train: 2018-08-02T11:16:00.444821: step 6222, loss 0.570825.
Train: 2018-08-02T11:16:00.663551: step 6223, loss 0.55477.
Train: 2018-08-02T11:16:00.882219: step 6224, loss 0.554811.
Train: 2018-08-02T11:16:01.100942: step 6225, loss 0.59488.
Train: 2018-08-02T11:16:01.319616: step 6226, loss 0.5389.
Train: 2018-08-02T11:16:01.538345: step 6227, loss 0.578861.
Train: 2018-08-02T11:16:01.757038: step 6228, loss 0.594809.
Train: 2018-08-02T11:16:01.991367: step 6229, loss 0.59478.
Train: 2018-08-02T11:16:02.210066: step 6230, loss 0.578858.
Test: 2018-08-02T11:16:03.397254: step 6230, loss 0.54923.
Train: 2018-08-02T11:16:03.600374: step 6231, loss 0.578859.
Train: 2018-08-02T11:16:03.819061: step 6232, loss 0.52352.
Train: 2018-08-02T11:16:04.037760: step 6233, loss 0.578862.
Train: 2018-08-02T11:16:04.256458: step 6234, loss 0.570973.
Train: 2018-08-02T11:16:04.490747: step 6235, loss 0.602512.
Train: 2018-08-02T11:16:04.709474: step 6236, loss 0.56313.
Train: 2018-08-02T11:16:04.928145: step 6237, loss 0.516003.
Train: 2018-08-02T11:16:05.162467: step 6238, loss 0.626029.
Train: 2018-08-02T11:16:05.365543: step 6239, loss 0.578873.
Train: 2018-08-02T11:16:05.584272: step 6240, loss 0.594556.
Test: 2018-08-02T11:16:06.771464: step 6240, loss 0.549799.
Train: 2018-08-02T11:16:06.974541: step 6241, loss 0.610185.
Train: 2018-08-02T11:16:07.193270: step 6242, loss 0.578889.
Train: 2018-08-02T11:16:07.411964: step 6243, loss 0.524395.
Train: 2018-08-02T11:16:07.630638: step 6244, loss 0.54001.
Train: 2018-08-02T11:16:07.849367: step 6245, loss 0.617794.
Train: 2018-08-02T11:16:08.068065: step 6246, loss 0.578908.
Train: 2018-08-02T11:16:08.286735: step 6247, loss 0.524597.
Train: 2018-08-02T11:16:08.505461: step 6248, loss 0.485782.
Train: 2018-08-02T11:16:08.724161: step 6249, loss 0.563342.
Train: 2018-08-02T11:16:08.942831: step 6250, loss 0.594491.
Test: 2018-08-02T11:16:10.130052: step 6250, loss 0.550036.
Train: 2018-08-02T11:16:10.348751: step 6251, loss 0.532015.
Train: 2018-08-02T11:16:10.567482: step 6252, loss 0.555384.
Train: 2018-08-02T11:16:10.786179: step 6253, loss 0.649545.
Train: 2018-08-02T11:16:11.020476: step 6254, loss 0.578871.
Train: 2018-08-02T11:16:11.239168: step 6255, loss 0.539586.
Train: 2018-08-02T11:16:11.457869: step 6256, loss 0.578869.
Train: 2018-08-02T11:16:11.676589: step 6257, loss 0.594606.
Train: 2018-08-02T11:16:11.910910: step 6258, loss 0.531653.
Train: 2018-08-02T11:16:12.129615: step 6259, loss 0.626126.
Train: 2018-08-02T11:16:12.363904: step 6260, loss 0.508015.
Test: 2018-08-02T11:16:13.535505: step 6260, loss 0.550485.
Train: 2018-08-02T11:16:13.738613: step 6261, loss 0.547341.
Train: 2018-08-02T11:16:13.957282: step 6262, loss 0.602543.
Train: 2018-08-02T11:16:14.175980: step 6263, loss 0.54727.
Train: 2018-08-02T11:16:14.394711: step 6264, loss 0.555141.
Train: 2018-08-02T11:16:14.613403: step 6265, loss 0.539274.
Train: 2018-08-02T11:16:14.832107: step 6266, loss 0.539195.
Train: 2018-08-02T11:16:15.050800: step 6267, loss 0.555001.
Train: 2018-08-02T11:16:15.269506: step 6268, loss 0.523047.
Train: 2018-08-02T11:16:15.488205: step 6269, loss 0.594864.
Train: 2018-08-02T11:16:15.706902: step 6270, loss 0.635004.
Test: 2018-08-02T11:16:16.878472: step 6270, loss 0.55053.
Train: 2018-08-02T11:16:17.097202: step 6271, loss 0.594915.
Train: 2018-08-02T11:16:17.315901: step 6272, loss 0.546784.
Train: 2018-08-02T11:16:17.534599: step 6273, loss 0.5548.
Train: 2018-08-02T11:16:17.753292: step 6274, loss 0.651109.
Train: 2018-08-02T11:16:17.971996: step 6275, loss 0.586879.
Train: 2018-08-02T11:16:18.190691: step 6276, loss 0.538874.
Train: 2018-08-02T11:16:18.409394: step 6277, loss 0.530928.
Train: 2018-08-02T11:16:18.628093: step 6278, loss 0.594838.
Train: 2018-08-02T11:16:18.846762: step 6279, loss 0.546932.
Train: 2018-08-02T11:16:19.065491: step 6280, loss 0.467128.
Test: 2018-08-02T11:16:20.252682: step 6280, loss 0.549321.
Train: 2018-08-02T11:16:20.455784: step 6281, loss 0.610861.
Train: 2018-08-02T11:16:20.674488: step 6282, loss 0.554842.
Train: 2018-08-02T11:16:20.893190: step 6283, loss 0.490679.
Train: 2018-08-02T11:16:21.111856: step 6284, loss 0.554752.
Train: 2018-08-02T11:16:21.346176: step 6285, loss 0.595009.
Train: 2018-08-02T11:16:21.549284: step 6286, loss 0.473861.
Train: 2018-08-02T11:16:21.767952: step 6287, loss 0.514021.
Train: 2018-08-02T11:16:21.986681: step 6288, loss 0.530037.
Train: 2018-08-02T11:16:22.205350: step 6289, loss 0.628081.
Train: 2018-08-02T11:16:22.424049: step 6290, loss 0.529702.
Test: 2018-08-02T11:16:23.611270: step 6290, loss 0.54838.
Train: 2018-08-02T11:16:23.814373: step 6291, loss 0.529563.
Train: 2018-08-02T11:16:24.033046: step 6292, loss 0.570756.
Train: 2018-08-02T11:16:24.251804: step 6293, loss 0.603934.
Train: 2018-08-02T11:16:24.517309: step 6294, loss 0.512604.
Train: 2018-08-02T11:16:24.736038: step 6295, loss 0.587422.
Train: 2018-08-02T11:16:24.970358: step 6296, loss 0.520711.
Train: 2018-08-02T11:16:25.189026: step 6297, loss 0.470432.
Train: 2018-08-02T11:16:25.407725: step 6298, loss 0.604364.
Train: 2018-08-02T11:16:25.626454: step 6299, loss 0.587625.
Train: 2018-08-02T11:16:25.845123: step 6300, loss 0.537087.
Test: 2018-08-02T11:16:27.032345: step 6300, loss 0.548459.
Train: 2018-08-02T11:16:27.829064: step 6301, loss 0.503264.
Train: 2018-08-02T11:16:28.047757: step 6302, loss 0.596219.
Train: 2018-08-02T11:16:28.266432: step 6303, loss 0.57083.
Train: 2018-08-02T11:16:28.485162: step 6304, loss 0.511431.
Train: 2018-08-02T11:16:28.719449: step 6305, loss 0.59635.
Train: 2018-08-02T11:16:28.938153: step 6306, loss 0.528317.
Train: 2018-08-02T11:16:29.156846: step 6307, loss 0.511246.
Train: 2018-08-02T11:16:29.375571: step 6308, loss 0.511147.
Train: 2018-08-02T11:16:29.594270: step 6309, loss 0.647874.
Train: 2018-08-02T11:16:29.812947: step 6310, loss 0.570892.
Test: 2018-08-02T11:16:31.000165: step 6310, loss 0.548642.
Train: 2018-08-02T11:16:31.218889: step 6311, loss 0.519568.
Train: 2018-08-02T11:16:31.437594: step 6312, loss 0.596567.
Train: 2018-08-02T11:16:31.656292: step 6313, loss 0.596548.
Train: 2018-08-02T11:16:31.874970: step 6314, loss 0.545257.
Train: 2018-08-02T11:16:32.109281: step 6315, loss 0.528218.
Train: 2018-08-02T11:16:32.328010: step 6316, loss 0.511187.
Train: 2018-08-02T11:16:32.546708: step 6317, loss 0.553811.
Train: 2018-08-02T11:16:32.765405: step 6318, loss 0.63912.
Train: 2018-08-02T11:16:32.984075: step 6319, loss 0.50272.
Train: 2018-08-02T11:16:33.218427: step 6320, loss 0.57937.
Test: 2018-08-02T11:16:34.405618: step 6320, loss 0.547551.
Train: 2018-08-02T11:16:34.608700: step 6321, loss 0.553838.
Train: 2018-08-02T11:16:34.827418: step 6322, loss 0.587842.
Train: 2018-08-02T11:16:35.046123: step 6323, loss 0.562349.
Train: 2018-08-02T11:16:35.280444: step 6324, loss 0.536938.
Train: 2018-08-02T11:16:35.499112: step 6325, loss 0.520043.
Train: 2018-08-02T11:16:35.717810: step 6326, loss 0.562358.
Train: 2018-08-02T11:16:35.936535: step 6327, loss 0.604647.
Train: 2018-08-02T11:16:36.155234: step 6328, loss 0.503248.
Train: 2018-08-02T11:16:36.373937: step 6329, loss 0.570808.
Train: 2018-08-02T11:16:36.592637: step 6330, loss 0.621439.
Test: 2018-08-02T11:16:37.764206: step 6330, loss 0.548309.
Train: 2018-08-02T11:16:37.967308: step 6331, loss 0.528686.
Train: 2018-08-02T11:16:38.186009: step 6332, loss 0.553968.
Train: 2018-08-02T11:16:38.420329: step 6333, loss 0.545578.
Train: 2018-08-02T11:16:38.639002: step 6334, loss 0.562388.
Train: 2018-08-02T11:16:38.857701: step 6335, loss 0.52883.
Train: 2018-08-02T11:16:39.076424: step 6336, loss 0.554003.
Train: 2018-08-02T11:16:39.295098: step 6337, loss 0.604338.
Train: 2018-08-02T11:16:39.513829: step 6338, loss 0.595917.
Train: 2018-08-02T11:16:39.732526: step 6339, loss 0.545683.
Train: 2018-08-02T11:16:39.966840: step 6340, loss 0.503961.
Test: 2018-08-02T11:16:41.138416: step 6340, loss 0.547321.
Train: 2018-08-02T11:16:41.341525: step 6341, loss 0.545719.
Train: 2018-08-02T11:16:41.528980: step 6342, loss 0.580233.
Train: 2018-08-02T11:16:41.747663: step 6343, loss 0.495641.
Train: 2018-08-02T11:16:41.966378: step 6344, loss 0.554059.
Train: 2018-08-02T11:16:42.185072: step 6345, loss 0.562409.
Train: 2018-08-02T11:16:42.403776: step 6346, loss 0.570775.
Train: 2018-08-02T11:16:42.622468: step 6347, loss 0.570776.
Train: 2018-08-02T11:16:42.841143: step 6348, loss 0.55403.
Train: 2018-08-02T11:16:43.059871: step 6349, loss 0.528908.
Train: 2018-08-02T11:16:43.278571: step 6350, loss 0.537262.
Test: 2018-08-02T11:16:44.465762: step 6350, loss 0.546841.
Train: 2018-08-02T11:16:44.668864: step 6351, loss 0.503681.
Train: 2018-08-02T11:16:44.903185: step 6352, loss 0.629624.
Train: 2018-08-02T11:16:45.121888: step 6353, loss 0.570789.
Train: 2018-08-02T11:16:45.340592: step 6354, loss 0.638033.
Train: 2018-08-02T11:16:45.559281: step 6355, loss 0.570782.
Train: 2018-08-02T11:16:45.777980: step 6356, loss 0.579147.
Train: 2018-08-02T11:16:45.996655: step 6357, loss 0.537365.
Train: 2018-08-02T11:16:46.215353: step 6358, loss 0.645794.
Train: 2018-08-02T11:16:46.449672: step 6359, loss 0.579066.
Train: 2018-08-02T11:16:46.683993: step 6360, loss 0.645243.
Test: 2018-08-02T11:16:47.855593: step 6360, loss 0.548397.
Train: 2018-08-02T11:16:48.074322: step 6361, loss 0.562525.
Train: 2018-08-02T11:16:48.293016: step 6362, loss 0.628111.
Train: 2018-08-02T11:16:48.527336: step 6363, loss 0.652216.
Train: 2018-08-02T11:16:48.746034: step 6364, loss 0.546557.
Train: 2018-08-02T11:16:48.964734: step 6365, loss 0.506585.
Train: 2018-08-02T11:16:49.183438: step 6366, loss 0.546874.
Train: 2018-08-02T11:16:49.402131: step 6367, loss 0.562919.
Train: 2018-08-02T11:16:49.620837: step 6368, loss 0.555019.
Train: 2018-08-02T11:16:49.855151: step 6369, loss 0.547146.
Train: 2018-08-02T11:16:50.058239: step 6370, loss 0.563028.
Test: 2018-08-02T11:16:51.229802: step 6370, loss 0.550391.
Train: 2018-08-02T11:16:51.448527: step 6371, loss 0.578861.
Train: 2018-08-02T11:16:51.667230: step 6372, loss 0.539386.
Train: 2018-08-02T11:16:51.885927: step 6373, loss 0.531513.
Train: 2018-08-02T11:16:52.120219: step 6374, loss 0.578862.
Train: 2018-08-02T11:16:52.323323: step 6375, loss 0.53937.
Train: 2018-08-02T11:16:52.557643: step 6376, loss 0.618392.
Train: 2018-08-02T11:16:52.760723: step 6377, loss 0.570957.
Train: 2018-08-02T11:16:52.979426: step 6378, loss 0.51565.
Train: 2018-08-02T11:16:53.198118: step 6379, loss 0.507671.
Train: 2018-08-02T11:16:53.416792: step 6380, loss 0.578859.
Test: 2018-08-02T11:16:54.604012: step 6380, loss 0.54988.
Train: 2018-08-02T11:16:54.807116: step 6381, loss 0.515292.
Train: 2018-08-02T11:16:55.025815: step 6382, loss 0.499153.
Train: 2018-08-02T11:16:55.244518: step 6383, loss 0.570858.
Train: 2018-08-02T11:16:55.463217: step 6384, loss 0.602986.
Train: 2018-08-02T11:16:55.681916: step 6385, loss 0.562761.
Train: 2018-08-02T11:16:55.900585: step 6386, loss 0.56273.
Train: 2018-08-02T11:16:56.134905: step 6387, loss 0.586992.
Train: 2018-08-02T11:16:56.338013: step 6388, loss 0.546468.
Train: 2018-08-02T11:16:56.572327: step 6389, loss 0.522053.
Train: 2018-08-02T11:16:56.806623: step 6390, loss 0.603349.
Test: 2018-08-02T11:16:57.978222: step 6390, loss 0.54769.
Train: 2018-08-02T11:16:58.212543: step 6391, loss 0.603388.
Train: 2018-08-02T11:16:58.446912: step 6392, loss 0.578929.
Train: 2018-08-02T11:16:58.665594: step 6393, loss 0.513697.
Train: 2018-08-02T11:16:58.884285: step 6394, loss 0.554448.
Train: 2018-08-02T11:16:59.118609: step 6395, loss 0.489062.
Train: 2018-08-02T11:16:59.337310: step 6396, loss 0.570763.
Train: 2018-08-02T11:16:59.556004: step 6397, loss 0.620024.
Train: 2018-08-02T11:16:59.774707: step 6398, loss 0.58719.
Train: 2018-08-02T11:16:59.993400: step 6399, loss 0.578974.
Train: 2018-08-02T11:17:00.212074: step 6400, loss 0.562549.
Test: 2018-08-02T11:17:01.399297: step 6400, loss 0.548281.
Train: 2018-08-02T11:17:02.211606: step 6401, loss 0.521519.
Train: 2018-08-02T11:17:02.430305: step 6402, loss 0.513284.
Train: 2018-08-02T11:17:02.649034: step 6403, loss 0.620101.
Train: 2018-08-02T11:17:02.883357: step 6404, loss 0.537863.
Train: 2018-08-02T11:17:03.102024: step 6405, loss 0.521388.
Train: 2018-08-02T11:17:03.320721: step 6406, loss 0.537798.
Train: 2018-08-02T11:17:03.539421: step 6407, loss 0.529484.
Train: 2018-08-02T11:17:03.758148: step 6408, loss 0.57903.
Train: 2018-08-02T11:17:03.976843: step 6409, loss 0.562471.
Train: 2018-08-02T11:17:04.195547: step 6410, loss 0.595652.
Test: 2018-08-02T11:17:05.382738: step 6410, loss 0.548977.
Train: 2018-08-02T11:17:05.585817: step 6411, loss 0.545856.
Train: 2018-08-02T11:17:05.804515: step 6412, loss 0.537535.
Train: 2018-08-02T11:17:06.034751: step 6413, loss 0.579076.
Train: 2018-08-02T11:17:06.253437: step 6414, loss 0.562443.
Train: 2018-08-02T11:17:06.487745: step 6415, loss 0.579083.
Train: 2018-08-02T11:17:06.722066: step 6416, loss 0.579081.
Train: 2018-08-02T11:17:06.940790: step 6417, loss 0.554133.
Train: 2018-08-02T11:17:07.159488: step 6418, loss 0.562451.
Train: 2018-08-02T11:17:07.378187: step 6419, loss 0.562455.
Train: 2018-08-02T11:17:07.596860: step 6420, loss 0.487763.
Test: 2018-08-02T11:17:08.784082: step 6420, loss 0.547428.
Train: 2018-08-02T11:17:09.002812: step 6421, loss 0.554143.
Train: 2018-08-02T11:17:09.221505: step 6422, loss 0.554127.
Train: 2018-08-02T11:17:09.455831: step 6423, loss 0.620719.
Train: 2018-08-02T11:17:09.690152: step 6424, loss 0.570762.
Train: 2018-08-02T11:17:09.908854: step 6425, loss 0.537498.
Train: 2018-08-02T11:17:10.143141: step 6426, loss 0.637273.
Train: 2018-08-02T11:17:10.361868: step 6427, loss 0.520973.
Train: 2018-08-02T11:17:10.580567: step 6428, loss 0.579047.
Train: 2018-08-02T11:17:10.799266: step 6429, loss 0.562478.
Train: 2018-08-02T11:17:11.033581: step 6430, loss 0.570756.
Test: 2018-08-02T11:17:12.205157: step 6430, loss 0.548329.
Train: 2018-08-02T11:17:12.423854: step 6431, loss 0.58727.
Train: 2018-08-02T11:17:12.642579: step 6432, loss 0.52955.
Train: 2018-08-02T11:17:12.861253: step 6433, loss 0.57899.
Train: 2018-08-02T11:17:13.079977: step 6434, loss 0.496761.
Train: 2018-08-02T11:17:13.298653: step 6435, loss 0.562534.
Train: 2018-08-02T11:17:13.517350: step 6436, loss 0.58721.
Train: 2018-08-02T11:17:13.736048: step 6437, loss 0.562536.
Train: 2018-08-02T11:17:13.970368: step 6438, loss 0.578978.
Train: 2018-08-02T11:17:14.173470: step 6439, loss 0.595397.
Train: 2018-08-02T11:17:14.392169: step 6440, loss 0.570762.
Test: 2018-08-02T11:17:15.579366: step 6440, loss 0.547776.
Train: 2018-08-02T11:17:15.798066: step 6441, loss 0.603511.
Train: 2018-08-02T11:17:16.016789: step 6442, loss 0.652436.
Train: 2018-08-02T11:17:16.235493: step 6443, loss 0.587045.
Train: 2018-08-02T11:17:16.454192: step 6444, loss 0.586989.
Train: 2018-08-02T11:17:16.672890: step 6445, loss 0.522477.
Train: 2018-08-02T11:17:16.891584: step 6446, loss 0.602967.
Train: 2018-08-02T11:17:17.110258: step 6447, loss 0.626878.
Train: 2018-08-02T11:17:17.344591: step 6448, loss 0.594786.
Train: 2018-08-02T11:17:17.578923: step 6449, loss 0.531319.
Train: 2018-08-02T11:17:17.797598: step 6450, loss 0.484125.
Test: 2018-08-02T11:17:18.984819: step 6450, loss 0.549739.
Train: 2018-08-02T11:17:19.203519: step 6451, loss 0.578864.
Train: 2018-08-02T11:17:19.422242: step 6452, loss 0.563112.
Train: 2018-08-02T11:17:19.640943: step 6453, loss 0.570998.
Train: 2018-08-02T11:17:19.859639: step 6454, loss 0.547416.
Train: 2018-08-02T11:17:20.078312: step 6455, loss 0.571007.
Train: 2018-08-02T11:17:20.312664: step 6456, loss 0.602453.
Train: 2018-08-02T11:17:20.547006: step 6457, loss 0.563164.
Train: 2018-08-02T11:17:20.750030: step 6458, loss 0.539629.
Train: 2018-08-02T11:17:20.968754: step 6459, loss 0.516066.
Train: 2018-08-02T11:17:21.187453: step 6460, loss 0.508097.
Test: 2018-08-02T11:17:22.374651: step 6460, loss 0.549915.
Train: 2018-08-02T11:17:22.577753: step 6461, loss 0.610415.
Train: 2018-08-02T11:17:22.796457: step 6462, loss 0.531453.
Train: 2018-08-02T11:17:23.015153: step 6463, loss 0.539253.
Train: 2018-08-02T11:17:23.233824: step 6464, loss 0.491462.
Train: 2018-08-02T11:17:23.468175: step 6465, loss 0.538951.
Train: 2018-08-02T11:17:23.702495: step 6466, loss 0.594909.
Train: 2018-08-02T11:17:23.921188: step 6467, loss 0.522526.
Train: 2018-08-02T11:17:24.139887: step 6468, loss 0.530381.
Train: 2018-08-02T11:17:24.374207: step 6469, loss 0.530172.
Train: 2018-08-02T11:17:24.592881: step 6470, loss 0.554444.
Test: 2018-08-02T11:17:25.780103: step 6470, loss 0.54924.
Train: 2018-08-02T11:17:25.983180: step 6471, loss 0.603568.
Train: 2018-08-02T11:17:26.201910: step 6472, loss 0.587214.
Train: 2018-08-02T11:17:26.436230: step 6473, loss 0.504786.
Train: 2018-08-02T11:17:26.639302: step 6474, loss 0.620398.
Train: 2018-08-02T11:17:26.857976: step 6475, loss 0.545898.
Train: 2018-08-02T11:17:27.092321: step 6476, loss 0.504357.
Train: 2018-08-02T11:17:27.311019: step 6477, loss 0.570762.
Train: 2018-08-02T11:17:27.529717: step 6478, loss 0.554084.
Train: 2018-08-02T11:17:27.748391: step 6479, loss 0.495548.
Train: 2018-08-02T11:17:27.967092: step 6480, loss 0.537241.
Test: 2018-08-02T11:17:29.154312: step 6480, loss 0.548524.
Train: 2018-08-02T11:17:29.373011: step 6481, loss 0.604442.
Train: 2018-08-02T11:17:29.591736: step 6482, loss 0.545515.
Train: 2018-08-02T11:17:29.826031: step 6483, loss 0.579253.
Train: 2018-08-02T11:17:30.049300: step 6484, loss 0.587721.
Train: 2018-08-02T11:17:30.268032: step 6485, loss 0.52008.
Train: 2018-08-02T11:17:30.486731: step 6486, loss 0.613142.
Train: 2018-08-02T11:17:30.721048: step 6487, loss 0.604662.
Train: 2018-08-02T11:17:30.939747: step 6488, loss 0.562363.
Train: 2018-08-02T11:17:31.158449: step 6489, loss 0.579239.
Train: 2018-08-02T11:17:31.377141: step 6490, loss 0.495026.
Test: 2018-08-02T11:17:32.564336: step 6490, loss 0.54813.
Train: 2018-08-02T11:17:32.767440: step 6491, loss 0.587624.
Train: 2018-08-02T11:17:32.986138: step 6492, loss 0.537164.
Train: 2018-08-02T11:17:33.173598: step 6493, loss 0.47277.
Train: 2018-08-02T11:17:33.392291: step 6494, loss 0.579205.
Train: 2018-08-02T11:17:33.610967: step 6495, loss 0.553958.
Train: 2018-08-02T11:17:33.845312: step 6496, loss 0.579221.
Train: 2018-08-02T11:17:34.063984: step 6497, loss 0.579221.
Train: 2018-08-02T11:17:34.298305: step 6498, loss 0.587634.
Train: 2018-08-02T11:17:34.501410: step 6499, loss 0.478289.
Train: 2018-08-02T11:17:34.735735: step 6500, loss 0.495062.
Test: 2018-08-02T11:17:35.907303: step 6500, loss 0.548873.
Train: 2018-08-02T11:17:36.750855: step 6501, loss 0.570801.
Train: 2018-08-02T11:17:36.969554: step 6502, loss 0.562365.
Train: 2018-08-02T11:17:37.188255: step 6503, loss 0.528557.
Train: 2018-08-02T11:17:37.406983: step 6504, loss 0.630068.
Train: 2018-08-02T11:17:37.641272: step 6505, loss 0.630046.
Train: 2018-08-02T11:17:37.859971: step 6506, loss 0.55392.
Train: 2018-08-02T11:17:38.078700: step 6507, loss 0.638236.
Train: 2018-08-02T11:17:38.297368: step 6508, loss 0.528785.
Train: 2018-08-02T11:17:38.516102: step 6509, loss 0.562399.
Train: 2018-08-02T11:17:38.734766: step 6510, loss 0.528977.
Test: 2018-08-02T11:17:39.921988: step 6510, loss 0.549252.
Train: 2018-08-02T11:17:40.140712: step 6511, loss 0.445579.
Train: 2018-08-02T11:17:40.375007: step 6512, loss 0.537349.
Train: 2018-08-02T11:17:40.578110: step 6513, loss 0.520569.
Train: 2018-08-02T11:17:40.796784: step 6514, loss 0.579165.
Train: 2018-08-02T11:17:41.015511: step 6515, loss 0.486817.
Train: 2018-08-02T11:17:41.234219: step 6516, loss 0.671837.
Train: 2018-08-02T11:17:41.468502: step 6517, loss 0.495021.
Train: 2018-08-02T11:17:41.687231: step 6518, loss 0.56237.
Train: 2018-08-02T11:17:41.905928: step 6519, loss 0.587679.
Train: 2018-08-02T11:17:42.124598: step 6520, loss 0.562365.
Test: 2018-08-02T11:17:43.311819: step 6520, loss 0.547303.
Train: 2018-08-02T11:17:43.530519: step 6521, loss 0.613001.
Train: 2018-08-02T11:17:43.764873: step 6522, loss 0.570801.
Train: 2018-08-02T11:17:43.983561: step 6523, loss 0.638099.
Train: 2018-08-02T11:17:44.217857: step 6524, loss 0.587552.
Train: 2018-08-02T11:17:44.436556: step 6525, loss 0.487217.
Train: 2018-08-02T11:17:44.655284: step 6526, loss 0.595789.
Train: 2018-08-02T11:17:44.873987: step 6527, loss 0.570762.
Train: 2018-08-02T11:17:45.092677: step 6528, loss 0.587359.
Train: 2018-08-02T11:17:45.311383: step 6529, loss 0.5211.
Train: 2018-08-02T11:17:45.545702: step 6530, loss 0.554233.
Test: 2018-08-02T11:17:46.717271: step 6530, loss 0.548349.
Train: 2018-08-02T11:17:46.935996: step 6531, loss 0.496508.
Train: 2018-08-02T11:17:47.154669: step 6532, loss 0.570756.
Train: 2018-08-02T11:17:47.373368: step 6533, loss 0.595507.
Train: 2018-08-02T11:17:47.592092: step 6534, loss 0.537787.
Train: 2018-08-02T11:17:47.810791: step 6535, loss 0.513078.
Train: 2018-08-02T11:17:48.045087: step 6536, loss 0.570756.
Train: 2018-08-02T11:17:48.279412: step 6537, loss 0.570756.
Train: 2018-08-02T11:17:48.513727: step 6538, loss 0.546001.
Train: 2018-08-02T11:17:48.732425: step 6539, loss 0.545987.
Train: 2018-08-02T11:17:48.951154: step 6540, loss 0.620326.
Test: 2018-08-02T11:17:50.138346: step 6540, loss 0.549093.
Train: 2018-08-02T11:17:50.341447: step 6541, loss 0.579015.
Train: 2018-08-02T11:17:50.560121: step 6542, loss 0.570756.
Train: 2018-08-02T11:17:50.778851: step 6543, loss 0.611937.
Train: 2018-08-02T11:17:51.028787: step 6544, loss 0.55433.
Train: 2018-08-02T11:17:51.247489: step 6545, loss 0.562558.
Train: 2018-08-02T11:17:51.466186: step 6546, loss 0.644445.
Train: 2018-08-02T11:17:51.684883: step 6547, loss 0.529986.
Train: 2018-08-02T11:17:51.903558: step 6548, loss 0.513815.
Train: 2018-08-02T11:17:52.122287: step 6549, loss 0.481353.
Train: 2018-08-02T11:17:52.340955: step 6550, loss 0.554503.
Test: 2018-08-02T11:17:53.528177: step 6550, loss 0.548272.
Train: 2018-08-02T11:17:53.731263: step 6551, loss 0.619661.
Train: 2018-08-02T11:17:53.949983: step 6552, loss 0.562633.
Train: 2018-08-02T11:17:54.168654: step 6553, loss 0.595205.
Train: 2018-08-02T11:17:54.387376: step 6554, loss 0.473173.
Train: 2018-08-02T11:17:54.606075: step 6555, loss 0.55449.
Train: 2018-08-02T11:17:54.824749: step 6556, loss 0.546311.
Train: 2018-08-02T11:17:55.059098: step 6557, loss 0.627937.
Train: 2018-08-02T11:17:55.277797: step 6558, loss 0.521778.
Train: 2018-08-02T11:17:55.496490: step 6559, loss 0.521736.
Train: 2018-08-02T11:17:55.715167: step 6560, loss 0.578951.
Test: 2018-08-02T11:17:56.902387: step 6560, loss 0.550014.
Train: 2018-08-02T11:17:57.121086: step 6561, loss 0.570763.
Train: 2018-08-02T11:17:57.339784: step 6562, loss 0.570761.
Train: 2018-08-02T11:17:57.574130: step 6563, loss 0.513321.
Train: 2018-08-02T11:17:57.777207: step 6564, loss 0.5872.
Train: 2018-08-02T11:17:57.995905: step 6565, loss 0.554308.
Train: 2018-08-02T11:17:58.214611: step 6566, loss 0.65309.
Train: 2018-08-02T11:17:58.433306: step 6567, loss 0.595416.
Train: 2018-08-02T11:17:58.652002: step 6568, loss 0.628175.
Train: 2018-08-02T11:17:58.870705: step 6569, loss 0.570771.
Train: 2018-08-02T11:17:59.089374: step 6570, loss 0.538186.
Test: 2018-08-02T11:18:00.276597: step 6570, loss 0.549455.
Train: 2018-08-02T11:18:00.495320: step 6571, loss 0.67645.
Train: 2018-08-02T11:18:00.714018: step 6572, loss 0.562714.
Train: 2018-08-02T11:18:00.932723: step 6573, loss 0.554715.
Train: 2018-08-02T11:18:01.151393: step 6574, loss 0.594918.
Train: 2018-08-02T11:18:01.370118: step 6575, loss 0.586855.
Train: 2018-08-02T11:18:01.588814: step 6576, loss 0.554978.
Train: 2018-08-02T11:18:01.807519: step 6577, loss 0.570925.
Train: 2018-08-02T11:18:02.026218: step 6578, loss 0.563043.
Train: 2018-08-02T11:18:02.244914: step 6579, loss 0.626191.
Train: 2018-08-02T11:18:02.463615: step 6580, loss 0.594587.
Test: 2018-08-02T11:18:03.650806: step 6580, loss 0.551238.
Train: 2018-08-02T11:18:03.853898: step 6581, loss 0.524096.
Train: 2018-08-02T11:18:04.072613: step 6582, loss 0.571081.
Train: 2018-08-02T11:18:04.306928: step 6583, loss 0.524363.
Train: 2018-08-02T11:18:04.541225: step 6584, loss 0.571114.
Train: 2018-08-02T11:18:04.759956: step 6585, loss 0.539996.
Train: 2018-08-02T11:18:04.994276: step 6586, loss 0.532198.
Train: 2018-08-02T11:18:05.212943: step 6587, loss 0.5711.
Train: 2018-08-02T11:18:05.431669: step 6588, loss 0.57889.
Train: 2018-08-02T11:18:05.650368: step 6589, loss 0.586699.
Train: 2018-08-02T11:18:05.869062: step 6590, loss 0.524176.
Test: 2018-08-02T11:18:07.056259: step 6590, loss 0.549787.
Train: 2018-08-02T11:18:07.259370: step 6591, loss 0.602366.
Train: 2018-08-02T11:18:07.478059: step 6592, loss 0.594547.
Train: 2018-08-02T11:18:07.696735: step 6593, loss 0.555373.
Train: 2018-08-02T11:18:07.915433: step 6594, loss 0.571038.
Train: 2018-08-02T11:18:08.149754: step 6595, loss 0.59456.
Train: 2018-08-02T11:18:08.352860: step 6596, loss 0.586716.
Train: 2018-08-02T11:18:08.587180: step 6597, loss 0.586713.
Train: 2018-08-02T11:18:08.821471: step 6598, loss 0.547563.
Train: 2018-08-02T11:18:09.024557: step 6599, loss 0.594538.
Train: 2018-08-02T11:18:09.243275: step 6600, loss 0.524131.
Test: 2018-08-02T11:18:10.430469: step 6600, loss 0.549256.
Train: 2018-08-02T11:18:11.305294: step 6601, loss 0.563226.
Train: 2018-08-02T11:18:11.539584: step 6602, loss 0.563214.
Train: 2018-08-02T11:18:11.758283: step 6603, loss 0.571034.
Train: 2018-08-02T11:18:11.977015: step 6604, loss 0.523957.
Train: 2018-08-02T11:18:12.195707: step 6605, loss 0.492406.
Train: 2018-08-02T11:18:12.414380: step 6606, loss 0.594613.
Train: 2018-08-02T11:18:12.633102: step 6607, loss 0.63429.
Train: 2018-08-02T11:18:12.851776: step 6608, loss 0.570923.
Train: 2018-08-02T11:18:13.070505: step 6609, loss 0.562998.
Train: 2018-08-02T11:18:13.289204: step 6610, loss 0.50742.
Test: 2018-08-02T11:18:14.460775: step 6610, loss 0.550016.
Train: 2018-08-02T11:18:14.663852: step 6611, loss 0.555056.
Train: 2018-08-02T11:18:14.898203: step 6612, loss 0.546927.
Train: 2018-08-02T11:18:15.116901: step 6613, loss 0.538609.
Train: 2018-08-02T11:18:15.335571: step 6614, loss 0.498311.
Train: 2018-08-02T11:18:15.554269: step 6615, loss 0.579335.
Train: 2018-08-02T11:18:15.772969: step 6616, loss 0.537596.
Train: 2018-08-02T11:18:15.991691: step 6617, loss 0.512513.
Train: 2018-08-02T11:18:16.210395: step 6618, loss 0.590383.
Train: 2018-08-02T11:18:16.429089: step 6619, loss 0.523352.
Train: 2018-08-02T11:18:16.647793: step 6620, loss 0.596507.
Test: 2018-08-02T11:18:17.834985: step 6620, loss 0.547499.
Train: 2018-08-02T11:18:18.038062: step 6621, loss 0.570798.
Train: 2018-08-02T11:18:18.256795: step 6622, loss 0.620972.
Train: 2018-08-02T11:18:18.475461: step 6623, loss 0.529449.
Train: 2018-08-02T11:18:18.694184: step 6624, loss 0.59569.
Train: 2018-08-02T11:18:18.912884: step 6625, loss 0.59551.
Train: 2018-08-02T11:18:19.147214: step 6626, loss 0.504889.
Train: 2018-08-02T11:18:19.365906: step 6627, loss 0.529567.
Train: 2018-08-02T11:18:19.584608: step 6628, loss 0.513021.
Train: 2018-08-02T11:18:19.803304: step 6629, loss 0.554224.
Train: 2018-08-02T11:18:20.022002: step 6630, loss 0.487928.
Test: 2018-08-02T11:18:21.209194: step 6630, loss 0.548376.
Train: 2018-08-02T11:18:21.412272: step 6631, loss 0.545825.
Train: 2018-08-02T11:18:21.630995: step 6632, loss 0.504046.
Train: 2018-08-02T11:18:21.865291: step 6633, loss 0.537275.
Train: 2018-08-02T11:18:22.083990: step 6634, loss 0.537147.
Train: 2018-08-02T11:18:22.302714: step 6635, loss 0.511685.
Train: 2018-08-02T11:18:22.521387: step 6636, loss 0.570836.
Train: 2018-08-02T11:18:22.740088: step 6637, loss 0.536785.
Train: 2018-08-02T11:18:22.974437: step 6638, loss 0.59654.
Train: 2018-08-02T11:18:23.208727: step 6639, loss 0.622335.
Train: 2018-08-02T11:18:23.427456: step 6640, loss 0.545182.
Test: 2018-08-02T11:18:24.614647: step 6640, loss 0.547014.
Train: 2018-08-02T11:18:24.848992: step 6641, loss 0.519423.
Train: 2018-08-02T11:18:25.130176: step 6642, loss 0.605301.
Train: 2018-08-02T11:18:25.333229: step 6643, loss 0.570928.
Train: 2018-08-02T11:18:25.520716: step 6644, loss 0.562335.
Train: 2018-08-02T11:18:25.739408: step 6645, loss 0.570918.
Train: 2018-08-02T11:18:25.958108: step 6646, loss 0.613776.
Train: 2018-08-02T11:18:26.176812: step 6647, loss 0.605104.
Train: 2018-08-02T11:18:26.395506: step 6648, loss 0.460031.
Train: 2018-08-02T11:18:26.614181: step 6649, loss 0.553824.
Train: 2018-08-02T11:18:26.832908: step 6650, loss 0.562343.
Test: 2018-08-02T11:18:28.020099: step 6650, loss 0.547364.
Train: 2018-08-02T11:18:28.238798: step 6651, loss 0.553844.
Train: 2018-08-02T11:18:28.473119: step 6652, loss 0.536867.
Train: 2018-08-02T11:18:28.691848: step 6653, loss 0.613283.
Train: 2018-08-02T11:18:28.910517: step 6654, loss 0.63862.
Train: 2018-08-02T11:18:29.129215: step 6655, loss 0.494805.
Train: 2018-08-02T11:18:29.347940: step 6656, loss 0.59609.
Train: 2018-08-02T11:18:29.566638: step 6657, loss 0.537158.
Train: 2018-08-02T11:18:29.785312: step 6658, loss 0.52043.
Train: 2018-08-02T11:18:30.019663: step 6659, loss 0.554011.
Train: 2018-08-02T11:18:30.238359: step 6660, loss 0.579156.
Test: 2018-08-02T11:18:31.425552: step 6660, loss 0.547855.
Train: 2018-08-02T11:18:31.644251: step 6661, loss 0.629344.
Train: 2018-08-02T11:18:31.862961: step 6662, loss 0.620831.
Train: 2018-08-02T11:18:32.097271: step 6663, loss 0.495952.
Train: 2018-08-02T11:18:32.315969: step 6664, loss 0.512699.
Train: 2018-08-02T11:18:32.534692: step 6665, loss 0.587332.
Train: 2018-08-02T11:18:32.753400: step 6666, loss 0.570758.
Train: 2018-08-02T11:18:32.972090: step 6667, loss 0.59555.
Train: 2018-08-02T11:18:33.190795: step 6668, loss 0.570758.
Train: 2018-08-02T11:18:33.409493: step 6669, loss 0.54607.
Train: 2018-08-02T11:18:33.628196: step 6670, loss 0.587172.
Test: 2018-08-02T11:18:34.799762: step 6670, loss 0.547555.
Train: 2018-08-02T11:18:35.018491: step 6671, loss 0.570788.
Train: 2018-08-02T11:18:35.252781: step 6672, loss 0.538061.
Train: 2018-08-02T11:18:35.471514: step 6673, loss 0.652431.
Train: 2018-08-02T11:18:35.690178: step 6674, loss 0.562647.
Train: 2018-08-02T11:18:35.924524: step 6675, loss 0.57079.
Train: 2018-08-02T11:18:36.143198: step 6676, loss 0.570788.
Train: 2018-08-02T11:18:36.361924: step 6677, loss 0.570819.
Train: 2018-08-02T11:18:36.596249: step 6678, loss 0.619123.
Train: 2018-08-02T11:18:36.814916: step 6679, loss 0.586877.
Train: 2018-08-02T11:18:37.033644: step 6680, loss 0.594836.
Test: 2018-08-02T11:18:38.220836: step 6680, loss 0.549853.
Train: 2018-08-02T11:18:38.439535: step 6681, loss 0.555003.
Train: 2018-08-02T11:18:38.658264: step 6682, loss 0.547155.
Train: 2018-08-02T11:18:38.876957: step 6683, loss 0.618387.
Train: 2018-08-02T11:18:39.095656: step 6684, loss 0.515847.
Train: 2018-08-02T11:18:39.329951: step 6685, loss 0.563142.
Train: 2018-08-02T11:18:39.548649: step 6686, loss 0.633837.
Train: 2018-08-02T11:18:39.767380: step 6687, loss 0.594538.
Train: 2018-08-02T11:18:39.986077: step 6688, loss 0.532064.
Train: 2018-08-02T11:18:40.204776: step 6689, loss 0.610057.
Train: 2018-08-02T11:18:40.423445: step 6690, loss 0.571138.
Test: 2018-08-02T11:18:41.626288: step 6690, loss 0.549755.
Train: 2018-08-02T11:18:41.829367: step 6691, loss 0.555663.
Train: 2018-08-02T11:18:42.063717: step 6692, loss 0.517011.
Train: 2018-08-02T11:18:42.266764: step 6693, loss 0.486048.
Train: 2018-08-02T11:18:42.501119: step 6694, loss 0.547882.
Train: 2018-08-02T11:18:42.735413: step 6695, loss 0.563343.
Train: 2018-08-02T11:18:42.938507: step 6696, loss 0.610097.
Train: 2018-08-02T11:18:43.172801: step 6697, loss 0.539821.
Train: 2018-08-02T11:18:43.375879: step 6698, loss 0.563219.
Train: 2018-08-02T11:18:43.610224: step 6699, loss 0.523942.
Train: 2018-08-02T11:18:43.828923: step 6700, loss 0.610356.
Test: 2018-08-02T11:18:45.016120: step 6700, loss 0.549919.
Train: 2018-08-02T11:18:45.906535: step 6701, loss 0.53943.
Train: 2018-08-02T11:18:46.125270: step 6702, loss 0.539332.
Train: 2018-08-02T11:18:46.375210: step 6703, loss 0.563001.
Train: 2018-08-02T11:18:46.578283: step 6704, loss 0.491396.
Train: 2018-08-02T11:18:46.812598: step 6705, loss 0.522958.
Train: 2018-08-02T11:18:47.015683: step 6706, loss 0.554789.
Train: 2018-08-02T11:18:47.234349: step 6707, loss 0.482101.
Train: 2018-08-02T11:18:47.468701: step 6708, loss 0.562673.
Train: 2018-08-02T11:18:47.687369: step 6709, loss 0.570772.
Train: 2018-08-02T11:18:47.906069: step 6710, loss 0.537956.
Test: 2018-08-02T11:18:49.093290: step 6710, loss 0.54951.
Train: 2018-08-02T11:18:49.311988: step 6711, loss 0.546032.
Train: 2018-08-02T11:18:49.546334: step 6712, loss 0.537634.
Train: 2018-08-02T11:18:49.765008: step 6713, loss 0.537483.
Train: 2018-08-02T11:18:49.999330: step 6714, loss 0.562412.
Train: 2018-08-02T11:18:50.218027: step 6715, loss 0.520426.
Train: 2018-08-02T11:18:50.452397: step 6716, loss 0.596092.
Train: 2018-08-02T11:18:50.655450: step 6717, loss 0.621553.
Train: 2018-08-02T11:18:50.874157: step 6718, loss 0.562356.
Train: 2018-08-02T11:18:51.108468: step 6719, loss 0.48608.
Train: 2018-08-02T11:18:51.327142: step 6720, loss 0.562347.
Test: 2018-08-02T11:18:52.514364: step 6720, loss 0.548522.
Train: 2018-08-02T11:18:52.717472: step 6721, loss 0.562343.
Train: 2018-08-02T11:18:52.936140: step 6722, loss 0.545294.
Train: 2018-08-02T11:18:53.154840: step 6723, loss 0.519661.
Train: 2018-08-02T11:18:53.373537: step 6724, loss 0.545231.
Train: 2018-08-02T11:18:53.592270: step 6725, loss 0.545197.
Train: 2018-08-02T11:18:53.810965: step 6726, loss 0.648187.
Train: 2018-08-02T11:18:54.029666: step 6727, loss 0.545172.
Train: 2018-08-02T11:18:54.248358: step 6728, loss 0.570914.
Train: 2018-08-02T11:18:54.467032: step 6729, loss 0.588055.
Train: 2018-08-02T11:18:54.685756: step 6730, loss 0.502408.
Test: 2018-08-02T11:18:55.872952: step 6730, loss 0.548436.
Train: 2018-08-02T11:18:56.076060: step 6731, loss 0.510982.
Train: 2018-08-02T11:18:56.294728: step 6732, loss 0.5709.
Train: 2018-08-02T11:18:56.513457: step 6733, loss 0.468111.
Train: 2018-08-02T11:18:56.747747: step 6734, loss 0.562335.
Train: 2018-08-02T11:18:56.966446: step 6735, loss 0.57093.
Train: 2018-08-02T11:18:57.185176: step 6736, loss 0.502122.
Train: 2018-08-02T11:18:57.419465: step 6737, loss 0.639893.
Train: 2018-08-02T11:18:57.622574: step 6738, loss 0.58818.
Train: 2018-08-02T11:18:57.872520: step 6739, loss 0.562336.
Train: 2018-08-02T11:18:58.106832: step 6740, loss 0.51077.
Test: 2018-08-02T11:18:59.278404: step 6740, loss 0.547196.
Train: 2018-08-02T11:18:59.497104: step 6741, loss 0.622475.
Train: 2018-08-02T11:18:59.715828: step 6742, loss 0.502304.
Train: 2018-08-02T11:18:59.934531: step 6743, loss 0.588047.
Train: 2018-08-02T11:19:00.153226: step 6744, loss 0.510982.
Train: 2018-08-02T11:19:00.371929: step 6745, loss 0.613672.
Train: 2018-08-02T11:19:00.590622: step 6746, loss 0.502546.
Train: 2018-08-02T11:19:00.809322: step 6747, loss 0.570876.
Train: 2018-08-02T11:19:01.043647: step 6748, loss 0.536747.
Train: 2018-08-02T11:19:01.277962: step 6749, loss 0.528231.
Train: 2018-08-02T11:19:01.496670: step 6750, loss 0.570868.
Test: 2018-08-02T11:19:02.699478: step 6750, loss 0.54869.
Train: 2018-08-02T11:19:02.949420: step 6751, loss 0.630547.
Train: 2018-08-02T11:19:03.168118: step 6752, loss 0.587871.
Train: 2018-08-02T11:19:03.386849: step 6753, loss 0.579324.
Train: 2018-08-02T11:19:03.605545: step 6754, loss 0.553894.
Train: 2018-08-02T11:19:03.839869: step 6755, loss 0.545483.
Train: 2018-08-02T11:19:04.058566: step 6756, loss 0.486563.
Train: 2018-08-02T11:19:04.292886: step 6757, loss 0.629735.
Train: 2018-08-02T11:19:04.527176: step 6758, loss 0.621205.
Train: 2018-08-02T11:19:04.730286: step 6759, loss 0.562401.
Train: 2018-08-02T11:19:04.948951: step 6760, loss 0.529022.
Test: 2018-08-02T11:19:06.120553: step 6760, loss 0.548328.
Train: 2018-08-02T11:19:06.339251: step 6761, loss 0.595757.
Train: 2018-08-02T11:19:06.573572: step 6762, loss 0.504299.
Train: 2018-08-02T11:19:06.792301: step 6763, loss 0.603944.
Train: 2018-08-02T11:19:06.995382: step 6764, loss 0.521088.
Train: 2018-08-02T11:19:07.214076: step 6765, loss 0.562488.
Train: 2018-08-02T11:19:07.432770: step 6766, loss 0.545978.
Train: 2018-08-02T11:19:07.667091: step 6767, loss 0.504725.
Train: 2018-08-02T11:19:07.870142: step 6768, loss 0.545978.
Train: 2018-08-02T11:19:08.088872: step 6769, loss 0.579023.
Train: 2018-08-02T11:19:08.307566: step 6770, loss 0.537677.
Test: 2018-08-02T11:19:09.494762: step 6770, loss 0.54961.
Train: 2018-08-02T11:19:09.713496: step 6771, loss 0.521096.
Train: 2018-08-02T11:19:09.932159: step 6772, loss 0.579048.
Train: 2018-08-02T11:19:10.150884: step 6773, loss 0.570758.
Train: 2018-08-02T11:19:10.369592: step 6774, loss 0.653793.
Train: 2018-08-02T11:19:10.588281: step 6775, loss 0.587337.
Train: 2018-08-02T11:19:10.806986: step 6776, loss 0.645207.
Train: 2018-08-02T11:19:11.025683: step 6777, loss 0.562517.
Train: 2018-08-02T11:19:11.244352: step 6778, loss 0.546128.
Train: 2018-08-02T11:19:11.463076: step 6779, loss 0.578952.
Train: 2018-08-02T11:19:11.681780: step 6780, loss 0.578934.
Test: 2018-08-02T11:19:12.868972: step 6780, loss 0.549615.
Train: 2018-08-02T11:19:13.072050: step 6781, loss 0.546368.
Train: 2018-08-02T11:19:13.290749: step 6782, loss 0.554553.
Train: 2018-08-02T11:19:13.509478: step 6783, loss 0.538388.
Train: 2018-08-02T11:19:13.728176: step 6784, loss 0.546522.
Train: 2018-08-02T11:19:13.946870: step 6785, loss 0.635506.
Train: 2018-08-02T11:19:14.165568: step 6786, loss 0.627302.
Train: 2018-08-02T11:19:14.384273: step 6787, loss 0.611038.
Train: 2018-08-02T11:19:14.618564: step 6788, loss 0.554844.
Train: 2018-08-02T11:19:14.837262: step 6789, loss 0.546948.
Train: 2018-08-02T11:19:15.087203: step 6790, loss 0.539077.
Test: 2018-08-02T11:19:16.258804: step 6790, loss 0.549709.
Train: 2018-08-02T11:19:16.477527: step 6791, loss 0.562973.
Train: 2018-08-02T11:19:16.696226: step 6792, loss 0.531272.
Train: 2018-08-02T11:19:16.914930: step 6793, loss 0.531288.
Train: 2018-08-02T11:19:17.133629: step 6794, loss 0.555058.
Train: 2018-08-02T11:19:17.321085: step 6795, loss 0.562977.
Train: 2018-08-02T11:19:17.539778: step 6796, loss 0.610651.
Train: 2018-08-02T11:19:17.758483: step 6797, loss 0.499391.
Train: 2018-08-02T11:19:17.977182: step 6798, loss 0.578859.
Train: 2018-08-02T11:19:18.195880: step 6799, loss 0.634636.
Train: 2018-08-02T11:19:18.430206: step 6800, loss 0.515155.
Test: 2018-08-02T11:19:19.601772: step 6800, loss 0.548146.
Train: 2018-08-02T11:19:20.414110: step 6801, loss 0.682454.
Train: 2018-08-02T11:19:20.632810: step 6802, loss 0.547052.
Train: 2018-08-02T11:19:20.851510: step 6803, loss 0.555038.
Train: 2018-08-02T11:19:21.070207: step 6804, loss 0.570926.
Train: 2018-08-02T11:19:21.304515: step 6805, loss 0.547157.
Train: 2018-08-02T11:19:21.523195: step 6806, loss 0.586782.
Train: 2018-08-02T11:19:21.741929: step 6807, loss 0.57886.
Train: 2018-08-02T11:19:21.960628: step 6808, loss 0.507661.
Train: 2018-08-02T11:19:22.179292: step 6809, loss 0.547191.
Train: 2018-08-02T11:19:22.413637: step 6810, loss 0.60264.
Test: 2018-08-02T11:19:23.585213: step 6810, loss 0.54921.
Train: 2018-08-02T11:19:23.803911: step 6811, loss 0.586788.
Train: 2018-08-02T11:19:24.022640: step 6812, loss 0.467869.
Train: 2018-08-02T11:19:24.241344: step 6813, loss 0.578858.
Train: 2018-08-02T11:19:24.460042: step 6814, loss 0.562937.
Train: 2018-08-02T11:19:24.694330: step 6815, loss 0.570885.
Train: 2018-08-02T11:19:24.913026: step 6816, loss 0.594837.
Train: 2018-08-02T11:19:25.131758: step 6817, loss 0.610835.
Train: 2018-08-02T11:19:25.350448: step 6818, loss 0.578862.
Train: 2018-08-02T11:19:25.569148: step 6819, loss 0.562892.
Train: 2018-08-02T11:19:25.787823: step 6820, loss 0.578861.
Test: 2018-08-02T11:19:26.959423: step 6820, loss 0.550878.
Train: 2018-08-02T11:19:27.178152: step 6821, loss 0.523039.
Train: 2018-08-02T11:19:27.396850: step 6822, loss 0.554927.
Train: 2018-08-02T11:19:27.615545: step 6823, loss 0.491042.
Train: 2018-08-02T11:19:27.834249: step 6824, loss 0.578864.
Train: 2018-08-02T11:19:28.052918: step 6825, loss 0.490654.
Train: 2018-08-02T11:19:28.271641: step 6826, loss 0.474248.
Train: 2018-08-02T11:19:28.490314: step 6827, loss 0.595077.
Train: 2018-08-02T11:19:28.709046: step 6828, loss 0.48953.
Train: 2018-08-02T11:19:28.927736: step 6829, loss 0.521751.
Train: 2018-08-02T11:19:29.146436: step 6830, loss 0.562542.
Test: 2018-08-02T11:19:30.318011: step 6830, loss 0.550223.
Train: 2018-08-02T11:19:30.521118: step 6831, loss 0.570756.
Train: 2018-08-02T11:19:30.755433: step 6832, loss 0.579054.
Train: 2018-08-02T11:19:30.958487: step 6833, loss 0.545788.
Train: 2018-08-02T11:19:31.177215: step 6834, loss 0.545711.
Train: 2018-08-02T11:19:31.395884: step 6835, loss 0.595919.
Train: 2018-08-02T11:19:31.614617: step 6836, loss 0.528798.
Train: 2018-08-02T11:19:31.833311: step 6837, loss 0.528705.
Train: 2018-08-02T11:19:32.052010: step 6838, loss 0.579248.
Train: 2018-08-02T11:19:32.270709: step 6839, loss 0.52007.
Train: 2018-08-02T11:19:32.489411: step 6840, loss 0.519957.
Test: 2018-08-02T11:19:33.660978: step 6840, loss 0.549708.
Train: 2018-08-02T11:19:33.879676: step 6841, loss 0.494313.
Train: 2018-08-02T11:19:34.098375: step 6842, loss 0.553801.
Train: 2018-08-02T11:19:34.317105: step 6843, loss 0.570902.
Train: 2018-08-02T11:19:34.535804: step 6844, loss 0.536565.
Train: 2018-08-02T11:19:34.770096: step 6845, loss 0.63124.
Train: 2018-08-02T11:19:34.973202: step 6846, loss 0.553718.
Train: 2018-08-02T11:19:35.191903: step 6847, loss 0.536467.
Train: 2018-08-02T11:19:35.410599: step 6848, loss 0.588227.
Train: 2018-08-02T11:19:35.629299: step 6849, loss 0.579595.
Train: 2018-08-02T11:19:35.847996: step 6850, loss 0.614066.
Test: 2018-08-02T11:19:37.035188: step 6850, loss 0.547571.
Train: 2018-08-02T11:19:37.238300: step 6851, loss 0.596749.
Train: 2018-08-02T11:19:37.456997: step 6852, loss 0.59665.
Train: 2018-08-02T11:19:37.675693: step 6853, loss 0.562337.
Train: 2018-08-02T11:19:37.910016: step 6854, loss 0.545305.
Train: 2018-08-02T11:19:38.128707: step 6855, loss 0.596318.
Train: 2018-08-02T11:19:38.363002: step 6856, loss 0.511587.
Train: 2018-08-02T11:19:38.581700: step 6857, loss 0.621458.
Train: 2018-08-02T11:19:38.800429: step 6858, loss 0.528735.
Train: 2018-08-02T11:19:39.019124: step 6859, loss 0.562393.
Train: 2018-08-02T11:19:39.237827: step 6860, loss 0.595876.
Test: 2018-08-02T11:19:40.409398: step 6860, loss 0.548496.
Train: 2018-08-02T11:19:40.628097: step 6861, loss 0.612469.
Train: 2018-08-02T11:19:40.846825: step 6862, loss 0.562453.
Train: 2018-08-02T11:19:41.065519: step 6863, loss 0.579031.
Train: 2018-08-02T11:19:41.284224: step 6864, loss 0.554273.
Train: 2018-08-02T11:19:41.518544: step 6865, loss 0.521476.
Train: 2018-08-02T11:19:41.737246: step 6866, loss 0.636333.
Train: 2018-08-02T11:19:41.987154: step 6867, loss 0.603434.
Train: 2018-08-02T11:19:42.190231: step 6868, loss 0.643961.
Train: 2018-08-02T11:19:42.408961: step 6869, loss 0.530387.
Train: 2018-08-02T11:19:42.643274: step 6870, loss 0.554731.
Test: 2018-08-02T11:19:43.814850: step 6870, loss 0.548518.
Train: 2018-08-02T11:19:44.017928: step 6871, loss 0.64301.
Train: 2018-08-02T11:19:44.236661: step 6872, loss 0.586836.
Train: 2018-08-02T11:19:44.455351: step 6873, loss 0.570924.
Train: 2018-08-02T11:19:44.674049: step 6874, loss 0.578862.
Train: 2018-08-02T11:19:44.892753: step 6875, loss 0.547428.
Train: 2018-08-02T11:19:45.111456: step 6876, loss 0.649375.
Train: 2018-08-02T11:19:45.330153: step 6877, loss 0.578896.
Train: 2018-08-02T11:19:45.548850: step 6878, loss 0.578916.
Train: 2018-08-02T11:19:45.783140: step 6879, loss 0.58666.
Train: 2018-08-02T11:19:45.986251: step 6880, loss 0.525162.
Test: 2018-08-02T11:19:47.157817: step 6880, loss 0.551201.
Train: 2018-08-02T11:19:47.376541: step 6881, loss 0.486989.
Train: 2018-08-02T11:19:47.595245: step 6882, loss 0.586649.
Train: 2018-08-02T11:19:47.813914: step 6883, loss 0.548328.
Train: 2018-08-02T11:19:48.048236: step 6884, loss 0.58665.
Train: 2018-08-02T11:19:48.266963: step 6885, loss 0.532962.
Train: 2018-08-02T11:19:48.485666: step 6886, loss 0.540573.
Train: 2018-08-02T11:19:48.719983: step 6887, loss 0.55587.
Train: 2018-08-02T11:19:48.938650: step 6888, loss 0.540371.
Train: 2018-08-02T11:19:49.172971: step 6889, loss 0.56345.
Train: 2018-08-02T11:19:49.376079: step 6890, loss 0.540103.
Test: 2018-08-02T11:19:50.547648: step 6890, loss 0.549769.
Train: 2018-08-02T11:19:50.750757: step 6891, loss 0.532158.
Train: 2018-08-02T11:19:51.000695: step 6892, loss 0.578882.
Train: 2018-08-02T11:19:51.219396: step 6893, loss 0.508205.
Train: 2018-08-02T11:19:51.438097: step 6894, loss 0.539412.
Train: 2018-08-02T11:19:51.656794: step 6895, loss 0.570927.
Train: 2018-08-02T11:19:51.875463: step 6896, loss 0.570892.
Train: 2018-08-02T11:19:52.109818: step 6897, loss 0.506873.
Train: 2018-08-02T11:19:52.328512: step 6898, loss 0.530635.
Train: 2018-08-02T11:19:52.547206: step 6899, loss 0.603141.
Train: 2018-08-02T11:19:52.765879: step 6900, loss 0.570788.
Test: 2018-08-02T11:19:53.953101: step 6900, loss 0.547915.
Train: 2018-08-02T11:19:54.781033: step 6901, loss 0.53007.
Train: 2018-08-02T11:19:54.999768: step 6902, loss 0.554428.
Train: 2018-08-02T11:19:55.218460: step 6903, loss 0.562561.
Train: 2018-08-02T11:19:55.437129: step 6904, loss 0.488486.
Train: 2018-08-02T11:19:55.655853: step 6905, loss 0.63687.
Train: 2018-08-02T11:19:55.874557: step 6906, loss 0.554191.
Train: 2018-08-02T11:19:56.093253: step 6907, loss 0.59566.
Train: 2018-08-02T11:19:56.327570: step 6908, loss 0.520904.
Train: 2018-08-02T11:19:56.546244: step 6909, loss 0.612385.
Train: 2018-08-02T11:19:56.764942: step 6910, loss 0.587418.
Test: 2018-08-02T11:19:57.952165: step 6910, loss 0.548343.
Train: 2018-08-02T11:19:58.155268: step 6911, loss 0.579088.
Train: 2018-08-02T11:19:58.373941: step 6912, loss 0.57908.
Train: 2018-08-02T11:19:58.592669: step 6913, loss 0.57076.
Train: 2018-08-02T11:19:58.811369: step 6914, loss 0.529271.
Train: 2018-08-02T11:19:59.030067: step 6915, loss 0.60393.
Train: 2018-08-02T11:19:59.248761: step 6916, loss 0.628719.
Train: 2018-08-02T11:19:59.483081: step 6917, loss 0.50471.
Train: 2018-08-02T11:19:59.701756: step 6918, loss 0.455339.
Train: 2018-08-02T11:19:59.936107: step 6919, loss 0.488222.
Train: 2018-08-02T11:20:00.154809: step 6920, loss 0.504553.
Test: 2018-08-02T11:20:01.357617: step 6920, loss 0.548009.
Train: 2018-08-02T11:20:01.560695: step 6921, loss 0.612287.
Train: 2018-08-02T11:20:01.779423: step 6922, loss 0.545795.
Train: 2018-08-02T11:20:01.998123: step 6923, loss 0.520726.
Train: 2018-08-02T11:20:02.216819: step 6924, loss 0.512235.
Train: 2018-08-02T11:20:02.451144: step 6925, loss 0.587565.
Train: 2018-08-02T11:20:02.685462: step 6926, loss 0.537147.
Train: 2018-08-02T11:20:02.904131: step 6927, loss 0.511776.
Train: 2018-08-02T11:20:03.122859: step 6928, loss 0.562358.
Train: 2018-08-02T11:20:03.357184: step 6929, loss 0.519938.
Train: 2018-08-02T11:20:03.575878: step 6930, loss 0.63042.
Test: 2018-08-02T11:20:04.763070: step 6930, loss 0.54831.
Train: 2018-08-02T11:20:04.966181: step 6931, loss 0.443079.
Train: 2018-08-02T11:20:05.184877: step 6932, loss 0.613622.
Train: 2018-08-02T11:20:05.403575: step 6933, loss 0.553773.
Train: 2018-08-02T11:20:05.622244: step 6934, loss 0.553762.
Train: 2018-08-02T11:20:05.840967: step 6935, loss 0.605258.
Train: 2018-08-02T11:20:06.059671: step 6936, loss 0.519415.
Train: 2018-08-02T11:20:06.278356: step 6937, loss 0.553746.
Train: 2018-08-02T11:20:06.512685: step 6938, loss 0.613893.
Train: 2018-08-02T11:20:06.715737: step 6939, loss 0.58809.
Train: 2018-08-02T11:20:06.934467: step 6940, loss 0.579477.
Test: 2018-08-02T11:20:08.121658: step 6940, loss 0.547461.
Train: 2018-08-02T11:20:08.324766: step 6941, loss 0.536677.
Train: 2018-08-02T11:20:08.543466: step 6942, loss 0.596496.
Train: 2018-08-02T11:20:08.762164: step 6943, loss 0.647533.
Train: 2018-08-02T11:20:08.980845: step 6944, loss 0.5369.
Train: 2018-08-02T11:20:09.199561: step 6945, loss 0.621541.
Train: 2018-08-02T11:20:09.386989: step 6946, loss 0.580332.
Train: 2018-08-02T11:20:09.621332: step 6947, loss 0.604282.
Train: 2018-08-02T11:20:09.840034: step 6948, loss 0.587428.
Train: 2018-08-02T11:20:10.058730: step 6949, loss 0.58733.
Train: 2018-08-02T11:20:10.277403: step 6950, loss 0.595476.
Test: 2018-08-02T11:20:11.449004: step 6950, loss 0.548515.
Train: 2018-08-02T11:20:11.667727: step 6951, loss 0.603529.
Train: 2018-08-02T11:20:11.902054: step 6952, loss 0.52194.
Train: 2018-08-02T11:20:12.120723: step 6953, loss 0.619404.
Train: 2018-08-02T11:20:12.339455: step 6954, loss 0.498318.
Train: 2018-08-02T11:20:12.558120: step 6955, loss 0.546756.
Train: 2018-08-02T11:20:12.776847: step 6956, loss 0.554843.
Train: 2018-08-02T11:20:13.011190: step 6957, loss 0.538908.
Train: 2018-08-02T11:20:13.229871: step 6958, loss 0.586844.
Train: 2018-08-02T11:20:13.448567: step 6959, loss 0.61074.
Train: 2018-08-02T11:20:13.667234: step 6960, loss 0.539112.
Test: 2018-08-02T11:20:14.854457: step 6960, loss 0.550277.
Train: 2018-08-02T11:20:15.088777: step 6961, loss 0.523302.
Train: 2018-08-02T11:20:15.307482: step 6962, loss 0.523314.
Train: 2018-08-02T11:20:15.541821: step 6963, loss 0.562973.
Train: 2018-08-02T11:20:15.760524: step 6964, loss 0.586809.
Train: 2018-08-02T11:20:15.994846: step 6965, loss 0.562948.
Train: 2018-08-02T11:20:16.213542: step 6966, loss 0.594773.
Train: 2018-08-02T11:20:16.447833: step 6967, loss 0.53908.
Train: 2018-08-02T11:20:16.666562: step 6968, loss 0.59478.
Train: 2018-08-02T11:20:16.900853: step 6969, loss 0.570895.
Train: 2018-08-02T11:20:17.119553: step 6970, loss 0.602716.
Test: 2018-08-02T11:20:18.306773: step 6970, loss 0.549677.
Train: 2018-08-02T11:20:18.525473: step 6971, loss 0.547052.
Train: 2018-08-02T11:20:18.744201: step 6972, loss 0.57891.
Train: 2018-08-02T11:20:18.978516: step 6973, loss 0.602697.
Train: 2018-08-02T11:20:19.197190: step 6974, loss 0.531286.
Train: 2018-08-02T11:20:19.415919: step 6975, loss 0.586784.
Train: 2018-08-02T11:20:19.650233: step 6976, loss 0.56302.
Train: 2018-08-02T11:20:19.853286: step 6977, loss 0.563029.
Train: 2018-08-02T11:20:20.087612: step 6978, loss 0.610512.
Train: 2018-08-02T11:20:20.290683: step 6979, loss 0.586764.
Train: 2018-08-02T11:20:20.509382: step 6980, loss 0.507854.
Test: 2018-08-02T11:20:21.696604: step 6980, loss 0.550448.
Train: 2018-08-02T11:20:21.915334: step 6981, loss 0.523628.
Train: 2018-08-02T11:20:22.149648: step 6982, loss 0.547259.
Train: 2018-08-02T11:20:22.368322: step 6983, loss 0.555117.
Train: 2018-08-02T11:20:22.602678: step 6984, loss 0.539213.
Train: 2018-08-02T11:20:22.821398: step 6985, loss 0.515271.
Train: 2018-08-02T11:20:23.040065: step 6986, loss 0.531002.
Train: 2018-08-02T11:20:23.258769: step 6987, loss 0.570856.
Train: 2018-08-02T11:20:23.477440: step 6988, loss 0.594945.
Train: 2018-08-02T11:20:23.696160: step 6989, loss 0.530539.
Train: 2018-08-02T11:20:23.930458: step 6990, loss 0.578888.
Test: 2018-08-02T11:20:25.117678: step 6990, loss 0.549727.
Train: 2018-08-02T11:20:25.320780: step 6991, loss 0.554596.
Train: 2018-08-02T11:20:25.586350: step 6992, loss 0.538309.
Train: 2018-08-02T11:20:25.805018: step 6993, loss 0.513782.
Train: 2018-08-02T11:20:26.023747: step 6994, loss 0.578933.
Train: 2018-08-02T11:20:26.258062: step 6995, loss 0.660946.
Train: 2018-08-02T11:20:26.492388: step 6996, loss 0.62814.
Train: 2018-08-02T11:20:26.726677: step 6997, loss 0.538021.
Train: 2018-08-02T11:20:26.945376: step 6998, loss 0.636208.
Train: 2018-08-02T11:20:27.179721: step 6999, loss 0.521805.
Train: 2018-08-02T11:20:27.398421: step 7000, loss 0.611534.
Test: 2018-08-02T11:20:28.569995: step 7000, loss 0.548876.
Train: 2018-08-02T11:20:29.397956: step 7001, loss 0.489441.
Train: 2018-08-02T11:20:29.616655: step 7002, loss 0.570782.
Train: 2018-08-02T11:20:29.835355: step 7003, loss 0.611429.
Train: 2018-08-02T11:20:30.069670: step 7004, loss 0.554554.
Train: 2018-08-02T11:20:30.288343: step 7005, loss 0.578901.
Train: 2018-08-02T11:20:30.502988: step 7006, loss 0.554605.
Train: 2018-08-02T11:20:30.721691: step 7007, loss 0.58698.
Train: 2018-08-02T11:20:30.940413: step 7008, loss 0.586964.
Train: 2018-08-02T11:20:31.174734: step 7009, loss 0.522444.
Train: 2018-08-02T11:20:31.393437: step 7010, loss 0.546649.
Test: 2018-08-02T11:20:32.580629: step 7010, loss 0.550423.
Train: 2018-08-02T11:20:32.783707: step 7011, loss 0.578878.
Train: 2018-08-02T11:20:33.002405: step 7012, loss 0.58693.
Train: 2018-08-02T11:20:33.221131: step 7013, loss 0.570829.
Train: 2018-08-02T11:20:33.439803: step 7014, loss 0.578872.
Train: 2018-08-02T11:20:33.674154: step 7015, loss 0.546754.
Train: 2018-08-02T11:20:33.892822: step 7016, loss 0.570844.
Train: 2018-08-02T11:20:34.111520: step 7017, loss 0.51471.
Train: 2018-08-02T11:20:34.330244: step 7018, loss 0.554792.
Train: 2018-08-02T11:20:34.548943: step 7019, loss 0.530674.
Train: 2018-08-02T11:20:34.767648: step 7020, loss 0.594979.
Test: 2018-08-02T11:20:35.954839: step 7020, loss 0.550247.
Train: 2018-08-02T11:20:36.189192: step 7021, loss 0.562763.
Train: 2018-08-02T11:20:36.423504: step 7022, loss 0.554706.
Train: 2018-08-02T11:20:36.642179: step 7023, loss 0.554675.
Train: 2018-08-02T11:20:36.860901: step 7024, loss 0.627369.
Train: 2018-08-02T11:20:37.079613: step 7025, loss 0.570816.
Train: 2018-08-02T11:20:37.313926: step 7026, loss 0.538551.
Train: 2018-08-02T11:20:37.548246: step 7027, loss 0.554681.
Train: 2018-08-02T11:20:37.751294: step 7028, loss 0.538535.
Train: 2018-08-02T11:20:37.969993: step 7029, loss 0.562732.
Train: 2018-08-02T11:20:38.188721: step 7030, loss 0.498043.
Test: 2018-08-02T11:20:39.375913: step 7030, loss 0.549529.
Train: 2018-08-02T11:20:39.579015: step 7031, loss 0.554586.
Train: 2018-08-02T11:20:39.797714: step 7032, loss 0.530168.
Train: 2018-08-02T11:20:40.016418: step 7033, loss 0.587071.
Train: 2018-08-02T11:20:40.235087: step 7034, loss 0.538114.
Train: 2018-08-02T11:20:40.453810: step 7035, loss 0.562581.
Train: 2018-08-02T11:20:40.672515: step 7036, loss 0.570762.
Train: 2018-08-02T11:20:40.891208: step 7037, loss 0.603623.
Train: 2018-08-02T11:20:41.109881: step 7038, loss 0.546098.
Train: 2018-08-02T11:20:41.328606: step 7039, loss 0.570758.
Train: 2018-08-02T11:20:41.547310: step 7040, loss 0.570758.
Test: 2018-08-02T11:20:42.734502: step 7040, loss 0.547826.
Train: 2018-08-02T11:20:42.937579: step 7041, loss 0.554291.
Train: 2018-08-02T11:20:43.156308: step 7042, loss 0.570757.
Train: 2018-08-02T11:20:43.374977: step 7043, loss 0.587231.
Train: 2018-08-02T11:20:43.593707: step 7044, loss 0.628381.
Train: 2018-08-02T11:20:43.828023: step 7045, loss 0.529684.
Train: 2018-08-02T11:20:44.062349: step 7046, loss 0.472293.
Train: 2018-08-02T11:20:44.296667: step 7047, loss 0.570763.
Train: 2018-08-02T11:20:44.515365: step 7048, loss 0.57898.
Train: 2018-08-02T11:20:44.749688: step 7049, loss 0.488519.
Train: 2018-08-02T11:20:44.968384: step 7050, loss 0.570776.
Test: 2018-08-02T11:20:46.139954: step 7050, loss 0.548717.
Train: 2018-08-02T11:20:46.358681: step 7051, loss 0.595506.
Train: 2018-08-02T11:20:46.592999: step 7052, loss 0.554294.
Train: 2018-08-02T11:20:46.811672: step 7053, loss 0.578985.
Train: 2018-08-02T11:20:47.045992: step 7054, loss 0.51295.
Train: 2018-08-02T11:20:47.264722: step 7055, loss 0.521168.
Train: 2018-08-02T11:20:47.483419: step 7056, loss 0.628735.
Train: 2018-08-02T11:20:47.702120: step 7057, loss 0.521056.
Train: 2018-08-02T11:20:47.936434: step 7058, loss 0.537591.
Train: 2018-08-02T11:20:48.170759: step 7059, loss 0.628888.
Train: 2018-08-02T11:20:48.389453: step 7060, loss 0.537572.
Test: 2018-08-02T11:20:49.576649: step 7060, loss 0.548793.
Train: 2018-08-02T11:20:49.779730: step 7061, loss 0.562461.
Train: 2018-08-02T11:20:50.014046: step 7062, loss 0.554162.
Train: 2018-08-02T11:20:50.232745: step 7063, loss 0.653748.
Train: 2018-08-02T11:20:50.451469: step 7064, loss 0.53763.
Train: 2018-08-02T11:20:50.670174: step 7065, loss 0.512865.
Train: 2018-08-02T11:20:50.888842: step 7066, loss 0.562488.
Train: 2018-08-02T11:20:51.123193: step 7067, loss 0.58729.
Train: 2018-08-02T11:20:51.341860: step 7068, loss 0.512939.
Train: 2018-08-02T11:20:51.560586: step 7069, loss 0.562494.
Train: 2018-08-02T11:20:51.794916: step 7070, loss 0.537701.
Test: 2018-08-02T11:20:52.966481: step 7070, loss 0.547534.
Train: 2018-08-02T11:20:53.200802: step 7071, loss 0.570756.
Train: 2018-08-02T11:20:53.419500: step 7072, loss 0.463214.
Train: 2018-08-02T11:20:53.638229: step 7073, loss 0.504411.
Train: 2018-08-02T11:20:53.856897: step 7074, loss 0.59573.
Train: 2018-08-02T11:20:54.075620: step 7075, loss 0.5374.
Train: 2018-08-02T11:20:54.294326: step 7076, loss 0.503873.
Train: 2018-08-02T11:20:54.513023: step 7077, loss 0.587565.
Train: 2018-08-02T11:20:54.731693: step 7078, loss 0.579203.
Train: 2018-08-02T11:20:54.950423: step 7079, loss 0.612923.
Train: 2018-08-02T11:20:55.169120: step 7080, loss 0.604499.
Test: 2018-08-02T11:20:56.340690: step 7080, loss 0.548318.
Train: 2018-08-02T11:20:56.559389: step 7081, loss 0.629719.
Train: 2018-08-02T11:20:56.778118: step 7082, loss 0.545595.
Train: 2018-08-02T11:20:56.996817: step 7083, loss 0.604292.
Train: 2018-08-02T11:20:57.231107: step 7084, loss 0.46216.
Train: 2018-08-02T11:20:57.449837: step 7085, loss 0.545708.
Train: 2018-08-02T11:20:57.668504: step 7086, loss 0.537358.
Train: 2018-08-02T11:20:57.887228: step 7087, loss 0.612537.
Train: 2018-08-02T11:20:58.105926: step 7088, loss 0.570762.
Train: 2018-08-02T11:20:58.324600: step 7089, loss 0.554133.
Train: 2018-08-02T11:20:58.543329: step 7090, loss 0.562452.
Test: 2018-08-02T11:20:59.714901: step 7090, loss 0.547785.
Train: 2018-08-02T11:20:59.933624: step 7091, loss 0.604046.
Train: 2018-08-02T11:21:00.152323: step 7092, loss 0.520972.
Train: 2018-08-02T11:21:00.371021: step 7093, loss 0.504436.
Train: 2018-08-02T11:21:00.605347: step 7094, loss 0.529291.
Train: 2018-08-02T11:21:00.824042: step 7095, loss 0.595665.
Train: 2018-08-02T11:21:01.058338: step 7096, loss 0.562456.
Train: 2018-08-02T11:21:01.230170: step 7097, loss 0.491609.
Train: 2018-08-02T11:21:01.448902: step 7098, loss 0.512557.
Train: 2018-08-02T11:21:01.667568: step 7099, loss 0.587434.
Train: 2018-08-02T11:21:01.901919: step 7100, loss 0.570768.
Test: 2018-08-02T11:21:03.073488: step 7100, loss 0.549038.
Train: 2018-08-02T11:21:03.885823: step 7101, loss 0.662677.
Train: 2018-08-02T11:21:04.104527: step 7102, loss 0.554081.
Train: 2018-08-02T11:21:04.323227: step 7103, loss 0.495773.
Train: 2018-08-02T11:21:04.557524: step 7104, loss 0.595768.
Train: 2018-08-02T11:21:04.776245: step 7105, loss 0.537447.
Train: 2018-08-02T11:21:04.994946: step 7106, loss 0.545778.
Train: 2018-08-02T11:21:05.229265: step 7107, loss 0.470808.
Train: 2018-08-02T11:21:05.447962: step 7108, loss 0.604157.
Train: 2018-08-02T11:21:05.666662: step 7109, loss 0.537353.
Train: 2018-08-02T11:21:05.885356: step 7110, loss 0.620958.
Test: 2018-08-02T11:21:07.056931: step 7110, loss 0.547485.
Train: 2018-08-02T11:21:07.291281: step 7111, loss 0.570772.
Train: 2018-08-02T11:21:07.509975: step 7112, loss 0.528989.
Train: 2018-08-02T11:21:07.744301: step 7113, loss 0.562422.
Train: 2018-08-02T11:21:07.962968: step 7114, loss 0.512282.
Train: 2018-08-02T11:21:08.197321: step 7115, loss 0.545687.
Train: 2018-08-02T11:21:08.416012: step 7116, loss 0.587529.
Train: 2018-08-02T11:21:08.634712: step 7117, loss 0.537277.
Train: 2018-08-02T11:21:08.853410: step 7118, loss 0.629437.
Train: 2018-08-02T11:21:09.072116: step 7119, loss 0.545662.
Train: 2018-08-02T11:21:09.306438: step 7120, loss 0.512217.
Test: 2018-08-02T11:21:10.478005: step 7120, loss 0.548623.
Train: 2018-08-02T11:21:10.681108: step 7121, loss 0.562406.
Train: 2018-08-02T11:21:10.899781: step 7122, loss 0.570775.
Train: 2018-08-02T11:21:11.118506: step 7123, loss 0.570774.
Train: 2018-08-02T11:21:11.337210: step 7124, loss 0.554043.
Train: 2018-08-02T11:21:11.555908: step 7125, loss 0.587496.
Train: 2018-08-02T11:21:11.774606: step 7126, loss 0.587477.
Train: 2018-08-02T11:21:11.993275: step 7127, loss 0.512384.
Train: 2018-08-02T11:21:12.211998: step 7128, loss 0.562428.
Train: 2018-08-02T11:21:12.430699: step 7129, loss 0.579097.
Train: 2018-08-02T11:21:12.649372: step 7130, loss 0.595737.
Test: 2018-08-02T11:21:13.836593: step 7130, loss 0.54857.
Train: 2018-08-02T11:21:14.039696: step 7131, loss 0.520896.
Train: 2018-08-02T11:21:14.258400: step 7132, loss 0.545844.
Train: 2018-08-02T11:21:14.492690: step 7133, loss 0.562457.
Train: 2018-08-02T11:21:14.711389: step 7134, loss 0.587356.
Train: 2018-08-02T11:21:14.930121: step 7135, loss 0.570758.
Train: 2018-08-02T11:21:15.164410: step 7136, loss 0.603879.
Train: 2018-08-02T11:21:15.383136: step 7137, loss 0.587283.
Train: 2018-08-02T11:21:15.601835: step 7138, loss 0.496577.
Train: 2018-08-02T11:21:15.820504: step 7139, loss 0.496638.
Train: 2018-08-02T11:21:16.039202: step 7140, loss 0.521307.
Test: 2018-08-02T11:21:17.226424: step 7140, loss 0.548716.
Train: 2018-08-02T11:21:17.445155: step 7141, loss 0.545992.
Train: 2018-08-02T11:21:17.663847: step 7142, loss 0.521143.
Train: 2018-08-02T11:21:17.882554: step 7143, loss 0.645366.
Train: 2018-08-02T11:21:18.101250: step 7144, loss 0.545886.
Train: 2018-08-02T11:21:18.319918: step 7145, loss 0.603936.
Train: 2018-08-02T11:21:18.538648: step 7146, loss 0.54589.
Train: 2018-08-02T11:21:18.772968: step 7147, loss 0.587332.
Train: 2018-08-02T11:21:18.976044: step 7148, loss 0.554197.
Train: 2018-08-02T11:21:19.194745: step 7149, loss 0.529386.
Train: 2018-08-02T11:21:19.413413: step 7150, loss 0.521109.
Test: 2018-08-02T11:21:20.585013: step 7150, loss 0.549214.
Train: 2018-08-02T11:21:20.803737: step 7151, loss 0.562474.
Train: 2018-08-02T11:21:21.022411: step 7152, loss 0.562469.
Train: 2018-08-02T11:21:21.241109: step 7153, loss 0.570758.
Train: 2018-08-02T11:21:21.459835: step 7154, loss 0.537575.
Train: 2018-08-02T11:21:21.678540: step 7155, loss 0.537552.
Train: 2018-08-02T11:21:21.897235: step 7156, loss 0.64556.
Train: 2018-08-02T11:21:22.131556: step 7157, loss 0.512639.
Train: 2018-08-02T11:21:22.350256: step 7158, loss 0.562455.
Train: 2018-08-02T11:21:22.568948: step 7159, loss 0.537541.
Train: 2018-08-02T11:21:22.787653: step 7160, loss 0.653849.
Test: 2018-08-02T11:21:23.974844: step 7160, loss 0.548038.
Train: 2018-08-02T11:21:24.177921: step 7161, loss 0.545874.
Train: 2018-08-02T11:21:24.396621: step 7162, loss 0.504483.
Train: 2018-08-02T11:21:24.630941: step 7163, loss 0.562472.
Train: 2018-08-02T11:21:24.849670: step 7164, loss 0.579042.
Train: 2018-08-02T11:21:25.083991: step 7165, loss 0.512785.
Train: 2018-08-02T11:21:25.302658: step 7166, loss 0.570757.
Train: 2018-08-02T11:21:25.536978: step 7167, loss 0.521018.
Train: 2018-08-02T11:21:25.755703: step 7168, loss 0.529261.
Train: 2018-08-02T11:21:25.989998: step 7169, loss 0.537504.
Train: 2018-08-02T11:21:26.208727: step 7170, loss 0.479123.
Test: 2018-08-02T11:21:27.395918: step 7170, loss 0.547676.
Train: 2018-08-02T11:21:27.599021: step 7171, loss 0.604228.
Train: 2018-08-02T11:21:27.817726: step 7172, loss 0.604309.
Train: 2018-08-02T11:21:28.052044: step 7173, loss 0.545622.
Train: 2018-08-02T11:21:28.270745: step 7174, loss 0.646322.
Train: 2018-08-02T11:21:28.489436: step 7175, loss 0.646232.
Train: 2018-08-02T11:21:28.708111: step 7176, loss 0.587487.
Train: 2018-08-02T11:21:28.942458: step 7177, loss 0.570764.
Train: 2018-08-02T11:21:29.161130: step 7178, loss 0.603963.
Train: 2018-08-02T11:21:29.379859: step 7179, loss 0.570756.
Train: 2018-08-02T11:21:29.598551: step 7180, loss 0.53782.
Test: 2018-08-02T11:21:30.785749: step 7180, loss 0.548275.
Train: 2018-08-02T11:21:30.988852: step 7181, loss 0.587178.
Train: 2018-08-02T11:21:31.207526: step 7182, loss 0.619854.
Train: 2018-08-02T11:21:31.426249: step 7183, loss 0.497471.
Train: 2018-08-02T11:21:31.660545: step 7184, loss 0.635787.
Train: 2018-08-02T11:21:31.879244: step 7185, loss 0.54652.
Train: 2018-08-02T11:21:32.097973: step 7186, loss 0.562746.
Train: 2018-08-02T11:21:32.316672: step 7187, loss 0.603013.
Train: 2018-08-02T11:21:32.535339: step 7188, loss 0.522736.
Train: 2018-08-02T11:21:32.754068: step 7189, loss 0.498836.
Train: 2018-08-02T11:21:32.988384: step 7190, loss 0.530854.
Test: 2018-08-02T11:21:34.159959: step 7190, loss 0.549291.
Train: 2018-08-02T11:21:34.378683: step 7191, loss 0.578865.
Train: 2018-08-02T11:21:34.597386: step 7192, loss 0.538811.
Train: 2018-08-02T11:21:34.816087: step 7193, loss 0.562829.
Train: 2018-08-02T11:21:35.034780: step 7194, loss 0.506623.
Train: 2018-08-02T11:21:35.253478: step 7195, loss 0.611061.
Train: 2018-08-02T11:21:35.472183: step 7196, loss 0.578878.
Train: 2018-08-02T11:21:35.722093: step 7197, loss 0.482165.
Train: 2018-08-02T11:21:35.956440: step 7198, loss 0.643526.
Train: 2018-08-02T11:21:36.175138: step 7199, loss 0.56272.
Train: 2018-08-02T11:21:36.393812: step 7200, loss 0.578889.
Test: 2018-08-02T11:21:37.581032: step 7200, loss 0.549583.
Train: 2018-08-02T11:21:38.518315: step 7201, loss 0.514195.
Train: 2018-08-02T11:21:38.737012: step 7202, loss 0.546503.
Train: 2018-08-02T11:21:38.971333: step 7203, loss 0.570798.
Train: 2018-08-02T11:21:39.190032: step 7204, loss 0.522085.
Train: 2018-08-02T11:21:39.408729: step 7205, loss 0.570744.
Train: 2018-08-02T11:21:39.643050: step 7206, loss 0.554423.
Train: 2018-08-02T11:21:39.861774: step 7207, loss 0.546224.
Train: 2018-08-02T11:21:40.080447: step 7208, loss 0.554493.
Train: 2018-08-02T11:21:40.299171: step 7209, loss 0.653165.
Train: 2018-08-02T11:21:40.517846: step 7210, loss 0.611844.
Test: 2018-08-02T11:21:41.720688: step 7210, loss 0.549496.
Train: 2018-08-02T11:21:41.923791: step 7211, loss 0.472632.
Train: 2018-08-02T11:21:42.142490: step 7212, loss 0.570766.
Train: 2018-08-02T11:21:42.361188: step 7213, loss 0.554397.
Train: 2018-08-02T11:21:42.579889: step 7214, loss 0.554388.
Train: 2018-08-02T11:21:42.798591: step 7215, loss 0.61173.
Train: 2018-08-02T11:21:43.017290: step 7216, loss 0.587141.
Train: 2018-08-02T11:21:43.235989: step 7217, loss 0.578946.
Train: 2018-08-02T11:21:43.454689: step 7218, loss 0.578937.
Train: 2018-08-02T11:21:43.673389: step 7219, loss 0.546313.
Train: 2018-08-02T11:21:43.907702: step 7220, loss 0.595211.
Test: 2018-08-02T11:21:45.079277: step 7220, loss 0.548889.
Train: 2018-08-02T11:21:45.313630: step 7221, loss 0.546393.
Train: 2018-08-02T11:21:45.547917: step 7222, loss 0.619508.
Train: 2018-08-02T11:21:45.766641: step 7223, loss 0.554596.
Train: 2018-08-02T11:21:46.000962: step 7224, loss 0.530382.
Train: 2018-08-02T11:21:46.219635: step 7225, loss 0.603117.
Train: 2018-08-02T11:21:46.438366: step 7226, loss 0.578881.
Train: 2018-08-02T11:21:46.657063: step 7227, loss 0.570828.
Train: 2018-08-02T11:21:46.875757: step 7228, loss 0.530677.
Train: 2018-08-02T11:21:47.094460: step 7229, loss 0.522689.
Train: 2018-08-02T11:21:47.313161: step 7230, loss 0.57887.
Test: 2018-08-02T11:21:48.484729: step 7230, loss 0.548667.
Train: 2018-08-02T11:21:48.703455: step 7231, loss 0.570841.
Train: 2018-08-02T11:21:48.922157: step 7232, loss 0.554788.
Train: 2018-08-02T11:21:49.140827: step 7233, loss 0.602955.
Train: 2018-08-02T11:21:49.359562: step 7234, loss 0.5548.
Train: 2018-08-02T11:21:49.593876: step 7235, loss 0.56283.
Train: 2018-08-02T11:21:49.812575: step 7236, loss 0.626984.
Train: 2018-08-02T11:21:50.031273: step 7237, loss 0.562864.
Train: 2018-08-02T11:21:50.265588: step 7238, loss 0.658777.
Train: 2018-08-02T11:21:50.484261: step 7239, loss 0.578853.
Train: 2018-08-02T11:21:50.718588: step 7240, loss 0.570924.
Test: 2018-08-02T11:21:51.905803: step 7240, loss 0.550008.
Train: 2018-08-02T11:21:52.108881: step 7241, loss 0.64215.
Train: 2018-08-02T11:21:52.343232: step 7242, loss 0.578875.
Train: 2018-08-02T11:21:52.577553: step 7243, loss 0.484825.
Train: 2018-08-02T11:21:52.796250: step 7244, loss 0.438066.
Train: 2018-08-02T11:21:53.014945: step 7245, loss 0.516182.
Train: 2018-08-02T11:21:53.233647: step 7246, loss 0.508127.
Train: 2018-08-02T11:21:53.467968: step 7247, loss 0.507818.
Train: 2018-08-02T11:21:53.655430: step 7248, loss 0.562986.
Train: 2018-08-02T11:21:53.889715: step 7249, loss 0.610762.
Train: 2018-08-02T11:21:54.124035: step 7250, loss 0.626879.
Test: 2018-08-02T11:21:55.326877: step 7250, loss 0.549266.
Train: 2018-08-02T11:21:55.529987: step 7251, loss 0.538791.
Train: 2018-08-02T11:21:55.764305: step 7252, loss 0.546743.
Train: 2018-08-02T11:21:55.982975: step 7253, loss 0.562775.
Train: 2018-08-02T11:21:56.201699: step 7254, loss 0.538542.
Train: 2018-08-02T11:21:56.451639: step 7255, loss 0.562714.
Train: 2018-08-02T11:21:56.670337: step 7256, loss 0.522144.
Train: 2018-08-02T11:21:56.889011: step 7257, loss 0.603315.
Train: 2018-08-02T11:21:57.107746: step 7258, loss 0.562628.
Train: 2018-08-02T11:21:57.326440: step 7259, loss 0.587095.
Train: 2018-08-02T11:21:57.545132: step 7260, loss 0.611616.
Test: 2018-08-02T11:21:58.732330: step 7260, loss 0.548025.
Train: 2018-08-02T11:21:58.951053: step 7261, loss 0.529937.
Train: 2018-08-02T11:21:59.185348: step 7262, loss 0.619791.
Train: 2018-08-02T11:21:59.404094: step 7263, loss 0.578934.
Train: 2018-08-02T11:21:59.638367: step 7264, loss 0.554469.
Train: 2018-08-02T11:21:59.857096: step 7265, loss 0.538196.
Train: 2018-08-02T11:22:00.091411: step 7266, loss 0.505632.
Train: 2018-08-02T11:22:00.310112: step 7267, loss 0.603384.
Train: 2018-08-02T11:22:00.560059: step 7268, loss 0.611539.
Train: 2018-08-02T11:22:00.763129: step 7269, loss 0.55449.
Train: 2018-08-02T11:22:00.997424: step 7270, loss 0.562643.
Test: 2018-08-02T11:22:02.169025: step 7270, loss 0.54907.
Train: 2018-08-02T11:22:02.387754: step 7271, loss 0.570782.
Train: 2018-08-02T11:22:02.606453: step 7272, loss 0.59516.
Train: 2018-08-02T11:22:02.825148: step 7273, loss 0.546451.
Train: 2018-08-02T11:22:03.059483: step 7274, loss 0.546477.
Train: 2018-08-02T11:22:03.278171: step 7275, loss 0.578898.
Train: 2018-08-02T11:22:03.496864: step 7276, loss 0.54651.
Train: 2018-08-02T11:22:03.715569: step 7277, loss 0.611272.
Train: 2018-08-02T11:22:03.949883: step 7278, loss 0.635475.
Train: 2018-08-02T11:22:04.168582: step 7279, loss 0.562759.
Train: 2018-08-02T11:22:04.387286: step 7280, loss 0.570833.
Test: 2018-08-02T11:22:05.574477: step 7280, loss 0.549248.
Train: 2018-08-02T11:22:05.793176: step 7281, loss 0.522726.
Train: 2018-08-02T11:22:06.011876: step 7282, loss 0.578865.
Train: 2018-08-02T11:22:06.230600: step 7283, loss 0.554861.
Train: 2018-08-02T11:22:06.464925: step 7284, loss 0.498926.
Train: 2018-08-02T11:22:06.683593: step 7285, loss 0.570863.
Train: 2018-08-02T11:22:06.917938: step 7286, loss 0.578864.
Train: 2018-08-02T11:22:07.136642: step 7287, loss 0.586874.
Train: 2018-08-02T11:22:07.355311: step 7288, loss 0.5068.
Train: 2018-08-02T11:22:07.589631: step 7289, loss 0.514727.
Train: 2018-08-02T11:22:07.808357: step 7290, loss 0.570835.
Test: 2018-08-02T11:22:08.995551: step 7290, loss 0.550063.
Train: 2018-08-02T11:22:09.198653: step 7291, loss 0.570824.
Train: 2018-08-02T11:22:09.417327: step 7292, loss 0.522413.
Train: 2018-08-02T11:22:09.636057: step 7293, loss 0.481838.
Train: 2018-08-02T11:22:09.854758: step 7294, loss 0.603276.
Train: 2018-08-02T11:22:10.089045: step 7295, loss 0.611506.
Train: 2018-08-02T11:22:10.307777: step 7296, loss 0.51367.
Train: 2018-08-02T11:22:10.526473: step 7297, loss 0.554412.
Train: 2018-08-02T11:22:10.745167: step 7298, loss 0.529778.
Train: 2018-08-02T11:22:10.979494: step 7299, loss 0.521436.
Train: 2018-08-02T11:22:11.198187: step 7300, loss 0.513012.
Test: 2018-08-02T11:22:12.385382: step 7300, loss 0.549401.
Train: 2018-08-02T11:22:13.244558: step 7301, loss 0.554189.
Train: 2018-08-02T11:22:13.478907: step 7302, loss 0.529181.
Train: 2018-08-02T11:22:13.697602: step 7303, loss 0.59582.
Train: 2018-08-02T11:22:13.916274: step 7304, loss 0.562405.
Train: 2018-08-02T11:22:14.135004: step 7305, loss 0.545601.
Train: 2018-08-02T11:22:14.353671: step 7306, loss 0.553966.
Train: 2018-08-02T11:22:14.572402: step 7307, loss 0.545506.
Train: 2018-08-02T11:22:14.791102: step 7308, loss 0.537011.
Train: 2018-08-02T11:22:15.009768: step 7309, loss 0.587764.
Train: 2018-08-02T11:22:15.259734: step 7310, loss 0.562351.
Test: 2018-08-02T11:22:16.446931: step 7310, loss 0.547584.
Train: 2018-08-02T11:22:16.650013: step 7311, loss 0.655732.
Train: 2018-08-02T11:22:16.868732: step 7312, loss 0.562352.
Train: 2018-08-02T11:22:17.087436: step 7313, loss 0.579288.
Train: 2018-08-02T11:22:17.306137: step 7314, loss 0.604615.
Train: 2018-08-02T11:22:17.540451: step 7315, loss 0.621363.
Train: 2018-08-02T11:22:17.759155: step 7316, loss 0.52042.
Train: 2018-08-02T11:22:17.977853: step 7317, loss 0.620998.
Train: 2018-08-02T11:22:18.212143: step 7318, loss 0.579103.
Train: 2018-08-02T11:22:18.446466: step 7319, loss 0.496028.
Train: 2018-08-02T11:22:18.680811: step 7320, loss 0.570757.
Test: 2018-08-02T11:22:19.868005: step 7320, loss 0.547164.
Train: 2018-08-02T11:22:20.071107: step 7321, loss 0.636881.
Train: 2018-08-02T11:22:20.289782: step 7322, loss 0.504888.
Train: 2018-08-02T11:22:20.508481: step 7323, loss 0.570759.
Train: 2018-08-02T11:22:20.742827: step 7324, loss 0.54617.
Train: 2018-08-02T11:22:20.961499: step 7325, loss 0.603501.
Train: 2018-08-02T11:22:21.195851: step 7326, loss 0.513624.
Train: 2018-08-02T11:22:21.430171: step 7327, loss 0.521844.
Train: 2018-08-02T11:22:21.664487: step 7328, loss 0.570774.
Train: 2018-08-02T11:22:21.883159: step 7329, loss 0.636001.
Train: 2018-08-02T11:22:22.117509: step 7330, loss 0.684709.
Test: 2018-08-02T11:22:23.304701: step 7330, loss 0.548422.
Train: 2018-08-02T11:22:23.507802: step 7331, loss 0.570797.
Train: 2018-08-02T11:22:23.726507: step 7332, loss 0.482092.
Train: 2018-08-02T11:22:23.960799: step 7333, loss 0.619129.
Train: 2018-08-02T11:22:24.195145: step 7334, loss 0.586896.
Train: 2018-08-02T11:22:24.413841: step 7335, loss 0.578863.
Train: 2018-08-02T11:22:24.648138: step 7336, loss 0.515056.
Train: 2018-08-02T11:22:24.866865: step 7337, loss 0.515156.
Train: 2018-08-02T11:22:25.085559: step 7338, loss 0.515164.
Train: 2018-08-02T11:22:25.304261: step 7339, loss 0.554945.
Train: 2018-08-02T11:22:25.538583: step 7340, loss 0.522984.
Test: 2018-08-02T11:22:26.772638: step 7340, loss 0.547842.
Train: 2018-08-02T11:22:27.006958: step 7341, loss 0.506856.
Train: 2018-08-02T11:22:27.225657: step 7342, loss 0.586893.
Train: 2018-08-02T11:22:27.444388: step 7343, loss 0.627191.
Train: 2018-08-02T11:22:27.663086: step 7344, loss 0.57889.
Train: 2018-08-02T11:22:27.881784: step 7345, loss 0.578867.
Train: 2018-08-02T11:22:28.100482: step 7346, loss 0.562753.
Train: 2018-08-02T11:22:28.334772: step 7347, loss 0.514359.
Train: 2018-08-02T11:22:28.553471: step 7348, loss 0.538501.
Train: 2018-08-02T11:22:28.787818: step 7349, loss 0.497967.
Train: 2018-08-02T11:22:29.006490: step 7350, loss 0.627598.
Test: 2018-08-02T11:22:30.193712: step 7350, loss 0.549439.
Train: 2018-08-02T11:22:30.412441: step 7351, loss 0.522019.
Train: 2018-08-02T11:22:30.631141: step 7352, loss 0.521852.
Train: 2018-08-02T11:22:30.865463: step 7353, loss 0.5625.
Train: 2018-08-02T11:22:31.084129: step 7354, loss 0.603741.
Train: 2018-08-02T11:22:31.334100: step 7355, loss 0.529704.
Train: 2018-08-02T11:22:31.568417: step 7356, loss 0.513122.
Train: 2018-08-02T11:22:31.787120: step 7357, loss 0.529508.
Train: 2018-08-02T11:22:32.005819: step 7358, loss 0.57913.
Train: 2018-08-02T11:22:32.255729: step 7359, loss 0.587484.
Train: 2018-08-02T11:22:32.490075: step 7360, loss 0.562369.
Test: 2018-08-02T11:22:33.677272: step 7360, loss 0.54854.
Train: 2018-08-02T11:22:33.880348: step 7361, loss 0.545834.
Train: 2018-08-02T11:22:34.099047: step 7362, loss 0.55409.
Train: 2018-08-02T11:22:34.317771: step 7363, loss 0.579159.
Train: 2018-08-02T11:22:34.552099: step 7364, loss 0.520707.
Train: 2018-08-02T11:22:34.786417: step 7365, loss 0.61254.
Train: 2018-08-02T11:22:35.005117: step 7366, loss 0.562415.
Train: 2018-08-02T11:22:35.223809: step 7367, loss 0.587479.
Train: 2018-08-02T11:22:35.442515: step 7368, loss 0.545723.
Train: 2018-08-02T11:22:35.661207: step 7369, loss 0.529042.
Train: 2018-08-02T11:22:35.879882: step 7370, loss 0.562421.
Test: 2018-08-02T11:22:37.067103: step 7370, loss 0.548862.
Train: 2018-08-02T11:22:37.285802: step 7371, loss 0.579117.
Train: 2018-08-02T11:22:37.504531: step 7372, loss 0.554077.
Train: 2018-08-02T11:22:37.723224: step 7373, loss 0.645853.
Train: 2018-08-02T11:22:37.957544: step 7374, loss 0.620708.
Train: 2018-08-02T11:22:38.176217: step 7375, loss 0.462914.
Train: 2018-08-02T11:22:38.410538: step 7376, loss 0.529318.
Train: 2018-08-02T11:22:38.613628: step 7377, loss 0.579044.
Train: 2018-08-02T11:22:38.847962: step 7378, loss 0.52935.
Train: 2018-08-02T11:22:39.066662: step 7379, loss 0.603887.
Train: 2018-08-02T11:22:39.300955: step 7380, loss 0.537657.
Test: 2018-08-02T11:22:40.456933: step 7380, loss 0.548669.
Train: 2018-08-02T11:22:40.675633: step 7381, loss 0.529395.
Train: 2018-08-02T11:22:40.909978: step 7382, loss 0.554206.
Train: 2018-08-02T11:22:41.128651: step 7383, loss 0.587315.
Train: 2018-08-02T11:22:41.347351: step 7384, loss 0.521093.
Train: 2018-08-02T11:22:41.566076: step 7385, loss 0.57904.
Train: 2018-08-02T11:22:41.784748: step 7386, loss 0.595608.
Train: 2018-08-02T11:22:42.003446: step 7387, loss 0.545923.
Train: 2018-08-02T11:22:42.237768: step 7388, loss 0.570757.
Train: 2018-08-02T11:22:42.456498: step 7389, loss 0.562486.
Train: 2018-08-02T11:22:42.675192: step 7390, loss 0.595552.
Test: 2018-08-02T11:22:43.862386: step 7390, loss 0.548337.
Train: 2018-08-02T11:22:44.081110: step 7391, loss 0.504723.
Train: 2018-08-02T11:22:44.299811: step 7392, loss 0.554248.
Train: 2018-08-02T11:22:44.518508: step 7393, loss 0.579011.
Train: 2018-08-02T11:22:44.752834: step 7394, loss 0.5625.
Train: 2018-08-02T11:22:44.987124: step 7395, loss 0.636765.
Train: 2018-08-02T11:22:45.205847: step 7396, loss 0.554287.
Train: 2018-08-02T11:22:45.424538: step 7397, loss 0.513221.
Train: 2018-08-02T11:22:45.643249: step 7398, loss 0.521463.
Train: 2018-08-02T11:22:45.830700: step 7399, loss 0.492386.
Train: 2018-08-02T11:22:46.049398: step 7400, loss 0.636658.
Test: 2018-08-02T11:22:47.236596: step 7400, loss 0.548948.
Train: 2018-08-02T11:22:48.189499: step 7401, loss 0.653142.
Train: 2018-08-02T11:22:48.423819: step 7402, loss 0.521424.
Train: 2018-08-02T11:22:48.642548: step 7403, loss 0.611811.
Train: 2018-08-02T11:22:48.861245: step 7404, loss 0.578973.
Train: 2018-08-02T11:22:49.079945: step 7405, loss 0.497172.
Train: 2018-08-02T11:22:49.298641: step 7406, loss 0.603462.
Train: 2018-08-02T11:22:49.517312: step 7407, loss 0.456496.
Train: 2018-08-02T11:22:49.751634: step 7408, loss 0.603459.
Train: 2018-08-02T11:22:49.970361: step 7409, loss 0.58712.
Train: 2018-08-02T11:22:50.189055: step 7410, loss 0.619779.
Test: 2018-08-02T11:22:51.376251: step 7410, loss 0.549378.
Train: 2018-08-02T11:22:51.579354: step 7411, loss 0.570772.
Train: 2018-08-02T11:22:51.798056: step 7412, loss 0.562645.
Train: 2018-08-02T11:22:52.016752: step 7413, loss 0.513906.
Train: 2018-08-02T11:22:52.235456: step 7414, loss 0.489553.
Train: 2018-08-02T11:22:52.454154: step 7415, loss 0.587048.
Train: 2018-08-02T11:22:52.672824: step 7416, loss 0.554502.
Train: 2018-08-02T11:22:52.907144: step 7417, loss 0.546328.
Train: 2018-08-02T11:22:53.125873: step 7418, loss 0.529978.
Train: 2018-08-02T11:22:53.344540: step 7419, loss 0.652497.
Train: 2018-08-02T11:22:53.563264: step 7420, loss 0.562604.
Test: 2018-08-02T11:22:54.734840: step 7420, loss 0.548214.
Train: 2018-08-02T11:22:54.953540: step 7421, loss 0.570774.
Train: 2018-08-02T11:22:55.172265: step 7422, loss 0.53812.
Train: 2018-08-02T11:22:55.406559: step 7423, loss 0.529956.
Train: 2018-08-02T11:22:55.625283: step 7424, loss 0.480917.
Train: 2018-08-02T11:22:55.843985: step 7425, loss 0.587157.
Train: 2018-08-02T11:22:56.093928: step 7426, loss 0.570752.
Train: 2018-08-02T11:22:56.312627: step 7427, loss 0.611854.
Train: 2018-08-02T11:22:56.531325: step 7428, loss 0.587182.
Train: 2018-08-02T11:22:56.750019: step 7429, loss 0.570779.
Train: 2018-08-02T11:22:56.968720: step 7430, loss 0.595392.
Test: 2018-08-02T11:22:58.140292: step 7430, loss 0.548701.
Train: 2018-08-02T11:22:58.358992: step 7431, loss 0.529798.
Train: 2018-08-02T11:22:58.593313: step 7432, loss 0.538013.
Train: 2018-08-02T11:22:58.827632: step 7433, loss 0.578954.
Train: 2018-08-02T11:22:59.030747: step 7434, loss 0.587134.
Train: 2018-08-02T11:22:59.265054: step 7435, loss 0.472653.
Train: 2018-08-02T11:22:59.483758: step 7436, loss 0.497088.
Train: 2018-08-02T11:22:59.702457: step 7437, loss 0.636422.
Train: 2018-08-02T11:22:59.921127: step 7438, loss 0.562547.
Train: 2018-08-02T11:23:00.139850: step 7439, loss 0.578976.
Train: 2018-08-02T11:23:00.374176: step 7440, loss 0.636496.
Test: 2018-08-02T11:23:01.530124: step 7440, loss 0.549236.
Train: 2018-08-02T11:23:01.764445: step 7441, loss 0.529745.
Train: 2018-08-02T11:23:01.967552: step 7442, loss 0.546173.
Train: 2018-08-02T11:23:02.186223: step 7443, loss 0.595344.
Train: 2018-08-02T11:23:02.404949: step 7444, loss 0.521659.
Train: 2018-08-02T11:23:02.654862: step 7445, loss 0.529847.
Train: 2018-08-02T11:23:02.873589: step 7446, loss 0.554386.
Train: 2018-08-02T11:23:03.092289: step 7447, loss 0.619935.
Train: 2018-08-02T11:23:03.310957: step 7448, loss 0.619904.
Train: 2018-08-02T11:23:03.529681: step 7449, loss 0.546246.
Train: 2018-08-02T11:23:03.748355: step 7450, loss 0.570771.
Test: 2018-08-02T11:23:04.919955: step 7450, loss 0.548636.
Train: 2018-08-02T11:23:05.185550: step 7451, loss 0.595228.
Train: 2018-08-02T11:23:05.404242: step 7452, loss 0.538245.
Train: 2018-08-02T11:23:05.622946: step 7453, loss 0.595157.
Train: 2018-08-02T11:23:05.841615: step 7454, loss 0.514033.
Train: 2018-08-02T11:23:06.075965: step 7455, loss 0.52217.
Train: 2018-08-02T11:23:06.294681: step 7456, loss 0.562684.
Train: 2018-08-02T11:23:06.528978: step 7457, loss 0.53023.
Train: 2018-08-02T11:23:06.763304: step 7458, loss 0.611397.
Train: 2018-08-02T11:23:06.982003: step 7459, loss 0.652007.
Train: 2018-08-02T11:23:07.200696: step 7460, loss 0.522164.
Test: 2018-08-02T11:23:08.372272: step 7460, loss 0.548431.
Train: 2018-08-02T11:23:08.590996: step 7461, loss 0.603189.
Train: 2018-08-02T11:23:08.809693: step 7462, loss 0.570805.
Train: 2018-08-02T11:23:09.028399: step 7463, loss 0.619233.
Train: 2018-08-02T11:23:09.262689: step 7464, loss 0.594969.
Train: 2018-08-02T11:23:09.481418: step 7465, loss 0.60293.
Train: 2018-08-02T11:23:09.715732: step 7466, loss 0.578862.
Train: 2018-08-02T11:23:09.950058: step 7467, loss 0.610701.
Train: 2018-08-02T11:23:10.184379: step 7468, loss 0.531307.
Train: 2018-08-02T11:23:10.403047: step 7469, loss 0.523554.
Train: 2018-08-02T11:23:10.621785: step 7470, loss 0.563087.
Test: 2018-08-02T11:23:11.808967: step 7470, loss 0.548683.
Train: 2018-08-02T11:23:12.012075: step 7471, loss 0.673404.
Train: 2018-08-02T11:23:12.230767: step 7472, loss 0.555327.
Train: 2018-08-02T11:23:12.449472: step 7473, loss 0.531927.
Train: 2018-08-02T11:23:12.683794: step 7474, loss 0.594512.
Train: 2018-08-02T11:23:12.902491: step 7475, loss 0.563301.
Train: 2018-08-02T11:23:13.121199: step 7476, loss 0.532197.
Train: 2018-08-02T11:23:13.371153: step 7477, loss 0.571121.
Train: 2018-08-02T11:23:13.589835: step 7478, loss 0.610015.
Train: 2018-08-02T11:23:13.808498: step 7479, loss 0.540068.
Train: 2018-08-02T11:23:14.042852: step 7480, loss 0.555611.
Test: 2018-08-02T11:23:15.230044: step 7480, loss 0.550758.
Train: 2018-08-02T11:23:15.433142: step 7481, loss 0.586676.
Train: 2018-08-02T11:23:15.651848: step 7482, loss 0.509009.
Train: 2018-08-02T11:23:15.870541: step 7483, loss 0.633353.
Train: 2018-08-02T11:23:16.104867: step 7484, loss 0.547794.
Train: 2018-08-02T11:23:16.323535: step 7485, loss 0.594464.
Train: 2018-08-02T11:23:16.542260: step 7486, loss 0.493317.
Train: 2018-08-02T11:23:16.760965: step 7487, loss 0.563303.
Train: 2018-08-02T11:23:16.995253: step 7488, loss 0.563265.
Train: 2018-08-02T11:23:17.213983: step 7489, loss 0.524093.
Train: 2018-08-02T11:23:17.432681: step 7490, loss 0.547471.
Test: 2018-08-02T11:23:18.619872: step 7490, loss 0.549415.
Train: 2018-08-02T11:23:18.822981: step 7491, loss 0.594619.
Train: 2018-08-02T11:23:19.057302: step 7492, loss 0.507807.
Train: 2018-08-02T11:23:19.291615: step 7493, loss 0.586783.
Train: 2018-08-02T11:23:19.525911: step 7494, loss 0.515288.
Train: 2018-08-02T11:23:19.744609: step 7495, loss 0.530991.
Train: 2018-08-02T11:23:19.963332: step 7496, loss 0.610928.
Train: 2018-08-02T11:23:20.182007: step 7497, loss 0.562807.
Train: 2018-08-02T11:23:20.416359: step 7498, loss 0.506372.
Train: 2018-08-02T11:23:20.635056: step 7499, loss 0.530363.
Train: 2018-08-02T11:23:20.853724: step 7500, loss 0.546381.
Test: 2018-08-02T11:23:22.040946: step 7500, loss 0.548792.
Train: 2018-08-02T11:23:22.931393: step 7501, loss 0.546254.
Train: 2018-08-02T11:23:23.165710: step 7502, loss 0.504948.
Train: 2018-08-02T11:23:23.400003: step 7503, loss 0.570715.
Train: 2018-08-02T11:23:23.649982: step 7504, loss 0.595866.
Train: 2018-08-02T11:23:23.868646: step 7505, loss 0.570806.
Train: 2018-08-02T11:23:24.118585: step 7506, loss 0.536562.
Train: 2018-08-02T11:23:24.337283: step 7507, loss 0.538057.
Train: 2018-08-02T11:23:24.587225: step 7508, loss 0.519786.
Train: 2018-08-02T11:23:24.805923: step 7509, loss 0.502793.
Train: 2018-08-02T11:23:25.040244: step 7510, loss 0.520652.
Test: 2018-08-02T11:23:26.211844: step 7510, loss 0.547653.
Train: 2018-08-02T11:23:26.430544: step 7511, loss 0.569758.
Train: 2018-08-02T11:23:26.649248: step 7512, loss 0.580536.
Train: 2018-08-02T11:23:26.867941: step 7513, loss 0.613998.
Train: 2018-08-02T11:23:27.102287: step 7514, loss 0.597409.
Train: 2018-08-02T11:23:27.336629: step 7515, loss 0.579473.
Train: 2018-08-02T11:23:27.555311: step 7516, loss 0.571097.
Train: 2018-08-02T11:23:27.774003: step 7517, loss 0.613223.
Train: 2018-08-02T11:23:27.992690: step 7518, loss 0.570749.
Train: 2018-08-02T11:23:28.211377: step 7519, loss 0.562383.
Train: 2018-08-02T11:23:28.445697: step 7520, loss 0.570777.
Test: 2018-08-02T11:23:29.617297: step 7520, loss 0.549415.
Train: 2018-08-02T11:23:29.836025: step 7521, loss 0.528983.
Train: 2018-08-02T11:23:30.070316: step 7522, loss 0.604141.
Train: 2018-08-02T11:23:30.289014: step 7523, loss 0.562441.
Train: 2018-08-02T11:23:30.507739: step 7524, loss 0.496063.
Train: 2018-08-02T11:23:30.726413: step 7525, loss 0.595634.
Train: 2018-08-02T11:23:30.959164: step 7526, loss 0.579035.
Train: 2018-08-02T11:23:31.177894: step 7527, loss 0.579018.
Train: 2018-08-02T11:23:31.396596: step 7528, loss 0.579001.
Train: 2018-08-02T11:23:31.615293: step 7529, loss 0.562534.
Train: 2018-08-02T11:23:31.833994: step 7530, loss 0.546143.
Test: 2018-08-02T11:23:33.005564: step 7530, loss 0.550587.
Train: 2018-08-02T11:23:33.224288: step 7531, loss 0.554379.
Train: 2018-08-02T11:23:33.443015: step 7532, loss 0.546222.
Train: 2018-08-02T11:23:33.677328: step 7533, loss 0.554418.
Train: 2018-08-02T11:23:33.895981: step 7534, loss 0.538089.
Train: 2018-08-02T11:23:34.130334: step 7535, loss 0.546258.
Train: 2018-08-02T11:23:34.364651: step 7536, loss 0.497195.
Train: 2018-08-02T11:23:34.598967: step 7537, loss 0.513426.
Train: 2018-08-02T11:23:34.833292: step 7538, loss 0.513235.
Train: 2018-08-02T11:23:35.051985: step 7539, loss 0.579013.
Train: 2018-08-02T11:23:35.270684: step 7540, loss 0.562307.
Test: 2018-08-02T11:23:36.442260: step 7540, loss 0.548338.
Train: 2018-08-02T11:23:36.660988: step 7541, loss 0.495823.
Train: 2018-08-02T11:23:36.879658: step 7542, loss 0.478866.
Train: 2018-08-02T11:23:37.098388: step 7543, loss 0.511149.
Train: 2018-08-02T11:23:37.317055: step 7544, loss 0.571818.
Train: 2018-08-02T11:23:37.535783: step 7545, loss 0.555566.
Train: 2018-08-02T11:23:37.754452: step 7546, loss 0.518685.
Train: 2018-08-02T11:23:37.973184: step 7547, loss 0.668069.
Train: 2018-08-02T11:23:38.207502: step 7548, loss 0.543633.
Train: 2018-08-02T11:23:38.441791: step 7549, loss 0.614084.
Train: 2018-08-02T11:23:38.613625: step 7550, loss 0.488888.
Test: 2018-08-02T11:23:39.785226: step 7550, loss 0.546968.
Train: 2018-08-02T11:23:40.003964: step 7551, loss 0.656248.
Train: 2018-08-02T11:23:40.222654: step 7552, loss 0.570844.
Train: 2018-08-02T11:23:40.456975: step 7553, loss 0.553936.
Train: 2018-08-02T11:23:40.691265: step 7554, loss 0.545221.
Train: 2018-08-02T11:23:40.909989: step 7555, loss 0.56223.
Train: 2018-08-02T11:23:41.159929: step 7556, loss 0.503086.
Train: 2018-08-02T11:23:41.378635: step 7557, loss 0.579214.
Train: 2018-08-02T11:23:41.597335: step 7558, loss 0.621611.
Train: 2018-08-02T11:23:41.816000: step 7559, loss 0.486583.
Train: 2018-08-02T11:23:42.050352: step 7560, loss 0.520239.
Test: 2018-08-02T11:23:43.221922: step 7560, loss 0.54714.
Train: 2018-08-02T11:23:43.440651: step 7561, loss 0.486552.
Train: 2018-08-02T11:23:43.674972: step 7562, loss 0.494854.
Train: 2018-08-02T11:23:43.909278: step 7563, loss 0.545409.
Train: 2018-08-02T11:23:44.143611: step 7564, loss 0.587722.
Train: 2018-08-02T11:23:44.362279: step 7565, loss 0.596412.
Train: 2018-08-02T11:23:44.581009: step 7566, loss 0.579392.
Train: 2018-08-02T11:23:44.799708: step 7567, loss 0.579252.
Train: 2018-08-02T11:23:45.049619: step 7568, loss 0.596353.
Train: 2018-08-02T11:23:45.268348: step 7569, loss 0.519781.
Train: 2018-08-02T11:23:45.487049: step 7570, loss 0.553892.
Test: 2018-08-02T11:23:46.689859: step 7570, loss 0.54776.
Train: 2018-08-02T11:23:46.892968: step 7571, loss 0.562201.
Train: 2018-08-02T11:23:47.111636: step 7572, loss 0.596337.
Train: 2018-08-02T11:23:47.330359: step 7573, loss 0.570886.
Train: 2018-08-02T11:23:47.564685: step 7574, loss 0.562416.
Train: 2018-08-02T11:23:47.783354: step 7575, loss 0.494892.
Train: 2018-08-02T11:23:48.002085: step 7576, loss 0.553846.
Train: 2018-08-02T11:23:48.236397: step 7577, loss 0.612857.
Train: 2018-08-02T11:23:48.470718: step 7578, loss 0.579222.
Train: 2018-08-02T11:23:48.689393: step 7579, loss 0.570792.
Train: 2018-08-02T11:23:48.908091: step 7580, loss 0.537241.
Test: 2018-08-02T11:23:50.095312: step 7580, loss 0.547819.
Train: 2018-08-02T11:23:50.298414: step 7581, loss 0.545607.
Train: 2018-08-02T11:23:50.517118: step 7582, loss 0.587272.
Train: 2018-08-02T11:23:50.735787: step 7583, loss 0.570829.
Train: 2018-08-02T11:23:50.954486: step 7584, loss 0.621168.
Train: 2018-08-02T11:23:51.188810: step 7585, loss 0.579096.
Train: 2018-08-02T11:23:51.407507: step 7586, loss 0.537536.
Train: 2018-08-02T11:23:51.626204: step 7587, loss 0.504726.
Train: 2018-08-02T11:23:51.844932: step 7588, loss 0.579037.
Train: 2018-08-02T11:23:52.079247: step 7589, loss 0.595424.
Train: 2018-08-02T11:23:52.297946: step 7590, loss 0.496835.
Test: 2018-08-02T11:23:53.485143: step 7590, loss 0.549401.
Train: 2018-08-02T11:23:53.703872: step 7591, loss 0.570754.
Train: 2018-08-02T11:23:53.938193: step 7592, loss 0.546141.
Train: 2018-08-02T11:23:54.172482: step 7593, loss 0.60358.
Train: 2018-08-02T11:23:54.422424: step 7594, loss 0.480593.
Train: 2018-08-02T11:23:54.641146: step 7595, loss 0.652827.
Train: 2018-08-02T11:23:54.859846: step 7596, loss 0.570778.
Train: 2018-08-02T11:23:55.078520: step 7597, loss 0.554412.
Train: 2018-08-02T11:23:55.312871: step 7598, loss 0.578941.
Train: 2018-08-02T11:23:55.547173: step 7599, loss 0.521817.
Train: 2018-08-02T11:23:55.765860: step 7600, loss 0.611558.
Test: 2018-08-02T11:23:56.953081: step 7600, loss 0.549778.
Train: 2018-08-02T11:23:57.749769: step 7601, loss 0.554486.
Train: 2018-08-02T11:23:57.999745: step 7602, loss 0.497548.
Train: 2018-08-02T11:23:58.218410: step 7603, loss 0.570778.
Train: 2018-08-02T11:23:58.452731: step 7604, loss 0.497478.
Train: 2018-08-02T11:23:58.687081: step 7605, loss 0.570772.
Train: 2018-08-02T11:23:58.905750: step 7606, loss 0.587111.
Train: 2018-08-02T11:23:59.124447: step 7607, loss 0.587121.
Train: 2018-08-02T11:23:59.343177: step 7608, loss 0.619833.
Train: 2018-08-02T11:23:59.577468: step 7609, loss 0.546268.
Train: 2018-08-02T11:23:59.796196: step 7610, loss 0.497324.
Test: 2018-08-02T11:24:00.967766: step 7610, loss 0.548774.
Train: 2018-08-02T11:24:01.186465: step 7611, loss 0.644277.
Train: 2018-08-02T11:24:01.420787: step 7612, loss 0.57893.
Train: 2018-08-02T11:24:01.639485: step 7613, loss 0.635946.
Train: 2018-08-02T11:24:01.858185: step 7614, loss 0.578908.
Train: 2018-08-02T11:24:02.108149: step 7615, loss 0.514122.
Train: 2018-08-02T11:24:02.326822: step 7616, loss 0.554638.
Train: 2018-08-02T11:24:02.561143: step 7617, loss 0.554662.
Train: 2018-08-02T11:24:02.795493: step 7618, loss 0.498208.
Train: 2018-08-02T11:24:03.014192: step 7619, loss 0.538514.
Train: 2018-08-02T11:24:03.248515: step 7620, loss 0.546547.
Test: 2018-08-02T11:24:04.451325: step 7620, loss 0.548986.
Train: 2018-08-02T11:24:04.670049: step 7621, loss 0.586994.
Train: 2018-08-02T11:24:04.888722: step 7622, loss 0.546474.
Train: 2018-08-02T11:24:05.107455: step 7623, loss 0.562674.
Train: 2018-08-02T11:24:05.357398: step 7624, loss 0.611412.
Train: 2018-08-02T11:24:05.576062: step 7625, loss 0.457012.
Train: 2018-08-02T11:24:05.826003: step 7626, loss 0.481162.
Train: 2018-08-02T11:24:06.044732: step 7627, loss 0.603483.
Train: 2018-08-02T11:24:06.263427: step 7628, loss 0.628203.
Train: 2018-08-02T11:24:06.497721: step 7629, loss 0.562536.
Train: 2018-08-02T11:24:06.716451: step 7630, loss 0.628287.
Test: 2018-08-02T11:24:07.903642: step 7630, loss 0.54695.
Train: 2018-08-02T11:24:08.122341: step 7631, loss 0.505078.
Train: 2018-08-02T11:24:08.341038: step 7632, loss 0.521468.
Train: 2018-08-02T11:24:08.575384: step 7633, loss 0.620149.
Train: 2018-08-02T11:24:08.809681: step 7634, loss 0.603661.
Train: 2018-08-02T11:24:09.044025: step 7635, loss 0.578987.
Train: 2018-08-02T11:24:09.278347: step 7636, loss 0.52976.
Train: 2018-08-02T11:24:09.512666: step 7637, loss 0.53798.
Train: 2018-08-02T11:24:09.731369: step 7638, loss 0.587156.
Train: 2018-08-02T11:24:09.981305: step 7639, loss 0.578955.
Train: 2018-08-02T11:24:10.199979: step 7640, loss 0.52166.
Test: 2018-08-02T11:24:11.387201: step 7640, loss 0.549099.
Train: 2018-08-02T11:24:11.605933: step 7641, loss 0.554395.
Train: 2018-08-02T11:24:11.824598: step 7642, loss 0.538015.
Train: 2018-08-02T11:24:12.043296: step 7643, loss 0.505213.
Train: 2018-08-02T11:24:12.277643: step 7644, loss 0.562551.
Train: 2018-08-02T11:24:12.511938: step 7645, loss 0.58721.
Train: 2018-08-02T11:24:12.730637: step 7646, loss 0.570754.
Train: 2018-08-02T11:24:12.964990: step 7647, loss 0.570758.
Train: 2018-08-02T11:24:13.183680: step 7648, loss 0.521309.
Train: 2018-08-02T11:24:13.402388: step 7649, loss 0.579008.
Train: 2018-08-02T11:24:13.621081: step 7650, loss 0.570758.
Test: 2018-08-02T11:24:14.808275: step 7650, loss 0.547944.
Train: 2018-08-02T11:24:15.042627: step 7651, loss 0.58727.
Train: 2018-08-02T11:24:15.276940: step 7652, loss 0.587274.
Train: 2018-08-02T11:24:15.480025: step 7653, loss 0.554257.
Train: 2018-08-02T11:24:15.714338: step 7654, loss 0.653185.
Train: 2018-08-02T11:24:15.933042: step 7655, loss 0.496779.
Train: 2018-08-02T11:24:16.167358: step 7656, loss 0.55434.
Train: 2018-08-02T11:24:16.386061: step 7657, loss 0.595379.
Train: 2018-08-02T11:24:16.604755: step 7658, loss 0.578957.
Train: 2018-08-02T11:24:16.839050: step 7659, loss 0.619836.
Train: 2018-08-02T11:24:17.057779: step 7660, loss 0.546309.
Test: 2018-08-02T11:24:18.244971: step 7660, loss 0.548681.
Train: 2018-08-02T11:24:18.463693: step 7661, loss 0.603325.
Train: 2018-08-02T11:24:18.682399: step 7662, loss 0.643803.
Train: 2018-08-02T11:24:18.916722: step 7663, loss 0.530432.
Train: 2018-08-02T11:24:19.135417: step 7664, loss 0.490336.
Train: 2018-08-02T11:24:19.369740: step 7665, loss 0.522597.
Train: 2018-08-02T11:24:19.588430: step 7666, loss 0.546706.
Train: 2018-08-02T11:24:19.807134: step 7667, loss 0.602983.
Train: 2018-08-02T11:24:20.025803: step 7668, loss 0.578867.
Train: 2018-08-02T11:24:20.244534: step 7669, loss 0.522593.
Train: 2018-08-02T11:24:20.463231: step 7670, loss 0.530486.
Test: 2018-08-02T11:24:21.650423: step 7670, loss 0.549467.
Train: 2018-08-02T11:24:21.884743: step 7671, loss 0.603152.
Train: 2018-08-02T11:24:22.103472: step 7672, loss 0.530447.
Train: 2018-08-02T11:24:22.322166: step 7673, loss 0.595197.
Train: 2018-08-02T11:24:22.572083: step 7674, loss 0.530309.
Train: 2018-08-02T11:24:22.775190: step 7675, loss 0.635626.
Train: 2018-08-02T11:24:22.993883: step 7676, loss 0.5787.
Train: 2018-08-02T11:24:23.228207: step 7677, loss 0.522569.
Train: 2018-08-02T11:24:23.446877: step 7678, loss 0.563018.
Train: 2018-08-02T11:24:23.681222: step 7679, loss 0.554861.
Train: 2018-08-02T11:24:23.915548: step 7680, loss 0.49016.
Test: 2018-08-02T11:24:25.118361: step 7680, loss 0.547924.
Train: 2018-08-02T11:24:25.321438: step 7681, loss 0.611058.
Train: 2018-08-02T11:24:25.555789: step 7682, loss 0.546622.
Train: 2018-08-02T11:24:25.805724: step 7683, loss 0.586786.
Train: 2018-08-02T11:24:26.040045: step 7684, loss 0.595122.
Train: 2018-08-02T11:24:26.258749: step 7685, loss 0.546534.
Train: 2018-08-02T11:24:26.477448: step 7686, loss 0.62742.
Train: 2018-08-02T11:24:26.696142: step 7687, loss 0.530434.
Train: 2018-08-02T11:24:26.930461: step 7688, loss 0.611221.
Train: 2018-08-02T11:24:27.149161: step 7689, loss 0.514356.
Train: 2018-08-02T11:24:27.367866: step 7690, loss 0.595035.
Test: 2018-08-02T11:24:28.555056: step 7690, loss 0.549699.
Train: 2018-08-02T11:24:28.789377: step 7691, loss 0.514481.
Train: 2018-08-02T11:24:29.008101: step 7692, loss 0.506402.
Train: 2018-08-02T11:24:29.242420: step 7693, loss 0.595041.
Train: 2018-08-02T11:24:29.461097: step 7694, loss 0.538466.
Train: 2018-08-02T11:24:29.695415: step 7695, loss 0.562724.
Train: 2018-08-02T11:24:29.929735: step 7696, loss 0.570801.
Train: 2018-08-02T11:24:30.148458: step 7697, loss 0.59504.
Train: 2018-08-02T11:24:30.382778: step 7698, loss 0.465366.
Train: 2018-08-02T11:24:30.601453: step 7699, loss 0.58703.
Train: 2018-08-02T11:24:30.851394: step 7700, loss 0.521937.
Test: 2018-08-02T11:24:32.038615: step 7700, loss 0.547805.
Train: 2018-08-02T11:24:32.866579: step 7701, loss 0.545007.
Train: 2018-08-02T11:24:33.085278: step 7702, loss 0.578943.
Train: 2018-08-02T11:24:33.335187: step 7703, loss 0.587251.
Train: 2018-08-02T11:24:33.553916: step 7704, loss 0.562622.
Train: 2018-08-02T11:24:33.772610: step 7705, loss 0.554493.
Train: 2018-08-02T11:24:34.006906: step 7706, loss 0.537879.
Train: 2018-08-02T11:24:34.241225: step 7707, loss 0.562397.
Train: 2018-08-02T11:24:34.459923: step 7708, loss 0.612269.
Train: 2018-08-02T11:24:34.694246: step 7709, loss 0.5212.
Train: 2018-08-02T11:24:34.928594: step 7710, loss 0.54604.
Test: 2018-08-02T11:24:36.115786: step 7710, loss 0.549065.
Train: 2018-08-02T11:24:36.334517: step 7711, loss 0.537602.
Train: 2018-08-02T11:24:36.553182: step 7712, loss 0.562476.
Train: 2018-08-02T11:24:36.771906: step 7713, loss 0.521079.
Train: 2018-08-02T11:24:36.990608: step 7714, loss 0.595719.
Train: 2018-08-02T11:24:37.209309: step 7715, loss 0.59571.
Train: 2018-08-02T11:24:37.428014: step 7716, loss 0.620651.
Train: 2018-08-02T11:24:37.662329: step 7717, loss 0.496156.
Train: 2018-08-02T11:24:37.881021: step 7718, loss 0.545892.
Train: 2018-08-02T11:24:38.099725: step 7719, loss 0.529292.
Train: 2018-08-02T11:24:38.334041: step 7720, loss 0.612254.
Test: 2018-08-02T11:24:39.505616: step 7720, loss 0.548989.
Train: 2018-08-02T11:24:39.724315: step 7721, loss 0.554163.
Train: 2018-08-02T11:24:39.958635: step 7722, loss 0.520993.
Train: 2018-08-02T11:24:40.177333: step 7723, loss 0.587357.
Train: 2018-08-02T11:24:40.396059: step 7724, loss 0.54586.
Train: 2018-08-02T11:24:40.645975: step 7725, loss 0.562457.
Train: 2018-08-02T11:24:40.880320: step 7726, loss 0.545851.
Train: 2018-08-02T11:24:41.099022: step 7727, loss 0.570759.
Train: 2018-08-02T11:24:41.317692: step 7728, loss 0.579066.
Train: 2018-08-02T11:24:41.567634: step 7729, loss 0.562456.
Train: 2018-08-02T11:24:41.801954: step 7730, loss 0.579058.
Test: 2018-08-02T11:24:42.989175: step 7730, loss 0.548425.
Train: 2018-08-02T11:24:43.207878: step 7731, loss 0.628806.
Train: 2018-08-02T11:24:43.426598: step 7732, loss 0.612122.
Train: 2018-08-02T11:24:43.645271: step 7733, loss 0.611984.
Train: 2018-08-02T11:24:43.864003: step 7734, loss 0.496862.
Train: 2018-08-02T11:24:44.082695: step 7735, loss 0.587147.
Train: 2018-08-02T11:24:44.301399: step 7736, loss 0.562599.
Train: 2018-08-02T11:24:44.535689: step 7737, loss 0.611526.
Train: 2018-08-02T11:24:44.770034: step 7738, loss 0.538294.
Train: 2018-08-02T11:24:44.988708: step 7739, loss 0.554589.
Train: 2018-08-02T11:24:45.223027: step 7740, loss 0.538454.
Test: 2018-08-02T11:24:46.394628: step 7740, loss 0.549983.
Train: 2018-08-02T11:24:46.597707: step 7741, loss 0.586965.
Train: 2018-08-02T11:24:46.816435: step 7742, loss 0.570816.
Train: 2018-08-02T11:24:47.035129: step 7743, loss 0.546665.
Train: 2018-08-02T11:24:47.269425: step 7744, loss 0.594966.
Train: 2018-08-02T11:24:47.488155: step 7745, loss 0.498541.
Train: 2018-08-02T11:24:47.722442: step 7746, loss 0.570836.
Train: 2018-08-02T11:24:47.956789: step 7747, loss 0.635122.
Train: 2018-08-02T11:24:48.191107: step 7748, loss 0.442479.
Train: 2018-08-02T11:24:48.409781: step 7749, loss 0.602979.
Train: 2018-08-02T11:24:48.644101: step 7750, loss 0.594952.
Test: 2018-08-02T11:24:49.831323: step 7750, loss 0.548818.
Train: 2018-08-02T11:24:50.034432: step 7751, loss 0.611024.
Train: 2018-08-02T11:24:50.253131: step 7752, loss 0.578868.
Train: 2018-08-02T11:24:50.487457: step 7753, loss 0.546805.
Train: 2018-08-02T11:24:50.706119: step 7754, loss 0.54683.
Train: 2018-08-02T11:24:50.940464: step 7755, loss 0.58687.
Train: 2018-08-02T11:24:51.159169: step 7756, loss 0.562865.
Train: 2018-08-02T11:24:51.393483: step 7757, loss 0.578861.
Train: 2018-08-02T11:24:51.612156: step 7758, loss 0.546902.
Train: 2018-08-02T11:24:51.830856: step 7759, loss 0.522948.
Train: 2018-08-02T11:24:52.080829: step 7760, loss 0.554878.
Test: 2018-08-02T11:24:53.268019: step 7760, loss 0.548937.
Train: 2018-08-02T11:24:53.471097: step 7761, loss 0.538844.
Train: 2018-08-02T11:24:53.689825: step 7762, loss 0.506708.
Train: 2018-08-02T11:24:53.924144: step 7763, loss 0.586916.
Train: 2018-08-02T11:24:54.142819: step 7764, loss 0.570813.
Train: 2018-08-02T11:24:54.377134: step 7765, loss 0.562727.
Train: 2018-08-02T11:24:54.595857: step 7766, loss 0.562722.
Train: 2018-08-02T11:24:54.814562: step 7767, loss 0.586983.
Train: 2018-08-02T11:24:55.048878: step 7768, loss 0.578879.
Train: 2018-08-02T11:24:55.267550: step 7769, loss 0.505888.
Train: 2018-08-02T11:24:55.486249: step 7770, loss 0.51396.
Test: 2018-08-02T11:24:56.673471: step 7770, loss 0.548442.
Train: 2018-08-02T11:24:56.892194: step 7771, loss 0.52183.
Train: 2018-08-02T11:24:57.126515: step 7772, loss 0.587056.
Train: 2018-08-02T11:24:57.345222: step 7773, loss 0.628225.
Train: 2018-08-02T11:24:57.563887: step 7774, loss 0.546195.
Train: 2018-08-02T11:24:57.813855: step 7775, loss 0.521337.
Train: 2018-08-02T11:24:58.048180: step 7776, loss 0.529531.
Train: 2018-08-02T11:24:58.266848: step 7777, loss 0.54608.
Train: 2018-08-02T11:24:58.485571: step 7778, loss 0.678386.
Train: 2018-08-02T11:24:58.704277: step 7779, loss 0.5623.
Train: 2018-08-02T11:24:58.938592: step 7780, loss 0.603836.
Test: 2018-08-02T11:25:00.125788: step 7780, loss 0.548504.
Train: 2018-08-02T11:25:00.344512: step 7781, loss 0.553841.
Train: 2018-08-02T11:25:00.563215: step 7782, loss 0.554465.
Train: 2018-08-02T11:25:00.781892: step 7783, loss 0.513213.
Train: 2018-08-02T11:25:01.000607: step 7784, loss 0.521286.
Train: 2018-08-02T11:25:01.219312: step 7785, loss 0.537727.
Train: 2018-08-02T11:25:01.438006: step 7786, loss 0.579223.
Train: 2018-08-02T11:25:01.687955: step 7787, loss 0.529357.
Train: 2018-08-02T11:25:01.922245: step 7788, loss 0.562702.
Train: 2018-08-02T11:25:02.156562: step 7789, loss 0.537128.
Train: 2018-08-02T11:25:02.375286: step 7790, loss 0.5788.
Test: 2018-08-02T11:25:03.562483: step 7790, loss 0.550205.
Train: 2018-08-02T11:25:03.781181: step 7791, loss 0.545672.
Train: 2018-08-02T11:25:03.999910: step 7792, loss 0.621637.
Train: 2018-08-02T11:25:04.234213: step 7793, loss 0.545015.
Train: 2018-08-02T11:25:04.452930: step 7794, loss 0.603893.
Train: 2018-08-02T11:25:04.671624: step 7795, loss 0.587217.
Train: 2018-08-02T11:25:04.905951: step 7796, loss 0.521328.
Train: 2018-08-02T11:25:05.124618: step 7797, loss 0.562242.
Train: 2018-08-02T11:25:05.343340: step 7798, loss 0.620884.
Train: 2018-08-02T11:25:05.562022: step 7799, loss 0.562937.
Train: 2018-08-02T11:25:05.796365: step 7800, loss 0.570958.
Test: 2018-08-02T11:25:06.983556: step 7800, loss 0.549372.
Train: 2018-08-02T11:25:07.920868: step 7801, loss 0.513292.
Train: 2018-08-02T11:25:08.170811: step 7802, loss 0.644686.
Train: 2018-08-02T11:25:08.389503: step 7803, loss 0.529916.
Train: 2018-08-02T11:25:08.608219: step 7804, loss 0.587081.
Train: 2018-08-02T11:25:08.842498: step 7805, loss 0.497567.
Train: 2018-08-02T11:25:09.061226: step 7806, loss 0.595179.
Train: 2018-08-02T11:25:09.295516: step 7807, loss 0.522042.
Train: 2018-08-02T11:25:09.529880: step 7808, loss 0.562661.
Train: 2018-08-02T11:25:09.748534: step 7809, loss 0.554535.
Train: 2018-08-02T11:25:09.982879: step 7810, loss 0.55453.
Test: 2018-08-02T11:25:11.170077: step 7810, loss 0.549447.
Train: 2018-08-02T11:25:11.373184: step 7811, loss 0.513866.
Train: 2018-08-02T11:25:11.607498: step 7812, loss 0.546346.
Train: 2018-08-02T11:25:11.841819: step 7813, loss 0.538139.
Train: 2018-08-02T11:25:12.060519: step 7814, loss 0.546238.
Train: 2018-08-02T11:25:12.279222: step 7815, loss 0.546176.
Train: 2018-08-02T11:25:12.497891: step 7816, loss 0.554323.
Train: 2018-08-02T11:25:12.732211: step 7817, loss 0.513101.
Train: 2018-08-02T11:25:12.950910: step 7818, loss 0.587288.
Train: 2018-08-02T11:25:13.185260: step 7819, loss 0.562451.
Train: 2018-08-02T11:25:13.403960: step 7820, loss 0.604033.
Test: 2018-08-02T11:25:14.591151: step 7820, loss 0.547245.
Train: 2018-08-02T11:25:14.794253: step 7821, loss 0.554169.
Train: 2018-08-02T11:25:15.012926: step 7822, loss 0.595691.
Train: 2018-08-02T11:25:15.247247: step 7823, loss 0.512618.
Train: 2018-08-02T11:25:15.465974: step 7824, loss 0.628952.
Train: 2018-08-02T11:25:15.700297: step 7825, loss 0.620595.
Train: 2018-08-02T11:25:15.934619: step 7826, loss 0.537605.
Train: 2018-08-02T11:25:16.153317: step 7827, loss 0.554205.
Train: 2018-08-02T11:25:16.372014: step 7828, loss 0.545959.
Train: 2018-08-02T11:25:16.606305: step 7829, loss 0.587275.
Train: 2018-08-02T11:25:16.825033: step 7830, loss 0.55426.
Test: 2018-08-02T11:25:18.012224: step 7830, loss 0.54724.
Train: 2018-08-02T11:25:18.230955: step 7831, loss 0.603714.
Train: 2018-08-02T11:25:18.449622: step 7832, loss 0.562535.
Train: 2018-08-02T11:25:18.668347: step 7833, loss 0.480471.
Train: 2018-08-02T11:25:18.902640: step 7834, loss 0.61181.
Train: 2018-08-02T11:25:19.121363: step 7835, loss 0.554358.
Train: 2018-08-02T11:25:19.355659: step 7836, loss 0.611743.
Train: 2018-08-02T11:25:19.574390: step 7837, loss 0.570766.
Train: 2018-08-02T11:25:19.808687: step 7838, loss 0.472776.
Train: 2018-08-02T11:25:20.027403: step 7839, loss 0.595278.
Train: 2018-08-02T11:25:20.246102: step 7840, loss 0.57077.
Test: 2018-08-02T11:25:21.433298: step 7840, loss 0.549917.
Train: 2018-08-02T11:25:21.636375: step 7841, loss 0.464662.
Train: 2018-08-02T11:25:21.855106: step 7842, loss 0.562584.
Train: 2018-08-02T11:25:22.089395: step 7843, loss 0.628106.
Train: 2018-08-02T11:25:22.323746: step 7844, loss 0.505242.
Train: 2018-08-02T11:25:22.558066: step 7845, loss 0.537935.
Train: 2018-08-02T11:25:22.776734: step 7846, loss 0.52968.
Train: 2018-08-02T11:25:22.995432: step 7847, loss 0.537781.
Train: 2018-08-02T11:25:23.214132: step 7848, loss 0.537579.
Train: 2018-08-02T11:25:23.464072: step 7849, loss 0.537563.
Train: 2018-08-02T11:25:23.682772: step 7850, loss 0.570524.
Test: 2018-08-02T11:25:24.885619: step 7850, loss 0.547359.
Train: 2018-08-02T11:25:25.104313: step 7851, loss 0.612922.
Train: 2018-08-02T11:25:25.291796: step 7852, loss 0.688013.
Train: 2018-08-02T11:25:25.510499: step 7853, loss 0.511875.
Train: 2018-08-02T11:25:25.744818: step 7854, loss 0.537494.
Train: 2018-08-02T11:25:25.979118: step 7855, loss 0.620641.
Train: 2018-08-02T11:25:26.197837: step 7856, loss 0.612248.
Train: 2018-08-02T11:25:26.416519: step 7857, loss 0.479812.
Train: 2018-08-02T11:25:26.650827: step 7858, loss 0.570849.
Train: 2018-08-02T11:25:26.869557: step 7859, loss 0.537767.
Train: 2018-08-02T11:25:27.103872: step 7860, loss 0.562501.
Test: 2018-08-02T11:25:28.291067: step 7860, loss 0.548583.
Train: 2018-08-02T11:25:28.509766: step 7861, loss 0.603633.
Train: 2018-08-02T11:25:28.728497: step 7862, loss 0.587206.
Train: 2018-08-02T11:25:28.962786: step 7863, loss 0.537938.
Train: 2018-08-02T11:25:29.181515: step 7864, loss 0.562562.
Train: 2018-08-02T11:25:29.415829: step 7865, loss 0.546198.
Train: 2018-08-02T11:25:29.634502: step 7866, loss 0.578955.
Train: 2018-08-02T11:25:29.868848: step 7867, loss 0.578939.
Train: 2018-08-02T11:25:30.087550: step 7868, loss 0.538113.
Train: 2018-08-02T11:25:30.321875: step 7869, loss 0.562614.
Train: 2018-08-02T11:25:30.540568: step 7870, loss 0.562615.
Test: 2018-08-02T11:25:31.727762: step 7870, loss 0.550132.
Train: 2018-08-02T11:25:31.962108: step 7871, loss 0.611538.
Train: 2018-08-02T11:25:32.180781: step 7872, loss 0.562638.
Train: 2018-08-02T11:25:32.415102: step 7873, loss 0.57078.
Train: 2018-08-02T11:25:32.633831: step 7874, loss 0.59514.
Train: 2018-08-02T11:25:32.852503: step 7875, loss 0.522197.
Train: 2018-08-02T11:25:33.086849: step 7876, loss 0.530308.
Train: 2018-08-02T11:25:33.305519: step 7877, loss 0.57887.
Train: 2018-08-02T11:25:33.539839: step 7878, loss 0.603173.
Train: 2018-08-02T11:25:33.758536: step 7879, loss 0.5627.
Train: 2018-08-02T11:25:33.977237: step 7880, loss 0.570734.
Test: 2018-08-02T11:25:35.164457: step 7880, loss 0.549768.
Train: 2018-08-02T11:25:35.383182: step 7881, loss 0.562749.
Train: 2018-08-02T11:25:35.601883: step 7882, loss 0.546618.
Train: 2018-08-02T11:25:35.836204: step 7883, loss 0.530144.
Train: 2018-08-02T11:25:36.054899: step 7884, loss 0.538669.
Train: 2018-08-02T11:25:36.273573: step 7885, loss 0.595163.
Train: 2018-08-02T11:25:36.492271: step 7886, loss 0.571252.
Train: 2018-08-02T11:25:36.711000: step 7887, loss 0.57018.
Train: 2018-08-02T11:25:36.929699: step 7888, loss 0.570325.
Train: 2018-08-02T11:25:37.148393: step 7889, loss 0.586964.
Train: 2018-08-02T11:25:37.367067: step 7890, loss 0.537459.
Test: 2018-08-02T11:25:38.538668: step 7890, loss 0.548477.
Train: 2018-08-02T11:25:38.741744: step 7891, loss 0.529844.
Train: 2018-08-02T11:25:38.960473: step 7892, loss 0.670053.
Train: 2018-08-02T11:25:39.194763: step 7893, loss 0.586314.
Train: 2018-08-02T11:25:39.397842: step 7894, loss 0.513949.
Train: 2018-08-02T11:25:39.632161: step 7895, loss 0.562692.
Train: 2018-08-02T11:25:39.850884: step 7896, loss 0.529835.
Train: 2018-08-02T11:25:40.069589: step 7897, loss 0.56981.
Train: 2018-08-02T11:25:40.303880: step 7898, loss 0.585864.
Train: 2018-08-02T11:25:40.506957: step 7899, loss 0.561544.
Train: 2018-08-02T11:25:40.725680: step 7900, loss 0.54482.
Test: 2018-08-02T11:25:41.912877: step 7900, loss 0.548304.
Train: 2018-08-02T11:25:42.803318: step 7901, loss 0.588946.
Train: 2018-08-02T11:25:43.037616: step 7902, loss 0.624679.
Train: 2018-08-02T11:25:43.271959: step 7903, loss 0.56231.
Train: 2018-08-02T11:25:43.490659: step 7904, loss 0.65583.
Train: 2018-08-02T11:25:43.709360: step 7905, loss 0.52097.
Train: 2018-08-02T11:25:43.928030: step 7906, loss 0.617769.
Train: 2018-08-02T11:25:44.146760: step 7907, loss 0.602463.
Train: 2018-08-02T11:25:44.365427: step 7908, loss 0.516165.
Train: 2018-08-02T11:25:44.599748: step 7909, loss 0.562894.
Train: 2018-08-02T11:25:44.818477: step 7910, loss 0.58645.
Test: 2018-08-02T11:25:46.021290: step 7910, loss 0.549603.
Train: 2018-08-02T11:25:46.224395: step 7911, loss 0.540448.
Train: 2018-08-02T11:25:46.443091: step 7912, loss 0.530233.
Train: 2018-08-02T11:25:46.661792: step 7913, loss 0.563928.
Train: 2018-08-02T11:25:46.880495: step 7914, loss 0.556624.
Train: 2018-08-02T11:25:47.099195: step 7915, loss 0.548211.
Train: 2018-08-02T11:25:47.317862: step 7916, loss 0.61839.
Train: 2018-08-02T11:25:47.536591: step 7917, loss 0.555821.
Train: 2018-08-02T11:25:47.755289: step 7918, loss 0.625525.
Train: 2018-08-02T11:25:47.989589: step 7919, loss 0.617993.
Train: 2018-08-02T11:25:48.208307: step 7920, loss 0.540384.
Test: 2018-08-02T11:25:49.395500: step 7920, loss 0.550938.
Train: 2018-08-02T11:25:49.614233: step 7921, loss 0.525284.
Train: 2018-08-02T11:25:49.832897: step 7922, loss 0.479257.
Train: 2018-08-02T11:25:50.051596: step 7923, loss 0.625068.
Train: 2018-08-02T11:25:50.270326: step 7924, loss 0.50198.
Train: 2018-08-02T11:25:50.504616: step 7925, loss 0.563508.
Train: 2018-08-02T11:25:50.738949: step 7926, loss 0.594405.
Train: 2018-08-02T11:25:50.957634: step 7927, loss 0.625447.
Train: 2018-08-02T11:25:51.176334: step 7928, loss 0.547876.
Train: 2018-08-02T11:25:51.426300: step 7929, loss 0.57114.
Train: 2018-08-02T11:25:51.644976: step 7930, loss 0.617789.
Test: 2018-08-02T11:25:52.832196: step 7930, loss 0.550008.
Train: 2018-08-02T11:25:53.050925: step 7931, loss 0.49337.
Train: 2018-08-02T11:25:53.285240: step 7932, loss 0.555524.
Train: 2018-08-02T11:25:53.519569: step 7933, loss 0.547658.
Train: 2018-08-02T11:25:53.738233: step 7934, loss 0.524086.
Train: 2018-08-02T11:25:53.956932: step 7935, loss 0.54745.
Train: 2018-08-02T11:25:54.175661: step 7936, loss 0.578864.
Train: 2018-08-02T11:25:54.394359: step 7937, loss 0.59468.
Train: 2018-08-02T11:25:54.628680: step 7938, loss 0.578859.
Train: 2018-08-02T11:25:54.847377: step 7939, loss 0.555036.
Train: 2018-08-02T11:25:55.066047: step 7940, loss 0.554991.
Test: 2018-08-02T11:25:56.253269: step 7940, loss 0.549785.
Train: 2018-08-02T11:25:56.456377: step 7941, loss 0.554944.
Train: 2018-08-02T11:25:56.675051: step 7942, loss 0.522938.
Train: 2018-08-02T11:25:56.909365: step 7943, loss 0.530788.
Train: 2018-08-02T11:25:57.128065: step 7944, loss 0.570832.
Train: 2018-08-02T11:25:57.362415: step 7945, loss 0.554683.
Train: 2018-08-02T11:25:57.596706: step 7946, loss 0.586983.
Train: 2018-08-02T11:25:57.815457: step 7947, loss 0.546468.
Train: 2018-08-02T11:25:58.034101: step 7948, loss 0.562657.
Train: 2018-08-02T11:25:58.252826: step 7949, loss 0.521909.
Train: 2018-08-02T11:25:58.471533: step 7950, loss 0.578936.
Test: 2018-08-02T11:25:59.658722: step 7950, loss 0.548718.
Train: 2018-08-02T11:25:59.877446: step 7951, loss 0.644442.
Train: 2018-08-02T11:26:00.111771: step 7952, loss 0.513459.
Train: 2018-08-02T11:26:00.346094: step 7953, loss 0.587155.
Train: 2018-08-02T11:26:00.564784: step 7954, loss 0.562563.
Train: 2018-08-02T11:26:00.783459: step 7955, loss 0.513343.
Train: 2018-08-02T11:26:01.002188: step 7956, loss 0.513256.
Train: 2018-08-02T11:26:01.220884: step 7957, loss 0.562521.
Train: 2018-08-02T11:26:01.455176: step 7958, loss 0.587258.
Train: 2018-08-02T11:26:01.673900: step 7959, loss 0.562497.
Train: 2018-08-02T11:26:01.892574: step 7960, loss 0.54594.
Test: 2018-08-02T11:26:03.079795: step 7960, loss 0.54788.
Train: 2018-08-02T11:26:03.282899: step 7961, loss 0.496217.
Train: 2018-08-02T11:26:03.501573: step 7962, loss 0.520937.
Train: 2018-08-02T11:26:03.751515: step 7963, loss 0.554102.
Train: 2018-08-02T11:26:03.970212: step 7964, loss 0.520632.
Train: 2018-08-02T11:26:04.188942: step 7965, loss 0.528875.
Train: 2018-08-02T11:26:04.407609: step 7966, loss 0.58764.
Train: 2018-08-02T11:26:04.657551: step 7967, loss 0.545466.
Train: 2018-08-02T11:26:04.891902: step 7968, loss 0.553925.
Train: 2018-08-02T11:26:05.110606: step 7969, loss 0.519975.
Train: 2018-08-02T11:26:05.329269: step 7970, loss 0.553848.
Test: 2018-08-02T11:26:06.516490: step 7970, loss 0.548298.
Train: 2018-08-02T11:26:06.719601: step 7971, loss 0.553829.
Train: 2018-08-02T11:26:06.953890: step 7972, loss 0.545288.
Train: 2018-08-02T11:26:07.172617: step 7973, loss 0.53666.
Train: 2018-08-02T11:26:07.406907: step 7974, loss 0.579469.
Train: 2018-08-02T11:26:07.625605: step 7975, loss 0.639611.
Train: 2018-08-02T11:26:07.844306: step 7976, loss 0.613833.
Train: 2018-08-02T11:26:08.063029: step 7977, loss 0.57946.
Train: 2018-08-02T11:26:08.297365: step 7978, loss 0.570878.
Train: 2018-08-02T11:26:08.516025: step 7979, loss 0.596419.
Train: 2018-08-02T11:26:08.734722: step 7980, loss 0.485936.
Test: 2018-08-02T11:26:09.921943: step 7980, loss 0.547414.
Train: 2018-08-02T11:26:10.125021: step 7981, loss 0.562352.
Train: 2018-08-02T11:26:10.343720: step 7982, loss 0.452325.
Train: 2018-08-02T11:26:10.578039: step 7983, loss 0.647063.
Train: 2018-08-02T11:26:10.812392: step 7984, loss 0.553899.
Train: 2018-08-02T11:26:11.031059: step 7985, loss 0.60461.
Train: 2018-08-02T11:26:11.249759: step 7986, loss 0.553939.
Train: 2018-08-02T11:26:11.499729: step 7987, loss 0.520304.
Train: 2018-08-02T11:26:11.718426: step 7988, loss 0.528756.
Train: 2018-08-02T11:26:11.937121: step 7989, loss 0.537171.
Train: 2018-08-02T11:26:12.155796: step 7990, loss 0.553978.
Test: 2018-08-02T11:26:13.343017: step 7990, loss 0.547569.
Train: 2018-08-02T11:26:13.546124: step 7991, loss 0.528756.
Train: 2018-08-02T11:26:13.764794: step 7992, loss 0.596031.
Train: 2018-08-02T11:26:13.983523: step 7993, loss 0.486684.
Train: 2018-08-02T11:26:14.233469: step 7994, loss 0.612904.
Train: 2018-08-02T11:26:14.452163: step 7995, loss 0.562377.
Train: 2018-08-02T11:26:14.670832: step 7996, loss 0.570795.
Train: 2018-08-02T11:26:14.889559: step 7997, loss 0.545552.
Train: 2018-08-02T11:26:15.123851: step 7998, loss 0.570792.
Train: 2018-08-02T11:26:15.358171: step 7999, loss 0.638037.
Train: 2018-08-02T11:26:15.576888: step 8000, loss 0.579167.
Test: 2018-08-02T11:26:16.764090: step 8000, loss 0.548828.
Train: 2018-08-02T11:26:17.638917: step 8001, loss 0.520595.
Train: 2018-08-02T11:26:17.857615: step 8002, loss 0.562418.
Train: 2018-08-02T11:26:18.045072: step 8003, loss 0.491294.
Train: 2018-08-02T11:26:18.279362: step 8004, loss 0.537417.
Train: 2018-08-02T11:26:18.498090: step 8005, loss 0.570766.
Train: 2018-08-02T11:26:18.716758: step 8006, loss 0.612479.
Train: 2018-08-02T11:26:18.935488: step 8007, loss 0.58743.
Train: 2018-08-02T11:26:19.154188: step 8008, loss 0.562442.
Train: 2018-08-02T11:26:19.388477: step 8009, loss 0.562453.
Train: 2018-08-02T11:26:19.607175: step 8010, loss 0.603931.
Test: 2018-08-02T11:26:20.794397: step 8010, loss 0.547904.
Train: 2018-08-02T11:26:21.013127: step 8011, loss 0.545937.
Train: 2018-08-02T11:26:21.247419: step 8012, loss 0.512953.
Train: 2018-08-02T11:26:21.466147: step 8013, loss 0.570756.
Train: 2018-08-02T11:26:21.684813: step 8014, loss 0.562511.
Train: 2018-08-02T11:26:21.919165: step 8015, loss 0.587236.
Train: 2018-08-02T11:26:22.137833: step 8016, loss 0.636583.
Train: 2018-08-02T11:26:22.372177: step 8017, loss 0.554354.
Train: 2018-08-02T11:26:22.590884: step 8018, loss 0.488934.
Train: 2018-08-02T11:26:22.825172: step 8019, loss 0.538053.
Train: 2018-08-02T11:26:23.043901: step 8020, loss 0.554409.
Test: 2018-08-02T11:26:24.231093: step 8020, loss 0.5493.
Train: 2018-08-02T11:26:24.449825: step 8021, loss 0.60349.
Train: 2018-08-02T11:26:24.684138: step 8022, loss 0.513547.
Train: 2018-08-02T11:26:24.902810: step 8023, loss 0.554411.
Train: 2018-08-02T11:26:25.121533: step 8024, loss 0.529852.
Train: 2018-08-02T11:26:25.340208: step 8025, loss 0.529799.
Train: 2018-08-02T11:26:25.574529: step 8026, loss 0.603594.
Train: 2018-08-02T11:26:25.855747: step 8027, loss 0.587189.
Train: 2018-08-02T11:26:26.090061: step 8028, loss 0.529688.
Train: 2018-08-02T11:26:26.324378: step 8029, loss 0.562537.
Train: 2018-08-02T11:26:26.558698: step 8030, loss 0.611886.
Test: 2018-08-02T11:26:27.730273: step 8030, loss 0.547863.
Train: 2018-08-02T11:26:27.948971: step 8031, loss 0.554319.
Train: 2018-08-02T11:26:28.198946: step 8032, loss 0.554326.
Train: 2018-08-02T11:26:28.417612: step 8033, loss 0.554329.
Train: 2018-08-02T11:26:28.636311: step 8034, loss 0.529687.
Train: 2018-08-02T11:26:28.855040: step 8035, loss 0.570754.
Train: 2018-08-02T11:26:29.089355: step 8036, loss 0.52142.
Train: 2018-08-02T11:26:29.323650: step 8037, loss 0.554285.
Train: 2018-08-02T11:26:29.542350: step 8038, loss 0.620231.
Train: 2018-08-02T11:26:29.776669: step 8039, loss 0.570759.
Train: 2018-08-02T11:26:29.995393: step 8040, loss 0.546042.
Test: 2018-08-02T11:26:31.182590: step 8040, loss 0.54914.
Train: 2018-08-02T11:26:31.416941: step 8041, loss 0.570753.
Train: 2018-08-02T11:26:31.631055: step 8042, loss 0.546052.
Train: 2018-08-02T11:26:31.849752: step 8043, loss 0.562524.
Train: 2018-08-02T11:26:32.099676: step 8044, loss 0.513108.
Train: 2018-08-02T11:26:32.318397: step 8045, loss 0.587238.
Train: 2018-08-02T11:26:32.552687: step 8046, loss 0.5625.
Train: 2018-08-02T11:26:32.787038: step 8047, loss 0.612006.
Train: 2018-08-02T11:26:33.021358: step 8048, loss 0.570763.
Train: 2018-08-02T11:26:33.255674: step 8049, loss 0.578999.
Train: 2018-08-02T11:26:33.474348: step 8050, loss 0.56254.
Test: 2018-08-02T11:26:34.645947: step 8050, loss 0.549787.
Train: 2018-08-02T11:26:34.864646: step 8051, loss 0.595389.
Train: 2018-08-02T11:26:35.083375: step 8052, loss 0.562567.
Train: 2018-08-02T11:26:35.302043: step 8053, loss 0.529886.
Train: 2018-08-02T11:26:35.520772: step 8054, loss 0.554438.
Train: 2018-08-02T11:26:35.755062: step 8055, loss 0.611583.
Train: 2018-08-02T11:26:35.973761: step 8056, loss 0.570777.
Train: 2018-08-02T11:26:36.208112: step 8057, loss 0.538248.
Train: 2018-08-02T11:26:36.442405: step 8058, loss 0.603287.
Train: 2018-08-02T11:26:36.661126: step 8059, loss 0.619456.
Train: 2018-08-02T11:26:36.911067: step 8060, loss 0.586977.
Test: 2018-08-02T11:26:38.098264: step 8060, loss 0.54967.
Train: 2018-08-02T11:26:38.301372: step 8061, loss 0.50634.
Train: 2018-08-02T11:26:38.520073: step 8062, loss 0.546684.
Train: 2018-08-02T11:26:38.754392: step 8063, loss 0.578873.
Train: 2018-08-02T11:26:38.988681: step 8064, loss 0.490523.
Train: 2018-08-02T11:26:39.207409: step 8065, loss 0.594949.
Train: 2018-08-02T11:26:39.441730: step 8066, loss 0.59495.
Train: 2018-08-02T11:26:39.676020: step 8067, loss 0.522638.
Train: 2018-08-02T11:26:39.894719: step 8068, loss 0.546724.
Train: 2018-08-02T11:26:40.113417: step 8069, loss 0.57083.
Train: 2018-08-02T11:26:40.332146: step 8070, loss 0.538628.
Test: 2018-08-02T11:26:41.519337: step 8070, loss 0.549114.
Train: 2018-08-02T11:26:41.722440: step 8071, loss 0.538579.
Train: 2018-08-02T11:26:41.941114: step 8072, loss 0.586956.
Train: 2018-08-02T11:26:42.191086: step 8073, loss 0.538473.
Train: 2018-08-02T11:26:42.409784: step 8074, loss 0.562701.
Train: 2018-08-02T11:26:42.628484: step 8075, loss 0.627571.
Train: 2018-08-02T11:26:42.847178: step 8076, loss 0.562679.
Train: 2018-08-02T11:26:43.081471: step 8077, loss 0.51405.
Train: 2018-08-02T11:26:43.300170: step 8078, loss 0.570789.
Train: 2018-08-02T11:26:43.534516: step 8079, loss 0.587026.
Train: 2018-08-02T11:26:43.753221: step 8080, loss 0.489554.
Test: 2018-08-02T11:26:44.940411: step 8080, loss 0.548667.
Train: 2018-08-02T11:26:45.159111: step 8081, loss 0.595178.
Train: 2018-08-02T11:26:45.377841: step 8082, loss 0.627866.
Train: 2018-08-02T11:26:45.596533: step 8083, loss 0.546348.
Train: 2018-08-02T11:26:45.815256: step 8084, loss 0.546366.
Train: 2018-08-02T11:26:46.033910: step 8085, loss 0.489396.
Train: 2018-08-02T11:26:46.252632: step 8086, loss 0.587108.
Train: 2018-08-02T11:26:46.502546: step 8087, loss 0.554449.
Train: 2018-08-02T11:26:46.721244: step 8088, loss 0.529909.
Train: 2018-08-02T11:26:46.955564: step 8089, loss 0.546203.
Train: 2018-08-02T11:26:47.174263: step 8090, loss 0.578966.
Test: 2018-08-02T11:26:48.361485: step 8090, loss 0.548068.
Train: 2018-08-02T11:26:48.564588: step 8091, loss 0.570758.
Train: 2018-08-02T11:26:48.798883: step 8092, loss 0.504969.
Train: 2018-08-02T11:26:49.033203: step 8093, loss 0.51306.
Train: 2018-08-02T11:26:49.251902: step 8094, loss 0.51288.
Train: 2018-08-02T11:26:49.470601: step 8095, loss 0.545855.
Train: 2018-08-02T11:26:49.689299: step 8096, loss 0.545771.
Train: 2018-08-02T11:26:49.923645: step 8097, loss 0.595871.
Train: 2018-08-02T11:26:50.142349: step 8098, loss 0.60432.
Train: 2018-08-02T11:26:50.361045: step 8099, loss 0.562404.
Train: 2018-08-02T11:26:50.595362: step 8100, loss 0.553993.
Test: 2018-08-02T11:26:51.782558: step 8100, loss 0.548352.
Train: 2018-08-02T11:26:52.610522: step 8101, loss 0.486763.
Train: 2018-08-02T11:26:52.829219: step 8102, loss 0.537112.
Train: 2018-08-02T11:26:53.047888: step 8103, loss 0.494837.
Train: 2018-08-02T11:26:53.282238: step 8104, loss 0.604706.
Train: 2018-08-02T11:26:53.500938: step 8105, loss 0.596296.
Train: 2018-08-02T11:26:53.750880: step 8106, loss 0.528369.
Train: 2018-08-02T11:26:53.969578: step 8107, loss 0.553834.
Train: 2018-08-02T11:26:54.203869: step 8108, loss 0.60491.
Train: 2018-08-02T11:26:54.422590: step 8109, loss 0.528283.
Train: 2018-08-02T11:26:54.641265: step 8110, loss 0.587901.
Test: 2018-08-02T11:26:55.828486: step 8110, loss 0.547141.
Train: 2018-08-02T11:26:56.062808: step 8111, loss 0.579388.
Train: 2018-08-02T11:26:56.281530: step 8112, loss 0.613384.
Train: 2018-08-02T11:26:56.515827: step 8113, loss 0.52841.
Train: 2018-08-02T11:26:56.718905: step 8114, loss 0.494563.
Train: 2018-08-02T11:26:56.937602: step 8115, loss 0.596248.
Train: 2018-08-02T11:26:57.156337: step 8116, loss 0.587753.
Train: 2018-08-02T11:26:57.390651: step 8117, loss 0.511652.
Train: 2018-08-02T11:26:57.609320: step 8118, loss 0.57081.
Train: 2018-08-02T11:26:57.843641: step 8119, loss 0.537046.
Train: 2018-08-02T11:26:58.077960: step 8120, loss 0.655177.
Test: 2018-08-02T11:26:59.249560: step 8120, loss 0.549098.
Train: 2018-08-02T11:26:59.468262: step 8121, loss 0.54555.
Train: 2018-08-02T11:26:59.686984: step 8122, loss 0.604377.
Train: 2018-08-02T11:26:59.921278: step 8123, loss 0.537283.
Train: 2018-08-02T11:27:00.155629: step 8124, loss 0.595833.
Train: 2018-08-02T11:27:00.374321: step 8125, loss 0.579091.
Train: 2018-08-02T11:27:00.608617: step 8126, loss 0.579063.
Train: 2018-08-02T11:27:00.827346: step 8127, loss 0.570758.
Train: 2018-08-02T11:27:01.046015: step 8128, loss 0.521263.
Train: 2018-08-02T11:27:01.280337: step 8129, loss 0.537829.
Train: 2018-08-02T11:27:01.499065: step 8130, loss 0.50499.
Test: 2018-08-02T11:27:02.686256: step 8130, loss 0.548994.
Train: 2018-08-02T11:27:02.889364: step 8131, loss 0.587202.
Train: 2018-08-02T11:27:03.123654: step 8132, loss 0.496795.
Train: 2018-08-02T11:27:03.342382: step 8133, loss 0.56253.
Train: 2018-08-02T11:27:03.561051: step 8134, loss 0.546056.
Train: 2018-08-02T11:27:03.779781: step 8135, loss 0.562511.
Train: 2018-08-02T11:27:03.998480: step 8136, loss 0.579022.
Train: 2018-08-02T11:27:04.217180: step 8137, loss 0.562491.
Train: 2018-08-02T11:27:04.467089: step 8138, loss 0.545976.
Train: 2018-08-02T11:27:04.685818: step 8139, loss 0.636872.
Train: 2018-08-02T11:27:04.904517: step 8140, loss 0.545986.
Test: 2018-08-02T11:27:06.091708: step 8140, loss 0.549306.
Train: 2018-08-02T11:27:06.326053: step 8141, loss 0.62024.
Train: 2018-08-02T11:27:06.544751: step 8142, loss 0.620134.
Train: 2018-08-02T11:27:06.779078: step 8143, loss 0.587163.
Train: 2018-08-02T11:27:07.013367: step 8144, loss 0.60345.
Train: 2018-08-02T11:27:07.263342: step 8145, loss 0.562646.
Train: 2018-08-02T11:27:07.482008: step 8146, loss 0.514066.
Train: 2018-08-02T11:27:07.716339: step 8147, loss 0.57889.
Train: 2018-08-02T11:27:07.950649: step 8148, loss 0.546615.
Train: 2018-08-02T11:27:08.169379: step 8149, loss 0.522506.
Train: 2018-08-02T11:27:08.403667: step 8150, loss 0.570827.
Test: 2018-08-02T11:27:09.575267: step 8150, loss 0.549164.
Train: 2018-08-02T11:27:09.872074: step 8151, loss 0.594963.
Train: 2018-08-02T11:27:10.090771: step 8152, loss 0.554767.
Train: 2018-08-02T11:27:10.309501: step 8153, loss 0.538728.
Train: 2018-08-02T11:27:10.496959: step 8154, loss 0.579941.
Train: 2018-08-02T11:27:10.731274: step 8155, loss 0.514674.
Train: 2018-08-02T11:27:10.949976: step 8156, loss 0.586903.
Train: 2018-08-02T11:27:11.184291: step 8157, loss 0.602975.
Train: 2018-08-02T11:27:11.402994: step 8158, loss 0.602958.
Train: 2018-08-02T11:27:11.621695: step 8159, loss 0.643005.
Train: 2018-08-02T11:27:11.855984: step 8160, loss 0.594845.
Test: 2018-08-02T11:27:13.058826: step 8160, loss 0.549818.
Train: 2018-08-02T11:27:13.277556: step 8161, loss 0.531083.
Train: 2018-08-02T11:27:13.511845: step 8162, loss 0.578858.
Train: 2018-08-02T11:27:13.730546: step 8163, loss 0.563008.
Train: 2018-08-02T11:27:13.964866: step 8164, loss 0.547221.
Train: 2018-08-02T11:27:14.199184: step 8165, loss 0.578862.
Train: 2018-08-02T11:27:14.417914: step 8166, loss 0.484183.
Train: 2018-08-02T11:27:14.636607: step 8167, loss 0.570962.
Train: 2018-08-02T11:27:14.870930: step 8168, loss 0.563045.
Train: 2018-08-02T11:27:15.089634: step 8169, loss 0.578873.
Train: 2018-08-02T11:27:15.339542: step 8170, loss 0.602606.
Test: 2018-08-02T11:27:16.526764: step 8170, loss 0.549438.
Train: 2018-08-02T11:27:16.729842: step 8171, loss 0.563028.
Train: 2018-08-02T11:27:16.964162: step 8172, loss 0.555094.
Train: 2018-08-02T11:27:17.182892: step 8173, loss 0.531349.
Train: 2018-08-02T11:27:17.417181: step 8174, loss 0.618548.
Train: 2018-08-02T11:27:17.635911: step 8175, loss 0.539204.
Train: 2018-08-02T11:27:17.870199: step 8176, loss 0.563002.
Train: 2018-08-02T11:27:18.088899: step 8177, loss 0.547083.
Train: 2018-08-02T11:27:18.338874: step 8178, loss 0.555045.
Train: 2018-08-02T11:27:18.573160: step 8179, loss 0.570985.
Train: 2018-08-02T11:27:18.807511: step 8180, loss 0.539005.
Test: 2018-08-02T11:27:19.994702: step 8180, loss 0.549197.
Train: 2018-08-02T11:27:20.197779: step 8181, loss 0.610828.
Train: 2018-08-02T11:27:20.416478: step 8182, loss 0.570859.
Train: 2018-08-02T11:27:20.635208: step 8183, loss 0.53096.
Train: 2018-08-02T11:27:20.853909: step 8184, loss 0.570868.
Train: 2018-08-02T11:27:21.072605: step 8185, loss 0.570862.
Train: 2018-08-02T11:27:21.291273: step 8186, loss 0.554838.
Train: 2018-08-02T11:27:21.509973: step 8187, loss 0.53878.
Train: 2018-08-02T11:27:21.728699: step 8188, loss 0.594932.
Train: 2018-08-02T11:27:21.947400: step 8189, loss 0.586908.
Train: 2018-08-02T11:27:22.197312: step 8190, loss 0.498498.
Test: 2018-08-02T11:27:23.368912: step 8190, loss 0.548213.
Train: 2018-08-02T11:27:23.587610: step 8191, loss 0.586928.
Train: 2018-08-02T11:27:23.806340: step 8192, loss 0.554698.
Train: 2018-08-02T11:27:24.025034: step 8193, loss 0.562743.
Train: 2018-08-02T11:27:24.243740: step 8194, loss 0.554647.
Train: 2018-08-02T11:27:24.462437: step 8195, loss 0.595074.
Train: 2018-08-02T11:27:24.681105: step 8196, loss 0.538423.
Train: 2018-08-02T11:27:24.915425: step 8197, loss 0.538386.
Train: 2018-08-02T11:27:25.118502: step 8198, loss 0.578906.
Train: 2018-08-02T11:27:25.352850: step 8199, loss 0.595155.
Train: 2018-08-02T11:27:25.571522: step 8200, loss 0.587035.
Test: 2018-08-02T11:27:26.758743: step 8200, loss 0.549479.
Train: 2018-08-02T11:27:27.649190: step 8201, loss 0.513943.
Train: 2018-08-02T11:27:27.883517: step 8202, loss 0.570784.
Train: 2018-08-02T11:27:28.102178: step 8203, loss 0.546388.
Train: 2018-08-02T11:27:28.336499: step 8204, loss 0.49753.
Train: 2018-08-02T11:27:28.555228: step 8205, loss 0.554455.
Train: 2018-08-02T11:27:28.773898: step 8206, loss 0.570767.
Train: 2018-08-02T11:27:29.008248: step 8207, loss 0.529796.
Train: 2018-08-02T11:27:29.226948: step 8208, loss 0.537896.
Train: 2018-08-02T11:27:29.445640: step 8209, loss 0.488384.
Train: 2018-08-02T11:27:29.695587: step 8210, loss 0.570766.
Test: 2018-08-02T11:27:30.882779: step 8210, loss 0.548587.
Train: 2018-08-02T11:27:31.117098: step 8211, loss 0.562458.
Train: 2018-08-02T11:27:31.335826: step 8212, loss 0.504097.
Train: 2018-08-02T11:27:31.570116: step 8213, loss 0.570756.
Train: 2018-08-02T11:27:31.804467: step 8214, loss 0.553989.
Train: 2018-08-02T11:27:32.023136: step 8215, loss 0.562296.
Train: 2018-08-02T11:27:32.273102: step 8216, loss 0.604716.
Train: 2018-08-02T11:27:32.491776: step 8217, loss 0.545605.
Train: 2018-08-02T11:27:32.710509: step 8218, loss 0.553968.
Train: 2018-08-02T11:27:32.944795: step 8219, loss 0.570883.
Train: 2018-08-02T11:27:33.163524: step 8220, loss 0.503055.
Test: 2018-08-02T11:27:34.335095: step 8220, loss 0.548774.
Train: 2018-08-02T11:27:34.553829: step 8221, loss 0.562349.
Train: 2018-08-02T11:27:34.772523: step 8222, loss 0.545379.
Train: 2018-08-02T11:27:34.991222: step 8223, loss 0.630347.
Train: 2018-08-02T11:27:35.225511: step 8224, loss 0.545346.
Train: 2018-08-02T11:27:35.444235: step 8225, loss 0.545373.
Train: 2018-08-02T11:27:35.678532: step 8226, loss 0.570832.
Train: 2018-08-02T11:27:35.897230: step 8227, loss 0.56235.
Train: 2018-08-02T11:27:36.115928: step 8228, loss 0.579304.
Train: 2018-08-02T11:27:36.350248: step 8229, loss 0.596218.
Train: 2018-08-02T11:27:36.584598: step 8230, loss 0.587709.
Test: 2018-08-02T11:27:37.756169: step 8230, loss 0.548106.
Train: 2018-08-02T11:27:37.974868: step 8231, loss 0.494961.
Train: 2018-08-02T11:27:38.193590: step 8232, loss 0.587631.
Train: 2018-08-02T11:27:38.412280: step 8233, loss 0.478341.
Train: 2018-08-02T11:27:38.646618: step 8234, loss 0.579196.
Train: 2018-08-02T11:27:38.865316: step 8235, loss 0.621215.
Train: 2018-08-02T11:27:39.115258: step 8236, loss 0.612731.
Train: 2018-08-02T11:27:39.333923: step 8237, loss 0.537311.
Train: 2018-08-02T11:27:39.552626: step 8238, loss 0.56242.
Train: 2018-08-02T11:27:39.786946: step 8239, loss 0.529112.
Train: 2018-08-02T11:27:40.005671: step 8240, loss 0.562441.
Test: 2018-08-02T11:27:41.192863: step 8240, loss 0.548379.
Train: 2018-08-02T11:27:41.411587: step 8241, loss 0.604002.
Train: 2018-08-02T11:27:41.630288: step 8242, loss 0.479539.
Train: 2018-08-02T11:27:41.848984: step 8243, loss 0.562466.
Train: 2018-08-02T11:27:42.067689: step 8244, loss 0.545886.
Train: 2018-08-02T11:27:42.286358: step 8245, loss 0.570758.
Train: 2018-08-02T11:27:42.520709: step 8246, loss 0.504431.
Train: 2018-08-02T11:27:42.739378: step 8247, loss 0.645463.
Train: 2018-08-02T11:27:42.973722: step 8248, loss 0.53759.
Train: 2018-08-02T11:27:43.192395: step 8249, loss 0.554184.
Train: 2018-08-02T11:27:43.411125: step 8250, loss 0.529323.
Test: 2018-08-02T11:27:44.613937: step 8250, loss 0.549003.
Train: 2018-08-02T11:27:44.817046: step 8251, loss 0.529304.
Train: 2018-08-02T11:27:45.035756: step 8252, loss 0.52926.
Train: 2018-08-02T11:27:45.270060: step 8253, loss 0.55414.
Train: 2018-08-02T11:27:45.504380: step 8254, loss 0.545785.
Train: 2018-08-02T11:27:45.723054: step 8255, loss 0.545746.
Train: 2018-08-02T11:27:45.957373: step 8256, loss 0.562417.
Train: 2018-08-02T11:27:46.160450: step 8257, loss 0.595879.
Train: 2018-08-02T11:27:46.394771: step 8258, loss 0.570778.
Train: 2018-08-02T11:27:46.613497: step 8259, loss 0.495423.
Train: 2018-08-02T11:27:46.847790: step 8260, loss 0.528858.
Test: 2018-08-02T11:27:48.050633: step 8260, loss 0.549323.
Train: 2018-08-02T11:27:48.284953: step 8261, loss 0.604388.
Train: 2018-08-02T11:27:48.519274: step 8262, loss 0.55398.
Train: 2018-08-02T11:27:48.753593: step 8263, loss 0.511915.
Train: 2018-08-02T11:27:48.987939: step 8264, loss 0.638188.
Train: 2018-08-02T11:27:49.222243: step 8265, loss 0.654986.
Train: 2018-08-02T11:27:49.456554: step 8266, loss 0.612764.
Train: 2018-08-02T11:27:49.706497: step 8267, loss 0.54568.
Train: 2018-08-02T11:27:50.065787: step 8268, loss 0.570765.
Train: 2018-08-02T11:27:50.393833: step 8269, loss 0.612317.
Train: 2018-08-02T11:27:50.628155: step 8270, loss 0.545925.
Test: 2018-08-02T11:27:51.815376: step 8270, loss 0.548351.
Train: 2018-08-02T11:27:52.034104: step 8271, loss 0.537761.
Train: 2018-08-02T11:27:52.252798: step 8272, loss 0.537847.
Train: 2018-08-02T11:27:52.487096: step 8273, loss 0.57076.
Train: 2018-08-02T11:27:52.705817: step 8274, loss 0.570762.
Train: 2018-08-02T11:27:52.924522: step 8275, loss 0.505304.
Train: 2018-08-02T11:27:53.143219: step 8276, loss 0.562586.
Train: 2018-08-02T11:27:53.393132: step 8277, loss 0.578945.
Train: 2018-08-02T11:27:53.627477: step 8278, loss 0.554425.
Train: 2018-08-02T11:27:53.861800: step 8279, loss 0.546265.
Train: 2018-08-02T11:27:54.080501: step 8280, loss 0.497257.
Test: 2018-08-02T11:27:55.283313: step 8280, loss 0.549111.
Train: 2018-08-02T11:27:55.486417: step 8281, loss 0.529862.
Train: 2018-08-02T11:27:55.720737: step 8282, loss 0.537969.
Train: 2018-08-02T11:27:55.955065: step 8283, loss 0.628292.
Train: 2018-08-02T11:27:56.189382: step 8284, loss 0.562534.
Train: 2018-08-02T11:27:56.408051: step 8285, loss 0.620136.
Train: 2018-08-02T11:27:56.657994: step 8286, loss 0.554318.
Train: 2018-08-02T11:27:56.876721: step 8287, loss 0.603621.
Train: 2018-08-02T11:27:57.111043: step 8288, loss 0.570761.
Train: 2018-08-02T11:27:57.329735: step 8289, loss 0.603515.
Train: 2018-08-02T11:27:57.564060: step 8290, loss 0.578937.
Test: 2018-08-02T11:27:58.766873: step 8290, loss 0.548468.
Train: 2018-08-02T11:27:59.001193: step 8291, loss 0.611501.
Train: 2018-08-02T11:27:59.235513: step 8292, loss 0.611365.
Train: 2018-08-02T11:27:59.454242: step 8293, loss 0.562729.
Train: 2018-08-02T11:27:59.672937: step 8294, loss 0.562781.
Train: 2018-08-02T11:27:59.907233: step 8295, loss 0.538773.
Train: 2018-08-02T11:28:00.125960: step 8296, loss 0.546867.
Train: 2018-08-02T11:28:00.360265: step 8297, loss 0.610801.
Train: 2018-08-02T11:28:00.578979: step 8298, loss 0.547003.
Train: 2018-08-02T11:28:00.813294: step 8299, loss 0.586807.
Train: 2018-08-02T11:28:01.032016: step 8300, loss 0.547131.
Test: 2018-08-02T11:28:02.219190: step 8300, loss 0.550701.
Train: 2018-08-02T11:28:03.125260: step 8301, loss 0.547176.
Train: 2018-08-02T11:28:03.359548: step 8302, loss 0.555113.
Train: 2018-08-02T11:28:03.578272: step 8303, loss 0.547206.
Train: 2018-08-02T11:28:03.796976: step 8304, loss 0.539278.
Train: 2018-08-02T11:28:04.000023: step 8305, loss 0.529196.
Train: 2018-08-02T11:28:04.234343: step 8306, loss 0.499449.
Train: 2018-08-02T11:28:04.453067: step 8307, loss 0.546982.
Train: 2018-08-02T11:28:04.671771: step 8308, loss 0.546867.
Train: 2018-08-02T11:28:04.906061: step 8309, loss 0.57084.
Train: 2018-08-02T11:28:05.124789: step 8310, loss 0.546651.
Test: 2018-08-02T11:28:06.311981: step 8310, loss 0.549032.
Train: 2018-08-02T11:28:06.530681: step 8311, loss 0.595059.
Train: 2018-08-02T11:28:06.749409: step 8312, loss 0.619418.
Train: 2018-08-02T11:28:06.968078: step 8313, loss 0.595122.
Train: 2018-08-02T11:28:07.186776: step 8314, loss 0.603229.
Train: 2018-08-02T11:28:07.405505: step 8315, loss 0.554595.
Train: 2018-08-02T11:28:07.624205: step 8316, loss 0.595084.
Train: 2018-08-02T11:28:07.874115: step 8317, loss 0.546555.
Train: 2018-08-02T11:28:08.092844: step 8318, loss 0.562733.
Train: 2018-08-02T11:28:08.311542: step 8319, loss 0.635383.
Train: 2018-08-02T11:28:08.545869: step 8320, loss 0.55472.
Test: 2018-08-02T11:28:09.748676: step 8320, loss 0.549745.
Train: 2018-08-02T11:28:09.951778: step 8321, loss 0.570835.
Train: 2018-08-02T11:28:10.186074: step 8322, loss 0.546778.
Train: 2018-08-02T11:28:10.404773: step 8323, loss 0.458663.
Train: 2018-08-02T11:28:10.639093: step 8324, loss 0.490571.
Train: 2018-08-02T11:28:10.857791: step 8325, loss 0.578878.
Train: 2018-08-02T11:28:11.076507: step 8326, loss 0.538504.
Train: 2018-08-02T11:28:11.295189: step 8327, loss 0.481679.
Train: 2018-08-02T11:28:11.513914: step 8328, loss 0.562639.
Train: 2018-08-02T11:28:11.748209: step 8329, loss 0.627996.
Train: 2018-08-02T11:28:11.982556: step 8330, loss 0.570763.
Test: 2018-08-02T11:28:13.154129: step 8330, loss 0.547519.
Train: 2018-08-02T11:28:13.372832: step 8331, loss 0.554342.
Train: 2018-08-02T11:28:13.591554: step 8332, loss 0.554311.
Train: 2018-08-02T11:28:13.825872: step 8333, loss 0.546042.
Train: 2018-08-02T11:28:14.075814: step 8334, loss 0.57901.
Train: 2018-08-02T11:28:14.310139: step 8335, loss 0.545963.
Train: 2018-08-02T11:28:14.528833: step 8336, loss 0.504544.
Train: 2018-08-02T11:28:14.763152: step 8337, loss 0.454584.
Train: 2018-08-02T11:28:14.981853: step 8338, loss 0.570765.
Train: 2018-08-02T11:28:15.200551: step 8339, loss 0.537293.
Train: 2018-08-02T11:28:15.419249: step 8340, loss 0.553981.
Test: 2018-08-02T11:28:16.622066: step 8340, loss 0.549448.
Train: 2018-08-02T11:28:16.825143: step 8341, loss 0.553936.
Train: 2018-08-02T11:28:17.043874: step 8342, loss 0.579281.
Train: 2018-08-02T11:28:17.278189: step 8343, loss 0.56235.
Train: 2018-08-02T11:28:17.496887: step 8344, loss 0.468871.
Train: 2018-08-02T11:28:17.731183: step 8345, loss 0.511172.
Train: 2018-08-02T11:28:17.949886: step 8346, loss 0.562337.
Train: 2018-08-02T11:28:18.168580: step 8347, loss 0.622485.
Train: 2018-08-02T11:28:18.387279: step 8348, loss 0.596757.
Train: 2018-08-02T11:28:18.621599: step 8349, loss 0.510687.
Train: 2018-08-02T11:28:18.840325: step 8350, loss 0.562336.
Test: 2018-08-02T11:28:20.043140: step 8350, loss 0.546744.
Train: 2018-08-02T11:28:20.246218: step 8351, loss 0.614083.
Train: 2018-08-02T11:28:20.480538: step 8352, loss 0.579572.
Train: 2018-08-02T11:28:20.699267: step 8353, loss 0.527912.
Train: 2018-08-02T11:28:20.917969: step 8354, loss 0.562335.
Train: 2018-08-02T11:28:21.167903: step 8355, loss 0.527968.
Train: 2018-08-02T11:28:21.386577: step 8356, loss 0.605278.
Train: 2018-08-02T11:28:21.605307: step 8357, loss 0.596637.
Train: 2018-08-02T11:28:21.823973: step 8358, loss 0.673551.
Train: 2018-08-02T11:28:22.042698: step 8359, loss 0.545315.
Train: 2018-08-02T11:28:22.276992: step 8360, loss 0.503008.
Test: 2018-08-02T11:28:23.479835: step 8360, loss 0.547267.
Train: 2018-08-02T11:28:23.682943: step 8361, loss 0.570815.
Train: 2018-08-02T11:28:23.901612: step 8362, loss 0.469628.
Train: 2018-08-02T11:28:24.120335: step 8363, loss 0.562373.
Train: 2018-08-02T11:28:24.339011: step 8364, loss 0.604476.
Train: 2018-08-02T11:28:24.573363: step 8365, loss 0.553979.
Train: 2018-08-02T11:28:24.792060: step 8366, loss 0.570783.
Train: 2018-08-02T11:28:25.041971: step 8367, loss 0.520514.
Train: 2018-08-02T11:28:25.260699: step 8368, loss 0.621.
Train: 2018-08-02T11:28:25.510660: step 8369, loss 0.5123.
Train: 2018-08-02T11:28:25.729336: step 8370, loss 0.587457.
Test: 2018-08-02T11:28:26.947774: step 8370, loss 0.549667.
Train: 2018-08-02T11:28:27.166503: step 8371, loss 0.537435.
Train: 2018-08-02T11:28:27.385202: step 8372, loss 0.570762.
Train: 2018-08-02T11:28:27.603894: step 8373, loss 0.504236.
Train: 2018-08-02T11:28:27.838216: step 8374, loss 0.56244.
Train: 2018-08-02T11:28:28.056920: step 8375, loss 0.562454.
Train: 2018-08-02T11:28:28.275614: step 8376, loss 0.587402.
Train: 2018-08-02T11:28:28.494313: step 8377, loss 0.595701.
Train: 2018-08-02T11:28:28.728607: step 8378, loss 0.562455.
Train: 2018-08-02T11:28:28.947345: step 8379, loss 0.545894.
Train: 2018-08-02T11:28:29.166035: step 8380, loss 0.603874.
Test: 2018-08-02T11:28:30.353226: step 8380, loss 0.548122.
Train: 2018-08-02T11:28:30.571956: step 8381, loss 0.52944.
Train: 2018-08-02T11:28:30.806271: step 8382, loss 0.570756.
Train: 2018-08-02T11:28:31.024975: step 8383, loss 0.579001.
Train: 2018-08-02T11:28:31.243668: step 8384, loss 0.554294.
Train: 2018-08-02T11:28:31.493609: step 8385, loss 0.595423.
Train: 2018-08-02T11:28:31.727905: step 8386, loss 0.55435.
Train: 2018-08-02T11:28:31.946630: step 8387, loss 0.480649.
Train: 2018-08-02T11:28:32.180925: step 8388, loss 0.611745.
Train: 2018-08-02T11:28:32.415274: step 8389, loss 0.587144.
Train: 2018-08-02T11:28:32.649589: step 8390, loss 0.611665.
Test: 2018-08-02T11:28:33.821164: step 8390, loss 0.549359.
Train: 2018-08-02T11:28:34.055485: step 8391, loss 0.578932.
Train: 2018-08-02T11:28:34.274213: step 8392, loss 0.530081.
Train: 2018-08-02T11:28:34.508534: step 8393, loss 0.538274.
Train: 2018-08-02T11:28:34.727234: step 8394, loss 0.497691.
Train: 2018-08-02T11:28:34.945932: step 8395, loss 0.570783.
Train: 2018-08-02T11:28:35.180246: step 8396, loss 0.562646.
Train: 2018-08-02T11:28:35.398945: step 8397, loss 0.611479.
Train: 2018-08-02T11:28:35.633271: step 8398, loss 0.562646.
Train: 2018-08-02T11:28:35.851971: step 8399, loss 0.587044.
Train: 2018-08-02T11:28:36.086261: step 8400, loss 0.554543.
Test: 2018-08-02T11:28:37.273481: step 8400, loss 0.548559.
Train: 2018-08-02T11:28:38.117032: step 8401, loss 0.505863.
Train: 2018-08-02T11:28:38.335732: step 8402, loss 0.595152.
Train: 2018-08-02T11:28:38.570078: step 8403, loss 0.570787.
Train: 2018-08-02T11:28:38.804402: step 8404, loss 0.513955.
Train: 2018-08-02T11:28:39.038723: step 8405, loss 0.538278.
Train: 2018-08-02T11:28:39.257417: step 8406, loss 0.546362.
Train: 2018-08-02T11:28:39.507359: step 8407, loss 0.546312.
Train: 2018-08-02T11:28:39.726064: step 8408, loss 0.603459.
Train: 2018-08-02T11:28:39.976004: step 8409, loss 0.480832.
Train: 2018-08-02T11:28:40.194703: step 8410, loss 0.587164.
Test: 2018-08-02T11:28:41.381894: step 8410, loss 0.549401.
Train: 2018-08-02T11:28:41.600620: step 8411, loss 0.603598.
Train: 2018-08-02T11:28:41.834912: step 8412, loss 0.472165.
Train: 2018-08-02T11:28:42.053641: step 8413, loss 0.562512.
Train: 2018-08-02T11:28:42.303577: step 8414, loss 0.579027.
Train: 2018-08-02T11:28:42.522281: step 8415, loss 0.554205.
Train: 2018-08-02T11:28:42.740981: step 8416, loss 0.52105.
Train: 2018-08-02T11:28:42.975296: step 8417, loss 0.570739.
Train: 2018-08-02T11:28:43.193969: step 8418, loss 0.554101.
Train: 2018-08-02T11:28:43.412701: step 8419, loss 0.545775.
Train: 2018-08-02T11:28:43.662610: step 8420, loss 0.562426.
Test: 2018-08-02T11:28:44.834211: step 8420, loss 0.548611.
Train: 2018-08-02T11:28:45.052908: step 8421, loss 0.51218.
Train: 2018-08-02T11:28:45.287228: step 8422, loss 0.654924.
Train: 2018-08-02T11:28:45.521549: step 8423, loss 0.621098.
Train: 2018-08-02T11:28:45.740250: step 8424, loss 0.528933.
Train: 2018-08-02T11:28:45.974595: step 8425, loss 0.570767.
Train: 2018-08-02T11:28:46.208917: step 8426, loss 0.587457.
Train: 2018-08-02T11:28:46.427587: step 8427, loss 0.595756.
Train: 2018-08-02T11:28:46.677555: step 8428, loss 0.512585.
Train: 2018-08-02T11:28:46.896227: step 8429, loss 0.662075.
Train: 2018-08-02T11:28:47.114957: step 8430, loss 0.653488.
Test: 2018-08-02T11:28:48.302148: step 8430, loss 0.548216.
Train: 2018-08-02T11:28:48.536499: step 8431, loss 0.562528.
Train: 2018-08-02T11:28:48.755167: step 8432, loss 0.587144.
Train: 2018-08-02T11:28:48.973866: step 8433, loss 0.587074.
Train: 2018-08-02T11:28:49.192564: step 8434, loss 0.587008.
Train: 2018-08-02T11:28:49.411263: step 8435, loss 0.562751.
Train: 2018-08-02T11:28:49.645609: step 8436, loss 0.57887.
Train: 2018-08-02T11:28:49.879923: step 8437, loss 0.546893.
Train: 2018-08-02T11:28:50.114256: step 8438, loss 0.515149.
Train: 2018-08-02T11:28:50.348544: step 8439, loss 0.586807.
Train: 2018-08-02T11:28:50.567273: step 8440, loss 0.555064.
Test: 2018-08-02T11:28:51.754466: step 8440, loss 0.550343.
Train: 2018-08-02T11:28:51.988810: step 8441, loss 0.555102.
Train: 2018-08-02T11:28:52.223106: step 8442, loss 0.563039.
Train: 2018-08-02T11:28:52.457453: step 8443, loss 0.602574.
Train: 2018-08-02T11:28:52.676124: step 8444, loss 0.531512.
Train: 2018-08-02T11:28:52.894853: step 8445, loss 0.539421.
Train: 2018-08-02T11:28:53.113553: step 8446, loss 0.547296.
Train: 2018-08-02T11:28:53.347869: step 8447, loss 0.515667.
Train: 2018-08-02T11:28:53.566553: step 8448, loss 0.515514.
Train: 2018-08-02T11:28:53.785265: step 8449, loss 0.666261.
Train: 2018-08-02T11:28:54.003968: step 8450, loss 0.547054.
Test: 2018-08-02T11:28:55.191160: step 8450, loss 0.550379.
Train: 2018-08-02T11:28:55.394268: step 8451, loss 0.578869.
Train: 2018-08-02T11:28:55.612967: step 8452, loss 0.618693.
Train: 2018-08-02T11:28:55.831635: step 8453, loss 0.594783.
Train: 2018-08-02T11:28:56.065956: step 8454, loss 0.578855.
Train: 2018-08-02T11:28:56.284685: step 8455, loss 0.555052.
Train: 2018-08-02T11:28:56.472111: step 8456, loss 0.579917.
Train: 2018-08-02T11:28:56.690834: step 8457, loss 0.52343.
Train: 2018-08-02T11:28:56.925128: step 8458, loss 0.578859.
Train: 2018-08-02T11:28:57.159448: step 8459, loss 0.547183.
Train: 2018-08-02T11:28:57.378178: step 8460, loss 0.570936.
Test: 2018-08-02T11:28:58.565369: step 8460, loss 0.54977.
Train: 2018-08-02T11:28:58.768472: step 8461, loss 0.539229.
Train: 2018-08-02T11:28:58.987146: step 8462, loss 0.53125.
Train: 2018-08-02T11:28:59.221465: step 8463, loss 0.570908.
Train: 2018-08-02T11:28:59.440197: step 8464, loss 0.554968.
Train: 2018-08-02T11:28:59.658891: step 8465, loss 0.63471.
Train: 2018-08-02T11:28:59.893210: step 8466, loss 0.650664.
Train: 2018-08-02T11:29:00.127530: step 8467, loss 0.547014.
Train: 2018-08-02T11:29:00.346235: step 8468, loss 0.578858.
Train: 2018-08-02T11:29:00.564931: step 8469, loss 0.555044.
Train: 2018-08-02T11:29:00.814869: step 8470, loss 0.618508.
Test: 2018-08-02T11:29:02.002065: step 8470, loss 0.549639.
Train: 2018-08-02T11:29:02.205172: step 8471, loss 0.563036.
Train: 2018-08-02T11:29:02.423842: step 8472, loss 0.62625.
Train: 2018-08-02T11:29:02.642541: step 8473, loss 0.59461.
Train: 2018-08-02T11:29:02.876891: step 8474, loss 0.539644.
Train: 2018-08-02T11:29:03.111211: step 8475, loss 0.53192.
Train: 2018-08-02T11:29:03.345533: step 8476, loss 0.54761.
Train: 2018-08-02T11:29:03.579822: step 8477, loss 0.571064.
Train: 2018-08-02T11:29:03.814165: step 8478, loss 0.531975.
Train: 2018-08-02T11:29:04.032839: step 8479, loss 0.492805.
Train: 2018-08-02T11:29:04.267185: step 8480, loss 0.523941.
Test: 2018-08-02T11:29:05.454381: step 8480, loss 0.550499.
Train: 2018-08-02T11:29:05.657489: step 8481, loss 0.586744.
Train: 2018-08-02T11:29:05.876182: step 8482, loss 0.594665.
Train: 2018-08-02T11:29:06.094856: step 8483, loss 0.610529.
Train: 2018-08-02T11:29:06.313586: step 8484, loss 0.570937.
Train: 2018-08-02T11:29:06.547875: step 8485, loss 0.57093.
Train: 2018-08-02T11:29:06.782208: step 8486, loss 0.578858.
Train: 2018-08-02T11:29:07.000921: step 8487, loss 0.531252.
Train: 2018-08-02T11:29:07.235245: step 8488, loss 0.62652.
Train: 2018-08-02T11:29:07.469568: step 8489, loss 0.555038.
Train: 2018-08-02T11:29:07.703883: step 8490, loss 0.531215.
Test: 2018-08-02T11:29:08.875455: step 8490, loss 0.54932.
Train: 2018-08-02T11:29:09.094179: step 8491, loss 0.674231.
Train: 2018-08-02T11:29:09.328502: step 8492, loss 0.55506.
Train: 2018-08-02T11:29:09.562821: step 8493, loss 0.594709.
Train: 2018-08-02T11:29:09.781493: step 8494, loss 0.594683.
Train: 2018-08-02T11:29:10.015815: step 8495, loss 0.563091.
Train: 2018-08-02T11:29:10.250162: step 8496, loss 0.563121.
Train: 2018-08-02T11:29:10.468845: step 8497, loss 0.571006.
Train: 2018-08-02T11:29:10.703152: step 8498, loss 0.594558.
Train: 2018-08-02T11:29:10.937498: step 8499, loss 0.63367.
Train: 2018-08-02T11:29:11.156171: step 8500, loss 0.571003.
Test: 2018-08-02T11:29:12.343393: step 8500, loss 0.549906.
Train: 2018-08-02T11:29:13.218193: step 8501, loss 0.547821.
Train: 2018-08-02T11:29:13.452508: step 8502, loss 0.579111.
Train: 2018-08-02T11:29:13.671208: step 8503, loss 0.555481.
Train: 2018-08-02T11:29:13.905527: step 8504, loss 0.601754.
Train: 2018-08-02T11:29:14.124256: step 8505, loss 0.625117.
Train: 2018-08-02T11:29:14.342957: step 8506, loss 0.533716.
Train: 2018-08-02T11:29:14.577273: step 8507, loss 0.494886.
Train: 2018-08-02T11:29:14.811565: step 8508, loss 0.578835.
Train: 2018-08-02T11:29:15.030299: step 8509, loss 0.540537.
Train: 2018-08-02T11:29:15.264585: step 8510, loss 0.486329.
Test: 2018-08-02T11:29:16.467427: step 8510, loss 0.5501.
Train: 2018-08-02T11:29:16.686150: step 8511, loss 0.555654.
Train: 2018-08-02T11:29:16.904850: step 8512, loss 0.516621.
Train: 2018-08-02T11:29:17.139160: step 8513, loss 0.547587.
Train: 2018-08-02T11:29:17.373496: step 8514, loss 0.626051.
Train: 2018-08-02T11:29:17.592164: step 8515, loss 0.547308.
Train: 2018-08-02T11:29:17.810887: step 8516, loss 0.602607.
Train: 2018-08-02T11:29:18.029593: step 8517, loss 0.602659.
Train: 2018-08-02T11:29:18.248291: step 8518, loss 0.54709.
Train: 2018-08-02T11:29:18.482606: step 8519, loss 0.539086.
Train: 2018-08-02T11:29:18.701307: step 8520, loss 0.539.
Test: 2018-08-02T11:29:19.888501: step 8520, loss 0.548792.
Train: 2018-08-02T11:29:20.091578: step 8521, loss 0.562879.
Train: 2018-08-02T11:29:20.310278: step 8522, loss 0.546813.
Train: 2018-08-02T11:29:20.544629: step 8523, loss 0.49049.
Train: 2018-08-02T11:29:20.763297: step 8524, loss 0.546601.
Train: 2018-08-02T11:29:20.997642: step 8525, loss 0.554571.
Train: 2018-08-02T11:29:21.231967: step 8526, loss 0.57079.
Train: 2018-08-02T11:29:21.466262: step 8527, loss 0.554468.
Train: 2018-08-02T11:29:21.684981: step 8528, loss 0.546154.
Train: 2018-08-02T11:29:21.903686: step 8529, loss 0.554321.
Train: 2018-08-02T11:29:22.137986: step 8530, loss 0.480044.
Test: 2018-08-02T11:29:23.325196: step 8530, loss 0.549384.
Train: 2018-08-02T11:29:23.543922: step 8531, loss 0.587326.
Train: 2018-08-02T11:29:23.778217: step 8532, loss 0.570764.
Train: 2018-08-02T11:29:23.996944: step 8533, loss 0.545735.
Train: 2018-08-02T11:29:24.215613: step 8534, loss 0.595882.
Train: 2018-08-02T11:29:24.449967: step 8535, loss 0.579132.
Train: 2018-08-02T11:29:24.668663: step 8536, loss 0.553996.
Train: 2018-08-02T11:29:24.902953: step 8537, loss 0.562453.
Train: 2018-08-02T11:29:25.121676: step 8538, loss 0.596004.
Train: 2018-08-02T11:29:25.355971: step 8539, loss 0.520414.
Train: 2018-08-02T11:29:25.590292: step 8540, loss 0.537192.
Test: 2018-08-02T11:29:26.761891: step 8540, loss 0.548728.
Train: 2018-08-02T11:29:26.980622: step 8541, loss 0.520339.
Train: 2018-08-02T11:29:27.230533: step 8542, loss 0.596056.
Train: 2018-08-02T11:29:27.449232: step 8543, loss 0.562399.
Train: 2018-08-02T11:29:27.683553: step 8544, loss 0.587628.
Train: 2018-08-02T11:29:27.917871: step 8545, loss 0.503371.
Train: 2018-08-02T11:29:28.136600: step 8546, loss 0.553984.
Train: 2018-08-02T11:29:28.370894: step 8547, loss 0.587631.
Train: 2018-08-02T11:29:28.620831: step 8548, loss 0.55392.
Train: 2018-08-02T11:29:28.839555: step 8549, loss 0.59623.
Train: 2018-08-02T11:29:29.073852: step 8550, loss 0.545457.
Test: 2018-08-02T11:29:30.261073: step 8550, loss 0.548883.
Train: 2018-08-02T11:29:30.464175: step 8551, loss 0.621277.
Train: 2018-08-02T11:29:30.682882: step 8552, loss 0.562423.
Train: 2018-08-02T11:29:30.917169: step 8553, loss 0.478484.
Train: 2018-08-02T11:29:31.135898: step 8554, loss 0.545645.
Train: 2018-08-02T11:29:31.385810: step 8555, loss 0.562521.
Train: 2018-08-02T11:29:31.604525: step 8556, loss 0.579154.
Train: 2018-08-02T11:29:31.854475: step 8557, loss 0.562448.
Train: 2018-08-02T11:29:32.069566: step 8558, loss 0.58752.
Train: 2018-08-02T11:29:32.303881: step 8559, loss 0.562419.
Train: 2018-08-02T11:29:32.522584: step 8560, loss 0.579112.
Test: 2018-08-02T11:29:33.694155: step 8560, loss 0.548329.
Train: 2018-08-02T11:29:33.912883: step 8561, loss 0.52911.
Train: 2018-08-02T11:29:34.131577: step 8562, loss 0.512496.
Train: 2018-08-02T11:29:34.350250: step 8563, loss 0.579087.
Train: 2018-08-02T11:29:34.568975: step 8564, loss 0.520816.
Train: 2018-08-02T11:29:34.787647: step 8565, loss 0.529117.
Train: 2018-08-02T11:29:35.021997: step 8566, loss 0.604156.
Train: 2018-08-02T11:29:35.256315: step 8567, loss 0.595803.
Train: 2018-08-02T11:29:35.490634: step 8568, loss 0.562439.
Train: 2018-08-02T11:29:35.709333: step 8569, loss 0.495843.
Train: 2018-08-02T11:29:35.928006: step 8570, loss 0.529116.
Test: 2018-08-02T11:29:37.115228: step 8570, loss 0.547538.
Train: 2018-08-02T11:29:37.333957: step 8571, loss 0.595784.
Train: 2018-08-02T11:29:37.568272: step 8572, loss 0.52906.
Train: 2018-08-02T11:29:37.786980: step 8573, loss 0.520671.
Train: 2018-08-02T11:29:38.005675: step 8574, loss 0.587509.
Train: 2018-08-02T11:29:38.224343: step 8575, loss 0.629353.
Train: 2018-08-02T11:29:38.458665: step 8576, loss 0.537324.
Train: 2018-08-02T11:29:38.677393: step 8577, loss 0.570778.
Train: 2018-08-02T11:29:38.896091: step 8578, loss 0.554071.
Train: 2018-08-02T11:29:39.114759: step 8579, loss 0.495665.
Train: 2018-08-02T11:29:39.333489: step 8580, loss 0.579129.
Test: 2018-08-02T11:29:40.520681: step 8580, loss 0.548656.
Train: 2018-08-02T11:29:40.739410: step 8581, loss 0.520647.
Train: 2018-08-02T11:29:40.989321: step 8582, loss 0.595858.
Train: 2018-08-02T11:29:41.223667: step 8583, loss 0.595859.
Train: 2018-08-02T11:29:41.442340: step 8584, loss 0.604196.
Train: 2018-08-02T11:29:41.676688: step 8585, loss 0.58745.
Train: 2018-08-02T11:29:41.895389: step 8586, loss 0.55412.
Train: 2018-08-02T11:29:42.145329: step 8587, loss 0.57076.
Train: 2018-08-02T11:29:42.364029: step 8588, loss 0.51276.
Train: 2018-08-02T11:29:42.582699: step 8589, loss 0.603871.
Train: 2018-08-02T11:29:42.817048: step 8590, loss 0.537703.
Test: 2018-08-02T11:29:44.004240: step 8590, loss 0.549857.
Train: 2018-08-02T11:29:44.222963: step 8591, loss 0.512978.
Train: 2018-08-02T11:29:44.441663: step 8592, loss 0.587267.
Train: 2018-08-02T11:29:44.660336: step 8593, loss 0.562505.
Train: 2018-08-02T11:29:44.894687: step 8594, loss 0.529526.
Train: 2018-08-02T11:29:45.113355: step 8595, loss 0.529518.
Train: 2018-08-02T11:29:45.347706: step 8596, loss 0.54598.
Train: 2018-08-02T11:29:45.566404: step 8597, loss 0.579011.
Train: 2018-08-02T11:29:45.800724: step 8598, loss 0.554227.
Train: 2018-08-02T11:29:46.035017: step 8599, loss 0.6287.
Train: 2018-08-02T11:29:46.253744: step 8600, loss 0.529344.
Test: 2018-08-02T11:29:47.456557: step 8600, loss 0.547336.
Train: 2018-08-02T11:29:48.331377: step 8601, loss 0.595548.
Train: 2018-08-02T11:29:48.550076: step 8602, loss 0.521233.
Train: 2018-08-02T11:29:48.784377: step 8603, loss 0.504582.
Train: 2018-08-02T11:29:49.003070: step 8604, loss 0.645205.
Train: 2018-08-02T11:29:49.237416: step 8605, loss 0.520985.
Train: 2018-08-02T11:29:49.456089: step 8606, loss 0.53763.
Train: 2018-08-02T11:29:49.643572: step 8607, loss 0.473912.
Train: 2018-08-02T11:29:49.893486: step 8608, loss 0.545986.
Train: 2018-08-02T11:29:50.112216: step 8609, loss 0.579405.
Train: 2018-08-02T11:29:50.346530: step 8610, loss 0.571238.
Test: 2018-08-02T11:29:51.533727: step 8610, loss 0.548104.
Train: 2018-08-02T11:29:51.736829: step 8611, loss 0.571535.
Train: 2018-08-02T11:29:51.955503: step 8612, loss 0.503418.
Train: 2018-08-02T11:29:52.189849: step 8613, loss 0.613144.
Train: 2018-08-02T11:29:52.408546: step 8614, loss 0.512183.
Train: 2018-08-02T11:29:52.642849: step 8615, loss 0.545604.
Train: 2018-08-02T11:29:52.861571: step 8616, loss 0.570741.
Train: 2018-08-02T11:29:53.095865: step 8617, loss 0.579174.
Train: 2018-08-02T11:29:53.314590: step 8618, loss 0.587556.
Train: 2018-08-02T11:29:53.533258: step 8619, loss 0.60428.
Train: 2018-08-02T11:29:53.767610: step 8620, loss 0.604216.
Test: 2018-08-02T11:29:54.954801: step 8620, loss 0.547152.
Train: 2018-08-02T11:29:55.173529: step 8621, loss 0.562423.
Train: 2018-08-02T11:29:55.407845: step 8622, loss 0.595717.
Train: 2018-08-02T11:29:55.626548: step 8623, loss 0.537589.
Train: 2018-08-02T11:29:55.860841: step 8624, loss 0.562493.
Train: 2018-08-02T11:29:56.079562: step 8625, loss 0.570762.
Train: 2018-08-02T11:29:56.313866: step 8626, loss 0.595489.
Train: 2018-08-02T11:29:56.548177: step 8627, loss 0.554322.
Train: 2018-08-02T11:29:56.766891: step 8628, loss 0.570762.
Train: 2018-08-02T11:29:56.985606: step 8629, loss 0.488949.
Train: 2018-08-02T11:29:57.219928: step 8630, loss 0.554409.
Test: 2018-08-02T11:29:58.407117: step 8630, loss 0.547991.
Train: 2018-08-02T11:29:58.610195: step 8631, loss 0.546234.
Train: 2018-08-02T11:29:58.828924: step 8632, loss 0.497149.
Train: 2018-08-02T11:29:59.047623: step 8633, loss 0.603541.
Train: 2018-08-02T11:29:59.266290: step 8634, loss 0.578961.
Train: 2018-08-02T11:29:59.484990: step 8635, loss 0.554361.
Train: 2018-08-02T11:29:59.703719: step 8636, loss 0.611774.
Train: 2018-08-02T11:29:59.922419: step 8637, loss 0.611735.
Train: 2018-08-02T11:30:00.141114: step 8638, loss 0.628008.
Train: 2018-08-02T11:30:00.375432: step 8639, loss 0.538181.
Train: 2018-08-02T11:30:00.594104: step 8640, loss 0.530144.
Test: 2018-08-02T11:30:01.781327: step 8640, loss 0.549306.
Train: 2018-08-02T11:30:02.015647: step 8641, loss 0.505862.
Train: 2018-08-02T11:30:02.250017: step 8642, loss 0.554556.
Train: 2018-08-02T11:30:02.468693: step 8643, loss 0.55455.
Train: 2018-08-02T11:30:02.718660: step 8644, loss 0.530171.
Train: 2018-08-02T11:30:02.937306: step 8645, loss 0.578915.
Train: 2018-08-02T11:30:03.171654: step 8646, loss 0.570779.
Train: 2018-08-02T11:30:03.405972: step 8647, loss 0.578921.
Train: 2018-08-02T11:30:03.640267: step 8648, loss 0.55449.
Train: 2018-08-02T11:30:03.858996: step 8649, loss 0.497465.
Train: 2018-08-02T11:30:04.077694: step 8650, loss 0.619737.
Test: 2018-08-02T11:30:05.264887: step 8650, loss 0.548412.
Train: 2018-08-02T11:30:05.483615: step 8651, loss 0.578932.
Train: 2018-08-02T11:30:05.733557: step 8652, loss 0.595258.
Train: 2018-08-02T11:30:05.967877: step 8653, loss 0.521852.
Train: 2018-08-02T11:30:06.186575: step 8654, loss 0.53815.
Train: 2018-08-02T11:30:06.420866: step 8655, loss 0.619743.
Train: 2018-08-02T11:30:06.639564: step 8656, loss 0.538149.
Train: 2018-08-02T11:30:06.873920: step 8657, loss 0.570762.
Train: 2018-08-02T11:30:07.092614: step 8658, loss 0.513686.
Train: 2018-08-02T11:30:07.311282: step 8659, loss 0.587081.
Train: 2018-08-02T11:30:07.545602: step 8660, loss 0.587116.
Test: 2018-08-02T11:30:08.732824: step 8660, loss 0.548596.
Train: 2018-08-02T11:30:08.951555: step 8661, loss 0.513638.
Train: 2018-08-02T11:30:09.170246: step 8662, loss 0.472726.
Train: 2018-08-02T11:30:09.404567: step 8663, loss 0.505144.
Train: 2018-08-02T11:30:09.623271: step 8664, loss 0.620293.
Train: 2018-08-02T11:30:09.841970: step 8665, loss 0.570828.
Train: 2018-08-02T11:30:10.076261: step 8666, loss 0.529351.
Train: 2018-08-02T11:30:10.310580: step 8667, loss 0.529349.
Train: 2018-08-02T11:30:10.544931: step 8668, loss 0.504069.
Train: 2018-08-02T11:30:10.763598: step 8669, loss 0.545654.
Train: 2018-08-02T11:30:10.982322: step 8670, loss 0.520481.
Test: 2018-08-02T11:30:12.169519: step 8670, loss 0.548055.
Train: 2018-08-02T11:30:12.372597: step 8671, loss 0.629768.
Train: 2018-08-02T11:30:12.591319: step 8672, loss 0.630176.
Train: 2018-08-02T11:30:12.810025: step 8673, loss 0.655436.
Train: 2018-08-02T11:30:13.028723: step 8674, loss 0.570657.
Train: 2018-08-02T11:30:13.247394: step 8675, loss 0.537186.
Train: 2018-08-02T11:30:13.466116: step 8676, loss 0.554091.
Train: 2018-08-02T11:30:13.684820: step 8677, loss 0.554089.
Train: 2018-08-02T11:30:13.903523: step 8678, loss 0.554075.
Train: 2018-08-02T11:30:14.153430: step 8679, loss 0.520812.
Train: 2018-08-02T11:30:14.387750: step 8680, loss 0.570771.
Test: 2018-08-02T11:30:15.574972: step 8680, loss 0.549143.
Train: 2018-08-02T11:30:15.809317: step 8681, loss 0.54584.
Train: 2018-08-02T11:30:16.043613: step 8682, loss 0.579072.
Train: 2018-08-02T11:30:16.262310: step 8683, loss 0.57906.
Train: 2018-08-02T11:30:16.512277: step 8684, loss 0.521019.
Train: 2018-08-02T11:30:16.746573: step 8685, loss 0.612195.
Train: 2018-08-02T11:30:16.965303: step 8686, loss 0.51279.
Train: 2018-08-02T11:30:17.199617: step 8687, loss 0.603867.
Train: 2018-08-02T11:30:17.449560: step 8688, loss 0.52942.
Train: 2018-08-02T11:30:17.668231: step 8689, loss 0.504618.
Train: 2018-08-02T11:30:17.886961: step 8690, loss 0.579023.
Test: 2018-08-02T11:30:19.089774: step 8690, loss 0.548079.
Train: 2018-08-02T11:30:19.292851: step 8691, loss 0.579022.
Train: 2018-08-02T11:30:19.511580: step 8692, loss 0.512803.
Train: 2018-08-02T11:30:19.745902: step 8693, loss 0.587328.
Train: 2018-08-02T11:30:19.964600: step 8694, loss 0.603946.
Train: 2018-08-02T11:30:20.183298: step 8695, loss 0.54593.
Train: 2018-08-02T11:30:20.401966: step 8696, loss 0.587313.
Train: 2018-08-02T11:30:20.620695: step 8697, loss 0.52113.
Train: 2018-08-02T11:30:20.854985: step 8698, loss 0.612101.
Train: 2018-08-02T11:30:21.073714: step 8699, loss 0.620307.
Train: 2018-08-02T11:30:21.308036: step 8700, loss 0.521345.
Test: 2018-08-02T11:30:22.479605: step 8700, loss 0.549179.
Train: 2018-08-02T11:30:23.338778: step 8701, loss 0.570756.
Train: 2018-08-02T11:30:23.573116: step 8702, loss 0.554338.
Train: 2018-08-02T11:30:23.791823: step 8703, loss 0.513357.
Train: 2018-08-02T11:30:24.010530: step 8704, loss 0.521555.
Train: 2018-08-02T11:30:24.229226: step 8705, loss 0.611807.
Train: 2018-08-02T11:30:24.463545: step 8706, loss 0.636415.
Train: 2018-08-02T11:30:24.697838: step 8707, loss 0.554385.
Train: 2018-08-02T11:30:24.932180: step 8708, loss 0.54624.
Train: 2018-08-02T11:30:25.150884: step 8709, loss 0.619767.
Train: 2018-08-02T11:30:25.385203: step 8710, loss 0.521899.
Test: 2018-08-02T11:30:26.572396: step 8710, loss 0.549242.
Train: 2018-08-02T11:30:26.775498: step 8711, loss 0.546371.
Train: 2018-08-02T11:30:27.072310: step 8712, loss 0.570785.
Train: 2018-08-02T11:30:27.290978: step 8713, loss 0.643908.
Train: 2018-08-02T11:30:27.525310: step 8714, loss 0.546486.
Train: 2018-08-02T11:30:27.743998: step 8715, loss 0.611237.
Train: 2018-08-02T11:30:27.978344: step 8716, loss 0.570818.
Train: 2018-08-02T11:30:28.212639: step 8717, loss 0.611033.
Train: 2018-08-02T11:30:28.446983: step 8718, loss 0.514782.
Train: 2018-08-02T11:30:28.665656: step 8719, loss 0.530896.
Train: 2018-08-02T11:30:28.884386: step 8720, loss 0.570874.
Test: 2018-08-02T11:30:30.071577: step 8720, loss 0.549756.
Train: 2018-08-02T11:30:30.274656: step 8721, loss 0.610781.
Train: 2018-08-02T11:30:30.493385: step 8722, loss 0.562929.
Train: 2018-08-02T11:30:30.712076: step 8723, loss 0.523191.
Train: 2018-08-02T11:30:30.930775: step 8724, loss 0.578858.
Train: 2018-08-02T11:30:31.149474: step 8725, loss 0.55502.
Train: 2018-08-02T11:30:31.368174: step 8726, loss 0.586804.
Train: 2018-08-02T11:30:31.586877: step 8727, loss 0.483571.
Train: 2018-08-02T11:30:31.805571: step 8728, loss 0.554997.
Train: 2018-08-02T11:30:32.039891: step 8729, loss 0.586828.
Train: 2018-08-02T11:30:32.274186: step 8730, loss 0.530994.
Test: 2018-08-02T11:30:33.445786: step 8730, loss 0.550813.
Train: 2018-08-02T11:30:33.648864: step 8731, loss 0.586856.
Train: 2018-08-02T11:30:33.867589: step 8732, loss 0.50683.
Train: 2018-08-02T11:30:34.086293: step 8733, loss 0.594919.
Train: 2018-08-02T11:30:34.320620: step 8734, loss 0.602988.
Train: 2018-08-02T11:30:34.539311: step 8735, loss 0.530615.
Train: 2018-08-02T11:30:34.757980: step 8736, loss 0.554715.
Train: 2018-08-02T11:30:34.992300: step 8737, loss 0.570816.
Train: 2018-08-02T11:30:35.226651: step 8738, loss 0.562733.
Train: 2018-08-02T11:30:35.445349: step 8739, loss 0.570804.
Train: 2018-08-02T11:30:35.679638: step 8740, loss 0.595079.
Test: 2018-08-02T11:30:36.866861: step 8740, loss 0.550122.
Train: 2018-08-02T11:30:37.069939: step 8741, loss 0.522241.
Train: 2018-08-02T11:30:37.304283: step 8742, loss 0.530286.
Train: 2018-08-02T11:30:37.522956: step 8743, loss 0.587022.
Train: 2018-08-02T11:30:37.757279: step 8744, loss 0.54641.
Train: 2018-08-02T11:30:37.975982: step 8745, loss 0.595189.
Train: 2018-08-02T11:30:38.194723: step 8746, loss 0.538221.
Train: 2018-08-02T11:30:38.429026: step 8747, loss 0.554481.
Train: 2018-08-02T11:30:38.647728: step 8748, loss 0.521833.
Train: 2018-08-02T11:30:38.866394: step 8749, loss 0.54625.
Train: 2018-08-02T11:30:39.100712: step 8750, loss 0.578955.
Test: 2018-08-02T11:30:40.272313: step 8750, loss 0.549425.
Train: 2018-08-02T11:30:40.491042: step 8751, loss 0.537949.
Train: 2018-08-02T11:30:40.709742: step 8752, loss 0.546101.
Train: 2018-08-02T11:30:40.928411: step 8753, loss 0.529574.
Train: 2018-08-02T11:30:41.162730: step 8754, loss 0.58727.
Train: 2018-08-02T11:30:41.412702: step 8755, loss 0.529393.
Train: 2018-08-02T11:30:41.647017: step 8756, loss 0.603932.
Train: 2018-08-02T11:30:41.865716: step 8757, loss 0.487769.
Train: 2018-08-02T11:30:42.053147: step 8758, loss 0.686694.
Train: 2018-08-02T11:30:42.303089: step 8759, loss 0.545816.
Train: 2018-08-02T11:30:42.521786: step 8760, loss 0.554147.
Test: 2018-08-02T11:30:43.740251: step 8760, loss 0.548771.
Train: 2018-08-02T11:30:43.943329: step 8761, loss 0.587373.
Train: 2018-08-02T11:30:44.162058: step 8762, loss 0.545862.
Train: 2018-08-02T11:30:44.380725: step 8763, loss 0.545876.
Train: 2018-08-02T11:30:44.615048: step 8764, loss 0.521003.
Train: 2018-08-02T11:30:44.849369: step 8765, loss 0.471176.
Train: 2018-08-02T11:30:45.083717: step 8766, loss 0.620687.
Train: 2018-08-02T11:30:45.302416: step 8767, loss 0.579092.
Train: 2018-08-02T11:30:45.552352: step 8768, loss 0.562432.
Train: 2018-08-02T11:30:45.771050: step 8769, loss 0.520763.
Train: 2018-08-02T11:30:45.989755: step 8770, loss 0.579109.
Test: 2018-08-02T11:30:47.192568: step 8770, loss 0.548676.
Train: 2018-08-02T11:30:47.426889: step 8771, loss 0.637534.
Train: 2018-08-02T11:30:47.661239: step 8772, loss 0.545764.
Train: 2018-08-02T11:30:47.895533: step 8773, loss 0.537465.
Train: 2018-08-02T11:30:48.114259: step 8774, loss 0.504198.
Train: 2018-08-02T11:30:48.348547: step 8775, loss 0.620727.
Train: 2018-08-02T11:30:48.582892: step 8776, loss 0.562441.
Train: 2018-08-02T11:30:48.817212: step 8777, loss 0.570761.
Train: 2018-08-02T11:30:49.051510: step 8778, loss 0.537534.
Train: 2018-08-02T11:30:49.301475: step 8779, loss 0.54585.
Train: 2018-08-02T11:30:49.535802: step 8780, loss 0.587364.
Test: 2018-08-02T11:30:50.754234: step 8780, loss 0.547461.
Train: 2018-08-02T11:30:50.972971: step 8781, loss 0.562462.
Train: 2018-08-02T11:30:51.207277: step 8782, loss 0.595627.
Train: 2018-08-02T11:30:51.425977: step 8783, loss 0.620418.
Train: 2018-08-02T11:30:51.675892: step 8784, loss 0.545997.
Train: 2018-08-02T11:30:51.910214: step 8785, loss 0.529584.
Train: 2018-08-02T11:30:52.144541: step 8786, loss 0.587207.
Train: 2018-08-02T11:30:52.410096: step 8787, loss 0.505085.
Train: 2018-08-02T11:30:52.628794: step 8788, loss 0.578968.
Train: 2018-08-02T11:30:52.847524: step 8789, loss 0.546157.
Train: 2018-08-02T11:30:53.081846: step 8790, loss 0.505166.
Test: 2018-08-02T11:30:54.310764: step 8790, loss 0.549786.
Train: 2018-08-02T11:30:54.529487: step 8791, loss 0.578979.
Train: 2018-08-02T11:30:54.764289: step 8792, loss 0.578973.
Train: 2018-08-02T11:30:54.999199: step 8793, loss 0.554333.
Train: 2018-08-02T11:30:55.234239: step 8794, loss 0.521462.
Train: 2018-08-02T11:30:55.479237: step 8795, loss 0.58721.
Train: 2018-08-02T11:30:55.697960: step 8796, loss 0.587216.
Train: 2018-08-02T11:30:55.932255: step 8797, loss 0.578985.
Train: 2018-08-02T11:30:56.166584: step 8798, loss 0.644747.
Train: 2018-08-02T11:30:56.400912: step 8799, loss 0.546164.
Train: 2018-08-02T11:30:56.666457: step 8800, loss 0.619862.
Test: 2018-08-02T11:30:57.869301: step 8800, loss 0.549934.
Train: 2018-08-02T11:30:58.728476: step 8801, loss 0.562617.
Train: 2018-08-02T11:30:58.947173: step 8802, loss 0.578915.
Train: 2018-08-02T11:30:59.165896: step 8803, loss 0.546469.
Train: 2018-08-02T11:30:59.384602: step 8804, loss 0.489908.
Train: 2018-08-02T11:30:59.634539: step 8805, loss 0.627417.
Train: 2018-08-02T11:30:59.868859: step 8806, loss 0.570811.
Train: 2018-08-02T11:31:00.118800: step 8807, loss 0.490225.
Train: 2018-08-02T11:31:00.337500: step 8808, loss 0.546632.
Train: 2018-08-02T11:31:00.571794: step 8809, loss 0.554679.
Train: 2018-08-02T11:31:00.790518: step 8810, loss 0.627339.
Test: 2018-08-02T11:31:02.008956: step 8810, loss 0.548338.
Train: 2018-08-02T11:31:02.227680: step 8811, loss 0.578883.
Train: 2018-08-02T11:31:02.461978: step 8812, loss 0.586942.
Train: 2018-08-02T11:31:02.680674: step 8813, loss 0.506424.
Train: 2018-08-02T11:31:02.899373: step 8814, loss 0.603032.
Train: 2018-08-02T11:31:03.133724: step 8815, loss 0.667375.
Train: 2018-08-02T11:31:03.352417: step 8816, loss 0.570848.
Train: 2018-08-02T11:31:03.586712: step 8817, loss 0.530891.
Train: 2018-08-02T11:31:03.805410: step 8818, loss 0.594822.
Train: 2018-08-02T11:31:04.039763: step 8819, loss 0.531087.
Train: 2018-08-02T11:31:04.258455: step 8820, loss 0.578858.
Test: 2018-08-02T11:31:05.476895: step 8820, loss 0.548978.
Train: 2018-08-02T11:31:05.679997: step 8821, loss 0.602685.
Train: 2018-08-02T11:31:05.898700: step 8822, loss 0.570933.
Train: 2018-08-02T11:31:06.117394: step 8823, loss 0.570949.
Train: 2018-08-02T11:31:06.336098: step 8824, loss 0.531481.
Train: 2018-08-02T11:31:06.586040: step 8825, loss 0.515725.
Train: 2018-08-02T11:31:06.804739: step 8826, loss 0.594661.
Train: 2018-08-02T11:31:07.023419: step 8827, loss 0.586764.
Train: 2018-08-02T11:31:07.257760: step 8828, loss 0.578863.
Train: 2018-08-02T11:31:07.476427: step 8829, loss 0.484138.
Train: 2018-08-02T11:31:07.710772: step 8830, loss 0.618399.
Test: 2018-08-02T11:31:08.913590: step 8830, loss 0.550012.
Train: 2018-08-02T11:31:09.116693: step 8831, loss 0.56304.
Train: 2018-08-02T11:31:09.366635: step 8832, loss 0.618429.
Train: 2018-08-02T11:31:09.585356: step 8833, loss 0.483977.
Train: 2018-08-02T11:31:09.819661: step 8834, loss 0.531349.
Train: 2018-08-02T11:31:10.053972: step 8835, loss 0.578859.
Train: 2018-08-02T11:31:10.288293: step 8836, loss 0.594759.
Train: 2018-08-02T11:31:10.522588: step 8837, loss 0.626606.
Train: 2018-08-02T11:31:10.741312: step 8838, loss 0.562953.
Train: 2018-08-02T11:31:10.991254: step 8839, loss 0.539114.
Train: 2018-08-02T11:31:11.209955: step 8840, loss 0.58681.
Test: 2018-08-02T11:31:12.412771: step 8840, loss 0.550595.
Train: 2018-08-02T11:31:12.678364: step 8841, loss 0.483456.
Train: 2018-08-02T11:31:12.897065: step 8842, loss 0.531061.
Train: 2018-08-02T11:31:13.131361: step 8843, loss 0.562885.
Train: 2018-08-02T11:31:13.365693: step 8844, loss 0.562847.
Train: 2018-08-02T11:31:13.584373: step 8845, loss 0.554785.
Train: 2018-08-02T11:31:13.803094: step 8846, loss 0.586923.
Train: 2018-08-02T11:31:14.021799: step 8847, loss 0.595002.
Train: 2018-08-02T11:31:14.256112: step 8848, loss 0.562748.
Train: 2018-08-02T11:31:14.474788: step 8849, loss 0.570811.
Train: 2018-08-02T11:31:14.709107: step 8850, loss 0.619274.
Test: 2018-08-02T11:31:15.911950: step 8850, loss 0.550008.
Train: 2018-08-02T11:31:16.115064: step 8851, loss 0.635377.
Train: 2018-08-02T11:31:16.333760: step 8852, loss 0.554726.
Train: 2018-08-02T11:31:16.568047: step 8853, loss 0.578871.
Train: 2018-08-02T11:31:16.786746: step 8854, loss 0.57085.
Train: 2018-08-02T11:31:17.021097: step 8855, loss 0.506862.
Train: 2018-08-02T11:31:17.239765: step 8856, loss 0.610855.
Train: 2018-08-02T11:31:17.458464: step 8857, loss 0.522957.
Train: 2018-08-02T11:31:17.692814: step 8858, loss 0.506993.
Train: 2018-08-02T11:31:17.911507: step 8859, loss 0.594858.
Train: 2018-08-02T11:31:18.145834: step 8860, loss 0.498852.
Test: 2018-08-02T11:31:19.333024: step 8860, loss 0.548514.
Train: 2018-08-02T11:31:19.551754: step 8861, loss 0.57085.
Train: 2018-08-02T11:31:19.817311: step 8862, loss 0.546735.
Train: 2018-08-02T11:31:20.036010: step 8863, loss 0.506415.
Train: 2018-08-02T11:31:20.254684: step 8864, loss 0.538492.
Train: 2018-08-02T11:31:20.489037: step 8865, loss 0.570794.
Train: 2018-08-02T11:31:20.707730: step 8866, loss 0.595185.
Train: 2018-08-02T11:31:20.957677: step 8867, loss 0.513715.
Train: 2018-08-02T11:31:21.176376: step 8868, loss 0.587119.
Train: 2018-08-02T11:31:21.410663: step 8869, loss 0.554374.
Train: 2018-08-02T11:31:21.629393: step 8870, loss 0.537913.
Test: 2018-08-02T11:31:22.816588: step 8870, loss 0.548021.
Train: 2018-08-02T11:31:23.050903: step 8871, loss 0.62014.
Train: 2018-08-02T11:31:23.269603: step 8872, loss 0.686106.
Train: 2018-08-02T11:31:23.503925: step 8873, loss 0.529674.
Train: 2018-08-02T11:31:23.738257: step 8874, loss 0.537935.
Train: 2018-08-02T11:31:23.956972: step 8875, loss 0.55436.
Train: 2018-08-02T11:31:24.191287: step 8876, loss 0.464196.
Train: 2018-08-02T11:31:24.409960: step 8877, loss 0.603613.
Train: 2018-08-02T11:31:24.644281: step 8878, loss 0.628294.
Train: 2018-08-02T11:31:24.894253: step 8879, loss 0.611817.
Train: 2018-08-02T11:31:25.112952: step 8880, loss 0.497013.
Test: 2018-08-02T11:31:26.347007: step 8880, loss 0.546628.
Train: 2018-08-02T11:31:26.565731: step 8881, loss 0.537997.
Train: 2018-08-02T11:31:26.784405: step 8882, loss 0.546181.
Train: 2018-08-02T11:31:27.003134: step 8883, loss 0.595359.
Train: 2018-08-02T11:31:27.237424: step 8884, loss 0.611747.
Train: 2018-08-02T11:31:27.503022: step 8885, loss 0.56258.
Train: 2018-08-02T11:31:27.721711: step 8886, loss 0.513553.
Train: 2018-08-02T11:31:27.956034: step 8887, loss 0.480854.
Train: 2018-08-02T11:31:28.174729: step 8888, loss 0.529815.
Train: 2018-08-02T11:31:28.393428: step 8889, loss 0.55434.
Train: 2018-08-02T11:31:28.627724: step 8890, loss 0.603677.
Test: 2018-08-02T11:31:29.814944: step 8890, loss 0.548568.
Train: 2018-08-02T11:31:30.018052: step 8891, loss 0.661386.
Train: 2018-08-02T11:31:30.252343: step 8892, loss 0.48848.
Train: 2018-08-02T11:31:30.471066: step 8893, loss 0.578989.
Train: 2018-08-02T11:31:30.689766: step 8894, loss 0.529601.
Train: 2018-08-02T11:31:30.939681: step 8895, loss 0.537808.
Train: 2018-08-02T11:31:31.158405: step 8896, loss 0.537769.
Train: 2018-08-02T11:31:31.377104: step 8897, loss 0.554237.
Train: 2018-08-02T11:31:31.611430: step 8898, loss 0.545939.
Train: 2018-08-02T11:31:31.861342: step 8899, loss 0.603903.
Train: 2018-08-02T11:31:32.080039: step 8900, loss 0.60392.
Test: 2018-08-02T11:31:33.282883: step 8900, loss 0.549207.
Train: 2018-08-02T11:31:34.157679: step 8901, loss 0.496191.
Train: 2018-08-02T11:31:34.392029: step 8902, loss 0.587343.
Train: 2018-08-02T11:31:34.610698: step 8903, loss 0.570758.
Train: 2018-08-02T11:31:34.829429: step 8904, loss 0.570758.
Train: 2018-08-02T11:31:35.048121: step 8905, loss 0.545891.
Train: 2018-08-02T11:31:35.282445: step 8906, loss 0.554181.
Train: 2018-08-02T11:31:35.501138: step 8907, loss 0.487873.
Train: 2018-08-02T11:31:35.719812: step 8908, loss 0.512642.
Train: 2018-08-02T11:31:35.907299: step 8909, loss 0.42038.
Train: 2018-08-02T11:31:36.125992: step 8910, loss 0.595882.
Test: 2018-08-02T11:31:37.328811: step 8910, loss 0.547579.
Train: 2018-08-02T11:31:37.563161: step 8911, loss 0.570788.
Train: 2018-08-02T11:31:37.781855: step 8912, loss 0.646667.
Train: 2018-08-02T11:31:38.016180: step 8913, loss 0.545495.
Train: 2018-08-02T11:31:38.266093: step 8914, loss 0.613031.
Train: 2018-08-02T11:31:38.500411: step 8915, loss 0.63833.
Train: 2018-08-02T11:31:38.719114: step 8916, loss 0.520267.
Train: 2018-08-02T11:31:38.953431: step 8917, loss 0.545559.
Train: 2018-08-02T11:31:39.172159: step 8918, loss 0.495183.
Train: 2018-08-02T11:31:39.390827: step 8919, loss 0.553976.
Train: 2018-08-02T11:31:39.625184: step 8920, loss 0.570795.
Test: 2018-08-02T11:31:40.827991: step 8920, loss 0.549104.
Train: 2018-08-02T11:31:41.046690: step 8921, loss 0.570794.
Train: 2018-08-02T11:31:41.265418: step 8922, loss 0.596022.
Train: 2018-08-02T11:31:41.484117: step 8923, loss 0.537184.
Train: 2018-08-02T11:31:41.702786: step 8924, loss 0.629552.
Train: 2018-08-02T11:31:41.952729: step 8925, loss 0.604284.
Train: 2018-08-02T11:31:42.171452: step 8926, loss 0.57912.
Train: 2018-08-02T11:31:42.390125: step 8927, loss 0.570762.
Train: 2018-08-02T11:31:42.608849: step 8928, loss 0.570758.
Train: 2018-08-02T11:31:42.827522: step 8929, loss 0.512873.
Train: 2018-08-02T11:31:43.093114: step 8930, loss 0.504726.
Test: 2018-08-02T11:31:44.295929: step 8930, loss 0.548916.
Train: 2018-08-02T11:31:44.514653: step 8931, loss 0.628511.
Train: 2018-08-02T11:31:44.748978: step 8932, loss 0.562523.
Train: 2018-08-02T11:31:44.983299: step 8933, loss 0.562539.
Train: 2018-08-02T11:31:45.201992: step 8934, loss 0.554349.
Train: 2018-08-02T11:31:45.436326: step 8935, loss 0.505203.
Train: 2018-08-02T11:31:45.654987: step 8936, loss 0.521586.
Train: 2018-08-02T11:31:45.889332: step 8937, loss 0.644604.
Train: 2018-08-02T11:31:46.108035: step 8938, loss 0.4806.
Train: 2018-08-02T11:31:46.342353: step 8939, loss 0.554351.
Train: 2018-08-02T11:31:46.576645: step 8940, loss 0.513254.
Test: 2018-08-02T11:31:47.763867: step 8940, loss 0.548205.
Train: 2018-08-02T11:31:47.998218: step 8941, loss 0.480176.
Train: 2018-08-02T11:31:48.232534: step 8942, loss 0.52933.
Train: 2018-08-02T11:31:48.498075: step 8943, loss 0.537668.
Train: 2018-08-02T11:31:48.716768: step 8944, loss 0.645729.
Train: 2018-08-02T11:31:48.935498: step 8945, loss 0.612641.
Train: 2018-08-02T11:31:49.154191: step 8946, loss 0.646672.
Train: 2018-08-02T11:31:49.372865: step 8947, loss 0.545662.
Train: 2018-08-02T11:31:49.607210: step 8948, loss 0.504184.
Train: 2018-08-02T11:31:49.825886: step 8949, loss 0.537496.
Train: 2018-08-02T11:31:50.044607: step 8950, loss 0.545811.
Test: 2018-08-02T11:31:51.247426: step 8950, loss 0.548349.
Train: 2018-08-02T11:31:51.466126: step 8951, loss 0.512506.
Train: 2018-08-02T11:31:51.700477: step 8952, loss 0.587436.
Train: 2018-08-02T11:31:51.919144: step 8953, loss 0.579109.
Train: 2018-08-02T11:31:52.137842: step 8954, loss 0.595798.
Train: 2018-08-02T11:31:52.356572: step 8955, loss 0.55409.
Train: 2018-08-02T11:31:52.590889: step 8956, loss 0.504101.
Train: 2018-08-02T11:31:52.825215: step 8957, loss 0.487377.
Train: 2018-08-02T11:31:53.043880: step 8958, loss 0.545699.
Train: 2018-08-02T11:31:53.262581: step 8959, loss 0.512144.
Train: 2018-08-02T11:31:53.481278: step 8960, loss 0.537182.
Test: 2018-08-02T11:31:54.684121: step 8960, loss 0.54791.
Train: 2018-08-02T11:31:54.887223: step 8961, loss 0.5708.
Train: 2018-08-02T11:31:55.121549: step 8962, loss 0.520123.
Train: 2018-08-02T11:31:55.355839: step 8963, loss 0.503043.
Train: 2018-08-02T11:31:55.590159: step 8964, loss 0.545335.
Train: 2018-08-02T11:31:55.824510: step 8965, loss 0.519665.
Train: 2018-08-02T11:31:56.058799: step 8966, loss 0.536633.
Train: 2018-08-02T11:31:56.308741: step 8967, loss 0.613934.
Train: 2018-08-02T11:31:56.543063: step 8968, loss 0.545103.
Train: 2018-08-02T11:31:56.761790: step 8969, loss 0.570972.
Train: 2018-08-02T11:31:56.980459: step 8970, loss 0.605559.
Test: 2018-08-02T11:31:58.183302: step 8970, loss 0.546514.
Train: 2018-08-02T11:31:58.402001: step 8971, loss 0.56234.
Train: 2018-08-02T11:31:58.620699: step 8972, loss 0.536424.
Train: 2018-08-02T11:31:58.839423: step 8973, loss 0.614171.
Train: 2018-08-02T11:31:59.073719: step 8974, loss 0.562338.
Train: 2018-08-02T11:31:59.308039: step 8975, loss 0.562336.
Train: 2018-08-02T11:31:59.542359: step 8976, loss 0.484959.
Train: 2018-08-02T11:31:59.761057: step 8977, loss 0.545144.
Train: 2018-08-02T11:32:00.011001: step 8978, loss 0.545141.
Train: 2018-08-02T11:32:00.245319: step 8979, loss 0.519356.
Train: 2018-08-02T11:32:00.479641: step 8980, loss 0.545136.
Test: 2018-08-02T11:32:01.682482: step 8980, loss 0.548155.
Train: 2018-08-02T11:32:01.901181: step 8981, loss 0.648439.
Train: 2018-08-02T11:32:02.119906: step 8982, loss 0.60532.
Train: 2018-08-02T11:32:02.354200: step 8983, loss 0.639505.
Train: 2018-08-02T11:32:02.572924: step 8984, loss 0.570876.
Train: 2018-08-02T11:32:02.807220: step 8985, loss 0.519832.
Train: 2018-08-02T11:32:03.025949: step 8986, loss 0.570829.
Train: 2018-08-02T11:32:03.244616: step 8987, loss 0.562362.
Train: 2018-08-02T11:32:03.463348: step 8988, loss 0.579222.
Train: 2018-08-02T11:32:03.682015: step 8989, loss 0.478421.
Train: 2018-08-02T11:32:03.900713: step 8990, loss 0.554008.
Test: 2018-08-02T11:32:05.103556: step 8990, loss 0.54783.
Train: 2018-08-02T11:32:05.337902: step 8991, loss 0.604288.
Train: 2018-08-02T11:32:05.556575: step 8992, loss 0.54569.
Train: 2018-08-02T11:32:05.790903: step 8993, loss 0.629199.
Train: 2018-08-02T11:32:06.025220: step 8994, loss 0.579084.
Train: 2018-08-02T11:32:06.243944: step 8995, loss 0.562463.
Train: 2018-08-02T11:32:06.462645: step 8996, loss 0.579026.
Train: 2018-08-02T11:32:06.696934: step 8997, loss 0.537784.
Train: 2018-08-02T11:32:06.931284: step 8998, loss 0.52964.
Train: 2018-08-02T11:32:07.181196: step 8999, loss 0.636457.
Train: 2018-08-02T11:32:07.399918: step 9000, loss 0.505266.
Test: 2018-08-02T11:32:08.587116: step 9000, loss 0.549879.
Train: 2018-08-02T11:32:09.430693: step 9001, loss 0.505361.
Train: 2018-08-02T11:32:09.649397: step 9002, loss 0.52171.
Train: 2018-08-02T11:32:09.883687: step 9003, loss 0.529842.
Train: 2018-08-02T11:32:10.118008: step 9004, loss 0.529771.
Train: 2018-08-02T11:32:10.336731: step 9005, loss 0.578975.
Train: 2018-08-02T11:32:10.571056: step 9006, loss 0.578986.
Train: 2018-08-02T11:32:10.805346: step 9007, loss 0.537804.
Train: 2018-08-02T11:32:11.024069: step 9008, loss 0.579008.
Train: 2018-08-02T11:32:11.258390: step 9009, loss 0.587265.
Train: 2018-08-02T11:32:11.492685: step 9010, loss 0.579014.
Test: 2018-08-02T11:32:12.695528: step 9010, loss 0.548907.
Train: 2018-08-02T11:32:12.914252: step 9011, loss 0.512971.
Train: 2018-08-02T11:32:13.132927: step 9012, loss 0.496406.
Train: 2018-08-02T11:32:13.351656: step 9013, loss 0.645304.
Train: 2018-08-02T11:32:13.585958: step 9014, loss 0.48797.
Train: 2018-08-02T11:32:13.835889: step 9015, loss 0.645387.
Train: 2018-08-02T11:32:14.054612: step 9016, loss 0.56247.
Train: 2018-08-02T11:32:14.304557: step 9017, loss 0.570757.
Train: 2018-08-02T11:32:14.538873: step 9018, loss 0.570756.
Train: 2018-08-02T11:32:14.757576: step 9019, loss 0.537691.
Train: 2018-08-02T11:32:14.991897: step 9020, loss 0.587283.
Test: 2018-08-02T11:32:16.194709: step 9020, loss 0.549475.
Train: 2018-08-02T11:32:16.429030: step 9021, loss 0.496462.
Train: 2018-08-02T11:32:16.647759: step 9022, loss 0.595535.
Train: 2018-08-02T11:32:16.866457: step 9023, loss 0.636812.
Train: 2018-08-02T11:32:17.116368: step 9024, loss 0.513077.
Train: 2018-08-02T11:32:17.319476: step 9025, loss 0.578991.
Train: 2018-08-02T11:32:17.538175: step 9026, loss 0.570758.
Train: 2018-08-02T11:32:17.772465: step 9027, loss 0.578974.
Train: 2018-08-02T11:32:17.991164: step 9028, loss 0.595365.
Train: 2018-08-02T11:32:18.225510: step 9029, loss 0.603498.
Train: 2018-08-02T11:32:18.444207: step 9030, loss 0.529978.
Test: 2018-08-02T11:32:19.647026: step 9030, loss 0.548097.
Train: 2018-08-02T11:32:19.865724: step 9031, loss 0.578921.
Train: 2018-08-02T11:32:20.100044: step 9032, loss 0.570785.
Train: 2018-08-02T11:32:20.334415: step 9033, loss 0.587011.
Train: 2018-08-02T11:32:20.553064: step 9034, loss 0.546533.
Train: 2018-08-02T11:32:20.771762: step 9035, loss 0.578886.
Train: 2018-08-02T11:32:20.990491: step 9036, loss 0.506339.
Train: 2018-08-02T11:32:21.209184: step 9037, loss 0.522475.
Train: 2018-08-02T11:32:21.443514: step 9038, loss 0.51437.
Train: 2018-08-02T11:32:21.662203: step 9039, loss 0.506173.
Train: 2018-08-02T11:32:21.896554: step 9040, loss 0.522166.
Test: 2018-08-02T11:32:23.083721: step 9040, loss 0.549245.
Train: 2018-08-02T11:32:23.318041: step 9041, loss 0.570781.
Train: 2018-08-02T11:32:23.536739: step 9042, loss 0.538123.
Train: 2018-08-02T11:32:23.771060: step 9043, loss 0.570764.
Train: 2018-08-02T11:32:23.989760: step 9044, loss 0.595403.
Train: 2018-08-02T11:32:24.224086: step 9045, loss 0.595446.
Train: 2018-08-02T11:32:24.442811: step 9046, loss 0.546049.
Train: 2018-08-02T11:32:24.677123: step 9047, loss 0.61198.
Train: 2018-08-02T11:32:24.911449: step 9048, loss 0.562514.
Train: 2018-08-02T11:32:25.130146: step 9049, loss 0.603718.
Train: 2018-08-02T11:32:25.348816: step 9050, loss 0.562528.
Test: 2018-08-02T11:32:26.567281: step 9050, loss 0.548811.
Train: 2018-08-02T11:32:26.786009: step 9051, loss 0.669399.
Train: 2018-08-02T11:32:27.067192: step 9052, loss 0.578954.
Train: 2018-08-02T11:32:27.301483: step 9053, loss 0.570772.
Train: 2018-08-02T11:32:27.520212: step 9054, loss 0.505729.
Train: 2018-08-02T11:32:27.770145: step 9055, loss 0.587023.
Train: 2018-08-02T11:32:28.004445: step 9056, loss 0.570797.
Train: 2018-08-02T11:32:28.238789: step 9057, loss 0.554642.
Train: 2018-08-02T11:32:28.488741: step 9058, loss 0.643431.
Train: 2018-08-02T11:32:28.723027: step 9059, loss 0.562791.
Train: 2018-08-02T11:32:28.894886: step 9060, loss 0.562833.
Test: 2018-08-02T11:32:30.097704: step 9060, loss 0.550069.
Train: 2018-08-02T11:32:30.316427: step 9061, loss 0.60285.
Train: 2018-08-02T11:32:30.535102: step 9062, loss 0.523071.
Train: 2018-08-02T11:32:30.753800: step 9063, loss 0.539082.
Train: 2018-08-02T11:32:30.972529: step 9064, loss 0.61065.
Train: 2018-08-02T11:32:31.206847: step 9065, loss 0.539193.
Train: 2018-08-02T11:32:31.441169: step 9066, loss 0.59471.
Train: 2018-08-02T11:32:31.659863: step 9067, loss 0.594687.
Train: 2018-08-02T11:32:31.894189: step 9068, loss 0.507782.
Train: 2018-08-02T11:32:32.112856: step 9069, loss 0.56307.
Train: 2018-08-02T11:32:32.345559: step 9070, loss 0.547279.
Test: 2018-08-02T11:32:33.532784: step 9070, loss 0.54914.
Train: 2018-08-02T11:32:33.767105: step 9071, loss 0.657862.
Train: 2018-08-02T11:32:34.001450: step 9072, loss 0.531546.
Train: 2018-08-02T11:32:34.220154: step 9073, loss 0.476403.
Train: 2018-08-02T11:32:34.454473: step 9074, loss 0.586759.
Train: 2018-08-02T11:32:34.688764: step 9075, loss 0.555142.
Train: 2018-08-02T11:32:34.907463: step 9076, loss 0.539272.
Train: 2018-08-02T11:32:35.141785: step 9077, loss 0.658203.
Train: 2018-08-02T11:32:35.360481: step 9078, loss 0.475752.
Train: 2018-08-02T11:32:35.610422: step 9079, loss 0.547071.
Train: 2018-08-02T11:32:35.844743: step 9080, loss 0.562928.
Test: 2018-08-02T11:32:37.031964: step 9080, loss 0.5514.
Train: 2018-08-02T11:32:37.250689: step 9081, loss 0.594827.
Train: 2018-08-02T11:32:37.485015: step 9082, loss 0.562876.
Train: 2018-08-02T11:32:37.703682: step 9083, loss 0.490824.
Train: 2018-08-02T11:32:37.922381: step 9084, loss 0.594925.
Train: 2018-08-02T11:32:38.172324: step 9085, loss 0.538655.
Train: 2018-08-02T11:32:38.391046: step 9086, loss 0.530499.
Train: 2018-08-02T11:32:38.609750: step 9087, loss 0.514186.
Train: 2018-08-02T11:32:38.844071: step 9088, loss 0.546428.
Train: 2018-08-02T11:32:39.078398: step 9089, loss 0.587078.
Train: 2018-08-02T11:32:39.297090: step 9090, loss 0.505359.
Test: 2018-08-02T11:32:40.499902: step 9090, loss 0.549598.
Train: 2018-08-02T11:32:40.702980: step 9091, loss 0.595385.
Train: 2018-08-02T11:32:40.921706: step 9092, loss 0.644841.
Train: 2018-08-02T11:32:41.140402: step 9093, loss 0.595464.
Train: 2018-08-02T11:32:41.359101: step 9094, loss 0.49666.
Train: 2018-08-02T11:32:41.593396: step 9095, loss 0.64494.
Train: 2018-08-02T11:32:41.827721: step 9096, loss 0.628405.
Train: 2018-08-02T11:32:42.030795: step 9097, loss 0.587189.
Train: 2018-08-02T11:32:42.265114: step 9098, loss 0.554378.
Train: 2018-08-02T11:32:42.483815: step 9099, loss 0.554421.
Train: 2018-08-02T11:32:42.733754: step 9100, loss 0.570773.
Test: 2018-08-02T11:32:43.967840: step 9100, loss 0.549225.
Train: 2018-08-02T11:32:44.905147: step 9101, loss 0.521924.
Train: 2018-08-02T11:32:45.123852: step 9102, loss 0.5301.
Train: 2018-08-02T11:32:45.342519: step 9103, loss 0.587054.
Train: 2018-08-02T11:32:45.576838: step 9104, loss 0.546384.
Train: 2018-08-02T11:32:45.795538: step 9105, loss 0.627709.
Train: 2018-08-02T11:32:46.014266: step 9106, loss 0.522069.
Train: 2018-08-02T11:32:46.232965: step 9107, loss 0.627603.
Train: 2018-08-02T11:32:46.467280: step 9108, loss 0.627503.
Train: 2018-08-02T11:32:46.701575: step 9109, loss 0.514286.
Train: 2018-08-02T11:32:46.920275: step 9110, loss 0.522451.
Test: 2018-08-02T11:32:48.123117: step 9110, loss 0.55005.
Train: 2018-08-02T11:32:48.326219: step 9111, loss 0.619165.
Train: 2018-08-02T11:32:48.544918: step 9112, loss 0.562788.
Train: 2018-08-02T11:32:48.763624: step 9113, loss 0.538715.
Train: 2018-08-02T11:32:48.997913: step 9114, loss 0.52269.
Train: 2018-08-02T11:32:49.216642: step 9115, loss 0.562812.
Train: 2018-08-02T11:32:49.435336: step 9116, loss 0.474443.
Train: 2018-08-02T11:32:49.654039: step 9117, loss 0.586932.
Train: 2018-08-02T11:32:49.888374: step 9118, loss 0.554676.
Train: 2018-08-02T11:32:50.122689: step 9119, loss 0.522298.
Train: 2018-08-02T11:32:50.341373: step 9120, loss 0.546474.
Test: 2018-08-02T11:32:51.544192: step 9120, loss 0.551135.
Train: 2018-08-02T11:32:51.747269: step 9121, loss 0.611432.
Train: 2018-08-02T11:32:51.965992: step 9122, loss 0.603344.
Train: 2018-08-02T11:32:52.184666: step 9123, loss 0.546345.
Train: 2018-08-02T11:32:52.419011: step 9124, loss 0.635974.
Train: 2018-08-02T11:32:52.653331: step 9125, loss 0.530075.
Train: 2018-08-02T11:32:52.872005: step 9126, loss 0.56264.
Train: 2018-08-02T11:32:53.106326: step 9127, loss 0.627743.
Train: 2018-08-02T11:32:53.325056: step 9128, loss 0.53829.
Train: 2018-08-02T11:32:53.574998: step 9129, loss 0.562673.
Train: 2018-08-02T11:32:53.809286: step 9130, loss 0.570792.
Test: 2018-08-02T11:32:54.996508: step 9130, loss 0.550094.
Train: 2018-08-02T11:32:55.230830: step 9131, loss 0.497873.
Train: 2018-08-02T11:32:55.449558: step 9132, loss 0.603227.
Train: 2018-08-02T11:32:55.683847: step 9133, loss 0.587007.
Train: 2018-08-02T11:32:55.918168: step 9134, loss 0.489791.
Train: 2018-08-02T11:32:56.136866: step 9135, loss 0.505918.
Train: 2018-08-02T11:32:56.355594: step 9136, loss 0.465116.
Train: 2018-08-02T11:32:56.574288: step 9137, loss 0.505458.
Train: 2018-08-02T11:32:56.808584: step 9138, loss 0.546139.
Train: 2018-08-02T11:32:57.027312: step 9139, loss 0.570756.
Train: 2018-08-02T11:32:57.246012: step 9140, loss 0.579041.
Test: 2018-08-02T11:32:58.448824: step 9140, loss 0.548948.
Train: 2018-08-02T11:32:58.667523: step 9141, loss 0.529198.
Train: 2018-08-02T11:32:58.886223: step 9142, loss 0.637511.
Train: 2018-08-02T11:32:59.120546: step 9143, loss 0.495568.
Train: 2018-08-02T11:32:59.339242: step 9144, loss 0.50375.
Train: 2018-08-02T11:32:59.557966: step 9145, loss 0.604425.
Train: 2018-08-02T11:32:59.776638: step 9146, loss 0.469674.
Train: 2018-08-02T11:33:00.026579: step 9147, loss 0.579277.
Train: 2018-08-02T11:33:00.245280: step 9148, loss 0.596285.
Train: 2018-08-02T11:33:00.479630: step 9149, loss 0.613333.
Train: 2018-08-02T11:33:00.713945: step 9150, loss 0.511353.
Test: 2018-08-02T11:33:01.901141: step 9150, loss 0.549115.
Train: 2018-08-02T11:33:02.119865: step 9151, loss 0.570851.
Train: 2018-08-02T11:33:02.354190: step 9152, loss 0.553833.
Train: 2018-08-02T11:33:02.588505: step 9153, loss 0.528281.
Train: 2018-08-02T11:33:02.807178: step 9154, loss 0.545294.
Train: 2018-08-02T11:33:03.025878: step 9155, loss 0.587933.
Train: 2018-08-02T11:33:03.260197: step 9156, loss 0.587935.
Train: 2018-08-02T11:33:03.494548: step 9157, loss 0.56234.
Train: 2018-08-02T11:33:03.728838: step 9158, loss 0.545303.
Train: 2018-08-02T11:33:03.947537: step 9159, loss 0.604911.
Train: 2018-08-02T11:33:04.166235: step 9160, loss 0.536847.
Test: 2018-08-02T11:33:05.369079: step 9160, loss 0.548756.
Train: 2018-08-02T11:33:05.587777: step 9161, loss 0.494431.
Train: 2018-08-02T11:33:05.806507: step 9162, loss 0.587822.
Train: 2018-08-02T11:33:06.025176: step 9163, loss 0.536889.
Train: 2018-08-02T11:33:06.259521: step 9164, loss 0.511438.
Train: 2018-08-02T11:33:06.493841: step 9165, loss 0.604806.
Train: 2018-08-02T11:33:06.712514: step 9166, loss 0.604788.
Train: 2018-08-02T11:33:06.931213: step 9167, loss 0.545403.
Train: 2018-08-02T11:33:07.149937: step 9168, loss 0.570821.
Train: 2018-08-02T11:33:07.399879: step 9169, loss 0.545456.
Train: 2018-08-02T11:33:07.618582: step 9170, loss 0.604582.
Test: 2018-08-02T11:33:08.821395: step 9170, loss 0.549078.
Train: 2018-08-02T11:33:09.040111: step 9171, loss 0.58765.
Train: 2018-08-02T11:33:09.258824: step 9172, loss 0.520367.
Train: 2018-08-02T11:33:09.493144: step 9173, loss 0.554002.
Train: 2018-08-02T11:33:09.711842: step 9174, loss 0.562399.
Train: 2018-08-02T11:33:09.946158: step 9175, loss 0.537306.
Train: 2018-08-02T11:33:10.164855: step 9176, loss 0.537332.
Train: 2018-08-02T11:33:10.383529: step 9177, loss 0.570771.
Train: 2018-08-02T11:33:10.617880: step 9178, loss 0.579122.
Train: 2018-08-02T11:33:10.836579: step 9179, loss 0.537392.
Train: 2018-08-02T11:33:11.086515: step 9180, loss 0.629144.
Test: 2018-08-02T11:33:12.273712: step 9180, loss 0.547582.
Train: 2018-08-02T11:33:12.492411: step 9181, loss 0.587408.
Train: 2018-08-02T11:33:12.711109: step 9182, loss 0.52925.
Train: 2018-08-02T11:33:12.929833: step 9183, loss 0.603911.
Train: 2018-08-02T11:33:13.164155: step 9184, loss 0.570756.
Train: 2018-08-02T11:33:13.429692: step 9185, loss 0.52952.
Train: 2018-08-02T11:33:13.648390: step 9186, loss 0.554288.
Train: 2018-08-02T11:33:13.882735: step 9187, loss 0.587207.
Train: 2018-08-02T11:33:14.117057: step 9188, loss 0.546131.
Train: 2018-08-02T11:33:14.351352: step 9189, loss 0.595361.
Train: 2018-08-02T11:33:14.570080: step 9190, loss 0.562581.
Test: 2018-08-02T11:33:15.772893: step 9190, loss 0.548955.
Train: 2018-08-02T11:33:16.038482: step 9191, loss 0.53809.
Train: 2018-08-02T11:33:16.272791: step 9192, loss 0.61158.
Train: 2018-08-02T11:33:16.491500: step 9193, loss 0.521908.
Train: 2018-08-02T11:33:16.741446: step 9194, loss 0.587056.
Train: 2018-08-02T11:33:16.960144: step 9195, loss 0.538279.
Train: 2018-08-02T11:33:17.194460: step 9196, loss 0.643878.
Train: 2018-08-02T11:33:17.428781: step 9197, loss 0.514088.
Train: 2018-08-02T11:33:17.663102: step 9198, loss 0.570801.
Train: 2018-08-02T11:33:17.881773: step 9199, loss 0.619308.
Train: 2018-08-02T11:33:18.100504: step 9200, loss 0.53049.
Test: 2018-08-02T11:33:19.287695: step 9200, loss 0.548573.
Train: 2018-08-02T11:33:20.131278: step 9201, loss 0.586933.
Train: 2018-08-02T11:33:20.349971: step 9202, loss 0.538663.
Train: 2018-08-02T11:33:20.584267: step 9203, loss 0.538694.
Train: 2018-08-02T11:33:20.802989: step 9204, loss 0.594941.
Train: 2018-08-02T11:33:21.052931: step 9205, loss 0.514639.
Train: 2018-08-02T11:33:21.271606: step 9206, loss 0.578871.
Train: 2018-08-02T11:33:21.490333: step 9207, loss 0.602976.
Train: 2018-08-02T11:33:21.724624: step 9208, loss 0.57887.
Train: 2018-08-02T11:33:21.943356: step 9209, loss 0.602931.
Train: 2018-08-02T11:33:22.177669: step 9210, loss 0.586872.
Test: 2018-08-02T11:33:23.380486: step 9210, loss 0.549352.
Train: 2018-08-02T11:33:23.552322: step 9211, loss 0.528788.
Train: 2018-08-02T11:33:23.771019: step 9212, loss 0.562895.
Train: 2018-08-02T11:33:23.989745: step 9213, loss 0.538974.
Train: 2018-08-02T11:33:24.208448: step 9214, loss 0.538972.
Train: 2018-08-02T11:33:24.442768: step 9215, loss 0.546926.
Train: 2018-08-02T11:33:24.677069: step 9216, loss 0.658794.
Train: 2018-08-02T11:33:24.895757: step 9217, loss 0.538943.
Train: 2018-08-02T11:33:25.130078: step 9218, loss 0.57088.
Train: 2018-08-02T11:33:25.348814: step 9219, loss 0.57886.
Train: 2018-08-02T11:33:25.583125: step 9220, loss 0.554943.
Test: 2018-08-02T11:33:26.801559: step 9220, loss 0.549425.
Train: 2018-08-02T11:33:27.004667: step 9221, loss 0.562919.
Train: 2018-08-02T11:33:27.223361: step 9222, loss 0.531045.
Train: 2018-08-02T11:33:27.442034: step 9223, loss 0.618737.
Train: 2018-08-02T11:33:27.676355: step 9224, loss 0.554946.
Train: 2018-08-02T11:33:27.895053: step 9225, loss 0.602769.
Train: 2018-08-02T11:33:28.113753: step 9226, loss 0.507204.
Train: 2018-08-02T11:33:28.348103: step 9227, loss 0.578859.
Train: 2018-08-02T11:33:28.566802: step 9228, loss 0.515112.
Train: 2018-08-02T11:33:28.801091: step 9229, loss 0.610783.
Train: 2018-08-02T11:33:29.035413: step 9230, loss 0.62676.
Test: 2018-08-02T11:33:30.253879: step 9230, loss 0.549412.
Train: 2018-08-02T11:33:30.456979: step 9231, loss 0.562912.
Train: 2018-08-02T11:33:30.691281: step 9232, loss 0.546994.
Train: 2018-08-02T11:33:30.956861: step 9233, loss 0.51515.
Train: 2018-08-02T11:33:31.175561: step 9234, loss 0.634661.
Train: 2018-08-02T11:33:31.409891: step 9235, loss 0.618688.
Train: 2018-08-02T11:33:31.628579: step 9236, loss 0.531156.
Train: 2018-08-02T11:33:31.862875: step 9237, loss 0.594746.
Train: 2018-08-02T11:33:32.112841: step 9238, loss 0.539194.
Train: 2018-08-02T11:33:32.331546: step 9239, loss 0.547143.
Train: 2018-08-02T11:33:32.550238: step 9240, loss 0.515422.
Test: 2018-08-02T11:33:33.753057: step 9240, loss 0.549898.
Train: 2018-08-02T11:33:33.971787: step 9241, loss 0.547095.
Train: 2018-08-02T11:33:34.237348: step 9242, loss 0.602724.
Train: 2018-08-02T11:33:34.456043: step 9243, loss 0.562936.
Train: 2018-08-02T11:33:34.690369: step 9244, loss 0.586827.
Train: 2018-08-02T11:33:34.909062: step 9245, loss 0.658558.
Train: 2018-08-02T11:33:35.143390: step 9246, loss 0.554997.
Train: 2018-08-02T11:33:35.377703: step 9247, loss 0.586801.
Train: 2018-08-02T11:33:35.596374: step 9248, loss 0.618499.
Train: 2018-08-02T11:33:35.830727: step 9249, loss 0.460277.
Train: 2018-08-02T11:33:36.049421: step 9250, loss 0.570954.
Test: 2018-08-02T11:33:37.252238: step 9250, loss 0.55002.
Train: 2018-08-02T11:33:37.455316: step 9251, loss 0.586769.
Train: 2018-08-02T11:33:37.689661: step 9252, loss 0.56305.
Train: 2018-08-02T11:33:37.908366: step 9253, loss 0.555148.
Train: 2018-08-02T11:33:38.142685: step 9254, loss 0.515609.
Train: 2018-08-02T11:33:38.361354: step 9255, loss 0.570941.
Train: 2018-08-02T11:33:38.595680: step 9256, loss 0.570929.
Train: 2018-08-02T11:33:38.814398: step 9257, loss 0.578858.
Train: 2018-08-02T11:33:39.048723: step 9258, loss 0.578858.
Train: 2018-08-02T11:33:39.267391: step 9259, loss 0.491414.
Train: 2018-08-02T11:33:39.486090: step 9260, loss 0.539019.
Test: 2018-08-02T11:33:40.688932: step 9260, loss 0.548617.
Train: 2018-08-02T11:33:40.892010: step 9261, loss 0.578862.
Train: 2018-08-02T11:33:41.110708: step 9262, loss 0.538823.
Train: 2018-08-02T11:33:41.329409: step 9263, loss 0.57084.
Train: 2018-08-02T11:33:41.563729: step 9264, loss 0.554731.
Train: 2018-08-02T11:33:41.782459: step 9265, loss 0.554681.
Train: 2018-08-02T11:33:42.016781: step 9266, loss 0.611234.
Train: 2018-08-02T11:33:42.235476: step 9267, loss 0.562706.
Train: 2018-08-02T11:33:42.454144: step 9268, loss 0.562695.
Train: 2018-08-02T11:33:42.672843: step 9269, loss 0.587009.
Train: 2018-08-02T11:33:42.891567: step 9270, loss 0.51402.
Test: 2018-08-02T11:33:44.078765: step 9270, loss 0.547419.
Train: 2018-08-02T11:33:44.313115: step 9271, loss 0.570787.
Train: 2018-08-02T11:33:44.531783: step 9272, loss 0.513871.
Train: 2018-08-02T11:33:44.750507: step 9273, loss 0.587072.
Train: 2018-08-02T11:33:45.000425: step 9274, loss 0.464709.
Train: 2018-08-02T11:33:45.250364: step 9275, loss 0.595327.
Train: 2018-08-02T11:33:45.484685: step 9276, loss 0.570761.
Train: 2018-08-02T11:33:45.703384: step 9277, loss 0.50498.
Train: 2018-08-02T11:33:45.922084: step 9278, loss 0.521274.
Train: 2018-08-02T11:33:46.140781: step 9279, loss 0.554204.
Train: 2018-08-02T11:33:46.390723: step 9280, loss 0.562455.
Test: 2018-08-02T11:33:47.577944: step 9280, loss 0.548335.
Train: 2018-08-02T11:33:47.796675: step 9281, loss 0.554107.
Train: 2018-08-02T11:33:48.015343: step 9282, loss 0.579119.
Train: 2018-08-02T11:33:48.249687: step 9283, loss 0.57914.
Train: 2018-08-02T11:33:48.484009: step 9284, loss 0.570777.
Train: 2018-08-02T11:33:48.702682: step 9285, loss 0.503718.
Train: 2018-08-02T11:33:48.921411: step 9286, loss 0.54559.
Train: 2018-08-02T11:33:49.171333: step 9287, loss 0.562379.
Train: 2018-08-02T11:33:49.390019: step 9288, loss 0.537092.
Train: 2018-08-02T11:33:49.624342: step 9289, loss 0.520155.
Train: 2018-08-02T11:33:49.843040: step 9290, loss 0.570819.
Test: 2018-08-02T11:33:51.045882: step 9290, loss 0.548976.
Train: 2018-08-02T11:33:51.248959: step 9291, loss 0.621693.
Train: 2018-08-02T11:33:51.467702: step 9292, loss 0.570828.
Train: 2018-08-02T11:33:51.702006: step 9293, loss 0.587775.
Train: 2018-08-02T11:33:51.920678: step 9294, loss 0.477703.
Train: 2018-08-02T11:33:52.139376: step 9295, loss 0.587768.
Train: 2018-08-02T11:33:52.358076: step 9296, loss 0.452237.
Train: 2018-08-02T11:33:52.592398: step 9297, loss 0.545372.
Train: 2018-08-02T11:33:52.842337: step 9298, loss 0.587862.
Train: 2018-08-02T11:33:53.076663: step 9299, loss 0.536795.
Train: 2018-08-02T11:33:53.295356: step 9300, loss 0.502653.
Test: 2018-08-02T11:33:54.498199: step 9300, loss 0.549245.
Train: 2018-08-02T11:33:55.404269: step 9301, loss 0.536699.
Train: 2018-08-02T11:33:55.622966: step 9302, loss 0.528072.
Train: 2018-08-02T11:33:55.857286: step 9303, loss 0.631039.
Train: 2018-08-02T11:33:56.091578: step 9304, loss 0.588115.
Train: 2018-08-02T11:33:56.310299: step 9305, loss 0.570926.
Train: 2018-08-02T11:33:56.544596: step 9306, loss 0.527995.
Train: 2018-08-02T11:33:56.763328: step 9307, loss 0.570919.
Train: 2018-08-02T11:33:56.982019: step 9308, loss 0.579492.
Train: 2018-08-02T11:33:57.200721: step 9309, loss 0.536629.
Train: 2018-08-02T11:33:57.435011: step 9310, loss 0.596586.
Test: 2018-08-02T11:33:58.637854: step 9310, loss 0.547273.
Train: 2018-08-02T11:33:58.840957: step 9311, loss 0.579434.
Train: 2018-08-02T11:33:59.059662: step 9312, loss 0.528216.
Train: 2018-08-02T11:33:59.278355: step 9313, loss 0.553822.
Train: 2018-08-02T11:33:59.512651: step 9314, loss 0.468751.
Train: 2018-08-02T11:33:59.747000: step 9315, loss 0.596399.
Train: 2018-08-02T11:33:59.965668: step 9316, loss 0.61341.
Train: 2018-08-02T11:34:00.199989: step 9317, loss 0.587838.
Train: 2018-08-02T11:34:00.418718: step 9318, loss 0.604742.
Train: 2018-08-02T11:34:00.684250: step 9319, loss 0.528556.
Train: 2018-08-02T11:34:00.918597: step 9320, loss 0.478048.
Test: 2018-08-02T11:34:02.105792: step 9320, loss 0.548876.
Train: 2018-08-02T11:34:02.340113: step 9321, loss 0.57923.
Train: 2018-08-02T11:34:02.558842: step 9322, loss 0.553953.
Train: 2018-08-02T11:34:02.793133: step 9323, loss 0.562378.
Train: 2018-08-02T11:34:03.027484: step 9324, loss 0.537157.
Train: 2018-08-02T11:34:03.261797: step 9325, loss 0.553978.
Train: 2018-08-02T11:34:03.496117: step 9326, loss 0.553982.
Train: 2018-08-02T11:34:03.714816: step 9327, loss 0.553985.
Train: 2018-08-02T11:34:03.980354: step 9328, loss 0.469992.
Train: 2018-08-02T11:34:04.199078: step 9329, loss 0.638107.
Train: 2018-08-02T11:34:04.433376: step 9330, loss 0.6465.
Test: 2018-08-02T11:34:05.620595: step 9330, loss 0.54818.
Train: 2018-08-02T11:34:05.839324: step 9331, loss 0.570784.
Train: 2018-08-02T11:34:06.089262: step 9332, loss 0.520537.
Train: 2018-08-02T11:34:06.323586: step 9333, loss 0.520601.
Train: 2018-08-02T11:34:06.557890: step 9334, loss 0.629281.
Train: 2018-08-02T11:34:06.776575: step 9335, loss 0.58745.
Train: 2018-08-02T11:34:06.995303: step 9336, loss 0.612369.
Train: 2018-08-02T11:34:07.245214: step 9337, loss 0.603928.
Train: 2018-08-02T11:34:07.479535: step 9338, loss 0.595531.
Train: 2018-08-02T11:34:07.713885: step 9339, loss 0.521437.
Train: 2018-08-02T11:34:07.932553: step 9340, loss 0.578957.
Test: 2018-08-02T11:34:09.135396: step 9340, loss 0.548589.
Train: 2018-08-02T11:34:09.354126: step 9341, loss 0.489105.
Train: 2018-08-02T11:34:09.572793: step 9342, loss 0.644178.
Train: 2018-08-02T11:34:09.791518: step 9343, loss 0.570783.
Train: 2018-08-02T11:34:10.010217: step 9344, loss 0.489724.
Train: 2018-08-02T11:34:10.228891: step 9345, loss 0.554598.
Train: 2018-08-02T11:34:10.447590: step 9346, loss 0.619369.
Train: 2018-08-02T11:34:10.681915: step 9347, loss 0.538487.
Train: 2018-08-02T11:34:10.900634: step 9348, loss 0.586956.
Train: 2018-08-02T11:34:11.134928: step 9349, loss 0.522459.
Train: 2018-08-02T11:34:11.353653: step 9350, loss 0.586937.
Test: 2018-08-02T11:34:12.556473: step 9350, loss 0.549697.
Train: 2018-08-02T11:34:12.822033: step 9351, loss 0.514464.
Train: 2018-08-02T11:34:13.056379: step 9352, loss 0.635269.
Train: 2018-08-02T11:34:13.275052: step 9353, loss 0.578875.
Train: 2018-08-02T11:34:13.493780: step 9354, loss 0.490487.
Train: 2018-08-02T11:34:13.712474: step 9355, loss 0.554754.
Train: 2018-08-02T11:34:13.946771: step 9356, loss 0.498418.
Train: 2018-08-02T11:34:14.181090: step 9357, loss 0.554688.
Train: 2018-08-02T11:34:14.399834: step 9358, loss 0.651637.
Train: 2018-08-02T11:34:14.634109: step 9359, loss 0.619305.
Train: 2018-08-02T11:34:14.868456: step 9360, loss 0.562739.
Test: 2018-08-02T11:34:16.055651: step 9360, loss 0.548358.
Train: 2018-08-02T11:34:16.289971: step 9361, loss 0.554689.
Train: 2018-08-02T11:34:16.477458: step 9362, loss 0.648713.
Train: 2018-08-02T11:34:16.696156: step 9363, loss 0.635126.
Train: 2018-08-02T11:34:16.914826: step 9364, loss 0.530842.
Train: 2018-08-02T11:34:17.133527: step 9365, loss 0.594823.
Train: 2018-08-02T11:34:17.367874: step 9366, loss 0.483391.
Train: 2018-08-02T11:34:17.586568: step 9367, loss 0.586808.
Train: 2018-08-02T11:34:17.805266: step 9368, loss 0.618558.
Train: 2018-08-02T11:34:18.023971: step 9369, loss 0.555094.
Train: 2018-08-02T11:34:18.258263: step 9370, loss 0.499779.
Test: 2018-08-02T11:34:19.461104: step 9370, loss 0.550742.
Train: 2018-08-02T11:34:19.679801: step 9371, loss 0.531403.
Train: 2018-08-02T11:34:19.914122: step 9372, loss 0.53135.
Train: 2018-08-02T11:34:20.132821: step 9373, loss 0.610593.
Train: 2018-08-02T11:34:20.351545: step 9374, loss 0.555041.
Train: 2018-08-02T11:34:20.585871: step 9375, loss 0.578858.
Train: 2018-08-02T11:34:20.804568: step 9376, loss 0.547051.
Train: 2018-08-02T11:34:21.023265: step 9377, loss 0.578859.
Train: 2018-08-02T11:34:21.241936: step 9378, loss 0.594791.
Train: 2018-08-02T11:34:21.476256: step 9379, loss 0.554961.
Train: 2018-08-02T11:34:21.694955: step 9380, loss 0.562923.
Test: 2018-08-02T11:34:22.897799: step 9380, loss 0.549971.
Train: 2018-08-02T11:34:23.116522: step 9381, loss 0.570888.
Train: 2018-08-02T11:34:23.335197: step 9382, loss 0.57886.
Train: 2018-08-02T11:34:23.569521: step 9383, loss 0.515073.
Train: 2018-08-02T11:34:23.803836: step 9384, loss 0.55491.
Train: 2018-08-02T11:34:24.022572: step 9385, loss 0.522897.
Train: 2018-08-02T11:34:24.241237: step 9386, loss 0.602908.
Train: 2018-08-02T11:34:24.459933: step 9387, loss 0.546772.
Train: 2018-08-02T11:34:24.694283: step 9388, loss 0.562798.
Train: 2018-08-02T11:34:24.928599: step 9389, loss 0.498386.
Train: 2018-08-02T11:34:25.147296: step 9390, loss 0.562739.
Test: 2018-08-02T11:34:26.365737: step 9390, loss 0.549558.
Train: 2018-08-02T11:34:26.584465: step 9391, loss 0.554608.
Train: 2018-08-02T11:34:26.803160: step 9392, loss 0.64384.
Train: 2018-08-02T11:34:27.068710: step 9393, loss 0.554544.
Train: 2018-08-02T11:34:27.303041: step 9394, loss 0.57891.
Train: 2018-08-02T11:34:27.537338: step 9395, loss 0.603298.
Train: 2018-08-02T11:34:27.771674: step 9396, loss 0.58703.
Train: 2018-08-02T11:34:28.006008: step 9397, loss 0.465329.
Train: 2018-08-02T11:34:28.240323: step 9398, loss 0.554541.
Train: 2018-08-02T11:34:28.458997: step 9399, loss 0.538248.
Train: 2018-08-02T11:34:28.708969: step 9400, loss 0.505586.
Test: 2018-08-02T11:34:29.911781: step 9400, loss 0.549694.
Train: 2018-08-02T11:34:30.802229: step 9401, loss 0.611666.
Train: 2018-08-02T11:34:31.020913: step 9402, loss 0.57892.
Train: 2018-08-02T11:34:31.239620: step 9403, loss 0.521572.
Train: 2018-08-02T11:34:31.489536: step 9404, loss 0.562488.
Train: 2018-08-02T11:34:31.723883: step 9405, loss 0.53786.
Train: 2018-08-02T11:34:31.942583: step 9406, loss 0.554262.
Train: 2018-08-02T11:34:32.176906: step 9407, loss 0.554336.
Train: 2018-08-02T11:34:32.395605: step 9408, loss 0.546154.
Train: 2018-08-02T11:34:32.676758: step 9409, loss 0.62892.
Train: 2018-08-02T11:34:32.895462: step 9410, loss 0.512701.
Test: 2018-08-02T11:34:34.098300: step 9410, loss 0.5484.
Train: 2018-08-02T11:34:34.332646: step 9411, loss 0.595724.
Train: 2018-08-02T11:34:34.551344: step 9412, loss 0.587342.
Train: 2018-08-02T11:34:34.785644: step 9413, loss 0.537621.
Train: 2018-08-02T11:34:35.004364: step 9414, loss 0.53762.
Train: 2018-08-02T11:34:35.238660: step 9415, loss 0.620487.
Train: 2018-08-02T11:34:35.457358: step 9416, loss 0.579037.
Train: 2018-08-02T11:34:35.676057: step 9417, loss 0.554217.
Train: 2018-08-02T11:34:35.910410: step 9418, loss 0.512932.
Train: 2018-08-02T11:34:36.129106: step 9419, loss 0.579018.
Train: 2018-08-02T11:34:36.347805: step 9420, loss 0.579015.
Test: 2018-08-02T11:34:37.550617: step 9420, loss 0.549293.
Train: 2018-08-02T11:34:37.753695: step 9421, loss 0.546.
Train: 2018-08-02T11:34:37.988049: step 9422, loss 0.595503.
Train: 2018-08-02T11:34:38.222359: step 9423, loss 0.52132.
Train: 2018-08-02T11:34:38.441064: step 9424, loss 0.570757.
Train: 2018-08-02T11:34:38.675380: step 9425, loss 0.554287.
Train: 2018-08-02T11:34:38.909676: step 9426, loss 0.636626.
Train: 2018-08-02T11:34:39.128404: step 9427, loss 0.521453.
Train: 2018-08-02T11:34:39.362724: step 9428, loss 0.505073.
Train: 2018-08-02T11:34:39.581422: step 9429, loss 0.480391.
Train: 2018-08-02T11:34:39.815743: step 9430, loss 0.52135.
Test: 2018-08-02T11:34:41.018555: step 9430, loss 0.549654.
Train: 2018-08-02T11:34:41.221663: step 9431, loss 0.603794.
Train: 2018-08-02T11:34:41.455954: step 9432, loss 0.55421.
Train: 2018-08-02T11:34:41.690274: step 9433, loss 0.529317.
Train: 2018-08-02T11:34:41.908973: step 9434, loss 0.612297.
Train: 2018-08-02T11:34:42.158914: step 9435, loss 0.529203.
Train: 2018-08-02T11:34:42.377612: step 9436, loss 0.554124.
Train: 2018-08-02T11:34:42.611958: step 9437, loss 0.570797.
Train: 2018-08-02T11:34:42.846277: step 9438, loss 0.554071.
Train: 2018-08-02T11:34:43.064976: step 9439, loss 0.59582.
Train: 2018-08-02T11:34:43.283681: step 9440, loss 0.512353.
Test: 2018-08-02T11:34:44.486492: step 9440, loss 0.548852.
Train: 2018-08-02T11:34:44.720815: step 9441, loss 0.554064.
Train: 2018-08-02T11:34:44.939543: step 9442, loss 0.570759.
Train: 2018-08-02T11:34:45.173858: step 9443, loss 0.537311.
Train: 2018-08-02T11:34:45.408184: step 9444, loss 0.570783.
Train: 2018-08-02T11:34:45.626876: step 9445, loss 0.503781.
Train: 2018-08-02T11:34:45.876818: step 9446, loss 0.646261.
Train: 2018-08-02T11:34:46.095516: step 9447, loss 0.53723.
Train: 2018-08-02T11:34:46.329812: step 9448, loss 0.554026.
Train: 2018-08-02T11:34:46.548535: step 9449, loss 0.537266.
Train: 2018-08-02T11:34:46.767233: step 9450, loss 0.562429.
Test: 2018-08-02T11:34:47.985673: step 9450, loss 0.548773.
Train: 2018-08-02T11:34:48.204372: step 9451, loss 0.604382.
Train: 2018-08-02T11:34:48.423096: step 9452, loss 0.554032.
Train: 2018-08-02T11:34:48.641770: step 9453, loss 0.58751.
Train: 2018-08-02T11:34:48.876121: step 9454, loss 0.59584.
Train: 2018-08-02T11:34:49.094813: step 9455, loss 0.620797.
Train: 2018-08-02T11:34:49.344760: step 9456, loss 0.628935.
Train: 2018-08-02T11:34:49.563459: step 9457, loss 0.51285.
Train: 2018-08-02T11:34:49.782129: step 9458, loss 0.570757.
Train: 2018-08-02T11:34:50.000860: step 9459, loss 0.496768.
Train: 2018-08-02T11:34:50.219550: step 9460, loss 0.537917.
Test: 2018-08-02T11:34:51.437990: step 9460, loss 0.548663.
Train: 2018-08-02T11:34:51.641095: step 9461, loss 0.562555.
Train: 2018-08-02T11:34:51.859766: step 9462, loss 0.578963.
Train: 2018-08-02T11:34:52.078491: step 9463, loss 0.570763.
Train: 2018-08-02T11:34:52.312817: step 9464, loss 0.644423.
Train: 2018-08-02T11:34:52.531509: step 9465, loss 0.53813.
Train: 2018-08-02T11:34:52.765836: step 9466, loss 0.530058.
Train: 2018-08-02T11:34:53.000125: step 9467, loss 0.611457.
Train: 2018-08-02T11:34:53.218861: step 9468, loss 0.578906.
Train: 2018-08-02T11:34:53.437553: step 9469, loss 0.562697.
Train: 2018-08-02T11:34:53.671867: step 9470, loss 0.530388.
Test: 2018-08-02T11:34:54.874685: step 9470, loss 0.549062.
Train: 2018-08-02T11:34:55.077789: step 9471, loss 0.595037.
Train: 2018-08-02T11:34:55.296461: step 9472, loss 0.554693.
Train: 2018-08-02T11:34:55.530813: step 9473, loss 0.594983.
Train: 2018-08-02T11:34:55.765136: step 9474, loss 0.546717.
Train: 2018-08-02T11:34:55.983828: step 9475, loss 0.538723.
Train: 2018-08-02T11:34:56.202499: step 9476, loss 0.49057.
Train: 2018-08-02T11:34:56.421228: step 9477, loss 0.57085.
Train: 2018-08-02T11:34:56.639929: step 9478, loss 0.562788.
Train: 2018-08-02T11:34:56.874218: step 9479, loss 0.546621.
Train: 2018-08-02T11:34:57.124158: step 9480, loss 0.578912.
Test: 2018-08-02T11:34:58.327001: step 9480, loss 0.548485.
Train: 2018-08-02T11:34:58.530110: step 9481, loss 0.554645.
Train: 2018-08-02T11:34:58.748811: step 9482, loss 0.562715.
Train: 2018-08-02T11:34:58.967477: step 9483, loss 0.603192.
Train: 2018-08-02T11:34:59.186200: step 9484, loss 0.562683.
Train: 2018-08-02T11:34:59.420526: step 9485, loss 0.538426.
Train: 2018-08-02T11:34:59.654840: step 9486, loss 0.587017.
Train: 2018-08-02T11:34:59.873540: step 9487, loss 0.578901.
Train: 2018-08-02T11:35:00.107861: step 9488, loss 0.595102.
Train: 2018-08-02T11:35:00.342177: step 9489, loss 0.530341.
Train: 2018-08-02T11:35:00.560883: step 9490, loss 0.619349.
Test: 2018-08-02T11:35:01.763697: step 9490, loss 0.548862.
Train: 2018-08-02T11:35:01.982396: step 9491, loss 0.603127.
Train: 2018-08-02T11:35:02.201095: step 9492, loss 0.546632.
Train: 2018-08-02T11:35:02.451037: step 9493, loss 0.586926.
Train: 2018-08-02T11:35:02.669760: step 9494, loss 0.474422.
Train: 2018-08-02T11:35:02.919678: step 9495, loss 0.53064.
Train: 2018-08-02T11:35:03.141885: step 9496, loss 0.635221.
Train: 2018-08-02T11:35:03.360585: step 9497, loss 0.554738.
Train: 2018-08-02T11:35:03.610556: step 9498, loss 0.546698.
Train: 2018-08-02T11:35:03.829255: step 9499, loss 0.522551.
Train: 2018-08-02T11:35:04.063549: step 9500, loss 0.586935.
Test: 2018-08-02T11:35:05.266388: step 9500, loss 0.549477.
Train: 2018-08-02T11:35:06.203694: step 9501, loss 0.619191.
Train: 2018-08-02T11:35:06.422376: step 9502, loss 0.538596.
Train: 2018-08-02T11:35:06.641066: step 9503, loss 0.578878.
Train: 2018-08-02T11:35:06.891007: step 9504, loss 0.651363.
Train: 2018-08-02T11:35:07.109737: step 9505, loss 0.602976.
Train: 2018-08-02T11:35:07.328435: step 9506, loss 0.586876.
Train: 2018-08-02T11:35:07.547128: step 9507, loss 0.578861.
Train: 2018-08-02T11:35:07.781454: step 9508, loss 0.491308.
Train: 2018-08-02T11:35:08.015744: step 9509, loss 0.539102.
Train: 2018-08-02T11:35:08.250064: step 9510, loss 0.515257.
Test: 2018-08-02T11:35:09.437285: step 9510, loss 0.549096.
Train: 2018-08-02T11:35:09.640407: step 9511, loss 0.570896.
Train: 2018-08-02T11:35:09.859088: step 9512, loss 0.539027.
Train: 2018-08-02T11:35:10.030896: step 9513, loss 0.426684.
Train: 2018-08-02T11:35:10.265242: step 9514, loss 0.562815.
Train: 2018-08-02T11:35:10.499550: step 9515, loss 0.586933.
Train: 2018-08-02T11:35:10.718266: step 9516, loss 0.554629.
Train: 2018-08-02T11:35:10.936966: step 9517, loss 0.595067.
Train: 2018-08-02T11:35:11.155666: step 9518, loss 0.554396.
Train: 2018-08-02T11:35:11.405605: step 9519, loss 0.546355.
Train: 2018-08-02T11:35:11.624304: step 9520, loss 0.612284.
Test: 2018-08-02T11:35:12.827117: step 9520, loss 0.54887.
Train: 2018-08-02T11:35:13.030195: step 9521, loss 0.521599.
Train: 2018-08-02T11:35:13.248924: step 9522, loss 0.587149.
Train: 2018-08-02T11:35:13.483222: step 9523, loss 0.579025.
Train: 2018-08-02T11:35:13.717558: step 9524, loss 0.537953.
Train: 2018-08-02T11:35:13.936231: step 9525, loss 0.488689.
Train: 2018-08-02T11:35:14.170583: step 9526, loss 0.496705.
Train: 2018-08-02T11:35:14.404898: step 9527, loss 0.570757.
Train: 2018-08-02T11:35:14.639193: step 9528, loss 0.620462.
Train: 2018-08-02T11:35:14.857891: step 9529, loss 0.496096.
Train: 2018-08-02T11:35:15.076621: step 9530, loss 0.512535.
Test: 2018-08-02T11:35:16.279433: step 9530, loss 0.548674.
Train: 2018-08-02T11:35:16.545021: step 9531, loss 0.612502.
Train: 2018-08-02T11:35:16.794968: step 9532, loss 0.579133.
Train: 2018-08-02T11:35:17.013666: step 9533, loss 0.554032.
Train: 2018-08-02T11:35:17.263604: step 9534, loss 0.554017.
Train: 2018-08-02T11:35:17.482307: step 9535, loss 0.503671.
Train: 2018-08-02T11:35:17.701005: step 9536, loss 0.587602.
Train: 2018-08-02T11:35:17.919705: step 9537, loss 0.545537.
Train: 2018-08-02T11:35:18.138398: step 9538, loss 0.562371.
Train: 2018-08-02T11:35:18.357105: step 9539, loss 0.528628.
Train: 2018-08-02T11:35:18.575795: step 9540, loss 0.57081.
Test: 2018-08-02T11:35:19.778614: step 9540, loss 0.548427.
Train: 2018-08-02T11:35:19.997314: step 9541, loss 0.528552.
Train: 2018-08-02T11:35:20.231634: step 9542, loss 0.697883.
Train: 2018-08-02T11:35:20.450332: step 9543, loss 0.579256.
Train: 2018-08-02T11:35:20.684652: step 9544, loss 0.596091.
Train: 2018-08-02T11:35:20.918997: step 9545, loss 0.596005.
Train: 2018-08-02T11:35:21.168940: step 9546, loss 0.487023.
Train: 2018-08-02T11:35:21.403236: step 9547, loss 0.520603.
Train: 2018-08-02T11:35:21.621932: step 9548, loss 0.620905.
Train: 2018-08-02T11:35:21.856253: step 9549, loss 0.520735.
Train: 2018-08-02T11:35:22.074982: step 9550, loss 0.637407.
Test: 2018-08-02T11:35:23.277794: step 9550, loss 0.548769.
Train: 2018-08-02T11:35:23.512141: step 9551, loss 0.554142.
Train: 2018-08-02T11:35:23.730845: step 9552, loss 0.512745.
Train: 2018-08-02T11:35:23.949542: step 9553, loss 0.579039.
Train: 2018-08-02T11:35:24.168238: step 9554, loss 0.56249.
Train: 2018-08-02T11:35:24.386910: step 9555, loss 0.512967.
Train: 2018-08-02T11:35:24.621255: step 9556, loss 0.562501.
Train: 2018-08-02T11:35:24.839966: step 9557, loss 0.537744.
Train: 2018-08-02T11:35:25.058628: step 9558, loss 0.545993.
Train: 2018-08-02T11:35:25.292973: step 9559, loss 0.620316.
Train: 2018-08-02T11:35:25.527268: step 9560, loss 0.562506.
Test: 2018-08-02T11:35:26.745732: step 9560, loss 0.548738.
Train: 2018-08-02T11:35:26.964431: step 9561, loss 0.521279.
Train: 2018-08-02T11:35:27.183163: step 9562, loss 0.504774.
Train: 2018-08-02T11:35:27.401864: step 9563, loss 0.595555.
Train: 2018-08-02T11:35:27.620558: step 9564, loss 0.545964.
Train: 2018-08-02T11:35:27.854848: step 9565, loss 0.620375.
Train: 2018-08-02T11:35:28.073578: step 9566, loss 0.537712.
Train: 2018-08-02T11:35:28.307867: step 9567, loss 0.537715.
Train: 2018-08-02T11:35:28.526592: step 9568, loss 0.52118.
Train: 2018-08-02T11:35:28.745290: step 9569, loss 0.636933.
Train: 2018-08-02T11:35:28.979590: step 9570, loss 0.579024.
Test: 2018-08-02T11:35:30.182427: step 9570, loss 0.547948.
Train: 2018-08-02T11:35:30.385506: step 9571, loss 0.595531.
Train: 2018-08-02T11:35:30.604203: step 9572, loss 0.603725.
Train: 2018-08-02T11:35:30.822932: step 9573, loss 0.529655.
Train: 2018-08-02T11:35:31.088496: step 9574, loss 0.537933.
Train: 2018-08-02T11:35:31.307188: step 9575, loss 0.464173.
Train: 2018-08-02T11:35:31.525893: step 9576, loss 0.480444.
Train: 2018-08-02T11:35:31.744563: step 9577, loss 0.562521.
Train: 2018-08-02T11:35:31.963290: step 9578, loss 0.570756.
Train: 2018-08-02T11:35:32.197582: step 9579, loss 0.579033.
Train: 2018-08-02T11:35:32.416280: step 9580, loss 0.587336.
Test: 2018-08-02T11:35:33.619123: step 9580, loss 0.548036.
Train: 2018-08-02T11:35:33.853444: step 9581, loss 0.628824.
Train: 2018-08-02T11:35:34.072142: step 9582, loss 0.56247.
Train: 2018-08-02T11:35:34.306464: step 9583, loss 0.603874.
Train: 2018-08-02T11:35:34.540783: step 9584, loss 0.545962.
Train: 2018-08-02T11:35:34.759506: step 9585, loss 0.488229.
Train: 2018-08-02T11:35:34.978206: step 9586, loss 0.471678.
Train: 2018-08-02T11:35:35.196910: step 9587, loss 0.686631.
Train: 2018-08-02T11:35:35.431200: step 9588, loss 0.554221.
Train: 2018-08-02T11:35:35.649928: step 9589, loss 0.496368.
Train: 2018-08-02T11:35:35.868621: step 9590, loss 0.545938.
Test: 2018-08-02T11:35:37.071439: step 9590, loss 0.547881.
Train: 2018-08-02T11:35:37.290139: step 9591, loss 0.637013.
Train: 2018-08-02T11:35:37.524458: step 9592, loss 0.529378.
Train: 2018-08-02T11:35:37.743194: step 9593, loss 0.562481.
Train: 2018-08-02T11:35:37.977493: step 9594, loss 0.603857.
Train: 2018-08-02T11:35:38.196207: step 9595, loss 0.587288.
Train: 2018-08-02T11:35:38.430496: step 9596, loss 0.545998.
Train: 2018-08-02T11:35:38.664843: step 9597, loss 0.529539.
Train: 2018-08-02T11:35:38.914783: step 9598, loss 0.595479.
Train: 2018-08-02T11:35:39.133487: step 9599, loss 0.562526.
Train: 2018-08-02T11:35:39.352155: step 9600, loss 0.496748.
Test: 2018-08-02T11:35:40.554998: step 9600, loss 0.548789.
Train: 2018-08-02T11:35:41.414203: step 9601, loss 0.595441.
Train: 2018-08-02T11:35:41.632901: step 9602, loss 0.554309.
Train: 2018-08-02T11:35:41.851594: step 9603, loss 0.55431.
Train: 2018-08-02T11:35:42.085892: step 9604, loss 0.587206.
Train: 2018-08-02T11:35:42.304613: step 9605, loss 0.603634.
Train: 2018-08-02T11:35:42.523317: step 9606, loss 0.529728.
Train: 2018-08-02T11:35:42.742017: step 9607, loss 0.537956.
Train: 2018-08-02T11:35:42.960715: step 9608, loss 0.529759.
Train: 2018-08-02T11:35:43.210658: step 9609, loss 0.611794.
Train: 2018-08-02T11:35:43.429356: step 9610, loss 0.52975.
Test: 2018-08-02T11:35:44.647791: step 9610, loss 0.548858.
Train: 2018-08-02T11:35:44.850868: step 9611, loss 0.58717.
Train: 2018-08-02T11:35:45.069598: step 9612, loss 0.496961.
Train: 2018-08-02T11:35:45.288290: step 9613, loss 0.578969.
Train: 2018-08-02T11:35:45.538206: step 9614, loss 0.55433.
Train: 2018-08-02T11:35:45.772527: step 9615, loss 0.521435.
Train: 2018-08-02T11:35:45.991226: step 9616, loss 0.480192.
Train: 2018-08-02T11:35:46.209924: step 9617, loss 0.686395.
Train: 2018-08-02T11:35:46.444245: step 9618, loss 0.529458.
Train: 2018-08-02T11:35:46.678566: step 9619, loss 0.537696.
Train: 2018-08-02T11:35:46.897294: step 9620, loss 0.579029.
Test: 2018-08-02T11:35:48.100107: step 9620, loss 0.548652.
Train: 2018-08-02T11:35:48.318836: step 9621, loss 0.612147.
Train: 2018-08-02T11:35:48.553135: step 9622, loss 0.620396.
Train: 2018-08-02T11:35:48.771825: step 9623, loss 0.554243.
Train: 2018-08-02T11:35:48.990549: step 9624, loss 0.537789.
Train: 2018-08-02T11:35:49.209222: step 9625, loss 0.554289.
Train: 2018-08-02T11:35:49.443542: step 9626, loss 0.578986.
Train: 2018-08-02T11:35:49.662241: step 9627, loss 0.521452.
Train: 2018-08-02T11:35:49.880939: step 9628, loss 0.59541.
Train: 2018-08-02T11:35:50.099663: step 9629, loss 0.554342.
Train: 2018-08-02T11:35:50.318337: step 9630, loss 0.603576.
Test: 2018-08-02T11:35:51.521180: step 9630, loss 0.550211.
Train: 2018-08-02T11:35:51.724260: step 9631, loss 0.554379.
Train: 2018-08-02T11:35:51.958580: step 9632, loss 0.587128.
Train: 2018-08-02T11:35:52.177307: step 9633, loss 0.538097.
Train: 2018-08-02T11:35:52.411623: step 9634, loss 0.603411.
Train: 2018-08-02T11:35:52.630297: step 9635, loss 0.603359.
Train: 2018-08-02T11:35:52.864647: step 9636, loss 0.505803.
Train: 2018-08-02T11:35:53.098973: step 9637, loss 0.611365.
Train: 2018-08-02T11:35:53.317634: step 9638, loss 0.554599.
Train: 2018-08-02T11:35:53.567577: step 9639, loss 0.578896.
Train: 2018-08-02T11:35:53.786275: step 9640, loss 0.530457.
Test: 2018-08-02T11:35:54.989118: step 9640, loss 0.549653.
Train: 2018-08-02T11:35:55.207849: step 9641, loss 0.57888.
Train: 2018-08-02T11:35:55.442164: step 9642, loss 0.586935.
Train: 2018-08-02T11:35:55.660866: step 9643, loss 0.530602.
Train: 2018-08-02T11:35:55.895188: step 9644, loss 0.56279.
Train: 2018-08-02T11:35:56.129508: step 9645, loss 0.554755.
Train: 2018-08-02T11:35:56.363825: step 9646, loss 0.586911.
Train: 2018-08-02T11:35:56.598116: step 9647, loss 0.52263.
Train: 2018-08-02T11:35:56.816845: step 9648, loss 0.570834.
Train: 2018-08-02T11:35:57.035539: step 9649, loss 0.578874.
Train: 2018-08-02T11:35:57.269866: step 9650, loss 0.619084.
Test: 2018-08-02T11:35:58.488299: step 9650, loss 0.549022.
Train: 2018-08-02T11:35:58.722620: step 9651, loss 0.578871.
Train: 2018-08-02T11:35:58.941350: step 9652, loss 0.522724.
Train: 2018-08-02T11:35:59.160047: step 9653, loss 0.586886.
Train: 2018-08-02T11:35:59.378741: step 9654, loss 0.610917.
Train: 2018-08-02T11:35:59.597445: step 9655, loss 0.51487.
Train: 2018-08-02T11:35:59.816114: step 9656, loss 0.530881.
Train: 2018-08-02T11:36:00.050434: step 9657, loss 0.490843.
Train: 2018-08-02T11:36:00.269132: step 9658, loss 0.59491.
Train: 2018-08-02T11:36:00.487830: step 9659, loss 0.538706.
Train: 2018-08-02T11:36:00.706530: step 9660, loss 0.578876.
Test: 2018-08-02T11:36:01.909373: step 9660, loss 0.549297.
Train: 2018-08-02T11:36:02.143724: step 9661, loss 0.514395.
Train: 2018-08-02T11:36:02.362392: step 9662, loss 0.578888.
Train: 2018-08-02T11:36:02.581091: step 9663, loss 0.586992.
Train: 2018-08-02T11:36:02.768547: step 9664, loss 0.614567.
Train: 2018-08-02T11:36:03.002894: step 9665, loss 0.554584.
Train: 2018-08-02T11:36:03.237188: step 9666, loss 0.538369.
Train: 2018-08-02T11:36:03.455916: step 9667, loss 0.570791.
Train: 2018-08-02T11:36:03.690206: step 9668, loss 0.554558.
Train: 2018-08-02T11:36:03.924556: step 9669, loss 0.489577.
Train: 2018-08-02T11:36:04.158848: step 9670, loss 0.54636.
Test: 2018-08-02T11:36:05.346068: step 9670, loss 0.549736.
Train: 2018-08-02T11:36:05.564799: step 9671, loss 0.489172.
Train: 2018-08-02T11:36:05.783496: step 9672, loss 0.570765.
Train: 2018-08-02T11:36:06.002189: step 9673, loss 0.570757.
Train: 2018-08-02T11:36:06.236510: step 9674, loss 0.554273.
Train: 2018-08-02T11:36:06.455184: step 9675, loss 0.545967.
Train: 2018-08-02T11:36:06.673882: step 9676, loss 0.579048.
Train: 2018-08-02T11:36:06.892580: step 9677, loss 0.545858.
Train: 2018-08-02T11:36:07.126927: step 9678, loss 0.562441.
Train: 2018-08-02T11:36:07.361253: step 9679, loss 0.495774.
Train: 2018-08-02T11:36:07.579953: step 9680, loss 0.587485.
Test: 2018-08-02T11:36:08.782763: step 9680, loss 0.549384.
Train: 2018-08-02T11:36:08.985873: step 9681, loss 0.545661.
Train: 2018-08-02T11:36:09.204541: step 9682, loss 0.520437.
Train: 2018-08-02T11:36:09.438891: step 9683, loss 0.537145.
Train: 2018-08-02T11:36:09.673181: step 9684, loss 0.51178.
Train: 2018-08-02T11:36:09.891903: step 9685, loss 0.553868.
Train: 2018-08-02T11:36:10.126229: step 9686, loss 0.579332.
Train: 2018-08-02T11:36:10.344899: step 9687, loss 0.613417.
Train: 2018-08-02T11:36:10.563627: step 9688, loss 0.56232.
Train: 2018-08-02T11:36:10.797941: step 9689, loss 0.579344.
Train: 2018-08-02T11:36:11.016646: step 9690, loss 0.562404.
Test: 2018-08-02T11:36:12.235079: step 9690, loss 0.547741.
Train: 2018-08-02T11:36:12.438158: step 9691, loss 0.545316.
Train: 2018-08-02T11:36:12.672516: step 9692, loss 0.562358.
Train: 2018-08-02T11:36:12.906799: step 9693, loss 0.536837.
Train: 2018-08-02T11:36:13.141119: step 9694, loss 0.536849.
Train: 2018-08-02T11:36:13.359848: step 9695, loss 0.553838.
Train: 2018-08-02T11:36:13.578541: step 9696, loss 0.613367.
Train: 2018-08-02T11:36:13.812867: step 9697, loss 0.579333.
Train: 2018-08-02T11:36:14.062778: step 9698, loss 0.570829.
Train: 2018-08-02T11:36:14.297097: step 9699, loss 0.57082.
Train: 2018-08-02T11:36:14.531448: step 9700, loss 0.528578.
Test: 2018-08-02T11:36:15.718639: step 9700, loss 0.548086.
Train: 2018-08-02T11:36:16.640329: step 9701, loss 0.562368.
Train: 2018-08-02T11:36:16.874645: step 9702, loss 0.612923.
Train: 2018-08-02T11:36:17.108938: step 9703, loss 0.528767.
Train: 2018-08-02T11:36:17.343313: step 9704, loss 0.537219.
Train: 2018-08-02T11:36:17.561988: step 9705, loss 0.579161.
Train: 2018-08-02T11:36:17.780687: step 9706, loss 0.579145.
Train: 2018-08-02T11:36:17.999385: step 9707, loss 0.57077.
Train: 2018-08-02T11:36:18.218054: step 9708, loss 0.554089.
Train: 2018-08-02T11:36:18.452377: step 9709, loss 0.545789.
Train: 2018-08-02T11:36:18.686696: step 9710, loss 0.554133.
Test: 2018-08-02T11:36:19.873916: step 9710, loss 0.548775.
Train: 2018-08-02T11:36:20.092614: step 9711, loss 0.512622.
Train: 2018-08-02T11:36:20.311314: step 9712, loss 0.579066.
Train: 2018-08-02T11:36:20.545665: step 9713, loss 0.537547.
Train: 2018-08-02T11:36:20.764333: step 9714, loss 0.545847.
Train: 2018-08-02T11:36:20.998678: step 9715, loss 0.520915.
Train: 2018-08-02T11:36:21.232973: step 9716, loss 0.587396.
Train: 2018-08-02T11:36:21.451703: step 9717, loss 0.570762.
Train: 2018-08-02T11:36:21.701644: step 9718, loss 0.529153.
Train: 2018-08-02T11:36:21.935958: step 9719, loss 0.529124.
Train: 2018-08-02T11:36:22.154662: step 9720, loss 0.54575.
Test: 2018-08-02T11:36:23.357476: step 9720, loss 0.548088.
Train: 2018-08-02T11:36:23.591822: step 9721, loss 0.554068.
Train: 2018-08-02T11:36:23.810531: step 9722, loss 0.512239.
Train: 2018-08-02T11:36:24.060461: step 9723, loss 0.537257.
Train: 2018-08-02T11:36:24.294787: step 9724, loss 0.61279.
Train: 2018-08-02T11:36:24.513479: step 9725, loss 0.612831.
Train: 2018-08-02T11:36:24.747776: step 9726, loss 0.579193.
Train: 2018-08-02T11:36:24.966505: step 9727, loss 0.537201.
Train: 2018-08-02T11:36:25.200795: step 9728, loss 0.59596.
Train: 2018-08-02T11:36:25.419494: step 9729, loss 0.554016.
Train: 2018-08-02T11:36:25.638221: step 9730, loss 0.520544.
Test: 2018-08-02T11:36:26.856656: step 9730, loss 0.548811.
Train: 2018-08-02T11:36:27.075354: step 9731, loss 0.495444.
Train: 2018-08-02T11:36:27.340943: step 9732, loss 0.554019.
Train: 2018-08-02T11:36:27.559647: step 9733, loss 0.579171.
Train: 2018-08-02T11:36:27.778342: step 9734, loss 0.671502.
Train: 2018-08-02T11:36:28.012660: step 9735, loss 0.595902.
Train: 2018-08-02T11:36:28.246958: step 9736, loss 0.579121.
Train: 2018-08-02T11:36:28.465654: step 9737, loss 0.520807.
Train: 2018-08-02T11:36:28.684353: step 9738, loss 0.58738.
Train: 2018-08-02T11:36:28.903078: step 9739, loss 0.562467.
Train: 2018-08-02T11:36:29.121783: step 9740, loss 0.579028.
Test: 2018-08-02T11:36:30.324594: step 9740, loss 0.548726.
Train: 2018-08-02T11:36:30.543293: step 9741, loss 0.579007.
Train: 2018-08-02T11:36:30.777639: step 9742, loss 0.513157.
Train: 2018-08-02T11:36:30.996342: step 9743, loss 0.505016.
Train: 2018-08-02T11:36:31.230634: step 9744, loss 0.546103.
Train: 2018-08-02T11:36:31.464983: step 9745, loss 0.554314.
Train: 2018-08-02T11:36:31.683652: step 9746, loss 0.51317.
Train: 2018-08-02T11:36:31.933592: step 9747, loss 0.554277.
Train: 2018-08-02T11:36:32.152290: step 9748, loss 0.587262.
Train: 2018-08-02T11:36:32.370990: step 9749, loss 0.537719.
Train: 2018-08-02T11:36:32.589719: step 9750, loss 0.554218.
Test: 2018-08-02T11:36:33.792532: step 9750, loss 0.548079.
Train: 2018-08-02T11:36:34.011230: step 9751, loss 0.595594.
Train: 2018-08-02T11:36:34.229955: step 9752, loss 0.579038.
Train: 2018-08-02T11:36:34.448657: step 9753, loss 0.496247.
Train: 2018-08-02T11:36:34.667357: step 9754, loss 0.579046.
Train: 2018-08-02T11:36:34.901674: step 9755, loss 0.512698.
Train: 2018-08-02T11:36:35.135999: step 9756, loss 0.587375.
Train: 2018-08-02T11:36:35.370319: step 9757, loss 0.545818.
Train: 2018-08-02T11:36:35.604608: step 9758, loss 0.56244.
Train: 2018-08-02T11:36:35.823306: step 9759, loss 0.562435.
Train: 2018-08-02T11:36:36.042005: step 9760, loss 0.43742.
Test: 2018-08-02T11:36:37.260470: step 9760, loss 0.549024.
Train: 2018-08-02T11:36:37.479169: step 9761, loss 0.48716.
Train: 2018-08-02T11:36:37.713515: step 9762, loss 0.679968.
Train: 2018-08-02T11:36:37.947839: step 9763, loss 0.495109.
Train: 2018-08-02T11:36:38.166537: step 9764, loss 0.57923.
Train: 2018-08-02T11:36:38.400828: step 9765, loss 0.6468.
Train: 2018-08-02T11:36:38.619556: step 9766, loss 0.520172.
Train: 2018-08-02T11:36:38.838256: step 9767, loss 0.579248.
Train: 2018-08-02T11:36:39.072577: step 9768, loss 0.579244.
Train: 2018-08-02T11:36:39.291246: step 9769, loss 0.56237.
Train: 2018-08-02T11:36:39.541211: step 9770, loss 0.562374.
Test: 2018-08-02T11:36:40.712786: step 9770, loss 0.548517.
Train: 2018-08-02T11:36:40.931485: step 9771, loss 0.5203.
Train: 2018-08-02T11:36:41.165836: step 9772, loss 0.621281.
Train: 2018-08-02T11:36:41.384505: step 9773, loss 0.511977.
Train: 2018-08-02T11:36:41.618851: step 9774, loss 0.528797.
Train: 2018-08-02T11:36:41.837549: step 9775, loss 0.570786.
Train: 2018-08-02T11:36:42.056222: step 9776, loss 0.59598.
Train: 2018-08-02T11:36:42.290573: step 9777, loss 0.579171.
Train: 2018-08-02T11:36:42.524886: step 9778, loss 0.554023.
Train: 2018-08-02T11:36:42.759182: step 9779, loss 0.545675.
Train: 2018-08-02T11:36:42.977881: step 9780, loss 0.57913.
Test: 2018-08-02T11:36:44.180724: step 9780, loss 0.548479.
Train: 2018-08-02T11:36:44.415044: step 9781, loss 0.503985.
Train: 2018-08-02T11:36:44.633773: step 9782, loss 0.620856.
Train: 2018-08-02T11:36:44.868088: step 9783, loss 0.587437.
Train: 2018-08-02T11:36:45.102387: step 9784, loss 0.512523.
Train: 2018-08-02T11:36:45.336703: step 9785, loss 0.604015.
Train: 2018-08-02T11:36:45.555435: step 9786, loss 0.520962.
Train: 2018-08-02T11:36:45.774131: step 9787, loss 0.612227.
Train: 2018-08-02T11:36:45.992830: step 9788, loss 0.487971.
Train: 2018-08-02T11:36:46.227152: step 9789, loss 0.628705.
Train: 2018-08-02T11:36:46.461442: step 9790, loss 0.570756.
Test: 2018-08-02T11:36:47.648662: step 9790, loss 0.547777.
Train: 2018-08-02T11:36:47.851770: step 9791, loss 0.562506.
Train: 2018-08-02T11:36:48.086085: step 9792, loss 0.595469.
Train: 2018-08-02T11:36:48.320410: step 9793, loss 0.56254.
Train: 2018-08-02T11:36:48.539103: step 9794, loss 0.652778.
Train: 2018-08-02T11:36:48.757803: step 9795, loss 0.57077.
Train: 2018-08-02T11:36:48.992122: step 9796, loss 0.619606.
Train: 2018-08-02T11:36:49.226417: step 9797, loss 0.595094.
Train: 2018-08-02T11:36:49.445116: step 9798, loss 0.554706.
Train: 2018-08-02T11:36:49.679437: step 9799, loss 0.627005.
Train: 2018-08-02T11:36:49.898166: step 9800, loss 0.642698.
Test: 2018-08-02T11:36:51.085357: step 9800, loss 0.548857.
Train: 2018-08-02T11:36:51.991420: step 9801, loss 0.547154.
Train: 2018-08-02T11:36:52.210118: step 9802, loss 0.570982.
Train: 2018-08-02T11:36:52.428817: step 9803, loss 0.547504.
Train: 2018-08-02T11:36:52.678735: step 9804, loss 0.586699.
Train: 2018-08-02T11:36:52.897433: step 9805, loss 0.532215.
Train: 2018-08-02T11:36:53.116162: step 9806, loss 0.586674.
Train: 2018-08-02T11:36:53.334854: step 9807, loss 0.594408.
Train: 2018-08-02T11:36:53.553559: step 9808, loss 0.594379.
Train: 2018-08-02T11:36:53.787850: step 9809, loss 0.663606.
Train: 2018-08-02T11:36:54.022169: step 9810, loss 0.556025.
Test: 2018-08-02T11:36:55.193770: step 9810, loss 0.552796.
Train: 2018-08-02T11:36:55.412493: step 9811, loss 0.579023.
Train: 2018-08-02T11:36:55.631196: step 9812, loss 0.541074.
Train: 2018-08-02T11:36:55.849897: step 9813, loss 0.548764.
Train: 2018-08-02T11:36:56.099808: step 9814, loss 0.594223.
Train: 2018-08-02T11:36:56.271668: step 9815, loss 0.547873.
Train: 2018-08-02T11:36:56.505994: step 9816, loss 0.609312.
Train: 2018-08-02T11:36:56.724688: step 9817, loss 0.556499.
Train: 2018-08-02T11:36:56.943390: step 9818, loss 0.548982.
Train: 2018-08-02T11:36:57.193335: step 9819, loss 0.639434.
Train: 2018-08-02T11:36:57.396410: step 9820, loss 0.579138.
Test: 2018-08-02T11:36:58.614844: step 9820, loss 0.55138.
Train: 2018-08-02T11:36:58.817952: step 9821, loss 0.57915.
Train: 2018-08-02T11:36:59.036621: step 9822, loss 0.601696.
Train: 2018-08-02T11:36:59.286593: step 9823, loss 0.541677.
Train: 2018-08-02T11:36:59.505260: step 9824, loss 0.556687.
Train: 2018-08-02T11:36:59.739606: step 9825, loss 0.549173.
Train: 2018-08-02T11:36:59.973933: step 9826, loss 0.556633.
Train: 2018-08-02T11:37:00.208223: step 9827, loss 0.534004.
Train: 2018-08-02T11:37:00.426944: step 9828, loss 0.54139.
Train: 2018-08-02T11:37:00.645650: step 9829, loss 0.624518.
Train: 2018-08-02T11:37:00.879938: step 9830, loss 0.579064.
Test: 2018-08-02T11:37:02.067160: step 9830, loss 0.550801.
Train: 2018-08-02T11:37:02.285890: step 9831, loss 0.579048.
Train: 2018-08-02T11:37:02.520181: step 9832, loss 0.571422.
Train: 2018-08-02T11:37:02.754503: step 9833, loss 0.586648.
Train: 2018-08-02T11:37:02.973229: step 9834, loss 0.586648.
Train: 2018-08-02T11:37:03.191896: step 9835, loss 0.563727.
Train: 2018-08-02T11:37:03.410627: step 9836, loss 0.52546.
Train: 2018-08-02T11:37:03.644916: step 9837, loss 0.632648.
Train: 2018-08-02T11:37:03.863645: step 9838, loss 0.540624.
Train: 2018-08-02T11:37:04.097966: step 9839, loss 0.563604.
Train: 2018-08-02T11:37:04.316635: step 9840, loss 0.571263.
Test: 2018-08-02T11:37:05.519477: step 9840, loss 0.549775.
Train: 2018-08-02T11:37:05.738176: step 9841, loss 0.524997.
Train: 2018-08-02T11:37:05.972496: step 9842, loss 0.54029.
Train: 2018-08-02T11:37:06.206843: step 9843, loss 0.640955.
Train: 2018-08-02T11:37:06.425539: step 9844, loss 0.571146.
Train: 2018-08-02T11:37:06.659836: step 9845, loss 0.571131.
Train: 2018-08-02T11:37:06.878564: step 9846, loss 0.516636.
Train: 2018-08-02T11:37:07.128513: step 9847, loss 0.602298.
Train: 2018-08-02T11:37:07.362832: step 9848, loss 0.5867.
Train: 2018-08-02T11:37:07.581520: step 9849, loss 0.586704.
Train: 2018-08-02T11:37:07.800219: step 9850, loss 0.484984.
Test: 2018-08-02T11:37:09.003036: step 9850, loss 0.549719.
Train: 2018-08-02T11:37:09.237357: step 9851, loss 0.578874.
Train: 2018-08-02T11:37:09.456055: step 9852, loss 0.508105.
Train: 2018-08-02T11:37:09.674779: step 9853, loss 0.555191.
Train: 2018-08-02T11:37:09.893483: step 9854, loss 0.539266.
Train: 2018-08-02T11:37:10.112178: step 9855, loss 0.562959.
Train: 2018-08-02T11:37:10.362123: step 9856, loss 0.594817.
Train: 2018-08-02T11:37:10.596445: step 9857, loss 0.578866.
Train: 2018-08-02T11:37:10.815142: step 9858, loss 0.562833.
Train: 2018-08-02T11:37:11.049433: step 9859, loss 0.602967.
Train: 2018-08-02T11:37:11.283783: step 9860, loss 0.627108.
Test: 2018-08-02T11:37:12.470974: step 9860, loss 0.548095.
Train: 2018-08-02T11:37:12.689704: step 9861, loss 0.514608.
Train: 2018-08-02T11:37:12.908402: step 9862, loss 0.578872.
Train: 2018-08-02T11:37:13.127070: step 9863, loss 0.594952.
Train: 2018-08-02T11:37:13.345801: step 9864, loss 0.562801.
Train: 2018-08-02T11:37:13.580123: step 9865, loss 0.522645.
Train: 2018-08-02T11:37:13.830058: step 9866, loss 0.578872.
Train: 2018-08-02T11:37:14.048760: step 9867, loss 0.506507.
Train: 2018-08-02T11:37:14.283081: step 9868, loss 0.546659.
Train: 2018-08-02T11:37:14.517401: step 9869, loss 0.562742.
Train: 2018-08-02T11:37:14.767311: step 9870, loss 0.562718.
Test: 2018-08-02T11:37:15.970155: step 9870, loss 0.548611.
Train: 2018-08-02T11:37:16.173234: step 9871, loss 0.562697.
Train: 2018-08-02T11:37:16.407552: step 9872, loss 0.554565.
Train: 2018-08-02T11:37:16.626251: step 9873, loss 0.481399.
Train: 2018-08-02T11:37:16.844950: step 9874, loss 0.489236.
Train: 2018-08-02T11:37:17.079271: step 9875, loss 0.66908.
Train: 2018-08-02T11:37:17.313621: step 9876, loss 0.496891.
Train: 2018-08-02T11:37:17.563532: step 9877, loss 0.562525.
Train: 2018-08-02T11:37:17.782231: step 9878, loss 0.512965.
Train: 2018-08-02T11:37:18.016552: step 9879, loss 0.504475.
Train: 2018-08-02T11:37:18.250896: step 9880, loss 0.562439.
Test: 2018-08-02T11:37:19.453714: step 9880, loss 0.549039.
Train: 2018-08-02T11:37:19.734925: step 9881, loss 0.503937.
Train: 2018-08-02T11:37:19.969218: step 9882, loss 0.495234.
Train: 2018-08-02T11:37:20.187947: step 9883, loss 0.62144.
Train: 2018-08-02T11:37:20.422293: step 9884, loss 0.460631.
Train: 2018-08-02T11:37:20.656559: step 9885, loss 0.587967.
Train: 2018-08-02T11:37:20.875283: step 9886, loss 0.553811.
Train: 2018-08-02T11:37:21.109602: step 9887, loss 0.545212.
Train: 2018-08-02T11:37:21.343926: step 9888, loss 0.441679.
Train: 2018-08-02T11:37:21.578242: step 9889, loss 0.623277.
Train: 2018-08-02T11:37:21.812538: step 9890, loss 0.544904.
Test: 2018-08-02T11:37:23.015380: step 9890, loss 0.547207.
Train: 2018-08-02T11:37:23.218489: step 9891, loss 0.562071.
Train: 2018-08-02T11:37:23.437182: step 9892, loss 0.606228.
Train: 2018-08-02T11:37:23.671480: step 9893, loss 0.527398.
Train: 2018-08-02T11:37:23.890201: step 9894, loss 0.571049.
Train: 2018-08-02T11:37:24.108873: step 9895, loss 0.536054.
Train: 2018-08-02T11:37:24.343194: step 9896, loss 0.553627.
Train: 2018-08-02T11:37:24.577545: step 9897, loss 0.553389.
Train: 2018-08-02T11:37:24.796244: step 9898, loss 0.535939.
Train: 2018-08-02T11:37:25.014942: step 9899, loss 0.536265.
Train: 2018-08-02T11:37:25.249263: step 9900, loss 0.562563.
Test: 2018-08-02T11:37:26.436454: step 9900, loss 0.546505.
Train: 2018-08-02T11:37:27.342493: step 9901, loss 0.606524.
Train: 2018-08-02T11:37:27.576843: step 9902, loss 0.571403.
Train: 2018-08-02T11:37:27.795541: step 9903, loss 0.527408.
Train: 2018-08-02T11:37:28.045453: step 9904, loss 0.562452.
Train: 2018-08-02T11:37:28.279799: step 9905, loss 0.571026.
Train: 2018-08-02T11:37:28.514119: step 9906, loss 0.527649.
Train: 2018-08-02T11:37:28.748413: step 9907, loss 0.562344.
Train: 2018-08-02T11:37:28.951521: step 9908, loss 0.527735.
Train: 2018-08-02T11:37:29.185811: step 9909, loss 0.519115.
Train: 2018-08-02T11:37:29.420130: step 9910, loss 0.501829.
Test: 2018-08-02T11:37:30.607352: step 9910, loss 0.546896.
Train: 2018-08-02T11:37:30.841674: step 9911, loss 0.562343.
Train: 2018-08-02T11:37:31.076024: step 9912, loss 0.622942.
Train: 2018-08-02T11:37:31.310339: step 9913, loss 0.562343.
Train: 2018-08-02T11:37:31.529011: step 9914, loss 0.53643.
Train: 2018-08-02T11:37:31.763334: step 9915, loss 0.54508.
Train: 2018-08-02T11:37:31.997682: step 9916, loss 0.545093.
Train: 2018-08-02T11:37:32.231973: step 9917, loss 0.596808.
Train: 2018-08-02T11:37:32.466292: step 9918, loss 0.605357.
Train: 2018-08-02T11:37:32.684990: step 9919, loss 0.596665.
Train: 2018-08-02T11:37:32.919337: step 9920, loss 0.502457.
Test: 2018-08-02T11:37:34.106532: step 9920, loss 0.548865.
Train: 2018-08-02T11:37:34.325263: step 9921, loss 0.579414.
Train: 2018-08-02T11:37:34.559553: step 9922, loss 0.511234.
Train: 2018-08-02T11:37:34.778275: step 9923, loss 0.545329.
Train: 2018-08-02T11:37:34.996974: step 9924, loss 0.604843.
Train: 2018-08-02T11:37:35.215674: step 9925, loss 0.587798.
Train: 2018-08-02T11:37:35.434377: step 9926, loss 0.587741.
Train: 2018-08-02T11:37:35.668667: step 9927, loss 0.596109.
Train: 2018-08-02T11:37:35.887381: step 9928, loss 0.596001.
Train: 2018-08-02T11:37:36.106090: step 9929, loss 0.520561.
Train: 2018-08-02T11:37:36.340411: step 9930, loss 0.537391.
Test: 2018-08-02T11:37:37.543228: step 9930, loss 0.550259.
Train: 2018-08-02T11:37:37.746336: step 9931, loss 0.520813.
Train: 2018-08-02T11:37:37.996272: step 9932, loss 0.529186.
Train: 2018-08-02T11:37:38.214946: step 9933, loss 0.620629.
Train: 2018-08-02T11:37:38.433646: step 9934, loss 0.587351.
Train: 2018-08-02T11:37:38.652373: step 9935, loss 0.554202.
Train: 2018-08-02T11:37:38.871043: step 9936, loss 0.52945.
Train: 2018-08-02T11:37:39.105388: step 9937, loss 0.727554.
Train: 2018-08-02T11:37:39.324094: step 9938, loss 0.554334.
Train: 2018-08-02T11:37:39.558412: step 9939, loss 0.578944.
Train: 2018-08-02T11:37:39.792711: step 9940, loss 0.570776.
Test: 2018-08-02T11:37:40.979923: step 9940, loss 0.548943.
Train: 2018-08-02T11:37:41.214245: step 9941, loss 0.53834.
Train: 2018-08-02T11:37:41.448564: step 9942, loss 0.570796.
Train: 2018-08-02T11:37:41.682917: step 9943, loss 0.675681.
Train: 2018-08-02T11:37:41.901583: step 9944, loss 0.570841.
Train: 2018-08-02T11:37:42.135903: step 9945, loss 0.546927.
Train: 2018-08-02T11:37:42.354627: step 9946, loss 0.594764.
Train: 2018-08-02T11:37:42.588922: step 9947, loss 0.563016.
Train: 2018-08-02T11:37:42.807620: step 9948, loss 0.570969.
Train: 2018-08-02T11:37:43.026320: step 9949, loss 0.53953.
Train: 2018-08-02T11:37:43.276304: step 9950, loss 0.555322.
Test: 2018-08-02T11:37:44.479104: step 9950, loss 0.55011.
Train: 2018-08-02T11:37:44.713449: step 9951, loss 0.547526.
Train: 2018-08-02T11:37:44.947769: step 9952, loss 0.563218.
Train: 2018-08-02T11:37:45.166469: step 9953, loss 0.547566.
Train: 2018-08-02T11:37:45.385173: step 9954, loss 0.563206.
Train: 2018-08-02T11:37:45.635114: step 9955, loss 0.618038.
Train: 2018-08-02T11:37:45.853812: step 9956, loss 0.508435.
Train: 2018-08-02T11:37:46.072512: step 9957, loss 0.586687.
Train: 2018-08-02T11:37:46.291210: step 9958, loss 0.625907.
Train: 2018-08-02T11:37:46.509908: step 9959, loss 0.555393.
Train: 2018-08-02T11:37:46.759845: step 9960, loss 0.563193.
Test: 2018-08-02T11:37:47.947041: step 9960, loss 0.550506.
Train: 2018-08-02T11:37:48.165770: step 9961, loss 0.602337.
Train: 2018-08-02T11:37:48.384470: step 9962, loss 0.539821.
Train: 2018-08-02T11:37:48.618785: step 9963, loss 0.492788.
Train: 2018-08-02T11:37:48.853105: step 9964, loss 0.586669.
Train: 2018-08-02T11:37:49.087432: step 9965, loss 0.523752.
Train: 2018-08-02T11:37:49.259265: step 9966, loss 0.563078.
Train: 2018-08-02T11:37:49.477934: step 9967, loss 0.578566.
Train: 2018-08-02T11:37:49.696631: step 9968, loss 0.658857.
Train: 2018-08-02T11:37:49.930952: step 9969, loss 0.570988.
Train: 2018-08-02T11:37:50.165272: step 9970, loss 0.570607.
Test: 2018-08-02T11:37:51.368115: step 9970, loss 0.549711.
Train: 2018-08-02T11:37:51.571218: step 9971, loss 0.650535.
Train: 2018-08-02T11:37:51.805549: step 9972, loss 0.570908.
Train: 2018-08-02T11:37:52.024237: step 9973, loss 0.523624.
Train: 2018-08-02T11:37:52.258563: step 9974, loss 0.539624.
Train: 2018-08-02T11:37:52.477261: step 9975, loss 0.58646.
Train: 2018-08-02T11:37:52.695954: step 9976, loss 0.586829.
Train: 2018-08-02T11:37:52.914653: step 9977, loss 0.500371.
Train: 2018-08-02T11:37:53.148979: step 9978, loss 0.554889.
Train: 2018-08-02T11:37:53.367648: step 9979, loss 0.508134.
Train: 2018-08-02T11:37:53.601988: step 9980, loss 0.531058.
Test: 2018-08-02T11:37:54.804811: step 9980, loss 0.550423.
Train: 2018-08-02T11:37:55.023534: step 9981, loss 0.523099.
Train: 2018-08-02T11:37:55.242208: step 9982, loss 0.506325.
Train: 2018-08-02T11:37:55.492175: step 9983, loss 0.538809.
Train: 2018-08-02T11:37:55.710849: step 9984, loss 0.6197.
Train: 2018-08-02T11:37:55.929581: step 9985, loss 0.607726.
Train: 2018-08-02T11:37:56.163893: step 9986, loss 0.586701.
Train: 2018-08-02T11:37:56.382566: step 9987, loss 0.563861.
Train: 2018-08-02T11:37:56.616917: step 9988, loss 0.604548.
Train: 2018-08-02T11:37:56.835616: step 9989, loss 0.627541.
Train: 2018-08-02T11:37:57.054314: step 9990, loss 0.514117.
Test: 2018-08-02T11:37:58.257127: step 9990, loss 0.548869.
Train: 2018-08-02T11:37:58.475827: step 9991, loss 0.554588.
Train: 2018-08-02T11:37:58.694524: step 9992, loss 0.594963.
Train: 2018-08-02T11:37:58.944498: step 9993, loss 0.514566.
Train: 2018-08-02T11:37:59.178818: step 9994, loss 0.546742.
Train: 2018-08-02T11:37:59.397486: step 9995, loss 0.570804.
Train: 2018-08-02T11:37:59.616211: step 9996, loss 0.514521.
Train: 2018-08-02T11:37:59.850530: step 9997, loss 0.635337.
Train: 2018-08-02T11:38:00.084824: step 9998, loss 0.5708.
Train: 2018-08-02T11:38:00.303527: step 9999, loss 0.56279.
Train: 2018-08-02T11:38:00.522252: step 10000, loss 0.514288.
Test: 2018-08-02T11:38:01.725065: step 10000, loss 0.548472.
Train: 2018-08-02T11:38:02.553022: step 10001, loss 0.538541.
Train: 2018-08-02T11:38:02.787347: step 10002, loss 0.627557.
Train: 2018-08-02T11:38:03.006047: step 10003, loss 0.54643.
Train: 2018-08-02T11:38:03.240366: step 10004, loss 0.578552.
Train: 2018-08-02T11:38:03.454455: step 10005, loss 0.595242.
Train: 2018-08-02T11:38:03.673183: step 10006, loss 0.546292.
Train: 2018-08-02T11:38:03.891857: step 10007, loss 0.546609.
Train: 2018-08-02T11:38:04.110590: step 10008, loss 0.538105.
Train: 2018-08-02T11:38:04.344906: step 10009, loss 0.530235.
Train: 2018-08-02T11:38:04.563573: step 10010, loss 0.570195.
Test: 2018-08-02T11:38:05.750796: step 10010, loss 0.549116.
Train: 2018-08-02T11:38:05.969495: step 10011, loss 0.562758.
Train: 2018-08-02T11:38:06.188224: step 10012, loss 0.555614.
Train: 2018-08-02T11:38:06.406918: step 10013, loss 0.595799.
Train: 2018-08-02T11:38:06.656864: step 10014, loss 0.530651.
Train: 2018-08-02T11:38:06.875566: step 10015, loss 0.480656.
Train: 2018-08-02T11:38:07.094231: step 10016, loss 0.594938.
Train: 2018-08-02T11:38:07.328552: step 10017, loss 0.612045.
Train: 2018-08-02T11:38:07.562875: step 10018, loss 0.595726.
Train: 2018-08-02T11:38:07.781600: step 10019, loss 0.562513.
Train: 2018-08-02T11:38:08.000270: step 10020, loss 0.554349.
Test: 2018-08-02T11:38:09.218734: step 10020, loss 0.548837.
Train: 2018-08-02T11:38:09.421836: step 10021, loss 0.497471.
Train: 2018-08-02T11:38:09.671782: step 10022, loss 0.53815.
Train: 2018-08-02T11:38:09.906073: step 10023, loss 0.63614.
Train: 2018-08-02T11:38:10.124797: step 10024, loss 0.587096.
Train: 2018-08-02T11:38:10.359116: step 10025, loss 0.595244.
Train: 2018-08-02T11:38:10.577790: step 10026, loss 0.546343.
Train: 2018-08-02T11:38:10.796515: step 10027, loss 0.595192.
Train: 2018-08-02T11:38:11.030809: step 10028, loss 0.481421.
Train: 2018-08-02T11:38:11.249508: step 10029, loss 0.62768.
Train: 2018-08-02T11:38:11.468207: step 10030, loss 0.578907.
Test: 2018-08-02T11:38:12.671050: step 10030, loss 0.547836.
Train: 2018-08-02T11:38:12.889774: step 10031, loss 0.546467.
Train: 2018-08-02T11:38:13.139690: step 10032, loss 0.546489.
Train: 2018-08-02T11:38:13.358420: step 10033, loss 0.578897.
Train: 2018-08-02T11:38:13.577118: step 10034, loss 0.497946.
Train: 2018-08-02T11:38:13.795786: step 10035, loss 0.619414.
Train: 2018-08-02T11:38:14.014485: step 10036, loss 0.562697.
Train: 2018-08-02T11:38:14.248806: step 10037, loss 0.538409.
Train: 2018-08-02T11:38:14.467529: step 10038, loss 0.546497.
Train: 2018-08-02T11:38:14.701850: step 10039, loss 0.668064.
Train: 2018-08-02T11:38:14.920554: step 10040, loss 0.522255.
Test: 2018-08-02T11:38:16.123367: step 10040, loss 0.549214.
Train: 2018-08-02T11:38:16.357689: step 10041, loss 0.514203.
Train: 2018-08-02T11:38:16.592032: step 10042, loss 0.55462.
Train: 2018-08-02T11:38:16.826352: step 10043, loss 0.635577.
Train: 2018-08-02T11:38:17.045050: step 10044, loss 0.562712.
Train: 2018-08-02T11:38:17.279372: step 10045, loss 0.554639.
Train: 2018-08-02T11:38:17.513691: step 10046, loss 0.522333.
Train: 2018-08-02T11:38:17.732366: step 10047, loss 0.506137.
Train: 2018-08-02T11:38:17.951063: step 10048, loss 0.619404.
Train: 2018-08-02T11:38:18.169763: step 10049, loss 0.603207.
Train: 2018-08-02T11:38:18.404109: step 10050, loss 0.586997.
Test: 2018-08-02T11:38:19.606926: step 10050, loss 0.547716.
Train: 2018-08-02T11:38:19.810033: step 10051, loss 0.603154.
Train: 2018-08-02T11:38:20.059956: step 10052, loss 0.530441.
Train: 2018-08-02T11:38:20.263054: step 10053, loss 0.546619.
Train: 2018-08-02T11:38:20.481751: step 10054, loss 0.522429.
Train: 2018-08-02T11:38:20.716073: step 10055, loss 0.578885.
Train: 2018-08-02T11:38:20.950361: step 10056, loss 0.554666.
Train: 2018-08-02T11:38:21.169060: step 10057, loss 0.522355.
Train: 2018-08-02T11:38:21.387758: step 10058, loss 0.595079.
Train: 2018-08-02T11:38:21.606458: step 10059, loss 0.53033.
Train: 2018-08-02T11:38:21.825189: step 10060, loss 0.595129.
Test: 2018-08-02T11:38:23.043621: step 10060, loss 0.550261.
Train: 2018-08-02T11:38:23.246698: step 10061, loss 0.570784.
Train: 2018-08-02T11:38:23.465398: step 10062, loss 0.570797.
Train: 2018-08-02T11:38:23.684121: step 10063, loss 0.562664.
Train: 2018-08-02T11:38:23.902822: step 10064, loss 0.570778.
Train: 2018-08-02T11:38:24.137151: step 10065, loss 0.554555.
Train: 2018-08-02T11:38:24.371435: step 10066, loss 0.481534.
Train: 2018-08-02T11:38:24.574537: step 10067, loss 0.521989.
Train: 2018-08-02T11:38:24.808833: step 10068, loss 0.55448.
Train: 2018-08-02T11:38:25.043184: step 10069, loss 0.529887.
Train: 2018-08-02T11:38:25.277473: step 10070, loss 0.554359.
Test: 2018-08-02T11:38:26.480316: step 10070, loss 0.548961.
Train: 2018-08-02T11:38:26.699017: step 10071, loss 0.620145.
Train: 2018-08-02T11:38:26.917739: step 10072, loss 0.587286.
Train: 2018-08-02T11:38:27.152035: step 10073, loss 0.529485.
Train: 2018-08-02T11:38:27.401975: step 10074, loss 0.57903.
Train: 2018-08-02T11:38:27.620675: step 10075, loss 0.562547.
Train: 2018-08-02T11:38:27.839373: step 10076, loss 0.603887.
Train: 2018-08-02T11:38:28.058102: step 10077, loss 0.554214.
Train: 2018-08-02T11:38:28.276796: step 10078, loss 0.54598.
Train: 2018-08-02T11:38:28.511119: step 10079, loss 0.628546.
Train: 2018-08-02T11:38:28.745412: step 10080, loss 0.57075.
Test: 2018-08-02T11:38:29.948254: step 10080, loss 0.548787.
Train: 2018-08-02T11:38:30.151362: step 10081, loss 0.570745.
Train: 2018-08-02T11:38:30.370060: step 10082, loss 0.562571.
Train: 2018-08-02T11:38:30.588761: step 10083, loss 0.529776.
Train: 2018-08-02T11:38:30.838671: step 10084, loss 0.546185.
Train: 2018-08-02T11:38:31.057396: step 10085, loss 0.554386.
Train: 2018-08-02T11:38:31.276068: step 10086, loss 0.611712.
Train: 2018-08-02T11:38:31.526010: step 10087, loss 0.56259.
Train: 2018-08-02T11:38:31.729088: step 10088, loss 0.627956.
Train: 2018-08-02T11:38:31.963432: step 10089, loss 0.554484.
Train: 2018-08-02T11:38:32.213372: step 10090, loss 0.570783.
Test: 2018-08-02T11:38:33.400571: step 10090, loss 0.548385.
Train: 2018-08-02T11:38:33.619300: step 10091, loss 0.538344.
Train: 2018-08-02T11:38:33.837969: step 10092, loss 0.562695.
Train: 2018-08-02T11:38:34.103532: step 10093, loss 0.619354.
Train: 2018-08-02T11:38:34.322229: step 10094, loss 0.522372.
Train: 2018-08-02T11:38:34.572172: step 10095, loss 0.586946.
Train: 2018-08-02T11:38:34.806499: step 10096, loss 0.538615.
Train: 2018-08-02T11:38:35.025221: step 10097, loss 0.498407.
Train: 2018-08-02T11:38:35.275158: step 10098, loss 0.619152.
Train: 2018-08-02T11:38:35.509452: step 10099, loss 0.522516.
Train: 2018-08-02T11:38:35.728176: step 10100, loss 0.54665.
Test: 2018-08-02T11:38:36.930994: step 10100, loss 0.550951.
Train: 2018-08-02T11:38:37.805813: step 10101, loss 0.570816.
Train: 2018-08-02T11:38:38.024518: step 10102, loss 0.530451.
Train: 2018-08-02T11:38:38.243219: step 10103, loss 0.546551.
Train: 2018-08-02T11:38:38.477539: step 10104, loss 0.506.
Train: 2018-08-02T11:38:38.696231: step 10105, loss 0.546412.
Train: 2018-08-02T11:38:38.914935: step 10106, loss 0.578925.
Train: 2018-08-02T11:38:39.133633: step 10107, loss 0.603448.
Train: 2018-08-02T11:38:39.352332: step 10108, loss 0.521689.
Train: 2018-08-02T11:38:39.602243: step 10109, loss 0.562567.
Train: 2018-08-02T11:38:39.820967: step 10110, loss 0.554338.
Test: 2018-08-02T11:38:41.023786: step 10110, loss 0.549742.
Train: 2018-08-02T11:38:41.258131: step 10111, loss 0.496727.
Train: 2018-08-02T11:38:41.476804: step 10112, loss 0.512997.
Train: 2018-08-02T11:38:41.711125: step 10113, loss 0.521057.
Train: 2018-08-02T11:38:41.945447: step 10114, loss 0.570776.
Train: 2018-08-02T11:38:42.179765: step 10115, loss 0.503962.
Train: 2018-08-02T11:38:42.398494: step 10116, loss 0.637897.
Train: 2018-08-02T11:38:42.585956: step 10117, loss 0.526473.
Train: 2018-08-02T11:38:42.820243: step 10118, loss 0.503345.
Train: 2018-08-02T11:38:43.038964: step 10119, loss 0.621642.
Train: 2018-08-02T11:38:43.257668: step 10120, loss 0.587762.
Test: 2018-08-02T11:38:44.476101: step 10120, loss 0.549157.
Train: 2018-08-02T11:38:44.679206: step 10121, loss 0.502907.
Train: 2018-08-02T11:38:44.897903: step 10122, loss 0.579442.
Train: 2018-08-02T11:38:45.116577: step 10123, loss 0.604923.
Train: 2018-08-02T11:38:45.335307: step 10124, loss 0.621833.
Train: 2018-08-02T11:38:45.554001: step 10125, loss 0.545396.
Train: 2018-08-02T11:38:45.788325: step 10126, loss 0.528502.
Train: 2018-08-02T11:38:46.006994: step 10127, loss 0.53699.
Train: 2018-08-02T11:38:46.241314: step 10128, loss 0.536996.
Train: 2018-08-02T11:38:46.460043: step 10129, loss 0.520082.
Train: 2018-08-02T11:38:46.694333: step 10130, loss 0.621595.
Test: 2018-08-02T11:38:47.897176: step 10130, loss 0.548626.
Train: 2018-08-02T11:38:48.115905: step 10131, loss 0.503165.
Train: 2018-08-02T11:38:48.350197: step 10132, loss 0.503136.
Train: 2018-08-02T11:38:48.568925: step 10133, loss 0.596246.
Train: 2018-08-02T11:38:48.787623: step 10134, loss 0.596259.
Train: 2018-08-02T11:38:49.006319: step 10135, loss 0.579298.
Train: 2018-08-02T11:38:49.240642: step 10136, loss 0.536967.
Train: 2018-08-02T11:38:49.490583: step 10137, loss 0.621566.
Train: 2018-08-02T11:38:49.709283: step 10138, loss 0.58769.
Train: 2018-08-02T11:38:49.927975: step 10139, loss 0.562376.
Train: 2018-08-02T11:38:50.146649: step 10140, loss 0.570786.
Test: 2018-08-02T11:38:51.349493: step 10140, loss 0.548409.
Train: 2018-08-02T11:38:51.568216: step 10141, loss 0.570778.
Train: 2018-08-02T11:38:51.833781: step 10142, loss 0.528993.
Train: 2018-08-02T11:38:52.052480: step 10143, loss 0.570766.
Train: 2018-08-02T11:38:52.271183: step 10144, loss 0.570763.
Train: 2018-08-02T11:38:52.505473: step 10145, loss 0.612303.
Train: 2018-08-02T11:38:52.755414: step 10146, loss 0.587324.
Train: 2018-08-02T11:38:52.989734: step 10147, loss 0.537735.
Train: 2018-08-02T11:38:53.208464: step 10148, loss 0.496649.
Train: 2018-08-02T11:38:53.427141: step 10149, loss 0.620123.
Train: 2018-08-02T11:38:53.677073: step 10150, loss 0.669286.
Test: 2018-08-02T11:38:54.864294: step 10150, loss 0.549131.
Train: 2018-08-02T11:38:55.083025: step 10151, loss 0.660682.
Train: 2018-08-02T11:38:55.317315: step 10152, loss 0.603276.
Train: 2018-08-02T11:38:55.536044: step 10153, loss 0.506268.
Train: 2018-08-02T11:38:55.754741: step 10154, loss 0.522653.
Train: 2018-08-02T11:38:55.989061: step 10155, loss 0.54684.
Train: 2018-08-02T11:38:56.223382: step 10156, loss 0.578861.
Train: 2018-08-02T11:38:56.442080: step 10157, loss 0.570891.
Train: 2018-08-02T11:38:56.660779: step 10158, loss 0.570908.
Train: 2018-08-02T11:38:56.879474: step 10159, loss 0.499516.
Train: 2018-08-02T11:38:57.113799: step 10160, loss 0.555059.
Test: 2018-08-02T11:38:58.332232: step 10160, loss 0.549007.
Train: 2018-08-02T11:38:58.535334: step 10161, loss 0.539185.
Train: 2018-08-02T11:38:58.754033: step 10162, loss 0.555032.
Train: 2018-08-02T11:38:58.988329: step 10163, loss 0.53115.
Train: 2018-08-02T11:38:59.207058: step 10164, loss 0.531057.
Train: 2018-08-02T11:38:59.441382: step 10165, loss 0.546908.
Train: 2018-08-02T11:38:59.660072: step 10166, loss 0.546819.
Train: 2018-08-02T11:38:59.878744: step 10167, loss 0.562799.
Train: 2018-08-02T11:39:00.097444: step 10168, loss 0.522465.
Train: 2018-08-02T11:39:00.331764: step 10169, loss 0.603155.
Train: 2018-08-02T11:39:00.566086: step 10170, loss 0.5789.
Test: 2018-08-02T11:39:01.753307: step 10170, loss 0.549292.
Train: 2018-08-02T11:39:01.987651: step 10171, loss 0.497703.
Train: 2018-08-02T11:39:02.206356: step 10172, loss 0.627796.
Train: 2018-08-02T11:39:02.440686: step 10173, loss 0.554462.
Train: 2018-08-02T11:39:02.690613: step 10174, loss 0.554437.
Train: 2018-08-02T11:39:02.940554: step 10175, loss 0.603477.
Train: 2018-08-02T11:39:03.174874: step 10176, loss 0.562587.
Train: 2018-08-02T11:39:03.393549: step 10177, loss 0.628036.
Train: 2018-08-02T11:39:03.612277: step 10178, loss 0.538089.
Train: 2018-08-02T11:39:03.846602: step 10179, loss 0.521782.
Train: 2018-08-02T11:39:04.065291: step 10180, loss 0.554434.
Test: 2018-08-02T11:39:05.268108: step 10180, loss 0.548009.
Train: 2018-08-02T11:39:05.486832: step 10181, loss 0.554425.
Train: 2018-08-02T11:39:05.721153: step 10182, loss 0.505351.
Train: 2018-08-02T11:39:05.939858: step 10183, loss 0.546184.
Train: 2018-08-02T11:39:06.174172: step 10184, loss 0.529709.
Train: 2018-08-02T11:39:06.408475: step 10185, loss 0.521367.
Train: 2018-08-02T11:39:06.627195: step 10186, loss 0.603786.
Train: 2018-08-02T11:39:06.845895: step 10187, loss 0.587308.
Train: 2018-08-02T11:39:07.080186: step 10188, loss 0.496217.
Train: 2018-08-02T11:39:07.298908: step 10189, loss 0.603993.
Train: 2018-08-02T11:39:07.533206: step 10190, loss 0.554114.
Test: 2018-08-02T11:39:08.736046: step 10190, loss 0.549116.
Train: 2018-08-02T11:39:08.954744: step 10191, loss 0.587442.
Train: 2018-08-02T11:39:09.173474: step 10192, loss 0.562445.
Train: 2018-08-02T11:39:09.407773: step 10193, loss 0.645666.
Train: 2018-08-02T11:39:09.626467: step 10194, loss 0.579066.
Train: 2018-08-02T11:39:09.845193: step 10195, loss 0.55418.
Train: 2018-08-02T11:39:10.079508: step 10196, loss 0.51284.
Train: 2018-08-02T11:39:10.313827: step 10197, loss 0.554218.
Train: 2018-08-02T11:39:10.548122: step 10198, loss 0.612084.
Train: 2018-08-02T11:39:10.782472: step 10199, loss 0.554251.
Train: 2018-08-02T11:39:11.016764: step 10200, loss 0.562515.
Test: 2018-08-02T11:39:12.219606: step 10200, loss 0.549346.
Train: 2018-08-02T11:39:13.110047: step 10201, loss 0.496671.
Train: 2018-08-02T11:39:13.344373: step 10202, loss 0.463704.
Train: 2018-08-02T11:39:13.563071: step 10203, loss 0.5625.
Train: 2018-08-02T11:39:13.797391: step 10204, loss 0.521109.
Train: 2018-08-02T11:39:14.016089: step 10205, loss 0.620548.
Train: 2018-08-02T11:39:14.234758: step 10206, loss 0.579068.
Train: 2018-08-02T11:39:14.453458: step 10207, loss 0.53751.
Train: 2018-08-02T11:39:14.672186: step 10208, loss 0.612371.
Train: 2018-08-02T11:39:14.890855: step 10209, loss 0.47094.
Train: 2018-08-02T11:39:15.109579: step 10210, loss 0.529104.
Test: 2018-08-02T11:39:16.312396: step 10210, loss 0.548281.
Train: 2018-08-02T11:39:16.531121: step 10211, loss 0.503969.
Train: 2018-08-02T11:39:16.749794: step 10212, loss 0.537275.
Train: 2018-08-02T11:39:16.968493: step 10213, loss 0.562386.
Train: 2018-08-02T11:39:17.202845: step 10214, loss 0.570795.
Train: 2018-08-02T11:39:17.421539: step 10215, loss 0.587696.
Train: 2018-08-02T11:39:17.655833: step 10216, loss 0.562351.
Train: 2018-08-02T11:39:17.890153: step 10217, loss 0.528507.
Train: 2018-08-02T11:39:18.140094: step 10218, loss 0.528483.
Train: 2018-08-02T11:39:18.358794: step 10219, loss 0.536887.
Train: 2018-08-02T11:39:18.593144: step 10220, loss 0.587855.
Test: 2018-08-02T11:39:19.780335: step 10220, loss 0.549495.
Train: 2018-08-02T11:39:20.061549: step 10221, loss 0.579404.
Train: 2018-08-02T11:39:20.311461: step 10222, loss 0.664511.
Train: 2018-08-02T11:39:20.530160: step 10223, loss 0.51988.
Train: 2018-08-02T11:39:20.748857: step 10224, loss 0.570834.
Train: 2018-08-02T11:39:20.967587: step 10225, loss 0.587764.
Train: 2018-08-02T11:39:21.186285: step 10226, loss 0.520101.
Train: 2018-08-02T11:39:21.420632: step 10227, loss 0.537039.
Train: 2018-08-02T11:39:21.639301: step 10228, loss 0.537058.
Train: 2018-08-02T11:39:21.873627: step 10229, loss 0.494891.
Train: 2018-08-02T11:39:22.092293: step 10230, loss 0.55392.
Test: 2018-08-02T11:39:23.295137: step 10230, loss 0.548633.
Train: 2018-08-02T11:39:23.513862: step 10231, loss 0.562361.
Train: 2018-08-02T11:39:23.748157: step 10232, loss 0.613113.
Train: 2018-08-02T11:39:23.966884: step 10233, loss 0.545453.
Train: 2018-08-02T11:39:24.185582: step 10234, loss 0.545462.
Train: 2018-08-02T11:39:24.419899: step 10235, loss 0.587709.
Train: 2018-08-02T11:39:24.638603: step 10236, loss 0.638338.
Train: 2018-08-02T11:39:24.888515: step 10237, loss 0.612891.
Train: 2018-08-02T11:39:25.107243: step 10238, loss 0.562393.
Train: 2018-08-02T11:39:25.357184: step 10239, loss 0.562412.
Train: 2018-08-02T11:39:25.591475: step 10240, loss 0.637423.
Test: 2018-08-02T11:39:26.778696: step 10240, loss 0.548042.
Train: 2018-08-02T11:39:26.997395: step 10241, loss 0.49612.
Train: 2018-08-02T11:39:27.231715: step 10242, loss 0.537679.
Train: 2018-08-02T11:39:27.450445: step 10243, loss 0.620279.
Train: 2018-08-02T11:39:27.669143: step 10244, loss 0.587209.
Train: 2018-08-02T11:39:27.919080: step 10245, loss 0.55437.
Train: 2018-08-02T11:39:28.137754: step 10246, loss 0.627966.
Train: 2018-08-02T11:39:28.356482: step 10247, loss 0.587051.
Train: 2018-08-02T11:39:28.590798: step 10248, loss 0.562696.
Train: 2018-08-02T11:39:28.825094: step 10249, loss 0.554677.
Train: 2018-08-02T11:39:29.075033: step 10250, loss 0.570829.
Test: 2018-08-02T11:39:30.277877: step 10250, loss 0.55.
Train: 2018-08-02T11:39:30.496574: step 10251, loss 0.530776.
Train: 2018-08-02T11:39:30.715304: step 10252, loss 0.562875.
Train: 2018-08-02T11:39:30.933975: step 10253, loss 0.570874.
Train: 2018-08-02T11:39:31.183925: step 10254, loss 0.538989.
Train: 2018-08-02T11:39:31.418235: step 10255, loss 0.578856.
Train: 2018-08-02T11:39:31.636934: step 10256, loss 0.539057.
Train: 2018-08-02T11:39:31.871253: step 10257, loss 0.515187.
Train: 2018-08-02T11:39:32.089952: step 10258, loss 0.507139.
Train: 2018-08-02T11:39:32.308679: step 10259, loss 0.554891.
Train: 2018-08-02T11:39:32.527381: step 10260, loss 0.650989.
Test: 2018-08-02T11:39:33.730193: step 10260, loss 0.547794.
Train: 2018-08-02T11:39:33.933301: step 10261, loss 0.554826.
Train: 2018-08-02T11:39:34.151994: step 10262, loss 0.578873.
Train: 2018-08-02T11:39:34.386289: step 10263, loss 0.578876.
Train: 2018-08-02T11:39:34.620611: step 10264, loss 0.546798.
Train: 2018-08-02T11:39:34.870578: step 10265, loss 0.602925.
Train: 2018-08-02T11:39:35.089276: step 10266, loss 0.506732.
Train: 2018-08-02T11:39:35.323570: step 10267, loss 0.586891.
Train: 2018-08-02T11:39:35.495433: step 10268, loss 0.494335.
Train: 2018-08-02T11:39:35.729725: step 10269, loss 0.538664.
Train: 2018-08-02T11:39:35.979677: step 10270, loss 0.546632.
Test: 2018-08-02T11:39:37.182509: step 10270, loss 0.550151.
Train: 2018-08-02T11:39:37.385612: step 10271, loss 0.546553.
Train: 2018-08-02T11:39:37.619938: step 10272, loss 0.578899.
Train: 2018-08-02T11:39:37.838637: step 10273, loss 0.554534.
Train: 2018-08-02T11:39:38.072926: step 10274, loss 0.570779.
Train: 2018-08-02T11:39:38.307247: step 10275, loss 0.578933.
Train: 2018-08-02T11:39:38.525975: step 10276, loss 0.595274.
Train: 2018-08-02T11:39:38.744669: step 10277, loss 0.5626.
Train: 2018-08-02T11:39:38.963343: step 10278, loss 0.497217.
Train: 2018-08-02T11:39:39.197694: step 10279, loss 0.578953.
Train: 2018-08-02T11:39:39.431983: step 10280, loss 0.660924.
Test: 2018-08-02T11:39:40.619204: step 10280, loss 0.547776.
Train: 2018-08-02T11:39:40.822309: step 10281, loss 0.636254.
Train: 2018-08-02T11:39:41.041011: step 10282, loss 0.554446.
Train: 2018-08-02T11:39:41.275341: step 10283, loss 0.619636.
Train: 2018-08-02T11:39:41.509621: step 10284, loss 0.627591.
Train: 2018-08-02T11:39:41.728321: step 10285, loss 0.498121.
Train: 2018-08-02T11:39:41.978292: step 10286, loss 0.603044.
Train: 2018-08-02T11:39:42.212614: step 10287, loss 0.554784.
Train: 2018-08-02T11:39:42.446927: step 10288, loss 0.578865.
Train: 2018-08-02T11:39:42.665601: step 10289, loss 0.530948.
Train: 2018-08-02T11:39:42.899962: step 10290, loss 0.523048.
Test: 2018-08-02T11:39:44.087143: step 10290, loss 0.54979.
Train: 2018-08-02T11:39:44.305842: step 10291, loss 0.475242.
Train: 2018-08-02T11:39:44.540187: step 10292, loss 0.570876.
Train: 2018-08-02T11:39:44.774509: step 10293, loss 0.610862.
Train: 2018-08-02T11:39:45.008802: step 10294, loss 0.546842.
Train: 2018-08-02T11:39:45.243148: step 10295, loss 0.530791.
Train: 2018-08-02T11:39:45.461851: step 10296, loss 0.538749.
Train: 2018-08-02T11:39:45.696141: step 10297, loss 0.514512.
Train: 2018-08-02T11:39:45.930463: step 10298, loss 0.506216.
Train: 2018-08-02T11:39:46.149160: step 10299, loss 0.570846.
Train: 2018-08-02T11:39:46.367859: step 10300, loss 0.546286.
Test: 2018-08-02T11:39:47.586323: step 10300, loss 0.548168.
Train: 2018-08-02T11:39:48.461136: step 10301, loss 0.538029.
Train: 2018-08-02T11:39:48.695463: step 10302, loss 0.570777.
Train: 2018-08-02T11:39:48.929784: step 10303, loss 0.57074.
Train: 2018-08-02T11:39:49.164111: step 10304, loss 0.520715.
Train: 2018-08-02T11:39:49.398420: step 10305, loss 0.612528.
Train: 2018-08-02T11:39:49.617123: step 10306, loss 0.571167.
Train: 2018-08-02T11:39:49.835827: step 10307, loss 0.537697.
Train: 2018-08-02T11:39:50.070118: step 10308, loss 0.562449.
Train: 2018-08-02T11:39:50.304462: step 10309, loss 0.571014.
Train: 2018-08-02T11:39:50.523136: step 10310, loss 0.562317.
Test: 2018-08-02T11:39:51.725979: step 10310, loss 0.548873.
Train: 2018-08-02T11:39:51.944713: step 10311, loss 0.504096.
Train: 2018-08-02T11:39:52.163401: step 10312, loss 0.545633.
Train: 2018-08-02T11:39:52.382074: step 10313, loss 0.570748.
Train: 2018-08-02T11:39:52.600806: step 10314, loss 0.562324.
Train: 2018-08-02T11:39:52.835095: step 10315, loss 0.554033.
Train: 2018-08-02T11:39:53.069416: step 10316, loss 0.58758.
Train: 2018-08-02T11:39:53.303735: step 10317, loss 0.53726.
Train: 2018-08-02T11:39:53.538056: step 10318, loss 0.579155.
Train: 2018-08-02T11:39:53.756784: step 10319, loss 0.528815.
Train: 2018-08-02T11:39:53.991073: step 10320, loss 0.587539.
Test: 2018-08-02T11:39:55.193917: step 10320, loss 0.548752.
Train: 2018-08-02T11:39:55.412646: step 10321, loss 0.587556.
Train: 2018-08-02T11:39:55.646961: step 10322, loss 0.587552.
Train: 2018-08-02T11:39:55.865660: step 10323, loss 0.60425.
Train: 2018-08-02T11:39:56.084364: step 10324, loss 0.50386.
Train: 2018-08-02T11:39:56.318654: step 10325, loss 0.529044.
Train: 2018-08-02T11:39:56.537382: step 10326, loss 0.503826.
Train: 2018-08-02T11:39:56.771697: step 10327, loss 0.604126.
Train: 2018-08-02T11:39:56.990397: step 10328, loss 0.554137.
Train: 2018-08-02T11:39:57.209100: step 10329, loss 0.529117.
Train: 2018-08-02T11:39:57.427794: step 10330, loss 0.537396.
Test: 2018-08-02T11:39:58.630611: step 10330, loss 0.548149.
Train: 2018-08-02T11:39:58.849312: step 10331, loss 0.537142.
Train: 2018-08-02T11:39:59.068011: step 10332, loss 0.604774.
Train: 2018-08-02T11:39:59.302369: step 10333, loss 0.646368.
Train: 2018-08-02T11:39:59.536650: step 10334, loss 0.528914.
Train: 2018-08-02T11:39:59.770998: step 10335, loss 0.654379.
Train: 2018-08-02T11:40:00.020937: step 10336, loss 0.56242.
Train: 2018-08-02T11:40:00.223989: step 10337, loss 0.52102.
Train: 2018-08-02T11:40:00.442688: step 10338, loss 0.612098.
Train: 2018-08-02T11:40:00.661412: step 10339, loss 0.562513.
Train: 2018-08-02T11:40:00.880085: step 10340, loss 0.529653.
Test: 2018-08-02T11:40:02.082928: step 10340, loss 0.548848.
Train: 2018-08-02T11:40:02.317248: step 10341, loss 0.603588.
Train: 2018-08-02T11:40:02.551569: step 10342, loss 0.538022.
Train: 2018-08-02T11:40:02.785920: step 10343, loss 0.538083.
Train: 2018-08-02T11:40:03.004619: step 10344, loss 0.554445.
Train: 2018-08-02T11:40:03.238908: step 10345, loss 0.587087.
Train: 2018-08-02T11:40:03.473255: step 10346, loss 0.513747.
Train: 2018-08-02T11:40:03.707575: step 10347, loss 0.53004.
Train: 2018-08-02T11:40:03.957490: step 10348, loss 0.530004.
Train: 2018-08-02T11:40:04.191840: step 10349, loss 0.529939.
Train: 2018-08-02T11:40:04.426165: step 10350, loss 0.578949.
Test: 2018-08-02T11:40:05.613351: step 10350, loss 0.54888.
Train: 2018-08-02T11:40:05.816460: step 10351, loss 0.59535.
Train: 2018-08-02T11:40:06.035128: step 10352, loss 0.546161.
Train: 2018-08-02T11:40:06.253858: step 10353, loss 0.578968.
Train: 2018-08-02T11:40:06.488147: step 10354, loss 0.52971.
Train: 2018-08-02T11:40:06.706870: step 10355, loss 0.578978.
Train: 2018-08-02T11:40:06.925544: step 10356, loss 0.644765.
Train: 2018-08-02T11:40:07.175517: step 10357, loss 0.537918.
Train: 2018-08-02T11:40:07.394185: step 10358, loss 0.595373.
Train: 2018-08-02T11:40:07.612916: step 10359, loss 0.521615.
Train: 2018-08-02T11:40:07.862856: step 10360, loss 0.58714.
Test: 2018-08-02T11:40:09.065668: step 10360, loss 0.548738.
Train: 2018-08-02T11:40:09.284367: step 10361, loss 0.587126.
Train: 2018-08-02T11:40:09.534335: step 10362, loss 0.554435.
Train: 2018-08-02T11:40:09.753007: step 10363, loss 0.513667.
Train: 2018-08-02T11:40:09.987358: step 10364, loss 0.619726.
Train: 2018-08-02T11:40:10.206057: step 10365, loss 0.530033.
Train: 2018-08-02T11:40:10.440373: step 10366, loss 0.530048.
Train: 2018-08-02T11:40:10.659076: step 10367, loss 0.505579.
Train: 2018-08-02T11:40:10.877769: step 10368, loss 0.562607.
Train: 2018-08-02T11:40:11.112076: step 10369, loss 0.554414.
Train: 2018-08-02T11:40:11.330763: step 10370, loss 0.595339.
Test: 2018-08-02T11:40:12.533606: step 10370, loss 0.549068.
Train: 2018-08-02T11:40:12.752331: step 10371, loss 0.578961.
Train: 2018-08-02T11:40:12.986626: step 10372, loss 0.611748.
Train: 2018-08-02T11:40:13.236592: step 10373, loss 0.587142.
Train: 2018-08-02T11:40:13.455266: step 10374, loss 0.587123.
Train: 2018-08-02T11:40:13.689617: step 10375, loss 0.538134.
Train: 2018-08-02T11:40:13.908310: step 10376, loss 0.530027.
Train: 2018-08-02T11:40:14.142606: step 10377, loss 0.570776.
Train: 2018-08-02T11:40:14.376955: step 10378, loss 0.587065.
Train: 2018-08-02T11:40:14.611245: step 10379, loss 0.562645.
Train: 2018-08-02T11:40:14.829974: step 10380, loss 0.513886.
Test: 2018-08-02T11:40:16.032787: step 10380, loss 0.547948.
Train: 2018-08-02T11:40:16.267108: step 10381, loss 0.481337.
Train: 2018-08-02T11:40:16.485834: step 10382, loss 0.530025.
Train: 2018-08-02T11:40:16.720157: step 10383, loss 0.611635.
Train: 2018-08-02T11:40:16.954477: step 10384, loss 0.562581.
Train: 2018-08-02T11:40:17.173145: step 10385, loss 0.587152.
Train: 2018-08-02T11:40:17.407465: step 10386, loss 0.595358.
Train: 2018-08-02T11:40:17.657437: step 10387, loss 0.562567.
Train: 2018-08-02T11:40:17.891726: step 10388, loss 0.570764.
Train: 2018-08-02T11:40:18.126048: step 10389, loss 0.570765.
Train: 2018-08-02T11:40:18.344745: step 10390, loss 0.562583.
Test: 2018-08-02T11:40:19.563210: step 10390, loss 0.548744.
Train: 2018-08-02T11:40:19.781911: step 10391, loss 0.546234.
Train: 2018-08-02T11:40:20.000607: step 10392, loss 0.562592.
Train: 2018-08-02T11:40:20.219307: step 10393, loss 0.521722.
Train: 2018-08-02T11:40:20.438042: step 10394, loss 0.578947.
Train: 2018-08-02T11:40:20.672336: step 10395, loss 0.538033.
Train: 2018-08-02T11:40:20.906662: step 10396, loss 0.578954.
Train: 2018-08-02T11:40:21.125344: step 10397, loss 0.578956.
Train: 2018-08-02T11:40:21.359666: step 10398, loss 0.60353.
Train: 2018-08-02T11:40:21.594015: step 10399, loss 0.603496.
Train: 2018-08-02T11:40:21.812714: step 10400, loss 0.513604.
Test: 2018-08-02T11:40:23.015527: step 10400, loss 0.549356.
Train: 2018-08-02T11:40:23.890322: step 10401, loss 0.521803.
Train: 2018-08-02T11:40:24.109051: step 10402, loss 0.554441.
Train: 2018-08-02T11:40:24.358993: step 10403, loss 0.603446.
Train: 2018-08-02T11:40:24.562064: step 10404, loss 0.529944.
Train: 2018-08-02T11:40:24.796361: step 10405, loss 0.603442.
Train: 2018-08-02T11:40:25.015085: step 10406, loss 0.570771.
Train: 2018-08-02T11:40:25.249411: step 10407, loss 0.546306.
Train: 2018-08-02T11:40:25.483729: step 10408, loss 0.61154.
Train: 2018-08-02T11:40:25.702422: step 10409, loss 0.554496.
Train: 2018-08-02T11:40:25.921097: step 10410, loss 0.538254.
Test: 2018-08-02T11:40:27.139562: step 10410, loss 0.549641.
Train: 2018-08-02T11:40:27.373881: step 10411, loss 0.554527.
Train: 2018-08-02T11:40:27.655065: step 10412, loss 0.578911.
Train: 2018-08-02T11:40:27.873764: step 10413, loss 0.55454.
Train: 2018-08-02T11:40:28.123736: step 10414, loss 0.546424.
Train: 2018-08-02T11:40:28.373647: step 10415, loss 0.546419.
Train: 2018-08-02T11:40:28.592348: step 10416, loss 0.611421.
Train: 2018-08-02T11:40:28.826691: step 10417, loss 0.570786.
Train: 2018-08-02T11:40:29.060988: step 10418, loss 0.554557.
Train: 2018-08-02T11:40:29.232845: step 10419, loss 0.493454.
Train: 2018-08-02T11:40:29.451546: step 10420, loss 0.505805.
Test: 2018-08-02T11:40:30.654363: step 10420, loss 0.548284.
Train: 2018-08-02T11:40:30.888683: step 10421, loss 0.570777.
Train: 2018-08-02T11:40:31.107382: step 10422, loss 0.513648.
Train: 2018-08-02T11:40:31.357349: step 10423, loss 0.63625.
Train: 2018-08-02T11:40:31.591669: step 10424, loss 0.456055.
Train: 2018-08-02T11:40:31.794721: step 10425, loss 0.504991.
Train: 2018-08-02T11:40:32.029041: step 10426, loss 0.579013.
Train: 2018-08-02T11:40:32.247770: step 10427, loss 0.57904.
Train: 2018-08-02T11:40:32.466440: step 10428, loss 0.562451.
Train: 2018-08-02T11:40:32.685162: step 10429, loss 0.595737.
Train: 2018-08-02T11:40:32.903861: step 10430, loss 0.604104.
Test: 2018-08-02T11:40:34.106681: step 10430, loss 0.547744.
Train: 2018-08-02T11:40:34.341001: step 10431, loss 0.5791.
Train: 2018-08-02T11:40:34.559729: step 10432, loss 0.60408.
Train: 2018-08-02T11:40:34.794018: step 10433, loss 0.51254.
Train: 2018-08-02T11:40:35.012747: step 10434, loss 0.495924.
Train: 2018-08-02T11:40:35.247038: step 10435, loss 0.570773.
Train: 2018-08-02T11:40:35.481388: step 10436, loss 0.562443.
Train: 2018-08-02T11:40:35.700088: step 10437, loss 0.479088.
Train: 2018-08-02T11:40:35.918779: step 10438, loss 0.554059.
Train: 2018-08-02T11:40:36.137484: step 10439, loss 0.554031.
Train: 2018-08-02T11:40:36.403016: step 10440, loss 0.461751.
Test: 2018-08-02T11:40:37.590239: step 10440, loss 0.547729.
Train: 2018-08-02T11:40:37.808937: step 10441, loss 0.537123.
Train: 2018-08-02T11:40:38.027666: step 10442, loss 0.587706.
Train: 2018-08-02T11:40:38.261987: step 10443, loss 0.528417.
Train: 2018-08-02T11:40:38.496277: step 10444, loss 0.570847.
Train: 2018-08-02T11:40:38.746249: step 10445, loss 0.494135.
Train: 2018-08-02T11:40:38.964916: step 10446, loss 0.511029.
Train: 2018-08-02T11:40:39.183646: step 10447, loss 0.562306.
Train: 2018-08-02T11:40:39.417935: step 10448, loss 0.545041.
Train: 2018-08-02T11:40:39.667912: step 10449, loss 0.571116.
Train: 2018-08-02T11:40:39.886606: step 10450, loss 0.579575.
Test: 2018-08-02T11:40:41.089419: step 10450, loss 0.549632.
Train: 2018-08-02T11:40:41.308118: step 10451, loss 0.492618.
Train: 2018-08-02T11:40:41.542472: step 10452, loss 0.614893.
Train: 2018-08-02T11:40:41.776759: step 10453, loss 0.544827.
Train: 2018-08-02T11:40:42.011109: step 10454, loss 0.545127.
Train: 2018-08-02T11:40:42.229778: step 10455, loss 0.553569.
Train: 2018-08-02T11:40:42.448501: step 10456, loss 0.509688.
Train: 2018-08-02T11:40:42.667176: step 10457, loss 0.562685.
Train: 2018-08-02T11:40:42.901530: step 10458, loss 0.579753.
Train: 2018-08-02T11:40:43.135840: step 10459, loss 0.535968.
Train: 2018-08-02T11:40:43.354538: step 10460, loss 0.518217.
Test: 2018-08-02T11:40:44.557357: step 10460, loss 0.547523.
Train: 2018-08-02T11:40:44.760466: step 10461, loss 0.596705.
Train: 2018-08-02T11:40:44.994793: step 10462, loss 0.527941.
Train: 2018-08-02T11:40:45.229101: step 10463, loss 0.536204.
Train: 2018-08-02T11:40:45.432183: step 10464, loss 0.632356.
Train: 2018-08-02T11:40:45.650881: step 10465, loss 0.47425.
Train: 2018-08-02T11:40:45.869549: step 10466, loss 0.563485.
Train: 2018-08-02T11:40:46.103877: step 10467, loss 0.528193.
Train: 2018-08-02T11:40:46.322600: step 10468, loss 0.491395.
Train: 2018-08-02T11:40:46.556921: step 10469, loss 0.500805.
Train: 2018-08-02T11:40:46.775613: step 10470, loss 0.641795.
Test: 2018-08-02T11:40:47.978431: step 10470, loss 0.547089.
Train: 2018-08-02T11:40:48.197130: step 10471, loss 0.579435.
Train: 2018-08-02T11:40:48.431451: step 10472, loss 0.596961.
Train: 2018-08-02T11:40:48.650174: step 10473, loss 0.545357.
Train: 2018-08-02T11:40:48.868878: step 10474, loss 0.553839.
Train: 2018-08-02T11:40:49.087546: step 10475, loss 0.606429.
Train: 2018-08-02T11:40:49.306277: step 10476, loss 0.544836.
Train: 2018-08-02T11:40:49.540565: step 10477, loss 0.545174.
Train: 2018-08-02T11:40:49.759294: step 10478, loss 0.536531.
Train: 2018-08-02T11:40:49.993585: step 10479, loss 0.51089.
Train: 2018-08-02T11:40:50.227905: step 10480, loss 0.613774.
Test: 2018-08-02T11:40:51.415126: step 10480, loss 0.547099.
Train: 2018-08-02T11:40:51.633832: step 10481, loss 0.53666.
Train: 2018-08-02T11:40:51.868146: step 10482, loss 0.511208.
Train: 2018-08-02T11:40:52.102466: step 10483, loss 0.613417.
Train: 2018-08-02T11:40:52.321165: step 10484, loss 0.587839.
Train: 2018-08-02T11:40:52.539893: step 10485, loss 0.545414.
Train: 2018-08-02T11:40:52.774208: step 10486, loss 0.663881.
Train: 2018-08-02T11:40:53.008534: step 10487, loss 0.596064.
Train: 2018-08-02T11:40:53.242855: step 10488, loss 0.554015.
Train: 2018-08-02T11:40:53.461552: step 10489, loss 0.554075.
Train: 2018-08-02T11:40:53.680247: step 10490, loss 0.562446.
Test: 2018-08-02T11:40:54.883064: step 10490, loss 0.548632.
Train: 2018-08-02T11:40:55.101764: step 10491, loss 0.58733.
Train: 2018-08-02T11:40:55.320494: step 10492, loss 0.59552.
Train: 2018-08-02T11:40:55.539191: step 10493, loss 0.5461.
Train: 2018-08-02T11:40:55.757884: step 10494, loss 0.554383.
Train: 2018-08-02T11:40:55.976558: step 10495, loss 0.521775.
Train: 2018-08-02T11:40:56.210911: step 10496, loss 0.603382.
Train: 2018-08-02T11:40:56.429578: step 10497, loss 0.54639.
Train: 2018-08-02T11:40:56.663923: step 10498, loss 0.513981.
Train: 2018-08-02T11:40:56.882622: step 10499, loss 0.603236.
Train: 2018-08-02T11:40:57.101326: step 10500, loss 0.473599.
Test: 2018-08-02T11:40:58.304139: step 10500, loss 0.548773.
Train: 2018-08-02T11:40:59.178958: step 10501, loss 0.546472.
Train: 2018-08-02T11:40:59.413253: step 10502, loss 0.587023.
Train: 2018-08-02T11:40:59.647604: step 10503, loss 0.595152.
Train: 2018-08-02T11:40:59.866297: step 10504, loss 0.611386.
Train: 2018-08-02T11:41:00.085001: step 10505, loss 0.562684.
Train: 2018-08-02T11:41:00.334931: step 10506, loss 0.595093.
Train: 2018-08-02T11:41:00.569233: step 10507, loss 0.554638.
Train: 2018-08-02T11:41:00.787957: step 10508, loss 0.611173.
Train: 2018-08-02T11:41:01.006631: step 10509, loss 0.619141.
Train: 2018-08-02T11:41:01.225359: step 10510, loss 0.570844.
Test: 2018-08-02T11:41:02.428172: step 10510, loss 0.550243.
Train: 2018-08-02T11:41:02.646872: step 10511, loss 0.522872.
Train: 2018-08-02T11:41:02.865594: step 10512, loss 0.626765.
Train: 2018-08-02T11:41:03.084293: step 10513, loss 0.539063.
Train: 2018-08-02T11:41:03.302998: step 10514, loss 0.586801.
Train: 2018-08-02T11:41:03.537288: step 10515, loss 0.610554.
Train: 2018-08-02T11:41:03.755987: step 10516, loss 0.58676.
Train: 2018-08-02T11:41:04.004343: step 10517, loss 0.578861.
Train: 2018-08-02T11:41:04.238664: step 10518, loss 0.610265.
Train: 2018-08-02T11:41:04.457331: step 10519, loss 0.547605.
Train: 2018-08-02T11:41:04.676031: step 10520, loss 0.532114.
Test: 2018-08-02T11:41:05.878874: step 10520, loss 0.550504.
Train: 2018-08-02T11:41:06.128840: step 10521, loss 0.594479.
Train: 2018-08-02T11:41:06.347545: step 10522, loss 0.586673.
Train: 2018-08-02T11:41:06.581874: step 10523, loss 0.540125.
Train: 2018-08-02T11:41:06.784941: step 10524, loss 0.555636.
Train: 2018-08-02T11:41:07.019260: step 10525, loss 0.51688.
Train: 2018-08-02T11:41:07.237930: step 10526, loss 0.563306.
Train: 2018-08-02T11:41:07.456654: step 10527, loss 0.524349.
Train: 2018-08-02T11:41:07.690949: step 10528, loss 0.571778.
Train: 2018-08-02T11:41:07.894055: step 10529, loss 0.586428.
Train: 2018-08-02T11:41:08.128348: step 10530, loss 0.563381.
Test: 2018-08-02T11:41:09.331191: step 10530, loss 0.549352.
Train: 2018-08-02T11:41:09.549888: step 10531, loss 0.539606.
Train: 2018-08-02T11:41:09.768613: step 10532, loss 0.618159.
Train: 2018-08-02T11:41:09.987312: step 10533, loss 0.571355.
Train: 2018-08-02T11:41:10.206011: step 10534, loss 0.499995.
Train: 2018-08-02T11:41:10.424721: step 10535, loss 0.523062.
Train: 2018-08-02T11:41:10.674635: step 10536, loss 0.515429.
Train: 2018-08-02T11:41:10.893324: step 10537, loss 0.498527.
Train: 2018-08-02T11:41:11.112023: step 10538, loss 0.570637.
Train: 2018-08-02T11:41:11.330722: step 10539, loss 0.537837.
Train: 2018-08-02T11:41:11.565068: step 10540, loss 0.551801.
Test: 2018-08-02T11:41:12.752265: step 10540, loss 0.548391.
Train: 2018-08-02T11:41:12.986584: step 10541, loss 0.598271.
Train: 2018-08-02T11:41:13.205283: step 10542, loss 0.5302.
Train: 2018-08-02T11:41:13.439604: step 10543, loss 0.572225.
Train: 2018-08-02T11:41:13.658302: step 10544, loss 0.56025.
Train: 2018-08-02T11:41:13.892650: step 10545, loss 0.567274.
Train: 2018-08-02T11:41:14.111352: step 10546, loss 0.616627.
Train: 2018-08-02T11:41:14.330046: step 10547, loss 0.580427.
Train: 2018-08-02T11:41:14.548719: step 10548, loss 0.631217.
Train: 2018-08-02T11:41:14.767448: step 10549, loss 0.580712.
Train: 2018-08-02T11:41:15.001763: step 10550, loss 0.571642.
Test: 2018-08-02T11:41:16.204581: step 10550, loss 0.549205.
Train: 2018-08-02T11:41:16.407658: step 10551, loss 0.652633.
Train: 2018-08-02T11:41:16.626387: step 10552, loss 0.619046.
Train: 2018-08-02T11:41:16.860708: step 10553, loss 0.530992.
Train: 2018-08-02T11:41:17.094999: step 10554, loss 0.59467.
Train: 2018-08-02T11:41:17.313722: step 10555, loss 0.554993.
Train: 2018-08-02T11:41:17.532419: step 10556, loss 0.555284.
Train: 2018-08-02T11:41:17.751127: step 10557, loss 0.484393.
Train: 2018-08-02T11:41:17.969825: step 10558, loss 0.618205.
Train: 2018-08-02T11:41:18.204113: step 10559, loss 0.625914.
Train: 2018-08-02T11:41:18.438432: step 10560, loss 0.53981.
Test: 2018-08-02T11:41:19.625654: step 10560, loss 0.550088.
Train: 2018-08-02T11:41:19.844384: step 10561, loss 0.571093.
Train: 2018-08-02T11:41:20.063076: step 10562, loss 0.680111.
Train: 2018-08-02T11:41:20.281751: step 10563, loss 0.547913.
Train: 2018-08-02T11:41:20.531692: step 10564, loss 0.540306.
Train: 2018-08-02T11:41:20.766014: step 10565, loss 0.532687.
Train: 2018-08-02T11:41:20.984735: step 10566, loss 0.571247.
Train: 2018-08-02T11:41:21.219031: step 10567, loss 0.571255.
Train: 2018-08-02T11:41:21.437730: step 10568, loss 0.602048.
Train: 2018-08-02T11:41:21.687672: step 10569, loss 0.525139.
Train: 2018-08-02T11:41:21.859508: step 10570, loss 0.662042.
Test: 2018-08-02T11:41:23.046728: step 10570, loss 0.550444.
Train: 2018-08-02T11:41:23.281083: step 10571, loss 0.56362.
Train: 2018-08-02T11:41:23.515397: step 10572, loss 0.586649.
Train: 2018-08-02T11:41:23.734098: step 10573, loss 0.586648.
Train: 2018-08-02T11:41:23.952767: step 10574, loss 0.533162.
Train: 2018-08-02T11:41:24.187112: step 10575, loss 0.609562.
Train: 2018-08-02T11:41:24.405815: step 10576, loss 0.586648.
Train: 2018-08-02T11:41:24.624483: step 10577, loss 0.601886.
Train: 2018-08-02T11:41:24.843183: step 10578, loss 0.548623.
Train: 2018-08-02T11:41:25.061911: step 10579, loss 0.647432.
Train: 2018-08-02T11:41:25.296227: step 10580, loss 0.563916.
Test: 2018-08-02T11:41:26.499045: step 10580, loss 0.55168.
Train: 2018-08-02T11:41:26.733396: step 10581, loss 0.548838.
Train: 2018-08-02T11:41:26.967685: step 10582, loss 0.518629.
Train: 2018-08-02T11:41:27.202007: step 10583, loss 0.609365.
Train: 2018-08-02T11:41:27.405082: step 10584, loss 0.594253.
Train: 2018-08-02T11:41:27.639404: step 10585, loss 0.556438.
Train: 2018-08-02T11:41:27.858132: step 10586, loss 0.541265.
Train: 2018-08-02T11:41:28.092458: step 10587, loss 0.594216.
Train: 2018-08-02T11:41:28.311151: step 10588, loss 0.541172.
Train: 2018-08-02T11:41:28.529844: step 10589, loss 0.53354.
Train: 2018-08-02T11:41:28.764141: step 10590, loss 0.57129.
Test: 2018-08-02T11:41:29.966982: step 10590, loss 0.551782.
Train: 2018-08-02T11:41:30.170085: step 10591, loss 0.556174.
Train: 2018-08-02T11:41:30.404381: step 10592, loss 0.633149.
Train: 2018-08-02T11:41:30.638701: step 10593, loss 0.532897.
Train: 2018-08-02T11:41:30.841808: step 10594, loss 0.563544.
Train: 2018-08-02T11:41:31.060477: step 10595, loss 0.501853.
Train: 2018-08-02T11:41:31.279176: step 10596, loss 0.555683.
Train: 2018-08-02T11:41:31.529156: step 10597, loss 0.469986.
Train: 2018-08-02T11:41:31.732220: step 10598, loss 0.563555.
Train: 2018-08-02T11:41:31.950918: step 10599, loss 0.539675.
Train: 2018-08-02T11:41:32.169616: step 10600, loss 0.578967.
Test: 2018-08-02T11:41:33.372435: step 10600, loss 0.548947.
Train: 2018-08-02T11:41:34.184776: step 10601, loss 0.563242.
Train: 2018-08-02T11:41:34.419065: step 10602, loss 0.489348.
Train: 2018-08-02T11:41:34.653417: step 10603, loss 0.571379.
Train: 2018-08-02T11:41:34.872108: step 10604, loss 0.505537.
Train: 2018-08-02T11:41:35.090783: step 10605, loss 0.58835.
Train: 2018-08-02T11:41:35.325136: step 10606, loss 0.561673.
Train: 2018-08-02T11:41:35.559450: step 10607, loss 0.537369.
Train: 2018-08-02T11:41:35.809367: step 10608, loss 0.528274.
Train: 2018-08-02T11:41:36.043709: step 10609, loss 0.536727.
Train: 2018-08-02T11:41:36.262416: step 10610, loss 0.569276.
Test: 2018-08-02T11:41:37.465228: step 10610, loss 0.547395.
Train: 2018-08-02T11:41:37.699547: step 10611, loss 0.520462.
Train: 2018-08-02T11:41:37.918276: step 10612, loss 0.573049.
Train: 2018-08-02T11:41:38.183838: step 10613, loss 0.535696.
Train: 2018-08-02T11:41:38.418159: step 10614, loss 0.643328.
Train: 2018-08-02T11:41:38.652479: step 10615, loss 0.535794.
Train: 2018-08-02T11:41:38.902422: step 10616, loss 0.553785.
Train: 2018-08-02T11:41:39.152356: step 10617, loss 0.545789.
Train: 2018-08-02T11:41:39.386652: step 10618, loss 0.528176.
Train: 2018-08-02T11:41:39.620973: step 10619, loss 0.544883.
Train: 2018-08-02T11:41:39.855293: step 10620, loss 0.553458.
Test: 2018-08-02T11:41:41.042514: step 10620, loss 0.548459.
Train: 2018-08-02T11:41:41.308077: step 10621, loss 0.596653.
Train: 2018-08-02T11:41:41.558019: step 10622, loss 0.54655.
Train: 2018-08-02T11:41:41.792376: step 10623, loss 0.571425.
Train: 2018-08-02T11:41:42.026684: step 10624, loss 0.545547.
Train: 2018-08-02T11:41:42.261008: step 10625, loss 0.54561.
Train: 2018-08-02T11:41:42.510945: step 10626, loss 0.579098.
Train: 2018-08-02T11:41:42.745278: step 10627, loss 0.570759.
Train: 2018-08-02T11:41:42.995215: step 10628, loss 0.595934.
Train: 2018-08-02T11:41:43.245124: step 10629, loss 0.604231.
Train: 2018-08-02T11:41:43.479484: step 10630, loss 0.620816.
Test: 2018-08-02T11:41:44.682286: step 10630, loss 0.549719.
Train: 2018-08-02T11:41:44.901011: step 10631, loss 0.537514.
Train: 2018-08-02T11:41:45.135306: step 10632, loss 0.545935.
Train: 2018-08-02T11:41:45.369656: step 10633, loss 0.512881.
Train: 2018-08-02T11:41:45.619598: step 10634, loss 0.529461.
Train: 2018-08-02T11:41:45.885131: step 10635, loss 0.554242.
Train: 2018-08-02T11:41:46.135097: step 10636, loss 0.471674.
Train: 2018-08-02T11:41:46.385038: step 10637, loss 0.670037.
Train: 2018-08-02T11:41:46.619334: step 10638, loss 0.595556.
Train: 2018-08-02T11:41:46.869306: step 10639, loss 0.554247.
Train: 2018-08-02T11:41:47.119216: step 10640, loss 0.611977.
Test: 2018-08-02T11:41:48.306438: step 10640, loss 0.54842.
Train: 2018-08-02T11:41:48.540789: step 10641, loss 0.496739.
Train: 2018-08-02T11:41:48.775109: step 10642, loss 0.57076.
Train: 2018-08-02T11:41:49.040666: step 10643, loss 0.595399.
Train: 2018-08-02T11:41:49.290614: step 10644, loss 0.529763.
Train: 2018-08-02T11:41:49.524929: step 10645, loss 0.546178.
Train: 2018-08-02T11:41:49.774845: step 10646, loss 0.628116.
Train: 2018-08-02T11:41:50.024823: step 10647, loss 0.61166.
Train: 2018-08-02T11:41:50.290373: step 10648, loss 0.505517.
Train: 2018-08-02T11:41:50.509072: step 10649, loss 0.554475.
Train: 2018-08-02T11:41:50.743367: step 10650, loss 0.578928.
Test: 2018-08-02T11:41:51.946211: step 10650, loss 0.548499.
Train: 2018-08-02T11:41:52.196153: step 10651, loss 0.603324.
Train: 2018-08-02T11:41:52.446093: step 10652, loss 0.530179.
Train: 2018-08-02T11:41:52.680439: step 10653, loss 0.570785.
Train: 2018-08-02T11:41:52.914735: step 10654, loss 0.562681.
Train: 2018-08-02T11:41:53.149079: step 10655, loss 0.55463.
Train: 2018-08-02T11:41:53.414618: step 10656, loss 0.505979.
Train: 2018-08-02T11:41:53.648972: step 10657, loss 0.505882.
Train: 2018-08-02T11:41:53.883258: step 10658, loss 0.595366.
Train: 2018-08-02T11:41:54.133199: step 10659, loss 0.570698.
Train: 2018-08-02T11:41:54.351898: step 10660, loss 0.538104.
Test: 2018-08-02T11:41:55.554741: step 10660, loss 0.549096.
Train: 2018-08-02T11:41:55.804712: step 10661, loss 0.620226.
Train: 2018-08-02T11:41:56.039007: step 10662, loss 0.65242.
Train: 2018-08-02T11:41:56.273353: step 10663, loss 0.505683.
Train: 2018-08-02T11:41:56.523265: step 10664, loss 0.578923.
Train: 2018-08-02T11:41:56.788827: step 10665, loss 0.562666.
Train: 2018-08-02T11:41:57.038799: step 10666, loss 0.546466.
Train: 2018-08-02T11:41:57.273120: step 10667, loss 0.51406.
Train: 2018-08-02T11:41:57.538683: step 10668, loss 0.627448.
Train: 2018-08-02T11:41:57.788597: step 10669, loss 0.505968.
Train: 2018-08-02T11:41:58.038559: step 10670, loss 0.538564.
Test: 2018-08-02T11:41:59.241377: step 10670, loss 0.547775.
Train: 2018-08-02T11:41:59.460077: step 10671, loss 0.432592.
Train: 2018-08-02T11:41:59.710018: step 10672, loss 0.579023.
Train: 2018-08-02T11:41:59.959961: step 10673, loss 0.529889.
Train: 2018-08-02T11:42:00.225523: step 10674, loss 0.59544.
Train: 2018-08-02T11:42:00.475465: step 10675, loss 0.529479.
Train: 2018-08-02T11:42:00.709784: step 10676, loss 0.512956.
Train: 2018-08-02T11:42:00.959726: step 10677, loss 0.520972.
Train: 2018-08-02T11:42:01.225314: step 10678, loss 0.604074.
Train: 2018-08-02T11:42:01.475254: step 10679, loss 0.55422.
Train: 2018-08-02T11:42:01.725184: step 10680, loss 0.604632.
Test: 2018-08-02T11:42:02.912393: step 10680, loss 0.547994.
Train: 2018-08-02T11:42:03.146739: step 10681, loss 0.553961.
Train: 2018-08-02T11:42:03.396654: step 10682, loss 0.6127.
Train: 2018-08-02T11:42:03.646621: step 10683, loss 0.545609.
Train: 2018-08-02T11:42:03.896587: step 10684, loss 0.545638.
Train: 2018-08-02T11:42:04.115236: step 10685, loss 0.595924.
Train: 2018-08-02T11:42:04.365177: step 10686, loss 0.562409.
Train: 2018-08-02T11:42:04.599499: step 10687, loss 0.620973.
Train: 2018-08-02T11:42:04.849441: step 10688, loss 0.554057.
Train: 2018-08-02T11:42:05.099381: step 10689, loss 0.59579.
Train: 2018-08-02T11:42:05.333701: step 10690, loss 0.545787.
Test: 2018-08-02T11:42:06.520923: step 10690, loss 0.548207.
Train: 2018-08-02T11:42:06.770895: step 10691, loss 0.529228.
Train: 2018-08-02T11:42:06.989594: step 10692, loss 0.653719.
Train: 2018-08-02T11:42:07.223908: step 10693, loss 0.645361.
Train: 2018-08-02T11:42:07.458234: step 10694, loss 0.587193.
Train: 2018-08-02T11:42:07.723767: step 10695, loss 0.529878.
Train: 2018-08-02T11:42:07.973738: step 10696, loss 0.58707.
Train: 2018-08-02T11:42:08.239301: step 10697, loss 0.562677.
Train: 2018-08-02T11:42:08.489243: step 10698, loss 0.514213.
Train: 2018-08-02T11:42:08.739154: step 10699, loss 0.562747.
Train: 2018-08-02T11:42:08.989098: step 10700, loss 0.578877.
Test: 2018-08-02T11:42:10.191938: step 10700, loss 0.549187.
Train: 2018-08-02T11:42:11.113598: step 10701, loss 0.594948.
Train: 2018-08-02T11:42:11.363550: step 10702, loss 0.586885.
Train: 2018-08-02T11:42:11.597890: step 10703, loss 0.666825.
Train: 2018-08-02T11:42:11.863452: step 10704, loss 0.554986.
Train: 2018-08-02T11:42:12.113390: step 10705, loss 0.523385.
Train: 2018-08-02T11:42:12.378954: step 10706, loss 0.563052.
Train: 2018-08-02T11:42:12.628896: step 10707, loss 0.641965.
Train: 2018-08-02T11:42:12.863212: step 10708, loss 0.531721.
Train: 2018-08-02T11:42:13.081912: step 10709, loss 0.594555.
Train: 2018-08-02T11:42:13.347474: step 10710, loss 0.508523.
Test: 2018-08-02T11:42:14.550292: step 10710, loss 0.54968.
Train: 2018-08-02T11:42:14.769016: step 10711, loss 0.539831.
Train: 2018-08-02T11:42:15.003341: step 10712, loss 0.524199.
Train: 2018-08-02T11:42:15.268875: step 10713, loss 0.531931.
Train: 2018-08-02T11:42:15.550059: step 10714, loss 0.578846.
Train: 2018-08-02T11:42:15.784410: step 10715, loss 0.571112.
Train: 2018-08-02T11:42:16.018725: step 10716, loss 0.610349.
Train: 2018-08-02T11:42:16.268675: step 10717, loss 0.523719.
Train: 2018-08-02T11:42:16.502991: step 10718, loss 0.484114.
Train: 2018-08-02T11:42:16.752903: step 10719, loss 0.523191.
Train: 2018-08-02T11:42:17.002844: step 10720, loss 0.577915.
Test: 2018-08-02T11:42:18.190065: step 10720, loss 0.548266.
Train: 2018-08-02T11:42:18.393167: step 10721, loss 0.597005.
Train: 2018-08-02T11:42:18.643084: step 10722, loss 0.521407.
Train: 2018-08-02T11:42:18.877429: step 10723, loss 0.544007.
Train: 2018-08-02T11:42:19.111726: step 10724, loss 0.604604.
Train: 2018-08-02T11:42:19.361667: step 10725, loss 0.58141.
Train: 2018-08-02T11:42:19.611638: step 10726, loss 0.585649.
Train: 2018-08-02T11:42:19.861584: step 10727, loss 0.555878.
Train: 2018-08-02T11:42:20.095907: step 10728, loss 0.563307.
Train: 2018-08-02T11:42:20.345842: step 10729, loss 0.445136.
Train: 2018-08-02T11:42:20.595777: step 10730, loss 0.493263.
Test: 2018-08-02T11:42:21.782974: step 10730, loss 0.548271.
Train: 2018-08-02T11:42:22.017294: step 10731, loss 0.651302.
Train: 2018-08-02T11:42:22.267260: step 10732, loss 0.598968.
Train: 2018-08-02T11:42:22.485964: step 10733, loss 0.657392.
Train: 2018-08-02T11:42:22.735876: step 10734, loss 0.53656.
Train: 2018-08-02T11:42:22.985817: step 10735, loss 0.587219.
Train: 2018-08-02T11:42:23.220139: step 10736, loss 0.594813.
Train: 2018-08-02T11:42:23.470104: step 10737, loss 0.56274.
Train: 2018-08-02T11:42:23.704426: step 10738, loss 0.594884.
Train: 2018-08-02T11:42:23.954365: step 10739, loss 0.522528.
Train: 2018-08-02T11:42:24.219905: step 10740, loss 0.594872.
Test: 2018-08-02T11:42:25.422747: step 10740, loss 0.549208.
Train: 2018-08-02T11:42:25.672688: step 10741, loss 0.52305.
Train: 2018-08-02T11:42:25.922662: step 10742, loss 0.507113.
Train: 2018-08-02T11:42:26.172602: step 10743, loss 0.56289.
Train: 2018-08-02T11:42:26.422543: step 10744, loss 0.562915.
Train: 2018-08-02T11:42:26.688100: step 10745, loss 0.554919.
Train: 2018-08-02T11:42:26.906804: step 10746, loss 0.634785.
Train: 2018-08-02T11:42:27.156722: step 10747, loss 0.602812.
Train: 2018-08-02T11:42:27.406657: step 10748, loss 0.570888.
Train: 2018-08-02T11:42:27.734706: step 10749, loss 0.539056.
Train: 2018-08-02T11:42:27.984676: step 10750, loss 0.578859.
Test: 2018-08-02T11:42:29.171869: step 10750, loss 0.55023.
Train: 2018-08-02T11:42:29.421856: step 10751, loss 0.555008.
Train: 2018-08-02T11:42:29.671752: step 10752, loss 0.594753.
Train: 2018-08-02T11:42:29.921719: step 10753, loss 0.602677.
Train: 2018-08-02T11:42:30.156016: step 10754, loss 0.507526.
Train: 2018-08-02T11:42:30.405991: step 10755, loss 0.555082.
Train: 2018-08-02T11:42:30.671517: step 10756, loss 0.555073.
Train: 2018-08-02T11:42:30.921492: step 10757, loss 0.586792.
Train: 2018-08-02T11:42:31.171426: step 10758, loss 0.602662.
Train: 2018-08-02T11:42:31.421367: step 10759, loss 0.610575.
Train: 2018-08-02T11:42:31.671284: step 10760, loss 0.578858.
Test: 2018-08-02T11:42:32.874126: step 10760, loss 0.549861.
Train: 2018-08-02T11:42:33.108447: step 10761, loss 0.531451.
Train: 2018-08-02T11:42:33.358414: step 10762, loss 0.531479.
Train: 2018-08-02T11:42:33.608361: step 10763, loss 0.586759.
Train: 2018-08-02T11:42:33.842680: step 10764, loss 0.594672.
Train: 2018-08-02T11:42:34.092622: step 10765, loss 0.515708.
Train: 2018-08-02T11:42:34.326912: step 10766, loss 0.55515.
Train: 2018-08-02T11:42:34.576852: step 10767, loss 0.578877.
Train: 2018-08-02T11:42:34.811199: step 10768, loss 0.626347.
Train: 2018-08-02T11:42:35.061139: step 10769, loss 0.539318.
Train: 2018-08-02T11:42:35.311087: step 10770, loss 0.531415.
Test: 2018-08-02T11:42:36.498279: step 10770, loss 0.550537.
Train: 2018-08-02T11:42:36.748244: step 10771, loss 0.563029.
Train: 2018-08-02T11:42:36.998162: step 10772, loss 0.602631.
Train: 2018-08-02T11:42:37.248102: step 10773, loss 0.586785.
Train: 2018-08-02T11:42:37.513683: step 10774, loss 0.547175.
Train: 2018-08-02T11:42:37.748014: step 10775, loss 0.555088.
Train: 2018-08-02T11:42:37.997928: step 10776, loss 0.547148.
Train: 2018-08-02T11:42:38.232248: step 10777, loss 0.602667.
Train: 2018-08-02T11:42:38.466598: step 10778, loss 0.562984.
Train: 2018-08-02T11:42:38.716509: step 10779, loss 0.547104.
Train: 2018-08-02T11:42:38.966480: step 10780, loss 0.539135.
Test: 2018-08-02T11:42:40.169293: step 10780, loss 0.549293.
Train: 2018-08-02T11:42:40.387993: step 10781, loss 0.5709.
Train: 2018-08-02T11:42:40.637934: step 10782, loss 0.59479.
Train: 2018-08-02T11:42:40.887876: step 10783, loss 0.586838.
Train: 2018-08-02T11:42:41.122228: step 10784, loss 0.57089.
Train: 2018-08-02T11:42:41.372136: step 10785, loss 0.531061.
Train: 2018-08-02T11:42:41.606487: step 10786, loss 0.57886.
Train: 2018-08-02T11:42:41.856405: step 10787, loss 0.57886.
Train: 2018-08-02T11:42:42.106365: step 10788, loss 0.515048.
Train: 2018-08-02T11:42:42.356312: step 10789, loss 0.538924.
Train: 2018-08-02T11:42:42.606255: step 10790, loss 0.546852.
Test: 2018-08-02T11:42:43.809065: step 10790, loss 0.548322.
Train: 2018-08-02T11:42:44.043387: step 10791, loss 0.554805.
Train: 2018-08-02T11:42:44.308975: step 10792, loss 0.586912.
Train: 2018-08-02T11:42:44.558921: step 10793, loss 0.522513.
Train: 2018-08-02T11:42:44.793211: step 10794, loss 0.578883.
Train: 2018-08-02T11:42:45.058803: step 10795, loss 0.514197.
Train: 2018-08-02T11:42:45.308748: step 10796, loss 0.530239.
Train: 2018-08-02T11:42:45.558687: step 10797, loss 0.538233.
Train: 2018-08-02T11:42:45.808629: step 10798, loss 0.546268.
Train: 2018-08-02T11:42:46.058540: step 10799, loss 0.554354.
Train: 2018-08-02T11:42:46.308482: step 10800, loss 0.546074.
Test: 2018-08-02T11:42:47.495702: step 10800, loss 0.549844.
Train: 2018-08-02T11:42:48.370505: step 10801, loss 0.529471.
Train: 2018-08-02T11:42:48.620472: step 10802, loss 0.579018.
Train: 2018-08-02T11:42:48.854790: step 10803, loss 0.554191.
Train: 2018-08-02T11:42:49.089110: step 10804, loss 0.545633.
Train: 2018-08-02T11:42:49.339051: step 10805, loss 0.553822.
Train: 2018-08-02T11:42:49.588994: step 10806, loss 0.57886.
Train: 2018-08-02T11:42:49.838929: step 10807, loss 0.570889.
Train: 2018-08-02T11:42:50.088846: step 10808, loss 0.536822.
Train: 2018-08-02T11:42:50.338787: step 10809, loss 0.588226.
Train: 2018-08-02T11:42:50.573133: step 10810, loss 0.581504.
Test: 2018-08-02T11:42:51.760329: step 10810, loss 0.548493.
Train: 2018-08-02T11:42:52.010295: step 10811, loss 0.544456.
Train: 2018-08-02T11:42:52.275857: step 10812, loss 0.579511.
Train: 2018-08-02T11:42:52.525775: step 10813, loss 0.502787.
Train: 2018-08-02T11:42:52.775716: step 10814, loss 0.647373.
Train: 2018-08-02T11:42:53.025690: step 10815, loss 0.503302.
Train: 2018-08-02T11:42:53.260008: step 10816, loss 0.570923.
Train: 2018-08-02T11:42:53.494299: step 10817, loss 0.612614.
Train: 2018-08-02T11:42:53.744239: step 10818, loss 0.579196.
Train: 2018-08-02T11:42:53.978561: step 10819, loss 0.512299.
Train: 2018-08-02T11:42:54.228534: step 10820, loss 0.595784.
Test: 2018-08-02T11:42:55.415722: step 10820, loss 0.548147.
Train: 2018-08-02T11:42:55.665667: step 10821, loss 0.504145.
Train: 2018-08-02T11:42:55.915606: step 10822, loss 0.595732.
Train: 2018-08-02T11:42:56.165549: step 10823, loss 0.579074.
Train: 2018-08-02T11:42:56.431134: step 10824, loss 0.520957.
Train: 2018-08-02T11:42:56.681105: step 10825, loss 0.58735.
Train: 2018-08-02T11:42:56.931024: step 10826, loss 0.579044.
Train: 2018-08-02T11:42:57.180934: step 10827, loss 0.562482.
Train: 2018-08-02T11:42:57.430901: step 10828, loss 0.496395.
Train: 2018-08-02T11:42:57.680842: step 10829, loss 0.60381.
Train: 2018-08-02T11:42:57.930794: step 10830, loss 0.463415.
Test: 2018-08-02T11:42:59.117981: step 10830, loss 0.550008.
Train: 2018-08-02T11:42:59.367953: step 10831, loss 0.587296.
Train: 2018-08-02T11:42:59.617888: step 10832, loss 0.61213.
Train: 2018-08-02T11:42:59.883456: step 10833, loss 0.595578.
Train: 2018-08-02T11:43:00.133404: step 10834, loss 0.521203.
Train: 2018-08-02T11:43:00.383335: step 10835, loss 0.628536.
Train: 2018-08-02T11:43:00.633277: step 10836, loss 0.570757.
Train: 2018-08-02T11:43:00.883223: step 10837, loss 0.578982.
Train: 2018-08-02T11:43:01.133159: step 10838, loss 0.570765.
Train: 2018-08-02T11:43:01.367455: step 10839, loss 0.505284.
Train: 2018-08-02T11:43:01.633018: step 10840, loss 0.578945.
Test: 2018-08-02T11:43:02.835860: step 10840, loss 0.549139.
Train: 2018-08-02T11:43:03.070182: step 10841, loss 0.57894.
Train: 2018-08-02T11:43:03.320153: step 10842, loss 0.562612.
Train: 2018-08-02T11:43:03.554469: step 10843, loss 0.578926.
Train: 2018-08-02T11:43:03.804385: step 10844, loss 0.521945.
Train: 2018-08-02T11:43:04.054349: step 10845, loss 0.562644.
Train: 2018-08-02T11:43:04.304269: step 10846, loss 0.50571.
Train: 2018-08-02T11:43:04.554209: step 10847, loss 0.578922.
Train: 2018-08-02T11:43:04.804175: step 10848, loss 0.521885.
Train: 2018-08-02T11:43:05.069712: step 10849, loss 0.595256.
Train: 2018-08-02T11:43:05.319679: step 10850, loss 0.562607.
Test: 2018-08-02T11:43:06.506875: step 10850, loss 0.548766.
Train: 2018-08-02T11:43:06.741201: step 10851, loss 0.505407.
Train: 2018-08-02T11:43:07.006783: step 10852, loss 0.56258.
Train: 2018-08-02T11:43:07.272352: step 10853, loss 0.521572.
Train: 2018-08-02T11:43:07.522294: step 10854, loss 0.59541.
Train: 2018-08-02T11:43:07.756613: step 10855, loss 0.628358.
Train: 2018-08-02T11:43:08.006555: step 10856, loss 0.56252.
Train: 2018-08-02T11:43:08.240878: step 10857, loss 0.570759.
Train: 2018-08-02T11:43:08.490819: step 10858, loss 0.529662.
Train: 2018-08-02T11:43:08.725137: step 10859, loss 0.554326.
Train: 2018-08-02T11:43:08.959427: step 10860, loss 0.611896.
Test: 2018-08-02T11:43:10.162270: step 10860, loss 0.548819.
Train: 2018-08-02T11:43:10.396616: step 10861, loss 0.496817.
Train: 2018-08-02T11:43:10.646562: step 10862, loss 0.636552.
Train: 2018-08-02T11:43:10.896473: step 10863, loss 0.546111.
Train: 2018-08-02T11:43:11.130819: step 10864, loss 0.480505.
Train: 2018-08-02T11:43:11.380760: step 10865, loss 0.51325.
Train: 2018-08-02T11:43:11.630677: step 10866, loss 0.52135.
Train: 2018-08-02T11:43:11.880655: step 10867, loss 0.545979.
Train: 2018-08-02T11:43:12.114966: step 10868, loss 0.52934.
Train: 2018-08-02T11:43:12.364880: step 10869, loss 0.620619.
Train: 2018-08-02T11:43:12.599199: step 10870, loss 0.570775.
Test: 2018-08-02T11:43:13.802048: step 10870, loss 0.550428.
Train: 2018-08-02T11:43:14.052008: step 10871, loss 0.595772.
Train: 2018-08-02T11:43:14.255091: step 10872, loss 0.491285.
Train: 2018-08-02T11:43:14.489413: step 10873, loss 0.587457.
Train: 2018-08-02T11:43:14.739323: step 10874, loss 0.554077.
Train: 2018-08-02T11:43:15.036154: step 10875, loss 0.537334.
Train: 2018-08-02T11:43:15.286070: step 10876, loss 0.62936.
Train: 2018-08-02T11:43:15.520421: step 10877, loss 0.562389.
Train: 2018-08-02T11:43:15.754736: step 10878, loss 0.604167.
Train: 2018-08-02T11:43:16.020303: step 10879, loss 0.620814.
Train: 2018-08-02T11:43:16.270216: step 10880, loss 0.579035.
Test: 2018-08-02T11:43:17.457436: step 10880, loss 0.54937.
Train: 2018-08-02T11:43:17.707379: step 10881, loss 0.620505.
Train: 2018-08-02T11:43:17.957333: step 10882, loss 0.595608.
Train: 2018-08-02T11:43:18.222883: step 10883, loss 0.578918.
Train: 2018-08-02T11:43:18.472854: step 10884, loss 0.546242.
Train: 2018-08-02T11:43:18.722766: step 10885, loss 0.521909.
Train: 2018-08-02T11:43:18.972732: step 10886, loss 0.554487.
Train: 2018-08-02T11:43:19.222678: step 10887, loss 0.497804.
Train: 2018-08-02T11:43:19.472620: step 10888, loss 0.570768.
Train: 2018-08-02T11:43:19.722556: step 10889, loss 0.505919.
Train: 2018-08-02T11:43:19.972503: step 10890, loss 0.60324.
Test: 2018-08-02T11:43:21.175316: step 10890, loss 0.549285.
Train: 2018-08-02T11:43:21.409636: step 10891, loss 0.619525.
Train: 2018-08-02T11:43:21.659576: step 10892, loss 0.497806.
Train: 2018-08-02T11:43:21.909519: step 10893, loss 0.513954.
Train: 2018-08-02T11:43:22.143869: step 10894, loss 0.546449.
Train: 2018-08-02T11:43:22.378160: step 10895, loss 0.636132.
Train: 2018-08-02T11:43:22.643747: step 10896, loss 0.505551.
Train: 2018-08-02T11:43:22.893688: step 10897, loss 0.554577.
Train: 2018-08-02T11:43:23.143636: step 10898, loss 0.652288.
Train: 2018-08-02T11:43:23.377955: step 10899, loss 0.644321.
Train: 2018-08-02T11:43:23.643518: step 10900, loss 0.513932.
Test: 2018-08-02T11:43:24.815090: step 10900, loss 0.548949.
Train: 2018-08-02T11:43:25.705529: step 10901, loss 0.570876.
Train: 2018-08-02T11:43:25.955471: step 10902, loss 0.586985.
Train: 2018-08-02T11:43:26.205419: step 10903, loss 0.627366.
Train: 2018-08-02T11:43:26.455355: step 10904, loss 0.570824.
Train: 2018-08-02T11:43:26.705304: step 10905, loss 0.57084.
Train: 2018-08-02T11:43:26.955237: step 10906, loss 0.554845.
Train: 2018-08-02T11:43:27.189535: step 10907, loss 0.570872.
Train: 2018-08-02T11:43:27.455126: step 10908, loss 0.523054.
Train: 2018-08-02T11:43:27.689416: step 10909, loss 0.594792.
Train: 2018-08-02T11:43:27.954978: step 10910, loss 0.570903.
Test: 2018-08-02T11:43:29.142200: step 10910, loss 0.549151.
Train: 2018-08-02T11:43:29.392175: step 10911, loss 0.499407.
Train: 2018-08-02T11:43:29.642131: step 10912, loss 0.586808.
Train: 2018-08-02T11:43:29.892055: step 10913, loss 0.547059.
Train: 2018-08-02T11:43:30.141991: step 10914, loss 0.602722.
Train: 2018-08-02T11:43:30.407529: step 10915, loss 0.499343.
Train: 2018-08-02T11:43:30.657500: step 10916, loss 0.539046.
Train: 2018-08-02T11:43:30.907411: step 10917, loss 0.538968.
Train: 2018-08-02T11:43:31.157383: step 10918, loss 0.562867.
Train: 2018-08-02T11:43:31.407320: step 10919, loss 0.602917.
Train: 2018-08-02T11:43:31.672885: step 10920, loss 0.538737.
Test: 2018-08-02T11:43:32.844458: step 10920, loss 0.548993.
Train: 2018-08-02T11:43:33.094430: step 10921, loss 0.53867.
Train: 2018-08-02T11:43:33.328750: step 10922, loss 0.562762.
Train: 2018-08-02T11:43:33.578670: step 10923, loss 0.522359.
Train: 2018-08-02T11:43:33.828633: step 10924, loss 0.538404.
Train: 2018-08-02T11:43:34.078574: step 10925, loss 0.546411.
Train: 2018-08-02T11:43:34.328517: step 10926, loss 0.554472.
Train: 2018-08-02T11:43:34.578428: step 10927, loss 0.488996.
Train: 2018-08-02T11:43:34.828394: step 10928, loss 0.562545.
Train: 2018-08-02T11:43:35.062690: step 10929, loss 0.562508.
Train: 2018-08-02T11:43:35.312660: step 10930, loss 0.587315.
Test: 2018-08-02T11:43:36.515473: step 10930, loss 0.549934.
Train: 2018-08-02T11:43:36.749796: step 10931, loss 0.562455.
Train: 2018-08-02T11:43:37.031009: step 10932, loss 0.537469.
Train: 2018-08-02T11:43:37.280950: step 10933, loss 0.512352.
Train: 2018-08-02T11:43:37.515272: step 10934, loss 0.528875.
Train: 2018-08-02T11:43:37.765211: step 10935, loss 0.537182.
Train: 2018-08-02T11:43:38.015124: step 10936, loss 0.562265.
Train: 2018-08-02T11:43:38.265095: step 10937, loss 0.519304.
Train: 2018-08-02T11:43:38.515036: step 10938, loss 0.484437.
Train: 2018-08-02T11:43:38.764971: step 10939, loss 0.589523.
Train: 2018-08-02T11:43:39.014919: step 10940, loss 0.572129.
Test: 2018-08-02T11:43:40.202111: step 10940, loss 0.547885.
Train: 2018-08-02T11:43:40.452083: step 10941, loss 0.624346.
Train: 2018-08-02T11:43:40.686403: step 10942, loss 0.562747.
Train: 2018-08-02T11:43:40.936345: step 10943, loss 0.536287.
Train: 2018-08-02T11:43:41.170664: step 10944, loss 0.58022.
Train: 2018-08-02T11:43:41.420574: step 10945, loss 0.60555.
Train: 2018-08-02T11:43:41.654922: step 10946, loss 0.553864.
Train: 2018-08-02T11:43:41.904866: step 10947, loss 0.51971.
Train: 2018-08-02T11:43:42.154803: step 10948, loss 0.519738.
Train: 2018-08-02T11:43:42.404719: step 10949, loss 0.621975.
Train: 2018-08-02T11:43:42.670309: step 10950, loss 0.485753.
Test: 2018-08-02T11:43:43.857504: step 10950, loss 0.548907.
Train: 2018-08-02T11:43:44.091849: step 10951, loss 0.562373.
Train: 2018-08-02T11:43:44.341766: step 10952, loss 0.528277.
Train: 2018-08-02T11:43:44.591709: step 10953, loss 0.604856.
Train: 2018-08-02T11:43:44.857283: step 10954, loss 0.545295.
Train: 2018-08-02T11:43:45.075994: step 10955, loss 0.485437.
Train: 2018-08-02T11:43:45.341556: step 10956, loss 0.545239.
Train: 2018-08-02T11:43:45.591497: step 10957, loss 0.572354.
Train: 2018-08-02T11:43:45.841415: step 10958, loss 0.562608.
Train: 2018-08-02T11:43:46.091382: step 10959, loss 0.570637.
Train: 2018-08-02T11:43:46.325703: step 10960, loss 0.630778.
Test: 2018-08-02T11:43:47.528519: step 10960, loss 0.547537.
Train: 2018-08-02T11:43:47.747248: step 10961, loss 0.536795.
Train: 2018-08-02T11:43:47.997160: step 10962, loss 0.579367.
Train: 2018-08-02T11:43:48.231505: step 10963, loss 0.604701.
Train: 2018-08-02T11:43:48.465825: step 10964, loss 0.65526.
Train: 2018-08-02T11:43:48.715742: step 10965, loss 0.528766.
Train: 2018-08-02T11:43:48.965713: step 10966, loss 0.595895.
Train: 2018-08-02T11:43:49.200028: step 10967, loss 0.570765.
Train: 2018-08-02T11:43:49.434354: step 10968, loss 0.545852.
Train: 2018-08-02T11:43:49.699890: step 10969, loss 0.554209.
Train: 2018-08-02T11:43:49.934237: step 10970, loss 0.513018.
Test: 2018-08-02T11:43:51.137049: step 10970, loss 0.548012.
Train: 2018-08-02T11:43:51.355774: step 10971, loss 0.537818.
Train: 2018-08-02T11:43:51.605714: step 10972, loss 0.636573.
Train: 2018-08-02T11:43:51.840036: step 10973, loss 0.554351.
Train: 2018-08-02T11:43:52.074361: step 10974, loss 0.52983.
Train: 2018-08-02T11:43:52.308686: step 10975, loss 0.619827.
Train: 2018-08-02T11:43:52.558622: step 10976, loss 0.538149.
Train: 2018-08-02T11:43:52.792942: step 10977, loss 0.530065.
Train: 2018-08-02T11:43:53.027233: step 10978, loss 0.513824.
Train: 2018-08-02T11:43:53.261554: step 10979, loss 0.587061.
Train: 2018-08-02T11:43:53.511493: step 10980, loss 0.587059.
Test: 2018-08-02T11:43:54.698714: step 10980, loss 0.549622.
Train: 2018-08-02T11:43:54.917446: step 10981, loss 0.521968.
Train: 2018-08-02T11:43:55.151765: step 10982, loss 0.538227.
Train: 2018-08-02T11:43:55.386055: step 10983, loss 0.627809.
Train: 2018-08-02T11:43:55.620401: step 10984, loss 0.570786.
Train: 2018-08-02T11:43:55.854723: step 10985, loss 0.52199.
Train: 2018-08-02T11:43:56.089015: step 10986, loss 0.611508.
Train: 2018-08-02T11:43:56.323366: step 10987, loss 0.554517.
Train: 2018-08-02T11:43:56.557655: step 10988, loss 0.513934.
Train: 2018-08-02T11:43:56.792001: step 10989, loss 0.578916.
Train: 2018-08-02T11:43:57.041917: step 10990, loss 0.538267.
Test: 2018-08-02T11:43:58.229138: step 10990, loss 0.550365.
Train: 2018-08-02T11:43:58.447838: step 10991, loss 0.570781.
Train: 2018-08-02T11:43:58.682158: step 10992, loss 0.530079.
Train: 2018-08-02T11:43:58.916504: step 10993, loss 0.521873.
Train: 2018-08-02T11:43:59.150828: step 10994, loss 0.603449.
Train: 2018-08-02T11:43:59.385119: step 10995, loss 0.521714.
Train: 2018-08-02T11:43:59.619466: step 10996, loss 0.603528.
Train: 2018-08-02T11:43:59.853789: step 10997, loss 0.595356.
Train: 2018-08-02T11:44:00.088103: step 10998, loss 0.578938.
Train: 2018-08-02T11:44:00.322430: step 10999, loss 0.578966.
Train: 2018-08-02T11:44:00.572371: step 11000, loss 0.546229.
Test: 2018-08-02T11:44:01.759561: step 11000, loss 0.548189.
Train: 2018-08-02T11:44:02.603147: step 11001, loss 0.595298.
Train: 2018-08-02T11:44:02.837460: step 11002, loss 0.595257.
Train: 2018-08-02T11:44:03.056164: step 11003, loss 0.660391.
Train: 2018-08-02T11:44:03.290455: step 11004, loss 0.546447.
Train: 2018-08-02T11:44:03.524775: step 11005, loss 0.554629.
Train: 2018-08-02T11:44:03.759119: step 11006, loss 0.530491.
Train: 2018-08-02T11:44:03.993417: step 11007, loss 0.643293.
Train: 2018-08-02T11:44:04.212143: step 11008, loss 0.554793.
Train: 2018-08-02T11:44:04.457974: step 11009, loss 0.554854.
Train: 2018-08-02T11:44:04.676672: step 11010, loss 0.499.
Test: 2018-08-02T11:44:05.879489: step 11010, loss 0.549191.
Train: 2018-08-02T11:44:06.098185: step 11011, loss 0.562893.
Train: 2018-08-02T11:44:06.316913: step 11012, loss 0.578861.
Train: 2018-08-02T11:44:06.566856: step 11013, loss 0.55492.
Train: 2018-08-02T11:44:06.801146: step 11014, loss 0.546941.
Train: 2018-08-02T11:44:07.035489: step 11015, loss 0.538943.
Train: 2018-08-02T11:44:07.238572: step 11016, loss 0.554884.
Train: 2018-08-02T11:44:07.488507: step 11017, loss 0.59487.
Train: 2018-08-02T11:44:07.722805: step 11018, loss 0.546836.
Train: 2018-08-02T11:44:07.941501: step 11019, loss 0.546807.
Train: 2018-08-02T11:44:08.160235: step 11020, loss 0.59492.
Test: 2018-08-02T11:44:09.363045: step 11020, loss 0.549771.
Train: 2018-08-02T11:44:09.597366: step 11021, loss 0.506605.
Train: 2018-08-02T11:44:09.831685: step 11022, loss 0.562785.
Train: 2018-08-02T11:44:10.019140: step 11023, loss 0.511183.
Train: 2018-08-02T11:44:10.253487: step 11024, loss 0.586971.
Train: 2018-08-02T11:44:10.472166: step 11025, loss 0.611295.
Train: 2018-08-02T11:44:10.706511: step 11026, loss 0.538372.
Train: 2018-08-02T11:44:10.925179: step 11027, loss 0.538326.
Train: 2018-08-02T11:44:11.175120: step 11028, loss 0.595173.
Train: 2018-08-02T11:44:11.409465: step 11029, loss 0.627737.
Train: 2018-08-02T11:44:11.628169: step 11030, loss 0.570783.
Test: 2018-08-02T11:44:12.830988: step 11030, loss 0.548163.
Train: 2018-08-02T11:44:13.034084: step 11031, loss 0.578909.
Train: 2018-08-02T11:44:13.268414: step 11032, loss 0.554565.
Train: 2018-08-02T11:44:13.518350: step 11033, loss 0.562688.
Train: 2018-08-02T11:44:13.737044: step 11034, loss 0.603197.
Train: 2018-08-02T11:44:13.955719: step 11035, loss 0.55463.
Train: 2018-08-02T11:44:14.174419: step 11036, loss 0.603118.
Train: 2018-08-02T11:44:14.408778: step 11037, loss 0.562758.
Train: 2018-08-02T11:44:14.627468: step 11038, loss 0.635202.
Train: 2018-08-02T11:44:14.846161: step 11039, loss 0.522726.
Train: 2018-08-02T11:44:15.080481: step 11040, loss 0.554849.
Test: 2018-08-02T11:44:16.267677: step 11040, loss 0.549709.
Train: 2018-08-02T11:44:16.486380: step 11041, loss 0.546888.
Train: 2018-08-02T11:44:16.720729: step 11042, loss 0.530936.
Train: 2018-08-02T11:44:16.955023: step 11043, loss 0.59484.
Train: 2018-08-02T11:44:17.173745: step 11044, loss 0.483039.
Train: 2018-08-02T11:44:17.392439: step 11045, loss 0.586861.
Train: 2018-08-02T11:44:17.626735: step 11046, loss 0.530827.
Train: 2018-08-02T11:44:17.845458: step 11047, loss 0.530744.
Train: 2018-08-02T11:44:18.064162: step 11048, loss 0.514549.
Train: 2018-08-02T11:44:18.282831: step 11049, loss 0.603088.
Train: 2018-08-02T11:44:18.517180: step 11050, loss 0.546545.
Test: 2018-08-02T11:44:19.688751: step 11050, loss 0.549151.
Train: 2018-08-02T11:44:19.907476: step 11051, loss 0.562688.
Train: 2018-08-02T11:44:20.157393: step 11052, loss 0.489549.
Train: 2018-08-02T11:44:20.376090: step 11053, loss 0.570774.
Train: 2018-08-02T11:44:20.610411: step 11054, loss 0.562585.
Train: 2018-08-02T11:44:20.829139: step 11055, loss 0.595371.
Train: 2018-08-02T11:44:21.079087: step 11056, loss 0.554321.
Train: 2018-08-02T11:44:21.297774: step 11057, loss 0.488441.
Train: 2018-08-02T11:44:21.516448: step 11058, loss 0.562488.
Train: 2018-08-02T11:44:21.750799: step 11059, loss 0.562477.
Train: 2018-08-02T11:44:21.969467: step 11060, loss 0.545851.
Test: 2018-08-02T11:44:23.156689: step 11060, loss 0.547577.
Train: 2018-08-02T11:44:23.391010: step 11061, loss 0.554134.
Train: 2018-08-02T11:44:23.609708: step 11062, loss 0.595793.
Train: 2018-08-02T11:44:23.828432: step 11063, loss 0.53735.
Train: 2018-08-02T11:44:24.078379: step 11064, loss 0.612612.
Train: 2018-08-02T11:44:24.297072: step 11065, loss 0.562399.
Train: 2018-08-02T11:44:24.547020: step 11066, loss 0.595879.
Train: 2018-08-02T11:44:24.765721: step 11067, loss 0.620932.
Train: 2018-08-02T11:44:24.984416: step 11068, loss 0.529076.
Train: 2018-08-02T11:44:25.203086: step 11069, loss 0.537459.
Train: 2018-08-02T11:44:25.437409: step 11070, loss 0.537488.
Test: 2018-08-02T11:44:26.624628: step 11070, loss 0.547982.
Train: 2018-08-02T11:44:26.843327: step 11071, loss 0.59571.
Train: 2018-08-02T11:44:27.062025: step 11072, loss 0.545839.
Train: 2018-08-02T11:44:27.280749: step 11073, loss 0.628866.
Train: 2018-08-02T11:44:27.515044: step 11074, loss 0.537628.
Train: 2018-08-02T11:44:27.780632: step 11075, loss 0.504602.
Train: 2018-08-02T11:44:28.014929: step 11076, loss 0.529415.
Train: 2018-08-02T11:44:28.233652: step 11077, loss 0.537664.
Train: 2018-08-02T11:44:28.452350: step 11078, loss 0.52935.
Train: 2018-08-02T11:44:28.686671: step 11079, loss 0.645412.
Train: 2018-08-02T11:44:28.952207: step 11080, loss 0.595629.
Test: 2018-08-02T11:44:30.123808: step 11080, loss 0.548461.
Train: 2018-08-02T11:44:30.342536: step 11081, loss 0.570757.
Train: 2018-08-02T11:44:30.576829: step 11082, loss 0.496358.
Train: 2018-08-02T11:44:30.795556: step 11083, loss 0.529419.
Train: 2018-08-02T11:44:31.029871: step 11084, loss 0.587305.
Train: 2018-08-02T11:44:31.264165: step 11085, loss 0.521109.
Train: 2018-08-02T11:44:31.482895: step 11086, loss 0.537627.
Train: 2018-08-02T11:44:31.717221: step 11087, loss 0.562464.
Train: 2018-08-02T11:44:31.935884: step 11088, loss 0.554153.
Train: 2018-08-02T11:44:32.154616: step 11089, loss 0.545824.
Train: 2018-08-02T11:44:32.373311: step 11090, loss 0.56244.
Test: 2018-08-02T11:44:33.560503: step 11090, loss 0.547943.
Train: 2018-08-02T11:44:33.779226: step 11091, loss 0.612422.
Train: 2018-08-02T11:44:34.013550: step 11092, loss 0.52912.
Train: 2018-08-02T11:44:34.263465: step 11093, loss 0.579096.
Train: 2018-08-02T11:44:34.497783: step 11094, loss 0.612415.
Train: 2018-08-02T11:44:34.732105: step 11095, loss 0.537487.
Train: 2018-08-02T11:44:34.950829: step 11096, loss 0.529201.
Train: 2018-08-02T11:44:35.169501: step 11097, loss 0.628941.
Train: 2018-08-02T11:44:35.419443: step 11098, loss 0.645436.
Train: 2018-08-02T11:44:35.638141: step 11099, loss 0.55422.
Train: 2018-08-02T11:44:35.872463: step 11100, loss 0.546028.
Test: 2018-08-02T11:44:37.075304: step 11100, loss 0.549938.
Train: 2018-08-02T11:44:38.043859: step 11101, loss 0.562536.
Train: 2018-08-02T11:44:38.278182: step 11102, loss 0.570761.
Train: 2018-08-02T11:44:38.496880: step 11103, loss 0.578951.
Train: 2018-08-02T11:44:38.731167: step 11104, loss 0.52994.
Train: 2018-08-02T11:44:38.949865: step 11105, loss 0.538157.
Train: 2018-08-02T11:44:39.168594: step 11106, loss 0.578925.
Train: 2018-08-02T11:44:39.387265: step 11107, loss 0.570779.
Train: 2018-08-02T11:44:39.621584: step 11108, loss 0.570783.
Train: 2018-08-02T11:44:39.840282: step 11109, loss 0.530178.
Train: 2018-08-02T11:44:40.074603: step 11110, loss 0.595146.
Test: 2018-08-02T11:44:41.277446: step 11110, loss 0.550253.
Train: 2018-08-02T11:44:41.480549: step 11111, loss 0.530233.
Train: 2018-08-02T11:44:41.730466: step 11112, loss 0.522126.
Train: 2018-08-02T11:44:41.964785: step 11113, loss 0.570788.
Train: 2018-08-02T11:44:42.199141: step 11114, loss 0.603282.
Train: 2018-08-02T11:44:42.417806: step 11115, loss 0.578908.
Train: 2018-08-02T11:44:42.667776: step 11116, loss 0.538328.
Train: 2018-08-02T11:44:42.886473: step 11117, loss 0.546443.
Train: 2018-08-02T11:44:43.136411: step 11118, loss 0.49772.
Train: 2018-08-02T11:44:43.370705: step 11119, loss 0.481301.
Train: 2018-08-02T11:44:43.589405: step 11120, loss 0.570771.
Test: 2018-08-02T11:44:44.792247: step 11120, loss 0.548706.
Train: 2018-08-02T11:44:45.010947: step 11121, loss 0.603527.
Train: 2018-08-02T11:44:45.245267: step 11122, loss 0.529733.
Train: 2018-08-02T11:44:45.463966: step 11123, loss 0.537857.
Train: 2018-08-02T11:44:45.698311: step 11124, loss 0.579004.
Train: 2018-08-02T11:44:45.917014: step 11125, loss 0.562493.
Train: 2018-08-02T11:44:46.151305: step 11126, loss 0.570757.
Train: 2018-08-02T11:44:46.385625: step 11127, loss 0.512743.
Train: 2018-08-02T11:44:46.604354: step 11128, loss 0.545842.
Train: 2018-08-02T11:44:46.838646: step 11129, loss 0.562438.
Train: 2018-08-02T11:44:47.088616: step 11130, loss 0.545744.
Test: 2018-08-02T11:44:48.291428: step 11130, loss 0.546725.
Train: 2018-08-02T11:44:48.525775: step 11131, loss 0.570771.
Train: 2018-08-02T11:44:48.760096: step 11132, loss 0.554036.
Train: 2018-08-02T11:44:48.978796: step 11133, loss 0.562398.
Train: 2018-08-02T11:44:49.197466: step 11134, loss 0.562392.
Train: 2018-08-02T11:44:49.416166: step 11135, loss 0.562388.
Train: 2018-08-02T11:44:49.666111: step 11136, loss 0.495167.
Train: 2018-08-02T11:44:49.884836: step 11137, loss 0.579213.
Train: 2018-08-02T11:44:50.119151: step 11138, loss 0.604512.
Train: 2018-08-02T11:44:50.337824: step 11139, loss 0.596082.
Train: 2018-08-02T11:44:50.572144: step 11140, loss 0.570795.
Test: 2018-08-02T11:44:51.774988: step 11140, loss 0.54892.
Train: 2018-08-02T11:44:52.024954: step 11141, loss 0.520339.
Train: 2018-08-02T11:44:52.243658: step 11142, loss 0.537167.
Train: 2018-08-02T11:44:52.462352: step 11143, loss 0.596009.
Train: 2018-08-02T11:44:52.696672: step 11144, loss 0.570786.
Train: 2018-08-02T11:44:52.930993: step 11145, loss 0.654687.
Train: 2018-08-02T11:44:53.149700: step 11146, loss 0.512229.
Train: 2018-08-02T11:44:53.368364: step 11147, loss 0.579116.
Train: 2018-08-02T11:44:53.602701: step 11148, loss 0.529118.
Train: 2018-08-02T11:44:53.821414: step 11149, loss 0.587397.
Train: 2018-08-02T11:44:54.055729: step 11150, loss 0.520946.
Test: 2018-08-02T11:44:55.258551: step 11150, loss 0.549946.
Train: 2018-08-02T11:44:55.492868: step 11151, loss 0.612234.
Train: 2018-08-02T11:44:55.695975: step 11152, loss 0.570757.
Train: 2018-08-02T11:44:55.914644: step 11153, loss 0.612063.
Train: 2018-08-02T11:44:56.133343: step 11154, loss 0.529579.
Train: 2018-08-02T11:44:56.367688: step 11155, loss 0.578977.
Train: 2018-08-02T11:44:56.602008: step 11156, loss 0.587161.
Train: 2018-08-02T11:44:56.820712: step 11157, loss 0.603477.
Train: 2018-08-02T11:44:57.039406: step 11158, loss 0.554477.
Train: 2018-08-02T11:44:57.258110: step 11159, loss 0.546408.
Train: 2018-08-02T11:44:57.492399: step 11160, loss 0.595116.
Test: 2018-08-02T11:44:58.695242: step 11160, loss 0.549402.
Train: 2018-08-02T11:44:58.913972: step 11161, loss 0.57889.
Train: 2018-08-02T11:44:59.132665: step 11162, loss 0.546631.
Train: 2018-08-02T11:44:59.382584: step 11163, loss 0.570829.
Train: 2018-08-02T11:44:59.616933: step 11164, loss 0.602958.
Train: 2018-08-02T11:44:59.835630: step 11165, loss 0.570857.
Train: 2018-08-02T11:45:00.054299: step 11166, loss 0.514969.
Train: 2018-08-02T11:45:00.288650: step 11167, loss 0.562903.
Train: 2018-08-02T11:45:00.507317: step 11168, loss 0.531027.
Train: 2018-08-02T11:45:00.741663: step 11169, loss 0.53899.
Train: 2018-08-02T11:45:00.944746: step 11170, loss 0.602804.
Test: 2018-08-02T11:45:02.163180: step 11170, loss 0.550673.
Train: 2018-08-02T11:45:02.381903: step 11171, loss 0.562899.
Train: 2018-08-02T11:45:02.631820: step 11172, loss 0.530973.
Train: 2018-08-02T11:45:02.881790: step 11173, loss 0.562883.
Train: 2018-08-02T11:45:03.069218: step 11174, loss 0.460507.
Train: 2018-08-02T11:45:03.287916: step 11175, loss 0.578869.
Train: 2018-08-02T11:45:03.506640: step 11176, loss 0.514486.
Train: 2018-08-02T11:45:03.740969: step 11177, loss 0.546569.
Train: 2018-08-02T11:45:03.975286: step 11178, loss 0.643792.
Train: 2018-08-02T11:45:04.193985: step 11179, loss 0.60328.
Train: 2018-08-02T11:45:04.412683: step 11180, loss 0.538276.
Test: 2018-08-02T11:45:05.599875: step 11180, loss 0.548498.
Train: 2018-08-02T11:45:05.818598: step 11181, loss 0.578916.
Train: 2018-08-02T11:45:06.037299: step 11182, loss 0.521944.
Train: 2018-08-02T11:45:06.287214: step 11183, loss 0.627833.
Train: 2018-08-02T11:45:06.505944: step 11184, loss 0.627818.
Train: 2018-08-02T11:45:06.755855: step 11185, loss 0.538245.
Train: 2018-08-02T11:45:06.974583: step 11186, loss 0.570785.
Train: 2018-08-02T11:45:07.208903: step 11187, loss 0.538322.
Train: 2018-08-02T11:45:07.458816: step 11188, loss 0.554563.
Train: 2018-08-02T11:45:07.693164: step 11189, loss 0.554565.
Train: 2018-08-02T11:45:07.927486: step 11190, loss 0.546449.
Test: 2018-08-02T11:45:09.130299: step 11190, loss 0.549113.
Train: 2018-08-02T11:45:09.348997: step 11191, loss 0.505845.
Train: 2018-08-02T11:45:09.583343: step 11192, loss 0.546382.
Train: 2018-08-02T11:45:09.802047: step 11193, loss 0.603376.
Train: 2018-08-02T11:45:10.020716: step 11194, loss 0.529985.
Train: 2018-08-02T11:45:10.270681: step 11195, loss 0.497233.
Train: 2018-08-02T11:45:10.504978: step 11196, loss 0.660909.
Train: 2018-08-02T11:45:10.739323: step 11197, loss 0.603553.
Train: 2018-08-02T11:45:10.958026: step 11198, loss 0.562572.
Train: 2018-08-02T11:45:11.176725: step 11199, loss 0.60351.
Train: 2018-08-02T11:45:11.411040: step 11200, loss 0.52173.
Test: 2018-08-02T11:45:12.598237: step 11200, loss 0.549334.
Train: 2018-08-02T11:45:13.457411: step 11201, loss 0.538093.
Train: 2018-08-02T11:45:13.676141: step 11202, loss 0.521748.
Train: 2018-08-02T11:45:13.910460: step 11203, loss 0.554408.
Train: 2018-08-02T11:45:14.129127: step 11204, loss 0.611708.
Train: 2018-08-02T11:45:14.347851: step 11205, loss 0.570764.
Train: 2018-08-02T11:45:14.582147: step 11206, loss 0.546212.
Train: 2018-08-02T11:45:14.800876: step 11207, loss 0.538025.
Train: 2018-08-02T11:45:15.050813: step 11208, loss 0.497052.
Train: 2018-08-02T11:45:15.269515: step 11209, loss 0.546139.
Train: 2018-08-02T11:45:15.503807: step 11210, loss 0.603661.
Test: 2018-08-02T11:45:16.706649: step 11210, loss 0.549341.
Train: 2018-08-02T11:45:16.925372: step 11211, loss 0.554289.
Train: 2018-08-02T11:45:17.159670: step 11212, loss 0.537787.
Train: 2018-08-02T11:45:17.378400: step 11213, loss 0.554247.
Train: 2018-08-02T11:45:17.612720: step 11214, loss 0.521154.
Train: 2018-08-02T11:45:17.862655: step 11215, loss 0.570758.
Train: 2018-08-02T11:45:18.081327: step 11216, loss 0.545845.
Train: 2018-08-02T11:45:18.331270: step 11217, loss 0.537462.
Train: 2018-08-02T11:45:18.565590: step 11218, loss 0.60409.
Train: 2018-08-02T11:45:18.784287: step 11219, loss 0.637773.
Train: 2018-08-02T11:45:19.034230: step 11220, loss 0.554069.
Test: 2018-08-02T11:45:20.237072: step 11220, loss 0.548917.
Train: 2018-08-02T11:45:20.471418: step 11221, loss 0.604067.
Train: 2018-08-02T11:45:20.690122: step 11222, loss 0.562446.
Train: 2018-08-02T11:45:20.908791: step 11223, loss 0.554179.
Train: 2018-08-02T11:45:21.127517: step 11224, loss 0.554204.
Train: 2018-08-02T11:45:21.361835: step 11225, loss 0.512897.
Train: 2018-08-02T11:45:21.580508: step 11226, loss 0.562491.
Train: 2018-08-02T11:45:21.799208: step 11227, loss 0.57902.
Train: 2018-08-02T11:45:22.049150: step 11228, loss 0.620312.
Train: 2018-08-02T11:45:22.267848: step 11229, loss 0.636703.
Train: 2018-08-02T11:45:22.502196: step 11230, loss 0.480407.
Test: 2018-08-02T11:45:23.689389: step 11230, loss 0.548103.
Train: 2018-08-02T11:45:23.923710: step 11231, loss 0.611777.
Train: 2018-08-02T11:45:24.142438: step 11232, loss 0.554398.
Train: 2018-08-02T11:45:24.361132: step 11233, loss 0.489094.
Train: 2018-08-02T11:45:24.579836: step 11234, loss 0.529933.
Train: 2018-08-02T11:45:24.829773: step 11235, loss 0.611636.
Train: 2018-08-02T11:45:25.048445: step 11236, loss 0.529922.
Train: 2018-08-02T11:45:25.282771: step 11237, loss 0.554424.
Train: 2018-08-02T11:45:25.501466: step 11238, loss 0.611647.
Train: 2018-08-02T11:45:25.720165: step 11239, loss 0.55443.
Train: 2018-08-02T11:45:25.954524: step 11240, loss 0.562605.
Test: 2018-08-02T11:45:27.157327: step 11240, loss 0.548979.
Train: 2018-08-02T11:45:27.422890: step 11241, loss 0.595257.
Train: 2018-08-02T11:45:27.657212: step 11242, loss 0.595231.
Train: 2018-08-02T11:45:27.891530: step 11243, loss 0.570781.
Train: 2018-08-02T11:45:28.125850: step 11244, loss 0.595153.
Train: 2018-08-02T11:45:28.360170: step 11245, loss 0.530289.
Train: 2018-08-02T11:45:28.594491: step 11246, loss 0.514168.
Train: 2018-08-02T11:45:28.813189: step 11247, loss 0.546535.
Train: 2018-08-02T11:45:29.047510: step 11248, loss 0.538424.
Train: 2018-08-02T11:45:29.297478: step 11249, loss 0.546489.
Train: 2018-08-02T11:45:29.531773: step 11250, loss 0.505886.
Test: 2018-08-02T11:45:30.718993: step 11250, loss 0.549248.
Train: 2018-08-02T11:45:30.937692: step 11251, loss 0.627739.
Train: 2018-08-02T11:45:31.172013: step 11252, loss 0.57892.
Train: 2018-08-02T11:45:31.406333: step 11253, loss 0.513778.
Train: 2018-08-02T11:45:31.640653: step 11254, loss 0.513691.
Train: 2018-08-02T11:45:31.874972: step 11255, loss 0.546243.
Train: 2018-08-02T11:45:32.109319: step 11256, loss 0.587156.
Train: 2018-08-02T11:45:32.343613: step 11257, loss 0.496863.
Train: 2018-08-02T11:45:32.577932: step 11258, loss 0.611938.
Train: 2018-08-02T11:45:32.796662: step 11259, loss 0.54601.
Train: 2018-08-02T11:45:33.046600: step 11260, loss 0.587283.
Test: 2018-08-02T11:45:34.249416: step 11260, loss 0.548864.
Train: 2018-08-02T11:45:34.468116: step 11261, loss 0.595568.
Train: 2018-08-02T11:45:34.702466: step 11262, loss 0.471514.
Train: 2018-08-02T11:45:34.952378: step 11263, loss 0.603905.
Train: 2018-08-02T11:45:35.186697: step 11264, loss 0.537585.
Train: 2018-08-02T11:45:35.405395: step 11265, loss 0.554153.
Train: 2018-08-02T11:45:35.639719: step 11266, loss 0.487636.
Train: 2018-08-02T11:45:35.858414: step 11267, loss 0.579099.
Train: 2018-08-02T11:45:36.077113: step 11268, loss 0.512316.
Train: 2018-08-02T11:45:36.295843: step 11269, loss 0.646135.
Train: 2018-08-02T11:45:36.514545: step 11270, loss 0.562401.
Test: 2018-08-02T11:45:37.717354: step 11270, loss 0.549178.
Train: 2018-08-02T11:45:37.936085: step 11271, loss 0.537264.
Train: 2018-08-02T11:45:38.185995: step 11272, loss 0.545629.
Train: 2018-08-02T11:45:38.404694: step 11273, loss 0.52044.
Train: 2018-08-02T11:45:38.654660: step 11274, loss 0.520369.
Train: 2018-08-02T11:45:38.873362: step 11275, loss 0.495003.
Train: 2018-08-02T11:45:39.092032: step 11276, loss 0.511667.
Train: 2018-08-02T11:45:39.341974: step 11277, loss 0.604762.
Train: 2018-08-02T11:45:39.560697: step 11278, loss 0.494326.
Train: 2018-08-02T11:45:39.779372: step 11279, loss 0.553808.
Train: 2018-08-02T11:45:39.998099: step 11280, loss 0.57945.
Test: 2018-08-02T11:45:41.200913: step 11280, loss 0.547028.
Train: 2018-08-02T11:45:41.435234: step 11281, loss 0.536611.
Train: 2018-08-02T11:45:41.653962: step 11282, loss 0.553742.
Train: 2018-08-02T11:45:41.872631: step 11283, loss 0.527898.
Train: 2018-08-02T11:45:42.091329: step 11284, loss 0.562338.
Train: 2018-08-02T11:45:42.310059: step 11285, loss 0.614194.
Train: 2018-08-02T11:45:42.544379: step 11286, loss 0.56234.
Train: 2018-08-02T11:45:42.763047: step 11287, loss 0.545059.
Train: 2018-08-02T11:45:42.997396: step 11288, loss 0.56234.
Train: 2018-08-02T11:45:43.216066: step 11289, loss 0.545067.
Train: 2018-08-02T11:45:43.466008: step 11290, loss 0.570972.
Test: 2018-08-02T11:45:44.653230: step 11290, loss 0.548725.
Train: 2018-08-02T11:45:44.887550: step 11291, loss 0.605472.
Train: 2018-08-02T11:45:45.121910: step 11292, loss 0.639831.
Train: 2018-08-02T11:45:45.340598: step 11293, loss 0.553756.
Train: 2018-08-02T11:45:45.574889: step 11294, loss 0.596537.
Train: 2018-08-02T11:45:45.824861: step 11295, loss 0.511249.
Train: 2018-08-02T11:45:46.043560: step 11296, loss 0.536871.
Train: 2018-08-02T11:45:46.277849: step 11297, loss 0.519983.
Train: 2018-08-02T11:45:46.512174: step 11298, loss 0.553893.
Train: 2018-08-02T11:45:46.730901: step 11299, loss 0.520084.
Train: 2018-08-02T11:45:46.965214: step 11300, loss 0.553907.
Test: 2018-08-02T11:45:48.167923: step 11300, loss 0.548246.
Train: 2018-08-02T11:45:49.027096: step 11301, loss 0.663787.
Train: 2018-08-02T11:45:49.292689: step 11302, loss 0.570801.
Train: 2018-08-02T11:45:49.526980: step 11303, loss 0.562383.
Train: 2018-08-02T11:45:49.761298: step 11304, loss 0.55401.
Train: 2018-08-02T11:45:49.980028: step 11305, loss 0.604236.
Train: 2018-08-02T11:45:50.214345: step 11306, loss 0.562427.
Train: 2018-08-02T11:45:50.448638: step 11307, loss 0.545818.
Train: 2018-08-02T11:45:50.667338: step 11308, loss 0.587346.
Train: 2018-08-02T11:45:50.886061: step 11309, loss 0.562486.
Train: 2018-08-02T11:45:51.120366: step 11310, loss 0.570756.
Test: 2018-08-02T11:45:52.323199: step 11310, loss 0.547656.
Train: 2018-08-02T11:45:52.557520: step 11311, loss 0.513171.
Train: 2018-08-02T11:45:52.791870: step 11312, loss 0.587192.
Train: 2018-08-02T11:45:53.010563: step 11313, loss 0.611771.
Train: 2018-08-02T11:45:53.229263: step 11314, loss 0.578945.
Train: 2018-08-02T11:45:53.463582: step 11315, loss 0.570774.
Train: 2018-08-02T11:45:53.713523: step 11316, loss 0.619572.
Train: 2018-08-02T11:45:53.932227: step 11317, loss 0.578897.
Train: 2018-08-02T11:45:54.166549: step 11318, loss 0.506273.
Train: 2018-08-02T11:45:54.385248: step 11319, loss 0.578876.
Train: 2018-08-02T11:45:54.619537: step 11320, loss 0.514607.
Test: 2018-08-02T11:45:55.822379: step 11320, loss 0.550702.
Train: 2018-08-02T11:45:56.025482: step 11321, loss 0.578869.
Train: 2018-08-02T11:45:56.244155: step 11322, loss 0.562828.
Train: 2018-08-02T11:45:56.478477: step 11323, loss 0.586879.
Train: 2018-08-02T11:45:56.697202: step 11324, loss 0.642893.
Train: 2018-08-02T11:45:56.900254: step 11325, loss 0.630994.
Train: 2018-08-02T11:45:57.118979: step 11326, loss 0.49146.
Train: 2018-08-02T11:45:57.368918: step 11327, loss 0.562999.
Train: 2018-08-02T11:45:57.587591: step 11328, loss 0.563025.
Train: 2018-08-02T11:45:57.821939: step 11329, loss 0.531416.
Train: 2018-08-02T11:45:58.040611: step 11330, loss 0.578861.
Test: 2018-08-02T11:45:59.227832: step 11330, loss 0.550402.
Train: 2018-08-02T11:45:59.462178: step 11331, loss 0.563055.
Train: 2018-08-02T11:45:59.680884: step 11332, loss 0.555157.
Train: 2018-08-02T11:45:59.899581: step 11333, loss 0.586765.
Train: 2018-08-02T11:46:00.118275: step 11334, loss 0.602565.
Train: 2018-08-02T11:46:00.352570: step 11335, loss 0.57097.
Train: 2018-08-02T11:46:00.602541: step 11336, loss 0.586749.
Train: 2018-08-02T11:46:00.821236: step 11337, loss 0.539493.
Train: 2018-08-02T11:46:01.055562: step 11338, loss 0.555252.
Train: 2018-08-02T11:46:01.274259: step 11339, loss 0.523762.
Train: 2018-08-02T11:46:01.524200: step 11340, loss 0.570983.
Test: 2018-08-02T11:46:02.727012: step 11340, loss 0.550993.
Train: 2018-08-02T11:46:02.945736: step 11341, loss 0.563082.
Train: 2018-08-02T11:46:03.179924: step 11342, loss 0.523561.
Train: 2018-08-02T11:46:03.414274: step 11343, loss 0.563022.
Train: 2018-08-02T11:46:03.617346: step 11344, loss 0.507438.
Train: 2018-08-02T11:46:03.851640: step 11345, loss 0.586822.
Train: 2018-08-02T11:46:04.070339: step 11346, loss 0.60282.
Train: 2018-08-02T11:46:04.289069: step 11347, loss 0.586854.
Train: 2018-08-02T11:46:04.523392: step 11348, loss 0.578858.
Train: 2018-08-02T11:46:04.757678: step 11349, loss 0.546839.
Train: 2018-08-02T11:46:05.007648: step 11350, loss 0.48266.
Test: 2018-08-02T11:46:06.210462: step 11350, loss 0.548988.
Train: 2018-08-02T11:46:06.413540: step 11351, loss 0.514561.
Train: 2018-08-02T11:46:06.647862: step 11352, loss 0.538523.
Train: 2018-08-02T11:46:06.866589: step 11353, loss 0.570803.
Train: 2018-08-02T11:46:07.085284: step 11354, loss 0.570867.
Train: 2018-08-02T11:46:07.303983: step 11355, loss 0.562627.
Train: 2018-08-02T11:46:07.538276: step 11356, loss 0.497134.
Train: 2018-08-02T11:46:07.757009: step 11357, loss 0.496844.
Train: 2018-08-02T11:46:07.975707: step 11358, loss 0.554262.
Train: 2018-08-02T11:46:08.210020: step 11359, loss 0.5293.
Train: 2018-08-02T11:46:08.444341: step 11360, loss 0.512439.
Test: 2018-08-02T11:46:09.631537: step 11360, loss 0.548409.
Train: 2018-08-02T11:46:09.850266: step 11361, loss 0.562402.
Train: 2018-08-02T11:46:10.084555: step 11362, loss 0.579212.
Train: 2018-08-02T11:46:10.318876: step 11363, loss 0.545468.
Train: 2018-08-02T11:46:10.537605: step 11364, loss 0.536921.
Train: 2018-08-02T11:46:10.756274: step 11365, loss 0.562344.
Train: 2018-08-02T11:46:11.006239: step 11366, loss 0.545276.
Train: 2018-08-02T11:46:11.224944: step 11367, loss 0.536671.
Train: 2018-08-02T11:46:11.459259: step 11368, loss 0.61381.
Train: 2018-08-02T11:46:11.677934: step 11369, loss 0.613868.
Train: 2018-08-02T11:46:11.896632: step 11370, loss 0.519407.
Test: 2018-08-02T11:46:13.099475: step 11370, loss 0.548585.
Train: 2018-08-02T11:46:13.318204: step 11371, loss 0.596688.
Train: 2018-08-02T11:46:13.536898: step 11372, loss 0.579499.
Train: 2018-08-02T11:46:13.802462: step 11373, loss 0.545193.
Train: 2018-08-02T11:46:14.036780: step 11374, loss 0.613708.
Train: 2018-08-02T11:46:14.255455: step 11375, loss 0.622132.
Train: 2018-08-02T11:46:14.474178: step 11376, loss 0.570854.
Train: 2018-08-02T11:46:14.708473: step 11377, loss 0.545393.
Train: 2018-08-02T11:46:14.927196: step 11378, loss 0.537004.
Train: 2018-08-02T11:46:15.145870: step 11379, loss 0.579233.
Train: 2018-08-02T11:46:15.364569: step 11380, loss 0.53716.
Test: 2018-08-02T11:46:16.567412: step 11380, loss 0.549153.
Train: 2018-08-02T11:46:16.770524: step 11381, loss 0.528833.
Train: 2018-08-02T11:46:17.004811: step 11382, loss 0.612673.
Train: 2018-08-02T11:46:17.223539: step 11383, loss 0.57913.
Train: 2018-08-02T11:46:17.457829: step 11384, loss 0.562429.
Train: 2018-08-02T11:46:17.676558: step 11385, loss 0.554132.
Train: 2018-08-02T11:46:17.895260: step 11386, loss 0.51269.
Train: 2018-08-02T11:46:18.113925: step 11387, loss 0.562471.
Train: 2018-08-02T11:46:18.332624: step 11388, loss 0.595598.
Train: 2018-08-02T11:46:18.551351: step 11389, loss 0.60382.
Train: 2018-08-02T11:46:18.801295: step 11390, loss 0.620221.
Test: 2018-08-02T11:46:19.988487: step 11390, loss 0.550721.
Train: 2018-08-02T11:46:20.222808: step 11391, loss 0.595399.
Train: 2018-08-02T11:46:20.457127: step 11392, loss 0.521697.
Train: 2018-08-02T11:46:20.660205: step 11393, loss 0.562619.
Train: 2018-08-02T11:46:20.878931: step 11394, loss 0.54638.
Train: 2018-08-02T11:46:21.097632: step 11395, loss 0.546433.
Train: 2018-08-02T11:46:21.331925: step 11396, loss 0.635655.
Train: 2018-08-02T11:46:21.566267: step 11397, loss 0.570805.
Train: 2018-08-02T11:46:21.784971: step 11398, loss 0.522451.
Train: 2018-08-02T11:46:22.003664: step 11399, loss 0.611074.
Train: 2018-08-02T11:46:22.222338: step 11400, loss 0.554781.
Test: 2018-08-02T11:46:23.425182: step 11400, loss 0.549823.
Train: 2018-08-02T11:46:24.268768: step 11401, loss 0.530781.
Train: 2018-08-02T11:46:24.487457: step 11402, loss 0.586872.
Train: 2018-08-02T11:46:24.706156: step 11403, loss 0.466901.
Train: 2018-08-02T11:46:24.924830: step 11404, loss 0.610898.
Train: 2018-08-02T11:46:25.143530: step 11405, loss 0.594884.
Train: 2018-08-02T11:46:25.377878: step 11406, loss 0.610883.
Train: 2018-08-02T11:46:25.596549: step 11407, loss 0.506937.
Train: 2018-08-02T11:46:25.815272: step 11408, loss 0.626814.
Train: 2018-08-02T11:46:26.033976: step 11409, loss 0.530978.
Train: 2018-08-02T11:46:26.283887: step 11410, loss 0.507065.
Test: 2018-08-02T11:46:27.471109: step 11410, loss 0.550469.
Train: 2018-08-02T11:46:27.689833: step 11411, loss 0.642754.
Train: 2018-08-02T11:46:27.939749: step 11412, loss 0.610781.
Train: 2018-08-02T11:46:28.158449: step 11413, loss 0.523101.
Train: 2018-08-02T11:46:28.377176: step 11414, loss 0.554975.
Train: 2018-08-02T11:46:28.595877: step 11415, loss 0.523137.
Train: 2018-08-02T11:46:28.830166: step 11416, loss 0.554953.
Train: 2018-08-02T11:46:29.064517: step 11417, loss 0.554919.
Train: 2018-08-02T11:46:29.283215: step 11418, loss 0.570873.
Train: 2018-08-02T11:46:29.517530: step 11419, loss 0.586844.
Train: 2018-08-02T11:46:29.720584: step 11420, loss 0.538802.
Test: 2018-08-02T11:46:30.923426: step 11420, loss 0.549054.
Train: 2018-08-02T11:46:31.126503: step 11421, loss 0.562817.
Train: 2018-08-02T11:46:31.345233: step 11422, loss 0.538651.
Train: 2018-08-02T11:46:31.563925: step 11423, loss 0.498333.
Train: 2018-08-02T11:46:31.782630: step 11424, loss 0.538394.
Train: 2018-08-02T11:46:32.001301: step 11425, loss 0.578696.
Train: 2018-08-02T11:46:32.220029: step 11426, loss 0.561388.
Train: 2018-08-02T11:46:32.438726: step 11427, loss 0.571706.
Train: 2018-08-02T11:46:32.657425: step 11428, loss 0.588582.
Train: 2018-08-02T11:46:32.876094: step 11429, loss 0.512482.
Train: 2018-08-02T11:46:33.094793: step 11430, loss 0.620353.
Test: 2018-08-02T11:46:34.297636: step 11430, loss 0.548677.
Train: 2018-08-02T11:46:34.516334: step 11431, loss 0.555325.
Train: 2018-08-02T11:46:34.735034: step 11432, loss 0.537454.
Train: 2018-08-02T11:46:34.953732: step 11433, loss 0.596219.
Train: 2018-08-02T11:46:35.172431: step 11434, loss 0.620086.
Train: 2018-08-02T11:46:35.391164: step 11435, loss 0.554082.
Train: 2018-08-02T11:46:35.625477: step 11436, loss 0.578936.
Train: 2018-08-02T11:46:35.844180: step 11437, loss 0.595176.
Train: 2018-08-02T11:46:36.062848: step 11438, loss 0.538239.
Train: 2018-08-02T11:46:36.281572: step 11439, loss 0.546443.
Train: 2018-08-02T11:46:36.500275: step 11440, loss 0.505922.
Test: 2018-08-02T11:46:37.703088: step 11440, loss 0.550431.
Train: 2018-08-02T11:46:37.921787: step 11441, loss 0.546447.
Train: 2018-08-02T11:46:38.140519: step 11442, loss 0.570786.
Train: 2018-08-02T11:46:38.359218: step 11443, loss 0.562654.
Train: 2018-08-02T11:46:38.577914: step 11444, loss 0.530108.
Train: 2018-08-02T11:46:38.796613: step 11445, loss 0.58707.
Train: 2018-08-02T11:46:39.030939: step 11446, loss 0.603383.
Train: 2018-08-02T11:46:39.249629: step 11447, loss 0.538179.
Train: 2018-08-02T11:46:39.468330: step 11448, loss 0.554472.
Train: 2018-08-02T11:46:39.702650: step 11449, loss 0.578929.
Train: 2018-08-02T11:46:39.921344: step 11450, loss 0.513687.
Test: 2018-08-02T11:46:41.108541: step 11450, loss 0.54897.
Train: 2018-08-02T11:46:41.342862: step 11451, loss 0.538111.
Train: 2018-08-02T11:46:41.561590: step 11452, loss 0.562588.
Train: 2018-08-02T11:46:41.780258: step 11453, loss 0.52162.
Train: 2018-08-02T11:46:42.014613: step 11454, loss 0.562551.
Train: 2018-08-02T11:46:42.248928: step 11455, loss 0.653018.
Train: 2018-08-02T11:46:42.467598: step 11456, loss 0.554313.
Train: 2018-08-02T11:46:42.701917: step 11457, loss 0.521432.
Train: 2018-08-02T11:46:42.920647: step 11458, loss 0.636576.
Train: 2018-08-02T11:46:43.154963: step 11459, loss 0.562541.
Train: 2018-08-02T11:46:43.389281: step 11460, loss 0.554342.
Test: 2018-08-02T11:46:44.576479: step 11460, loss 0.549238.
Train: 2018-08-02T11:46:44.779556: step 11461, loss 0.546154.
Train: 2018-08-02T11:46:44.998285: step 11462, loss 0.570762.
Train: 2018-08-02T11:46:45.232619: step 11463, loss 0.611739.
Train: 2018-08-02T11:46:45.451299: step 11464, loss 0.59531.
Train: 2018-08-02T11:46:45.701246: step 11465, loss 0.538124.
Train: 2018-08-02T11:46:45.935535: step 11466, loss 0.619672.
Train: 2018-08-02T11:46:46.154265: step 11467, loss 0.562657.
Train: 2018-08-02T11:46:46.372963: step 11468, loss 0.522153.
Train: 2018-08-02T11:46:46.622876: step 11469, loss 0.506023.
Train: 2018-08-02T11:46:46.841599: step 11470, loss 0.554597.
Test: 2018-08-02T11:46:48.044416: step 11470, loss 0.548596.
Train: 2018-08-02T11:46:48.263149: step 11471, loss 0.595107.
Train: 2018-08-02T11:46:48.497464: step 11472, loss 0.659919.
Train: 2018-08-02T11:46:48.716161: step 11473, loss 0.522321.
Train: 2018-08-02T11:46:48.950479: step 11474, loss 0.570812.
Train: 2018-08-02T11:46:49.184800: step 11475, loss 0.603059.
Train: 2018-08-02T11:46:49.372256: step 11476, loss 0.52847.
Train: 2018-08-02T11:46:49.590959: step 11477, loss 0.546736.
Train: 2018-08-02T11:46:49.809653: step 11478, loss 0.570837.
Train: 2018-08-02T11:46:50.028328: step 11479, loss 0.538731.
Train: 2018-08-02T11:46:50.262672: step 11480, loss 0.514623.
Test: 2018-08-02T11:46:51.449869: step 11480, loss 0.548245.
Train: 2018-08-02T11:46:51.652949: step 11481, loss 0.546695.
Train: 2018-08-02T11:46:51.871673: step 11482, loss 0.498291.
Train: 2018-08-02T11:46:52.105965: step 11483, loss 0.506117.
Train: 2018-08-02T11:46:52.340317: step 11484, loss 0.538292.
Train: 2018-08-02T11:46:52.559009: step 11485, loss 0.55445.
Train: 2018-08-02T11:46:52.793335: step 11486, loss 0.619997.
Train: 2018-08-02T11:46:53.012029: step 11487, loss 0.595463.
Train: 2018-08-02T11:46:53.246325: step 11488, loss 0.521362.
Train: 2018-08-02T11:46:53.465022: step 11489, loss 0.570801.
Train: 2018-08-02T11:46:53.683751: step 11490, loss 0.545952.
Test: 2018-08-02T11:46:54.886565: step 11490, loss 0.54847.
Train: 2018-08-02T11:46:55.089672: step 11491, loss 0.612139.
Train: 2018-08-02T11:46:55.323963: step 11492, loss 0.628738.
Train: 2018-08-02T11:46:55.542661: step 11493, loss 0.521178.
Train: 2018-08-02T11:46:55.777007: step 11494, loss 0.554233.
Train: 2018-08-02T11:46:55.995680: step 11495, loss 0.579017.
Train: 2018-08-02T11:46:56.230026: step 11496, loss 0.562501.
Train: 2018-08-02T11:46:56.448730: step 11497, loss 0.579006.
Train: 2018-08-02T11:46:56.667398: step 11498, loss 0.570757.
Train: 2018-08-02T11:46:56.886130: step 11499, loss 0.488432.
Train: 2018-08-02T11:46:57.104825: step 11500, loss 0.546042.
Test: 2018-08-02T11:46:58.292017: step 11500, loss 0.549118.
Train: 2018-08-02T11:46:59.119972: step 11501, loss 0.56251.
Train: 2018-08-02T11:46:59.338678: step 11502, loss 0.512984.
Train: 2018-08-02T11:46:59.572998: step 11503, loss 0.537683.
Train: 2018-08-02T11:46:59.791691: step 11504, loss 0.521039.
Train: 2018-08-02T11:47:00.026023: step 11505, loss 0.56245.
Train: 2018-08-02T11:47:00.244716: step 11506, loss 0.570764.
Train: 2018-08-02T11:47:00.463414: step 11507, loss 0.612498.
Train: 2018-08-02T11:47:00.697729: step 11508, loss 0.529021.
Train: 2018-08-02T11:47:00.932024: step 11509, loss 0.503906.
Train: 2018-08-02T11:47:01.166370: step 11510, loss 0.612658.
Test: 2018-08-02T11:47:02.353566: step 11510, loss 0.549751.
Train: 2018-08-02T11:47:02.556668: step 11511, loss 0.604304.
Train: 2018-08-02T11:47:02.775343: step 11512, loss 0.554024.
Train: 2018-08-02T11:47:02.994042: step 11513, loss 0.537284.
Train: 2018-08-02T11:47:03.244017: step 11514, loss 0.612643.
Train: 2018-08-02T11:47:03.447090: step 11515, loss 0.528959.
Train: 2018-08-02T11:47:03.665783: step 11516, loss 0.604206.
Train: 2018-08-02T11:47:03.884480: step 11517, loss 0.545731.
Train: 2018-08-02T11:47:04.103187: step 11518, loss 0.562429.
Train: 2018-08-02T11:47:04.337508: step 11519, loss 0.520802.
Train: 2018-08-02T11:47:04.556200: step 11520, loss 0.529136.
Test: 2018-08-02T11:47:05.774640: step 11520, loss 0.548139.
Train: 2018-08-02T11:47:05.977719: step 11521, loss 0.545774.
Train: 2018-08-02T11:47:06.212063: step 11522, loss 0.604109.
Train: 2018-08-02T11:47:06.446358: step 11523, loss 0.545767.
Train: 2018-08-02T11:47:06.680702: step 11524, loss 0.562432.
Train: 2018-08-02T11:47:06.915024: step 11525, loss 0.579094.
Train: 2018-08-02T11:47:07.133730: step 11526, loss 0.537466.
Train: 2018-08-02T11:47:07.368045: step 11527, loss 0.512501.
Train: 2018-08-02T11:47:07.602362: step 11528, loss 0.52911.
Train: 2018-08-02T11:47:07.821066: step 11529, loss 0.562423.
Train: 2018-08-02T11:47:08.039734: step 11530, loss 0.512291.
Test: 2018-08-02T11:47:09.226956: step 11530, loss 0.549384.
Train: 2018-08-02T11:47:09.445686: step 11531, loss 0.554027.
Train: 2018-08-02T11:47:09.680006: step 11532, loss 0.461699.
Train: 2018-08-02T11:47:09.898704: step 11533, loss 0.587703.
Train: 2018-08-02T11:47:10.117372: step 11534, loss 0.621502.
Train: 2018-08-02T11:47:10.336072: step 11535, loss 0.520056.
Train: 2018-08-02T11:47:10.554770: step 11536, loss 0.621638.
Train: 2018-08-02T11:47:10.789121: step 11537, loss 0.528467.
Train: 2018-08-02T11:47:11.007823: step 11538, loss 0.502976.
Train: 2018-08-02T11:47:11.226488: step 11539, loss 0.579249.
Train: 2018-08-02T11:47:11.445212: step 11540, loss 0.587801.
Test: 2018-08-02T11:47:12.648030: step 11540, loss 0.5475.
Train: 2018-08-02T11:47:12.851108: step 11541, loss 0.596344.
Train: 2018-08-02T11:47:13.085431: step 11542, loss 0.622321.
Train: 2018-08-02T11:47:13.304151: step 11543, loss 0.596292.
Train: 2018-08-02T11:47:13.522850: step 11544, loss 0.50327.
Train: 2018-08-02T11:47:13.741525: step 11545, loss 0.570788.
Train: 2018-08-02T11:47:13.975874: step 11546, loss 0.48678.
Train: 2018-08-02T11:47:14.210195: step 11547, loss 0.570787.
Train: 2018-08-02T11:47:14.428893: step 11548, loss 0.587574.
Train: 2018-08-02T11:47:14.647587: step 11549, loss 0.646242.
Train: 2018-08-02T11:47:14.881916: step 11550, loss 0.587487.
Test: 2018-08-02T11:47:16.069104: step 11550, loss 0.546609.
Train: 2018-08-02T11:47:16.303450: step 11551, loss 0.529121.
Train: 2018-08-02T11:47:16.522148: step 11552, loss 0.595681.
Train: 2018-08-02T11:47:16.756444: step 11553, loss 0.587318.
Train: 2018-08-02T11:47:16.990763: step 11554, loss 0.653271.
Train: 2018-08-02T11:47:17.225083: step 11555, loss 0.570761.
Train: 2018-08-02T11:47:17.459429: step 11556, loss 0.63609.
Train: 2018-08-02T11:47:17.678132: step 11557, loss 0.546453.
Train: 2018-08-02T11:47:17.912453: step 11558, loss 0.578883.
Train: 2018-08-02T11:47:18.131155: step 11559, loss 0.546762.
Train: 2018-08-02T11:47:18.349820: step 11560, loss 0.60284.
Test: 2018-08-02T11:47:19.552663: step 11560, loss 0.548748.
Train: 2018-08-02T11:47:19.771395: step 11561, loss 0.586814.
Train: 2018-08-02T11:47:20.005688: step 11562, loss 0.602611.
Train: 2018-08-02T11:47:20.208790: step 11563, loss 0.578866.
Train: 2018-08-02T11:47:20.427489: step 11564, loss 0.53185.
Train: 2018-08-02T11:47:20.661809: step 11565, loss 0.532019.
Train: 2018-08-02T11:47:20.880511: step 11566, loss 0.547709.
Train: 2018-08-02T11:47:21.099201: step 11567, loss 0.485444.
Train: 2018-08-02T11:47:21.317906: step 11568, loss 0.555479.
Train: 2018-08-02T11:47:21.552220: step 11569, loss 0.547614.
Train: 2018-08-02T11:47:21.770894: step 11570, loss 0.578796.
Test: 2018-08-02T11:47:22.973743: step 11570, loss 0.550919.
Train: 2018-08-02T11:47:23.192461: step 11571, loss 0.492434.
Train: 2018-08-02T11:47:23.411134: step 11572, loss 0.578982.
Train: 2018-08-02T11:47:23.629858: step 11573, loss 0.515177.
Train: 2018-08-02T11:47:23.848533: step 11574, loss 0.546519.
Train: 2018-08-02T11:47:24.082852: step 11575, loss 0.578574.
Train: 2018-08-02T11:47:24.301581: step 11576, loss 0.563115.
Train: 2018-08-02T11:47:24.535879: step 11577, loss 0.60531.
Train: 2018-08-02T11:47:24.754594: step 11578, loss 0.513557.
Train: 2018-08-02T11:47:24.973293: step 11579, loss 0.601318.
Train: 2018-08-02T11:47:25.207589: step 11580, loss 0.648357.
Test: 2018-08-02T11:47:26.394811: step 11580, loss 0.549416.
Train: 2018-08-02T11:47:26.613511: step 11581, loss 0.530659.
Train: 2018-08-02T11:47:26.832238: step 11582, loss 0.538511.
Train: 2018-08-02T11:47:27.066554: step 11583, loss 0.579739.
Train: 2018-08-02T11:47:27.300881: step 11584, loss 0.635666.
Train: 2018-08-02T11:47:27.519578: step 11585, loss 0.538613.
Train: 2018-08-02T11:47:27.753867: step 11586, loss 0.619093.
Train: 2018-08-02T11:47:27.972597: step 11587, loss 0.586875.
Train: 2018-08-02T11:47:28.191290: step 11588, loss 0.514935.
Train: 2018-08-02T11:47:28.441209: step 11589, loss 0.570877.
Train: 2018-08-02T11:47:28.691173: step 11590, loss 0.594815.
Test: 2018-08-02T11:47:29.893991: step 11590, loss 0.549623.
Train: 2018-08-02T11:47:30.175175: step 11591, loss 0.578859.
Train: 2018-08-02T11:47:30.393905: step 11592, loss 0.547044.
Train: 2018-08-02T11:47:30.628228: step 11593, loss 0.483499.
Train: 2018-08-02T11:47:30.878161: step 11594, loss 0.594774.
Train: 2018-08-02T11:47:31.096859: step 11595, loss 0.594783.
Train: 2018-08-02T11:47:31.315564: step 11596, loss 0.547016.
Train: 2018-08-02T11:47:31.549879: step 11597, loss 0.562932.
Train: 2018-08-02T11:47:31.784206: step 11598, loss 0.594794.
Train: 2018-08-02T11:47:32.002873: step 11599, loss 0.539033.
Train: 2018-08-02T11:47:32.221597: step 11600, loss 0.562921.
Test: 2018-08-02T11:47:33.408793: step 11600, loss 0.549595.
Train: 2018-08-02T11:47:34.252380: step 11601, loss 0.570886.
Train: 2018-08-02T11:47:34.486695: step 11602, loss 0.674588.
Train: 2018-08-02T11:47:34.721019: step 11603, loss 0.539064.
Train: 2018-08-02T11:47:34.939685: step 11604, loss 0.578858.
Train: 2018-08-02T11:47:35.174006: step 11605, loss 0.507431.
Train: 2018-08-02T11:47:35.392734: step 11606, loss 0.531229.
Train: 2018-08-02T11:47:35.611402: step 11607, loss 0.578859.
Train: 2018-08-02T11:47:35.845722: step 11608, loss 0.578859.
Train: 2018-08-02T11:47:36.080074: step 11609, loss 0.578858.
Train: 2018-08-02T11:47:36.298767: step 11610, loss 0.570901.
Test: 2018-08-02T11:47:37.501585: step 11610, loss 0.550384.
Train: 2018-08-02T11:47:37.704692: step 11611, loss 0.531109.
Train: 2018-08-02T11:47:37.923387: step 11612, loss 0.507164.
Train: 2018-08-02T11:47:38.142090: step 11613, loss 0.554905.
Train: 2018-08-02T11:47:38.392001: step 11614, loss 0.60288.
Train: 2018-08-02T11:47:38.610725: step 11615, loss 0.538786.
Train: 2018-08-02T11:47:38.845051: step 11616, loss 0.57084.
Train: 2018-08-02T11:47:39.063750: step 11617, loss 0.482348.
Train: 2018-08-02T11:47:39.298065: step 11618, loss 0.643461.
Train: 2018-08-02T11:47:39.532390: step 11619, loss 0.554646.
Train: 2018-08-02T11:47:39.751060: step 11620, loss 0.562706.
Test: 2018-08-02T11:47:40.953901: step 11620, loss 0.549349.
Train: 2018-08-02T11:47:41.157004: step 11621, loss 0.538385.
Train: 2018-08-02T11:47:41.375679: step 11622, loss 0.562675.
Train: 2018-08-02T11:47:41.609999: step 11623, loss 0.595176.
Train: 2018-08-02T11:47:41.844345: step 11624, loss 0.611453.
Train: 2018-08-02T11:47:42.063042: step 11625, loss 0.578912.
Train: 2018-08-02T11:47:42.281716: step 11626, loss 0.522055.
Train: 2018-08-02T11:47:42.469202: step 11627, loss 0.528003.
Train: 2018-08-02T11:47:42.687896: step 11628, loss 0.546387.
Train: 2018-08-02T11:47:42.937842: step 11629, loss 0.562635.
Train: 2018-08-02T11:47:43.156537: step 11630, loss 0.570774.
Test: 2018-08-02T11:47:44.359354: step 11630, loss 0.548422.
Train: 2018-08-02T11:47:44.578084: step 11631, loss 0.570772.
Train: 2018-08-02T11:47:44.796757: step 11632, loss 0.513618.
Train: 2018-08-02T11:47:45.031104: step 11633, loss 0.54623.
Train: 2018-08-02T11:47:45.249800: step 11634, loss 0.537982.
Train: 2018-08-02T11:47:45.468470: step 11635, loss 0.488618.
Train: 2018-08-02T11:47:45.687199: step 11636, loss 0.611977.
Train: 2018-08-02T11:47:45.921488: step 11637, loss 0.612078.
Train: 2018-08-02T11:47:46.155834: step 11638, loss 0.554217.
Train: 2018-08-02T11:47:46.374541: step 11639, loss 0.570765.
Train: 2018-08-02T11:47:46.593236: step 11640, loss 0.554196.
Test: 2018-08-02T11:47:47.780428: step 11640, loss 0.548256.
Train: 2018-08-02T11:47:47.999127: step 11641, loss 0.603888.
Train: 2018-08-02T11:47:48.249068: step 11642, loss 0.570783.
Train: 2018-08-02T11:47:48.483419: step 11643, loss 0.57076.
Train: 2018-08-02T11:47:48.702087: step 11644, loss 0.57904.
Train: 2018-08-02T11:47:48.952054: step 11645, loss 0.603765.
Train: 2018-08-02T11:47:49.186348: step 11646, loss 0.63661.
Train: 2018-08-02T11:47:49.420669: step 11647, loss 0.537969.
Train: 2018-08-02T11:47:49.639398: step 11648, loss 0.603467.
Train: 2018-08-02T11:47:49.858095: step 11649, loss 0.611498.
Train: 2018-08-02T11:47:50.076766: step 11650, loss 0.554579.
Test: 2018-08-02T11:47:51.279609: step 11650, loss 0.54906.
Train: 2018-08-02T11:47:51.482686: step 11651, loss 0.611191.
Train: 2018-08-02T11:47:51.717031: step 11652, loss 0.554756.
Train: 2018-08-02T11:47:51.951351: step 11653, loss 0.59488.
Train: 2018-08-02T11:47:52.170049: step 11654, loss 0.53899.
Train: 2018-08-02T11:47:52.388754: step 11655, loss 0.507315.
Train: 2018-08-02T11:47:52.607447: step 11656, loss 0.602677.
Train: 2018-08-02T11:47:52.857364: step 11657, loss 0.570935.
Train: 2018-08-02T11:47:53.076063: step 11658, loss 0.563042.
Train: 2018-08-02T11:47:53.310409: step 11659, loss 0.61835.
Train: 2018-08-02T11:47:53.529081: step 11660, loss 0.563112.
Test: 2018-08-02T11:47:54.731925: step 11660, loss 0.549661.
Train: 2018-08-02T11:47:54.950624: step 11661, loss 0.56315.
Train: 2018-08-02T11:47:55.169323: step 11662, loss 0.492563.
Train: 2018-08-02T11:47:55.403643: step 11663, loss 0.563172.
Train: 2018-08-02T11:47:55.637963: step 11664, loss 0.586727.
Train: 2018-08-02T11:47:55.841070: step 11665, loss 0.547443.
Train: 2018-08-02T11:47:56.075385: step 11666, loss 0.531691.
Train: 2018-08-02T11:47:56.294086: step 11667, loss 0.563113.
Train: 2018-08-02T11:47:56.512784: step 11668, loss 0.547302.
Train: 2018-08-02T11:47:56.731488: step 11669, loss 0.539324.
Train: 2018-08-02T11:47:56.965807: step 11670, loss 0.586788.
Test: 2018-08-02T11:47:58.168621: step 11670, loss 0.549885.
Train: 2018-08-02T11:47:58.387349: step 11671, loss 0.531192.
Train: 2018-08-02T11:47:58.606051: step 11672, loss 0.554961.
Train: 2018-08-02T11:47:58.824747: step 11673, loss 0.514954.
Train: 2018-08-02T11:47:59.074688: step 11674, loss 0.522735.
Train: 2018-08-02T11:47:59.308978: step 11675, loss 0.570823.
Train: 2018-08-02T11:47:59.527707: step 11676, loss 0.570804.
Train: 2018-08-02T11:47:59.746410: step 11677, loss 0.660045.
Train: 2018-08-02T11:47:59.965105: step 11678, loss 0.53831.
Train: 2018-08-02T11:48:00.199428: step 11679, loss 0.587042.
Train: 2018-08-02T11:48:00.433716: step 11680, loss 0.546382.
Test: 2018-08-02T11:48:01.620936: step 11680, loss 0.548669.
Train: 2018-08-02T11:48:01.839636: step 11681, loss 0.562639.
Train: 2018-08-02T11:48:02.073981: step 11682, loss 0.53819.
Train: 2018-08-02T11:48:02.292685: step 11683, loss 0.4892.
Train: 2018-08-02T11:48:02.511353: step 11684, loss 0.554403.
Train: 2018-08-02T11:48:02.745673: step 11685, loss 0.628194.
Train: 2018-08-02T11:48:02.964403: step 11686, loss 0.537911.
Train: 2018-08-02T11:48:03.198719: step 11687, loss 0.488522.
Train: 2018-08-02T11:48:03.433043: step 11688, loss 0.521268.
Train: 2018-08-02T11:48:03.651742: step 11689, loss 0.521091.
Train: 2018-08-02T11:48:03.901652: step 11690, loss 0.47102.
Test: 2018-08-02T11:48:05.088875: step 11690, loss 0.549029.
Train: 2018-08-02T11:48:05.307604: step 11691, loss 0.579131.
Train: 2018-08-02T11:48:05.526272: step 11692, loss 0.595979.
Train: 2018-08-02T11:48:05.745005: step 11693, loss 0.528677.
Train: 2018-08-02T11:48:05.979319: step 11694, loss 0.570816.
Train: 2018-08-02T11:48:06.197990: step 11695, loss 0.528445.
Train: 2018-08-02T11:48:06.416719: step 11696, loss 0.536838.
Train: 2018-08-02T11:48:06.651040: step 11697, loss 0.570868.
Train: 2018-08-02T11:48:06.869710: step 11698, loss 0.545242.
Train: 2018-08-02T11:48:07.104029: step 11699, loss 0.4938.
Train: 2018-08-02T11:48:07.307136: step 11700, loss 0.631094.
Test: 2018-08-02T11:48:08.509949: step 11700, loss 0.54757.
Train: 2018-08-02T11:48:09.431607: step 11701, loss 0.596752.
Train: 2018-08-02T11:48:09.650338: step 11702, loss 0.562335.
Train: 2018-08-02T11:48:09.869039: step 11703, loss 0.553734.
Train: 2018-08-02T11:48:10.087735: step 11704, loss 0.59673.
Train: 2018-08-02T11:48:10.337679: step 11705, loss 0.588097.
Train: 2018-08-02T11:48:10.556346: step 11706, loss 0.528057.
Train: 2018-08-02T11:48:10.775044: step 11707, loss 0.596569.
Train: 2018-08-02T11:48:10.993770: step 11708, loss 0.587955.
Train: 2018-08-02T11:48:11.212441: step 11709, loss 0.562342.
Train: 2018-08-02T11:48:11.446788: step 11710, loss 0.579331.
Test: 2018-08-02T11:48:12.633982: step 11710, loss 0.549192.
Train: 2018-08-02T11:48:12.837085: step 11711, loss 0.528493.
Train: 2018-08-02T11:48:13.055784: step 11712, loss 0.562363.
Train: 2018-08-02T11:48:13.274488: step 11713, loss 0.520229.
Train: 2018-08-02T11:48:13.508778: step 11714, loss 0.503446.
Train: 2018-08-02T11:48:13.743128: step 11715, loss 0.562376.
Train: 2018-08-02T11:48:13.961823: step 11716, loss 0.629728.
Train: 2018-08-02T11:48:14.196149: step 11717, loss 0.511959.
Train: 2018-08-02T11:48:14.414817: step 11718, loss 0.52879.
Train: 2018-08-02T11:48:14.649161: step 11719, loss 0.671585.
Train: 2018-08-02T11:48:14.867835: step 11720, loss 0.612669.
Test: 2018-08-02T11:48:16.070678: step 11720, loss 0.548479.
Train: 2018-08-02T11:48:16.273787: step 11721, loss 0.545724.
Train: 2018-08-02T11:48:16.492484: step 11722, loss 0.56244.
Train: 2018-08-02T11:48:16.742430: step 11723, loss 0.612255.
Train: 2018-08-02T11:48:16.976717: step 11724, loss 0.48808.
Train: 2018-08-02T11:48:17.195444: step 11725, loss 0.570756.
Train: 2018-08-02T11:48:17.414144: step 11726, loss 0.554281.
Train: 2018-08-02T11:48:17.648464: step 11727, loss 0.562533.
Train: 2018-08-02T11:48:17.867166: step 11728, loss 0.578973.
Train: 2018-08-02T11:48:18.101481: step 11729, loss 0.587161.
Train: 2018-08-02T11:48:18.320151: step 11730, loss 0.62804.
Test: 2018-08-02T11:48:19.522995: step 11730, loss 0.549756.
Train: 2018-08-02T11:48:19.741718: step 11731, loss 0.538162.
Train: 2018-08-02T11:48:19.960422: step 11732, loss 0.587046.
Train: 2018-08-02T11:48:20.179120: step 11733, loss 0.546469.
Train: 2018-08-02T11:48:20.444683: step 11734, loss 0.586982.
Train: 2018-08-02T11:48:20.663378: step 11735, loss 0.570814.
Train: 2018-08-02T11:48:20.897703: step 11736, loss 0.53058.
Train: 2018-08-02T11:48:21.116371: step 11737, loss 0.522603.
Train: 2018-08-02T11:48:21.350728: step 11738, loss 0.546722.
Train: 2018-08-02T11:48:21.569416: step 11739, loss 0.554752.
Train: 2018-08-02T11:48:21.788120: step 11740, loss 0.498422.
Test: 2018-08-02T11:48:22.990933: step 11740, loss 0.548917.
Train: 2018-08-02T11:48:23.225286: step 11741, loss 0.546628.
Train: 2018-08-02T11:48:23.443951: step 11742, loss 0.498058.
Train: 2018-08-02T11:48:23.678303: step 11743, loss 0.578903.
Train: 2018-08-02T11:48:23.896996: step 11744, loss 0.587059.
Train: 2018-08-02T11:48:24.131290: step 11745, loss 0.546295.
Train: 2018-08-02T11:48:24.350005: step 11746, loss 0.578946.
Train: 2018-08-02T11:48:24.568712: step 11747, loss 0.61172.
Train: 2018-08-02T11:48:24.803033: step 11748, loss 0.537991.
Train: 2018-08-02T11:48:25.021706: step 11749, loss 0.603566.
Train: 2018-08-02T11:48:25.240406: step 11750, loss 0.562573.
Test: 2018-08-02T11:48:26.427628: step 11750, loss 0.549445.
Train: 2018-08-02T11:48:26.646357: step 11751, loss 0.554363.
Train: 2018-08-02T11:48:26.880647: step 11752, loss 0.611746.
Train: 2018-08-02T11:48:27.099373: step 11753, loss 0.57895.
Train: 2018-08-02T11:48:27.318070: step 11754, loss 0.57894.
Train: 2018-08-02T11:48:27.552364: step 11755, loss 0.570773.
Train: 2018-08-02T11:48:27.771088: step 11756, loss 0.603346.
Train: 2018-08-02T11:48:28.036651: step 11757, loss 0.546426.
Train: 2018-08-02T11:48:28.255350: step 11758, loss 0.603213.
Train: 2018-08-02T11:48:28.489647: step 11759, loss 0.595053.
Train: 2018-08-02T11:48:28.708344: step 11760, loss 0.627216.
Test: 2018-08-02T11:48:29.911187: step 11760, loss 0.549247.
Train: 2018-08-02T11:48:30.129916: step 11761, loss 0.522724.
Train: 2018-08-02T11:48:30.364207: step 11762, loss 0.546875.
Train: 2018-08-02T11:48:30.582904: step 11763, loss 0.634721.
Train: 2018-08-02T11:48:30.801630: step 11764, loss 0.547054.
Train: 2018-08-02T11:48:31.020327: step 11765, loss 0.555071.
Train: 2018-08-02T11:48:31.239035: step 11766, loss 0.49182.
Train: 2018-08-02T11:48:31.504565: step 11767, loss 0.547206.
Train: 2018-08-02T11:48:31.707672: step 11768, loss 0.515509.
Train: 2018-08-02T11:48:31.926371: step 11769, loss 0.539186.
Train: 2018-08-02T11:48:32.160692: step 11770, loss 0.531131.
Test: 2018-08-02T11:48:33.363504: step 11770, loss 0.549388.
Train: 2018-08-02T11:48:33.582227: step 11771, loss 0.650684.
Train: 2018-08-02T11:48:33.800926: step 11772, loss 0.554901.
Train: 2018-08-02T11:48:34.019625: step 11773, loss 0.482924.
Train: 2018-08-02T11:48:34.238332: step 11774, loss 0.522733.
Train: 2018-08-02T11:48:34.472665: step 11775, loss 0.635223.
Train: 2018-08-02T11:48:34.691343: step 11776, loss 0.530492.
Train: 2018-08-02T11:48:34.910018: step 11777, loss 0.514214.
Train: 2018-08-02T11:48:35.097517: step 11778, loss 0.579978.
Train: 2018-08-02T11:48:35.316196: step 11779, loss 0.56267.
Train: 2018-08-02T11:48:35.534901: step 11780, loss 0.55451.
Test: 2018-08-02T11:48:36.737714: step 11780, loss 0.549314.
Train: 2018-08-02T11:48:36.956431: step 11781, loss 0.529832.
Train: 2018-08-02T11:48:37.175142: step 11782, loss 0.61176.
Train: 2018-08-02T11:48:37.393840: step 11783, loss 0.546237.
Train: 2018-08-02T11:48:37.612509: step 11784, loss 0.570719.
Train: 2018-08-02T11:48:37.831238: step 11785, loss 0.636625.
Train: 2018-08-02T11:48:38.081149: step 11786, loss 0.51321.
Train: 2018-08-02T11:48:38.299877: step 11787, loss 0.521407.
Train: 2018-08-02T11:48:38.534198: step 11788, loss 0.562518.
Train: 2018-08-02T11:48:38.752897: step 11789, loss 0.620223.
Train: 2018-08-02T11:48:39.002809: step 11790, loss 0.488345.
Test: 2018-08-02T11:48:40.205651: step 11790, loss 0.549482.
Train: 2018-08-02T11:48:40.408753: step 11791, loss 0.496486.
Train: 2018-08-02T11:48:40.643079: step 11792, loss 0.562482.
Train: 2018-08-02T11:48:40.861777: step 11793, loss 0.579052.
Train: 2018-08-02T11:48:41.096099: step 11794, loss 0.545837.
Train: 2018-08-02T11:48:41.330412: step 11795, loss 0.545796.
Train: 2018-08-02T11:48:41.549086: step 11796, loss 0.537415.
Train: 2018-08-02T11:48:41.783437: step 11797, loss 0.595836.
Train: 2018-08-02T11:48:42.002133: step 11798, loss 0.646047.
Train: 2018-08-02T11:48:42.236455: step 11799, loss 0.537353.
Train: 2018-08-02T11:48:42.455154: step 11800, loss 0.529022.
Test: 2018-08-02T11:48:43.657968: step 11800, loss 0.54828.
Train: 2018-08-02T11:48:44.517142: step 11801, loss 0.529017.
Train: 2018-08-02T11:48:44.735841: step 11802, loss 0.478846.
Train: 2018-08-02T11:48:44.954539: step 11803, loss 0.545645.
Train: 2018-08-02T11:48:45.173239: step 11804, loss 0.570786.
Train: 2018-08-02T11:48:45.391937: step 11805, loss 0.511897.
Train: 2018-08-02T11:48:45.626262: step 11806, loss 0.57924.
Train: 2018-08-02T11:48:45.860577: step 11807, loss 0.528556.
Train: 2018-08-02T11:48:46.063685: step 11808, loss 0.596236.
Train: 2018-08-02T11:48:46.282372: step 11809, loss 0.570831.
Train: 2018-08-02T11:48:46.501053: step 11810, loss 0.56235.
Test: 2018-08-02T11:48:47.703896: step 11810, loss 0.548958.
Train: 2018-08-02T11:48:47.922620: step 11811, loss 0.570835.
Train: 2018-08-02T11:48:48.141321: step 11812, loss 0.494471.
Train: 2018-08-02T11:48:48.360022: step 11813, loss 0.638802.
Train: 2018-08-02T11:48:48.578721: step 11814, loss 0.494447.
Train: 2018-08-02T11:48:48.797414: step 11815, loss 0.60481.
Train: 2018-08-02T11:48:49.031741: step 11816, loss 0.570834.
Train: 2018-08-02T11:48:49.250438: step 11817, loss 0.503005.
Train: 2018-08-02T11:48:49.469136: step 11818, loss 0.57931.
Train: 2018-08-02T11:48:49.703428: step 11819, loss 0.553877.
Train: 2018-08-02T11:48:49.953394: step 11820, loss 0.545411.
Test: 2018-08-02T11:48:51.140591: step 11820, loss 0.548593.
Train: 2018-08-02T11:48:51.359289: step 11821, loss 0.545409.
Train: 2018-08-02T11:48:51.593640: step 11822, loss 0.570828.
Train: 2018-08-02T11:48:51.827956: step 11823, loss 0.613174.
Train: 2018-08-02T11:48:52.062250: step 11824, loss 0.553904.
Train: 2018-08-02T11:48:52.296570: step 11825, loss 0.511705.
Train: 2018-08-02T11:48:52.515268: step 11826, loss 0.528606.
Train: 2018-08-02T11:48:52.749620: step 11827, loss 0.537039.
Train: 2018-08-02T11:48:52.968321: step 11828, loss 0.503231.
Train: 2018-08-02T11:48:53.202641: step 11829, loss 0.596205.
Train: 2018-08-02T11:48:53.436952: step 11830, loss 0.545422.
Test: 2018-08-02T11:48:54.624152: step 11830, loss 0.549179.
Train: 2018-08-02T11:48:54.842881: step 11831, loss 0.587771.
Train: 2018-08-02T11:48:55.061579: step 11832, loss 0.562354.
Train: 2018-08-02T11:48:55.295897: step 11833, loss 0.52848.
Train: 2018-08-02T11:48:55.530224: step 11834, loss 0.494586.
Train: 2018-08-02T11:48:55.748919: step 11835, loss 0.56235.
Train: 2018-08-02T11:48:55.967586: step 11836, loss 0.570841.
Train: 2018-08-02T11:48:56.201905: step 11837, loss 0.485843.
Train: 2018-08-02T11:48:56.420616: step 11838, loss 0.596414.
Train: 2018-08-02T11:48:56.639303: step 11839, loss 0.587917.
Train: 2018-08-02T11:48:56.858002: step 11840, loss 0.596444.
Test: 2018-08-02T11:48:58.060845: step 11840, loss 0.54792.
Train: 2018-08-02T11:48:58.263947: step 11841, loss 0.468647.
Train: 2018-08-02T11:48:58.498273: step 11842, loss 0.528236.
Train: 2018-08-02T11:48:58.716970: step 11843, loss 0.536724.
Train: 2018-08-02T11:48:58.951317: step 11844, loss 0.562336.
Train: 2018-08-02T11:48:59.169960: step 11845, loss 0.553774.
Train: 2018-08-02T11:48:59.404282: step 11846, loss 0.545195.
Train: 2018-08-02T11:48:59.638627: step 11847, loss 0.579493.
Train: 2018-08-02T11:48:59.857333: step 11848, loss 0.570917.
Train: 2018-08-02T11:49:00.107241: step 11849, loss 0.519437.
Train: 2018-08-02T11:49:00.325969: step 11850, loss 0.605254.
Test: 2018-08-02T11:49:01.528783: step 11850, loss 0.547812.
Train: 2018-08-02T11:49:01.731884: step 11851, loss 0.6138.
Train: 2018-08-02T11:49:01.966209: step 11852, loss 0.519537.
Train: 2018-08-02T11:49:02.200526: step 11853, loss 0.519589.
Train: 2018-08-02T11:49:02.419230: step 11854, loss 0.562337.
Train: 2018-08-02T11:49:02.653545: step 11855, loss 0.664837.
Train: 2018-08-02T11:49:02.872249: step 11856, loss 0.638985.
Train: 2018-08-02T11:49:03.090945: step 11857, loss 0.613211.
Train: 2018-08-02T11:49:03.325277: step 11858, loss 0.553941.
Train: 2018-08-02T11:49:03.557977: step 11859, loss 0.545618.
Train: 2018-08-02T11:49:03.776648: step 11860, loss 0.587473.
Test: 2018-08-02T11:49:04.979490: step 11860, loss 0.548945.
Train: 2018-08-02T11:49:05.182593: step 11861, loss 0.587388.
Train: 2018-08-02T11:49:05.416916: step 11862, loss 0.521115.
Train: 2018-08-02T11:49:05.635617: step 11863, loss 0.620227.
Train: 2018-08-02T11:49:05.854316: step 11864, loss 0.53793.
Train: 2018-08-02T11:49:06.073017: step 11865, loss 0.562588.
Train: 2018-08-02T11:49:06.291683: step 11866, loss 0.554471.
Train: 2018-08-02T11:49:06.526042: step 11867, loss 0.627692.
Train: 2018-08-02T11:49:06.744726: step 11868, loss 0.59509.
Train: 2018-08-02T11:49:06.963431: step 11869, loss 0.562757.
Train: 2018-08-02T11:49:07.182099: step 11870, loss 0.57084.
Test: 2018-08-02T11:49:08.369321: step 11870, loss 0.549315.
Train: 2018-08-02T11:49:08.572425: step 11871, loss 0.586864.
Train: 2018-08-02T11:49:08.791122: step 11872, loss 0.531037.
Train: 2018-08-02T11:49:09.009797: step 11873, loss 0.547056.
Train: 2018-08-02T11:49:09.228495: step 11874, loss 0.531237.
Train: 2018-08-02T11:49:09.447226: step 11875, loss 0.586791.
Train: 2018-08-02T11:49:09.665919: step 11876, loss 0.570935.
Train: 2018-08-02T11:49:09.900239: step 11877, loss 0.555109.
Train: 2018-08-02T11:49:10.118942: step 11878, loss 0.657992.
Train: 2018-08-02T11:49:10.353232: step 11879, loss 0.539404.
Train: 2018-08-02T11:49:10.571963: step 11880, loss 0.570987.
Test: 2018-08-02T11:49:11.759153: step 11880, loss 0.550358.
Train: 2018-08-02T11:49:11.977856: step 11881, loss 0.547401.
Train: 2018-08-02T11:49:12.196582: step 11882, loss 0.57887.
Train: 2018-08-02T11:49:12.415249: step 11883, loss 0.618141.
Train: 2018-08-02T11:49:12.633979: step 11884, loss 0.578877.
Train: 2018-08-02T11:49:12.852677: step 11885, loss 0.594527.
Train: 2018-08-02T11:49:13.086969: step 11886, loss 0.594497.
Train: 2018-08-02T11:49:13.321320: step 11887, loss 0.617809.
Train: 2018-08-02T11:49:13.555609: step 11888, loss 0.578918.
Train: 2018-08-02T11:49:13.774336: step 11889, loss 0.617565.
Train: 2018-08-02T11:49:13.993006: step 11890, loss 0.586653.
Test: 2018-08-02T11:49:15.180227: step 11890, loss 0.549991.
Train: 2018-08-02T11:49:15.398955: step 11891, loss 0.609631.
Train: 2018-08-02T11:49:15.617649: step 11892, loss 0.594273.
Train: 2018-08-02T11:49:15.836347: step 11893, loss 0.548708.
Train: 2018-08-02T11:49:16.055055: step 11894, loss 0.616907.
Train: 2018-08-02T11:49:16.273721: step 11895, loss 0.556541.
Train: 2018-08-02T11:49:16.492419: step 11896, loss 0.541628.
Train: 2018-08-02T11:49:16.711148: step 11897, loss 0.52672.
Train: 2018-08-02T11:49:16.929845: step 11898, loss 0.481746.
Train: 2018-08-02T11:49:17.148547: step 11899, loss 0.526537.
Train: 2018-08-02T11:49:17.367245: step 11900, loss 0.55647.
Test: 2018-08-02T11:49:18.554436: step 11900, loss 0.550554.
Train: 2018-08-02T11:49:19.382397: step 11901, loss 0.556332.
Train: 2018-08-02T11:49:19.601097: step 11902, loss 0.540962.
Train: 2018-08-02T11:49:19.819790: step 11903, loss 0.594301.
Train: 2018-08-02T11:49:20.038488: step 11904, loss 0.540549.
Train: 2018-08-02T11:49:20.257193: step 11905, loss 0.517189.
Train: 2018-08-02T11:49:20.475892: step 11906, loss 0.617725.
Train: 2018-08-02T11:49:20.710183: step 11907, loss 0.594481.
Train: 2018-08-02T11:49:20.944527: step 11908, loss 0.500731.
Train: 2018-08-02T11:49:21.178848: step 11909, loss 0.594574.
Train: 2018-08-02T11:49:21.397551: step 11910, loss 0.563111.
Test: 2018-08-02T11:49:22.584743: step 11910, loss 0.550221.
Train: 2018-08-02T11:49:22.803471: step 11911, loss 0.578861.
Train: 2018-08-02T11:49:23.037781: step 11912, loss 0.618479.
Train: 2018-08-02T11:49:23.256486: step 11913, loss 0.507462.
Train: 2018-08-02T11:49:23.475184: step 11914, loss 0.539091.
Train: 2018-08-02T11:49:23.693891: step 11915, loss 0.554927.
Train: 2018-08-02T11:49:23.912586: step 11916, loss 0.570861.
Train: 2018-08-02T11:49:24.146902: step 11917, loss 0.570845.
Train: 2018-08-02T11:49:24.365575: step 11918, loss 0.570832.
Train: 2018-08-02T11:49:24.584305: step 11919, loss 0.498306.
Train: 2018-08-02T11:49:24.803003: step 11920, loss 0.56272.
Test: 2018-08-02T11:49:25.990195: step 11920, loss 0.549513.
Train: 2018-08-02T11:49:26.208894: step 11921, loss 0.554573.
Train: 2018-08-02T11:49:26.443246: step 11922, loss 0.603316.
Train: 2018-08-02T11:49:26.677567: step 11923, loss 0.603367.
Train: 2018-08-02T11:49:26.896257: step 11924, loss 0.546318.
Train: 2018-08-02T11:49:27.114962: step 11925, loss 0.505497.
Train: 2018-08-02T11:49:27.333661: step 11926, loss 0.50535.
Train: 2018-08-02T11:49:27.583572: step 11927, loss 0.546148.
Train: 2018-08-02T11:49:27.817917: step 11928, loss 0.578989.
Train: 2018-08-02T11:49:28.005378: step 11929, loss 0.615321.
Train: 2018-08-02T11:49:28.224077: step 11930, loss 0.504669.
Test: 2018-08-02T11:49:29.426891: step 11930, loss 0.547889.
Train: 2018-08-02T11:49:29.661241: step 11931, loss 0.595593.
Train: 2018-08-02T11:49:29.879934: step 11932, loss 0.52103.
Train: 2018-08-02T11:49:30.098608: step 11933, loss 0.579062.
Train: 2018-08-02T11:49:30.317308: step 11934, loss 0.49594.
Train: 2018-08-02T11:49:30.536035: step 11935, loss 0.554094.
Train: 2018-08-02T11:49:30.770352: step 11936, loss 0.620894.
Train: 2018-08-02T11:49:30.989051: step 11937, loss 0.520613.
Train: 2018-08-02T11:49:31.207754: step 11938, loss 0.545662.
Train: 2018-08-02T11:49:31.426452: step 11939, loss 0.612691.
Train: 2018-08-02T11:49:31.645152: step 11940, loss 0.554007.
Test: 2018-08-02T11:49:32.832343: step 11940, loss 0.548194.
Train: 2018-08-02T11:49:33.097906: step 11941, loss 0.595953.
Train: 2018-08-02T11:49:33.316637: step 11942, loss 0.545618.
Train: 2018-08-02T11:49:33.535334: step 11943, loss 0.545666.
Train: 2018-08-02T11:49:33.754032: step 11944, loss 0.503788.
Train: 2018-08-02T11:49:34.003980: step 11945, loss 0.545609.
Train: 2018-08-02T11:49:34.222673: step 11946, loss 0.587604.
Train: 2018-08-02T11:49:34.456994: step 11947, loss 0.562387.
Train: 2018-08-02T11:49:34.675692: step 11948, loss 0.562387.
Train: 2018-08-02T11:49:34.894391: step 11949, loss 0.545663.
Train: 2018-08-02T11:49:35.144326: step 11950, loss 0.495204.
Test: 2018-08-02T11:49:36.331523: step 11950, loss 0.547951.
Train: 2018-08-02T11:49:36.534627: step 11951, loss 0.511923.
Train: 2018-08-02T11:49:36.753331: step 11952, loss 0.58768.
Train: 2018-08-02T11:49:36.972023: step 11953, loss 0.579242.
Train: 2018-08-02T11:49:37.190725: step 11954, loss 0.570809.
Train: 2018-08-02T11:49:37.440667: step 11955, loss 0.54547.
Train: 2018-08-02T11:49:37.674989: step 11956, loss 0.51166.
Train: 2018-08-02T11:49:37.893688: step 11957, loss 0.562358.
Train: 2018-08-02T11:49:38.112356: step 11958, loss 0.596235.
Train: 2018-08-02T11:49:38.346705: step 11959, loss 0.613173.
Train: 2018-08-02T11:49:38.581031: step 11960, loss 0.528527.
Test: 2018-08-02T11:49:39.768218: step 11960, loss 0.547856.
Train: 2018-08-02T11:49:39.971297: step 11961, loss 0.545457.
Train: 2018-08-02T11:49:40.205642: step 11962, loss 0.562363.
Train: 2018-08-02T11:49:40.439962: step 11963, loss 0.663692.
Train: 2018-08-02T11:49:40.658665: step 11964, loss 0.638146.
Train: 2018-08-02T11:49:40.892986: step 11965, loss 0.528877.
Train: 2018-08-02T11:49:41.111678: step 11966, loss 0.554067.
Train: 2018-08-02T11:49:41.330384: step 11967, loss 0.520812.
Train: 2018-08-02T11:49:41.549082: step 11968, loss 0.504289.
Train: 2018-08-02T11:49:41.783397: step 11969, loss 0.545845.
Train: 2018-08-02T11:49:42.002096: step 11970, loss 0.612276.
Test: 2018-08-02T11:49:43.189292: step 11970, loss 0.549572.
Train: 2018-08-02T11:49:43.392369: step 11971, loss 0.645385.
Train: 2018-08-02T11:49:43.611095: step 11972, loss 0.562492.
Train: 2018-08-02T11:49:43.876633: step 11973, loss 0.611955.
Train: 2018-08-02T11:49:44.110982: step 11974, loss 0.578968.
Train: 2018-08-02T11:49:44.314062: step 11975, loss 0.513551.
Train: 2018-08-02T11:49:44.548374: step 11976, loss 0.497389.
Train: 2018-08-02T11:49:44.782679: step 11977, loss 0.538181.
Train: 2018-08-02T11:49:45.016989: step 11978, loss 0.538181.
Train: 2018-08-02T11:49:45.235713: step 11979, loss 0.472926.
Train: 2018-08-02T11:49:45.470039: step 11980, loss 0.505355.
Test: 2018-08-02T11:49:46.657231: step 11980, loss 0.549411.
Train: 2018-08-02T11:49:46.891586: step 11981, loss 0.54614.
Train: 2018-08-02T11:49:47.110281: step 11982, loss 0.603715.
Train: 2018-08-02T11:49:47.328974: step 11983, loss 0.661609.
Train: 2018-08-02T11:49:47.547675: step 11984, loss 0.579007.
Train: 2018-08-02T11:49:47.766345: step 11985, loss 0.529511.
Train: 2018-08-02T11:49:47.985075: step 11986, loss 0.537759.
Train: 2018-08-02T11:49:48.219365: step 11987, loss 0.512978.
Train: 2018-08-02T11:49:48.438096: step 11988, loss 0.512885.
Train: 2018-08-02T11:49:48.656793: step 11989, loss 0.570758.
Train: 2018-08-02T11:49:48.875492: step 11990, loss 0.579064.
Test: 2018-08-02T11:49:50.062683: step 11990, loss 0.548365.
Train: 2018-08-02T11:49:50.281407: step 11991, loss 0.537497.
Train: 2018-08-02T11:49:50.515702: step 11992, loss 0.629072.
Train: 2018-08-02T11:49:50.750025: step 11993, loss 0.520793.
Train: 2018-08-02T11:49:50.953131: step 11994, loss 0.612436.
Train: 2018-08-02T11:49:51.187451: step 11995, loss 0.545776.
Train: 2018-08-02T11:49:51.406149: step 11996, loss 0.562436.
Train: 2018-08-02T11:49:51.624848: step 11997, loss 0.570762.
Train: 2018-08-02T11:49:51.843546: step 11998, loss 0.504216.
Train: 2018-08-02T11:49:52.077861: step 11999, loss 0.654.
Train: 2018-08-02T11:49:52.296566: step 12000, loss 0.57076.
Test: 2018-08-02T11:49:53.499378: step 12000, loss 0.548226.
Train: 2018-08-02T11:49:54.436683: step 12001, loss 0.595645.
Train: 2018-08-02T11:49:54.670981: step 12002, loss 0.570757.
Train: 2018-08-02T11:49:54.905299: step 12003, loss 0.587274.
Train: 2018-08-02T11:49:55.139649: step 12004, loss 0.537806.
Train: 2018-08-02T11:49:55.358351: step 12005, loss 0.562536.
Train: 2018-08-02T11:49:55.577047: step 12006, loss 0.578969.
Train: 2018-08-02T11:49:55.795716: step 12007, loss 0.472456.
Train: 2018-08-02T11:49:56.014444: step 12008, loss 0.56257.
Train: 2018-08-02T11:49:56.233138: step 12009, loss 0.546179.
Train: 2018-08-02T11:49:56.451838: step 12010, loss 0.513374.
Test: 2018-08-02T11:49:57.639034: step 12010, loss 0.549028.
Train: 2018-08-02T11:49:57.842136: step 12011, loss 0.58718.
Train: 2018-08-02T11:49:58.060842: step 12012, loss 0.644703.
Train: 2018-08-02T11:49:58.295130: step 12013, loss 0.562554.
Train: 2018-08-02T11:49:58.529475: step 12014, loss 0.587158.
Train: 2018-08-02T11:49:58.748150: step 12015, loss 0.521651.
Train: 2018-08-02T11:49:58.982500: step 12016, loss 0.603492.
Train: 2018-08-02T11:49:59.216791: step 12017, loss 0.546257.
Train: 2018-08-02T11:49:59.451140: step 12018, loss 0.587098.
Train: 2018-08-02T11:49:59.669837: step 12019, loss 0.521858.
Train: 2018-08-02T11:49:59.888539: step 12020, loss 0.595226.
Test: 2018-08-02T11:50:01.091350: step 12020, loss 0.548475.
Train: 2018-08-02T11:50:01.294456: step 12021, loss 0.578921.
Train: 2018-08-02T11:50:01.528774: step 12022, loss 0.505723.
Train: 2018-08-02T11:50:01.747476: step 12023, loss 0.538247.
Train: 2018-08-02T11:50:01.966178: step 12024, loss 0.530084.
Train: 2018-08-02T11:50:02.184844: step 12025, loss 0.587075.
Train: 2018-08-02T11:50:02.403573: step 12026, loss 0.570774.
Train: 2018-08-02T11:50:02.622272: step 12027, loss 0.554455.
Train: 2018-08-02T11:50:02.840971: step 12028, loss 0.538118.
Train: 2018-08-02T11:50:03.059664: step 12029, loss 0.619798.
Train: 2018-08-02T11:50:03.293991: step 12030, loss 0.529925.
Test: 2018-08-02T11:50:04.481181: step 12030, loss 0.54782.
Train: 2018-08-02T11:50:04.684262: step 12031, loss 0.554424.
Train: 2018-08-02T11:50:04.918604: step 12032, loss 0.505356.
Train: 2018-08-02T11:50:05.137308: step 12033, loss 0.603525.
Train: 2018-08-02T11:50:05.356003: step 12034, loss 0.578958.
Train: 2018-08-02T11:50:05.590322: step 12035, loss 0.644534.
Train: 2018-08-02T11:50:05.824618: step 12036, loss 0.505297.
Train: 2018-08-02T11:50:06.043316: step 12037, loss 0.595311.
Train: 2018-08-02T11:50:06.262044: step 12038, loss 0.611638.
Train: 2018-08-02T11:50:06.480714: step 12039, loss 0.505506.
Train: 2018-08-02T11:50:06.699439: step 12040, loss 0.521849.
Test: 2018-08-02T11:50:07.886634: step 12040, loss 0.548803.
Train: 2018-08-02T11:50:08.089712: step 12041, loss 0.587088.
Train: 2018-08-02T11:50:08.308410: step 12042, loss 0.546303.
Train: 2018-08-02T11:50:08.542755: step 12043, loss 0.562614.
Train: 2018-08-02T11:50:08.761429: step 12044, loss 0.562612.
Train: 2018-08-02T11:50:08.995750: step 12045, loss 0.595257.
Train: 2018-08-02T11:50:09.214473: step 12046, loss 0.513673.
Train: 2018-08-02T11:50:09.433147: step 12047, loss 0.595257.
Train: 2018-08-02T11:50:09.651871: step 12048, loss 0.538132.
Train: 2018-08-02T11:50:09.870570: step 12049, loss 0.570771.
Train: 2018-08-02T11:50:10.089274: step 12050, loss 0.513624.
Test: 2018-08-02T11:50:11.292086: step 12050, loss 0.548192.
Train: 2018-08-02T11:50:11.510816: step 12051, loss 0.570768.
Train: 2018-08-02T11:50:11.729515: step 12052, loss 0.513503.
Train: 2018-08-02T11:50:11.948213: step 12053, loss 0.554372.
Train: 2018-08-02T11:50:12.166881: step 12054, loss 0.57897.
Train: 2018-08-02T11:50:12.385580: step 12055, loss 0.620079.
Train: 2018-08-02T11:50:12.635547: step 12056, loss 0.537885.
Train: 2018-08-02T11:50:12.854225: step 12057, loss 0.57898.
Train: 2018-08-02T11:50:13.072951: step 12058, loss 0.521435.
Train: 2018-08-02T11:50:13.291651: step 12059, loss 0.653026.
Train: 2018-08-02T11:50:13.525939: step 12060, loss 0.61184.
Test: 2018-08-02T11:50:14.697539: step 12060, loss 0.548496.
Train: 2018-08-02T11:50:14.931892: step 12061, loss 0.562565.
Train: 2018-08-02T11:50:15.134968: step 12062, loss 0.562585.
Train: 2018-08-02T11:50:15.353666: step 12063, loss 0.578936.
Train: 2018-08-02T11:50:15.572334: step 12064, loss 0.595223.
Train: 2018-08-02T11:50:15.822276: step 12065, loss 0.554528.
Train: 2018-08-02T11:50:16.025355: step 12066, loss 0.489681.
Train: 2018-08-02T11:50:16.244085: step 12067, loss 0.530249.
Train: 2018-08-02T11:50:16.462752: step 12068, loss 0.578903.
Train: 2018-08-02T11:50:16.681477: step 12069, loss 0.538336.
Train: 2018-08-02T11:50:16.900149: step 12070, loss 0.530193.
Test: 2018-08-02T11:50:18.087371: step 12070, loss 0.548889.
Train: 2018-08-02T11:50:18.306070: step 12071, loss 0.578913.
Train: 2018-08-02T11:50:18.524768: step 12072, loss 0.578917.
Train: 2018-08-02T11:50:18.759138: step 12073, loss 0.530079.
Train: 2018-08-02T11:50:18.962166: step 12074, loss 0.578925.
Train: 2018-08-02T11:50:19.212131: step 12075, loss 0.627852.
Train: 2018-08-02T11:50:19.430831: step 12076, loss 0.546335.
Train: 2018-08-02T11:50:19.649534: step 12077, loss 0.595209.
Train: 2018-08-02T11:50:19.868203: step 12078, loss 0.660267.
Train: 2018-08-02T11:50:20.086905: step 12079, loss 0.562682.
Train: 2018-08-02T11:50:20.290004: step 12080, loss 0.510954.
Test: 2018-08-02T11:50:21.477202: step 12080, loss 0.549057.
Train: 2018-08-02T11:50:21.680279: step 12081, loss 0.48196.
Train: 2018-08-02T11:50:21.899009: step 12082, loss 0.619301.
Train: 2018-08-02T11:50:22.117702: step 12083, loss 0.546576.
Train: 2018-08-02T11:50:22.352028: step 12084, loss 0.514276.
Train: 2018-08-02T11:50:22.570726: step 12085, loss 0.595058.
Train: 2018-08-02T11:50:22.789422: step 12086, loss 0.530375.
Train: 2018-08-02T11:50:23.008093: step 12087, loss 0.611267.
Train: 2018-08-02T11:50:23.226824: step 12088, loss 0.578893.
Train: 2018-08-02T11:50:23.461137: step 12089, loss 0.611243.
Train: 2018-08-02T11:50:23.679839: step 12090, loss 0.530429.
Test: 2018-08-02T11:50:24.867032: step 12090, loss 0.550004.
Train: 2018-08-02T11:50:25.085763: step 12091, loss 0.578884.
Train: 2018-08-02T11:50:25.304463: step 12092, loss 0.635337.
Train: 2018-08-02T11:50:25.523160: step 12093, loss 0.546689.
Train: 2018-08-02T11:50:25.757487: step 12094, loss 0.578871.
Train: 2018-08-02T11:50:25.976173: step 12095, loss 0.506694.
Train: 2018-08-02T11:50:26.194877: step 12096, loss 0.546798.
Train: 2018-08-02T11:50:26.413573: step 12097, loss 0.554811.
Train: 2018-08-02T11:50:26.632275: step 12098, loss 0.602935.
Train: 2018-08-02T11:50:26.850983: step 12099, loss 0.570848.
Train: 2018-08-02T11:50:27.085264: step 12100, loss 0.53879.
Test: 2018-08-02T11:50:28.272485: step 12100, loss 0.548152.
Train: 2018-08-02T11:50:29.178524: step 12101, loss 0.482664.
Train: 2018-08-02T11:50:29.412843: step 12102, loss 0.546737.
Train: 2018-08-02T11:50:29.631572: step 12103, loss 0.506412.
Train: 2018-08-02T11:50:29.850270: step 12104, loss 0.627359.
Train: 2018-08-02T11:50:30.084586: step 12105, loss 0.586985.
Train: 2018-08-02T11:50:30.303292: step 12106, loss 0.603195.
Train: 2018-08-02T11:50:30.521984: step 12107, loss 0.4898.
Train: 2018-08-02T11:50:30.756280: step 12108, loss 0.546452.
Train: 2018-08-02T11:50:30.959381: step 12109, loss 0.619552.
Train: 2018-08-02T11:50:31.178057: step 12110, loss 0.587046.
Test: 2018-08-02T11:50:32.380898: step 12110, loss 0.548886.
Train: 2018-08-02T11:50:32.583977: step 12111, loss 0.513869.
Train: 2018-08-02T11:50:32.802707: step 12112, loss 0.578918.
Train: 2018-08-02T11:50:33.021399: step 12113, loss 0.562635.
Train: 2018-08-02T11:50:33.240098: step 12114, loss 0.56263.
Train: 2018-08-02T11:50:33.474423: step 12115, loss 0.570775.
Train: 2018-08-02T11:50:33.693116: step 12116, loss 0.63599.
Train: 2018-08-02T11:50:33.911791: step 12117, loss 0.473092.
Train: 2018-08-02T11:50:34.130520: step 12118, loss 0.530046.
Train: 2018-08-02T11:50:34.349218: step 12119, loss 0.587087.
Train: 2018-08-02T11:50:34.599153: step 12120, loss 0.570771.
Test: 2018-08-02T11:50:35.786351: step 12120, loss 0.549154.
Train: 2018-08-02T11:50:36.005083: step 12121, loss 0.546272.
Train: 2018-08-02T11:50:36.223779: step 12122, loss 0.562597.
Train: 2018-08-02T11:50:36.442474: step 12123, loss 0.513527.
Train: 2018-08-02T11:50:36.676800: step 12124, loss 0.611716.
Train: 2018-08-02T11:50:36.895496: step 12125, loss 0.611731.
Train: 2018-08-02T11:50:37.114195: step 12126, loss 0.619889.
Train: 2018-08-02T11:50:37.332888: step 12127, loss 0.513565.
Train: 2018-08-02T11:50:37.551595: step 12128, loss 0.529938.
Train: 2018-08-02T11:50:37.770262: step 12129, loss 0.578937.
Train: 2018-08-02T11:50:37.988961: step 12130, loss 0.570771.
Test: 2018-08-02T11:50:39.191803: step 12130, loss 0.548417.
Train: 2018-08-02T11:50:39.394906: step 12131, loss 0.603417.
Train: 2018-08-02T11:50:39.613580: step 12132, loss 0.562624.
Train: 2018-08-02T11:50:39.832309: step 12133, loss 0.538213.
Train: 2018-08-02T11:50:40.082245: step 12134, loss 0.554505.
Train: 2018-08-02T11:50:40.316566: step 12135, loss 0.578916.
Train: 2018-08-02T11:50:40.535239: step 12136, loss 0.562652.
Train: 2018-08-02T11:50:40.753964: step 12137, loss 0.578911.
Train: 2018-08-02T11:50:40.972676: step 12138, loss 0.570787.
Train: 2018-08-02T11:50:41.206957: step 12139, loss 0.546452.
Train: 2018-08-02T11:50:41.425655: step 12140, loss 0.61134.
Test: 2018-08-02T11:50:42.628499: step 12140, loss 0.549175.
Train: 2018-08-02T11:50:42.831576: step 12141, loss 0.489816.
Train: 2018-08-02T11:50:43.050303: step 12142, loss 0.611302.
Train: 2018-08-02T11:50:43.284620: step 12143, loss 0.497941.
Train: 2018-08-02T11:50:43.503295: step 12144, loss 0.457367.
Train: 2018-08-02T11:50:43.721994: step 12145, loss 0.53015.
Train: 2018-08-02T11:50:43.940691: step 12146, loss 0.595239.
Train: 2018-08-02T11:50:44.175037: step 12147, loss 0.587117.
Train: 2018-08-02T11:50:44.378114: step 12148, loss 0.546201.
Train: 2018-08-02T11:50:44.596817: step 12149, loss 0.619977.
Train: 2018-08-02T11:50:44.815512: step 12150, loss 0.587172.
Test: 2018-08-02T11:50:46.018329: step 12150, loss 0.549423.
Train: 2018-08-02T11:50:46.237055: step 12151, loss 0.61178.
Train: 2018-08-02T11:50:46.471348: step 12152, loss 0.578956.
Train: 2018-08-02T11:50:46.705700: step 12153, loss 0.587127.
Train: 2018-08-02T11:50:46.924400: step 12154, loss 0.521782.
Train: 2018-08-02T11:50:47.143097: step 12155, loss 0.578931.
Train: 2018-08-02T11:50:47.361765: step 12156, loss 0.611519.
Train: 2018-08-02T11:50:47.596085: step 12157, loss 0.489465.
Train: 2018-08-02T11:50:47.814814: step 12158, loss 0.611431.
Train: 2018-08-02T11:50:48.033513: step 12159, loss 0.505835.
Train: 2018-08-02T11:50:48.252181: step 12160, loss 0.651986.
Test: 2018-08-02T11:50:49.455025: step 12160, loss 0.54878.
Train: 2018-08-02T11:50:49.658132: step 12161, loss 0.530269.
Train: 2018-08-02T11:50:49.876832: step 12162, loss 0.578896.
Train: 2018-08-02T11:50:50.095531: step 12163, loss 0.473742.
Train: 2018-08-02T11:50:50.314224: step 12164, loss 0.603183.
Train: 2018-08-02T11:50:50.532897: step 12165, loss 0.530318.
Train: 2018-08-02T11:50:50.751628: step 12166, loss 0.497876.
Train: 2018-08-02T11:50:50.970297: step 12167, loss 0.578907.
Train: 2018-08-02T11:50:51.189019: step 12168, loss 0.521991.
Train: 2018-08-02T11:50:51.407718: step 12169, loss 0.595227.
Train: 2018-08-02T11:50:51.642039: step 12170, loss 0.595257.
Test: 2018-08-02T11:50:52.844856: step 12170, loss 0.549718.
Train: 2018-08-02T11:50:53.047935: step 12171, loss 0.562604.
Train: 2018-08-02T11:50:53.282268: step 12172, loss 0.448224.
Train: 2018-08-02T11:50:53.500977: step 12173, loss 0.554378.
Train: 2018-08-02T11:50:53.719684: step 12174, loss 0.529684.
Train: 2018-08-02T11:50:53.938376: step 12175, loss 0.554276.
Train: 2018-08-02T11:50:54.157074: step 12176, loss 0.603813.
Train: 2018-08-02T11:50:54.407020: step 12177, loss 0.579035.
Train: 2018-08-02T11:50:54.625719: step 12178, loss 0.504462.
Train: 2018-08-02T11:50:54.844418: step 12179, loss 0.537542.
Train: 2018-08-02T11:50:55.063111: step 12180, loss 0.579086.
Test: 2018-08-02T11:50:56.265930: step 12180, loss 0.549464.
Train: 2018-08-02T11:50:56.469044: step 12181, loss 0.570765.
Train: 2018-08-02T11:50:56.687736: step 12182, loss 0.579115.
Train: 2018-08-02T11:50:56.906433: step 12183, loss 0.529006.
Train: 2018-08-02T11:50:57.140756: step 12184, loss 0.620945.
Train: 2018-08-02T11:50:57.359424: step 12185, loss 0.604211.
Train: 2018-08-02T11:50:57.593745: step 12186, loss 0.562419.
Train: 2018-08-02T11:50:57.796854: step 12187, loss 0.554087.
Train: 2018-08-02T11:50:58.015546: step 12188, loss 0.595756.
Train: 2018-08-02T11:50:58.234249: step 12189, loss 0.512545.
Train: 2018-08-02T11:50:58.452918: step 12190, loss 0.537514.
Test: 2018-08-02T11:50:59.640141: step 12190, loss 0.547997.
Train: 2018-08-02T11:50:59.858863: step 12191, loss 0.662176.
Train: 2018-08-02T11:51:00.093189: step 12192, loss 0.562466.
Train: 2018-08-02T11:51:00.296236: step 12193, loss 0.529385.
Train: 2018-08-02T11:51:00.514936: step 12194, loss 0.645129.
Train: 2018-08-02T11:51:00.733634: step 12195, loss 0.554278.
Train: 2018-08-02T11:51:00.967984: step 12196, loss 0.505011.
Train: 2018-08-02T11:51:01.186684: step 12197, loss 0.529713.
Train: 2018-08-02T11:51:01.405368: step 12198, loss 0.480487.
Train: 2018-08-02T11:51:01.624081: step 12199, loss 0.529668.
Train: 2018-08-02T11:51:01.842751: step 12200, loss 0.51312.
Test: 2018-08-02T11:51:03.045594: step 12200, loss 0.547952.
Train: 2018-08-02T11:51:03.873554: step 12201, loss 0.595525.
Train: 2018-08-02T11:51:04.092222: step 12202, loss 0.537676.
Train: 2018-08-02T11:51:04.326545: step 12203, loss 0.579043.
Train: 2018-08-02T11:51:04.545241: step 12204, loss 0.529272.
Train: 2018-08-02T11:51:04.763971: step 12205, loss 0.554136.
Train: 2018-08-02T11:51:04.982669: step 12206, loss 0.570763.
Train: 2018-08-02T11:51:05.201363: step 12207, loss 0.662474.
Train: 2018-08-02T11:51:05.451305: step 12208, loss 0.495805.
Train: 2018-08-02T11:51:05.654387: step 12209, loss 0.587428.
Train: 2018-08-02T11:51:05.873087: step 12210, loss 0.570764.
Test: 2018-08-02T11:51:07.075899: step 12210, loss 0.548918.
Train: 2018-08-02T11:51:07.279009: step 12211, loss 0.587412.
Train: 2018-08-02T11:51:07.513296: step 12212, loss 0.51255.
Train: 2018-08-02T11:51:07.716399: step 12213, loss 0.579076.
Train: 2018-08-02T11:51:07.935073: step 12214, loss 0.595695.
Train: 2018-08-02T11:51:08.153771: step 12215, loss 0.537552.
Train: 2018-08-02T11:51:08.372516: step 12216, loss 0.529277.
Train: 2018-08-02T11:51:08.606826: step 12217, loss 0.529276.
Train: 2018-08-02T11:51:08.825489: step 12218, loss 0.612266.
Train: 2018-08-02T11:51:09.044189: step 12219, loss 0.612243.
Train: 2018-08-02T11:51:09.262911: step 12220, loss 0.570757.
Test: 2018-08-02T11:51:10.450108: step 12220, loss 0.548105.
Train: 2018-08-02T11:51:10.653211: step 12221, loss 0.554217.
Train: 2018-08-02T11:51:10.887530: step 12222, loss 0.587272.
Train: 2018-08-02T11:51:11.106235: step 12223, loss 0.578999.
Train: 2018-08-02T11:51:11.324929: step 12224, loss 0.554307.
Train: 2018-08-02T11:51:11.543627: step 12225, loss 0.587182.
Train: 2018-08-02T11:51:11.762332: step 12226, loss 0.587151.
Train: 2018-08-02T11:51:11.996652: step 12227, loss 0.595289.
Train: 2018-08-02T11:51:12.215351: step 12228, loss 0.530027.
Train: 2018-08-02T11:51:12.434044: step 12229, loss 0.546379.
Train: 2018-08-02T11:51:12.652717: step 12230, loss 0.538295.
Test: 2018-08-02T11:51:13.855560: step 12230, loss 0.549303.
Train: 2018-08-02T11:51:14.027397: step 12231, loss 0.562672.
Train: 2018-08-02T11:51:14.246119: step 12232, loss 0.554567.
Train: 2018-08-02T11:51:14.464794: step 12233, loss 0.587011.
Train: 2018-08-02T11:51:14.683524: step 12234, loss 0.465463.
Train: 2018-08-02T11:51:14.902221: step 12235, loss 0.587018.
Train: 2018-08-02T11:51:15.152135: step 12236, loss 0.603267.
Train: 2018-08-02T11:51:15.386478: step 12237, loss 0.578907.
Train: 2018-08-02T11:51:15.605182: step 12238, loss 0.554561.
Train: 2018-08-02T11:51:15.823875: step 12239, loss 0.497777.
Train: 2018-08-02T11:51:16.042550: step 12240, loss 0.595154.
Test: 2018-08-02T11:51:17.229770: step 12240, loss 0.548901.
Train: 2018-08-02T11:51:17.432848: step 12241, loss 0.578911.
Train: 2018-08-02T11:51:17.651546: step 12242, loss 0.562659.
Train: 2018-08-02T11:51:17.870270: step 12243, loss 0.530153.
Train: 2018-08-02T11:51:18.088971: step 12244, loss 0.538252.
Train: 2018-08-02T11:51:18.307669: step 12245, loss 0.554493.
Train: 2018-08-02T11:51:18.541964: step 12246, loss 0.497399.
Train: 2018-08-02T11:51:18.745072: step 12247, loss 0.644331.
Train: 2018-08-02T11:51:18.963764: step 12248, loss 0.554411.
Train: 2018-08-02T11:51:19.182465: step 12249, loss 0.538033.
Train: 2018-08-02T11:51:19.401166: step 12250, loss 0.521612.
Test: 2018-08-02T11:51:20.588360: step 12250, loss 0.547903.
Train: 2018-08-02T11:51:20.807059: step 12251, loss 0.529726.
Train: 2018-08-02T11:51:21.025787: step 12252, loss 0.562532.
Train: 2018-08-02T11:51:21.244489: step 12253, loss 0.521302.
Train: 2018-08-02T11:51:21.494424: step 12254, loss 0.488116.
Train: 2018-08-02T11:51:21.713127: step 12255, loss 0.637125.
Train: 2018-08-02T11:51:21.947416: step 12256, loss 0.595693.
Train: 2018-08-02T11:51:22.166139: step 12257, loss 0.512534.
Train: 2018-08-02T11:51:22.384845: step 12258, loss 0.587429.
Train: 2018-08-02T11:51:22.619136: step 12259, loss 0.595786.
Train: 2018-08-02T11:51:22.822214: step 12260, loss 0.520724.
Test: 2018-08-02T11:51:24.025054: step 12260, loss 0.547136.
Train: 2018-08-02T11:51:24.228157: step 12261, loss 0.604156.
Train: 2018-08-02T11:51:24.446862: step 12262, loss 0.495664.
Train: 2018-08-02T11:51:24.665555: step 12263, loss 0.512296.
Train: 2018-08-02T11:51:24.884229: step 12264, loss 0.570774.
Train: 2018-08-02T11:51:25.102952: step 12265, loss 0.554018.
Train: 2018-08-02T11:51:25.321625: step 12266, loss 0.554001.
Train: 2018-08-02T11:51:25.540325: step 12267, loss 0.553986.
Train: 2018-08-02T11:51:25.759049: step 12268, loss 0.537152.
Train: 2018-08-02T11:51:25.977722: step 12269, loss 0.562375.
Train: 2018-08-02T11:51:26.212042: step 12270, loss 0.553941.
Test: 2018-08-02T11:51:27.399265: step 12270, loss 0.548858.
Train: 2018-08-02T11:51:27.602368: step 12271, loss 0.545491.
Train: 2018-08-02T11:51:27.821065: step 12272, loss 0.503235.
Train: 2018-08-02T11:51:28.039765: step 12273, loss 0.613138.
Train: 2018-08-02T11:51:28.258469: step 12274, loss 0.460735.
Train: 2018-08-02T11:51:28.477164: step 12275, loss 0.545372.
Train: 2018-08-02T11:51:28.695860: step 12276, loss 0.587867.
Train: 2018-08-02T11:51:28.914534: step 12277, loss 0.545303.
Train: 2018-08-02T11:51:29.133263: step 12278, loss 0.5794.
Train: 2018-08-02T11:51:29.367553: step 12279, loss 0.613556.
Train: 2018-08-02T11:51:29.586279: step 12280, loss 0.562339.
Test: 2018-08-02T11:51:30.789095: step 12280, loss 0.547122.
Train: 2018-08-02T11:51:30.992198: step 12281, loss 0.519717.
Train: 2018-08-02T11:51:31.210873: step 12282, loss 0.596437.
Train: 2018-08-02T11:51:31.429601: step 12283, loss 0.545309.
Train: 2018-08-02T11:51:31.663891: step 12284, loss 0.579364.
Train: 2018-08-02T11:51:31.882590: step 12285, loss 0.570846.
Train: 2018-08-02T11:51:32.101318: step 12286, loss 0.502927.
Train: 2018-08-02T11:51:32.320018: step 12287, loss 0.528403.
Train: 2018-08-02T11:51:32.538685: step 12288, loss 0.604792.
Train: 2018-08-02T11:51:32.773006: step 12289, loss 0.638687.
Train: 2018-08-02T11:51:32.991730: step 12290, loss 0.536974.
Test: 2018-08-02T11:51:34.194548: step 12290, loss 0.548842.
Train: 2018-08-02T11:51:34.460135: step 12291, loss 0.596147.
Train: 2018-08-02T11:51:34.694456: step 12292, loss 0.545524.
Train: 2018-08-02T11:51:34.913163: step 12293, loss 0.545568.
Train: 2018-08-02T11:51:35.147450: step 12294, loss 0.528818.
Train: 2018-08-02T11:51:35.366179: step 12295, loss 0.53724.
Train: 2018-08-02T11:51:35.600500: step 12296, loss 0.595922.
Train: 2018-08-02T11:51:35.819193: step 12297, loss 0.595887.
Train: 2018-08-02T11:51:36.037866: step 12298, loss 0.554061.
Train: 2018-08-02T11:51:36.272219: step 12299, loss 0.537404.
Train: 2018-08-02T11:51:36.490912: step 12300, loss 0.570764.
Test: 2018-08-02T11:51:37.678107: step 12300, loss 0.548928.
Train: 2018-08-02T11:51:38.537313: step 12301, loss 0.537479.
Train: 2018-08-02T11:51:38.756011: step 12302, loss 0.545817.
Train: 2018-08-02T11:51:38.974679: step 12303, loss 0.595694.
Train: 2018-08-02T11:51:39.193410: step 12304, loss 0.554155.
Train: 2018-08-02T11:51:39.427723: step 12305, loss 0.520993.
Train: 2018-08-02T11:51:39.646421: step 12306, loss 0.562464.
Train: 2018-08-02T11:51:39.865125: step 12307, loss 0.54588.
Train: 2018-08-02T11:51:40.083825: step 12308, loss 0.512702.
Train: 2018-08-02T11:51:40.302520: step 12309, loss 0.471127.
Train: 2018-08-02T11:51:40.536850: step 12310, loss 0.512482.
Test: 2018-08-02T11:51:41.724034: step 12310, loss 0.548463.
Train: 2018-08-02T11:51:41.942764: step 12311, loss 0.537353.
Train: 2018-08-02T11:51:42.161445: step 12312, loss 0.562396.
Train: 2018-08-02T11:51:42.380162: step 12313, loss 0.503522.
Train: 2018-08-02T11:51:42.598855: step 12314, loss 0.562366.
Train: 2018-08-02T11:51:42.848772: step 12315, loss 0.562356.
Train: 2018-08-02T11:51:43.067500: step 12316, loss 0.545369.
Train: 2018-08-02T11:51:43.286202: step 12317, loss 0.579366.
Train: 2018-08-02T11:51:43.504878: step 12318, loss 0.494128.
Train: 2018-08-02T11:51:43.723592: step 12319, loss 0.562337.
Train: 2018-08-02T11:51:43.973509: step 12320, loss 0.553766.
Test: 2018-08-02T11:51:45.176351: step 12320, loss 0.549775.
Train: 2018-08-02T11:51:45.379453: step 12321, loss 0.631019.
Train: 2018-08-02T11:51:45.598127: step 12322, loss 0.545161.
Train: 2018-08-02T11:51:45.816851: step 12323, loss 0.562335.
Train: 2018-08-02T11:51:46.051172: step 12324, loss 0.639637.
Train: 2018-08-02T11:51:46.269871: step 12325, loss 0.545187.
Train: 2018-08-02T11:51:46.488543: step 12326, loss 0.52809.
Train: 2018-08-02T11:51:46.707243: step 12327, loss 0.605106.
Train: 2018-08-02T11:51:46.925973: step 12328, loss 0.596491.
Train: 2018-08-02T11:51:47.160294: step 12329, loss 0.647505.
Train: 2018-08-02T11:51:47.378986: step 12330, loss 0.621717.
Test: 2018-08-02T11:51:48.581804: step 12330, loss 0.548471.
Train: 2018-08-02T11:51:48.784906: step 12331, loss 0.528618.
Train: 2018-08-02T11:51:49.003606: step 12332, loss 0.54558.
Train: 2018-08-02T11:51:49.222306: step 12333, loss 0.646132.
Train: 2018-08-02T11:51:49.456631: step 12334, loss 0.679068.
Train: 2018-08-02T11:51:49.675300: step 12335, loss 0.554211.
Train: 2018-08-02T11:51:49.894021: step 12336, loss 0.554316.
Train: 2018-08-02T11:51:50.112720: step 12337, loss 0.587119.
Train: 2018-08-02T11:51:50.331396: step 12338, loss 0.546391.
Train: 2018-08-02T11:51:50.565745: step 12339, loss 0.611265.
Train: 2018-08-02T11:51:50.784445: step 12340, loss 0.530572.
Test: 2018-08-02T11:51:51.987256: step 12340, loss 0.550545.
Train: 2018-08-02T11:51:52.190359: step 12341, loss 0.538773.
Train: 2018-08-02T11:51:52.409058: step 12342, loss 0.530892.
Train: 2018-08-02T11:51:52.627732: step 12343, loss 0.507039.
Train: 2018-08-02T11:51:52.846462: step 12344, loss 0.538972.
Train: 2018-08-02T11:51:53.065155: step 12345, loss 0.594821.
Train: 2018-08-02T11:51:53.283828: step 12346, loss 0.570882.
Train: 2018-08-02T11:51:53.502558: step 12347, loss 0.531009.
Train: 2018-08-02T11:51:53.736847: step 12348, loss 0.58684.
Train: 2018-08-02T11:51:53.955546: step 12349, loss 0.538963.
Train: 2018-08-02T11:51:54.174275: step 12350, loss 0.554907.
Test: 2018-08-02T11:51:55.377088: step 12350, loss 0.548796.
Train: 2018-08-02T11:51:55.580173: step 12351, loss 0.578862.
Train: 2018-08-02T11:51:55.814485: step 12352, loss 0.60285.
Train: 2018-08-02T11:51:56.048805: step 12353, loss 0.610835.
Train: 2018-08-02T11:51:56.267530: step 12354, loss 0.522978.
Train: 2018-08-02T11:51:56.486234: step 12355, loss 0.530968.
Train: 2018-08-02T11:51:56.704932: step 12356, loss 0.578861.
Train: 2018-08-02T11:51:56.923639: step 12357, loss 0.594843.
Train: 2018-08-02T11:51:57.157921: step 12358, loss 0.594838.
Train: 2018-08-02T11:51:57.376644: step 12359, loss 0.522993.
Train: 2018-08-02T11:51:57.595343: step 12360, loss 0.54693.
Test: 2018-08-02T11:51:58.798161: step 12360, loss 0.549178.
Train: 2018-08-02T11:51:59.001265: step 12361, loss 0.658739.
Train: 2018-08-02T11:51:59.219962: step 12362, loss 0.586836.
Train: 2018-08-02T11:51:59.438636: step 12363, loss 0.586821.
Train: 2018-08-02T11:51:59.657363: step 12364, loss 0.562966.
Train: 2018-08-02T11:51:59.876064: step 12365, loss 0.578859.
Train: 2018-08-02T11:52:00.094732: step 12366, loss 0.539264.
Train: 2018-08-02T11:52:00.329079: step 12367, loss 0.586772.
Train: 2018-08-02T11:52:00.547778: step 12368, loss 0.523543.
Train: 2018-08-02T11:52:00.766485: step 12369, loss 0.634178.
Train: 2018-08-02T11:52:00.985180: step 12370, loss 0.563082.
Test: 2018-08-02T11:52:02.187992: step 12370, loss 0.549396.
Train: 2018-08-02T11:52:02.406691: step 12371, loss 0.507934.
Train: 2018-08-02T11:52:02.641042: step 12372, loss 0.618285.
Train: 2018-08-02T11:52:02.859741: step 12373, loss 0.610377.
Train: 2018-08-02T11:52:03.078410: step 12374, loss 0.515947.
Train: 2018-08-02T11:52:03.297133: step 12375, loss 0.578869.
Train: 2018-08-02T11:52:03.531430: step 12376, loss 0.555287.
Train: 2018-08-02T11:52:03.765749: step 12377, loss 0.57887.
Train: 2018-08-02T11:52:03.980371: step 12378, loss 0.56315.
Train: 2018-08-02T11:52:04.199066: step 12379, loss 0.539569.
Train: 2018-08-02T11:52:04.433392: step 12380, loss 0.665395.
Test: 2018-08-02T11:52:05.620582: step 12380, loss 0.549503.
Train: 2018-08-02T11:52:05.839306: step 12381, loss 0.578872.
Train: 2018-08-02T11:52:06.026762: step 12382, loss 0.579921.
Train: 2018-08-02T11:52:06.245467: step 12383, loss 0.56322.
Train: 2018-08-02T11:52:06.464165: step 12384, loss 0.539782.
Train: 2018-08-02T11:52:06.682834: step 12385, loss 0.547614.
Train: 2018-08-02T11:52:06.917155: step 12386, loss 0.586703.
Train: 2018-08-02T11:52:07.135884: step 12387, loss 0.578884.
Train: 2018-08-02T11:52:07.354582: step 12388, loss 0.524169.
Train: 2018-08-02T11:52:07.573251: step 12389, loss 0.508472.
Train: 2018-08-02T11:52:07.807595: step 12390, loss 0.586717.
Test: 2018-08-02T11:52:09.010414: step 12390, loss 0.550226.
Train: 2018-08-02T11:52:09.229112: step 12391, loss 0.531745.
Train: 2018-08-02T11:52:09.447835: step 12392, loss 0.578867.
Train: 2018-08-02T11:52:09.666510: step 12393, loss 0.649865.
Train: 2018-08-02T11:52:09.885240: step 12394, loss 0.53942.
Train: 2018-08-02T11:52:10.135181: step 12395, loss 0.586756.
Train: 2018-08-02T11:52:10.369495: step 12396, loss 0.563072.
Train: 2018-08-02T11:52:10.603791: step 12397, loss 0.49988.
Train: 2018-08-02T11:52:10.822516: step 12398, loss 0.6026.
Train: 2018-08-02T11:52:11.041215: step 12399, loss 0.531335.
Train: 2018-08-02T11:52:11.291167: step 12400, loss 0.562991.
Test: 2018-08-02T11:52:12.493972: step 12400, loss 0.549872.
Train: 2018-08-02T11:52:13.368795: step 12401, loss 0.586805.
Train: 2018-08-02T11:52:13.603113: step 12402, loss 0.547032.
Train: 2018-08-02T11:52:13.821788: step 12403, loss 0.554954.
Train: 2018-08-02T11:52:14.040511: step 12404, loss 0.578861.
Train: 2018-08-02T11:52:14.259215: step 12405, loss 0.546895.
Train: 2018-08-02T11:52:14.477913: step 12406, loss 0.578865.
Train: 2018-08-02T11:52:14.712234: step 12407, loss 0.546824.
Train: 2018-08-02T11:52:14.946523: step 12408, loss 0.530729.
Train: 2018-08-02T11:52:15.165254: step 12409, loss 0.570832.
Train: 2018-08-02T11:52:15.383947: step 12410, loss 0.619137.
Test: 2018-08-02T11:52:16.571143: step 12410, loss 0.549303.
Train: 2018-08-02T11:52:16.805464: step 12411, loss 0.530539.
Train: 2018-08-02T11:52:17.024192: step 12412, loss 0.554663.
Train: 2018-08-02T11:52:17.242860: step 12413, loss 0.562749.
Train: 2018-08-02T11:52:17.477206: step 12414, loss 0.514141.
Train: 2018-08-02T11:52:17.695908: step 12415, loss 0.595124.
Train: 2018-08-02T11:52:17.930199: step 12416, loss 0.578986.
Train: 2018-08-02T11:52:18.148928: step 12417, loss 0.538249.
Train: 2018-08-02T11:52:18.367633: step 12418, loss 0.53006.
Train: 2018-08-02T11:52:18.601948: step 12419, loss 0.587264.
Train: 2018-08-02T11:52:18.820640: step 12420, loss 0.578975.
Test: 2018-08-02T11:52:20.007837: step 12420, loss 0.548975.
Train: 2018-08-02T11:52:20.242158: step 12421, loss 0.562628.
Train: 2018-08-02T11:52:20.460881: step 12422, loss 0.595275.
Train: 2018-08-02T11:52:20.695184: step 12423, loss 0.611578.
Train: 2018-08-02T11:52:20.913906: step 12424, loss 0.505566.
Train: 2018-08-02T11:52:21.148196: step 12425, loss 0.578927.
Train: 2018-08-02T11:52:21.382547: step 12426, loss 0.587076.
Train: 2018-08-02T11:52:21.601245: step 12427, loss 0.578922.
Train: 2018-08-02T11:52:21.819944: step 12428, loss 0.619597.
Train: 2018-08-02T11:52:22.038644: step 12429, loss 0.570788.
Train: 2018-08-02T11:52:22.257337: step 12430, loss 0.546491.
Test: 2018-08-02T11:52:23.444533: step 12430, loss 0.549202.
Train: 2018-08-02T11:52:23.663232: step 12431, loss 0.546534.
Train: 2018-08-02T11:52:23.881961: step 12432, loss 0.530397.
Train: 2018-08-02T11:52:24.100656: step 12433, loss 0.522321.
Train: 2018-08-02T11:52:24.319352: step 12434, loss 0.530364.
Train: 2018-08-02T11:52:24.553680: step 12435, loss 0.578897.
Train: 2018-08-02T11:52:24.772347: step 12436, loss 0.546468.
Train: 2018-08-02T11:52:24.991072: step 12437, loss 0.473362.
Train: 2018-08-02T11:52:25.209772: step 12438, loss 0.546345.
Train: 2018-08-02T11:52:25.444090: step 12439, loss 0.578939.
Train: 2018-08-02T11:52:25.678412: step 12440, loss 0.554383.
Test: 2018-08-02T11:52:26.881228: step 12440, loss 0.549595.
Train: 2018-08-02T11:52:27.099953: step 12441, loss 0.5215.
Train: 2018-08-02T11:52:27.318652: step 12442, loss 0.496647.
Train: 2018-08-02T11:52:27.552973: step 12443, loss 0.554221.
Train: 2018-08-02T11:52:27.787293: step 12444, loss 0.529266.
Train: 2018-08-02T11:52:28.005965: step 12445, loss 0.545769.
Train: 2018-08-02T11:52:28.224665: step 12446, loss 0.50387.
Train: 2018-08-02T11:52:28.443393: step 12447, loss 0.604398.
Train: 2018-08-02T11:52:28.662093: step 12448, loss 0.562367.
Train: 2018-08-02T11:52:28.896413: step 12449, loss 0.562366.
Train: 2018-08-02T11:52:29.130704: step 12450, loss 0.562355.
Test: 2018-08-02T11:52:30.333545: step 12450, loss 0.547795.
Train: 2018-08-02T11:52:30.552268: step 12451, loss 0.579309.
Train: 2018-08-02T11:52:30.802212: step 12452, loss 0.587829.
Train: 2018-08-02T11:52:31.036531: step 12453, loss 0.596317.
Train: 2018-08-02T11:52:31.270827: step 12454, loss 0.570835.
Train: 2018-08-02T11:52:31.489524: step 12455, loss 0.528443.
Train: 2018-08-02T11:52:31.708255: step 12456, loss 0.587775.
Train: 2018-08-02T11:52:31.942568: step 12457, loss 0.66394.
Train: 2018-08-02T11:52:32.161269: step 12458, loss 0.587684.
Train: 2018-08-02T11:52:32.411211: step 12459, loss 0.553972.
Train: 2018-08-02T11:52:32.629908: step 12460, loss 0.478563.
Test: 2018-08-02T11:52:33.832725: step 12460, loss 0.548226.
Train: 2018-08-02T11:52:34.051448: step 12461, loss 0.545657.
Train: 2018-08-02T11:52:34.270153: step 12462, loss 0.562408.
Train: 2018-08-02T11:52:34.488846: step 12463, loss 0.528981.
Train: 2018-08-02T11:52:34.707552: step 12464, loss 0.579126.
Train: 2018-08-02T11:52:34.926246: step 12465, loss 0.595819.
Train: 2018-08-02T11:52:35.144938: step 12466, loss 0.562427.
Train: 2018-08-02T11:52:35.363642: step 12467, loss 0.520804.
Train: 2018-08-02T11:52:35.597938: step 12468, loss 0.554117.
Train: 2018-08-02T11:52:35.816636: step 12469, loss 0.579081.
Train: 2018-08-02T11:52:36.035360: step 12470, loss 0.612326.
Test: 2018-08-02T11:52:37.238178: step 12470, loss 0.548602.
Train: 2018-08-02T11:52:37.441280: step 12471, loss 0.60395.
Train: 2018-08-02T11:52:37.659956: step 12472, loss 0.479715.
Train: 2018-08-02T11:52:37.909896: step 12473, loss 0.562485.
Train: 2018-08-02T11:52:38.128624: step 12474, loss 0.54596.
Train: 2018-08-02T11:52:38.347324: step 12475, loss 0.512917.
Train: 2018-08-02T11:52:38.566018: step 12476, loss 0.562488.
Train: 2018-08-02T11:52:38.831556: step 12477, loss 0.504571.
Train: 2018-08-02T11:52:39.034665: step 12478, loss 0.570757.
Train: 2018-08-02T11:52:39.268977: step 12479, loss 0.587352.
Train: 2018-08-02T11:52:39.487682: step 12480, loss 0.562457.
Test: 2018-08-02T11:52:40.674874: step 12480, loss 0.548777.
Train: 2018-08-02T11:52:40.893597: step 12481, loss 0.520933.
Train: 2018-08-02T11:52:41.112272: step 12482, loss 0.545818.
Train: 2018-08-02T11:52:41.346622: step 12483, loss 0.579086.
Train: 2018-08-02T11:52:41.565320: step 12484, loss 0.50412.
Train: 2018-08-02T11:52:41.784014: step 12485, loss 0.470629.
Train: 2018-08-02T11:52:42.002689: step 12486, loss 0.520534.
Train: 2018-08-02T11:52:42.237038: step 12487, loss 0.553952.
Train: 2018-08-02T11:52:42.455706: step 12488, loss 0.587694.
Train: 2018-08-02T11:52:42.674404: step 12489, loss 0.613118.
Train: 2018-08-02T11:52:42.893130: step 12490, loss 0.587705.
Test: 2018-08-02T11:52:44.080325: step 12490, loss 0.547434.
Train: 2018-08-02T11:52:44.283430: step 12491, loss 0.604658.
Train: 2018-08-02T11:52:44.517724: step 12492, loss 0.613161.
Train: 2018-08-02T11:52:44.736447: step 12493, loss 0.570789.
Train: 2018-08-02T11:52:44.955151: step 12494, loss 0.553963.
Train: 2018-08-02T11:52:45.173844: step 12495, loss 0.553984.
Train: 2018-08-02T11:52:45.392519: step 12496, loss 0.570782.
Train: 2018-08-02T11:52:45.626839: step 12497, loss 0.487051.
Train: 2018-08-02T11:52:45.845562: step 12498, loss 0.537293.
Train: 2018-08-02T11:52:46.064237: step 12499, loss 0.621009.
Train: 2018-08-02T11:52:46.282966: step 12500, loss 0.646035.
Test: 2018-08-02T11:52:47.470157: step 12500, loss 0.547542.
Train: 2018-08-02T11:52:48.313710: step 12501, loss 0.60412.
Train: 2018-08-02T11:52:48.532437: step 12502, loss 0.545833.
Train: 2018-08-02T11:52:48.766761: step 12503, loss 0.620462.
Train: 2018-08-02T11:52:48.985452: step 12504, loss 0.554253.
Train: 2018-08-02T11:52:49.204156: step 12505, loss 0.504975.
Train: 2018-08-02T11:52:49.422856: step 12506, loss 0.587175.
Train: 2018-08-02T11:52:49.641554: step 12507, loss 0.562576.
Train: 2018-08-02T11:52:49.891490: step 12508, loss 0.587111.
Train: 2018-08-02T11:52:50.110194: step 12509, loss 0.587077.
Train: 2018-08-02T11:52:50.328893: step 12510, loss 0.505753.
Test: 2018-08-02T11:52:51.516085: step 12510, loss 0.548362.
Train: 2018-08-02T11:52:51.734783: step 12511, loss 0.55455.
Train: 2018-08-02T11:52:51.953512: step 12512, loss 0.570791.
Train: 2018-08-02T11:52:52.187832: step 12513, loss 0.595107.
Train: 2018-08-02T11:52:52.406526: step 12514, loss 0.611259.
Train: 2018-08-02T11:52:52.625201: step 12515, loss 0.6031.
Train: 2018-08-02T11:52:52.843929: step 12516, loss 0.554732.
Train: 2018-08-02T11:52:53.078250: step 12517, loss 0.546759.
Train: 2018-08-02T11:52:53.328186: step 12518, loss 0.642971.
Train: 2018-08-02T11:52:53.546883: step 12519, loss 0.554902.
Train: 2018-08-02T11:52:53.796826: step 12520, loss 0.570895.
Test: 2018-08-02T11:52:54.984023: step 12520, loss 0.549153.
Train: 2018-08-02T11:52:55.187124: step 12521, loss 0.547081.
Train: 2018-08-02T11:52:55.421444: step 12522, loss 0.515417.
Train: 2018-08-02T11:52:55.640149: step 12523, loss 0.610568.
Train: 2018-08-02T11:52:55.858849: step 12524, loss 0.55511.
Train: 2018-08-02T11:52:56.077547: step 12525, loss 0.507666.
Train: 2018-08-02T11:52:56.296247: step 12526, loss 0.563033.
Train: 2018-08-02T11:52:56.514938: step 12527, loss 0.499633.
Train: 2018-08-02T11:52:56.733643: step 12528, loss 0.626518.
Train: 2018-08-02T11:52:56.952338: step 12529, loss 0.562958.
Train: 2018-08-02T11:52:57.171038: step 12530, loss 0.58681.
Test: 2018-08-02T11:52:58.358232: step 12530, loss 0.55003.
Train: 2018-08-02T11:52:58.561309: step 12531, loss 0.58683.
Train: 2018-08-02T11:52:58.795629: step 12532, loss 0.586799.
Train: 2018-08-02T11:52:58.983116: step 12533, loss 0.579896.
Train: 2018-08-02T11:52:59.201816: step 12534, loss 0.531225.
Train: 2018-08-02T11:52:59.420483: step 12535, loss 0.610632.
Train: 2018-08-02T11:52:59.639183: step 12536, loss 0.634423.
Train: 2018-08-02T11:52:59.873528: step 12537, loss 0.539266.
Train: 2018-08-02T11:53:00.092202: step 12538, loss 0.523516.
Train: 2018-08-02T11:53:00.310926: step 12539, loss 0.634193.
Train: 2018-08-02T11:53:00.529600: step 12540, loss 0.547293.
Test: 2018-08-02T11:53:01.716821: step 12540, loss 0.549928.
Train: 2018-08-02T11:53:01.951143: step 12541, loss 0.531558.
Train: 2018-08-02T11:53:02.169871: step 12542, loss 0.570978.
Train: 2018-08-02T11:53:02.388591: step 12543, loss 0.523664.
Train: 2018-08-02T11:53:02.607267: step 12544, loss 0.563082.
Train: 2018-08-02T11:53:02.825936: step 12545, loss 0.555147.
Train: 2018-08-02T11:53:03.060281: step 12546, loss 0.602587.
Train: 2018-08-02T11:53:03.278987: step 12547, loss 0.578846.
Train: 2018-08-02T11:53:03.513277: step 12548, loss 0.523424.
Train: 2018-08-02T11:53:03.732006: step 12549, loss 0.555059.
Train: 2018-08-02T11:53:03.966325: step 12550, loss 0.539149.
Test: 2018-08-02T11:53:05.153516: step 12550, loss 0.549263.
Train: 2018-08-02T11:53:05.387862: step 12551, loss 0.570956.
Train: 2018-08-02T11:53:05.606566: step 12552, loss 0.594814.
Train: 2018-08-02T11:53:05.825265: step 12553, loss 0.538941.
Train: 2018-08-02T11:53:06.043964: step 12554, loss 0.610823.
Train: 2018-08-02T11:53:06.293900: step 12555, loss 0.586888.
Train: 2018-08-02T11:53:06.512603: step 12556, loss 0.522901.
Train: 2018-08-02T11:53:06.731303: step 12557, loss 0.514862.
Train: 2018-08-02T11:53:06.949994: step 12558, loss 0.578841.
Train: 2018-08-02T11:53:07.184316: step 12559, loss 0.474428.
Train: 2018-08-02T11:53:07.418610: step 12560, loss 0.514304.
Test: 2018-08-02T11:53:08.605832: step 12560, loss 0.548945.
Train: 2018-08-02T11:53:08.824561: step 12561, loss 0.627857.
Train: 2018-08-02T11:53:09.043261: step 12562, loss 0.578927.
Train: 2018-08-02T11:53:09.261954: step 12563, loss 0.587231.
Train: 2018-08-02T11:53:09.480658: step 12564, loss 0.530218.
Train: 2018-08-02T11:53:09.699350: step 12565, loss 0.570735.
Train: 2018-08-02T11:53:09.918050: step 12566, loss 0.529986.
Train: 2018-08-02T11:53:10.136724: step 12567, loss 0.636065.
Train: 2018-08-02T11:53:10.355422: step 12568, loss 0.587107.
Train: 2018-08-02T11:53:10.574149: step 12569, loss 0.521781.
Train: 2018-08-02T11:53:10.808472: step 12570, loss 0.546269.
Test: 2018-08-02T11:53:11.995664: step 12570, loss 0.548761.
Train: 2018-08-02T11:53:12.198771: step 12571, loss 0.554431.
Train: 2018-08-02T11:53:12.417467: step 12572, loss 0.59529.
Train: 2018-08-02T11:53:12.636173: step 12573, loss 0.513519.
Train: 2018-08-02T11:53:12.870488: step 12574, loss 0.60351.
Train: 2018-08-02T11:53:13.089184: step 12575, loss 0.619896.
Train: 2018-08-02T11:53:13.307855: step 12576, loss 0.587117.
Train: 2018-08-02T11:53:13.542177: step 12577, loss 0.603438.
Train: 2018-08-02T11:53:13.760906: step 12578, loss 0.538191.
Train: 2018-08-02T11:53:13.995223: step 12579, loss 0.57078.
Train: 2018-08-02T11:53:14.213894: step 12580, loss 0.611388.
Test: 2018-08-02T11:53:15.401116: step 12580, loss 0.549542.
Train: 2018-08-02T11:53:15.619815: step 12581, loss 0.603194.
Train: 2018-08-02T11:53:15.838513: step 12582, loss 0.570812.
Train: 2018-08-02T11:53:16.057212: step 12583, loss 0.60303.
Train: 2018-08-02T11:53:16.275911: step 12584, loss 0.522704.
Train: 2018-08-02T11:53:16.510264: step 12585, loss 0.642914.
Train: 2018-08-02T11:53:16.728960: step 12586, loss 0.507068.
Train: 2018-08-02T11:53:16.963281: step 12587, loss 0.578859.
Train: 2018-08-02T11:53:17.181949: step 12588, loss 0.515293.
Train: 2018-08-02T11:53:17.416294: step 12589, loss 0.531212.
Train: 2018-08-02T11:53:17.634999: step 12590, loss 0.562971.
Test: 2018-08-02T11:53:18.822190: step 12590, loss 0.549692.
Train: 2018-08-02T11:53:19.040920: step 12591, loss 0.515281.
Train: 2018-08-02T11:53:19.259622: step 12592, loss 0.642537.
Train: 2018-08-02T11:53:19.493911: step 12593, loss 0.53907.
Train: 2018-08-02T11:53:19.712637: step 12594, loss 0.52313.
Train: 2018-08-02T11:53:19.931306: step 12595, loss 0.554943.
Train: 2018-08-02T11:53:20.165652: step 12596, loss 0.59483.
Train: 2018-08-02T11:53:20.384325: step 12597, loss 0.594844.
Train: 2018-08-02T11:53:20.603025: step 12598, loss 0.546896.
Train: 2018-08-02T11:53:20.837344: step 12599, loss 0.594854.
Train: 2018-08-02T11:53:21.056072: step 12600, loss 0.562873.
Test: 2018-08-02T11:53:22.227643: step 12600, loss 0.548971.
Train: 2018-08-02T11:53:23.024359: step 12601, loss 0.570868.
Train: 2018-08-02T11:53:23.243060: step 12602, loss 0.522909.
Train: 2018-08-02T11:53:23.461760: step 12603, loss 0.530861.
Train: 2018-08-02T11:53:23.680458: step 12604, loss 0.51476.
Train: 2018-08-02T11:53:23.914780: step 12605, loss 0.522632.
Train: 2018-08-02T11:53:24.133476: step 12606, loss 0.546637.
Train: 2018-08-02T11:53:24.352170: step 12607, loss 0.514188.
Train: 2018-08-02T11:53:24.586490: step 12608, loss 0.554544.
Train: 2018-08-02T11:53:24.805194: step 12609, loss 0.578928.
Train: 2018-08-02T11:53:25.039509: step 12610, loss 0.570767.
Test: 2018-08-02T11:53:26.226706: step 12610, loss 0.549999.
Train: 2018-08-02T11:53:26.429783: step 12611, loss 0.570762.
Train: 2018-08-02T11:53:26.648507: step 12612, loss 0.595414.
Train: 2018-08-02T11:53:26.882833: step 12613, loss 0.537845.
Train: 2018-08-02T11:53:27.117123: step 12614, loss 0.595479.
Train: 2018-08-02T11:53:27.367095: step 12615, loss 0.546016.
Train: 2018-08-02T11:53:27.585763: step 12616, loss 0.587265.
Train: 2018-08-02T11:53:27.804489: step 12617, loss 0.587264.
Train: 2018-08-02T11:53:28.023192: step 12618, loss 0.595505.
Train: 2018-08-02T11:53:28.257511: step 12619, loss 0.60374.
Train: 2018-08-02T11:53:28.476179: step 12620, loss 0.529611.
Test: 2018-08-02T11:53:29.725887: step 12620, loss 0.548618.
Train: 2018-08-02T11:53:29.944586: step 12621, loss 0.611876.
Train: 2018-08-02T11:53:30.210148: step 12622, loss 0.537943.
Train: 2018-08-02T11:53:30.458005: step 12623, loss 0.5298.
Train: 2018-08-02T11:53:30.692327: step 12624, loss 0.505256.
Train: 2018-08-02T11:53:30.926621: step 12625, loss 0.537924.
Train: 2018-08-02T11:53:31.129701: step 12626, loss 0.57898.
Train: 2018-08-02T11:53:31.379642: step 12627, loss 0.595333.
Train: 2018-08-02T11:53:31.598338: step 12628, loss 0.59539.
Train: 2018-08-02T11:53:31.817067: step 12629, loss 0.60379.
Train: 2018-08-02T11:53:32.051387: step 12630, loss 0.563022.
Test: 2018-08-02T11:53:33.254200: step 12630, loss 0.550042.
Train: 2018-08-02T11:53:33.488551: step 12631, loss 0.570851.
Train: 2018-08-02T11:53:33.707220: step 12632, loss 0.619725.
Train: 2018-08-02T11:53:33.925948: step 12633, loss 0.538245.
Train: 2018-08-02T11:53:34.144616: step 12634, loss 0.50584.
Train: 2018-08-02T11:53:34.363340: step 12635, loss 0.595134.
Train: 2018-08-02T11:53:34.613256: step 12636, loss 0.570794.
Train: 2018-08-02T11:53:34.831955: step 12637, loss 0.619377.
Train: 2018-08-02T11:53:35.050684: step 12638, loss 0.56273.
Train: 2018-08-02T11:53:35.269378: step 12639, loss 0.611129.
Train: 2018-08-02T11:53:35.488082: step 12640, loss 0.546715.
Test: 2018-08-02T11:53:36.879270: step 12640, loss 0.548502.
Train: 2018-08-02T11:53:37.191696: step 12641, loss 0.490621.
Train: 2018-08-02T11:53:37.426016: step 12642, loss 0.562826.
Train: 2018-08-02T11:53:37.644740: step 12643, loss 0.514707.
Train: 2018-08-02T11:53:37.863413: step 12644, loss 0.490555.
Train: 2018-08-02T11:53:38.113379: step 12645, loss 0.490327.
Train: 2018-08-02T11:53:38.347702: step 12646, loss 0.578889.
Train: 2018-08-02T11:53:38.566400: step 12647, loss 0.554574.
Train: 2018-08-02T11:53:38.785099: step 12648, loss 0.521971.
Train: 2018-08-02T11:53:39.019418: step 12649, loss 0.546274.
Train: 2018-08-02T11:53:39.238092: step 12650, loss 0.63633.
Test: 2018-08-02T11:53:40.440935: step 12650, loss 0.548083.
Train: 2018-08-02T11:53:40.659665: step 12651, loss 0.55434.
Train: 2018-08-02T11:53:40.878359: step 12652, loss 0.529639.
Train: 2018-08-02T11:53:41.112679: step 12653, loss 0.504819.
Train: 2018-08-02T11:53:41.362627: step 12654, loss 0.603817.
Train: 2018-08-02T11:53:41.581317: step 12655, loss 0.570774.
Train: 2018-08-02T11:53:41.815653: step 12656, loss 0.57901.
Train: 2018-08-02T11:53:42.049959: step 12657, loss 0.512504.
Train: 2018-08-02T11:53:42.268666: step 12658, loss 0.545768.
Train: 2018-08-02T11:53:42.487361: step 12659, loss 0.570971.
Train: 2018-08-02T11:53:42.706030: step 12660, loss 0.621242.
Test: 2018-08-02T11:53:43.893252: step 12660, loss 0.548622.
Train: 2018-08-02T11:53:44.111950: step 12661, loss 0.553982.
Train: 2018-08-02T11:53:44.330680: step 12662, loss 0.579174.
Train: 2018-08-02T11:53:44.565002: step 12663, loss 0.570783.
Train: 2018-08-02T11:53:44.799291: step 12664, loss 0.620748.
Train: 2018-08-02T11:53:45.018012: step 12665, loss 0.545808.
Train: 2018-08-02T11:53:45.252339: step 12666, loss 0.579055.
Train: 2018-08-02T11:53:45.471038: step 12667, loss 0.512777.
Train: 2018-08-02T11:53:45.705359: step 12668, loss 0.570757.
Train: 2018-08-02T11:53:45.939679: step 12669, loss 0.595566.
Train: 2018-08-02T11:53:46.173967: step 12670, loss 0.479927.
Test: 2018-08-02T11:53:47.361190: step 12670, loss 0.548132.
Train: 2018-08-02T11:53:47.579890: step 12671, loss 0.529458.
Train: 2018-08-02T11:53:47.798587: step 12672, loss 0.537688.
Train: 2018-08-02T11:53:48.032908: step 12673, loss 0.529369.
Train: 2018-08-02T11:53:48.251605: step 12674, loss 0.570758.
Train: 2018-08-02T11:53:48.470331: step 12675, loss 0.537546.
Train: 2018-08-02T11:53:48.704651: step 12676, loss 0.554127.
Train: 2018-08-02T11:53:48.907733: step 12677, loss 0.495796.
Train: 2018-08-02T11:53:49.126402: step 12678, loss 0.562417.
Train: 2018-08-02T11:53:49.360722: step 12679, loss 0.570775.
Train: 2018-08-02T11:53:49.579420: step 12680, loss 0.520465.
Test: 2018-08-02T11:53:50.782263: step 12680, loss 0.548346.
Train: 2018-08-02T11:53:50.985352: step 12681, loss 0.596005.
Train: 2018-08-02T11:53:51.219662: step 12682, loss 0.604457.
Train: 2018-08-02T11:53:51.454011: step 12683, loss 0.646546.
Train: 2018-08-02T11:53:51.641472: step 12684, loss 0.45484.
Train: 2018-08-02T11:53:51.860168: step 12685, loss 0.604413.
Train: 2018-08-02T11:53:52.078867: step 12686, loss 0.59599.
Train: 2018-08-02T11:53:52.313156: step 12687, loss 0.470104.
Train: 2018-08-02T11:53:52.531880: step 12688, loss 0.520418.
Train: 2018-08-02T11:53:52.750582: step 12689, loss 0.604407.
Train: 2018-08-02T11:53:52.984903: step 12690, loss 0.486736.
Test: 2018-08-02T11:53:54.187716: step 12690, loss 0.54735.
Train: 2018-08-02T11:53:54.422036: step 12691, loss 0.604466.
Train: 2018-08-02T11:53:54.656389: step 12692, loss 0.562375.
Train: 2018-08-02T11:53:54.875090: step 12693, loss 0.62133.
Train: 2018-08-02T11:53:55.093784: step 12694, loss 0.56238.
Train: 2018-08-02T11:53:55.312482: step 12695, loss 0.495175.
Train: 2018-08-02T11:53:55.546798: step 12696, loss 0.528776.
Train: 2018-08-02T11:53:55.796713: step 12697, loss 0.587606.
Train: 2018-08-02T11:53:56.031065: step 12698, loss 0.596016.
Train: 2018-08-02T11:53:56.249732: step 12699, loss 0.663178.
Train: 2018-08-02T11:53:56.484053: step 12700, loss 0.512154.
Test: 2018-08-02T11:53:57.655654: step 12700, loss 0.548451.
Train: 2018-08-02T11:53:58.499235: step 12701, loss 0.579132.
Train: 2018-08-02T11:53:58.733527: step 12702, loss 0.58745.
Train: 2018-08-02T11:53:58.967846: step 12703, loss 0.520836.
Train: 2018-08-02T11:53:59.186576: step 12704, loss 0.579069.
Train: 2018-08-02T11:53:59.420897: step 12705, loss 0.570758.
Train: 2018-08-02T11:53:59.655185: step 12706, loss 0.579035.
Train: 2018-08-02T11:53:59.873914: step 12707, loss 0.521189.
Train: 2018-08-02T11:54:00.108233: step 12708, loss 0.603766.
Train: 2018-08-02T11:54:00.326936: step 12709, loss 0.546047.
Train: 2018-08-02T11:54:00.557197: step 12710, loss 0.554308.
Test: 2018-08-02T11:54:01.760010: step 12710, loss 0.54939.
Train: 2018-08-02T11:54:01.963087: step 12711, loss 0.595407.
Train: 2018-08-02T11:54:02.197411: step 12712, loss 0.554359.
Train: 2018-08-02T11:54:02.416111: step 12713, loss 0.546196.
Train: 2018-08-02T11:54:02.634830: step 12714, loss 0.513495.
Train: 2018-08-02T11:54:02.884775: step 12715, loss 0.570766.
Train: 2018-08-02T11:54:03.103470: step 12716, loss 0.652591.
Train: 2018-08-02T11:54:03.337796: step 12717, loss 0.497273.
Train: 2018-08-02T11:54:03.556490: step 12718, loss 0.546282.
Train: 2018-08-02T11:54:03.775197: step 12719, loss 0.61975.
Train: 2018-08-02T11:54:03.993893: step 12720, loss 0.554469.
Test: 2018-08-02T11:54:05.196707: step 12720, loss 0.547904.
Train: 2018-08-02T11:54:05.415404: step 12721, loss 0.595213.
Train: 2018-08-02T11:54:05.649751: step 12722, loss 0.570782.
Train: 2018-08-02T11:54:05.868453: step 12723, loss 0.562667.
Train: 2018-08-02T11:54:06.087152: step 12724, loss 0.578901.
Train: 2018-08-02T11:54:06.305845: step 12725, loss 0.562703.
Train: 2018-08-02T11:54:06.524519: step 12726, loss 0.635483.
Train: 2018-08-02T11:54:06.758865: step 12727, loss 0.659501.
Train: 2018-08-02T11:54:06.993179: step 12728, loss 0.562819.
Train: 2018-08-02T11:54:07.227482: step 12729, loss 0.562878.
Train: 2018-08-02T11:54:07.461831: step 12730, loss 0.586822.
Test: 2018-08-02T11:54:08.649022: step 12730, loss 0.548461.
Train: 2018-08-02T11:54:08.867751: step 12731, loss 0.547122.
Train: 2018-08-02T11:54:09.086444: step 12732, loss 0.602596.
Train: 2018-08-02T11:54:09.305148: step 12733, loss 0.665606.
Train: 2018-08-02T11:54:09.539466: step 12734, loss 0.57103.
Train: 2018-08-02T11:54:09.758137: step 12735, loss 0.539848.
Train: 2018-08-02T11:54:09.992488: step 12736, loss 0.602244.
Train: 2018-08-02T11:54:10.211186: step 12737, loss 0.516908.
Train: 2018-08-02T11:54:10.429881: step 12738, loss 0.586664.
Train: 2018-08-02T11:54:10.648585: step 12739, loss 0.50176.
Train: 2018-08-02T11:54:10.867253: step 12740, loss 0.509493.
Test: 2018-08-02T11:54:12.070095: step 12740, loss 0.55022.
Train: 2018-08-02T11:54:12.288818: step 12741, loss 0.50939.
Train: 2018-08-02T11:54:12.507523: step 12742, loss 0.493685.
Train: 2018-08-02T11:54:12.726222: step 12743, loss 0.555549.
Train: 2018-08-02T11:54:12.944921: step 12744, loss 0.500708.
Train: 2018-08-02T11:54:13.163621: step 12745, loss 0.531711.
Train: 2018-08-02T11:54:13.397949: step 12746, loss 0.539319.
Train: 2018-08-02T11:54:13.616609: step 12747, loss 0.634565.
Train: 2018-08-02T11:54:13.835308: step 12748, loss 0.546908.
Train: 2018-08-02T11:54:14.054036: step 12749, loss 0.546813.
Train: 2018-08-02T11:54:14.272707: step 12750, loss 0.570789.
Test: 2018-08-02T11:54:15.475548: step 12750, loss 0.549238.
Train: 2018-08-02T11:54:15.694248: step 12751, loss 0.538497.
Train: 2018-08-02T11:54:15.912976: step 12752, loss 0.522158.
Train: 2018-08-02T11:54:16.147290: step 12753, loss 0.562626.
Train: 2018-08-02T11:54:16.365964: step 12754, loss 0.52991.
Train: 2018-08-02T11:54:16.600315: step 12755, loss 0.554424.
Train: 2018-08-02T11:54:16.818984: step 12756, loss 0.58725.
Train: 2018-08-02T11:54:17.037682: step 12757, loss 0.545969.
Train: 2018-08-02T11:54:17.256410: step 12758, loss 0.545898.
Train: 2018-08-02T11:54:17.475110: step 12759, loss 0.462689.
Train: 2018-08-02T11:54:17.709440: step 12760, loss 0.529042.
Test: 2018-08-02T11:54:18.896622: step 12760, loss 0.549315.
Train: 2018-08-02T11:54:19.130973: step 12761, loss 0.528818.
Train: 2018-08-02T11:54:19.349666: step 12762, loss 0.553995.
Train: 2018-08-02T11:54:19.568371: step 12763, loss 0.528292.
Train: 2018-08-02T11:54:19.802685: step 12764, loss 0.630365.
Train: 2018-08-02T11:54:20.036980: step 12765, loss 0.510521.
Train: 2018-08-02T11:54:20.255680: step 12766, loss 0.501977.
Train: 2018-08-02T11:54:20.490000: step 12767, loss 0.596393.
Train: 2018-08-02T11:54:20.708728: step 12768, loss 0.554924.
Train: 2018-08-02T11:54:20.943020: step 12769, loss 0.586818.
Train: 2018-08-02T11:54:21.161743: step 12770, loss 0.55264.
Test: 2018-08-02T11:54:22.348943: step 12770, loss 0.547241.
Train: 2018-08-02T11:54:22.571156: step 12771, loss 0.660128.
Train: 2018-08-02T11:54:22.805507: step 12772, loss 0.588971.
Train: 2018-08-02T11:54:23.024175: step 12773, loss 0.597977.
Train: 2018-08-02T11:54:23.258527: step 12774, loss 0.51118.
Train: 2018-08-02T11:54:23.477225: step 12775, loss 0.571065.
Train: 2018-08-02T11:54:23.695923: step 12776, loss 0.528087.
Train: 2018-08-02T11:54:23.914616: step 12777, loss 0.502834.
Train: 2018-08-02T11:54:24.133316: step 12778, loss 0.579252.
Train: 2018-08-02T11:54:24.367635: step 12779, loss 0.545503.
Train: 2018-08-02T11:54:24.586311: step 12780, loss 0.54539.
Test: 2018-08-02T11:54:25.789153: step 12780, loss 0.548236.
Train: 2018-08-02T11:54:25.992231: step 12781, loss 0.511615.
Train: 2018-08-02T11:54:26.210958: step 12782, loss 0.545442.
Train: 2018-08-02T11:54:26.445249: step 12783, loss 0.528508.
Train: 2018-08-02T11:54:26.679626: step 12784, loss 0.596236.
Train: 2018-08-02T11:54:26.882646: step 12785, loss 0.65553.
Train: 2018-08-02T11:54:27.101346: step 12786, loss 0.596175.
Train: 2018-08-02T11:54:27.320075: step 12787, loss 0.646681.
Train: 2018-08-02T11:54:27.570019: step 12788, loss 0.621157.
Train: 2018-08-02T11:54:27.788685: step 12789, loss 0.562417.
Train: 2018-08-02T11:54:28.007409: step 12790, loss 0.604011.
Test: 2018-08-02T11:54:29.194606: step 12790, loss 0.547535.
Train: 2018-08-02T11:54:29.413335: step 12791, loss 0.620371.
Train: 2018-08-02T11:54:29.647659: step 12792, loss 0.5872.
Train: 2018-08-02T11:54:29.897566: step 12793, loss 0.538086.
Train: 2018-08-02T11:54:30.116291: step 12794, loss 0.465085.
Train: 2018-08-02T11:54:30.350615: step 12795, loss 0.522117.
Train: 2018-08-02T11:54:30.584938: step 12796, loss 0.668015.
Train: 2018-08-02T11:54:30.866090: step 12797, loss 0.530417.
Train: 2018-08-02T11:54:31.084812: step 12798, loss 0.595.
Train: 2018-08-02T11:54:31.303513: step 12799, loss 0.570822.
Train: 2018-08-02T11:54:31.522187: step 12800, loss 0.546748.
Test: 2018-08-02T11:54:32.725028: step 12800, loss 0.549645.
Train: 2018-08-02T11:54:33.564005: step 12801, loss 0.55482.
Train: 2018-08-02T11:54:33.782733: step 12802, loss 0.514864.
Train: 2018-08-02T11:54:34.001432: step 12803, loss 0.595149.
Train: 2018-08-02T11:54:34.220125: step 12804, loss 0.538716.
Train: 2018-08-02T11:54:34.438825: step 12805, loss 0.611036.
Train: 2018-08-02T11:54:34.657531: step 12806, loss 0.587077.
Train: 2018-08-02T11:54:34.876221: step 12807, loss 0.562798.
Train: 2018-08-02T11:54:35.094895: step 12808, loss 0.586766.
Train: 2018-08-02T11:54:35.313619: step 12809, loss 0.61862.
Train: 2018-08-02T11:54:35.532324: step 12810, loss 0.507533.
Test: 2018-08-02T11:54:36.719515: step 12810, loss 0.549619.
Train: 2018-08-02T11:54:36.922617: step 12811, loss 0.602612.
Train: 2018-08-02T11:54:37.141322: step 12812, loss 0.602579.
Train: 2018-08-02T11:54:37.360015: step 12813, loss 0.563086.
Train: 2018-08-02T11:54:37.594336: step 12814, loss 0.594617.
Train: 2018-08-02T11:54:37.815049: step 12815, loss 0.500279.
Train: 2018-08-02T11:54:38.033747: step 12816, loss 0.657441.
Train: 2018-08-02T11:54:38.252444: step 12817, loss 0.500479.
Train: 2018-08-02T11:54:38.471143: step 12818, loss 0.578877.
Train: 2018-08-02T11:54:38.721085: step 12819, loss 0.516215.
Train: 2018-08-02T11:54:38.939784: step 12820, loss 0.547519.
Test: 2018-08-02T11:54:40.142597: step 12820, loss 0.549703.
Train: 2018-08-02T11:54:40.345698: step 12821, loss 0.516077.
Train: 2018-08-02T11:54:40.564373: step 12822, loss 0.523788.
Train: 2018-08-02T11:54:40.798724: step 12823, loss 0.539393.
Train: 2018-08-02T11:54:41.017392: step 12824, loss 0.570937.
Train: 2018-08-02T11:54:41.236116: step 12825, loss 0.531173.
Train: 2018-08-02T11:54:41.454820: step 12826, loss 0.554929.
Train: 2018-08-02T11:54:41.673518: step 12827, loss 0.546841.
Train: 2018-08-02T11:54:41.892218: step 12828, loss 0.498519.
Train: 2018-08-02T11:54:42.110885: step 12829, loss 0.538518.
Train: 2018-08-02T11:54:42.329618: step 12830, loss 0.595128.
Test: 2018-08-02T11:54:43.532428: step 12830, loss 0.549787.
Train: 2018-08-02T11:54:43.751157: step 12831, loss 0.538208.
Train: 2018-08-02T11:54:43.969854: step 12832, loss 0.595291.
Train: 2018-08-02T11:54:44.219792: step 12833, loss 0.529781.
Train: 2018-08-02T11:54:44.438491: step 12834, loss 0.513205.
Train: 2018-08-02T11:54:44.625953: step 12835, loss 0.5977.
Train: 2018-08-02T11:54:44.844620: step 12836, loss 0.595583.
Train: 2018-08-02T11:54:45.063320: step 12837, loss 0.52104.
Train: 2018-08-02T11:54:45.313260: step 12838, loss 0.554168.
Train: 2018-08-02T11:54:45.531960: step 12839, loss 0.579095.
Train: 2018-08-02T11:54:45.750685: step 12840, loss 0.58743.
Test: 2018-08-02T11:54:46.953502: step 12840, loss 0.548304.
Train: 2018-08-02T11:54:47.172232: step 12841, loss 0.61249.
Train: 2018-08-02T11:54:47.406520: step 12842, loss 0.637462.
Train: 2018-08-02T11:54:47.625220: step 12843, loss 0.454327.
Train: 2018-08-02T11:54:47.875191: step 12844, loss 0.495887.
Train: 2018-08-02T11:54:48.093892: step 12845, loss 0.545774.
Train: 2018-08-02T11:54:48.328211: step 12846, loss 0.612484.
Train: 2018-08-02T11:54:48.546880: step 12847, loss 0.587461.
Train: 2018-08-02T11:54:48.765577: step 12848, loss 0.52071.
Train: 2018-08-02T11:54:48.984302: step 12849, loss 0.562426.
Train: 2018-08-02T11:54:49.218622: step 12850, loss 0.520677.
Test: 2018-08-02T11:54:50.405818: step 12850, loss 0.54865.
Train: 2018-08-02T11:54:50.640165: step 12851, loss 0.620906.
Train: 2018-08-02T11:54:50.874484: step 12852, loss 0.529006.
Train: 2018-08-02T11:54:51.093157: step 12853, loss 0.487225.
Train: 2018-08-02T11:54:51.327478: step 12854, loss 0.570776.
Train: 2018-08-02T11:54:51.546207: step 12855, loss 0.62105.
Train: 2018-08-02T11:54:51.780522: step 12856, loss 0.495409.
Train: 2018-08-02T11:54:52.014817: step 12857, loss 0.461806.
Train: 2018-08-02T11:54:52.233540: step 12858, loss 0.528762.
Train: 2018-08-02T11:54:52.467868: step 12859, loss 0.528648.
Train: 2018-08-02T11:54:52.686539: step 12860, loss 0.570815.
Test: 2018-08-02T11:54:53.889378: step 12860, loss 0.548191.
Train: 2018-08-02T11:54:54.108104: step 12861, loss 0.519959.
Train: 2018-08-02T11:54:54.326809: step 12862, loss 0.502813.
Train: 2018-08-02T11:54:54.545499: step 12863, loss 0.596474.
Train: 2018-08-02T11:54:54.764173: step 12864, loss 0.613665.
Train: 2018-08-02T11:54:54.982880: step 12865, loss 0.52808.
Train: 2018-08-02T11:54:55.217220: step 12866, loss 0.562336.
Train: 2018-08-02T11:54:55.435916: step 12867, loss 0.579507.
Train: 2018-08-02T11:54:55.654589: step 12868, loss 0.562337.
Train: 2018-08-02T11:54:55.888909: step 12869, loss 0.527989.
Train: 2018-08-02T11:54:56.123229: step 12870, loss 0.588077.
Test: 2018-08-02T11:54:57.310452: step 12870, loss 0.547206.
Train: 2018-08-02T11:54:57.529175: step 12871, loss 0.5795.
Train: 2018-08-02T11:54:57.747873: step 12872, loss 0.570913.
Train: 2018-08-02T11:54:57.966547: step 12873, loss 0.622309.
Train: 2018-08-02T11:54:58.185278: step 12874, loss 0.639278.
Train: 2018-08-02T11:54:58.419593: step 12875, loss 0.56235.
Train: 2018-08-02T11:54:58.653911: step 12876, loss 0.536898.
Train: 2018-08-02T11:54:58.872611: step 12877, loss 0.570816.
Train: 2018-08-02T11:54:59.091284: step 12878, loss 0.494932.
Train: 2018-08-02T11:54:59.310014: step 12879, loss 0.503464.
Train: 2018-08-02T11:54:59.528683: step 12880, loss 0.562379.
Test: 2018-08-02T11:55:00.731525: step 12880, loss 0.548145.
Train: 2018-08-02T11:55:00.934628: step 12881, loss 0.621241.
Train: 2018-08-02T11:55:01.168948: step 12882, loss 0.512029.
Train: 2018-08-02T11:55:01.387652: step 12883, loss 0.604332.
Train: 2018-08-02T11:55:01.621942: step 12884, loss 0.554028.
Train: 2018-08-02T11:55:01.856295: step 12885, loss 0.554048.
Train: 2018-08-02T11:55:02.074986: step 12886, loss 0.554065.
Train: 2018-08-02T11:55:02.293689: step 12887, loss 0.512363.
Train: 2018-08-02T11:55:02.512358: step 12888, loss 0.620826.
Train: 2018-08-02T11:55:02.731093: step 12889, loss 0.604093.
Train: 2018-08-02T11:55:02.965377: step 12890, loss 0.545817.
Test: 2018-08-02T11:55:04.168220: step 12890, loss 0.548213.
Train: 2018-08-02T11:55:04.386919: step 12891, loss 0.637165.
Train: 2018-08-02T11:55:04.590025: step 12892, loss 0.587306.
Train: 2018-08-02T11:55:04.808726: step 12893, loss 0.570756.
Train: 2018-08-02T11:55:05.027394: step 12894, loss 0.6694.
Train: 2018-08-02T11:55:05.261757: step 12895, loss 0.521703.
Train: 2018-08-02T11:55:05.464819: step 12896, loss 0.497464.
Train: 2018-08-02T11:55:05.683515: step 12897, loss 0.505754.
Train: 2018-08-02T11:55:05.902220: step 12898, loss 0.595155.
Train: 2018-08-02T11:55:06.136540: step 12899, loss 0.53834.
Train: 2018-08-02T11:55:06.355241: step 12900, loss 0.522147.
Test: 2018-08-02T11:55:07.558052: step 12900, loss 0.549136.
Train: 2018-08-02T11:55:08.432864: step 12901, loss 0.627564.
Train: 2018-08-02T11:55:08.651571: step 12902, loss 0.514083.
Train: 2018-08-02T11:55:08.870244: step 12903, loss 0.587001.
Train: 2018-08-02T11:55:09.088969: step 12904, loss 0.570797.
Train: 2018-08-02T11:55:09.307668: step 12905, loss 0.595086.
Train: 2018-08-02T11:55:09.541962: step 12906, loss 0.522283.
Train: 2018-08-02T11:55:09.760686: step 12907, loss 0.554631.
Train: 2018-08-02T11:55:09.979385: step 12908, loss 0.586978.
Train: 2018-08-02T11:55:10.198084: step 12909, loss 0.538467.
Train: 2018-08-02T11:55:10.416758: step 12910, loss 0.651667.
Test: 2018-08-02T11:55:11.635222: step 12910, loss 0.548699.
Train: 2018-08-02T11:55:11.853953: step 12911, loss 0.562738.
Train: 2018-08-02T11:55:12.072645: step 12912, loss 0.562758.
Train: 2018-08-02T11:55:12.291318: step 12913, loss 0.522518.
Train: 2018-08-02T11:55:12.510043: step 12914, loss 0.578876.
Train: 2018-08-02T11:55:12.744374: step 12915, loss 0.594967.
Train: 2018-08-02T11:55:12.963066: step 12916, loss 0.570835.
Train: 2018-08-02T11:55:13.181736: step 12917, loss 0.627044.
Train: 2018-08-02T11:55:13.400459: step 12918, loss 0.570854.
Train: 2018-08-02T11:55:13.634780: step 12919, loss 0.562875.
Train: 2018-08-02T11:55:13.869099: step 12920, loss 0.546944.
Test: 2018-08-02T11:55:15.056296: step 12920, loss 0.549794.
Train: 2018-08-02T11:55:15.259398: step 12921, loss 0.57089.
Train: 2018-08-02T11:55:15.478100: step 12922, loss 0.586819.
Train: 2018-08-02T11:55:15.696801: step 12923, loss 0.626551.
Train: 2018-08-02T11:55:15.915495: step 12924, loss 0.634355.
Train: 2018-08-02T11:55:16.165436: step 12925, loss 0.452495.
Train: 2018-08-02T11:55:16.368515: step 12926, loss 0.531513.
Train: 2018-08-02T11:55:16.587188: step 12927, loss 0.539399.
Train: 2018-08-02T11:55:16.805911: step 12928, loss 0.59466.
Train: 2018-08-02T11:55:17.024616: step 12929, loss 0.507756.
Train: 2018-08-02T11:55:17.258912: step 12930, loss 0.483906.
Test: 2018-08-02T11:55:18.446127: step 12930, loss 0.54954.
Train: 2018-08-02T11:55:18.664851: step 12931, loss 0.555042.
Train: 2018-08-02T11:55:18.883556: step 12932, loss 0.515142.
Train: 2018-08-02T11:55:19.102223: step 12933, loss 0.466904.
Train: 2018-08-02T11:55:19.336545: step 12934, loss 0.522569.
Train: 2018-08-02T11:55:19.555272: step 12935, loss 0.51415.
Train: 2018-08-02T11:55:19.789588: step 12936, loss 0.611506.
Train: 2018-08-02T11:55:20.008292: step 12937, loss 0.59532.
Train: 2018-08-02T11:55:20.226961: step 12938, loss 0.546115.
Train: 2018-08-02T11:55:20.461306: step 12939, loss 0.546025.
Train: 2018-08-02T11:55:20.680003: step 12940, loss 0.537662.
Test: 2018-08-02T11:55:21.882822: step 12940, loss 0.54859.
Train: 2018-08-02T11:55:22.085899: step 12941, loss 0.504338.
Train: 2018-08-02T11:55:22.304598: step 12942, loss 0.545748.
Train: 2018-08-02T11:55:22.523322: step 12943, loss 0.654499.
Train: 2018-08-02T11:55:22.757618: step 12944, loss 0.595937.
Train: 2018-08-02T11:55:22.976348: step 12945, loss 0.486853.
Train: 2018-08-02T11:55:23.210637: step 12946, loss 0.570796.
Train: 2018-08-02T11:55:23.413752: step 12947, loss 0.570798.
Train: 2018-08-02T11:55:23.632443: step 12948, loss 0.629825.
Train: 2018-08-02T11:55:23.882379: step 12949, loss 0.461247.
Train: 2018-08-02T11:55:24.101053: step 12950, loss 0.494855.
Test: 2018-08-02T11:55:25.288275: step 12950, loss 0.548816.
Train: 2018-08-02T11:55:25.506987: step 12951, loss 0.587738.
Train: 2018-08-02T11:55:25.725673: step 12952, loss 0.545412.
Train: 2018-08-02T11:55:25.959992: step 12953, loss 0.562361.
Train: 2018-08-02T11:55:26.178716: step 12954, loss 0.570832.
Train: 2018-08-02T11:55:26.397415: step 12955, loss 0.638835.
Train: 2018-08-02T11:55:26.616119: step 12956, loss 0.562346.
Train: 2018-08-02T11:55:26.834813: step 12957, loss 0.579312.
Train: 2018-08-02T11:55:27.069107: step 12958, loss 0.570819.
Train: 2018-08-02T11:55:27.287831: step 12959, loss 0.553911.
Train: 2018-08-02T11:55:27.506532: step 12960, loss 0.655217.
Test: 2018-08-02T11:55:28.709348: step 12960, loss 0.548909.
Train: 2018-08-02T11:55:28.912451: step 12961, loss 0.570796.
Train: 2018-08-02T11:55:29.131156: step 12962, loss 0.503698.
Train: 2018-08-02T11:55:29.365445: step 12963, loss 0.59588.
Train: 2018-08-02T11:55:29.584143: step 12964, loss 0.545727.
Train: 2018-08-02T11:55:29.802871: step 12965, loss 0.562434.
Train: 2018-08-02T11:55:30.037163: step 12966, loss 0.562447.
Train: 2018-08-02T11:55:30.271519: step 12967, loss 0.56246.
Train: 2018-08-02T11:55:30.505833: step 12968, loss 0.653594.
Train: 2018-08-02T11:55:30.724501: step 12969, loss 0.529482.
Train: 2018-08-02T11:55:30.943201: step 12970, loss 0.55429.
Test: 2018-08-02T11:55:32.146044: step 12970, loss 0.548822.
Train: 2018-08-02T11:55:32.349147: step 12971, loss 0.578975.
Train: 2018-08-02T11:55:32.583467: step 12972, loss 0.554369.
Train: 2018-08-02T11:55:32.802166: step 12973, loss 0.505318.
Train: 2018-08-02T11:55:33.020866: step 12974, loss 0.529887.
Train: 2018-08-02T11:55:33.239568: step 12975, loss 0.578944.
Train: 2018-08-02T11:55:33.473890: step 12976, loss 0.529891.
Train: 2018-08-02T11:55:33.692582: step 12977, loss 0.636198.
Train: 2018-08-02T11:55:33.926907: step 12978, loss 0.5626.
Train: 2018-08-02T11:55:34.145575: step 12979, loss 0.587098.
Train: 2018-08-02T11:55:34.364300: step 12980, loss 0.603376.
Test: 2018-08-02T11:55:35.567118: step 12980, loss 0.548694.
Train: 2018-08-02T11:55:35.770225: step 12981, loss 0.587047.
Train: 2018-08-02T11:55:36.004541: step 12982, loss 0.595128.
Train: 2018-08-02T11:55:36.223214: step 12983, loss 0.570802.
Train: 2018-08-02T11:55:36.441943: step 12984, loss 0.603085.
Train: 2018-08-02T11:55:36.660612: step 12985, loss 0.514544.
Train: 2018-08-02T11:55:36.848095: step 12986, loss 0.511448.
Train: 2018-08-02T11:55:37.082413: step 12987, loss 0.538756.
Train: 2018-08-02T11:55:37.301118: step 12988, loss 0.53875.
Train: 2018-08-02T11:55:37.519816: step 12989, loss 0.602959.
Train: 2018-08-02T11:55:37.738510: step 12990, loss 0.69127.
Test: 2018-08-02T11:55:38.925706: step 12990, loss 0.54985.
Train: 2018-08-02T11:55:39.222513: step 12991, loss 0.618895.
Train: 2018-08-02T11:55:39.441240: step 12992, loss 0.523021.
Train: 2018-08-02T11:55:39.659909: step 12993, loss 0.554983.
Train: 2018-08-02T11:55:39.878634: step 12994, loss 0.634467.
Train: 2018-08-02T11:55:40.097339: step 12995, loss 0.586779.
Train: 2018-08-02T11:55:40.316006: step 12996, loss 0.626231.
Train: 2018-08-02T11:55:40.534705: step 12997, loss 0.555282.
Train: 2018-08-02T11:55:40.753430: step 12998, loss 0.63373.
Train: 2018-08-02T11:55:40.972102: step 12999, loss 0.586693.
Train: 2018-08-02T11:55:41.190831: step 13000, loss 0.532303.
Test: 2018-08-02T11:55:42.378023: step 13000, loss 0.550143.
Train: 2018-08-02T11:55:43.237227: step 13001, loss 0.478243.
Train: 2018-08-02T11:55:43.455920: step 13002, loss 0.547961.
Train: 2018-08-02T11:55:43.690216: step 13003, loss 0.53247.
Train: 2018-08-02T11:55:43.908915: step 13004, loss 0.493658.
Train: 2018-08-02T11:55:44.127643: step 13005, loss 0.540041.
Train: 2018-08-02T11:55:44.361964: step 13006, loss 0.555496.
Train: 2018-08-02T11:55:44.580633: step 13007, loss 0.578881.
Train: 2018-08-02T11:55:44.814978: step 13008, loss 0.578873.
Train: 2018-08-02T11:55:45.033682: step 13009, loss 0.547407.
Train: 2018-08-02T11:55:45.267997: step 13010, loss 0.539426.
Test: 2018-08-02T11:55:46.470814: step 13010, loss 0.550552.
Train: 2018-08-02T11:55:46.673893: step 13011, loss 0.626328.
Train: 2018-08-02T11:55:46.923863: step 13012, loss 0.555081.
Train: 2018-08-02T11:55:47.142533: step 13013, loss 0.523298.
Train: 2018-08-02T11:55:47.361231: step 13014, loss 0.562934.
Train: 2018-08-02T11:55:47.579958: step 13015, loss 0.554923.
Train: 2018-08-02T11:55:47.814250: step 13016, loss 0.570791.
Train: 2018-08-02T11:55:48.017327: step 13017, loss 0.514747.
Train: 2018-08-02T11:55:48.251673: step 13018, loss 0.587007.
Train: 2018-08-02T11:55:48.470372: step 13019, loss 0.59513.
Train: 2018-08-02T11:55:48.689075: step 13020, loss 0.570814.
Test: 2018-08-02T11:55:49.891888: step 13020, loss 0.550515.
Train: 2018-08-02T11:55:50.094977: step 13021, loss 0.61954.
Train: 2018-08-02T11:55:50.329316: step 13022, loss 0.554652.
Train: 2018-08-02T11:55:50.548015: step 13023, loss 0.595021.
Train: 2018-08-02T11:55:50.766708: step 13024, loss 0.514371.
Train: 2018-08-02T11:55:51.001030: step 13025, loss 0.627291.
Train: 2018-08-02T11:55:51.235348: step 13026, loss 0.57888.
Train: 2018-08-02T11:55:51.454039: step 13027, loss 0.594983.
Train: 2018-08-02T11:55:51.672752: step 13028, loss 0.594954.
Train: 2018-08-02T11:55:51.891450: step 13029, loss 0.610964.
Train: 2018-08-02T11:55:52.110150: step 13030, loss 0.634875.
Test: 2018-08-02T11:55:53.312962: step 13030, loss 0.549426.
Train: 2018-08-02T11:55:53.516041: step 13031, loss 0.54698.
Train: 2018-08-02T11:55:53.734738: step 13032, loss 0.594748.
Train: 2018-08-02T11:55:53.953470: step 13033, loss 0.602612.
Train: 2018-08-02T11:55:54.172137: step 13034, loss 0.531539.
Train: 2018-08-02T11:55:54.390860: step 13035, loss 0.539533.
Train: 2018-08-02T11:55:54.640777: step 13036, loss 0.55531.
Train: 2018-08-02T11:55:54.843881: step 13037, loss 0.500423.
Train: 2018-08-02T11:55:55.062585: step 13038, loss 0.531775.
Train: 2018-08-02T11:55:55.296897: step 13039, loss 0.571009.
Train: 2018-08-02T11:55:55.499950: step 13040, loss 0.578867.
Test: 2018-08-02T11:55:56.687172: step 13040, loss 0.550498.
Train: 2018-08-02T11:55:56.905897: step 13041, loss 0.594621.
Train: 2018-08-02T11:55:57.124571: step 13042, loss 0.594625.
Train: 2018-08-02T11:55:57.343294: step 13043, loss 0.531597.
Train: 2018-08-02T11:55:57.577589: step 13044, loss 0.531567.
Train: 2018-08-02T11:55:57.796317: step 13045, loss 0.563074.
Train: 2018-08-02T11:55:58.030638: step 13046, loss 0.483995.
Train: 2018-08-02T11:55:58.249331: step 13047, loss 0.539201.
Train: 2018-08-02T11:55:58.468032: step 13048, loss 0.610703.
Train: 2018-08-02T11:55:58.686705: step 13049, loss 0.586826.
Train: 2018-08-02T11:55:58.905433: step 13050, loss 0.514928.
Test: 2018-08-02T11:56:00.108246: step 13050, loss 0.549088.
Train: 2018-08-02T11:56:00.311349: step 13051, loss 0.610927.
Train: 2018-08-02T11:56:00.530052: step 13052, loss 0.562818.
Train: 2018-08-02T11:56:00.748746: step 13053, loss 0.594944.
Train: 2018-08-02T11:56:00.967452: step 13054, loss 0.562778.
Train: 2018-08-02T11:56:01.201740: step 13055, loss 0.506419.
Train: 2018-08-02T11:56:01.420463: step 13056, loss 0.514415.
Train: 2018-08-02T11:56:01.639169: step 13057, loss 0.619523.
Train: 2018-08-02T11:56:01.857835: step 13058, loss 0.570767.
Train: 2018-08-02T11:56:02.076563: step 13059, loss 0.530222.
Train: 2018-08-02T11:56:02.310885: step 13060, loss 0.530203.
Test: 2018-08-02T11:56:03.513698: step 13060, loss 0.548862.
Train: 2018-08-02T11:56:03.716801: step 13061, loss 0.56259.
Train: 2018-08-02T11:56:03.935499: step 13062, loss 0.578914.
Train: 2018-08-02T11:56:04.154180: step 13063, loss 0.603587.
Train: 2018-08-02T11:56:04.388495: step 13064, loss 0.521869.
Train: 2018-08-02T11:56:04.607223: step 13065, loss 0.56255.
Train: 2018-08-02T11:56:04.825891: step 13066, loss 0.59527.
Train: 2018-08-02T11:56:05.044618: step 13067, loss 0.49709.
Train: 2018-08-02T11:56:05.263319: step 13068, loss 0.497032.
Train: 2018-08-02T11:56:05.482013: step 13069, loss 0.570639.
Train: 2018-08-02T11:56:05.716332: step 13070, loss 0.52133.
Test: 2018-08-02T11:56:06.903529: step 13070, loss 0.548265.
Train: 2018-08-02T11:56:07.106632: step 13071, loss 0.512672.
Train: 2018-08-02T11:56:07.325331: step 13072, loss 0.586905.
Train: 2018-08-02T11:56:07.544035: step 13073, loss 0.570747.
Train: 2018-08-02T11:56:07.762729: step 13074, loss 0.554444.
Train: 2018-08-02T11:56:07.997023: step 13075, loss 0.563046.
Train: 2018-08-02T11:56:08.215722: step 13076, loss 0.562895.
Train: 2018-08-02T11:56:08.450067: step 13077, loss 0.537054.
Train: 2018-08-02T11:56:08.668767: step 13078, loss 0.579136.
Train: 2018-08-02T11:56:08.918683: step 13079, loss 0.588021.
Train: 2018-08-02T11:56:09.137411: step 13080, loss 0.596125.
Test: 2018-08-02T11:56:10.324603: step 13080, loss 0.5482.
Train: 2018-08-02T11:56:10.543302: step 13081, loss 0.587435.
Train: 2018-08-02T11:56:10.762027: step 13082, loss 0.59592.
Train: 2018-08-02T11:56:10.980700: step 13083, loss 0.545752.
Train: 2018-08-02T11:56:11.199427: step 13084, loss 0.612355.
Train: 2018-08-02T11:56:11.433719: step 13085, loss 0.504377.
Train: 2018-08-02T11:56:11.652418: step 13086, loss 0.637063.
Train: 2018-08-02T11:56:11.871142: step 13087, loss 0.521157.
Train: 2018-08-02T11:56:12.089835: step 13088, loss 0.595517.
Train: 2018-08-02T11:56:12.308540: step 13089, loss 0.587228.
Train: 2018-08-02T11:56:12.527244: step 13090, loss 0.537899.
Test: 2018-08-02T11:56:13.730057: step 13090, loss 0.54981.
Train: 2018-08-02T11:56:13.933135: step 13091, loss 0.562561.
Train: 2018-08-02T11:56:14.151859: step 13092, loss 0.595326.
Train: 2018-08-02T11:56:14.386197: step 13093, loss 0.619785.
Train: 2018-08-02T11:56:14.604854: step 13094, loss 0.53006.
Train: 2018-08-02T11:56:14.823549: step 13095, loss 0.505776.
Train: 2018-08-02T11:56:15.042279: step 13096, loss 0.546427.
Train: 2018-08-02T11:56:15.260947: step 13097, loss 0.578906.
Train: 2018-08-02T11:56:15.510915: step 13098, loss 0.562678.
Train: 2018-08-02T11:56:15.729588: step 13099, loss 0.554575.
Train: 2018-08-02T11:56:15.948312: step 13100, loss 0.505942.
Test: 2018-08-02T11:56:17.151130: step 13100, loss 0.547629.
Train: 2018-08-02T11:56:18.010304: step 13101, loss 0.57079.
Train: 2018-08-02T11:56:18.244654: step 13102, loss 0.595149.
Train: 2018-08-02T11:56:18.463354: step 13103, loss 0.570787.
Train: 2018-08-02T11:56:18.682047: step 13104, loss 0.570788.
Train: 2018-08-02T11:56:18.947614: step 13105, loss 0.587023.
Train: 2018-08-02T11:56:19.166309: step 13106, loss 0.595124.
Train: 2018-08-02T11:56:19.384983: step 13107, loss 0.562697.
Train: 2018-08-02T11:56:19.603706: step 13108, loss 0.53844.
Train: 2018-08-02T11:56:19.822379: step 13109, loss 0.538459.
Train: 2018-08-02T11:56:20.056700: step 13110, loss 0.627414.
Test: 2018-08-02T11:56:21.243921: step 13110, loss 0.548685.
Train: 2018-08-02T11:56:21.447031: step 13111, loss 0.538501.
Train: 2018-08-02T11:56:21.665722: step 13112, loss 0.506232.
Train: 2018-08-02T11:56:21.900018: step 13113, loss 0.546573.
Train: 2018-08-02T11:56:22.134339: step 13114, loss 0.578891.
Train: 2018-08-02T11:56:22.353067: step 13115, loss 0.578893.
Train: 2018-08-02T11:56:22.571766: step 13116, loss 0.489852.
Train: 2018-08-02T11:56:22.790469: step 13117, loss 0.4978.
Train: 2018-08-02T11:56:23.009163: step 13118, loss 0.521967.
Train: 2018-08-02T11:56:23.243478: step 13119, loss 0.554451.
Train: 2018-08-02T11:56:23.462154: step 13120, loss 0.464239.
Test: 2018-08-02T11:56:24.664995: step 13120, loss 0.54952.
Train: 2018-08-02T11:56:24.883694: step 13121, loss 0.644925.
Train: 2018-08-02T11:56:25.102427: step 13122, loss 0.504665.
Train: 2018-08-02T11:56:25.321117: step 13123, loss 0.595646.
Train: 2018-08-02T11:56:25.571061: step 13124, loss 0.545787.
Train: 2018-08-02T11:56:25.789757: step 13125, loss 0.47905.
Train: 2018-08-02T11:56:26.008461: step 13126, loss 0.587659.
Train: 2018-08-02T11:56:26.227129: step 13127, loss 0.587613.
Train: 2018-08-02T11:56:26.445858: step 13128, loss 0.495096.
Train: 2018-08-02T11:56:26.680173: step 13129, loss 0.520213.
Train: 2018-08-02T11:56:26.898873: step 13130, loss 0.621559.
Test: 2018-08-02T11:56:28.101690: step 13130, loss 0.5482.
Train: 2018-08-02T11:56:28.304799: step 13131, loss 0.503011.
Train: 2018-08-02T11:56:28.523466: step 13132, loss 0.562197.
Train: 2018-08-02T11:56:28.757818: step 13133, loss 0.57949.
Train: 2018-08-02T11:56:28.992107: step 13134, loss 0.519712.
Train: 2018-08-02T11:56:29.210833: step 13135, loss 0.596183.
Train: 2018-08-02T11:56:29.445127: step 13136, loss 0.519632.
Train: 2018-08-02T11:56:29.632583: step 13137, loss 0.580752.
Train: 2018-08-02T11:56:29.866903: step 13138, loss 0.613757.
Train: 2018-08-02T11:56:30.085625: step 13139, loss 0.579231.
Train: 2018-08-02T11:56:30.304330: step 13140, loss 0.468253.
Test: 2018-08-02T11:56:31.507143: step 13140, loss 0.548016.
Train: 2018-08-02T11:56:31.710221: step 13141, loss 0.597052.
Train: 2018-08-02T11:56:31.944566: step 13142, loss 0.622441.
Train: 2018-08-02T11:56:32.194519: step 13143, loss 0.510999.
Train: 2018-08-02T11:56:32.413181: step 13144, loss 0.622116.
Train: 2018-08-02T11:56:32.631879: step 13145, loss 0.638854.
Train: 2018-08-02T11:56:32.850609: step 13146, loss 0.511544.
Train: 2018-08-02T11:56:33.084932: step 13147, loss 0.579241.
Train: 2018-08-02T11:56:33.303622: step 13148, loss 0.53712.
Train: 2018-08-02T11:56:33.522320: step 13149, loss 0.595991.
Train: 2018-08-02T11:56:33.740995: step 13150, loss 0.562399.
Test: 2018-08-02T11:56:34.928217: step 13150, loss 0.54903.
Train: 2018-08-02T11:56:35.131325: step 13151, loss 0.545694.
Train: 2018-08-02T11:56:35.365645: step 13152, loss 0.537398.
Train: 2018-08-02T11:56:35.568720: step 13153, loss 0.562433.
Train: 2018-08-02T11:56:35.803036: step 13154, loss 0.520847.
Train: 2018-08-02T11:56:36.021710: step 13155, loss 0.52087.
Train: 2018-08-02T11:56:36.240410: step 13156, loss 0.54581.
Train: 2018-08-02T11:56:36.459108: step 13157, loss 0.504188.
Train: 2018-08-02T11:56:36.693429: step 13158, loss 0.47075.
Train: 2018-08-02T11:56:36.896507: step 13159, loss 0.554052.
Train: 2018-08-02T11:56:37.115205: step 13160, loss 0.604314.
Test: 2018-08-02T11:56:38.318048: step 13160, loss 0.547009.
Train: 2018-08-02T11:56:38.521152: step 13161, loss 0.663161.
Train: 2018-08-02T11:56:38.755446: step 13162, loss 0.554001.
Train: 2018-08-02T11:56:38.974170: step 13163, loss 0.562393.
Train: 2018-08-02T11:56:39.192843: step 13164, loss 0.537251.
Train: 2018-08-02T11:56:39.411542: step 13165, loss 0.537259.
Train: 2018-08-02T11:56:39.645889: step 13166, loss 0.537253.
Train: 2018-08-02T11:56:39.864586: step 13167, loss 0.545624.
Train: 2018-08-02T11:56:40.083285: step 13168, loss 0.570787.
Train: 2018-08-02T11:56:40.301959: step 13169, loss 0.61276.
Train: 2018-08-02T11:56:40.520662: step 13170, loss 0.537232.
Test: 2018-08-02T11:56:41.723500: step 13170, loss 0.548008.
Train: 2018-08-02T11:56:41.926578: step 13171, loss 0.537244.
Train: 2018-08-02T11:56:42.145303: step 13172, loss 0.528859.
Train: 2018-08-02T11:56:42.379628: step 13173, loss 0.604337.
Train: 2018-08-02T11:56:42.598296: step 13174, loss 0.57078.
Train: 2018-08-02T11:56:42.816995: step 13175, loss 0.612675.
Train: 2018-08-02T11:56:43.051340: step 13176, loss 0.604232.
Train: 2018-08-02T11:56:43.270014: step 13177, loss 0.51236.
Train: 2018-08-02T11:56:43.488712: step 13178, loss 0.637423.
Train: 2018-08-02T11:56:43.723033: step 13179, loss 0.562451.
Train: 2018-08-02T11:56:43.941757: step 13180, loss 0.570757.
Test: 2018-08-02T11:56:45.128953: step 13180, loss 0.547926.
Train: 2018-08-02T11:56:45.363273: step 13181, loss 0.488103.
Train: 2018-08-02T11:56:45.581997: step 13182, loss 0.612045.
Train: 2018-08-02T11:56:45.816318: step 13183, loss 0.471849.
Train: 2018-08-02T11:56:46.050621: step 13184, loss 0.554271.
Train: 2018-08-02T11:56:46.284969: step 13185, loss 0.603736.
Train: 2018-08-02T11:56:46.519253: step 13186, loss 0.554275.
Train: 2018-08-02T11:56:46.737978: step 13187, loss 0.562517.
Train: 2018-08-02T11:56:46.956675: step 13188, loss 0.554299.
Train: 2018-08-02T11:56:47.175385: step 13189, loss 0.570762.
Train: 2018-08-02T11:56:47.409699: step 13190, loss 0.521419.
Test: 2018-08-02T11:56:48.596891: step 13190, loss 0.548788.
Train: 2018-08-02T11:56:48.799999: step 13191, loss 0.579001.
Train: 2018-08-02T11:56:49.018673: step 13192, loss 0.513175.
Train: 2018-08-02T11:56:49.237392: step 13193, loss 0.570755.
Train: 2018-08-02T11:56:49.456066: step 13194, loss 0.587237.
Train: 2018-08-02T11:56:49.690415: step 13195, loss 0.620185.
Train: 2018-08-02T11:56:49.924738: step 13196, loss 0.504932.
Train: 2018-08-02T11:56:50.143429: step 13197, loss 0.603669.
Train: 2018-08-02T11:56:50.362134: step 13198, loss 0.644749.
Train: 2018-08-02T11:56:50.596425: step 13199, loss 0.603566.
Train: 2018-08-02T11:56:50.830770: step 13200, loss 0.587116.
Test: 2018-08-02T11:56:52.033586: step 13200, loss 0.549771.
Train: 2018-08-02T11:56:52.908384: step 13201, loss 0.587071.
Train: 2018-08-02T11:56:53.127106: step 13202, loss 0.570789.
Train: 2018-08-02T11:56:53.345778: step 13203, loss 0.554617.
Train: 2018-08-02T11:56:53.564510: step 13204, loss 0.595022.
Train: 2018-08-02T11:56:53.783206: step 13205, loss 0.57083.
Train: 2018-08-02T11:56:54.017528: step 13206, loss 0.554807.
Train: 2018-08-02T11:56:54.236221: step 13207, loss 0.618867.
Train: 2018-08-02T11:56:54.470518: step 13208, loss 0.538989.
Train: 2018-08-02T11:56:54.689247: step 13209, loss 0.586814.
Train: 2018-08-02T11:56:54.907913: step 13210, loss 0.531248.
Test: 2018-08-02T11:56:56.095135: step 13210, loss 0.549779.
Train: 2018-08-02T11:56:56.345077: step 13211, loss 0.713558.
Train: 2018-08-02T11:56:56.579425: step 13212, loss 0.523648.
Train: 2018-08-02T11:56:56.798098: step 13213, loss 0.476635.
Train: 2018-08-02T11:56:57.016796: step 13214, loss 0.547433.
Train: 2018-08-02T11:56:57.251140: step 13215, loss 0.578872.
Train: 2018-08-02T11:56:57.469838: step 13216, loss 0.508153.
Train: 2018-08-02T11:56:57.688542: step 13217, loss 0.578867.
Train: 2018-08-02T11:56:57.907236: step 13218, loss 0.523751.
Train: 2018-08-02T11:56:58.125941: step 13219, loss 0.507864.
Train: 2018-08-02T11:56:58.360259: step 13220, loss 0.594685.
Test: 2018-08-02T11:56:59.547452: step 13220, loss 0.549562.
Train: 2018-08-02T11:56:59.750529: step 13221, loss 0.586774.
Train: 2018-08-02T11:56:59.984850: step 13222, loss 0.531158.
Train: 2018-08-02T11:57:00.203548: step 13223, loss 0.53112.
Train: 2018-08-02T11:57:00.437868: step 13224, loss 0.5388.
Train: 2018-08-02T11:57:00.656567: step 13225, loss 0.546645.
Train: 2018-08-02T11:57:00.875265: step 13226, loss 0.547028.
Train: 2018-08-02T11:57:01.093994: step 13227, loss 0.545728.
Train: 2018-08-02T11:57:01.312696: step 13228, loss 0.547291.
Train: 2018-08-02T11:57:01.547010: step 13229, loss 0.538223.
Train: 2018-08-02T11:57:01.781335: step 13230, loss 0.595968.
Test: 2018-08-02T11:57:02.968525: step 13230, loss 0.547974.
Train: 2018-08-02T11:57:03.187249: step 13231, loss 0.521326.
Train: 2018-08-02T11:57:03.421575: step 13232, loss 0.562498.
Train: 2018-08-02T11:57:03.640243: step 13233, loss 0.537923.
Train: 2018-08-02T11:57:03.858944: step 13234, loss 0.545735.
Train: 2018-08-02T11:57:04.093276: step 13235, loss 0.537757.
Train: 2018-08-02T11:57:04.311989: step 13236, loss 0.60381.
Train: 2018-08-02T11:57:04.530690: step 13237, loss 0.570142.
Train: 2018-08-02T11:57:04.749383: step 13238, loss 0.546008.
Train: 2018-08-02T11:57:04.983707: step 13239, loss 0.562043.
Train: 2018-08-02T11:57:05.202405: step 13240, loss 0.595808.
Test: 2018-08-02T11:57:06.389600: step 13240, loss 0.547975.
Train: 2018-08-02T11:57:06.623921: step 13241, loss 0.53694.
Train: 2018-08-02T11:57:06.842649: step 13242, loss 0.570611.
Train: 2018-08-02T11:57:07.076939: step 13243, loss 0.579598.
Train: 2018-08-02T11:57:07.295638: step 13244, loss 0.620978.
Train: 2018-08-02T11:57:07.529983: step 13245, loss 0.537353.
Train: 2018-08-02T11:57:07.748656: step 13246, loss 0.511846.
Train: 2018-08-02T11:57:07.967355: step 13247, loss 0.57125.
Train: 2018-08-02T11:57:08.186081: step 13248, loss 0.587157.
Train: 2018-08-02T11:57:08.420407: step 13249, loss 0.562665.
Train: 2018-08-02T11:57:08.639099: step 13250, loss 0.571057.
Test: 2018-08-02T11:57:09.841915: step 13250, loss 0.547024.
Train: 2018-08-02T11:57:10.060647: step 13251, loss 0.570908.
Train: 2018-08-02T11:57:10.279314: step 13252, loss 0.595786.
Train: 2018-08-02T11:57:10.529255: step 13253, loss 0.537762.
Train: 2018-08-02T11:57:10.747978: step 13254, loss 0.562614.
Train: 2018-08-02T11:57:10.966654: step 13255, loss 0.529683.
Train: 2018-08-02T11:57:11.200973: step 13256, loss 0.669295.
Train: 2018-08-02T11:57:11.435323: step 13257, loss 0.554387.
Train: 2018-08-02T11:57:11.685259: step 13258, loss 0.57894.
Train: 2018-08-02T11:57:11.903958: step 13259, loss 0.603381.
Train: 2018-08-02T11:57:12.122657: step 13260, loss 0.530148.
Test: 2018-08-02T11:57:13.325475: step 13260, loss 0.549321.
Train: 2018-08-02T11:57:13.528579: step 13261, loss 0.570791.
Train: 2018-08-02T11:57:13.778532: step 13262, loss 0.554608.
Train: 2018-08-02T11:57:13.997224: step 13263, loss 0.53039.
Train: 2018-08-02T11:57:14.215892: step 13264, loss 0.538498.
Train: 2018-08-02T11:57:14.434590: step 13265, loss 0.546577.
Train: 2018-08-02T11:57:14.668936: step 13266, loss 0.603128.
Train: 2018-08-02T11:57:14.903233: step 13267, loss 0.530429.
Train: 2018-08-02T11:57:15.121930: step 13268, loss 0.595044.
Train: 2018-08-02T11:57:15.340654: step 13269, loss 0.530434.
Train: 2018-08-02T11:57:15.559353: step 13270, loss 0.554652.
Test: 2018-08-02T11:57:16.762170: step 13270, loss 0.547922.
Train: 2018-08-02T11:57:16.965273: step 13271, loss 0.651635.
Train: 2018-08-02T11:57:17.199570: step 13272, loss 0.554668.
Train: 2018-08-02T11:57:17.402646: step 13273, loss 0.586945.
Train: 2018-08-02T11:57:17.621371: step 13274, loss 0.562771.
Train: 2018-08-02T11:57:17.855695: step 13275, loss 0.57083.
Train: 2018-08-02T11:57:18.074363: step 13276, loss 0.490493.
Train: 2018-08-02T11:57:18.308709: step 13277, loss 0.522599.
Train: 2018-08-02T11:57:18.527382: step 13278, loss 0.514472.
Train: 2018-08-02T11:57:18.746106: step 13279, loss 0.530463.
Train: 2018-08-02T11:57:18.964785: step 13280, loss 0.538427.
Test: 2018-08-02T11:57:20.167623: step 13280, loss 0.547989.
Train: 2018-08-02T11:57:20.386327: step 13281, loss 0.562669.
Train: 2018-08-02T11:57:20.620673: step 13282, loss 0.58706.
Train: 2018-08-02T11:57:20.839366: step 13283, loss 0.595247.
Train: 2018-08-02T11:57:21.058065: step 13284, loss 0.529927.
Train: 2018-08-02T11:57:21.276769: step 13285, loss 0.570759.
Train: 2018-08-02T11:57:21.526710: step 13286, loss 0.56258.
Train: 2018-08-02T11:57:21.745409: step 13287, loss 0.513337.
Train: 2018-08-02T11:57:21.932835: step 13288, loss 0.562518.
Train: 2018-08-02T11:57:22.151533: step 13289, loss 0.595488.
Train: 2018-08-02T11:57:22.370258: step 13290, loss 0.620241.
Test: 2018-08-02T11:57:23.557454: step 13290, loss 0.549513.
Train: 2018-08-02T11:57:23.791807: step 13291, loss 0.578975.
Train: 2018-08-02T11:57:24.010499: step 13292, loss 0.587226.
Train: 2018-08-02T11:57:24.244794: step 13293, loss 0.496754.
Train: 2018-08-02T11:57:24.463517: step 13294, loss 0.529637.
Train: 2018-08-02T11:57:24.682223: step 13295, loss 0.56254.
Train: 2018-08-02T11:57:24.947786: step 13296, loss 0.603706.
Train: 2018-08-02T11:57:25.182074: step 13297, loss 0.595457.
Train: 2018-08-02T11:57:25.400774: step 13298, loss 0.603669.
Train: 2018-08-02T11:57:25.635124: step 13299, loss 0.620026.
Train: 2018-08-02T11:57:25.869414: step 13300, loss 0.619897.
Test: 2018-08-02T11:57:27.056635: step 13300, loss 0.548991.
Train: 2018-08-02T11:57:28.009536: step 13301, loss 0.538142.
Train: 2018-08-02T11:57:28.243883: step 13302, loss 0.595184.
Train: 2018-08-02T11:57:28.462557: step 13303, loss 0.570793.
Train: 2018-08-02T11:57:28.681279: step 13304, loss 0.554642.
Train: 2018-08-02T11:57:28.899952: step 13305, loss 0.651434.
Train: 2018-08-02T11:57:29.134273: step 13306, loss 0.554785.
Train: 2018-08-02T11:57:29.352997: step 13307, loss 0.562864.
Train: 2018-08-02T11:57:29.587292: step 13308, loss 0.531009.
Train: 2018-08-02T11:57:29.805992: step 13309, loss 0.578859.
Train: 2018-08-02T11:57:30.055960: step 13310, loss 0.602688.
Test: 2018-08-02T11:57:31.243154: step 13310, loss 0.549783.
Train: 2018-08-02T11:57:31.461853: step 13311, loss 0.578859.
Train: 2018-08-02T11:57:31.696205: step 13312, loss 0.555154.
Train: 2018-08-02T11:57:31.899276: step 13313, loss 0.507882.
Train: 2018-08-02T11:57:32.117950: step 13314, loss 0.523678.
Train: 2018-08-02T11:57:32.336648: step 13315, loss 0.51575.
Train: 2018-08-02T11:57:32.555347: step 13316, loss 0.523535.
Train: 2018-08-02T11:57:32.774079: step 13317, loss 0.563009.
Train: 2018-08-02T11:57:33.008397: step 13318, loss 0.531188.
Train: 2018-08-02T11:57:33.227089: step 13319, loss 0.56292.
Train: 2018-08-02T11:57:33.445788: step 13320, loss 0.538898.
Test: 2018-08-02T11:57:34.632986: step 13320, loss 0.549068.
Train: 2018-08-02T11:57:34.851684: step 13321, loss 0.602926.
Train: 2018-08-02T11:57:35.070415: step 13322, loss 0.643156.
Train: 2018-08-02T11:57:35.289115: step 13323, loss 0.594943.
Train: 2018-08-02T11:57:35.523432: step 13324, loss 0.53067.
Train: 2018-08-02T11:57:35.757722: step 13325, loss 0.586907.
Train: 2018-08-02T11:57:35.976450: step 13326, loss 0.594942.
Train: 2018-08-02T11:57:36.195148: step 13327, loss 0.522662.
Train: 2018-08-02T11:57:36.429439: step 13328, loss 0.530678.
Train: 2018-08-02T11:57:36.648168: step 13329, loss 0.546712.
Train: 2018-08-02T11:57:36.898106: step 13330, loss 0.611082.
Test: 2018-08-02T11:57:38.085302: step 13330, loss 0.549692.
Train: 2018-08-02T11:57:38.302387: step 13331, loss 0.619139.
Train: 2018-08-02T11:57:38.552328: step 13332, loss 0.538652.
Train: 2018-08-02T11:57:38.786679: step 13333, loss 0.53866.
Train: 2018-08-02T11:57:39.005347: step 13334, loss 0.586917.
Train: 2018-08-02T11:57:39.255313: step 13335, loss 0.530608.
Train: 2018-08-02T11:57:39.474013: step 13336, loss 0.61107.
Train: 2018-08-02T11:57:39.708332: step 13337, loss 0.586912.
Train: 2018-08-02T11:57:39.927030: step 13338, loss 0.530645.
Train: 2018-08-02T11:57:40.161328: step 13339, loss 0.522608.
Train: 2018-08-02T11:57:40.395676: step 13340, loss 0.514501.
Test: 2018-08-02T11:57:41.598489: step 13340, loss 0.549658.
Train: 2018-08-02T11:57:41.848430: step 13341, loss 0.546612.
Train: 2018-08-02T11:57:42.067155: step 13342, loss 0.546577.
Train: 2018-08-02T11:57:42.285855: step 13343, loss 0.595098.
Train: 2018-08-02T11:57:42.520179: step 13344, loss 0.530246.
Train: 2018-08-02T11:57:42.738872: step 13345, loss 0.611495.
Train: 2018-08-02T11:57:42.957545: step 13346, loss 0.546388.
Train: 2018-08-02T11:57:43.191896: step 13347, loss 0.554501.
Train: 2018-08-02T11:57:43.426213: step 13348, loss 0.570776.
Train: 2018-08-02T11:57:43.644884: step 13349, loss 0.636016.
Train: 2018-08-02T11:57:43.879206: step 13350, loss 0.587074.
Test: 2018-08-02T11:57:45.066427: step 13350, loss 0.549985.
Train: 2018-08-02T11:57:45.285125: step 13351, loss 0.635891.
Train: 2018-08-02T11:57:45.503856: step 13352, loss 0.611376.
Train: 2018-08-02T11:57:45.738145: step 13353, loss 0.578892.
Train: 2018-08-02T11:57:45.956843: step 13354, loss 0.578881.
Train: 2018-08-02T11:57:46.175570: step 13355, loss 0.554764.
Train: 2018-08-02T11:57:46.409864: step 13356, loss 0.554825.
Train: 2018-08-02T11:57:46.644213: step 13357, loss 0.554875.
Train: 2018-08-02T11:57:46.878529: step 13358, loss 0.570879.
Train: 2018-08-02T11:57:47.112823: step 13359, loss 0.443396.
Train: 2018-08-02T11:57:47.331551: step 13360, loss 0.562904.
Test: 2018-08-02T11:57:48.534365: step 13360, loss 0.549361.
Train: 2018-08-02T11:57:48.753097: step 13361, loss 0.554898.
Train: 2018-08-02T11:57:48.987413: step 13362, loss 0.578863.
Train: 2018-08-02T11:57:49.206108: step 13363, loss 0.530825.
Train: 2018-08-02T11:57:49.424813: step 13364, loss 0.554808.
Train: 2018-08-02T11:57:49.659133: step 13365, loss 0.651174.
Train: 2018-08-02T11:57:49.877832: step 13366, loss 0.546745.
Train: 2018-08-02T11:57:50.127742: step 13367, loss 0.482477.
Train: 2018-08-02T11:57:50.346471: step 13368, loss 0.538634.
Train: 2018-08-02T11:57:50.565141: step 13369, loss 0.619226.
Train: 2018-08-02T11:57:50.783869: step 13370, loss 0.570806.
Test: 2018-08-02T11:57:51.986682: step 13370, loss 0.549787.
Train: 2018-08-02T11:57:52.189784: step 13371, loss 0.627373.
Train: 2018-08-02T11:57:52.424079: step 13372, loss 0.595038.
Train: 2018-08-02T11:57:52.658399: step 13373, loss 0.57081.
Train: 2018-08-02T11:57:52.877128: step 13374, loss 0.611106.
Train: 2018-08-02T11:57:53.095822: step 13375, loss 0.56279.
Train: 2018-08-02T11:57:53.330119: step 13376, loss 0.586891.
Train: 2018-08-02T11:57:53.564468: step 13377, loss 0.514815.
Train: 2018-08-02T11:57:53.783167: step 13378, loss 0.506867.
Train: 2018-08-02T11:57:54.001861: step 13379, loss 0.53083.
Train: 2018-08-02T11:57:54.236155: step 13380, loss 0.53878.
Test: 2018-08-02T11:57:55.423377: step 13380, loss 0.549578.
Train: 2018-08-02T11:57:55.657734: step 13381, loss 0.562817.
Train: 2018-08-02T11:57:55.876396: step 13382, loss 0.611019.
Train: 2018-08-02T11:57:56.095094: step 13383, loss 0.627193.
Train: 2018-08-02T11:57:56.313818: step 13384, loss 0.594952.
Train: 2018-08-02T11:57:56.532519: step 13385, loss 0.594938.
Train: 2018-08-02T11:57:56.751216: step 13386, loss 0.506676.
Train: 2018-08-02T11:57:56.969889: step 13387, loss 0.498707.
Train: 2018-08-02T11:57:57.188613: step 13388, loss 0.562784.
Train: 2018-08-02T11:57:57.407287: step 13389, loss 0.546648.
Train: 2018-08-02T11:57:57.641633: step 13390, loss 0.498095.
Test: 2018-08-02T11:57:58.844451: step 13390, loss 0.548187.
Train: 2018-08-02T11:57:59.078797: step 13391, loss 0.489282.
Train: 2018-08-02T11:57:59.297470: step 13392, loss 0.513404.
Train: 2018-08-02T11:57:59.516193: step 13393, loss 0.587843.
Train: 2018-08-02T11:57:59.734899: step 13394, loss 0.443878.
Train: 2018-08-02T11:57:59.969220: step 13395, loss 0.54474.
Train: 2018-08-02T11:58:00.203509: step 13396, loss 0.584231.
Train: 2018-08-02T11:58:00.422231: step 13397, loss 0.525383.
Train: 2018-08-02T11:58:00.640905: step 13398, loss 0.458068.
Train: 2018-08-02T11:58:00.859603: step 13399, loss 0.552275.
Train: 2018-08-02T11:58:01.093924: step 13400, loss 0.579373.
Test: 2018-08-02T11:58:02.281146: step 13400, loss 0.547543.
Train: 2018-08-02T11:58:03.093456: step 13401, loss 0.563173.
Train: 2018-08-02T11:58:03.327776: step 13402, loss 0.574404.
Train: 2018-08-02T11:58:03.546505: step 13403, loss 0.52558.
Train: 2018-08-02T11:58:03.765173: step 13404, loss 0.628468.
Train: 2018-08-02T11:58:03.983896: step 13405, loss 0.636078.
Train: 2018-08-02T11:58:04.202596: step 13406, loss 0.630525.
Train: 2018-08-02T11:58:04.436916: step 13407, loss 0.54567.
Train: 2018-08-02T11:58:04.655621: step 13408, loss 0.502785.
Train: 2018-08-02T11:58:04.874314: step 13409, loss 0.595144.
Train: 2018-08-02T11:58:05.093018: step 13410, loss 0.578641.
Test: 2018-08-02T11:58:06.295830: step 13410, loss 0.546938.
Train: 2018-08-02T11:58:06.498910: step 13411, loss 0.544878.
Train: 2018-08-02T11:58:06.717633: step 13412, loss 0.503322.
Train: 2018-08-02T11:58:06.936330: step 13413, loss 0.571105.
Train: 2018-08-02T11:58:07.155006: step 13414, loss 0.612214.
Train: 2018-08-02T11:58:07.373729: step 13415, loss 0.537271.
Train: 2018-08-02T11:58:07.592427: step 13416, loss 0.537381.
Train: 2018-08-02T11:58:07.811100: step 13417, loss 0.553865.
Train: 2018-08-02T11:58:08.029826: step 13418, loss 0.579218.
Train: 2018-08-02T11:58:08.248525: step 13419, loss 0.546083.
Train: 2018-08-02T11:58:08.467228: step 13420, loss 0.562423.
Test: 2018-08-02T11:58:09.670040: step 13420, loss 0.549287.
Train: 2018-08-02T11:58:09.888743: step 13421, loss 0.562382.
Train: 2018-08-02T11:58:10.107439: step 13422, loss 0.611783.
Train: 2018-08-02T11:58:10.326163: step 13423, loss 0.50506.
Train: 2018-08-02T11:58:10.544860: step 13424, loss 0.521351.
Train: 2018-08-02T11:58:10.763559: step 13425, loss 0.58706.
Train: 2018-08-02T11:58:10.982235: step 13426, loss 0.521445.
Train: 2018-08-02T11:58:11.216579: step 13427, loss 0.554445.
Train: 2018-08-02T11:58:11.435283: step 13428, loss 0.479937.
Train: 2018-08-02T11:58:11.653983: step 13429, loss 0.562459.
Train: 2018-08-02T11:58:11.872677: step 13430, loss 0.521237.
Test: 2018-08-02T11:58:13.075493: step 13430, loss 0.547802.
Train: 2018-08-02T11:58:13.294217: step 13431, loss 0.562338.
Train: 2018-08-02T11:58:13.528544: step 13432, loss 0.512566.
Train: 2018-08-02T11:58:13.747236: step 13433, loss 0.545847.
Train: 2018-08-02T11:58:13.965909: step 13434, loss 0.529079.
Train: 2018-08-02T11:58:14.184610: step 13435, loss 0.554146.
Train: 2018-08-02T11:58:14.418955: step 13436, loss 0.646679.
Train: 2018-08-02T11:58:14.637652: step 13437, loss 0.494857.
Train: 2018-08-02T11:58:14.856351: step 13438, loss 0.629686.
Train: 2018-08-02T11:58:15.043808: step 13439, loss 0.562484.
Train: 2018-08-02T11:58:15.262506: step 13440, loss 0.520268.
Test: 2018-08-02T11:58:16.449703: step 13440, loss 0.548033.
Train: 2018-08-02T11:58:16.699675: step 13441, loss 0.545499.
Train: 2018-08-02T11:58:16.918345: step 13442, loss 0.629825.
Train: 2018-08-02T11:58:17.137072: step 13443, loss 0.570971.
Train: 2018-08-02T11:58:17.355767: step 13444, loss 0.494557.
Train: 2018-08-02T11:58:17.574439: step 13445, loss 0.613128.
Train: 2018-08-02T11:58:17.808786: step 13446, loss 0.503202.
Train: 2018-08-02T11:58:18.027490: step 13447, loss 0.57915.
Train: 2018-08-02T11:58:18.246157: step 13448, loss 0.528086.
Train: 2018-08-02T11:58:18.464881: step 13449, loss 0.545812.
Train: 2018-08-02T11:58:18.683586: step 13450, loss 0.528416.
Test: 2018-08-02T11:58:19.902020: step 13450, loss 0.547071.
Train: 2018-08-02T11:58:20.105123: step 13451, loss 0.579723.
Train: 2018-08-02T11:58:20.323826: step 13452, loss 0.613334.
Train: 2018-08-02T11:58:20.558117: step 13453, loss 0.612938.
Train: 2018-08-02T11:58:20.776846: step 13454, loss 0.562308.
Train: 2018-08-02T11:58:21.011134: step 13455, loss 0.46983.
Train: 2018-08-02T11:58:21.229860: step 13456, loss 0.612922.
Train: 2018-08-02T11:58:21.448557: step 13457, loss 0.537306.
Train: 2018-08-02T11:58:21.667231: step 13458, loss 0.545767.
Train: 2018-08-02T11:58:21.885929: step 13459, loss 0.562677.
Train: 2018-08-02T11:58:22.120281: step 13460, loss 0.537385.
Test: 2018-08-02T11:58:23.307471: step 13460, loss 0.549207.
Train: 2018-08-02T11:58:23.526173: step 13461, loss 0.612486.
Train: 2018-08-02T11:58:23.744901: step 13462, loss 0.562624.
Train: 2018-08-02T11:58:23.963598: step 13463, loss 0.637601.
Train: 2018-08-02T11:58:24.197920: step 13464, loss 0.562523.
Train: 2018-08-02T11:58:24.432235: step 13465, loss 0.52103.
Train: 2018-08-02T11:58:24.650933: step 13466, loss 0.554241.
Train: 2018-08-02T11:58:24.869606: step 13467, loss 0.603801.
Train: 2018-08-02T11:58:25.088330: step 13468, loss 0.554265.
Train: 2018-08-02T11:58:25.307035: step 13469, loss 0.587206.
Train: 2018-08-02T11:58:25.541348: step 13470, loss 0.464067.
Test: 2018-08-02T11:58:26.744167: step 13470, loss 0.548092.
Train: 2018-08-02T11:58:26.962866: step 13471, loss 0.587172.
Train: 2018-08-02T11:58:27.181596: step 13472, loss 0.521545.
Train: 2018-08-02T11:58:27.400295: step 13473, loss 0.54614.
Train: 2018-08-02T11:58:27.634584: step 13474, loss 0.61182.
Train: 2018-08-02T11:58:27.853307: step 13475, loss 0.562552.
Train: 2018-08-02T11:58:28.071983: step 13476, loss 0.587172.
Train: 2018-08-02T11:58:28.290680: step 13477, loss 0.611753.
Train: 2018-08-02T11:58:28.509409: step 13478, loss 0.497118.
Train: 2018-08-02T11:58:28.728102: step 13479, loss 0.570766.
Train: 2018-08-02T11:58:28.946801: step 13480, loss 0.61165.
Test: 2018-08-02T11:58:30.133999: step 13480, loss 0.550474.
Train: 2018-08-02T11:58:30.352697: step 13481, loss 0.546277.
Train: 2018-08-02T11:58:30.587018: step 13482, loss 0.570773.
Train: 2018-08-02T11:58:30.821339: step 13483, loss 0.530036.
Train: 2018-08-02T11:58:31.040063: step 13484, loss 0.619653.
Train: 2018-08-02T11:58:31.258765: step 13485, loss 0.497572.
Train: 2018-08-02T11:58:31.493086: step 13486, loss 0.562646.
Train: 2018-08-02T11:58:31.711785: step 13487, loss 0.530102.
Train: 2018-08-02T11:58:31.977317: step 13488, loss 0.562636.
Train: 2018-08-02T11:58:32.196046: step 13489, loss 0.562628.
Train: 2018-08-02T11:58:32.414745: step 13490, loss 0.595233.
Test: 2018-08-02T11:58:33.617558: step 13490, loss 0.548633.
Train: 2018-08-02T11:58:33.820660: step 13491, loss 0.530014.
Train: 2018-08-02T11:58:34.039334: step 13492, loss 0.538146.
Train: 2018-08-02T11:58:34.258059: step 13493, loss 0.595266.
Train: 2018-08-02T11:58:34.476757: step 13494, loss 0.603439.
Train: 2018-08-02T11:58:34.695430: step 13495, loss 0.562609.
Train: 2018-08-02T11:58:34.914161: step 13496, loss 0.521829.
Train: 2018-08-02T11:58:35.132861: step 13497, loss 0.505493.
Train: 2018-08-02T11:58:35.367173: step 13498, loss 0.529909.
Train: 2018-08-02T11:58:35.601469: step 13499, loss 0.538011.
Train: 2018-08-02T11:58:35.820193: step 13500, loss 0.488691.
Test: 2018-08-02T11:58:37.007389: step 13500, loss 0.548956.
Train: 2018-08-02T11:58:37.850966: step 13501, loss 0.611937.
Train: 2018-08-02T11:58:38.069666: step 13502, loss 0.554249.
Train: 2018-08-02T11:58:38.288368: step 13503, loss 0.587296.
Train: 2018-08-02T11:58:38.507072: step 13504, loss 0.595598.
Train: 2018-08-02T11:58:38.741383: step 13505, loss 0.637026.
Train: 2018-08-02T11:58:38.960057: step 13506, loss 0.537663.
Train: 2018-08-02T11:58:39.178781: step 13507, loss 0.529418.
Train: 2018-08-02T11:58:39.413107: step 13508, loss 0.645165.
Train: 2018-08-02T11:58:39.647424: step 13509, loss 0.645035.
Train: 2018-08-02T11:58:39.866096: step 13510, loss 0.513176.
Test: 2018-08-02T11:58:41.053316: step 13510, loss 0.548273.
Train: 2018-08-02T11:58:41.272040: step 13511, loss 0.554341.
Train: 2018-08-02T11:58:41.490738: step 13512, loss 0.546175.
Train: 2018-08-02T11:58:41.709413: step 13513, loss 0.677184.
Train: 2018-08-02T11:58:41.928140: step 13514, loss 0.595246.
Train: 2018-08-02T11:58:42.146812: step 13515, loss 0.570784.
Train: 2018-08-02T11:58:42.365509: step 13516, loss 0.578896.
Train: 2018-08-02T11:58:42.599860: step 13517, loss 0.562745.
Train: 2018-08-02T11:58:42.818528: step 13518, loss 0.586918.
Train: 2018-08-02T11:58:43.052873: step 13519, loss 0.618953.
Train: 2018-08-02T11:58:43.271546: step 13520, loss 0.562892.
Test: 2018-08-02T11:58:44.474390: step 13520, loss 0.548929.
Train: 2018-08-02T11:58:44.677498: step 13521, loss 0.531126.
Train: 2018-08-02T11:58:44.896193: step 13522, loss 0.59473.
Train: 2018-08-02T11:58:45.130490: step 13523, loss 0.563032.
Train: 2018-08-02T11:58:45.349216: step 13524, loss 0.602548.
Train: 2018-08-02T11:58:45.567910: step 13525, loss 0.539505.
Train: 2018-08-02T11:58:45.786614: step 13526, loss 0.539583.
Train: 2018-08-02T11:58:46.005282: step 13527, loss 0.563174.
Train: 2018-08-02T11:58:46.239612: step 13528, loss 0.563187.
Train: 2018-08-02T11:58:46.458326: step 13529, loss 0.523995.
Train: 2018-08-02T11:58:46.676999: step 13530, loss 0.571029.
Test: 2018-08-02T11:58:47.864221: step 13530, loss 0.550606.
Train: 2018-08-02T11:58:48.082921: step 13531, loss 0.555324.
Train: 2018-08-02T11:58:48.317241: step 13532, loss 0.618154.
Train: 2018-08-02T11:58:48.535970: step 13533, loss 0.547453.
Train: 2018-08-02T11:58:48.770291: step 13534, loss 0.594585.
Train: 2018-08-02T11:58:48.988989: step 13535, loss 0.523888.
Train: 2018-08-02T11:58:49.207657: step 13536, loss 0.531703.
Train: 2018-08-02T11:58:49.426381: step 13537, loss 0.563119.
Train: 2018-08-02T11:58:49.660701: step 13538, loss 0.523658.
Train: 2018-08-02T11:58:49.879374: step 13539, loss 0.515612.
Train: 2018-08-02T11:58:50.098098: step 13540, loss 0.65819.
Test: 2018-08-02T11:58:51.285295: step 13540, loss 0.549528.
Train: 2018-08-02T11:58:51.488407: step 13541, loss 0.555033.
Train: 2018-08-02T11:58:51.722724: step 13542, loss 0.562954.
Train: 2018-08-02T11:58:51.925769: step 13543, loss 0.570897.
Train: 2018-08-02T11:58:52.144469: step 13544, loss 0.562918.
Train: 2018-08-02T11:58:52.363199: step 13545, loss 0.507044.
Train: 2018-08-02T11:58:52.581898: step 13546, loss 0.506882.
Train: 2018-08-02T11:58:52.816241: step 13547, loss 0.538745.
Train: 2018-08-02T11:58:53.034917: step 13548, loss 0.53861.
Train: 2018-08-02T11:58:53.253593: step 13549, loss 0.595058.
Train: 2018-08-02T11:58:53.472283: step 13550, loss 0.489728.
Test: 2018-08-02T11:58:54.659505: step 13550, loss 0.549796.
Train: 2018-08-02T11:58:54.878203: step 13551, loss 0.578919.
Train: 2018-08-02T11:58:55.096903: step 13552, loss 0.603435.
Train: 2018-08-02T11:58:55.315603: step 13553, loss 0.529849.
Train: 2018-08-02T11:58:55.534326: step 13554, loss 0.472311.
Train: 2018-08-02T11:58:55.753024: step 13555, loss 0.570761.
Train: 2018-08-02T11:58:55.971699: step 13556, loss 0.620356.
Train: 2018-08-02T11:58:56.190422: step 13557, loss 0.446527.
Train: 2018-08-02T11:58:56.409126: step 13558, loss 0.495933.
Train: 2018-08-02T11:58:56.627824: step 13559, loss 0.495581.
Train: 2018-08-02T11:58:56.846493: step 13560, loss 0.587584.
Test: 2018-08-02T11:58:58.033715: step 13560, loss 0.547501.
Train: 2018-08-02T11:58:58.236823: step 13561, loss 0.503313.
Train: 2018-08-02T11:58:58.455522: step 13562, loss 0.553861.
Train: 2018-08-02T11:58:58.674191: step 13563, loss 0.502726.
Train: 2018-08-02T11:58:58.892889: step 13564, loss 0.639375.
Train: 2018-08-02T11:58:59.111588: step 13565, loss 0.648149.
Train: 2018-08-02T11:58:59.330287: step 13566, loss 0.536575.
Train: 2018-08-02T11:58:59.549015: step 13567, loss 0.51078.
Train: 2018-08-02T11:58:59.767715: step 13568, loss 0.613908.
Train: 2018-08-02T11:58:59.986407: step 13569, loss 0.56235.
Train: 2018-08-02T11:59:00.205112: step 13570, loss 0.510733.
Test: 2018-08-02T11:59:01.392303: step 13570, loss 0.550337.
Train: 2018-08-02T11:59:01.595381: step 13571, loss 0.553746.
Train: 2018-08-02T11:59:01.829732: step 13572, loss 0.55373.
Train: 2018-08-02T11:59:02.048426: step 13573, loss 0.545133.
Train: 2018-08-02T11:59:02.267130: step 13574, loss 0.614027.
Train: 2018-08-02T11:59:02.485798: step 13575, loss 0.605358.
Train: 2018-08-02T11:59:02.704500: step 13576, loss 0.570921.
Train: 2018-08-02T11:59:02.938843: step 13577, loss 0.52805.
Train: 2018-08-02T11:59:03.157546: step 13578, loss 0.60513.
Train: 2018-08-02T11:59:03.376244: step 13579, loss 0.613573.
Train: 2018-08-02T11:59:03.594944: step 13580, loss 0.528296.
Test: 2018-08-02T11:59:04.797756: step 13580, loss 0.546995.
Train: 2018-08-02T11:59:05.016480: step 13581, loss 0.58782.
Train: 2018-08-02T11:59:05.235184: step 13582, loss 0.545424.
Train: 2018-08-02T11:59:05.453882: step 13583, loss 0.604591.
Train: 2018-08-02T11:59:05.672581: step 13584, loss 0.545539.
Train: 2018-08-02T11:59:05.891249: step 13585, loss 0.495221.
Train: 2018-08-02T11:59:06.109974: step 13586, loss 0.579168.
Train: 2018-08-02T11:59:06.328678: step 13587, loss 0.587521.
Train: 2018-08-02T11:59:06.547376: step 13588, loss 0.57077.
Train: 2018-08-02T11:59:06.766077: step 13589, loss 0.604116.
Train: 2018-08-02T11:59:06.953502: step 13590, loss 0.580183.
Test: 2018-08-02T11:59:08.156345: step 13590, loss 0.547288.
Train: 2018-08-02T11:59:08.375068: step 13591, loss 0.54589.
Train: 2018-08-02T11:59:08.593743: step 13592, loss 0.53768.
Train: 2018-08-02T11:59:08.812471: step 13593, loss 0.678065.
Train: 2018-08-02T11:59:09.031170: step 13594, loss 0.537872.
Train: 2018-08-02T11:59:09.249869: step 13595, loss 0.578959.
Train: 2018-08-02T11:59:09.468568: step 13596, loss 0.562599.
Train: 2018-08-02T11:59:09.702857: step 13597, loss 0.595216.
Train: 2018-08-02T11:59:09.905934: step 13598, loss 0.570787.
Train: 2018-08-02T11:59:10.124661: step 13599, loss 0.489855.
Train: 2018-08-02T11:59:10.343364: step 13600, loss 0.50613.
Test: 2018-08-02T11:59:11.530555: step 13600, loss 0.550146.
Train: 2018-08-02T11:59:12.358517: step 13601, loss 0.546549.
Train: 2018-08-02T11:59:12.577185: step 13602, loss 0.635517.
Train: 2018-08-02T11:59:12.811535: step 13603, loss 0.578888.
Train: 2018-08-02T11:59:13.030235: step 13604, loss 0.578884.
Train: 2018-08-02T11:59:13.248927: step 13605, loss 0.546644.
Train: 2018-08-02T11:59:13.467635: step 13606, loss 0.562773.
Train: 2018-08-02T11:59:13.686299: step 13607, loss 0.586921.
Train: 2018-08-02T11:59:13.920645: step 13608, loss 0.546725.
Train: 2018-08-02T11:59:14.139319: step 13609, loss 0.578871.
Train: 2018-08-02T11:59:14.358048: step 13610, loss 0.578869.
Test: 2018-08-02T11:59:15.545239: step 13610, loss 0.549812.
Train: 2018-08-02T11:59:15.748342: step 13611, loss 0.594902.
Train: 2018-08-02T11:59:15.982664: step 13612, loss 0.594875.
Train: 2018-08-02T11:59:16.201366: step 13613, loss 0.546904.
Train: 2018-08-02T11:59:16.420036: step 13614, loss 0.546945.
Train: 2018-08-02T11:59:16.638760: step 13615, loss 0.57886.
Train: 2018-08-02T11:59:16.857431: step 13616, loss 0.570894.
Train: 2018-08-02T11:59:17.091752: step 13617, loss 0.586817.
Train: 2018-08-02T11:59:17.310476: step 13618, loss 0.547064.
Train: 2018-08-02T11:59:17.529174: step 13619, loss 0.547084.
Train: 2018-08-02T11:59:17.747880: step 13620, loss 0.55503.
Test: 2018-08-02T11:59:18.935070: step 13620, loss 0.549336.
Train: 2018-08-02T11:59:19.153769: step 13621, loss 0.531191.
Train: 2018-08-02T11:59:19.372498: step 13622, loss 0.531144.
Train: 2018-08-02T11:59:19.591166: step 13623, loss 0.507164.
Train: 2018-08-02T11:59:19.809865: step 13624, loss 0.578862.
Train: 2018-08-02T11:59:20.028595: step 13625, loss 0.634917.
Train: 2018-08-02T11:59:20.247294: step 13626, loss 0.602903.
Train: 2018-08-02T11:59:20.481614: step 13627, loss 0.538813.
Train: 2018-08-02T11:59:20.700306: step 13628, loss 0.546811.
Train: 2018-08-02T11:59:20.919011: step 13629, loss 0.578867.
Train: 2018-08-02T11:59:21.137711: step 13630, loss 0.458518.
Test: 2018-08-02T11:59:22.340522: step 13630, loss 0.550089.
Train: 2018-08-02T11:59:22.543602: step 13631, loss 0.627144.
Train: 2018-08-02T11:59:22.777952: step 13632, loss 0.578877.
Train: 2018-08-02T11:59:22.996649: step 13633, loss 0.57888.
Train: 2018-08-02T11:59:23.215348: step 13634, loss 0.530505.
Train: 2018-08-02T11:59:23.434017: step 13635, loss 0.506244.
Train: 2018-08-02T11:59:23.668339: step 13636, loss 0.530359.
Train: 2018-08-02T11:59:23.887061: step 13637, loss 0.684338.
Train: 2018-08-02T11:59:24.105787: step 13638, loss 0.497806.
Train: 2018-08-02T11:59:24.324434: step 13639, loss 0.587027.
Train: 2018-08-02T11:59:24.543157: step 13640, loss 0.595161.
Test: 2018-08-02T11:59:25.730354: step 13640, loss 0.547783.
Train: 2018-08-02T11:59:25.964699: step 13641, loss 0.570785.
Train: 2018-08-02T11:59:26.167782: step 13642, loss 0.570786.
Train: 2018-08-02T11:59:26.386451: step 13643, loss 0.489585.
Train: 2018-08-02T11:59:26.605150: step 13644, loss 0.49761.
Train: 2018-08-02T11:59:26.823849: step 13645, loss 0.603379.
Train: 2018-08-02T11:59:27.058199: step 13646, loss 0.497318.
Train: 2018-08-02T11:59:27.276891: step 13647, loss 0.578948.
Train: 2018-08-02T11:59:27.495590: step 13648, loss 0.546161.
Train: 2018-08-02T11:59:27.714289: step 13649, loss 0.587191.
Train: 2018-08-02T11:59:27.932994: step 13650, loss 0.59545.
Test: 2018-08-02T11:59:29.120185: step 13650, loss 0.549917.
Train: 2018-08-02T11:59:29.338910: step 13651, loss 0.546067.
Train: 2018-08-02T11:59:29.557613: step 13652, loss 0.570758.
Train: 2018-08-02T11:59:29.776307: step 13653, loss 0.587237.
Train: 2018-08-02T11:59:29.995010: step 13654, loss 0.562517.
Train: 2018-08-02T11:59:30.213712: step 13655, loss 0.578993.
Train: 2018-08-02T11:59:30.448027: step 13656, loss 0.546058.
Train: 2018-08-02T11:59:30.682336: step 13657, loss 0.554297.
Train: 2018-08-02T11:59:30.901049: step 13658, loss 0.455524.
Train: 2018-08-02T11:59:31.119751: step 13659, loss 0.562502.
Train: 2018-08-02T11:59:31.338448: step 13660, loss 0.529449.
Test: 2018-08-02T11:59:32.541260: step 13660, loss 0.548639.
Train: 2018-08-02T11:59:32.744367: step 13661, loss 0.504491.
Train: 2018-08-02T11:59:32.963061: step 13662, loss 0.612285.
Train: 2018-08-02T11:59:33.181759: step 13663, loss 0.529119.
Train: 2018-08-02T11:59:33.400464: step 13664, loss 0.545697.
Train: 2018-08-02T11:59:33.634753: step 13665, loss 0.528949.
Train: 2018-08-02T11:59:33.853452: step 13666, loss 0.570821.
Train: 2018-08-02T11:59:34.072181: step 13667, loss 0.621267.
Train: 2018-08-02T11:59:34.290876: step 13668, loss 0.54533.
Train: 2018-08-02T11:59:34.509580: step 13669, loss 0.545715.
Train: 2018-08-02T11:59:34.743899: step 13670, loss 0.562344.
Test: 2018-08-02T11:59:35.915469: step 13670, loss 0.549418.
Train: 2018-08-02T11:59:36.118578: step 13671, loss 0.536914.
Train: 2018-08-02T11:59:36.337270: step 13672, loss 0.570264.
Train: 2018-08-02T11:59:36.555975: step 13673, loss 0.503013.
Train: 2018-08-02T11:59:36.774674: step 13674, loss 0.605522.
Train: 2018-08-02T11:59:37.008964: step 13675, loss 0.511011.
Train: 2018-08-02T11:59:37.227687: step 13676, loss 0.545019.
Train: 2018-08-02T11:59:37.446392: step 13677, loss 0.656818.
Train: 2018-08-02T11:59:37.680681: step 13678, loss 0.59683.
Train: 2018-08-02T11:59:37.915031: step 13679, loss 0.579534.
Train: 2018-08-02T11:59:38.133726: step 13680, loss 0.570854.
Test: 2018-08-02T11:59:39.320921: step 13680, loss 0.549297.
Train: 2018-08-02T11:59:39.539651: step 13681, loss 0.579355.
Train: 2018-08-02T11:59:39.742729: step 13682, loss 0.587546.
Train: 2018-08-02T11:59:39.961427: step 13683, loss 0.537432.
Train: 2018-08-02T11:59:40.195718: step 13684, loss 0.587379.
Train: 2018-08-02T11:59:40.414416: step 13685, loss 0.545895.
Train: 2018-08-02T11:59:40.633113: step 13686, loss 0.537677.
Train: 2018-08-02T11:59:40.851813: step 13687, loss 0.653333.
Train: 2018-08-02T11:59:41.070541: step 13688, loss 0.513143.
Train: 2018-08-02T11:59:41.304857: step 13689, loss 0.644693.
Train: 2018-08-02T11:59:41.523530: step 13690, loss 0.546208.
Test: 2018-08-02T11:59:42.710753: step 13690, loss 0.5486.
Train: 2018-08-02T11:59:42.991970: step 13691, loss 0.529957.
Train: 2018-08-02T11:59:43.210666: step 13692, loss 0.464859.
Train: 2018-08-02T11:59:43.429335: step 13693, loss 0.554475.
Train: 2018-08-02T11:59:43.648061: step 13694, loss 0.587086.
Train: 2018-08-02T11:59:43.866732: step 13695, loss 0.595235.
Train: 2018-08-02T11:59:44.101077: step 13696, loss 0.627808.
Train: 2018-08-02T11:59:44.304160: step 13697, loss 0.53013.
Train: 2018-08-02T11:59:44.522859: step 13698, loss 0.481451.
Train: 2018-08-02T11:59:44.741553: step 13699, loss 0.660181.
Train: 2018-08-02T11:59:44.960256: step 13700, loss 0.611368.
Test: 2018-08-02T11:59:46.147449: step 13700, loss 0.548245.
Train: 2018-08-02T11:59:47.053485: step 13701, loss 0.53031.
Train: 2018-08-02T11:59:47.272209: step 13702, loss 0.570804.
Train: 2018-08-02T11:59:47.490885: step 13703, loss 0.603115.
Train: 2018-08-02T11:59:47.725230: step 13704, loss 0.546639.
Train: 2018-08-02T11:59:47.959525: step 13705, loss 0.54668.
Train: 2018-08-02T11:59:48.193844: step 13706, loss 0.611042.
Train: 2018-08-02T11:59:48.412569: step 13707, loss 0.570841.
Train: 2018-08-02T11:59:48.631242: step 13708, loss 0.506734.
Train: 2018-08-02T11:59:48.849940: step 13709, loss 0.514762.
Train: 2018-08-02T11:59:49.084262: step 13710, loss 0.562827.
Test: 2018-08-02T11:59:50.271482: step 13710, loss 0.548854.
Train: 2018-08-02T11:59:50.474584: step 13711, loss 0.610978.
Train: 2018-08-02T11:59:50.708911: step 13712, loss 0.586895.
Train: 2018-08-02T11:59:50.927603: step 13713, loss 0.578868.
Train: 2018-08-02T11:59:51.161930: step 13714, loss 0.610926.
Train: 2018-08-02T11:59:51.380622: step 13715, loss 0.546858.
Train: 2018-08-02T11:59:51.599297: step 13716, loss 0.530901.
Train: 2018-08-02T11:59:51.818026: step 13717, loss 0.578865.
Train: 2018-08-02T11:59:52.052315: step 13718, loss 0.594846.
Train: 2018-08-02T11:59:52.255429: step 13719, loss 0.530966.
Train: 2018-08-02T11:59:52.489739: step 13720, loss 0.586839.
Test: 2018-08-02T11:59:53.676934: step 13720, loss 0.550312.
Train: 2018-08-02T11:59:53.880013: step 13721, loss 0.538967.
Train: 2018-08-02T11:59:54.098736: step 13722, loss 0.530967.
Train: 2018-08-02T11:59:54.317409: step 13723, loss 0.554888.
Train: 2018-08-02T11:59:54.551731: step 13724, loss 0.482864.
Train: 2018-08-02T11:59:54.770428: step 13725, loss 0.546773.
Train: 2018-08-02T11:59:54.989128: step 13726, loss 0.562818.
Train: 2018-08-02T11:59:55.223478: step 13727, loss 0.530479.
Train: 2018-08-02T11:59:55.457793: step 13728, loss 0.546325.
Train: 2018-08-02T11:59:55.707735: step 13729, loss 0.562798.
Train: 2018-08-02T11:59:55.910786: step 13730, loss 0.529856.
Test: 2018-08-02T11:59:57.098009: step 13730, loss 0.547332.
Train: 2018-08-02T11:59:57.316733: step 13731, loss 0.611816.
Train: 2018-08-02T11:59:57.551028: step 13732, loss 0.587334.
Train: 2018-08-02T11:59:57.769752: step 13733, loss 0.488616.
Train: 2018-08-02T11:59:57.988456: step 13734, loss 0.521395.
Train: 2018-08-02T11:59:58.207155: step 13735, loss 0.54608.
Train: 2018-08-02T11:59:58.425824: step 13736, loss 0.53716.
Train: 2018-08-02T11:59:58.644553: step 13737, loss 0.587951.
Train: 2018-08-02T11:59:58.878867: step 13738, loss 0.596125.
Train: 2018-08-02T11:59:59.113163: step 13739, loss 0.595355.
Train: 2018-08-02T11:59:59.331862: step 13740, loss 0.579507.
Test: 2018-08-02T12:00:00.534704: step 13740, loss 0.549821.
Train: 2018-08-02T12:00:00.706539: step 13741, loss 0.580114.
Train: 2018-08-02T12:00:00.925268: step 13742, loss 0.528985.
Train: 2018-08-02T12:00:01.159557: step 13743, loss 0.554156.
Train: 2018-08-02T12:00:01.378283: step 13744, loss 0.570743.
Train: 2018-08-02T12:00:01.612577: step 13745, loss 0.562423.
Train: 2018-08-02T12:00:01.815680: step 13746, loss 0.570645.
Train: 2018-08-02T12:00:02.049975: step 13747, loss 0.53751.
Train: 2018-08-02T12:00:02.268675: step 13748, loss 0.59581.
Train: 2018-08-02T12:00:02.502995: step 13749, loss 0.562592.
Train: 2018-08-02T12:00:02.721723: step 13750, loss 0.570813.
Test: 2018-08-02T12:00:03.908914: step 13750, loss 0.547878.
Train: 2018-08-02T12:00:04.143265: step 13751, loss 0.545864.
Train: 2018-08-02T12:00:04.377593: step 13752, loss 0.545897.
Train: 2018-08-02T12:00:04.611874: step 13753, loss 0.570742.
Train: 2018-08-02T12:00:04.830574: step 13754, loss 0.554206.
Train: 2018-08-02T12:00:05.049303: step 13755, loss 0.529491.
Train: 2018-08-02T12:00:05.283593: step 13756, loss 0.620386.
Train: 2018-08-02T12:00:05.517913: step 13757, loss 0.545981.
Train: 2018-08-02T12:00:05.752233: step 13758, loss 0.579017.
Train: 2018-08-02T12:00:05.970930: step 13759, loss 0.611952.
Train: 2018-08-02T12:00:06.189643: step 13760, loss 0.504969.
Test: 2018-08-02T12:00:07.392473: step 13760, loss 0.548246.
Train: 2018-08-02T12:00:07.611171: step 13761, loss 0.537881.
Train: 2018-08-02T12:00:07.845523: step 13762, loss 0.578978.
Train: 2018-08-02T12:00:08.079843: step 13763, loss 0.57076.
Train: 2018-08-02T12:00:08.314157: step 13764, loss 0.587183.
Train: 2018-08-02T12:00:08.532831: step 13765, loss 0.529748.
Train: 2018-08-02T12:00:08.751530: step 13766, loss 0.521556.
Train: 2018-08-02T12:00:08.985850: step 13767, loss 0.661022.
Train: 2018-08-02T12:00:09.220170: step 13768, loss 0.644499.
Train: 2018-08-02T12:00:09.454517: step 13769, loss 0.546269.
Train: 2018-08-02T12:00:09.688842: step 13770, loss 0.562632.
Test: 2018-08-02T12:00:10.876033: step 13770, loss 0.549269.
Train: 2018-08-02T12:00:11.110384: step 13771, loss 0.530145.
Train: 2018-08-02T12:00:11.344672: step 13772, loss 0.505852.
Train: 2018-08-02T12:00:11.563397: step 13773, loss 0.562671.
Train: 2018-08-02T12:00:11.766449: step 13774, loss 0.587024.
Train: 2018-08-02T12:00:11.985147: step 13775, loss 0.497764.
Train: 2018-08-02T12:00:12.235089: step 13776, loss 0.58703.
Train: 2018-08-02T12:00:12.438196: step 13777, loss 0.611411.
Train: 2018-08-02T12:00:12.672512: step 13778, loss 0.627626.
Train: 2018-08-02T12:00:12.906838: step 13779, loss 0.587003.
Train: 2018-08-02T12:00:13.141128: step 13780, loss 0.522287.
Test: 2018-08-02T12:00:14.328349: step 13780, loss 0.547569.
Train: 2018-08-02T12:00:14.547074: step 13781, loss 0.554655.
Train: 2018-08-02T12:00:14.781395: step 13782, loss 0.619238.
Train: 2018-08-02T12:00:15.000066: step 13783, loss 0.611101.
Train: 2018-08-02T12:00:15.234411: step 13784, loss 0.506561.
Train: 2018-08-02T12:00:15.453115: step 13785, loss 0.52269.
Train: 2018-08-02T12:00:15.687431: step 13786, loss 0.514669.
Train: 2018-08-02T12:00:15.890508: step 13787, loss 0.570838.
Train: 2018-08-02T12:00:16.109208: step 13788, loss 0.570832.
Train: 2018-08-02T12:00:16.343532: step 13789, loss 0.578874.
Train: 2018-08-02T12:00:16.577824: step 13790, loss 0.538635.
Test: 2018-08-02T12:00:17.780665: step 13790, loss 0.548387.
Train: 2018-08-02T12:00:17.999400: step 13791, loss 0.619155.
Train: 2018-08-02T12:00:18.233710: step 13792, loss 0.594983.
Train: 2018-08-02T12:00:18.452384: step 13793, loss 0.586919.
Train: 2018-08-02T12:00:18.655490: step 13794, loss 0.522641.
Train: 2018-08-02T12:00:18.889812: step 13795, loss 0.635084.
Train: 2018-08-02T12:00:19.124129: step 13796, loss 0.506716.
Train: 2018-08-02T12:00:19.342801: step 13797, loss 0.634968.
Train: 2018-08-02T12:00:19.577145: step 13798, loss 0.538859.
Train: 2018-08-02T12:00:19.811443: step 13799, loss 0.530902.
Train: 2018-08-02T12:00:20.045786: step 13800, loss 0.514921.
Test: 2018-08-02T12:00:21.248603: step 13800, loss 0.549313.
Train: 2018-08-02T12:00:22.123424: step 13801, loss 0.538857.
Train: 2018-08-02T12:00:22.342127: step 13802, loss 0.554822.
Train: 2018-08-02T12:00:22.576442: step 13803, loss 0.522671.
Train: 2018-08-02T12:00:22.810763: step 13804, loss 0.530588.
Train: 2018-08-02T12:00:23.029466: step 13805, loss 0.64345.
Train: 2018-08-02T12:00:23.263756: step 13806, loss 0.586964.
Train: 2018-08-02T12:00:23.482487: step 13807, loss 0.538477.
Train: 2018-08-02T12:00:23.716800: step 13808, loss 0.5708.
Train: 2018-08-02T12:00:23.951097: step 13809, loss 0.538423.
Train: 2018-08-02T12:00:24.169794: step 13810, loss 0.578901.
Test: 2018-08-02T12:00:25.357017: step 13810, loss 0.54913.
Train: 2018-08-02T12:00:25.591392: step 13811, loss 0.554562.
Train: 2018-08-02T12:00:25.810060: step 13812, loss 0.489578.
Train: 2018-08-02T12:00:26.044380: step 13813, loss 0.57892.
Train: 2018-08-02T12:00:26.263079: step 13814, loss 0.562621.
Train: 2018-08-02T12:00:26.481780: step 13815, loss 0.652557.
Train: 2018-08-02T12:00:26.716098: step 13816, loss 0.529931.
Train: 2018-08-02T12:00:26.934804: step 13817, loss 0.538108.
Train: 2018-08-02T12:00:27.153502: step 13818, loss 0.56259.
Train: 2018-08-02T12:00:27.387791: step 13819, loss 0.538063.
Train: 2018-08-02T12:00:27.637769: step 13820, loss 0.546169.
Test: 2018-08-02T12:00:28.824954: step 13820, loss 0.548121.
Train: 2018-08-02T12:00:29.028032: step 13821, loss 0.57897.
Train: 2018-08-02T12:00:29.246761: step 13822, loss 0.472328.
Train: 2018-08-02T12:00:29.465454: step 13823, loss 0.578954.
Train: 2018-08-02T12:00:29.699776: step 13824, loss 0.562484.
Train: 2018-08-02T12:00:29.949721: step 13825, loss 0.661728.
Train: 2018-08-02T12:00:30.184041: step 13826, loss 0.562536.
Train: 2018-08-02T12:00:30.418362: step 13827, loss 0.521287.
Train: 2018-08-02T12:00:30.652651: step 13828, loss 0.521201.
Train: 2018-08-02T12:00:30.902602: step 13829, loss 0.546011.
Train: 2018-08-02T12:00:31.105704: step 13830, loss 0.562427.
Test: 2018-08-02T12:00:32.308513: step 13830, loss 0.548075.
Train: 2018-08-02T12:00:32.511591: step 13831, loss 0.537549.
Train: 2018-08-02T12:00:32.792806: step 13832, loss 0.587417.
Train: 2018-08-02T12:00:33.011474: step 13833, loss 0.587473.
Train: 2018-08-02T12:00:33.230198: step 13834, loss 0.529181.
Train: 2018-08-02T12:00:33.464493: step 13835, loss 0.579257.
Train: 2018-08-02T12:00:33.667569: step 13836, loss 0.562355.
Train: 2018-08-02T12:00:33.901921: step 13837, loss 0.529259.
Train: 2018-08-02T12:00:34.136239: step 13838, loss 0.637318.
Train: 2018-08-02T12:00:34.386183: step 13839, loss 0.520925.
Train: 2018-08-02T12:00:34.620508: step 13840, loss 0.554246.
Test: 2018-08-02T12:00:35.807694: step 13840, loss 0.548809.
Train: 2018-08-02T12:00:36.026392: step 13841, loss 0.587336.
Train: 2018-08-02T12:00:36.245123: step 13842, loss 0.521141.
Train: 2018-08-02T12:00:36.479442: step 13843, loss 0.537639.
Train: 2018-08-02T12:00:36.698110: step 13844, loss 0.521049.
Train: 2018-08-02T12:00:36.932430: step 13845, loss 0.504392.
Train: 2018-08-02T12:00:37.151129: step 13846, loss 0.570757.
Train: 2018-08-02T12:00:37.385480: step 13847, loss 0.537445.
Train: 2018-08-02T12:00:37.635415: step 13848, loss 0.554082.
Train: 2018-08-02T12:00:37.854120: step 13849, loss 0.528924.
Train: 2018-08-02T12:00:38.088441: step 13850, loss 0.663063.
Test: 2018-08-02T12:00:39.302265: step 13850, loss 0.54859.
Train: 2018-08-02T12:00:39.520964: step 13851, loss 0.562367.
Train: 2018-08-02T12:00:39.739688: step 13852, loss 0.486979.
Train: 2018-08-02T12:00:39.973983: step 13853, loss 0.562413.
Train: 2018-08-02T12:00:40.192683: step 13854, loss 0.545588.
Train: 2018-08-02T12:00:40.395789: step 13855, loss 0.57919.
Train: 2018-08-02T12:00:40.630086: step 13856, loss 0.579187.
Train: 2018-08-02T12:00:40.864400: step 13857, loss 0.587627.
Train: 2018-08-02T12:00:41.083129: step 13858, loss 0.646435.
Train: 2018-08-02T12:00:41.317443: step 13859, loss 0.554021.
Train: 2018-08-02T12:00:41.551764: step 13860, loss 0.554052.
Test: 2018-08-02T12:00:42.738960: step 13860, loss 0.54753.
Train: 2018-08-02T12:00:42.973281: step 13861, loss 0.587453.
Train: 2018-08-02T12:00:43.207601: step 13862, loss 0.470875.
Train: 2018-08-02T12:00:43.426330: step 13863, loss 0.570762.
Train: 2018-08-02T12:00:43.645012: step 13864, loss 0.579078.
Train: 2018-08-02T12:00:43.879318: step 13865, loss 0.587378.
Train: 2018-08-02T12:00:44.098025: step 13866, loss 0.529272.
Train: 2018-08-02T12:00:44.332360: step 13867, loss 0.562466.
Train: 2018-08-02T12:00:44.566687: step 13868, loss 0.529328.
Train: 2018-08-02T12:00:44.800979: step 13869, loss 0.554186.
Train: 2018-08-02T12:00:45.035298: step 13870, loss 0.603902.
Test: 2018-08-02T12:00:46.222520: step 13870, loss 0.548079.
Train: 2018-08-02T12:00:46.456840: step 13871, loss 0.487966.
Train: 2018-08-02T12:00:46.675538: step 13872, loss 0.521049.
Train: 2018-08-02T12:00:46.909890: step 13873, loss 0.662013.
Train: 2018-08-02T12:00:47.144178: step 13874, loss 0.603913.
Train: 2018-08-02T12:00:47.378501: step 13875, loss 0.537656.
Train: 2018-08-02T12:00:47.628473: step 13876, loss 0.56249.
Train: 2018-08-02T12:00:47.847169: step 13877, loss 0.603787.
Train: 2018-08-02T12:00:48.081459: step 13878, loss 0.570757.
Train: 2018-08-02T12:00:48.300184: step 13879, loss 0.611895.
Train: 2018-08-02T12:00:48.534478: step 13880, loss 0.587173.
Test: 2018-08-02T12:00:49.737321: step 13880, loss 0.549487.
Train: 2018-08-02T12:00:49.956030: step 13881, loss 0.562586.
Train: 2018-08-02T12:00:50.174720: step 13882, loss 0.587089.
Train: 2018-08-02T12:00:50.393417: step 13883, loss 0.554517.
Train: 2018-08-02T12:00:50.627770: step 13884, loss 0.554565.
Train: 2018-08-02T12:00:50.846451: step 13885, loss 0.5627.
Train: 2018-08-02T12:00:51.080758: step 13886, loss 0.562724.
Train: 2018-08-02T12:00:51.299487: step 13887, loss 0.538535.
Train: 2018-08-02T12:00:51.518180: step 13888, loss 0.554693.
Train: 2018-08-02T12:00:51.752502: step 13889, loss 0.554704.
Train: 2018-08-02T12:00:51.986823: step 13890, loss 0.530543.
Test: 2018-08-02T12:00:53.174016: step 13890, loss 0.548929.
Train: 2018-08-02T12:00:53.392742: step 13891, loss 0.514403.
Train: 2018-08-02T12:00:53.580196: step 13892, loss 0.579959.
Train: 2018-08-02T12:00:53.830143: step 13893, loss 0.611227.
Train: 2018-08-02T12:00:54.064435: step 13894, loss 0.554652.
Train: 2018-08-02T12:00:54.283163: step 13895, loss 0.635457.
Train: 2018-08-02T12:00:54.517453: step 13896, loss 0.522393.
Train: 2018-08-02T12:00:54.751797: step 13897, loss 0.506275.
Train: 2018-08-02T12:00:54.986093: step 13898, loss 0.611187.
Train: 2018-08-02T12:00:55.220437: step 13899, loss 0.595034.
Train: 2018-08-02T12:00:55.454763: step 13900, loss 0.619225.
Test: 2018-08-02T12:00:56.657576: step 13900, loss 0.549134.
Train: 2018-08-02T12:00:57.516776: step 13901, loss 0.514447.
Train: 2018-08-02T12:00:57.751095: step 13902, loss 0.538629.
Train: 2018-08-02T12:00:57.969799: step 13903, loss 0.522526.
Train: 2018-08-02T12:00:58.188497: step 13904, loss 0.578879.
Train: 2018-08-02T12:00:58.438440: step 13905, loss 0.570818.
Train: 2018-08-02T12:00:58.672754: step 13906, loss 0.482087.
Train: 2018-08-02T12:00:58.891428: step 13907, loss 0.562722.
Train: 2018-08-02T12:00:59.125780: step 13908, loss 0.562698.
Train: 2018-08-02T12:00:59.344471: step 13909, loss 0.57079.
Train: 2018-08-02T12:00:59.578792: step 13910, loss 0.546407.
Test: 2018-08-02T12:01:00.765989: step 13910, loss 0.548484.
Train: 2018-08-02T12:01:00.984713: step 13911, loss 0.578919.
Train: 2018-08-02T12:01:01.203417: step 13912, loss 0.464837.
Train: 2018-08-02T12:01:01.437737: step 13913, loss 0.53807.
Train: 2018-08-02T12:01:01.672029: step 13914, loss 0.69378.
Train: 2018-08-02T12:01:01.906378: step 13915, loss 0.619979.
Train: 2018-08-02T12:01:02.140697: step 13916, loss 0.61993.
Train: 2018-08-02T12:01:02.375015: step 13917, loss 0.546235.
Train: 2018-08-02T12:01:02.609335: step 13918, loss 0.595264.
Train: 2018-08-02T12:01:02.843653: step 13919, loss 0.619659.
Train: 2018-08-02T12:01:03.077948: step 13920, loss 0.562664.
Test: 2018-08-02T12:01:04.280791: step 13920, loss 0.550661.
Train: 2018-08-02T12:01:04.499491: step 13921, loss 0.546497.
Train: 2018-08-02T12:01:04.718189: step 13922, loss 0.595051.
Train: 2018-08-02T12:01:04.936914: step 13923, loss 0.538569.
Train: 2018-08-02T12:01:05.171239: step 13924, loss 0.603023.
Train: 2018-08-02T12:01:05.405529: step 13925, loss 0.538711.
Train: 2018-08-02T12:01:05.639872: step 13926, loss 0.586882.
Train: 2018-08-02T12:01:05.858572: step 13927, loss 0.610867.
Train: 2018-08-02T12:01:06.092894: step 13928, loss 0.499039.
Train: 2018-08-02T12:01:06.327212: step 13929, loss 0.538944.
Train: 2018-08-02T12:01:06.561531: step 13930, loss 0.642706.
Test: 2018-08-02T12:01:07.748728: step 13930, loss 0.549252.
Train: 2018-08-02T12:01:07.967452: step 13931, loss 0.570844.
Train: 2018-08-02T12:01:08.201778: step 13932, loss 0.586848.
Train: 2018-08-02T12:01:08.436067: step 13933, loss 0.602621.
Train: 2018-08-02T12:01:08.654766: step 13934, loss 0.578848.
Train: 2018-08-02T12:01:08.873497: step 13935, loss 0.547343.
Train: 2018-08-02T12:01:09.107814: step 13936, loss 0.602561.
Train: 2018-08-02T12:01:09.326484: step 13937, loss 0.54742.
Train: 2018-08-02T12:01:09.560805: step 13938, loss 0.61811.
Train: 2018-08-02T12:01:09.795155: step 13939, loss 0.547558.
Train: 2018-08-02T12:01:10.029478: step 13940, loss 0.625783.
Test: 2018-08-02T12:01:11.232288: step 13940, loss 0.549387.
Train: 2018-08-02T12:01:11.451011: step 13941, loss 0.524326.
Train: 2018-08-02T12:01:11.669711: step 13942, loss 0.563329.
Train: 2018-08-02T12:01:11.888409: step 13943, loss 0.571126.
Train: 2018-08-02T12:01:12.107107: step 13944, loss 0.52451.
Train: 2018-08-02T12:01:12.341434: step 13945, loss 0.555583.
Train: 2018-08-02T12:01:12.575754: step 13946, loss 0.547783.
Train: 2018-08-02T12:01:12.810073: step 13947, loss 0.571108.
Train: 2018-08-02T12:01:13.028742: step 13948, loss 0.524303.
Train: 2018-08-02T12:01:13.247440: step 13949, loss 0.555436.
Train: 2018-08-02T12:01:13.466170: step 13950, loss 0.539712.
Test: 2018-08-02T12:01:14.653362: step 13950, loss 0.550941.
Train: 2018-08-02T12:01:14.903329: step 13951, loss 0.60244.
Train: 2018-08-02T12:01:15.106411: step 13952, loss 0.539508.
Train: 2018-08-02T12:01:15.340726: step 13953, loss 0.539408.
Train: 2018-08-02T12:01:15.575052: step 13954, loss 0.594675.
Train: 2018-08-02T12:01:15.809371: step 13955, loss 0.515346.
Train: 2018-08-02T12:01:16.043692: step 13956, loss 0.523238.
Train: 2018-08-02T12:01:16.262390: step 13957, loss 0.546849.
Train: 2018-08-02T12:01:16.496707: step 13958, loss 0.562748.
Train: 2018-08-02T12:01:16.731000: step 13959, loss 0.529955.
Train: 2018-08-02T12:01:16.965322: step 13960, loss 0.521124.
Test: 2018-08-02T12:01:18.168164: step 13960, loss 0.547698.
Train: 2018-08-02T12:01:18.371266: step 13961, loss 0.480767.
Train: 2018-08-02T12:01:18.605589: step 13962, loss 0.625178.
Train: 2018-08-02T12:01:18.808664: step 13963, loss 0.502554.
Train: 2018-08-02T12:01:19.042984: step 13964, loss 0.554799.
Train: 2018-08-02T12:01:19.261660: step 13965, loss 0.545066.
Train: 2018-08-02T12:01:19.495977: step 13966, loss 0.551439.
Train: 2018-08-02T12:01:19.730300: step 13967, loss 0.622023.
Train: 2018-08-02T12:01:19.933376: step 13968, loss 0.561044.
Train: 2018-08-02T12:01:20.167727: step 13969, loss 0.578937.
Train: 2018-08-02T12:01:20.386396: step 13970, loss 0.588123.
Test: 2018-08-02T12:01:21.589238: step 13970, loss 0.54998.
Train: 2018-08-02T12:01:21.807963: step 13971, loss 0.579888.
Train: 2018-08-02T12:01:22.026666: step 13972, loss 0.512283.
Train: 2018-08-02T12:01:22.260986: step 13973, loss 0.554242.
Train: 2018-08-02T12:01:22.495300: step 13974, loss 0.563319.
Train: 2018-08-02T12:01:22.713974: step 13975, loss 0.628418.
Train: 2018-08-02T12:01:22.948325: step 13976, loss 0.511365.
Train: 2018-08-02T12:01:23.167018: step 13977, loss 0.519605.
Train: 2018-08-02T12:01:23.385717: step 13978, loss 0.623183.
Train: 2018-08-02T12:01:23.604392: step 13979, loss 0.646255.
Train: 2018-08-02T12:01:23.854357: step 13980, loss 0.562124.
Test: 2018-08-02T12:01:25.041554: step 13980, loss 0.549205.
Train: 2018-08-02T12:01:25.260253: step 13981, loss 0.546072.
Train: 2018-08-02T12:01:25.478953: step 13982, loss 0.603501.
Train: 2018-08-02T12:01:25.697681: step 13983, loss 0.537877.
Train: 2018-08-02T12:01:25.916349: step 13984, loss 0.521515.
Train: 2018-08-02T12:01:26.135073: step 13985, loss 0.627989.
Train: 2018-08-02T12:01:26.369368: step 13986, loss 0.562532.
Train: 2018-08-02T12:01:26.588098: step 13987, loss 0.619522.
Train: 2018-08-02T12:01:26.822418: step 13988, loss 0.546513.
Train: 2018-08-02T12:01:27.056732: step 13989, loss 0.603105.
Train: 2018-08-02T12:01:27.291028: step 13990, loss 0.498366.
Test: 2018-08-02T12:01:28.478250: step 13990, loss 0.548995.
Train: 2018-08-02T12:01:28.712595: step 13991, loss 0.506525.
Train: 2018-08-02T12:01:28.931293: step 13992, loss 0.570833.
Train: 2018-08-02T12:01:29.149996: step 13993, loss 0.53867.
Train: 2018-08-02T12:01:29.384318: step 13994, loss 0.554741.
Train: 2018-08-02T12:01:29.603010: step 13995, loss 0.562763.
Train: 2018-08-02T12:01:29.837306: step 13996, loss 0.538603.
Train: 2018-08-02T12:01:30.071657: step 13997, loss 0.530488.
Train: 2018-08-02T12:01:30.305965: step 13998, loss 0.530391.
Train: 2018-08-02T12:01:30.524675: step 13999, loss 0.587.
Train: 2018-08-02T12:01:30.758997: step 14000, loss 0.562674.
Test: 2018-08-02T12:01:31.961809: step 14000, loss 0.54926.
Train: 2018-08-02T12:01:32.821015: step 14001, loss 0.611458.
Train: 2018-08-02T12:01:33.055302: step 14002, loss 0.49754.
Train: 2018-08-02T12:01:33.289654: step 14003, loss 0.595309.
Train: 2018-08-02T12:01:33.523974: step 14004, loss 0.546299.
Train: 2018-08-02T12:01:33.773910: step 14005, loss 0.546264.
Train: 2018-08-02T12:01:33.992583: step 14006, loss 0.570776.
Train: 2018-08-02T12:01:34.226933: step 14007, loss 0.5953.
Train: 2018-08-02T12:01:34.461254: step 14008, loss 0.603565.
Train: 2018-08-02T12:01:34.679948: step 14009, loss 0.554435.
Train: 2018-08-02T12:01:34.914242: step 14010, loss 0.570775.
Test: 2018-08-02T12:01:36.101465: step 14010, loss 0.548593.
Train: 2018-08-02T12:01:36.304573: step 14011, loss 0.578939.
Train: 2018-08-02T12:01:36.523265: step 14012, loss 0.578936.
Train: 2018-08-02T12:01:36.757592: step 14013, loss 0.570781.
Train: 2018-08-02T12:01:36.991908: step 14014, loss 0.562647.
Train: 2018-08-02T12:01:37.210605: step 14015, loss 0.643929.
Train: 2018-08-02T12:01:37.429303: step 14016, loss 0.546477.
Train: 2018-08-02T12:01:37.648010: step 14017, loss 0.59507.
Train: 2018-08-02T12:01:37.866675: step 14018, loss 0.506261.
Train: 2018-08-02T12:01:38.085375: step 14019, loss 0.586941.
Train: 2018-08-02T12:01:38.319704: step 14020, loss 0.522524.
Test: 2018-08-02T12:01:39.506917: step 14020, loss 0.548783.
Train: 2018-08-02T12:01:39.710024: step 14021, loss 0.594972.
Train: 2018-08-02T12:01:39.944314: step 14022, loss 0.55475.
Train: 2018-08-02T12:01:40.178634: step 14023, loss 0.578872.
Train: 2018-08-02T12:01:40.397334: step 14024, loss 0.56281.
Train: 2018-08-02T12:01:40.600410: step 14025, loss 0.506644.
Train: 2018-08-02T12:01:40.819109: step 14026, loss 0.530686.
Train: 2018-08-02T12:01:41.037839: step 14027, loss 0.570832.
Train: 2018-08-02T12:01:41.272128: step 14028, loss 0.570825.
Train: 2018-08-02T12:01:41.490861: step 14029, loss 0.514409.
Train: 2018-08-02T12:01:41.709557: step 14030, loss 0.465846.
Test: 2018-08-02T12:01:42.912369: step 14030, loss 0.550645.
Train: 2018-08-02T12:01:43.193579: step 14031, loss 0.578899.
Train: 2018-08-02T12:01:43.412252: step 14032, loss 0.530131.
Train: 2018-08-02T12:01:43.646599: step 14033, loss 0.472863.
Train: 2018-08-02T12:01:43.865303: step 14034, loss 0.529759.
Train: 2018-08-02T12:01:44.084001: step 14035, loss 0.562519.
Train: 2018-08-02T12:01:44.302693: step 14036, loss 0.603865.
Train: 2018-08-02T12:01:44.536989: step 14037, loss 0.487666.
Train: 2018-08-02T12:01:44.771309: step 14038, loss 0.554103.
Train: 2018-08-02T12:01:44.990009: step 14039, loss 0.562393.
Train: 2018-08-02T12:01:45.208737: step 14040, loss 0.553871.
Test: 2018-08-02T12:01:46.411550: step 14040, loss 0.546894.
Train: 2018-08-02T12:01:46.645895: step 14041, loss 0.537015.
Train: 2018-08-02T12:01:46.880215: step 14042, loss 0.562419.
Train: 2018-08-02T12:01:47.052044: step 14043, loss 0.580825.
Train: 2018-08-02T12:01:47.270754: step 14044, loss 0.579313.
Train: 2018-08-02T12:01:47.489448: step 14045, loss 0.545267.
Train: 2018-08-02T12:01:47.723775: step 14046, loss 0.502585.
Train: 2018-08-02T12:01:47.958087: step 14047, loss 0.55373.
Train: 2018-08-02T12:01:48.176786: step 14048, loss 0.528065.
Train: 2018-08-02T12:01:48.411083: step 14049, loss 0.545267.
Train: 2018-08-02T12:01:48.629812: step 14050, loss 0.561983.
Test: 2018-08-02T12:01:49.817002: step 14050, loss 0.548299.
Train: 2018-08-02T12:01:50.020105: step 14051, loss 0.570909.
Train: 2018-08-02T12:01:50.254400: step 14052, loss 0.536489.
Train: 2018-08-02T12:01:50.473105: step 14053, loss 0.527279.
Train: 2018-08-02T12:01:50.691828: step 14054, loss 0.579084.
Train: 2018-08-02T12:01:50.926148: step 14055, loss 0.56231.
Train: 2018-08-02T12:01:51.176093: step 14056, loss 0.598024.
Train: 2018-08-02T12:01:51.394788: step 14057, loss 0.553338.
Train: 2018-08-02T12:01:51.613482: step 14058, loss 0.561991.
Train: 2018-08-02T12:01:51.847802: step 14059, loss 0.553257.
Train: 2018-08-02T12:01:52.066475: step 14060, loss 0.597599.
Test: 2018-08-02T12:01:53.269319: step 14060, loss 0.549182.
Train: 2018-08-02T12:01:53.472396: step 14061, loss 0.500962.
Train: 2018-08-02T12:01:53.691095: step 14062, loss 0.553788.
Train: 2018-08-02T12:01:53.925442: step 14063, loss 0.527876.
Train: 2018-08-02T12:01:54.128523: step 14064, loss 0.483915.
Train: 2018-08-02T12:01:54.362840: step 14065, loss 0.5282.
Train: 2018-08-02T12:01:54.581537: step 14066, loss 0.519402.
Train: 2018-08-02T12:01:54.815856: step 14067, loss 0.544425.
Train: 2018-08-02T12:01:55.034561: step 14068, loss 0.652604.
Train: 2018-08-02T12:01:55.253254: step 14069, loss 0.63313.
Train: 2018-08-02T12:01:55.487574: step 14070, loss 0.596396.
Test: 2018-08-02T12:01:56.659151: step 14070, loss 0.548001.
Train: 2018-08-02T12:01:56.893470: step 14071, loss 0.476681.
Train: 2018-08-02T12:01:57.127790: step 14072, loss 0.502425.
Train: 2018-08-02T12:01:57.346522: step 14073, loss 0.587828.
Train: 2018-08-02T12:01:57.580842: step 14074, loss 0.528462.
Train: 2018-08-02T12:01:57.815131: step 14075, loss 0.570734.
Train: 2018-08-02T12:01:58.033853: step 14076, loss 0.502989.
Train: 2018-08-02T12:01:58.252555: step 14077, loss 0.587781.
Train: 2018-08-02T12:01:58.486872: step 14078, loss 0.579314.
Train: 2018-08-02T12:01:58.689926: step 14079, loss 0.570823.
Train: 2018-08-02T12:01:58.939891: step 14080, loss 0.486254.
Test: 2018-08-02T12:02:00.127088: step 14080, loss 0.547647.
Train: 2018-08-02T12:02:00.330196: step 14081, loss 0.545441.
Train: 2018-08-02T12:02:00.548898: step 14082, loss 0.545432.
Train: 2018-08-02T12:02:00.783216: step 14083, loss 0.613161.
Train: 2018-08-02T12:02:01.017555: step 14084, loss 0.579282.
Train: 2018-08-02T12:02:01.251826: step 14085, loss 0.629984.
Train: 2018-08-02T12:02:01.470556: step 14086, loss 0.553938.
Train: 2018-08-02T12:02:01.704874: step 14087, loss 0.562379.
Train: 2018-08-02T12:02:01.923544: step 14088, loss 0.553993.
Train: 2018-08-02T12:02:02.142273: step 14089, loss 0.621071.
Train: 2018-08-02T12:02:02.360940: step 14090, loss 0.612548.
Test: 2018-08-02T12:02:03.563783: step 14090, loss 0.548345.
Train: 2018-08-02T12:02:03.782513: step 14091, loss 0.579098.
Train: 2018-08-02T12:02:04.016827: step 14092, loss 0.562472.
Train: 2018-08-02T12:02:04.219909: step 14093, loss 0.587281.
Train: 2018-08-02T12:02:04.454238: step 14094, loss 0.504917.
Train: 2018-08-02T12:02:04.688520: step 14095, loss 0.611816.
Train: 2018-08-02T12:02:04.907249: step 14096, loss 0.538022.
Train: 2018-08-02T12:02:05.125948: step 14097, loss 0.529936.
Train: 2018-08-02T12:02:05.344646: step 14098, loss 0.587084.
Train: 2018-08-02T12:02:05.578962: step 14099, loss 0.546357.
Train: 2018-08-02T12:02:05.797635: step 14100, loss 0.570783.
Test: 2018-08-02T12:02:07.000479: step 14100, loss 0.549294.
Train: 2018-08-02T12:02:07.844031: step 14101, loss 0.530188.
Train: 2018-08-02T12:02:08.062732: step 14102, loss 0.554557.
Train: 2018-08-02T12:02:08.281459: step 14103, loss 0.57079.
Train: 2018-08-02T12:02:08.515779: step 14104, loss 0.56268.
Train: 2018-08-02T12:02:08.750099: step 14105, loss 0.530247.
Train: 2018-08-02T12:02:08.984421: step 14106, loss 0.546452.
Train: 2018-08-02T12:02:09.203087: step 14107, loss 0.578907.
Train: 2018-08-02T12:02:09.421817: step 14108, loss 0.570787.
Train: 2018-08-02T12:02:09.656132: step 14109, loss 0.530168.
Train: 2018-08-02T12:02:09.874836: step 14110, loss 0.578913.
Test: 2018-08-02T12:02:11.077649: step 14110, loss 0.548126.
Train: 2018-08-02T12:02:11.296347: step 14111, loss 0.58705.
Train: 2018-08-02T12:02:11.530667: step 14112, loss 0.554514.
Train: 2018-08-02T12:02:11.749379: step 14113, loss 0.603319.
Train: 2018-08-02T12:02:11.952474: step 14114, loss 0.619552.
Train: 2018-08-02T12:02:12.186764: step 14115, loss 0.554565.
Train: 2018-08-02T12:02:12.405493: step 14116, loss 0.554595.
Train: 2018-08-02T12:02:12.624191: step 14117, loss 0.570801.
Train: 2018-08-02T12:02:12.874115: step 14118, loss 0.578888.
Train: 2018-08-02T12:02:13.092832: step 14119, loss 0.570813.
Train: 2018-08-02T12:02:13.327122: step 14120, loss 0.611119.
Test: 2018-08-02T12:02:14.529965: step 14120, loss 0.548803.
Train: 2018-08-02T12:02:14.733068: step 14121, loss 0.651252.
Train: 2018-08-02T12:02:14.967393: step 14122, loss 0.570854.
Train: 2018-08-02T12:02:15.186092: step 14123, loss 0.578861.
Train: 2018-08-02T12:02:15.404760: step 14124, loss 0.483399.
Train: 2018-08-02T12:02:15.639082: step 14125, loss 0.666254.
Train: 2018-08-02T12:02:15.873432: step 14126, loss 0.539265.
Train: 2018-08-02T12:02:16.092124: step 14127, loss 0.507755.
Train: 2018-08-02T12:02:16.310825: step 14128, loss 0.563071.
Train: 2018-08-02T12:02:16.545123: step 14129, loss 0.673562.
Train: 2018-08-02T12:02:16.763816: step 14130, loss 0.555257.
Test: 2018-08-02T12:02:17.935418: step 14130, loss 0.548422.
Train: 2018-08-02T12:02:18.169765: step 14131, loss 0.523898.
Train: 2018-08-02T12:02:18.404091: step 14132, loss 0.523944.
Train: 2018-08-02T12:02:18.607160: step 14133, loss 0.571023.
Train: 2018-08-02T12:02:18.857102: step 14134, loss 0.547466.
Train: 2018-08-02T12:02:19.060180: step 14135, loss 0.531719.
Train: 2018-08-02T12:02:19.278884: step 14136, loss 0.500151.
Train: 2018-08-02T12:02:19.497582: step 14137, loss 0.499906.
Train: 2018-08-02T12:02:19.716281: step 14138, loss 0.547106.
Train: 2018-08-02T12:02:19.950597: step 14139, loss 0.546984.
Train: 2018-08-02T12:02:20.169270: step 14140, loss 0.57888.
Test: 2018-08-02T12:02:21.372113: step 14140, loss 0.549737.
Train: 2018-08-02T12:02:21.590836: step 14141, loss 0.554771.
Train: 2018-08-02T12:02:21.809535: step 14142, loss 0.522364.
Train: 2018-08-02T12:02:22.043832: step 14143, loss 0.530179.
Train: 2018-08-02T12:02:22.262559: step 14144, loss 0.603412.
Train: 2018-08-02T12:02:22.481260: step 14145, loss 0.578701.
Train: 2018-08-02T12:02:22.715578: step 14146, loss 0.562796.
Train: 2018-08-02T12:02:22.934278: step 14147, loss 0.570529.
Train: 2018-08-02T12:02:23.168598: step 14148, loss 0.545875.
Train: 2018-08-02T12:02:23.402922: step 14149, loss 0.553581.
Train: 2018-08-02T12:02:23.605997: step 14150, loss 0.59593.
Test: 2018-08-02T12:02:24.808808: step 14150, loss 0.548262.
Train: 2018-08-02T12:02:25.011910: step 14151, loss 0.595232.
Train: 2018-08-02T12:02:25.230609: step 14152, loss 0.655114.
Train: 2018-08-02T12:02:25.464905: step 14153, loss 0.530126.
Train: 2018-08-02T12:02:25.668040: step 14154, loss 0.578715.
Train: 2018-08-02T12:02:25.886711: step 14155, loss 0.562748.
Train: 2018-08-02T12:02:26.105407: step 14156, loss 0.587497.
Train: 2018-08-02T12:02:26.339729: step 14157, loss 0.554388.
Train: 2018-08-02T12:02:26.574053: step 14158, loss 0.513646.
Train: 2018-08-02T12:02:26.792743: step 14159, loss 0.578921.
Train: 2018-08-02T12:02:27.011448: step 14160, loss 0.513821.
Test: 2018-08-02T12:02:28.214261: step 14160, loss 0.54923.
Train: 2018-08-02T12:02:28.432959: step 14161, loss 0.521935.
Train: 2018-08-02T12:02:28.682901: step 14162, loss 0.570775.
Train: 2018-08-02T12:02:28.901600: step 14163, loss 0.578931.
Train: 2018-08-02T12:02:29.120298: step 14164, loss 0.578934.
Train: 2018-08-02T12:02:29.339027: step 14165, loss 0.546278.
Train: 2018-08-02T12:02:29.557728: step 14166, loss 0.554434.
Train: 2018-08-02T12:02:29.792047: step 14167, loss 0.63615.
Train: 2018-08-02T12:02:30.026367: step 14168, loss 0.538114.
Train: 2018-08-02T12:02:30.260681: step 14169, loss 0.570772.
Train: 2018-08-02T12:02:30.479388: step 14170, loss 0.578929.
Test: 2018-08-02T12:02:31.682199: step 14170, loss 0.548641.
Train: 2018-08-02T12:02:31.885301: step 14171, loss 0.51373.
Train: 2018-08-02T12:02:32.119596: step 14172, loss 0.562623.
Train: 2018-08-02T12:02:32.338295: step 14173, loss 0.546312.
Train: 2018-08-02T12:02:32.557025: step 14174, loss 0.497345.
Train: 2018-08-02T12:02:32.853831: step 14175, loss 0.562583.
Train: 2018-08-02T12:02:33.072499: step 14176, loss 0.628097.
Train: 2018-08-02T12:02:33.306849: step 14177, loss 0.529829.
Train: 2018-08-02T12:02:33.525550: step 14178, loss 0.578952.
Train: 2018-08-02T12:02:33.759837: step 14179, loss 0.546161.
Train: 2018-08-02T12:02:33.978537: step 14180, loss 0.693855.
Test: 2018-08-02T12:02:35.181379: step 14180, loss 0.548534.
Train: 2018-08-02T12:02:35.400108: step 14181, loss 0.578944.
Train: 2018-08-02T12:02:35.618776: step 14182, loss 0.538113.
Train: 2018-08-02T12:02:35.837506: step 14183, loss 0.652288.
Train: 2018-08-02T12:02:36.071831: step 14184, loss 0.538289.
Train: 2018-08-02T12:02:36.290521: step 14185, loss 0.570795.
Train: 2018-08-02T12:02:36.524845: step 14186, loss 0.530381.
Train: 2018-08-02T12:02:36.743538: step 14187, loss 0.570811.
Train: 2018-08-02T12:02:36.977865: step 14188, loss 0.514384.
Train: 2018-08-02T12:02:37.212153: step 14189, loss 0.570819.
Train: 2018-08-02T12:02:37.446476: step 14190, loss 0.578879.
Test: 2018-08-02T12:02:38.649317: step 14190, loss 0.549875.
Train: 2018-08-02T12:02:38.852419: step 14191, loss 0.546661.
Train: 2018-08-02T12:02:39.086714: step 14192, loss 0.546664.
Train: 2018-08-02T12:02:39.321060: step 14193, loss 0.538598.
Train: 2018-08-02T12:02:39.492894: step 14194, loss 0.545553.
Train: 2018-08-02T12:02:39.742836: step 14195, loss 0.586957.
Train: 2018-08-02T12:02:39.961543: step 14196, loss 0.578887.
Train: 2018-08-02T12:02:40.195862: step 14197, loss 0.538488.
Train: 2018-08-02T12:02:40.414559: step 14198, loss 0.651667.
Train: 2018-08-02T12:02:40.633253: step 14199, loss 0.554655.
Train: 2018-08-02T12:02:40.867573: step 14200, loss 0.554672.
Test: 2018-08-02T12:02:42.054769: step 14200, loss 0.548906.
Train: 2018-08-02T12:02:42.945188: step 14201, loss 0.538552.
Train: 2018-08-02T12:02:43.163909: step 14202, loss 0.546615.
Train: 2018-08-02T12:02:43.382614: step 14203, loss 0.611171.
Train: 2018-08-02T12:02:43.601314: step 14204, loss 0.554675.
Train: 2018-08-02T12:02:43.835635: step 14205, loss 0.635334.
Train: 2018-08-02T12:02:44.069951: step 14206, loss 0.538621.
Train: 2018-08-02T12:02:44.288646: step 14207, loss 0.643217.
Train: 2018-08-02T12:02:44.522967: step 14208, loss 0.554807.
Train: 2018-08-02T12:02:44.741665: step 14209, loss 0.578862.
Train: 2018-08-02T12:02:44.944748: step 14210, loss 0.554898.
Test: 2018-08-02T12:02:46.131940: step 14210, loss 0.549407.
Train: 2018-08-02T12:02:46.366285: step 14211, loss 0.546956.
Train: 2018-08-02T12:02:46.569362: step 14212, loss 0.539021.
Train: 2018-08-02T12:02:46.803657: step 14213, loss 0.570894.
Train: 2018-08-02T12:02:47.022386: step 14214, loss 0.586827.
Train: 2018-08-02T12:02:47.241089: step 14215, loss 0.531117.
Train: 2018-08-02T12:02:47.459754: step 14216, loss 0.531111.
Train: 2018-08-02T12:02:47.678483: step 14217, loss 0.570883.
Train: 2018-08-02T12:02:47.897182: step 14218, loss 0.570915.
Train: 2018-08-02T12:02:48.131502: step 14219, loss 0.538965.
Train: 2018-08-02T12:02:48.350202: step 14220, loss 0.514961.
Test: 2018-08-02T12:02:49.553014: step 14220, loss 0.548558.
Train: 2018-08-02T12:02:49.771720: step 14221, loss 0.514818.
Train: 2018-08-02T12:02:49.990441: step 14222, loss 0.522675.
Train: 2018-08-02T12:02:50.224731: step 14223, loss 0.554684.
Train: 2018-08-02T12:02:50.427840: step 14224, loss 0.506049.
Train: 2018-08-02T12:02:50.662161: step 14225, loss 0.595154.
Train: 2018-08-02T12:02:50.880858: step 14226, loss 0.595212.
Train: 2018-08-02T12:02:51.099558: step 14227, loss 0.538056.
Train: 2018-08-02T12:02:51.318256: step 14228, loss 0.513303.
Train: 2018-08-02T12:02:51.552548: step 14229, loss 0.480136.
Train: 2018-08-02T12:02:51.786892: step 14230, loss 0.545917.
Test: 2018-08-02T12:02:52.974087: step 14230, loss 0.5493.
Train: 2018-08-02T12:02:53.177164: step 14231, loss 0.479265.
Train: 2018-08-02T12:02:53.411485: step 14232, loss 0.528256.
Train: 2018-08-02T12:02:53.630185: step 14233, loss 0.638499.
Train: 2018-08-02T12:02:53.833262: step 14234, loss 0.553963.
Train: 2018-08-02T12:02:54.067612: step 14235, loss 0.56224.
Train: 2018-08-02T12:02:54.270692: step 14236, loss 0.527671.
Train: 2018-08-02T12:02:54.504979: step 14237, loss 0.537452.
Train: 2018-08-02T12:02:54.723710: step 14238, loss 0.493168.
Train: 2018-08-02T12:02:54.942406: step 14239, loss 0.598269.
Train: 2018-08-02T12:02:55.176698: step 14240, loss 0.545883.
Test: 2018-08-02T12:02:56.363918: step 14240, loss 0.547593.
Train: 2018-08-02T12:02:56.582642: step 14241, loss 0.55448.
Train: 2018-08-02T12:02:56.785725: step 14242, loss 0.553326.
Train: 2018-08-02T12:02:57.020040: step 14243, loss 0.501068.
Train: 2018-08-02T12:02:57.254335: step 14244, loss 0.55381.
Train: 2018-08-02T12:02:57.488687: step 14245, loss 0.623281.
Train: 2018-08-02T12:02:57.722977: step 14246, loss 0.579901.
Train: 2018-08-02T12:02:57.941674: step 14247, loss 0.623188.
Train: 2018-08-02T12:02:58.160404: step 14248, loss 0.562438.
Train: 2018-08-02T12:02:58.410315: step 14249, loss 0.570728.
Train: 2018-08-02T12:02:58.629025: step 14250, loss 0.579624.
Test: 2018-08-02T12:02:59.816235: step 14250, loss 0.548631.
Train: 2018-08-02T12:03:00.034960: step 14251, loss 0.562268.
Train: 2018-08-02T12:03:00.269285: step 14252, loss 0.562348.
Train: 2018-08-02T12:03:00.472364: step 14253, loss 0.613355.
Train: 2018-08-02T12:03:00.691061: step 14254, loss 0.553884.
Train: 2018-08-02T12:03:00.909759: step 14255, loss 0.604562.
Train: 2018-08-02T12:03:01.144077: step 14256, loss 0.545579.
Train: 2018-08-02T12:03:01.347159: step 14257, loss 0.503771.
Train: 2018-08-02T12:03:01.581479: step 14258, loss 0.554052.
Train: 2018-08-02T12:03:01.800175: step 14259, loss 0.512345.
Train: 2018-08-02T12:03:02.018845: step 14260, loss 0.637496.
Test: 2018-08-02T12:03:03.221688: step 14260, loss 0.54873.
Train: 2018-08-02T12:03:03.424793: step 14261, loss 0.520823.
Train: 2018-08-02T12:03:03.643489: step 14262, loss 0.504253.
Train: 2018-08-02T12:03:03.877816: step 14263, loss 0.604015.
Train: 2018-08-02T12:03:04.096511: step 14264, loss 0.579066.
Train: 2018-08-02T12:03:04.315181: step 14265, loss 0.52928.
Train: 2018-08-02T12:03:04.549501: step 14266, loss 0.58734.
Train: 2018-08-02T12:03:04.768230: step 14267, loss 0.595604.
Train: 2018-08-02T12:03:05.002522: step 14268, loss 0.537683.
Train: 2018-08-02T12:03:05.221253: step 14269, loss 0.628572.
Train: 2018-08-02T12:03:05.455564: step 14270, loss 0.546038.
Test: 2018-08-02T12:03:06.642761: step 14270, loss 0.54842.
Train: 2018-08-02T12:03:06.861491: step 14271, loss 0.611879.
Train: 2018-08-02T12:03:07.064568: step 14272, loss 0.570761.
Train: 2018-08-02T12:03:07.314479: step 14273, loss 0.488968.
Train: 2018-08-02T12:03:07.533179: step 14274, loss 0.521732.
Train: 2018-08-02T12:03:07.736287: step 14275, loss 0.603463.
Train: 2018-08-02T12:03:07.970609: step 14276, loss 0.578942.
Train: 2018-08-02T12:03:08.173653: step 14277, loss 0.562615.
Train: 2018-08-02T12:03:08.408004: step 14278, loss 0.644118.
Train: 2018-08-02T12:03:08.642294: step 14279, loss 0.546401.
Train: 2018-08-02T12:03:08.861018: step 14280, loss 0.61135.
Test: 2018-08-02T12:03:10.048214: step 14280, loss 0.548274.
Train: 2018-08-02T12:03:10.251291: step 14281, loss 0.498004.
Train: 2018-08-02T12:03:10.501233: step 14282, loss 0.570807.
Train: 2018-08-02T12:03:10.704310: step 14283, loss 0.595025.
Train: 2018-08-02T12:03:10.923040: step 14284, loss 0.546651.
Train: 2018-08-02T12:03:11.141738: step 14285, loss 0.538624.
Train: 2018-08-02T12:03:11.376059: step 14286, loss 0.603011.
Train: 2018-08-02T12:03:11.610349: step 14287, loss 0.602968.
Train: 2018-08-02T12:03:11.829047: step 14288, loss 0.562837.
Train: 2018-08-02T12:03:12.063394: step 14289, loss 0.490761.
Train: 2018-08-02T12:03:12.297718: step 14290, loss 0.546797.
Test: 2018-08-02T12:03:13.469288: step 14290, loss 0.549803.
Train: 2018-08-02T12:03:13.703609: step 14291, loss 0.562796.
Train: 2018-08-02T12:03:13.937953: step 14292, loss 0.570795.
Train: 2018-08-02T12:03:14.172280: step 14293, loss 0.530649.
Train: 2018-08-02T12:03:14.406595: step 14294, loss 0.562734.
Train: 2018-08-02T12:03:14.609676: step 14295, loss 0.587199.
Train: 2018-08-02T12:03:14.843991: step 14296, loss 0.56271.
Train: 2018-08-02T12:03:15.078317: step 14297, loss 0.530356.
Train: 2018-08-02T12:03:15.297009: step 14298, loss 0.595185.
Train: 2018-08-02T12:03:15.500062: step 14299, loss 0.586934.
Train: 2018-08-02T12:03:15.734407: step 14300, loss 0.546434.
Test: 2018-08-02T12:03:16.905984: step 14300, loss 0.548431.
Train: 2018-08-02T12:03:17.780804: step 14301, loss 0.505944.
Train: 2018-08-02T12:03:17.999477: step 14302, loss 0.611323.
Train: 2018-08-02T12:03:18.218177: step 14303, loss 0.538397.
Train: 2018-08-02T12:03:18.452524: step 14304, loss 0.497607.
Train: 2018-08-02T12:03:18.686843: step 14305, loss 0.562411.
Train: 2018-08-02T12:03:18.921167: step 14306, loss 0.512952.
Train: 2018-08-02T12:03:19.155490: step 14307, loss 0.563387.
Train: 2018-08-02T12:03:19.405398: step 14308, loss 0.546409.
Train: 2018-08-02T12:03:19.624097: step 14309, loss 0.5219.
Train: 2018-08-02T12:03:19.858417: step 14310, loss 0.504169.
Test: 2018-08-02T12:03:21.045639: step 14310, loss 0.548584.
Train: 2018-08-02T12:03:21.248742: step 14311, loss 0.537426.
Train: 2018-08-02T12:03:21.483036: step 14312, loss 0.604792.
Train: 2018-08-02T12:03:21.717389: step 14313, loss 0.612375.
Train: 2018-08-02T12:03:21.920466: step 14314, loss 0.527733.
Train: 2018-08-02T12:03:22.139163: step 14315, loss 0.613889.
Train: 2018-08-02T12:03:22.357856: step 14316, loss 0.604866.
Train: 2018-08-02T12:03:22.592185: step 14317, loss 0.587633.
Train: 2018-08-02T12:03:22.826473: step 14318, loss 0.553789.
Train: 2018-08-02T12:03:23.045200: step 14319, loss 0.571056.
Train: 2018-08-02T12:03:23.279515: step 14320, loss 0.521047.
Test: 2018-08-02T12:03:24.451092: step 14320, loss 0.546924.
Train: 2018-08-02T12:03:24.685412: step 14321, loss 0.587425.
Train: 2018-08-02T12:03:24.935383: step 14322, loss 0.59549.
Train: 2018-08-02T12:03:25.169698: step 14323, loss 0.611959.
Train: 2018-08-02T12:03:25.403994: step 14324, loss 0.546102.
Train: 2018-08-02T12:03:25.638314: step 14325, loss 0.54616.
Train: 2018-08-02T12:03:25.888277: step 14326, loss 0.587141.
Train: 2018-08-02T12:03:26.091332: step 14327, loss 0.521732.
Train: 2018-08-02T12:03:26.325682: step 14328, loss 0.505445.
Train: 2018-08-02T12:03:26.559998: step 14329, loss 0.546262.
Train: 2018-08-02T12:03:26.794323: step 14330, loss 0.595293.
Test: 2018-08-02T12:03:27.981515: step 14330, loss 0.548753.
Train: 2018-08-02T12:03:28.215860: step 14331, loss 0.578942.
Train: 2018-08-02T12:03:28.434558: step 14332, loss 0.636137.
Train: 2018-08-02T12:03:28.637610: step 14333, loss 0.538152.
Train: 2018-08-02T12:03:28.856335: step 14334, loss 0.538196.
Train: 2018-08-02T12:03:29.090656: step 14335, loss 0.554498.
Train: 2018-08-02T12:03:29.340571: step 14336, loss 0.562642.
Train: 2018-08-02T12:03:29.574892: step 14337, loss 0.570781.
Train: 2018-08-02T12:03:29.809243: step 14338, loss 0.56265.
Train: 2018-08-02T12:03:30.043563: step 14339, loss 0.562655.
Train: 2018-08-02T12:03:30.277853: step 14340, loss 0.562658.
Test: 2018-08-02T12:03:31.496316: step 14340, loss 0.550029.
Train: 2018-08-02T12:03:31.715040: step 14341, loss 0.554538.
Train: 2018-08-02T12:03:31.933765: step 14342, loss 0.595157.
Train: 2018-08-02T12:03:32.152414: step 14343, loss 0.562671.
Train: 2018-08-02T12:03:32.386733: step 14344, loss 0.587016.
Train: 2018-08-02T12:03:32.574190: step 14345, loss 0.700995.
Train: 2018-08-02T12:03:32.792918: step 14346, loss 0.627318.
Train: 2018-08-02T12:03:33.027209: step 14347, loss 0.60297.
Train: 2018-08-02T12:03:33.261553: step 14348, loss 0.618818.
Train: 2018-08-02T12:03:33.495873: step 14349, loss 0.578858.
Train: 2018-08-02T12:03:33.730168: step 14350, loss 0.523557.
Test: 2018-08-02T12:03:34.933012: step 14350, loss 0.550529.
Train: 2018-08-02T12:03:35.151741: step 14351, loss 0.484434.
Train: 2018-08-02T12:03:35.386061: step 14352, loss 0.539587.
Train: 2018-08-02T12:03:35.620376: step 14353, loss 0.563171.
Train: 2018-08-02T12:03:35.839075: step 14354, loss 0.555332.
Train: 2018-08-02T12:03:36.073400: step 14355, loss 0.641646.
Train: 2018-08-02T12:03:36.307715: step 14356, loss 0.563212.
Train: 2018-08-02T12:03:36.526414: step 14357, loss 0.492828.
Train: 2018-08-02T12:03:36.760740: step 14358, loss 0.563224.
Train: 2018-08-02T12:03:36.979408: step 14359, loss 0.477037.
Train: 2018-08-02T12:03:37.198137: step 14360, loss 0.586727.
Test: 2018-08-02T12:03:38.385328: step 14360, loss 0.549789.
Train: 2018-08-02T12:03:38.619650: step 14361, loss 0.58674.
Train: 2018-08-02T12:03:38.853994: step 14362, loss 0.50789.
Train: 2018-08-02T12:03:39.069054: step 14363, loss 0.586769.
Train: 2018-08-02T12:03:39.303398: step 14364, loss 0.491679.
Train: 2018-08-02T12:03:39.522101: step 14365, loss 0.547041.
Train: 2018-08-02T12:03:39.756392: step 14366, loss 0.602814.
Train: 2018-08-02T12:03:39.990737: step 14367, loss 0.586869.
Train: 2018-08-02T12:03:40.209441: step 14368, loss 0.554806.
Train: 2018-08-02T12:03:40.443758: step 14369, loss 0.635126.
Train: 2018-08-02T12:03:40.678084: step 14370, loss 0.611028.
Test: 2018-08-02T12:03:41.865274: step 14370, loss 0.549762.
Train: 2018-08-02T12:03:42.084003: step 14371, loss 0.554776.
Train: 2018-08-02T12:03:42.318318: step 14372, loss 0.570841.
Train: 2018-08-02T12:03:42.552612: step 14373, loss 0.610968.
Train: 2018-08-02T12:03:42.786933: step 14374, loss 0.594892.
Train: 2018-08-02T12:03:43.021286: step 14375, loss 0.490886.
Train: 2018-08-02T12:03:43.255574: step 14376, loss 0.626848.
Train: 2018-08-02T12:03:43.489893: step 14377, loss 0.50698.
Train: 2018-08-02T12:03:43.739834: step 14378, loss 0.594837.
Train: 2018-08-02T12:03:43.974187: step 14379, loss 0.562893.
Train: 2018-08-02T12:03:44.208505: step 14380, loss 0.554918.
Test: 2018-08-02T12:03:45.411318: step 14380, loss 0.549571.
Train: 2018-08-02T12:03:45.645639: step 14381, loss 0.538958.
Train: 2018-08-02T12:03:45.879958: step 14382, loss 0.570876.
Train: 2018-08-02T12:03:46.114310: step 14383, loss 0.538917.
Train: 2018-08-02T12:03:46.348598: step 14384, loss 0.554871.
Train: 2018-08-02T12:03:46.567328: step 14385, loss 0.610891.
Train: 2018-08-02T12:03:46.801619: step 14386, loss 0.59488.
Train: 2018-08-02T12:03:47.051560: step 14387, loss 0.474819.
Train: 2018-08-02T12:03:47.285905: step 14388, loss 0.586882.
Train: 2018-08-02T12:03:47.520233: step 14389, loss 0.635032.
Train: 2018-08-02T12:03:47.754550: step 14390, loss 0.554812.
Test: 2018-08-02T12:03:48.941742: step 14390, loss 0.549817.
Train: 2018-08-02T12:03:49.176088: step 14391, loss 0.578866.
Train: 2018-08-02T12:03:49.394794: step 14392, loss 0.586877.
Train: 2018-08-02T12:03:49.629111: step 14393, loss 0.554853.
Train: 2018-08-02T12:03:49.863402: step 14394, loss 0.530869.
Train: 2018-08-02T12:03:50.097746: step 14395, loss 0.54686.
Train: 2018-08-02T12:03:50.347688: step 14396, loss 0.602883.
Train: 2018-08-02T12:03:50.582014: step 14397, loss 0.562854.
Train: 2018-08-02T12:03:50.816302: step 14398, loss 0.530839.
Train: 2018-08-02T12:03:51.050656: step 14399, loss 0.562845.
Train: 2018-08-02T12:03:51.300590: step 14400, loss 0.602914.
Test: 2018-08-02T12:03:52.472165: step 14400, loss 0.548713.
Train: 2018-08-02T12:03:53.362583: step 14401, loss 0.562836.
Train: 2018-08-02T12:03:53.596928: step 14402, loss 0.538791.
Train: 2018-08-02T12:03:53.831253: step 14403, loss 0.538764.
Train: 2018-08-02T12:03:54.065567: step 14404, loss 0.506607.
Train: 2018-08-02T12:03:54.284240: step 14405, loss 0.562774.
Train: 2018-08-02T12:03:54.534214: step 14406, loss 0.578889.
Train: 2018-08-02T12:03:54.784148: step 14407, loss 0.554594.
Train: 2018-08-02T12:03:55.018474: step 14408, loss 0.546515.
Train: 2018-08-02T12:03:55.252763: step 14409, loss 0.595077.
Train: 2018-08-02T12:03:55.487116: step 14410, loss 0.62769.
Test: 2018-08-02T12:03:56.674306: step 14410, loss 0.54775.
Train: 2018-08-02T12:03:56.908627: step 14411, loss 0.481384.
Train: 2018-08-02T12:03:57.127355: step 14412, loss 0.505375.
Train: 2018-08-02T12:03:57.361646: step 14413, loss 0.553963.
Train: 2018-08-02T12:03:57.595995: step 14414, loss 0.562163.
Train: 2018-08-02T12:03:57.814704: step 14415, loss 0.628902.
Train: 2018-08-02T12:03:58.049014: step 14416, loss 0.576943.
Train: 2018-08-02T12:03:58.267713: step 14417, loss 0.520831.
Train: 2018-08-02T12:03:58.502003: step 14418, loss 0.573588.
Train: 2018-08-02T12:03:58.736325: step 14419, loss 0.578862.
Train: 2018-08-02T12:03:58.986290: step 14420, loss 0.518778.
Test: 2018-08-02T12:04:00.173486: step 14420, loss 0.549356.
Train: 2018-08-02T12:04:00.392216: step 14421, loss 0.519325.
Train: 2018-08-02T12:04:00.610909: step 14422, loss 0.581685.
Train: 2018-08-02T12:04:00.845229: step 14423, loss 0.527418.
Train: 2018-08-02T12:04:01.079525: step 14424, loss 0.475193.
Train: 2018-08-02T12:04:01.313876: step 14425, loss 0.505497.
Train: 2018-08-02T12:04:01.548190: step 14426, loss 0.56021.
Train: 2018-08-02T12:04:01.782515: step 14427, loss 0.569279.
Train: 2018-08-02T12:04:02.016806: step 14428, loss 0.579423.
Train: 2018-08-02T12:04:02.251126: step 14429, loss 0.525538.
Train: 2018-08-02T12:04:02.485446: step 14430, loss 0.520738.
Test: 2018-08-02T12:04:03.672667: step 14430, loss 0.548009.
Train: 2018-08-02T12:04:03.907012: step 14431, loss 0.6082.
Train: 2018-08-02T12:04:04.141338: step 14432, loss 0.494487.
Train: 2018-08-02T12:04:04.375629: step 14433, loss 0.554017.
Train: 2018-08-02T12:04:04.625593: step 14434, loss 0.580558.
Train: 2018-08-02T12:04:04.859914: step 14435, loss 0.581173.
Train: 2018-08-02T12:04:05.094210: step 14436, loss 0.548363.
Train: 2018-08-02T12:04:05.328560: step 14437, loss 0.596364.
Train: 2018-08-02T12:04:05.562849: step 14438, loss 0.613333.
Train: 2018-08-02T12:04:05.797200: step 14439, loss 0.620379.
Train: 2018-08-02T12:04:06.031491: step 14440, loss 0.54596.
Test: 2018-08-02T12:04:07.234332: step 14440, loss 0.548064.
Train: 2018-08-02T12:04:07.453056: step 14441, loss 0.496865.
Train: 2018-08-02T12:04:07.687379: step 14442, loss 0.562564.
Train: 2018-08-02T12:04:07.937325: step 14443, loss 0.587155.
Train: 2018-08-02T12:04:08.171613: step 14444, loss 0.611688.
Train: 2018-08-02T12:04:08.405964: step 14445, loss 0.497291.
Train: 2018-08-02T12:04:08.640285: step 14446, loss 0.538142.
Train: 2018-08-02T12:04:08.890224: step 14447, loss 0.578929.
Train: 2018-08-02T12:04:09.124543: step 14448, loss 0.570775.
Train: 2018-08-02T12:04:09.358835: step 14449, loss 0.587068.
Train: 2018-08-02T12:04:09.593187: step 14450, loss 0.587053.
Test: 2018-08-02T12:04:10.780378: step 14450, loss 0.54928.
Train: 2018-08-02T12:04:10.999110: step 14451, loss 0.562661.
Train: 2018-08-02T12:04:11.249044: step 14452, loss 0.603243.
Train: 2018-08-02T12:04:11.483338: step 14453, loss 0.514124.
Train: 2018-08-02T12:04:11.717689: step 14454, loss 0.514172.
Train: 2018-08-02T12:04:11.952003: step 14455, loss 0.562707.
Train: 2018-08-02T12:04:12.186298: step 14456, loss 0.473646.
Train: 2018-08-02T12:04:12.420645: step 14457, loss 0.627591.
Train: 2018-08-02T12:04:12.654970: step 14458, loss 0.611387.
Train: 2018-08-02T12:04:12.904911: step 14459, loss 0.595139.
Train: 2018-08-02T12:04:13.139228: step 14460, loss 0.53836.
Test: 2018-08-02T12:04:14.326422: step 14460, loss 0.54822.
Train: 2018-08-02T12:04:14.576389: step 14461, loss 0.538374.
Train: 2018-08-02T12:04:14.810714: step 14462, loss 0.489726.
Train: 2018-08-02T12:04:15.045029: step 14463, loss 0.538303.
Train: 2018-08-02T12:04:15.279349: step 14464, loss 0.513812.
Train: 2018-08-02T12:04:15.529290: step 14465, loss 0.52996.
Train: 2018-08-02T12:04:15.763626: step 14466, loss 0.505249.
Train: 2018-08-02T12:04:15.997936: step 14467, loss 0.578982.
Train: 2018-08-02T12:04:16.232256: step 14468, loss 0.570756.
Train: 2018-08-02T12:04:16.466577: step 14469, loss 0.678317.
Train: 2018-08-02T12:04:16.700897: step 14470, loss 0.537659.
Test: 2018-08-02T12:04:17.903710: step 14470, loss 0.54789.
Train: 2018-08-02T12:04:18.122438: step 14471, loss 0.529365.
Train: 2018-08-02T12:04:18.356729: step 14472, loss 0.579044.
Train: 2018-08-02T12:04:18.591074: step 14473, loss 0.612215.
Train: 2018-08-02T12:04:18.840991: step 14474, loss 0.570757.
Train: 2018-08-02T12:04:19.075311: step 14475, loss 0.612156.
Train: 2018-08-02T12:04:19.309661: step 14476, loss 0.554226.
Train: 2018-08-02T12:04:19.543975: step 14477, loss 0.595513.
Train: 2018-08-02T12:04:19.778271: step 14478, loss 0.628402.
Train: 2018-08-02T12:04:20.012592: step 14479, loss 0.578969.
Train: 2018-08-02T12:04:20.246942: step 14480, loss 0.578947.
Test: 2018-08-02T12:04:21.434132: step 14480, loss 0.549757.
Train: 2018-08-02T12:04:21.668483: step 14481, loss 0.538164.
Train: 2018-08-02T12:04:21.902799: step 14482, loss 0.538254.
Train: 2018-08-02T12:04:22.137127: step 14483, loss 0.587024.
Train: 2018-08-02T12:04:22.371444: step 14484, loss 0.554595.
Train: 2018-08-02T12:04:22.605765: step 14485, loss 0.570804.
Train: 2018-08-02T12:04:22.855700: step 14486, loss 0.481999.
Train: 2018-08-02T12:04:23.074404: step 14487, loss 0.514277.
Train: 2018-08-02T12:04:23.308720: step 14488, loss 0.514194.
Train: 2018-08-02T12:04:23.543039: step 14489, loss 0.587005.
Train: 2018-08-02T12:04:23.777360: step 14490, loss 0.603261.
Test: 2018-08-02T12:04:24.980177: step 14490, loss 0.549658.
Train: 2018-08-02T12:04:25.198907: step 14491, loss 0.587031.
Train: 2018-08-02T12:04:25.417575: step 14492, loss 0.570785.
Train: 2018-08-02T12:04:25.651895: step 14493, loss 0.562664.
Train: 2018-08-02T12:04:25.886240: step 14494, loss 0.603273.
Train: 2018-08-02T12:04:26.104939: step 14495, loss 0.562676.
Train: 2018-08-02T12:04:26.308018: step 14496, loss 0.562687.
Train: 2018-08-02T12:04:26.557958: step 14497, loss 0.562695.
Train: 2018-08-02T12:04:26.776662: step 14498, loss 0.554609.
Train: 2018-08-02T12:04:27.010984: step 14499, loss 0.514151.
Train: 2018-08-02T12:04:27.245298: step 14500, loss 0.530302.
Test: 2018-08-02T12:04:28.432494: step 14500, loss 0.548016.
Train: 2018-08-02T12:04:29.322936: step 14501, loss 0.554569.
Train: 2018-08-02T12:04:29.557256: step 14502, loss 0.570787.
Train: 2018-08-02T12:04:29.807172: step 14503, loss 0.611442.
Train: 2018-08-02T12:04:30.041525: step 14504, loss 0.562648.
Train: 2018-08-02T12:04:30.275842: step 14505, loss 0.595178.
Train: 2018-08-02T12:04:30.510157: step 14506, loss 0.546402.
Train: 2018-08-02T12:04:30.744483: step 14507, loss 0.587039.
Train: 2018-08-02T12:04:30.978798: step 14508, loss 0.505833.
Train: 2018-08-02T12:04:31.213119: step 14509, loss 0.53829.
Train: 2018-08-02T12:04:31.447443: step 14510, loss 0.578915.
Test: 2018-08-02T12:04:32.634635: step 14510, loss 0.549612.
Train: 2018-08-02T12:04:32.868987: step 14511, loss 0.53823.
Train: 2018-08-02T12:04:33.150169: step 14512, loss 0.48116.
Train: 2018-08-02T12:04:33.384491: step 14513, loss 0.587107.
Train: 2018-08-02T12:04:33.618804: step 14514, loss 0.497104.
Train: 2018-08-02T12:04:33.853136: step 14515, loss 0.587179.
Train: 2018-08-02T12:04:34.087446: step 14516, loss 0.504932.
Train: 2018-08-02T12:04:34.337386: step 14517, loss 0.612031.
Train: 2018-08-02T12:04:34.571712: step 14518, loss 0.554218.
Train: 2018-08-02T12:04:34.790404: step 14519, loss 0.603884.
Train: 2018-08-02T12:04:35.024731: step 14520, loss 0.570757.
Test: 2018-08-02T12:04:36.227544: step 14520, loss 0.548815.
Train: 2018-08-02T12:04:36.461864: step 14521, loss 0.545889.
Train: 2018-08-02T12:04:36.680588: step 14522, loss 0.637105.
Train: 2018-08-02T12:04:36.914882: step 14523, loss 0.554192.
Train: 2018-08-02T12:04:37.149233: step 14524, loss 0.579035.
Train: 2018-08-02T12:04:37.383548: step 14525, loss 0.645147.
Train: 2018-08-02T12:04:37.617867: step 14526, loss 0.521305.
Train: 2018-08-02T12:04:37.852188: step 14527, loss 0.537852.
Train: 2018-08-02T12:04:38.086484: step 14528, loss 0.578978.
Train: 2018-08-02T12:04:38.320834: step 14529, loss 0.578965.
Train: 2018-08-02T12:04:38.555144: step 14530, loss 0.537995.
Test: 2018-08-02T12:04:39.757967: step 14530, loss 0.549479.
Train: 2018-08-02T12:04:39.976690: step 14531, loss 0.595318.
Train: 2018-08-02T12:04:40.210985: step 14532, loss 0.554427.
Train: 2018-08-02T12:04:40.445331: step 14533, loss 0.636054.
Train: 2018-08-02T12:04:40.679653: step 14534, loss 0.521951.
Train: 2018-08-02T12:04:40.929593: step 14535, loss 0.554534.
Train: 2018-08-02T12:04:41.148267: step 14536, loss 0.603251.
Train: 2018-08-02T12:04:41.382612: step 14537, loss 0.570797.
Train: 2018-08-02T12:04:41.616937: step 14538, loss 0.514216.
Train: 2018-08-02T12:04:41.851254: step 14539, loss 0.595045.
Train: 2018-08-02T12:04:42.085548: step 14540, loss 0.586952.
Test: 2018-08-02T12:04:43.288391: step 14540, loss 0.549125.
Train: 2018-08-02T12:04:43.522741: step 14541, loss 0.619161.
Train: 2018-08-02T12:04:43.757055: step 14542, loss 0.611014.
Train: 2018-08-02T12:04:43.991352: step 14543, loss 0.498775.
Train: 2018-08-02T12:04:44.225701: step 14544, loss 0.578863.
Train: 2018-08-02T12:04:44.459991: step 14545, loss 0.507004.
Train: 2018-08-02T12:04:44.694311: step 14546, loss 0.610793.
Train: 2018-08-02T12:04:44.928661: step 14547, loss 0.626708.
Train: 2018-08-02T12:04:45.162951: step 14548, loss 0.618642.
Train: 2018-08-02T12:04:45.397272: step 14549, loss 0.58679.
Train: 2018-08-02T12:04:45.631622: step 14550, loss 0.570955.
Test: 2018-08-02T12:04:46.818813: step 14550, loss 0.550299.
Train: 2018-08-02T12:04:47.037542: step 14551, loss 0.602512.
Train: 2018-08-02T12:04:47.256244: step 14552, loss 0.56316.
Train: 2018-08-02T12:04:47.490565: step 14553, loss 0.618042.
Train: 2018-08-02T12:04:47.724851: step 14554, loss 0.500852.
Train: 2018-08-02T12:04:47.959196: step 14555, loss 0.50878.
Train: 2018-08-02T12:04:48.193516: step 14556, loss 0.532153.
Train: 2018-08-02T12:04:48.427842: step 14557, loss 0.563298.
Train: 2018-08-02T12:04:48.677783: step 14558, loss 0.53986.
Train: 2018-08-02T12:04:48.912101: step 14559, loss 0.516332.
Train: 2018-08-02T12:04:49.146424: step 14560, loss 0.586717.
Test: 2018-08-02T12:04:50.333615: step 14560, loss 0.550032.
Train: 2018-08-02T12:04:50.552314: step 14561, loss 0.555298.
Train: 2018-08-02T12:04:50.802281: step 14562, loss 0.531612.
Train: 2018-08-02T12:04:51.036606: step 14563, loss 0.570963.
Train: 2018-08-02T12:04:51.270897: step 14564, loss 0.531343.
Train: 2018-08-02T12:04:51.505249: step 14565, loss 0.483522.
Train: 2018-08-02T12:04:51.739562: step 14566, loss 0.57886.
Train: 2018-08-02T12:04:51.973886: step 14567, loss 0.63497.
Train: 2018-08-02T12:04:52.208207: step 14568, loss 0.554776.
Train: 2018-08-02T12:04:52.426906: step 14569, loss 0.450102.
Train: 2018-08-02T12:04:52.661196: step 14570, loss 0.643559.
Test: 2018-08-02T12:04:53.848417: step 14570, loss 0.550095.
Train: 2018-08-02T12:04:54.082737: step 14571, loss 0.497877.
Train: 2018-08-02T12:04:54.317058: step 14572, loss 0.603294.
Train: 2018-08-02T12:04:54.535755: step 14573, loss 0.595213.
Train: 2018-08-02T12:04:54.754486: step 14574, loss 0.562598.
Train: 2018-08-02T12:04:54.988775: step 14575, loss 0.529907.
Train: 2018-08-02T12:04:55.223127: step 14576, loss 0.554407.
Train: 2018-08-02T12:04:55.441796: step 14577, loss 0.636393.
Train: 2018-08-02T12:04:55.676144: step 14578, loss 0.537828.
Train: 2018-08-02T12:04:55.910465: step 14579, loss 0.57893.
Train: 2018-08-02T12:04:56.129159: step 14580, loss 0.521303.
Test: 2018-08-02T12:04:57.347598: step 14580, loss 0.548137.
Train: 2018-08-02T12:04:57.566297: step 14581, loss 0.513319.
Train: 2018-08-02T12:04:57.800643: step 14582, loss 0.578298.
Train: 2018-08-02T12:04:58.034968: step 14583, loss 0.537701.
Train: 2018-08-02T12:04:58.253666: step 14584, loss 0.470249.
Train: 2018-08-02T12:04:58.487986: step 14585, loss 0.654022.
Train: 2018-08-02T12:04:58.722277: step 14586, loss 0.579792.
Train: 2018-08-02T12:04:58.956627: step 14587, loss 0.572305.
Train: 2018-08-02T12:04:59.190947: step 14588, loss 0.494708.
Train: 2018-08-02T12:04:59.425267: step 14589, loss 0.605687.
Train: 2018-08-02T12:04:59.643965: step 14590, loss 0.588321.
Test: 2018-08-02T12:05:00.846778: step 14590, loss 0.54796.
Train: 2018-08-02T12:05:01.081125: step 14591, loss 0.49552.
Train: 2018-08-02T12:05:01.299797: step 14592, loss 0.57927.
Train: 2018-08-02T12:05:01.534119: step 14593, loss 0.571463.
Train: 2018-08-02T12:05:01.768463: step 14594, loss 0.528943.
Train: 2018-08-02T12:05:02.002788: step 14595, loss 0.678457.
Train: 2018-08-02T12:05:02.237077: step 14596, loss 0.504753.
Train: 2018-08-02T12:05:02.455776: step 14597, loss 0.653196.
Train: 2018-08-02T12:05:02.690123: step 14598, loss 0.595411.
Train: 2018-08-02T12:05:02.924444: step 14599, loss 0.562572.
Train: 2018-08-02T12:05:03.143146: step 14600, loss 0.521761.
Test: 2018-08-02T12:05:04.361580: step 14600, loss 0.549002.
Train: 2018-08-02T12:05:05.220754: step 14601, loss 0.530004.
Train: 2018-08-02T12:05:05.455100: step 14602, loss 0.546336.
Train: 2018-08-02T12:05:05.689420: step 14603, loss 0.578921.
Train: 2018-08-02T12:05:05.923741: step 14604, loss 0.521958.
Train: 2018-08-02T12:05:06.158034: step 14605, loss 0.578918.
Train: 2018-08-02T12:05:06.392385: step 14606, loss 0.554506.
Train: 2018-08-02T12:05:06.626705: step 14607, loss 0.546367.
Train: 2018-08-02T12:05:06.861023: step 14608, loss 0.513794.
Train: 2018-08-02T12:05:07.095341: step 14609, loss 0.595231.
Train: 2018-08-02T12:05:07.329636: step 14610, loss 0.529991.
Test: 2018-08-02T12:05:08.516857: step 14610, loss 0.547838.
Train: 2018-08-02T12:05:08.751178: step 14611, loss 0.57077.
Train: 2018-08-02T12:05:08.969906: step 14612, loss 0.578941.
Train: 2018-08-02T12:05:09.204228: step 14613, loss 0.570767.
Train: 2018-08-02T12:05:09.438517: step 14614, loss 0.636196.
Train: 2018-08-02T12:05:09.672838: step 14615, loss 0.521764.
Train: 2018-08-02T12:05:09.891566: step 14616, loss 0.570771.
Train: 2018-08-02T12:05:10.125856: step 14617, loss 0.513649.
Train: 2018-08-02T12:05:10.360176: step 14618, loss 0.513615.
Train: 2018-08-02T12:05:10.594521: step 14619, loss 0.521703.
Train: 2018-08-02T12:05:10.813241: step 14620, loss 0.488809.
Test: 2018-08-02T12:05:12.000416: step 14620, loss 0.547854.
Train: 2018-08-02T12:05:12.234770: step 14621, loss 0.554311.
Train: 2018-08-02T12:05:12.469089: step 14622, loss 0.554255.
Train: 2018-08-02T12:05:12.719022: step 14623, loss 0.54593.
Train: 2018-08-02T12:05:12.953343: step 14624, loss 0.537558.
Train: 2018-08-02T12:05:13.172042: step 14625, loss 0.545785.
Train: 2018-08-02T12:05:13.406362: step 14626, loss 0.670982.
Train: 2018-08-02T12:05:13.625036: step 14627, loss 0.537351.
Train: 2018-08-02T12:05:13.843735: step 14628, loss 0.587495.
Train: 2018-08-02T12:05:14.078081: step 14629, loss 0.56241.
Train: 2018-08-02T12:05:14.312405: step 14630, loss 0.604219.
Test: 2018-08-02T12:05:15.515218: step 14630, loss 0.54808.
Train: 2018-08-02T12:05:15.733948: step 14631, loss 0.545709.
Train: 2018-08-02T12:05:15.983883: step 14632, loss 0.529027.
Train: 2018-08-02T12:05:16.202583: step 14633, loss 0.52068.
Train: 2018-08-02T12:05:16.436902: step 14634, loss 0.545708.
Train: 2018-08-02T12:05:16.671228: step 14635, loss 0.528967.
Train: 2018-08-02T12:05:16.905549: step 14636, loss 0.562404.
Train: 2018-08-02T12:05:17.139837: step 14637, loss 0.579161.
Train: 2018-08-02T12:05:17.374189: step 14638, loss 0.495306.
Train: 2018-08-02T12:05:17.608505: step 14639, loss 0.52878.
Train: 2018-08-02T12:05:17.827202: step 14640, loss 0.495044.
Test: 2018-08-02T12:05:19.014399: step 14640, loss 0.548653.
Train: 2018-08-02T12:05:19.248725: step 14641, loss 0.604574.
Train: 2018-08-02T12:05:19.483039: step 14642, loss 0.48624.
Train: 2018-08-02T12:05:19.717360: step 14643, loss 0.638695.
Train: 2018-08-02T12:05:19.951680: step 14644, loss 0.621801.
Train: 2018-08-02T12:05:20.186030: step 14645, loss 0.664157.
Train: 2018-08-02T12:05:20.420319: step 14646, loss 0.587749.
Train: 2018-08-02T12:05:20.623422: step 14647, loss 0.652387.
Train: 2018-08-02T12:05:20.857743: step 14648, loss 0.60437.
Train: 2018-08-02T12:05:21.092037: step 14649, loss 0.579122.
Train: 2018-08-02T12:05:21.326359: step 14650, loss 0.504271.
Test: 2018-08-02T12:05:22.513579: step 14650, loss 0.548072.
Train: 2018-08-02T12:05:22.747900: step 14651, loss 0.545912.
Train: 2018-08-02T12:05:22.982244: step 14652, loss 0.603787.
Train: 2018-08-02T12:05:23.200919: step 14653, loss 0.537843.
Train: 2018-08-02T12:05:23.419618: step 14654, loss 0.611793.
Train: 2018-08-02T12:05:23.669587: step 14655, loss 0.5953.
Train: 2018-08-02T12:05:23.903878: step 14656, loss 0.595216.
Train: 2018-08-02T12:05:24.138229: step 14657, loss 0.522114.
Train: 2018-08-02T12:05:24.356897: step 14658, loss 0.60316.
Train: 2018-08-02T12:05:24.591247: step 14659, loss 0.498261.
Train: 2018-08-02T12:05:24.825563: step 14660, loss 0.55473.
Test: 2018-08-02T12:05:26.012760: step 14660, loss 0.549001.
Train: 2018-08-02T12:05:26.231458: step 14661, loss 0.578872.
Train: 2018-08-02T12:05:26.450188: step 14662, loss 0.546761.
Train: 2018-08-02T12:05:26.668887: step 14663, loss 0.546787.
Train: 2018-08-02T12:05:26.903178: step 14664, loss 0.667058.
Train: 2018-08-02T12:05:27.137522: step 14665, loss 0.562868.
Train: 2018-08-02T12:05:27.371848: step 14666, loss 0.562899.
Train: 2018-08-02T12:05:27.606136: step 14667, loss 0.523098.
Train: 2018-08-02T12:05:27.840487: step 14668, loss 0.547017.
Train: 2018-08-02T12:05:28.074798: step 14669, loss 0.602737.
Train: 2018-08-02T12:05:28.309098: step 14670, loss 0.562954.
Test: 2018-08-02T12:05:29.496319: step 14670, loss 0.548047.
Train: 2018-08-02T12:05:29.715020: step 14671, loss 0.475551.
Train: 2018-08-02T12:05:29.949340: step 14672, loss 0.626606.
Train: 2018-08-02T12:05:30.183690: step 14673, loss 0.586817.
Train: 2018-08-02T12:05:30.417979: step 14674, loss 0.562949.
Train: 2018-08-02T12:05:30.636708: step 14675, loss 0.459569.
Train: 2018-08-02T12:05:30.871027: step 14676, loss 0.515103.
Train: 2018-08-02T12:05:31.105317: step 14677, loss 0.618832.
Train: 2018-08-02T12:05:31.339641: step 14678, loss 0.594878.
Train: 2018-08-02T12:05:31.589580: step 14679, loss 0.506741.
Train: 2018-08-02T12:05:31.823924: step 14680, loss 0.554779.
Test: 2018-08-02T12:05:33.026742: step 14680, loss 0.549342.
Train: 2018-08-02T12:05:33.229846: step 14681, loss 0.546687.
Train: 2018-08-02T12:05:33.464171: step 14682, loss 0.603078.
Train: 2018-08-02T12:05:33.714107: step 14683, loss 0.611188.
Train: 2018-08-02T12:05:33.948401: step 14684, loss 0.603116.
Train: 2018-08-02T12:05:34.182730: step 14685, loss 0.611168.
Train: 2018-08-02T12:05:34.417067: step 14686, loss 0.546646.
Train: 2018-08-02T12:05:34.651364: step 14687, loss 0.514476.
Train: 2018-08-02T12:05:34.901329: step 14688, loss 0.60303.
Train: 2018-08-02T12:05:35.135654: step 14689, loss 0.594967.
Train: 2018-08-02T12:05:35.354357: step 14690, loss 0.651197.
Test: 2018-08-02T12:05:36.557166: step 14690, loss 0.548905.
Train: 2018-08-02T12:05:36.775895: step 14691, loss 0.56284.
Train: 2018-08-02T12:05:36.994563: step 14692, loss 0.658787.
Train: 2018-08-02T12:05:37.228915: step 14693, loss 0.531108.
Train: 2018-08-02T12:05:37.447582: step 14694, loss 0.56299.
Train: 2018-08-02T12:05:37.666306: step 14695, loss 0.531379.
Train: 2018-08-02T12:05:37.900632: step 14696, loss 0.555157.
Train: 2018-08-02T12:05:38.134946: step 14697, loss 0.586756.
Train: 2018-08-02T12:05:38.353645: step 14698, loss 0.507923.
Train: 2018-08-02T12:05:38.587940: step 14699, loss 0.578865.
Train: 2018-08-02T12:05:38.822261: step 14700, loss 0.610399.
Test: 2018-08-02T12:05:40.025103: step 14700, loss 0.550503.
Train: 2018-08-02T12:05:40.868687: step 14701, loss 0.641878.
Train: 2018-08-02T12:05:41.103011: step 14702, loss 0.531725.
Train: 2018-08-02T12:05:41.352948: step 14703, loss 0.594568.
Train: 2018-08-02T12:05:41.571641: step 14704, loss 0.516205.
Train: 2018-08-02T12:05:41.805968: step 14705, loss 0.578878.
Train: 2018-08-02T12:05:42.024666: step 14706, loss 0.531908.
Train: 2018-08-02T12:05:42.243364: step 14707, loss 0.547547.
Train: 2018-08-02T12:05:42.477680: step 14708, loss 0.516151.
Train: 2018-08-02T12:05:42.712005: step 14709, loss 0.476725.
Train: 2018-08-02T12:05:42.946326: step 14710, loss 0.547306.
Test: 2018-08-02T12:05:44.149138: step 14710, loss 0.549423.
Train: 2018-08-02T12:05:44.367837: step 14711, loss 0.578861.
Train: 2018-08-02T12:05:44.617778: step 14712, loss 0.594758.
Train: 2018-08-02T12:05:44.820886: step 14713, loss 0.539013.
Train: 2018-08-02T12:05:45.055207: step 14714, loss 0.602841.
Train: 2018-08-02T12:05:45.289527: step 14715, loss 0.570854.
Train: 2018-08-02T12:05:45.523846: step 14716, loss 0.554813.
Train: 2018-08-02T12:05:45.758167: step 14717, loss 0.570839.
Train: 2018-08-02T12:05:45.976860: step 14718, loss 0.562794.
Train: 2018-08-02T12:05:46.211181: step 14719, loss 0.57082.
Train: 2018-08-02T12:05:46.445501: step 14720, loss 0.570814.
Test: 2018-08-02T12:05:47.632697: step 14720, loss 0.549641.
Train: 2018-08-02T12:05:47.929503: step 14721, loss 0.506265.
Train: 2018-08-02T12:05:48.163825: step 14722, loss 0.562717.
Train: 2018-08-02T12:05:48.398170: step 14723, loss 0.562698.
Train: 2018-08-02T12:05:48.632463: step 14724, loss 0.578902.
Train: 2018-08-02T12:05:48.866783: step 14725, loss 0.570784.
Train: 2018-08-02T12:05:49.101128: step 14726, loss 0.603312.
Train: 2018-08-02T12:05:49.335455: step 14727, loss 0.56265.
Train: 2018-08-02T12:05:49.569775: step 14728, loss 0.49761.
Train: 2018-08-02T12:05:49.804089: step 14729, loss 0.554498.
Train: 2018-08-02T12:05:50.022789: step 14730, loss 0.505549.
Test: 2018-08-02T12:05:51.225606: step 14730, loss 0.548004.
Train: 2018-08-02T12:05:51.444305: step 14731, loss 0.611628.
Train: 2018-08-02T12:05:51.678656: step 14732, loss 0.480757.
Train: 2018-08-02T12:05:51.897354: step 14733, loss 0.636392.
Train: 2018-08-02T12:05:52.131674: step 14734, loss 0.521484.
Train: 2018-08-02T12:05:52.350369: step 14735, loss 0.570762.
Train: 2018-08-02T12:05:52.584665: step 14736, loss 0.578984.
Train: 2018-08-02T12:05:52.818983: step 14737, loss 0.529559.
Train: 2018-08-02T12:05:53.053302: step 14738, loss 0.529487.
Train: 2018-08-02T12:05:53.287623: step 14739, loss 0.62036.
Train: 2018-08-02T12:05:53.506322: step 14740, loss 0.496306.
Test: 2018-08-02T12:05:54.709165: step 14740, loss 0.549398.
Train: 2018-08-02T12:05:54.927895: step 14741, loss 0.670166.
Train: 2018-08-02T12:05:55.162185: step 14742, loss 0.579012.
Train: 2018-08-02T12:05:55.396504: step 14743, loss 0.488072.
Train: 2018-08-02T12:05:55.662119: step 14744, loss 0.58733.
Train: 2018-08-02T12:05:55.896388: step 14745, loss 0.545963.
Train: 2018-08-02T12:05:56.115112: step 14746, loss 0.612115.
Train: 2018-08-02T12:05:56.349437: step 14747, loss 0.570747.
Train: 2018-08-02T12:05:56.583757: step 14748, loss 0.636783.
Train: 2018-08-02T12:05:56.818047: step 14749, loss 0.54606.
Train: 2018-08-02T12:05:57.052367: step 14750, loss 0.537893.
Test: 2018-08-02T12:05:58.239589: step 14750, loss 0.548664.
Train: 2018-08-02T12:05:58.473934: step 14751, loss 0.537932.
Train: 2018-08-02T12:05:58.708260: step 14752, loss 0.595365.
Train: 2018-08-02T12:05:58.942579: step 14753, loss 0.521638.
Train: 2018-08-02T12:05:59.176895: step 14754, loss 0.61987.
Train: 2018-08-02T12:05:59.395598: step 14755, loss 0.562594.
Train: 2018-08-02T12:05:59.629912: step 14756, loss 0.513648.
Train: 2018-08-02T12:05:59.864238: step 14757, loss 0.619714.
Train: 2018-08-02T12:06:00.098528: step 14758, loss 0.521904.
Train: 2018-08-02T12:06:00.332879: step 14759, loss 0.554478.
Train: 2018-08-02T12:06:00.567196: step 14760, loss 0.660334.
Test: 2018-08-02T12:06:01.770011: step 14760, loss 0.549281.
Train: 2018-08-02T12:06:01.973119: step 14761, loss 0.603273.
Train: 2018-08-02T12:06:02.207410: step 14762, loss 0.530297.
Train: 2018-08-02T12:06:02.441731: step 14763, loss 0.53846.
Train: 2018-08-02T12:06:02.676051: step 14764, loss 0.554652.
Train: 2018-08-02T12:06:02.910395: step 14765, loss 0.546589.
Train: 2018-08-02T12:06:03.144689: step 14766, loss 0.603092.
Train: 2018-08-02T12:06:03.379053: step 14767, loss 0.570821.
Train: 2018-08-02T12:06:03.597738: step 14768, loss 0.578894.
Train: 2018-08-02T12:06:03.832059: step 14769, loss 0.546704.
Train: 2018-08-02T12:06:04.066367: step 14770, loss 0.546736.
Test: 2018-08-02T12:06:05.269192: step 14770, loss 0.549388.
Train: 2018-08-02T12:06:05.487917: step 14771, loss 0.570839.
Train: 2018-08-02T12:06:05.722213: step 14772, loss 0.538724.
Train: 2018-08-02T12:06:05.956531: step 14773, loss 0.578873.
Train: 2018-08-02T12:06:06.190882: step 14774, loss 0.586891.
Train: 2018-08-02T12:06:06.425203: step 14775, loss 0.538721.
Train: 2018-08-02T12:06:06.659492: step 14776, loss 0.530694.
Train: 2018-08-02T12:06:06.878191: step 14777, loss 0.546709.
Train: 2018-08-02T12:06:07.112544: step 14778, loss 0.562787.
Train: 2018-08-02T12:06:07.346863: step 14779, loss 0.562771.
Train: 2018-08-02T12:06:07.581176: step 14780, loss 0.554669.
Test: 2018-08-02T12:06:08.768375: step 14780, loss 0.549614.
Train: 2018-08-02T12:06:09.002693: step 14781, loss 0.522331.
Train: 2018-08-02T12:06:09.221421: step 14782, loss 0.546516.
Train: 2018-08-02T12:06:09.455743: step 14783, loss 0.522114.
Train: 2018-08-02T12:06:09.690032: step 14784, loss 0.554466.
Train: 2018-08-02T12:06:09.940004: step 14785, loss 0.627921.
Train: 2018-08-02T12:06:10.174324: step 14786, loss 0.579129.
Train: 2018-08-02T12:06:10.392993: step 14787, loss 0.464541.
Train: 2018-08-02T12:06:10.627343: step 14788, loss 0.52977.
Train: 2018-08-02T12:06:10.861659: step 14789, loss 0.48033.
Train: 2018-08-02T12:06:11.095984: step 14790, loss 0.570818.
Test: 2018-08-02T12:06:12.283174: step 14790, loss 0.548057.
Train: 2018-08-02T12:06:12.517495: step 14791, loss 0.51272.
Train: 2018-08-02T12:06:12.751845: step 14792, loss 0.587329.
Train: 2018-08-02T12:06:12.986166: step 14793, loss 0.520697.
Train: 2018-08-02T12:06:13.220457: step 14794, loss 0.503534.
Train: 2018-08-02T12:06:13.454775: step 14795, loss 0.56246.
Train: 2018-08-02T12:06:13.689110: step 14796, loss 0.579492.
Train: 2018-08-02T12:06:13.923446: step 14797, loss 0.536987.
Train: 2018-08-02T12:06:14.095251: step 14798, loss 0.50781.
Train: 2018-08-02T12:06:14.345217: step 14799, loss 0.510903.
Train: 2018-08-02T12:06:14.563891: step 14800, loss 0.536354.
Test: 2018-08-02T12:06:15.751113: step 14800, loss 0.548907.
Train: 2018-08-02T12:06:16.735287: step 14801, loss 0.527718.
Train: 2018-08-02T12:06:16.953986: step 14802, loss 0.605832.
Train: 2018-08-02T12:06:17.188306: step 14803, loss 0.571067.
Train: 2018-08-02T12:06:17.422602: step 14804, loss 0.579861.
Train: 2018-08-02T12:06:17.656941: step 14805, loss 0.615459.
Train: 2018-08-02T12:06:17.891236: step 14806, loss 0.553454.
Train: 2018-08-02T12:06:18.125588: step 14807, loss 0.553665.
Train: 2018-08-02T12:06:18.359907: step 14808, loss 0.588396.
Train: 2018-08-02T12:06:18.594222: step 14809, loss 0.536358.
Train: 2018-08-02T12:06:18.828548: step 14810, loss 0.570966.
Test: 2018-08-02T12:06:20.000118: step 14810, loss 0.547961.
Train: 2018-08-02T12:06:20.234463: step 14811, loss 0.545142.
Train: 2018-08-02T12:06:20.453167: step 14812, loss 0.63972.
Train: 2018-08-02T12:06:20.687486: step 14813, loss 0.605177.
Train: 2018-08-02T12:06:20.921779: step 14814, loss 0.562376.
Train: 2018-08-02T12:06:21.156129: step 14815, loss 0.61333.
Train: 2018-08-02T12:06:21.390448: step 14816, loss 0.536972.
Train: 2018-08-02T12:06:21.624768: step 14817, loss 0.553937.
Train: 2018-08-02T12:06:21.859058: step 14818, loss 0.503535.
Train: 2018-08-02T12:06:22.093408: step 14819, loss 0.621135.
Train: 2018-08-02T12:06:22.327698: step 14820, loss 0.646086.
Test: 2018-08-02T12:06:23.499298: step 14820, loss 0.548901.
Train: 2018-08-02T12:06:23.733625: step 14821, loss 0.537438.
Train: 2018-08-02T12:06:23.952317: step 14822, loss 0.570759.
Train: 2018-08-02T12:06:24.186670: step 14823, loss 0.570756.
Train: 2018-08-02T12:06:24.420966: step 14824, loss 0.595496.
Train: 2018-08-02T12:06:24.655307: step 14825, loss 0.554329.
Train: 2018-08-02T12:06:24.858386: step 14826, loss 0.578953.
Train: 2018-08-02T12:06:25.108327: step 14827, loss 0.595258.
Train: 2018-08-02T12:06:25.358263: step 14828, loss 0.603311.
Train: 2018-08-02T12:06:25.592583: step 14829, loss 0.530304.
Train: 2018-08-02T12:06:25.826908: step 14830, loss 0.595034.
Test: 2018-08-02T12:06:27.014100: step 14830, loss 0.549712.
Train: 2018-08-02T12:06:27.248445: step 14831, loss 0.570828.
Train: 2018-08-02T12:06:27.482767: step 14832, loss 0.58689.
Train: 2018-08-02T12:06:27.732681: step 14833, loss 0.506902.
Train: 2018-08-02T12:06:27.967035: step 14834, loss 0.570876.
Train: 2018-08-02T12:06:28.201352: step 14835, loss 0.507132.
Train: 2018-08-02T12:06:28.435643: step 14836, loss 0.610743.
Train: 2018-08-02T12:06:28.685583: step 14837, loss 0.467385.
Train: 2018-08-02T12:06:28.904313: step 14838, loss 0.491125.
Train: 2018-08-02T12:06:29.138634: step 14839, loss 0.578808.
Train: 2018-08-02T12:06:29.372954: step 14840, loss 0.538772.
Test: 2018-08-02T12:06:30.560144: step 14840, loss 0.549126.
Train: 2018-08-02T12:06:30.794492: step 14841, loss 0.530478.
Train: 2018-08-02T12:06:31.028786: step 14842, loss 0.619469.
Train: 2018-08-02T12:06:31.247484: step 14843, loss 0.514037.
Train: 2018-08-02T12:06:31.481835: step 14844, loss 0.538091.
Train: 2018-08-02T12:06:31.716126: step 14845, loss 0.570567.
Train: 2018-08-02T12:06:31.966066: step 14846, loss 0.530178.
Train: 2018-08-02T12:06:32.184794: step 14847, loss 0.554538.
Train: 2018-08-02T12:06:32.403493: step 14848, loss 0.55415.
Train: 2018-08-02T12:06:32.622163: step 14849, loss 0.644975.
Train: 2018-08-02T12:06:32.856508: step 14850, loss 0.645978.
Test: 2018-08-02T12:06:34.043704: step 14850, loss 0.549428.
Train: 2018-08-02T12:06:34.278050: step 14851, loss 0.562705.
Train: 2018-08-02T12:06:34.527967: step 14852, loss 0.604047.
Train: 2018-08-02T12:06:34.762316: step 14853, loss 0.636125.
Train: 2018-08-02T12:06:34.996636: step 14854, loss 0.562612.
Train: 2018-08-02T12:06:35.230963: step 14855, loss 0.554598.
Train: 2018-08-02T12:06:35.465272: step 14856, loss 0.530415.
Train: 2018-08-02T12:06:35.699598: step 14857, loss 0.586947.
Train: 2018-08-02T12:06:35.933887: step 14858, loss 0.546676.
Train: 2018-08-02T12:06:36.168237: step 14859, loss 0.514557.
Train: 2018-08-02T12:06:36.418173: step 14860, loss 0.611028.
Test: 2018-08-02T12:06:37.605370: step 14860, loss 0.549029.
Train: 2018-08-02T12:06:37.839721: step 14861, loss 0.546749.
Train: 2018-08-02T12:06:38.074041: step 14862, loss 0.602949.
Train: 2018-08-02T12:06:38.308365: step 14863, loss 0.546804.
Train: 2018-08-02T12:06:38.542676: step 14864, loss 0.546826.
Train: 2018-08-02T12:06:38.776996: step 14865, loss 0.554841.
Train: 2018-08-02T12:06:39.011291: step 14866, loss 0.586875.
Train: 2018-08-02T12:06:39.245611: step 14867, loss 0.602882.
Train: 2018-08-02T12:06:39.475370: step 14868, loss 0.578861.
Train: 2018-08-02T12:06:39.725314: step 14869, loss 0.594833.
Train: 2018-08-02T12:06:39.959666: step 14870, loss 0.570887.
Test: 2018-08-02T12:06:41.146857: step 14870, loss 0.549647.
Train: 2018-08-02T12:06:41.349934: step 14871, loss 0.666413.
Train: 2018-08-02T12:06:41.584281: step 14872, loss 0.499553.
Train: 2018-08-02T12:06:41.818581: step 14873, loss 0.515525.
Train: 2018-08-02T12:06:42.068546: step 14874, loss 0.618429.
Train: 2018-08-02T12:06:42.302861: step 14875, loss 0.539348.
Train: 2018-08-02T12:06:42.537186: step 14876, loss 0.563065.
Train: 2018-08-02T12:06:42.771501: step 14877, loss 0.5236.
Train: 2018-08-02T12:06:43.005822: step 14878, loss 0.578859.
Train: 2018-08-02T12:06:43.240149: step 14879, loss 0.539348.
Train: 2018-08-02T12:06:43.474463: step 14880, loss 0.586775.
Test: 2018-08-02T12:06:44.677280: step 14880, loss 0.550898.
Train: 2018-08-02T12:06:44.895979: step 14881, loss 0.555109.
Train: 2018-08-02T12:06:45.130326: step 14882, loss 0.563026.
Train: 2018-08-02T12:06:45.348999: step 14883, loss 0.547147.
Train: 2018-08-02T12:06:45.583318: step 14884, loss 0.539169.
Train: 2018-08-02T12:06:45.817668: step 14885, loss 0.570906.
Train: 2018-08-02T12:06:46.051990: step 14886, loss 0.539033.
Train: 2018-08-02T12:06:46.301926: step 14887, loss 0.626742.
Train: 2018-08-02T12:06:46.520629: step 14888, loss 0.586855.
Train: 2018-08-02T12:06:46.754949: step 14889, loss 0.594846.
Train: 2018-08-02T12:06:46.989271: step 14890, loss 0.475091.
Test: 2018-08-02T12:06:48.176460: step 14890, loss 0.549335.
Train: 2018-08-02T12:06:48.395160: step 14891, loss 0.522885.
Train: 2018-08-02T12:06:48.629510: step 14892, loss 0.546805.
Train: 2018-08-02T12:06:48.863840: step 14893, loss 0.538674.
Train: 2018-08-02T12:06:49.098121: step 14894, loss 0.530499.
Train: 2018-08-02T12:06:49.332468: step 14895, loss 0.570801.
Train: 2018-08-02T12:06:49.582416: step 14896, loss 0.49758.
Train: 2018-08-02T12:06:49.816733: step 14897, loss 0.546686.
Train: 2018-08-02T12:06:50.051052: step 14898, loss 0.603585.
Train: 2018-08-02T12:06:50.285374: step 14899, loss 0.513224.
Train: 2018-08-02T12:06:50.519693: step 14900, loss 0.554024.
Test: 2018-08-02T12:06:51.706884: step 14900, loss 0.547442.
Train: 2018-08-02T12:06:52.628575: step 14901, loss 0.58732.
Train: 2018-08-02T12:06:52.862864: step 14902, loss 0.571123.
Train: 2018-08-02T12:06:53.081588: step 14903, loss 0.628793.
Train: 2018-08-02T12:06:53.315907: step 14904, loss 0.512325.
Train: 2018-08-02T12:06:53.550233: step 14905, loss 0.537082.
Train: 2018-08-02T12:06:53.784548: step 14906, loss 0.561982.
Train: 2018-08-02T12:06:54.018842: step 14907, loss 0.503733.
Train: 2018-08-02T12:06:54.253195: step 14908, loss 0.477326.
Train: 2018-08-02T12:06:54.487514: step 14909, loss 0.449861.
Train: 2018-08-02T12:06:54.721836: step 14910, loss 0.491321.
Test: 2018-08-02T12:06:55.909024: step 14910, loss 0.54776.
Train: 2018-08-02T12:06:56.143345: step 14911, loss 0.545318.
Train: 2018-08-02T12:06:56.377690: step 14912, loss 0.620205.
Train: 2018-08-02T12:06:56.612016: step 14913, loss 0.542039.
Train: 2018-08-02T12:06:56.846333: step 14914, loss 0.604285.
Train: 2018-08-02T12:06:57.080656: step 14915, loss 0.634813.
Train: 2018-08-02T12:06:57.314976: step 14916, loss 0.558781.
Train: 2018-08-02T12:06:57.564917: step 14917, loss 0.690315.
Train: 2018-08-02T12:06:57.799208: step 14918, loss 0.573253.
Train: 2018-08-02T12:06:58.033558: step 14919, loss 0.544956.
Train: 2018-08-02T12:06:58.267848: step 14920, loss 0.487163.
Test: 2018-08-02T12:06:59.486312: step 14920, loss 0.547526.
Train: 2018-08-02T12:06:59.705017: step 14921, loss 0.51168.
Train: 2018-08-02T12:06:59.939356: step 14922, loss 0.554584.
Train: 2018-08-02T12:07:00.173681: step 14923, loss 0.553865.
Train: 2018-08-02T12:07:00.407971: step 14924, loss 0.604058.
Train: 2018-08-02T12:07:00.642322: step 14925, loss 0.520745.
Train: 2018-08-02T12:07:00.861015: step 14926, loss 0.612336.
Train: 2018-08-02T12:07:01.095344: step 14927, loss 0.562444.
Train: 2018-08-02T12:07:01.329659: step 14928, loss 0.579049.
Train: 2018-08-02T12:07:01.563984: step 14929, loss 0.529392.
Train: 2018-08-02T12:07:01.829538: step 14930, loss 0.545975.
Test: 2018-08-02T12:07:03.016735: step 14930, loss 0.549092.
Train: 2018-08-02T12:07:03.251086: step 14931, loss 0.545986.
Train: 2018-08-02T12:07:03.485401: step 14932, loss 0.545995.
Train: 2018-08-02T12:07:03.719729: step 14933, loss 0.521234.
Train: 2018-08-02T12:07:03.954047: step 14934, loss 0.603795.
Train: 2018-08-02T12:07:04.188366: step 14935, loss 0.521215.
Train: 2018-08-02T12:07:04.422683: step 14936, loss 0.59554.
Train: 2018-08-02T12:07:04.656976: step 14937, loss 0.570756.
Train: 2018-08-02T12:07:04.891328: step 14938, loss 0.595523.
Train: 2018-08-02T12:07:05.141268: step 14939, loss 0.53777.
Train: 2018-08-02T12:07:05.391206: step 14940, loss 0.554273.
Test: 2018-08-02T12:07:06.578401: step 14940, loss 0.548947.
Train: 2018-08-02T12:07:06.812752: step 14941, loss 0.587234.
Train: 2018-08-02T12:07:07.047042: step 14942, loss 0.611916.
Train: 2018-08-02T12:07:07.281387: step 14943, loss 0.537892.
Train: 2018-08-02T12:07:07.515684: step 14944, loss 0.554346.
Train: 2018-08-02T12:07:07.750003: step 14945, loss 0.496968.
Train: 2018-08-02T12:07:07.984353: step 14946, loss 0.529745.
Train: 2018-08-02T12:07:08.218644: step 14947, loss 0.562545.
Train: 2018-08-02T12:07:08.452962: step 14948, loss 0.513212.
Train: 2018-08-02T12:07:08.640449: step 14949, loss 0.685498.
Train: 2018-08-02T12:07:08.874770: step 14950, loss 0.570752.
Test: 2018-08-02T12:07:10.061961: step 14950, loss 0.54843.
Train: 2018-08-02T12:07:10.296311: step 14951, loss 0.628328.
Train: 2018-08-02T12:07:10.530626: step 14952, loss 0.554364.
Train: 2018-08-02T12:07:10.764920: step 14953, loss 0.488922.
Train: 2018-08-02T12:07:10.999256: step 14954, loss 0.570766.
Train: 2018-08-02T12:07:11.233579: step 14955, loss 0.529872.
Train: 2018-08-02T12:07:11.467881: step 14956, loss 0.505322.
Train: 2018-08-02T12:07:11.717856: step 14957, loss 0.497036.
Train: 2018-08-02T12:07:11.952169: step 14958, loss 0.562543.
Train: 2018-08-02T12:07:12.170867: step 14959, loss 0.537826.
Train: 2018-08-02T12:07:12.405193: step 14960, loss 0.578998.
Test: 2018-08-02T12:07:13.592385: step 14960, loss 0.549435.
Train: 2018-08-02T12:07:13.826705: step 14961, loss 0.554178.
Train: 2018-08-02T12:07:14.061055: step 14962, loss 0.612216.
Train: 2018-08-02T12:07:14.295375: step 14963, loss 0.595658.
Train: 2018-08-02T12:07:14.529695: step 14964, loss 0.512676.
Train: 2018-08-02T12:07:14.764010: step 14965, loss 0.579153.
Train: 2018-08-02T12:07:15.013927: step 14966, loss 0.537556.
Train: 2018-08-02T12:07:15.248277: step 14967, loss 0.537564.
Train: 2018-08-02T12:07:15.482597: step 14968, loss 0.479329.
Train: 2018-08-02T12:07:15.716887: step 14969, loss 0.554092.
Train: 2018-08-02T12:07:15.966853: step 14970, loss 0.50397.
Test: 2018-08-02T12:07:17.154050: step 14970, loss 0.547814.
Train: 2018-08-02T12:07:17.388401: step 14971, loss 0.553948.
Train: 2018-08-02T12:07:17.607068: step 14972, loss 0.537017.
Train: 2018-08-02T12:07:17.841417: step 14973, loss 0.553945.
Train: 2018-08-02T12:07:18.075710: step 14974, loss 0.528817.
Train: 2018-08-02T12:07:18.310054: step 14975, loss 0.580185.
Train: 2018-08-02T12:07:18.544350: step 14976, loss 0.553617.
Train: 2018-08-02T12:07:18.778700: step 14977, loss 0.613553.
Train: 2018-08-02T12:07:19.013015: step 14978, loss 0.511279.
Train: 2018-08-02T12:07:19.262931: step 14979, loss 0.528332.
Train: 2018-08-02T12:07:19.497277: step 14980, loss 0.579385.
Test: 2018-08-02T12:07:20.684473: step 14980, loss 0.548097.
Train: 2018-08-02T12:07:20.918793: step 14981, loss 0.553806.
Train: 2018-08-02T12:07:21.153115: step 14982, loss 0.579447.
Train: 2018-08-02T12:07:21.387464: step 14983, loss 0.596473.
Train: 2018-08-02T12:07:21.637406: step 14984, loss 0.587931.
Train: 2018-08-02T12:07:21.871725: step 14985, loss 0.647491.
Train: 2018-08-02T12:07:22.106015: step 14986, loss 0.477472.
Train: 2018-08-02T12:07:22.324745: step 14987, loss 0.604734.
Train: 2018-08-02T12:07:22.559060: step 14988, loss 0.570819.
Train: 2018-08-02T12:07:22.793355: step 14989, loss 0.52861.
Train: 2018-08-02T12:07:23.027674: step 14990, loss 0.528665.
Test: 2018-08-02T12:07:24.214896: step 14990, loss 0.548897.
Train: 2018-08-02T12:07:24.433596: step 14991, loss 0.51186.
Train: 2018-08-02T12:07:24.667916: step 14992, loss 0.579215.
Train: 2018-08-02T12:07:24.902235: step 14993, loss 0.528713.
Train: 2018-08-02T12:07:25.136581: step 14994, loss 0.62129.
Train: 2018-08-02T12:07:25.370906: step 14995, loss 0.520354.
Train: 2018-08-02T12:07:25.605196: step 14996, loss 0.503556.
Train: 2018-08-02T12:07:25.839517: step 14997, loss 0.520324.
Train: 2018-08-02T12:07:26.089491: step 14998, loss 0.596059.
Train: 2018-08-02T12:07:26.323808: step 14999, loss 0.621339.
Train: 2018-08-02T12:07:26.558097: step 15000, loss 0.545543.
Test: 2018-08-02T12:07:27.760941: step 15000, loss 0.547755.
Train: 2018-08-02T12:07:28.729495: step 15001, loss 0.663303.
Train: 2018-08-02T12:07:28.963818: step 15002, loss 0.579172.
Train: 2018-08-02T12:07:29.213726: step 15003, loss 0.595865.
Train: 2018-08-02T12:07:29.448051: step 15004, loss 0.612437.
Train: 2018-08-02T12:07:29.682396: step 15005, loss 0.54586.
Train: 2018-08-02T12:07:29.916717: step 15006, loss 0.554207.
Train: 2018-08-02T12:07:30.151037: step 15007, loss 0.59549.
Train: 2018-08-02T12:07:30.385356: step 15008, loss 0.488561.
Train: 2018-08-02T12:07:30.619672: step 15009, loss 0.521491.
Train: 2018-08-02T12:07:30.853998: step 15010, loss 0.513262.
Test: 2018-08-02T12:07:32.056810: step 15010, loss 0.548984.
Train: 2018-08-02T12:07:32.275508: step 15011, loss 0.54617.
Train: 2018-08-02T12:07:32.509829: step 15012, loss 0.529515.
Train: 2018-08-02T12:07:32.728553: step 15013, loss 0.521507.
Train: 2018-08-02T12:07:32.962881: step 15014, loss 0.537279.
Train: 2018-08-02T12:07:33.197197: step 15015, loss 0.61433.
Train: 2018-08-02T12:07:33.431488: step 15016, loss 0.537286.
Train: 2018-08-02T12:07:33.665838: step 15017, loss 0.637438.
Train: 2018-08-02T12:07:33.900154: step 15018, loss 0.562573.
Train: 2018-08-02T12:07:34.134449: step 15019, loss 0.595802.
Train: 2018-08-02T12:07:34.368769: step 15020, loss 0.570805.
Test: 2018-08-02T12:07:35.571612: step 15020, loss 0.549339.
Train: 2018-08-02T12:07:35.805957: step 15021, loss 0.546044.
Train: 2018-08-02T12:07:36.024661: step 15022, loss 0.570774.
Train: 2018-08-02T12:07:36.258951: step 15023, loss 0.587153.
Train: 2018-08-02T12:07:36.493301: step 15024, loss 0.537992.
Train: 2018-08-02T12:07:36.727622: step 15025, loss 0.587097.
Train: 2018-08-02T12:07:36.961911: step 15026, loss 0.611611.
Train: 2018-08-02T12:07:37.196256: step 15027, loss 0.603506.
Train: 2018-08-02T12:07:37.430582: step 15028, loss 0.586974.
Train: 2018-08-02T12:07:37.680518: step 15029, loss 0.538319.
Train: 2018-08-02T12:07:37.914817: step 15030, loss 0.505948.
Test: 2018-08-02T12:07:39.117656: step 15030, loss 0.548786.
Train: 2018-08-02T12:07:39.351976: step 15031, loss 0.497811.
Train: 2018-08-02T12:07:39.586330: step 15032, loss 0.627514.
Train: 2018-08-02T12:07:39.820645: step 15033, loss 0.643733.
Train: 2018-08-02T12:07:40.054964: step 15034, loss 0.5462.
Train: 2018-08-02T12:07:40.304909: step 15035, loss 0.489492.
Train: 2018-08-02T12:07:40.539199: step 15036, loss 0.489242.
Train: 2018-08-02T12:07:40.773567: step 15037, loss 0.521521.
Train: 2018-08-02T12:07:41.007868: step 15038, loss 0.578523.
Train: 2018-08-02T12:07:41.242160: step 15039, loss 0.538456.
Train: 2018-08-02T12:07:41.476479: step 15040, loss 0.596688.
Test: 2018-08-02T12:07:42.679322: step 15040, loss 0.547852.
Train: 2018-08-02T12:07:42.898051: step 15041, loss 0.593511.
Train: 2018-08-02T12:07:43.116745: step 15042, loss 0.579158.
Train: 2018-08-02T12:07:43.351071: step 15043, loss 0.528211.
Train: 2018-08-02T12:07:43.601007: step 15044, loss 0.56806.
Train: 2018-08-02T12:07:43.835335: step 15045, loss 0.501792.
Train: 2018-08-02T12:07:44.069652: step 15046, loss 0.620938.
Train: 2018-08-02T12:07:44.303967: step 15047, loss 0.594162.
Train: 2018-08-02T12:07:44.569530: step 15048, loss 0.563015.
Train: 2018-08-02T12:07:44.803825: step 15049, loss 0.543729.
Train: 2018-08-02T12:07:45.038145: step 15050, loss 0.633314.
Test: 2018-08-02T12:07:46.240988: step 15050, loss 0.548387.
Train: 2018-08-02T12:07:46.459712: step 15051, loss 0.526632.
Train: 2018-08-02T12:07:46.694009: step 15052, loss 0.53927.
Train: 2018-08-02T12:07:46.943979: step 15053, loss 0.53802.
Train: 2018-08-02T12:07:47.178300: step 15054, loss 0.58435.
Train: 2018-08-02T12:07:47.412613: step 15055, loss 0.587787.
Train: 2018-08-02T12:07:47.646909: step 15056, loss 0.61213.
Train: 2018-08-02T12:07:47.896876: step 15057, loss 0.578339.
Train: 2018-08-02T12:07:48.131170: step 15058, loss 0.578643.
Train: 2018-08-02T12:07:48.365522: step 15059, loss 0.587206.
Train: 2018-08-02T12:07:48.599837: step 15060, loss 0.602814.
Test: 2018-08-02T12:07:49.802654: step 15060, loss 0.54964.
Train: 2018-08-02T12:07:50.115081: step 15061, loss 0.554532.
Train: 2018-08-02T12:07:50.349432: step 15062, loss 0.554649.
Train: 2018-08-02T12:07:50.583721: step 15063, loss 0.523066.
Train: 2018-08-02T12:07:50.818072: step 15064, loss 0.63421.
Train: 2018-08-02T12:07:51.052388: step 15065, loss 0.54714.
Train: 2018-08-02T12:07:51.302329: step 15066, loss 0.539772.
Train: 2018-08-02T12:07:51.536654: step 15067, loss 0.555751.
Train: 2018-08-02T12:07:51.770969: step 15068, loss 0.594449.
Train: 2018-08-02T12:07:52.005295: step 15069, loss 0.602094.
Train: 2018-08-02T12:07:52.255232: step 15070, loss 0.516127.
Test: 2018-08-02T12:07:53.442427: step 15070, loss 0.548845.
Train: 2018-08-02T12:07:53.661156: step 15071, loss 0.633284.
Train: 2018-08-02T12:07:53.895447: step 15072, loss 0.539628.
Train: 2018-08-02T12:07:54.129797: step 15073, loss 0.59466.
Train: 2018-08-02T12:07:54.364087: step 15074, loss 0.531525.
Train: 2018-08-02T12:07:54.614028: step 15075, loss 0.602456.
Train: 2018-08-02T12:07:54.848378: step 15076, loss 0.570806.
Train: 2018-08-02T12:07:55.082693: step 15077, loss 0.594658.
Train: 2018-08-02T12:07:55.317013: step 15078, loss 0.594655.
Train: 2018-08-02T12:07:55.566930: step 15079, loss 0.539938.
Train: 2018-08-02T12:07:55.801250: step 15080, loss 0.618411.
Test: 2018-08-02T12:07:57.004093: step 15080, loss 0.550314.
Train: 2018-08-02T12:07:57.222823: step 15081, loss 0.509371.
Train: 2018-08-02T12:07:57.457137: step 15082, loss 0.57884.
Train: 2018-08-02T12:07:57.707056: step 15083, loss 0.563413.
Train: 2018-08-02T12:07:57.925777: step 15084, loss 0.563333.
Train: 2018-08-02T12:07:58.160100: step 15085, loss 0.555787.
Train: 2018-08-02T12:07:58.394418: step 15086, loss 0.570822.
Train: 2018-08-02T12:07:58.628712: step 15087, loss 0.508514.
Train: 2018-08-02T12:07:58.878680: step 15088, loss 0.555869.
Train: 2018-08-02T12:07:59.112975: step 15089, loss 0.5472.
Train: 2018-08-02T12:07:59.347294: step 15090, loss 0.556192.
Test: 2018-08-02T12:08:00.534516: step 15090, loss 0.549043.
Train: 2018-08-02T12:08:00.768861: step 15091, loss 0.539413.
Train: 2018-08-02T12:08:01.003203: step 15092, loss 0.53151.
Train: 2018-08-02T12:08:01.253129: step 15093, loss 0.547072.
Train: 2018-08-02T12:08:01.503066: step 15094, loss 0.602569.
Train: 2018-08-02T12:08:01.768633: step 15095, loss 0.546821.
Train: 2018-08-02T12:08:02.002952: step 15096, loss 0.61883.
Train: 2018-08-02T12:08:02.237244: step 15097, loss 0.530006.
Train: 2018-08-02T12:08:02.471565: step 15098, loss 0.531953.
Train: 2018-08-02T12:08:02.737151: step 15099, loss 0.562674.
Train: 2018-08-02T12:08:02.940204: step 15100, loss 0.648063.
Test: 2018-08-02T12:08:04.143046: step 15100, loss 0.54857.
